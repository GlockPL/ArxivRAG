{"title": "Graph Neural Networks at a Fraction", "authors": ["Rucha Bhalchandra Joshi", "Sagar Prakash Barad", "Nidhi Tiwari", "Subhankar Mishra"], "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of graph-structured data. In addition to real-valued GNNs, quaternion GNNs also perform well on tasks on graph-structured data. With the aim of reducing the energy footprint, we reduce the model size while maintaining accuracy comparable to that of the original-sized GNNs. This paper introduces Quaternion Message Passing Neural Networks (QMPNNs), a framework that leverages quaternion space to compute node representations. Our approach offers a generalizable method for incorporating quaternion representations into GNN architectures at one-fourth of the original parameter count. Furthermore, we present a novel perspective on Graph Lottery Tickets, redefining their applicability within the context of GNNs and QMPNNs. We specifically aim to find the initialization lottery from the subnetwork of the GNNS that can achieve comparable performance to the original GNN upon training. Thereby reducing the trainable model parameters even further. To validate the effectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs, we evaluate their performance on real-world datasets across three fundamental graph-based tasks: node classification, link prediction, and graph classification. Our code is available at project's GitHub repository.", "sections": [{"title": "1 Introduction", "content": "Quaternions, a hypercomplex number system, offer several advantages over traditional real and complex numbers, making them well-suited for various tasks in deep learning. Incorporating quaternions into deep neural networks holds significant promise for enhancing their expressive power and versatility. When used in a quaternion setting, Quaternion deep neural networks perform better than"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Notations and Definitions", "content": "We use the following notations in this paper. A graph G = {V,E} has a set of vertices V and a set of edges E. The feature vector corresponding to a node v \u2208 V is given as x in RF. The feature matrix for the graph is given as X \u2208 R|V|\u00d7F. The edge eij = (vi, vj) \u2208 E if and only if there is an edge between two nodes vi and vj. In the adjacency matrix representation A \u2208 R|V|\u00d7|| of the graph, the Aij = 1 if eij \u2208 E, and 0 otherwise.\nThe intermediate representations in layer I corresponding to a node v are given by h. Similarly, we denote representations in layer I for node v by hl),, using the additional superscript Q to represent quaternion."}, {"title": "2.2 Quaternions", "content": "The set of quaternions is denoted by H. A quaternion q is denoted as q = qr + qii + qjj + qkk, where qr, qi, qj, qk \u2208 R and i, j, k are imaginary units. They follow the relation i\u00b2 = j\u00b2 = k\u00b2 = ijk = \u22121.\nAddition: The quaternion addition of two quaternoons q and p is component-wise: (qr + qii + qjj + qkk) + (pr + pii + Pjj + pkk) = (qr + Pr) + (qi + Pi)i + (qj +\nPj)j + (qk + Pk)k."}, {"title": "Multiplication", "content": "Quaternions can be multiplied by each other. The multiplication is associative, i.e., (pq)r = p(qr) for p,q,r \u2208 H. It is also distributive, i.e., (p+q)r = pr+qr. However, the multiplication is not commutative, i.e., pq \u2260 qp. When multiplied with the scalar \u5165, it gives q = \\qr + dq;i + Aqjj + Xqkk. Multiplication of two quaternions q and p is defined by the Hamilton product qp. This is given in the matrix form as follows:"}, {"title": "Conjugation", "content": ": \u1fb7 = qr \u2013 qii - qjj - qkk is the conjugation of the quaternion q given above."}, {"title": "Norm", "content": ": The norm of quaternion q is given as ||q|| ="}, {"title": "3 Quaternion Message Passing Neural Networks", "content": "Graph Neural Networks (GNNs) analyze graph-structured data by updating node representations through message passing. At each layer, messages from neighboring nodes are aggregated and used to update node features. The process is formalized as:\nMuv = MESSAGE(h),h), luv) (2)\nh = AGGREGATE(muv), Vv \u2208 N(u) (3)\nh(1+1) = UPDATE(h),h) (4)\nHere, MESSAGE computes messages based on node features hd, hd and edge features euv; AGGREGATE collects messages from neighbors N(u); and UPDATE refines the node representation. These functions, often learnable and differentiable, enable GNNs to be trained via backpropagation.\nQuaternion Graph Neural Networks (QGNNs) refer to neural networks that utilize quaternion weights to perform computations within the quaternion vector space. Notably, QGNN, as developed by [8] is a quaternion variant of the Graph Convolutional Network (GCN) layer only. This is the first limitation of QGNN, as it cannot be generalized to other GNN variants. The second limitation of QGNN is by construction, number of trainable parameters remain the same as that of real-valued GCN. While leveraging the quaternion space to generate expressive representations for nodes, we also seek to take advantage of the quaternions by reducing the number of trainable parameters. To this end, we propose a generalized framework to give the quaternion equivalent of any graph neural network method.\nIn the quaternion variant of any GNNs, in the equations 2, 3, 4, we ensure that the MESSAGE, AGGREGATE and UPDATE functions follow the quaternion operations as given in section 2.2. We describe this in more detail below:"}, {"title": "3.1 Differentiability", "content": "For the QMPNNs to learn, the quaternion versions of learnable functions from MESSAGE, AGGREGATE, and UPDATE should be differentiable. For our framework, the differentiability follows from the generalized complex chain rule for a real-valued loss function, which is provided in much detail in Deep Complex Networks [11] and Deep Quaternion Networks [3]."}, {"title": "3.2 Computational Complexity", "content": "Four feature values are clubbed together to form a single quaternion in QMPNNS, hence F/4 quaternions are necessary to transform the feature vector. The transformed feature space is of dimension F'. While using quaternion weights, these"}, {"title": "4 Lottery Ticket Hypothesis on QMPNNS", "content": "The lottery ticket hypothesis (LTH) [2] states that a randomly initialized, dense neural network contains a sub-network that is initialized such that when trained in isolation-it can match the test accuracy of the original network after training for at most the same number of iterations.\nDifferent from the lottery tickets defined previously [1,12,13,14] - that consider a subgraph of a graph with or without the subnetwork of the GNN as a ticket, we define the LTH for Graph Neural Networks and Quaternion GNNs. In a graph neural network, f(x; W), with initial parameters W(0), using Stochastic Gradient Descent (SGD), we train on the training set, minimizes to a minimum validation loss I in j number of iterations achieving accuracy a. The lottery ticket hypothesis is that there exists m when optimizing with SGD on the same training set, f attains the test accuracy a' \u2265 a with a validation loss l' < l in j' \u2264 j iterations, such that ||m|| < ||W||, thereby reducing the number of parameters. The sub-network with parameters m\u2299WQ is the winning lottery ticket, with the mask m. Algorithm 1 sparsifies the Graph Neural Network which is required to find the winning lottery ticket. The iterative algorithm that finds it is described in algorithm 2."}, {"title": "4.1 Quaternion Graph Lottery Tickets", "content": "Similar to finding a winning lottery ticket in GNNs, given a QMPNN f(\u00b7, WQ) and a graph G = (V,E), the subnetwork, i.e., winning lottery ticket, of the QMPNN is defined as f(., (\u00b7, m\u2299 m W\u00ba), where m is the binary mask on parameters. To find the winning lottery ticket in a QMPNN, we use the algorithms 1 and 2 with the quaternion-values model weights and input features."}, {"title": "5 Experiments", "content": "The effectiveness of QMPNNs for different GNN variants is validated with extensive experimentation on different real-world standard datasets. We also verify the existence of the GLTs as per our definition. We evaluated their performance on three tasks, node classification, link prediction, and graph classification."}, {"title": "5.1 Datasets", "content": "Tables 1 and 2 summarize the datasets and their statistics. For quaternion models, dataset features and classes were adjusted to be divisible by 4 by padding features with the average value and adding dummy classes. For instance, Cora's feature size was padded from 1,433 to 1,436, and the number of classes was increased from 7 to 8."}, {"title": "5.2 Experimental Setup", "content": "GNN Architecture: Two-layer GCN, GAT, and GraphSAGE models with 128 hidden units were used for node classification and link prediction on Cora, Citeseer, PubMed, ogbn-arxiv, and ogbn-collab. For graph classification on MUTAG, PROTEINS, ENZYMES, and ogbg-molhiv, three-layer models with 128 hidden units per layer were employed.\nTraining and Hyperparameters: Node classification datasets followed an 80-10-10 split, while link prediction and graph classification used an 85-5-10 split. Models were trained with a learning rate of 0.01, weight decay of 5\u00d710-4, dropout rate of 0.6, and the Adam optimizer, for up to 1000 epochs with early stopping after 200 epochs of no improvement. A prune fraction of 0.3 was applied.\nEvaluation and Infrastructure: Evaluation metrics are provided in Ta- bles 1 and 2. Experiments were conducted on NVIDIA RTX 3090 GPUs with 24GB VRAM."}, {"title": "5.3 Training and Inference Details", "content": "Our evaluation metrics, detailed in Tables 1 and 2 and following [5], include both accuracy and ROC-AUC. Accuracy measures the proportion of correctly classified instances among the total instances, providing a straightforward metric for model performance. The ROC-AUC (Receiver Operating Characteristic Area Under the Curve) score, representing the degree of separability, is particularly valuable for assessing performance on imbalanced datasets, such as in link prediction tasks with a substantial class imbalance between positive and negative edges. Summarizing the results across these diverse tasks, tables 3, 4,"}, {"title": "5.4 Results", "content": "GCN, GAT, and GraphSAGE results on Cora, Citesser, PubMed, and ogbn- arxiv for node classification are collected in table 3 and 4 shows results for link prediction on Cora, Citesser, PubMed, and ogbl-collab. The results pertaining to graph classification tasks on MUTAG, PROTEINS, ENZYMES, and ogbg- molhiv datasets are presented in table 5. These tables display the accuracy and parameter count (Params) in millions for each model and dataset, with better- performing models highlighted in bold for easy identification. Pruning results for the same models on ogbn-arxiv for node classification are shown in the top section of figure 2. The middle section of figure 2 presents results for link prediction on"}, {"title": "5.5 Key Findings", "content": "We summarize the effectiveness of magnitude pruning and quaternion-based models in GNNs:\n1. QMPNNs operate at 1/4th parameters of real-valued models: QMPNNS achieve comparable or better performance than real-valued models with just 1/4th of the parameters, showcasing their efficiency and expressiveness, as shown in Tables 3, 4, and 5.\n2. GLTs exist at 1/5th or smaller of original size: Magnitude pruning identifies lottery tickets at 1-20% of the original model size without perfor- mance loss across QGCN, QGAT, and QGraphSAGE tasks, as observed in Tables 3 and 4.\n3. GAT and GraphSAGE sparsify well; Cora is pruning-sensitive: GATs and GraphSAGE produce sparse GLTs due to attention and sam- pling techniques, while Cora exhibits significant sensitivity to pruning, with performance dropping at half model size, as detailed in the comprehensive experiments available on the project's GitHub repository.\n4. Less sparsity in graph classification GLTs: Graph classification tasks require less sparse GLTs to capture complex global features, with smaller parameter counts due to the datasets' limited graph size, as shown in Table 5 and Figure 2.\n5. Large graphs maintain performance: QMPNNs scale well to large graphs, as seen with ogbn-arxiv, ogbn-collab, and ogbn-molhiv datasets in Tables 3, 4, and 5, delivering comparable or better performance relative to trainable parameters."}, {"title": "6 Conclusion and Future Work", "content": "This paper introduces Quaternion Message Passing Neural Networks (QMPNNs) as a versatile framework for graph representation learning, leveraging quaternion representations to capture intricate relationships in graph-structured data. The framework generalizes easily to existing GNN architectures with minimal adjustments, enhancing flexibility and performance across tasks like node classification, link prediction, and graph classification. By redefining graph lottery tickets, we identified key subnetworks that enable efficient training and inference, demonstrating the scalability and effectiveness of QMPNNs on real-world datasets. Future work could explore dynamic graphs, hybrid modalities, and improving interpretability to further expand QMPNN capabilities."}]}