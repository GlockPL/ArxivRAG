{"title": "GAUSSIANANYTHING: INTERACTIVE POINT CLOUD\nLATENT DIFFUSION FOR 3D GENERATION", "authors": ["Yushi Lan", "Shangchen Zhou", "Zhaoyang Lyu", "Fangzhou Hong", "Shuai Yang", "Bo Dai", "Xingang Pan", "Chen Change Loy"], "abstract": "While 3D content generation has advanced significantly, existing methods still\nface challenges with input formats, latent space design, and output representa-\ntions. This paper introduces a novel 3D generation framework that addresses\nthese challenges, offering scalable, high-quality 3D generation with an interac-\ntive Point Cloud-structured Latent space. Our framework employs a Variational\nAutoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as\ninput, using a unique latent space design that preserves 3D shape information, and\nincorporates a cascaded latent diffusion model for improved shape-texture disen-\ntanglement. The proposed method, GAUSSIANANYTHING, supports multi-modal\nconditional 3D generation, allowing for point cloud, caption, and single/multi-\nview image inputs. Notably, the newly proposed latent space naturally enables\ngeometry-texture disentanglement, thus allowing 3D-aware editing. Experimental\nresults demonstrate the effectiveness of our approach on multiple datasets, outper-\nforming existing methods in both text- and image-conditioned 3D generation.", "sections": [{"title": "INTRODUCTION", "content": "3D content generation holds great potential for transforming the virtual reality, film, and gaming\nindustries. Current approaches typically follow one of two paths: either a 2D-lifting method or\nthe design of native 3D diffusion models. While the 2D-lifting approach (Shi et al., 2023b; Liu\net al., 2023b) benefits from leveraging 2D diffusion model priors, it is often hindered by expensive\noptimization, the Janus problem, and inconsistencies between views. In contrast, native 3D diffusion\nmodels (Jun & Nichol, 2023; Lan et al., 2024; Zhang et al., 2024) are trained from scratch for 3D\ngeneration, offering improved generality, efficiency, and control.\nDespite the progress in native 3D diffusion models, several design challenges still persist: (1) Input\nformat to the 3D VAE. Most methods (Zhang et al., 2024; Li et al., 2024) directly adopt point cloud\nas input. However, it fails to encode the high-frequency details from textures. Besides, this limits the\navailable training dataset to artist-created 3D assets, which are challenging to collect on a large scale.\nLN3Diff (Lan et al., 2024) adopt multi-view images as input. Though straightforward, it lacks direct\n3D information input and cannot comprehensively encode the given object. (2) 3D latent space\nstructure. Since 3D objects are diverse in geometry, color, and size, most 3D VAE models adopt\nthe permutation-invariant set latent (Zhang et al., 2023a; Sajjadi et al., 2022; Zhang et al., 2024) to\nencode incoming 3D objects. Though flexible, this design lacks the image-latent correspondence as\nin Stable Diffusion VAE (Rombach et al., 2022), where the VAE latent code can directly serve as\nthe proxy for editing input image (Mou et al., 2023b;a). Other methods adopt latent tri-plane (Wu\net al., 2024; Lan et al., 2024) as the 3D latent representation. However, the latent tri-plane is still\nunsuitable for interactive 3D editing as changes in one plane may not map to the exact part of the\nobjects that need editing. (3) Choice of 3D output representations. Existing solutions either output\ntexture-less SDF (Wu et al., 2024; Zhang et al., 2024), which requires additional shading model\nfor post-processing; or volumetric tri-plane (Lan et al., 2024), which struggles with high-resolution\nrendering due to extensive memory required by volumetric rendering (Mildenhall et al., 2020).\nIn this study, we propose a novel 3D generation framework that resolves the problems above and en-\nables scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space.\nThe resulting method, dubbed GAUSSIANANYTHING, supports multi-modal conditional 3D gener-\nation, including point cloud, caption, and image. Specifically, we propose a 3D VAE that adopts\nmulti-view posed RGB-D(epth)-N(ormal) renderings as the input, which are easy to render and con-\ntain comprehensive 3D attributes corresponding to the input 3D object. The information of each\ninput view is channel-wise concatenated and efficiently encoded with the scene representation trans-\nformer (Sajjadi et al., 2022), yielding a set latent that compactly encodes the given 3D input. Instead\nof directly applying it for diffusion learning (Zhang et al., 2024; Li et al., 2024), our novel design\nconcretizes the unordered tokens into the shape of the 3D input. Specifically, this is achieved by\ncross-attending (Huang et al., 2024b) the set latent via a sparse point cloud sampled from the input\n3D shape, as visualized in Fig. 2. The resulting point-cloud structured latent space significantly fa-\ncilitate shape-texture disentanglement and 3D editing. Afterward, a DiT-based 3D decoder (Peebles\n& Xie, 2023; Lan et al., 2024) gradually decodes and upsamples the latent point cloud into a set of\ndense surfel Gaussians (Huang et al., 2024a), which are rasterized to high-resolution renderings to\nsupervise 3D VAE training.\nAfter the 3D VAE is trained, we conduct cascaded latent diffusion modeling on the latent space\nthrough flow matching (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023c) using the\nDiT (Peebles & Xie, 2023) framework. To encourage better shape-texture disentanglement, a point\ncloud diffusion model is first trained to carve the overall layout of the input shape. Then, a point-\ncloud feature diffusion model is cascaded to output the corresponding feature conditioned on the\ngenerated point cloud. The generated featured point cloud is then decoded into surfel Gaussians via\npre-trained VAE for downstream applications.\nIn summary, we contribute a comprehensive 3D generation framework with a point cloud-structured\n3D latent space. The redesigned 3D VAE efficiently encodes the 3D input into an interactive latent\nspace, which is further decoded into high-quality surfel Gaussians. The diffusion models trained\non the compressed latent space have shown superior performance in text-conditioned 3D generation\nand editing, as well as impressive image-conditioned 3D generation on general real world data."}, {"title": "RELATED WORK", "content": "3D Generation via 2D Diffusion Models. The success of 2D diffusion models (Song et al., 2021;\nHo et al., 2020) has inspired their application to 3D generation. Score distillation sampling (Poole\net al., 2022; Wang et al., 2023) distills 3D from a 2D diffusion model, but faces challenges like ex-\npensive optimization, mode collapse, and the Janus problem. More recent methods propose learning\nthe 3D via a two-stage pipeline: multi-view images generation (Shi et al., 2023b; Long et al., 2024;\nShi et al., 2023a) and feed-forward 3D reconstruction (Hong et al., 2024b; Xu et al., 2024; Tang\net al., 2024). Though promising results have been achieved, their performance is bounded by the\nmulti-view generation results, which usually violate view consistency (Liu et al., 2023b) and fails\nto scale up to higher resolution (Shi et al., 2023a). Moreover, this two-stage pipeline limits the 3D\nediting capability due to the lack of a 3D-aware latent space.\nNative 3D Diffusion Models. Native 3D diffusion models (Zhang et al., 2023a; Zeng et al., 2022;\nZhang et al., 2024; Lan et al., 2024; Li et al., 2024) are recently proposed to achieve high-quality,\nefficient and scalable 3D generation. A native 3D diffusion pipeline involves a two-stage training\nprocess: encoding 3D objects into the VAE latent space (Kingma & Welling, 2013; Kosiorek et al.,\n2021), and latent diffusion model on the corresponding latent codes. Though straightforward, ex-\nisting methods differ in VAE input formats, latent space structure and output 3D representations.\nWhile most methods adopt point alone as the VAE input (Zhang et al., 2023a; 2024; Li et al., 2024),\nour proposed method encodes a hybrid 3D information through convolutional encoder. Moreover,\ncomparing to the latent set (Zhang et al., 2023a; Sajjadi et al., 2022) representation, our proposed\nmethod adopts a point cloud-structured latent space, which can be directly used for interactive 3D\nediting. Besides, rather than producing textureless SDF, our method directly decodes the 3D la-\ntent codes into high-quality surfel Gaussians (Huang et al., 2024a), which can be directly used for\nefficient rendering.\nPoint-based Shape Representation and Rendering. The proliferation of 3D scanners and RGB-\nD cameras makes the capture and processing of 3D point clouds commonplace (Gross & Pfister,\n2011). In the era of deep learning, learning-based methods are emerging for point set process-\ning (Qi et al., 2016; Zhao et al., 2021), up-sampling (Yu et al., 2018), shape representation (Genova\net al., 2020; Lan et al., 2023b), and rendering (Pfister et al., 2000; Yifan et al., 2019; Lassner &\nZollh\u00f6fer, 2021; Xu et al., 2022; Kerbl et al., 2023). Moreover, given its affinity for modern network\narchitectures (Huang et al., 2024b; Zhao et al., 2021), more explicit nature against other 3D repre-\nsentations (Chan et al., 2022; Mildenhall et al., 2020; M\u00fcller et al., 2022), efficient rendering (Kerbl\net al., 2023), and even high-quality surface modeling (Huang et al., 2024a), point-based 3D repre-\nsentations are rapidly developing towards the canonical 3D representation for learning 3D shapes.\nThus, we choose (featured) point cloud as the representation for the 3D VAE latent space, and 2D\nGaussians (Huang et al., 2024a) as the output 3D representations.\nFeed-forward 3D Reconstruction and View Synthesis. To bypass the per-scene optimization of\nNeRF, researchers have proposed learning a prior model through image-based rendering (Wang\net al., 2021; Yu et al., 2021). However, these methods are primarily designed for view synthe-\nsis and lack explicit 3D representations. Sajjadi et al. (2022; 2023) propose Scene representation\ntransformer (SRT) to process RGB images with Vision Transformer (Dosovitskiy et al., 2021) and\ninfers a \"set-latent scene representation\u201d. Though benefiting from the flexible design, its geometry-\nfree paradigm also fails to generate explicit 3D outputs. Recently, LRM-line of work (Hong et al.,\n2024b; Tang et al., 2024; Wang et al., 2024) have proposed a feed-forward framework for generalized\nmonocular reconstruction. However, they are still regression-based models and lack the latent space\ndesigned for generative modeling and 3D editing. Besides, they are limited to 3D reconstruction\nonly and fail to support other modalities."}, {"title": "PRELIMINARIES", "content": "2D Gaussian Splatting (2DGS). Since 3DGS (Kerbl et al., 2023) models the entire angular radiance\nin a blob, it fails to reconstruct high-quality object surfaces. To resolve this issue, Huang et al.\n(2024a) proposed 2DGS (surfel-based GS) that simplifies the 3-dimensional modeling by adopting\n\"flat\" 2D Gaussians embedded in 3D space, which enables better alignment with thin surfaces."}, {"title": "", "content": "Notation-wise, the 2D splat is characterized by its central point $p_k$, two principal tangential vectors\n$t_u$ and $t_v$, and a scaling vector $S = (s_u, s_v)$ that controls the variances of the 2D Gaussian. Notice\nthat the primitive normal is defined by two orthogonal tangential vectors $t_w = t_u \\times t_v$. Thus, the\n2D Gaussian is parameterized with\n\\begin{equation}\nP(u, v) = p_k + s_u t_u u + s_v t_v v = H(u, v, 1, 1)^T\n\\end{equation}\nwhere H\n\\begin{equation}\n=\n\\begin{bmatrix}\ns_u t_u & s_v t_v & 0 & p_k \\\\\nRS & Pk & 0 & 1 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\end{equation}\nWhere H parameterizes the local 2D Gaussian geometry. For the point $u = (u, v)$ in uv space, its 2D\nGaussian value can then be evaluated by standard Gaussian $G(u) = exp(-\\frac{u^2}{2\\sigma^2})$, and the center\n$p_k$, scaling $(s_u, s_v)$, and the rotation $(t_u, t_v)$ are all learnable parameters. Following 3DGS Kerbl\net al. (2023), each 2D Gaussian primitive has opacity $\\alpha$ and view-dependent appearance $c$, and can\nbe rasterized via volumetric alpha blending:\n\\begin{equation}\nc(x) = \\sum_{i=1}^{I} c_i \\alpha_i G_i(u(x)) \\prod_{j=1}^{i-1} (1 - \\alpha_j G_j(u(x))),\n\\end{equation}\nwhere the integration process is terminated when the accumulated opacity reaches saturation. During\noptimization, pruning and densification operations are iteratively applied.\nFlow Matching and Diffusion Model. Diffusion models create data from noise (Song et al., 2021)\nand are trained to invert forward paths of data towards random noise. The forward path is constructed\nas $z_t = a_t x_0 + b_t \\epsilon$, where $\\epsilon \\sim N(0, I)$, $a_t$ and $b_t$ are hyper parameters. The choice of forward\nprocess has proven to have important implications for the backward process of data sampling (Lin\net al., 2023).\nRecently, flow matching (Liu et al., 2023c; Albergo et al., 2023; Lipman et al., 2023) has introduced\na particular choice for the forward path, which has better theoretical properties and has been verified\non the large-scale study (Esser et al., 2024). Given a unified diffusion objective (Karras et al., 2022):\n\\begin{equation}\n\\mathcal{L}_{w}(x_0) = \\frac{1}{2} \\mathbb{E}_{t \\sim U(0,1), \\epsilon \\sim \\mathcal{N}(0,I)} \\left[ w_t \\|\\epsilon(z_t, t) - \\epsilon \\|_2^2 \\right],\n\\end{equation}\nwhere $\\bar{\\Lambda}_t := \\log \\frac{1-t}{t}$ denotes signal-to-noise ratio, and $\\Lambda_t$ denotes its derivative. By setting $w_t = \\frac{1}{t(1-t)}$\nwith $z_t = (1 - t)x_0 + t\\epsilon$, flow matching defines the forward process as a straight path between the\ndata distribution and the Normal distribution. The network $\\epsilon$ directly predicts the velocity $v_\\epsilon$."}, {"title": "GAUSSIANANYTHING", "content": "This section introduces our native 3D diffusion model, which learns 3D-aware diffusion prior over\nthe novel point-cloud structured latent space through a dedicated 3D VAE. The goal of training is to\nlearn\n1. An encoder $E_\\theta$ that maps a set of posed RGB-D-N images $\\mathcal{R} = \\{R_i, ..., R_V \\}$, correspond-\ning to the given 3D object to a point-cloud structured latent $z = [z_x \\oplus z_h]$;\n2. A conditional cascaded transformer denoiser $\\epsilon_\\phi(z_{h,t}, t, z_x, \\Theta, t, c) \\epsilon \\{z_{x,\\Theta}, t, c\\}$ to denoise\nthe noisy latent code $z_t$ given diffusion time step $t$ and condition prompt $c$;\n3. A decoder $D_\\Psi$ (including a Transformer $D_T$ and a cascaded attention-base Upsampler\n$D_U$) to map $z_0$ to the surfel Gaussian $G$ corresponding to the input object. Moreover, our\nattention-based decoding of dense surfel Gaussian also provides a novel way for efficient\nGaussian prediction\nBeyond the advantages shared by existing 3D LDM (Zhang et al., 2024; Lan et al., 2024), our design\noffers a flexible point-cloud structured latent space and enables interactive 3D editing.\nIn the following subsections, we first discuss the proposed 3D VAE with a detailed framework design\nin Sec 4.1. Based on that, we introduce the cascaded conditional 3D diffusion stage in Sec. 4.2. The\nmethod overview is shown in Fig. 2."}, {"title": "POINT-CLOUD STRUCTURED 3D VAE", "content": "Unlike image and video, the 3D domain is un-uniform and represented differently for different\npurposes. Thus, how to encode 3D objects into the latent space for diffusion learning plays a crucial\nrole in the 3D generation performance. This challenge is two-fold: what 3D representations to\nencode, and what network architecture to process the input.\nVersatile 3D Input. Instead of using dense point cloud (Zhang et al., 2024; Li et al., 2024), we\nadopt multi-view posed RGB-D(epth)-N(ormal) images as input, which encode the 3D input more\ncomprehensively and can be efficiently processed by well-established network architectures (Sajjadi\net al., 2022; Wu et al., 2023) in a flexible manner. Specifically, the input is a set of multi-view ren-\nderings $\\mathcal{R}$ of a 3D object, where each rendering within the set $\\mathcal{R} = (I, \\Delta, N, \\pi)$ contains thorough\n3D attributes that depict the underlying 3D object from the given viewpoint: the rendered RGB im-\nage $I \\in \\mathbb{R}^{H\\times W\\times 3}$, depth map $\\Delta \\in \\mathbb{R}^{H\\times W}$, normal map $N \\in \\mathbb{R}^{H\\times W\\times 3}$, and the corresponding\ncamera pose $\\pi$.\nTo unify these 3D attributes in the same format, we further process the camera $\\pi$ into Pl\u00fccker coor-\ndinates (Sitzmann et al., 2021) $p_{\\pi} = (o \\times d_{u,v}, d_{u,v}) \\in \\mathbb{R}^6$, where $o_i \\in \\mathbb{R}^3$ is the camera origin,\n$d_{u,v} \\in \\mathbb{R}^3$ is the normalized ray direction, and $\\times$ denotes the cross product. Thus, the Pl\u00fccker\nembedding of a given camera $\\pi$ can be expressed as $P \\in \\mathbb{R}^{H\\times W\\times 6}$. Besides, following MCC (Wu\net al., 2023), we use $\\pi$ to unproject the depth map into their 3D positions $X \\in \\mathbb{R}^{H\\times W\\times 3}$. The result-\ning information is channel-wise concatenated, giving $\\mathcal{R} = [I \\oplus X \\oplus N \\oplus P] \\in \\mathbb{R}^{H\\times W\\times (3+3+3+6=15)}$.\nTransformer-based 3D Encoding. Given the 3D renderings $\\mathcal{R}$, encoding them into a 3D latent\nspace remains a significant challenge. Independently processing each input rendering $\\mathcal{R}$ with exist-\ning network architecture (Wu et al., 2023; Dosovitskiy et al., 2021) overlooks the information from\nother views, leading to 3D inconsistency and content drift across views (Liu et al., 2023b).\nExisting multi-view generation alleviates this issue by injecting 3D attention (Shi et al., 2023b;\nTang et al., 2024; Shi et al., 2023a) into the U-Net architecture. Inspired by its effectiveness, here\nwe directly adopt Scene Representation Transformer (SRT)-like encoder (Sajjadi et al., 2022; 2023)\nto process the multi-view inputs, which fully adopts 3D attention transformer block for the 3D\nrepresentation learning. Specifically, the encoder first down-samples the multi-view inputs via a\nshared CNN backbone, and then processes the aggregated multi-view tokens through the transformer\nencoder (Dosovitskiy et al., 2021):\n\\begin{equation}\nz_z = E_X(E_{CNN}(\\{\\mathcal{R}\\})),\n\\end{equation}\nwhere $z_z$ is the set latent corresponding to the 3D input. This can be seen as the full-attention\nversion of existing 3D attention-augmented architecture. The resulting latent codes $z_z$ fully capture\nthe intact 3D information corresponding to the input. Compared to existing work that adopts point"}, {"title": "", "content": "clouds only as input (Zhang et al., 2024; Li et al., 2024), our proposed solution supports more 3D\nproperties as input in a flexible way. Besides, the attention operations can be well optimized over\nmodern GPU architecture (Dao et al., 2022; Dao, 2024).\nPoint Cloud-structured Latent Space. Though $z_z$ fully captures the given 3D input, it cannot be\ndirectly used for diffusion learning due to the lack of a regularized, low-rank latent space (Rombach\net al., 2022). Specifically, $z_z$ has a shape of $V \\times (H/f) \\times (W/f) \\times C$, where $V$ is the number of\nviews used for input, $H, W$ is the input resolution and $f$ is the down-sampling factor of the CNN\nbackbone. Given $V = 8, f = 8$, and $H = W = 512$, the resulting latent codes will have a shape of\n32768 \u00d7 C, which are unsuitable for diffusion learning. Moreover, using the set (Lee et al., 2019)\nas the 3D latent space (Zhang et al., 2023a; 2024; Li et al., 2024) also sacrifices an explicit, editable\nlatent space (Mou et al., 2023a) for flexibility.\nHere, we resolve these issues by proposing a point cloud-structured latent space. Specifically, we\nproject the un-structured features $z_z$ onto the manifold of the input 3D shape through the cross\nattention layer:\n\\begin{equation}\nz_h := CrossAttn(PE(z_x), z_z, z_z),\n\\end{equation}\nwhere $CrossAttn(Q, K, V)$ denotes a cross attention block with query Q, key K, and value V. $z_x \\in\n\\mathbb{R}^{3 \\times N}$ is a sparse point cloud sampled from the surface of the 3D input with Farthest Point Sampling\n(FPS) (Qi et al., 2017), and $PE$ denotes positional embedding (Tancik et al., 2020). Intuitively, we\ndefine a \\textit{read} cross attention block (Huang et al., 2024b) that cross attends information from un-\nstructured representation $z_z$ into the point-cloud structured feature $z_h \\in \\mathbb{R}^{C_h \\times N}$, with $C_h \\ll C$. In\nthis way, we obtain the point-cloud structured latent code $z = [z_x \\oplus z_h] \\in \\mathbb{R}^{(3+C_h) \\times N}$ for diffusion\nlearning.\nHigh-quality 3D Gaussian Decoding. Given the point cloud-structured latent codes, how to decode\nthem into high-quality 3D representation for supervision remains challenging. Though dense point\ncloud (Huang et al., 2024b) is a straightforward solution, it fails to depict high-quality 3D structure\nwith limited point quantity. Here, we resort to surfel Gaussian (Huang et al., 2024a), an augmented\npoint-based 3D representation that supports high-fidelity 3D surface modeling and efficient ren-\ndering. Specifically, our decoder first decodes the input through the 3D-DiT blocks (Peebles & Xie,\n2023; Lan et al., 2024), which has shown superior performance against traditional transformer layer:\n\\begin{equation}\n\\tilde{z} := D_T(MLP(z)),\n\\end{equation}\nwhere an MLP layer first projects the input latent to the corresponding dimension, and $D_T$ is the DiT\ntransformer. Since dense Gaussians are preferred for high-quality splatting (Kerbl et al., 2023), we\ngradually upsample the latent features through transformer blocks. Specifically, given a learnable\nembedding $z_u \\in \\mathbb{R}^{f_u \\times C}$ where $f_u$ is the up-sampling ratio, we prepend it to each token in the latent\nsequence. Then, $H$ layers of transformer blocks are used to model the upsampling process:\n\\begin{equation}\nz_T^{(k+1)} := D_\\Theta ([z_u \\oplus \\tilde{z_i}]),\n\\end{equation}\nwhere $D_\\Theta$ is a transformer block for predicting the k-th levels of details (LoD) Gaussian as shown\nin Fig. 2, and $z_T^{(k+1)} \\in \\mathbb{R}^{f_u \\times C}$ are the upsampled set of tokens. The overall tokens $z^{(k+1)} \\in\n\\mathbb{R}^{(f_u \\times N) \\times C}$ after up-sampling are used to predict the 13-dim attributes of surfel Gaussians.\nTo achieve denser Gaussians prediction, we cascade the upsampling transformer defined in Eq. (8)\nfor K times, giving the final Upsampler $D_U$ for high-quality Gaussian output. Note that our solution\noutputs a set of Gaussians that are uniformly distributed on the 3D object surface with near 100%\nGaussian utilization ratio. Existing pixel-aligned Gaussian prediction models (Tang et al., 2024;\nYinghao et al., 2024; Szymanowicz et al., 2023), however, usually waste 50% Gaussians due to\nview overlaps and empty background color. Besides, our intermediate Gaussians output naturally\nserves as K LoD (Takikawa et al., 2021), which can be used in different scenarios to balance the\nrendering speed and quality.\nTraining. Our 3D VAE model is end-to-end optimized across both input views and randomly chosen\nviews, minimizing image reconstruction objectives between the splatting renderings and ground-\ntruth renderings. Besides image reconstruction loss, we also impose loss over geometry regulariza-\ntions, KL constraints, and adversarial loss:\n\\begin{equation}\n\\mathcal{L}(\\theta, \\Psi) = \\mathcal{L}_{render} + \\mathcal{L}_{geo} + \\lambda_{kl} \\mathcal{L}_{KL} + \\lambda_{GAN} \\mathcal{L}_{GAN},\n\\end{equation}"}, {"title": "CASCADED 3D GENERATION WITH FLOW MATCHING", "content": "After training the point-cloud structured 3D VAE, we get a dataset of D shapes paired with condition\nvectors (e.g., caption or images), $\\{(z_i, C_i)\\}_{i \\in [D]}$, where the shape is represented by latent code $z$\nthrough the 3D VAE aforementioned. Our goal is to train a flow-matching generative model to learn\na diffusion prior on top of it. Below we present how we adapt flow-based models to our case.\nCascaded Flow Matching over Symmetric Data. As detailed in Sec. 3, flow matching involves\ntraining a neural network $\\epsilon_\\phi$ to predict the velocity $v$ of the noisy input $z_t$ with the straight-line tra-\njectory. After training, $\\epsilon$ can sample from a standard Normal prior $\\mathcal{N}(0, I)$ by solving the reverse\nODE/SDE (Karras et al., 2022). In our case, the training data point is the point-cloud structured"}, {"title": "", "content": "latent code $z = [z_x \\oplus z_h] \\in \\mathbb{R}^{(3+C_h) \\times N}$, which is symmetric and permutation invariant (Zeng\net al., 2022; Nichol et al., 2022). Based on this property, we opt for diffusion transformer (Peebles\n& Xie, 2023) without positional encoding as the $\\epsilon$ parameterization.\nHere, rather than modeling $z_x$ and $z_h$ jointly, we empirically found that a cascaded framework (Ho\net al., 2021; Lyu et al., 2024; 2023) leads to better performance. Specifically, a conditioned sparse\npoint cloud generative model $\\epsilon_\\phi$ is first trained to generate the overall structure of the given object:\n\\begin{equation}\n\\mathcal{L}_{c}(x_0) = \\frac{1}{2} \\mathbb{E}_{t \\sim U(t), \\epsilon \\sim \\mathcal{N}(0,1)} \\left[ w_t \\|\\epsilon_c (z_{x,t}, t, c) - \\epsilon \\|_2^2 \\right],\n\\end{equation}\nand a point cloud feature generative model $\\epsilon_\\phi$ is cascaded to learn the corresponding KL-\nregularized feature conditioned on the sparse point cloud:\n\\begin{equation}\n\\mathcal{L}_{h}(x_0) = \\frac{1}{2} \\mathbb{E}_{t \\sim U(t), \\epsilon \\sim \\mathcal{N}(0,1)} \\left[ w_M \\|\\epsilon_g (z_{h,t}, z_u, t, c) - \\epsilon \\|_2^2 \\right].\n\\end{equation}\nThe detailed cascading process is detailed Fig. 3 (c). Our proposed design enables better geometry-\ntexture disentanglement and facilitates 3D editing over specific shape properties.\nConditioning Mechanism. Compared to LRM (Hong et al., 2024b; Tang et al., 2024) line of work\nwhich only handles image conditions, our native diffusion-based method enables more flexible 3D\ngeneration from diverse conditions. As shown in Fig. 3 (a-b), for the text-conditioned model, we\nadopt CLIP (Radford et al., 2021) to extract penultimate tokens as the condition embeddings; and\nfor the image conditioned model, we use DINOv2 (Oquab et al., 2023) to extract global and patch\nfeatures. All conditions are injected into the DiT architecture through a pre-norm (Xiong et al.,\n2020) cross-attention block. All the models are trained with Classifier-free Guidance (CFG) (Ho,\n2021) by randomly dropping the conditions with a probability of 10%.\nTo cascade two diffusion models, we encode the output of stage-1 model $\\epsilon_c$ with $PE(z_x)$ as in\nEq. (6), and add it to the first-layer features of $\\epsilon_g$. This guarantees that generated features are paired\nwith the input sparse point cloud structure."}, {"title": "EXPERIMENTS", "content": "Datasets. To train our 3D VAE, we use the renderings provided by G-Objaverse (Qiu et al., 2023;\nDeitke et al., 2023) and choose a high-quality subset with around 176K 3D instances, where each\nconsists of 40 random views with RGB, normal, depth map and camera pose. For text-conditioned\ndiffusion training, we use the caption provided by Cap3D (Luo et al., 2023; 2024) and 3DTopia Hong\net al. (2024a). For"}]}