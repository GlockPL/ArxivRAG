{"title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens", "authors": ["Gagan Jain", "Nidhi Hegde", "Aditya Kusupati", "Arsha Nagrani", "Shyamal Buch", "Prateek Jain", "Anurag Arnab", "Sujoy Paul"], "abstract": "The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over two-fold. We validate our approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. We further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.", "sections": [{"title": "Introduction", "content": "Visual tokens, the fundamental building blocks of image and video representations, often exhibit strong inter-dependencies, spatially in images and spatio-temporally in videos. This offers a potential avenue for optimization in visual processing, as processing every token with equal emphasis may not be necessary for achieving optimal results. Traditional Vision Transformer (ViT) [17] and Video Vision Transformer (ViViT) [2] based models, however, process all tokens with equal emphasis, disregarding this inherent codependency and leading to unnecessary computational burden. This becomes a major bottleneck when deploying these models in real-world scenarios, where computational resources may be limited and real-time processing is required.\nTo this end, conditional computation has become a promising line of research to increase the capacity of a network, while only conditionally activating a part of it during inference. Sparse Mixture of Experts (MoEs) was initially popularized for Natural Language Processing (NLP) [37, 19], but it has been gaining attention for furthering conditional computation ideas in vision [34, 1, 30, 45] as well. While MoEs bring in improved performance at a given inference cost, they also increase the overall parameter count, leading to increased storage requirements. Moreover, these works rely on experts that have the same parameter count and compute, limiting their ability to reduce computational costs without resorting to skipping tokens entirely."}, {"title": "Related Work", "content": "Transformers [40] have become the de-facto architecture for processing data across multiple modalities spanning language [9, 31], images [17, 14], video [2, 44] and audio [20] and combinations thereof [33]. Consequently, there have been numerous efforts to improve the efficiency of transformers to make them more amenable for deployment in real-world applications [39]. These include approaches like efficient approximations of attention [10, 43], local attention [28, 3, 11] and reducing the number of tokens in the transformer [35, 26, 7] among others. Our work focuses on conditional computation [4], observing that some input tokens are easier to process than others, and therefore require less computation during inference.\nMixtures-of-Experts (MoE) transformers learn to route tokens to one of multiple expert MLPs [37, 19]. Although such models conditionally process input tokens, each expert has the same parameter- and FLOP-count, meaning that the total computation is constant for each input. More relevant to our approach, Mixture of Depths [32] extends the routing logic of MoE to conditionally skip an expert completely, thus total computation for each input varies dynamically. Completely skipping tokens being a hard unretrievable decision, our work chooses from an array of nested network, which effectively process information and help to stabilize training by getting rid of discontinuities.\nNested architectures [42, 27, 48] on the other hand, learn hierarchical representations of the input, where the first k hidden dimensions encode the most relevant information. This allows to extract multiple models with varying inference compute from a single trained model, similar to 'Mix-n-Match' in [16]. However, these models do not process tokens adaptively. Our model, in contrast, consists of a learned router which dynamically routes tokens to experts of different hidden dimensions based on the given compute constraints. Therefore, instead of requiring the user to select the hidden dimensions of"}, {"title": "Preliminaries", "content": "In this section, we first discuss the concept of nested models, on top of which we build our proposed methodology, Mixture of Nested Experts (MoNE). We then briefly discuss Mixture of Experts (MoE), and highlight its differences from MoNE."}, {"title": "Nested Models", "content": "For the purposes of this work, we use the Vision Transformer (ViT) [17] as an example of a full model, from which nested submodels can be derived. Inspired by MatFormer [16], we define these submodels for every layer of the network, for both Self-Attention and MLP. The key idea is that in a feature projection operation $Wx$, where $W = [W_{[:\\frac{D}{m}]}, W_{[:D]}]$, and $W_{[:\\frac{D}{m}]}$ denotes \u201cslicing\u201d the first $\\frac{D}{m}$ dimensions, we can extract a partial projection $W_{[:\\frac{D}{m}]\\times[[:\\frac{D}{m}]}$. This can be done for any projection in the transformer, and we can extract smaller models from it. We refer to these as nested models, and $D/m$ as the nested model dimension. This is shown in Figure 2a. The Extract operation extracts the first $D/m$ features and applies the corresponding projection sub-matrix to it, while the Pad operation pads it back to full dimension $D$ before residual connections and LayerNorm. While MatFormer applies the nested structure only to the hidden dimension of the MLP layer, in our approach we extend it to the in- and out-projections of both the Self-Attention (SA) and MLP layer. In the SA block, irrespective of the sub-model used in the in-projections, it is always projected to the model dimension $D$ for the $(QKT)V$ operation. The same thing is performed in MLP, where the hidden dimension is always $4D$, as in ViT, irrespective of the dimension of the in/out-projection.\nWe extract $E$ nested models with exponentially-spaced model dimensions. Therefore, for a typical value of $E = 4$, the model dimension for the nested models are $[\\frac{D}{8}, \\frac{D}{4}, \\frac{D}{2}, D]$. Note that while we build upon the idea of nested models from MatFormer, we do not share their training strategy which involves joint optimization through a weighted loss over these submodels. In contrast, we treat these nested models as distinct experts with varying compute requirements. The Mixture of Nested Experts (MONE) framework (described in detail in Sec. 4.1) then dynamically routes input tokens to these nested experts based on their information content, with the idea that more informative tokens should be processed by larger (and thus more computationally expensive) nested models."}, {"title": "Mixture of Experts", "content": "A traditional Mixture of Experts (MoE) layer in a transformer can be represented as $MoE(x) = \\Sigma_{i=1}^{E} g(x)_i e_i(x)$, where $E$ is the number of experts, $e_i(.)$ are the expert models each having their own parameters, $g : R^D \\rightarrow R^E$ is the routing/gating function, which decides the experts which should process $x$. Note that $g$ is sparse with only $k << E$ terms being non-zero. Hence, during inference, only those experts need to be active.\nMoE strictly increases the parameter count, but maintains the same inference FLOPs by setting $k = 1$. However, it still needs to process all tokens with the same pre-defined compute. In contrast, in MoNE, we do not extend the parameter count of the model, due to the nesting structure (see Sec. 3.1), and dynamically choose a nested expert during inference. Unlike in MoE, where all experts have the same capacity, in MoNE, $e_i \\subset e_{i+1}$ with $k = 1$, which allows us to dynamically allocate compute."}, {"title": "Methodology", "content": "In this section, we describe the details of our Mixture of Nested Experts (MoNE) framework for efficient inference. We assume a Vision Transformer (ViT) [17] based architecture for our approach, and then extend it to Video ViT (ViViT) [2] as well."}, {"title": "Mixture of Nested Experts (MoNE)", "content": "Tokenization: In this paper, as our primary focus is images and videos, the model input is in $R^{H\\times W\\times 3\\times T}$, where $T = 1$ for images and $T > 1$ for videos. After tokenization, the input to the transformer is $X \\in R^{D\\times N}$ where $N$ is the number of tokens, and $D$ their model dimension. For images, we have $N = H/p_h \\cdot W/p_w$, and for video, $N = T/p_t \\cdot H/p_h \\cdot W/p_w$, where $H, W,T$ are the input height, width and duration respectively. $p_h, p_w$ and $p_t$ are the patch sizes along these respective dimensions. We use the ViT [17] and ViViT [2] architectures to tokenize images and videos respectively, obtaining a list of tokens $X = \\{x_i\\}_{i=1}^N$.\nMoNE Block: The Mixture of Nested Experts (MoNE) framework is a dynamic routing mechanism that processes visual tokens using nested models with varying computational capacities, instead of processing all tokens with the full model. A pictorial repsentation of the model is presented in Figure 2b. Let $B^l = \\{B_1,...,B_E^l\\}$ denote the nested blocks at a certain layer $l$ with increasing parameter sizes, $B(.)^l$ being the full model block. A router network decides the appropriate nested block to use for every token. Hence information from tokens of different model dimension interact with each other. This is enabled by performing self-attention at the full model dimension $D$ as discussed before. For each token $x_i$, a router produces a probability distribution over the $E$ nested experts, $r_i = softmax(W_r x_i + b_r)$, where $W_r$ and $b_r$ denote the router weights and bias respectively.\nThese router predictions are sent to an assignment algorithm, which assigns every token to a single appropriate nested expert. Based on the assignments, we update the features for the $i^{th}$ token in the $l^{th}$ layer as follows -\n$x_i^{l+1} = x_i^l + \\alpha (r_i^l \\cdot b_j^{l+1})(x_i^l)$\n$x_i^{l+1} = x_i^l + B^{SA}_{B_j}(x_i^l)$\n(1)"}, {"title": "Token to Nested Expert Assignments", "content": "where the $j^{th}$ nested expert is chosen by the Expert Preferred Router $[EPR(.)]$ algorithm for the $i^{th}$ token as per Eq. 2:\n$j^* = EPR(i; \\{r_i^l\\}_{i=1}^E)$\n(2)\nNote that the multiplication of the router predictions with the model output in Eq. 1 allows gradient propagation through the router weights. We also introduce a learnable parameter $\\alpha \\in [0, 1)$, initialized to 0, which ensures proper gradient flow during the initial training stages, specifically during finetuning from a pre-trained MatFormer model. Without scaling, a low initial router prediction would dampen the block output, whereas the initial multiplicative factor being 1 ensures a stable starting point.\nFeatures and Loss: The feature of the last layer $x^L$ is used for downstream applications. For classification tasks, we apply global average pooling on all the token features and apply a linear classifier layer to predict the categories.\nWithin the MoNE framework, the routing strategy is crucial for achieving an optimal balance between performance and computational efficiency. Traditionally there are two primary routing strategies - token choice [37] and expert choice [34]. In token-choice routing, the router predicts the probability distribution over the available experts, and picks the expert with the highest probability. However, this can suffer from load balancing issues, with most of the tokens being routed to one or few experts. Hence, inference time compute is only bounded by the compute of the full model. On the other hand, in expert choice routing, each expert selects the top-k tokens with the highest preference for that expert. This guarantees perfect bounds on computation. Potential conflicts due to token selection by multiple experts are resolved by prioritizing based on model size.\nFormally, we consider a given distribution of nested models applied to the tokens, represented as $c = \\{c_1,...,c_E\\}$, s.t., $\\sum_i C_i = 1$, which we call the capacity distribution over the nested models. The method for obtaining a suitable capacity distribution, given the inference time compute requirements, will be discussed in Sec. 4.3. Given router probabilities $r_i$ for $N$ tokens across $E$ experts, we employ an Expert Preferred Routing algorithm (Algorithm 1). This is a greedy assignment approach that gives higher preference to larger nested models, aiming to identify the most important tokens first. We begin by examining the router predictions for the biggest to the smallest model, assigning $k_j = [c_jN]$ of the remaining tokens to $j^{th}$ nested model. Any remaining tokens, arising from integer packing constraints, are assigned to the smallest model. Algorithm 1 presents the proposed Expert Preferred Routing (EPR) algorithm."}, {"title": "Capacity Distribution Across Experts", "content": "The Expert Preferred Routing (EPR) as described in Section 4.2 needs the individual expert's capacity bounds $c_i$ to be specified. To get this, we define a metric called the effective capacity : $e_c = \\sum_{i=1}^E C_i d_i/D$, where $d_i = D/2^{E-i}$ is the model dimension of the $i^{th}$ nested model. Given a certain inference FLOP requirement, we can translate that to an equivalent effective capacity $e_c$. Since every token gets processed through exactly one nested expert, this along with the given budget imposes two constraints on the unknown capacity distribution $c$. However, since the individual expert capacities vary log-linearly, multiple distributions $c$ can lead to the same $e_c$ for $E > 2$ and it is non-trivial to choose"}, {"title": "Videos", "content": "one over the other. MoEs generally use auxilliary loss functions [34, 37] to promote equal usage of experts. But in MoNE, that would render a certain fixed capacity, missing out on the flexibility that the framework provides to function with any capacity. Hence, we invoke intuitive constraints to solve for $c$. Specifically, we incentivize the usage of larger models, while also adding an entropy term to ensure uniformity of capacities across experts. Given these constraints, we solve the following optimization problem:\n$\\begin{aligned}\n&\\underset{c}{\\text{maximize}} & \\sum_{i=1}^E c_i \\delta^{i-1} - \\beta \\sum_{i=1}^E c_i log c_i\\\\\n&\\text{subject to} & \\sum_{i=1}^E c_i = 1\\\\ && \\sum_{i=1}^E c_i \\frac{d_i}{D} = e_c\\\\\n&& 0 \\leq c_i \\leq 1 &\\text{ } \\forall i \\in \\{1, ..., E\\}\\\\\n\\end{aligned}$\n(3)\ngiven $0 <e_c <1$, $E$, $\\delta> 1$, $\\beta> 0$\nIn practice, we set $(\\beta, \\delta)$ to $(10,2)$ and use a Sequential Least Squares Programming (SLSQP) optimizer to solve Eq. 3 for the capacity distribution $c$, which is then used by EPR (Algorithm 1) to get token to expert mappings. We empirically verify these choices in Section 6.\nMoNE can be seamlessly adapted for video-based tasks. In videos, there exists another dimension - time - which adds to the significant redundancy in the tokens. Given the large number of tokens that can be obtained from a video, the computational costs grow drastically. To tackle this problem, works in literature factorize computation along space and time [2, 5], perform local windowed computation [29], etc. MoNE being a token based approach, directly extends to video encoders.\nFor video processing, we leverage the Factorized Encoder architecture of ViViT [2]. This architecture employs two distinct transformers: spatial and temporal. After tokenization, each temporal index yields a set of tokens representing information from local spatio-temporal neighborhoods. These spatial tokens interact within their temporal index for $L_s$ layers, culminating in a single global token per index. Subsequently, a temporal transformer processes these global tokens across $L_t$ layers. Given that the spatial transformer significantly dominates computational costs in this model, we integrate MoNE into the spatial component while maintaining full capacity for the temporal transformer. The router predicts expert assignments for all temporal frames independently, which are then consumed by the EPR(.) algorithm to produce frame-wise expert assignments."}, {"title": "Results", "content": "In this section, we empirically evaluate MoNE on multiple datasets spanning images and videos, for different sizes and assess its adaptability to stringent FLOP constraints during inference.\nImplementation details: We empirically evaluate MoNE on image and video classification. For image classification, we train the network with random initialization. As for video classification, we follow previous literature and start from a pre-trained MatViT [16] model due to the inherent nested structure required in MoNE. We follow the joint training strategy of MatViT, with separate losses an all model granularities. We implement MoNE on JAX [8] using BigVision [6] for image classification and Scenic [13] for video classification. We follow the AugReg [38] training strategy to train all our image classification models. For video classification tasks, we inherit all augmentations and hyperparameter values directly from the ViViT [2] paper.\nFor all experiments in this section, we place a single router at the first transformer layer, and propagate the router decisions to all the layers. We also multiply the router predictions (Eqn 1) to all layers, which ensures differentiable paths through the router network in all layers and allows the more evolved features from later layers to influence router learning. We also perform analysis of router placement in Section 6.\nBaselines: We first compare with MatViT's nested models. As mentioned in the paper [16], we perform joint training over all four nested models that we consider in this work - $\\{\\frac{D}{8},\\frac{D}{4},\\frac{D}{2},D\\}$. MatViT is equivalent to MoNE, with a deterministic router to pass all tokens to the same nested model. We show that adaptively mixing tokens with different model dimensions performs much"}, {"title": "Router Analysis", "content": "In this section, we discuss, analyse and visualise the design choices in implementing the router network. We choose the SSv2 dataset for this analysis.\nRouter Position: As discussed before, we use a single router at the first layer, and propagate its decisions for all layers. While a delayed router might benefit from a more processed feature representation as input, this also diminishes the compute gains, as the initial layers operate at full capacity. We reason this choice by monitoring performance while placing the router at different layers in the network. As Figure 6a suggests, the gains through richer features from the later layers is outweighed by the shift in the curve to the right, and an equivalent capacity with our default router produces higher points on the curve.\nNumber of Routers: We vary the number of routers, placing them at different regular intervals in the network in Figure 6b. The decision from one router is carried out until the next router block is encountered. We notice a clear downtrend in performance as we increase the number of routers from being present in the first layer to being present in all layers. Intuitively, more routers demands learning more decisions, and the main network in turn has to adapt to these decisions making it harder to optimize."}, {"title": "Conclusion", "content": "In this work, we presented Mixture of Nested Experts (MoNE), a novel framework for adaptive processing of visual tokens by dynamically allocating computational resources to different tokens. Through a nested structure with shared parameters and the proposed expert-choice routing algorithm, MoNE achieves significant reductions in inference time (over two-fold) without sacrificing accuracy on benchmark image and video datasets. Future works can be centered around extending MoNE to denser tasks like object detection, captioning, etc.\nLimitations: Extending this to auto-regressive decoding in LLMs is non-trivial, as this is designed primarily with an encoder architecture in mind. We leave this further exploration for future work.\nSocietal Impact: The MoNE framework dynamically allocates computational resources with a given budget, thereby significantly minimizing energy usage and carbon emissions during inference of"}, {"title": "Appendix", "content": null}, {"title": "MatFormer Structure on Model Dimension", "content": "Following MatFormer convention, we define $E$ ViT blocks $B_i$, such that $B_i \\subset B_{i+1}$ for all $i \\in [E]$, meaning that the parameters of $B_i$ are contained in those of $B_{i+1}$. With $d_i$ denoting the hidden dimension corresponding to nested model $B_i$ such that $d_1 < d_2 < ... d_E = D$, the block operation for a nesting $B_i$ on an input token set $X = \\{x_i\\}_{i=1}^N$ for $x_i \\in R^D$ is given by:\n$B_i(X) = B(x, d_i) = B^{FFN}(Z, d_i),  Z = B^{SA}(X, d_i) + X,  d_i = (d_i, d_i,...,d_i)$ (4)\nThe modified Self-Attention $B^{SA}$ and Feed-Forward $B^{FFN}$ subroutines are shown below.\n$B^{SA}(X, d) = LN \\left[ \\sigma\\left(\\frac{(X\\Theta_d W_Q) (X\\Theta_d W_K)^T}{\\sqrt{d_m}}\\right)  (X\\Theta_d W_V) \\right] \\Theta_d WSA_{\\circ}$\n(5)\n$B^{FFN}(X, d) = LN \\left[ \\sigma(X\\Theta_d W_{FF_1}) \\Theta_d W_{FF_{\\circ}} \\right]$\n(6)\nwhere $\\Theta_d$ and $\\Theta_d$ respectively denote the sliced in and out projection operators, such that:\n$(\\Theta W)_i = (x_i)_{\\left[:d_i\\right]}  \\cdot W_{\\left[:d_i\\right]}\\cdot W_{\\left[:d_i\\right]}^T,$\n$\\Theta_d (XW)_i = x_i \\cdot (W_{\\left[:d_i\\right]})^{\\top}$ (7)\nIn the general Mixture of Nested Experts (MoNE) setting discussed in Section 4.1, the overall block computation for the set of tokens X requires knowledge of the expert assignments for each token beforehand. Given these assignments $m \\in R^N$, such that $m_i \\in \\{1, 2, ..., E\\}$, the computation for the $i^{th}$ token processed by the $j^{th}$ expert can be represented as:\n$B_j(x_i) = [B(X, d)]_i,  d = (d_{m_i})_{i=1}^N;$"}]}