{"title": "YES, Q-LEARNING HELPS OFFLINE IN-CONTEXT RL", "authors": ["Denis Tarasov", "Alexander Nikulin", "Ilya Zisman", "Albina Klepach", "Andrei Polubarov", "Nikita Lyubaykin", "Alexander Derevyagin", "Igor Kiselev", "Vladislav Kurenkov"], "abstract": "In this work, we explore the integration of Reinforcement Learning (RL) approaches within a scalable offline In-Context RL (ICRL) framework. Through experiments across more than 150 datasets derived from GridWorld and MuJoCo environments, we demonstrate that optimizing RL objectives improves performance by approximately 40% on average compared to the widely established Algorithm Distillation (AD) baseline across various dataset coverages, structures, expertise levels, and environmental complexities. Our results also reveal that offline RL-based methods, outperform online approahces, which are not specifically designed for offline scenarios. These findings underscore the importance of aligning the learning objectives with RL's reward-maximization goal and demonstrates that offline RL is a promising direction for applying in ICRL settings.", "sections": [{"title": "INTRODUCTION", "content": "The advent of sequence generation models, particularly those based on the Transformer architecture (Vaswani, 2017), has revolutionized numerous fields by enabling models to generalize beyond their training scope. Notably, large language models can perform novel tasks by processing a textual description and a few examples provided as input, without requiring parameter updates, a phenomenon known as In-Context (IC) learning (Brown et al., 2020). This capability is highly desirable for solving meta Reinforcement Learning (RL) tasks (Beck et al., 2023), where the goal is to produce model capable of generalization to unseen tasks. Recently appeared In-Context RL research direction (Moeini et al., 2025) aims to create general meta RL solvers with scalabel architectures analagous to Large Language Models. However, training such models online is not feasible and might be unsafe. Offline pretraining enhances the applicability of ICRL by eliminating the need for potentially costly or hazardous online interactions, as seen in domains like robotics, autonomous driving, and healthcare. However, current offline ICRL methods face critical limitations.\nScalable approaches such as Algorithm Distillation (AD) (Laskin et al., 2022) and Decision-Pretrained Transformer (DPT) (Lee et al., 2024), along with their variants, have shown promise in offline ICRL. Yet, none of these methods explicitly optimize the RL objective \u2013 maximizing cumulative reward. This oversight introduces significant challenges when tackling offline RL tasks, where leveraging RL to achieve optimal behavior is crucial (Kumar et al., 2022). While recent works (Grigsby et al., 2023; Elawady et al., 2024) have explored scalable online ICRL, these methods rely on numerous heuristics for effective performance and remain untested in offline settings, which are inherently more challenging (Levine et al., 2020).\nOur main goal is to investigate whether methods that optimize RL objective can achieve significantly better results in offline ICRL and whether this improvements are universal across various axis. In particular, we aim to address the following questions: 1) Does explicit optimization of the RL objective improve performance in offline ICRL? 2) How does the effectiveness of this optimization depend on the coverage and quality of offline datasets? 3) Do we need specialized RL techniques from the offline RL family for effective offline ICRL? 4) How would algorithms behave if we do not have access to learning histories but rather a bunch of data? 5) Does RL better handle mixture of dynamics and out-of-distribution dynamics? To answer these questions, we conduct an empirical study using more than 150 datasets derived from widely used GridWorld and MuJoCo (Todorov et al., 2012) tasks. We compare multiple RL-based approaches with Algorithm Distillation, a strong supervised baseline, to evaluate the impact of explicitly optimizing for reward in offline ICRL. To the best of our knowledge, this is the first study to explicitly optimize the RL objective in an offline ICRL setting using a scalable Transformer architecture."}, {"title": "RELATED WORK", "content": "Offline Reinforcement Learning\nOffline RL aims to train agents that maximize reward using pre-collected datasets without interacting with the environment. This setup introduces unique challenges, particularly in handling out-of-distribution (OOD) state-action pairs (Levine et al., 2020). Over the years, this field has witnessed rapid development, with various methods proposed to address these challenges (Kumar et al., 2020; An et al., 2021; Kostrikov et al., 2021; Fujimoto & Gu, 2021). In our study we test widely adopted offline RL baselines for offline ICRL setting in order to demonstrate benefits they bring as reward maximization algorithms. For discrete environments we used Conservative Q-learning (CQL) (Kumar et al., 2020) and Implicit Q-learning (IQL) (Kostrikov et al., 2021). Based on findings from Tarasov et al. (2024b), for continuous environments we used IQL and simple yet effective (Tarasov et al., 2024a) TD3+BC (Fujimoto & Gu, 2021) approach.\nA prominent direction in offline RL involves modeling trajectories with Transformers through supervised learning, as first introduced by Decision Transformer (DT) (Chen et al., 2021). However, subsequent studies (Yamagata et al., 2023; Hu et al., 2024; Zhuang et al., 2024) demonstrated that supervised approaches, which lack explicit reward maximization, often fail with low-quality datasets or datasets which do not contain problem solving trajectories. They struggle to \"stitch\" suboptimal trajectories into optimal policies \u2013 a limitation that can be addressed by methods that directly optimize RL objectives. Our intuition tells that in the context of offline ICRL similar issues might arise when reward is not maximized which is confirmed by our experiments.\nScalable In-Context Reinforcement Learning\nAlgorithm Distillation (AD) (Laskin et al., 2022) marked a significant step towards scalable In-Context RL (ICRL) by leveraging Transformer architectures to learn an \"improvement\" operator. It does so by distilling information from the training histories of single-task agents across various environments. AD assumes access to complete training histories, which may not always be available. In this work we demonstrate that RL-based approaches can levarage datasets more efficiently (especially datasets with low-quality demonstrations) and are able to handle unstructured data better.\nDecision-Pretrained Transformer (DPT) (Lee et al., 2024) introduced another approach, focusing on predicting optimal actions from historical data and a given state. However, this method assumes access to an oracle for optimal action sampling, which is often impractical. RL-based methods that we test in this work do not require access to the oracle.\nNeither AD, DPT, nor their follow-up modifications (Sinii et al., 2023; Schmied et al., 2024; Dai et al., 2024; Huang et al., 2024; Son et al., 2024; Zisman et al., 2024) optimize RL objectives during offline training. This omission can result in suboptimal policies, as these methods essentially adapt supervised learning techniques like DT to the offline ICRL setting, without addressing the fundamental reward maximization goal of RL.\nRecent works such as AMAGO (Grigsby et al., 2023) and ReLIC (Elawady et al., 2024) have explored scalable In-Context RL by incorporating off-policy RL techniques. These methods outperform AD and DT in online RL setups but have yet to be tested in offline environments. Offline RL presents distinct challenges\u2014such as the inability to interact with the environment\u2014that make direct application of online approaches less effective (Fujimoto et al., 2019; Levine et al., 2020). This gap underscores the need for offline-specific methods that explicitly optimize RL objectives. Moreover, AMAGO and ReLIC rely on many implementation details and in this work we demonstrate that solid performance can be achieved without complex modifications."}, {"title": "METHODOLOGY", "content": "RL Incorporation\nIn this study, we use Algorithm Distillation (AD) (Laskin et al., 2022), a transformer-based (Vaswani, 2017) architecture, as our baseline. AD's objective is to predict the next action given the improving learning history as context, where each step is represented as a tuple (state, previous action, previous reward). These tuples are encoded into a single token through concatenation.\nWe retain the same Transformer backbone as AD but introduce several modifications to incorporate RL objectives: inspired by Grigsby et al. (2023), input tuples are augmented with previous done flags to indicate episode termination and current episode step; the next-action prediction head is replaced with value-function heads trained using corresponding RL loss functions. For continuous problems, we also add the policy head.\nWe evaluate three RL methods for discrete environments. Twin Deep Q Network (DQN) (Mnih, 2013): A simple RL method without offline-specific components. Conservative Q-Learning (CQL) (Kumar et al., 2020): A widely used offline RL approach that incorporates value-function pessimism. And Implicit Q-Learning (IQL) (Kostrikov et al., 2021): A popular offline RL method based on implicit regularization, known for its strong performance across diverse tasks. Inspired by the adaptation of IQL for the NLP tasks with ILQL (Snell et al., 2022) we add the CQL term to the IQL loss in discrete environments. We also run tests with continuous environments where we use TD3 (Fujimoto et al., 2018) as online baseline, it's minimalist TD3+BC (Fujimoto & Gu, 2021) offline modification and continuous IQL. During inference, discrete approaches predict actions using the arg max operator. We refer to all of the RL methods with IC- (In-Context) prefix, i.e. IC-DQN, IC-CQL, IC-IQL, IC-TD3 and IC-TD3+BC.\nEnvironments and Datasets\nFor most of our experiments we utilize two environments used by Laskin et al. (2022): Dark Room (DR) and Dark Key-to-Door (K2D). DR is a discrete Markov Decision Process (MDP), while K2D is a partially observable MDP (POMDP). Both environments involve a 2D grid where the agent can move up, down, left, right, or remain stationary, observing only its current position at each step. These are popular environments that allow us to scale our experiments under the limited computational budget for obtaining reliable conclusions. In Section 4.5 we introduce modified DR environment for a separate set of experiments. We also run tests using popular continuous MuJoCo environments widely used in meta RL research (Rakelly et al., 2019): HalfCheetahVel (HCV), AntDir (ANT), HopperParams (HPP) and Walker2DParams (WLP).\nIn DR, the agent starts at the center of the grid and must navigate to an unknown target goal to receive a reward of 1. In K2D, the agent starts at a random location and must first find a \"key\" (reward: 1) and then reach a \"door\" (reward: 1), both of which have unknown locations. In both environments episodes terminate either upon task completion or after a fixed number of steps. In HCV agent must run with a fixed unknown velocity, in ANT agent has to navigate to the unknown point, and in HPP and WLP agent must move as fast as possible avoiding falling but in different instances of environments system parameters (e.g. gravity or masses) are randomized.\nFor DR we consider 9x9 version with episode length of 20 and 19x19 version with episode length of 100. For K2D we test 9x9 version with 50 steps per episode and 13x13 version with 100 steps per episode. This allows us to test performance across different levels of complexity. MuJoCo environments are truncated after 200 steps.\nFor discrete environments, we collect training histories using the Q-learning (Watkins & Dayan, 1992) algorithm, varying the number of histories per target goal (1 or 5). For DR 9x9 we form datasets for 70, 40 and 20 train targets, for DR 19x19 we collect histories with 300, 150 and 75 train targets, and for both K2D versions we created datasets with 1000, 500 and 250 train goals. For continuous environments we collect learning histories from 100, 50 and 25 environments instances using Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018). To analyze the impact of data quality, we partition the trajectory datasets into three expertise levels early, mid and late by dividing original datasets into three equal parts trajectory-wise.\nWe name datasets using the following convention: {environment name}[{grid size}]-{num training targets}-{num histories per target}[-{expertise level}]. Absence of the expertise level in the name indicates the full dataset. For example, \"DR9-70-5\" refers to a dataset from the 9x9 DR environment with 70 training targets and 5 histories per target. Additional dataset details are provided in Appendix B.\nEvaluation\nTo assess the performance of trained policies, we roll out each policy over 100 (4) successive episodes for all discrete (continuous) environments and track performance after 25, 50, and 100 (1, 2, 4) episodes. Additionally, we compute the Normalized Area Under the Curve (NAUC) of episode performance. We also report rliable (Agarwal et al., 2021) metrics above the tracked ones for the reliability.\nThe NAUC provides a single numerical value for comparing agents, as it captures the progression of performance over episodes while being more robust to noise than fixed-episode evaluations. Tracking performance at fixed episodes helps identify convergence rates and potential degradation during rollouts. NAUC is used for selecting the best hyperparameters (details in Appendix A).\nFor the DR environments, we evaluate on all target goals that are excluded from the training set. For all other environments, we use a fixed set of 100 random test targets or configurations that are not part of the training data. To ensure robustness, we follow the evaluation protocol from Tarasov et al. (2024a), using different random seeds for hyperparameter search and final evaluation."}, {"title": "EXPERIMENTAL RESULTS", "content": "We begin this section by comparing the overall performance of the selected methods using discrete environments, demonstrating the suitability of the newly introduced NAUC metric for evaluation. Subsequently, we analyze the performance of these methods across critical offline RL dimensions, including data quality and coverage. Further, we investigate the impact of removing the assumption of access to learning histories, which may not always hold in real-world scenarios (Zisman et al., 2023). In addition, we test the ability to learn in a mixture of dynamics along with the handling of out-of-distribution (OOD) dynamics and run the experiment in a challenging XLand-Minigrid (Nikulin et al., 2023) environment. In the end, we show that benefits from using RL extend to continuous environments.\nOverall Performance\nFor this analysis, we utilized all available discrete datasets. The top graphs in Figure 1 show the averaged metrics for both test and train targets. Across the Dark Room (DR) and Dark Key-to-Door (K2D) environments, we observe that the tested methods maintain stable performance throughout rollouts, confirming that NAUC is a reliable metric for comparative analysis.\nThe bottom plots in Figure 1 display performance profiles based on NAUC. From these results, several key observations can be made. First, RL-based approaches consistently outperform Algorithm Distillation (AD) on average. Second, while all RL methods show similar performance on train targets, there are notable differences on test targets. CQL achieves the best performance on test targets (with a 28.8% average improvement compared to AD), IQL follows closely behind (23.7% improvement) and DQN exhibits the weakest performance among the RL methods on average (16.6% improvement). However, it is worth noting that we did not tune hyperparameters for DQN and reused parameters from CQL, so the DQN's performance has a potential for improvement.\nVarious Coverage\nIn this part of the analysis, we explore the impact of dataset coverage, a critical property in the offline RL setup (Schweighofer et al., 2021). Coverage is examined along two axes: the number of unique training targets and the number of learning histories per target. While multihistory coverage is essential for AD, it may not be feasible in real-world scenarios. To investigate these aspects, we use the complete datasets across all environments.\nThe NAUC scores for each dataset are shown in Figure 2. As expected, all methods perform better with increased target coverage and more histories per target. However, the results highlight notable differences in performance across methods. Offline RL approaches outperform AD across most configurations. The exception is the DR9 datasets, where AD slightly surpasses offline RL in scenarios with 20 and 40 targets when using only one history per target. DQN also outperforms AD on average and, surprisingly, shows superior performance over offline RL approaches on DR19 tasks. In the more complex K2D environments, RL-based methods are significantly more robust to the absence of multiple histories per target. For K2D, RL approaches achieve close performance with the five histories per target setup once a sufficient target coverage level is reached. In contrast, AD experiences a notable drop in performance without repeated targets, underscoring its reliance on repetitive learning histories.\nThe key takeaway is that RL-based approaches are more data-efficient than AD and demonstrate greater tolerance for limited target repetition in learning histories. This robustness makes RL methods more suitable for real-world applications, where complete coverage and extensive learning histories are often unattainable.\nVarious Expertise\nDataset expertise is another critical factor in offline RL (Schweighofer et al., 2021). In this part of the analysis, we evaluate the performance of different methods across datasets of varying expertise levels. The \"complete\" datasets represent full learning histories, which were originally proposed for AD. The mid datasets include interpolation between low-quality and near-convergence trajectories, resembling truncated versions of the complete datasets. In contrast, early and late datasets reflect real-world scenarios: the former consists of low-quality data that is relatively easy to gather, while the latter comprises near-optimal examples of problem solutions. Detailed statistics for these datasets can be found in Appendix B.\nFigure 3 shows the test NAUC scores for the various dataset types. AD performs notably poorly on early datasets, failing to produce policies with NAUC scores higher than 0.4. In contrast, all RL approaches achieve significantly higher scores. DQN emerges as the best-performing method in this setup, likely because low-quality datasets require less regularization, allowing the agent to pursue higher rewards. CQL, which is implemented with cross-entropy loss in discrete cases, can be viewed as an interpolation between DQN and AD. Its performance is closer to AD when the regularization coefficient is high, which limits its potential on low-quality datasets. Reducing CQL's pessimism might yield better results than DQN, but we did not test it due to computational constraints.\nAs anticipated, the mid and complete datasets exhibit similar performance trends, with AD remaining the weakest approach and CQL leading, closely followed by IQL. Surprisingly, AD performs competitively on late datasets with high coverage, despite the lower data diversity in these datasets. In contrast, DQN's performance on late datasets is significantly weaker than the offline RL methods, further underscoring the importance of offline regularization. Additional rliable metrics and final performance scores in Appendix D.2 statistically validate these observations.\nIn summary, the experiments highlight that RL-based methods outperform AD across datasets of varying expertise levels. However, the results also suggest that the hyperparameters of offline RL methods need to be carefully tuned to match the quality of the available data, an aspect not fully explored in this iteration of the study.\nNo Learning Histories Structure\nAlgorithm Distillation relies on the availability of progressing learning histories, with multiple behavior policies collecting data for each task. In practice, however, such structured data is rarely available. To address this, we conducted experiments to evaluate how all considered approaches perform when the inherent ordering of learning histories is absent. First, we tested the algorithms using a randomly shuffled dataset, which disrupts the sequential improvement that AD is designed to distill. To counteract this, we also investigated an approach to build some order: after randomly sampling trajectories, we sort them based on their discounted return values. Although this sorting method may be sensitive to the choice of discount factor, we found it to work effectively for some tasks.\nFor these experiments, we used complete datasets from the DR19 and K2D13 environments with one learning history per target, as this represents a more realistic scenario.\nAs illustrated in Figure 4, on average, RL approaches outperform AD when the data is randomly ordered, with the sole exception of CQL on the DR19-300-1 dataset. Under random ordering, there is little difference among the RL methods on K2D13, while on DR19 DQN exhibits notable superiority \u2013 a result that is consistent with observations on highly sub-optimal (early) datasets. When the unordered data is sorted by discounted return, offline RL methods consistently outperform both AD and DQN in K2D environment. In the DR19 environment, however, only CQL (which can be seen as an interpolation between AD and DQN) maintains its performance lead, while IQL shows diminished results. Surprisingly, on DR19, both DQN and IQL perform better with randomly ordered data than with sorted samples. We do not yet have a plausible explanation for this phenomenon or why it appears exclusively in the simpler DR19 environment.\nIn summary, CQL demonstrates the best performance across different environments and dataset coverages without learning histories access. Additional metrics presented in Appendix D.3 further support this finding.\nMixture of Dynamics\nIn this experiment, we investigate how algorithms perform when trained on environments with different dynamics and subsequently deployed into an environment featuring OOD dynamics. To this end, we introduce a modified version of the DR19 environment, called Janus. In the Janus setup, learning histories are independently collected from two distinct instances of DR19, each governed by a different dynamic function (for example, actions in the second instance may map to inverted directions). Consequently, the training dataset includes examples of behavior under both dynamics, yet no single history contains a mixture of these dynamics. After training the agent on this combined dataset, we deploy it into a grid where the first half exhibits one dynamic and the second half the other, as illustrated in the left graph of Figure 5. The complete datasets are collected using the same configuration as for DR19, with the only modification being that for each learning history, the underlying environment dynamic is uniformly selected at random.\nThis experimental setup allows us to assess how effectively different approaches learn and generalize across multiple dynamics, as well as how they cope with an environment that blends these dynamics in a single deployment. Given the increased task complexity, we extended the number of rollout episodes to 200. The results, presented in the right plot of Figure 5, indicate that when an agent is trained on both dynamics and deployed into an environment featuring only one of them, its performance remains comparable across dynamics. However, when tested in an environment that combines both dynamics, performance decreases for both, with a more pronounced drop in one of the dynamics. This discrepancy is likely influenced by the asymmetry of the test environment, where the central starting position falls within the second dynamic. Ideally, a robust agent should perform uniformly well under both dynamics.\nAcross all conditions, RL-based approaches continue to demonstrate superiority over AD. In isolated tests, IQL outperforms AD by approximately 50%, while DQN and CQL achieve roughly twice the performance of AD. In the Janus environment, RL methods better preserve performance under one dynamic (as evidenced by the slopes of the performance lines) and exhibit slightly improved performance under the other dynamic. It is not surprising that offline RL counterparts do not provide benefits due to the fact that offline algorithms are supposed to avoid the OOD state-action pairs which are unavoidable in the Janus setup and considered offline approaches do not provide guarantees for this case. Tabular scores can be found in Appendix E.8.\nXLand-Minigrid Evaluation\nTo further validate our approach in more challenging settings, we evaluate our methods on the XLand-Minigrid trivial environment (Nikulin et al., 2023) using datasets provided by Nikulin et al. (2024). In order to keep the experiments tractable, we reduce the dataset size by selecting only one learning history per rule set and retaining only the first third of transitions from each history. This subsampling results in a dataset that is just 1% of the original size, which we refer to as the tiny dataset.\nTable 1 presents the performance results on this dataset, reporting both the NAUC and the mean return from the last episode. The results show that RL-based approaches significantly outperform AD: NAUC and mean return scores for RL methods are approximately twice as high as those for AD. Among the RL approaches, DQN slightly outperforms CQL, while IQL achieves marginally better performance than DQN.\nIt is noteworthy that in the original work (Nikulin et al., 2024), AD achieved a mean performance of approximately 0.4 using 100 times more data and three times more rollout episodes (500 episodes compared to 150 in our experiments). These findings clearly demonstrate that the benefits of explicitly optimizing RL objectives extend to more complex and data-sparse environments.\nContinuous State and Action Spaces\nThus far, our experiments have focused on discrete environments, which offer a controlled setting to analyze the behavior of RL-based ICRL methods. However, many real-world applications \u2013 ranging from robotics and autonomous driving to control tasks \u2013 operate in continuous state and action spaces. In this subsection, we extend our experimental analysis to continuous environments to determine how explicit RL objective optimization performs when faced with the additional challenges posed by infinite state and action spaces. This exploration aims to bridge the gap between our current discrete experiments and the demands of real-world applications, ultimately paving the way for broader adoption and further refinement of offline ICRL methods.\nIn Figure 6, we present performance profiles for AD, the online RL method TD3 (Fujimoto et al., 2018), its lightweight offline RL counterpart TD3+BC (Fujimoto & Gu, 2021) and IQL. Consistent with our findings in Section 4.3, the offline RL approaches (TD3+BC and IQL) significantly outperform AD across most setups, with AD matching performance only on late (near-expert behavior) datasets. IQL performs slightly worse than TD3+BC on average. Notably, a key difference emerges between continuous and discrete environments: the online RL method (TD3) demonstrates lower performance than AD in continuous domains, likely due to the more severe challenges posed by out-of-distribution states and actions. These results underscore that ICRL methods incorporating offline RL components not only achieve better performance but also highlight the critical importance of the offline component in continuous settings. See Appendix D and Appendix E for more results and metrics."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this work, we have demonstrated that explicitly optimizing RL objectives is highly beneficial for offline ICRL. Our experiments reveal that incorporating RL optimization leads to improved performance across a variety of environments, dataset coverage levels, and dataset expertise and structure conditions. In particular, even under much smaller hyperparameters tunning budget offline RL approaches consistently outperform Algorithm Distillation and are usually more effective than online methods, highlighting the advantages of offline-specific regularizations and methodologies in many ICRL scenarios.\nFuture work should extend this study by evaluating more complex environments such as NetHack (K\u00fcttler et al., 2020; Kurenkov et al., 2024) or more setups of XLand-MiniGrid (Nikulin et al., 2023; 2024). Additionally, it is essential to explore ICRL in settings with observations, e.g. Meta-World (Yu et al., 2020), to further validate and generalize our findings.\nIt is also important to investigate usefulness of RL approaches in creating generalist models which are trained to operate in various environments which was recently done for Algorithm Distillation (Polubarov et al., 2025) or generalization to completely new environments as it was done by Raparthy et al. (2023).\nAnother promising direction for future work is to investigate the application of offline In-Context RL methods in an offline-to-online setting (Nair et al., 2020; Lee et al., 2022), where model weights are updated during rollouts. While this approach does not offer benefits when using the supervised objectives, the explicit RL objectives we optimize have the potential to further improve performance.\nOverall, our results underscore the importance of aligning learning objectives with the intrinsic goals of Reinforcement Learning, setting the stage for more robust and efficient offline ICRL methods."}, {"title": "ADDITIONAL EXPERIMENTAL DETAILS", "content": "All experiments were conducted using NVIDIA H100 GPUs.\nImplementation Details\nOur implementation of Algorithm Distillation (AD) is based on the Decision Transformer (DT) codebase from Tarasov et al. (2024b). In our version, we remove the return-to-go input and merge state, action, and reward into a single token. This tokenization strategy reduces the overall Transformer sequence length, thereby decreasing both computation time and memory usage. When solving XLand-Minigrid we use similar implementation from Nikulin et al. (2024).\nWhen adapting AD for Reinforcement Learning, we add value function and policy heads (for continuous problems) on top of the original AD backbone. The value heads heads are implemented as two-layer multilayer perceptrons (MLPs) with a Leaky ReLU activation function between layers. Policy heads are three-layer MLPs and analagous to AMAGO (Grigsby et al., 2023) we do not pass gradients to the Transformer backbone from these heads to improve training stability. There are also standard for RL target value function heads. To provide richer input information, we merge the additional previous done flag and step number with the (state, previous action, previous reward) token. In continuous environments Q value heads get the current action as additional input. For continuous IQL we also had to add LayerNorm (Ba et al., 2016) into the heads in order to stabilize learning process.\nHyperparameters Choice\nFor hyperparameter tuning, we use the NAUC metric to select the best model configuration. The tuning is performed on the largest complete dataset with five histories per target, and the best hyperparameters are then applied across other datasets, even though this may result in suboptimal performance, especialy for offline RL approaches.\nFor AD, we tuned parameters that strongly influence performance, including attention dropout, embedding dropout, and residual dropout (each varied over values {0.1, 0.3, 0.5}), label smoothing for discrete environments (tested with values {0.1, 0.3}) and Transformer sequence length (tested over {100, 200}). This tuning resulted in 54 candidate hyperparameter sets. The best values, along with other general hyperparameter settings, are documented in Appendix C.\nDue to computational constraints, the hyperparameters identified for AD were reused for the RL approaches. In the case of CQL, we tuned the discount factor \\(\\gamma\\) over values {0.8, 0.9, 0.95} for XLand-Minigrid and {0.7,0.8, 0.9} for other environments, and adjusted the CQL weight to be within 0.1, 0.3, 0.5 for DR environments, within 0.3, 0.5, 1.0 for Dark K2D environments, within 0.01, 0.05, 0.1 for Janus, and within 0.01, 0.1, 0.5 for XLand-Minigrid. For DQN, we simply adopted the \\(\\gamma\\) values found for CQL without additional tuning. Discrete IQL was tuned over a discount factor \\(\\gamma\\) with the same configuration as for CQL, an IQL parameter \\(\\tau\\) over {0.5, 0.7, 0.9}, and used a CQL weight of either 0.0 or the best value found for CQL. For TD3+BC we tuned discount factor over {0.9, 0.95, 0.99} and BC weight over {0.1, 0.3, 1.0}, for TD3 we reuse TD3+BC best discount factor and set BC weight to zero. For continuous IQL, we ran discount factor search over the same values as for TD3+BC, IQL \\(\\tau\\) over {0.5, 0.7, 0.9} and IQL \\(\\beta\\) over {1,3, 10}. This tuning yielded 9 candidate configurations for CQL and TD3+BC, 18 for discrete IQL, substantially fewer than those evaluated for AD. For continuous IQL, it resulted in 27 candidates, which is twice less than AD search space.\nEach approach was trained over a fixed number of epochs to account for varying dataset sizes: 30 epochs for DR9, HPP and WLP, 15 epochs for HCV, 10 epochs for DR19, K2D9 and ANT, 6 epochs for K2D13. We track metrics after each epoch and report the mean value across multiple seeds for the epoch according to the best NAUC value. Notably, we observed that AD exhibits greater instability during training compared to the RL approaches, meaning that our design choices in the experimental protocol tend to favor AD, yet its performance remains inferior.\nPreliminary experiments indicated that an important hyperparameter controlling AD subsampling is best set to 4 for DR and continous environments and 8 for K2D. When evaluating on incomplete datasets, we reduced these values to 1 for DR and continuous environments and to 2 for K2D to maintain a consistent number of trajectories. In Section 4.4 the subsample parameter was set to 1, as it does not have any motivation there and would just discard a large number of trajectories.\nFor a complete list of hyperparameter values, please refer to Appendix C."}, {"title": "DATASETS DETAILS", "content": "In this section, we describe the data collection process and provide detailed statistics for each of the obtained datasets. We do not provide data for Janus datasets as they are very similar to DR19.\nData Collection\nDiscrete Environments. To construct the complete discrete datasets, we employed a tabular Q-learning algorithm (Watkins & Dayan, 1992) with a linearly decayed \\(\\epsilon\\)-greedy exploration strategy. For the K2D environments, which are originally formulated as POMDPs and require memory to solve, we doubled the state space by mapping each grid position to two distinct states: one corresponding to the scenario where the key has not been collected and the other where it has. This transformation effectively converts K2D into a fully observable MDP. The hyperparameter values used for Q-learning across all dataset collections are provided in Table 2.\nContinous Environments. For collecting the learning histories in continuous environments we used SAC implementation from Clean RL (Huang et al., 2022). We kept most of the SAC hyperparameters default and we present the varied subset in Table 3.\nDatasets representing various levels of expertise are derived by segmenting the complete learning histories into three equal parts on a trajectory-wise basis.\nLearning Curves\nThe learning curves presented in the following graphs illustrate the average returns for all datasets as a function of the episode number within the learning history. By concatenating the early, mid, and late segments, we obtain the complete dataset curves. We do not provide curves for the HPP and WLP due to the variable amount of episodes for each learning history."}]}