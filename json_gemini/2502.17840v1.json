{"title": "A COMBINATORIAL IDENTITIES BENCHMARK FOR THEOREM\nPROVING VIA AUTOMATED THEOREM GENERATION", "authors": ["Beibei Xiong", "Hangyu Lv", "Haojia Shan", "Jianlin Wang", "Zhengfeng Yang", "Lihong Zhi"], "abstract": "Large language models (LLMs) have significantly advanced formal theorem proving, yet the scarcity\nof high-quality training data constrains their capabilities in complex mathematical domains. Com-\nbinatorics, a cornerstone of mathematics, provides essential tools for analyzing discrete structures\nand solving optimization problems. However, its inherent complexity makes it particularly challeng-\ning for automated theorem proving (ATP) for combinatorial identities. To address this, we manu-\nally construct LeanComb, combinatorial identities benchmark in Lean, which is, to our knowledge,\nthe first formalized theorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which combines candidate\ntactics suggested by a self-improving large language model with a Reinforcement Learning Tree\nSearch approach for tactic prediction. By utilizing ATG4CI, we generate a LeanComb-Enhanced\ndataset comprising 260K combinatorial identities theorems, each with a complete formal proof in\nLean, and experimental evaluations demonstrate that models trained on this dataset can generate\nmore effective tactics, thereby improving success rates in automated theorem proving for combina-\ntorial identities.", "sections": [{"title": "1 Introduction", "content": "The formal theorem proving entails translating theorems and their proofs into a machine-readable format, which allows\ncomputers to autonomously execute logical reasoning and verification, thereby ensuring the absolute correctness of\nthe theorems [25]. This process is vital to eliminate potential reasoning errors. Formal proofs have been successfully\napplied to significant results such as the Kepler Conjecture [30], the Four Color Theorem [26], and the Clausen-\nScholze Conjecture [15]. However, interactive theorem proving (ITP) is labor-intensive, particularly when tackling\ncomplex theorems, demanding a high degree of expertise from human specialists. For instance, the formalization\nof the Kepler Conjecture spanned 11 years and involved more than 20 researchers, culminating in a proof through\nhundreds of thousands of logical steps [30].\nTo reduce the high cost of manual formalization, a practical approach is to implement automated theorem proving\n(ATP), which streamlines the interaction between human experts and proof assistants. In recent years, with the rapid\nadvancement of artificial intelligence (AI), large language models (LLMs) have begun to play a role in ATP, integrating\nwith proof assistants to enhance the capabilities of theorem provers. Some approaches exploit the generative capabili-\nties of LLMs to build complete proofs of theorems in a single decoding process [42, 103, 9]. At the same time, other\nmainstream methods utilize fine-tuned models to generate single-step candidate proof tactics based on the current\nstate of the proof [93, 105, 44, 72, 32], gradually building complete proofs by integrating various search algorithms.\nThe works described above primarily focus on fundamental mathematical problems, typically at the high school and\nundergraduate levels.\nRegarding advanced mathematics or more complex mathematical fields, theorem provers based on LLMs still face sig-\nnificant challenges. AlphaProof and AlphaGeometry 2 [88] successfully solved four of the six problems in this year's"}, {"title": "2 Related Work", "content": "Automated theorem proving has gained prominence in artificial intelligence [79, 94, 101], yet its application remains\nlargely restricted to relatively simple mathematical problems and struggles with proving more complex statements.\nData scarcity and its uneven distribution across different mathematical domains is a key challenge.\nAmong the largest publicly available datasets in AI4Maths, Numina [58] contains 860K competition-level math prob-\nlems with solutions following the Chain of Thought (CoT) [98] reasoning paradigm. In contrast, Mathlib [61], the\ncommunity-maintained Lean formal mathematics dataset, provides a rigorously structured repository spanning algebra,\nnumber theory, and combinatorics. However, both datasets rely heavily on manual formalization, leading to limited\ncoverage and uneven representation across specialized mathematical fields.\nTo address the limitations of data scarcity, researchers have explored automated theorem generation [70]. In recent\nyears, theorem generation techniques based on neural networks and LLMs have emerged as a promising direction,\nopening up new possibilities for both theorem generation and automated proving [24, 99]. Existing approaches\nleverage various strategies for theorem generation: INT [99] integrates inequalities to construct new theorems, Lean-\ndojo [105] retrieves and synthesizes formal statements from Mathlib, and MetaGen [95] transplants proof trees while\nemploying reinforcement learning to align theorem generation with human reasoning. Meanwhile, another class of\ngenerators relies on LLMs to assist in theorem generation. Specifically, these models are often employed as premise\nselectors to identify key reasoning steps or to iteratively sample from existing theorems, generating new ones that\nadhere to specific rules [89, 92, 67, 72, 100]. Furthermore, the PACT method [32] extracts data from theorems and\ngenerates nine distinct language modeling tasks, enabling data augmentation within the Lean theorem prover.\nHowever, most existing theorem generators expand datasets by extracting sub-theorems from complete theorems or\nsynthesizing new ones. Against this backdrop, we combine LLMs' broad knowledge coverage with RL's exploration-\ndriven capabilities to conduct tactic prediction research based on our manually constructed benchmark, LeanComb.\nOur goal is to generate high-quality, novel theorems that contribute to developing data resources in specialized mathe-\nmatical fields while advancing the capabilities of automated theorem-proving systems."}, {"title": "3 A Combinatorial Identities Benchmark", "content": "In this section, we introduce LeanComb, a manually formalized benchmark for combinatorial identities based on Lean\n4 [19]. To the best of our knowledge, LeanComb is the first formal theorem benchmark specifically dedicated to\ncombinatorial identities, covering a broad range of combinatorics topics, including classical and modern identities.\nThe dataset aims to provide a rigorously verified and formalized collection of combinatorial identities that can be used"}, {"title": "4 Lean4Kit", "content": "Lean4Kit, built on Lean 4, provides offline data extraction and interaction capabilities, significantly enhancing au-\ntomated theorem generation and proving. First, Lean4Kit extracts state-tactic pairs from Lean code to construct a\ntraining set. Second, it enables programmatic interaction with the Lean system, supporting tactic prediction and auto-\nmated theorem proving.\nData Extraction. Lean4Kit can access the current proof state in run-time during the Lean theorem proving process.\nBy interacting with the Lean system, it tracks the proof process and dynamically updates the proof state.\nState Information: Our toolkit constructs the sequence of proof states by extracting the Lean code and the proof goal.\nAdditionally, it captures the error message if an error occurs after invoking a tactic and identifies whether a proof state\nhas been successfully resolved after executing a tactic.\nPremises and Tactics: For the Lean code of a given theorem, the toolkit extracts all premises and variable types,\nensuring that the premises' names align with those of the Lean libraries. If the proof state is valid at each step of the\nproof, it also captures the corresponding tactic used.\nInteraction. Another key feature of the toolkit is its ability to interact with Lean 4. It can initialize the proof state,\nupdate it after executing the tactic, and trace the feedback information, including error messages or proof completion.\nLean4kit can also be utilized for tactic prediction, filtering candidate tactics that can efficiently modify the proof state.\nBelow is a detailed description of the interactive features:\nget_init_state (theorem): Given a theorem, our toolkit obtains its initial proof state and extracts data from that state. If\na theorem, including statement and proof, fails validation during interaction, its initial state will be marked as an error."}, {"title": "5 The Framework of Automated Theorem Generator for Combinatorial Identities", "content": "This section introduces ATG4CI, an iterative framework for automated theorem generation for combinatorial identi-\nties. We propose a tactic prediction method combining a fine-tuned LLM with a reinforcement learning algorithm.\nThe framework generates candidate theorems using tactic prediction, verifies them through theorem validation, and\ncompiles the results to build an enhanced dataset. The generated theorem dataset can fine-tune the LLM through iter-\native refinements, enhancing its tactic prediction capabilities. Meanwhile, with its enhanced predictive capability, the\nimproved LLM can boost the quantity and quality of automatically generated theorems.\nAs shown in Fig. 1, ATG4CI comprises three essential stages: Partial Proof Paths (P3s) Construction, Candidate\nTheorem Generation, and Theorem Validation. The procedure begins with the LeanComb training set  $L_t$, consisting\nof formalized combinatorial identities, serving as the foundational data for data augmentation and model fine-tuning.\nBuilding upon this, the pipeline enters the P3s Construction phase, where the Lean4Kit tool is employed to transform\neach theorem into a proof tree. Their corresponding state-policy pairs are extracted from these trees. The states are\nthen corrected to ensure their quality and correctness.\nFollowing this, in the Candidate Theorem Generation phase (top right of Fig. 1), the procedure begins with generating\ncandidate tactics using a fine-tuned model $M_{ct}$. The candidate tactics are then refined by selecting the most appropriate\nones for each partial proof path (P3) through a Reinforcement Learning-based search tailored to address the specific\nrequirements of the combinatorial identities domain. This tactic prediction process is repeated to ultimately derive\ncandidate theorems, followed by the Theorem Validation stage (bottom of Fig. 1), where redundant theorems are\neliminated, and the correctness of the theorems is verified. Specifically, this stage consists of two key steps: Theorem\nDeduplication and Error Correction, both of which ensure the uniqueness and correctness of the dataset.\nUltimately, the validated theorems are compiled into a new dataset $G^*$, fed to train the model $M_{ct}$, enhancing its ability\nto generate more effective candidate tactics. Finally, the generated dataset is combined with the LeanComb training set\n$L$ to form the LeanComb-Enhanced dataset $E^*$, which subsequently improves the performance of automated theorem\nproving."}, {"title": "5.1 Partial Proof Paths (P3s) Construction", "content": "We focus on the P3s construction process, explaining how to use Lean4Kit to extract partial proof path $P_s$ from\nformalized theorems. To facilitate the exploration of new proof paths for P3s, we visualize all tactics within the proof\nenvironment, capturing the state transitions that occur before and after applying each tactic. A fully formalized proven\ntheorem, consisting of $n$ tactics, can be represented as a proof tree: the root theorem forms the root node, tactics are the\nedges, intermediate states are the child nodes, and the \u201cno goals\u201d state is the leaf node. From this tree, $P_s = \\{p_i\\}_{i=1}^n$\nis extracted by tracing the paths from the root to the intermediate states.\nAs shown in Example 1, the root theorem, represented by the root node in (1), transits through three sequential tactics\nand eventually reaches the \"no goals\" state at the leaf node, resulting in a four-layer proof tree. From this tree, two\nP3s are extracted: one from the root to the state after applying \u201crw[mul_sum]\u201d, and another from the root to the state\nafter applying \"rw[sum_Ico_eq_sum_range]\"."}, {"title": "5.2 Candidate Theorem Generation", "content": "The section explains how to generate candidate theorems from a given partial proof path, which consists of two key\nsteps: candidate tactic generation, produced by a fine-tuned model, and tactic prediction, based on a reinforcement\nlearning search for combinatorial identities (RLSCI).\nCandidate Tactic Generation. We start with employing a fine-tuned model $M_{ct}$, trained by the training set of Lean-\nComb, to generate candidate tactics $cts$ for $P_s$. To enhance the quality and diversity of candidate tactics, we adopt\nan iterative refinement tactic inspired by self-improvement techniques for fine-tuning models. Initially, the model is\nfine-tuned on the training set of the LeanComb benchmark $L$ and Lean's foundational library, Mathlib4. Given a\npartial proof path, the improved model can generate $t$ candidate tactics, where $t$ is a prior positive integer. The model\nwill be continuously refined in subsequent iterations through fine-tuning with the augmented theorems generated from\nthe previous iteration. This iterative framework not only enhances the model's capacity to propose effective tactics but\nalso broadens its exploration of diverse tactic spaces."}, {"title": "Tactic Prediction", "content": "The tactic prediction, based on RLSCI, consists of three primary steps: selecting candidate tactics,\nexpanding the P3s, and back-propagating values from the leaf nodes to the root. These steps are iteratively performed\nuntil the proof is completed or no viable tactic is identified. If successful, the process discovers a complete proof path\nfor the root theorem $R_T$; otherwise, it generates a candidate proof path $cpk$ for $k = 0, 1, . . ., S$.\nOur RL framework comprises a critic model $C_c$ and a policy model $P_o$. Completed proof nodes are assigned a value\nof 1, while failed nodes are assigned a value of -1. Unresolved nodes are evaluated using the Polynomial Upper\nConfidence Trees (PUCT) method [83]:\n$Q_{PUCT}(s) = Q(s,t) + C_{puct} \\cdot P(s, t) \\cdot \\sqrt{\\frac{\\sum_{b}N(s,b)}{N(s,t) + 1}}$,\nwhere $Q(s, t)$ is the estimated value of the state-tactic pair, obtained from the value network or learned from past\nsimulations, and $P(s, t)$ is the probability of selecting a tactic in state $s$ based on the policy network, and $C_{puct}$ is the\nexploration coefficient. In contrast, $N(s, t)$ is the count of executions of tactic $t$ in state $s$ during the prediction process.\nAll successful tactics are stored as training data for LLMs. During backpropagation, values from the leaf nodes are\npropagated to the root, updating the visit counts $N(s, t)$ and cumulative action values.\nThe process aims to propagate proof paths as far as possible, terminating when the proof is complete or when no viable\ntactic can be found. In the former case, the leaf node is marked as \u201cno goals\u201d, indicating the discovery of a new proof\nfor the root theorem $R_T$. In the latter case, the goals corresponding to the leaf nodes of the candidate proof paths are\ntreated as candidate theorems, $CT_m = {CT_i}_{i=0}^n$.\nTo generate new proofs, we incorporate the original root theorem $R_T$ into the hypotheses and then apply tactics\nfrom the candidate path until the goal aligns with the target. Once aligned, the \"assumption\" tactic resolves the goal,\ncompleting the proof. The following example illustrates the above procedure for ATG4CI."}, {"title": "5.3 Theorem Validation", "content": "Note that not all candidate theorems are correct or unique, so a validation process, including theorem deduplication\nand correction, is necessary to retain the valid theorems.\nTheorem Deduplication: Candidate theorems are deduplicated from two perspectives. First, textual duplication is\nidentified by comparing the goals, premises, and proof steps. Identical theorems are merged, retaining only one.\nSecond, mathematical equivalence is checked by simplifying redundant terms (e.g., +0, -0, *1, and /1), ensuring that\nonly one mathematically equivalent version is kept.\nTheorem Correction: After deduplication, as some candidate theorems may still contain expression errors or fail to\npass the proof process, correctness refinement is performed. After candidate theorems are verified successfully by\ninteracting with Lean, they are directly added to the generated dataset $G^*$. Those that fail verification are categorized\nby error type and corrected using the corresponding correction methods. The corrected theorems are then added to $G^*$.\nThe following section elaborates on the error types and their corresponding correction tactics.\n\u2022 Incomplete Errors. An incomplete error occurs when a candidate tactic generates multiple subgoals, all of\nwhich are correct, but at least one subgoal remains unproven, preventing the completion of the theorem's\nproof. The standard MCTS [17] method can be applied to recover incomplete theorems and generate addi-\ntional proof steps to complete the proof.\n\u2022 Type Errors. Type errors arise due to Lean's representation methods, where variable types in subgoals are\nnot always explicitly stated during interactions with Lean or during the data extraction process. For example,\nif the original theorem's goal contains the term \u201c(-1 : R)\u201d, the extraction may capture only \u201c(-1) ^ \u2191 \u201d. To\naddress this, we identify the type information from the original theorem and annotate the newly generated\ntheorems with the correct type labels.\n\u2022 Logical Errors. Logical errors occur when applying a candidate tactic generates multiple subgoals, but at\nleast one contains a logical inconsistency, preventing further progress in the proof. For instance, consider the\nfollowing theorem where n must be a natural number: $\\sum_{k=1}^{n} C_{k-1}^n = n \\sum_{l=1}^{n-1} C_{l-1}^n$. After applying the\ntactic \u201crw[sum_Ico_succ_top]\u201d, two subgoals are generated, one of which is $1 < n$, which contradicts the\ndefinition of natural numbers. Theorems with such logical inconsistencies are considered irreparable at this\nstage and must be discarded.\nThen, we briefly introduce the main steps of ATG4CI implemented in Algorithm 1. The procedure takes as inputs the\ntraining set of LeanComb Benchmark L, the model $M_{ct}$ that provides candidate tactics cts, search method RLSCI,\nthe maximum number of iterations $n$, and returns the LeanComb-Enhanced dataset $E^*$"}, {"title": "6 Experiments", "content": "In this section, we employ ATG4CI to construct the LeanComb-Enhanced dataset for combinatorial identities. Subse-\nquently, we evaluate the performance of fine-tuned LLMs in automated theorem proving on both the LeanComb test\nset and the miniF2F dataset. Our models, trained on both the LeanComb benchmark and the Enhanced dataset, consis-\ntently achieve higher proof success rates compared to the baseline, demonstrating the effectiveness of our dataset."}, {"title": "6.1 LeanComb-Enhanced Dataset Generated by ATG4CI", "content": "In this experiment, we iteratively fine-tune the general-purpose LLM Llama3.1-8b [6] to assist the theorem generation\nprocess by providing candidate tactics for tactic prediction. The number of tactics suggested by the model for each\nnode is limited to 16.\nTactic Prediction Model Architecture. In tactic prediction process, the RL component of the RLSCI search algorithm\nis implemented using two neural networks: a policy network and a critic network. Each network comprises two linear\nlayers, each with a dimensionality of 16. The training process spans a total of 10 iterations. During each decision-\nmaking step, the algorithm performs 100 simulations, with the total number of full events per iteration fixed at 20.\nFurthermore, each node is allowed to propose up to 16 candidate theorems. The search time is restricted to 300\nseconds."}, {"title": "6.2 Theorem Proving on LeanComb Benchmark and MiniF2F", "content": "We evaluate the theorem-proving capabilities of LLMs on the test sets of LeanComb benchmark and miniF2F [107],\nutilizing Pass@1 (%) as the primary evaluation metric. Pass@1 is defined as the success rate of constructing a valid\nproof within a single attempt, constrained by a strict wall time limit of 10 minutes. The experiment involves four\nmodels: Gemma2 [86], Mathstral3 [2], Llama3 [28], and Mistral [41]. The baseline models are initially pre-trained\non Mathlib4 and subsequently fine-tuned on the LeanComb training set. To further enhance their performance, an\nadditional fine-tuning stage is conducted using an augmented dataset generated by our theorem generator. During\ninference, the number of candidate tactics generated by the models is restricted to a maximum of 16.\nEvaluation on LeanComb Benchmark. We utilize the LeanComb benchmark to optimize the tactic selection of\nLLMs, thereby substantially enhancing their theorem-proving capabilities. As presented in Table 3, experimental\nresults indicate that all models exhibit significant performance improvements after fine-tuning on the LeanComb and\nLeanComb-Enhanced datasets, with success rate increases ranging from 5% to 17%. Notably, models trained on the\nfirst iteration of generated data (1st Iter) show substantial improvements, with success rates increasing by up to 13%\nover the baseline. However, further refinement in the second iteration (2nd Iter) leads to even more significant gains,\nwith a maximum success rate of 26%. This iterative process underscores the necessity of multiple fine-tuning stages\nto fully unlock the potential of the generated datasets."}, {"title": "7 Conclusion", "content": "In this work, we presented a novel automated theorem generator, ATG4CI, for combinatorial identities, which started\nby calling LLM to yield the candidate tactics and then applied deep reinforcement learning for tactic prediction.\nWe manually built LeanComb benchmark for combinatorial identities and automatically generated the LeanComb-\nEnhanced dataset obtained from ATG4CI. To evaluate its impact, we fine-tuned LLMs on these datasets. Experimental\nresults indicate that our benchmark and dataset significantly improve the performance of LLMs in automated theorem\nproving."}, {"title": "A Identities in LeanComb Benchmark", "content": "The Lean Comb benchmark dataset is constructed based on authoritative literature in the field of classical combina-\ntorics [84, 27, 82], aiming to provide a high-quality mathematical reasoning benchmark to support automated theorem\nproving for combinatorial identities. The dataset comprises a total of 727 theorems, with the training set consisting\nof 627 theorems (including 418 combinatorial identities and 209 general theorems), while the test set contains 100\ntheorems. Additionally, we have formally introduced 9 new mathematical definitions, covering fundamental concepts\nsuch as Bell numbers, Stirling numbers of the first and second kinds, and combinations and permutations, thereby\nenhancing the expressiveness of the benchmark dataset.\nThis dataset encompasses numerous classical theorems in combinatorics, including the Negative Binomial Series,\nBinet's Formula, Vandermonde's Identity, Trinomial Revision. For example:\nNegative Binomial Series: $\\forall x \\in [0, 1] \\text{ and } n \\in Z_{\\geq 0}, \\quad \\frac{1}{(1-x)^{n+1}} = \\sum_{k=0}^{\\infty} {n+k \\choose n} x^k,$\nBinet's Formula: $F_n = \\frac{\\phi^n - \\psi^n}{\\sqrt{5}}$\nThe expressions ${n+k \\choose n}$ and $C_n^k$ are equivalent. To systematically illustrate the diversity of the dataset and the core math-\nematical structures involved, we have selected several representative combinatorial identity examples and categorized\nthem based on their roles within the dataset. The selected examples are divided into two parts: one from the training set\nand the other from the test set. Each part includes various types of combinatorial identities, covering key topics such\nas the properties of Stirling numbers, classical combinatorial identities, and their variants. These examples not only\nprovide an intuitive understanding of the dataset's composition but also establish a solid foundation for subsequent\ntheoretical analysis and model evaluation."}, {"title": "A.1 Training Set", "content": "idt_15\nGoal: $\\sum_{k=0}^m {n+k \\choose n} = {n+m+1 \\choose m}$\nidt_101\nGoal: $\\sum_{k=0}^{n} \\frac{(x^{k+1} - x_1^{k+1})y^{n-k}}{(k+1)^2} = \\sum_{k=0}^{n} \\frac{((x_2+y)^{k+1} - (x_1+y)^{k+1})y^{n-k}}{(k+1)^2}$\nidt_105\nPremises: a, b \u2208 R\u22650\nGoal: $B(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$\nidt_106\nPremises: n - k > -1\nGoal: $\\frac{\\partial}{\\partial k} = (n + 1) \\int_0^1 x^k (1 - x)^{n-k}dx$\nidt_109\nPremises: x > 0\nGoal: $\\sum_{k=0}^{n} (-1)^k {n \\choose k} \\frac{k!}{k+x} = \\frac{n!}{x(x+1)...(x+n)}$\nidt_112"}, {"title": "A.2 Test Set", "content": "test_002"}, {"title": "B Lean4Kit", "content": "In our automated theorem generator ATG4CI, Lean4Kit plays a key role in data extraction and interaction. During the\nexperimental evaluation and testing phase, Lean4Kit assists LLMs in theorem proving through interaction with Lean\n4. In fact, given any repo in Lean 4, our offline toolkit can convert Lean files into JSON format data, extracting all\nstate-tactic pairs."}, {"title": "B.1 Data Extraction from Lean Codes", "content": "The complete Lean code (including imports) is converted into a JSON-formatted data structure tree (infotree) in the\nstatic extraction process. The conversion function run_all_tactics(self, code, env=None, verbose=True) returns data\nin a format that includes the tactic, as well as the before-and-after states of the goals (goalsBefore and goalsAfter), for\nexample:"}, {"title": "B.2 Dynamic Interaction with Lean for Theorem Proving", "content": "In the dynamic interaction process, we interact with Lean to perform automated theorem proving. The process mainly\nrelies on the following functions:\n\u2022 run_import(self, code, env=None, verbose=False): Used to import necessary environments and dependen-\ncies.\n\u2022 new_thm(self, code, env=None, verbose=False): Generates a new theorem based on the provided theorem\ndescription.\nThe provided code parameter represents the description of the initial theorem, for example:\n1 theorem idt_84(n: N) (h: m < n): \u2211 k in range (n + 1), n.choose k * (n - k)^m (-1: R)^k = 0\n2 := by sorry\nThis function returns an initial state for subsequent tactic applications:\n\u2022 run_tactic(self, tactic, proofState, cmd_type= 'tactic', verbose=False): Executes a tactic and returns the new\nstate after the tactic is applied.\n\u2022 run_have_tactic(self, tactic, proofState, cmd_type='have', verbose=False) : Executes a \"have\" tactic.\nDuring the interaction, we can also use function is_correct_and_finished(self, code, verbose\n= False, timeout = 160) to check if the theorem is correct and whether the proof is complete, with the judg-\nment based on the information view (Lean Infoview) on the right side."}, {"title": "C An Illustrative Example for ATG4CI", "content": "For the LeanComb benchmark, data augmentation is performed using our theorem generator, ATG4CI. Some theorems\nin the dataset are augmented to generate up to 4,000 new instances in one iteration, resulting in a total of 260,466 newly\ngenerated theorems. Among these, 56,747 erroneous theorems are successfully corrected. Below, we demonstrate the\nworkflow of ATG4CI using a typical example, which corresponds to the mathematical formula $\\sum_{k=0}^{n} {n \\choose k} F_{m+k} =$\n$F_{2n+m}$:"}, {"title": "D Dataset Statistic", "content": "We analyzed the proof steps of the LeanComb benchmark and the enhanced dataset on a per-tactic basis and visualized\ntheir distributions in Fig. 3 and 4. Proof steps per tactic serve as an indicator of theorem complexity. Notably, as proof\nsteps increase, the distribution of theorem steps becomes less concentrated. This phenomenon may result from the\nbroader range of theorem steps observed in more complex cases. For instance, short proof steps (e.g., between 1 and\n10) correspond to a higher number of theorems, which are more densely distributed. In contrast, when the proof steps\nexceed 40, the number of theorems decreases significantly, and their distribution becomes sparser.\nSpecifically, in the LeanComb benchmark, the maximum proof steps reach 166, with 10 theorems having proof steps\nexceeding 100 lines. This suggests that the LeanComb benchmark contains a small number of highly complex theo-\nrems. In contrast, the maximum number of proof steps in the enhanced dataset is 124, with 800 theorems having proof\nsteps exceeding 100 lines. This indicates that the enhanced dataset contains a more significant number of long-proof"}, {"title": "D.2 Tactic Types Distribution", "content": "This section presents a statistical analysis of the distribution of two datasets based on the number of tactic types\ninvolved in the proof of each theorem. A greater number of tactic types employed in the proof of a theorem gener-\nally indicates a higher level of complexity in the proof, as well as reflecting the diversity and depth of the problem.\nMoreover, the more tactic types used in the generated theorems, the stronger the model's ability to explore and apply\ndifferent tactics.\nFigure 6 illustrates the distribution of theorem counts as a function of the number of tactic types in the enhanced\ndataset $E^*$. The theorem count peaks at 25,753 when the number of tactic types reaches six, after which a gradual\ndecline is observed as the number of tactic types increases. This decline becomes particularly steep when the number\nof tactic types exceeds 20. This trend suggests that most theorems can be proven using a relatively small set of tactic\ncombinations, reflecting the dominance of simpler theorems in combinatorial mathematics. However, the sparsity of\ntheorems requiring many tactic types points to an underrepresentation of complex theorems in the dataset, presenting\na significant challenge for automated theorem-proving systems.\nFigure 7 presents the distribution of theorem counts across different numbers of tactic types in the LeanComb bench-\nmark $L^*$. The chart shows that the majority of theorems are concentrated in the lower ranges of tactic types, with a\npeak occurring at tactic type 6. This suggests that simpler tactics are sufficient to prove most theorems, highlighting\nthe prevalence of less complex theorem structures. However, as the number of tactic types increases, the theorem\ncounts decrease significantly, indicating that highly complex theorems are relatively rare. This distribution implies\nthat while simpler tactics are effective for many theorems, more sophisticated tactics are necessary for handling sparse\nbut complex cases.\nThe distribution of tactic types is predominantly concentrated within the first ten types, with the highest theorem count\noccurring at six tactic types. Beyond this, the theorem count gradually decreases; however, hundreds to thousands\nof theorems remain even for tactic types ranging from 20 to 40. This demonstrates that the dataset contains many\ncomplex theorems, offering a significant challenge and utility for evaluating automated theorem-proving systems."}, {"title": "E More Evaluation Results on LeanComb Benchmark", "content": "E.1 Evaluation on LeanComb with Varying Time Limits and Candidate Tactic Numbers\nThe results in Table 5 demonstrate the performance of several models on the LeanComb test set under the constraints of\na 300-second time limit and a maximum of 8 candidate tactics. The success rates across different models consistently\nshow improvements when fine-tuned on LeanComb and further enhanced with LeanComb-Enhanced."}, {"title": "E.2 Automated Proof Results of Our Models", "content": "In this section, we analyze the proof process of the models. The table summarizes the average proof lengths across all\nmodels, revealing that the proof lengths range from 2.7 to 3.9. Notably, for both the Mathstral and Llama models, the\nenhanced versions consistently exhibit shorter proof lengths compared to their comb counterparts. This suggests that\nthe models have learned to adopt more efficient and simpler tactics for generating proofs through extensive training on\nlarge datasets. For instance, as shown in the example for test_087, Mathstral comb requires 11 steps to complete the\nproof, whereas Llama3 enhanced accomplishes the proof in just four steps."}, {"title": "F Experiments Details", "content": "During the theorem generation process, we classify theorems and verify the correctness of candidate theorems. Incor-\nrect theorems are grouped separately, and different correction methods are applied based on their types.\nThe standard MCTS with our fine-tuned Llama 3.1 corrects these theorems. We set the number of candidate tactics (i.e.,\nvisit counts) per node to 16 and the number of simulations per node to 100. It generates full-proof search trees through\nselection, expansion, and backpropagation, and complete-proof steps are engendered accordingly. The selection phase\nconsiders the average reward and exploration of nodes, using the UCB1 algorithm to select the optimal node [3]:\n$UCB1 = \\frac{W_i}{N_i} + C \\sqrt{\\frac{3ln N_p}{N_i}}$"}]}