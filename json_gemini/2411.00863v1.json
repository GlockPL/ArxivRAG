{"title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation", "authors": ["Chenyang An", "Shima Imani", "Feng Yao", "Chengyu Dong", "Ali Abbasi", "Harsh Shrivastava", "Samuel Buss", "Jingbo Shang", "Gayathri Mahalingam", "Pramod Sharma", "Maurice Diesendruck"], "abstract": "In the field of large language model (LLM)-based proof generation, despite being trained on extensive corpora such as OpenWebMath and Arxiv, these models still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the suboptimal order of each proof data used in training. Published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. However, this order aims to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders are substantial \u2013 with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order.", "sections": [{"title": "1 Introduction", "content": "Recent works have shown the potential of large language models (LLMs) to perform mathematical reasoning and proof generation in both natural language and formalized environments (Yang et al., 2023a; Welleck et al., 2021; Lample et al., 2022; Miku\u0142a et al., 2023; Wang et al., 2023).\nMeanwhile, leading models consume far more data than a human could (datasets like OpenWebMath contain 14B tokens), and still perform sub-optimally on reasoning benchmarks (Paster et al., 2023; Azerbayev et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Paster et al., 2023). This is also true for downstream reasoning tasks where LLMs need further fine-tuning (Yang et al., 2023b; An et al., 2024). We identify this phenomenon as the training inefficiency problem.\nOne possible explanation for such low training efficiency in proof generation for LLMs is poor internal order within each training data sample in the math domain. For instance, consider the purely logical order that many textbook proofs follow, where each step logically proceeds from the previous steps based on the deductive rules. This order is widely adopted for presenting proofs publicly because it allows for straightforward verification of the proof's correctness. However, from a proof discovery perspective, this order is not intuitive, i.e. not the order in which the proof is actually discovered. For human learners, the most intuitive order of proof steps is one in which all steps contributing to the derivation of a particular proof step are presented prior to it. We call such order the intuitively sequential order. These steps, which aid in the derivation of a given proof step, are referred to as intermediate supervision for that step.\nConsider the following concrete scenario where the intuitively sequential order is preferred over the purely logical order in human learning and model training. A researcher wants to prove a math statement. He identifies a set of lemmas useful for the proof. But in order to transform the statement into a form where these lemmas can be applied, several preliminary steps are required. A textbook proof usually presents the preliminary steps first and then applies the lemmas, following the purely logical order. However, for LLM training \u2013 and for human learning - the helper lemmas are easy to figure out"}, {"title": "2 Related Work", "content": "Problem of next-token prediction training Existing research has exposed weaknesses in the next-token prediction mechanism. For example, (Berglund et al., 2023) showed the Reversal Curse: a model trained on \"A is B\" does not know that \"B is A\". (Golovneva et al., 2024) proposed to train a model both sequentially and reversely, using two tasks together to enhance performance. In contrast, our work demonstrates that optimal orders exist for next-token prediction learning, specifically by the rule of positioning intermediate supervision to the left. (Bachmann and Nagarajan, 2024) highlighted a similar toy problem known as the star-path problem, where initial errors substantially reduced the model's ability to predict later tokens.\nOrder effect on LLM: Context Several works have studied how disrupting order in prompts might affect model performance. (Chen et al., 2024) showed that LLMs achieve the best performance on GSM8K (Cobbe et al., 2021) when the premise order aligns with the context required in intermediate reasoning steps. (Liu et al., 2024) discover the lost in-the-middle phenomenon in the long-context scenario: LLMs perform best when relevant information is placed at the start or end of the input context, and perform poorly when important information is located in the middle.\nOrder effect on LLM: Digits For long-digit addition, prior works demonstrated that reversing numbers in training data can help LLMs achieve better performance on digit-addition tasks (Lee et al., 2023; Zhou et al., 2023b, 2024). While their work only applies to digit computation since they employed digit-wise reversal, our work studies the positional role of intermediate supervision in general for the proof generation task."}, {"title": "3 Preliminary", "content": "3.1 Problem setting\nGiven algorithm A and input $r_0$ from the problem space, we assume:\n1. Finiteness. Algorithm A generates a finite sequence of steps $A(r_0) = r_1, A({r_0, r_1}) = r_2, ..., A({r_i}) = r_{n_{r_0}}$, where $r_{n_{r_0}}$ is the last step that A will generate when given $r_0$, which usually is the final answer.\n2. Strongest right-to-left dependent order among all possible orders. Each step $r_i$ highly depends on $r_j$ for $1 <= j < i$, i.e. the probability for $A({r_k}_{k=1}^{j-1}) = r_j$ is high for all $r_0$ and j. For every $r_0$, ${r_i}_{i=0}^{n_{r_0}}$ serves as intermediate supervision for models to predict $r_{m+1}$ for all $0 <= m < n_{r_0}$ in the sense of (Wies et al., 2022). We call this order the intuitively sequential order. All other orders of the sequence have a weaker right-to-left dependency than the aforementioned order.\nGiven a model $M_\\theta$ with $\\theta$ as weights of the model, our goal is to enable $M_\\theta$ to imitate algorithm A through the next-token prediction task. Specifically, we aim to find weights $\\theta_0$ such that $\\theta_0 = \\arg \\min_{\\theta} \\mathbb{E}_{r_0} \\sum_{i=0}^{n_{r_0}} |M_\\theta({r_i}_{i=1}^{n_{r_0}}) A({r_i}_{i=1}^{n_{r_0}})|$. To put it in another way, we want the model to predict the entire sequence ${r_i}_{i=0}^{n_{r_0}}$ given $r_0$ for all $r_0$. Importantly, the loss function does not exclusively focus on $r_{n_{r_0}}$ but also accounts for the intermediate steps $r_m$ for $m = 1, ..., \\forall_{n_{r_0}-1}$ when computing loss. This scenario arises in practice in theorem-proving tasks, where we are interested in both the intermediate proofs and the final answer.\n3.2 Causes of the order effect\nIn preparation for training model $M_\\theta$ to learn A, we collect a set of training data. Each data sample is a finite sequence of steps ${r_i}_{i=0}^{n_{r_0}}$. In the next-token prediction task, given a training data point r = ${r_i}_{i=0}^{n_{r_0}}$, autoregressive LLMs update their parameter by minimizing the loss of $\\mathbb{E}_{i=0}^{n_{r_0}-1} |M_\\theta({r_i}_{i=0}) - A({r_i}_{i=0})|$. Now, without loss of generality, if the data point ${r_i}_{i=0}^{n_{r_0}}$ in the set of training data satisfies the assumption above, and is reordered into ${r_0, r_{n_{r_0}}, r_1, ..., r_{n_{r_0}-1}}$, the following things happen:\n\u2022 Model cannot look forward: intermediate supervision fails to help if placed on the"}, {"title": "4 Experimental Setup", "content": "4.1 Tasks and Datasets\n4.1.1 Intuitionistic Propositional Logic Theorem-Proving\nFor the first dataset, we use theorem statements and proofs in intuitionistic propositional logic (PropL), as introduced in (An et al., 2024). A proof consists of tactics and intermediate states, where a tactic refers to an actual proof step, and a state represents what remains to be proved after a tactic is applied. Both the statements and the proofs are written in Lean, a popular formalized environment for theorem-proving in the mathematical community (de Moura et al., 2015). We have two types of PropL data. In the first type of data, all proofs follow the original sequential order (referred to as SEQ order). We note that SEQ order follows the intuitively sequential order: all the previous tactics and states help generate the next tactic and state. For the second type, the proofs are reversed (referred to as SER order), such that the last tactic and the last state become the first tactic and the first state, the second-to-last tactic and the second-to-last state become the second tactic and the second state, and so forth. Note that this dataset was published after the Llama-2 family was released so the model hasn't seen this dataset during pre-training.\n4.1.2 4-by-4 digit multiplication\nFor our second dataset, we choose the 4-digit by 4-digit multiplication task as a proof generation task. After a pair of 4-digit numbers (num1, num2) is picked, the problem is generated as: \u201cTell me what is num1*num2 and prove it\". A proof of a problem consists of the steps where we induce different axioms of the integer ring to derive the middle steps for computation, and the last step of the proof is the final answer of the multiplication.\nWe use an algorithm to generate the problems, the proofs, and the final answers of 4-digit by 4-digit multiplication. It has been shown that such problems, even prompted with \"Let's think step by step\", is challenging for cutting-edge models like GPT-4 (Liu and Low, 2023). We create three types of proofs with the same proof steps but in different orders, whose compositions are summarized in the table. The orders are listed from the optimal to the worst. Examples of all three types of data can be found in the Appendix A.2 for demonstration. Note that for the multiplication task, for proofs in the SEQ order, given a proof step, all previous steps serve as the natural intermediate supervision toward that step, hence making this dataset an ideal dataset to test our claim.\nFor training and testing, we first enumerated all 4-digit by 4-digit tuples. The list of tuples was then randomly shuffled. We used the first 1,000,000 tuples as our training number tuples. Together with the proofs, they formed our training dataset. We used the last 1,000 number tuples to construct our testing sets for model evaluation. During training process, a problem together with its proof and final answer form one training data sample."}, {"title": "4.2 Metrics", "content": "Metrics for the intuitionistic propositional logic theorem-proving task. For the theorem-proving task, we evaluate model performance using two key"}, {"title": "5 Main Results and Analysis", "content": "Zero-shot performance of Gemma-2b and Llama-2-7b-hf (no need for model name) on 4-by-4 multiplication and theorem-proving tasks. We begin by evaluating the zero-shot performance of both Gemma-2b and Llama-2-7b-hf on two tasks."}, {"title": "6 Other Results and Discussion", "content": "Pre-train data is not the major cause of the order effect. One might argue that the order bias in pre-training dataset is the primary cause of the order effects observed in this study. However, for the multiplication task, all of our experiments mentioned above were conducted on both pre-trained"}, {"title": "7 Conclusion and Future work", "content": "In conclusion, our study highlights the significant impact of order for each data sample on LLMs' training efficiency for the proof generation tasks. We claim that the optimal order of a data sample, which we call the intuitively sequential order, is to put intermediate supervision toward a particular proof step on the left for all proof steps in a data sample. We show the two causes for this order effect: given suboptimal data order, training inefficiency arises from (i) inability to look forward during training, i.e. learning to use future intermediate supervision to predict the current token, (ii) propensity to learn spurious token dependencies in the early training stage. Our experiments on intuitionistic propositional logic theorem-proving and 4-by-4-digit multiplication provide strong support for these explanations, highlighting the importance of finding the optimal data order in model training. A promising direction for future work is to investigate this claim during the pre-training stage of LLMs for reasoning tasks. We hypothesize that the claim will hold in this context as well, given that both fine-tuning on downstream tasks and pre-training on large corpora are fundamentally based on the next-token prediction task for decoder-only LLMs. Another future work is to employ machine learning methods to estimate and discover optimal data order, specifically, to identify for every token which other tokens in a data sample provide intermediate supervision. This holds significance on its own, independent of the gain it can bring to model training. For example, suppose the true supervision structure is discovered for mathematical proofs and those proofs are reordered accordingly. In that case, it might enable students to learn more quickly and reveal new mathematical intuition."}, {"title": "8 Limitations", "content": "In this paper, we conduct experiments on models with sizes of 2B and 7B. We did not extend our experiments to larger models due to limitations in computational resources.\nFurthermore, due to computational constraints, we did not repeat our training-from-scratch experiments multiple times."}, {"title": "A.1 A graduate-level example of a math theorem proof that follows the logic sequential order instead of the optimal order", "content": "We use Chapter 10, Proposition 3.6 (2) in a popular graduate textbook called Abstract Algebra, as an easy example ((Dummit and Foote, 2004)) to show why the purely logical order is often not the optimal order. Note that the proof here is stated in the purely logical order.\nTheorem: Let Q be an R-module: If R is a\nP.I.D, then Q is injective if and only if rQ =\nQ for every nonzero $r \u2208 R$. In particular,\na Z-module is injective if and only if it is\ndivisible.\nProof: Suppose R is a P.I.D. Any nonzero\nideal I of R is of the form I = (r) for some\nnonzero element r of R. An R-module ho-\nmomorphism f:I\u2194 Q is completely\ndetermined by the image f(r) = q in Q.\nThis homomorphism can be extended to a\nhomomorphism F : R \u2192 Q if and only if\nthere is an element q' in Q with F(a) = q'\nsatisfying q = f(r) = F(r) = rq'. It fol-\nlows that Baer's criterion for Q is satisfied\nif and only if $rQ = Q$, which proves the\nfirst two statements in (2).\nBaer's Criterion: The module Q is injective\nif and only if for every left ideal I of R\nany R-module homomorphism g : I \u2192 Q\ncan be extended to an R-module homomor-\nphism G: RH Q.\nChange to figure, explain everything"}, {"title": "A.1.1 Analysis of the example", "content": "In order to prove the theorem, the first natural instinct, after reading the statement, would be to apply Baer's criterion to prove the theorem. This is because Baer's criterion list one condition for Q to be injective, and that condition seems to be related to a P.I.D ring R well. Assuming this approach, one can easily construct the proof by extending maps on each left ideal I of R to a homomorphism from R to Q, as R is a P.I.D, hence finished the proof. However, without recognizing the necessity of Baer's criterion from the outset, the proof would be significantly more challenging to write up. Applying Baer's criterion act as the intermediate supervision toward the first few steps of the proof."}, {"title": "A.2 Data Examples in training", "content": "We show here examples of our data for the multiplication task and the intuitionistic propositional logic theorem-proving task. See Figure 6, 7."}]}