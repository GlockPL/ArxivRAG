{"title": "MMEVALPRO: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation", "authors": ["Jinsheng Huang", "Liang Chen", "Taian Guo", "Fu Zeng", "Yusheng Zhao", "Bohan Wu", "Ye Yuan", "Haozhe Zhao", "Zhihui Guo", "Yichi Zhang", "Jingyang Yuan", "Wei Ju", "Luchen Liu", "Tianyu Liu", "Baobao Chang", "Ming Zhang"], "abstract": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEVALPRO, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process. MMEVALPRO comprises 2, 138 question triplets, totaling 6, 414 distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, our experiments with the latest LLMs and LMMs demonstrate that MMEVALPRO is more challenging (the best LMM lags behind human performance by 31.73%, compared to an average gap of 8.03% in previous benchmarks) and more trustworthy (the best LLM trails the best LMM by 23.09%, whereas the gap for previous benchmarks is just 14.64%). Our in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research.", "sections": [{"title": "Introduction", "content": "Ever since the birth of standardized testing, the credibility of its conclusions has been a significant concern. The same problem goes for the evaluation of recently popular Large Multimodal Models (LMMs) such as GPT4-0 [22], Gemini-1.5 [24], Qwen-VL [2] and LLaVA [16]. One classic composition of such an evaluation is the multiple-choice question (MCQ), which includes an image, a question, possible choices, and an answer. This form of evaluation has higher usability compared to other evaluation methods, such as text-based evaluation and human evaluation. Many benchmarks [9, 17-19, 28, 29, 3, 4, 11, 23] designed for multimodal foundation models include a large portion of MCQs and are widely adopted in testing from basic to most advanced multimodal models [22, 2, 16], with some models reaching or even surpassing human scores on certain benchmarks [19, 11]. However, it is important to consider whether these evaluation results truly reflect the absolute capabilities of the models, especially when we are comparing them with human beings in the pursuit of AGI. In fact, several work such as PCA-Bench [4], MMStar [6] and MathVerse [30] have pointed out that the multimodal MCQ evaluation has intrinsic bias, which provides Large Language Models (LLMs) with shortcut to hack the question. FastV [5] finds that LMM could even achieve better performance on some benchmarks with only partial visual tokens. In this paper, we mainly study three popular multimodal benchmarks MMMU [28], ScienceQA [19] and MathVista [18].\nIn our preliminary Seeing-or-Not Comparison experiment, we found that LLMs could achieve high scores on the tested benchmarks without processing the visual data, attributed to possible data leakage, visual information problems being not related to answering the question or simply guessing the answer. Notably, the average performance gap between best LLM and best LMM is just 14.64%, which is even smaller than the gap within LMM themselves, revealing the unreliable problem of such evaluations.\nOur further Answer Consistency Test into multiple choice questions (MCQ) reveals a prevalent Type-I Error in such evaluation's conclusion, where models could output correct answers without actual comprehension. For example, the model could calculate the degree for a particular angle, but could not recognize the correct angle's name in the figure, which is a prerequisite to compute the degree.\nTo this end, we propose MMEVALPRO to truthfully reflect the true multimodal capabilities of tested models and keep the simplicity of MCQ evaluation. We achieve this by augmenting the original MCQ with prerequisite perception and knowledge questions. We propose Genuine Accuracy as the main metric, which depends on whether the model answers the triplet questions concurrently. Overall, in MMEVALPRO we annotate 2, 138 question triplets, originating from MMMU, ScienceQA, and MathVista, resulting in 6, 414 individual questions.\nWe carry out experiments and analyses involving 17 different models and human experts. The findings indicate that MMEVALPRO offers a more precise reflection of the true capabilities of the tested LMMs and poses a more demanding challenge. We find MMEVALPRO is more trustworthy than the base benchmarks as the best LLM trails the best LMM by 23.09% in MMEVALPRO whereas the gap for previous benchmarks is just 14.64%. Significantly, even the most advanced models, including GPT-40 and Qwen-VL-Max, lag considerably behind human performance, with a notable gap of over"}, {"title": "Probing the Credibility of Multimodal Benchmarks", "content": "The fundamental assumption of any benchmark is that models achieving higher scores possess superior capabilities compared to those with lower scores. In this section, we question the credibility of such an assumption for existing benchmarks used to evaluate LMMs. We find that the multimodal benchmarks we tested are not trustworthy enough in either relative or absolute perspectives, which is concluded from two probing experiments: Seeing-or-Not Comparison and Answer Consistency Test. The processes are illustrated in Figure 2. We test the MCQ evaluation in three multimodal benchmarks across various domains including MMMU, ScienceQA-Image and MathVista. We provide the detail of dataset statistics in section 3.1, model inference hyper-parameters and prompts in Appendix-B."}, {"title": "Seeing-or-Not Comparison", "content": "As shown in Figure 2-(a), we prepare two data versions for each benchmark: \"Seeing\" (with image, question, options, and answer) and \u201cNot-Seeing\" (without image). We test leading LMMs on \"Seeing\" data and non-vision LLMs on \"Not-Seeing\u201d data, then compare the results. The outcomes are shown in the first row of Figure 1.\nThe figure indicates that the performance gap between LMMs and LLMs is significantly narrower than anticipated. Intuitively, one might assume that LLMs, which lack the ability to process visual information, would perform considerably worse on multimodal benchmarks. In fact, if we compare the scores between the best-performing LLM and LMM, we observe that for MMMU, the best LMM's performance is only 1.1 times that of the best LLM. This performance gap is even smaller than the variability observed within LMMs themselves. Similar results go for the other tested benchmarks. What's more surprising is that LLMs sometimes outperform their vision-enabled counterparts (GPT4-o without vision ability outperforms the LLaVA-1.5 series according to Figure 1) and there is not an apparent performance boundary between the two kinds of models. These results suggest that those benchmarks do not accurately reflect the true multimodal understanding capabilities of the tested models. The reason for this phenomenon is three-fold:\n1. Image is not needed: Some benchmark questions can be answered solely through textual information, making visual input unnecessary. This diminishes the advantage of vision-enabled models (LMMs). For instance, as shown in the ScienceQA question in Figure 2, the knowledge that a falcon uses its feet to capture prey is common and does not require an image for verification."}, {"title": "Answer Consistency Test", "content": "To determine whether a model truly understands a question or is simply \"hacking\" the answer, we simulate the human problem-solving process by creating \"anchor questions\" that must be answered before the main question. When humans tackle multimodal reasoning questions, they typically follow two key steps: (1) identifying relevant visual clues in the image, and (2) applying their knowledge to reason through the problem before arriving at the final answer. Omitting either step usually leads to an incorrect answer. Similarly, we create a perception anchor question and a knowledge anchor question related to the original question. If a model can answer both anchor questions and the final question, it demonstrates genuine comprehension and reasoning, rather than mere guessing.\nAn example is shown in Figure 2-(b), we set a perception question and a knowledge question to the MCQ from MathVista. For human examinees, the perception question and knowledge question are easier to answer than the original one since they are set to be the prerequisites in the solution path. We found that even the most advanced LMM such as GPT4o struggles at answering all related questions even if it answers the original question correctly, which is easy for human experts. A detailed case analysis is in Figure 5.\nThe phenomenon raises another concern for multiple-choice question (MCQ)-based multimodal evaluation benchmarks: correctly answering a question does not necessarily indicate that the model genuinely knows how to derive the final answer. A direct solution to accurately diagnose whether the model truly has the capability to solve the question is to let humans evaluate the model's reasoning process, as done in previous works like MathVista [18]. However, human evaluation is labor-intensive and not reproducible, which complicates the broader application of the method."}, {"title": "MMEVALPRO: Calibrating Multimodal Evaluation", "content": "In this section, we delve into the detailed process of constructing the MMEVALPRO benchmark dataset and elucidate our methodologies for ensuring high-quality evaluation standards. Through these efforts, the MMEVALPRO framework sets a new paradigm in the assessment of multimodal models, aiming to foster both accuracy and efficiency in multimodal evaluation."}, {"title": "Data Source", "content": "To enhance the diversity of our benchmark data, MMEVALPRO integrates content from three prominent multimodal evaluation benchmarks: MMMU, ScienceQA, and MathVista. These benchmarks span educational levels from junior high to undergraduate and cover various subjects including Science, Math, and Art. The detailed descriptions of the source datasets are in Appendix A.1. Considering annotator expertise and budget, we selected 328 questions from MMMU (dev), 1, 200 from ScienceQA-Image, and all 540 from MathVista (testmini), totaling 2, 138 distinct multimodal MCQs."}, {"title": "Annotation Pipeline", "content": "As illustrated in Figure 6, we design the following pipeline for MMEVALPRO to generate question triplets. The triplet consists of an original question, a perception question, and a knowledge question.\n1) Data Preparation: Annotators begin by thoroughly reviewing the original question to ensure a deep understanding of the underlying concepts and the solution process.\n2) View and Analyse: Annotators are tasked with extracting crucial visual information and the logical framework implicit in the original problem, paving the way for the creation of nuanced perception and knowledge questions.\n3) Question Annotation: Building on the insights gathered, annotators then proceed to enrich the original question by formulating corresponding perception and knowledge questions, thereby expanding the scope of the evaluation.\n4) Double Check: To maintain the integrity of the MMEVALPRO dataset, each annotated question triplet undergoes a rigorous verification process. Two independent checkers, who are not part of the annotation team, review each triplet for any errors or logical inconsistencies. Any issues identified prompt a re-annotation of the affected questions.\nWe provide the annotator guide in the supplement material. The final distribution of MMEVALPRO is shown in Figure 3. We list the key statistics of the benchmark in Table 3 from Appendix 3.1, the annotation guidelines in Appendix C and D."}, {"title": "Evaluation Metrics", "content": "We propose Genuine Accuracy (GA) as the primary metric for MMEVALPRO. GA equals 1 only if the model correctly answers the original question and the corresponding perception and knowledge prerequisite questions simultaneously. The second metric is the Average Accuracy (AA), which treats all questions equally and computes the average accuracy.\nMMEVALPRO can also be viewed as a multi-view evaluation process, where we naturally derive the Perception Accuracy (PA) score and the Knowledge Accuracy (KA) score by computing the average accuracy for the perception and knowledge anchor questions, respectively.\nThe Consistency Gap (CG) is measured by subtracting the Genuine Accuracy from the accuracy of the original question. This metric reflects the proportion of instances where the model correctly answers the original question but fails on more in-depth perception and knowledge ques-"}, {"content": "As shown in Table 1, we compare the performance of different models including LLMs and LMMs on MMEVALPRO and the original benchmarks. We evaluated the performance of graduate students on the benchmarks as a strong baseline. The human evaluation guideline is in Appendix E. The details of human evaluation guide are in the supplement material. We also include random guess as a weak baseline performance."}, {"title": "Fine-grained Analysis", "content": "To better explain models' performance on MMEVALPRO, we conducted a fine-grained analysis on the experiments' result according to the metrics proposed in section 3.3. We select the best performing open-source and proprietary LLMs and LMMs to analyze. The results are shown in Table 2. A detailed case analysis is in Figure 5. More comparisons of different models are listed in Figure 18 from the Appendix."}, {"title": "Why MMEVALPRO is challenging and more trustworthy?", "content": "When comparing human performance with that of LLMs and LMMs, we observe a more significant gap in MMEVALPRO evaluations than in original benchmarks. This indicates that MMEVALPRO is inherently a more challenging task. The primary difficulty stems from the issue of answer consistency, which demonstrates whether the model genuinely understands how to leverage perceptual abilities and knowledge to solve a given problem. To illustrate this, we compare the Consistency Gap (CG) scores among humans, the best-performing LLMs and LMMs. The results suggest that LLMs generally exhibit a larger Consistency Gap than LMMs, while human experts display a considerably smaller CG. This trend is consistent across both open-source and proprietary models. A large Consistency Gap indicates that a model's robustness and generalization abilities are limited: it may be able to answer the original question but fails to respond accurately to related prerequisite questions based on the same image.\nThis weakness is difficult to capture for the original benchmarks due to the single MCQ format. In fact, if we only consider the Average Accuracy of MMEVALPRO, as shown in Table 1, we observe that the performance difference compared to the original benchmarks is much smaller than the difference in Genuine Accuracy (GA). This suggests that LLMs can often guess correct answers for questions even without image input. MMEVALPRO addresses this issue effectively through the GA metric, making it a more reliable evaluation method compared to previous benchmarks."}, {"title": "Why the Consistency Gap is large?", "content": "The gap between Genuine Accuracy and Average Accuracy on the original benchmarks reveals the answer inconsistency problem. We are further interested in what causes the problem. There are two possible reasons for a large Consistency Gap, that the model correctly answer the original question however fails on the perception or knowledge one. We compare the Perception Consistency (PC) and Knowledge Consistency (KC) of the evaluated models and humans. We find that there is a clear performance border between humans and LMMs, LMMs and LLMs. Humans could reach at least 90%PC and 80%KC in various sub-tasks, showing strong answer consistency. While the numbers for LMMs are 50%PC and 55%KC, for LLMs are 23%PC and 41%KC according to Table 2.\nPC and KC intuitively reflect the model's likelihood of correctly answering perception and knowledge questions if it has already solved the original question. Low PC and KC scores lead to a significant consistency gap. Beyond PC and KC, we visualize all conditional probabilities for the best open-source LLM and LMM in Figure 7. Humans generally exhibit high probabilities of correctly answering one question given a correct answer to another, indicating consistent thinking. In contrast, LLMs show the lowest probabilities compared to LMMs and humans. This is expected, as LLMs lack consistent multimodal problem-solving paths due to their absence of visual perception, thus supporting the credibility of the benchmarks.\nExamining the Perception Accuracy (PA) and Knowledge Accuracy (KA) in Table 2, we find that LMMs demonstrate a greater advantage in PA compared to KA when contrasted with LLMs. This is because PA depends directly on the models' visual capabilities, which LLMs lack. The above conclusion explains why the tested models have larger CG compared to humans, which is a potential and promising direction for future LMMs to improve on."}, {"title": "Related Work", "content": "There have been several benchmarks built for evaluating LMMs, such as MMBench, MME, Seed-Bench [17, 9, 12] that assess LMMs performance from multiple fine-grained dimensions. LVLM-eHub, M3IT [27, 13] focus on the general instruction following ability. MMMU, MathVista, ScienceQA [28, 18, 19] require perception from the vision part and knowledge in the language part.\nNonetheless, critiques have been raised regarding the limitations of these existing benchmarks in effectively evaluating LMMs. PCA-Bench, MathVerse [4, 30] adopt strong LLMs such as GPT4 and GPT4-Vision to score the reasoning process of LMMs in embodied-AI and math diagram questions, in order to pinpoint cases where the LMM gets the correct answer by a fluke. Yet, using a proprietary model to conduct evaluation hinders the broader usage of the method, moreover, the evaluation result has bias itself due to the proxy model and would change over time. MMStar [6] filters out the questions that do not rely on visual information in existing multimodal benchmarks. However, it does not address the issue inherent in MCQ, where models can potentially get the correct answer without truly understanding the content. Compared with those benchmarks, MMEVALPRO is more economical, easy-to-use, and calibrated for evaluating multimodal models."}, {"title": "Conclusion", "content": "We propose MMEVALPRO, a multimodal benchmark designed to address issues identified in previous evaluations and built upon MMMU, ScienceQA-Image, and MathVista. MMEVALPRO introduces twin perception and knowledge anchors to the original framework and defines Genuine Accuracy as its primary metric, thereby reducing the likelihood of LLMs manipulating the questions. Our extensive experiments and analyses on a wide array of models and human experts demonstrate that MMEVALPRO more accurately reflects the true capabilities of the tested LMMs and presents a more challenging task. Notably, even the most advanced models, such as GPT-40 and Qwen-VL-Max, trail behind human performance by a substantial gap of more than 30% in Genuine Accuracy. Our analysis into the reasons behind the consistency gap problem elucidates the disparity and provides valuable insights for future research."}, {"title": "Data Source", "content": "MMMU [28]: The MMMU is a benchmark designed to evaluate multimodal models on massive multi-discipline tasks. The benchmark sources its questions from college examinations, quizzes, and textbooks, encompassing 30 subjects and 183 subfields across six core disciplines, i.e. Art & Design, Business, Science, Health & Medicine, Humanities & Social Science and Tech & Engineering. This benchmark is meticulously designed to assess models' capabilities in handling multi-disciplinary tasks, drawing upon college-level subject knowledge.\nScienceQA [19]: The Science Question Answering (ScienceQA) is another pivotal resource, which consists of 21,208 multimodal multiple choice questions with diverse science topics. There are only 48.7% questions of ScienceQA that have an image context. It is renowned for its application in multimodal tasks and features a domain diversity spanning three primary science subjects, i.e., natural, language, and social. The dataset comprises multimodal science questions that are collated from elementary and high school science curricula, ensuring a breadth of scientific inquiry and comprehension. The subjects of the questions can be categorized by Biology, Physics, Chemistry, and others.\nMathVista [18]: The MathVista benchmark is developed to evaluate the reasoning ability of the multimodal models, which consists of 6, 141 examples from 31 datasets (28 mathematics and IQTest, FunctionQA, PaperQA). The dataset offers exclusively mathematical and visual tasks. This source enriches our dataset with rigorous computational and analytical problems, providing a robust framework for evaluating quantitative reasoning in multimodal contexts.\nFinally, we select the validation set of MMMU (722 questions), the questions with images in ScienceQA (2, 097 questions), and the testmini set of MathVista (540 questions) to construct the MMEVALPRO. From the 722 questions in the validation set of MMMU, we chose the questions with topics suited for the annotator's major and other questions in easy-level to annotate, which resulted in 339 final questions. And we annotated all questions selected from the testmini set of MathVista and 1, 259 of ScienceQA-Image.\nThe distribution of MMEVALPRO is shown in the Figure 3. There are 58.89% questions originating from ScienceQA, 25.26%from MathVista, and 15.86% from MMMU. We further categorize the problems by task categories, subjects, and domains, computing their relative percentages to the total questions. In the annotated questions originated from ScienceQA, there are 16.68% of them subject to Geography, 12.61% subject to Physics, 12.02% with the Biology topic, and the remaining questions distributed in History, Chemistry, Economics, etc. In MathVista, the annotated questions contain 5 tasks, including GPS(geometry problem solving), VQA(visual question answering), FQA(figure question answering), TQA(textbook question answering), and MWP(math word problem), the last one only accounted for a very small proportion and is not annotated in the figure. According to the subjects of questions from MMMU, there are 4.91% questions in Medicine, 3.27% in Art, 3.65% in Business, 2.71% in Science, and 1.32% from other subjects with easy-level."}, {"title": "License", "content": "We check all the datasets' licenses and they all permit customization and redistribution for non-commercial use. MMMU is under Apache 2.0 License, ScienceQA is under MIT License and MathVista is covered by CC BY-SA 4.0.\nMMEVALPRO can be used commercially as a test set, but using it as a training set is prohibited. By accessing or using this dataset, users acknowledge and agree to abide by these terms in conjunction with the CC BY-SA 4.0 license. We make sure there is no offensive content found during the whole annotation process."}, {"title": "Prompt Format for Different Models", "content": "For LLM \"Given a question you need to choose the best answer from the given options. I will first give you an example, you need to follow the output format of the answer. Example: Question: {example question} Options: {example options} Output: {example answer}. Just answer the follow-ing question with only the letter of the correct option, or you will get no credit. Question:{question} Options: {options}\"\nWe use a fixed demonstration for all inferences to format the output. The example is:\"Question: Baxter Company has a relevant range of production between 15,000 and 30,000 units. The following cost data represents the average variable costs per unit for 25,000 units of production. If 30,000 units are produced, what are the per unit manufacturing overhead costs incurred? Options: (A) $6 (B) $7 (C) $8 (D) $9 Output: A\".\nFor LMM \"Analyse the image and choose the best answer for the following question: {question} Options: {options} Just output the letter of the correct answer.\""}, {"title": "Model Hyper-parameters", "content": "For open-source models, we use the default inference script provided in corresponding papers and githubs. For proprietary models, we follow the official guide to call the API. In particular, we do not use sampling techniques during generation to ensure our results are reproducible. All experiments are done on a local server with 4 NVIDIA-A100 GPUs."}, {"title": "MMEVALPRO Triplet Annotation Guideline", "content": "Our research requires the construction of a more trustable multimodal evaluation dataset. The collection and arrangement of original questions have been completed. You, as annotators, need to perform data annotations based on the original questions, expanding the original question into a set of three questions (original, perception, knowledge). Every annotators should carefully read the following content before embarking on the annotation task.\n1. The data annotation task will be completed on the annotation UI interface we have built.\nEach annotator will be allocated an account and password.\nEach account corresponds to an independent subset of questions needing annotation.\n2. Each original question to be annotated includes an image, a question text, possible answers, and the correct answer. You need to read the original question and understand the logic of solution firstly.\n3. You need to provide two entirely new questions, a perceptual question, and a knowledge question based on step 2.\nThe perceptual question should be a question related to the content of the image corresponding to the original question.\nThe knowledge question should be related to the logic of solving the original question.\nYou should input the text of the question, the variable options, and the correct answer. Each question must be a multiple-choice question. Once confirmed to be correct, you can click to submit.\nWe developed an annotation Web UI to enable expert annotators to construct question triplets of MMEVALPRO. The Web UI is shown in the figure 13. The annotators of MMEVALPRO were trained with the guideline shown in figure 14 before formal work."}, {"title": "MMEVALPRO Triplet Checking Guideline", "content": "Our research requires the construction of a more trustable multimodal evaluation dataset. The data annotation process has been completed. The reviewers need to check the questions annotated by the annotators, and see if there are any issues with the corresponding question descriptions and the provided correct answers. Please read the following content before starting your review.\n1. The review process of the dataset will be carried out on a dedicated webpage. You will be assigned a corresponding account and password.\n2. During the review of the questions, please check all the content annotated by the annotators, including:\nQuestion Text (Ambiguous description, mismatch with the image, grammatical errors, etc.)\nQuestion Options (Duplication, ambiguity, and other problems.)\nAnswer (Is the answer correct?)\n3. You need to mark the questions without problems as Pass and mark problematic questions as Problematic. Appropriate comments also need to be made on the problematic questions for the data annotator to correct (for example, \"ambiguous\", \"the answer is incorrect\", etc.)\nIn our study, we employed double checking to maintain the quality of MMEVALPRO. We also developed a web page shown in figure 15 for checking. The checkers were trained with checking instructions shown in figure 16."}, {"title": "Human Evaluation Guide", "content": "To ensure the comprehensiveness of our study, we employed five graduates with special knowledge to do human evaluation in MMEVALPRO. The designed web ui of human evaluation is shown in figure 17."}, {"title": "Comparison of Different Models", "content": "We list three cases comparing the output of GPT-40 [22], GPT-40 (non-vision) [22], InternVL-Chat [7] and LLaVA-1.5 [16]. We observed that, while both GPT-40 (non-vision) and LLaVA-1.5 could answer all the origin questions correctly, they struggled with most perception and knowledge questions. This highlights a Type-I error in current evaluation benchmarks: correctly answering a question does not necessarily indicate genuine understanding by the model. On the other hand, more advanced models like GPT-40 and InternVL-Chat demonstrate higher consistency in answering different types of questions."}]}