{"title": "Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations", "authors": ["Supriya Manna", "Niladri Sett"], "abstract": "Faithfulness is a critical metric to assess the reliability of explainable AI. In NLP, current methods for faithfulness evaluation are fraught with discrepancies and biases, often failing to capture the true reasoning of models. We introduce Adversarial Sensitivity as a novel approach to faithfulness evaluation, focusing on the explainer's response when the model is under adversarial attack. Our method accounts for the faithfulness of explainers by capturing sensitivity to adversarial input changes. This work addresses significant limitations in existing evaluation techniques, and furthermore, quantifies faithfulness from a crucial yet under-explored paradigm.", "sections": [{"title": "1 Introduction", "content": "Deep learning-based Language Models (LMs) are increasingly used in high-stakes Natural Language Processing (NLP) tasks (Minaee et al., 2021; Samant et al., 2022). However, these models are extremely opaque. To build user trust in these models' decisions, various post-hoc explanation methods (Madsen et al., 2022) have been proposed (Jacovi et al., 2021). Despite their popularity, these explainers are frequently criticized for their 'faithfulness', which is loosely defined as how well the explainer reflects the underlying reasoning of the model (Lyu et al., 2024; Jacovi and Goldberg, 2020). In the context of NLP, explainers assign weights to each token indicating their importance in prediction, and faithfulness is measured by how consistent these assignments are with the model's reasoning. However, since the explainer is not the model itself (Rudin, 2019), practitioners have developed several heuristics to measure the quality of these assignments (DeYoung et al., 2019;"}, {"title": "2 Adversarial Sensitivity", "content": "In this section, we introduce the notion of adversarial sensitivity, exploring its significance and relation with faithfulness. Thereafter, we propose the guideline for evaluating faithfulness with adversarial sensitivity.\nDefinition 1. Adversarial Example (AE): Given a model $f: X \\rightarrow Y$, where $X$ is the space of textual inputs and $Y$ is the set of classes, if there exists $x'$ for a given input $x \\in X$ such that:\n$\\left\\{x^{\\prime} \\in X \\mid S\\left(x, x^{\\prime}\\right)>0 \\text { and } f(x) \\neq f\\left(x^{\\prime}\\right)\\right\\}$\nwe call $x'$ an adversarial example (AE), where $S(.,.)$ is a similarity measure and $\\Theta$ is a predefined similarity threshold.\nDefinition 2. Local Explanation: A local feature importance function $I$ takes an instance $x \\in X$ and the model $f$ as input, and produces a weight vector as output:\n$I(f,x) = W_{x,f} = (w_1, w_2, ..., w_n)$,\nwhere $w_i$ represents the importance of the i-th token $x_i$ for the prediction $f(x)$.\nDefinition 3. Adversarial Sensitivity: Adversarial Sensitivity for a local explainer $I$ for $(x, x')$ is"}, {"title": "Faithfulness Test Setup", "content": "3.1 Obtaining AEs\nPrimarily, obtaining AEs (an adversarial attack on the model) is a greedy or brute-force procedure, where a search algorithm iteratively selects locally optimal constrained perturbations until the label changes (Morris et al., 2020). As mentioned in Section 2, we devise our attacks in three constraint classes: word level, character level, and behaviorul invarince. We brief the implementation details of these attacks as follows.\n3.1.1 Word Level (A1)\nWe adhere to the constraints proposed in the strong baseline 'TextFooler' (Jin et al., 2020) while implementing our word-level attack (A1). Initially, we assign weights to each word based on its impact on the model's prediction when removed. Then, in decreasing order of importance, we take each word (except stopwords), find semantically and grammatically correct K (we set K = 50) words to replace the selected word, and generate all possible intermediate corpus and query the model. If the best result (which alters the prediction the most) from this pool exceeds the one from the previous iteration, we select the new one as the current result; otherwise, we stick to the previous one. This process iterates until the current result yields a dif-"}, {"title": "3.1.2 Character Level (A2)", "content": "For character-level attack (A2), we assign weights to each word based on its impact on the model's prediction when replaced with an unknown token ('[UNK]'). The rest of the procedure is the same as the Al, but instead of semantically similar words, we replace the selected word after applying a combination of character-level perturbations proposed by Gao et al. (Gao et al., 2018), subject to a predefined edit distance threshold, proposed in (Gao et al., 2018). Li et al. (Li et al., 2018) empirically showed that character-level perturbation can change semantic alignment in the embedding space. Therefore, after filtering with edit distance, we also employ the universal sentence encoder (Cer et al., 2018) and use the similarity threshold proposed in (Li et al., 2018) to select the final candidate."}, {"title": "3.1.3 Behaviorul Invarience (A3)", "content": "Recently, Ribeiro et al. (Ribeiro et al., 2020) emphasised that models are hypersensitive not only to minute perturbations but also to 'invariant' tokens. Ribeiro et al. proposed 'Checklist' that evaluates models across diverse linguistic capabilities such as vocabulary, syntax, semantics, and pragmatics. For our setting, we adopt the 'Invariance Testing' they proposed (A3). We change names, locations, numbers, etc., wherever feasible in the sentences and check if these alterations affect the prediction. As Ribeiro et al. (Ribeiro et al., 2020) showed, a model should not be sensitive to such parameters. If it is, it indicates an inability to handle commonly used linguistic phenomena, which are subsequently characterised as a type of adversarial example (Morris et al., 2020). We employ the off-the-shelf implementation of the invariance test-"}, {"title": "3.2 Measuring the Distance", "content": "To measure the dissimilarity of the explanations, we follow the distance measure given by Ivankay et al. (Ivankay et al., 2022), that is:\n$d=1 - \\frac{\\tau(W_{x,f},W_{x^{\\prime},f})+1}{2}$ (1)\nwhere $(\\cdot,\\cdot)$ is a correlation measure. Ivankay et al. (Ivankay et al., 2022) chose Pearson correla- tion for their distance measure. But while creating adversarial examples, a common phenomenon is obtaining unequal token vectors for $(x, x')$ due to tokenisation (Sinha et al., 2021). Correlation mea- sures like Pearson, Kendall, and Spearman cannot handle disjoint and unequal ranked lists. Sinha et al. (Sinha et al., 2021) used heuristics like Location of Mass (LOM) (Ghorbani et al., 2019) to mitigate such issues. But Burger et al. (Burger et al., 2023) highlighted their shortcomings and employed Rank Based Overlap (RBO) (Webber et al., 2010) metric. While RBO may be robust, it introduces com- plications, particularly with its selection of free"}, {"title": "3.3 Interpreting the Results", "content": "Our proposed test is a necessary test for faithfulness based on the desideratum that the explainers should produce different explanations for AEs. Obtaining AEs is always subject to different sets of constraints. As a result, each attack type i.e. Al, A2, A3 is disjoint in nature thus, each of them independently conducts a necessary test given they produce successful AEs. Theoretically, there can be finitely many AEs if we keep changing the set of constraints but in this paper, we followed three extensively evaluated, diverse sets of constraints to empirically demonstrate the adversarial sensitivity of explainers around these disjoint constraint sets. As a result, our setup consists of three disjoint necessary tests for inspecting faithfulness using adversarial sensitivity. We evaluate the explainers on the basis of how much sensitivity they obtain for how many number of discrete constraint sets. However, as these are all necessary tests, the primary objective is to reject the unfaithful ones. Also, it is highly seek-worthy that explainers perform consistently well across constraint sets. Now, if the results across constraint sets are fluctuating for a given setup, it could be confusing for the end user to evaluate the explainers holistically. This is why, for an aggregated ranking we recommend using a consensus aggregation (e.g., Kemeny-young aggregation (Kemeny, 1959)) over empirical evaluation. Although, in our experiments, we obtained consistent results across A1, A2, A3."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets and Models\nWe conducted our experiments on SST-2 (Socher et al., 2013), and Tweet-Eval (Hate) (Barbieri et al., 2020) for binary classification, and on AG News (Zhang et al., 2015) for multi-class classification. We fine-tuned a Distill BERT and a BERT-based model (Devlin et al., 2018) until it achieved a certain level of accuracy for each dataset, and attacked it with the three attack methods A1, A2, and A3 described in the Section 3.1. We report the models' accuracy before and after each attacks in"}, {"title": "4.2 Explainers and Faithfulness Metrics Details", "content": "Commonly used post-hoc local explainers can be broadly categorised in two types: perturbation-based and gradient-based explainers (Madsen et al., 2022). We have considered two commonly used perturbation-based model agnostic explainers: LIME (LIME) (Ribeiro et al., 2016) and SHAP (SHAP) (Lundberg and Lee, 2017). For SHAP, we use the default selection of partition shap. From gradient based ones, we have chosen Gradient (Grad.) (Simonyan et al., 2013), Integrated Gradient (Int. Grad.) (Sundararajan et al., 2017) and their xInput version: Gradient \u00d7 Input (Grad. \u00d7 Input), and Integrated Gradient \u00d7 Input (Int. Grad. \u00d7 Input). We compare our findings with extensively used erasure (Jacovi and Goldberg, 2020) based metrics: comprehensiveness, sufficiency (DeYoung et al., 2019), and correlation with 'Leave-One-Out' scores (Jain and Wallace, 2019) for faithfulness comparison. The Appendix contains the description of erasure-based faithfulness metrics and post hoc explainers used in our experiments.\nWe run our experiments on an NVIDIA DGX workstation, leveraging Tesla V100 32GB GPUs. We use ferret with default (hyper)parameter selection (Attanasio et al., 2022) for both erasure metrics and explanation methods, TextAttack (Morris et al., 2020), universal sentence encoder (Cer et al., 2018) across attacking mechanism. We wrote all experiments in Python 3.10. Our total computational time to execute all experiments is roughly 18 hours. We report the consolidated findings for both models below in Table 2."}, {"title": "5 Results & Discussion", "content": "From Table 2, it is clearly observable that as per Adv. Sens., LIME, SHAP, Gradient \u00d7 Input, and Integrated Gradient \u00d7 Input all perform competitively across various datasets and attacks. However, the vanilla versions of gradient-based methods are not as effective. Notably, the Gradient itself exhibits the least sensitivity to adversarial inputs, followed by Integrated Gradient. Furthermore, Integrated Gradient's adv. sens. remains almost invariant to the type of attacks across all datasets, unlike comp. and suff. Interestingly, all explainers except Gradient show a drop in sensitivity in the AG News dataset across all attacks. Gradient performs best on all attacks in AG News amongst datasets. Perturbation-based explainers like LIME and SHAP are among the best performers across datasets. Gradient \u00d7 Input and Integrated Gradient \u00d7 Input perform well within the group of white-box explainers, with LIME and SHAP.\nUnder erasure methods across all datasets, Gradient is a moderately well-performing explainer, whereas Gradient \u00d7 Input performs much worse. However, according to Adversarial Sensitivity, Gradient x Input is one of the best performers, with Gradient being the worst among all. Like Gradient \u00d7 Input, Integrated Gradient also largely performs worse than Gradient in erasure, but it remains consistently moderate according to Adv. Sens. Both LIME and SHAP not only perform very well in both Adv. Sens. and erasure metrics but also the difference b/w their magnitudes for both erasure metrics and adv. sens. are (considerably) nominal. Integrated Gradient \u00d7 Input is substantially similar to LIME, SHAP in adv sens., but we observe a considerable drop in comprehensiveness for SST-2 and AG News for both the models, unlike adv. sens.\nTo demonstrate, how to evaluate the explainers based on the consensus ranking, we are considering the case of SST-2 for the BERT Model. We use the Kemeny-Young method here; as this has been extensively used for Condorcet ranking (Young, 1988); it also satisfies highly desirable social choice properties for fair voting (Owen and Grofman, 1986; Young, 1995). Kemeny-Young aggregation also have been used in biology and social science extensively (Brancotte et al., 2014; Andrieu et al., 2021; Arrow et al., 2010). We first convert the columns of A1, A2, A3 into ranking vectors using a ranking function. In our case, we used the traditional ranking: the higher the score"}, {"title": "6 Related Works", "content": "Faithfulness evaluation, based on previous literature, can be broadly categorised in six ways: ax-"}, {"title": "7 Conclusion and Further Work", "content": "In this work, we explored the shortcomings of widely used faithfulness measures in NLP and proposed a test to evaluate explainers based on their sensitivity to adversarial inputs. Through extensive experiments on six post-hoc explainers, we found that gradient & integrated gradient aren't (sufficiently) sensitive, while LIME, SHAP, and Gradient \u00d7 Input, and Integrated Gradient \u00d7 Input show better sensitivity. We also observed notable differences between our evaluation and traditional erasure-based faithfulness measures.\nFuture work will explore adversarial sensitivity for multilingual datasets, low-resource languages, and advanced lms.\nBroader Impact\nDeep models are not only fragile but also opaque. Our work lies at the intersection of these two critical aspects. Building on the arguments presented by Jacovi et al. (Jacovi and Goldberg, 2020), we introduce a necessary test for assessing faithfulness. Given that the underlying assumption of adversarial sensitivity is applicable to (nearly) all data types and models, this concept can be extended across (almost) all domains and explanation mechanisms. Faithfulness is a key component in explainable AI (Miller, 2019). When a model behaves deceptively under any form of adversarial intervention, it becomes imperative that explainers provide faithful explanations in such scenarios, rather than merely those where the model performs according to user expectations. Adversarial sensitivity aids end-users in identifying explainers that are responsive to adversarial instances. We strongly believe that the nuanced notion of adversarial sensitivity opens up a new direction for evaluating explainers, particularly in situations where being unfaithful could lead to a misinterpretation of why the model produces deceptive results.\nLimitation\nAdversarial attacks are computationally expensive. Our work therefore is much computationally expensive and non-trivial than erasures. Our work is a necessary test faithfulness of explainers therefore, from a practitioner's perspective (Lyu et al., 2024) we employ our tests primarily to identify unfaithful explainers. It's important to note that our test does not take into account other criteria, such as biases in models, during the evaluation process. The scope of the work, for the time being, is restricted to NLP."}, {"title": "8 Appendix", "content": "8.1 Short Description of the Erasure Methods\nWe compare our findings with extensively used erasure (Jacovi and Goldberg, 2020) based metrics: comprehensiveness, sufficiency (De Young et al., 2019), and correlation with 'Leave-One-Out' scores (Jain and Wallace, 2019) for faithfulness comparison. Below are the definitions of these metrics.\nComprehensiveness (\u2191) This metric evaluates the extent to which an explanation captures the tokens crucial for the model's prediction. It is quantified by:\nComprehensiveness = $f_j(x) \u2013 f_j(x \\setminus r_j)$ (2)\nwhere x is the input sentence, $f_j(x)$ is the model's prediction probability for class j, and $r_j$ is the set of tokens supporting this prediction. $x \\setminus r_j$ denotes x with $r_j$ tokens removed. A higher value indicates greater relevance of $r_j$ tokens.\nFor continuous feature attribution methods, we compute comprehensiveness multiple times, considering the top k% (from 10% to 100%, in 10% increments) of positively contributing tokens. The final score is the average across these computations.\nSufficiency (\u2193) This metric assesses whether the explanation tokens suffice for the model's prediction:\nSufficiency = $f(x) \u2013 f_j(r_j)$ (3)\nA lower score suggests that $r_j$ tokens drive the prediction. As in comprehensiveness, we calculate the aggregate sufficiency.\nCorrelation with Leave-One-Out scores (\u2191) We compute Leave-One-Out (LOO) scores by iteratively omitting each token and measuring the change in model prediction. LOO scores represent individual feature importance under the linearity assumption (Jacovi and Goldberg, 2020). We then calculate the Kendall rank correlation coefficient $\\tau$ between the explanation and LOO score:\n$\\tau_{loo}$ = $corr_{Kendall}$(explanation, LOO scores) (4)\nA $\\tau_{loo}$ closer to 1 indicates higher faithfulness to LOO importance. We have addressed $\\tau_{loo}$ as LOO in Table 2."}, {"title": "8.2 Short Description of the Explainers", "content": "Local Interpretable Model-agnostic Explanations (LIME), introduced by Ribeiro et al. (2016) (Ribeiro et al., 2016), operates on the principle of local approximation. LIME generates explanations by fitting interpretable models to local regions around specific instances, providing insights into the model's behavior for individual predictions. This approach is particularly valuable for understanding non-linear models in a localized context.\nSHapley Additive exPlanations (SHAP), developed by Lundberg and Lee (2017) (Lundberg and Lee, 2017), draws from cooperative game theory, specifically Shapley values (Shapley, 1951). SHAP assigns each feature an importance value for a particular prediction, ensuring a fair distribution of the model output among the input features. This method offers a unified framework that encompasses several existing feature attribution methods.\nGradient-based attribution methods leverage the model's gradients with respect to input features to quantify their importance. The simple Gradient method (Simonyan et al., 2013) computes the partial derivatives of the output with respect to each input feature, providing a first-order approximation of feature importance. However, this approach can suffer from saturation issues in deep networks.\nTo address these limitations, Sundararajan et al. (2017) (Sundararajan et al., 2017) proposed Integrated Gradients, which considers the integral of gradients along a straight path from a baseline to the input. This method satisfies desirable axioms such as sensitivity and implementation invariance, making it a robust choice for attribution.\nVariants of these methods, namely Gradient \u00d7 Input and Integrated Gradient \u00d7 Input, incorporate element-wise multiplication with the input to account for feature magnitude. These approaches can provide more intuitive explanations, especially in scenarios where the input scale is significant."}]}