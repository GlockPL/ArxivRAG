{"title": "COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMS", "authors": ["Jihyung Kil", "Zheda Mai", "Justin Lee", "Zihe Wang", "Kerrie Cheng", "Lemeng Wang", "Ye Liu", "Arpita Chowdhury", "Wei-Lun Chao"], "abstract": "The ability to compare objects, scenes, or situations is crucial for effective decision-making and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce COMPBENCH, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMS). COMPBENCH mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use COMPBENCH to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe COMPBENCH not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.", "sections": [{"title": "1 Introduction", "content": "The concept of \u201crelativity\u201d is integral in our daily lives. For example, relative freshness affects our decision to purchase fruits; relative spaciousness affects our decision to choose living or working space; relative crowdedness indicates which paths to select; (relative) change between two scenes reveals what happened to the environment. In short, the ability to compare objects, scenes, or situations and reason about their relativity is vital for us to make informed decisions, solve problems effectively, and acquire knowledge efficiently, enabling us to make sense of the surrounding world.\n\nThe recent advance of multimodal large language models (MLLMs), a.k.a. large multimodal models (LMMs), [1, 3, 54, 32, 31, 14, 6] has demonstrated promising progress toward artificial general intelligence (AGI) [60, 35] and achieved unprecedented results in a variety of vision and language (V&L) tasks, ranging from free-formed visual recognition [15, 10, 13] and visual captioning [10, 2] to visual question answering [21, 22, 49]. Yet, much less attention has been paid to tasks that involve relativity and comparison between multiple visual inputs, e.g., two images. In essence, most of the existing datasets for visual recognition [15, 10, 13] and V&L tasks [21, 2, 39, 30, 16, 60] comprise examples with only single visual inputs (e.g., an image or a video clip), making them infeasible to assess MLLMs' comparative capability."}, {"title": "2 Related Work", "content": "Multimodal LLMs (MLLMs). Large Language Models (LLMs) [1, 54, 4, 5, 23, 55, 58] have made significant strides in various NLP and AI tasks. Many recent works [1, 3, 54, 32, 31, 14, 6, 28, 63, 43, 57] have extended LLMs' capabilities into the multimodal domain, particularly for vision and language (V&L) tasks. At a higher level, this advancement involves integrating a pre-trained vision encoder (e.g., CLIP [46]) with LLMs via a bridge module (e.g., an adaptor [32, 14]). Different strategies are developed to pre-train these multimodal LLMs (MLLMs), such as optimizing the LLMs and bridge module while keeping the vision encoder frozen [32] or training the bridge part only [14].\n\nMLLM benchmarks. Earlier, MLLMs were evaluated on traditional V&L tasks, such as visual question answering (VQA) [21, 22, 49], image captioning [10, 2], and image-text retreival [30, 11]. Recently, a range of new and intriguing V&L tasks [36, 52] have emerged to assess MLLMs' capabilities across various dimensions. These include comprehension and reasoning about charts [37], diagrams [38], scene text [50, 39], web navigation [16], expert-level multimodal understanding [60], etc. Our COMPBENCH complements these efforts by focusing on a new dimension, MLLMs' comparative reasoning capacity on a pair of visually or semantically relevant images.\n\nMulti-image datasets. Several existing datasets [42, 53, 17, 25, 62, 27] provide multi-image data (e.g., pairs of images), but they serve different purposes (e.g., not for evaluating MLLMs) or have relatively limited scopes. NLVR2 [53] labels each image pair with a caption that may or may not be relevant to the images, asking models to predict the caption's relevance (i.e., image-text matching). A few datasets [27, 7, 61] synthesize multi-image data for instruction tuning (e.g., image editing). More relevant to ours are [17, 25, 62, 42]. Birds-to-Words [17] aims to describe the difference between two birds; Sopt-the-diff [25] focuses on the difference between two outdoor scenes; Q-bench2 [62] compares the quality (e.g., blurriness) between two images; Relative Attributes [42] compares the relativeness of attributes between two facial or natural images. However, these datasets have limited scopes, only targeting specific domains or questions. In contrast, our COMPBENCH defines eight relative comparisons, covering a wide range of relativities in the real world. Our image pairs are curated from fourteen diverse visual domains. We believe this offers the V&L community a more comprehensive benchmark to assess the comparative capabilities of current leading MLLMs.\n\nLearning to rank & learning with preference. Several research topics are relevant to ours and may benefit from our COMPBENCH. Learning to rank (LTR) [29, 33, 8] aims to realize a scoring function that can rank examples (e.g., images) based on certain aspects, such as facial ages [40, 9] and degrees of attributes' presence [42]. Typically, an LTR model takes one example as input; the model is trained with pairs of examples such that the output scores match the ground-truth orders. Recently, learning with preference information [18] has become a mainstream approach to fine-tuning LLMs for alignment [47, 12]. Unlike our focus, these works usually collect pairs of outputs (e.g., answers to a question) with humans' preferences to supervise model fine-tuning."}, {"title": "3 Why Do We Study Comparative Reasoning?", "content": "To date, most of the existing visual recognition and V&L benchmarks focus on a single visual input (e.g., an image or a video clip), aiming to assess and promote absolute inference and reasoning within it, for example, identifying objects, recognizing their properties/states/actions, and describing and reasoning about their interactions within in the scene.\n\nIn reality, not all the inference and reasoning could be made absolute, or need to be absolute. For example, it is hard and ambiguous to describe the absolute degree of smiling [42], but it is relatively easy to compare two faces and tell which one smiles more. This fact applies to other visual properties like attributes (e.g., length), states (e.g., steps in cooking), and spatial locations (e.g., longitude and latitude). Often, comprehending the relativity is sufficient for us to make sense of the real world.\n\nFurthermore, learning to infer and reason about relativity could naturally and more efficiently facilitate AI models to grasp fine-grained details. For instance, learning to describe a complex scene (e.g., captioning) often results in a model mastering common objects and properties but missing rare and subtle ones. In contrast, learning to tell the difference between two scenes promotes the model to identify subtle changes and describe them.\n\nLast but not least, the ability to perform comparative reasoning is integral to our daily decision-making and problem-solving (see \u00a71 for some examples). Humans' comparative capability, e.g., providing preferences between instances, has also been widely leveraged to supervise foundation models like LLMs to align their outputs with application requirements and societal expectations [47, 12]. We thus believe it is crucial to assess and promote comparative reasoning about relativity in AGI."}, {"title": "4 COMPBENCH Benchmark", "content": "We introduce COMPBENCH, a multimodal benchmark designed to assess the comparative reasoning abilities of MLLMs across various dimensions. In what follows, we first describe the types of comparative capabilities that COMPBENCH aims to evaluate (\u00a74.1). Next, we outline our methodology for collecting images, followed by how we annotate associated questions and answers to evaluate these capabilities (\u00a74.2). Lastly, we provide detailed statistics on COMPBENCH and discuss its data quality (\u00a74.3)."}, {"title": "4.1 Types of Relativity", "content": "Building upon \u00a73, we consider eight comparison categories to evaluate MLLMs' abilities to discern differences between two similar images (Figure 1).\n\n(1) Visual Attribute focuses on five common visual properties \u2013 Size, Color, Texture, Shape, and Pattern \u2013 and tests whether the model can identify the relative magnitude of these attributes between images. (2) Existence assesses the model's capacity to identify fine-grained variations by detecting subtle changes between images. (3) State involves comparing the conditions or status of objects. (4) Emotion assesses the model's capability to interpret degrees of human emotions. (5) Temporality and (6) Spatiality evaluate the model's ability to recognize differences in images caused by temporal or spatial differences. These categories require both commonsense and comprehension of the physical world. Lastly, (7) Quantity measures the relative counting skills, and (8) Quality compares the quality of two images, examining the model's low-level visual perceptual skills."}, {"title": "4.2 Dataset Curation", "content": "One major challenge in constructing COMPBENCH is mining image pairs that reflect the aforementioned relativities. Fortunately, many publicly accessible datasets in vision and V&L offer detailed annotations and metadata. We carefully investigate these datasets and identify a seed set of fourteen datasets that align with the eight relativity types (\u00a74.1), covering a wide range of domains like open-domain, fashion, animal, sports, automotive, facial, and both outdoor and indoor scenes. Below, we outline the datasets for each relativity type and the process for generating triplets of image pairs, a question, and an answer. Please see the supplementary material for details."}, {"title": "4.2.1 Visual Attribute", "content": "Data collection. We consider five visual attribute datasets. MIT-States [24] includes 245 objects with 115 visual attributes, from online sources such as food or device websites. Fashionpedia [26] is tailored to clothing and accessories and contains 27 types of apparel along with 294 detailed attributes. VAW [45], similar to MIT-States, offers a large-scale collection of 620 unique attributes, including color, shape, and texture. CUB-200-2011 [56] and Wildfish++ [45] specifically provide attributes for birds and fish. The former catalogs 15 bird parts and their attributes (e.g., \"notched tail\"); the latter details 22 characteristics (e.g., \"yellow pelvic fins\u201d) of various fish species. For each dataset, we cluster images by objects or parts with the same attributes (e.g., \"round table\", \"asymmetrical blouse\", \"curved bill\", \"yellow dorsal fin\") and extract visually similar image pairs from each group.\n\nAnnotation. We apply rule-based approaches to generate questions about relative degrees of attributes between objects (e.g., \"Which coat is more floral?\u201d). We then pair the questions with the corresponding image pairs and present them to six human annotators. The annotators are tasked with labeling the correct answers (binary: left/right) and filtering out any irrelevant or nonsensical questions about the images. In total, we construct a collection of 5.3K triplets."}, {"title": "4.2.2 Existence", "content": "Data collection. We consider datasets for image editing, which provide image pairs with similar layouts but subtle changes. We adopt MagicBrush [61], a recently released dataset for instruction-guided editing. It consists of (source image, instruction, target image) triplets, where the instruction specifies a subtle change between the source and target images. We also consider Spot-the-diff [25], which provides image pairs in outdoor scenes, along with descriptions of their differences.\n\nAnnotation. We curate multiple-choice questions to ease automatic evaluation. We prompt GPT-4V [1] with in-context learning to generate questions; the options are formed by the extracted objects and their attributes from images. We then pass the questions (along with image pairs) to the annotators to verify the options and label the correct ones. In total, we curate 2.2K triplets."}, {"title": "4.2.3 State", "content": "Data collection. We explore vision datasets covering the condition or status of objects (e.g., \"pureed tomato\" or \"mashed potatoes\u201d). Specifically, we use two large-scale, open-domain visual attribute datasets: MIT-States [24] and VAW [45]. They annotate not only the five common visual properties used in Visual Attribute but also some other properties about object states. We ask human annotators to manually review the datasets to identify image pairs relevant to state attributes.\n\nAnnotation. We follow the annotation protocol in \u00a74.2.1 to curate a total of 1.1K triplets."}, {"title": "4.2.4 Emotion", "content": "Data collection. We gather facial images from two publicly available datasets, CelebA [34] and FER-2013 [20], focusing on eight annotated human emotional states: smiling, angry, disgusted, fearful, happy, neutral, sad, and surprised. We form image pairs from the same emotional state.\n\nAnnotation. We follow the annotation protocol in \u00a74.2.1 to curate a total of 5.3K triplets."}, {"title": "4.2.5 Temporality", "content": "Data collection. We consider images with time-related tags. One pertinent source is videos. Specifically, we use SoccerNet [19], a dataset for soccer video understanding. It annotates various soccer actions (e.g., free-kicks, corner-kicks, etc.) and specifies their exact periods (start-end frame indices). Using this temporal metadata, we extract two frames from each annotated action, creating an image pair that allows temporal comparison. We also consider CompCars [59], a dataset designed for fine-grained categorization of vehicles. This dataset offers a detailed ontology of car attributes, such as make, model, and year. We generate image pairs that feature the same car model from different production years, for instance, a 2017 Honda Civic vs. its 2015 counterpart.\n\nAnnotation. We automatically generate (rule-based) questions and answers about which frame or object is associated with an earlier/later time-related tag, for example, \u201cWhich frame happened first during the free-kick?\" To ensure that the two images are relevant enough to offer sufficient temporal\""}, {"title": "4.2.6 Spatiality", "content": "Data collection. We collect images with spatial tags, e.g., object locations. Specifically, we use NYU-Depth V2 [51], featuring indoor scenes with object segments and depths. Using the segmentation maps, we identify objects within each image, and group images containing the same objects.\n\nAnnotation. We follow the annotation protocol in \u00a74.2.1, leveraging pre-defined templates and object information to generate questions about spatial relative comparisons (e.g., \u201cWhich shelf is closer to the camera?\"), followed by human answer annotation. Overall, we curate 1.9K triplets.\""}, {"title": "4.2.7 Quantity", "content": "Data collection. We consider images with labels related to object instances. One prominent source is object detection datasets. Here, we use VQAv2 [21], which is built upon MSCOCO [10] and encompasses a variety of question types, such as object counting and color. We focus on the counting questions, grouping images with similar questions and sampling image pairs within each group.\n\nAnnotation. We use GPT-4 [1] to convert original absolute counting questions (e.g., \u201cHow many elephants are there?", "Which image has more elements?\"). The answers are derived automatically from VQAv2's ground-truth answers. We curate 9.8K triplets.\"\n    },\n    {\n      \"title\"": "4.2.8 Quality"}, {"content": "Data collection. We use Q-bench2 [62], a recently introduced dataset to evaluate low-level visual perception. Concretely, it challenges MLLMs to determine the quality (e.g., blurriness or distortion) of a single image or to compare the quality between two images.\n\nAnnotation. Through a meticulous filtering process (cf. \u00a74.2.1), we select paired images from Q-bench2, along with the annotated multiple-choice questions and answers, resulting in 1K triplets."}, {"title": "4.3 Quality Control and Dataset Statistics", "content": "To ensure the integrity of COMPBENCH, we ask annotators to exclude poor-quality examples, such as those with low-resolution images or questions that are irrelevant or nonsensical about the images. The annotators also filter out image pairs with ambiguous relativities, for example, image pairs with indistinguishable smiling degrees. To faithfully assess fine-grained capabilities, we also apply the CLIP visual similarity to Existence, removing image pairs with salient differences. Additionally, we implement a rigorous cross-verification process, where each annotator confirms the accuracy of others' answers. Only samples that receive unanimous approval from annotators are kept. Consequently, our COMPBENCH benchmark comprises 39.8K diverse triplets (eight relativities from fourteen visual domains) with high quality and reliability."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\n\nBaselines. We use our COMPBENCH to evaluate several leading MLLMs. This includes two powerful proprietary models, GPT-4V(ision) [1] and Gemini1.0-Pro2 [54], and two open-source alternatives, LLaVA-1.6 [32] and VILA-1.5 [31]. GPT-4V(ision) and Gemini excel in various vision and language tasks, such as VQA [21], OCR interpretation [39], spatial reasoning [37], and college-level subject knowledge [60]. LLaVA-1.6 and VILA-1.5 also demonstrate competitive performance against these proprietary giants on some tasks. Our focus is to investigate whether these cutting-edge models can extend their capabilities to the realm of multi-image relative comparison. We evaluate proprietary models via their official APIs and open-source models using (or fine-tuning on) NVIDIA RTX 6000 Ada GPUs. For more details, please refer to the supplementary material.\n\nEvaluation tasks & metrics. We divide our COMPBENCH into a test split (31.8K) and a held-out split (7.9K), using an 80:20 ratio. The latter is reserved for future developments (e.g., prompt engineering). By default, we concatenate the image pairs horizontally (i.e., left and right) as the visual input to MLLMs, and prompt MLLMs to answer questions about the relativity between these images. To facilitate automated evaluation, we include the possible answers as options in the questions. For Existence and Quality, there are multiple options (typically more than two). For Quantity, there are three options: left/right/same. For other types, there are binary options: left/right. We employ the standard accuracy as our evaluation metric. A question is answered correctly if the model prediction exactly matches the ground-truth answer. Further details are included in the supplementary material."}, {"title": "5.2 Main Results", "content": "Overall challenges in COMPBENCH. We observe that current MLLMs face challenges in answering relative questions in COMPBENCH (see Table 2). All MLLMs achieve averaged accuracies over the sixteen tasks (columns) below 80%, with GPT-4V reaching the highest accuracy at 74.7%. Further, a human evaluation study on a subset of our examples indicates that GPT-4V's performance remains notably behind human capabilities, highlighting the need for substantial improvement.\n\nSuperiority in State & Emotion. State relativity is an area where MLLMs demonstrate strength. For instance, GPT-4V/LLaVA-1.6 achieve 92.2%/89.7%, respectively, on MIT-states [24] for state relativity. Similarly, they demonstrate impressive performance in emotion relativity (91.8%/96.2% on CelebA [34]). Our preliminary analysis suggests that their capacity to determine the degree of emotion (e.g., smiling) relies on specific facial features such as lip curvature or visible teeth.\n\nChallenges in Existence. All MLLMs show weak performance in existence relativity tasks. We attribute this to the multiple capabilities these tasks demand, including spatial understanding and precise object recognition/comparison. For instance, when an object in the left image is moved to a different location in the right image, the models need to not only recognize the same object in the right image but also understand the relative change in its position. This necessitates both robust object recognition and accurate spatial reasoning. Given that an image can contain numerous objects, the model should have a deep understanding of how the existence of them changes between images.\n\nChallenges in Temporality and Spatiality. MLLMs encounter difficulties with both temporal relativity, which requires commonsense, and spatial relativity, which demands comprehension of depth perception between objects. Specifically, for the spatial task, all MLLMs perform below 70%, and notably, both proprietary models, GPT-4V and Gemini1.0-Pro, only achieve slightly above chance levels (56.1% and 56.6%, respectively). This underscores the need for further research in improving spatial relativity to advance models towards artificial general intelligence (AGI).\n\nChallenges in Quantity & Quality. We observe the mediocre performance of MLLMs in quantity relativity (e.g., GPT-4V: 63.8%, VILA-1.5: 47.7%). We attribute this to the models' weak capability in accurately counting objects in images. Similarly, MLLMs struggle with assessing image quality (e.g., 73.0% of GPT-4V's accuracy). These capabilities are crucial for making informed decisions in our daily lives (cf. \u00a71), highlighting the need for MLLMs to improve in these aspects.\n\nVariability in performance across domains. The performance of MLLMs varies in different domains. For instance, they excel at comparing visual attributes of daily objects [24] and clothing [26] while struggling with those of animals (e.g., birds [56], fish [64]). This could be due to the complexity of animal features, such as feathers, scales, or markings, which are more challenging for the model to interpret compared to simpler attributes in everyday objects."}, {"title": "5.3 Further Analyses", "content": "Two-stage reasoning. What if we first ask MLLMs to analyze each image in a pair separately (e.g., \"How far is the table from the camera that took this photo? Return a number in feet.\") and use their language responses to answer a follow-up pure language question (e.g., \"Based on the responses, which object is closer to the camera?\u201d)? We evaluate this two-stage reasoning approach on three comparison tasks: Existence, Emotion, and Spatiality. We find that GPT-4V, using this two-stage reasoning, performs less effectively on all three tasks. This is likely because analyzing images separately can sometimes be more challenging than comparing images directly. For instance, calculating the exact distance from an object to the camera may be difficult, leading to inaccurate numbers. In contrast, directly answering a question, \"Which object is closer to the camera?\u201d may be easier, as models only need to determine the relative closeness between objects.\n\nFine-tuning experiments. We conduct a study to see if fine-tuning helps improve the comparative capabilities of MLLMs. We focus on two comparative tasks, temporality and quantity. For temporality, we construct a total of 20.6K training examples from SoccerNet [19], following the similar data collection and annotation protocol described in \u00a74.2.5. For quantity, we curate 20.9K training samples from VQAv2 [21], based on the protocol in \u00a74.2.7. We then fine-tune LLaVA-1.6 [32] on each of these training datasets separately, using LoRA techniques. As shown , fine-tuning significantly benefits LLaVA-1.6 in the temporal task (SoccerNet). However, interestingly, it only marginal gains in quantity questions. We attribute this to its vision encoder, CLIP [46], which may have weak capabilities in counting the number of objects, as reported by several prior works [46, 41, 44]. This suggests considering new architectures or training strategies to improve its counting capabilities as future work."}, {"title": "6 Conclusion", "content": "We introduce COMPBENCH, a comprehensive benchmark designed to evaluate comparative reasoning in multimodal LLMs (MLLMs). COMPBENCH offers extensive coverage of eight relative comparisons between pairs of images drawn from fourteen diverse domains. COMPBENCH evaluates recent MLLMs, offering detailed analyses and insights for future advancements."}, {"title": "A Discussions", "content": "A.1 Limitations\n\nWhile we conducted a human evaluation study to establish the upper bound performance on COMP-BENCH, the study is currently limited to 140 samples assessed by five evaluators (cf. \u00a75.3 in the main text). We plan to expand the study to a larger scale in future work.\n\nA.2 Social impacts\n\nCOMPBENCH evaluates the comparative reasoning abilities of MLLMs in images. A potential negative impact of our work is that malicious users might exploit our concept (i.e., comparison) to compare ethical or offensive content. Therefore, it is essential to incorporate effective safeguards in MLLMs to filter out any inappropriate materials.\n\nA.3 Ethical considerations\n\nAll fourteen datasets (cf. Table 1 in the main text) that we used to curate COMPBENCH adhere to strict guidelines to exclude any harmful, unethical, or offensive content. Additionally, we instruct human annotators to avoid generating any personally identifiable information or offensive content during our annotation process. Finally, we do not conduct any study to compare harmful, ethical, or offensive content between the two images.\n\nA.4 License of assets\n\nAll fourteen datasets are publicly available, and Table 5 details the licensing information for the assets in each dataset. We release our COMPBENCH under a Creative Commons Attribution 4.0 License (CC BY 4.0) to enhance global accessibility and foster innovation and collaboration in research."}, {"title": "B COMPBENCH Curation Details", "content": "B.1 Annotation Details\n\nWe create UI interfaces for annotation using Python in Jupyter Notebook and store the annotations in JSON files. In the following sections, we provide detailed descriptions of the annotation process for each dataset, which are omitted in the main text.\n\nMagicBrush [61] is a large-scale, manually annotated dataset for instruction-guided real image editing. For each image, MagicBrush utilizes DALL-E 2 [48] to generate an edited version of the image based on language instructions, such as let the flowers in the vase be blue.\" Our goal is to identify pairs of similar images. We thus use CLIP [46] to evaluate the visual similarity between the original and edited images. Only pairs exceeding a predetermined similarity threshold are selected as candidate samples for our COMPBENCH. For each selected pair, we then construct a multiple-choice question to ask the difference between two images in the pairs. Concretely, we first use GPT-4V [1] to extract all relevant objects and their attributes from the edited image with the following prompt:"}]}