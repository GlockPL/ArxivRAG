{"title": "DATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models", "authors": ["Fei Wang", "Ninareh Mehrabi", "Palash Goyal", "Rahul Gupta", "Kai-Wei Chang", "Aram Galstyan"], "abstract": "Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose DATA ADVISOR, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, DATA ADVISOR monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. DATA ADVISOR can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of DATA ADVISOR in enhancing model safety against various fine-grained safety issues without sacrificing model utility. Warning: this paper contains example data that may be offensive or harmful.", "sections": [{"title": "Introduction", "content": "Data serves as a crucial element in the alignment of large language models (LLMs), as data quality and coverage profoundly impact the utility and safety of LLMs (Wang et al., 2023a; Ouyang et al., 2022; K\u00f6pf et al., 2023; Yin et al., 2023; Conover et al., 2023). Since human annotation is costly and does not scale easily, recent studies have utilized LLMs to produce new datasets (Wang et al., 2023b; Yuan et al., 2024; Xu et al., 2023b; Honovich et al., 2023; Xu et al., 2023a; Mehrabi et al., 2023), with the main human involvement being the provision of a small set of seed data as in-context examples.\nAlthough LLM-generated data can readily scale, it often suffers from known quality issues (Chen et al., 2023; Yu et al., 2024; Liu et al., 2023). Previous methods typically generate new data via in-context learning (Wang et al., 2023b; Yuan et al., 2024), without considering dataset-level properties (e.g., coverage and diversity). Without additional guidance, the data generator is unaware of the overall dataset statistics, which can lead to the omission of specific aspects and the amplification of its own biases over iterations (Das et al., 2024; Chung et al., 2023; Felkner et al., 2024). Thus, the generated data can fail to align LLMs with diverse goals, such as addressing fine-grained safety issues (Bhardwaj et al., 2024; Inan et al., 2023; Ji et al., 2023). Moreover, some issues can manifest as low-quality datapoints, such as ambiguous or redundant questions. Although filtering out and refining low-quality data is possible (Chen et al., 2023; Liu et al., 2023; Parkar et al., 2024; Bai et al., 2022b), the postprocessing pipelines lead to a notable reduction in preserved data. For instance, Alpagasus (Chen et al., 2023) noted that 83% of Alpaca (Taori et al., 2023) data should be discarded due to its detrimental impact on LLM alignment. These observations underscore the significance of proactively generating expected data, a direction that remains under-explored in existing literature.\nIn this paper, we propose DATA ADVISOR, which enhances LLM-based data generation by dynamically and proactively incorporating guiding principles of the target dataset (for safety alignment).\u00b9 DATA ADVISOR instructs the data generator to create alignment data with predefined principles, involving both quality and directional control of an independent prompt, as well as the overall statistics of the dataset. With a set of principles in hand, DATA ADVISOR monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of"}, {"title": "Preliminaries", "content": "LLMs have demonstrated advanced capabilities in instruction following (Zhang et al., 2023) and in-context learning (Brown et al., 2020). Building upon these capabilities, recent studies have applied LLMs to generate data automatically for further training themselves or other LLMs, reducing the need for extensive human annotation (Wang et al., 2023b; Yuan et al., 2024). As shown in the bottom of Fig. 1, the typical data generation process begins with a set of seed data serving as the exemplar pool. This process is performed iteratively. In each"}, {"title": "DATA ADVISOR", "content": "DATA ADVISOR seeks to enhance LLM-based data generation methods by dynamically guiding the process with principles aligned to the desired dataset. With an LLM acting as the advisor, the advice for data generation is achieved through a series of automatic communications between the advisor and the existing data. With a set of guiding principles, DATA ADVISOR monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. These principles for data generation specify the purpose of the dataset, key properties to focus on, and additional requirements throughout the generation process. These principles are in the same spirit as collecting human supervision based on a set of guidelines to govern AI behavior, akin to the concept of Constitutional AI (Bai et al., 2022b). They can vary depending on the application scenarios. We leave further discussion of data generation principles in different scenarios to applied researchers and legal experts. In the following paragraphs, we use diversity and coverage of safety issues as example principles to introduce the details of the method.\nData Summarization. Initially, given the existing data, DATA ADVISOR generates a concise report about the data properties, including the distribution of data across various perspectives. This step is formulated as query-focused summarization. The principles (such as topics and domains to cover) for guiding the generation of expected data are converted into a meta-summary and provided to DATA ADVISOR as a prompt. The detailed prompt template for this step is shown in Appx. \u00a7A. The advisor then completes the report based on the existing data. However, as the dataset size could continuously expand, it becomes impractical to provide all data to the advisor as a holistic prompt every time. Therefore, we adopt an iterative approach to updating the summary. In each iteration, the advisor receives the newly generated data point along with the previous summary as input. At the outset, we query the advisor to summarize the seed data from scratch without any previous summary available. This iterative process allows for a more efficient and scalable monitoring of the dataset's properties and evolution. The typical prompt template for this step is shown as follows, with a detailed version in Appx. \u00a7A.\nThis step is visualized as the top right part of Fig. 1. In safety alignment, DATA ADVISOR initializes the data summary with the fine-grained safety issues contained in the seed data. For example, the seed data covers self-harm, violence, and illegal activities. Then, when a new data point is gen-"}, {"title": "Data Weakness Identification", "content": "Next, DATA ADVISOR identifies data weaknesses according to the data summary and the predefined principles. Each iteration, the data advisor is prompted to discern a specific weakness. We provide the data summary along with data generation principles as a prompt to the data advisor. The detailed prompt template for this step is shown in Appx. \u00a7A. By translating the summary into actionable insights, the DATA ADVISOR enables the data generator to focus on addressing specific weaknesses afterwards, thereby facilitating the iterative improvement of the generated data. The typical prompt template for this step is shown as follows, with a detailed version in Appx. \u00a7A."}, {"title": "Data Generation with Advice", "content": "Finally, DATA ADVISOR generates the new data point targeting the identified weakness. This step is formulated as controlled generation. The weakness is converted into a prompt, which is then forwarded to the data generator, providing guidance for the generation of the next data point. In this way, standard data generation is combined with control signals, guiding the generator to focus on specific aspects to fulfill specific goals. As a result, the newly generated data can enhance the overall quality of the dataset. This iterative process ensures that the dataset remains diverse, relevant, and aligned with the desired objectives. The typical prompt template for this step is shown as follows, with a detailed version in Appx. \u00a7A."}, {"title": "Experiment", "content": "In this section, we first introduce the experimental setup (\u00a74.1), with a particular focus on the evaluation of safety and utility. This is followed by the presentation of the main results on three representative LLMs (\u00a74.2). Finally, we provide detailed analyses of fine-grained model performance, data diversity, data mixture, and qualitative results (\u00a74.3)."}, {"title": "Experimental Setup", "content": "Evaluation Protocol. We evaluate the quality of the LLM-generated data by assessing how well LLMs perform after finetuned on the data. Following previous works (Ge et al., 2023; Touvron et al., 2023), we finetune base LLMs with a mixture of safety alignment data and additional instruction-tuning data to balance the model's safety and utility. Then, we evaluate the model's safety by prompting the finetuned LLMs with harmful questions and evaluate the harmful rate of their responses. We also evaluate the model's utility on a multitask language understanding benchmark.\nEvaluation Datasets. For safety evaluation, we use two harmful question datasets with detailed harmful categories designed for evaluating fine-grained LLM safety. CatQA (Bhardwaj et al., 2024) consists of 550 harmful questions evenly distributed on 11 categories, where each category have five sub-categories. Fig. 3 presents all the categories, such as economic harm and malware viruses. BeaverTails (Ji et al., 2023) has 700 harmful questions covering 14 harm categories, such as adult content and child abuse. Fig. 4 presents all the categories. For utility evaluation, we use MMLU (Hendrycks et al., 2020), a multitask language understanding benchmark that is widely used to evaluate the utility of LLMs. Specifically, we use the validation set consisting of 1,530 multiple-choice"}, {"title": "Evaluation Metrics", "content": "Following Zhou et al. (2024), we use LlamaGuard (Inan et al., 2023) as an automatic evaluation metric. LlamaGuard can classify each prompt-response pair into safe or unsafe. We report the ratio of safe responses as safety score and the ratio of unsafe responses as harmful rate on each dataeset. For MMLU, we report the average accuracy as the utility score."}, {"title": "Base Models", "content": "We conduct experiments on three representative LLMs. Mistral-v0.1 (Jiang et al., 2023) is a pretrained language model released under the Apache 2.0 license. Llama2 (Touvron et al., 2023) is pretrained on 2 trillion tokens of public data. Falcon (Almazrouei et al., 2023) is trained on 1,500B tokens of RefinedWeb (Penedo et al., 2023) and is released under the Apache 2.0 license. For all the three models, we use the base version of 7 billion parameters without instruction tuning and safety alignment."}, {"title": "Baseline", "content": "We compare DATA ADVISOR with the widely used LLM-based data generation method, Self-Instruct (Wang et al., 2023b). Starting from a small set of seed data, it generates new data with in-context learning. After each iteration of data generation, the candidate pool of in-context examples is updated and enlarged. Self-Instruct is originally proposed to generate instructions, inputs, and outputs at the same time. We follow Yuan et al. (2024) to generate 10K prompts independently for safety alignment."}, {"title": "Implementation Details", "content": "For both Self-Instruct and DATA ADVISOR, we use Mistral-7B-Instruct-v0.2 as the data generator. We use a safety-aligned LLM (i.e., Llama2-Chat-7B) to pair each prompt with a safe response. For DATA ADVISOR, we randomly sample three in-context examples for 10 times in each iteration and generate 10 prompts"}, {"title": "Main Results", "content": "Fig. 2 presents the safety and utility metrics of three models before and after training with LLM-"}, {"title": "Analysis", "content": "We provide detailed analyses from four perspectives: results on fine-grained safety issues, data diversity, the effect of data mixture, and qualitative results of data generated by DATA ADVISOR."}, {"title": "DATA ADVISOR improves model performance on all harmful categories", "content": "We further analyze the fine-grained results by harmful category. Fig. 3 shows the results by category on CatQA. DATA ADVISOR achieves better or comparable harmful rates across all categories, with the rates for Adult Content, Child Abuse, Hate/Harass/Violence, and Tailored Financial Advice dropping to zero, whereas Self-Instruct may generate harmful responses across all categories. The category where DATA ADVISOR outperforms Self-Instruct the most is Economic Harm, with a performance gap of 24%. This is followed by Adult Content, Child Abuse, and Illegal Activity, each with a performance gap of 20%. Fig. 4 shows the results on BeaverTails. Similarly, DATA ADVISOR achieves lower harmful rates across all categories compared to Self-Instruct. The largest performance gap appears in the categories of organized crime and terrorism, where DATA ADVISOR reduces harmful rates by an additional 28%. Following this, DATA ADVISOR outperforms Self-Instruct in aiding and abetting, incitement, and violence by 22%."}, {"title": "DATA ADVISOR can improve data diversity", "content": "To evaluate data diversity, we measure the ratio of distinct n-grams (Li et al., 2016) in prompts from both LLM-generated safety alignment data and human-annotated evaluation data. As shown in Fig. 5, the evaluation data, which is carefully cu-"}, {"title": "Mixture of safety alignment and instruction tuning data is necessary", "content": "Fig. 6 shows the performance of Mistral-based models trained with different alignment data. The results suggest that both the safety alignment data generated by DATA ADVISOR and the instruction tuning data from Alpagasus are essential for balanced performance. Without training data targeting safety, model performance on CatQA and BeaverTails drops by 51.5% and 23.0%, respectively. Conversely, without training data targeting utility, although model safety can exceed 99%, utility drops by 16.9%, which is worse than the base model before training. Combining both types of data balances the safety and utility of the aligned model, resulting in a model that is both safer and more helpful. Notably, the model's utility after training with the mixture of data is better than when trained with Alpagasus data alone."}, {"title": "Correctness of Intermediate Outputs", "content": "We further analyze the quality of summarization and weakness identification in each iteration. The summaries and weaknesses are presented in a structured format. We extract the updated part in each iteration and check their quality. For summaries, we assess if the newly added weaknesses are not included or if partial content from the last summary is missing. Overall, 84% of the summaries are updated accurately. For weaknesses, we assess if they introduce new safety issues not identified in prior iterations by comparing key words. Overall, 75% of the weaknesses introduce new safety issues. Notably, this ratio does not change significantly as the iterations increase. In the first 500 iterations, the summary accuracy is 85% and the weakness accuracy is 77%. In the last 500 iterations, the summary accuracy is 83% and the weakness accuracy is 71%. We argue that the data advisor is noise-tolerant. Even if no weakness is identified in an iteration, the data advisor can still benefit from the more diverse exemplar pool accumulated in prior iterations and generate more diverse data than Self-Instruct. As we use a highly structured summary and weakness format which only requires minimal updates each iteration, future work can improve the stability of summarization with rule checks and correct the errors based on the feedback of the checker."}, {"title": "Qualitative Results", "content": "We present examples of prompts with safety concerns generated by DATA ADVISOR in Tab. 1. These prompts are distributed throughout the generation iterations, covering diverse categories of safety issues. We observe that DATA ADVISOR can identify underrepresented or missing safety issues in the existing data and suggest new directions for the next iteration of data generation. The capabilities of identifying weaknesses and advising new directions do not degrade with iterations. Even around iteration 1,000, DATA ADVISOR continues to propose new safety issues (e.g., Moral Dilemma Inducing), thereby increasing data diversity. Some of the generated safety issues are rarely explored in previous datasets, such as challenges to personal beliefs, threats to linguistic diversity, and moral dilemmas."}, {"title": "Related Work", "content": "In this section, we briefly review two relevant research directions."}, {"title": "LLM-based Data Curation", "content": "The landscape of data curation with LLMs has seen significant advancements recently. In terms of instruction tuning data generation, Wang et al. (2023b) introduce Self-Instruct, where LLMs generate instruction-following data. Yuan et al. (2024) follow the Self-Instruct method to iteratively generate data and updating the LLM. Other works, such as Taori et al. (2023), explore using a strong LLM like GPT-4 to generate complex instructions. Instruction Backtranslation (Li et al., 2023) augments and curates training data by backtranslating between instructions and responses. Prior work has also explored generating preference data with LLMs (Lee et al., 2024; Shi et al., 2024). In addition to data generation, another line of work investigates data cleaning with LLMs. Chen et al. (2023) use advanced LLMs to assess the quality of generated data. Bai et al. (2022b) prompt LLMs to refine the generated data. These works collectively contribute to the enhancement of data curation capabilities in LLMs. However, the proactive generation of datasets with targeted properties remains underexplored, which is the focus of our paper."}, {"title": "Safety Alignment", "content": "The increasing prominence of LLMs has underscored the critical importance of enhancing their safety and reliability (Touvron et al., 2023; Inan et al., 2023). Various techniques have been proposed to address safety concerns, notably during the phases of supervised fine-tuning, instruction tuning, and preference alignment (Bai et al., 2022a; Ge et al., 2023). Among these techniques, a com-"}, {"title": "Conclusion", "content": "In this paper, we propose DATA ADVISOR, an LLM-based data generation method dynamically and proactivelyguiding the process with principles aligned to the target dataset. With a set of predefined principles in hand, DATA ADVISOR monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Experiments on safety alignment of three representative LLMs demonstrate the effectiveness of DATA ADVISOR in enhancing model safety against various fine-grained safety issues without sacrificing model utility. Further analyses show that DATA ADVISOR exhibits better data diversity than Self-Instruct, and its ability to identify dataset weaknesses does not degrade with iterations of data generation. Future work can extend DATA ADVISOR to other scenarios, such as mitigating backdoor in instruction tuning data (Xu et al., 2024), preventing data bias in preference optimization (Wang et al., 2024b), and integrating constraints for task adaptation (Wang et al., 2024a)."}, {"title": "Limitation", "content": "While we have conducted comprehensive experiments on safety alignment to demonstrate the effectiveness of DATA ADVISOR, there are still several limitations. First, applying DATA ADVISOR to generate other types of data, such as instruction tuning data, remains unexplored. Future work could investigate the potential of DATA ADVISOR in these areas to further validate its versatility and efficacy. Second, the scale of our experiments is limited to 7B models and a dataset size of 10K. Larger-scale experiments involving bigger models and more extensive datasets could provide additional insights into the robustness and scalability of DATA ADVISOR. Third, there are multiple choices for some components in DATA ADVISOR, but we have only experimented with a subset of these options. Exploring a wider range of configurations and parameters could uncover more optimal settings and enhance the overall performance of DATA ADVISOR. Addressing these limitations in future research will help to solidify the practical applications of DATA ADVISOR and ensure its effectiveness across a broader spectrum of use cases."}, {"title": "Ethical Consideration", "content": "We recognize that LLMs, if not properly trained, can inadvertently produce responses that are biased, offensive, or otherwise harmful. Our experiments focus on mitigating unethical responses from LLMs. To achieve this goal, our method generates prompts that cover a wide range of real-world scenarios that may have ethical concerns. While one could respond with harmful information, our dataset contains only harmless responses. The dataset is intended to be used to enhance the harmlessness of LLMs."}, {"title": "A Prompt Template", "content": "Harmfulness Scenario Summary:\n{PREVIOUS_SUMMARY}\nPlease add the follwing scenario into the above summary:\n{NEW_INSTANCE_CATEGORY}\nYou must add no more than five words.\nIf the new issue is not covered in the existing summary, add a new line to the summary.\nEnd your summary with \"-\".\nThe summary must be concise. Please ensure that each line has no more than five words.\nHarmfulness Scenario Summary:\nHarmfulness Categories:\n{DATA_SUMMARY}\nWhat category is not covered in this list? Avoid using words in the existing ones. Try to creatively explore diverse harmfulness.\nNew Harmfulness Category:\nYou are asking a chatbot to help you do something harmful.\nBelow are some example queries from you:\n{IN_CONTEXT_EXAMPLES}\nPlease generate a new query. Avoid using words in the examples.\nYou aim to do harmful activities related to:\n{IDENTIFIED_WEAKNESS}\nTo ensure a harmful response from the chatbot, what should you say?\nYou:"}]}