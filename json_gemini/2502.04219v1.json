{"title": "NLP-Based .NET CLR Event Logs Analyzer", "authors": ["Maxim Stavtsev", "Sergey Shershakov"], "abstract": "In this paper, we present a tool for analyzing .NET CLR event logs based on a novel method inspired by Natural Language Processing (NLP) approach. Our research addresses the growing need for effective monitoring and optimization of software systems through detailed event log analysis. We utilize a BERT-based architecture with an enhanced tokenization process customized to event logs. The tool, developed using PYTHON, its libraries, and an SQLITE database, allows both conducting experiments for academic purposes and efficiently solving industry-emerging tasks. Our experiments demonstrate the efficacy of our approach in compressing event sequences, detecting recurring patterns, and identifying anomalies. The trained model shows promising results, with a high accuracy rate in anomaly detection, which demonstrates the potential of NLP methods to improve the reliability and stability of software systems.", "sections": [{"title": "1 Introduction", "content": "Most organizations use various software systems, necessitating effective monitoring and resource allocation. Event logs, as primary artifacts of software operations, are crucial to understanding system functions and identifying optimization opportunities. Process mining combines process science and data analysis methods to extract value from such logs, which allows the optimization of processes. This project extends the approach proposed by Stepanov and Mitsyuk [6] by enhancing low-level .NET event log analysis in two ways: 1) pattern detection, to understand system interactions, and 2) anomaly detection, to identify and prevent abnormal behaviors.\nWe apply neural network models for automated and scalable analysis. Unsupervised learning is utilized due to large, unlabeled datasets typical in software systems. Using transformer-based NLP methods, we tokenize event traces to identify patterns and anomalies. This work demonstrates the effective application of NLP techniques to .NET CLR event log analysis.\nThis article is organized as follows: Section 2 provides an overview of existing solutions for event log analysis, Section 3 describes the algorithms used in the project, Section 4 presents the proposed method for event log analysis, including training the machine learning model and implementation of the proposed algorithms, and finally Section 5 offers a summary of the article."}, {"title": "2 Related Work", "content": "In [6] authors propose a method for extracting high-level activities from low-level event logs of program execution. To achieve this, they developed a tool called PROCFILER, which collects events that occur during the execution of programs written in the C# programming language in the .NET CLR runtime environment and creates a log from them. They then apply a predefined hierarchy to raise the abstraction level of the events. \nIn our project, we utilized the results of the Procfiler tool, specifically logs with the lowest level of abstraction, meaning they contain events that are the leaves in the hierarchical tree shown in Figure 1.\nFor the task of anomaly detection in event logs, supervised learning methods have been applied, treating this task as a binary classification problem [3]. However, this significantly reduces the applicability of such methods in real systems, as it requires a prelabeled dataset, and more importantly, limits the ability to detect previously unseen anomalies. There are works that use unsupervised learning approaches [2] based on LSTM, as well as the BERT model [5], which is based on the transformer architecture and employs preliminary tokenization of the event log."}, {"title": "3 Algorithms", "content": "There are some major limitations to the rapid and efficient analysis by process mining methods. Among them are the large volume of event logs and the need to perform a preliminary analysis of the input data to understand the structure of interacting process elements. One of the most important hypotheses in this work is that approaches adapted from the field of NLP can remove such limitations."}, {"title": "3.1 Event Log Encoding", "content": "Most NLP algorithms are applied to sequential data. For the task of event log analysis, we need to represent logs as sequences for further application of the algorithm.\nBy using this approach, we obtained the final set of event log traces.\nIn order to represent traces as textual sequences, each activity from the set of all allowed activities A in this work is encoded with a unique non-control Unicode character\u00b9, let U be the subset of these characters. Thus, we formed a bijection between the set A and the set U, $f : A\u2192 U$.\nFor example, consider a bijective function $f_0 \u2286 f$, defined by the set of pairs.\nThen the trace $trace_1$ can be represented as the sequence $seq_1$ = \"acbd\".\nWe reduced the task of representing a trace to a sequence of Unicode characters, solvable using NLP tokenization algorithms such as BPE, WORDPIECE, and UNIGRAM. We chose the BPE algorithm because it preserves a dictionary of tokens, merging the most common pairs. After completing the tokenizer training, we obtained the final set of permitted tokens, denoted as T, where each token represents a subsequence of Unicode characters.\nAny sequence $seq_1$ can be represented as $tokens_1$ = ($t_j : t_j \u2208 T, i = 1,..., q(seq)$), where $q(seq)$ determines the number of tokens. The tokenization process, defined as $T : seq_i \u2192 tokens$, converts $seq_i$ into the set $tokens_i$ = ['ac',' bd']. Tokenization also compresses traces, reducing the input sequence length significantly.\nEach token is encoded with a unique number corresponding to a value in the embedding table (numeric vectors), which in turn are trainable parameters of the neural network, the configuration of which we will describe in Section 3.2. Using numeric vectors, we can encode traces and feed them to the neural network input."}, {"title": "3.2 Neural Network Configuration", "content": "A key algorithm in deep learning, especially in NLP, is the transformer architecture [7], which consists of an encoder and a decoder. The encoder processes the input sequence with the attention mechanism and feed-forward layers, producing vectors that the decoder further transforms into a probability vector. BERT [1] is a transformer model that is based only on the encoder and applies attention to tokens based on their context within a sequence.\nOne of the ways to train BERT is the Masked Language Modeling (MLM) approach, which involves masking a certain percentage of randomly selected tokens with a special token [MASK]. During training, the model aims to minimize the loss function's error by predicting the token hidden behind the [MASK]. We claim that a model trained on unlabeled correct (without anomalies) event logs can detect events that do not match the context of normal behavior. For this reason, in this work, we apply a BERT-based model for anomaly detection.\nAfter analyzing existing BERT-based model architectures, we selected the SQUEEZEBERT architecture [4] for this work. The authors of the work presenting this architecture show"}, {"title": "4 Method", "content": ""}, {"title": "4.1 Patterns Detection", "content": "Tokens in the tokenizer's dictionary reflect frequently occurring interactions, thus considered patterns in this work. For instance, a group of events events encoded by symbols a, b, c appearing as the token bac in the event trace is a pattern. We trained 13 tokenizers with dictionary sizes from 512 to 20,000 tokens, some with a maximum token length limit, to analyze these patterns at different abstraction levels \u2013 higher levels encode larger numbers of events into single tokens.\nThe pattern detection process involves two algorithms. Algorithm 1 describes obtaining a list of traces from raw CLR low-level event logs by extracting necessary columns and combining events by timestamps. Algorithm 2 extracts tokens from event traces at a specified abstraction level (LoA), forming a list of acceptable events and applying a mapping to create sequences. These sequences are then tokenized using the trained tokenizers, resulting in a list of tokens for each trace.\nConsider an example of the results of the pattern search algorithms on event logs of 25 C# program runs."}, {"title": "4.2 Anomalies Detection", "content": "Anomaly detection is based on the SQUEEZEBERT neural network architecture, and there are two main approaches in the NLP field for using machine learning models. The first approach involves fine-tuning an already trained model for specific tasks, which is usually optimal. However, this approach is not feasible in our case as the BERT-based model has not been previously applied to .NET CLR event logs. Therefore, we train the model from scratch using a tokenizer with a maximum abstraction level of 13, a dictionary size of 20,000 tokens, and a maximum token length of 300 characters.\nWe used the LAMB optimizer [8] for training. The final model consists of 43.6 million parameters and was trained on the same dataset used for tokenizers, employing a tokenizer with a maximum abstraction level of 13, which has a vocabulary size of 20,000 tokens, and a maximum token length of 300 characters. The context window size was set to 512 tokens, with shorter traces padded using the [PAD] token. Training was conducted for 300 epochs in the Google Colab environment on an Nvidia Tesla A100 GPU.\nThe anomaly detection algorithm is described in Algorithm 3. It takes a list of traces as input, which are subsequently tokenized based on Algorithm 2. Then, it performs an evaluation using two methods: probability-based and loss-based, as well as Brier score evaluation.\nThe probability and loss evaluation is performed by masking each token in a trace with the [MASK] token, applying the SQUEEZEBERT model, and comparing the model output with the observed value. If the probability of the observed token is less than 0.85, it is considered anomalous. Similarly, if the loss function value is greater than 0.05, the token is considered anomalous.\nThe Brier score helps detect anomalies at group token levels, increasing the accuracy of detecting incorrect behavior. By masking 20% of randomly selected tokens and calculating the Brier score for them, we determine if the entire trace is anomalous if the score exceeds 0.5.\nWe also used a SQLite database to store these three evaluations for each trace. If an identical trace appears again, we retrieve the scores from the database instead of rerunning the model. This approach saves time and computational resources.\nModel validation was performed on the basis of 25 synthetically generated anomalous traces, as well as 25 traces with normal behavior. Anomalous traces were obtained by adding five random events at random positions, simulating the expected anomalous behavior in the trace. The validation results are presented in the confusion matrix in Figure 4.\nAs observed in Figure 4, the developed model shows satisfactory results; however, it makes errors in 6 cases. This error is less problematic because falsely labeling a normal trace as anomalous is preferable to missing an actual anomaly. Increasing the volume of training data could improve the model's quality, so further training on a larger dataset is recommended."}, {"title": "5 Conclusion", "content": "In this study, existing NLP approaches applied to the analysis of event logs were examined. We developed a tool in PYTHON, which is designed for analyzing patterns and anomalies in logs of .NET CLR applications. Our tool supports multiple levels of abstraction for pattern detection and utilizes an SQLITE database to store and reference previously analyzed traces, optimizing performance. A model based on the BERT architecture, specifically SQUEEZEBERT, was trained from scratch. Validation results demonstrate that the model performs well in anomaly detection, and it is expected that increasing the volume of data can improve its quality. Thus, we have shown that NLP approaches can be effectively applied to the analysis of event logs in the .NET CLR runtime environment."}]}