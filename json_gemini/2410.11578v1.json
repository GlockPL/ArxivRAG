{"title": "STA-Unet: Rethink the semantic redundant for Medical Imaging Segmentation", "authors": ["Vamsi Krishna Vasa", "Wenhui Zhu", "Xiwen Chen", "Peijie Qiu", "Xuanzhao Dong", "Yalin Wang"], "abstract": "In recent years, significant progress has been made in the medical image analysis domain using convolutional neu-ral networks (CNNs). In particular, deep neural networks based on a U-shaped architecture (UNet) with skip connec-tions have been adopted for several medical imaging tasks, including organ segmentation. Despite their great success, CNNs are not good at learning global or semantic features. Especially ones that require human-like reasoning to un-derstand the context. Many UNet architectures attempted to adjust with the introduction of Transformer-based self-attention mechanisms, and notable gains in performance have been noted. However, the transformers are inherently flawed with redundancy to learn at shallow layers, which often leads to an increase in the computation of attention from the nearby pixels offering limited information. The re-cently introduced Super Token Attention (STA) mechanism adapts the concept of superpixels from pixel space to to-ken space, using super tokens as compact visual represen-tations. This approach tackles the redundancy by learning efficient global representations in vision transformers, es-pecially for the shallow layers. In this work, we introduce the STA module in the UNet architecture (STA-UNet), to limit redundancy without losing rich information. Experimental results on four publicly available datasets demonstrate the superiority of STA-UNet over existing state-of-the-art architectures in terms of Dice score and IOU for organ segmentation tasks.", "sections": [{"title": "1. Introduction", "content": "Leveraging advancements in deep learning, computer vision techniques have become integral to medical image analysis. Among these techniques, image segmentation holds significant importance. Specifically, precise and reliable segmentation of medical images is crucial, serving as a foundational component in computer-aided diagnosis and image-guided surgical procedures [5, 10].\n\nCurrent approaches to medical image segmentation pre-dominantly utilize fully convolutional neural networks (FC-NNs) with a U-shaped architecture [16, 18, 30]. The widely recognized U-Net [30], a classic example of this architec-ture, features a symmetric Encoder-Decoder design linked by skip connections. The encoder extracts deep features with extensive receptive fields through multiple convolu-tional and down-sampling layers. The decoder then up-samples these deep features back to the original resolu-tion for precise pixel-level semantic predictions, while the skip connections merge high-resolution features from var-ious scales within the encoder to mitigate spatial informa-tion loss due to down-sampling. This well-crafted design has enabled U-Net to succeed significantly across numer-ous medical imaging tasks. The remarkable performance of these FCNN-based methods in cardiac segmentation, organ delineation, and lesion detection underscores CNNs' robust capability in learning distinguishing features.\n\nWhile CNN-based techniques have demonstrated im-pressive results in medical image segmentation, they still fall short of the high accuracy standards required for clin-ical applications. Medical image segmentation remains a challenging problem, primarily due to the convolution op-eration's inherent focus on local features, making it diffi-cult for CNNs to capture explicit global and long-range se-mantic interactions. Recently, inspired by the success of Transformers in natural language processing (NLP) [34], researchers have started to explore their application in the vision domain [4, 8,27] to address the limitations of CNNS using self-attention. While the Vision Transformer (ViT) excels at capturing long-range dependencies across image patches with its large receptive field, it faces challenges in retaining fine-grained local context due to its lack of inher-ent locality."}, {"title": "2. Related Work", "content": "UNet-based architectures: Early methods for medi-cal image segmentation primarily relied on contour-based approaches and traditional machine learning techniques [12, 33]. However, the advent of deep convolutional neural networks (CNNs) brought significant advancements, with the introduction of Unet [30] specifically designed for med-ical image segmentation. The U-Net's distinctive U-shaped architecture, noted for its simplicity and exceptional per-formance, has inspired numerous variations, including Res-Unet [37], Dense-Unet [25], U-Net++ [39], and UNet3+ [14]. CNN-based architectures are flawed in capturing re-dundant information and do not focus on learning the de-pendencies between different regions of the canvas.\n\nTransformer based UNet architectures : Vaswani et al. [34] introduced the Self-attention mechanism using Transformers in Natural Language Processing to weigh the importance of different words relative to each other. This advancement led to the development of the Vision Trans-formers (ViT) [8, 27], which adapts the transformer archi-tecture to achieve comparable success in image processing tasks. These transformers are integrated with the UNet de-signs [3, 5, 38], with the aim to combine the strengths of CNNs and Transformers.\n\nChen et al. [5] attempted to combine the Transformers in the encoder and decoder of the UNet architecture. The Transformer block in the encoder tokenizes image patches from a CNN feature map to capture global context. Mean-while, the decoder upsamples these encoded features and merges them with high-resolution feature maps from the CNN. Although the ViT excels in capturing long-range de-pendencies between image patches (tokens) due to its large receptive field, it faces challenges in maintaining detailed local context because of its lack of inherent locality. To overcome this limitation, Swin-Unet [3] adapts the atten-tion mechanism using shifting window tokens [27]. This allows the model to restrict window-based attention to local regions. Although this adaptation limits the redundancy, it is not completely eradicated from the shallow layers. Mean-while, Zhu et al. [40] proposed Seg-SwinUNet, which ad-dresses performance issues in UNet for medical image seg-mentation by balancing supervision between the encoder and decoder and reducing feature redundancy. It tried to enhance UNet by using feature distillation to provide addi-tional supervision from the most semantically rich feature map, improving segmentation accuracy with minimal com-putational overhead. However, the work is still limited to Swin-UNet, and no further study has been conducted to in-corporate this approach with other architectures.\n\nXu et al. [38] proposed LeViT-Unet, where LeViT [9] as the encoder within the LeViT-UNet framework, as it ef-fectively balances accuracy and efficiency in Transformer blocks. Additionally, skip connections integrate multi-scale"}, {"title": "3. Preliminary Analysis", "content": "Here, we employ Centered Kernel Alignment (CKA) [19] to investigate the recent popular U-net models, includ-ing SwinUnet, LeViT-Unet, TransUnet, and HiFormer. This technique allows us to compute the block-wise similarity even when the layers' sizes are different. The block-wise similarity matrices are able to offer insights into how differ-ent neural network architectures learn and represent infor-mation at various layers (or blocks) throughout the training process.\n\nMathematically, given two sets of representations X and Y, we first compute their gram matrices K, L via the Ra-dial Basis Function (RBF),\n\n$K_{ij} = exp(-\\frac{||X_i \u2013 X_j||^2}{\\gamma}),$\n\n$L_{ij} = exp(-\\frac{||Y_i \u2013 Y_j||^2}{\\gamma}),$\n\nwhere $K_{ij}$ and $L_{ij}$ denotes the ith row and jth column ele-ment. $X_i, X_j, Y_i \u2013 Y_j$ denotes the ith and jth sample of the set X, Y, respectively. Afterward, the similarity matrix is computed through RBF-CKA as,\n\nRBF-C\u041a\u0410 =$\\frac{tr(KHLH)}{\\sqrt{tr(KHKH)tr(LHLH)}}$,\n\nwhere $H_n = I_n - \\frac{1}{n}11^T$ denotes the centering matrix. $I_n$ denotes a identity matrix with shape n x n, where n denotes the number of samples in the set.\n\nIn Fig. 1, we present the results of our investigation, where high values in off-diagonal elements typically in-dicate strong similarities between corresponding blocks. Across various architectures, the similarity matrices gen-erally exhibit higher degrees of similarity among blocks in the shallow blocks, suggesting a high level of redundancy in these early layers. This immediately implies that the neu-ral network is lazy at the shallow blocks and fails to learn rich information. This observation motivates our efforts to address and reduce such redundancy in the proposed work."}, {"title": "4. Method", "content": ""}, {"title": "4.1. Super Token Attention Module", "content": "Based on the analysis presented in Section 3, there is clear evidence of redundancy in the shallow layers of transformer-based architectures, which results in inefficient information retention. Super tokens [15] can mitigate this flaw by learning efficient global representation. Super to-kens are considered as a concise representation of visual information by adapting the concepts of superpixels [17] from pixel domain to token domain. This method combines sparse association learning, self-attention, and token space mapping to improve the efficiency of visual token process-ing. We re-introduce the Super Token Attention in form of"}, {"title": "4.2. STA-UNet architecture", "content": "We dedicate this section to providing a brief overview of the proposed Unet architecture (same is illustrated in Fig 3). Similar to any other UNt architecture, the proposed model comprises Encoder, Decoder, Bottleneck, and Skip connec-tions. The key performance enhancer in this architecture is the Super Token Attention (STA) modules integrated at each stage of the encoder and decoder. On the contrary, with the Transformer Blocks leveraged in the latest UNet architectures [3, 5, 38], we implement dimensional changes in the convolution layers, filtering essential information be-fore applying the attention mechanism. The input image is downsampled to the half dimensions (H/2 \u00d7 W/2) and channel (C)-dimension is doubled at each stage. The po-sitional embeddings are extracted in the STA Module (in"}, {"title": "5. Experiments and results", "content": ""}, {"title": "5.1. Datasets", "content": "We validated the effectiveness of the proposed method on four publicly available datasets: Synapse Multi-Organ Segmentation, Automated cardiac diagnosis challenge (ACDC) dataset [2], Nuclear segmentation (MoNuSeg) [20, 21], and Gland segmentation in Colon Histology Im-ages (GlaS) [31]. Following [3], [5], [11], we trained the proposed method using the Synapse Multi-Organ Segmen-tation dataset. The dataset includes 30 cases, encompassing a total of 3,779 axial abdominal CT images. The segmen-tation masks are provided for 13 abdominal organs, out of which we used 9 classes for training the proposed model. For model development, 18 cases are allocated for training, while 12 cases are designated for testing. Performance is assessed based on the segmentation of eight abdominal or-gans, with the Average Dice Similarity Coefficient (DSC) used as the primary evaluation metric. The ACDC dataset consists of 100 cardiac MRI scans from a diverse patient cohort, with annotations for the left ventricle (LV), right ventricle (RV), and myocardium (Myo). In line with prior work [5, 29], we partition the dataset into 70 cases (1,930"}, {"title": "5.2. Implementation details", "content": "We followed the straightforward training regime for easy reproducibility mehtods [3, 5, 36, 40]. The Synapse CT dataset consists of 3D CT scans with each slice mapped in the Grayscale domain. To train on this dataset, we extracted each slice and center-cropped it to retain the 224 x 224 im-age for the input. We trained our model for 300 epochs using a Stochastic Gradient Descent (SGD) optimizer for smoother convergence. The batch size was set to 8. The ini-tial learning rate lrinitial was set to be 1 \u00d7 10-2. The learn-ing rate for each iteration lrt of the epoch is determined by the Eq. 9. Where t denotes the current iteration, N denotes the maximum number of iterations in one epoch.\n\n$l_{rt} = l_{rinitial} X(1.0-\\frac{t}{N})^{0.9}$ (9)\n\nWe trained the model to converge on the summation of Cross-Entropy and Dice loss, maintaining the weights of 0.4 and 0.6, respectively. To tackle the limited dataset problem, we incorporated the following data augmentation traits: Random flips (Horizontal) and rotations with a probability of 0.5. We followed the same experimental setup for the ACDC dataset. For GlaS and MoNuSeg datasets, the batch size was 18. We used the initial learning rate of 1 \u00d7 10-3 and updated the learning rate using Cosine Scheduler. We utilized the computational power of Nvidia RTX3090 with 24G memory to conduct our experiments. We compare our methods with recent SOTA models, including UNet [30], R50 U-Net [7], Att-UNet [28], TransUNet [5], Swi-nUNet [3], LeViT-UNet [38], HiFormer [11], and Seg-SwinUNet [40]."}, {"title": "5.3. Ablation Study", "content": "Understanding the impact of individual parameters on model performance is crucial for determining the optimal architecture. To gain insights into the effects of varying To-ken size and Attention heads in our proposed model, we conducted an ablation study on the Glas dataset."}, {"title": "5.4. Results", "content": "The performance analysis is reported in Table 4 for the Synapse dataset, Table 5 for the ACDC dataset, and Ta-ble 6 for Glas and MoNuSeg datasets. Our main conclusion is that our proposed architecture is effective and computa-tionally rational and achieved significant improvement over quantitative metrics. We reported the gain/loss in percent with respect to the UNet [30] and the first-ever proposed transformer-based UNet architecture, TransUNet [5]. We observe substantial improvements of 4.99%, 2.86%, 6.53%, and 6.03% across the four datasets when compared to UNet. Similarly, compared to TransUNet, our approach demon-strates gains of 4.14%, 2.83%, 2.97%, and 3.22%, respec-tively. When compared with recently established works like HiFormer [11] and Seg-Swinunet [40], we achieved 0.49% and 0.18% DSC improvement, respectively. This huge gain in DSC resulted from segmenting the difficult organs such as the Kidneys (L&R) and Pancreas more accurately. The same is illustrated from Fig 6. We throw the light on Pan-creas and Stomach segmentation highlighted in the Yellow (First row in Fig. 6). Notably, SwinUNet could not seg-ment either of them and other models [5, 38, 40] have not"}, {"title": "6. Conclusion", "content": "In this study, we re-introduce Super Token Atten-tion (STA) in UNet architecture as an STA Module to tackle the feature redundancy inherently present in exist-ing transformer-based architectures while enhancing perfor-mance over organ segmentation tasks. We reported the pre-liminary analysis to mathematically make the redundancy present in Transformer-based architectures evident and en-courage the research to mitigate the same. Our findings demonstrated a notable improvement over existing bench-marks across four publicly available datasets, evidencing the potential of STA-UNet in Medical Image Segmentation. Our extensive ablation study explains the impact on perfor-"}]}