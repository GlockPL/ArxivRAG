{"title": "ConceptDrift: Uncovering Biases through the Lens of Foundational Models", "authors": ["Cristian Daniel P\u0103duraru", "Antonio B\u0103rb\u0103lau", "Radu Filipescu", "Andrei Liviu Nicolicioiu", "Elena Burceanu"], "abstract": "Datasets and pre-trained models come with intrinsic biases. Most methods rely on spotting them by analysing misclassified samples, in a semi-automated human-computer validation. In contrast, we propose ConceptDrift, a method which analyzes the weights of a linear probe, learned on top a foundational model. We capitalize on the weight update trajectory, which starts from the embedding of the textual representation of the class, and proceeds to drift towards embeddings that disclose hidden biases. Different from prior work, with this approach we can pin-point unwanted correlations from a dataset, providing more than just possible explanations for the wrong predictions. We empirically prove the efficacy of our method, by significantly improving zero-shot performance with biased-augmented prompting. Our method is not bounded to a single modality, and we experiment in this work with both image (Waterbirds, CelebA, Nico++) and text datasets (CivilComments).", "sections": [{"title": "Introduction", "content": "Deep neural networks, and especially fine-tuned versions of foundational models, are commonly deployed in critical areas such as healthcare, finance, and criminal justice, where biased predictions can have significant societal consequences [1]. Despite their impact, these models are often employed in their natural black-box state, i.e. as highly non-linear, multi-layered decision processes, lacking transparency or interpretability. Even if the pretrained model has been validated by the community, the dataset leveraged in the fine-tuning process can, and usually does, imprint the model with new biases. This issue is particularly concerning as biases from these datasets can lead to undesired outcomes [6], reinforcing existing inequalities or creating new forms of discrimination. This scenario finds its representation in subpopulation shift setups, where biases can naturally occur in samples.\nWithin the context of subpopulation shift setups, efforts employing foundational models [13, 37] have been recently made towards identifying and preventing biases. However, these methods limit themselves to data analysis alone. For instance, Kim et al. [13] focus on investigating misclassified validation samples. Their method relies on validating the presence of a given object within the set of mistakes and its absence from within the set of the correctly classified samples, in order to label it as a bias. The actual internal decision-making process of the model is never investigated nor referred to.\nAs an example, a method focusing on analyzing misclassified samples, such as B2T [13], is restricted to highlighting only the biases present in the validation set. Furthermore, some of the biases found in the dataset might not have been imprinted upon the model weights. As an example, fine-tuning a ViT-L-14 CLIP [25] model on the O2O-Hard setup from Spawrious [19], a dataset specifically designed to instill biases at train-time and expose them at test-time, results in a 96% test-time accuracy. This demonstrates that biases in the data need not necessarily translate to biases in the model, and that model investigation is imperative in confirming whether or not a bias seen in the data is a contributing factor in the decision making process of the model.\nWe endeavor to expand upon the current usage of foundational models, beyond the restricted scope of simple data analysis, and propose a new direction for bias identification within the context of subpopulation shift setups. Our method focuses on investigating the skewness of the model's weights towards detecting and prioritizing spurious features as part of the decision-making process of the investigated model. We hereby propose a novel protocol for uncovering biases using foundational models such as CLIP [25] and mGTE [36], leveraging the topology of their embedding space to identify and name biases instilled by linear probing. Our protocol, dubbed ConceptDrift, is illustrated in Fig. 1. We showcase how, during training, the weights of the final classification layer drift away from the textual representation of their associated class, towards representations of spurious attributes. We propose a ranking system based on embedding-space arithmetic to extract keywords from concepts which factor in the activation of class neurons, and leverage a dictionary-based approach to delineate concepts outside the semantic scope of the classes, as biases.\nWe summarize our main contributions as follows:\n1.  We introduce ConceptDrift, a method capable to pin-point concepts relevant for the decision-making process of a model. We are the first to propose a weight-space approach for identifying the biases of fine-tuned foundational models, diverging from the current data-restricted protocols.\n2.  We propose a novel, embedding-space scoring method, able to reveal concepts which discriminatively impact the class prediction.\n3.  We show how our procedure is suited to assist in bias investigation. We reveal previously untapped biases on four datasets: Waterbirds [31], CelebA [17], Nico++[35] and CivilComments [5], showcasing significant improvements in terms of zero-shot bias prevention, upon state-of-the-art bias identification methods. Validated over image and text data, it can work on other modalities, with a foundational model with text processing capabilities as well."}, {"title": "Our Method", "content": "For a standard classification task {(xj, yj)} \u2282 X \u00d7 Y, we propose a method for pin-pointing concepts that are erroneously correlated to the task's classes. In order to achieve this, we train a linear layer on top of a frozen, pre-trained representations of the input data, obtained from a foundational model M. Next, we find concepts (Ci)1<i<q in textual form, that are present in the training data and strongly influence the predictions of the classifier.\nWe require that the model M is capable of embedding both the concepts c\u2081 and the input samples xj into the RD vector space, such that their cosine similarity cos(M(ci), M(xj)) is high when the concept represented by ci is present in sample xj.\nThe main steps of our method are the following:\nStep 1: Initialization We initialize the weights wk, 1 \u2264 k \u2264 |Y| = N, of the linear layer with the embedding of the corresponding class name, extracted by the model M, for each class k.\nStep 2: Drifting towards biases, through learning We perform ERM [29] training on our dataset of interest, while keeping the weights we on the unit sphere. Through learning, the weights in the linear layer naturally shift from the original initialization, towards concepts that can effectively distinguish the samples of different classes. In an ideal, unbiased dataset w.r.t. the foundational model, the learned weights would be the embeddings of the class names. But in all the other cases, concepts used for classification drift, like visually presented in Fig. 1.\nStep 3: Dataset concepts extraction For image classification task, we first use a captioning model to obtain descriptions of the images in the dataset. Next, for both image and text classification, we extract concepts from the captions or directly from the text samples.\nStep 4: Rank the concepts For each class, we want to keep only the candidate concepts, which favour the prediction of that class with respect to another subset of classes. Since the weights wk of each class are normalized, the prediction rule of the classifier can be formulated as:\n\u0177j = arg max cos(wk, M(xj)).\nkey\nThis further motivates the need for wk to point closer to samples in class k, than the weight of the other classes. Consider now a concept ci, that has a high cosine with the weight wk and a training example xj containing the concept ci. Based on the following inequality (proof in appendix A):\ncos(M(xj), wk) \u2265 cos(M(xj), M(ci)) \u2013 \u221a2(1 \u2013 cos(wk, M(ci))),\nit follows that: as long as our assumption from the beginning of this section holds, and we is highly similar with M (ci), then wk is also guaranteed to have a high similarity with samples containing the concept ci. Since we seek the concepts which favour the prediction of class k as opposed to at least one other class, we rank them by the difference in similarity of M(ci) with wk, and the weight of any other class, wp:\nscorek(ci) = cos(wk, M(ci)) \u2013 min cos(wp, M(ci))\n1<p<N;p\u2260k\nStep 5: Filtering concepts Among the concepts with high rank, based on the score in Eq.3 we also expect to find those that refer to the class itself, or specific instances of it. We thus apply a filtering procedure to remove instances of the class from the keywords, leaving only associated attributes or keywords of completely different concepts."}, {"title": "Experimental analysis", "content": "Foundational models (FM) We used mGTE (gte-large-en-v1.5 [36]) for text embeddings in Civil-Comments [5], and OpenAI CLIP ViT-L/14 [25] for text and images in the other datasets.\nWe train the linear layer on L2 normalized embeddings extracted by these models using the Py-Torch [22] AdamW optimizer with a learning rate of 1e - 4, a weight decay of 1e - 5, a batch size of 1024 and a cosine annealing learning rate scheduler. We use the cross entropy loss with balanced class weights as the objective. The weights of the layer are normalized after each update and we also learn a temperature to scale the logits. As early stopping criterion, we use the class-balanced accuracy on the validation set.\nKeyword extraction For image captioning we use the GIT-Large model [32], trained on MSCOCO [15]. Next, to extract concepts we use YAKE [7], taking the top 256 n-gram concepts, for both n = 3, 5. For post-processing the selected concepts, we split them into individual words to remove stopwords, substrings from the class names, and hypernims or hyponims of the class concepts using WordNet [20] (e.g. 'seagull' for 'landbird' class). We remove keywords common for all classes, as they are usually in top because they are part of n-grams containing the class names."}, {"title": "Datasets", "content": "Waterbirds [31] is a common datasets for generalization and bias mitigation. It is created from CUB [33], by grouping different species of birds into two categories, landbirds and waterbirds, each being associated with a spurious correlation regarding its background, land and water respectively.\nCelebA [17] is a large-scale collection of celebrity images (over 200000), widely used in computer vision research. The setup for using it in a generalization context [18] consists of using the Blond_Hair attribute as the class label and the Male attribute as the spurious variable.\nNico++ [35] image dataset has annotations for a main object and its context (e.g. dog on the beach). Unlike other datasets, NICO++ includes over 50 classes and 6 contexts, providing a richer context for evaluating model generalization performance across diverse scenarios. For this work, we build a setup with spurious correlations between 4 classes and 3 contexts (more details in Appx. A.2).\nCivilComments [5] is a large collection (1.8 millions) online user comments, used also for research-ing bias and fairness in NLP, across different social and identity groups."}, {"title": "Quantitative analysis through zero-shot prompting", "content": "In this experiment, we validate the ability of our method to identify biases. We follow B2T [13] setup and choose the zero-shot prediction task. We augment the initial, class-only related prompt, with the bias, through a minimal intervention (e.g. 'a photo of a {cls} in the {bias}' (see Appx. A.1). For each class, we test one prompt for each bias identified in the dataset, taking into account the score for the best one (zero-shot with max over templates). The results in Tab. 1 show how the biases, automatically selected by our method, improve the worst group accuracy, over the initial zero-shot baseline and other bias-extracting solutions, in all four tested datasets. This emphasises on the quality of the biases automatically extracted by ConceptShift. The better they are, the more capable the zero-shot prompt approach is to generalize, by adapting the prompt better to the new dataset context.\nAblations We validate key decisions in our algorithm in Tab. 2. We changed the ranking score (score) in Eq. 3 to the difference in cosine similarity of a concept with the final weights and the initial ones for each class. This highlights the concepts that the weights of a class have become more similar to, but does not take their similarity to other class weights into account. We also notice that the number of chosen concepts (top-q concepts) is important, as taking too many adds noise to the prompts and lowers the performance. We leave finding a good cut-off strategy for future work."}, {"title": "Extracting qualitative biases", "content": "In Fig. 2, we analyse the scores for the n-gram concepts on Waterbirds, for both classes, extracted as explained in Sec. 2). Notice how the score variation for each class is steep at the margins, becoming almost flat as soon as similarity decreases, showing that there are only a few candidates with high similarity scores, worth to be taken into account next for extracting the biases."}, {"title": "Qualitative examples", "content": "We present in Tab. 3 the identified biases. Notice how our method comes with lots of new proposals for biases (in green). This might be case because our approach is fundamentally different, when compared with others [13, 37, 4], relying on the decision-making process of the model being investigated. See Nico++ and CivilComments in Appx. A.2."}, {"title": "Related Work", "content": "To enable a more meaningful comparison, we have distilled in Tab. 4 existing methods down to the aspects we consider fundamental to bias detection.\nBiases and generalization Machine learning methods easily capture relevant factors to solve a task. Nevertheless, many times, models capture shortcuts [10], that are helpful in solving a task, but are not fundamental or essential for it. These shortcuts represent spurious correlations or biases, that don't always hold, and should not be used for reliable generalization outside of training distribution, often leading to degraded performance [24, 3, 11]. Prior works [13, 37, 4] have thus focused on identifying dataset biases, through data analysis procedures.\nDebiasing Debiasing and bias extraction techniques have become crucial in ensuring the fairness and accuracy of machine learning models [28], with extensive research dedicated to removing harmful biases across various domains. Some existing methods use bias annotations to train unbiased model, by means of group balanced subsampling [12], reweighting [27] or data augmentations [34]. In the absence of these annotations, other works [21, 16, 23] have proposed to first learn a biased model and then focus on its mistakes to train an unbiased one.\nFairness Fairness in machine learning has been extensively studied, with numerous approaches [8, 30] proposed to facilitate ethical research and ensure equitable outcomes across different subpopulations. Most of those methods overlap with domain generalization and worst-group performance improve-ments. This is also a field where model interpretability plays a crucial role [26], as understanding how decisions are made can help in identifying and mitigating biases.\nInvariant Learning Robustness to out-of-distribution changes can be obtained by enforcing that the learning model is invariant to different environments or domains [2, 14, 34]. But there are many cases where we don't have access to such environments and we must discover them. Approaches like [9] partition the data into subsets that maximally contradict an invariant constraint, and apply algorithms for distributional robustness, like groupDRO [27], on those subsets, called environments."}, {"title": "Conclusions", "content": "We introduce ConceptDrift, the first method to identify biases using a weight-space approach, moving beyond traditional data-restricted protocols. Our novel embedding-space scoring method highlights concepts that significantly influence class predictions. We empirically demonstrate its effectiveness in bias investigation across four datasets: Waterbirds, CelebA, Nico++, and CivilComments, revealing previously undetected biases and achieving notable improvements in zero-shot bias prevention over current state-of-the-art methods. Validated on image and text datasets, with a foundational model also endowed with text processing capabilities, ConceptDrift can accommodate any other modality."}, {"title": "Appendix", "content": "Finding biases in models We discuss so far how our approach can be used to find biases in datasets, but it can also be used for finding biases of a model w.r.t. the foundational model. We apply the same procedure, such that the new ground-truth labels of the dataset entries are the predictions of our model of interest.\nBroader impacts We emphasize that our method should not be used in a stand-alone fashion for automated discovery of biases in every field and that human assistance is needed in order to interpret the model output before any further actions of consequence. Our tool is meant to aid and assist humans in the process of bias identification, not to replace them.\nLimitations An important limitation in our method is the captioning model used for image classifications task. Zhao et al. [37] acknowledged as well that these models usually do not extract all the details in the images, so methods relying on them are limited to discovering the biases that they can extract. Another limitation is the keyword extracting procedure - using a more sophisticated one could bring forth new biases (e.g. extracting topics or taking into account synonymy). The method also relies on known hierarchies of concepts to detect biases by filtering concepts related to the desired class. These hierarchies and the relations they provide thus limit the type of filtering that we can ensure.\nBound on cosine similarity of vectors Let u, v, t \u2208 RD be three vectors of unit length, with u and v being fixed. We are interesting in finding the vector t that maximizes the difference in cosine similarity with the two fixed vectors:\narg max (t\u00b7u - t\u00b7v),\n||t||2=1\nwhere represents the standard dot product of vectors. This can be rewritten as:\narg max t. (u - v) = arg max cos(t, u \u2013 v) ||u \u2013 v||2\n||t||2=1   ||t||2=1\n= arg max cos(t, u \u2013 v),\n||t||2=1\nas ||u \u2013 v||2 is a constant. It is now easy to see that the solution to this problem is t = ||uv||2 (u \u2013 v), the unit length vector with the same orientation as u v. Using this we can place an upper bound on the initial difference:\nt \u00b7 u - t \u00b7 v \u2264 ||u - v ||2,\nwhich we then rearrange as\nt\u00b7v \u2265 t\u00b7u - ||u - v||2 .\nThe norm ||u \u2013 v||\u2082 can be equivalently expressed as\n||u \u2013 v||2 = \u221a(u \u2013 v) \u00b7 (u \u2013 v) = \u221a2 \u2212 2u \u2022 v = \u221a2(1 \u2212 u \u00b7 v) .\nIntroducing this in the previous inequality we obtain\nt\u00b7v \u2265 t\u00b7u -\u221a2(1 \u2013 u \u00b7 v).\nSince u, v and t are vectors of unit length we can replace the dot products with the cosine similarity. By then setting u = M(ci), v = wk and t = M(xj) we finally obtain the inequality:\ncos(M(xj), wk) \u2265 cos(M(xj), M(ci)) - \u221a2(1 \u2013 cos(M(ci), wk))"}, {"title": "Zero-Shot Prompts", "content": "The basic prompts we used for each dataset are the following:\n\u2022 Waterbirds: 'a photo of a {class name}'\n\u2022 CelebA: 'a photo of a person with {class name}',\n\u2022 CivilComments: '{class name}'\n\u2022 Nico++: 'a photo of a {class name}'.\nNext, we change them to accomodate the biases wildcard:\n\u2022 Waterbirds: 'a photo of a {class name} in the {bias}'"}, {"title": "Nico++ and Civil Comments Biases", "content": "See Tab. 5 and Tab. 6 for the biases extracted with our method for Nico++ and CivilComments datasets.\nCustom Nico++ subset For the experiments on Nico++ we selected only the first four classes and paired them with the environments that they had the most samples in, resulting in the following associations: (car, outdoor), (flower, grass), (chair, water), (truck, water). Notice how the classes chair and truck shared the same bias, in contrast to most popular subpopulation shift datasets that only have one-to-one associations of classes and biases. For the training set we keep for each class 300 samples from its associated environment and only 25 from the other ones, while for validation we keep 50 from the associated one and 25 from the others. The test set is made up of all the remaining samples."}]}