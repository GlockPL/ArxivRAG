{"title": "Dynamic Reinforcement Learning for Actors", "authors": ["Katsunari Shibata"], "abstract": "Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration.\n\nDynamic RL controls global system dynamics using a local index called \u201csensitivity,\u201d which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron's processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error.\n\nDynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain.\n\nDrawing parallels between 'exploration' and 'thinking,' the author hypothesizes that \u201cexploration grows into thinking through learning\" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has evolved into a core technology for autonomous, non-supervised learning in modern artificial intelligence (AI) through several significant qualitative advancements. As the next major step in its evolution, the author introduces 'Dynamic RL.' Unlike conventional methods, Dynamic RL treats exploration as an inherent aspect of actions rather than an independent process. Instead of using external noise for stochastic selection, exploration is generated as chaotic system dynamics through the action-generating (actor) network, and RL controls these dynamics directly."}, {"title": "1.1 Evolution of RL toward Learning of All Kinds of Human Functions", "content": "RL initially involved only learning appropriate tables or mappings from a discrete state space to a discrete action space within a Markov decision process (MDP) [Sutton and Barto 2018]. A neural network (NN) was then introduced, providing many degrees of freedom and parameters. This enabled an agent to learn not only continuous nonlinear mappings but also the entire sensor-to-motor process based on a value function using reinforcement signals with the help of the gradient-based method. Consequently, RL has advanced from learning of actions to learning of various functions, including recognition and prediction [Shibata and Okabe 1997, Shibata 2011, 2017b]. A notable achievement occurred when an RL agent learned the game strategy of \u201ctunneling\u201d in the game \u201cBreakout\" solely by using changes in the game score as a reinforcement signal [Mnih et al. 2015]. RL has since become a driving force in achieving human-level, and sometimes superhuman, performance particularly in gaming [OpenAI 2019, Vinyals et al. 2019, Schrittwieser et al. 2020].\n\nFurthermore, the introduction of a recurrent neural network (RNN) has opened the way for RL to process and learn along the time axis. This allowed agents to extract necessary information from large amounts of past information and then retain and utilize it. This advancement allowed learning in partially observable situations, such as partially observable MDPs (POMDPs), and solving memory-dependent tasks. As a result, the functions acquired through RL span a wide range, including recognition, memory, attention, prediction, and communication [Shibata and Ito 1999, Shibata 2011, 2017b,a].\n\nDoes this suggest that RL agents can acquire all functions that humans have, even simple ones, through learning? The current answer is 'No'. Unfortunately, the author has yet to accept any of these as functions that could be called 'thinking.' Therefore, he has set the emergence of such a function through learning as his major future target and has considered what would be required for the emergence for more than a decade."}, {"title": "1.2 'Thinking' and Recent Advancements in Generative AI", "content": "Humans are intelligent, and it would not be an exaggeration to say that 'thinking' is the most typical form of intelligence. 'Thinking' has been a central topic in philosophy for a long time since the time of Plato [Woolf 2013], with numerous ideas explored [Berlyne and Vinacke 2025]. About 80 years ago, attempts began to explain human intelligence from a computational perspective by considering the mind or neural networks, which are thought to be the source of intelligence in living organisms, as computational machines [McCulloch and Pitts 1943, Turing 1950, Newell et al. 1959].\n\nRecently, generative AI, especially large-scale language models (LLMs) such as GPT [OpenAI 2023] and Gemini [Pichai et al. 2024], appear to understand what we say, think about it, and come up with an appropriate response. We, at least the author, feel(s) as if we are interacting with a human who has intelligence. It was reported that ChatGPT powered by GPT-3.5 performed near or over the passing threshold on the United States Medical Licensing Exam"}, {"title": "1.3 Exploration and Thinking", "content": "While the exact definition of 'thinking' is difficult to pin down, let us explore the more dynamic form of thinking that we typically associate with the term. Humans can think even when remaining still with eyes closed and ears covered, which seems different from other functions like recognition or prediction. In order to survive as living organisms, we must at least avoid converging and becoming stagnant. Therefore, many would likely agree that autonomous, rational state transitions, even without external triggers, are required for 'thinking.' However, acquiring multistep state transitions through learning from scratch in RNNs is very difficult [Sawatsubashi et al. 2012]. In particular, forming autonomous state transitions without external triggers is challenging. Moreover, considering human abilities such as inspiration and discovery, these autonomous state transitions should not only be rational but also sometimes unexpected.\n\nComparing modern RL with human learning reveals one major difference: 'exploration.' In conventional RL, stochastic selection uses external random noise, independent of the motion or action generation process, as shown in Fig. 1(a).\n\nIn this paper, the term 'motion' is used as continuous, and 'action' is used as discrete, while the term 'actor' is used as a generator in both cases. Action is usually abstract such that the action \"turn left\" itself is not a motor command, but is broken down into a series of motor commands for several motors. Therefore, in the proposed RL, the actor outputs represent continuous motor commands based on the end-to-end learning concept [Shibata 2017b]."}, {"title": "1.4 Learning of Local Dynamics", "content": "In contrast, humans often act while wondering about this and that. Exploration seems to work during the action or motion generation process. When we wonder which path to take at a fork in the road, we do not move our hands erratically for exploration. Exploration is not always uniform; it should depend on the direction in the state space and also the current state. Furthermore, it should be improved through learning [Shibata 2006, Goto and Shibata 2010]. Therefore, the author considers that exploration represents the degree of irregularity in motion generation and is updated together by the learning of motion generation, as shown in Fig. 1(b). Exploration requires non-convergent, irregular, and unexpected state transitions. It can be embedded in motor commands by making the system dynamics chaotic. In other words, by making the dynamics sensitive to minute differences, agent behaviors vary even from similar states, leading to the generation of deterministic exploratory state transitions. Dynamic RL proposed here employs this type of exploration.\n\nFrom the above, 'exploration' is similar to \u2018thinking' in terms of autonomous state transitions, including unexpectedness. Both require chaotic system dynamics with a positive Lyapunov exponent rather than convergent dynamics. The aforementioned points suggest that 'exploration' and 'thinking' cannot be clearly separated and may continuously exist along a line within the range of chaotic dynamics. In 'exploration,' state transitions need to be more irregular with stronger chaotic dynamics. In contrast, those in \u2018thinking' need to be less irregular but still non-convergent, with weaker chaotic dynamics; furthermore, they must be more rational. This means that as shown in Fig. 2, they exist on a diagonal line in the two-dimensional space with irregularity and rationality as axes. If the state transitions are very irregular, they cannot be rational. In other words, they cannot exist in the upper right portion of Fig. 2. Therefore, \"being less irregular\" is a necessary condition for \"being more rational.\"\n\nIf the system dynamics are highly chaotic, the agent can explore widely. The agent cannot 'think' before acquiring basic knowledge, such as various cause-and-effect relationships in the world. Converging the flows around better state transitions makes the transitions more rational, thereby, the agent acquires the necessary basic knowledge. That is expected to Dynamic RL. If autonomous state transitions have already been formed as explorations, just adjusting the convergence or divergence of transitions is enough to achieve autonomous and rational transitions. Thus, this should be much easier than creating such transitions from scratch by RL using an RNN with convergent dynamics. Then, the author posits the following bold hypothesis: \u201cExploration grows into thinking through learning\" by shifting state transitions from \u201cmore irregular\" to \"more rational\" while maintaining system dynamics chaotic. Furthermore, when situations change, and so expected rewards cannot be obtained, Dynamic RL is expected to resume more exploratory behaviors autonomously in the agent."}, {"title": "1.5 Introduction of Dynamic Reinforcement Learning (Dynamic RL)", "content": "Comprehensively considering the above with the hypothesis in mind, the author introduces a novel RL framework again that significantly redefines the concepts of exploration and learning in an RNN, as follows:\n\n1.  An RL agent does not perform stochastic motion selection using external random noise. The chaotic dynamics produced by the agent's RNN and the environment together embed exploration factors in its motion outputs.\n2.  The dynamics are trained to enhance reproducibility when the TD error is positive (i.e., the value is better than expected) and to enhance exploratory behavior when the TD error is negative (i.e., the value is worse than expected) while maintaining chaotic dynamics.\n3.  To achieve the objective (item 2), sensitivity [Shibata et al. 2021] is used. It is a local index that represents the degree of convergence or divergence of the input neighborhood to the corresponding output neighborhood through the processing of each neuron.\n\nThe author's group already proposed the above hypothesis and item 1 of the concept in [Shibata and Sakashita 2015]. It was also demonstrated that chaotic dynamics operate as exploration without stochastic selection in actor-critic [Shibata and Sakashita 2015], reward-modulated Hebbian learning [Matsuki and Shibata 2020], and TD3 [Matsuki et al. 2024] and that the \"edge of chaos\" was a favorable choice for network dynamics, leading to high learning performance in [Matsuki and Shibata 2020].\n\nIn Dynamic RL, an agent can explore using many DOFs because the motions with exploration are generated through its RNN and can be learned. The flexibility in exploration is expected to carry over directly into thinking through learning while maintaining chaos in the global dynamics. Furthermore, if the RNN architecture is hierarchical, learning is expected to control irregularity even in the abstract state space. In this way, the author expects that a Dynamic RL agent, learning through high-DOF exploration, will eventually be able to think about a variety of things, including abstract ones, and sometimes generate completely new ideas on its own by flexibly utilizing both the irregular and the rational.\n\nCurrently, to improve learning efficiency or stability, many excellent RL methods, such as PPO [Schulman et al. 2017], SAC [Haarnoja et al. 2018], A3C [Mnih et al. 2016], TD3 [Fujimoto et al. 2018], and experience replay with PER [Schaul et al. 2016] or HER [Andrychowicz et al. 2017] have been used. However, since Dynamic RL presents a newborn learning concept, comparison targets are limited to simple conventional RL for a fair evaluation of basic performance.\n\nThis paper first introduces Dynamic RL and then examines whether this entirely new type of RL functions effectively as reinforcement learning in a simple memory-required task and a dynamic pattern generation task, compared with conventional RL."}, {"title": "2 Learning Method", "content": "As a base architecture for RL, the actor-critic model is used, in which the actor outputs do not represent probabilities for actions but instead represent continuous motor commands. Dynamic RL is applied solely to the actor, while the critic is trained by conventional RL using BPTT [Rumelhart and McClelland 1986] although ideally, all learning should be dynamic. For clarity, each actor and critic consists of a separate RNN with sensor signals as inputs.\n\nIn each dynamic neuron, its internal state $u$ at time $t$ is derived as the first-order lag of the inner product of the connection weight vector $w = (w_1, ..., w_m)$ and input vector $x_t = (x_{1t}, ..., x_{mt})^T$ where $m$ is the number of inputs as\n\n$u_t = \\frac{1}{\\tau} u_{t-1} + \\frac{\\Delta t}{\\tau} w^T x_t$ (1)\n\nwhere $\\tau$ is a time constant and $\\Delta t$ is the step width, which is 1.0 in this paper. For static-type neurons, the internal state $u$ is just the inner product as\n\n$u_t = w^T x_t$. (2)\n\nThe inputs $x_t$ can be the external inputs or the pre-synaptic neuron outputs at time $t$, but for the feedback connections, where the inputs come from the same or an upper layer, they are the outputs of the pre-synaptic neuron at time $t-1$. The output $o_t$ is derived from the internal state $u_t$ as\n\n$o_t = f(U_t) = f(u_t + \\theta)$ (3)\n\nwhere $U_t = u_t + \\theta$, $\\theta$ is the bias, and $f(\\cdot)$ is an activation function, which is a hyperbolic tangent in this paper.\n\nDynamic RL controls the dynamics of the system, including RNN, directly by adjusting the sensitivity [Shibata et al. 2021] in each neuron. Sensitivity is an index for each neuron that is the Euclidian norm of the output gradient with respect to the input vector x. It is defined as\n\n$s(U; w) = ||\\nabla_x o|| = f'(U)||w||$. (4)\n\nHere, $||v|| = \\sqrt{\\sum_{i=1}^m v_i^2}$ for a vector $v = (v_1, ..., v_m)^T$. In the form of a vector elements, the sensitivity is represented as\n\n$s(U; w) = \\sqrt{\\sum_{i=1}^m (\\frac{\\partial o}{\\partial x_i})^2} = f'(U) \\sqrt{\\sum_{i=1}^m w_i^2}$. (5)\n\nSensitivity refers to the maximum ratio of the absolute value of the output deviation $do$ to the magnitude of the infinitesimal variation $dx$ in the input vector space. It represents the degree of contraction or expansion from the neighborhood around the current inputs to the corresponding neighborhood around the current output through the neuron's processing. In the previous work [Shibata et al. 2021], it was defined only for static-type neurons (Eq. (2)). In this study, the same definition is also applied to dynamic neurons (Eq. (1)), assuming that the infinitesimal variation $dx$ of the input x changes slowly enough compared to the time constant $\\tau$.\n\nIn the previous research [Shibata et al. 2021], the author's group proposed sensitivity adjustment learning (SAL). SAL was applied to ensure the sensitivity of each neuron in parallel with gradient-based supervised learning. This approach is beneficial not only for maintaining sensitivity during forward computation in the neural network but also for avoiding diverging or vanishing gradients during backward computation. Because Dynamic RL incorporates SAL and sensitivity-controlled RL (SRL), which is an extension of SAL for RL, SAL will be explained first.\n\nIn SAL, the moving average of sensitivity $\\bar{s}$ is computed first as\n\n$\\bar{s}_t \\leftarrow (1-\\alpha) \\bar{s}_{t-1} + \\alpha s_t$ (6)\n\nwhere $\\alpha$ is a small constant, and this computation is performed across episodes. When the average sensitivity $\\bar{s}$ is below a predetermined constant $s_{th}$, the weights and bias in each neuron are updated locally to the gradient direction of the sensitivity as\n\n$\\Delta w_t = \\eta_{SAL} \\frac{\\Delta t}{\\tau} \\nabla_w s(U_t; w) = \\eta_{SRL} \\frac{\\Delta t}{\\tau} \\frac{w}{||w||} f'(U_t)$ (7)\n\n$\\Delta \\theta_t = \\eta_{SAL} \\frac{\\Delta t}{\\tau} \\frac{\\partial s(U_t; w)}{\\partial \\theta} = \\eta_{SRL} \\frac{\\Delta t}{\\tau} ||w|| f'(U_t)$. (8)"}, {"title": "2", "content": "$\\Delta w_t = \\eta_{SAL} \\frac{\\Delta t}{\\tau} (1-o_t^2) (\\frac{w}{||w||})^3 (-2u_t ||w|| x_t)$ (9)\n\n$\\Delta \\theta_t = -2 \\eta_{SAL} \\frac{\\Delta t}{\\tau} o_t (1-o_t^2) ||w||$ (10)\n\nare derived.\n\nIn Dynamic RL proposed here, as shown in Fig.4, SAL is applied when the moving average of the sensitivity $\\bar{s}$ is less than a constant $s_{th}$, otherwise sensitivity-controlled RL (SRL) is applied in each neuron. SAL always tries to increase the sensitivity in each neuron, but whether SRL tries to increase or decrease the sensitivity depends on the temporal difference (TD) error $\\hat{r}$ as\n\n$\\Delta w_t = -\\eta_{SRL} \\frac{\\Delta t}{\\tau} \\hat{r} \\nabla_w s(U_t; w)$ (11)\n\n$\\Delta \\theta_t = -\\eta_{SRL} \\frac{\\Delta t}{\\tau} \\hat{r} \\frac{\\partial s(U_t; w)}{\\partial \\theta}$ (12)\n\nwhere $\\eta_{SRL}$ is the learning rate for SRL. TD error is computed as\n\n$\\hat{r}_t = (C_{t+1} - \\gamma C_{t+1} + r_{t+1} - C_t) = \\gamma (\\frac{C_{t+1} - C_t - r_{t+1}}{\\gamma})$ (13)\n\nwhere $\\gamma$ (0.0 < $\\gamma$ < 1.0) is the discount factor, $C_t$ is the critic output (state value), and $r_t$ is the reinforcement signal, which can be a reward or a penalty, at time $t$. As the basic concept summarized in Fig.5, when TD error is positive, in other words, the new critic (state value) $C_{t+1}$ is greater than the expected value $C_t - r_{t+1}$, RL reduces the sensitivity to reinforce the reproducibility. When it is negative, i.e., the new state value is less than expected, RL makes the sensitivity greater to reinforce the exploratory nature. This is expected to control the local convergence or divergence, depending on how good or bad the state is."}, {"title": "2", "content": "$\\Delta w_t = -\\eta_{SRL} \\frac{\\Delta t}{\\tau} \\hat{r} \\frac{w}{||w||} f'(U_t)$ (14)\n\n$\\Delta \\theta_t = -\\eta_{SRL} \\frac{\\Delta t}{\\tau} \\hat{r} ||w|| f'(U_t)$. (15)"}, {"title": "2", "content": "$\\Delta w_t = \\frac{\\Delta t}{\\tau} \\eta_{SRL} \\hat{r} (1-o_t^2) (\\frac{w}{||w||})^3 ||w||$ (16)\n\n$\\Delta \\theta_t = \\frac{\\Delta t}{\\tau} 2\\eta_{SRL} \\hat{r} o_t (1 \u2013 o_t^2) ||w||$. (17)"}, {"title": "3 Simulations", "content": "To examine the learning ability of Dynamic RL, the author applied it to two dynamic tasks: a sequential navigation task, which served as a memory-required task, and a slider-crank control task, which served as a dynamic pattern generation task.\n\nThe network architecture used is shown in the left figure in Fig. 8, regardless of the learning method or task, although the parameters used are different. The architecture consists of an actor network and a critic network; as mentioned, Dynamic RL was applied only to the actor network here. The critic network was trained in the same manner as conventional RL. The critic network was an Elman-type RNN with a single hidden layer comprising 50 neurons, having feedback connections to itself. It was trained to minimize the square of the TD-error using BPTT with the training signal as in Eq. (20).\n\nThe actor network used a multiple timescale RNN (MTRNN) [Yamashita and Tani 2010] to improve performance and verify whether the proposed RL would function on a more complex architecture than simple Elman-type RNNs. The MTRNN comprised two hidden layers with different time constants and one output layer. The two hidden layers were interconnected, and one of them, called upper hidden layer (100 neurons), did not have any connection to the input or output layer; however, it had self-feedback connections. Although the other hidden layer, called lower hidden layer (200 neurons), had connections with the input and output layers, it did not have self-feedback connections. Neurons in the upper hidden layer in the actor network were dynamic, while the rest, including all neurons in the critic network, were static (time constant $r = 1$). All connected layers are fully connected in both networks.\n\nIn the proposed Dynamic RL, either SRL or SAL was applied to each hidden neuron depending on its average sensitivity $\\bar{s}$ as shown in Fig. 4; meanwhile, SAL was not applied to each output neuron to avoid the risk that SAL would cause the output to deviate from the ideal value. To see the difference from the conventional RL and also how SRL and SAL worked respectively, six learning conditions varying the method of applying learning were compared in the following as in Table 1."}, {"title": "3.1 Sequential navigation (memory-required) task", "content": "In this simulation, as shown in the right figure of Fig. 8, there is a 20 \u00d7 20 size walled field centered at the origin. An agent starts each episode at a randomly chosen location weighted on the peripheral area of the field to accelerate learning (A). The agent received a reward of $r = 0.8$ (r: reinforcement signal) when the center of the agent reached the final goal at (0.0, -3.0) after reaching the subgoal at (0.0, 4.0). At every step when the agent center is on the final goal before reaching the subgoal, it receives a small penalty of $r = -0.03$. The shape of each goal (final goal or subgoal) is a circle with a radius of 2.0, and when the center of the agent comes within the circle, the agent is considered to have reached the goal. There is a visual sensor with 11 \u00d7 11 = 121 visual cells, and only the agent appears on it. Each sensor cell covers a 1 \u00d7 1 square, which is identical to the agent's size. The agent cannot know the location of each goal from the sensor signals. The agent also receives one more sensor signal. Only when the agent's center is within the subgoal, it is 2.0; otherwise, it is 0.0. Therefore, the agent must memorize whether it has already reached the subgoal or not in the RNN and switch its behavior according to the memory."}, {"title": "3.2 Slider-Crank control (Dynamic pattern generation) task", "content": "To evaluate whether the proposed Dynamic RL works in a task where a learning agent is required to generate dynamic patterns, a slider-crank control task, as shown in Fig. 14, was employed as the next learning task. In this task, an agent learns to apply an appropriate time-varying force $f$ in the $x$-direction to the slider. This force is then transmitted to the rotor through the connecting rod, causing the rotor to rotate around its central axis $O$.\n\nThe actor network receives only four binary sensor signals: $s1$ to $s4$. Each signal is 1.0 only when the rotor angle $\\theta$ passes through a specific angle:$\\pi/4, 3\\pi/4, \u2013\\pi/4$, or $\u20133\\pi/4$ radian respectively, regardless of the rotational direction. The actor network receives the four signals and outputs only one motor command representing the force $f$ applied to the slider in the $x$-direction. Therefore, the agent cannot always perceive the rotational angle $\\theta$ or the slider's location $x$, nor can it directly perceive the rotational direction or speed. The agent has to generate a force sequence that is not defined as a static function of its four inputs. The actor output is modified by multiplying it by 1.25 and clipping it between -1.0 and 1.0 to convert it into a motor command (force $f$). This is expected to prevent the output from entering the saturation range of the neuron's activation function. The uniform disk-type rotor, with mass, makes this system dynamic while considering the negligible masses of the slider and link. The equations of motion for this system are provided in D,"}, {"title": "3.2", "content": "In each episode, the initial rotation angle $ \\theta_0$ is set randomly to an angle from $-3\\pi/4 <= \\theta_0 <= -\\pi/4$ or $ \\pi/4 <= \\theta_0 <= 3\\pi/4$ radian. The initial rotational speed is 0.0. Since the agent does not perceive the joint location directly, it cannot determine the current rotational direction or identify the necessary direction of force to rotate the rotor in the rewarded direction until it receives the first \u201con\u201d signal. Since Dynamic RL was not applied to the critic network, it was designed to receive $cos\\theta$ and $sin\\theta$ as sensor inputs, which are sufficient for identifying the rotor angle, for simplicity. When the angle passes through $\\pi$ with a positive rotational speed, the agent receives a reward of 0.15, and when with a negative rotational speed, it receives a penalty of -0.15. In each episode, the agent performed 400 steps of force loading, and the episode did not end even though the agent received a reward or penalty halfway through. In one simulation run, 10,000 episodes of learning were conducted. After 5,000 episodes, the reward sign was reversed as an environmental change. Tests were performed after 5,000 and 10,000 episodes. In each test, the rotor started from each of 50 initial angles at intervals of 0.02$\\pi$, and the average revolutions per episode were calculated. Learning is defined as a \"failure\u201d if the average is less than 20 in the first test or more than -20 in the final test; otherwise, it is defined as a \"success.\u201d As well as the previous task, 40 simulation runs were performed for each learning condition. The parameters used here were almost the same as in the previous task, but some of them are different as in Table 3."}, {"title": "4 Discussion and conclusion", "content": "In this paper, the author proposed Dynamic Reinforcement Learning (Dynamic RL), a new RL framework, expecting it to be a fundamental technique for the emergence of \u2018thinking.' This paper could not clearly show its critical advantages in this regard. However, the author demonstrated that despite the major shift in learning from static to dynamic, this unconventional RL approach effectively functions as RL in two simple dynamic tasks. Learning recurrent connection weights among hidden neurons without error backpropagation improved learning performance, which was not achievable with previous chaos-based RL [Shibata and Sakashita 2015]. The autonomous change in the state transitions from \"more irregular\u201d to \u201cmore rational\u201d through learning observed in the simulations appears to have solved the balance between exploration and exploitation [Sutton and Barto 2018] and suggests the potential for the emergence of thinking. The learning characteristics of Dynamic RL, as observed in the simulation results of the two tasks, are summarized first, followed by a discussion of its possibilities and challenges from broader perspectives. Finally, the risks and concerns of Dynamic RL are discussed."}, {"title": "4.1 Summary as one method of Reinforcement Learning", "content": "Dynamic RL allows for significantly lower computational costs per step than conventional RL due to the absence of backward computation for learning, particularly along the time axis. Nevertheless, the success rate was almost the same as that of (A) and (B-1) in both Figs. 9 and 15, and the learning curve with episodes as the horizontal axis was also comparable to that of conventional RL using BPTT and SAL as (A) and (B-1) in both Figs. 10 and 16, regardless of the learning task. Furthermore, as for adapting to new environments, conversely, Dynamic RL was considerably faster. The agent's behaviors became more exploratory immediately after entering a new environment and less exploratory as learning progressed thereafter, as shown in Fig. 12(A). This was also reflected in the exploration exponent, a type of Lyapunov exponent for the entire system, including the external world, as shown in Fig. 13(A) in the sequential navigation task. In the slider-crank control task, it was reflected in the internal state change of the RNN, as shown in Figs. 17 and 18.\n\nComprehensively considering the results of varying the application of SAL (Sensitivity Adjustment Learning) or SRL (Sensitivity-Controlled RL), the author suggests that the following mechanisms are functioning. If neurons are linear, larger weights generally increase sensitivity. However, due to the saturation property of each non-linear neuron, excessively large weights often reduce sensitivity conversely by decreasing f'(U) in Eq. (4). This generally decreases the Lyapunov exponent of the system and also the learning activities. Such problems occur when the initial weights are very large or often when another learning method (SRL in this case) progresses, as shown in the early phase of Fig. 13(A-2). SAL maintains the sensitivity of each neuron, thereby preserving the exploration exponent positive (chaotic) in the learning environment, as shown in Fig.13(A). This prevents excessive convergence of the system and a decline in learning activity. Therefore, the absence of SAL leads to a decrease in both the learning success rate and learning speed, as shown in the difference between (A) and (A-2) in Figs. 9, 10, and 15. This is the case not only for Dynamic RL but also for BPTT, as shown in the difference between (B) and (B-1) in Figs. 9 and 15, which is compatible with the results in the previous work [Shibata et al. 2021].\n\nAs learning progresses, while avoiding over-convergence by using SAL, SRL adjusts the dynamics around the state transitions with positive TD-errors to be more convergent; thus, the state transitions become more reproducible. Conversely, it adjusts the dynamics around the state transitions with negative TD-errors to be more exploratory. Since the irregularity sometimes increases and sometimes decreases depending on the TD-errors, it seems that the irregularity remains unchanged in total. However, actually, since the frequency of better state transitions increases, the irregularity around actual state transitions decreases, as the exploration exponent in Fig. 13(A). Moreover, the state transitions that are adjusted to be more reproducible had positive TD-errors. Therefore, SRL makes the state transitions not only \"less irregular\" but also \"more rational.\"\n\nAfter the environment changed during learning, the agent rapidly resumed exploration, and its performance quickly recovered in Dynamic RL. In the navigation task, the dynamics immediately before entering the new environment were neither chaotic nor exploratory for the new environment Env2, as the exploration exponent for Env2 was slightly below zero at 20,000 episodes in Fig. 13(A). Observing the agent's behavior from two slightly separated initial positions, the agent moved to the same corner of the field and remained trapped there in most cases. When the agent moved toward or was trapped in the corner, the TD-error was negative very frequently. In the slider-crank control task, due to the reversal of the reward's sign, large negative TD-errors frequently occurred. Accordingly, in both cases, it can be argued that SRL restored the strong chaotic nature of the system dynamics with the help of SAL and resumed exploratory behaviors, resulting in the quick recovery of performance.\n\nFor agents, the dynamics are shaped not only by the RNN but by the entire system integrated with the outside world, including the physical systems of the agents and their environment. Nevertheless, the gap is interesting where only adjusting or controlling the sensitivity of individual neurons locally by updating their weights and bias can globally control the system dynamics, thereby regulating the system and enabling the agent to respond effectively to the external world. In this paper, the sensitivity targets for SAL are set as appropriate values at 1.3 for the navigation task and 1.6 for the slider-crank control task. A value of 1.0 was appropriate for the sensitivity target in the preceding study investigating SAL in supervised learning with BP or BPTT [Shibata et al. 2021]. In reward-modulated Hebbian learning using chaotic dynamics, performance was optimized near the \"edge of chaos\" [Matsuki and Shibata 2020]. The author is considering the possibility that a large value may be suitable for RL's exploration, which requires stronger chaoticity.\n\nFor massively parallel and flexible systems, asynchronous pulse-and-analog coexisting systems would be far superior to a synchronous digital system. The introduction of Dynamic RL into brain models or brain-like hardware based on such systems offers significant advantages. Local learning without backward computation significantly reduces communication between neurons, as well as the computational cost in each neuron, compared to conventional RL using BPTT. In particular, eliminating backward computation through time is highly beneficial, as it removes the need for time management and memory to store past states. Furthermore, a separate random number generator for stochastic"}, {"title": "4.2 Towards the future from a higher perspective", "content": "Even from the broader perspective of neural network (NN) learning not limited to RL, the Dynamic Learning approach used in Dynamic RL is still quite different in quality from conventional approaches. The state vector of an NN represents a point in its internal state space. In general NN learning, a gradient-based"}]}