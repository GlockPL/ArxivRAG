{"title": "Enhancing Model Interpretability with Local Attribution over Global Exploration", "authors": ["Zhiyu Zhu", "Jiayu Zhang", "Zhibo Jin", "Huaming Chen"], "abstract": "In the field of artificial intelligence, AI models are frequently described as 'black boxes' due to the obscurity of their internal mechanisms. It has ignited research interest on model interpretability, especially in attribution methods that offers precise explanations of model decisions. Current attribution algorithms typically evaluate the importance of each parameter by exploring the sample space. A large number of intermediate states are introduced during the exploration process, which may reach the model's Out-of-Distribution (OOD) space. Such intermediate states will impact the attribution results, making it challenging to grasp the relative importance of features. In this paper, we firstly define the local space and its relevant properties, and we propose the Local Attribution (LA) algorithm that leverages these properties. The LA algorithm comprises both targeted and untargeted exploration phases, which are designed to effectively generate intermediate states for attribution that thoroughly encompass the local space. Compared to the state-of-the-art attribution methods, our approach achieves an average improvement of 38.21% in attribution effectiveness. Extensive ablation studies in our experiments also validate the significance of each component in our algorithm.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the emergence of deep learning, which has significantly advanced the development of artificial intelligence (AI). It has enabled computers to learn from extensive data and achieve remarkable performance in areas such as image recognition and natural language processing [9, 11], contributing to almost every aspect of our daily life. For instance, in healthcare, deep learning aids doctors in diseases diagnosis and treatments planning [4]. In transportation, it powers autonomous vehicles that navigate cities and highways safely and efficiently [16]. It also enhance customer service by answering inquiries and solving issues [1]. Moreover, AI is becoming pivotal in industries such as finance and manufacturing, where it optimizes operations and boosts efficiency [3, 13, 29].\nDeapite Al's increasing practice, its models are often regarded as 'black box', reflecting the transparency and trust concerns in understanding how these models make decisions. It leads to several significant challenges and potential issues. First, it undermines users' trust in Al systems. In critical sectors like healthcare and finance, a transparent decision-making process is essential for users to trust the recommendations [12, 14]. A lack of trust will greatly reduce the practical value of even the state-of-the-art technology [33]. Secondly, it complicates the identification and mitigation of errors or biases. For example, addressing gender or racial bias in AI assisted hiring is challenging without insights into the decision-making criteria [28]. Furthermore, the 'black box' nature of AI poses challenges for legal and ethical responsibility [6]. In cases where Al systems cause harm or disputes, pinpointing responsibility is difficult if the principles behind the behavior cannot be explained. Lastly, this opacity can also hinder the regulation and public oversight of AI technologies, leading to technological developments that deviate from societal ethics and values.\nTo address these challenges, Explainable AI (XAI) becomes a trending topic for research, aiming to increase the transparency and interpretability of artificial intelligence decision-making processes. LIME is one of the earliest methods which approximates the behavior of complex models around given inputs [19]. However, it fails to provide comprehensive and precise insights, and can"}, {"title": "2 RELATED WORK", "content": "In this section, we explore different methods used for explaining Deep Neural Networks (DNNs) and provide a critical discussion of these approaches, which are grouped by three types: local approximation methods, gradient-based attribution methods, and adversarial-sample-based attribution methods."}, {"title": "2.1 Local Approximation Methods", "content": "Local approximation methods seek to understand the behavior of the original model near specific inputs by constructing an approximate, more interpretable model. A well-known method is LIME [19], which approximates local explainability by using multiple interpretable structures near the sample. However, LIME's local explainability requires assumptions, which may not always be accurate. Moreover, LIME can be time-consuming for individual samples. While rudimentary for neural network applications, LIME has been foundational in advancing local explainability methods. Following developments include Layer-wise Relevance Propagation [2] and DeepLIFT [21]. DeepLIFT quantifies the importance of features by comparing the differences between input features and predefined reference points. Although DeepLIFT performs well in local explanation of nonlinear models, its high sensitivity to the choice of reference points can lead to inconsistencies in attribution results. Additionally, DeepLIFT does not satisfy the Implementation invariance axiom proposed in IG [25], leading to potential biases."}, {"title": "2.2 Gradient-based Attribution Methods", "content": "Training neural networks inherently utilize gradients, which has inspired the gradient-based methods that use model gradient information to explain decisions. Early methods like Saliency Map (SM) [22] identify the most important features for model predictions by calculating the gradients of input features relative to the model output. However, SM is prone to gradient saturation, resulting in unstable attribution results, and it does not meet the Sensitivity axiom mentioned in subsequent IG [25], meaning it can yield a zero attribution even if the model output changes. Later methods such as Grad-cam [20] and Score-cam [30] use intermediate layer gradient information but cannot provide high-resolution fine-grained explainability results, and thus cannot be considered true attribution methods (refer to Section 3.1 problem definition).\nThe IG method addresses the insufficient gradient issue of SM by integrating gradients along the path from baseline to input, introducing the axioms of sensitivity and implementation invariance, which are fundamental guarantees for attribution algorithms. Our design also provides proofs of compliance with these axioms. However, IG's main challenge lies in its high computational cost, requiring multiple forward and backward passes. To improve computational efficiency, Fast IG (FIG) [8] optimizes the IG method by improving numerical integration techniques to speed up the attribution process. Although this optimization enhances efficiency, the approximate nature of numerical integration might introduce new errors, affecting the accuracy of attribution results. Additionally,"}, {"title": "2.3 Adversarial-sample-based Attribution Methods", "content": "Adversarial-sample-based attribution methods provide deep explanations of models by generating adversarial samples and exploring model decision boundaries, meaning the attribution process no longer relies on manually specified baseline points. Adversarial Gradient Integration (AGI) [17] is a representative work that uses adversarial samples to explore decision boundaries and improves attribution performance through nonlinear path integral gradients. While AGI offers an innovative method of explanation, its performance highly depends on the quality of the adversarial samples, which may not be stable in some cases.\nBoundary-based Integrated Gradients (BIG) [31] introduces a boundary search mechanism to optimize the baseline selection, thereby obtaining more accurate feature attributions. However, BIG relies on a linear integration path, which may limit its ability to capture the nonlinearity and complexity in model decision. AttEXplore [35] improves feature attribution by combining adversarial attacks with model parameter exploration, emphasizing the ability to transition between different decision boundaries. Although AttEXplore shows foresight in enhancing the generalization ability of model explanations, its high computational complexity may limit its application on large-scale models and datasets. MFABA (More Faithful and Accelerated Boundary-based Attribution) [36] enhances the accuracy and computational efficiency of explanations through second-order Taylor expansion and decision boundary exploration, particularly suited for complex model explanations. Nonetheless, its reliance on higher-order derivatives may increase the computational burden, especially when dealing with large deep learning models. This class of adversarial-sample-based attribution methods introduces a large number of intermediate states from the OOD space during the adversarial attack process, as shown in Figure. 1, affecting the accuracy of attribution (discussed in Section 3.2)."}, {"title": "3 METHOD", "content": "In this section, we define the attribution task, the local properties of attribution, and the algorithmic procedure of the LA (Local Attribution) method. Ensuring local properties is key to the rationality of attribution results, and within these constraints, it is still possible to achieve results that satisfy the remaining axioms of attribution. We will describe these in detail below and provide rigorous mathematical derivations. Additionally, the LA algorithm consists of two parts: targeted and untargeted attribution, which can be combined under the premise of maintaining local properties."}, {"title": "3.1 Problem Definition", "content": "Given neural network parameters \\(w \\in \\mathbb{R}^m\\) and a sample \\(x \\in \\mathbb{R}^n\\) to be attributed, we aim to use an attribution method to obtain attribution results \\(A(x) \\in \\mathbb{R}^n\\), where \\(A_i(x)\\) represents the importance of the \\(i\\)-th feature dimension. The greater the attribution result, the more important the dimension is for the model's decision. We use \\(f(x) \\in \\mathbb{R}^c\\) to represent the model output, where \\(c\\) denotes the number of classes."}, {"title": "3.2 Local Space of Attribution", "content": "Before introducing local properties, we present a critical research question: RQ1: Does the significance of the attribution results still hold if there is a critical deviation in the features? Current mainstream attribution algorithms overlook this question. To illustrate, consider a toy example where a data sample x has four dimensions x = [6, 8, 6, 10]. During the use of IG [25], gradients of intermediate variables accumulated from the sample to the baseline are considered. Suppose there is only one intermediate state, and the baseline is b = [0, 0, 0, 0]. Thus, the intermediate variable \\(x'\\) lies between x and b at \\(x'\\) = [3, 4, 3, 5]. At this moment, we need to compute the gradient information, but is this gradient information truly valuable? Given the vast input space neural networks face-a"}, {"title": "3.3 Deep Analysis of Untargeted and Targeted Adversarial Attacks", "content": "The direct output of our neural network is defined as \\(z = f(x) \\in \\mathbb{R}^C\\), and after passing through a softmax function, z becomes a probability distribution \\(p = \\text{softmax}(z) \\in \\mathbb{R}^C\\) with \\(p_i \\in (0, 1)\\). Observing the gradient \\(\\frac{\\partial z_i}{\\partial x} \\in \\mathbb{R}^n\\), updating x along the direction of \\(\\frac{\\partial z_i}{\\partial x}\\) increases \\(z_i\\) (proof refers to Eq. 3, the Taylor expansion). We examine the gradient information of z during the computation of cross-entropy loss as shown in Eq. 8.\n\\[\\frac{\\partial L(x; y, w)}{\\partial z_i} = \\frac{\\delta p_i}{\\delta z_i} = \\begin{cases} p_i - 1 & \\text{if } i = \\text{class of } y \\\\ p_i & \\text{otherwise} \\end{cases}\\]\nUsing the chain rule for gradients, we find \\(\\frac{\\partial L(x; y, w)}{\\partial x} = \\frac{\\partial L(x; y, w)}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial x}\\). Combining with Eq. 8, we observe that when i is the original category (the most probable category), the gradient information \\(\\frac{\\partial z_i}{\\partial x}, j \\neq i\\), will be low, since the probability values \\(p_j\\) are lower than for the original category. Thus, relying solely on untargeted adversarial attacks to explore the local space might neglect the information from categories other than the original. This necessitates the introduction of targeted attacks for other categories. As shown in Figure. 1, considering that the sign of targeted adversarial attacks is opposite to that of untargeted attacks, analyzing with \\(1 - p_j\\) becomes relevant, where \\(1 - p_j\\) is large when \\(p_j\\) is small, allowing the preservation of gradient information. From a gradient perspective, it is crucial to incorporate both forms of adversarial attack in the local space, and since \\(\\epsilon_i\\) remains the same under both attack conditions, their effects can be combined additively."}, {"title": "3.4 Local space sampling optimization", "content": "Finally, for the sampling process from B(x) to obtain x, we can approximate it iteratively, using the gradient calculated from the previous sample step to perform a one-step attack from the original sample. If the sign of the gradient in the same dimension changes within a local space, it indicates that the dimension is sensitive and requires further exploration. If the dimension remains unchanged, it implies that maintaining the current dimension does not require alteration, thus reducing the scope of space that random sampling needs to explore. The obtained x still resides within the local space B(x), and we provide rigorous proof in Appendix D, Complexity Analysis in Appendix E, and pseudocode in Appendix F."}, {"title": "4 EXPERIMENTS", "content": "In this section, we provide a detailed description of the series of experiments conducted using the Local Attribution (LA) algorithm, including the choice of datasets, models, baseline methods, evaluation metrics, and experimental analysis."}, {"title": "4.1 Dataset and Models", "content": "Our experiments randomly selected 1000 images from the ImageNet dataset, following the precedent set by existing methods such as AGI [17], MFABA [36], and AttEXplore [35]. Furthermore, we tested the LA algorithm using four different convolutional neural network architectures to assess its effectiveness and generality, namely Inception-v3 [26], ResNet-50 [7], VGG16 [23], and MaxViT-T [27]."}, {"title": "4.2 Baselines", "content": "To comprehensively evaluate the performance of the Local Attribution (LA) algorithm and ensure the fairness of our assessments, we have compared LA against eleven baseline methods. These baselines cover a wide range of XAI methods, including AGI [17], AttEXplore [35], BIG [31], DeepLIFT [21], EG [5], FIG [8], GIG [10], IG [25], MFABA [36], SG [24], and SM [22]. These methods represent various technical approaches in the field of model explainability, providing a broad reference standard for evaluation."}, {"title": "4.3 Evaluated Metrics", "content": "We utilized two traditional metrics, the Insertion Score and the Deletion Score, to evaluate the explanatory power of various explainability methods. These metrics measure the model's reliance on different parts of the input data by observing changes in its performance.\nThe Insertion Score is calculated by progressively converting pixels from a baseline state (typically a state with no meaningful information, such as an all-black or all-white image) to the pixels of the original image. This conversion involves incorporating a specified number of the most important pixels, as identified by the explainability method, from the baseline state to their values in the original image. The model's performance is re-evaluated at each step until all pixels have been converted from the baseline state to their corresponding original values. The Insertion Score is defined as the area under the curve (AUC) of the change in output probability for the current class as the pixels are inserted.\nThe Deletion Score, on the other hand, is computed by progressively removing the most important pixels from the original"}, {"title": "4.4 Parameters", "content": "In this series of experiments, we set the number of sampling to 30 for the MaxViT-T model and 20 for the other models. The spatial range s was consistently set to 20 across all experiments. The size of the local exploration space was set to a range of 1 pixel."}, {"title": "4.5 Experimental Results", "content": "As shown in Tab. 1, our LA method achieved significant improvements. Compared to other methods, the average increase in Insertion across the four models was 0.31758, and the average reduction in Deletion was 0.028883. Specifically, the average improvements in Insertion for the Inception-v3, ResNet-50, VGG16, and MaxViT-T models were 0.30948, 0.36262, 0.28626, and 0.31197 respectively; while the reductions in Deletion were 0.019774, 0.017429, 0.025277, and 0.053052 respectively. Compared to the latest attribution methods like AGI, MFABA, and AttEXplore, LA showed clear advancements. Notably, LA not only improved in terms of Insertion but also reduced Deletion, indicating a comprehensive enhancement in explainability performance compared to these methods. While some methods slightly outperformed LA in terms of Deletion, their Insertion scores were substantially lower than LA. As previously mentioned, the significance of Insertion outweighs that of Deletion, thereby firmly establishing the efficacy of the LA method. More results are provided in Appendix G and H."}, {"title": "4.6 Ablation Studies", "content": "4.6.1 Impact of Constant vs. Linear Space Constraints on Effectiveness. This section discusses the impact of Constant and Linear space constraints on the effectiveness of LA. We fixed the number of samples and the spatial range s at 20. As shown in Figure. 6, across different models, the attribution performance under Linear space constraints was comprehensively better than under Constant constraints, with significantly higher Insertion Scores and lower Deletion Scores under Linear constraints.\n4.6.2 Impact of Attack Type on Effectiveness. We discuss the impact of targeted and untargeted attacks on the effectiveness of LA. The number of samples and the spatial range s were kept constant at 20. As depicted in Figure. 7, across different models, the Insertion Scores from Untargeted Attacks was higher than from Targeted Attacks. However, the Deletion Scores were relatively similar, showing little variation.\n4.6.3 Impact of Sampling Times on Effectiveness. This part discusses how the number of Sampling affects the effectiveness of LA. We kept the spatial range s at 20. As shown in Figure. 8 with an increase in sampling rate, LA's Insertion Score increased and showed a trend towards convergence, while the increase in Deletion Score was more abrupt.\n4.6.4 Impact of Spatial Ranges on Effectiveness. In this section, we explore the impact of the spatial range s on the effectiveness of LA. The number of samples was fixed at 20. As illustrated in Figure. 9, with an increase in s, the Insertion Score initially increased and then decreased, peaking when s was at 10."}, {"title": "5 CONCLUSION", "content": "In this paper, we identifies the challenge of ineffective intermediate states in current attribution algorithms, which has significantly impacted the attribution results. To better investigate this issue, we introduces the concept of Local Space to ensure the validity of intermediate states during the attribution process. With these findings, we propose the LA algorithm, which can comprehensively explore the Local Space using both targeted and untargeted adversarial attacks, thereby achieving state-of-the-art attribution performance in comparison with other methods. We provide rigorous mathematical derivations and ablation study to validate the significance of each component in our algorithm. We anticipate this work will facilitate the attribution method in XAI research."}, {"title": "A PROOF OF AXIOMS", "content": "Sensitivity Proof. Given that:\n\\[\\mathbb{E}_{x = u(x), x \\sim B_{\\epsilon}(x)}[L(x; y, w) - L(x; y, w)] = \\sum_{i=1}^{n} \\mathbb{E}_{x = u(x), x \\sim B_{\\epsilon}(x)} [(x_i - x_i) \\cdot \\frac{\\partial L(x; y, w)}{\\partial x_i}]\\]\nthe total attribution on the right side equals the expected change in the loss function. A change in the loss function necessarily results in a non-zero attribution on the right, proving sensitivity."}, {"title": "B IMPLEMENTATION INVARIANCE PROOF", "content": "Implementation Invariance Proof. The Local Attribution (LA) algorithm adheres to the chain rule. Based on the properties of gradients, the LA algorithm satisfies implementation invariance, ensuring that results are consistent across different valid implementations of the same functional relationship."}, {"title": "CPROOF OF \\(\\epsilon\\)-Local Space", "content": "Proof. Consider the functions \\(u^+\\) and \\(u^-\\) defined for perturbation within the local space:\n\\[u^+(x) = x + \\frac{\\epsilon}{2} \\cdot \\text{sign}(\\frac{\\partial L(x, y, w)}{\\partial x})\\]\n\\[u^-(x) = x - \\frac{\\epsilon}{2} \\cdot \\text{sign}(\\frac{\\partial L(x, y, w)}{\\partial x})\\]"}, {"title": "D PROOF OF SPACE CONSTRAINT", "content": "Proof of Space Constraint. The iterative process for updating positions in an adversarial example generation context can be described as follows:\n\\[x_u^k = x^{k-1} + \\frac{\\epsilon}{2} \\cdot \\text{sign}(\\frac{\\partial L(x^{k-1}; y, w)}{\\partial x^{k-1}})\\]\n\\[x_t^k = x^{k} - \\frac{\\epsilon}{2} \\cdot \\text{sign}(\\frac{\\partial L(x^{k}; y', w)}{\\partial x^{k}})\\]"}, {"title": "E COMPLEXITY ANALYSIS", "content": "Specifically, the total time complexity of the LA method is O(s \u00d7 N \u00d7 n), where s is the spatial range, N is the number of iterations, and n is the dimension of the input sample. The total number of explorations per image is approximately (s + 1) \u00d7 N."}]}