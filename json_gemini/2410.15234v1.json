{"title": "Bias Amplification: Language Models as Increasingly Biased Media", "authors": ["Ze Wang", "Zekun Wu", "Jeremy Zhang", "Navya Jain", "Xin Guan", "Adriano Koshiyama"], "abstract": "As Large Language Models (LLMs) become increasingly integrated into various facets of society, a significant portion of online text consequently become synthetic. This raises concerns about bias amplification, a phenomenon where models trained on synthetic data amplify the pre-existing biases over successive training iterations. Previous literature seldom discusses bias amplification as an independent issue from model collapse. In this work, we address the gap in understanding the bias amplification of LLMs with four main contributions. Firstly, we propose a theoretical framework, defining the necessary and sufficient conditions for its occurrence, and emphasizing that it occurs independently of model collapse. Using statistical simulations with weighted maximum likelihood estimation, we demonstrate the framework and show how bias amplification arises without the sampling and functional form issues that typically drive model collapse. Secondly, we conduct experiments with GPT-2 to empirically demonstrate bias amplification, specifically examining open-ended generational political bias with a benchmark we developed. We observe that GPT-2 exhibits a right-leaning bias in sentence continuation tasks and that the bias progressively increases with iterative fine-tuning on synthetic data generated by previous iterations. Thirdly, we explore three potential mitigation strategies: Overfitting, Preservation, and Accumulation. We find that both Preservation and Accumulation effectively mitigate bias amplification and model collapse. Finally, using novel mechanistic interpretation techniques, we demonstrate that in the GPT-2 experiments, bias amplification and model collapse are driven by distinct sets of neurons, which aligns with our theoretical framework.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are trained on vast amounts of text scraped from the internet, which plays a crucial role in improving their capabilities, whether through emergent abilities (Wei et al., 2022) or scaling laws (Kaplan et al., 2020). However, as LLMs become more widely integrated into human society\u2014for example, in content creation and summarization in media, academia, and business (Maslej et al., 2024)\u2014concerns are mounting that a significant portion of online text in the future may be generated, either entirely or partially, by LLMs (Pe\u00f1a-Fern\u00e1ndez et al., 2023; Porlezza and Ferri, 2022; Nishal and Diakopoulos, 2024). This highlights a significant and underexplored risk: bias amplification, referring to the degradation of fairness within LLMs over self-consuming training loops (Mehrabi et al., 2022; Taori and Hashimoto, 2022), where models progressively amplify the pre-existing biases. This concern initially arises from the tendency of LLMs to learn from biased datasets. For example, Parrish et al. (2022); Wang et al. (2024); Bender et al. (2021) show that LLMs absorb inherent stereotypes, such as racial and gender discrimination, embedded in human-generated text. Additionally, Haller et al. (2023); Rettenberger et al. (2024a) demonstrated that LLMs can be aligned with specific political ideologies by fine-tuning them on biased datasets. Moreover, Wyllie et al. (2024) shows that classifiers trained on synthetic data increasingly favor a particular class label over successive generations, while the shrinking diversity observed by Alemohammad et al. (2023); Hamilton (2024) suggests a risk that certain demographic groups may become progressively under-represented in the outputs of LLMs.\nThe amplification of biases has profound societal implications. It can lead to the perpetuation of stereotypes, reinforcement of social inequalities, and the marginalization of underrepresented groups. In the context of political bias, this can influence public opinion, skew democratic processes, and exacerbate polarization. Understanding and mitigating bias amplification is therefore crucial to ensure that LLMs contribute positively to society and do not inadvertently cause harm. Nevertheless, despite the literature on bias amplification in discriminative models, there is a notable lack of comprehensive frameworks and empirical studies specifically addressing bias amplification for LLMs, as shown in Section 2. In contrast to discriminative models, where bias amplification can be attributed to overfitting the dominant features in the training dataset, its occurrence in LLMs may stem from a more nuanced issue. As we will show in Sections 3 and 5, even when the training data is unbiased, the model can still amplify its pre-existing biases during the training process.\nIn this paper, we seek to fill this research gap by proposing a theoretical framework that establishes and explains the causes of bias amplification in LLMs. Additionally, we perform both statistical simulation and LLM experiments to demonstrate and exemplify the theorem. In summary, our main contributions are as follows:\n1. Theoretical Framework: We establish the necessary and sufficient conditions for bias amplification (i.e. Theorem 1). The theorem aids in understanding the cause of bias amplification and in distinguishing between bias amplification and model collapse. We conduct statistical simulations using weighted maximum likelihood estimation to illustrate the theorem (see Section 3.2).\n2. Benchmarking Tool: We trained a highly accurate classifier capable of detecting political leaning in long-text content. With this classifier, we offer a benchmark for evaluating political bias in LLMs through open-generation tasks, filling a gap not covered in current bias studies (see Sections 2 and 4.3).\n3. Empirical Demonstration and Mitigation Strategies: We demonstrate bias amplification in terms of political bias in GPT-2 using our benchmarking tool: the model exhibits a right-leaning bias in sentence continuation tasks and becomes increasingly right-leaning over successive generations (see Section 5.1). Additionally, we conducted experiments with three potential mitigation strategies, i.e. Overfitting, Preservation, and Accumulation, comparing their effectiveness, and found that some are surprisingly effective (see Section 5.3).\n4. Mechanistic Interpretation: Building on our framework, we propose an innovative mechanistic interpretation pipeline that identifies two distinct sets of neurons responsible for bias amplification and model collapse during iterative fine-tuning experiments with GPT-2. We found minimal overlap between these two sets, supporting our theorem that bias amplification can occur independently of model collapse (see Section 5.4).\nThe rest of the paper is organized as follows: We first discuss related work in Section 2. In Section 3, we present our theoretical framework for bias amplification and conduct a statistical simulation. Section 4 describes the experimental setup for our LLM experiments, including data preparation and model fine-tuning. In Section 5, we present our empirical findings on bias amplification and the effectiveness of mitigation strategies. Finally, in Section 6, we discuss the implications of our work, concluding with limitations in Section 7."}, {"title": "2 Background and Related Work", "content": "Bias Amplification. Bias amplification in machine learning models has been seriously studied in the context of visual recognition tasks. For example, Zhao et al. (2017) found that Conditional Random Fields can exacerbate social biases present in the training data. This study proposed an in-process mitigation approach, employing Lagrangian Relaxation to enforce constraints that ensure the model's bias performance remains closely aligned with the biases in the training data. Following this, Mehrabi et al. (2022) proposed the concept of bias amplification in feedback loops, where biased models not only amplify the bias present in their training data but also interact with the world in ways that generate more biased data for future models. Xu et al. (2023); Zhou et al. (2024) examined bias amplification in recommendation models, showing how these models reinforce their understanding of mainstream user preferences from training data, leading to an overrepresentation of such preferences in historical data and neglecting rarely exposed items\u2014similar to the concept of sampling error discussed in (Shumailov et al., 2024). Additionally, Wyllie et al. (2024); Taori and Hashimoto (2022) demonstrated that classifiers trained on synthetic data increasingly favor specific class labels over successive generations. Likewise, Ferbach et al. (2024); Chen et al. (2024) observed bias amplification in generative models such as Stable Diffusion, characterized by the overrepresentation of features from the training dataset. Moreover, we conducted a comprehensive review of the literature on model collapse which we put in Appendix L.\nPolitical Biases. LLMs become a new form of"}, {"title": "3 Theoretical Framework", "content": "In this section, we formalize the conditions under which bias amplification occurs during recursive training on synthetic data. This framework is designed to intuitively illustrate the primary causes of bias amplification. Next, we demonstrate bias amplification and the theorem using Weighted Maximum Likelihood Estimation (WMLE)."}, {"title": "3.1 The Conditions for Bias Amplification", "content": "Bias amplification can arise from two key factors. The first is referred to as bias projection. It arises when the bias projection coefficient (explained below) is negative, indicating that the gradient update during training will fully align with the existing bias in the model parameters, thus amplifying it. Consider a fine-tuning process where the pre-trained model parameters \\(\\theta_t\\) can be decomposed into unbiased and biased components:\n\n\n\n\\(\\theta_t = \\theta_{t,unbiased} + \\theta_{t,biased}.\\)\n\n\nDuring gradient-based optimization, the update rule is:\n\n\n\n\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta}L_{ft}(\\theta_t),\\)\n\n\nwhere \\(\\eta\\) is the learning rate, and \\(L_{ft}\\) denotes the fine-tuning loss function. Substituting the decomposition of \\(\\theta_t\\) and taking the projection, we have:\n\n\n\n\\(\\theta_{t+1} = \\theta_{t,unbiased} + \\theta_{t,biased} - \\eta  c_t \\frac{\\theta_{t, biased}}{\\|\\theta_{t,biased}\\|} ,\\)\n\n\nwhere \\(c_t\\) is the bias projection coefficient, measuring the projection of the gradient onto the normalized biased component of the parameters:\n\n\n\n\n\n\\( c_t = \\frac{\\theta_{t, biased}}{\\|\\theta_{t,biased}\\|}^T \\nabla_{\\theta}L_{ft}(\\theta_t).\\)\n\n\n\nIf \\(c_t < 0\\), the gradient update will reinforce the biased component, leading to bias amplification, i.e. \\(|\\theta_{biased}| > 0\\). This occurs because the gradient descent step moves the parameters further in the direction of the existing bias. Intuitively, the bias in the output is influenced by specific neurons, and increasing the weights of these neurons best aligns with the training objective for the given dataset. This does not require the dataset itself to be biased, but rather that it contains features, aside from bias, that align more closely with the functionalities of biased neurons relative to the unbiased neurons considering the loss function.\nThe second factor is referred to as constraint deficiency. It occurs when the loss or objective function lacks explicit constraints or regularization to counteract bias projection. As a result, the optimization process does not prevent the reinforcement of bias when bias projection is present. To prevent bias amplification theoretically, the loss function can incorporate de-biasing constraints. Consider a modified fine-tuning objective: \\(L_{new}(\\theta) = L_{original}(\\theta) - \\lambda \\sum_{i=1}^{n} max(0, c_i^2)^2 + \\gamma \\sum_{i=1}^{n} min(0, c_i^2)^2\\), where \\(c_i^2\\) represents the bias projection coefficient for a specific dimension. \\(\\lambda, \\gamma > 0\\) are the Lagrangian multipliers for the two constraints:\n\n\n(1) \\(\\sum_{i=1}^{n} max(0, c_i^2)^2 \\geq 0\\), i.e. encouraging positive components.\n\n(2) \\(\\sum_{i=1}^{n} min(0, c_i^2)^2 \\leq 0\\), i.e. eliminating negative components.\n\nThe absence of such constraints allows bias amplification when bias projection is present. Based on these findings, we establish the following:\nTheorem 1 (Condition for Bias Amplification). Bias amplification, a self-reinforcing process where a model trained on synthetic data amplifies the pre-existing biases from previous training, leading to an increased bias in the data it generates for future training, occurs if both bias projection and constraint deficiency are present. It occurs only if there is bias projection in at least one parameter dimension and constraint deficiency is present.\nAlongside understanding the causes of bias amplification, this also implies that bias amplification can occur even without model collapse. Specifically, statistical approximation and functional expressivity errors\u00b9\u2014common causes of model collapse (Shumailov et al., 2024)\u2014are unrelated with bias projection and constraint deficiency. This is further supported by our experiments in Sections 3.2 and 5."}, {"title": "3.2 Weighted Maximum Likelihood Estimation (WMLE)", "content": "To demonstrate bias amplification in a controlled setting, we consider a statistical estimation cycle using WMLE, devoid of statistical approximation and functional expressivity errors. The mathematical formulation of WMLE is provided in Appendix A. Specifically, we generate a \"biased\" pre-training dataset \\(D_{pre}\\) consisting of \\(N = 10,000\\) samples from a \"biased\" Beta(\\(\u03b1 = 3, \u03b2 = 2\\)) distribution, with a mean of \\(x = 0.6\\). Maximum likelihood estimation (MLE) is used to estimate the probability density function (pdf) of this beta distribution, resulting in the 'biased' pre-trained pdf (referred to as \\(f_{pre}\\)). Our goal in the estimation cycle is to estimate the pdf of the true distribution, Beta(\\(\u03b1 = 2, \u03b2 = 2\\)), with a mean of \\(x = 0.5\\). We generate N samples from the true distribution, referred to as \\(D_{real}\\), serving as data for the initial round of training. In the first round, we estimate the true pdf using WMLE with \\(D_{real}\\) and weights derived from \\(f_{pre}\\), simulating the impact of the biased pre-trained model during the current training. The newly estimated pdf is referred to as \\(f_{Generation 0}\\). Then, we generate the synthetic data \\(D_{Generation 0}\\) of size N using \\(f_{Generation 0}\\). In the second round, we use WMLE with \\(D_{Generation 0}\\) and weights derived from \\(f_{Generation 0}\\) to obtain \\(f_{Generation 1}\\). We then repeat this process, recursively estimating \\(f_{Generation 2}\\) through \\(f_{Generation 10}\\), simulating an iterative training process on synthetic data.\nResults: As depicted in Figure 1, the estimated distributions progressively shift towards the mean of the biased Beta(\\(\u03b1 = 3, \u03b2 = 2\\)) distribution, at \\(x = 0.6\\), becoming increasingly peaked with each generation. This behavior exemplifies bias amplification. The distortion arises from two sources: bias projection, where the estimation process emphasizes data regions where the 'biased' pre-trained distribution assigns higher probability; and constraint deficiency, where WMLE lacks mechanisms to counteract the bias introduced by the weights, allowing the bias to be amplified over generations. For comparison, Figure 2 shows the results using standard MLE without weighting. The estimated distributions remain consistent across generations, accurately capturing the true Beta(2, 2) distribution. This indicates that without the influence of biased pre-trained pdf and the weighted learning process, bias amplification does not occur."}, {"title": "4 Experiment Design", "content": "In this section, we present and explain in detail the experiments designed to investigate the bias amplification of LLMs (see results in Section 5). Specifically, we analyze the sequential and synthetic fine-tuning of GPT-2. While similar experiments could be conducted with larger models such as GPT-4 or Claude in non-fine-tuning contexts, doing so would likely incur significant environmental costs. The focus of this study is on political bias within the US political spectrum, particularly in the context of sentence continuation tasks. This is important because LLMs are increasingly becoming a source of information for global news consumption (Maslej et al., 2024; Pe\u00f1a-Fern\u00e1ndez et al., 2023; Porlezza and Ferri, 2022), and traditional news outlets, such as the Associated Press, are beginning to incorporate LLMs to automate content generation from structured data (The Associated Press, 2024)."}, {"title": "4.1 Experiment Dataset Preparation", "content": "We randomly selected 1,518 articles from the Webis-Bias-Flipper-18 dataset (Chen et al., 2018), which contains political articles from a range of U.S. media outlets published between 2012 and 2018, along with bias ratings assigned at the time for each media source. These ratings were provided by Allsides and were determined through a multi-stage process involving both bipartisan experts and members of the general public (AllSides, 2024a). Based on the bias ratings, the random selection was stratified to ensure that the 1,518 articles were evenly divided into three groups of 506 articles each, representing left-leaning, right-leaning, and center-leaning media."}, {"title": "4.2 Fine-tuning and Synthetic Data Generation", "content": "Inspired by (Shumailov et al., 2024; Dohmatob et al., 2024b), each training cycle begins with GPT-2 fine-tuned on the dataset of human-written political articles. The fine-tuned GPT-2 is then used to generate the first set of synthetic data, which is subsequently employed to further fine-tune the model, producing model generation 1. This iterative process continues until generation 10 is reached. For details on the fine-tuning setup, refer to Appendix B.\nSynthetic datasets are created as follows: we begin by tokenizing each original article and dividing it into 64-token blocks. For each block, the model generates a continuation by predicting the next 64 tokens based on the tokenized input. We employ three generation methods: (1) deterministic, (2) beam search (num_beams = 10) (Holtzman et al., 2020; Taori and Hashimoto, 2022), (3) Nucleus sampling (top_p = 0.9) (Graves, 2012; Taori and Hashimoto, 2022). Once all blocks are processed, the synthetic tokens are decoded back into text. This procedure is applied to all articles, resulting in a synthetic dataset of the same size as the original."}, {"title": "4.3 Political Bias Metric", "content": "We develop a metric model to classify the political leaning of synthetic outputs generated by large language models. The classifier assigns labels corresponding to left, center, and right-leaning, trained on data from AllSides (AllSides, 2024a), a source that assigns bias ratings to each media outlet. The methodology for the classifier is as follows:\nDataset and Preprocessing. We use a dataset consisting of political articles with labeled bias categories for the source media: left, center, and right. We concatenate the article titles with their body text to form the input. To address class imbalance, we resample the center-leaning articles, balancing the dataset with respect to each category. The dataset is then split into training (70%), validation (15%), and test (15%) subsets, stratified by bias label. Moreover, we conduct a human review of the test subsets to eliminate any identifiable information about media sources or writers.\nModel Architecture. We experiment with multiple transformer-based models, including BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and other state-of-the-art architectures, to select the best-performing model based on macro F1 score. Each model is fine-tuned using the HuggingFace 'Trainer' class, with a learning rate of 2 \u00d7 10-5, a batch size of 16, and 5 training epochs. We employ a cross-entropy loss function for multi-class classification.\nTraining Procedure. Tokenization is performed using each model's respective tokenizer with a maximum sequence length of 512 tokens. To mitigate overfitting, weight decay of 0.01 is applied during training. The model checkpoints are saved after each epoch, and the best model is selected based on macro F1 score evaluated on the validation set. We use a weighted random sampler during training to ensure balanced class representation.\nEvaluation Metrics. We evaluate the models using the macro F1 score to account for the multi-class nature of the task, ensuring that performance is balanced across all bias categories. The final evaluation is conducted on the held-out test set. Additionally, we report the loss, runtime, and sample processing rates for completeness.\nResults. After conducting a grid search across several models, we find that the roberta-base model achieves the best performance with an evaluation loss of 0.4035 and a macro F1 score of 0.9196 on the test set Thus, roberta-base"}, {"title": "4.4 Generation Quality Metric", "content": "We introduced a metric for evaluating generation quality, specifically addressing the issue of repetitive content in later model iterations, which can skew traditional perplexity metrics. This metric is based on the Gibberish Detector (Jindal, 2021), which identifies incoherent or nonsensical text. The detector classifies text into four categories: (1) Noise, where individual words hold no meaning; (2) Word Salad, where phrases are incoherent; (3) Mild Gibberish, where grammatical or syntactical errors distort meaning; and (4) Clean, representing coherent, meaningful sentences. To evaluate generation quality, we assign the Gibberish score to each sentence: 3 for Clean, 2 for Mild Gibberish, 1 for Word Salad, and 0 for Noise. An average score is calculated for each article, resulting in the text quality index. It focuses on the coherence and meaning of the generated text and outperforms perplexity in accurately reflecting actual generation quality, as demonstrated in Section 5.2."}, {"title": "5 Results", "content": "In this section, we present the results on bias and generation performance of GPT-2 throughout iterative training, both with and without the deployment of mitigation strategies."}, {"title": "5.1 Political Bias", "content": "Using an unbiased dataset of 1,518 human-written articles evenly distributed across three political-leaning categories, we generated 1,518 synthetic articles by deterministically predicting the next 64 tokens based on the previous 64 tokens. If GPT-2 had no pre-existing bias, the resulting distribution would be evenly spread across the three political-leaning labels. However, as shown in Figure 3, the model generates more center (47.9%) and right-leaning articles (46.8%), indicating a bias toward these categories prior to any fine-tuning.\nWe then fine-tuned GPT-2 to produce Generation 0 and continued the process iteratively using synthetic datasets, up to Generation 10. As shown in Figure 4, the fine-tuned model displayed a significant amplification of bias toward the Right across all three generation setups after Generation 2. Interestingly, Generation 0 in the deterministic setup (the 'Synthetic' line), which was fine-tuned on an unbiased dataset of real articles, produces an even higher percentage of right-leaning articles (54.5%) compared to the unfine-tuned GPT-2.\nTo ensure bias classification aligns with human intuition, we further examine model responses with human assistance."}, {"title": "5.2 Generation Quality", "content": "As shown in Figure 5, there is a model collapse in terms of the deterioration of the generation quality across all three generation setups: the average text quality index (defined in Section 4.4) decreases over the iterative training. Moreover, the distributions of the text quality index for all three generation setups shift markedly toward the low-quality region and starts to produce data that is never produced by Generation 0. These findings are consistent with the existing literature on model collapse, such as (Shumailov et al., 2024). However, we did not observe significant variation in variance, as seen in Figure 5 and distribution plots in Appendix D. In contrast, perplexity measurements show a steady decrease across generations for the deterministic and beam search generation setups, generally indicating improved generation quality (see Figure 11 in Appendix E).\nFor a closer look, as shown by the examples in Appendix F, the generated articles progressively lose coherence and relevance with each successive generation, as repetition and fragmented sentences become more common. By Generation 10, the text is largely incoherent and disconnected from the original content, diminishing its readability and meaning. However, despite the clear deterioration in generation quality, perplexity decreases over generations, as shown by the results at the end of each synthetic output example. This pattern is observed in most other synthetic outputs as well, suggesting that perplexity fails to accurately reflect the model's true generation capabilities and may be biased toward repetitive content."}, {"title": "5.3 Mitigation Strategies", "content": "We applied three potential mitigation strategies under the deterministic generation setup: (1) Overfitting, which involved increasing the training epochs to 25 (five times the baseline) and setting weight decay to 0 to reduce regularization and encourage overfitting, as proposed by Taori and Hashimoto (2022) based on their uniformly faithful theorem of bias amplification; (2) Preserving 10% of randomly selected real articles during each round of synthetic fine-tuning, a method proposed and used in (Shumailov et al., 2024; Alemohammad et al., 2023; Dohmatob et al., 2024b; Guo et al., 2024); and (3) Accumulating all previous fine-tuning datasets along with the new synthetic dataset in each fine-tuning cycle, which was introduced by Gerstgrasser et al. (2024). As shown in Figure 6, overfitting helps reduce bias amplification in the early generations compared to the no-mitigation setup (the 'Synthetic' line), but it fails to prevent bias amplification in the later generations. Additionally, it incurs a significant cost-further deterioration in generation quality, as shown in Figure 7. Notably, both the preservation and accumulation strategies perform well in mitigating both bias amplification and model collapse. Interestingly, these strategies even result in bias reduction."}, {"title": "5.4 Mechanistic Interpretation", "content": "To gain a clearer understanding of the causes of bias amplification and how it empirically differs from model collapse, we investigate how neurons behave and vary across different versions 2 of fine-tuned GPT-2 with differing levels of generation quality and bias performance. First, we examined how the weight of each neuron changes across different versions. For each of the 9,216 neurons, we calculated the correlation between its weight and the model's bias performance (i.e. the percentage of right-leaning articles) across the 66 versions, as shown in Figure 12 in Appendix G. To statistically test the significance of the correlations, we calculated Newey-West adjusted p-values for the 9,216 neurons, using the null hypothesis that no significant relationship exists between the change in neuron weight and the change in bias performance, and applying the Bonferroni correction to maintain an overall 0.05 significance level. This yielded a set of 553 neurons whose weight value changes are significantly correlated with changes in bias performance (i.e. p-value < 1.36 \u00d7 10\u22126), indicating they are likely primary contributors to the shift in bias 3. In contrast, we identified only one neuron whose weight changes are significantly correlated with changes in generation quality. This suggests that shifts in bias performance and generation quality are likely driven by different underlying mechanisms. The formula for the statistical tests is provided in Appendix K.\nSecondly, we examined how the activation values of each neuron change across different versions of GPT-2 when provided with the same input used to generate synthetic data (see Section 4.2). Specifically, we attached activation hooks to capture activation values (i.e., outputs from these layers during a forward pass) for 9,216 neurons. With the extracted activation values, we identified two sets: one consisting of 3,062 neurons whose activation value changes are significantly correlated with bias performance changes (i.e. p-value < 1.36 \u00d7 10-6), and another consisting of only 2 neurons whose activation value changes are significantly correlated with changes in generation quality."}, {"title": "6 Discussion", "content": "We now discuss the implications of bias amplification for the future development of large language models. As demonstrated, bias amplification is a parallel concept to model collapse and warrants equal emphasis in future research. In our case, we successfully identified ad hoc mitigation strategies, such as Preservation and Accumulation, which act as natural constraints on gradient updates during training by ensuring the model still considers the initial unbiased dataset. However, if the training dataset itself is biased or overrepresents certain demographic or political groups, these methods may fail, while pre-processing strategies could be too resource-intensive. Therefore, there is an urgent need to develop in-process mitigation strategies to address the constraint deficiency and promote more equitable and fair model development."}, {"title": "7 Limitations", "content": "While this work introduces a comprehensive framework for understanding bias amplification in large language models and provides empirical evidence using GPT-2, several limitations must be acknowledged. First, the scope of our experiments is restricted to political bias in the context of U.S. media. Additionally, our experiments were conducted using GPT-2, a relatively smaller model compared to state-of-the-art architectures like GPT-4 or LLaMA-2. Future research should extend our empirical approach to other contexts and larger LLMs.\nAnother limitation lies in our choice of mitigation strategies. While Preservation and Accumulation show promise in reducing bias amplification, their computational cost and scalability must be considered. Moreover, the mitigation strategies were tested primarily in the context of synthetic data generation, and their efficacy in real-world deployments requires further investigation."}, {"title": "8 Ethical Considerations", "content": "This study addresses bias amplification in LLMs, a technical phenomenon with profound ethical implications, particularly regarding fairness and the integrity of AI systems. The risk of bias amplification is especially concerning in systems that are iteratively trained on synthetic data, as it can lead to unintended distortions in model outputs. These distortions may propagate harmful biases, influencing downstream tasks in areas such as automated content generation, decision-making, and user interactions with AI.\nFrom an ethical standpoint, this work underlines the need for transparency in the training and deployment of LLMs. Our findings demonstrate that even without biased initial datasets, iterative training can amplify subtle biases embedded within a model's architecture, thus raising concerns about accountability in models that are widely deployed in public-facing applications. This amplification can mislead users or result in models perpetuating one-sided perspectives, which could be especially problematic in sensitive domains like news summarization, policy generation, or social media content moderation.\nMoreover, the identification of distinct neural mechanisms for bias amplification and model collapse brings to light the challenges of ensuring equitable performance across all dimensions of model behavior and raises important ethical questions regarding the adequacy of current mitigation techniques, particularly in high-stakes scenarios where the cost of algorithmic bias is substantial.\nFuture research should prioritize the development of more comprehensive and domain-specific bias mitigation techniques, with a clear focus on minimizing ethical risks. Additionally, rigorous testing and validation across diverse datasets and real-world applications will be critical to ensuring that models trained using these methods do not exacerbate existing inequalities or produce harmful outcomes."}, {"title": "C Qualitative Bias Analysis Framework and Example of Bias Amplification Across Generations", "content": "We employed qualitative methods to confirm our findings in media bias. Specifically, we utilized a media bias identification framework grounded in foundational works such as Entman's framing theory (Entman, 1993) and other research on media bias detection (Rodrigo-Gin\u00e9s et al., 2024; Groeling, 2013). This framework provides a robust lens to evaluate political biases in the framing and language use of media texts (explained below). Given the nature of our data-text exclusive of visual or contextual cues like formatting-certain types of media bias commonly seen in formatted articles or televised programs (e.g., visual bias or tone) may not apply. Therefore, our focus was on the two key aspects of political bias that are particularly relevant in textual analysis:\nStory Framing and Selection Bias: This type of bias emerges when inherent leanings are found in the way topics, arguments, or narratives are structured. For instance, some aspects of reality are highlighted while others are obscured, shaping how the audience understands and interprets the events or issues at hand (Entman, 1993; Groeling, 2013). In extreme cases, opposing viewpoints are entirely excluded, leading to a one-sided representation of the issue. This selective omission restricts the audience's comprehension of the full spectrum of perspectives, resulting in a distorted portrayal of the issue (Rodrigo-Gin\u00e9s et al., 2024; Groeling, 2013). Entman described this as the selection and salience of specific facts that promote particular definitions, evaluations, and recommendations.\nLoaded Language Bias: This bias is identified through the use of charged or emotive words that signal political or ideological leanings. A common example is the difference in connotation between terms such as \"undocumented\" versus \"illegal\" immigrants. Such language choices often shape the audience's perception by evoking specific emotional responses (Rodrigo-Gin\u00e9s et al., 2024; Groeling, 2013).\nBelow is an example of GPT-2 text outputs influenced by iterative synthetic training. The original article, titled \"First Read: Why It's So Hard for Trump to Retreat on Immigration, is a political opinion piece from NBC News, a left-leaning outlet as rated by AllSides (NBC News, 2016; AllSides, 2024b). The analysis follows the qualitative framework.\n\u2022 Original Article: Why Its So Hard for Trump to Retreat on Immigration First Read is a morning briefing from Meet the Press and the NBC Political Unit on the day's most important political stories and why they matter. Why its so hard for Trump to retreat on immigration Since launching his presidential candidacy 14 months ago, Donald Trumps most consistent and uncompromising policy issue has been immigration. Indeed, it was the subject of his first general-election TV ad that started airing on Friday. Yet over the weekend, his top aides and advisers suggested that Trump might be shifting on his past position that all of the 11 million undocumented immigrants living in the United States must be deported forcibly. To be determined, is what newly minted Campaign Manager Kellyanne Conway said on CNN when asked if Trump was retreating on the deportation force he talked about during the primary season. But here's why its so hard \u2013 if not impossible \u2013 for Trump to retreat on immigration: Hes caught between his clear, unambiguous past statements and a base that might not willing to see him moderate on the issue. His past statements: Aug. 16, 2015 \"\"We're going to keep the families together, but they have to go,\"\" Trump said on NBCs Meet the Press. More Trump: \"\"We will work with them. They have to go. Chuck, we either have a country, or we don't have a country,\"\" he said. Nov. 11, 2015 You are going to have a deportation force, and you are going to do it humanely, Trump said on MSNBCs Morning Joe when asked how he would round up the nations 11 million undocumented immigrants. April 21, 2016 Look, were either going to have a country or were not going to have a country. But many people are very fine people. And I'm sure these are very, very fine people. They're going to go, and were going to create a path where we can get them into this country legally, okay? But it has to be done legally \u2013 when asked by a questioner at a Today town hall that persons undocumented relatives would have to be deported if Trump becomes president. Trump cant ignore a base that has cheered his uncompromising immigration position And then there are the Trump supporters who've cheered the GOP presidential nominee for being so uncompromising on immigration. Classification Probability: 0.9946 for left-leaning, 0.0051 for center-leaning, 0.0002 for right-leaning"}, {"title": "Analysis of Story Framing and Selection Bias:", "content": "Omission"}]}