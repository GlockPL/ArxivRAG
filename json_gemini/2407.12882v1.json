{"title": "InstructAV: Instruction Fine-tuning Large Language Models\nfor Authorship Verification", "authors": ["Yujia Hu", "Zhiqiang Hu", "Chun-Wei Seah", "Roy Ka-Wei Lee"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency\nin a wide range of NLP tasks. However, when it comes to authorship verifi-\ncation (AV) tasks, which involve determining whether two given texts share\nthe same authorship, even advanced models like ChatGPT exhibit notable\nlimitations. This paper introduces a novel approach, termed InstructAV, for\nauthorship verification. This approach utilizes LLMs in conjunction with a\nparameter-efficient fine-tuning (PEFT) method to simultaneously improve\naccuracy and explainability. The distinctiveness of InstructAV lies in its\nability to align classification decisions with transparent and understandable\nexplanations, representing a significant progression in the field of author-\nship verification. Through comprehensive experiments conducted across\nvarious datasets, InstructAV demonstrates its state-of-the-art performance\non the AV task, offering high classification accuracy coupled with enhanced\nexplanation reliability.", "sections": [{"title": "Introduction", "content": "Authorship Verification (AV) is a task aimed at determining if two texts were written by the\nsame author, with significant implications across forensics, literature, and digital security\ndomains Halvani et al. (2019); Stamatatos (2016). Traditionally, AV relied on stylometric\nanalysis, utilizing linguistic and stylistic features, such as word and sentence lengths, and\nfunction word frequencies, to distinguish between authors Seroussi et al. (2011); Bevendorff\net al. (2019). However, the advent of machine learning, particularly deep learning models\nlike BERT Devlin et al. (2018) and ROBERTa Jones et al. (2022), has revolutionized this\nfield. These modern approaches, leveraging complex patterns in text, have shown superior\nperformance over conventional stylometric techniques in identifying authorship ?Saedi\n& Dras (2021); Konstantinou et al. (2022); Valdez-Valenzuela et al. (2023). This paradigm\nshift underscores a significant evolution in AV methodologies, emphasizing the increasing\neffectiveness of machine learning in text analysis Zheng & Jin (2023).\nWhile current AV models have made notable advancements, they predominantly focus on\nbinary classification and notably lack in providing explanatory insights. Explainability is not\nonly of academic interest. It is fundamental to understanding a model's decision-making\nlogic, and it also enhances trust and reliability in the model's output. Additionally, the\nlack of clear explanations makes it hard to find and fix any biases that may be hidden\ninside these models, creating a significant problem for ensuring they are fair and unbiased.\nTherefore, it's critical for AI models not only to be accurate but also to provide transparency\nand interpretability.\nThis paper presents the InstructAV framework, an innovative approach tailored for AV\ntasks. Unlike existing models, InstructAV is designed to accurately verify authorship across\ntexts while concurrently furnishing detailed linguistic explanations for its determinations."}, {"title": "Related Work", "content": "In the last two decades, AV has evolved significantly, transitioning from traditional methods\nfocusing on linguistic features like spelling and style to machine learning techniques (Boen-ninghoff et al., 2019). However, traditional machine learning, such as support vector\nmachines, showed limited effectiveness (Konstantinou et al., 2022). Recent advancements\ninvolve contextual embeddings from language models like BERT, T5, and MPNET (Devlin\net al., 2018; Raffel et al., 2020; Song et al., 2020), and further studies have explored graph\nconvolutional networks and BiLSTM with attention mechanisms (Valdez-Valenzuela et al.,\n2023; Sun et al., 2023). Moreover, Huang et al. (2024) explored different representations of\nauthorship to verify their effectiveness in encoding writing styles. They concluded that\nauthorship representations might be expected to remain robust against certain types of data\nshifts.\nPrevious neural network-based methods, such as BERT and MPNET, have been crucial\nin advancing classification tasks, including AV. However, these models inherently lack\nmechanisms to elucidate their decision-making processes, a critical gap as the demand for\nexplainability in AI grows Hung et al. (2023). Addressing this, Hung et al. (2023) intro-\nduced PromptAV, an innovative technique that leverages the capabilities of large language\nmodels (LLMs) for AV, employing step-by-step stylometric explanation prompts to en-\nhance interpretability. Their findings demonstrate that PromptAV significantly outperforms\ntraditional approaches like chain of thought (CoT) prompting (Wei et al., 2022) and PS+\nprompting (Wang et al., 2023) in both accuracy and interpretability, marking a noteworthy\nadvancement in the application of LLMs to AV tasks. Huang et al. (2024) utilized LLMs\nwith the Linguistically Informed Prompting (LIP) technique for authorship verification,\nrevealing that even without domain-specific fine-tuning, the LIP method guides LLMs to\nsatisfactory performance.\nDespite the progress, these methods' reliance on a few-shot demonstration model poses\nchallenges in ensuring the consistency and relevance of their explanations. This limitation\nunderscores a critical need for a more robust solution that can deliver both precise classifica-\ntion and meaningful explanations across a broader range of scenarios. To address this need,\nwe introduce the InstructAV framework, which adopts an instruction fine-tuning approach\nto significantly enhance the classification accuracy and explanation quality in AV tasks. By\nrefining the model's ability to generate relevant and consistent explanations, InstructAV\nnot only builds on the foundation established by PromptAV but also addresses its primary\nlimitations, offering a comprehensive solution that advances the field of explainable AV."}, {"title": "2.1 Authorship Verification", "content": "In the last two decades, AV has evolved significantly, transitioning from traditional methods\nfocusing on linguistic features like spelling and style to machine learning techniques (Boen-ninghoff et al., 2019). However, traditional machine learning, such as support vector\nmachines, showed limited effectiveness (Konstantinou et al., 2022). Recent advancements\ninvolve contextual embeddings from language models like BERT, T5, and MPNET (Devlin\net al., 2018; Raffel et al., 2020; Song et al., 2020), and further studies have explored graph\nconvolutional networks and BiLSTM with attention mechanisms (Valdez-Valenzuela et al.,\n2023; Sun et al., 2023). Moreover, Huang et al. (2024) explored different representations of\nauthorship to verify their effectiveness in encoding writing styles. They concluded that\nauthorship representations might be expected to remain robust against certain types of data\nshifts.\nPrevious neural network-based methods, such as BERT and MPNET, have been crucial\nin advancing classification tasks, including AV. However, these models inherently lack\nmechanisms to elucidate their decision-making processes, a critical gap as the demand for\nexplainability in AI grows Hung et al. (2023). Addressing this, Hung et al. (2023) intro-\nduced PromptAV, an innovative technique that leverages the capabilities of large language\nmodels (LLMs) for AV, employing step-by-step stylometric explanation prompts to en-\nhance interpretability. Their findings demonstrate that PromptAV significantly outperforms\ntraditional approaches like chain of thought (CoT) prompting (Wei et al., 2022) and PS+\nprompting (Wang et al., 2023) in both accuracy and interpretability, marking a noteworthy\nadvancement in the application of LLMs to AV tasks. Huang et al. (2024) utilized LLMs\nwith the Linguistically Informed Prompting (LIP) technique for authorship verification,\nrevealing that even without domain-specific fine-tuning, the LIP method guides LLMs to\nsatisfactory performance.\nDespite the progress, these methods' reliance on a few-shot demonstration model poses\nchallenges in ensuring the consistency and relevance of their explanations. This limitation\nunderscores a critical need for a more robust solution that can deliver both precise classifica-\ntion and meaningful explanations across a broader range of scenarios. To address this need,\nwe introduce the InstructAV framework, which adopts an instruction fine-tuning approach\nto significantly enhance the classification accuracy and explanation quality in AV tasks. By\nrefining the model's ability to generate relevant and consistent explanations, InstructAV\nnot only builds on the foundation established by PromptAV but also addresses its primary\nlimitations, offering a comprehensive solution that advances the field of explainable AV."}, {"title": "Methodology", "content": "The overview of the InstructAV framework, as depicted in Figure 1, comprises three primary\nsteps: data collection, consistency verification, and the fine-tuning of LLMs using the LORA\nmethod (Hu et al., 2021). Initially, the framework focuses on the aggregation of explanatory\ndata for AV samples. This approach uses the binary classification labels available in existing\nAV datasets. Following this, a strict quality check is implemented, aimed at verifying the\nalignment and consistency of the explanations with the corresponding classification labels.\nThe final stage involves the synthesis of instruction-tuning data, which is a fusion of the\ngathered classification labels and their associated explanations. This composite data then\nserves as the foundation for fine-tuning LLMs in conjunction with the LoRA adaptation"}, {"title": "2.2 Parameter-Efficient Fine-Tuning of Large Language Models", "content": "The emergence of LLMs like GPT-3 represents a significant advancement in AI, expanding\nthe capabilities of machine learning technologies. However, deploying these models poses\nchallenges due to their high computational and memory demands. Parameter-Efficient\nFine-Tuning (PEFT) addresses these issues. It selectively adjusts a small subset of model\nparameters, customizing it for specific downstream tasks with minimal resource usage Ding\net al. (2023); Liu et al. (2022).\nWithin the PEFT paradigm, adapters are notable. These compact modules integrate into\npre-trained networks, providing a resource-efficient way to customize models for specific\ntasks or datasets (Hu et al., 2023). By enabling targeted specialization without extensive\nretraining, adapters expand the accessibility of LLMs for various applications, enhancing\ntheir utility and flexibility.\nThe Low-Rank Adaptation (LoRA) adapter, a notable PEFT method, fine-tunes model\nweight matrices with low-rank modifications to boost task-specific trainability while retain-ing model strengths (Hu et al., 2021). Another method, Prefix-Tuning, adds task-specific\nparameters to transformer layers for alignment (Li & Liang, 2021). These methods merge\nadaptability with computational efficiency, enabling advanced LLMs to operate in resource-limited settings. LoRA, especially, preserves LLMs' general capabilities while enabling\ntask-specific fine-tuning, marking a significant advancement in the field (Ding et al., 2023).\nIn our study, we outperform AV tasks with LoRA fine-tuning."}, {"title": "3.1 Explanation Data Collection", "content": "To augment the explanatory capabilities of the InstructAV model, particularly in generating\ndependable explanations for AV predictions, we initiated a comprehensive collection of\nexplanations using ChatGPT, with the crucial step of informing ChatGPT about the classifi-cation labels beforehand. This process involved three datasets widely used in AV studies:\nthe IMDB dataset (Seroussi et al., 2014), the Twitter dataset (Schwartz et al., 2013), and the\nYelp Reviews dataset\u00b2. This selection strategically covers various dimensions of textual\ndata, thereby ensuring a diverse and comprehensive analysis.\nThe IMDB dataset, characterized by its longer text length (averaging 303 words), exemplifies\nlong-form content. In contrast, the Twitter dataset, with an average text length of 16\nwords, epitomizes short-form content. The Yelp Reviews dataset, averaging 154 words per\ntext, represents medium-length content. These datasets, initially curated for authorship\nattribution tasks, were adapted for our study. We extracted 11,000 samples from each dataset\nto facilitate a robust AV evaluation. Each sample comprises two texts, either written by the\nsame author or by different authors.\nFor generating explanations, we employed ChatGPT with true classification labels and\nfew-shot prompts, focusing on 11 linguistic features for each AV sample. These features,\nidentified in the research by (Boenninghoff et al., 2019), are crucial for analyzing writing\nstyles in textual content. The linguistic features generated by known-label ChatGPT were\nrecorded as labels for the AV explanations in our dataset if they accurately predict and meet\nthe consistency check.\nTo link classification labels closely with linguistic explanations during the data collection\nphase, we crafted prompts that incorporate the classification labels. For example, a prompt\nmight state \"The following Text1 and Text2 are written by different authors. Please analyze their\nwriting styles and explain why they are written by different authors.\" to guide the explanation\nprocess. A detailed example of the explanation generation prompt is shown in Appendix B.2\nTable 8. These prompts enhance the relevance of the collected explanations, thereby improv-ing the explanatory capacity of the InstructAV model."}, {"title": "Consistency Verification", "content": "Alignment between classification labels and their linguistic\nexplanations is essential for explanation data integrity and trustworthiness. Models like\nknown-label ChatGPT are skilled at generating explanations but face challenges in align\nexplanations with classification labels. For instance, despite being informed that \"The\nfollowing Text1 and Text2 are written by the same author,\u201d ChatGPT might incorrectly respond\nwith, \"The correct answer is no.\u201d Instances like this create a mismatch between explanations\nand classification labels, reducing trust in the model's explanations. To enhance user\ntrust in automated decisions, it is important to guarantee the consistency and reliability\nof both classifications and explanations. As shown in Figure 1, we employ a consistency\nverification method to verify the alignment between the model's analytical explanations and\nits classification decisions. We have instituted a comprehensive verification process to ensure\nthe explanations are consistent with the classification. This process leverages demonstration\ntemplates that inherently guide the model to incorporate specific expressions\u2014such as\n\u2018same/similarities' or \u2018different/differences'\u2014into its outputs. During the consistency verification\nstage, keyword searching was performed on terms within the generated text. Matching\nthe phrases 'written by the same author' (resp. \u2018written by different authors') with classification\nlabels allows us to assess the quality of ChatGPT's linguistic explanations.\nThe key to this process is the construction of instruction-tuning data, which serves a dual\npurpose: facilitating AV classification prediction and supporting the generation of explana-tions. An example of the instruction-tuning data is shown in Appendix B.3 Table 9. This"}, {"title": "3.2 Fine-tuning with LoRA", "content": "Adapting LLMs for AV tasks can be a demanding process, particularly due to the significant\ncomputational resources and extensive labeled data typically required for fine-tuning. To\naddress this challenge, we have incorporated a PEFT method known as LoRA (Hu et al.,\n2021) into our approach for adapting LLMs to AV tasks.\nLORA presents a novel approach to updating weight matrix parameters. It decomposes\nthe matrix into two low-rank matrices, reducing trainable parameters while processing\nhigh-dimensional matrices effectively. During the forward pass, LoRA matrices compute\nrank-decomposed weights, used in attention or feed-forward activations. Importantly, this\nmethod keeps the LLM's core parameters unchanged, enabling adaptation to new data\ndistributions or tasks without overhauling the model completely.\nThe LoRA architecture is encapsulated in the following equation:\n$h = W_ox + \\Delta W_ox = W_ox + BAx$\nwhere $W_o \\in \\mathbb{R}^{d \\times d}$ is a pre-trained weight matrix in LLMs, which is associated with two\nadjustable low-rank matrices, $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times d}$, d corresponds to the hidden\ndimension of the attention layers, and r is the adaptation rank, which is selected such\nthat $r < d$. Within the training phase, $W_o$ remains frozen, exempt from gradient updates,\nwhereas A and B are dynamic, containing the parameters subject to training."}, {"title": "4 Experiments", "content": "The InstructAV framework evaluation utilized three distinct Authorship Verifica-\ntion (AV) datasets: IMDB62 (Seroussi et al., 2014), Twitter (Schwartz et al., 2013), and Yelp\nReviews 3, chosen for diversity and relevance.\nTo evaluate the AV classification and explanation tasks performance, we constructed two\ndistinct types of dataset settings, each incorporating varying levels of information:\n1.  Classification: This dataset involves integrating a question alongside two texts as the\ninput and employs the LoRA method to fine-tune the model for classification tasks.\nThe expected output is a straightforward binary classification indicating whether\nthe two texts are written by the same author, formatted as \u201cThe correct answer is\nyes/no\".\n2.  Classification and Explanation: In this setting, we augment the classification data\nwith linguistic analysis to empower the model to generate robust explanations for\nthe AV predictions. The LLMs are fine-tuned to not only predict the classification\nlabels, but also provide an analysis of eleven linguistic features of the two texts.\nThis added layer of analysis offers a reasoned explanation behind the classification\ndecision, thereby enhancing the model's interpretability and reliability.\nFor the Classification setting, We randomly sampled 11,000 balanced samples from the\nIMDB62, Twitter, and Yelp datasets, respectively. The data was then divided into a training\nset comprising 10,000 samples and a test set comprising 1,000 samples, with both sets\nmaintaining balanced class distributions.\nFor the Classification and Explanation setting, we initially sampled 20,000 examples from each\ndataset and used the GPT-3.5-turbo API for linguistic analysis. To ensure the high quality of\nthe generated explanations, samples with incorrect linguistic analysis were dropped during\nthe initial consistency verification phase. For a comprehensive evaluation, balanced subsets\nconsisting of 10,000 training samples and 1,000 testing samples were randomly selected\nfrom the verified examples within each subset. These samples formed instruction-tuning\ndata, including text and corresponding linguistic explanations (see Section 3.1). Details\nregarding dataset statistics and characteristics, including the splits between training and\ntest sets, are presented in Table 1.\nThese two dataset settings enable a thorough investigation into how the addition of explana-\ntory components influences the performance of the classification task, providing insights\ninto the efficacy and adaptability of the InstructAV framework.\nBaselines. For the Classification task, our baseline models comprised BERT (Devlin et al.,\n2018) and its variants, DistilBERT (Sanh et al., 2019), and AlBERT (Lan et al., 2019). These\nmodels were selected for their widespread usage and proven effectiveness in AV classifi-\ncation tasks (Brad et al., 2021; Tyo et al., 2022; Fabien et al., 2020). However, these models\nare not inherently designed for text generation tasks. Another baseline related to LLMs\nis the application of the LIP method proposed by Huang et al. (2024) on GPT-4-turbo,\nLLAMA-2-70B, and Mistral-7B.\nFor tasks involving Classification and Explanation, autoregressive models like GPT, which\ngenerate text sequences based on preceding text, are suitable. We adopted the PromptAV\nmethodology on GPT-3.5-Turbo as a baseline for the ChatGPT model, as proposed by (Hung\net al., 2023). Although PromptAV may be slightly less effective than BERT in classification\naccuracy, it offers advantages such as bypassing extensive training and generating explana-\ntions. Additionally, we integrated PromptAV, using GPT-4 as the foundational model, to\nestablish a more competitive baseline."}, {"title": "4.1 Experiment Settings", "content": "The InstructAV framework evaluation utilized three distinct Authorship Verifica-\ntion (AV) datasets: IMDB62 (Seroussi et al., 2014), Twitter (Schwartz et al., 2013), and Yelp\nReviews 3, chosen for diversity and relevance.\nTo evaluate the AV classification and explanation tasks performance, we constructed two\ndistinct types of dataset settings, each incorporating varying levels of information:\n1.  Classification: This dataset involves integrating a question alongside two texts as the\ninput and employs the LoRA method to fine-tune the model for classification tasks.\nThe expected output is a straightforward binary classification indicating whether\nthe two texts are written by the same author, formatted as \u201cThe correct answer is\nyes/no\".\n2.  Classification and Explanation: In this setting, we augment the classification data\nwith linguistic analysis to empower the model to generate robust explanations for\nthe AV predictions. The LLMs are fine-tuned to not only predict the classification\nlabels, but also provide an analysis of eleven linguistic features of the two texts.\nThis added layer of analysis offers a reasoned explanation behind the classification\ndecision, thereby enhancing the model's interpretability and reliability.\nFor the Classification setting, We randomly sampled 11,000 balanced samples from the\nIMDB62, Twitter, and Yelp datasets, respectively. The data was then divided into a training\nset comprising 10,000 samples and a test set comprising 1,000 samples, with both sets\nmaintaining balanced class distributions.\nFor the Classification and Explanation setting, we initially sampled 20,000 examples from each\ndataset and used the GPT-3.5-turbo API for linguistic analysis. To ensure the high quality of\nthe generated explanations, samples with incorrect linguistic analysis were dropped during\nthe initial consistency verification phase. For a comprehensive evaluation, balanced subsets\nconsisting of 10,000 training samples and 1,000 testing samples were randomly selected\nfrom the verified examples within each subset. These samples formed instruction-tuning\ndata, including text and corresponding linguistic explanations (see Section 3.1). Details\nregarding dataset statistics and characteristics, including the splits between training and\ntest sets, are presented in Table 1.\nThese two dataset settings enable a thorough investigation into how the addition of explana-\ntory components influences the performance of the classification task, providing insights\ninto the efficacy and adaptability of the InstructAV framework.\nBaselines. For the Classification task, our baseline models comprised BERT (Devlin et al.,\n2018) and its variants, DistilBERT (Sanh et al., 2019), and AlBERT (Lan et al., 2019). These\nmodels were selected for their widespread usage and proven effectiveness in AV classifi-\ncation tasks (Brad et al., 2021; Tyo et al., 2022; Fabien et al., 2020). However, these models\nare not inherently designed for text generation tasks. Another baseline related to LLMs\nis the application of the LIP method proposed by Huang et al. (2024) on GPT-4-turbo,\nLLAMA-2-70B, and Mistral-7B.\nFor tasks involving Classification and Explanation, autoregressive models like GPT, which\ngenerate text sequences based on preceding text, are suitable. We adopted the PromptAV\nmethodology on GPT-3.5-Turbo as a baseline for the ChatGPT model, as proposed by (Hung\net al., 2023). Although PromptAV may be slightly less effective than BERT in classification\naccuracy, it offers advantages such as bypassing extensive training and generating explana-\ntions. Additionally, we integrated PromptAV, using GPT-4 as the foundational model, to\nestablish a more competitive baseline."}, {"title": "4.3 Experiment Results", "content": "Classification Results. We evaluate the InstructAV framework for AV tasks, utilizing both\nclassification and explanatory dataset settings. The primary goal is to explore how linguistic\nanalysis can improve the model's performance in AV classification tasks. The results,\ndetailed in Table 2, include the accuracy of InstructAV with various LLMs and baseline\nmodels. A key finding is that InstructAV, particularly when paired with LLaMA-1-7B and\nLLaMA-2-7B, outperforms baseline models in all datasets when using only classification\ndata. Notably, with LLaMA-2-7B, InstructAV achieves a 25.2% improvement over the highest-performing baseline, BERT, on the IMDB dataset.\nIt's imperative to underline that in our experimental setup, models were evaluated on their\nability to concurrently generate classification predictions and conduct linguistic analysis\nwithout incorporating explanation data in the input. This methodology ensured fair com-parison with approaches that focus only on classification. Our results clearly demonstrate\nthat all variants of the InstructAV framework, utilizing different LLMs, significantly benefit\nfrom training on explanatory labels. Notably, InstructAV with LLaMA-2-7B showcased a\nremarkable 27.1% improvement in classification accuracy over the PromptAV approach\nusing 2-shot prompts based on GPT-3.5. Furthermore, when compared to PromptAV-2shot\nemploying GPT-4, InstructAV exhibited superior performance across all three evaluated\ndatasets. These findings highlight the substantial benefits of incorporating explanatory\ntraining, indicating that providing InstructAV with the dual function of AV classification and\ngenerating linguistic explanations significantly boosts its classification precision.\nAutomatic Evaluation Results on Explanations. Our InstructAV framework demonstrates\nthat training in explanation labels can significantly improve AV classification performance.\nAdditionally, we aim to assess whether InstructAV can generate high-quality explanations.\nFor this purpose, we selected PromptAV-2shot (GPT-3.5), the best-performing variation\nof the framework, alongside GPT-4, to benchmark against all variations of InstructAV. we\nsubjected them to both automatic and human evaluations focused on the quality of their\ngenerated linguistic explanations. The results of the automatic evaluation are presented\nin Table 3. These results reveal that InstructAV consistently surpasses PromptAV models\n(GPT-3.5 and GPT-4) across all datasets and all evaluation metrics. Notably, ROUGE-1 and\nROUGE-2 scores highlight InstructAV's superior performance in achieving content overlap\nat both the word and phrase levels. Moreover, the ROUGE-L metric indicates that InstructAV\nis more proficient in maintaining sentence-level structure and fluency. Furthermore, the\nBERT_Score supports the observation that the explanations generated by InstructAV are\nsemantically closer to the explanation labels. This comprehensive evaluation underscores In-structAV's capability not only in improving AV classification accuracy but also in generating\nlinguistically coherent and contextually relevant explanations.\nHuman Evaluation Results on Explanations. To comprehensively evaluate the generated\nexplanations and to assess the quality of the linguistic analysis produced by ChatGPT,\nwhich serves as our explanation labels, we have conducted a human evaluation using four\nkey metrics: Coverage, Relevance, Reasonableness, and Persuasiveness. The results of\nthis human evaluation are presented in Table 4. Our findings from this human evaluation\nprocess show that our explanation labels achieves the highest scores. This result validates\nour methodological choice of using explanations generated by known-label ChatGPT as both\na source for training data and a benchmark for explanation labels in our testing scenarios.\nImportantly, the results also reveal that InstructAV, particularly with the LLaMA-2-7B model,\nnot only surpasses the performance of PromptAV-2shot models (GPT-3.5 and GPT-4) but\nalso attains a level of explanation quality that is comparable to known-label ChatGPT. This\noutcome is significant as it demonstrates that InstructAV can produce explanations that are\nnot only accurate but also contextually relevant, logically sound, and convincing to human\nevaluators. Such a capability is essential for applications where understanding the rationale\nbehind model predictions is as important as the predictions themselves.\nCorrelation between Explanation and Classification. To explore the relationship between\nexplanation quality and classification accuracy, we have selected two distinct subsets of\nInstructAV samples: the top 25% with the highest average human evaluation scores and\nthe bottom 25% with the lowest average human evaluation scores. We then calculated the"}, {"title": "5 Ablation Study", "content": "Ablation experiments were conducted to investigate potential data contamination issues,\nspecifically whether the data might have been included in the training corpora of LLMs.\nWe performed 0-shot, 2-shot, 4-shot, and 8-shot in-context tests on each dataset using the\noriginal, untuned LLaMA-2-7B, LLaMA-1-7B, and OPT-6.7B models. Each experiment was\nreplicated three times, with the mean results presented in Table 6. The results indicate that\nthe original models struggle to perform AV tests effectively on our dataset, especially in\nthe 0-shot setting where the outputs are highly random. Both LLaMA-1-7B and OPT-6.7B\nfailed to make correct judgments, and LLaMA-2-7B showed weak judgment capabilities.\nConsequently, we postulate that it is unlikely that our dataset was included in the training\ndata of the original models."}, {"title": "6 Conclusion", "content": "This research presents InstructAV, an innovative approach to AV tasks that leverages LLMs\nwith a PEFT method. Our study establishes InstructAV as a significant advancement in\nthe AV domain, showcasing its ability to enhance classification accuracy and provide clear\nand coherent explanations for its decisions. The contributions of this paper, including the\ndevelopment of the InstructAV framework, the creation of three instruction-tuning datasets\nwith reliable linguistic explanations, and the demonstration of the framework's effectiveness\nthrough both automated and human evaluations, mark a crucial progress in AV research.\nInstructAV, with its dual priority on high accuracy and the ability to provide high quality\nexplanations, positions it as a state-of-the-art AV solution."}, {"title": "A Limitations", "content": "The InstructAV framework, while achieving state-of-the-art performance in AV classification\nand explanation tasks, does face a notable limitation in its current form. The human\nevaluations of model-generated explanations differ from typical evaluations in other tasks.\nSpecifically, our evaluations are intended for high-level verification to ensure that the\nexplanations produced by ChatGPT are coherent and align with the text, not as a full\nassessment of authorial features. Author analysis involves critical frequency and nuanced\nfeatures better captured by computational methods then humans (Stamatatos et al., 1999).\nTheoretically, an ideal human evaluation would ask human participants to justify why\ntwo texts are written by the same author, but such research is costly and problematic, as\nhumans do not perform well in tasks that require analysis of writing styles. Considering\nthese limitations, we consider ChatGPT-generated explanations to be a reasonable and\ncost-effective alternative. Future work will consider a more suitable evaluation method that\nincorporates computational features.\nOn the other hand, when tasked with generating both AV classification predictions and\nlinguistic explanations, InstructAV is constrained to produce a maximum of 512 new tokens.\nThis restriction can lead to significantly longer inference times. For example, generating\nexplanations for 1,000 samples in a long-text dataset like IMDB can take around 4-5 days,\nwhile in shorter-text datasets such as Twitter, this time is reduced to approximately 2 days.\nAddressing this challenge, future work will focus on developing methods to enhance the\nefficiency of the inference process within InstructAV, particularly in the context of generating\nexplanations."}, {"title": "B Appendix", "content": "We have included randomly selected examples to showcase the capabilities of InstructAV in\ngenerating both classification predictions and language feature-based explanations. One\nsuch example from the IMDB dataset is detailed in Table 7. This example demonstrates\nhow InstructAV first provides a classification prediction and then follows it with detailed\nexplanations anchored in specific language features pertinent to the classification.\nThese examples highlight InstructAV's proficiency in delivering accurate classifications\npaired with clear and credible explanations. Notably, the explanations are tailored to\neach specific sample, rather than being generic or broad. This customization enhances the\nexplanations' effectiveness, as they are directly correlated with the content of the original\ntext. For instance, the model explicates features such as writing style and use of expressions\nby incorporating words and phrases directly from the text in question. This approach not\nonly bolsters the relevance of the explanations but also their trustworthiness, providing\nusers with insightful and contextually grounded rationale behind each classification decision\nmade by InstructAV."}, {"title": "B.1 Case Study for InstructAV", "content": "We have included randomly selected examples to showcase the capabilities of InstructAV in\ngenerating both classification predictions and language feature-based explanations. One\nsuch example from the IMDB dataset is detailed in Table 7. This example demonstrates\nhow InstructAV first provides a classification prediction and then follows it with detailed\nexplanations anchored in specific language features pertinent to the classification.\nThese examples highlight InstructAV's proficiency in delivering accurate classifications\npaired with clear and credible explanations. Notably, the explanations are tailored to\neach specific sample, rather than being generic or broad. This customization enhances the\nexplanations' effectiveness, as they are directly correlated with the content of the original\ntext. For instance, the model explicates features such as writing style and use of expressions\nby incorporating words and phrases directly from the text in question. This approach not\nonly bolsters the relevance of the explanations but also their trustworthiness, providing\nusers with insightful and contextually grounded rationale behind each classification decision\nmade by InstructAV."}, {"title": "B.2 Prompt Templates", "content": null}, {"title": "B.3 Collected Datasets", "content": null}]}