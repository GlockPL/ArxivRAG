{"title": "HES-UNet: A U-Net for Hepatic Echinococcosis\nLesion Segmentation", "authors": ["Jiayan Chen", "Kai Li", "Zhanjin Wang", "Zhan Wang", "Jianqiang Huang"], "abstract": "Hepatic echinococcosis (HE) is a prevalent disease\nin economically underdeveloped pastoral areas, where adequate\nmedical resources are usually lacking. Existing methods often\nignore multi-scale feature fusion or focus only on feature fusion\nbetween adjacent levels, which may lead to insufficient feature\nfusion. To address these issues, we propose HES-UNet, an efficient\nand accurate model for HE lesion segmentation. This model\ncombines convolutional layers and attention modules to capture\nlocal and global features. During downsampling, the multi-\ndirectional downsampling block (MDB) is employed to integrate\nhigh-frequency and low-frequency features, effectively extracting\nimage details. The multi-scale aggregation block (MAB) aggre-\ngates multi-scale feature information. In contrast, the multi-scale\nupsampling Block (MUB) learns highly abstract features and\nsupplies this information to the skip connection module to fuse\nmulti-scale features. Due to the distinct regional characteristics of\nHE, there is currently no publicly available high-quality dataset\nfor training our model. We collected CT slice data from 268\npatients at a certain hospital to train and evaluate the model.\nThe experimental results show that HES-UNet achieves state-\nof-the-art performance on our dataset, achieving an overall\nDice Similarity Coefficient (DSC) of 89.21%, which is 1.09%\nhigher than that of TransUNet. The project page is available at\nhttps://chenjiayan-qhu.github.io/HES-UNet-page/.", "sections": [{"title": "I. INTRODUCTION", "content": "Hepatic echinococcosis (HE) is a severe zoonotic disease\ncaused by parasitic infection, typically occurring in remote\npastoral or high-altitude regions with harsh climate conditions\nand limited medical resources [1]. Computed tomography (CT)\nis generally the primary means of diagnosing HE [2]; however,\nmanual segmentation of lesions is time-consuming and reliant\non subjective judgments based on physician experience, mak-\ning it difficult to replicate. Early medical image segmentation\nmethods typically relied on traditional edge detection [3]\nand region-growing algorithms [4]. However, these traditional\nmethods usually depend on hand-crafted features and are\nless robust to variations in lesion shape and size, making\nit challenging to handle the irregular shapes and blurred\nboundaries of HE lesions.\nIn recent years, with the rapid development of deep learning,\nconvolutional neural networks (CNNs) [5] have shown great\npotential in the field of medical image segmentation. U-Net\n[6] is a widely used network architecture. However, due to\nthe local receptive field inherent in convolution operations,\nthe model struggles to effectively capture global image fea-\ntures. Additionally, the max-pooling operation still leads to\nthe loss of fine details, which is critical for abdominal CT\nimages with unclear boundaries, limiting the model's ability to\ncapture subtle features. Trans-UNet [7] combines Transformer\n[8] and CNN, utilizing self-attention mechanisms to capture\nglobal features, but this significantly increases the number\nof parameters, making the model more challenging to train.\nAdditionally, training such a complex model requires a large\namount of annotated data, which is often difficult to obtain\nin medical applications. Swin-UNet [9] proposes a U-shaped\narchitecture using pure Transformers without relying on con-\nvolutions for feature extraction. While this design excels at\nmodeling global features, it reduces the model's ability to learn\nlocal features, especially when dealing with fine boundaries\nand texture details in medical images. Furthermore, simplistic\nskip connections fail to effectively aggregate features across\nhierarchical levels.\nTherefore, to address the aforementioned issues, we propose\nHES-UNet, an efficient and accurate segmentation model for\nHE that precisely delineates lesion areas to assist physicians\nin disease diagnosis while achieving advanced accuracy. This\nmodel is composed of four main components: a multi-scale\nfeature integration encoder (MFSI encoder), a multi-scale\nglobal feature filtering module (MGF module), a progressive\nfusion decoder (PF decoder), and a deep supervision module\n(DS module). In the encoder section, we replace the traditional\nmax-pooling in the U-Net with an MDB, which effectively\npreserves the multi-frequency features within the image. Next,\nwe use the MAB module to aggregate global feature represen-\ntations. Following this, we apply the MUB to extract multi-\nscale global features, which selectively enhances features at\ndifferent scales. Unlike traditional skip connections between\nadjacent layers, MUB enables global adjustment across all\nfeature levels. Finally, the PF decoder generates the prediction."}, {"title": "II. RELATED WORK", "content": "Designing a high-performance network architecture has\nbeen one of the most important research directions in the\nfield of computer vision. Since the introduction of CNNs,\nthey have been widely applied in medical image segmentation.\nAmong them, U-Net [6] and its variants (such as 3D U-Net\n[10], V-Net [11] and U-Net++ [12]) have become the most\ninfluential models due to their symmetric encoder-decoder\nstructure, which enables pixel-level prediction through multi-\nscale feature extraction and feature fusion via skip connec-\ntions. However, traditional CNNs are limited by the nature of\nconvolutional kernels, which can only extract local feature in-\nformation from images and are insufficient for capturing long-\nrange dependencies and global information [13]. Additionally,\nsimply using max-pooling operations during downsampling\nmay lead to the loss of detailed information, reducing the\nsegmentation accuracy of lesion boundaries [14]."}, {"title": "III. METHODS", "content": "HE lesion segmentation typically involves the following\nchallenges: (1) CT images often contain noise and have low\ncontrast; (2) HE can be classified into cystic echinococcosis\n(CE) and alveolar echinococcosis (AE), and the lesions of\nthese two types differ significantly, making it challenging\nfor the model to simultaneously learn both the common and\ndistinct features. (3) The death of the parasite can result\nin calcifications in the lesion area, which appear as high-\ndensity, well-defined white spots in the image and can be\neasily confused with bones [16]. Based on these challenges,\nwe aim to retain more abstract and rich features and introduce\nglobal feature capture."}, {"title": "A. Overall Architecture", "content": "Given a CT image $X \\in \\mathbb{R}^{C\\times H\\times W}$, where C denotes the\nnumber of channels, and with the input being a grayscale im-\nage, C = 1. H = 512 and W = 512 represent the height and\nwidth of the image, respectively. Our objective is to segment\nechinococcosis lesions from the CT image X. Specifically,\nwe propose a novel encoder-decoder model named HES-UNet\nfor segmenting echinococcosis lesions. This model consists\nof four main components: a multi-scale feature integration\nencoder (MFSI encoder), a multi-scale global feature filtering\nmodule (MGF module), a progressive fusion decoder (PF\ndecoder), and a deep supervision module (DS module), as\nshown in Fig. 1.\nThe overall pipeline of HES-UNet can be summarized\nas follows: first, the MFSI encoder is employed to extract\nfeatures from the CT image X, generating a set of multi-\nscale features ${E_i \\in \\mathbb{R}^{(32 \\times 2^{i-1}) \\times \\frac{H}{2^{i-1}} \\times \\frac{W}{2^{i-1}}} | i \\in [1,5]}$,\nwhere each scale is processed by an encoder block (EB)\nas shown in Fig. 2A. Next, the set ${E_i}$ is fed into the\nmulti-scale aggregation block (MAB) to capture the global\nfeature representation $F \\in \\mathbb{R}^{(32 \\times 2^5) \\times \\frac{H}{2^5} \\times \\frac{W}{2^5}}$. Subsequently,\nF is fed into the MGF module, starting with the multi-\nscale upsampling block (MUB), resulting in multi-scale global\nfeatures ${G_i \\in \\mathbb{R}^{(32 \\times 2^i) \\times \\frac{H}{2^{i-1}} \\times \\frac{W}{2^{i-1}}} | i \\in [1,5]}$, which\nselectively emphasize different scales of features ${E_i}$. Then,\nG5 is passed into the PF decoder to estimate the prediction\nat the finest scale, $P_5 \\in \\mathbb{R}^{1 \\times \\frac{H}{2^4} \\times \\frac{W}{2^4}}$. Additionally, $E_5, P_5$,\nand G5 are used as inputs to the MGF module, which\ncontains several global attention modules (GAMs) to filter\nredundant features, thereby generating refined features ${Q_i \\in\n\\mathbb{R}^{(32 \\times 2^{i-1}) \\times \\frac{H}{2^{i-1}} \\times \\frac{W}{2^{i-1}}} | i \\in [1,5]}$. Finally, $Q_5$ is combined with\nthe output $D_6 \\in \\mathbb{R}^{(32 \\times 2^4) \\times \\frac{H}{2^5} \\times \\frac{W}{2^5}}$ from the PF decoder and\nserves as the input to the next-level decoder block, namely\n\"DB 5\", to generate the prediction $P_4 \\in \\mathbb{R}^{1 \\times \\frac{H}{2^4} \\times \\frac{W}{2^4}}$. The\nstructure of the decoder block (DB) is shown in Fig. 2B. By"}, {"title": "B. Multi-scale Feature Integration Encoder", "content": "To more effectively aggregate multi-scale feature infor-\nmation from input CT image X, we designed an encoder\nstructure named the multi-scale feature integration encoder\n(MFSI encoder), which consists of 5 encoder blocks (EBs)\nand a multi-scale aggregation block (MAB).\nEncoder blocks (EBs) capture both local and contextual\ninformation while progressively downsampling the feature\nmaps. Each EB consists of convolutional layers, attention"}, {"title": "C. Multi-scale Global Feature Filtering Module", "content": "To filter redundant features, we propose the multi-scale\nglobal feature filtering module (MGF module), which consists\nof a module called multi-scale upsampling block (MUB) for\nlearning deep abstract features of the image and 5 global\nattention modules (GAMs) for aggregating multi-scale image\nfeatures [20].\nTo enhance the model's capability to represent complex\nimage features, we propose a multi-scale upsampling block\n(MUB), which is a convolutional module incorporating pixel\nshuffle techniques (Fig. 2G). The specific architecture is shown\nin Fig. 2D. Specifically, MUB takes the global feature rep-\nresentation $F \\in \\mathbb{R}^{1024\\times16\\times16}$ as input. First, we apply two\nconsecutive GHPA modules to expand the channel dimension\nto 4096. Next, pixel shuffle is used to restore the channels\nto 1024 while doubling the width and height, achieving an\nupsampling effect. Following this, F passes through a set of\nconvolutional and bilinear interpolation layers to adjust the\nfeature map size for input to the GAM. The output of MUB,\ndenoted as ${G_i \\in \\mathbb{R}^{(32 \\times 2^i) \\times \\frac{H}{2^{i-1}} \\times \\frac{W}{2^{i-1}}} | i \\in [1,5]}$, selec-\ntively enhances features of $E_i$ at various scales. Additionally,\nto improve gradient flow, we add two extra paths: one using\nthe GHPA without pixel shuffle, and another direct connection\npath to preserve the initial information in the feature map and\nprevent information loss.\nTo enable efficient multi-scale information interaction, as\nillustrated in Fig. 2F, we introduce a series of global attention"}, {"title": "D. Progressive Fusion decoder", "content": "The progressive fusion decoder consists of 6 consecutive\ndecoder blocks (DBs), each designed as shown in Fig. 2B.\nFirst, DB6 receives G5 from the MUB. We then apply GHPA\nand bilinear interpolation to G5, with the result serving as\nthe output of DB6, denoted as D6. Simultaneously, D6 is\npassed through a 1 \u00d7 1 convolution to directly generate the\nmost abstract prediction, P5. Subsequently, P5 is passed to\nGAM5 to obtain refined features Q5 after filtering redundant\nfeatures. Then, Q5 and D6 are added element-wise and input\ninto the next Decoder Block, DB5. This process is repeated,\neventually producing the final lesion region prediction, Po. It\nis worth noting that, as in the Encoder Block (EB), in DB1,\nwe replace GHPA with a 3 \u00d7 3 convolution."}, {"title": "E. Loss Function", "content": "To address the issue of gradient vanishing during neural\nnetwork training, we adopted deep supervision to compute\nthe loss functions at different stages. The binary cross-entropy\n(BCE) loss function is commonly used for image binary\nclassification,\n$Bce(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^N [y_i log(\\hat{y}_i) + (1 - y_i) log(1 - \\hat{y}_i)],\\qquad(1)$\nwhere N represents the total number of pixels, y is the ground\ntruth, and \\hat{y} is the predicted image. However, since lesion\nregions are usually small, the Dice loss function is more\neffective in handling class imbalance,\n$Dice(y, \\hat{y}) = 1- \\frac{2\\sum_{i=1}^N y_i \\hat{y}_i}{\\sum_{i=1}^N y_i + \\sum_{i=1}^N \\hat{y}_i},\\qquad(2)$\nIn summary, our loss function can be expressed by (2) and\n(1),\n$\\mathcal{L} = \\sum_{i=0}^{5}(Dice(\\mathcal{L}_i, P_i) + Bce(\\mathcal{L}_i, P_i)) \\times \\lambda_i.\\qquad(3)$\nThe $\\lambda_i$ is set to {0.1, 0.2, 0.3, 0.4, 0.5, 1} to balance the losses\nfrom different stages of the network."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Dataset. We collected CT image data from 268 patients\nwith HE (including 137 cases of CE and 131 cases of AE) from\na certain hospital and performed preprocessing on the dataset.\nFirst, to protect patient privacy, we removed all metadata\ncontaining patient identification information. Next, the original\ngrey values (HU values) of the CT images are very broad,\nexceeding the grey levels that the human eye and display can\nperceive simultaneously. Therefore, we performed windowing\nto better highlight the details of different tissues and lesions.\nUnder the guidance of three professional physicians, we se-\nlected a window width of 150 HU and a window level of 35\nHU to standardize the HU values to the range of [-150, 35].\nThis value was chosen based on the considerations of liver\ncontours, lesion areas, and bone information. Subsequently,\nwe saved the CT slice data, using 80% for the training set and\n10% each for the testing and validation sets.\nImplementation Details. As the input consisted of\ngrayscale images, the model's input channel was set to 1, and\nthe output channel, corresponding to the number of classes,\nwas also set to 1. The number of channels in the feature\nmaps output by each EB was set to [32, 64, 128, 256, 512],\nrespectively.\nWe set the total number of epochs to 200 and employed an\nearly stopping mechanism to monitor the minimum loss on\nthe validation set. If no improvement in validation loss was\nobserved for 50 epochs, the training was halted to prevent\noverfitting. We selected AdamW [21] as the optimizer, with\nan initial learning rate of 0.001, and utilized the ReduceL-\nROnPlateau scheduler to reduce the learning rate by half if\nno improvement in validation loss was seen for 5 consecutive\nepochs. The batch size was set to 4. Additionally, since obtain-\ning sufficient data from patients with hepatic echinococcosis\nwas challenging, we worked with a relatively small dataset. We\napplied data augmentation techniques to prevent overfitting,\nincluding random image rotations, random scaling, random\nGaussian smoothing, and random Gaussian noise."}, {"title": "B. Comparison with State-of-the-art Methods", "content": "We conducted extensive experiments on the dataset to\nquantitatively compare the segmentation performance of our\nproposed HES-UNet with some existing medical image seg-\nmentation models. The results are shown in Table 1. We\nselected typical models already showing good segmentation\nperformance, including U-Net [6], U-Net++ [12], TransUNet\n[7], Swin-UNet [9], Res-UNet [22], and EGE-UNet [17].\nHES-UNet achieved improvements of 2.63%, 1.56%, and\n1.09% in the DSC metric compared to CNN-based seg-\nmentation methods (U-Net and U-Net++), the Transformer-\nbased Swin-UNet, and the hybrid architecture TransUNet,\nrespectively. Additionally, our model demonstrates advantages\nin precision and recall metrics. These experimental results\nindicate that the proposed HES-UNet performs excellently in\nliver lesion segmentation tasks, surpassing other models in seg-\nmentation accuracy. Fig. 3 displays some of the segmentation\nresults."}, {"title": "C. Ablation Analysis", "content": "To evaluate the individual contributions of the MUB, MDB,\nand MAB modules to our model's performance, we conducted\nan ablation study using the vanilla U-Net as the baseline. The\ndetailed results are presented in Table II. The findings demon-\nstrate that integrating the MUB, MDB, and MAB modules\nsignificantly enhanced the model's performance, confirming\nthe effectiveness of our proposed architecture. Specifically, the\nDSC, precision, and recall metrics achieved improvements of\n1.55%, 1.79%, and 1.25%, respectively, over the baseline."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed an efficient and accurate seg-\nmentation model, HES-UNet, for the problem of insufficient\nmulti-scale feature fusion in the segmentation of hepatic\nechinococcosis (HE) lesions. The model effectively combines\nconvolutional layers and attention mechanisms by introducing\nan MFSI encoder, MGF module, PF decoder and DS module,\nwhich can simultaneously capture local and global features\nof the image. On the CT dataset of 268 patients collected\nby us, HES-UNet demonstrated excellent performance, with\nan overall Dice similarity coefficient (DSC) of 89.21%. This\nresult is 1.09% higher than that of TransUNet. Through\ncomparative experiments and ablation analysis with various\nadvanced segmentation models, we verified the effectiveness\nof the proposed modules and the overall model in improving\nsegmentation accuracy. HES-UNet achieves remarkable results\nin HE lesion segmentation, providing a powerful auxiliary tool\nfor diagnosing HE in resource-poor areas."}]}