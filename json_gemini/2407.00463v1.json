{"title": "Open-Source Conversational AI with SpeechBrain 1.0", "authors": ["Mirco Ravanelli", "Titouan Parcollet", "Adel Moumen", "Sylvain de Langen", "Cem Subakan", "Peter Plantinga", "Yingzhi Wang", "Pooneh Mousavi", "Luca Della Libera", "Artem Ploujnikov", "Francesco Paissan", "Davide Borra", "Salah Zaiem", "Zeyu Zhao", "Shucong Zhang", "Georgios Karakasidis", "Sung-Lin Yeh", "Aku Rouhe", "Rudolf Braun", "Florian Mai", "Juan Zuluaga-Gomez", "Seyed Mahed Mousavi", "Andreas Nautsch", "Xuechen Liu", "Sangeet Sagar", "Jarod Duret", "Salima Mdhaffar", "Ga\u00eblle Laperri\u00e8re", "Renato De Mori", "Yannick Est\u00e8ve"], "abstract": "SpeechBrain\u00b9 is an open-source Conversational AI toolkit based on PyTorch, focused partic-ularly on speech processing tasks such as speech recognition, speech enhancement, speakerrecognition, text-to-speech, and much more. It promotes transparency and replicability byreleasing both the pre-trained models and the complete \"recipes\" of code and algorithmsrequired for training them. This paper presents SpeechBrain 1.0, a significant milestone inthe evolution of the toolkit, which now has over 200 recipes for speech, audio, and languageprocessing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0introduces new technologies to support diverse learning modalities, Large Language Model(LLM) integration, and advanced decoding strategies, along with novel models, tasks, andmodalities. It also includes a new benchmark repository, offering researchers a unifiedplatform for evaluating models across diverse tasks.\nKeywords: Conversational AI, open-source, speech processing, deep learning.", "sections": [{"title": "1 Introduction", "content": "Conversational AI is experiencing extraordinary progress, with Large Language Models\n(LLMs) and speech assistants rapidly evolving and becoming widely adopted in the daily\nlives of millions of users (McTear, 2021). However, this quick evolution poses a challenge to\na fundamental pillar of science: reproducibility. Replicating recent findings is often difficult\nor impossible for many researchers due to limited access to data, computational resources, or\ncode (Kapoor and Narayanan, 2023). The open-source community is making a remarkable\ncollective effort to mitigate this \u201creproducibility crisis\", yet many contributors primarily\nrelease pre-trained models only, known as open-weight (Liesenfeld and Dingemanse, 2024).\nWhile this is a step forward, it is still very common for the data and algorithms used to train\nthem to remain undisclosed. We helped address this problem by releasing SpeechBrain (Ra-vanelli et al., 2021), a PyTorch-based open-source toolkit designed for accelerating research\""}, {"title": "2 Overview of SpeechBrain", "content": "Since its launch in March 2021,\nSpeechBrain has grown rapidly and\nemerged as one of the most popular\ntoolkits for speech processing. It is\ndownloaded 2.5 million times monthly,\nused in 1,923 repositories, has 8.2k\nGitHub stars, and 148 contributors.\nDespite its constant evolution, we re-main faithful to the original design\nprinciples. We prioritized replicability\nby releasing both training recipes and\npre-trained models. Moreover, 95%\nof our recipes utilize freely available\ndata and include comprehensive train-ing logs, checkpoints, and other essen-tial information. We made Speech-Brain easy to use by providing com-prehensive documentation, examples,\nand tutorials. Our modular architecture facilitates easy integration or modification of mod-ules. We built it on PyTorch standard interfaces (e.g., torch.nn.Module, torch.optim,\ntorch.utils.data.Dataset), enabling seamless integration with the PyTorch ecosys-tem (Rouhe et al., 2022). It is released under the permissive Apache 2.0 license."}, {"title": "2.1 Architecture Overview", "content": "Training a model with SpeechBrain involves combining three components, as depicted inFigure 1: the training script, the hyperparameter file, and the data manifest files. For\nsimplicity, the training script is integrated into a single Python script. This script leverages"}, {"title": "3 Recent Developments", "content": "SpeechBrain has rapidly evolved to support a wide array of tasks. Please, refer to Table 1\nfor a complete list as of June 2024. The main improvements in SpeechBrain 1.0 include:\n\u2022 Learning Modalities: We expanded the support for emerging deep learning modal-ities. For continual learning, we implemented methods like Rehearsal, Architecture,\nand Regularization-based approaches (Della Libera et al., 2023). For interpretability,\nwe developed both post-hoc and design-based methods, including Post-hoc Interpreta-tion via Quantization (Paissan et al., 2023), Listen to Interpret (Parekh et al., 2022),\nActivation Map Thresholding (AMT) for Focal Networks (Della Libera et al., 2024),\nand Listenable Maps for Audio Classifiers (Paissan et al., 2024). We also implemented\naudio generation using standard and latent diffusion techniques, along with DiffWave\n(Kong et al., 2020) as a novel vocoder based on diffusion. Lastly, efficient fine-tuning\nstrategies have been introduced for faster inference using speech self-supervised mod-els (Zaiem et al., 2023a). We implemented wav2vec2 SSL pretraining from scratch\nas described by (Baevski et al., 2020). This enabled efficient training of a 1-billion-parameter SSL model for French on 14,000 hours of speech using over 100 A100 GPUs,\nshowcasing the scalability of SpeechBrain (Parcollet et al., 2024).\n\u2022 Models and Tasks: We developed several new models and expanded support for\nvarious tasks. For speech recognition, we introduced new alternatives to the Trans-former architecture like HyperConformer (Mai et al., 2023) and Branchformer (Peng\net al., 2022), along with a Streamable Conformer Transducer. We support models\nfor discrete audio tokens (e.g., discrete wav2vec, HuBERT, WavLM, EnCodec, DAC,"}, {"title": "4 Conclusion", "content": "We presented SpeechBrain 1.0, a significant advancement in the evolution of the Speech-Brain project. We outlined the main updates, including novel learning modalities, models,tasks, and decoding strategies, alongside our efforts in benchmarking initiatives. For anoverview of further improvements, please visit the project website. Looking ahead, weplan to keep serving our community with future advancements on both large-scale, small-footprint, and multi-modal models."}]}