{"title": "Open-Source Conversational AI with SpeechBrain 1.0", "authors": ["Mirco Ravanelli", "Titouan Parcollet", "Adel Moumen", "Sylvain de Langen", "Cem Subakan", "Peter Plantinga", "Yingzhi Wang", "Pooneh Mousavi", "Luca Della Libera", "Artem Ploujnikov", "Francesco Paissan", "Davide Borra", "Salah Zaiem", "Zeyu Zhao", "Shucong Zhang", "Georgios Karakasidis", "Sung-Lin Yeh", "Aku Rouhe", "Rudolf Braun", "Florian Mai", "Juan Zuluaga-Gomez", "Seyed Mahed Mousavi", "Andreas Nautsch", "Xuechen Liu", "Sangeet Sagar", "Jarod Duret", "Salima Mdhaffar", "Ga\u00eblle Laperri\u00e8re", "Renato De Mori", "Yannick Est\u00e8ve"], "abstract": "SpeechBrain\u00b9 is an open-source Conversational AI toolkit based on PyTorch, focused partic-ularly on speech processing tasks such as speech recognition, speech enhancement, speakerrecognition, text-to-speech, and much more. It promotes transparency and replicability byreleasing both the pre-trained models and the complete \"recipes\" of code and algorithmsrequired for training them. This paper presents SpeechBrain 1.0, a significant milestone inthe evolution of the toolkit, which now has over 200 recipes for speech, audio, and languageprocessing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0introduces new technologies to support diverse learning modalities, Large Language Model(LLM) integration, and advanced decoding strategies, along with novel models, tasks, andmodalities. It also includes a new benchmark repository, offering researchers a unifiedplatform for evaluating models across diverse tasks.", "sections": [{"title": "1 Introduction", "content": "Conversational AI is experiencing extraordinary progress, with Large Language Models\n(LLMs) and speech assistants rapidly evolving and becoming widely adopted in the daily\nlives of millions of users (McTear, 2021). However, this quick evolution poses a challenge to\na fundamental pillar of science: reproducibility. Replicating recent findings is often difficult\nor impossible for many researchers due to limited access to data, computational resources, or\ncode (Kapoor and Narayanan, 2023). The open-source community is making a remarkable\ncollective effort to mitigate this \u201creproducibility crisis\", yet many contributors primarily\nrelease pre-trained models only, known as open-weight (Liesenfeld and Dingemanse, 2024).\nWhile this is a step forward, it is still very common for the data and algorithms used to train\nthem to remain undisclosed. We helped address this problem by releasing SpeechBrain (Ra-\nvanelli et al., 2021), a PyTorch-based open-source toolkit designed for accelerating research\nin speech, audio, and text processing. We ensure replicability by releasing pre-trained\nmodels for various tasks and providing the \"recipe\u201d for training them from scratch, con-\nveniently including all necessary algorithms and code. A few other open-source toolkits,\nlike NeMo and ESPnet, also support multiple Conversational AI tasks, each excelling in\ndifferent applications. NeMo is industry-focused, offering ready-to-use solutions, but may\nprovide less flexibility for extensive customization compared to SpeechBrain, which is more\nresearch-oriented. ESPnet also supports various tasks with competitive performance, but\nSpeechBrain stands out for its comprehensive documentation, beginner-friendly tutorials,\nsimplicity, and lightweight design with fewer dependencies.\nThis paper introduces SpeechBrain 1.0, a remarkable milestone resulting from years of\ncollaboration between the core development team and our community volunteers. We will\noutline the key technical updates to support novel learning modalities, LLM integration,\nand advanced decoding strategies, along with novel models, tasks, and new modalities. We\nalso present our new benchmark repository, which is designed for researchers to evaluate\nand compare their models against state-of-the-art baselines across various tasks."}, {"title": "2 Overview of SpeechBrain", "content": "Since its launch in March 2021,\nSpeechBrain has grown rapidly and\nemerged as one of the most popular\ntoolkits for speech processing. It is\ndownloaded 2.5 million times monthly,\nused in 1,923 repositories, has 8.2k\nGitHub stars, and 148 contributors.\nDespite its constant evolution, we re-\nmain faithful to the original design\nprinciples. We prioritized replicability\nby releasing both training recipes and\npre-trained models. Moreover, 95%\nof our recipes utilize freely available\ndata and include comprehensive train-\ning logs, checkpoints, and other essen-\ntial information. We made Speech-\nBrain easy to use by providing com-\nprehensive documentation, examples,\nand tutorials. Our modular architecture facilitates easy integration or modification of mod-\nules. We built it on PyTorch standard interfaces (e.g., torch.nn.Module, torch.optim,\ntorch.utils.data.Dataset), enabling seamless integration with the PyTorch ecosys-\ntem (Rouhe et al., 2022). It is released under the permissive Apache 2.0 license."}, {"title": "2.1 Architecture Overview", "content": "Training a model with SpeechBrain involves combining three components, as depicted in\nFigure 1: the training script, the hyperparameter file, and the data manifest files. For\nsimplicity, the training script is integrated into a single Python script. This script leverages"}, {"title": "3 Recent Developments", "content": "SpeechBrain has rapidly evolved to support a wide array of tasks. Please, refer to Table 1\nfor a complete list as of June 2024. The main improvements in SpeechBrain 1.0 include:\n\u2022 Learning Modalities: We expanded the support for emerging deep learning modal-\nities. For continual learning, we implemented methods like Rehearsal, Architecture,\nand Regularization-based approaches (Della Libera et al., 2023). For interpretability,\nwe developed both post-hoc and design-based methods, including Post-hoc Interpreta-\ntion via Quantization (Paissan et al., 2023), Listen to Interpret (Parekh et al., 2022),\nActivation Map Thresholding (AMT) for Focal Networks (Della Libera et al., 2024),\nand Listenable Maps for Audio Classifiers (Paissan et al., 2024). We also implemented\naudio generation using standard and latent diffusion techniques, along with DiffWave\n(Kong et al., 2020) as a novel vocoder based on diffusion. Lastly, efficient fine-tuning\nstrategies have been introduced for faster inference using speech self-supervised mod-\nels (Zaiem et al., 2023a). We implemented wav2vec2 SSL pretraining from scratch\nas described by (Baevski et al., 2020). This enabled efficient training of a 1-billion-\nparameter SSL model for French on 14,000 hours of speech using over 100 A100 GPUs,\nshowcasing the scalability of SpeechBrain (Parcollet et al., 2024).\n\u2022 Models and Tasks: We developed several new models and expanded support for\nvarious tasks. For speech recognition, we introduced new alternatives to the Trans-\nformer architecture like HyperConformer (Mai et al., 2023) and Branchformer (Peng\net al., 2022), along with a Streamable Conformer Transducer. We support models\nfor discrete audio tokens (e.g., discrete wav2vec, HuBERT, WavLM, EnCodec, DAC,"}, {"title": "\u2022 Decoding Strategies:", "content": "We improved beam search algorithms for tasks like speech\nrecognition and translation. Our update simplifies code with separate scoring and\nsearch functions. This update allows easy integration of various scorers, including\nn-gram language models and custom heuristics. Additionally, we support pure CTC\ntraining, RNN-T latency controlled beamsearch (Jain et al., 2019), batch and GPU\ndecoding (Kim et al., 2017), and N-best hypothesis output with neural language model\nrescoring (Salazar et al., 2019). We also offer an interface to Kaldi2 (k2) for search\nbased on Finite State Transducers (FST) (Kang et al., 2023) and KenLM for fast\nlanguage model rescoring (Heafield, 2011)."}, {"title": "\u2022 Integration with LLMs:", "content": "LLMs are crucial in modern Conversational AI. We en-\nhanced our interfaces with popular models like GPT-2 (Radford et al., 2019) and\nLlama 2/3 (Touvron et al., 2023), enabling easy fine-tuning for tasks such as dialogue\nmodeling and response generation (Mousavi et al., 2024c). We also implemented\nLTU-AS (Gong et al., 2023), a speech LLM designed to jointly understand audio and\nspeech. Additionally, LLMs can be used to rescore n-best hypotheses provided by\nspeech recognizers (Salazar et al., 2019)."}, {"title": "\u2022 Benchmarks:", "content": "We have launched a new benchmark repository for facilitating com-\nmunity standardization across various areas of broad interest. Currently, we host\nfour benchmarks: CL-MASR for multilingual ASR continual learning (Della Lib-\nera et al., 2023), MP3S for speech self-supervised models with customizable probing\nheads (Zaiem et al., 2023b), DASB for discrete audio token assessment (Mousavi\net al., 2024b), and SpeechBrain-MOABB for fair evaluation of deep learning models\non EEG datasets. The goal of these benchmarks is to provide researchers with a com-\nmon framework, baselines, and evaluation protocols for tasks of significant research\ninterest."}, {"title": "4 Conclusion", "content": "We presented SpeechBrain 1.0, a significant advancement in the evolution of the Speech-Brain project. We outlined the main updates, including novel learning modalities, models,tasks, and decoding strategies, alongside our efforts in benchmarking initiatives. For anoverview of further improvements, please visit the project website. Looking ahead, weplan to keep serving our community with future advancements on both large-scale, small-footprint, and multi-modal models."}]}