{"title": "Analysis of Code and Test-Code generated by Large Language Models", "authors": ["Robin Beer", "Alexander Feix", "Tim Guttzeit", "Tamara Muras", "Vincent M\u00fcller", "Maurice Rauscher", "Florian Sch\u00e4ffler", "Welf L\u00f6we"], "abstract": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming software development by automating code generation and, arguably, enable rapid prototyping, support education, and boost productivity. Therefore, correctness and quality of the generated code should be on par with manually written code. To assess the current state of LLMs in generating correct code of high quality, we conducted controlled experiments with ChatGPT and Copilot: we let the LLMs generate simple algorithms in Java and Python along with the corresponding unit tests and assessed the correctness and the quality (coverage) of the generated (test) codes. We observed significant differences between the LLMs, between the languages, between algorithm and test codes, and over time. The present paper reports these results together with the experimental methods allowing repeated and comparable assessments for more algorithms, languages, and LLMs over time.", "sections": [{"title": "INTRODUCTION", "content": "Artificial Intelligence (AI) has already established a significant role in modern software development. Al-powered coding assistants of various kinds have been introduced including tools for bug and vulnerability detection, program analysis, test automation and fuzzing, and code generation. Due to the popularity and improvements of Large Language Models (LLMs), fully automated code generation seems to be in reach. Expectations range from an increase in programming productivity, over lifting the abstraction level in programming, to the end of programming as a profession, substitute by prompts that describe what software is supposed to do instead of how.\nThis raises several questions: How advanced are LLMs today, i.e., can they generate correct and clean code and can they generate test code to provide confidence in the correctness? Also, are their capabilities different for different programming languages and, finally, are they improving over time? Our study aimed at investigating these questions in controlled experiments.\nHowever, our study is restricted in several ways. First, we selected basic textbook algorithms, published in many textbooks and repositories. This setup should be in favor of LLMs as they learn from examples. Second, we restricted ourselves to Python and Java ranking second and third in the top programming languages on GitHub [1], which again was in favor of LLMs. The reason for not choosing JavaScript topping the ranks is that we wanted to include a compiled, typed language (Java) together with an interpreted, untyped language (Python). Third, we analyzed the code generation performance of only two LLM tools, ChatGPT [2] and GitHub Copilot [3], representing general purpose (ChatGPT) and programming specific (Copilot) LLMs; we acknowledge that other LLM code generation tools might perform differently, cf. a selection of such tools [4]."}, {"title": "Background", "content": "The code generating Al systems considered in the present study are based on Large Language Models.\nA Large Language Model is a type of artificial intelligence used for processing language [5]. LLMs are machine learning models, more specifically deep neural networks, and a subset of language models, or language modelling, which aims to improve the ability of machines to understand language. Typically, this is done by modelling the probabilities of word sequences in text, and thus calculating the likelihood of these sequences occurring in the rest of the text [6, 7]. The distinction between conventional language models and LLMs is the amount of mostly unlabeled data used for training. The capacity of LLMs is scaled with the size of the training data set [7].\nChatGPT [2] is an LLM based chat system created by OpenAl; it is a fine-tuned variant of the LLM GPT-3.5. A user can access the system by texting with a chatbot via a web interface or an API. The output has been optimized to simulate human interaction with the implementation of human feedback and correction [8]. It is not specifically trained for code generation. GPT is an abbreviation for \"Generative Pre-trained Transformer\", which nowadays represents a set of LLMs developed by OpenAl. Transformer (deep machine learning) models rely on a mechanism mimicking cognitive attention [9]. The quantity of training data and the number of parameters for the models have increased with each version [10, 11]. However, official information on GPT-4's parameter count is not yet available. GPT-3 was never officially released, but was instead offered via an API to restrict usage of the model [12]. ChatGPT can currently be used with both GPT-3.5 and GPT-4. However, access to the latter requires payment [13].\nGitHub Copilot [3] is an LLM based coding assistant designed to enhance developers' productivity by enabling them to write code fast and with little effort. It retrieves data from comments and code to propose complete functions or individual lines of code. GitHub Copilot is based on a generative Al model developed by GitHub, OpenAl and Microsoft, trained on publicly available natural language and source code samples, including public repository code"}, {"title": "Problem Formulation", "content": "This study addresses three main questions.\n1. What is the current capacity of LLM based tools to generate correct and clean code?\n2. Is there a significant difference between different LLM tools? As we extend the work documented in [16], this also includes the question of their improvements over the last year.\n3. Is there a significant difference between the programming languages, the code is generated for?\nGiven the huge interest in and the fast advancement of the assessed tools, the answers to the above questions can only be a snapshot and are, hence, expected to expire rather sooner than later. Therefore, the foremost problem to be solved by our work is to provide an evaluation framework-including benchmark problems, metrics, and statistical tools-to reevaluate the questions in a longitudinal study to see trends and to speculate about future opportunities of LLM based code generation in a more profound way."}, {"title": "Structure", "content": "The remainder of the paper is structured as follows: Section 2 introduces the scientific method and experimental setup for addressing the problem and answering the above questions. Sections 3 and 4 describe details of the conduction and the results of the experiment, resp. Section 5 discusses the results, answers the research questions, and lists possible threads to validity of these answers. Section 6 compares related studies with ours. Finally, Section 7 concludes the paper and shows possible directions of future work."}, {"title": "METHOD", "content": "To assess the Large Language Models' code generation performance, we instructed them to generate code in multiple programming languages. Such code was either an algorithm implementation or a unit test suite for the same. The resulting generations were then evaluated using various metrics to assess the code correctness and quality answering the following concrete questions:\n1. How well does ChatGPT (GitHub Copilot) provide correct code in Python (Java) from specified instructions?\n2. How good is ChatGPT (GitHub Copilot) at generating Python (Java) code of high quality?\n3. How well does ChatGPT (GitHub Copilot) provide correct Unit Tests in Python (Java) from specified instructions?\n4. How good is ChatGPT (GitHub Copilot) at generating Python (Java) Unit Tests of high quality?\n5. Can we observe significant differences between Java and Python?\n6. Can we observe significant differences between ChatGPT and GitHub Copilot?\n7. Can we observe significant changes compared to the baseline study [16], differences over time?"}, {"title": "Algorithm Selection", "content": "We have chosen twelve algorithms that the LLMs are supposed to generate. Since the study of Hansson and Ellr\u00e9us [16] served as the baseline for our experiments, we selected the same six algorithms. They started with the first five algorithms described in [17]. In order to not argue about any bias in this selection, we included the missing algorithms of the top ten listed in this article. Following the arguments in [16] for including Binary-to-Decimal conversion to increase the diversity in the set of algorithms, we added the Egyptian Fractions algorithm. This way we have at least two different examples for any type of algorithm in our set that includes the following twelve algorithms (type of algorithm in parentheses):\n\u2022 Bellman-Ford*, (graph, optimization algorithm)\n\u2022 Binary Search*, (search algorithm)\n\u2022 Binary To Decimal*, (number encoding algorithm)\n\u2022 Breadth First Search (BFS)*, (search algorithm)\n\u2022 Depth First Search (DFS)**, (search algorithm)\n\u2022 Dijkstra**, (graph, optimization algorithm)\n\u2022 Egyptian Fractions***, (number encoding algorithm)\n\u2022 Floyd-Warshall**, (graph, optimization algorithm)\n\u2022 Knapsack*, (optimization algorithm)\n\u2022 Kruskal**, (graph, optimization algorithm)\n\u2022 Merge Sort*, (sorting algorithm)\n\u2022 Quick Sort*, (sorting algorithm)\nIn short, they were selected because they * were contained in [16], ** fill the gaps of the top ten algorithms in [18], and *** were needed to have at least two different number encoding algorithms."}, {"title": "Formulating Prompts", "content": "Both LLMs are instructed with prompts. Depending on the amount and type of information in a prompt, the models generate different results. We created individual prompts for each combination of programming language and algorithm."}, {"title": "Source Code Generation Prompts", "content": "Generally, all source code prompts contain the algorithm, the class name, and detailed information describing the class structure. This includes inner classes, constructors and method declarations with their access modifiers, return values and parameters. The syntax for the class structure given for Java was sufficient for the Al to identify the intended programming language. This did not continue to hold for Python. Therefore, we included the programming language in the prompt. Below, an example of two prompts for Java and Python, respectively, to generate a binary search algorithm implementation."}, {"title": "Test Case Generation Prompts", "content": "For the unit test case generations, three different approaches were used to prompt the LLMs; all yielded promising results in preliminary tests. Documentation of these tests can be found in the source code repository in Appendix A. The first approach provides only the structure of the algorithm in text form, including the names of the variables and the functions, similar to the prompt generating the algorithm source code. The Binary Search unit tests prompt in Python is provided as an example.\nThe second and third approach provides a source code implementation of the algorithm to be tested, complemented with a short comment requesting unit tests for the code. For instance, an example comment for the Binary Search unit test in Python is shown below:\nThe difference between the second and the third approach was the actual source code provided. In the second approach, the manually written algorithm reference implementation was provided. It was the same implementation as used for validating the functionality of the respective unit tests for that algorithm.\nThe supplied code in the third approach was a randomly selected generated implementation for the respective algorithm, taken from the first part of our experiment. The selected implementation, however, needed to pass the manually written reference unit tests as a prerequisite, i.e., the unit tests used for validating the source code generation for that algorithm.\nAll prompts used in the experiments are listed in the Appendix C."}, {"title": "Generating Code", "content": "We generated code with ChatGPT by using its web interface. ChatGPT is set up as a chatbot and offers multiple chat windows with different conversations. Since this LLM uses the existing conversation in the current window as the context for its next generation, a new window was required for each generation. This step ensures that the"}, {"title": "Manual Correction of Source Code Generations", "content": "GitHub Copilot is, by design, unable to generate lines of code outside the current scope. This led to some missing import statements. Without adding these manually, the results of the experiment would have been significantly affected. Therefore, we decided to include them using the refactoring functionality of the IDEs. This made the results of ChatGPT and GitHub Copilot more comparable.\nIn the event that the code was not able to be compiled or interpreted, we opted to regenerate the sample using the same method as previously outlined in order to achieve an adequate sample size."}, {"title": "Manual Correction of Test Case Generations", "content": "For both Java and Python test case generation, our analysis required the code to be compilable or, in the case of Python, interpretable. Consequently, in instances where this was not the case, the generated code had to be manually corrected. The sole intention of these corrections was to make the code syntactically correct with as few modifications as possible. To ensure this, we decided to do the corrections in pair programming.\nThe only exception in this case was the treatment of uncompleted assertion statements. Since filling these statements with syntactically but not semantically correct content would have affected the results of later checks of the code, we decided to remove uncompleted assertion statements."}, {"title": "Evaluation Metrics", "content": "We defined different code correctness and quality metrics in order to evaluate the generated code, which will be described below."}, {"title": "Code Correctness", "content": "To verify that the code generated by the LLMs is correct, we integrated several reference unit tests into our project. These implementations reuse by-and-large unit tests from third-party sources. To ensure that they are implemented correctly, we tested them against reference implementations of the algorithms, reused from third-party sources as well. All reference unit tests defined for an algorithm must pass for a generated code to be considered correct.\nTo capture the correctness of generated unit tests, we tested their code against the above-mentioned reference implementations of each algorithm. We manually checked that the reference implementation was correct and passed all reference unit tests. For the generated test code to be considered correct, all its unit tests must pass the reference source code implementation."}, {"title": "Code Quality", "content": "To measure the quality of the generated code in addition to its correctness, we've established guidelines to follow for high-quality code. For the programming language Java, we've adopted quality rules from the referenced bachelor thesis by Hansson and Ellr\u00e9us [16], which have originally been derived from the book Clean Code by Robert C. Martin [19]. The principles used for Java are following:\n\u2022 The files should not be over 500 lines long.\n\u2022 A line should not be more than 120 characters long.\n\u2022 Magic numbers should be hidden behind constants.\n\u2022 Functions should not be more than 20 lines long.\n\u2022 Functions should not have more than three arguments.\n\u2022 There should not be nested loops of a depth of more than one level.\n\u2022 There should not be more than one statement per line.\n\u2022 There should not be any inner assignments [16]\nFor Python, we followed the guidelines outlined in the PEP-8 Style guide to ensure referring to appropriate standards. We therefore implemented the following quality rules:\n\u2022 A line should not exceed 79 characters.\n\u2022 A file should not be longer than 500 lines.\n\u2022 Magic values should be hidden behind constants.\n\u2022 Functions and methods should have a maximum of three arguments.\n\u2022 Nested blocks should not contain more than two blocks.\n\u2022 There should be only one statement per line.\n\u2022 Function and method names should not exceed 20 characters. [20]"}, {"title": "Code Coverage", "content": "As code quality metric for generating tests, we measured the test coverage of the source code. We analyzed the coverage of instructions and branches in combination, as described in more detail in Section 2.5.2."}, {"title": "Modification Rate of Generated Test Code", "content": "Besides the correctness and coverage of the generated tests, we evaluated the number of required modifications; recall the modifications explained in Section 2.3.2. These were quantified using the Levenshtein distance, a metric for measuring the difference between two strings, here the generated and the manually modified test code (strings). It is informally defined as the minimum number of edit operations (insertions, deletions or substitutions of single characters) required to change one string into the other [21]. To calculate the modification rate, we divided the Levenshtein distance by the number of characters in each code sample, excluding the length of imports and comments."}, {"title": "Automated Code Evaluation", "content": "The generated code was evaluated for the metrics explained above. The evaluation scripts were written in JavaScript, while the automation of those scripts was implemented as a pipeline in Bash."}, {"title": "Source Code Evaluation", "content": "The assessment of code quality relied primarily on the use of the linting tools Checkstyle for Java and Pylint for Python. We configured both tools to detect only those quality defects that violate our defined quality rules. A separate script counts the lines of code in each file, ignoring any type of comment and imported package, as the number of lines for each file is also needed in the later analysis. We opted to omit import statements, since importing complete libraries and packages can be substituted by importing only necessary implementations. This may result in numerous import lines instead of one, which could have influenced the outcomes.\nCode correctness was checked using reference unit tests from third-party sources; we refer to the discussion in Section 2.4.1 and the documentation in the BookExample directories on GitHub https://github.com/tguttzeit/AI-Code-Examination. To guarantee the consistency of the test suite with the generated code, we incorporated the necessary code structure into the prompt, as previously explained. We utilized the JUnit framework for Java and the unittest.py package for Python code, resp."}, {"title": "Test Case Evaluation", "content": "Similar to the experiment for the generated source code, the evaluation of the generated test codes was automated using scripts as much as possible. The pipeline for the test case generation experiment had to be split into two stages, allowing for the explained manual changes in between."}, {"title": "Statistical Analysis", "content": "We used Matlab and Python libraries and the Al-Therapy Statistics tool to analyze the data provided by the pipelines. To visualize the results in the form of graphs, we developed MATLAB scripts. As the data is separated per algorithm in the respective JSON files, we first merged the data with the scripts before calculating the frequencies and creating all graphs provided in Section 3.\nFor descriptive statistics and to detect significant differences within the data, we used the web-based tool Al-Therapy Statistics that provides statistical functions for data analysis. Appendix B contains the links to all analyses performed, including mean, mode, median, and dispersion for all the metrics mentioned above. A normality test was performed on each dataset before proceeding with hypothesis testing. As the datasets did not follow a normal distribution, we used non-parametric tests. To compare two groups, e.g., for the programming languages Java and Python or for the Al models ChatGPT and GitHub Copilot, we used the Mann-Whitney U test. This test uses mean ranks or medians to compare the scores of two groups on a variable. To test for significant differences between the three approaches to generating test cases, we used the Kruskal-Wallis test that can compare more than two groups on a variable. As the Kruskal-Wallis test is an extension of the Mann-Whitney U test, it is also based on mean ranks or medians [22].\nFor analyzing the differences in correctness and quality over time, i.e., between Spring 2023 (t0) and Fall 2023 (t1), we first apply the Mann-Whitney U test again. However, the samples generated at t0 and t1 are arguably not independent. Therefore, we analyze pairs of outcome at t0 and t1 for the same algorithm when measuring correctness (quality) of ChatGPT (Copilot) generated codes, respectively. We calculate the means of the outcomes for each algorithm over its 50 generations separately at t0 and t1 and assess whether these population means differ using paired difference tests. As the differences may not be normally distributed, we apply the (non-parametric) Sign test and, as this test may find the differences falsely non-significant, we crosscheck with the (also non-parametric) Wilcoxon signed-rank"}, {"title": "CONDUCTING THE EXPERIMENT", "content": "According to the methodology described previously, we conducted the experiment. Using the prompts and instructions described above, we generated code samples with ChatGPT and GitHub Copilot. We generated source code and unit tests separately for both Java and Python. An equal number of samples was produced for both programming languages. The precise distribution of the sample size is outlined below.\nFor the purpose of evaluating source code, we generated 50 code samples for each algorithm using ChatGPT and GitHub Copilot. As we have selected twelve algorithms, this results in a combined sample size of 600 per Al model and 1200 code samples overall for each programming language.\nTo evaluate the test code, we generated 10 samples for each of the three approaches, resulting in 30 unit test samples for each combination of algorithm and Al model. As a consequence, there are 360 samples per Al model for twelve algorithms and 720 code samples in total for each programming language.\nAfter generating, we added all code samples to our project and executed the two pipelines described in Section 2.5. To analyze the provided data, we transferred the resulting JSON files to our analysis repository. The files can be accessed and downloaded from our repository linked in appendix A to reproduce our findings. For the analysis, we used the tools mentioned above, generated graphs, and evaluated differences between the Al models and programming languages, which will be described in the following."}, {"title": "EVALUATION OF EXAMINATION RESULTS", "content": "The experiment was divided into source code generation and test case generation, as previously mentioned. Source code generation analyzed the generated implementations of an algorithm according to instructions provided in the prompt, while test case generation generated unit tests for the algorithm.\nIn the following sections, we explain the results of our research. The results for all the metrics are presented with graphs that illustrate the relative outcomes for each metric. Tables display the pure outcomes for the metrics (absolute results) and can be found in the appendix, together with the statistical analysis results including the detailed results of the hypothesis tests.\nFurther results and diagrams including deeper analysis on the topics stated below can be found in our repository on GitHub."}, {"title": "Source Code Generation", "content": "Firstly, we examined the code correctness and quality of the generated source code for both programming languages and all algorithms."}, {"title": "Code Correctness", "content": "Java\nGenerating source code for twelve algorithms in Java yields the following outcomes for the code correctness: Both Al models can generate correct Java code. There is a noticeable difference between the models, with ChatGPT outperforming GitHub Copilot by about 14%, see Figure 1. This can be further demonstrated through a hypothesis test. The Mann-Whitney U test reveals a statistically significant difference between the two at a significance level of 0.01.\nPython\nThe results for generating source code in Python are as follows: ChatGPT and GitHub Copilot are capable of producing valid Python code. There is a clear difference between the Al models again, with ChatGPT performing better with approximately 17%, see Figure 2. Comparing the two models using the Mann-Whitney U test also indicates a statistically significant difference at a significance level of 0.01.\nComparing Java and Python\nAs previously seen, the results for code correctness in Java and Python are similar. The Al models produce accurate code in both languages, although the results for Java show a clear advantage over Python. For ChatGPT, there is roughly a 10% difference comparing (the correctness of) Java and Python codes and for GitHub Copilot, a 13% difference comparing Java and Python.\nWhen analyzing the ChatGPT correctness results of the two programming languages, the Mann-Whitney U test reveals a statistically significant difference between Java and Python, based on a significance level of 0.01. Doing this for the correctness results of GitHub Copilot yields the same outcome. However, the differences between ChatGPT and GitHub Copilot within the languages are almost identical between the two, around a mean of about 14% in Java and 17% in Python."}, {"title": "Code Quality", "content": "Java\nThe quality results of the generated source code indicate that both Al models produce high-quality Java code, according to our defined quality guidelines in Section 2.4.2, see Figure 3. GitHub Copilot and ChatGPT do not differ in terms of code quality, as determined by a Mann-Whitney U test at a significance level of 0.05.\nComparing the number of quality violations detected for each algorithm generation, both Al models mostly produce implementations free from violations. In general, there is no large difference between ChatGPT and GitHub Copilot regarding the number of quality violations within generated code. Both achieve a maximum of 4 quality errors per implementation, see Figure 4 for the distribution of violations in the generated files.\nPython\nLooking at the results of generated Python code, we observe a high level of quality for both Al models. The perfor-"}, {"title": "Test Case Generation", "content": "After generating implementations for different algorithms, we examined the generation of test cases for given instructions or given implementations of the algorithms.\nThe following sections compare three approaches for generating test cases: The first strategy, called PromptOnly, generates unit tests without any implementation of the algorithm given in the prompt. The second approach, referred to as BookExampleCode, generates test cases for a given correct reference implementation of the algorithm. The last approach, named AlGenerated, is very close to the second approach, but with the modification that it uses a verified implementation of the algorithm generated by the Al model itself. The three approaches have been detailed in Section 2.2.2."}, {"title": "Test Code Correctness", "content": "Java\nThe outcomes regarding the correctness of generated Java test code are as follows: ChatGPT and GitHub Copilot produce test code at an intermediate level, but hardly do not exceed 50% correctness in any approach, with Copilot performing better than ChatGPT, see Figures 7. The Mann-Whitney U test indicates that there is a statistically significant difference between the two Al models at a 0.01 significance level.\nTest case generation appears to work better with an implementation provided in the prompt, as shown by the fact that the PromptOnly approach produces the lowest results. However, according to the Kruskal Wallis test executed for ChatGPT and Copilot individually, the data demonstrates no statistically remarkable differences at a significance level of 0.05.\nPython\nFor Python, Copilot appears to perform better than ChatGPT, but not well either, reaching 45% correctness at its peak, see Figures 8. ChatGPT on the other hand, achieves a correctness of only about 30% across the three approaches. The"}, {"title": "Coverage", "content": "To assess the quality of test code, we use test coverage metrics, as described in Section 2.5.2.\nJava\nThe evaluation of the coverage outcomes for generated Java unit tests indicate that across all approaches the coverage hardly reaches 60%, see Figure 9. Although the results appear similar, using a reference algorithm implementation in the prompt (BookExampleCode) produces the best results for generating Java test cases with ChatGPT and GitHub Copilot. There are minor differences between the approaches, with the results for Copilot showing slightly larger differences than those for ChatGPT. This is verified by a Kruskal-Wallis test performed separately for the Al models. The results reveal a statistically significant difference in the three approaches for Copilot on a significance level of 0.05, but none for ChatGPT.\nComparing the two models shows small variations for the PromptOnly and AlGenerated approaches. The Mann-"}, {"title": "Modification Rate", "content": "As discussed in Section 2.3.2, we needed to manually modify some unit test cases in order to compile these files and receive a report on the coverage. Evaluating these required modifications demonstrates that only minor adjustments to the code samples were needed. This result applies to both Java and Python, see Figures 11 and 12.\nIn general, we made very few modifications. Comparing ChatGPT and GitHub Copilot, there is hardly any noticeable difference between them for the two languages. However, this difference between ChatGPT and Copilot is still sta-"}, {"title": "Differences Over Time", "content": "We investigate differences in the outcomes over time by rerunning the experiment of the baseline study [16] and generating code for the identical six Java algorithms half a year later. Our objective was to identify noticeable and significant changes in the correctness and quality of the code.\nThe difference over time in ChatGPT's performance is substantial, while Copilot's correctness is almost similar. Comparing our results with theirs for both Al models in the six algorithms of the baseline study using the Mann-Whitney U test, we identify a significant difference in code correctness for ChatGPT at the 0.05 significance level, while the difference is not significant for GitHub Copilot.\nWe also identify variations in code quality. The baseline study attained a 98.52% code quality using ChatGPT and 94.07% with GitHub Copilot. While ChatGPT achieved a similar performance with a 98% rate of quality compliant generated lines in our study, GitHub Copilot improved its performance by 4% to 98.05%. The statistical analysis confirms a significant difference in the code quality of GitHub Copilot between our results and the baseline study at the 0.05 level of significance. However, ChatGPT did not show any significant improvement. To summarize these tests, there is an improvement of Al-generated code over time. Specifically, ChatGPT excels in ensuring code correctness, while GitHub Copilot enhanced its code quality to match that of ChatGPT.\nFigures 13 and 14 show the correctness and quality differences over time, i.e., the differences of the means of 50 generations for each algorithm generated at t0 and t1, respectively. Here, only quality score differences for Copilot are noticeably larger than zero, indicating a noticeable improvement from t0 to t1: on average, the number of quality issues has dropped by \u2248 1. However, according to the Sign test this difference is not significant at the 0.05 significance level. In contrast to that, the less conservative Wilcoxon signed-rank test does indicate a significant improvement at this level. In the three other cases (correctness changes for both LLMs and quality changes for ChatGPT), both tests fail in rejecting the null hypothesis that the (barely noticeable) differences are just random at the 0.05 significance"}, {"title": "DISCUSSION", "content": "We can summarize that ChatGPT does very well in generating correct code. ChatGPT achieved 89.33% for Java code and 79.17% for Python code in terms of code correctness. On the other hand, GitHub Copilot does also manage to generate correct code. It reached 75.50% for Java code and 62.50% for Python code in terms of code correctness.\nThese results answer the first research question: 1. How well does ChatGPT (GitHub Copilot) provide correct code in Python (Java) from specified instructions? However, there are threads to validity, as discussed in Section 5.2.\nThe results from ChatGPT in code quality were also good, with 98.09% for Java code and 88.20% for Python code. In contrast to the code correctness of code generated by GitHub Copilot, the code quality of the GitHub Copilot generated code showed better results. It achieved 98.13% for Java code and 90.15% for Python code. These results answer the second research question: 2. How good is ChatGPT (GitHub Copilot) at generating Python (Java) code of high quality?\nFor test case generation, ChatGPT achieves only intermediate results in code correctness. The outcomes were 37.50% on average for the three approaches for Java code and 28.61% on average for the three approaches for Python code. GitHub Copilot performed slightly better and achieved 49.72% on average for the three approaches for Java code and 39.17% on average for the three approaches for Python code in terms of code correctness. Only little manual work, about 1%, was needed to correct the generated test cases. This answers the third research questions 3. How well does ChatGPT (GitHub Copilot) provide correct Unit Tests in Python (Java) from specified instructions?\nRegarding test code quality, ChatGPT shows intermediate coverage in Java and good coverage in Python, i.e., 58.79% on average for the three approaches for Java code and 85.94% on average for the three approaches for Python code. GitHub Copilot similarly in coverage; the code coverage was 57.18% on average for the three approaches for Java code and 82.03% on average for the three approaches for Python code. This answers the third research questions 4. How good is ChatGPT (GitHub Copilot) at generating Python (Java) Unit Tests of high quality?"}, {"title": "Threats to Validity", "content": "In this section, we discuss internal and external threats to the validity of our study. Internal validity refers to the degree of confidence that the causal relationships being tested in this study are trustworthy and not influenced by other factors or variables. External validity refers to the extent to which results from this study can be applied and generalized to other situations, groups, or events."}, {"title": "Internal Validity", "content": "Due to the non-deterministic nature of the LLMs, the results of one and the same prompt are not repeatable to 100%. We mitigate this problem by generating several implementations resetting the session context in between but differences remain. Although the differences appear to be minor, an analysis of the statistical significance of these differences would be one of the possibilities to extend this study, as explained in Section 7.\nAnother threat to the validity of the above results was already observed during the course of our experiment. We run the generations in parallel using devices with different specifications and operating systems. Due to the nondeterminism in some assessment tools and even in some of the generated algorithms, results were not the same for different users and machines, even though the data generated was the same. Even after using the same versions of each tool involved in the examination process, the results on different devices still differed from time to time. We did our best to find the reason for these differences and correct them where possible, but we were unable to correct them all. Again"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "The study indicates that LLMs can generate algorithms and test codes for both Java and Python. Algorithm generation works, however, way better than test case generation. It can also be stated that (currently) ChatGPT outperforms GitHub Copilot in generating correct code, although both yield code of excellent quality. When producing test cases, both Al models produce similar results in terms of test coverage, with ChatGPT performing better on some (prompting) approaches. However, in terms of the correctness of generated test cases, GitHub Copilot produces more correct outcomes than ChatGPT. Regardless of the LLM, the code generated in Java achieved significantly better results overall than the code generated for Python, with test coverage results the only exception.\nHowever, given the shortfall of 10% left to attain faultless code in rather well-known standard algorithms, it is reasonable to assume that the era of artificial intelligence superseding human software developers is yet to come. Nevertheless, both ChatGPT and GitHub Copilot prove to be useful assisting tools in developing software, enabling the developers to focus on more complex tasks in their job.\nFurther research is required to provide a more elaborate conclusion on the methods and opportunities available for supporting and automating software development, as discussed in Section 5. This future work involves categorizing the Al tasks according to their complexity, which may reveal potential effects on correctness and quality of the generated (test) codes. Since the research on the code generation capabilities of LLMs is an ongoing topic, one could compare the results of this and other studies to try to identify a trend in code correctness and code quality over time. Another way to increase the variety of the code generation tasks, is to assess the capabilities of LLMs to refactor or debug already written code. Finally, it is interesting to statistically analyze the influence of non-determinism in (test) code generation, in the generated algorithms, and experimental environment on the study results."}, {"title": "STATISTICAL ANALYSIS", "content": "This section contains additional information that outlines our study's findings. Frequency tables are used to provide precise data on the quality and correctness of the produced Python and Java code. Furthermore, all results from hypothesis tests and statistical analysis are presented in detail."}, {"title": "Frequency Tables for Correctness", "content": "The tables below display the count of generations per Al model and in case of test generation"}]}