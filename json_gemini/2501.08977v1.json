{"title": "Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models", "authors": ["Emma Croxford", "Yanjun Gao PhD", "Nicholas Pellegrino", "Karen K. Wong MD", "Graham Wills PhD", "Elliot First", "Miranda Schnier", "Kyle Burton MD", "Cris G. Ebby MD, MS", "Jillian Gorskic MD", "Matthew Kalscheur MD", "Samy Khalil MD", "Marie Pisani BS", "Tyler Rubeor MD", "Peter Stetson, MD MA", "Frank Liao PhD", "Cherodeep Goswami", "Brian Patterson MD", "Majid Afshar MD"], "abstract": "Background: As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation and as models and documentation practices evolve. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. This study aimed to validate the PDSQI-9 across key aspects of construct validity.\nMethods: Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-40, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation analyses for substantive validity, factor analysis and Cronbach's \u03b1 for structural validity, inter-rater reliability (ICC and Krippendorff's \u03b1) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Raters underwent standardized training to ensure consistent application of the instrument.\nResults: Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's \u03b1 =\n0.879; 95% CI: 0.867\u20130.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867\u20130.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (p = -0.200, p = 0.029) and Organized (p = 0.190, p = 0.037). The semi-Delphi process ensured clinically relevant attributes, and discriminant validity distinguished high- from low-quality summaries (p < 0.001).\nConclusions: The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer, more effective integration of LLMs into healthcare workflows.", "sections": [{"title": "Introduction", "content": "The volume of notes in the electronic health record (EHR) has increased in the past decade, exacerbating the burden on providers and highlighting the growing difficulty of the unassisted \u201cchart biopsy\" task. One in five patients arrives at the hospital with a chart comparable to the size of \"Moby Dick\" (206,000 words) [1]. While the EHR's centralized storage of medical notes is beneficial, and studies have shown that access to prior records improves diagnostic accuracy, the growing volume of data presents a significant challenge [2]. The tension between the EHR's role as a documentation repository and its function as a tool for retrieving actionable information has become increasingly unmanageable without additional filtering and summarization tools [3].\nIn the field of clinical Natural Language Generation (NLG), multi-document summarization has emerged as an important task to address the challenge of note bloat and reduce the cognitive burden on healthcare providers. As Large Language Models (LLMs) continue to advance NLG capabilities, they offer a promising alternative for summarizing clinical documentation and alleviating the time-intensive nature of human-authored summaries. However, this capability also introduces challenges, including performance degradation with chronological errors or missed details [4]. Despite their potential, the rapid advancements of LLMs have outpaced the development of robust evaluation instruments to assess the quality of their outputs.\nThere remains a paucity of evidence on human evaluation instruments designed for LLM summarizations developed from real-world, multi-document EHR data and supported by psychometric validation [5]. Tam et al.'s systematic review of 142 studies on human evaluation methodologies for LLMs in healthcare highlighted significant gaps, including the lack of sample size calculations, insufficient details on the number of evaluators, and inadequate evaluator training [6]. While their paper provided areas for improvement, the suggested strategies were primarily based on patterns observed in the literature rather than being grounded in statistical frameworks. There is a significant gap in evaluation methodologies designed to address the complexities of healthcare applications and the unique challenges posed by LLMs.\nExisting evaluative instruments for provider notes are designed primarily for provider-authored documentation. One widely adopted instrument, which has also been applied to AI-generated notes [7, 8, 9], is the Physician Documentation Quality Instrument (PDQI). The instrument has demonstrated strong reliability, achieving high inter-rater reliability and internal consistency. [10]. Although the PDQI-9 tool is validated and reliable for evaluating provider-authored notes, it was not designed to address the unique challenges posed by LLM summarization of notes. Summaries generated by LLMs must be assessed for additional factors such as relevancy, hallucinations, omissions, and factual accuracy, areas where LLMs have demonstrated limitations [11]. To address these gaps, this study introduces the Provider Documentation Summarization Quality Instrument (PDSQI-9), an LLM-centric adaptation of the PDQI-9. The PDSQI-9 is specifically designed to evaluate LLM-generated summaries, developed and validated on real-world EHR data, and rigorously tested for psychometric properties with adequate statistical power."}, {"title": "Methods", "content": ""}, {"title": "Study Design, Setting, and Data", "content": "The corpus of notes was designed for multi-document summarization and evaluation using inpatient and outpatient encounters from the University of Wisconsin Hospitals and Clinics (UW Health) in Wisconsin and Illinois between March 22, 2023 and December 13, 2023. The evaluation was conducted from the perspective of the provider during their initial office visit with the patient (\"index encounter\"), representing a real-world clinic appointment where the provider benefits from a summary of the patient's prior encounters with outside providers. Other inclusion and exclusion criteria were the following: (1) patient was alive at time of index encounter with provider; (2) patient had at least one encounter in 2023; and (3) excluded psychiatry notes. The corpus was further filtered to patients with 3 or more encounters before the index encounter, to provide a multi-document occurrence and fit the context window of many large language models. The resultant corpus consisted of 2,123 patients, with 554 having 3 encounters, 389 having 4 encounters, and 1,180 having 5 encounters. The derived dataset, consisting of encounters with concatenated provider notes leading to the index encounter of interest, was built as a random sample from the corpus with stratification across 11 specialties (25% from gynecology, urgent care, neurosurgery, neurology, or urology; 40% from dermatology, surgery, orthopedics, or ophthalmology; 35% from family medicine or internal medicine) The sample size needed for evaluation was determined a priori (see sample size estimation) and included additional examples for pilot testing by the instrument developers and training by the raters. The final dataset was 200 unique patients and their encounters and 22.5%, 22%, and 55.5% had 3, 4, and 5 notes per patient summarized prior to the index encounter. This study was approved by the University Wisconsin-Madison Institutional Review Board and qualified as exempt human subjects research."}, {"title": "Development of Provider Document Summarization Quality Instrument (PDSQI)- 9", "content": "The instrument development process employed a semi-Delphi methodology, an iterative consensus-driven approach commonly used for gathering expert opinions and refining complex frameworks. [12]. The semi-Delphi process consisted of three iterative rounds, each involving nine stakeholders with diverse expertise: three physicians who were also clinical informaticists with specialized knowledge in human factors design and natural language processing (MA, BP, KW); two software developers with experience in generative AI (NP, EF); one quality improvement specialist (MS); two data scientists (EC, GW); and one computer scientist with expertise in computational linguistics (YG).\nRound 1 - Literature Review and Domain Identification: The panel reviewed existing literature and methodologies for evaluating clinical text summarizations. The PDQI-9 was selected as the benchmark due to its demonstrated validity, interpretability, and applicability to evaluating physician clinical documentation. The panel then identified key domains essential for high-quality multi-document summarization, as well as dimensions where LLMs are known to underperform, such as hallucinations, omissions, and relevancy.\nThe identified dimensions were mapped to existing PDQI-9 attributes where feasible, with modifications to improve applicability of clarity and relevance attributes. Two PDQI-9 attributes, Up-to-Date and Consistent, were removed as their conceptual scope was adequately captured by modifications to other attributes. Two new attributes were added to address concerns in LLM-generated summaries: use of stigmatizing language and inclusion of citations linking facts in the summary to the original documentation. Additionally, the panel placed a particular focus on the vulnerability of each domain to hallucinations, defined as falsifications or fabrications.\nRound 2 - Attribute Refinement and Mapping to Likert Scales: The instrument definitions for each attribute were refined, and detailed instructions were developed for scoring on a five-point Likert scale. The panel iteratively revised attribute definitions to ensure clarity and usability. Special emphasis was placed on designing attribute definitions that captured the nuances of clinical text summarization, including factors such as relevancy, factual accuracy, and faithfulness to the source documentation.\nRound 3 - Pilot Testing and Consensus Adjudication: Pilot testing was performed with three senior physicians (MA, KW, BP) from different specialties to evaluate the usability and clarity of the attributes and scoring instructions. Feedback from these testers was incorporated iteratively, with the adjudication of disagreements conducted by the expert panel to achieve consensus. Pilot testing continued until all testers agreed on the final instrument definitions and scoring instructions, ensuring content validity of the instrument through the semi-Delphi process. The final instrument is in Appendix C."}, {"title": "LLM Summarizations for Validating the PDSQI-9", "content": "To generate summaries of varying quality, we employed different prompts across different LLMs to summarize notes for each patient encounter leading up to the index encounter. The LLMs utilized in this study included OpenAI's GPT-40 [27], Mixtral 8x7B [28], and Meta's Llama 3-8B [29] (Table 2). GPT-4 operates within the secure environment of the health system's HIPAA-compliant Azure cloud. No PHI was transmitted, stored, or used by OpenAI for model training or human review. All interactions with proprietary closed-source LLMs were fully compliant with HIPAA regulations, maintaining the confidentiality of patient data. The open-source LLMs, Mixtral 8x7B and Llama-3-8B, were downloaded from HuggingFace [30] to HIPAA-compliant, on-premise servers.\nWe used four strategies for engineering the evaluation prompts: minimizing perplexity, in-context examples, chain-of-thought reasoning, and self-consistency. The prompt for each LLM included a persona with the following instruction: \"You are an expert doctor. Your task is to write a summary for a specialty of [target specialty], after reviewing a set of notes about a patient.\" To generate lower-quality summaries, additional variations of the prompt removed instructions or encouraged the inclusion of false information. The persona and instruction were followed by two chains of thought: Rules and Anti-Rules, delineating positive and negative summarization steps. The Rules targeted specific attributes in the PDQSI-9 to generate high-quality summaries. Anti-Rules introduced intentional errors (e.g., hallucinations, omissions) to include mistakes. For each patient, the LLM was provided a randomized subset of Rules and Anti-Rules, ensuring heterogeneity in the generated summaries. The open-source models, Mixtral 8x7b and Llama 3-8b, were tasked with producing the lowest-quality summaries and were exclusively provided Anti-Rules as instructions. This approach aimed to provide a wide distribution of PDSQI-9 scores and to allow for discriminant validity testing. The prompts are available at https://git.doit.wisc.edu/smph-public/dom/uw-icu-data-science-lab-public/pdsqi-9.\nThe final corpus had 100 summaries generated by GPT-40, 50 by Mixtral 8x7b, and 50 by Llama 3-8b."}, {"title": "Sample Size Estimation and Rater Training", "content": "Sample size calculations were performed assuming a minimum of five raters and an even score distribution across a 5-point Likert scale. To achieve a desired statistical power of 80% with a precision of 0.1, each rater was required to complete at least 84 evaluations [31].\nFive junior physician raters with 1 to 5 years of post-graduate experience were recruited (MP, KB, CE, SK, TR). Additionally, two senior physician raters (JG, MK), each with at least 10 years of post-graduate experience, were recruited to complete a subset of evaluations for further validation. To standardize evaluation criteria and scoring, a group of three senior physician trainers (MA, KW, BP) conducted evaluations on three exemplar cases. These cases were subsequently used as reference materials for all raters during a live training session. Given the varying levels of expertise among the raters, all were provided with the Center for Health Care Strategies' documentation on identifying bias and stigmatizing language in EHRs [32]. Following the training session, raters were encouraged to pose questions or highlight disagreements during subsequent practice days. After the training period, all raters independently completed three additional example cases. Agreement among the raters was established before proceeding with independent evaluations of the full dataset."}, {"title": "Analysis Plan and Validation", "content": "Baseline characteristics of the corpus notes and evaluators were analyzed. Distributions of evaluative scores for each attribute were visualized using density ridge plots. Token counts were derived using the Llama 3-8b tokenizer.\nThe PDSQI-9 was evaluated through multiple metrics to assess its validity and reliability, informed by Messick's Framework of validity [33]. To examine the theoretical alignment of the PDSQI-9, Pearson correlation coefficients were calculated to evaluate relationships between input characteristics (e.g., note length) and attribute scores (e.g., Succinct and Organized). This tested whether the instrument captured expected relationships consistent with the theoretical underpinnings of summarization challenges and provided substantive validity. For generalizability, inter-rater reliability was assessed using the Intraclass Correlation Coefficient (ICC) and Krippendorf's [34] \u03b1, ensuring the instrument produced consistent results across evaluators with varying levels of expertise. ICC is derived from an analysis of variance (ANOVA) and has several forms tailored to specific use cases [35]. In this study, a two-way mixed-effects model was used for consistency among multiple raters, specifically ICC(3,k) [36]. Unlike ICC, which is a variance-based measure, Krippendorff's \u03b1 was calculated based on observed disagreement among raters and adjusted for chance agreement. The performance of junior physician evaluators was tested against senior physician evaluators (JG, MK) using the Wilcoxon signed-rank test, to assess differences in median scores between the two groups. To assess the PDSQI-9's discriminative validity, a Mann-Whitney U test was performed between the lowest and highest quality summaries. The summaries generated by GPT-40 with error-free prompts were considered the highest quality, while those generated by Llama 3-8b and Mixtral 8x7b with error-prone prompts were considered the lowest quality.\nFor structural validity, internal consistency was measured using Cronbach's \u03b1 [37], which evaluates whether the instrument items reliably measure the same underlying construct. Confirmatory factor analysis was conducted to identify latent structures underlying the survey attributes and to evaluate alignment with theoretical constructs. Factor loadings were used to assess variance within the instrument. A four-factor model was selected based on eigenvalues, the scree plot, and model fit indices (Appendix \u0412).\nCronbach's \u03b1, ICC, and Krippendorff's \u03b1 produced coefficients ranging between 0 and 1, where higher values indicate greater reliability or agreement. 95% Confidence Intervals (CI) were provided for all coefficients and calculated using the Feldt procedure [38], Shrout & Fleiss procedure [39], and bootstrap"}, {"title": "Results", "content": "Seven physician raters evaluated 779 summaries and scored 8,329 PDSQI-9 items to achieve greater than 80% power for examining inter-rater reliability. No difference was observed in the scores by rater expertise when comparing junior and senior physicians (n = 48 summarizations; p-value = 0.187). The median time required for the junior physicians to complete a single evaluation, including reading the provider notes and the LLM-generated summary, was 10.93 minutes (IQR: 7.92-14.98). Senior physician raters completed evaluations with a median time of 9.82 minutes (IQR: 6.28\u201313.75) (Appendix A).\nThe provider notes, concatenated into a single input for each patient, had a median word count of 2,971 (IQR: 2,179-3,371) and a median token count of 5,101 (IQR: 3,614\u20137,464). The provider note types included notes from 20 specialties (medicine, family medicine, orthopedics, ophthalmology, emergency medicine, surgery, dermatology, urgent care, urology, neurology, gynecology, psychiatry, anesthesiology, neurosurgery, somnology, pediatrics, audiology, and radiology). The LLM-generated summaries of the provider notes had a median length of 328.5 words (IQR: 205.8\u2013519.8) and 452.5 tokens (IQR: 313.5\u2013749.5). A modest positive correlation was observed between the input text's length and the generated summaries' length (p = 0.221 with p-value = 0.002).\nFigure 1 illustrates the average scores for each attribute of a summary, as evaluated by our raters, in relation to the length of the notes being summarized. As the length of the notes increased, the quality of the generated summaries was rated lower in the attributes of Organized (p = -0.190 with p-value = 0.037), Succinct (p = -0.200 with p-value = 0.029), and Thorough (p = -0.31 with p-value < 0.001). Additionally, the variance in scores among the raters increased with longer note lengths for the attributes of Thorough (p = 0.26 with p-value = 0.004) and Useful (p = -0.28 with p-value = 0.003) (Figure 2)."}, {"title": "Discussion", "content": "This study introduces the PDSQI-9 as a novel and rigorously validated instrument designed to assess the quality of LLM-generated summaries of clinical documentation. Using Messick's Framework, multiple aspects of construct validity were demonstrated, ensuring that the PDSQI-9 provides a well-developed and reliable tool for evaluating summarization quality in complex, real-world EHR data. Strong inter-rater reliability (ICC: 0.867) and moderate agreement (Krippendorff's \u03b1: 0.575), combined with consistent performance across evaluative attributes, were key findings. No differences in scoring were observed between junior and senior physician raters, underscoring the instrument's reliability across varying levels of clinical experience. Strong discriminant validity was shown between high- and low-quality summaries. To our knowledge, the PDSQI-9 is the first evaluation instrument developed using a semi-Delphi consensus process, applied to real-world EHR data, and supported by a well-powered study design with nearly 800 patient summaries.\nThe strong inter-rater reliability (ICC = 0.867) was comparable to the results reported in the original PDQI-9 study, which highlighted the reliability of the instrument in evaluating clinician-authored notes [10]. The moderate Krippendorff's \u03b1 (0.575) reflects robust agreement despite the complexity of the evaluation tasks. The strong internal consistency (Cronbach's \u03b1 = 0.879) supports the structural validity of the instrument, demonstrating that its attributes cohesively measure the construct of summarization quality. The 4-factor model further demonstrated strong construct validity, aligning attributes with theoretical constructs relevant to evaluating LLM-generated clinical summaries. The identified factors capture key dimensions of clinical summarization quality, including organization, clarity, accuracy, and utility, validating the instrument's use for this purpose. [5, 6, 47, 48].\nThe semi-Delphi process facilitated the inclusion of clinically relevant attributes, grounded in expert consensus, to ensure the instrument's applicability in real-world settings. This iterative process refined the PDSQI-9 to address critical issues unique to LLM-generated text, such as hallucinations, omissions, and stigmatizing language. By incorporating attributes specifically designed to evaluate LLM outputs, such as hallucinations and omissions, the PDSQI-9 effectively identifies risks associated with LLM-generated summaries, reinforcing safer applications of LLMs in clinical practice. The inclusion of a stigmatizing language attribute further enhances the instrument by identifying potentially harmful language in notes or summaries. Given the importance of equitable care, LLMs tasked with summarization must avoid introducing language that could perpetuate provider bias or negatively influence clinical decision-making. [49].\nThe evaluation process revealed notable differences in efficiency between junior and senior physician raters, with senior physicians completing evaluations more quickly compared to the overall median time; however, this did not affect the scores between the groups and shows the instrument is reliable across different levels of experience. Notable differences were the observed correlations between input note length and declining quality scores highlighting the need for careful consideration of input complexity when deploying LLMs in clinical workflows [50]. These findings align with the \"lost in the middle\u201d phenomenon, where LLM performance can degrade for content located within the middle of a large context window [4].\nThe selected LLMs included state-of-the-art models such as GPT-40, alongside smaller, open-source models that are more prone to errors, allowing for a comparative evaluation across diverse capabilities. The smallest context window among the models was 8K tokens, and the median input length of approximately 5K tokens provided a long yet manageable input size with available compute resources. With 3-5 provider notes from the EHR per case, the design allowed for realistic testing of LLM performance in a clinical context, highlighting their strengths and limitations in processing multi-document inputs and generating specialty-relevant summaries.\nAlthough the generated summaries were designed to represent varying levels of quality, achieving an even distribution of scores across attributes such as Comprehensible, Synthesized, and Accurate proved challenging. The skewed distributions impacted reliability metrics, with the degree of impact varying based on each metric's robustness to unbalanced data. The Comprehensible attribute was likely influenced by advancements in LLMs, which can produce coherent text regardless of relevancy. In contrast, Accurate and Synthesized attributes highlight the challenges of evaluating extractive versus abstractive summarization. Extractive summarization reflects content directly from the notes, while abstractive summarization requires synthesizing and expanding on information, both of which are critical but more subjective. To address this, an additional step was added to the instrument, asking raters to determine whether abstraction opportunities existed in each note/summary pair. Factor analysis results emphasized the importance of these attributes, with Synthesized showing weak factor association, reflecting the difficulty of evaluating this skill even for humans. Nevertheless, abstraction in synthesis is important in clinical contexts and remains a challenge for both humans and LLMs.\nIn conclusion, the PDSQI-9 is introduced as a comprehensive tool for evaluating clinical text generated through multi-document summarization. This human evaluation framework was developed with a strong emphasis on aspects of construct validity. The PDSQI-9 offers an evaluative schema tailored to the complexities of the clinical domain, prioritizing patient safety while addressing LLM-specific challenges that could adversely affect clinical outcomes."}]}