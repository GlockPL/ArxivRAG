{"title": "APAR: Modeling Irregular Target Functions in Tabular Regression via Arithmetic-Aware Pre-Training and Adaptive-Regularized Fine-Tuning", "authors": ["Hong-Wei Wu", "Wei-Yao Wang", "Kuang-Da Wang", "Wen-Chih Peng"], "abstract": "Tabular data are fundamental in common machine learning applications, ranging from finance to genomics and healthcare. This paper focuses on tabular regression tasks, a field where deep learning (DL) methods are not consistently superior to machine learning (ML) models due to the challenges posed by irregular target functions inherent in tabular data, causing sensitive label changes with minor variations from features. To address these issues, we propose a novel Arithmetic-Aware Pre-training and Adaptive-Regularized Fine-tuning framework (APAR), which enables the model to fit irregular target function in tabular data while reducing the negative impact of overfitting. In the pre-training phase, APAR introduces an arithmetic-aware pretext objective to capture intricate sample-wise relationships from the perspective of continuous labels. In the fine-tuning phase, a consistency-based adaptive regularization technique is proposed to self-learn appropriate data augmentation. Extensive experiments across 10 datasets demonstrated that APAR outperforms existing GBDT-, supervised NN-, and pretrain-finetune NN-based methods in RMSE (+9.43% ~ 20.37%), and empirically validated the effects of pre-training tasks, including the study of arithmetic operations. Our code and data are publicly available at https://github.com/johnnyhwu/APAR.", "sections": [{"title": "1 Introduction", "content": "The tabular regression task, prevalent in sectors such as healthcare (Rao et al. 2023; Jain et al. 2024) and finance (Du, Wang, and Peng 2023; Deng et al. 2024), has commonly been addressed using Gradient Boosting Decision Tree (GBDT) models (e.g., CatBoost (Prokhorenkova et al. 2018)). Despite recent advancements in neural networks (NNs), they often fail to consistently outperform GBDT models in this domain (Wang et al. 2024). This is attributed to the understanding of important features from distinct characteristics of tabular data, such as feature heterogeneity and the presence of uninformative features, which make it challenging to identify important features. On the other hand, the irregular target functions prevent NNs from learning high-frequency components of heterogeneous tabular datasets, and negatively impact NN performance due to overfitting (Beyazit et al. 2023)."}, {"title": "2 Related Work", "content": "Recently, deep learning approaches for tabular data have demonstrated effective performance. For instance, Song et al. (2019) employed multi-head self-attention for intra-sample feature interactions. Huang et al. (2020) and Gorishniy et al. (2021) adapted the Transformer architecture to tabular data, targeting both categorical and numerical features. Nonetheless, GBDT-based approaches (Chen and Guestrin 2016; Ke et al. 2017; Prokhorenkova et al. 2018) are still competitive in tabular benchmarks due to the highly complex and heterogeneous characteristics of tabular data, causing NNs to overfit on irregular target functions.\nTo prevent NN from overfitting on samples, prior research has proposed to considering sample-wise relationships while learning individual representations. NPT (Kossen et al. 2021) and SAINT (Somepalli et al. 2021) utilize self-attention to explicitly reason about relationships between samples. Similarly, Ucar, Hajiramezanali, and Edwards (2021) employed self-supervised contrastive loss to ensure that the model outputs similar representations for different feature subsets of the same sample, and dissimilar representations for subsets of different samples. Although these approaches effectively capture relationships between samples, they require relatively large batch sizes (e.g., 4096 in NPT) to include more samples computed under self-attention, leading to high computational and memory consumption. Moreover, they do not leverage supervised labels to learn contextualized representations based on explicit information. To incorporate supervised lavels, Supervised Contrastive Loss (Khosla et al. 2020) and TransTab (Wang and Sun 2022) enable models to learn improved sample representations by making samples with the same label similar, and samples with different labels dissimilar. Similarly, Cui et al. (2024) improved positive samples by augmenting anchors with features from samples of the same class to learn better representations based on explicit labels. Nonetheless, these approaches rely on discrete labels to determine positive or negative samples, which becomes extremely sparse for regression tasks where continuous labels are used.\nTo prevent NN from overfitting features, another line of research has utilized regularization techniques to avoid the model paying too much attention to a single feature. For instance, VIME (Yoon et al. 2020), SAINT (Somepalli et al. 2021), TabNet (Arik and Pfister 2021) and ReConTab (Chen et al. 2023b) use feature corruption, which encourages the model to output consistent predictions when features are randomly masked or mixed-up with other samples. However, randomly masking or mixing-up might inadvertently corrupt important features, causing the model to learn to predict based on uninformative features, and thus deteriorating learning representations. On the other hand, our proposed approach incorporates arithmetic from continuous labels as the pre-training task, and adaptively self-learns proper regu-"}, {"title": "3 Problem Formulation", "content": "In this paper, we focus on regression tasks within the tabular domain. A dataset is denoted as $D = \\{(x_i, y_i)\\}_{i=1}^T$, where $x_i = (x_{i1}, ..., x_{ik}) \\in \\mathbb{R}^k$ represents an object consisting of k features, with $T \\in \\{num, cat\\}$ indicating whether the feature is numerical or categorical. The corresponding label is denoted as $y_i \\in \\mathbb{R}$. Given an input sample x, our goal is to learn a representation Z of the sample that effectively encapsulates both feature-wise and sample-wise information that is able to maintain robustness against heterogeneous and uninformative features to precisely predict the corresponding target y."}, {"title": "4 The Proposed Approach", "content": "The APAR framework employs a pretrain-finetune framework, outlined in Figures 2 (pre-training phase) and 3 (finetuning phase). Our APAR framework consists of two modules: the Feature Tokenizer and Feature Encoder. In the pre-training stage, a pair of two samples are encoded by the feature tokenizer and feature encoder to obtain their corresponding representations, and the contextualized [CLS] token is used for predicting arithmetic outcomes based on their numerical labels. In the fine-tuning stage, a test sample is augmented by applying it with a gate vector. Both the original and augmented samples are then encoded by the pre-trained feature tokenizer and feature encoder, generating two contextualized [CLS] tokens. The model is trained to adapt the gate vector based on self-learned feature importance, ensuring consistent predictions across the original and augmented samples."}, {"title": "4.1 Model Architecture", "content": "Feature Tokenizer To transform input features into representations, the feature tokenizer is introduced to convert categorical and numerical features of a sample into a sequence of embeddings. Similar to (Grinsztajn, Oyallon, and Varoquaux 2022), the feature tokenizer can prevent the rotational invariance of NNs by learning distinct embeddings for each feature.\nGiven j-th feature $x_{ij}$ of the i-th sample, the tokenizer generates a d-dimensional embedding $z_{ij} \\in \\mathbb{R}^d$. Formally, the embedding is computed as:\n$z_{ij} = b + f(x_{ij}) \\in \\mathbb{R}^d,$\\nwhere b is a bias term and f represents a transformation function. For numerical features, f involves an element-wise product with a weighting vector $W^{(num)} \\in \\mathbb{R}^d$.\n$z_{ij}^{(num)} = b^{(num)} + x_{ij}^{(num)} \\cdot W^{(num)} \\in \\mathbb{R}^d.$\nFor categorical features, f applies a lookup in the embedding matrix $W^{(cat)} \\in \\mathbb{R}^{c\\times d}$, where c is the number of categories, and $e_{ij}$ is a one-hot vector for the corresponding categorical feature:\n$z_{ij}^{(cat)} = b^{(cat)} + e_{ij} \\cdot W^{(cat)} \\in \\mathbb{R}^d.$\nFinally, the output from the feature tokenizer $Z_i$ is concatenated by the embeddings of all features of a sample $x_i$:\n$Z_i = stack[Z_{i1}, ..., Z_{ik}] \\in \\mathbb{R}^{k\\times d}.$\nFeature Encoder Since our aim is to explore the strategies of the pre-training and fine-tuning stages similar to (Huang et al. 2020; Somepalli et al. 2021), the Transformer blocks encompassing multi-head self-attention and feed-forward networks are adopted as the feature encoder to encode intricate interrelations among heterogeneous and unformative features and to align the comparison.\nSpecifically, the embedding of a [CLS] token is first appended to the output $Z_i$ of the feature tokenizer, which is then fed into L Transformer layers, $F_1, ..., F_L$:\n$Z_{i0} = stack[[CLS], Z_i],$\n$Z_{il} = F_i(Z_{i(l-1)}); l = 1, 2, ..., L.$\nThe output of the encoder can then be used to learn contextualized knowledge from the pre-training stage and downstream tasks from the fine-tuning stage."}, {"title": "4.2 Arithmetic-Aware Pre-Training", "content": "The goal of the pre-training phase is to integrate sample-wise information into the representation of each sample; however, existing methods such as supervised contrastive learning (Khosla et al. 2020; Wang and Sun 2022; Cui et al. 2024) are ineffective in regression scenarios due to their reliance on discrete class labels, as opposed to regression's continuous labels. Also, simply relying on attention mechanisms (e.g., (Kossen et al. 2021; Somepalli et al. 2021)) underutilizes the relationship between label information across samples. To that end, we introduce a novel arithmetic-aware pretext task by conditioning continuous labels.\nAnalogous to solving for an unknown in a set of simultaneous equations in mathematics, our pre-training goal is to introduce constraints that are able to narrow the possible outcomes (i.e., search space) of the unknown. Therefore, the arithmetic-aware pre-training task is proposed to enable the model to discern relationships between samples by utilizing continuous labels in tabular regression. Intuitively, pairing sample A with different samples B, C, and D generates unique aggregated outcomes, such as A+B, A+C, and A+D. These pairs impose constraints and guide the model in learning a fine-grained representation of A that considers the context from not only itself but also other paired samples. In our work, we opt for a simple yet effective pre-training task by incorporating an arithmetic operator with two samples at a time as the pretext objective.\nAs shown in Figure 2, the arithmetic-aware pre-training process starts by selecting two random samples, $x_i$ and $x_j$, from the dataset, each with corresponding labels $y_i$ and $y_j$. These samples undergo processing through the Feature Tokenizer and Feature Encoder to produce their respective representations, $Z_{iL}$ and $Z_{jL}$:\n$Z_{i|j} = FeatureTokenizer(x_{i|j}),$\n$Z_{iL|jL} = FeatureEncoder(stack[[CLS], Z_{i|j}]),$  where $i|j$ indicates the term is either the i- or j-th sample. Subsequently, the representations of the [CLS] token $Z_{i|j}^{[CLS]}$ and $Z_{j|i}^{[CLS]}$ are extracted from $Z_{iL}$ and $Z_{jL}$, respectively. They are then concatenated and fed into a Multilayer Perceptron (MLP) to predict the outcome $\\hat{y}^{AP}$ of the arithmetic operation:\n$\\hat{y}^{AP} = MLP(concat[Z_i^{[CLS]}, Z_j^{[CLS]}]).$\nThe pre-training task involves applying arithmetic operations on the sample labels $y_i$ and $y_j$, including addition, subtraction, multiplication, and division. The resulting ground truth $y^{AP}$ for the arithmetic task is represented as:\n$y^{AP} = \\begin{cases} y_i + y_j, & \\text{for addition}; \\\\ y_i - y_j, & \\text{for subtraction}; \\\\ y_i \\cdot y_j, & \\text{for multiplication}; \\\\ y_i / y_j, & \\text{for division}. \\end{cases}$\nFinally, the model is then trained to minimize $L^{AP}$:\n$L^{AP} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i^{AP} - \\hat{y}_i^{AP})^2.$\nThis pre-training task embeds an awareness of arithmetic relationships between samples into the model, thereby enabling it to adeptly handle the irregularities of target functions. The detailed procedure of arithmetic-aware pre-training is summarized in Algorithm 1 in Appendix A.1."}, {"title": "4.3 Adaptive-Regularized Fine-Tuning", "content": "In the fine-tuning phase of APAR, we introduce an adaptiveregularized fine-tuning method that adaptively augments samples by considering feature importance and their correlated structure. As shown in Figure 3, an input sample is processed by the pre-trained feature tokenizer to produce feature embeddings, which are subsequently augmented using a dynamically adaptive gate vector. The model is fine-tuned to predict a consistent label from these variant inputs, which enables the model to perform data augmentation, guided by the model-learned importance of each feature, to improve performance on the downstream task."}, {"title": "5 Experiments", "content": "In this section, we attempt to answer the following research questions on a wide range of real-world datasets:\nRQ1: Does our proposed framework, APAR, outperform the existing NN-based and GBDT-based approaches?\nRQ2: How does the performance of the proposed arithmetic-aware pre-training task compare to other pretraining approaches in tabular regression?\nRQ3: Does the adaptive regularization enhance the model's performance during the fine-tuning phase?\nRQ4: How do different arithmetic operations affect the performance across various scenarios?"}, {"title": "5.1 Experimental Setup", "content": "Datasets Overview. In our experiments, we utilized 10 publicly real-world datasets across diverse tabular regression applications (i.e., property valuation, environmental monitoring, urban applications, and performance analysis), spanning a range of scales from large-scale (over 100K samples) to medium-scale (50K to 100K samples) and small-"}, {"title": "5.2 Quantitative Results (RQ1)", "content": "Table 2 presents the quantitative performance of APAR and the baselines. Quantitatively, APAR was consistently superior to all approaches in overall ranking across 10 diverse datasets, achieving an average RMSE improvement of 9.18% compared to the second-best ranking method. We summarize the observations as follows:\nSelection of the Feature Encoder. We can observe that TabNet(*) and FT-Transformer demonstrate better performance compared with the other baselines in all three categories since they adopt Transformer architectures as their backbones to model intricate characteristics across tabular samples as well as features. Nonetheless, the comparison of APAR, which employs the Transformer architecture in the feature encoder, and these baselines reveals the importance of considering the advantage of learning contextualized representations in a two-stage manner.\nAdvantages of the Pretrain-Finetune Approach. It is evident that comparing TabNet* with TabNet illustrates notable improvements with pre-training, demonstrating the value of the pretrain-finetune framework. However, VIME substantially hinders all performance due to not only the relatively simplified MLP architecture but also the lack of considering feature heterogeneity and rotational invariance, which again raises the need for leveraging the Transformer architecture with a feature tokenizer for tabular regression tasks. The effectiveness of our APAR highlights the capability of arithmetic-related pertaining tasks and adaptively learning contexts of features during the finetuning stage."}, {"title": "5.3 Effects of the Pre-Training Task (RQ2)", "content": "To testify the design of the pre-training task in APAR, we evaluate arithmetic-aware pre-training with four variants: 1) remove (w/o AP), replacing it with 2) feature reconstruction"}, {"title": "5.4 Effects of Adaptive Regularization (RQ3)", "content": "To investigate the impact of incorporating adaptiveregularized fine-tuning in APAR, the performance of the removal of this design (w/o AR) was compared, as shown in the RQ3 row in Table 3. Specifically, we fixed the target task loss weight $\\alpha$ at 1 and optimized the regularization loss weight $\\beta$ and sparsity loss weight $\\gamma$ using the validation dataset. Removing the adaptive-regularized technique causes the model to be prone to overfitting on uninformative features, which degrades the performance across all datasets. In contrast, APAR mitigates this limitation by adaptive regularization, leading to a substantial improvement."}, {"title": "5.5 Variants of Arithmetic Operations (RQ4)", "content": "We studied the performance of addition, subtraction, multiplication, and division across all datasets, as detailed in Table 4. It can be seen that both addition and multiplication operations are more effective than subtraction and division operations, indicating positively changing the representation of two numerical labels introduces less offset of information to learn relations compared with negatively changing. In addition, using either addition or multiplication may depend on the scale of the labels of the dataset. For example, if the labels are small (e.g., < 1), it is expected that all multiplication pairs will become near 0, leading to ambiguity for model learning. Moreover, division-based pre-training was the least consistent, often failing to converge during pre-training, as indicated by the \"-\" symbol. This is because the divided changes are too significant to learn the relations. These results highlight the adaptability of the arithmetic-aware pretraining method that is able to benefit different regression scenarios from various arithmetic operators."}, {"title": "6 Conclusion and Future Works", "content": "This paper proposes APAR, a novel arithmetic-aware pretraining and adaptive-regularized fine-tuning framework for tabular regression tasks. Distinct from existing works that ineffectively corrupt important features and transfer to regression labels due to the sparsity of the continuous space, our proposed pre-training task is able to take sample-wise interactions into account, allowing the capability of modeling from the aspects of continuous labels. Meanwhile, our adaptive-regularized fine-tuning design dynamically adjusts appropriate data augmentation by conditioning it on the selflearned feature importance. Experiments on 10 real-world datasets show that our APAR significantly outperforms state-of-the-art approaches by between 9.43% to 20.37%. We believe that APAR serves as a generic framework for tabular regression applications due to the flexible design for pretrain-finetune frameworks, and multiple interesting directions could be further explored within the framework, such as automatically selecting appropriate arithmetic operations for effective pre-training, or extending APAR to classification tasks with Boolean operations (e.g., AND), etc."}]}