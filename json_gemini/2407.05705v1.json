{"title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA", "authors": ["Jiajun Liu", "Wenjun Ke", "Peng Wang", "Jiahao Wang", "Jinhua Gao", "Ziyu Shang", "Guozheng Li", "Zijie Xu", "Ke Ji", "Yining Li"], "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLORA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.", "sections": [{"title": "1 Introduction", "content": "Knowledge graph embedding (KGE) [Wang et al., 2017; Rossi et al., 2021] aims to embed entities and relations in knowledge graphs (KGs) [Dong et al., 2014] into vectors, which is crucial for various downstream applications, such as question answering [Bordes et al., 2014], recommendation systems [Zhang et al., 2016], fact detecting [Shang et al., 2024], and information extraction [Li et al., 2022; Xu et al., 2023; Li et al., 2024]. Traditional KGE meth-ods [Bordes et al., 2013; Sun et al., 2019; Liu et al., 2020; Pan and Wang, 2021; Shang et al., 2023; Liu et al., 2023] primarily focus on dealing with static KGs. However, real-world KGs, such as Wikidata [Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014] and YAGO [Suchanek et al., 2007], are dynamic and con-tinuously evolving with emerging knowledge. For instance, Wikidata expanded from 16M entities to 46M between 2014 and 2018 [Wikidata, 2023]. A challenge with traditional KGE is that updating the embeddings of entities and re-lations requires retraining the entire KG, leading to heavy training costs in large-scale KGs. To address this issue, the continual knowledge graph embedding (CKGE) task has re-ceived growing attention to fine-tuning with only new knowl-edge [Song and Park, 2018; Daruna et al., 2021].\nThe primary challenge in CKGE lies in alleviating catas-trophic forgetting [Kirkpatrick et al., 2017; Liu et al., 2024] while simultaneously reducing training costs. One solu-tion for CKGE, known as the full-parameter fine-tuning paradigm, memorizes old knowledge by replaying a core old data set [Lopez-Paz and Ranzato, 2017; Wang et al., 2019; Kou et al., 2020] or introducing additional regularization constraints [Zenke et al., 2017; Kirkpatrick et al., 2017; Cui et al., 2023]. Although this paradigm effectively miti-gates catastrophic forgetting, it significantly increases train-ing costs, especially when handling large-scale KGs. Another solution adopts the incremental-parameter fine-tuning paradigm, with only a few parameters to learn emerging knowledge [Rusu et al., 2016; Lomonaco and Maltoni, 2017]. Despite eliminating explicit knowledge replay, the straight-forward alignment of new and old parameter dimensions may still result in an unacceptable increase in parameters and training time. Meanwhile, in recent years, to effi-ciently reduce the training time of large language models (LLMs) [Radford et al., 2019; Brown et al., 2020] for down-stream tasks, some work has employed low-rank adapters, such as LoRA [Hu et al., 2022], to lower the parameter di-mension and enable efficient parameter fine-tuning. Namely, LORA freezes the pre-trained model weights and injects train-able rank decomposition matrices into original model archi-tectures. In this paper, we try to innovatively adapt this mech-anism to address continual learning problems, i.e., CKGE.\nInspired by low-rank adaptation [Hu et al., 2022] in pa-rameter fine-tuning for LLMs, we are among the first to store new knowledge in KGs via low-rank adapters (LoRAs) to re-duce training costs. As shown in Figure 1, in contrast to the old entity Ron Weasley, new incremental LoRAs should be assigned to the new entity Remus Lupin. Moreover, recent work has discovered that new parameters of different layers should be adaptively assigned distinct dimensions [Ansuini et al., 2019], which can also be applied to emerging knowl-edge with different structural features in CKGE. As depicted in Figure 1, Remus Lupin connects with more entities than Lord Voldemort. Consequently, it retains a broader influ-ence, requiring more parameters for effective learning. In light of this consideration, we propose a fast CKGE frame-work (FastKGE), incorporating a novel incremental low-rank adapter (IncLoRA) mechanism, to learn new knowledge both effectively and efficiently. To alleviate catastrophic forget-ting, FastKGE isolates new knowledge to specific layers. Concretely, FastKGE sorts and divides the new entity rep-resentations into explicit layers according to their distance from the old KG and the degree centrality. To reduce training costs, IncLoRA embeds the specific layers into incremental low-rank adapters with fewer training parameters. Specifically, the rank scales of each layer are allocated adaptively with the awareness of the KG structure.\nTo make up for the deficiency of small initial KG size in current CKGE datasets [Hamaguchi et al., 2017; Kou et al., 2020; Daruna et al., 2021; Cui et al., 2023], we construct two new CKGE datasets: FB-CKGE and WN-CKGE based on FB15K-237 [Dettmers et al., 2018] and WN-18RR [Toutanova et al., 2015]. FB-CKGE and WN-CKGE allocate 60% of total triples to generate the larger initial KG. Results on four traditional datasets show that FastKGE re-duces training time by 34%-49% while still achieving com-petitive MRR scores in link prediction tasks against SOTAS (average in 21.0% vs. 21.1%). Meanwhile, on the new datasets FB-CKGE and WN-CKGE, FastKGE reduces 51%-68% training time while improving link prediction perfor-mance by 1.5% in MRR on average.\nTo sum up, the contributions of this paper are three-fold:\n\u2022 To the best of our knowledge, we are among the first to introduce low-rank adapters to CKGE, namely, emerg-ing knowledge can be stored in low-rank adapters to re-duce training costs and preserve old knowledge well.\n\u2022 We devise a fast CKGE framework (FastKGE), which isolates knowledge into specific layers to alleviate catas-trophic forgetting and utilizes an incremental low-rank adapter (IncLoRA) mechanism to reduce training costs.\n\u2022 Experimental results demonstrate that FastKGE signif-icantly reduces training time with competitive perfor-mance in link prediction tasks compared with SOTAs. Additionally, two new open CKGE datasets with large-scale initial KGs are released."}, {"title": "2 Related Work", "content": "In contrast to traditional knowledge graph embedding (KGE) approaches [Bordes et al., 2013; Trouillon et al., 2016; Kazemi and Poole, 2018], continual knowledge graph embed-ding (CKGE) methods [Song and Park, 2018; Daruna et al., 2021] enable KGE models to acquire new knowledge while retaining previously learned knowledge. Existing CKGE methods can be categorized into two main groups. First, full-parameter fine-tuning methods preserve learned knowl-edge by replaying old data [Lopez-Paz and Ranzato, 2017; Wang et al., 2019; Kou et al., 2020], or introducing con-straints on weight updates in neural networks [Zenke et al., 2017; Kirkpatrick et al., 2017; Cui et al., 2023]. Second, incremental-parameter fine-tuning methods [Rusu et al., 2016; Lomonaco and Maltoni, 2017] adaptively adjust archi-tectural properties to accommodate new information while preserving old parameters, thus facilitating continual learn-ing. However, these methods only focus on preserving knowl-edge while ignoring training efficiency when KGs evolve.\nIn the field of large language models (LLMs), some work tries to utilize low-rank adapters (LoRAs) to fine-tune LLMs efficiently [Hu et al., 2022; Zhang et al., 2023]. Based on it, recent work tries to utilize LoRAs to separately store knowl-edge to alleviate catastrophic forgetting in continual learn-ing [Wang et al., 2023]. In the field of KGE, recent work tries to learn a small set of reserved entities to represent all entities for parameter-efficient learning [Chen et al., 2023]. However, few works focus on efficient fine-tuning for CKGE."}, {"title": "3 Methodology", "content": "3.1 Preliminary and Problem Statement\nGrowing Knowledge Graph. A growing knowledge graph (KG) is denoted as snapshot sequences, i.e., G = {S0, S1, ..., Sn}. Snapshot Si is a triplet (Ei, Ri, Ti), where Ei, Ri and Ti denote the set of entities, relations, and triples at time i, respectively. Furthermore, we denote \u2206Ti = \u03a4i \u2013 \u03a4i-1, \u0394\u0395i = Ei - Ei-1 and ARi = Ri - Ri\u22121 as new triples, entities, and relations, respectively.\nContinual Knowledge Graph Embedding. Continual knowledge graph embedding (CKGE) aims to embed en-tities and relations into vectors in the growing KG G = {S0, S1, ..., Sn}. Specifically, when new triples ATi emerge in time i, CKGE learns the representations of new entities \u0394\u03b5i and relations ARi, and updates the representations of old entities Ei\u22121 and relations Ri\u22121 to adapt AT. Finally, all representations of entities Ei and relations Ri are obtained.\n3.2 Framework\nThe framework of FastKGE is illustrated in Figure 2. Overall, as a KG grows in each snapshot, we utilize incremental low-rank adapters (LoRAs) for different KG layers to learn and preserve new entities and relations. First, during the knowl-edge graph layering stage, we divide new entities and rela-tions into several layers based on their distances from the old graph and node degrees. Second, in the IncLoRA learning stage, embeddings of entities and relations in each layer are represented by incremental LoRAs with adaptive rank alloca-tion. Finally, in the link predicting stage, we compose all new LoRAs into a LoRA group and concat all LoRA groups and initial embeddings for inference.\n3.3 Graph Layering\nIn order to achieve separate storage of KGs and assign differ-ent ranks to LoRAs of different layers, we divide new knowl-edge into several layers according to a graph structure. For new triples ATi emerging in a new snapshot i > 0, we firstly get new entities \u0394\u03b5i and relations ARi and initiate embed-dings of them as shown in Stage 1 of Figure 2. Then, to sort and divide embeddings of AEi in snapshot i, we calculate the importance of AE by the distance from the old graph and degree centrality. Specifically, we use the breadth-first search (BFS) algorithm to progressively expand \u2206\u03b5i in snapshot i from Si\u22121. Then, we get the sorted entity sequence Sentity as follows:\n$$Sentity = [1, 2, ..., \\Delta\u03b5i]$$ (1)\nwhere for ej, ek \u2208 Sentity, if j \u2264 k, the distance of ej from the old graph is closer than ek. To further sort entities with the same distance, we denote fac(e) as the degree centrality of e in the new graph composed of new triples ATi as follows:\n$$fac(e) = \\frac{fneighbor (e)}{| \\Delta\u03b5i | \u2013 1}$$ (2)\nwhere fneighbor (e) denotes the number of the neighbors of e in AT. For entities with the same distance from the old graph in Sentity, we use fac for further sorting. Then, we divide the entities in Sentitiy, previously sorted with importance, equally into N distinct layers E = {E1, E2, ..., En}, where Ek de-notes the k-th layer of entities, and N is a hyper-parameter. For relations, we only put all new relations into a layer R rather than layering. That is because the number of entities is increasing more significantly than relations in the dynamic evolution of KGs, and the number of total embedding param-eters linearly to the number of entities in practice [Chen et al., 2023]. Therefore, we focus on storage and training optimiza-tion for new entities. Finally, we get the layering entities and relations E and R.\n3.4 IncLoRA Learning\nIncremental Low-Rank Decomposition\nTo accelerate learning, we propose an incremental low-rank adapter learning mechanism (IncLoRA) to reduce training costs. Specifically, we obtain entity and relation layers E and R from graph layering stage. Then, for each layer in E and R, we use an incremental low-rank adapter to learn and store knowledge. Take E as example, the embeddings of k-th layer Ek in E can be denoted as Ek. For learning the embeddings Ek \u2208 Rn\u00d7d, we learn matrices Ak \u2208 Rn\u00d7r and Bk \u2208 Rr\u00d7d, making it meet the following condition:\n$$Ek = AkBk$$ (3)\nwhere n denotes the number of entities in a layer Ek, d stands for embedding dimension and r refers to the rank of (Ak, Bk). We denote (Ak, Bk) as an incremental LoRA. To make sure that parameters of low-rank learning is fewer than normal training for faster learning, the decomposition should meet the following condition:\n$$r < \\frac{n \\times d}{n + d}$$ (4)\nBy this way, we compose Ek to an incremental LoRA (Ak, Bk). Finally, we get a LoRA group G for all em-beddings of \u0394\u03b5i as follows:\n$$GE = concat({(Ak \\times Bk)|1 \\leq k \\leq N})$$ (5)\nwhere concat() denotes the concatenation of several matri-ces. We can also get the LoRA group Gr for new relations by the same decomposition.\nAdaptive Rank Allocation\nIn order to preserve more information for more important en-tities, we utilize an adaptive rank allocation strategy to assign different ranks to LoRAs in different layers. Specifically, in-stead of assigning a fixed base rank rbase to all incremental LoRAs in G, we assign more ranks to more important low-rank LoRAs with higher fac. Firstly, we denote the total sum of degree centrality in the k-th layer Sumk and the average degree centrality Avgde in all layers as follows:\n$$Sumk = \\sum_{e \\in Ek}fac(e)$$ (6)\n$$Avgdc = \\frac{\\sum_{k \\in N} Sumk}{N}$$ (7)\nThen, the rk of the k-th LoRA in GE is denoted as follows:\n$$Tk = rbase \\frac{Sumk}{Avgdc}$$ (8)\nFinally, the rank of all adapters in Ge is determined. Since there is only one layer in R, we do not utilize adaptive rank allocation for GR.\nIncLoRA Training\nFrom above, we obtain the LoRA group G and G, and adaptively determine the rank of each LoRA. Then, the em-beddings of all entities Eall and all relations Rall can be de-noted as follows:\n$$Eall = concat(Eorigin, {GE|1 \\leq j \\leq i})$$ (9)\n$$Rall = concat(Rorigin, {GR|1 \\leq j \\leq i})$$ (10)\nwhere Eorigin and Rorigin denote the origin embeddings of entities and relations in snapshot 0, respectively. Finally, we train all LoRAs in G and GR with new triples \u2206T. We take TransE [Bordes et al., 2013] as the base KGE model, and the loss can be denoted as follows:\n$$L = \\sum_{(h,r,t)\\in \\Delta T} max(0, f(h, r,t) \u2013 f(h',r,t') + \\gamma)$$ (11)\nwhere (h', r, t') is the negative triple of (h, r, t) \u2208 \u0394\u03a4i, and f(h, r,t) = |h + r - t|L1/L2 is the score function of TransE. h\u2208 Eall, r\u2208 Rall, and t \u2208 Eall denote embeddings of h, r, and t, respectively. We only train the parameters in GE and Gr, and fix parameters in all other LoRA groups and origin embeddings. Finally, all representations of entities and relations, i.e., Eall and Rall are obtained.\n3.5 Link Predicting\nIn the stages of link predicting, we compose all LoRA groups as Equation 9 and 10. Taking link prediction as an example, we freeze all parameters of LoRA groups and initial embed-dings. For a given query (h, r, ?), we calculate the tail entity t that gives the highest triple score of TransE as the prediction result. Notably, FastKGE only involves assembling LoRAs into a comprehensive embedding module before the inference stage without requiring additional operations, resulting in no additional time consumption in inference."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets\nWe use six datasets in the main experiments: ENTITY, RELATION, FACT, HYBRID, FB-CKGE, and WN-CKGE. ENTITY, RELATION, FACT, and HYBRID are traditional datasets for CKGE [Cui et al., 2023], in which the num-ber of entities, relations, triples and mix of them are grow-ing equally in each snapshot, respectively. However, real-world KGs are typically built on a substantial foundational graph, to which a small increment of new knowledge merges in each snapshot. To make up for the deficiency of small ini-tial KGs in current CKGE datasets [Hamaguchi et al., 2017; Kou et al., 2020; Daruna et al., 2021; Cui et al., 2023], we construct two new datasets for CKGE: FB-CKGE and WN-CKGE, which are based on two widely-used KGE datasets FB15K-237 [Dettmers et al., 2018] and WN18RR [Toutanova et al., 2015]. In FB-CKGE and WN-CKGE, we assign 60% of the total triples to the initial snapshot, and 10% of the total triples to each next snapshot. Compared to traditional datasets ENTITY, RELATION, FACT, and HYBRID based on FB15K-237, FB-CKGE has 2 to 4 times triples in base KGs in snapshot 0. We set the snapshots for all datasets to 5. The ratio of train, valid, and test sets for all datasets is 3:1:1. The details of the datasets are shown in Table 1. The datasets are available at https://github.com/seukgcode/FastKGE.\nBaselines\nWe choose two standard baselines: incremental-parameter fine-tuning methods and full-parameter fine-tuning methods. For incremental-parameter fine-tuning methods, we choose PNN [Rusu et al., 2016] and CWR [Lomonaco and Maltoni, 2017]. For full-parameter fine-tuning methods, we choose replay-based methods GEM [Lopez-Paz and Ranzato, 2017], EMR [Wang et al., 2019], DiCGRL [Kou et al., 2020], and regularization-based methods SI [Zenke et al., 2017], EWC [Kirkpatrick et al., 2017], LKGE [Cui et al., 2023]."}, {"title": "4.2 Results", "content": "Main Results\nThe main results are shown in Table 2 and Table 3. Over-all, our method FastKGE achieves competitive performance compared to the state-of-the-art methods on all datasets. Furthermore, it outperforms all other baselines regarding time efficiency, highlighting its superior training speed.\nFirstly, FastKGE outperforms all other baselines across all datasets regarding training time efficiency. Specifically, on the four traditional datasets ENTITY, RELATION, FACT, and HYBRID, FastKGE can save 34%-49% training time compared to the fastest baselines. Notably, on the two newly constructed datasets FB-CKGE and WN-CKGE, FastKGE can save 51%-68% training time compared to the fastest baselines. This demonstrates that our method with low-rank adapters will be more efficient in larger initial graphs.\nSecondly, FastKGE has strong competitiveness in per-formance compared to the best baselines. Specifically, FastKGE achieves the best performance on two traditional datasets (ENTITY and HYBRID), and two newly constructed datasets (FB-CKGE, and WN-CKGE). Compared to the best baselines, FastKGE achieves 0.4%-1.5%, 0.7%-1.8%, 0.2%-2.8%, and 0.2%-0.9% higher in MRR, Hits@1, Hits@3, and Hits@10, respectively. Notably, on two newly constructed datasets (FB-CKGE and WN-CKGE) with extensive initial triples, FastKGE can significantly improve performance by 1.5%, 1.3%, 2.4%, and 0.6% in MRR, Hits@1, Hits@3, and Hits@10 on average compared to the best methods, respec-tively. This proves that FastKGE performs better on KGs with larger initial scales, which is more in line with the changes in real life. Besides, FastKGE also demonstrates strong com-petitiveness on two other traditional datasets (RELATION and FACT), with only 0.7%, 0.2%, 0.6%, and 0.5% decrease compared to the optimal baselines in MRR, Hits @1, Hits@3, and Hits@ 10 on average, respectively. This performance de-cline on RELATION and FACT can be attributed to the lim-ited number of entities they contain. Since our method ad-dresses this issue by employing low-rank decomposition and knowledge storage for representing new entities. Therefore, the small changes in entity number on RELATION and FACT pose a challenge in showcasing the benefits of our method.\nAblation Results\nThis section investigates the effectiveness of incremental low-rank adapter (IncLoRA) learning and graph layering strat-egy (GL). The results are shown in Table 4 and Table 5. Firstly, we find that if we remove IncLoRA and fine-tune the KGE model directly, the model performance will signifi-cantly decrease by 2.8%-9.1%, 1.3%-6.8%, 3.3%-9.4%, and 2.5%-13.8% in MRR, Hits@1, Hits@3, and Hits@10 on all datasets, respectively. This proves that IncLoRA can effec-tively preserve learned knowledge with incremental low-rank adapters. Meanwhile, with IncLoRA the training time will decrease obviously by 36%-65% on all datasets. This proves that using IncLoRA can significantly improve training effi-ciency with low-rank composition compared to direct fine-tuning. Secondly, if we remove GL and use one LoRA for all entities, the performance will decrease slightly by 0.4%-3.1% on all datasets. This proves that our multi-layer Lo-RAs are more effective than one LoRA with adaptive rank allocation. Also, the training time will decrease by 3%-18%, indicating that training and assembling multiple LoRAs re-quire additional time. Notably, the additional training time only makes the 3%-9% time of the total training time on FB-CKGE and WN-CKGE with more initial triples, where the additional time is much smaller than the whole training time.\nEffectiveness of Base Ranks\nThis section investigates the effectiveness of different base ranks on the model performance and newly added parameters. Since the number of new entities is much more significant than new relations, the number of total embedding parameters is linear to the number of entities. Therefore, we only analyze the rank rbase for entities. We conduct our experiments on ENTITY, HYBRID, FB-CKGE, and WN-CKGE. The results are shown in Figure 3. Firstly, we find a significant increase in model performance from 1.5% to 5.2% in MRR when the rank changes from 10 to 150 in all datasets. However, when the rank size exceeds 150, there is only a slight improve-ment from 0.1% to 0.2% in MRR on ENTITY, FB-CKGE, and WN-CKGE. Notably, on HYBRID, there is a slight de-crease of 0.1% in MRR when the rank changes from 150 to 200. This demonstrates that when the base rank is small, the larger the base rank, the better the performance. When the size of the base rank reaches an upper limit, the model perfor-mance hardly improves. Secondly, we find that the number of newly added parameters in the model increases continuously with the increase of rank. Therefore, the model performance does not continually improve with the increase of parameter quantity, but there is a specific upper limit. Combining Equa-tion 4, we find that when the number of new entities is much greater than the dimension, the upper-rank limit is approxi-mately equal to the dimension, which is consistent with the upper limit of the rank 200 shown in the experimental results.\nEffectiveness of Layer Numbers\nIn this section, we investigate the effectiveness of different layer numbers N on model performance. We conduct exper-iments on two traditional datasets (ENTITY and HYBRID) and two new datasets (FB-CKGE and WN-CKGE), as shown in Figure 4. Firstly, on ENTITY and HYBRID, the model performance peaks when layer number equals five and then decreases by 0.4%-0.5% in MRR from 5 to 20 layers. This indicates that the layer should be set to a small number for the datasets with small initial graphs. Secondly, we observe that on FB-CKGE and WN-CKGE, the model performance in-creases by 0.4%-0.6% in MRR when the layer number ranges from 2 to 20. It proves that graph layering can achieve more significant results with larger initial KGs.\nDifferent Base Models\nIn this paper, although we choose the most typical model, TransE, as the base KGE model, our method can still be extended easily to other base KGE models. To verify the scalability of our method FastKGE, we extend FastKGE to two other different types of KGE models, a standard bilinear-based model ComplEx [Trouillon et al., 2016] and a typical roto-translation-based Model RotatE [Sun et al., 2019]. We conduct the experiments on FB-CKGE, and the results are shown in Figure 5. Firstly, we observe that FastKGE out-performs direct fine-tuning by 2.6%-3.1% in MRR on Com-plEx and Rotate, respectively. Meanwhile, for KGE models with better performance, applying our method will also lead to better performance. This indicates that FastKGE can effec-tively store the learned knowledge for different KGEs, allevi-ating catastrophic forgetting in continual learning. Secondly, our method can significantly shorten the time from 61.3% to 70.1% for different KGEs. This demonstrates that FastKGE has robust scalability for different KGE models to accelerate the training process.\n4.3 Case Study\nWe conduct a case study to verify that FastKGE learns more critical entities well by graph layering with dynamic rank al-location. As shown in Table 6, we select two new entities with different importance, namely, with the same distance from the old graph (dis=1) but different fdc, and test the prediction re-sults with layering and non-layering strategies. Results show that for entity Sedona with small fdc = 4.7e - 4, both strate-gies can rank it first. However, for entity Robert Morse with larger fdc = 1.18e \u2013 3, the rank of the correct answer will increase after layering. It proves that graph layering learns critical new entities effectively without affecting others."}, {"title": "5 Conclusion", "content": "This paper proposes a novel fast learning framework for CKGE, FastKGE, which utilizes an IncLoRA mechanism to preserve learned knowledge well and accelerate fine-tuning. Firstly, to alleviate catastrophic forgetting, we conduct graph layering for new knowledge to achieve separate storage of old and new knowledge. Moreover, to reduce training costs, we propose incremental low-rank adapter learning to learn new knowledge efficiently with adaptive rank allocation. In the future, we will explore how to conduct CKGE learning when the knowledge of growing KGs is forgotten or modified."}]}