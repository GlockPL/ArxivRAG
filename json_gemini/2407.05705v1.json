{"title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA", "authors": ["Jiajun Liu", "Wenjun Ke", "Peng Wang", "Jiahao Wang", "Jinhua Gao", "Ziyu Shang", "Guozheng Li", "Zijie Xu", "Ke Ji", "Yining Li"], "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLORA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.", "sections": [{"title": "1 Introduction", "content": "Knowledge graph embedding (KGE) aims to embed entities and relations in"}, {"title": "2 Related Work", "content": "In contrast to traditional knowledge graph embedding (KGE) approaches, continual knowledge graph embedding (CKGE) methods enable KGE models to acquire new knowledge while retaining previously learned knowledge. Existing CKGE methods can be categorized into two main groups. First, full-parameter fine-tuning methods preserve learned knowledge by replaying old data, or introducing constraints on weight updates in neural networks. Second, incremental-parameter fine-tuning methods adaptively adjust architectural properties to accommodate new information while preserving old parameters, thus facilitating continual learning. However, these methods only focus on preserving knowledge while ignoring training efficiency when KGs evolve.\nIn the field of large language models (LLMs), some work tries to utilize low-rank adapters (LoRAs) to fine-tune LLMs efficiently. Based on it, recent work tries to utilize LoRAs to separately store knowledge to alleviate catastrophic forgetting in continual learning. In the field of KGE, recent work tries to learn a small set of reserved entities to represent all entities for parameter-efficient learning. However, few works focus on efficient fine-tuning for CKGE."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminary and Problem Statement", "content": "Growing Knowledge Graph. A growing knowledge graph (KG) is denoted as snapshot sequences, i.e., G = {S_0, S_1, ..., S_n}. Snapshot S_i is a triplet (E_i, R_i, T_i), where E_i, R_i and T_i denote the set of entities, relations, and triples at time i, respectively. Furthermore, we denote \\Delta T_i = T_i \u2013 T_{i-1}, \\Delta E_i = E_i - E_{i-1} and \\Delta R_i = R_i - R_{i-1} as new triples, entities, and relations, respectively."}, {"title": "3.2 Framework", "content": "The framework of FastKGE is illustrated in Figure 2. Overall, as a KG grows in each snapshot, we utilize incremental low-rank adapters (LoRAs) for different KG layers to learn and preserve new entities and relations. First, during the knowledge graph layering stage, we divide new entities and relations into several layers based on their distances from the old graph and node degrees. Second, in the IncLoRA learning stage, embeddings of entities and relations in each layer are represented by incremental LoRAs with adaptive rank allocation. Finally, in the link predicting stage, we compose all new LoRAs into a LoRA group and concat all LoRA groups and initial embeddings for inference."}, {"title": "3.3 Graph Layering", "content": "In order to achieve separate storage of KGs and assign different ranks to LoRAs of different layers, we divide new knowledge into several layers according to a graph structure. For new triples \\Delta T_i emerging in a new snapshot i > 0, we firstly get new entities \\Delta E_i and relations \\Delta R_i and initiate embeddings of them as shown in Stage 1 of Figure 2. Then, to sort and divide embeddings of \\Delta E_i in snapshot i, we calculate the importance of \\Delta E_i by the distance from the old graph and degree centrality. Specifically, we use the breadth-first search (BFS) algorithm to progressively expand \\Delta \\varepsilon_i in snapshot i from S_{i-1}. Then, we get the sorted entity sequence S_{entity} as\n\nS_{entity} = [1, 2, ..., \\Delta \\varepsilon_i ] \\tag{1}\nwhere for e_j, e_k \\in S_{entity}, if j \\le k, the distance of e_j from the old graph is closer than e_k. To further sort entities with the same distance, we denote fac(e) as the degree centrality of e in the new graph composed of new triples \\Delta T_i as follows:\n\nf_{ac}(e) = \\frac{f_{neighbor}(e)}{|\\Delta E_i| \u2013 1} \\tag{2}\nwhere f_{neighbor}(e) denotes the number of the neighbors of e in \\Delta T_i. For entities with the same distance from the old graph in S_{entity}, we use fac for further sorting. Then, we divide the entities in S_{entitiy}, previously sorted with importance, equally into N distinct layers E = {E_1, E_2, ..., E_N}, where E_k denotes the k-th layer of entities, and N is a hyper-parameter. For relations, we only put all new relations into a layer R rather than layering. That is because the number of entities is increasing more significantly than relations in the dynamic evolution of KGs, and the number of total embedding parameters linearly to the number of entities in practice . Therefore, we focus on storage and training optimization for new entities. Finally, we get the layering entities and relations E and R."}, {"title": "3.4 IncLoRA Learning", "content": "Incremental Low-Rank Decomposition\nTo accelerate learning, we propose an incremental low-rank adapter learning mechanism (IncLoRA) to reduce training costs. Specifically, we obtain entity and relation layers E and R from graph layering stage. Then, for each layer in E and R, we use an incremental low-rank adapter to learn and store knowledge. Take E_k as example, the embeddings of k-th layer E_k in E can be denoted as E_k. For learning the embeddings E_k \\in \\mathbb{R}^{n \\times d}, we learn matrices A_k \\in \\mathbb{R}^{n \\times r} and B_k \\in \\mathbb{R}^{r \\times d},\n\nmaking it meet the following condition:\n\nE_k = A_k B_k \\tag{3}\nwhere n denotes the number of entities in a layer E_k, d stands for embedding dimension and r refers to the rank of (A_k, B_k). We denote (A_k, B_k) as an incremental LoRA. To make sure that parameters of low-rank learning is fewer than normal training for faster learning, the decomposition should meet the following condition:\n\nr < \\frac{n \\times d}{n + d} \\tag{4}\nBy this way, we compose E_k to an incremental LoRA (A_k, B_k). Finally, we get a LoRA group G^E for all embeddings of \\Delta E_i as follows:\n\nG^E = concat({(A_k \\times B_k )|1 \\le k \\le N}) \\tag{5}\nwhere concat() denotes the concatenation of several matrices. We can also get the LoRA group G^R for new relations by the same decomposition.\nAdaptive Rank Allocation\nIn order to preserve more information for more important entities, we utilize an adaptive rank allocation strategy to assign different ranks to LoRAs in different layers. Specifically, instead of assigning a fixed base rank r_{base} to all incremental LoRAs in G^E, we assign more ranks to more important low-rank LoRAs with higher f_{ac}. Firstly, we denote the total sum of degree centrality in the k-th layer Sum_k and the average degree centrality Avg_{dc} in all layers as follows:\n\nSum_k = \\sum_{e \\in E_k} f_{ac}(e) \\tag{6}\n\nAvg_{dc} = \\frac{\\sum_{k \\in E} Sum_k}{N} \\tag{7}\nThen, the r_k of the k-th LoRA in G^E is denoted as follows:\n\nr_k = r_{base} \\frac{Sum_k}{Avg_{dc}} \\tag{8}\nFinally, the rank of all adapters in G^E is determined. Since there is only one layer in R, we do not utilize adaptive rank allocation for G^R.\nIncLoRA Training\nFrom above, we obtain the LoRA group G^E and G^R, and adaptively determine the rank of each LoRA. Then, the embeddings of all entities E_{all} and all relations R_{all} can be denoted as follows:\n\nE_{all} = concat(E_{origin}, {G^E | 1 \\le j \\le i}) \\tag{9}\n\nR_{all} = concat(R_{origin}, {G^R | 1 \\le j \\le i}) \\tag{10}\nwhere E_{origin} and R_{origin} denote the origin embeddings of entities and relations in snapshot 0, respectively. Finally, we train all LoRAs in G^E and G^R with new triples \\Delta T. We take TransE  as the base KGE model, and the loss can be denoted as follows:\n\n\\mathcal{L} = \\sum_{(h, r, t) \\in \\Delta T_i} max(0, f(h, r, t) \u2013 f(h', r, t') + \\gamma) \\tag{11}\nwhere (h', r, t') is the negative triple of (h, r, t) \\in \\Delta T_i, and f(h, r, t) = |h + r - t|_{L1/L2} is the score function of TransE. h \\in E_{all}, r \\in R_{all}, and t \\in E_{all} denote embeddings of h, r, and t, respectively. We only train the parameters in G^E and G^R, and fix parameters in all other LoRA groups and origin embeddings. Finally, all representations of entities and relations, i.e., E_{all} and R_{all} are obtained."}, {"title": "3.5 Link Predicting", "content": "In the stages of link predicting, we compose all LoRA groups as Equation 9 and 10. Taking link prediction as an example, we freeze all parameters of LoRA groups and initial embeddings. For a given query (h, r, ?), we calculate the tail entity t that gives the highest triple score of TransE as the prediction result. Notably, FastKGE only involves assembling LoRAs into a comprehensive embedding module before the inference stage without requiring additional operations, resulting in no additional time consumption in inference."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets\nWe use six datasets in the main experiments: ENTITY, RELATION, FACT, HYBRID, FB-CKGE, and WN-CKGE. ENTITY, RELATION, FACT, and HYBRID are traditional datasets for CKGE , in which the number of entities, relations, triples and mix of them are growing equally in each snapshot, respectively. However, real-world KGs are typically built on a substantial foundational graph, to which a small increment of new knowledge merges in each snapshot. To make up for the deficiency of small initial KGs in current CKGE datasets, we construct two new datasets for CKGE: FB-CKGE and WN-CKGE, which are based on two widely-used KGE datasets FB15K-237  and WN18RR. In FB-CKGE and WN-CKGE, we assign 60% of the total triples to the initial snapshot, and 10% of the total triples to each next snapshot. Compared to traditional datasets ENTITY, RELATION, FACT, and HYBRID based on FB15K-237, FB-CKGE has 2 to 4 times triples in base KGs in snapshot 0. We set the snapshots for all datasets to 5. The ratio of train, valid, and test sets for all datasets is 3:1:1. The details of the datasets are shown in Table 1. The datasets are available at https://github.com/seukgcode/FastKGE.\nBaselines\nWe choose two standard baselines: incremental-parameter fine-tuning methods and full-parameter fine-tuning methods. For incremental-parameter fine-tuning methods, we choose PNN  and CWR. For full-parameter fine-tuning methods, we choose replay-based methods GEM , EMR , DiCGRL, and regularization-based methods SI, EWC , LKGE."}, {"title": "Metrics", "content": "We assess the performance of our model in the link prediction task. Specifically, we substitute the head or tail entity of the triples in the test set with all other entities and then calculate and rank the scores for each triple. Subsequently, we measure the Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 as evaluation metrics. The higher MRR, Hits@1, Hits@3, and Hits@10 indicate better model performance. For each snapshot i, we compute the average of the above metrics evaluated on all the test sets of current and previous snapshots as the final metric. The main results are derived from the model generated at the final time. In addition, we calculate the total training time of all snapshots to evaluate the time efficiency."}, {"title": "Settings", "content": "All experiments are conducted on the NVIDIA RTX 3090Ti GPU. The codes of the experiments are supported by Py-Torch. The number of snapshots is set to 5. We choose the batch size from [258, 512, 1024] and the learning rate from [1e-1, 2e-1, 3e-1]. We choose the Adam as the optimizer. In our experiments, we set the entity base rank of LORA from the range [10, 50, 100, 150, 200] and the relation base rank to 20. Also, we set the number of LoRA layers N from the range [2, 5, 10, 20]. We set the embedding size for all methods to 200. To fairness, all the results are from the average of random five running times, and the patience of early stopping is 3 for all methods to compare time efficiency."}, {"title": "4.2 Results", "content": ""}, {"title": "Main Results", "content": "The main results are shown in Table 2 and Table 3. Overall, our method FastKGE achieves competitive performance compared to the state-of-the-art methods on all datasets. Furthermore, it outperforms all other baselines regarding time efficiency, highlighting its superior training speed.\nFirstly, FastKGE outperforms all other baselines across all datasets regarding training time efficiency. Specifically, on the four traditional datasets ENTITY, RELATION, FACT, and HYBRID, FastKGE can save 34%-49% training time compared to the fastest baselines. Notably, on the two newly constructed datasets FB-CKGE and WN-CKGE, FastKGE can save 51%-68% training time compared to the fastest baselines. This demonstrates that our method with low-rank adapters will be more efficient in larger initial graphs.\nSecondly, FastKGE has strong competitiveness in performance compared to the best baselines. Specifically, FastKGE achieves the best performance on two traditional datasets (ENTITY and HYBRID), and two newly constructed datasets (FB-CKGE, and WN-CKGE). Compared to the best baselines, FastKGE achieves 0.4%-1.5%, 0.7%-1.8%, 0.2%-2.8%, and 0.2%-0.9% higher in MRR, Hits@1, Hits@3, and Hits@10, respectively. Notably, on two newly constructed datasets (FB-CKGE and WN-CKGE) with extensive initial triples, FastKGE can significantly improve performance by 1.5%, 1.3%, 2.4%, and 0.6% in MRR, Hits@1, Hits@3, and Hits@10 on average compared to the best methods, respectively. This proves that FastKGE performs better on KGs with larger initial scales, which is more in line with the changes in real life. Besides, FastKGE also demonstrates strong competitiveness on two other traditional datasets (RELATION and FACT), with only 0.7%, 0.2%, 0.6%, and 0.5% decrease compared to the optimal baselines in MRR, Hits @1, Hits@3, and Hits@10 on average, respectively. This performance decline on RELATION and FACT can be attributed to the limited number of entities they contain. Since our method addresses this issue by employing low-rank decomposition and knowledge storage for representing new entities. Therefore, the small changes in entity number on RELATION and FACT pose a challenge in showcasing the benefits of our method."}, {"title": "Ablation Results", "content": "This section investigates the effectiveness of incremental low-rank adapter (IncLoRA) learning and graph layering strategy (GL). The results are shown in Table 4 and Table 5. Firstly, we find that if we remove IncLoRA and fine-tune the KGE model directly, the model performance will significantly decrease by 2.8%-9.1%, 1.3%-6.8%, 3.3%-9.4%, and 2.5%-13.8% in MRR, Hits@1, Hits@3, and Hits@10 on all datasets, respectively. This proves that IncLoRA can effectively preserve learned knowledge with incremental low-rank adapters. Meanwhile, with IncLoRA the training time will decrease obviously by 36%-65% on all datasets. This proves that using IncLoRA can significantly improve training efficiency with low-rank composition compared to direct fine-tuning. Secondly, if we remove GL and use one LoRA for all entities, the performance will decrease slightly by 0.4%-3.1% on all datasets. This proves that our multi-layer Lo-RAs are more effective than one LoRA with adaptive rank allocation. Also, the training time will decrease by 3%-18%, indicating that training and assembling multiple LoRAs require additional time. Notably, the additional training time only makes the 3%-9% time of the total training time on FB-CKGE and WN-CKGE with more initial triples, where the"}, {"title": "Effectiveness of Base Ranks", "content": "This section investigates the effectiveness of different base ranks on the model performance and newly added parameters. Since the number of new entities is much more significant than new relations, the number of total embedding parameters is linear to the number of entities. Therefore, we only analyze the rank r_{base} for entities. We conduct our experiments on ENTITY, HYBRID, FB-CKGE, and WN-CKGE. The results are shown in Figure 3. Firstly, we find a significant increase in model performance from 1.5% to 5.2% in MRR when the rank changes from 10 to 150 in all datasets. However, when the rank size exceeds 150, there is only a slight improvement from 0.1% to 0.2% in MRR on ENTITY, FB-CKGE, and WN-CKGE. Notably, on HYBRID, there is a slight decrease of 0.1% in MRR when the rank changes from 150 to 200. This demonstrates that when the base rank is small, the larger the base rank, the better the performance. When the size of the base rank reaches an upper limit, the model performance hardly improves. Secondly, we find that the number of newly added parameters in the model increases continuously with the increase of rank. Therefore, the model performance does not continually improve with the increase of parameter quantity, but there is a specific upper limit. Combining Equation 4, we find that when the number of new entities is much"}, {"title": "Effectiveness of Layer Numbers", "content": "In this section, we investigate the effectiveness of different layer numbers N on model performance. We conduct experiments on two traditional datasets (ENTITY and HYBRID) and two new datasets (FB-CKGE and WN-CKGE), as shown in Figure 4. Firstly, on ENTITY and HYBRID, the model performance peaks when layer number equals five and then decreases by 0.4%-0.5% in MRR from 5 to 20 layers. This indicates that the layer should be set to a small number for the datasets with small initial graphs. Secondly, we observe that on FB-CKGE and WN-CKGE, the model performance increases by 0.4%-0.6% in MRR when the layer number ranges from 2 to 20. It proves that graph layering can achieve more significant results with larger initial KGs."}, {"title": "Different Base Models", "content": "In this paper, although we choose the most typical model, TransE, as the base KGE model, our method can still be extended easily to other base KGE models. To verify the scalability of our method FastKGE, we extend FastKGE to two other different types of KGE models, a standard bilinear-based model ComplEx and a typical roto-translation-based Model RotatE . We conduct the experiments on FB-CKGE, and the results are shown in Figure 5. Firstly, we observe that FastKGE outperforms direct fine-tuning by 2.6%-3.1% in MRR on ComplEx and RotatE, respectively. Meanwhile, for KGE models with better performance, applying our method will also lead to better performance. This indicates that FastKGE can effectively store the learned knowledge for different KGEs, alleviating catastrophic forgetting in continual learning. Secondly, our method can significantly shorten the time from 61.3% to 70.1% for different KGEs. This demonstrates that FastKGE has robust scalability for different KGE models to accelerate the training process."}, {"title": "4.3 Case Study", "content": "We conduct a case study to verify that FastKGE learns more critical entities well by graph layering with dynamic rank allocation. As shown in Table 6, we select two new entities with different importance, namely, with the same distance from the old graph (dis=1) but different f_{dc}, and test the prediction results with layering and non-layering strategies. Results show that for entity Sedona with small f_{dc} = 4.7e - 4, both strategies can rank it first. However, for entity Robert Morse with larger f_{dc} = 1.18e \u2013 3, the rank of the correct answer will increase after layering. It proves that graph layering learns critical new entities effectively without affecting others."}, {"title": "5 Conclusion", "content": "This paper proposes a novel fast learning framework for CKGE, FastKGE, which utilizes an IncLoRA mechanism to preserve learned knowledge well and accelerate fine-tuning. Firstly, to alleviate catastrophic forgetting, we conduct graph layering for new knowledge to achieve separate storage of old and new knowledge. Moreover, to reduce training costs, we propose incremental low-rank adapter learning to learn new knowledge efficiently with adaptive rank allocation. In the future, we will explore how to conduct CKGE learning when the knowledge of growing KGs is forgotten or modified."}]}