{"title": "Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance", "authors": ["Nicolas Devatine", "Louis Abraham"], "abstract": "Assessing the extent of human edits on texts generated by Large Language Models (LLMs) is crucial to understanding the human-AI interactions and improving the quality of automated text generation systems. Existing edit distance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to accurately measure the effort required for post-editing, especially when edits involve substantial modifications, such as block operations. In this paper, we introduce a novel compression-based edit distance metric grounded in the Lempel-Ziv-77 algorithm, designed to quantify the amount of post-editing applied to LLM-generated texts. Our method leverages the properties of text compression to measure the informational difference between the original and edited texts. Through experiments on real-world human edits datasets, we demonstrate that our proposed metric is highly correlated with actual edit time and effort. We also show that LLMs exhibit an implicit understanding of editing speed, that aligns well with our metric. Furthermore, we compare our metric with existing ones, highlighting its advantages in capturing complex edits with linear computational efficiency. Our code and data are available at: https://github.com/ NDV-tiime/CompressionDistance.", "sections": [{"title": "1 Introduction", "content": "Recent advances in LLMs and text generation models have enabled the production of high-quality texts for a wide range of applications. Despite their impressive capabilities, LLMs often produce outputs that require human intervention to refine, correct, or adapt to specific contexts (Gehrmann et al., 2022). For example, in the context of a company using LLMs to draft customer emails, assessing the level of human intervention can guide system developers to refine models, enhance user experiences, and reduce costs by understanding how much work humans still need to perform. However,\nquantifying this editing effort is challenging, as it often involves not just minor fixes but also substantial restructuring or extensive content changes. Understanding the extent of human edits on LLM-generated texts is therefore essential to evaluate model performance and improve human-AI collaboration.\nExisting metrics to measure the difference between texts, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005), and BERTScore (Zhang et al., 2020), have been widely used in machine translation, summarization, and text generation tasks. However, these metrics often fail to capture the complexity of human edits, especially when edits involve substantial rephrasing, restructuring, or content modifications.\nIn the context of post-editing effort in machine translation, metrics such as HTER (Specia and Farzindar, 2010) and CharacTER (Wang et al., 2016) have been proposed to estimate the amount of human effort required to correct machine-generated translations. However, these metrics focus primarily on surface-level changes and may not fully account for the deeper semantic and structural edits commonly made on LLM-generated texts.\nIn this paper, we propose a novel approach for measuring human edits on LLM-generated texts using a compression-based edit distance. Our metric is inspired by the concept of compression distance, which has been explored in the context of segment rearrangements by Erg\u00fcn et al. (2003). Unlike traditional edit distance metrics that focus on character-level operations, such as insertions, deletions, and substitutions, Erg\u00fcn et al. (2003) consider a richer set of operations, including segment rearrangements, like substring relocations, duplications, and deletions. Their work provides approximation algorithms for efficiently computing similarity between sequences undergoing such complex transformations. Inspired by this, our compression-"}, {"title": "2 Related Work", "content": "Measuring the similarity or difference between texts is a fundamental task in natural language processing, with applications in machine translation evaluation, text summarization, plagiarism detection, and more. Traditional edit distance metrics, such as the Levenshtein distance (Levenshtein, 1966), compute the minimum number of character-level insertions, deletions, and substitutions required to transform one string into another. While useful, these metrics often fail to capture semantic differences and are sensitive to surface-level variations, like swapping two paragraphs.\nIn machine translation, metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) have been widely adopted to evaluate the quality of generated translations and summaries by comparing n-gram overlaps with reference texts. However, these metrics are limited in their ability to account for paraphrasing and do not always correlate well with human judgments (Callison-Burch et al., 2006).\nTranslation Edit Rate (TER) (Snover et al., 2006) and its human-targeted variant HTER (Specia and Farzindar, 2010) were introduced to better measure the post-editing effort required to correct machine translations. TER considers the number of\nedit operations at the word level, including shifts (block movements), to transform a system output into a reference translation. However, even with these enhancements, TER may not fully capture the complexity of human edits, particularly when significant restructuring is involved.\nRecent work has explored alternative approaches to better estimate the post-editing effort and capture semantic similarities. Metrics such as METEOR (Banerjee and Lavie, 2005) incorporate synonymy and paraphrasing through the use of linguistic resources such as WordNet (Miller, 1994). BERTScore (Zhang et al., 2020) leverages contextual embeddings from pre-trained language models to compute similarity at the semantic level. Although these methods improve correlation with human judgments, they can be computationally intensive and may still miss structural changes.\nCompression-based distances offer a different perspective by measuring the amount of information shared between sequences. The Normalized Compression Distance (NCD) (Cilibrasi and Vitanyi, 2004) is a metric derived from the Kolmogorov complexity, approximated using standard compression algorithms. NCD has been applied in various domains, including clustering and anomaly detection (Jiang et al., 2023). However, NCD can be sensitive to the choice of compression algorithm and its parameters. In the context of text classification, Jiang et al. (2023) proposed a parameter-free classification method using compressors, demonstrating that compression can be effective in low-resource settings. Their work highlights the potential of compression-based methods in deep learning and NLP tasks.\nOur work focuses on a specific compression distance designed to compare sequences with segment rearrangements, as introduced by Erg\u00fcn et al. (2003). This distance allows operations on substrings, such as move, copy, and delete, that align closely with the types of edits humans perform when refining texts. Computing the exact segment rearrangement distance between two sequences is NP-hard; however, Erg\u00fcn et al. (2003) proposed a constant-factor approximation using compression distances as upper bounds. The authors showed that the segment rearrangement distance can be approximated in linear time using the Lempel-Ziv-77 compression. We used the efficient algorithm proposed by Crochemore et al. (2008) for computing the Lempel-Ziv factorization, enabling linear-time computation suitable for practical applications."}, {"title": "3 Compression Distance", "content": "Our compression-based distance metric is inspired by the work of Erg\u00fcn et al. (2003), who formalized sequence similarity in the presence of both character-level and segment rearrangement edits. They proposed an efficient approximation algorithm that estimates the segment rearrangement distance up to a constant factor using compression-inspired techniques.\nGiven two sequences S (source) and T (target), the problem involves computing the minimum number of edit operations\u2014character edits, substring deletions, relocations, and duplications\u2014required to transform S into T. This problem generalizes the classical edit distance with additional flexibility for substring-level operations. Unfortunately, computing the exact solution is NP-hard due to the combinatorial explosion of potential edits. Thus, Erg\u00fcn et al. (2003) proposed an approximation based on data compression, specifically leveraging properties of the Lempel-Ziv-77 (LZ77) algorithm, which provides an efficient approximation of the segment rearrangement distance. Formally, for a given sequence S, LZ77 partitions S into a sequence of non-overlapping phrases\u00b9 such that each new phrase is the longest match that occurs earlier in the sequence. The number of phrases in this factorization is a proxy for the complexity of S. The compression distance d(S \u2192 T) is then defined as:\nd(S \u2192 T) = LZ(S | T) \u2013 LZ(S),\nwhere S | T is the concatenation of S and T with a delimiter, and LZ(\u00b7) denotes the number of phrases in the LZ77 factorization. Intuitively, this measures the additional complexity of concatenating T to S. Erg\u00fcn et al. (2003) proved that it gives a 4-approximation upper bound to the exact distance, that can be efficiently computed. We apply the algorithm proposed by Crochemore et al. (2008) for computing the Lempel-Ziv factorization in linear time from suffix arrays. as implemented in pydivsufsort\u00b2 (Abraham, 2023)."}, {"title": "4 Experimental Settings", "content": "We now turn to the empirical evaluation of our compression-based edit distance. Our goals are to:\n(1) validate that it correlates strongly with actual human editing effort, and (2) compare it against traditional and well-established edit distance metrics."}, {"title": "4.1 Datasets", "content": "In the following, we introduce a new dataset of LLM-generated answers to accounting questions. This dataset includes both synthetic edits produced by an LLM itself (under different editing scenarios) and human edits produced by expert annotators. Additionally, we experimented with the publicly available IWSLT 2019 post-editing dataset (Scarton et al., 2019)."}, {"title": "4.1.1 Accounting Q&A Edits", "content": "We first randomly sampled 200 French accounting-related questions from a proprietary Q&A knowledge base. Each question has an associated expert gold standard answer that we refer to as expert knowledge throughout this paper, providing accurate domain-specific information. We then used claude-3-sonnet-20240229 (March 2024) from Anthropic (Anthropic, 2024) to generate initial answers to these questions without providing the LLM with the expert knowledge, potentially leading to wrong or incomplete answers.\nHuman Edits. We recruited four expert annotators with domain experience in accounting. Each annotator received the same set of 200 initial LLM-generated responses, together with the relevant associated expert knowledge. They were asked to"}, {"title": "4.1.2 IWSLT 2019", "content": "For further comparison with existing edit distance metrics, we draw on a publicly available dataset\n(Scarton et al., 2019)3. This dataset consists of 1,047 English\u2013Spanish segments (totaling 26,875 words) that were machine-translated by 41 different systems. Five professional translators performed a post-editing task on every segment. The translators used the PET tool (Aziz et al., 2012), which logs all edit operations along with the time taken to post-edit. However, block edit operations are not directly supported in this tool. The dataset thus provides a rich set of human edits, including the total edit time and the number of keystrokes per edit."}, {"title": "4.2 Evaluation", "content": "We evaluated how well our compression-based edit distance and various baselines correlate with human post-editing effort across the different datasets. First, we computed the distance values for all tested metrics between the initial text and its edited version. We use Pearson's correlation as our primary statistic, as it provides a straightforward measure of linear relationship between a metric's output and actual editing effort. All reported correlation coefficients are statistically significant (p < 0.05). We compare our compression-"}, {"title": "5 Results", "content": "From Table 1, our compression-based distance consistently shows stronger correlations with human editing times than most other metrics, notably reaching 0.81 when the original LLM output is concatenated with expert knowledge. The Levenshtein distance also exhibits a relatively high correlation (0.59-0.68), but tends to be outperformed by the\ncompression distance when the knowledge text is included. BLEU, ROUGE-L, and BERTScore exhibit generally weaker correlations with observed edit times, suggesting that n-gram or semantic overlap alone is less effective at capturing complex structural edits. Overall, these findings suggest that when relying on externally provided knowledge for corrections (e.g., copy-pasting relevant text), metrics that model substring-level transformations (such as our compression-based method) better reflect actual editing time. Figure 4 visually compares the edit distance metrics against measured edit times on our human edits using linear regressions, again revealing that traditional metrics do not effectively capture the edit time. In contrast, our compression-based metric aligns the best with actual edit times.\nTable 2 confirms the consistency of compression distance on the IWSLT2019 dataset, showing robust correlations with both edit time and keystrokes across multiple annotators. Although the Levenshtein distance demonstrates comparable results, this could be attributed to the fact that block operations, which the compression distance is particularly effective at measuring (as shown in Table 1), are not directly supported by the PET edit tool. Other metrics such as TER or CharacTER lag behind in capturing post-editing effort.\nTables 3 and 4 present the R2 scores from our 5-NN regressor approach. On the IWSLT2019 dataset (Table 3), the compression distance yields a high R2 for keystrokes (0.6053) and a moderate R2 for the edit time, closely matching Levenshtein. On our human post-edited dataset (Table 4), the compression distance emerges as the top metric for predicting actual edit times, confirming its ability to model the cognitive and manual aspects of editing when used in the context of LLM-generated texts. We also tested a range of metric combinations, but these multi-feature regressors did not provide meaningful improvements in R2.\nRegarding our synthetic edits, Figure 2 and 3 show that the compression distances in the similar scenario are typically smaller than those in the normal and fast scenarios. A linear regression between normal-edit compression distances and each of the other two scenarios reveals slopes around 0.75-0.81. This reflects that the faster or more structure-preserving the edit, the lower the compression distance, again suggesting the met-"}, {"title": "6 Discussion", "content": "Our experimental results indicate that substring-level transformations, as captured by compression-based distances, mirror the true effort involved in post-editing. Linear correlation patterns confirm that these substring-based metrics better predict editing times and keystrokes compared to traditional n-gram overlap measures or purely semantic similarity scores.\nA notable outcome is the improved correlation of compression distance when the expert knowledge text is concatenated to the original LLM output. This suggests that editors frequently copy or reuse sentences directly from the expert knowledge to refine the answers. Traditional edit distances (e.g., Levenshtein) are not as robust to such copy-paste patterns, yielding weaker correlations once the knowledge text is included. In contrast, our compression-based metric detects repeated substrings of arbitrary length, effectively capturing the\nreuse of entire segments from the expert knowledge text. Although the Levenshtein distance also showed high correlations, it tends to over-penalize large blocks of reordering or insertion. It is particularly apparent when block edit operations are allowed (which was not the case in the IWSLT2019 dataset).\nOur additional regression experiments offer complementary insights. Regression with single features provides a coherent measure of how strongly each metric alone can account for editing variance. In particular, the compression distance outperforms other metrics in predicting both the edit time (Tables 4 and 3) and keystrokes. Interestingly, attempts to combine multiple metrics within KNN did not yield further gains. One possible explanation is that the compression distance already capture a large portion of the editing signal, making the additional information from n-gram overlaps or semantic similarity somewhat redundant.\nOur synthetic edits reveal that in the scenario in which the LLM is asked to produce fast edits, it results in smaller compression distances than in the normal edit scenario (Figure 3), indicating reduced editing effort. This result highlights that the LLM's understanding of fast editing, when explicitly instructed, is aligned with a reduced edit distance and, notably, with the compression distance. Furthermore, the regression plot and distance distributions reveal that the compression distances of edits produced when the LLM is instructed to preserve the main structure of the initial answer or to edit quickly (the similar and fast scenarios) are lower compared to unconstrained edits. This indicates that the LLM demonstrates a human-like understanding of varying levels of editing effort.\nAnother important aspect is the computation time. Many of the commonly used metrics (e.g., BLEU, ROUGE, and Levenshtein distance) have quadratic (or higher) complexities that are acceptable for moderately sized texts. Our compression-based approach can be performed in linear time relative to text length (Erg\u00fcn et al., 2003; Crochemore et al., 2008), making it scalable for large documents compared to resource-intensive metrics (e.g., BERTScore)."}, {"title": "7 Conclusion", "content": "We introduced a novel compression-based edit distance metric that leverages substring-level transformations to better capture the complexity of human"}, {"title": "A Prompts", "content": "This appendix details the prompts used in our experiments to generate and edit answers. Although the original prompts were in French to match our dataset's language, we provide English translations here. The original French prompts are provided in our code repository."}, {"title": "A.1 Initial Answer Generation", "content": "The first prompt was designed to generate baseline answers without specialized knowledge:\n\"You have to answer an accounting ques-tion asked in an email. I'm going to giveyou the question <QUESTION>. Youmust answer the question in detail andprecisely. Answer with only the contentof the answer and nothing else.\""}, {"title": "A.2 Editing Scenarios", "content": "For the editing phase, we developed three distinct prompts corresponding to our experimental scenarios:\nNormal Edit Scenario This prompt allows for unrestricted editing while incorporating expert knowledge:\n\"You have to answer an accounting ques-tion asked in an email. I am going togive you the question <QUESTION>, ananswer generated by a non-specializedmodel <LLM_ANSWER>, and specificand specialized knowledge that allowsyou to answer this question <KNOWL-EDGE>. You must edit the model answerbased on the knowledge passed on in or-der to improve the answer where neces-sary. You can modify the answer as muchas you like in order to improve the initialanswer with this knowledge. You shouldnot copy and paste the knowledge intothe answer, but use the relevant elementsof this knowledge to update the initialanswer. Answer with only the content ofthe answer and nothing else.\"\nSimilar Edit Scenario This prompt emphasizes preserving the original structure while improving content:\n\"You have to answer an accounting ques-tion asked in an email. I am going togive you the question <QUESTION>, ananswer generated by a non-specializedmodel <LLM_ANSWER>, and specificand specialized knowledge that allowsyou to answer this question <KNOWL-EDGE>. You must edit the model an-swer based on the knowledge passed onin order to improve the answer wherenecessary. You must modify the answerin a way that keeps the same structureand outline as the initial answer, bymodifying if necessary only the contentbased on the knowledge transmitted. Youshould not copy and paste the knowledgeinto the answer, but use the relevant el-ements of this knowledge to update theinitial answer. Answer with only the con-tent of the answer and nothing else.\"\nFast Edit Scenario This prompt prioritizes quick, efficient edits:\n\"You have to answer an accounting ques-tion asked in an email. I am going togive you the question <QUESTION>, ananswer generated by a non-specializedmodel <LLM_ANSWER>, and specificand specialized knowledge that allowsyou to answer this question <KNOWL-EDGE>. You must edit the model's an-swer based on the knowledge passed onin order to improve the answer wherenecessary. You must take as little time aspossible to edit the initial answer whilecompleting the task correctly. You shouldnot copy and paste the knowledge intothe answer, but use the relevant elementsof this knowledge to update the initialanswer. Answer with only the content ofthe answer and nothing else.\"\nEach prompt was designed to maintain consistency in the basic task structure while introducing specific constraints or objectives that characterize the different editing scenarios analyzed in our study."}]}