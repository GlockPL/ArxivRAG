{"title": "Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis", "authors": ["Tingxuan Chen", "Kun Yuan", "Vinkle Srivastava", "Nassir Navab", "Nicolas Padoy"], "abstract": "Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.\nMethods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.\nResults: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.\nConclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in https://github.com/TingxuanSix/Surg-FTDA.", "sections": [{"title": "1. Introduction", "content": "Surgical workflow analysis is crucial for computer-assisted interventions [6, 10]. It requires precise surgical scene understanding and human intent anticipation to provide peripheral assistance feedback to surgeons. Early approaches, due to the lack of sufficient data, rely heavily on feature engineering, incorporating semantic features [17], surgical scene interaction information [21, 7], and other domain-specific knowledge to improve scene understanding. With the rise of deep neural networks and the availability of annotated datasets, recent methods have shifted toward designing architectures that better capture temporal dependencies [2, 3] and other complex features. While these architectures have led to notable improvements, they often exhibit limited generalizability and require large amounts of paired image-label data for training, limiting their scalability.\nSurgical foundation models are an emerging field leveraging multimodal representation learning to enable systems to interpret visual concepts using natural language [25, 23, 24]. These models have started to demonstrate good generalizability, allowing adaptation to various surgical procedures and tasks, such as surgical workflow recognition, triplet recognition [9], and visual question answering [16, 22].\nThe zero-shot adaptation of these methods, however, still shows a considerable performance gap w.r.t fully-supervised"}, {"title": "2. Methodology", "content": "In this section, we outline the process of few-shot data anchor selection, text-driven adaptation, and application of our model to both generative and classification tasks."}, {"title": "2.1. Few-shot Data Anchor Selection", "content": "Few-shot data anchor selection minimizes the need for large-scale annotated image-label pairs during the adaptation of the surgical foundation model. These selected data anchors play a key role in the subsequent text-driven adaptation process, as they help address the modality gap. It is crucial to choose data anchors that are evenly distributed and representative of the unlabeled dataset to ensure effective modality alignment and to enhance the performance of the text-driven adaptation method.\nAs shown in Fig. 2 (a), for a downstream dataset with large-scale unlabeled images, we first project these images into the embedding space using the surgical foundation model's image encoder, obtaining the image embedding vectors \\(V_i \\in \\mathbb{R}^d\\), where i represents each image and d is the dimensionality of the embedding space. To identify the data anchors, we apply either KMeans clustering [5] or Farthest Point Sampling (FPS) [12].\nFor KMeans, the image embeddings are clustered into K clusters, and the embedding vectors closest to the centroids are selected. Alternatively, FPS selects K image embeddings by maximizing the minimum distance between sampled points, ensuring a diverse selection. These methods preserve the structure of the embedding space by selecting a set of diverse data anchors, referred to as \\(V_{\\text{image}}^{\\text{sampled}}\\) . Once the anchor image embeddings are selected, we retrieve their corresponding textual label embeddings, denoted as \\(V_{\\text{text}}^{\\text{sampled}}\\).\nBased on the selected data anchors, we train a Multilayer Perceptron (MLP) to learn a transformation that aligns the image and text modalities. The MLP, denoted as \\(f_{\\text{MLP}}\\) , takes an image embedding as input and outputs an aligned image embedding. Let \\(\\theta\\) represent the parameters of MLP. The MLP is"}, {"title": "2.2. Text-driven Adaptation", "content": "After aligning the vision and language modalities, we propose a text-driven approach to adapt the foundation model for various surgical workflow analysis tasks, treating both discriminative and generative tasks as a text generation problem. We train a text decoder to generate class labels or captions, as depicted in Fig. 2 (b). Specifically, we extract possible target texts from downstream datasets, such as class labels or captions, and fine-tune the decoder to reconstruct these texts based on the extracted text embeddings.\nDuring the training process, we first extract text embeddings \\(V_{\\text{text}}\\), using the frozen text encoder from the foundation model. The decoder is then fine-tuned to reconstruct the original texts from these embeddings: \\(\\hat{T} = f_{\\text{decoder}}(V_{\\text{text}}; \\theta)\\), where \\(f_{\\text{decoder}}\\) represents the decoder with parameters \\(\\theta\\). The learning objective during the fine-tuning is to minimize the autoregressive cross-entropy loss \\(l\\) over all tokens in the target text T:"}, {"title": "2.3. Multi-task Text Decoder", "content": "In our model, the text decoder behaves differently across different types of tasks. In discriminative tasks, such as triplet recognition and phase recognition, the decoder is trained as"}, {"title": "3. Experiments", "content": "In this section, we first visualize the effect of our modality alignment function on reducing the modality gap. Second, we evaluate the model's performance on individual downstream tasks. Third, we conduct ablation studies to assess the contributions of various model components. Last, we test the performance of a multi-task decoder trained with mixed input for different tasks. In these experiments, both the CLIP [13] and SurgVLP [25] foundation models are used to assess the generalizability of our proposed pipeline. We use GPT-2 [14] as the text decoder.\nThe architecture of the MLP for modality alignment consists of an input layer with dimensions matching the input features, two hidden layers with 128 neurons each and ReLU activation, and an output layer with dimensions matching the target features. The model is optimized using the Adam optimizer with a learning rate of 0.001, trained for 15 epochs with a batch size of 16, and the loss function used is Mean Squared Error (MSELoss).\nFor text-driven adaptation, the GPT-2 decoder [14] is trained over 10 epochs using the AdamW optimizer with a learning rate of 2e-5. The batch size is set to 34 by default but can be adjusted based on GPU memory and dataset size."}, {"title": "3.1. Modality Gap", "content": "To better demonstrate the effectiveness of our few-shot anchor selection-based modality alignment, we visualize the original distributions of the two modalities and the distributions after modality alignment using various sampling methods on the SVL-Caption validation set, which contains 1351 image-text pairs. These visualizations provide an intuitive understanding of how our alignment approach reduces the modality gap and improves the coherence between the embeddings.\nFig. 3 demonstrates a significant reduction in the modality gap after aligning vision and text embeddings. Notably, embeddings from different modalities are more closely aligned when using 500 data anchors compared with using 100 anchors. Additionally, SurgVLP [25] provides better initialization for alignment than CLIP [13], with a smaller initial modality gap. These results confirm that our proposed modality alignment function based on few-shot data anchor selection effectively bridges the modality gap."}, {"title": "3.2. Downstream Tasks", "content": "In this work, we investigate two discriminative tasks, phase and action triplet recognition, and generative task, image captioning. We compare our text-driven adaptation method with the fully-supervised models that are finetuned on large-scale image-label pairs, and with the weakly supervised CapDec [8] approach, which optimizes noise parameters using image-label pairs."}, {"title": "3.2.1. Discriminative Tasks", "content": "Triplet Recognition: We use triplet text samples from the CholecT50 [9] dataset to train the decoder through our text-driven adaptation and test the model on images from the test split. The model's performance is evaluated using precision, recall, and F1 score with macro averaging. As shown in Tab. 1, our model consistently outperforms CapDec across various metrics.\nPhase Recognition: We report accuracy, precision, recall, and F1 score on the test split of Cholec80 [17] dataset, with macro averaging applied. As shown in Tab. 2, our model's performance on these metrics closely approaches that of fully-supervised approaches and significantly outperforms CapDec. This demonstrates that our text-driven adaptation trains the text decoder to become a semantic classifier. Additionally, it highlights the capability of the pre-trained foundation model to comprehend both coarse and fine-grained semantics, such as phases and triplets."}, {"title": "3.2.2. Generative Tasks", "content": "Image Caption: We train the model using caption text in the SVL-Caption dataset. We evaluate the models using standard captioning metrics: BLEU[11] (B@1, B@4), METEOR[1], and CIDEr[18]. As shown in Tab. 3, our model outperforms CapDec[8] across various metrics on both foundation models, demonstrating its effectiveness for generative tasks."}, {"title": "3.3. Ablation Study", "content": "In this section, we evaluate the impact of our few-shot selection techniques and foundation model choice on both discriminative and generative tasks. We compare KMeans and FPS sampling strategies using 100 and 500 sampled anchors, and no-anchor (without modality alignment) on SurgVLP and CLIP models. We further compare Surg-FTDA with fully-supervised models trained using 10%, 30%, 50%, and 100% of the image-text pairs from the dataset.\nAs shown in Tab. 4, Tab. 5, and Tab. 6, demonstrate that KMeans generally outperforms FPS across most tasks. This is likely due to KMean's ability to identify more representative data anchors within the dataset, resulting in better alignment between the visual and textual modalities. Also, increasing the number of sampling points improves model's performance by learning a robust modality alignment function. SurgVLP consistently outperforms CLIP, demonstrating the benefit of surgical domain-specific pre-training.\nAs shown in Tab. 7, for the phase recognition task, our Surg-FTDA significantly outperforms fully supervised models trained with 10% (833 image-text pairs), 30% (2499 pairs), and 50% (4165 pairs) of the dataset. It performs only slightly below the fully supervised model trained on the entire dataset (100%, 8331 pairs). Remarkably, Surg-FTDA achieves this using only 500 image-text pairs.\nThe fully supervised model trained on 10% of the data fails to generate phase outputs in the correct format under the same experimental settings (e.g., epochs, learning rate, and hyperparameters), resulting in all metrics being zero. This highlights the limitations of fully supervised models in low-data"}, {"title": "3.4. Multi-task Text Decoder for Better Decision Boundary", "content": "In this section, we evaluate the effectiveness of training a single decoder with mixed input for multiple tasks, i.e., phase recognition and triplet recognition. As shown in Tab. 9 and 10, the multi-task decoder consistently outperforms task-specific decoders trained on single-task texts, demonstrating that the multi-task learning of text-driven adaptation enhances performance on individual tasks. This suggests that the text decoder learns more robust decision boundaries, enabling it to better distinguish complex semantic features."}, {"title": "4. Conclusion", "content": "In this work, we introduced Surg-FTDA (Few-shot Text-driven Adaptation), a novel approach designed to address the challenges of surgical workflow analysis with minimal reliance on large-scale annotated datasets. By leveraging few-shot data selection and text-driven adaptation, Surg-FTDA bridges the modality gap between vision and text, allowing the model to handle various downstream tasks such as phase recognition, triplet recognition, and image captioning. Our method demonstrates that by selecting a small, diverse subset of image-label pairs and aligning visual and textual embeddings, a text-trained decoder can generalize effectively to visual tasks without requiring large amounts of paired image-label data. The results show that Surg-FTDA outperforms existing baselines, contributing to the field of surgical workflow analysis by enabling more scalable, data-efficient, and versatile models. This few-shot, text-driven adaptation opens up new possibilities for applying foundation models to a broader range of tasks, where limited annotations and multi-modality learning are key challenges."}]}