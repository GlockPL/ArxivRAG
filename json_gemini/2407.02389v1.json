{"title": "SAFARI:Adaptive Sequence Transformer for\nWeakly Supervised Referring Expression\nSegmentation", "authors": ["Sayan Nag", "Koustava Goswami", "Srikrishna Karanam"], "abstract": "Referring Expression Segmentation (RES) aims to provide\na segmentation mask of the target object in an image referred to by\nthe text (i.e., referring expression). Existing methods require large-scale\nmask annotations. Moreover, such approaches do not generalize well to\nunseen/zero-shot scenarios. To address the aforementioned issues, we\npropose a weakly-supervised bootstrapping architecture for RES with\nseveral new algorithmic innovations. To the best of our knowledge, ours\nis the first approach that considers only a fraction of both mask and\nbox annotations (shown in Figure 1 and Table 1) for training. To enable\nprincipled training of models in such low-annotation settings, improve\nimage-text region-level alignment, and further enhance spatial localiza-\ntion of the target object in the image, we propose Cross-modal Fusion\nwith Attention Consistency module. For automatic pseudo-labeling of\nunlabeled samples, we introduce a novel Mask Validity Filtering routine\nbased on a spatially aware zero-shot proposal scoring approach. Exten-\nsive experiments show that with just 30% annotations, our model SA-\nFARI achieves 59.31 and 48.26 mIoUs as compared to 58.93 and 48.19\nmIoUs obtained by the fully-supervised SOTA method SeqTR respec-\ntively on RefCOCO+@testA and RefCOCO+testB datasets. SAFARI\nalso outperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on\nRefCOCO+testB) in a fully-supervised setting and demonstrates strong\ngeneralization capabilities in unseen/zero-shot tasks.", "sections": [{"title": "1 Introduction", "content": "We consider the problem of referring expression segmentation (RES) [5,14,23,34,\n38,39,48,51,60,68,73,74,79] where given an image and a text sentence, the goal is\nto segment out parts of the image the text is referring to. Naturally, much existing\nwork in RES blends vision-language understanding [9, 16,35,44,46,54,70,77] and\ninstance segmentation [2,6,13,20,40], allowing for object segmentation using free-\nform textual expressions rather than predefined categories [14,18,22,24,25,30,66,\n69]. Existing RES methods are predominantly fully-supervised in nature [5,14,14,\n18,22,24,38,39,60,68,73,79]. Hence, they necessarily require expensive-to-obtain\nand extensive manually-annotated segmentation Ground-Truth (GT) masks for\ntraining and typically do not generalize well to unseen/zero-shot scenarios.\nAs noted above, obtaining large-scale GT masks is very expensive and tedious\nwhich makes it challenging to scale up such methods. To address this, a recent\nwork (Partial-RES) [53] propose a partially (weakly) supervised solution for RES\ntask. However, they consider abundant (100%) bounding box annotations to\nbegin with (Figure 1 and Table 1) and pretrain the model with these boxes on the\nReferring Expression Comprehension (REC) task (text referred box prediction)\nin a fully-supervised manner. Moreover, they use these 100% box annotations for\nfiltering pseudo-labels. Their limitations are: (i) in a more pragmatic and true\nweakly-supervised setting the percentage of boxes should equal the percentage of\nmasks, (ii) in their pretraining stage the model is already aware of the grounding\ninformation of the same dataset used in the weak-supervision stage, (iii) their\napproach does not account for the cross-modal region-level interaction between\nthe image and language features which is crucial for localization tasks [32, 76].\nOn the contrary, we investigate a more realistic, challenging and unexplored\nproblem of Weakly-Supervised Referring Expression Segmentation (WSRES)\nwith limited human-annotated mask and box annotations, specifically where\nbox % equals mask % (Figure 1, see Table 1 for comparisons). To tackle our\nproposed novel WSRES task, we present SAFARI, an auto-regressive contour-\nprediction-based RES method capable of demonstrating excellent performance\nunder challenging scenarios with few available mask and box annotations. We"}, {"title": "3 SAFARI", "content": "We present SAFARI, an adaptive multi-modal sequence transformer with our\nnovel components: (i) Cross-modal (X-) Fusion with Attention Consistency (X-\nFACT) module, (ii) bootstrapped Weak-Supervision with y-Scheduling (WSGS),\nand (iii) Mask Validity Filtering (MVF) with SPARC."}, {"title": "3.1 X-FACT.", "content": "X-FACT consists of Fused Feature Extractors (with cross-modal fusion) and At-\ntention Mask Consistency Regularization (AMCR) as discussed below:\n3.1.1 Fused Feature Extractors. We use Swin transformer [42] and RoBERTa"}, {"title": "", "content": "\\[\nXk = xk + (1 \u2212 \u03b2)\u00b7 S-MHA(Xk-1) + \u03b2\u00b7C-MHA(S-MHA(Xk\u22121), Yk)\n\\]\n\\[\nXk = Xk + FFN(xk)\n\\]\nwhere xk\u22121 is the output from the (k\u22121)th layer, S-MHA and C-MHA represent\nSelf- and Cross-Multi-Head Attention, FFN denotes cross-feed-forward network,\nand \u1e9e is a learnable weighted gating parameter initialized from 0. The fused\nfeature then goes into a sequence transformer for decoding the object contour\npoints as outputs. It is worth noting that unlike previous works [16,31] our gated\nmethodology preserves the uni-modal embeddings (i.e., self-attention features),\nensuring to learn the weighted representations during training. It also ensures"}, {"title": "3.1.2 Attention Mask Consistency Regularization.", "content": "Feature fusion through\ncross-attention strategy is highly effective in attending those parts of the image\nwhich the texts are referring to. However, in Figure 4, we observe that the\nattended regions are scattered across the objects including some background\npixels (more pronounced in WSRES task). The lack of fidelity in being able to\nattend fine-grained information may be attributed to the lack of training data\n(in WSRES). It has also been found in [57] that vision transformers tend to\ndemonstrate more uniform representations across all layers, enabled by early\naggregation of global information and propagation of features from lower to\nhigher layers. Moreover, in the absence of abundant masks (and boxes), the\nlocalization capability of the model needs to be further enhanced, especially, to\nyield superior pseudo-masks for the bootstrapping stage. Therefore, to address\nsuch issues, we formulate Attention Mask Consistency Regularization (AMCR)\nLoss to localize fine-grained cross-attention within the target object:"}, {"title": "", "content": "\\[\nLAMCR =\nN\nb=1\n1\ni,j\nLocalization\n\u03a3i, A\u00ef,j Mi,j\n\u03a3i, A\u00ef\nQN\n| + VKL (UN ||QN)\nCollapse-Reduction\n\\]\n\\[\nn(Ai,j)\n2,j Mi,j\nVbe N\nb\n\\]\nwhere Aij and Mi,j represent the Cross-Attention map from the last layer and\nMask, respectively, (i,j) represents pixel location, KL denotes KL divergence\nloss, UN(0, 1) denotes uniform distribution of minimum value 0 and maximum\nvalue 1, QN represents computed normalized frequency distribution over batch\nwith a size N. V is the loss-balancing term which we empirically set to 0.001.\nLAMCR consists of two terms: a Localization term and a Collapse-Reduction\nterm. The localization term guarantees accurate localization and alignment of\nthe generated cross-attention map within the mask of the object. The collapse-\nreduction term prevents collapsing of the cross-attention map within the mask."}, {"title": "", "content": "\\[\nLtotal = LCE + A\u30fbLAMCR\n\\]\nwhere A is responsible for weighting the LAMCR loss term. We provide more\ndetails regarding the formulation of LAMCR in the Supplementary.\nTakeaways: (1) X-FACT's cross-modal fusion scheme is an effective, flexible\nand parameter efficient strategy (when compared to VLP models [16, 31, 32]\nusing extra fusion-specific layers) and is particularly beneficial in the context\nof weakly-supervised RES task (Tables 2,3,4,7). (2) The AMCR component\nin X-FACT fosters prediction of high quality masks by spatially constrain-\ning the cross-attention map within the target object boundary. This improves\ncross-modal region-level alignment quality, especially in the limited annotations\nscenario. Such gains are not achieved upon using L2 loss (Table 6)."}, {"title": "3.2\nWeak-Supervision with y-Scheduling.", "content": "It is worth reiterating that unlike Partial-RES [53], we do not have box anno-\ntations for the full dataset. Therefore, we do not pretrain our model on any\nbox-prediction tasks such as Referring Expression Comprehension (REC) unlike\nPartial-RES. Our goal is to best adapt our architecture to the target RES task\ngiven limited mask annotations. Particularly, in a dataset consisting of N train-\ning image-referring expression pairs, we consider mask (and box) annotations\nfor x% data. These x% = {10%, 20%, 30%} samples are selected randomly from\nthe original training set. Our overall pipeline (Figure 2) is as follows:\n\u2022 Step 1: Initial RES Training. In this initial step, we train SAFARI using\nusing Equation 3 with x% labeled mask data on the RES task. Subsequently, we\nobtain a trained pseudo-labeler with updated model parameters.\n\u2022 Step 2: Pseudo-labeling. We infer masks by running inference on the re-\nmaining unlabeled (100-x)% training samples using model trained from Step\n1. Inferred masks are subsequently passed through the proposed Mask Validity\nFiltering (MVF) (Section 3.3) to verify the validity of these generated masks in\na zero-shot fashion. Contour points are next sampled from the valid masks and\nadded to the corresponding image-text pairs as pseudo-masks.\n\u2022 Step 3: Retraining with y-Scheduling. We retrain SAFARI (initialized\nfrom previous training) with the updated training dataset containing both the\nx% Ground Truth (GT) Masks (M) and Pseudo-Masks (M) and minimize the\nfinal loss LSAFARI using a Pseudo-Mask Loss weighting hyperparameter \u03b3:"}, {"title": "", "content": "\\[\nLSAFARI =\nGT\nLtotal\nLoss with GT Masks\n+y.\nPseudo\nLtotal\nLoss with Pseduo-Masks\n\\]\nThe role of y is to balance the amount of GT masks against that of pseudo-\nmasks during the retraining step. We introduce a y-scheduling strategy to sys-\ntematically change the value of y over a set of iterations with the starting (mini-\nmum) value of y being % = 0.9 (found empirically through ablation studies) and\nmaximum value being 1.0. The intuition is to provide more weighting to pseudo-\nmasks as the model becomes more confident of the predictions as the training\nsteps progress. We outline the detailed steps in Algorithm 1 in Supplementary."}, {"title": "3.3 Mask Validity Filtering with SPARC.", "content": "Mask Validity Filtering (MVF) is the process of identifying which predicted (in-\nferred) masks actually localize the object (present in the image) being referred\nto in the text (referring expression) [53]. Partial-RES [53] achieves this task by\nusing ground-truth bounding-box annotations. However, as noted in Section 1,\nour proposed setup considers a true weakly-supervised scenario where the num-\nber of bounding box and mask annotations are equal. Therefore, to address this\ngap in the literature, we propose a zero-shot approach to obtain these boxes to\nbe used in the MVF stage. This can also be formulated as a Zero-Shot Refer-\nring Expression Comprehension (ZS-REC) task. Our proposed MVF approach\nis composed of two parts - (i) obtaining bounding boxes using ZS-REC, and (ii)\nvalidating the inferred masks using the obtained bounding boxes.\n3.3.1 ZS-REC with SPARC module. We introduce Spatially Aware Red-\nBox Clip (SPARC), a ZS-REC module, comprising two components: a proposal\nscoring approach with visual prompts (red-box), and a spatial reasoning compo-\nnent accounting for spatial understanding of proposals in a rule-based manner.\nProposal scoring with red-box prompting: Our ZS framework considers an input\nimage and the referring expression along with NB box proposals as generated by\na pretrained object detector [58]. ZS-REC task can be conveniently converted\nto a ZS-retrieval task using an ensemble of image (visual) prompts with a single\ntext as described below. Each of these visual prompts (images) must correspond\nto a single isolated proposal from the original image generating NB such images\n(visual prompts). Each proposal object region is isolated by employing a Gaus-\nsian blur on the background and by adding a red box (red border) of uniform\nthickness surrounding the object proposal. These image prompts along with the\nreferring expression is passed through CLIP to obtain the CLIP score. Notably,\nusing a red border acts as a positive visual prompt highlighting the target area,\nwhereas blurring the background reduces the impact of weakly related informa-\ntion. However, these CLIP scores do not consider the spatial understanding of\nthe objects in the different bounding boxes. Therefore, injecting spatial under-\nstanding is important which we discuss next.\nSpatial Reasoning component: A major limitation of CLIP for the ZS-REC task\nis the lack of fine-grained spatial understanding. In Figure 3, the text \"black dog\"\nrefers to the two black dogs in the picture. CLIP will generate high scores for\nboth these proposal regions since both fall under the class \"black dog\". Further-\nmore, although addition of visual prompt improves detection performance [61],\nit neither incorporates nor enhances spatial understanding capabilities of the\nmodel. To mitigate this, we introduce a simple rule-based approach. We divide\nthe referring expression into two constituents noun chunks and their relations.\nIn the previous example, it refers to the \"black dog\" in the picture which is\nthe subject. The phrases connecting these noun chunks form the relationships"}, {"title": "", "content": "(e.g., right/east, left/west, smaller/tinier/further, bigger/larger/closer, between,\nwithin/inside, above/north/top, below/under/south, back/behind, and front). In\nthe presence of two boxes referring to the black dog, the probability of the pro-\nposal on the left will be the most (as decided based on the location of the\ncentroid of the box computed using the obtained box coordinates). To provide\nmore clarity on this, we consider the example: \"black dog on the left of a white\ndog\", relation R between two nouns black dog (X) and white dog (Y) is \"left\".\nCoordinates of proposals X and Y, and relation R are the inputs of Spatial\nReasoning component. If center point of box X is to the \"left\" of that of box\nY, the output probability Pr[R(X, Y)] = 1, otherwise 0. These probabilities are\nmultiplied with the CLIP scores of the respective proposals. Note that in rare\noccasions where the referring texts in the RefCOCO datasets do not have any\nspatial relations, we resort to using only CLIP scores for scoring the proposals.\nMore examples, with those of complex relations are given in Supplementary.\n3.3.2 Validation of Inferred Masks with SPARC. In the Pseudo-labeling\nstep, SAFARI predicts contour points which are connected to generate binary\nmasks. We generate a bounding box from the outermost (top-most, bottom-\nmost, right-most, left-most) points of each mask and compute Dice Similarity\nCoefficient (DSC) [49] between the generated box and the box obtained using the\nZS-REC step using SPARC. We reject the noisy pseudo-masks with DSC value\nless than T = 0.1 (ablation in Supplementary). Contour points are resampled\nfrom the filtered pseudo-masks and added to the training set (Figure 2).\nTakeaways: (1) Visual prompting with red border together with the spatial\nawareness component in SPARC improves mIoU (as demonstrated in Table\n5). (2) Mask Validity Filtering with SPARC enhances the quality of train-\ning samples in the retraining stage, which has a positive effect on the model\nperformance (Tables 2-3) for the weakly-supervised RES task."}, {"title": "4 Experiments and Results", "content": "4.1 Datasets, Implementation Details and Metrics\n4.1.1 Dataset. For RES, we conduct our experiments on the three major RES\ndatasets: RefCOCO [75], RefCOCO+ [75], and RefCOCOg [47,50]. For Zero-\nshot Transfer to Referring Video Object Segmentation (ZS-R-VOS), we conduct\nzero-shot (ZS) evaluations on two popular R-VOS datasets: RefDAVIS17 [29]\nand JHMDB-Sentences [19].\n4.1.2 Implementation. We use AdamW optimizer [43] with a batch size of 128.\nWe use an initial learning rate (LR) of 5e-4 with (\u03b2\u2081, \u03b22) = (0.9, 0.98), \u0454 = 10\u00af\u00ba,\nand Multi-Step LR Warmup of 5 epochs, decay steps of 75, and a decay ratio of\n0.1. We use A100 40GB GPUs for all experiments.\n4.1.3 Metrics. For RES, we report mean Intersection-over-Union (mIoU) val-\nues. For ZS-R-VOS, we report mean of region similarity (I) and contour ac-\ncuracy (F) denoted as J&F on RefDAVIS17, and mIoU scores on JHMDB-\nSentences. Additional dataset and implementation details are provided in Sup-\nplementary."}, {"title": "4.2 Main Results", "content": "4.2.1 Referring Expression Segmentation (RES). We compare SAFARI\nwith the state-of-the-art methods on the RefCOCO/+/g validation and test\nsets and report the mean IoU (mIoU) in the Table 2 under different label-rates.\nFull Supervision: SAFARI outperforms existing benchmark methods on each\nsplit of the three datasets by a significant margin (Table 2). For a fair compari-\nson with PolyFormer [39], we additionally train SAFARI on a combined dataset\nafter removing all data from validation and test sets [39]. Unlike PolyFormer,\nSAFARI is not pretrained on the REC task. However, SAFARI still outperforms\nPolyFormer on all three datasets. Note that we report fully-supervised results\nto show an upper-bound on the performance of the weakly-supervised models.\nWeak Supervision: We consider label rates of 10%, 20%, and 30% for the\nWSRES task (Table 2). Despite methods such as Partial-RES using 100% box\nannotations, SAFARI shows substantial improvements over Partial-RES (note\nthat Partial-RES originally used DN-53 backbone and we re-implemented with\nSwin-B backbone for fair comparisons) for all three label-rates. With just 30%\nlabeled data and without any REC pretraining, SAFARI often surpasses the\nfully-supervised performance of SeqTR on RefCOCO/+g validation sets (e.g.,\n59.31 vs 58.93 on RefCOCO+@testA, 55.83 vs 55.64 on RefCOCOg@test sets).\nWeakly-supervised SAFARI significantly outperforms other fully-supervised base-\nlines (e.g., VLT [14] and LTS [27]). These results bespeak the role of our proposed"}, {"title": "4.2.2 Zero-shot Referring Video Object Segmentation (ZS-R-VOS).", "content": "We conduct evaluation on the R-VOS datasets (RefDAVIS17 and JHMDB) as a\nZS transfer task. We consider the video frames as a sequence of images without\ninvolving any temporal information while predicting masks. As shown in Ta-\nble 3, SAFARI achieves state-of-the-art results with just the spatial information,\ndisplaying strong generalization capabilities.\nWe also conduct evaluations on the ZS-REC task with our proposed SPARC\nmodule which significantly outperforms SOTA RedCircle [61] achieving 39.3%\nand 16.2% gains on RefCOCO@testB and RefCOCO+@testB sets respectively.\nResults and visualizations are given in Supplementary."}, {"title": "4.3 Ablation Study", "content": "4.3.1 Impact of X-FACT. We show the impact of X-FACT containing Cross-\nAttention (CA) based fusion and the AMCR components in Table 4. Inclusion\nof CA is shown to improve mIoU values consistently across varying label-rates.\nThe impact of AMCR is more pronounced in the cases of limited annotations.\nFor instance, with a label-rate of 100%, addition of AMCR boosts mIoU by\n0.94 points on RefCOCO@val, whereas with a label-rate of 10%, this difference\nincreases substantially to 3.91. Additionally, in Figure 4 we demonstrate that\nincorporating AMCR qualitatively improves both the cross-attention maps and\nthe predicted masks, highlighting the efficacy of AMCR in our pipeline. We fur-\nther assess the importance of AMCR loss balancing factor (\u03bb) in the Figure 5b\nwhen evaluated on RefCOCO@val. Increasing A improves mIoU initially and the\nmaximum is achieved at 0.4, beyond which the performance drops significantly."}, {"title": "4.3.2 Retraining with MVF using SPARC.", "content": "In the presence of limited mask\n(and box) annotations, retraining with y scheduling leads to a marked improve-\nment in the mIoU values as depicted in Figure 5a. Here, we consider label-rates\nof 30% (solid lines) and 10% (dashed lines) with two different Yo values denoting\nthe starting values of the y variable. We note that as the number of retraining\nsteps (runs) increases, the mIoU values (for both o values) in each label-rate\nincreases. However, with o = 0.9 we achieve slightly better performance. Addi-\ntionally, we evaluate the effectiveness of the MVF stage with SPARC in Sup-\nplementary. Results indicate that without MVF, generated pseudo-labels cannot\nbe validated which negatively impacts model's performance. In contrast, when\npseudo-labels are validated using MVF, the mIoU values on RefCOCO@val set\nare shown to improve by 4.36 mIoU, demonstrating the importance of MVF.\n4.3.3 Impact of various components of SPARC. In Table 5 we assess the\nimpact of individual components of SPARC. We note that red-box prompting\nand blurring together boost performance. Adding spatial awareness via the rule-\nbased reasoning module also substantially improves mIoU on the WSRES task.\n4.3.4 AMCR vs L2 loss in X-FACT. To evaluate the impact of the AMCR\ncomponent in X-FACT, we analyze SAFARI's performance after replacing the\nproposed formulation of AMCR with a L2 loss. As shown in Table 6, we observe\nthat using L2 (in place of AMCR) leads to substantially poor performance, as\ncompared to when AMCR is used in X-FACT. This corroborates the strong lo-\ncalization capabilities introduced in the system by the AMCR formulation which\nis not achieved with a L2 loss.\n4.3.5 Impact of Label Rates. We notice that as the label-rate increases from\n10% to 100%, the performance of the model improves significantly across all the"}, {"title": "4.4 Qualitative Assessment and Error Analysis", "content": "4.4.1 Backbone Cross-Attentions and Predicted Masks. In Figure 7, we\nshow how cross-modal attention maps attend to different objects in the images,\nguided by the referring texts. The cross-attention scores between the image and\nthe associated expression are extracted and bilinearly interpolated to match the\nimage dimension and superimposed on the original image. Notably, strong cross-\nmodal interactions aid in predicting high-quality masks as displayed in Figure 7.\nIn Figure 10, we also provide high-quality segmentation masks generated by SA-\nFARI for the ZS-R-VOS tasks without any finetuning on the respective datasets.\n4.4.2 Varying Label-Rates. In Figure 8, we display segmentation masks un-\nder varying label-rates using SAFARI. We clearly notice that the quality of masks\nimprove substantially with an increase in the ground-truth annotations.\n4.4.3 Varying Retraining Steps. In Figure 9 we show that with increasing\nretraining steps (runs), SAFARI becomes more confident which results in sig-\nnificant qualitative improvements in the predictions. For example, in Figure 9\nalthough SAFARI could not recognize the magazine partially behind laptop in the\nfirst step, it was successful in predicting the mask accurately in the final step\nthis shows the efficacy of the retraining stage.\n4.4.4 Comparisons with Partial-RES. We provide qualitative examples for\n30% label-rate in Figure 1. It is evident from Figure 1 that SAFARI succeeds\nin challenging cases requiring extensive linguistic understanding (attained with\nX-FACT and SPARC modules) where Partial-RES [53] fails.\nWe provide extended versions of these qualitative visualizations in Supple-\nmentary. Additionally, we show examples of cases in Supplementary where SA-"}, {"title": "5 Conclusion", "content": "We present a weakly-supervised learning framework for RES considering limited\nmask (and box) annotations and employing a contour-based sequence predic-\ntion approach. Unlike Partial-RES [53], we do not consider box annotations\nto be fully available and therefore we do not pretrain our model on the fully-\nsupervised REC task. We incorporate lightweight gated cross-modal attention\nin the feature backbones along with an Attention Mask Consistency Regulariza-\ntion module to facilitate strong cross-modal alignment and improve the quality\nof predicted masks. We introduce a bootstrapping pipeline with self-labeling ca-\npabilities where pseudo-labels are validated using our proposed Mask Validity\nFiltering approach. We conduct extensive experiments to demonstrate that SA-\nFARI consistently achieves state-of-the-art performance across all RES datasets.\nFinally, we demonstrate excellent generalization capabilities of SAFARI on zero-\nshot referring video-object segmentation task. Extending our approach to multi-\nimage and video settings can be looked upon as a promising future work."}]}