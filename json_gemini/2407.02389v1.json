{"title": "SAFARI:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation", "authors": ["Sayan Nag", "Koustava Goswami", "Srikrishna Karanam"], "abstract": "Referring Expression Segmentation (RES) aims to provide a segmentation mask of the target object in an image referred to by the text (i.e., referring expression). Existing methods require large-scale mask annotations. Moreover, such approaches do not generalize well to unseen/zero-shot scenarios. To address the aforementioned issues, we propose a weakly-supervised bootstrapping architecture for RES with several new algorithmic innovations. To the best of our knowledge, ours is the first approach that considers only a fraction of both mask and box annotations for training. To enable principled training of models in such low-annotation settings, improve image-text region-level alignment, and further enhance spatial localization of the target object in the image, we propose Cross-modal Fusion with Attention Consistency module. For automatic pseudo-labeling of unlabeled samples, we introduce a novel Mask Validity Filtering routine based on a spatially aware zero-shot proposal scoring approach. Extensive experiments show that with just 30% annotations, our model SAFARI achieves 59.31 and 48.26 mIoUs as compared to 58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR respectively on RefCOCO+@testA and RefCOCO+testB datasets. SAFARI also outperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a fully-supervised setting and demonstrates strong generalization capabilities in unseen/zero-shot tasks.", "sections": [{"title": "1 Introduction", "content": "We consider the problem of referring expression segmentation (RES) where given an image and a text sentence, the goal is to segment out parts of the image the text is referring to. Naturally, much existing work in RES blends vision-language understanding and instance segmentation, allowing for object segmentation using free-form textual expressions rather than predefined categories. Existing RES methods are predominantly fully-supervised in nature. Hence, they necessarily require expensive-to-obtain and extensive manually-annotated segmentation Ground-Truth (GT) masks for training and typically do not generalize well to unseen/zero-shot scenarios.\nAs noted above, obtaining large-scale GT masks is very expensive and tedious which makes it challenging to scale up such methods. To address this, a recent work (Partial-RES) propose a partially (weakly) supervised solution for RES task. However, they consider abundant (100%) bounding box annotations to begin with and pretrain the model with these boxes on the Referring Expression Comprehension (REC) task (text referred box prediction) in a fully-supervised manner. Moreover, they use these 100% box annotations for filtering pseudo-labels. Their limitations are: (i) in a more pragmatic and true weakly-supervised setting the percentage of boxes should equal the percentage of masks, (ii) in their pretraining stage the model is already aware of the grounding information of the same dataset used in the weak-supervision stage, (iii) their approach does not account for the cross-modal region-level interaction between the image and language features which is crucial for localization tasks.\nOn the contrary, we investigate a more realistic, challenging and unexplored problem of Weakly-Supervised Referring Expression Segmentation (WSRES) with limited human-annotated mask and box annotations, specifically where box % equals mask % (Figure 1, see Table 1 for comparisons). To tackle our proposed novel WSRES task, we present SAFARI, an auto-regressive contour-prediction-based RES method capable of demonstrating excellent performance under challenging scenarios with few available mask and box annotations. We specifically choose segmentation as an auto-regressive point prediction task along the object's contour instead of an independent pixel prediction approach. This is because the latter does not account for relationships between neighbouring pixels and therefore lacks structural information of the object being segmented, resulting in poor performances in the absence of abundant mask annotations.\nIn SAFARI, firstly, we introduce a novel Cross-modal (X-) Fusion with Attention Consistency (X-FACT) module for facilitating excellent inter-domain alignment. Since we have only few annotated data, we make the cross-fusion component in X-FACT to be flexible and parameter efficient. Further, in X-FACT we actively leverage the cross-attention heatmaps by formulating a regularization technique that encourages these heatmaps to be consistent with the referred object in the image. This is enabled by confining the attended regions within the object contour, enhancing the fidelity of predicted masks. This is particularly beneficial for the limited annotation scenarios (in WSRES) where it explicitly helps features improving visual grounding, without relying on lots of data. Secondly, we systematically devise a novel bootstrapping strategy which uses a handful of labeled masks (e.g., 10%) and iteratively trains the model by utilizing pseudo-masks obtained from a pseudo-labeling procedure. Finally, in contrast with conventional pseudo-labeling pipelines which directly use the model predictions as pseudo-labels, we design a new Mask Validity Filtering (MVF) routine that is responsible for selection of pseudo-masks (for unannotated data) by validating whether they spatially align with the boundaries (boxes) of the referred objects or not. Since we do not possess 100% bounding boxes, we propose SPARC, a novel REC technique with spatial reasoning capabilities for obtaining these boxes in an entirely zero-shot manner. Equipped with the above modules, unlike Partial-RES, our system learns meaningful, transferable and generalizable representations with rich semantic understanding, empowering it to reason and make accurate predictions on unseen data.\nWe summarize our main contributions as: (i) To the best of our knowledge, ours is the first to consider an accurate representation of Weakly-Supervised Re-"}, {"title": "2 Related Work", "content": "Fully-supervised RES. These methods chiefly focus on enhancing the quality of multi-modal fusion of extracted vision-language features. Recent efforts have been made to develop powerful multi-modal transformer-based methods showing significant improvements over previous baselines.\nWeakly Supervised RES. Weakly supervised instance segmentation methods have also been investigated in the recent past. However, in the context of RES, there is one recent study (Partial-RES) which adopts partial or weak-supervision. They specifically showed that employing a unified multi-modal transformer model, e.g., SeqTR is more beneficial (as compared to MDETR) in the presence of limited annotations. In their weakly supervised RES setting, Partial-RES considered abundant bounding box annotations albeit limited mask annotations. They utilized a fully-supervised REC pretrained model to train on limited mask-annotations. Furthermore, for filtering pseudo-masks, they again used the fully available (100%) bounding box annotations. However, since this involved 100% box annotations, our proposed setup is much more challenging and truly weakly supervised since we consider both box and mask annotations to be limited. In their pseudo-labeling step on the data without masks (but with boxes), the model has already seen the boxes in the REC pretraining stage and thus the grounding information is already present.\nUnsupervised Referring Expression Comprehension (REC). Referring Expression Comprehension (REC) is a grounding task which involves localizing an object present in an image with respect to a textual description of that object. With the introduction of Vision-Language Pretrained (VLP) models, it has been possible to develop Unsupervised or Zero-Shot REC (ZS-REC) methods. CPT colors region proposal boxes and utilizes a captioning model for predicting the colored proposals that are linked to the textual expressions. RedCircle employs visual prompting by drawing circular contours outside the detected object proposals and subse-"}, {"title": "3 SAFARI", "content": "We present SAFARI, an adaptive multi-modal sequence transformer with our novel components: (i) Cross-modal (X-) Fusion with Attention Consistency (X-FACT) module, (ii) bootstrapped Weak-Supervision with y-Scheduling (WSGS), and (iii) Mask Validity Filtering (MVF) with SPARC."}, {"title": "3.1 X-FACT.", "content": "X-FACT consists of Fused Feature Extractors (with cross-modal fusion) and Attention Mask Consistency Regularization (AMCR) as discussed below:"}, {"title": "3.1.1 Fused Feature Extractors.", "content": "We use Swin transformer and RoBERTa as our image and text feature extractors. A simple concatenation-based encoding strategy fails to capture cross-modal interactions, leading to poor fine-grained multi-modal representations. To address this, previous Vision-Language studies employed cross-modal fusion modules in their respective frameworks. However, such strategies introduce extra trainable fusion-specific layers. For instance, the early fusion scheme adopted in GLIP, used 6 vision-language fusion layers and 6 additional BERT layers for feature enhancement. However, such a scheme is not suitable in the presence of limited annotations. Therefore, as shown in Figure 3, we introduce a simple and lightweight fusion mechanism through normalized gated cross-attention into the layers of uni-modal feature extractors to learn high-quality language aware visual representations:\n$$x_k = x_k + (1 - \\beta) \\cdot \\text{S-MHA}(x_{k-1}) + \\beta \\cdot \\text{C-MHA}(\\text{S-MHA}(x_{k-1}), Y_k)$$\n$$x_k = x_k + \\text{FFN}(x_k)$$\nwhere $x_{k-1}$ is the output from the $(k-1)^{th}$ layer, S-MHA and C-MHA represent Self- and Cross-Multi-Head Attention, FFN denotes cross-feed-forward network, and $ \\beta$ is a learnable weighted gating parameter initialized from 0. The fused feature then goes into a sequence transformer for decoding the object contour points as outputs. It is worth noting that unlike previous works our gated methodology preserves the uni-modal embeddings (i.e., self-attention features), ensuring to learn the weighted representations during training. It also ensures"}, {"title": "3.1.2 Attention Mask Consistency Regularization.", "content": "Feature fusion through cross-attention strategy is highly effective in attending those parts of the image which the texts are referring to. However, in Figure 4, we observe that the attended regions are scattered across the objects including some background pixels (more pronounced in WSRES task). The lack of fidelity in being able to attend fine-grained information may be attributed to the lack of training data (in WSRES). It has also been found in [57] that vision transformers tend to demonstrate more uniform representations across all layers, enabled by early aggregation of global information and propagation of features from lower to higher layers. Moreover, in the absence of abundant masks (and boxes), the localization capability of the model needs to be further enhanced, especially, to yield superior pseudo-masks for the bootstrapping stage. Therefore, to address such issues, we formulate Attention Mask Consistency Regularization (AMCR) Loss to localize fine-grained cross-attention within the target object:\n$$\\mathcal{L}_{AMCR} = \\sum_{b=1}^{N} \\left[ \\frac{\\sum_{i,j} A_{i,j} M_{i,j}}{\\sum_{i,j} A_{i,j}} \\right]_{Localization} + \\mathcal{V} \\mathcal{K} \\mathcal{L} \\left( \\mathcal{U}_N \\|\\| \\mathcal{Q}_N \\right)_{Collapse-Reduction}$$\n$$\\mathcal{Q}_N = \\left[ \\frac{(\\mathcal{A}_{i,j} \\odot M_{i,j})}{\\sum_{i,j} (\\mathcal{A}_{i,j} \\odot M_{i,j})} \\right]_{ \\forall b \\in N}$$\nwhere $A_{i,j}$ and $M_{i,j}$ represent the Cross-Attention map from the last layer and Mask, respectively, (i,j) represents pixel location, KL denotes KL divergence loss, $\\mathcal{U}_N(0, 1)$ denotes uniform distribution of minimum value 0 and maximum value 1, $\\mathcal{Q}_N$ represents computed normalized frequency distribution over batch with a size N. V is the loss-balancing term which we empirically set to 0.001. $\\mathcal{L}_{AMCR}$ consists of two terms: a Localization term and a Collapse-Reduction term. The localization term guarantees accurate localization and alignment of the generated cross-attention map within the mask of the object. The collapse-reduction term prevents collapsing of the cross-attention map within the mask."}, {"title": "3.2 Weak-Supervision with y-Scheduling.", "content": "It is worth reiterating that unlike Partial-RES, we do not have box annotations for the full dataset. Therefore, we do not pretrain our model on any box-prediction tasks such as Referring Expression Comprehension (REC) unlike Partial-RES. Our goal is to best adapt our architecture to the target RES task given limited mask annotations. Particularly, in a dataset consisting of N training image-referring expression pairs, we consider mask (and box) annotations for x% data. These x% = {10%, 20%, 30%} samples are selected randomly from the original training set. Our overall pipeline is as follows:\n\u2022 Step 1: Initial RES Training. In this initial step, we train SAFARI using using Equation 3 with x% labeled mask data on the RES task. Subsequently, we obtain a trained pseudo-labeler with updated model parameters.\n\u2022 Step 2: Pseudo-labeling. We infer masks by running inference on the remaining unlabeled (100-x)% training samples using model trained from Step 1. Inferred masks are subsequently passed through the proposed Mask Validity Filtering (MVF) (Section 3.3) to verify the validity of these generated masks in a zero-shot fashion. Contour points are next sampled from the valid masks and added to the corresponding image-text pairs as pseudo-masks.\n\u2022 Step 3: Retraining with y-Scheduling. We retrain SAFARI (initialized from previous training) with the updated training dataset containing both the x% Ground Truth (GT) Masks (M) and Pseudo-Masks (M) and minimize the final loss LSAFARI using a Pseudo-Mask Loss weighting hyperparameter \u03b3:\n$$L_{SAFARI} =  \\frac{L_{total}^{GT}}{Loss with GT Masks} + \\gamma \\cdot  \\frac{L_{total}^{Pseudo}}{Loss with Pseduo-Masks}$$\nThe role of \u03b3 is to balance the amount of GT masks against that of pseudo-masks during the retraining step. We introduce a y-scheduling strategy to systematically change the value of y over a set of iterations with the starting (mini-mum) value of y being % = 0.9 (found empirically through ablation studies) and maximum value being 1.0. The intuition is to provide more weighting to pseudo-masks as the model becomes more confident of the predictions as the training steps progress. We outline the detailed steps in Algorithm 1 in Supplementary."}, {"title": "3.3 Mask Validity Filtering with SPARC.", "content": "Mask Validity Filtering (MVF) is the process of identifying which predicted (in-ferred) masks actually localize the object (present in the image) being referred to in the text (referring expression). Partial-RES achieves this task by using ground-truth bounding-box annotations. However, as noted in Section 1, our proposed setup considers a true weakly-supervised scenario where the number of bounding box and mask annotations are equal. Therefore, to address this gap in the literature, we propose a zero-shot approach to obtain these boxes to be used in the MVF stage. This can also be formulated as a Zero-Shot Referring Expression Comprehension (ZS-REC) task. Our proposed MVF approach is composed of two parts - (i) obtaining bounding boxes using ZS-REC, and (ii) validating the inferred masks using the obtained bounding boxes.\n3.3.1 ZS-REC with SPARC module. We introduce Spatially Aware Red-Box Clip (SPARC), a ZS-REC module, comprising two components: a proposal scoring approach with visual prompts (red-box), and a spatial reasoning compo-nent accounting for spatial understanding of proposals in a rule-based manner. Proposal scoring with red-box prompting: Our ZS framework considers an input image and the referring expression along with NB box proposals as generated by a pretrained object detector. ZS-REC task can be conveniently converted to a ZS-retrieval task using an ensemble of image (visual) prompts with a single text as described below. Each of these visual prompts (images) must correspond to a single isolated proposal from the original image generating NB such images (visual prompts). Each proposal object region is isolated by employing a Gaus-sian blur on the background and by adding a red box (red border) of uniform thickness surrounding the object proposal. These image prompts along with the referring expression is passed through CLIP to obtain the CLIP score. Notably, using a red border acts as a positive visual prompt highlighting the target area, whereas blurring the background reduces the impact of weakly related informa-tion. However, these CLIP scores do not consider the spatial understanding of the objects in the different bounding boxes. Therefore, injecting spatial under-standing is important which we discuss next.\nSpatial Reasoning component: A major limitation of CLIP for the ZS-REC task is the lack of fine-grained spatial understanding. In Figure 3, the text \"black dog\" refers to the two black dogs in the picture. CLIP will generate high scores for both these proposal regions since both fall under the class \"black dog\". Further-more, although addition of visual prompt improves detection performance, it neither incorporates nor enhances spatial understanding capabilities of the model. To mitigate this, we introduce a simple rule-based approach. We divide the referring expression into two constituents noun chunks and their relations. In the previous example, it refers to the \"black dog\" in the picture which is the subject. The phrases connecting these noun chunks form the relationships"}, {"title": "Experiments and Results", "content": "4.1 Datasets, Implementation Details and Metrics\n4.1.1 Dataset. For RES, we conduct our experiments on the three major RES datasets: RefCOCO, RefCOCO+, and RefCOCOg. For Zero-shot Transfer to Referring Video Object Segmentation (ZS-R-VOS), we conduct zero-shot (ZS) evaluations on two popular R-VOS datasets: RefDAVIS17 and JHMDB-Sentences.\n4.1.2 Implementation. We use AdamW optimizer with a batch size of 128. We use an initial learning rate (LR) of 5e-4 with (\u03b2\u2081, \u03b22) = (0.9, 0.98), \u0454 = 10\u00af\u00ba, and Multi-Step LR Warmup of 5 epochs, decay steps of 75, and a decay ratio of 0.1. We use A100 40GB GPUs for all experiments.\n4.1.3 Metrics. For RES, we report mean Intersection-over-Union (mIoU) val-ues. For ZS-R-VOS, we report mean of region similarity (I) and contour ac-curacy (F) denoted as J&F on RefDAVIS17, and mIoU scores on JHMDB-Sentences. Additional dataset and implementation details are provided in Sup-plementary."}, {"title": "4.2 Main Results", "content": "4.2.1 Referring Expression Segmentation (RES). We compare SAFARI with the state-of-the-art methods on the RefCOCO/+/g validation and test sets and report the mean IoU (mIoU) in the Table 2 under different label-rates.\nFull Supervision: SAFARI outperforms existing benchmark methods on each split of the three datasets by a significant margin (Table 2). For a fair compari-son with PolyFormer, we additionally train SAFARI on a combined dataset after removing all data from validation and test sets. Unlike PolyFormer, SAFARI is not pretrained on the REC task. However, SAFARI still outperforms PolyFormer on all three datasets. Note that we report fully-supervised results to show an upper-bound on the performance of the weakly-supervised models.\nWeak Supervision: We consider label rates of 10%, 20%, and 30% for the WSRES task (Table 2). Despite methods such as Partial-RES using 100% box annotations, SAFARI shows substantial improvements over Partial-RES (note that Partial-RES originally used DN-53 backbone and we re-implemented with Swin-B backbone for fair comparisons) for all three label-rates. With just 30% labeled data and without any REC pretraining, SAFARI often surpasses the fully-supervised performance of SeqTR on RefCOCO/+g validation sets (e.g., 59.31 vs 58.93 on RefCOCO+@testA, 55.83 vs 55.64 on RefCOCOg@test sets). Weakly-supervised SAFARI significantly outperforms other fully-supervised base-lines (e.g., VLT and LTS). These results bespeak the role of our proposed X-FACT (containing cross-modal fusion together with AMCR) and SPARC modules in obtaining state-of-the-art results in the limited annotation scenarios.\n4.2.2 Zero-shot Referring Video Object Segmentation (ZS-R-VOS). We conduct evaluation on the R-VOS datasets (RefDAVIS17 and JHMDB) as a ZS transfer task. We consider the video frames as a sequence of images without involving any temporal information while predicting masks. As shown in Table 3, SAFARI achieves state-of-the-art results with just the spatial information, displaying strong generalization capabilities.\nWe also conduct evaluations on the ZS-REC task with our proposed SPARC module which significantly outperforms SOTA RedCircle achieving 39.3% and 16.2% gains on RefCOCO@testB and RefCOCO+@testB sets respectively. Results and visualizations are given in Supplementary."}, {"title": "4.3 Ablation Study", "content": "4.3.1 Impact of X-FACT. We show the impact of X-FACT containing Cross-Attention (CA) based fusion and the AMCR components in Table 4. Inclusion of CA is shown to improve mIoU values consistently across varying label-rates. The impact of AMCR is more pronounced in the cases of limited annotations. For instance, with a label-rate of 100%, addition of AMCR boosts mIoU by 0.94 points on RefCOCO@val, whereas with a label-rate of 10%, this difference increases substantially to 3.91. Additionally, in Figure 4 we demonstrate that incorporating AMCR qualitatively improves both the cross-attention maps and the predicted masks, highlighting the efficacy of AMCR in our pipeline. We fur-ther assess the importance of AMCR loss balancing factor (\u03bb) in the Figure 5b when evaluated on RefCOCO@val. Increasing A improves mIoU initially and the maximum is achieved at 0.4, beyond which the performance drops significantly."}, {"title": "5 Conclusion", "content": "We present a weakly-supervised learning framework for RES considering limited mask (and box) annotations and employing a contour-based sequence predic-tion approach. Unlike Partial-RES, we do not consider box annotations to be fully available and therefore we do not pretrain our model on the fully-supervised REC task. We incorporate lightweight gated cross-modal attention in the feature backbones along with an Attention Mask Consistency Regulariza-tion module to facilitate strong cross-modal alignment and improve the quality of predicted masks. We introduce a bootstrapping pipeline with self-labeling ca-pabilities where pseudo-labels are validated using our proposed Mask Validity Filtering approach. We conduct extensive experiments to demonstrate that SA-FARI consistently achieves state-of-the-art performance across all RES datasets. Finally, we demonstrate excellent generalization capabilities of SAFARI on zero-shot referring video-object segmentation task. Extending our approach to multi-image and video settings can be looked upon as a promising future work."}]}