{"title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment", "authors": ["Qinfeng Li", "Yangfan Xie", "Tianyu Du", "Zhiqiang Shen", "Zhenghan Qin", "Hao Peng", "Xinkui Zhao", "XianWei Zhu", "Jianwei Yin", "Xuhong Zhang"], "abstract": "Proprietary large language models (LLMs) demonstrate exceptional generalization ability across various tasks. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security threats: attackers who obtain an edge-deployed LLM can easily use it as a base model for various tasks due to its high generalization ability, which we call foundational capability stealing. Unfortunately, existing model protection mechanisms are often task-specific and fail to protect general-purpose LLMs, as they mainly focus on protecting task-related parameters using trusted execution environments (TEEs). Although some recent TEE-based methods are able to protect the overall model parameters in a computation-efficient way, they still suffer from prohibitive communication costs between TEE and CPU/GPU, making it impractical to deploy for edge LLMs. To protect the foundational capabilities of edge LLMs, we propose CoreGuard, a computation- and communication-efficient model protection approach against model stealing on edge devices. The core component of CoreGuard is a lightweight and propagative authorization module residing in TEE. Extensive experiments show that CoreGuard achieves the same security protection as the black-box security guarantees with negligible overhead.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), especially proprietary ones such as ChatGPT [10] and Claude [25], demonstrate exceptional generalization ability across various tasks [4, 28]. Additionally, deploying LLMs on edge devices is a growing trend for latency- and privacysensitive tasks, e.g., Apple Inc. introduced Apple Intelligence, which integrates a 3-billion-parameter LLM into users' devices in the latest iOS version [3]. To protect edge-deployed models from stealing, many methods [33, 44] have been proposed. However, these methods are mainly designed for task-specific models, while for general-purpose LLMs, their edge deployment introduces new security: attackers who obtain an edge-deployed LLM can easily use it as a base model for various tasks due to its high generalization ability. We refer to this kind of theft as foundational capability stealing. Leakage of foundational capabilities can lead to significant harm to model owners, including substantial financial losses, the erosion of technological advantage, etc. Therefore, preventing edge-deployed LLMs from unauthorized usage is crucial.\nUnfortunately, existing model protection solutions [22, 31, 44] can hardly be applied to protect the foundational capabilities of LLMs, as traditionally deployed models are mostly task-specific, whose threat is inherently different from that of LLMs (as shown in Figure 1). Specifically, a task-specific model is usually trained from scratch or fine-tuned on a pre-trained model with a task-specific dataset, which is typically proprietary to the model owner. In this case, the scarcity of the task-specific dataset is the primary barrier for others to replicate this model, which is why most model-stealing methods [26, 27] usually assume the attacker possesses limited (e.g., 1%) task-specific data. Consequently, task-specific stealing is fundamentally data-oriented stealing. However, with the emergence of LLMs, its foundational capabilities have become a new scarce resource, making it a new target for stealing. Specifically, proprietary LLMs, like GPT-4, developed with prohibitive costs, can serve as exceptional backbones for various tasks. However, access to these models usually involves considerable financial costs to users. Therefore, to achieve good performance on various tasks without financial cost, foundational capability stealing becomes attractive. In this way, attackers could use the backbone on various tasks, where they might have plenty of data. Therefore, securing backbones demands effective protection across various tasks faced with sufficient training data, while protecting task-specific models only requires safeguarding a single task against limited data. Essentially, for task-specific models, the task-related parameters present the most valuable components, while for LLMs, all parameters must be protected. Therefore, existing task-specific solutions fail to protect the LLMs. For instance, Zhang et al. [44] protect task-specific parameters within a Trusted Execution Environment (TEE), which is an isolated hardware enclave that securely stores and executes sensitive data, but leaves the backbone unprotected, exposing its foundational capabilities to be misused.\nTo protect the backbone, a straightforward idea is to place the entire model in the TEE. However, straightforward black-box protection by TEEs is impractical because shielding entire LLMs within TEEs results in a roughly 50x reduction in model execution efficiency due to TEEs' limited computational speed [37]. To reduce TEE execution overhead, some researchers [22, 31] propose only placing a subset of the model (e.g., the last layer) in TEEs and offloading the rest of the computation to GPUs, i.e., Partial Parameter TEE Execution (PPTE). However, prior work [44] proves that PPTE even fails to protect task-specific models due to an unresolved balance between model security and TEE execution overhead. Essentially, PPTEs crudely load the sensitive parameters into TEE for protection, where the limited computational power of TEEs restricts the number of protected parameters, thus compromising security.\nTo increase the number of protected parameters, a promising approach is to protect parameters through shuffling, i.e., Parameter Shuffling Protection (PSP) [18, 33]. For example, ShadowNet [33] proposes executing all linear transformation layers (e.g., convolutional and linear layers) on the GPU. To secure these GPU-executed parameters, ShadowNet shuffles the channels of the convolutional kernels, ensuring that only the corresponding shuffled feature maps can be correctly computed with these shuffled parameters. This feature map shuffling process is performed within the TEE thus ensuring its security. However, the excessive data transfer overhead"}, {"title": "2 BACKGROUND AND THREAT MODEL", "content": "2.1 Background\nTEE. A Trusted Execution Environment (TEE) is an isolated hardware enclave that stores and processes sensitive data. Popular TEE implementations include Intel SGX [20], AMD SEV [14], and TrustZone [2]. In this paper, we follow prior work [44] and deem TEE a secure area on a potential adversary host device (including GPUs). This means the data, code, and computation processes inside TEES are secure. Although there are side-channel attacks that may leak sensitive data from TEE, they are out of the scope of this paper.\n2.2 Threat Model\nIn this paper, we consider two parties: the defender and the attacker. The defender is the party that owns the edge-deployed model. The attacker aims to steal the edge-deployed model.\nDefender's Goal. The goal of the defender is to deploy a locked model on the device and ensure that it only works when proper authorization is given by the trusted hardware (i.e., TEE) within the device.\nAdversary's Goal. The attacker aims to abuse the foundational capabilities of the deployed model for their task. A straightforward way to achieve this is trying to reverse the authorization process so that the locked model can be used independently of the device. Another more practical new way is to fine-tune the locked model to obtain a model that excels at a desired task.\nAdversary's Capability. The attacker could decide the target task, where they possess sufficient well-labeled datasets for the desired task. In this paper, we consider TEE to be a secure world, that cannot be compromised, while other hardware (e.g., GPU and CPU) could be exposed to attackers in a white-box manner. Therefore, the attacker can have white-box access to the details, e.g., model architecture and weights, of the locked model resizes outside TEE."}, {"title": "3 DESIGN OF COREGUARD", "content": "This section presents our proposed protection method, CoreGuard, which utilizes a permutation strategy to address the key requirements outlined in Section 1.\n3.1 Approach Overview\nAs shown in Figure 2, CoreGuard operates in two phases: model locking (before deployment) and inference authorization (post-deployment).\nIn the model locking phase, CoreGuard locks a trained model by applying a protection protocol to the weights of linear layers, i.e., swapping rows of the weight matrix. These row permutations essentially act as a lock, rendering the linear layers dysfunctional, thus making the overall model unusable. These row-permuted layers can only function properly with inputs that are correspondingly column-permuted, which essentially acts as authorization. However, authorizing the input features of each permuted layer using the TEE would result in significant TEE-GPU transfer overhead. To address this, CoreGuard proposes a propagation protocol, which enables the features to be column-permuted by the network itself. Specifically, CoreGuard permutes the columns of certain layers, thus through their operation, their output features are column-permuted, which achieve authorization similarly. In this way, the TEE only needs to manage the initial authorization, and the authorization can be propagated to all subsequent layers.\nIn the inference authorization phase, the TEE manages the initial authorization by applying a column permutation to the input features for authorized use. However, using TEE directly for this column permutation can leave the process vulnerable to input-output speculation (i.e., inferring the permutation order by comparing features before and after the permutation). To avoid this, CoreGuard encrypts the input feature in advance. However, encrypting the features just before they enter the TEE is ineffective, as the attacker can obtain the previous layer's output, which is unencrypted. Furthermore, placing the encryption process before the previous add-norm layer is still ineffective, as the add-norm layer does not change the relative magnitudes of the elements, allowing the attacker to utilize the unencrypted output feature of the layer before the add-norm layer. Therefore, we place the encryption process before the output linear layer of the FFN block. Specifically, a one-time pad (OTP) encryption is applied, which generates a random mask and adds the mask to the feature for concealment [30]. To further secure this encryption process, we introduce a permutation to obfuscate the masked features and apply a corresponding permutation to the linear layer to prevent this permutation from affecting the computation's correctness.\n3.2 Model Locking\nGiven a classic transformer layer, we describe how to permute its weights for protection. This permutation first applies the protection protocol to layers involved in input feature projection (e.g., the QKV projection layer in the attention block and the input linear layer in the FFN block) to secure the model. Then, the propagation protocol is applied to layers managing output projection (e.g., the output projection layer in the attention block and the output linear layer in the FFN block) to propagate authorization.\nTransformer Layer Formalization. We begin by formalizing a standard transformer layer. Let $x$, and $z \\in R^{l \\times d}$ denote its input and output, where $l$ is the sequence length (e.g., the number of tokens) and $d$ is the model dimension. We define a classic attention block as a function $f_w : R^{l \\times d} \\rightarrow R^{I \\times d}$ with weight parameters $w$. The transformer layer, i.e., $f_w(x) = z$, is computed as follows:\n$Q = xWq, K = xWk, V = xW\u028a, $\\ \\ //Q,K,V projections\n$o = softmax(\\frac{QKT}{\\sqrt{d/h}})MVWo, $\\ \\ //Attention calculation\n$y = \u03b31(\\frac{o + x - \u00b5_{o + x}}{\u03c3_{o+x}}) + \u03b21, $\\ \\ // Add & norm \\ \\ (1)\n$m = ReLU(yWm + bm), $\\ \\ //FFN input\n$n = mWn + bn, $\\ \\ //FFN output\n$z = \u03b32(\\frac{n + y - \u00b5_{y + n}}{\u03c3_{y+n}}) + \u03b22, $\\ \\ // Add & norm\nwhere $w$ includes the attention weights $Wq, Wk, W\u028a$, and $Wo \\in R^{d \\times d}$, the add-norm weights $\u03b31, B1, \u03b32$, and $\u03b22 \\in R^d$, the FFN weights $Wm$ and $Wn \\in R^{d \\times d}$, the bias $bm$ and $bn \\in R^d$. $h$ is the number of attention heads. The mask $M\\in R^{d \\times d}$ is an all-zero matrix in the encoder and has negative infinity in its upper right corner in the decoder. Among these layers, we refer to $Wq, Wk$, and $Wo$ in the attention block, and $Wm$ in the FFN block as input-processing layers as they process the input feature of their respective blocks. Meanwhile, we refer to $Wo, Wn$, and the add-norm weights in each block as output-processing layers as they manage the output of the blocks.\nProtection Protocol. To achieve protection, we row-permutate the parameters of input-processing layers for protection. These layers process the module's inputs directly, thus they have the ability to cause immediate computation failures if the input is not authorized, leading to incorrect results in all subsequent calculations.\nLet $\u03c0\\in \\{0, 1\\}^{d \\times d}$ denote a permutation matrix, where $\u2200\u03c0, \u03c0\u03c0] = I$, with $I$ as the identity matrix, a property of the permutation matrix. We row-permutate the parameters $w$ as follows:\n$WqW \\pi Wk, W \\pi W, Wm = \\pi^{T} Wm. $\\ \\ (2)\nThus, only the corresponding column-permuted input could be computed with these layers. For instance:\n$W_\u2081 = \u03c0\u00b9W\\W\u2081 = x\u03c0\u03c0\u00b9 Wq = xWq = Q,$ \\ \\ (3)\nwhere column (i.e., $\u03c0$, the authorization) and row (i.e., $\u03c0^{-1}$, the lock) permutations offset each other ($\u03c0\u03c0^{-1} = I$), resulting in the same computation as the original.\nPropagation Protocol. Additionally, to achieve the same effect as TEE authorization (i.e., column-permutate the feature), we column-permutate output-processing layers. These layers handle the module's outputs directly, giving them the ability to ensure the feature is re-authorized before leaving the module. The specific propagation protocol is as follows:\n$W\u03bf = W\u03bf\u03c0, \u03b31 = \u03b3\u03b9\u03c0, \u03b21 = \u03b2\u03b9\u03c0,$\n$W = W\u03b7\u03c0, bn = bn\u03c0, \u03b32 = \u03b32\u03c0, \u03b2\u03842 = \u03b22\u03c0.$\nThus, the features are automatically column-permuted (i.e., authorized) after these layers. For instance:\n$n' = mW = mW\u03b7\u03c0 + bnn = \u03b7\u03c0,$\nwhere $n'$ is equal to column-permuted $n$ (i.e., $\u03b7\u03c0$).\nPermuted Transformer Layer Formalization. With the permuted weights (denoted as $w'$), taking $\u03c7\u03c0$ as authorized input, its functionality can be described as follows:\n$Q' = x\u03c0\u03c0\u00b9Wq = xWq = Q,$\n$\u039a' = \u03c7\u03c0\u03c0\u00b9Wk = xWk = K,$\n$V' = \u03c7\u03c0\u03c0\u00b9 W\u2081 = xW\u2081 = V,$\n$o' = softmax(\\frac{Q'K'T}{\\sqrt{d/h}}) + - MV'W\u03bf\u03c0 = \u03bf\u03c0,$\n$y' = \u03b3\u03b91\u03c0 \u039f + \u03b21\u03c0 = \u03a5\u03c0.\\ \\ \\ (6)\n$m' = ReLU(y' \u03c0\u00b9 Wm + bm) = m.$\n$n' = m' W\u03b7\u03c0 + b\u03b7\u03c0 = \u03b7\u03c0,$\n$z' = \u03b3\u03b5\u03c0 \u039f + \u03b22\u03c0 = \u0396\u03c0.$\nThus, the functionality of the permuted layer can be represented as $f_{w'}(x') = z\u03c0 = f_w(x)\u03c0$, valid only when $x' = \u03c7\u03c0$, prevents unauthorized access (without \u03c0). However, when the permuted transformer layer takes the authorized input (\u03c7\u03c0), its output ($zr$) is equivalent to the original output ($z$) with a column permutation, which serves as the authorization for the next layer. This propagation remains consistent across all subsequent layers. In other words, authorization is required only for the first permuted layer.\nHowever, the authorization process requires an initial point, which determines how many layers CoreGuard will permute, impacting the overall security. To ensure security, CoreGuard sets the initial authorization point in the middle of the network and permutes the layers in the latter half of the transformer, with a detailed experimental discussion provided in Section 4.2.\n3.3 Inference Authorization\nDuring the inference authorization phase, it is required to generate the initial column-permutated feature (i.e., $x\u03c0$) for authorized usage while also ensuring the security of the authorization process. As mentioned in the overview, to ensure security, we integrate the TEE authorization to the preceding FFN block. Specifically, TEE"}, {"title": "4 EXPERIMENTS", "content": "In this section, we perform extensive experiments to answer the following research questions (RQs):\nRQ1: How does CoreGuard's security compare to other defenses, particularly in protecting the foundational capabilities of LLMs? RQ2: How does CoreGuard's computation and communication overhead compare to other defenses? RQ3: Does CoreGuard sacrifice the accuracy of the model?\n4.1 Experimental Settings\nDatasets. To evaluate CoreGuard's effectiveness, we assess it on four representative domain-specific tasks: GSM8k (mathematics) [6], Spider (code generation) [43], PubMedQA (medical question answering) [13], and SQUAD (reading comprehension) [29].\nModels. We choose four representative LLMs for validation. Two of them are specifically designed for on-device use: Qwen2-0.5B-Instruct [42], Gamma2-2B-it [35]. The other two are larger models: ChatGLM3-6B-32k [8] and LLaMA3-8B-Instruct [36]."}, {"title": "5 LIMITATION AND DISCUSSION", "content": "Real-World Efficiency Evaluation. Although platform-irrelevant metrics (e.g., FLOPs and KB) demonstrate that CoreGuard incurs minimal additional overhead, the diversity of hardware and variations in testing environments prevent us from systematically evaluating the actual overhead. Future research could conduct extensive performance tests across various hardware platforms and environments to ensure a comprehensive efficiency analysis.\nSide Channel Attacks. CoreGuard uses TEEs as a security measure, which makes it vulnerable to side-channel attacks [1, 32, 38]. However, various defense methods have emerged in recent years to mitigate the risk of side-channel leaks, and both these software- [16, 19] and hardware-based [7, 39] defense solutions can be integrated into our approach. For software-based defense, CoreGuard"}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduce a protection method named CoreGuard for edge-deployed LLMs. CoreGuard utilizes a permutation strategy to safeguard these models, thus achieving black-box-level protection. Importantly, to reduce transfer overhead during inference, CoreGuard proposes an authorization propagation protocol, thus only a single authorization is required to authorize the entire model, which is optimal. The comprehensive experiments indicate that CoreGuard exhibits exceptional security and efficiency compared to the existing approaches without accuracy loss. In conclusion, CoreGuard is an effective solution to protect the edge-deployed proprietary LLMs, providing model owners with the means to safeguard their valuable intellectual property."}]}