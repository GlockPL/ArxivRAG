{"title": "CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories", "authors": ["Yijia Xiao", "Runhui Wang", "Luyang Kong", "Davor Golac", "Wei Wang"], "abstract": "The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.", "sections": [{"title": "Introduction", "content": "With the rapid evolution of Large Language Models (LLMs), it has been demonstrated that LLMs have increased reasoning ability over the last few years, making intelligent agents based on LLM possible. Current agent-related applications in computer science include code writing (Codex (Chen et al., 2021), Deepseek Code (Guo et al., 2024), CodeLLaMA (Rozi\u00e8re et al., 2023), etc.), code base generation (MetaGPT (Hong et al., 2023), Agentless (Xia et al., 2024a) and CodeStar(Li et al., 2023)), code correction (SWEBench (Jimenez et al., 2024)), and more.\nIn computer science research projects, the associated codebases grow very rapidly, and a self-consistent codebase typically has several parts, including the instruction file (e.g., the README file), associated code packages, and related data. The instruction file usually contains an overview of the project, including environment setup, data preparation (e.g., data download and pre-processing, model weights download and preparation), model training process, performance evaluation, and the setup of a demo project (e.g., a chatbot project that interacts with users). In computer science, prestigious conferences, including NAACL, ACL, ICLR, NeurIPS, CVPR, KDD, etc., encourage researchers to release source code for reproducibility of their accepted papers, and GitHub is the top choice for maintaining codebases for most researchers. A major step of computer science research is reproducing existing work, which is essential to gain insights and propose novel methodologies. However, even for well-documented and self-consistent projects, the setup process requires manual efforts and cannot be fully automated; many steps of setting up a code repository are rather mechanical, such as installing/updating dependency packages to configure the environment, downloading data, updating the relevant script/data directories, etc., which is tedious and often time-consuming.\nTo tackle such challenges, we propose to use LLM agents for automating the deployment of code repositories of research projects, and build a benchmark, Computer Science Research Benchmark (CSR-Bench) for evaluating LLM Agents on code repository deployment tasks. We also propose a multi-agent collaborative framework, CSR-Agents, to automate the deployment of code repositories by coordinating multiple LLM agents with different"}, {"title": "Related Work", "content": "Coding LLMs. Large Language Models (LLMs) have become the go-to solution for a wide array of coding tasks due to their exceptional performance in both code generation and comprehension (Chen et al., 2021). These models have been successfully applied to various software engineering activities, including program synthesis (Patton et al., 2024; Chen et al., 2021; Li et al., 2022a; Iyer et al., 2018), code translation (Pan et al., 2024; Roziere et al., 2020, 2021), program repair (Xia et al., 2023; Xia and Zhang, 2023; Monperrus, 2018; Bouzenia et al., 2024), and test generation (Deng et al., 2023; Xia et al., 2024b; Deng et al., 2024; Lemieux et al., 2023; Kang et al., 2023). Beyond general-purpose LLMs, specialized models have been developed by further training on extensive datasets of open-source code snippets. Notable examples of these code-specific LLMs include CODEX (Chen et al., 2021), CodeLlama (Rozi\u00e8re et al., 2023), StarCoder (Li et al., 2023; Lozhkov et al., 2024), and DeepSeek-Coder (Guo et al., 2024). Additionally, instruction-following code models have emerged, refined through instruction-tuning techniques. These include models such as CodeLlama-Inst (Rozi\u00e8re et al., 2023), DeepSeek-Coder-Inst (Guo et al., 2024), WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2023), and OpenCodeInterpreter (Zheng et al., 2024).\nBenchmarking LLM-based coding tasks. To assess the capabilities of LLMs in coding, a variety of benchmarks have been proposed. Among the most widely utilized are HUMANEVAL (Chen et al., 2021) and MBPP (Austin et al., 2021), which are handcrafted benchmarks for code generation that include test cases to validate the correctness of LLM outputs. Other benchmarks have been developed to offer more rigorous tests (Liu et al., 2023a), cover additional programming languages (Zheng et al., 2023; Cassano et al., 2023), and address different programming domains (Jain et al., 2024; Hendrycks et al., 2021; Li et al., 2022b; Lai et al., 2023; Yin et al., 2022).\nMore recently, research has shifted towards evaluating LLMs on real-world software engineering challenges by operating on entire code reposito-"}, {"title": "CSR-Bench", "content": "In this section, we provide the problem statement for code deployment in CSR-Bench, introduce the code repository collection process of computer science research projects, and show their statistics."}, {"title": "Problem Statement", "content": "The CSR-Bench consists of a collection of computer science research repositories from GitHub and these repositories are used for evaluating the capabilities of LLMs in code deployment tasks. For each repository, the deployment tasks typically include: (1) setting up the environment; (2) preparing necessary data and model files; (3) conducting model training; (4) demonstration of inference; (5) performance evaluation. To complete these tasks, we prompt LLMs to generate executable bash commands by using the README file as the primary source of information and other repository contents (source code, bash scripts, directory structure, and etc.) as supplementary information.\nMetric. During the evaluation in CSR-Bench, the large language model will be prompted to generate executable commands for the corresponding sections for each repository. we use the completion rate as a key metric, defined as the ratio between number of successfully executed commands and the total number of commands executed."}, {"title": "Repository Collection", "content": "In CSR-Bench, we aim to collect a diverse and comprehensive collection of code repositories of computer science-related research projects. GitHub is a good data source for this purpose and it provides tags for identifying most relevant repositories. Some example tags are \u201cnlp\u201d, \u201cnaacl\u201d, and \"emnlp2024\u201d. Since CSR-Bench focuses on computer science-related repositories, we filter the repositories by tags of various conference names and categories to ensure they include diverse topics. For repository selection, we use GitHub tags to obtain an initial set of over 1500 repositories that are relevant to computer science research topics and categorizing them into five areas: Natural Language Processing, Computer Vision, Large Language Models, Machine Learning, and Interdisciplinary topics. Notably, we collect repositories related to large language models because nowadays LLM-related research projects are increasingly popular due to its foundational impact in various areas of computer science.\nWe obtain 100 high-quality code repositories for CSR-Bench in the following steps. First, we categorize this initial set and sort them by the number of GitHub stars. Next, we manually check the content of each repository starting from the top of the sorted list. In this step, we only keep reposi-"}, {"title": "Statistics of CSR-Bench", "content": "This section provides an in-depth analysis of the traits of repositories in CSR-Bench. We examine the diversity and breadth of topics covered, as well as detailed statistics about the documentation and structure of these repositories.\nIn CSR-Bench, the README files and directory structures provide critical insights into the usability and organization of repositories. We use the following figures to analyze the lengths of README file and number of files, and offer a quantitative view of content complexity and organizational depth. The length of README file is an important metric because the most LLMs have limits on the input token length. The number of files indicate the complexity of the code repository."}, {"title": "CSR-Agents", "content": "In this section, we propose CSR-Agents, a multi-agent framework that leverages LLMs for different tasks and achieves effective cooperation among agents for code deployment tasks. We introduce our standardized environment for code deployment, functionalities of different agents, and their cooperation workflow."}, {"title": "Code Deployment with LLM", "content": "For each repository, we use the README file under the root folder as the major source of information and prompt LLMs to generate executable bash commands for different steps of deployment including environment setup, data preparation, training, inference and evaluation.\nTo achieve reproducbility and safety, we use the Docker container to isolate the code deployment environment. We build a standard Docker image that is equipped with essential tools like bash, Conda, GCC, Make, Python, etc. for evaluations across all repositories and various LLMs. In the container, we use a counter to count the total number of scripts that needed to be executed and the number of scripts that were executed successfully. In this way, we safeguard the entire computing system, especially from bash command executions"}, {"title": "CSR-Agent: LLM Agent Design", "content": "In CSR-Agents, we adopt an iterative trial-and-error process for successful code deployment. Specifically, LLM takes README, directory structure, and error logs from failed command execution as input and generates bash commands to complete the deployment tasks. The system comprises five agents: Command Drafter, Script Executor, Log Analyzer, Issue Retriver, and Web Searcher. These agents collectively facilitate the deployment of a code repository. The complete workflow is shown in Figure 7.\nCommand Drafter: This agent reads the README files and directory structure and generates a draft script containing bash commands for deployment. Then, it divides the entire script into five sections, each corresponding to a step in the code repository deployment. This sectional division also serves as an evaluation standard later on.\nScript Executor: This agent receives the draft commands from the Command Drafter and execute the commands in our standardized Docker environment. After execution, it collects logs from bash, including standard output and errors. Note that during bash script execution, no explicit return code"}, {"title": "LLM Coorporation Framwfork", "content": "The workflow operates as follows: Deployment commands are drafted from repository documentation, executed in a bash environment, and adjusted based on log analysis if errors arise. Additional information is retrieved from an issue database and web search if needed. The process concludes with"}, {"title": "Evaluations", "content": "In this section, we evaluate CSR-Agents in CSR-Bench with a wide range of popular foundation LLM families, including Claude 4, GPT-4 (Achiam et al., 2023), Llama-3 (Dubey et al., 2024), and Mistral 5. For each LLM family, we experimented with different model sizes for thorough comparisons. We show the completion rate in Table 1, Table 2, Table 3, and Table 4. In these tables, we use S to stand for the Setup stage, D to stand for the ownload stage, T to stand for the Training stage, E to stand for the Evaluation stage, and I for the inference stage."}, {"title": "Initial Drafter", "content": "As shown in Table 1, all models perform well on Setup and Download tasks (success rates around 0.23 to 0.28) but struggle with Training, Evaluation, and Inference, where success rates are close to zero. This indicates that the drafter agent handles basic installation effectively but has difficulty with complex tasks requiring updates to file paths and environment variables.\nWe note that this is also similar to the deployment process of a real researcher, where their first execution is more likely to fail and they need to analyze the errors and leverage tools like GitHub issues or search engines to solve the problem."}, {"title": "Log Analyzer", "content": "Table 2 shows noticeable improvements in all tasks compared to the drafter stage. Success rates for Setup and Download increase to around 0.34 to 0.40, while complex tasks see gains up to 0.18.\nAnalyzers leverage dynamic feedback from the execution of commands to refine scripts and try to correct errors from executed commands."}, {"title": "Issue Retriever", "content": "In the Issue Retriever stage (Table 3), success rates continue to improve, especially for complex tasks like Training, Evaluation, and Inference, reaching up to 0.25.\nThe results show that access to a knowledge base with informative discussions on issues of the code repository allows LLMs to retrieve solutions to execution errors from earlier stages, enhancing performance in complex operations."}, {"title": "Web Searcher", "content": "The Searcher Success Metrics in Table 4 exhibits the highest performance. Success rates for Setup and Download reach up to 0.46, and complex tasks improve to between 0.15 and 0.29."}, {"title": "Aggregated Results", "content": "We show the aggregated results of a single LLM on different tasks with different level of engagements of multi-agents in Figure 8, Figure 9, Figure 10, and Figure 11. In short, with more agents contributing to solving the tasks, the success rate increases across all tasks for all LLMs, which demonstrates the effectiveness of our proposed CSR-Agents."}, {"title": "Results Interpretation", "content": "The evaluation of the CSR-Bench involved assessing the performance of various large language models (LLMs) across key tasks: Setup, Download, Training, Inference, and Evaluation. These tasks are essential for deploying repositories within the CSR-Bench."}, {"title": "Task-Specific Outcomes", "content": "Setup and Download: Most models consistently performed well, reflecting their capability to initiate and manage basic deployment processes."}, {"title": "Overall Performance", "content": "The success metrics across different tasks and models indicate a wide variability in performance. Generally, models showed higher success rates in Setup and Download tasks, with performance tapering off in more complex tasks such as Inference, Evaluation, and Training. This pattern highlights the challenges LLMs face in handling the full deployment process autonomously.\nThe results demonstrate that while LLMs have made significant strides in automating repository deployment, their ability to manage complex tasks remains limited. Improvements are needed, particularly in the areas of Inference and Training, to achieve fully autonomous and reliable deployment of science repositories.\nHowever, there is still a large gap between LLMs and real scientists even if the advnanced tools are provided to the LLMs. To explain, it is not trivial to"}, {"title": "Conclusion", "content": "The work introduces CSR-Bench, a benchmark designed to evaluate the capabilities of LLM agents in automating the deployment of GitHub repositories for scientific research. Our study highlights that while LLMs show potential in handling tasks like environment setup and data preparation, they face challenges in complex tasks such as training and inference, where success rates are notably lower.\nOur multi-agent framework, CSR-Agents, exemplifies how LLMs can collaborate to tackle deployment challenges, offering a promising approach to improving automation in software engineering. However, the results indicate that further advancements are needed to fully realize autonomous and reliable deployment processes.\nOverall, CSR-Bench serves as a crucial tool for assessing and improving LLM-driven deployment workflows in scientific research, paving the way for more efficient and automated computer science projects exploration."}, {"title": "Limitations", "content": "Although our benchmark framework supports several tools to facilitate large language model agents in the code deployment task, it does not actually improve the original reasoning capabilities of the large language models that are used in the agents. To improve LLMs' reasoning capabilities for this specific task, the community may resort to techniques like RLHF, which is orthogonal to this work.\nOur benchmark only focuses on code repositories that related to computer science research topics, and does not involve other types of repositories. Although this framework can be reused for other types of the repositories, we do not explore that direction in this work, and leave it to future works."}]}