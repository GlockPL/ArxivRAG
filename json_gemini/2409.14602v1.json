{"title": "Can pre-trained language models generate titles for research papers?", "authors": ["Tohida Rehman", "Debarshi Kumar Sanyal", "Samiran Chattopadhyay"], "abstract": "The title of a research paper communicates in a succinct style the main theme and, sometimes, the findings of the paper. Coming up with the right title is often an arduous task, and therefore, it would be beneficial to authors if title generation can be automated. In this paper, we fine-tune pre-trained and large language models to generate titles of papers from their abstracts. We also use ChatGPT in a zero-shot setting to generate paper titles. The performance of the models is measured with ROUGE, METEOR, MoverScore, BERTScore and SciBERTScore metrics. Our observations indicate that AI-generated paper titles are generally accurate and appropriate.", "sections": [{"title": "Introduction", "content": "One of the most important aspects in writing a research paper is coming up with a concise and informative title for the paper. In academic publishing, a title should be not only concise and informative but also engaging, ensuring that authors quickly grasp the paper's insights. At a first glance, readers should gain a clear understanding of the contributions of the paper. Effective title contain keywords describing the reported research, and sentences in the paper containing words in the title typically reflect the paper's main theme. Many studies have been devoted to exploring the connection between the title of a paper and its readership as captured by citations and downloads [1,2,3]. Short titles reportedly attract more citations [2].\nGiven the tremendous success of neural language models, especially large language models (LLMs), in various natural language processing (NLP) tasks, it is natural to ask if titles of scientific papers can also be generated by these models. Given the expectation that a title should capture the key import of a paper and yet be a short sequence of words that appeal to a large readership, the task is a challenging one. Title generation can be considered as a special"}, {"title": "Literature survey", "content": "Automatic text summarization has a long history. As far back as 1958, Luhn et al. [14] pioneered an extractive summarization technique that selects sentences"}, {"title": "Datasets", "content": "We have used the CSPubSum dataset provided by Collins et al. [31], which contains URLs of 10147 computer science publications from ScienceDirects. We crawled the dataset and organized each example as a pair of abstract and title."}, {"title": "Methodology", "content": "In this section, we detail the fine-tuning process of various pre-trained language models (PLMs) and large language models (LLMs). We have chosen the following models:\n1. T5-base [4]: It is an encoder-decoder model which is a slight variation of the original Transformer model [19]. Formulating every text processing problem, including translation, question answering, and classification, as a \"text-to-text\" transformation problem, the same model is trained to perform these diverse tasks. To pre-train the model, random text spans are corrupted/dropped and the model is trained to generate them. T5-base contains 220M parameters. We fine-tune the model with the train subset of CSPubSum.\n2. BART-base [5]: A denoising autoencoder, it combines Bidirectional and Auto-Regressive Transformers exemplified by BERT [21] and GPT [32], respectively. To train BART, the input text is first corrupted with a noising function, and then the model reconstructs the original text. It is particularly suitable for text generation problems. BART-base is configured with 139M parameters. Like T5-base, it is also fine-tuned on CSPubSum train set."}, {"title": "Data processing", "content": "We removed only extra spaces from the documents in both datasets. We only retain examples where the abstract length is at least twenty tokens and the paper title length is at least three tokens. Since we impose a limit of 20 tokens on generated title length, we stipulate the abstract to be longer than 20 tokens so that the task can be treated as a text summarization problem."}, {"title": "Implementation details", "content": "We have chosen the following pre-trained models from the Hugging Face repos-itory for fine-tuning on the CSPubSum dataset: T5-base12, BART-base13,"}, {"title": "Evaluation metrics", "content": "We used commonly used automatic text summarization evaluation metrics, including ROUGE [9], METEOR [10], MoverScore [11], BERTScore [12] and SciB-ERTScore [33], to assess the quality of generated title with the author-written title. ROUGE scores measure n-gram overlap between the generated and ground-truth titles. We use ROUGE-1, ROUGE-2, and ROUGE-L where the first uses unigram overlap, the second bigram overlap and the last compares the longest common subsequence between the generated title and the golden title. METEOR measures sentence-level accuracy based on the alignment between the", "equations": []}, {"title": "Quantitative comparison of different fine-tuned models", "content": "In this sub-section, we report the results of experiments on the test dataset of CSPubSum dataset as well as LREC-COLING-2024 dataset. Table 1 shows the performance in terms of ROUGE, METEOR, and semantic metrics like MoverScore, BERTScore and SciBERTScore, for the CSPubSum test dataset. Evaluation with entity-level factual consistency metrics precision-source(precNU,prec), precision-target (precNU, prec\u0173), recall-target (recall\u0145U, recally), and F1-target (F1NU, F1\u0172) are shown in Table 2. The overall finding is that PEGASUS-large fine-tuned on the CSPubSum dataset achieves the highest scores for all the above metrics except precision-target metric. Table 3 and Table 4 show that the same PEGASUS-large model, that has been fine-tuned on the CSPub-Sum dataset, achieves the best performance on the LREC-COLING-2024 dataset except on BERTScore metric. Thus, the superlative performance of PEGASUS-large carries over to this dataset although it is not fine-tuned on it."}, {"title": "Case studies", "content": "We show three representative examples of title generation one from CSPubSum (test subset) and two from LREC-COLING-2024 to illustrate the behavior of the models. Figure 1 shows the example from CSPubSum: the fine-tuned T5-base, BART-base, and PEGASUS-large models have generated titles that have significant similarity with the author-written titles. However, T5-base appears to generate a very long title. BART-base and PEGASUS-large generated the same titles. In contrast, LLaMA-3-8B without fine-tuning, produced a title that resembled a one-sentence summary and even that is incomplete; it is more of an extract from the abstract. After fine-tuning, the LLaMA-3-8B model generated an acceptable title. ChatGPT 3.5, using prompt-based tech-niques, successfully generated a title that also captures the essence of the paper and is similar to other generated titles and the author-written title. However, despite its stylistic flair, the evaluation metrics (in Table 1) indicate that the titles generated by ChatGPT-3.5 scored lower than those generated by the fine-tuned PEGASUS-large model. This discrepancy also highlights a limitation of the automated metrics in accurately evaluating titles generated in a highly abstractive and stylistic manner.\nNow let us look at Fig. 2 which shows the outputs generated by the models for an example from LREC-COLING-2024 dataset. In case of BART-base, the generated title is incomplete. This is due to the hard limit on the output token count. We have observed this problem with other models, too. A sim-ple workaround (without retraining the models) is to increase the output token"}, {"title": "Manual evaluation", "content": "We selected 20 papers, ten from each dataset (note that we use the test subset in case of CSPubSum) for human evaluation. A human annotator working in the field of NLP was asked to choose the most appropriate title among the generated ones. It is found that for CSPubSum dataset, in 80% of the cases, the title generated by PEGASUS-large is most preferred, while for LREC-COLING-2024, in 50% cases, the output of PEGASUS-large wins while in 40% cases, the title generated by ChatGPT-3.5 is considered the best."}, {"title": "Demo", "content": "A demo of the application is hosted at https://title-generation-researchpapers.onrender.com/ where the user can input an abstract into a text box, select a suitable fine-tuned language model, and the maximum token count for the title,"}, {"title": "Conclusion", "content": "We applied six different pre-trained/LLM models to generate titles of research papers. The fine-tuned PEGASUS-large model has achieved the best performance on the CSPubSum dataset as well the LREC-COLING-2024 dataset. Our extensive experiments suggest that pre-trained and large language models can generate engaging and suitable titles for research papers. The success of PEGASUS-large suggests that lighter pre-trained language models, fine-tuned on domain-specific datasets, may be satisfactory and even more suitable for this task than large language models with billions of parameters and substantial computational requirements. The high scores achieved by the models on LREC-COLING-2024 dataset on which they are not fine-tuned indicates that fine-tuning on a related scholarly dataset suffices to generate acceptable performance. Although the generated titles are generally of high quality, authors may further refine them to better suit their papers. This tool can be particularly useful for non-native English speakers and novice researchers. In future, we would like to analyze in more detail the differences between author-assigned and AI-"}]}