{"title": "ACTIVATION STEERING IN NEURAL THEOREM PROVERS", "authors": ["Shashank Kirtania"], "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language mod- els struggle to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle we use activation steering to guide LLMs re- sponses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valu- able in resource-constrained environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Interactive proof assistants such as Lean de Moura et al. (2015), Isabelle Wenzel et al. (2008), and Coq Barras et al. (1999) enable the formal verification of mathematical proofs and software by leveraging specialized programming languages Avigad (2023); Ringer et al. (2019). Neural theorem proving, which integrates neural language models with interactive proof assistants, has emerged as a promising approach to automating formal reasoning First et al. (2023); Polu & Sutskever (2020b); Polu et al. (2022); Yang et al. (2023b); Welleck (2023). This integration is mutually beneficial: proof assistants enforce formal correctness, while language models assist in proof construction by predicting and suggesting logical steps. A central challenge in this setting is tactic prediction\u2014determining the appropriate next step at each proof state.\nIn this work, we investigate activation steering Panickssery et al. (2024); Turner et al. (2024); Lucchetti & Guha (2024) as a technique to enhance tactic prediction in Llemma Azerbayev et al. (2024a) and InternLM2 Ying et al. (2024a). These language models are designed for theorem proving by training and fine-tuning on mathematical data. Activation steering is an inference-time model edit- ing method that modifies a model's internal representations to guide its behavior toward desired outputs. We propose its application in refining tactic selection, aiming to improve both the accuracy and interpretability of proof automation. By systematically influencing LLMs' reasoning process, our approach enables structured interventions that enhance model-driven theorem proving, leading to more reliable and controllable predictions.\nWe present an approach for steering tactic selection from a pair of prompts (p1, p2) that contain a LEAN state s to generate the next step (or tactic) t. However, the LLM successfully predicts t for P1 but mispredicts for p2. In each pair p2 is natural data and p\u2081 is synthetically generated using p2. We systematically add the attributes and a high level of structure the proof should follow. These additional attributes guide the model to a specific and more grounded chain of thought to follow while predicting the tactics. These abstractions over the proof help the LLM to do critical decision making while predicting the next step."}, {"title": "2 RELATED WORK", "content": "Formal theorem proving encodes theorems and proofs in a machine-verifiable format, ensuring cor- rectness through rigid logical rules. A key component of this field is Interactive Theorem Prov- ing (ITP), where humans collaborate with proof assistants such as Isabelle Wenzel et al. (2008), Lean de Moura et al. (2015), and Coq Barras et al. (1999) to formally verify proofs. These assistants allow users to express theorems in higher-order logic and construct verifiable proofs.\nIn Lean de Moura et al. (2015), proofs are built using tactics, which either solve a goal or decompose it into sub-goals."}, {"title": "2.2 PROOFSTEP GENERATION WITH LARGE LANGUAGE MODELS", "content": "Generating intermediate proof steps is a fundamental challenge in theorem proving, particularly in tactic-based automated theorem provers (ATPs). Early neural approaches (Whalen, 2016; Huang et al., 2019; Bansal et al., 2019; Paliwal et al., 2020; Sanchez-Stern et al., 2020) framed proofstep generation as a classification task, employing models like TreeLSTM and RNN to predict tactics and their arguments. ASTactic (Yang & Deng, 2019) later introduced a grammar-constrained decoder for structured tactic generation.\nRecent advances leverage large language models (LLMs) for proof generation, casting tactic predic- tion as an extitauto-regressive sequence modeling problem. GPT-f (Polu & Sutskever, 2020a) pio- neered this approach by training transformers only with decoders to generate structured proof steps. Baldur (First et al., 2023) extended this by producing entire proofs, while POETRY (Wang et al., 2024) adopted a recursive decomposition strategy. Other works (Szegedy et al., 2021; Tworkowski et al., 2022; Welleck et al., 2022; Jiang et al., 2022; Yang et al., 2023a) integrate the selection of premises with tactic prediction, employing retrieval-augmented methods and constrained decoding to improve the coherence of the proof.\nLean-Star Lin et al. (2024) introduced Self-Taught Reasoning, incorporating Chain-of-Thought Wei et al. (2023) reasoning before each tactic to generate synthetic data for fine-tuning LLMs via self-play Chen et al. (2024). Our work leverages these randomly sample data points from Lean-STaR- base as natural data."}, {"title": "2.3 MECHANISTIC INTERPRETABILITY", "content": "Previous work has focused on localizing and editing factual associations within transformers (Meng et al., 2022) and probing hidden representations for high-level knowledge (Li et al., 2024b; Dong et al., 2023). Such studies perform implicit evaluations of model ability, complementing explicit benchmarks (Dong et al., 2023). A key technique in mechanistic interpretability is activation patch- ing (Vig et al., 2020; Variengien & Winsor, 2023), which modifies model activations to influence outputs. This research has suggested the existence of task vectors (Hendel et al., 2023; Ilharco et al., 2022)-representations encoding abstract task information. Activation steering has been employed to mitigate model deceitfulness and sycophancy (Rimsky et al., 2023; Li et al., 2024b), further sup- porting the presence of task vectors. Steering is based on the linear representation hypothesis (Park et al., 2023), which posits that concepts exist as directions in the embedding space of the model.\nFor theorem proving, mechanistic interpretability provides insights into how LLMs represent logical structures and reasoning processes. By dissecting these representations, we can identify failure cases, refine tactic prediction, and enhance proof generation. We hypothesize that effective steering transforms activations to align the model's reasoning trajectory with a more structured and verifiable direction."}, {"title": "2.4 MODEL STEERING", "content": "Activation-based interventions can directly influence the language model output during infer- ence Dathathri et al. (2019); Subramani et al. (2022). Recent studies demonstrate that activation"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 CONSTRUCTING STEERING DATASET", "content": "We build steering datasets for 7B parameter Llemma Azerbayev et al. (2024b) and 7B parameter InternLM2 Ying et al. (2024b). These models are trained to generate formal theorems and code, which is important for the tactic prediction task.\nSource Dataset We constructed steering pairs from a randomly sampled subset S from Lean-STaR data. The subset consists of approximately ten thousand unique proof stages and tactics. We treat this subset as natural prompt data p2 and generate a new set p\u2081 by adding reasoning steps in p2 representing a lean stage l. This generates a set of pairs (p1, p2) which we then prompt InternLM2 to predict the next tactic t\u2081 and t2 respectively. We then take pairs where t\u2081 \u2260 t2 creating a subset s of prompt pairs. We then validate tactics t1, t2 \u2208 s with Lean Prover to generate s'.\nThis process generates a quadruple {P1,P2,t1, t2} where t\u2081 and t2 are valid tactics for a lean stage l. We assume that t\u2081 is a more optimal tactic for l.\nIn Fig. 2, we illustrate the process of improving theorem-proving LLMs using StepBackReasoning Zheng et al. (2024). Initially, the model is given a prompt asking it to predict the next tactic in a Lean 4 proof. Without additional reasoning guidance, the model produces an incorrect output, such as rfl. which does not align with the required proof strategy. To address this, we introduce a step-back reasoning mechanism. First, we extract the proof state from the given"}, {"title": "3.2 CONSTRUCTING STEERING VECTORS", "content": "Given dataset of steering pairs and tactics {P1,P2, t1, t2} \u2208 s' and a model M, we apply a forward pass to every M(p1),M(p2) to collect values of the residual stream vector on queries at the last token of the input layer l \u2208 {1, ..., L}. We isolate the internal representation corresponding to the reasoning by computing the difference in residual stream vectors v1,e and v2,\u2113. More formally, we compute the vector ue representing the direction of the steering at layer l:\n$u_l = \\frac{v_e}{||v_e||}$, where $v_e = \\frac{1}{N} \\sum_i (P_{1,l,i} - P_{2,l,i})$\nAveraging our different proof states to capture activation values most closely associated with the structured reasoning step independent of the query. The calculation of the direction of the steering is carried out using the representations in the last token of the input, which effectively encapsulates the behavior of the model not only for the next token prediction task, but also for the entire generation following Todd et al. (2024); Scalena et al. (2024). After identifying steering direction, we compute steering vector by re-scailing unit vector ue by a coefficient c. We use a systematic scaling approach where the value of c is selected to ensure that residual stream activations are assigned to their mean value on inputs that contain the structured reasoning steps. In particular, we compute a new example with residual stream values p' at a given token.\n$c = \\frac{1}{Z} \\sum_{z} p'_e u_e$, where $Z = \\frac{1}{N} \\sum_i  P_{1,i,e}u_e$.\nThe steering vector cue is then added to the corresponding residual stream layer and the forward pass is resumed with the updated residual stream value $x' = x + cu_e$. This procedure is carried out"}, {"title": "4 RESULTS AND ANALYSIS", "content": "We evaluated the technique using Best First Search. It is one of the most popular meth- ods to evaluate the theorem-proving ability of a language model Polu & Sutskever (2020b); Yang et al. (2023b); Azerbayev et al. (2023); Lin et al. (2024). For a given language model M, we keep all unexpanded states si; each time we expand the best state si and use the language model to sample S net tactics ai,1...s for the current state si. Following standard practice Polu & Sutskever (2020b); Yang et al. (2023b); Welleck & Saha (2023); Lin et al. (2024) we assume the state with maximum negative log-probabilities is the \"best\" state. Specifically, we select state si with maximum $\\sum_{j=1}^{i-1} - log p(a_j, s_j)$, where (so, ao), ..., (Si\u22121, Ai\u22121) is proof trajectory before state si and log p(aj, sj) is the average log probability of each generated token. We expand upto N states and we get successful proofs search when we reach any proof state with no goals.\nWe evaluated our technique on MiniF2F benchmark Zheng et al. (2022). Which consists of 244 theorems in lean 4. We use the same evaluation setting as previous works Yang et al. (2023b); Welleck & Saha (2023); Ying et al. (2024a).\nSampling We evaluate two different decoding strategies: sampling and search. Sampling involves drawing multiple proof steps stochastically based on the model's output distribution, promoting diversity in the generated proofs. In contrast, search incorporates structured exploration techniques to improve proof discovery. Specifically, we consider two widely used search methods: beam search and best-first search.\nBeam Search. Beam search maintains a fixed number k of proof trajectories at each step, selecting the top-ranked candidates based on the model's confidence scores. By preserving multiple plausible proof paths instead of greedily committing to the highest-confidence step, beam search mitigates early pruning errors and allows exploration of alternative reasoning chains. However, the trade- off between beam width and computational cost remains a key consideration: while larger beams enhance robustness, they also introduce significant overhead.\nBest-First Search. Best-first search prioritizes proof states according to a heuristic function, typ- ically based on model confidence or learned value estimates. Unlike depth-first or breadth-first strategies, best-first search expands the most promising proof state first, dynamically adjusting the exploration process. In our experiments, we observe that combining best-first search with sampling yields notable improvements. We hypothesize that this effect arises because traditional reranking,"}, {"title": "4.1 MAIN RESULTS", "content": "Our main results are reported in Table 1. Steering the model's activation significantly improves performance over the base model. Notably, we observe that steering markedly increases pass rates when using Best First Search with sampling. We hypothesize that this improvement occurs because reranking may be too narrowly focused-potentially getting trapped in local optima\u2014even though it boosts log probabilities for tactics that follow the highest reward path. In contrast, combining sampling with Best First Search allows for imperfect scoring, which in turn enables the exploration of nodes that lead to better intermediate states."}, {"title": "4.2 ABLATIONS", "content": "A potential validity concern with any intervention involving activation patching is that the observed improvements might not stem from genuine performance enhancements but rather from activating fallback mechanisms McGrath et al. (2023); Lucchetti & Guha (2024). For instance, patching could merely introduce noise into the embedding space, inadvertently triggering alternative pathways that lead to the desired outcome. This phenomenon complicates the interpretability of both patches and steering vectors. To examine this, we conduct an experiment using a randomly generated steering vector (denoted as \"Random\" in Table 2). Our findings show that even random steering achieves a nonzero accuracy, albeit significantly lower than that of our computed steering vectors. We hy- pothesize that this residual accuracy arises due to backup circuits. Nevertheless, the substantially higher performance of our computed steering vectors suggests that our approach induces meaningful transformations toward the correct target."}, {"title": "5 CONCLUSION", "content": "We investigate activation steering for tactic prediction by making language models adhere to struc- tured reasoning approaches in theorem proving. We find that by constructing steering pairs using synthetic metadata and natural proof states, we can construct effective steering vectors that improve tactic selection. Our experiments show that steering vectors enhance model performance beyond random interventions and generalize well across different theorem-proving strategies. The effec- tiveness of our steering approach demonstrates the existence of underlying reasoning pathways that can be systematically influenced within language models. Activation steering proves to be a power- ful technique for improving model performance on formal reasoning tasks where fine-tuning may be impractical or resource-intensive. As language models continue to evolve in their theorem-proving capabilities, activation steering may serve as a lightweight alternative to specialized fine-tuning ap- proaches. Rather than training separate models for different theorem-proving strategies, the same base model could be adapted through steering vectors that guide its reasoning process as needed. This could be particularly valuable for interactive theorem proving environments where computa- tional resources are limited. Our reasoning-based steering vectors, for example, could provide an efficient way to enhance proof assistants, particularly useful for applications like automated tactic suggestion. In future work, we aim to study the underlying mechanisms in language models re- sponsible for structured mathematical reasoning. We further wish to explore how reasoning-focused"}, {"title": "6 FUTURE WORKS", "content": "Our work opens several promising research directions for improving theorem proving with activation steering. From a theoretical perspective, we aim to investigate the geometric properties of steering vectors in the context of formal reasoning, studying how different model layers represent logical structures and how steering vectors interact with these representations. Understanding these proper- ties could lead to more efficient steering methods and deeper insights into how LLMs encode mathe- matical concepts. On the practical side, several extensions could enhance theorem proving systems, including the development of adaptive steering mechanisms that dynamically adjust based on proof state complexity, the investigation of steering vector composition for handling compound mathe- matical concepts, and the integration of steering with existing proof search heuristics to improve exploration efficiency. Scalability remains a challenge, and future work should explore techniques for reducing the computational overhead of steering during inference, methods for distilling steer- ing vectors while maintaining their effectiveness, and approaches for generalizing steering vectors across different mathematical domains. Our findings suggest that activation steering could become a powerful tool for enhancing LLM-based theorem provers, particularly in resource-constrained en- vironments where fine-tuning is impractical. We believe that exploring these directions will lead to more robust and efficient theorem proving systems."}]}