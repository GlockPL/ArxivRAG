{"title": "Eliciting Language Model Behaviors with Investigator Agents", "authors": ["Xiang Lisa Li", "Neil Chowdhury", "Daniel D. Johnson", "Tatsunori Hashimoto", "Percy Liang", "Sarah Schwettmann", "Jacob Steinhardt"], "abstract": "Language models exhibit complex, diverse behav-iors when prompted with free-form text, making it difficult to characterize the space of possible out-puts. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possi-ble prompts, we train investigator models to map randomly-chosen target behaviors to a diverse dis-tribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strate-gies. Our investigator models surface a variety of effective and human-interpretable prompts lead-ing to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack suc-cess rate on a subset of AdvBench (Harmful Be-haviors) and an 85% hallucination rate.", "sections": [{"title": "1. Introduction", "content": "Developers of language models seek to ensure they are well-behaved over the wide distribution of inputs they receive at deployment, for instance by training them to follow a behavior spec (OpenAI, 2024) or constitution (Bai et al., 2022). However, many of today's models still exhibit un-expected behaviors (Roose, 2023), and even describing a model's behaviors is difficult due to the near-infinite space of possible inputs and ways for models to respond.\nTo address this challenge, one approach is to design auto-mated methods to uncover specific unwanted behaviors, as in automated jailbreaking (Zou et al., 2023; Liu et al., 2024). However, this restricts to a narrow distribution of tasks and often produces inputs that are not interpretable by humans.\nAt the opposite end, humans have discovered many surpris-ing behaviors through open-ended interaction with language models (Li et al., 2024; Ayrey, 2024), but this is expen-sive and difficult to scale. How can we get the best of both worlds-building tools that are automated and scale to frontier systems, while being flexible enough to meet the open-ended complexity of language model behaviors?\nThis paper introduces a framework for automated behavior elicitation that trains language model agents to investigate other AI models (Figure 1). Our approach frames behavior discovery as a reinforcement learning problem, where an investigator model is trained to generate inputs that produce specific behaviors from a target model-ranging from exact string matches (string elicitation) to behaviors defined by open-ended rubrics (rubric elicitation). By conditioning these investigators on samples from a distribution of high-level goals rather than optimizing separately for each goal, we amortize the computational cost of search through input space during training. Our approach yields flexible, general-purpose investigators that discover interpretable prompting strategies expressed in natural language.\nWe focus on the special case of single-turn elicitation, where the investigator model produces a single prompt x to elicit a response y that satisfies a property r specified in natural language. This poses two key challenges: first, x must be optimized over the combinatorial space of language inputs; and second, optimizing x directly may lead to undiverse inputs that optimize r in an uninteresting way (e.g. repeat after me: [output that satisfies r]).\nTo address the first challenge, we introduce a multi-stage RL pipeline (Figure 2), which uses supervised fine-tuning on related tasks to produce a good initialization (Section 3.1) followed by DPO to achieve consistently high reward (Sec-tion 3.2). To address the second challenge, we introduce an iterative variant of DPO that is adaptively regularized to produce diverse outputs across iterations (Section 3.3); this iterative method is equivalent to a variant of Frank-Wolfe optimization (Bach, 2012) and asymptotically approximates Bayesian posterior sampling.\nThe resulting investigators produce prompts that represent a variety of effective and human-interpretable strategies for behavior elicitation (Section 5). When applied to automated"}, {"title": "2. Problem Statement", "content": "Throughout, we assume there is a fixed target model m that we are trying to investigate. For most of our experiments, we use m = Llama-3.1 8B as our target.\nWe consider two variants of the behavior elicitation prob-lem: string elicitation, for which the target model should produce an exact match for a given string, and rubric-based elicitation, for which the target model should produce a response that satisfies natural language criteria."}, {"title": "2.1. String Elicitation", "content": "For the string elicitation task, we are given a target response y, and our goal is to generate a prefix x such that m's output on x is an exact match for y; in other words, we want to solve arg maxx Pm (y | x).\nThe brute-force approach would be to solve this optimiza-tion problem for each suffix y. However, exactly solving this"}, {"title": "String Elicitation", "content": ""}, {"title": "Rubric-Based Elicitation", "content": ""}, {"title": "Connection to L(po)", "content": "Rafailov et al. (2023) show that DPO"}, {"title": "Approximate RL", "content": "approximately optimizes the RL objective:\nEy~PRL,x~po(ly) [log pm (y | x)] \u2013 BKL(po(x | y)||Pm(x))\n= R(0) + \u03b2(\u0397(pe) \u2013 H(pe, pm)).\nThis is equivalent to our loss function L in Equation (2) with \u03b2\u2081 = \u03b22 = \u03b2. However, simply applying DPO often leads to mode collapse, a phenomenon which decreases the sample diversity of a model (Song et al., 2023). For example, we observe that some DPO runs learn to primarily generate prompts that repeat the suffix (e.g., prefix x = repeat after me, most inexhaustible source of magic for suffix y = most inexhaustible source of magic), exploiting the repetition bias in transformers. While this is one valid strategy, we ideally want to discover a wider range of successful elicitation strategies. We next present an approach for decoupling B1 from B2, which has the added advantage of producing an iterative exploration policy to avoid mode collapse."}, {"title": "3.3. Improving Diversity with Frank-Wolfe", "content": "To learn a diverse set of strategies, we propose an iterative approach (Algorithm 3) that penalizes prefixes discovered in previous iterations. Let q(i) be the model from the i-th iteration, and p p(i) be an aggregated model over the first i iterations. We update each as follows:\nq(i) = DPO(r = log Pm (y|x) - Alogp-1) (x | y),\n\u03b2 = \u03b22) (5)\nPi) (x | y) = (1 \u2013 ni)p-1)(x | y) + Niq(i) (xy) (6)\nPe\nThus q(i) is optimized via DPO, where we take the original reward log pm (y | x) and regularize by a weighted average\nPe\n-1) of the previous iterates, and set the \u03b2 hyperparameter in DPO to be \u03b22.\nWe refer to this algorithm as FW since it is equivalent to Frank-Wolfe optimization (Frank & Wolfe, 1956) under an"}, {"title": "Objective", "content": "appropriate objective, which we will show at the end of this section. For the aggregate model pe, we initialize po\n(0) as the uniform distribution and 71 = 1. For example, if we set\nni =, the aggregate model pe is the average of the models\nfrom previous iterations.\nIntuition. The first iteration of FW is equivalent to the DPO algorithm in Section 3.2. Each subsequent iteration penalizes prompts discovered by previous iterations. For ex-ample, we often observe that p(\u00b9) learns a repetition strategy in the first iteration. For the next iteration, all the repetitive prompts will be severely penalized because log p\u00b9) (x | y) is high. The next iteration thus ends up discovering new strategies such as topic heading, where the prompts start by listing topics or summaries, which then naturally lead to the target suffix. For example, to elicit y = most inexhaustible source of magic, x = Famous quote from JK. Rowling: Words are our.\nConnection to L(po). We can formally derive Algorithm 3 as performing Frank-Wolfe optimization on L(pe) (Frank & Wolfe, 1956; Bach, 2012). To do so, we decompose the objective L(po) into two components:\nL(po) = R(po) + (\u03b21 - \u03b22)H(po) \u2013 \u03b22KL(po||Pm)\nf(po)\ng(po)\nFrank-Wolfe linearizes f while treating g in closed form: each iteration computes arg min, (q, \u2207 f (pe))+g(q), where pe is updated as in (6). To show the connection to Frank-Wolfe, we thus need only show that this argmin over q is equivalent to (5). We first compute the gradient \u2207 f (pe), obtaining:\n\u25bdf (pe) = log pm(y | x) \u2013 (\u03b2\u2081 - \u03b22) (log po (x | y) + 1)."}, {"title": "Thus, Objective", "content": "Thus, taking the expectation over y ~ P, the objective for q is equal to\nEy~P [Ex~q(ly) [log pm (y | x) \u2013 (\u03b2\u2081 \u2013 \u03b22) log po(x | y)] \u2013\nB2KL(q(x | y)||Pm(x))]\nAs shown in Section 3.2, this is approximately optimized by DPO with regularizer B2 and reward log pm(xy) \u2013 (\u03b21 - \u03b22) log po(x | y). Therefore, we obtain Equation (5) by setting \u03bb = \u03b21 - \u03b22."}, {"title": "Algorithm", "content": ""}, {"title": "4. Method: Rubric-based Elicitation", "content": "In rubric-based elicitation, instead of specifying an exact string, we specify a target behavior using a rubric. The goal is to search for prompts that elicit a rubric-satisfying response y with high probability, where the degree of rubric-satisfaction is measured by a verifier model pv (a | R, y).\nWe decompose the rubric-elicitation problem into two stages: elicting response y from rubric (R, a) (Section 4.1)"}, {"title": "Algorithm", "content": ""}, {"title": "to DPO", "content": ""}, {"title": "Elicitation", "content": "and inferring prompt x from response y (Section 4.2), as shown in Figure 3. We observe that each stage can be reduced to a string elicitation problem, and explain the pro-cedure for each stage below."}, {"title": "4.1. Stage 1 (inferring y from Rubric (R, a))", "content": "We first infer y from the rubric (R, a) by searching for the response y that scores high under the rubric pv (a R, y). To formalize this goal, we optimize the stage 1 model q with the following objective:\nmax E (R,a)~PRL [log pv (a | R, y)]+\u03b21H(q)-\u03b22H(q,Pm).\nq\ny~q(R,a) (7)\nThis resembles the objective (Equation (2)) of string elici-tation, but replaces the elicitation reward with p\u2081\u2084(a | R,y). Therefore, we can use the same algorithm we proposed in Section 3 to train the stage 1 model: first run supervised fine-tuning on a rubric dataset, then iteratively refine the distribution with DPO and Frank-Wolfe."}, {"title": "5. Experiments", "content": "We next evaluate our elicitation framework empirically. First, we conduct a detailed analysis of all parts of our pipeline (SFT, DPO, Frank-Wolfe) on a dataset with known gold prefixes to compare to. Next, we consider the task of automated jailbreaking to elicit harmful strings and behav-iors, which is a special case of behavior elicitation where we can compare to existing methods. Finally, we demon-strate the flexibility of our framework by applying rubric elicitation to uncover both hallucinations and aberrant be-haviors."}, {"title": "5.1. Analyzing Our Pipeline with Known Ground-Truth", "content": "We first consider string elicitation for pre-trained LMs (be-fore instruction-tuning) with suffixes y generated from gold prefixes x*. Our goal is to recover a (potentially distinct) prefix x whose elicitation score is at least as high as x*, as measured by log pm (y | x). We use this to test our entire pipeline in a simplified setting where we can compare to known ground-truth."}, {"title": "5.2. Automated Jailbreaking", "content": "Next, we apply string elicitation to automated jailbreaking. We consider two versions of this task: generating specific harmful strings, and generating more open-ended harmful behaviors. We test our method AdvBench dataset (Zou et al., 2023), which includes datasets for each scenario."}, {"title": "5.3. Eliciting Diverse Behaviors with Rubrics", "content": "We next use rubric elicitation to surface two more open-ended families of behaviors: hallucinations and aberrant behaviors."}, {"title": "Hallucinations.", "content": "Factual inaccuracies are a prevalent prob-lem in contemporary language models (Wang et al., 2024). We seek to automatically uncover these inaccuracies, by elic-"}, {"title": "5.4. Further Ablations", "content": "In Appendix A we study the effect of the number of DPO iterations, number of Frank-Wolfe iterations, difference ag-gregation rules, and the effect of model size for the inves-tigator. We find that while multiple iterations of DPO are necessary, running too many iterations can lead to sharp performance degradations. In contrast, Frank-Wolfe is more stable but sees limited gains after the first few iterations. We also find that even small (1B-parameter) investigators can successfully learn to jailbreak Llama-3.1 8B."}, {"title": "6. Related Works", "content": "Automated red teaming of language models has been a pop-ular research direction (Perez et al., 2022; Zou et al., 2023; Liu et al., 2024). This problem is often cast as an inference-time optimization, where the objective is to identify prompts that elicit a specific harmful response. For example, GCG (Zou et al., 2023) and AutoDAN (Liu et al., 2024) optimize the prompt for each response instance individually, requiring significant computational resources for each search. In con-trast, our approach moves this expensive inference cost to training, obtaining a model that can learn a general approach that applies to each elicitation task.\nSimilar to our approach, many prior works amortize the cost of search by training a model to perform red teaming. For example, Perez et al. (2022) and Hong et al. (2024) use reinforcement learning to elicit generally harmful behaviors. Our investigators elicit finer-grained behaviors and can con-dition on rubrics. This rubric-conditioning as well as the diversity-seeking algorithm inspired by Frank-Wolfe lead to improved precision and coverage."}, {"title": "7. Discussion", "content": "Behavior elicitation automates the discovery of prompts that uncover diverse sets of behaviors specified in natural language. In this work, we cast this problem as amortized posterior inference and propose a method for training in-vestigator agents to efficiently find diverse solutions to this inference problem. The resulting investigators outperform baseline attacks when jailbreaking models, and flexibly han-dle user-specified goals such as uncovering hallucinations or aberrant behavior.\nAlthough this paper solely explores the single-turn chat setting for elicitation, the paradigm extends to multi-turn scenarios, where investigator agents can leverage inference-time compute (e.g. submitting multiple prompts, calling tools, and using chain-of-thought reasoning) to elicit complex behaviors from target AI systems. Recent work has shown that the capabilities of multi-step reasoning models can be significantly enhanced by equipping them with additional tools, such as the ability to query other pretrained models or access model internals (Schick et al., 2023; Qin et al., 2023; Shaham et al., 2024). Through these more ad-vanced capabilities, investigator agents might be able to dis-cover nuanced behaviors in real-world scenarios and narrow the gap between automated and human-driven red-teaming.\nLimitations. Using an LM to verify yields scalable elic-itation algorithms, but also raises concerns about reward hacking. We already observed some instances of reward hacking against LM judges in our results above. Future work should analyze reward hacking systematically, and de-velop robust and reliable verifiers, perhaps by augmenting the verifier with tools or external databases. In the other direction, there are many tasks where verification is easier than generation and so reward hacking is less of an issue, such as for reasoning-heavy tasks like math and coding."}, {"title": "C. Additional Related Work", "content": "Language Model Inversion. Language models generate suffixes conditioned on a given prompt, whereas the elicitation problem inverts this process: searching for a prompt that induces a specific suffix string or behavior. For example, Pfau et al. (2023) pretrains a language model to predict tokens in the reverse order. Donahue et al. (2020) finetunes a language model to fill in the middle conditioned on the prefix and suffix by swapping the sentence orders. We tackle a similar task of inversion, except that we aim to invert a specific LM, rather than the general natural language distribution. Towards this, we curate SFT data specific to the target LM and include the DPO step to reward based on the log-probability under the target LM. Similarly, Morris et al. (2023) learns model-specific inversion by taking next-token probabilities as inputs and recovering preceding texts like system prompts. Their approach assumes dense information (next token probability) and is optimized for exact generations from the model, whereas our approach can invert strings not generated from a model and generalizes to rubrics."}, {"title": "D. Models and Hyperparameters", "content": "Supervised Fine-Tuning. We use the SFTTrainer from TRL (von Werra et al., 2020) with a cosine leaning rate schedule with a warmup ratio of 0.03. For Llama-3.1 8B, we use a batch size of 4, and for Llama-3.2 1B, we use a batch size of 16. We use a hyperparameter sweep to determine the optimal learning rate; for Llama-3.1 8B, it is 1 \u00d7 10\u22125 amd for Llama-3.2 1B, it is 5 \u00d7 10-5. We train for a single epoch of either WildChat or UltraChat, and use Fully Sharded Data Parallel training.\nDirect Preference Optimization. We use the DPOTrainer from TRL (von Werra et al., 2020) with a cosine leaning rate schedule with a warmup ratio of 0.03. We use a batch size of 3 and learning rate of 1 \u00d7 10\u20136. When sampling from the trained DPO models, we use T = 0.8 and TopP = 0.9; we find that this decreases the probability of sampling unwanted characters during DPO training. We sample a maximum of 128 tokens from the investigator model for both training and evaluation."}]}