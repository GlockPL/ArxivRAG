{"title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving", "authors": ["Ran Xin", "Chenguang Xi", "Jie Yang", "Feng Chen", "Hang Wu", "Xia Xiao", "Yifan Sun", "Shen Zheng", "Kai Shen"], "abstract": "Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a score of 71.31 on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.", "sections": [{"title": "1 Introduction", "content": "Automatic theorem proving (ATP) in formal languages has recently become a critical benchmark for evalu-ating the reasoning capabilities of large language models (LLMs). By encoding mathematical problems into formal systems like Lean4, ATP enables the generation of machine-verified proofs for complex mathemat-ical propositions, ensuring logical correctness [10,14]. Despite the remarkable success of LLMs in natural language mathematics and reasoning tasks [7,11], theorem proving in formal languages presents unique chal-lenges [24]. Unlike informal reasoning, formal systems require strict adherence to syntax and semantics, as well as the ability to generate valid steps within a highly constrained formal framework. Additionally, the action (tactic) space in ATP is vast, as each proof state can lead to numerous potential tactics, making the search process for valid proofs computationally intensive [13]."}, {"title": null, "content": "Tree search algorithms are fundamental to ATP, allowing policy models to navigate large and complex proof space efficiently [14]. Among these methods, Monte Carlo Tree Search (MCTS) [4] has gained popularity due to its ability to balance exploration and exploitation using value functions (critic models) or intrinsic rewards [3,16]. MCTS has demonstrated remarkable success in AlphaZero-style frameworks for games like chess and Go [17], where the underlying state spaces are vast but terminal states are well-defined. However, applying MCTS and/or value functions to ATP comes with distinct complications. While games have clear winning and losing conditions, proof search lacks such definitive end states: a proof attempt can theoretically continue indefinitely until a proof or counterexample is found, making it challenging to assess intermediate progress [5, 6]. Additionally, ATP involves a much larger and more dynamic branching factor, sparse and delayed feedback, and an open-ended reasoning process. These differences highlight the unique demands of ATP and advocate the need for specialized adaptations of search methods to address its complexities.\nBest-First Search (BFS) [12] offers a simpler and more lightweight alternative to MCTS by prioritizing expan-sions based on accumulated log probabilities from the current node to the root. While its straightforward nature and computational simplicity make it attractive, BFS is often considered suboptimal for theorem proving in the existing literature [9, 20, 22] due to the following hypothesis:\n\u2022 Lack of exploration: BFS prioritizes high-probability paths, overlooking less likely but valid solutions. Without exploration mechanisms like upper confidence bounds or value functions, it struggles to bal-ance exploiting promising nodes and exploring diverse paths.\n\u2022 Bias against deep reasoning paths: BFS's reliance on cumulative log probabilities intrinsically penalizes longer paths, as deeper expansions tend to accumulate lower scores. This bias makes BFS less effective for theorems requiring deep proofs, where intermediate states may seem unpromising but are essential for finding a solution."}, {"title": "1.1 Our Contributions", "content": "This paper challenges the prevailing perception that BFS is inherently unsuitable for large-scale ATP. We present BFS-Prover system, which transforms BFS into a simple yet powerful algorithm through targeted scaling strategies. Our key contributions are as follows.\n\u2022 Expert iteration with self-filtering. We develop an expert iteration [1] framework that strategically filters out problems solvable by beam search [18] node expansions in each round. This filtering is critical as it directs training data accumulation towards harder theorems. As expert iteration progresses, the policy LLM continuously improves, learning a diverse range of tactics and deeper proofs via BFS.\n\u2022 Direct preference optimization (DPO) from compiler feedback. We use DPO [15] to refine the policy LLM by leveraging preference pairs naturally generated during tree search. From a given proof state, each preference pair consists of a positive tactic, which lies on the proof path, and a negative tactic, which leads to a Lean compiler error. DPO sharpens policy's distribution, enabling it to avoid unproductive tactics and thereby improving the sample efficiency of BFS.\n\u2022 Length normalization for deeper exploration. We incorporate a length-normalized scoring func-tion in BFS to mitigate its inherent bias against deeper reasoning paths. By normalizing log probabil-ities relative to path length, BFS can explore deeper proof paths more effectively, enabling it to solve theorems requiring long tactic chains.\n\u2022 Empirical Validation on MiniF2F. BFS-Prover achieves a score of 70.08 on the MiniF2F test set, demonstrating that BFS can scale to competitive levels in ATP. This result highlights the viability of BFS as a lightweight yet powerful alternative to more complex methods like MCTS and value functions.\nRoadmap. The rest of this paper is organized as follows. Section 2 gives an overview of the BFS-Prover system, detailing the expert iteration framework, data filtering, DPO for policy refinement, and length normalization in BFS. Section 3 describes practical implementation and experimental results on the MiniF2F benchmark against leading prover systems. Section 4 concludes the paper."}, {"title": "2 The BFS-Prover system", "content": "In this section, we detail the BFS-Prover system design; see Fig. 1 for an illustration."}, {"title": "2.1 Lean4 Environment and Policy LLM", "content": "We adopt LeanDojo [25] as an interactive Python interface for Lean4 integration within BFS-Prover. It transforms Lean4 into a gym-like environment [2], facilitating interaction between the policy LLM and the formal proof assistant. Specifically, LeanDojo manages state transitions by executing tactics generated by the policy LLM in the Lean4 compiler. If a tactic cannot be executed, LeanDojo captures and returns the corresponding error messages, providing critical feedback for DPO to refine the policy LLM."}, {"title": "2.2 Length-Normalized Best-First Tree Search", "content": "BFS-Prover incorporates a version of Best-First Tree Search (BFS) to navigate the vast state-tactic space of proof search. The BFS engine maintains a priority queue of proof nodes (states), where the priority of"}, {"title": null, "content": "each node (state) is defined by a length-normalized scoring heuristic:\n$$score(s_L) = \\frac{\\prod_{t=0}^{L-1} log p(a_t | s_t)}{L^\\alpha}$$,\nwhere we have the following:\n\u2022 $s_t$ represents the proof state at step $t$,\n\u2022 $a_t$ is the tactic applied at step $t$,\n\u2022 ${ (s_t, a_t) : t = 0, 1, ..., L \u2212 1 }$ denotes the proof path formed by applied tactics and state transitions,\n\u2022 $p(a_t|s_t)$ is the model's predicted probability of applying tactic $a_t$ at state $s_t$,\n\u2022 $L$ is the total length of the path from the root to the current state $s_L$, and\n\u2022 $\u03b1\u2208 [0, 1]$ is a tunable length normalization parameter.\nThis scoring mechanism, combined with a tunable node expansion width, enables BFS to dynamically allocate computational resources across the proof space, balancing the trade-off between exploration and exploitation. For example, increasing \u03b1 and/or reducing the expansion width drives the search system toward exploring deeper paths, encouraging the discovery of complex proofs that may require a long chain of tactics.\nAt each node expansion step, the policy LLM generates, via a certain sampling mechanism, a list of tactics which correspond to edges in the proof tree. LeanDojo then executes these sampled tactics in Lean4 compiler and returns their outcomes. For each tactic application, three outcomes are possible: (1) if the tactic results in a valid proof state, a regular tree node is created and added to the node queue; (2) if the tactic completes the proof, a proof finish node is created and the proof is returned; (3) otherwise, a terminal error node is generated to represent an invalid path."}, {"title": "2.3 Expert Iteration", "content": "BFS-Prover employs an expert iteration pipeline to iteratively enhance the policy LLM's ability to navigate complex proof spaces. Given a corpus of unsolved formal statements in Lean4, each round of expert iteration consists of the following steps.\n1. Beam Search Filtering: Formal statements that can be proved through BFS with beam search node expansions are identified. These statements are then removed from the corpus and their corresponding proof data, although new, is deliberately not added to the cumulative training dataset. Beam search, being deterministic, reliably selects the most confident tactics produced by the current policy LLM. Therefore, proofs solvable under this approach are considered relatively simple, as they align closely with the strengths of the current BFS-Prover system. By strategically filtering out these simpler proofs, the training data corpus is iteratively enriched with more challenging and diverse examples. This itera-tive refinement ensures that the policy LLM is progressively exposed to increasingly complex reasoning patterns, thereby enhancing its capability to address harder theorems in subsequent iterations.\n2. Data Collection: We then perform BFS with temperature-based sampling expansions to search for proofs of the remaining unproved formal statements in the corpus. Upon completion, the system collects all valid (proof state, tactic) pairs encountered along successful proof paths, which are subsequently added to the cumulative training dataset. The corresponding proved statements are then removed from the corpus. Additionally, invalid tactics that result in Lean compiler errors are recorded as on-policy negative examples to support DPO refinement.\n3. Supervised Fine-Tuning (SFT): After each data collection phase, a new policy LLM is trained by performing SFT on a base model using the full accumulated training data corpus, which consists of all (proof state, tactic) pairs generated during previous expert iterations."}, {"title": null, "content": "4. Direct Preference Optimization (DPO): Instead of re-training the policy LLM using SFT, DPO can be applied as an alternative method to refine the current policy model by leveraging the on-policy Lean error data collected in step 2. Along the proof path, certain generated tactics, expanded from a proof state but not part of the verified proof path, result in Lean compiler errors. These erroneous tactics naturally serve as negative examples when compared to their corresponding valid proof tactics, forming preference tuples. Specifically, for a state s on a proof path, we may form pairs (aw, ar) where aw is the tactic on the proof path and ar is an error-causing tactic expanded from s if exists; see Fig. 1 for an illustration. The DPO loss is then defined as:\n$$L_{DPO}(\\theta) = -E_{(s, a_w, a_r)} [log \\sigma(\\beta(r_\\theta(s, a_w) \u2013 r_\\theta(s, a_r)))] $$,\nwhere $r_\\theta(s, a) = log(p_\\theta(a|s)) \u2013 log(p_{ref}(a|s))$ is the log probability ratio between the policy model $p_\\theta$ and reference model $p_{ref}$, $\u03b2$ is a KL regularization parameter controlling the sharpness of the learned preferences, and $\u03c3$ is the sigmoid function. By explicitly incorporating these negative signals that are absent in SFT, DPO sharpens the LLM's output distribution, thereby improving the sample efficiency and scalability of BFS within the expert iteration framework.\nThis expert iteration pipeline enables the policy LLM to continually enhance its ability to generate productive tactics while adapting to progressively more challenging proof scenarios as the training data corpus grows."}, {"title": "3 Practical Implementation and Benchmark Results", "content": "In this section, we discuss practical implementation details of the BFS-Prover system and present its bench-mark results on MiniF2F test."}, {"title": "3.1 Model, Data, and Training Setup", "content": "Base model and initial training data. For illustration purposes, we use Qwen2.5-Math-7B [23] as the base model for fine-tuning the policy LLM in BFS-Prover. In order to initialize the expert iteration process, we leverage proof data extracted by LeanDojo [25] from Mathlib [10], which serves as the cold-start dataset. As expert iteration progresses, we further incorporate datasets from Lean-Github [21], a compilation of Lean4 repositories from GitHub, and Lean-Workbook [26], which focuses on Olympia-level algebra and analysis. These datasets span a diverse range of mathematical topics and formal reasoning tasks, equipping the policy model with the foundational skills necessary for effective tactic generation and proof navigation.\nFormal statement corpus. To construct our expert iteration data corpus, we autoformalize the NuminaMath-CoT dataset [8] using proprietary tools. We augment this with unproven theorems from Mathlib and formal statements from Lean-Workbook. The resulting corpus comprises approximately 900,000 formal mathemat-ical statements without proofs, providing a comprehensive foundation for expert iteration.\nTraining setup. At each expert iteration round, we obtain a new policy LLM through either SFT or DPO. For SFT, we conduct training over 3 epochs using cosine learning rate decay, initializing at 2 \u00d7 10-5 and decaying to 1 \u00d7 10-6, with a global batch size of 128. For DPO, we perform a single epoch of training with cosine learning rate decay, beginning at 5 \u00d7 10-6 and decaying to 5 \u00d7 10\u22127, utilizing a global batch size of 16, while maintaining a KL regularization parameter of \u1e9e = 10. All training is performed on A100 80GB GPUs.\nThe selection between SFT and DPO depends on the volume of data generated in each iteration: SFT is implemented when an iteration yields substantial new data, whereas DPO is preferred for iterations with limited data generation, leveraging its sample efficiency through the incorporation of negative examples.\nBFS configuration in expert iteration. We set the length normalization parameter \u03b1 to 0.0 throughout. During the beam search filtering stage, we utilize a beam width of 32 to identify easily solvable theorems. In the subsequent data collection phase, we employ temperature-based sampling with a temperature of 1.0, nucleus sampling parameter 1.0, and a sampling width of either 4 or 8 to explore diverse proof paths."}, {"title": "3.2 Distributed Best-First Search Infrastructure", "content": "To enable efficient large-scale proof search, we implement a distributed system using Ray for parallel theorem proving across multiple machines, each equipped with 8 A100 80GB GPUs and 128 CPU cores. The target theorems are evenly divided across machines, with each machine running an independent proving pipeline. The system is composed of three main components:\n\u2022 GPU-Based Policy LLM Pool: Each local machine deploys 8 instances of our 7B policy LLM, each powered by an asynchronous vLLM engine running on a dedicated A100 GPU. These instances form a shared pool that processes concurrent tactic generation requests.\n\u2022 CPU-Based Prover Pool: Each local machine runs 96 concurrent prover instances, reserving re-maining CPU cores for system operations. Each prover instance executes independent best-first search runs for its assigned theorems. In order to achieve balanced GPU utilization, provers distribute their requests across the policy LLM instances in a round-robin fashion based on their index modulo 8. Each prover asynchronously interacts with its assigned policy LLMs and LeanDojo environments.\n\u2022 Asynchronous Orchestration: The overall system leverages asyncio to manage the high-concurrency workflow between provers and policy LLMs. Both the policy LLM and prover pools are implemented as Ray actors, enabling dynamic resource management through Ray's runtime system. To ensure sys-tem responsiveness, we enforce timeout thresholds for both tactic execution (via LeanDojo) and model inference (via vLLM).\nThis design achieves near-linear scaling through efficient theorem distribution across machines while maxi-mizing hardware utilization within each machine without cross-machine communication overhead."}, {"title": "3.3 Distribution Shift in Expert Iteration", "content": "Proof level. A critical indicator of the effectiveness of BFS-Prover is its ability to discover deep proofs. We define proof length as the number of tactics found by our system to complete a proof. We observe that the distribution of proof length at each expert iteration round often exhibits a Gaussian or mixture of Gaussians, reflecting the diversity of theorem complexities within the formal statement corpus. Intriguingly, as expert iteration progresses, the mean proof length tends to increase, indicating that BFS can find increasingly deeper and more challenging proofs as the policy LLM improves; see Fig. 2 for an illustration. This observation highlights the effectiveness of the expert iteration framework and the scalability of BFS.\nTactic level. Beyond proof-level evolution, we also observe interesting distributional shifts at the tactic level throughout expert iteration; see Fig. 3. Notably, the BFS-Prover system maintains a diverse range of tactic lengths without collapsing to a narrow distribution - a common failure mode in reinforcement learning where models tend to converge to a limited set of high-reward actions [19]. Instead, we observe a moderate but meaningful shift from very simple tactics (1-10 tokens) towards more commonly used tactical patterns (11-50 tokens). This shift suggests that through expert iteration, BFS-Prover learns to generate more sophisticated tactics while maintaining the capability to use simpler ones when appropriate. The preservation of tactic diversity is crucial for effective theorem proving, as different proof states require different levels of tactical complexity, from simple term rewriting to complex algebraic manipulations."}, {"title": "3.4 Results on MiniF2F", "content": "In this section, we present the performance of BFS-Prover on the MiniF2F test benchmark [27], which is a widely recognized dataset for evaluating formal mathematics systems. MiniF2F consists of a diverse collection of formalized high-school and competition-level mathematics problems. The policy LLM checkpoint for tactic generation used for evaluation was obtained by performing SFT on all state-tactic pairs accumulated through the expert iteration pipeline of BFS-Prover, followed by an additional round of DPO refinement using Lean error signals as described in Section 2.3."}, {"title": "3.4.1 Comparison with the State of the Art", "content": "Fixed tactic generation budget. We compare BFS-Prover developed in this work against leading theorem provers in the literature, including DeepSeek-Prover-V1.5 [22], InternLM2.5-StepProver [20], and Hunyuan-Prover [9]. For fair comparison, we first evaluate under a fixed tactic generation budget, defined as K\u00d7W\u00d7N, where K is the number of passes, W is the expansion width, and N is the maximum number of proof state expansions per pass. During BFS proof search, BFS-Prover uses a sampling temperature of 1.1, expansion width of 2, and length normalization factor of \u03b1 = 0.7 defined in (1) to maximize the diversity of explored proof paths across passes. As shown in Table 1, BFS-Prover achieves state-of-the-art performance without requiring either a critic model (value function) or MCTS-based search. We note that the incorporation of a critic model effectively doubles the number of inference calls since each state expansion requires both policy and value computations.\nAccumulative results. To further explore the proof search space, we tune BFS-Prover with multiple hyperparameter configurations and take the union of successful proofs. Specifically, we vary the BFS length normalization factor \u2208 {0.7,0.9}, sampling temperature \u2208 {0.9,1.1}, and expansion width \u2208 {2,4}. This comprehensive search strategy enables BFS-Prover to achieve 70.04 score on the MiniF2F benchmark, es-tablishing a new state-of-the-art result."}, {"title": "3.4.2 Scaling Law of BFS and Advantage of DPO", "content": "In this subsection, we study how the performance of BFS-Prover scales with the number of proof search passes, as well as the role of DPO in enhancing the system. We perform a total of pass@2048 and evaluate intermediate performance at pass@64, pass@128, pass@256, and pass@1024, where the confidence band for each intermediate pass count is computed by sampling multiple pass@64 runs. The experimental results are presented in Fig. 4, where x-axis is in the log scale. In the following, we observe several key characteristics of BFS scaling in formal theorem proving.\n\u2022 Both SFT and SFT+DPO training methods demonstrate logarithmic scaling with increasing passes. Specifically, SFT improves from 62.60% to 67.21% and SFT+DPO from 63.23% to 68.85% when scaling passes from 64 to 2048, showing consistent but diminishing returns with doubled computational budget.\n\u2022 The DPO-enhanced model consistently achieves a performance advantage over the SFT baseline, with the gap growing from 0.63% at 64 passes to 1.64% at 2048 passes. This widening gap suggests that the benefits of DPO become more pronounced with increased computational budget."}, {"title": null, "content": "\u2022 Examining the min-max ranges, both methods exhibit similar variance in performance (around 2-3% range). This suggests that while DPO improves overall performance, it maintains comparable stability in proof search to the SFT baseline."}, {"title": "4 Conclusion", "content": "This work shows that Best-First Search (BFS) can scale efficiently and achieve state-of-the-art performance in automatic theorem proving. Our results challenge the conventional belief that more complex search methods like MCTS and/or value function are necessary for large-scale formal theorem proving with LLMs. Through the development of BFS-Prover, we argue that a carefully designed BFS system with expert iteration which incorporates strategic data filtering, direct preference optimization, and length normalization can exceed the performance of existing approaches while maintaining computational simplicity. Our empirical results on the MiniF2F benchmark, achieving a score of 71.31, validate the scalability of this approach."}]}