{"title": "Chasing Random \u2013 Investigating the \"Gains\" achieved through Instruction Selection Strategies at Scale", "authors": ["Harshita Diddee", "Daphne Ippolito"], "abstract": "Prior work (Zhou et al., 2023a) has shown that language models can be tuned to follow user instructions using only a small set of high-quality instructions. This has accelerated the development of methods that filter a large, noisy instruction-tuning datasets down to high-quality subset which works just as well. However, typically, the performance of these methods is not demonstrated across a uniform experimental setup and thus their generalization capabilities are not well established. In this work, we analyze popular selection strategies across different source datasets, selection budgets and evaluation benchmarks: Our results indicate that selection strategies generalize poorly, often failing to consistently outperform even random baselines. We also analyze the cost-performance trade-offs of using data selection. Our findings reveal that data selection can often exceed the cost of fine-tuning on the full dataset, yielding only marginal\u2014and sometimes no gains compared to tuning on the full dataset or a random subset.", "sections": [{"title": "Introduction", "content": "Instruction fine-tuning is often considered a crucial step in training large language models, LLMs, to effectively meet the needs of users. By training LLMs over tens of thousands instruction-response tuples that highlight user preferences, models can demonstrate instruction-following capabilities which position them as useful tools for a wide variety of tasks. There has been a rapid increase in the development of instruction selection strategies (Qin et al., 2024b; Wang et al., 2024) to curate a subset of high-quality instructions to train competitive instruction following models more efficiently.\n\nThe experimental setups of general-purpose instruction data selection can be very varied (Qin et al., 2024b); Unlike task-specific data selection, they are not geared towards optimizing performance for some specific goal (Xie et al., 2023a)."}, {"title": "Literature Review", "content": "We focus on general-purpose instruction selection methods, which aim to equip models to follow user queries that aren't specific to a fixed task, capability or domain (Wang et al., 2024).Such methods involve strategies including rule-based metrics (Cao et al., 2023), length (Zhao et al., 2024), diversity (Liu et al., 2023) and model derived uncertainity measurements (Li et al., 2023a) to subsample large instruction tuning datasets. They sit in contrast to task-specific data selection strategies which optimize for performance on a known test distribution or task specification (Xia et al., 2024; Xie et al., 2023b; Pan et al., 2024).\n\nDue to broad definition of instructing following, experimental setups for such work can show significant variation: While some adopt source distributions of varying origins (Zhou et al., 2023a; Li et al., 2024; Shen, 2024), some even synthetically augment subsets of data during their selection process (Liu et al., 2023). Similarly, the selection budget applied on these datasets can vary from anywhere between a mere 200 samples (Wei et al., 2023) to 15K (Du et al., 2023; Xia et al., 2024). \nA few methods also (Mekala et al., 2024; Xia et al., 2024) acknowledge and accordingly attempt to address the relatively high cost of selection by either exploring the use of cheaper proxies like smaller models, low-rank approximations or adopting sequential pipelines (Ge et al., 2024) to make instruction data selection more efficient. Finally, work like (Liu et al., 2024; Wang et al., 2024) have highlighted the concerns in comparing between the performance of instruction selection strategies. Through a unified comparison based on efficiency and feasibility, Liu et al. (2024) provide strong evidence that the comparison between instruction selection strategies needs to be more holistic. Distinctive from this work though, their evaluation does not focus on a comparison including random baselines."}, {"title": "Experimental Setup", "content": "In this section, we briefly describe our source datasets, the selection strategies we study and our evaluation setup."}, {"title": "Source Datasets and Evaluation Setups", "content": "Table 1 provides a concise description of all our datasets and evaluation benchmarks. For FLAN, as a precaution against including disproportionately high representations towards tasks that are overly-represented in the original composition, we curate a smaller subset of FLAN, by limiting the datapoints sampled per task to 50 for our evaluation. The resulting dataset contains 88K examples and we refer to this version as FLAN. For ALPACAEVAL,"}, {"title": "Selection Strategies", "content": "Alpagasus ($\\mathcal{S}_{alpagasus}$) Chen et al. (2023) use GPT-3.5 as scorer (between 1-5) to score samples from ALPACA and include the highest scoring samples.\n\nLongest ($\\mathcal{S}_{longest}$) Zhao et al. (2024) include the instructions with the longest responses.\n\nCherry ($\\mathcal{S}_{cherry}$) Li et al. (2023a) use a sequential approach of selecting instructions: they apply k-means clustering to the last hidden state embeddings of all instruction in a source dataset to get a set of 1000 instructions (100 clusters and 10 samples per cluster). Then, they use this subset of instructions to finetune a model, referred to as the pre-experienced model. Finally, this model scores each sample with an Instruction Difficulty or IFD and the highest scoring samples are included in the selected subset.\n\nDEITA ($\\mathcal{S}_{deita}$) Liu et al. (2023) train a scorer akin to Alpagasus to first score the entire dataset cheaply and then, rather than choosing the entire budget of instructions in one shot - iteratively construct the selected subset by checking the similarity of a candidate instruction to the current pool of instructions.\n\nUniform Random ($\\mathcal{S}_{random}$) This is the naivest form of sampling and acts as our baseline. We report numbers with error bars for trials across 3 such random seeds. We also resample for any random subset that ends up having more than 30% overlap with the data sampled with any strategy for all datasets except dolly (due to Dolly's limited size, a maximum overlap of about 50% is possible only for the highest budget 10000).\n\nStrict Random ($\\mathcal{S}_{strictrandom}$) We also create a special variant of our random-baselines called the \"strictrandom\" baselines which is created by sampling from the dataset after removing all the target instructions that have been deemed high-quality by any of the selection strategies. In practice, the strictrandom baselines can also be considered as sampling data from the complement set of all strategies' \"high-quality\" subsets of budget 10000.\n\nFull Dataset: The entire dataset is used to train the model. Note that we include this variant without tuning optimally for each dataset and include this only to compare the gains that can be naively procured by avoiding selection altogether."}, {"title": "Base Model", "content": "We use the LLaMa-7B (Touvron et al., 2023) for all our experiments.This model's choice is dictated by it's use as a common choice for demonstrating and ablating the performance of the instruction selection strategies that we study (Table 2 (Qin et al., 2024b)). We provide all details of the 3 hyperparameter sets we test in the Appendix A.1)."}, {"title": "Results", "content": "In this section, we present evidence supporting our conclusions on the brittle generalization of instruction selection strategies (\u00a74.1 and \u00a74.2) as well as the negative utility of expending cost on data selection \u00a74.3."}, {"title": "Most Strategies Fail to Beat Random Sampling Consistently", "content": "In the space of instruction data selection, it is very common to show that $M_{selected}$ outperform $M_{full-dataset}$ by over 50% (i.e., an LLM judge prefers the outputs of the $M_{selected}$ more than the $M_{full-dataset}$). We modify this experimental setup to perform these comparisons between the $M_{random}$ and $M_{selected}$ on ALPACAEVAL. Specifically, for each model in $M_{selected}$, we pair the output of the $M_{selected}$ with a randomly chosen inference generated by a random baseline from the $M_{random}$ trained for the same budget and dataset. We then compute the Mean-Adjusted Win-Rate by taking the signed difference between the win-rate of the $M_{selected}$ from 50%. Our results across two budgets are summarized in Figure 2.\n\nFindings on ALPACAEVAL No strategy except $\\mathcal{S}_{deita}$, consistently dominates over the $M_{random}$ across all experimental configurations. To illustrate the practical implications of this observation, consider an NLP practitioner who intends to apply data selection on the DOLLY dataset with a budget of 10,000 samples. They evaluate the performance of various selection strategies on DOLLY at a smaller budget of 5,000 samples and conclude that $\\mathcal{S}_{cherry}$ is the most effective strategy (Figure 2). However, when this strategy is applied and empirically tested at the intended budget of 10,000 samples, the results are the opposite: $\\mathcal{S}_{cherry}$ delivers the lowest performance among all strategies (Figure 2). While we give an example with $\\mathcal{S}_{cherry}$, it is reasonable to assume that other strategies experience similar inflection points in their performance with the change in budget. For example, even though $\\mathcal{S}_{deita}$ consistently outperforms random in this evaluation, it loses nearly 15% of its dominance over $M_{random}$ at budget 10000 (when scaled from 5000) indicating the potential for an inflection point in performance for some larger budget."}, {"title": "Findings with OPENLLM", "content": "To corroborate this trend, we evaluate $M_{selected}$ with $M_{random}$ on OPENLLM. In Figure 3, we demonstrate the performance of $M_{selected}$ across different budgets on both (a)"}, {"title": "Measuring Instruction Following for $M_{selected}$ produces contradictory trends", "content": "Measuring instruction following capabilities is generally more complex than task-specific accuracy evaluation as instruction following models are expected to demonstrate a wide range of capabilities (Lou et al., 2024). Consequently, the subjectivity in the coverage of topics and the performance ranges of each instruction following benchmark can further influence our estimates of a selection strategy's performance. Recently, an emerging class of benchmarks recommend evaluating models with instructions which have more objective requirements (Qin et al., 2024a; Zhou et al., 2023b). Accordingly, we conduct an evaluation of $M_{selected}$ on another popular instruction following benchmark that complies with this format, IFEVAL (Zhou et al., 2023b). IFEVAL defines its own metrics, prompt-level and instruction-level accuracy, to measure how well a model response covers all the requirements delineated by each prompts and ultimately the test instruction. As in our previous evaluation with AlpacaEval and OpenLLM, we compare the performance of $M_{selected}$ and $M_{random}$ on this benchmark."}, {"title": "Findings from IFEVAL", "content": "We include complete results on IFEVAL in the Appendix (Figure 10), where we observe similarly competitive performance from $M_{random}$; Here, we highlight another interesting observation derived through this benchmark: In Figure 4, we show the correlation between the Win-Rates for $M_{selected}$ and their IFEVAL accuracy 4. The performance trends on both benchmarks appear very weakly correlated for our lowest budget, and show almost negative correlation after scaling $M_{selected}$ to the larget budget. This is particularly concerning as both benchmarks are widely used as indicators of instruction following capabilities and hence, at least by definition it is hard to pick the conclusions of one over the other. The practical implication of this correlation is observed when these setups disagree on what"}, {"title": "Findings from LLMBAR", "content": "In Figure 5, we observe that both $M_{selected}$ and $M_{random}$ perform poorly on this benchmark. Interestingly, unlike all other benchmarks we study where $M_{full-dataset}$ are either comparable in performance or even underperform $M_{selected}$, on LLMBAR we clearly see consistent performance improvement when the model is trained on the entire data. This result, hence, sits in complete contrast to all other benchmark evaluations as it exposes another facet of evaluation where selection is not advantageous at all."}, {"title": "Cost of Instruction Data Selection is Non-Trivial when compared to the cost of Tuning on the Entire Data", "content": "A strong motivation for designing instruction selection strategies, and more broadly, data selection strategies draws from the need to train competitive models efficiently, both in terms of time and resource consumption. While the advantages towards this goal are more explicitly observed when source datasets are very large (pretraining datasets of the order of billions of tokens), instruction tuning datasets are typically much smaller in magnitude and thus the efficiency gains of selection can be less obvious to gauge. Accordingly, we evaluate if the proposed selection strategies consistently provide this intended benefit by comparing the effective cost of selection against the performance of $M_{selected}$ and $M_{full-dataset}$.\n\nSetup We compute the Cost of Selection as a product of the per-hour cost to user for renting a fixed compute infrastructure and the wall clock run time for running the selection for that strategy end-end. \u00a7B describes the full details of this computation including the wall clock time of running each selection (Table 6), while the total cost to user in summarized in Table 2. In Figure 6, we plot the cost of selection per dataset compared to the performances of $M_{selected}$ on IFEVAL (all budgets are included in \u00a7B.3 in the Appendix).\n\nFinding The effective cost of selecting data can often overshoot the cost of finetuning $M_{full-dataset}$ in some cases and the gains achieved through selection are marginal in comparison to the additional cost expended at carrying out the selection. While one potential cause of this could be the lack of more aggressive strategy-specific hyperparameter tuning, that is impractical for multiple reasons; For one, hyperparameter tuning in this space involves tuning for strategy dependent parameters such as the similarity threshold, A in $\\mathcal{S}_{deita}$, the number of pre-experienced samples in $\\mathcal{S}_{cherry}$, etc. in addition to traditional model training parameters like learning rate, scheduler and batch size. Jointly optimizing for both these class of hyperparameters can significantly bloat the set of combinations to explore for hyperparameter optimization thus significantly increasing the cost of tuning. Secondly, under a practical setup where an NLP pracitioner expects to choose the best selection strategy amongst several candidate strategies, a hyperparameter sweep for each candidate strategy would mandate tuning all the strategies being examined. From 2, this would imply summing the cost estimates across any row. We can clearly see that such an estimate would quickly overshoot the cost of full finetuning for any strategy.\n\nOne interesting and consistent observation from this cost-benefit analysis is the surprising performance gain shown by $M_{selected}$ over $M_{full-dataset}$. Both, $M_{selected}$ and $M_{random}$ often beat the $M_{full-dataset}$ across several experimental configurations. While some of these gains may be attributed to the lack of hyperparameter tuning for $M_{full-dataset}$, supporting evidence from literature in the space of instruction data selection ((Qin et al., 2024b; Zhou et al., 2023a; Zhao et al., 2024; Ge et al., 2024) does imply that training on selected data can be beneficial (even though not necessarily cost effective). Empirically, this is also visible from the performance of our $\\mathcal{S}_{strictrandom}$ baselines: through the majority of our evaluation, the $\\mathcal{S}_{strictrandom}$ baselines underperform all other strategies indicating that systematically excluding datapoints that are selected by selection strategies definitely harms performance."}, {"title": "Discussion and Conclusion", "content": "This work demonstrates that selection strategies are not consistently competitive across setups and this puts them at a risk of falling short of even random sampling under a wider range of instruction tuning datasets, selection budgets and benchmarks. We also highlight that selection cost often surpasses the cost of full fine-tuning, without consistently delivering proportional benefits."}, {"title": "Random Baselines offer consistency, reasonable and cost-effective performance", "content": "Our conclusions on the performance of random baselines in this setting can be considered aligned to contemporary work demonstrating the unreasonable effectiveness of random baselines in several other domains; Yauney and Mimno (2024) discuss the significant competence of maximum expectancy random baselines in in-context learning by highlighting how standard random baselines may be severely underestimated on validation sets that are smaller in size. Similarly, Lu et al. (2023) find that random baselines for prompt optimization can prove to be effective separators for prompt-style classification even challenging the assumptions that mandate task relevance and human readability in such tasks. Accordingly, our construction of random baselines must improve at scale to get a realistic calibration of the performance of our proposed methods."}, {"title": "Instruction selection performance claims do not stand agnostic to the adopted experimental configurations", "content": "This dependence significantly harms their ease of adoption. Conversely, proposed instruction selection strategies may be more usable to NLP practioners if the efficacy of methods are tested across a wider range of experimental parameters (more budgets, datasets of differing distributions, etc.)."}, {"title": "The Limits of Selective Training in General-Purpose Instruction Following", "content": "General purpose instruction following is an unbounded recall problem as it can involve a fairly vast set of capabilities depending upon the context. There isn't a clear consensus on what are the sufficient conditions for claiming competence in general purpose instruction following: Models trained on selected data may show performance improvement against few limited facets but degrade it on unseen ones. Even using automatic metrics that act as proxies for human judgement seems unreliable as these metrics are also fallible (Zheng et al., 2024) and susceptible to bias (Panickssery et al., 2024). Finally, since instruction following has evolving expectations, standardizing the choice of evaluation through human corroboration may only provide a stopgap solution (van der Meer et al., 2024; Shen et al., 2023). As the complexity of such evaluation can be simplified for known test distributions, selection design effort may be more reliable and consistent in such fields."}, {"title": "Limitations", "content": "Since our work's goal is study the competency of models on a highly subjective goal, general purpose instruction following, conducting a comprehensive human evaluation to support our conclusions was not feasible. Our work also does not address other attributes that selection strategies differ by: including but not limited to the use of different base models, their impact on tuning strategies (like preference optimization) and alignment objectives."}, {"title": "Ethics Statement", "content": "This work highlights the potential of availing negative utility in the field of instruction data selection. Through the evidence in this work, we encourage a more conscious allocation of compute and dollar cost to reduce unnecessary computational overheads. Our code base and training logs (to validate wall clock times) will be released under the MIT License."}, {"title": "Appendix", "content": null}, {"title": "Hyperparameter Configurations", "content": "We do our evaluations across 3 setups, trying to maximize the coverage of training setups that have been adopted by the strategies we reproduce. Additionally, we carried out one evaluation with LORA (Hu et al., 2021) to test if some weak correlation about the performance trends of selection strategies could be gleaned from low-rank finetuned models. The results for that evaluation are shown in \u00a77. We present results from the hyperparameter configurations that matches the MMLU performance of reported for each strategy. The standard deviation with reported numbers along with confidence values for our hyperparameter runs across MMLU are provided in Table 4. Since the work we study did not report IFEVAL, LLMBAR or ALPACAEVAL length-controlled win-rates we were only able to utilize MMLU numbers (reported by all) as our sanity check for replication.\n\nTo replicate $\\mathcal{S}_{cherry}$, we used the code opensourced by the authors on Github, making minor adaptations to add support for new datasets. Following the default setup advised in (Li et al., 2023a), we train our pre-experienced model for 1000 samples using the training configurations specified by the authors. For $\\mathcal{S}_{deita}$ also, we adopt the code opensourced by the authors on Github. We use the Mistral-7B-v0.1 for embedding generation, along with the quality scorer. For our similarity metrics, we used the same distance metric: cosine but different thresholds as keeping the default threshold led to an underflow for few of the models."}, {"title": "Detailed Cost Estimation Across Data Selection Budgets", "content": "All our estimates are provided assuming the following infrastructure: 8 A6000s, 128 CPUs provided by Google Cloud Estimator. The Dollar Cost of renting our infrastructure per hour is about 8 USD. A detailed breakdown of the costs associated with each step of the selection is provided in Table 6. Note that the cost of selection doesn't vary significantly with the change in the selection budget as the entire dataset needs to be sorted in accordance with the strategy guided metric, irrespective of the final budget."}, {"title": "Estimating Performance Using Cost-Effective Proxies", "content": "While it is not possible to largely modify the cost of a selection strategy, it might be possible to offset the cost of finetuning the models on subsets generated via different selection strategies through parameter efficient techniques. If such trends are correlated with the performance of the selected data on the full variant of the model, NLP practioners can potentially design a set of relatively low-cost experiments to rapidly identify the optimal selection strategy to further carry out their selection. Recent work like (Xia et al., 2024), even leverage such correlation to achieve great efficiency in task-specific instruction selection. For preliminary experimentation, we rerun all our experiments with the modification of including LORA modules in our finetuning. This reduces the memory footprint of training by to only 0.0038 times of the memory footprint of full finetuning along with faster training by half of its full-finetuning counterpart. In 7 we plot the correlation between the instruct-level-accuracy on IFEVAL for models trained with and without LORA. However, we don't find any reasonable correlation between these performances highlighlting a need to identify cost-effective methods of predicting the suitability of a custom budget and source distribution to a given selection strategy."}, {"title": "Benchmark Evaluations for All Configurations", "content": null}, {"title": "Correlation between all Benchmarks", "content": null}, {"title": "Cost Versus Performance Trade-Offs for All Budgets", "content": null}]}