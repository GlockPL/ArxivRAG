{"title": "Unveiling Language Skills under Circuits", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "abstract": "The exploration of language skills in language models (LMs) has always been one of the central goals in mechanistic interpretability. However, existing circuit analyses often fall short in representing the full functional scope of these models, primarily due to the exclusion of Feed-Forward layers. Additionally, isolating the effect of a single language skill from a text, which inherently involves multiple entangled skills, poses a significant challenge. To address these gaps, we introduce a novel concept, Memory Circuit, a minimum unit that fully and independently manipulates the memory-reading functionality of a language model, and disentangle the transformer model precisely into a circuit graph which is an ensemble of paths connecting different memory circuits. Based on this disentanglement, we identify salient circuit paths, named as skill paths, responsible for three crucial language skills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning (ICL) Skill, leveraging causal effect estimation through interventions and counterfactuals. Our experiments on various datasets confirm the correspondence between our identified skill paths and language skills, and validate three longstanding hypotheses: 1) Language skills are identifiable through circuit dissection; 2) Simple language skills reside in shallow layers, whereas complex language skills are found in deeper layers; 3) Complex language skills are formed on top of simpler language skills. Our codes are available at: https://github.com/Zodiark-ch/Language-Skill-of-LLMS.", "sections": [{"title": "Introduction", "content": "Circuit analysis [10, 7] presents a milestone in the mechanistic interpretability of transformer language models. It focuses on interpreting the internal mechanisms and pathways within the complex network by breaking down the model's structure into independent and modular circuits, making it possible to trace how certain behaviors (e.g., linguistic patterns) arise from different circuits. However, existing research poses two well-known challenges.\nFirstly, the existing circuit model, as a rough approximation of the original transformer model, is incomplete and does not fully capture the exact behavior of the transformer model. These studies often exclude feed-forward layers (i.e., MLPs), which turn out to be crucial [11], from the mathematical models of circuits [10, 16, 28]. Removing MLPs when extracting circuits enables easy extrapolation"}, {"title": "Realted Work", "content": "How LLMs learn language skills has always been a mysterious and intriguing research topic [20, 12, 8, 13, 21, 5, 6]. To date, various efforts have been made to discover language skills based on the foundational techniques of transformer circuits and causal effects [29, 19, 7, 17]. We categorize these methods into two types: The first category includes those that depend on specific tasks and the way to select salient attention heads. For example, the induction and duplication heads discovered in the 'IOI' task [28] or the context gathering and correct letter heads found in the multiple-choice QA task [16]. The second category uncovers the local part of the LLM that handles different types of knowledge through an analysis of gradients or matrix properties. For example, [3] and [4] discovered local circuits that handle distinct world knowledge. Additionally, some theoretical methods deduce or hypothesize the existence of language skills, framing them as known quantum units [18] or nodes [2].\nHowever, these methods do not fully extract the complete trajectory of language skills. This is primarily due to a lack of a completely interpretable circuit model to express relationships between various modules, especially the information flow between attention and feed-forward (MLP) layers. Second, it's challenging to isolate the role of an individual skill."}, {"title": "Method", "content": "In this paper we propose a novel 3-step framework to extract the target language skills.\n\u2022 Step 1 (Section 3.1): We decouple the architecture of transformer language models into a combination of individual \u201cMemory Circuits\", which independently represents the minimum unit for reading memory. This results in a Complete Circuit Graph, G.\n\u2022 Step 2 (Section 3.2): Keeping the destination token unchanged, we adopt greedy search to remove redundant edges in G, retaining only those paths necessary for predicting the last (destination) token and resulting in an Irreducible Circuit Graph, G*.\n\u2022 Step 3 (Section 3.3): We estimate the causal effect of each path in G* on the target skill and select those paths rendering most significant changes as the skill paths. The final graph formed by the skill paths is named as Skill Circuit Graph, denoted as GS.\""}, {"title": "Memory Circuit", "content": "Building on the foundation of the Transformer Circuit [10], we propose a complete decomposition of the transformer model including the MLP layers. Using tensor products ($\\otimes$), we can represent any layer of the transformer model:\n$\\begin{equation}\n\\text{output} = (Id + \\sum_{h \\in H} A_h \\otimes W_{ov} + Id \\otimes W_{MLP} + \\sum_{h \\in H}A_h \\otimes W_{MLP}W_{OV})\\cdot X\n\\end{equation}\nwhere X represents the input representation in each layer and H represents the number of attention heads. Matrix A is given by the attention mechanism $A = \\text{softmax}((XW_Q)(XW_K)^T)$, and $W_{MLP}$ involves the MLP operation with activation given by $atv(XW_{M1})W_{M2}$. $W_{ov} = W_OW_V$ refers to an \"output-value\" matrix which computes how each token affects the output if attended to, while $W_Q$, $W_K$, $W_V$ are parameter matrices for query, key and value. $W_{M1}$ and $W_{M2}$ are weight parameters in two linear layers. This equation simplifies both the attention and MLP modules into linear matrix mappings, describing how the paths from input to output for each layer are decoupled into four independent circuits: 1) $C_{self} = Id\\cdot X$; 2) $C_{attn} = \\sum_{h \\in H} A_h \\otimes W_{ov} \\cdot X$; 3) $C_{mlp} = Id \\otimes W_{MLP} \\cdot X$; 4) $C_{attn+mlp} = \\sum_{h \\in H} A_h \\otimes W_{MLP}W_{OV} \\cdot X$. Moreover, three of"}, {"title": "Greedy Search", "content": "Given the input tokens for LMs, $X = \\{x_1,\\cdots, x_{N-1}\\}$, the whole optimization loss is:\n$\\begin{equation}\n\\mathcal{L} = -\\sum_{n=1}^{N} \\log P(X_{n+1}|X_1,...,x_n)\n\\end{equation}\nWithout loss of generality and to facilitate our analysis, we focus on predicting the last destination token, $x_N$, given the historical context, i.e., $\\mathcal{L}_{dst} = -\\log P(x_N|x_1,\\dots, X_{N-1})$. It can be reasonably hypothesized that many circuits and paths are not dedicated to the prediction of the destination token $x_N$ but related to other source tokens. Therefore, we need to prune the circuit graph and retain those paths that are essential for the prediction of destination tokens. This will afford a more explicit and causal view of the efforts made by the language model to generate $x_N$.\nSpecifically, we use a greedy search strategy to prune unnecessary paths between Memory Circuits while ensuring that the top n5 candidates for the prediction of the destination token remain unchanged. Given that a depth-first search is more likely to remove shallow paths, we employ a breadth-first search (We compared different search strategies and constraints in Appendix B) as shown in Algorithm 1: We"}, {"title": "Estimation of Causal Effects for Language Skills", "content": "It is widely recognized that most texts require more than one language skill for inference [2]. Therefore, determining which paths are associated with the observed skill can be challenging. For this reason and motivated by endeavors in causal effect analysis [28, 27], we divide the effects of any text on the output token into 3 components: skill effects, background effects, and self effects for destination (abbreviated as self effects).\nSkill effects refer to the impact of the observed language skill on the output which is the focus of this paper. Self effects denote the impact of using a single destination token to predict, which functions like a \"bi-gram model\" (a model associating one input token with its output token). Background effects propose a counterfactual scenario, i.e., what would the effect be if this skill is not present"}, {"title": "Experimental Design", "content": "This paper focuses on 3 language skills, spanning from basic to advanced levels:\nPrevious Token Skill: This is a skill to receive information from the previous token.\nInduction Skill: This skill involves identifying patterns in prefix matching and replicating recurring token sequences.\nICL Skill: This is a complex skill to recognize and replicate the demonstration context, thereby producing outputs based on similar patterns.\nExtensive research has shown that these three skills build on one another in a sequentially encompass-ing manner [1, 20, 22, 9]. The Induction Skill inherently includes the Previous Token Skill. In simple terms, for induction to occur in the sequence \u201cA B ... A\u201d, the token B must retrieve information from"}, {"title": "Validation", "content": "To understand whether the identified skill paths are responsible for their corresponding language skills, we design an intervention experiment by removing different sets of paths and observe the output of the LM. Table 5 displays the accuracy of 6 types of samples under different configurations of the Circuit Graphs when treating the original output as the ground-truth. For each language skill S, we randomly select 500 samples from its corresponding dataset. As a result, 9 different configurations of Circuit Graphs are tested: G* which represents the original output; - R50 which signifies the removal of 50 paths at random from G*; -R500 after the deletion of 500 paths randomly from G*, which approximately equals the number of skill paths. The remaining 6 configurations encompass the removal of paths from G* that correspond to the skill of Previous Token, Induction, ICL1, ICL2, ICL3, and ICL4, respectively.\nThe results indicate that almost all samples were unable to produce the original token when these skill paths were excluded (as indicated in the last 6 columns), yet random removal of paths does not lead to such significant impact. Additionally, Figure 2 visualizes the t-SNE representation of the top 5 candidate outputs associated with different Circuit Graphs. It is clear that when a skill path is removed, the output (blue) shifts from red towards green (or yellow), indicating a transition from a text output distribution that includes skills to a distinct space resulted from the removal of these skills."}, {"title": "How Skill Effects Are Confounded", "content": "Another question is whether the background effect and self effect, mentioned in Section 3.3, potentially exist as confounders or share the circuits with observed skills. To answer this question, we conduct two experiments, with the results shown in Appendix D. Initially, Table 7 checks the overlap between the paths with $Eff > 0.5$ in the background/self text and the skill paths, illustrating that a small portion (approximately 10%-20%) of those paths does not belong to any observed skill. This corresponds to the confounding originating from other latent skills that we envisioned. Secondly, Figure 6 visualizes these different-effect paths' bivariate probability density function with the original input and background/self text. One intriguing discovery is that the confounding skills are more likely"}, {"title": "Discovery of Language Skills", "content": "Table 3 displays the circuits receiving more than 10 circuit paths (receivers) in the skill graphs. We use [l, i] to denote the circuit $C_{l,i}$ in the l-th layer and i-th circuit. The complete Skill Circuit Graph can be found in Appendix G. From Table 3, we identify 3 interesting patterns:\n1. Identifiability: The paths of each skill are identifiable and remain unchanged across most data instances.\n2. Stratification: The Previous Token Skill (PVT) is one of the simplest language skills, and thus it is located across layers 0-2. The Induction Skill (IDT) is slightly more complex and thus spreads across layers 0-6. Meanwhile, ICL is the most complex skill and has key receivers across nearly all layers. Additionally, all skills share the 11-th layer (final layer).\n3. Inclusiveness: Higher-level skills always entail the key circuits of lower-level skills. It is universally acknowledged that the Previous Token Skill is an integral part of the Induction Skill, which is why circuits such as [2, 14], [2, 18] and [2, 20] (presented in PVT) can be found in the Induction Skill Graph. Similarly, the ICL skill encapsulates the Previous Token Skill and Induction Skill as necessary sub-skills, which is why circuits that are evident in the Previous Token Skill (such as [2, 14], [2, 20], [2, 24]) and those identified in the Induction Skill (such as [3, 14], [4, 5]) can be found in the ICL Skill Graph. Furthermore, we list all multi-step paths with inclusive sub-path in Appendix E.\nAdditionally, we have observed some differences in the receivers of different ICL tasks. Combined with the insights provided by [3] and [4], we suspect that these differences arise from distinct circuits required to process domain-specific knowledge across different tasks. Based on the paths, attention weights, and cosine similarities of the representations (detailed results on attention weights can be found in Appendix F), we have identified several circuits with distinct characteristics:"}, {"title": "Exploration - Why Wrong Outputs?", "content": "In this section, we present a new direction for explaining and exploring common erroneous answers using Skill Circuit Graphs. Specifically, by contrasting the Skill Graphs of \"incorrect\" outputs with those of correct outputs, we can further diagnose what leads to the failure in skill execution. Table 4 illustrates the key circuits exhibiting the highest absent rate7 between 3 \u201cincorrect\u201d and correct output types. Specifically, we investigate one erroneous type of output from an induction skill sample (F_IDT), and two types from ICL skill samples (F1_ICL, F2_ICL).\nF_IDT refers to those samples wherein the input possesses an Induction pattern (\"A B ... A\"), but ultimately does not output B. F1_ICL denotes those samples wherein the output includes a word outside of the label options from the demonstrations, for example, a case where the input text \u201c[review1], label: positive, [review2], label: negative, [review3], label:", "the": "Such an error indicates that the language model did not capture the ICL template pattern in this case. F2_ICL involves samples that capture the template pattern yet still produce incorrect outputs, for example, cases where the correct output should be \u201cpositive", "negative": "We compare the circuit graphs of these \u201cincorrect", "A \u2192 B\" in the induction template \\\"A B... A\", as well as patterns such as \\\"label \u2192 positive": "n ICL. The loss of this skill\u2014failure during the execution of the previous token skill-means that both the Induction skill and ICL skill cannot pass the duplicated prefix information to the next token, leading to template-based errors.\nTo further understand why these samples do not successfully execute the previous token skill, we perform a bi-clustering operation on the Previous Token Skill (experiment details are shown in Appendix C.2), yielding a cluster with $Eff < 0.2$ across most of all paths. We compared this cluster (termed the low-effect cluster) with another cluster (named high-effect cluster), with some samples as follows (All samples are from the original text of the Previous Token Skill, tokenized into two tokens):"}, {"title": "Limitation and Conclusion", "content": "We have identified three pressing limitations that need to be addressed. The first is the time complexity of the greedy search the second is the lack of further examination on the representational study, and the third is scalability. Assuming the time for one inference of LLM as O(1), the time complexity of a single greedy search would then be O(L2N2), i.e., the square of the layer number times the number of circuits. If we can overlook this time-consuming process, then the G* for each input would effectively facilitate training. In other words, G* could directly instruct LLM which paths are essential and which are not, thus streamlining the training process. Despite the time complexity, we recall our contribution on the analysis of LMs which is usually more challenging and does not require large-scale inference. Additionally, the lack of research at the representational level hinders our progress in answering more complex questions such as why certain samples fail to trigger a skill. Recognized that this is a rather challenging topic, we leave it as a promising future work. Finally, we recognized the limitations of testing on a single model and specific skills. Although many studies have validated the GPT-2 series to have public trustworthiness for research in mechanistic interpretability, making us confident in its capacity to support our contribution\u2014the pioneering work in discovering the theoretical foundation and experimental design of language skills\u2014there remains ample scope for scalability across a variety of models and skills for future work.\nIn conclusion, we propose a novel framework to completely dissect the language model and discover key components leading to meaningful language skills. Our framework contains three steps, involving decomposing the LM into paths among Memory Circuits, pruning paths preserving the inference outcome, and identifying salient paths for language skills via causal analysis. Through this process, we are able to identify the skill paths necessary for a language model to process texts. Furthermore, we demonstrate several interesting findings validating existing hypotheses. For example, each language skill is bound to specific circuits, and more complex skills are associated with deeper circuits. Additionally, we find that the evolution of complex skills extends along the path of simpler skills they encompass, providing strong experimental support for research on emergence discoveries. Lastly, we explored attributions of error samples to the absence of certain skill circuits. These findings could potentially offer novel feedback for the training process. Overall, we believe that our thorough discovery of language skills can generate more insights into the exploration of language models."}, {"title": "Analysis about Memory Circuits", "content": "Why A$\\otimes$X is not the circuit with complete function?\nWe use $X^{l,n}$ to denote the hidden state representation corresponding to the n-th token at the l-th layer, and U represents the unembedding matrix. Therefore, for any representation $X^{l,n}$, we can obtain its vocabulary distribution, i.e., the logits for each token candidate, using $X^{l,n}U$. We adopt a sample text, \"Beats Music is owned by\", as the input. Table 5 shows the logits corresponding to the words \"the\" and \"Apple\" when these tokens are converted to vocabulary embeddings.\nOur expected correct output is such that after the last layer's representation is unembedded, the logits for \"Apple\" reach their peak. However, as shown in Table 5, after conducting an A$\\otimes$X operation on"}, {"title": "How to explain Memory Circuits?", "content": "Let's likewise map all the Memory Circuits into the vocabulary space:\n$\\begin{equation}\nV = C \\cdot U = f(X) \\cdot W \\cdot U = f(x) \\cdot WU\n\\end{equation}\nSimply put, we assume $X \\in \\mathbb{R}^{N,D}$, $f(X) \\in \\mathbb{R}^{N,M}$, $W \\in \\mathbb{R}^{M,D}$, and $U \\in \\mathbb{R}^{D,E}$, where N represents the number of tokens, D denotes the dimensions in the residual stream, M refers to the dimensions in the circuit (such as the dimensions in QKV or MLP), and E signifies the length of the vocabulary list. Naturally, $WU \\in \\mathbb{R}^{M,E}$, which could be seen as a collection of M vocabulary distributions. These vocabulary distributions are unaffected by the input tokens and thus can be considered as the acquired memory from training.\nThe function $f(X) \\in \\mathbb{R}^{N,M}$ acts like a weight which specifies how much each vocabulary distribution contributes to the output. This confirms why MLP is generally regarded as a memory storage, as its dimensions are usually significantly larger than those of QKV. Simultaneously, it also explains the advantage of MoE: providing a wider range of options for vocabulary distribution.\nIn the final analysis, the inference process of a language model can be seen as constituting 3 key components: \u201cmemory\u201d, \u201cmovement\u201d, and \u201censemble\u201d. \u201cMemory\u201d pertains to acquiring a new distribution from memory distribution, while \u201cmovement\u201d involves transferring token information to subsequent tokens. Finally, \u201censemble\u201d refers to the process of combining representations from multiple circuits to produce the final representation. Within this process, Memory Circuits serve as the smallest units responsible for \u201cmemory\u201d and also encompass independent operations of \u201cmovement\u201d ($C^{1-12}$ and $C^{14-25}$). Furthermore, they form individual elements of the \u201censemble\u201d. Therefore, we examine the interrelationships (necessary paths) between Memory Circuits to understand the language skills of language models."}, {"title": "Search Strategies", "content": "We conducted extensive comparisons w.r.t. two elements: breadth-first search and top1 candidate consistency. 1000 samples, each less than 30 tokens in length, were randomly selected from the WIKIQA dataset [30] and applied to different search strategies:\n\u2022 Breadth-1: Breadth-first search was conducted on $C^{l,i}$ where l varies from 0 to 11, and i from 1 to 25.\n\u2022 Breadth-2: The same breadth-first search was done on $C^{l,i}$, but with l running from 0 to 11 and i from 25 to 1."}, {"title": "Data Preparation and Implementations", "content": "We randomly selected 40k text samples comprising two tokens - \"token0 token1\" - from the WIKIQA, OpenOrca, and OpenHermes corpora. In 20k of these samples, the two tokens made up one word, while in the remaining 20k, \u201ctoken0\u201d and \u201ctokenl\u201d belonged to two separate words. For the background text, we chose \u201ctoken0", "tokenl\". A complete sample is as follows": "n{text: \" that most\", backgound_text: \u201c that\", self_text: \u201c most\u201d, GPT2-XL_output: \" of\"}"}, {"title": "Induction Skill", "content": "The samples for the Induction Skill also come from WIKIQA, OpenOrca, and OpenHermes. We randomly selected 14k samples with the template \u201c... A1 B ... A2", "A2": "s the same as the preceding token \"Al"}, {"A2": "nd had GPT2-XL produce a new but different token to replace \"A2", "... Al B ... C\". Since \u201cC": "s semantically supplemented by the preceding text and differs from \"A2"}, {"A2": "A complete sample is as follows:\n{text: \"chinese lesson 1.2: chinese\", backgound_text: \u201cchinese lesson 1.2: The\", self_text: \u201c chinese\u201d, GPT2-XL_output: \u201clesson\"}"}, {"title": "ICL Skill", "content": "The 4 types of ICL skill samples come from SST-2 dataset and the object_counting, qawikidata, reasoning_about_colored_objects datasets in BIGBENCH. These samples have been named by us as icl_sst2, icl_oc, icl_qa, icl_raco, with quantities of 1000, 284, 1000, and 135 respectively. Each sample is required to contain two different labelled demonstrations and should be answerable correctly by GPT2-XL. Here are examples of the four types of samples:"}, {"title": "Implementation", "content": "In implementation, following the 3-step process from Section 3, we obtained the skill circuit graph, GS. We found that the skill effect values in GS for the Previous Token Skill and the Induction Skill were not high, with the highest $Ef f_{skill}$ being only 0.54 and 0.61, respectively. However, the highest $Ef f_{skill}$ for the ICL Skill reached 0.98. We speculated that because the Previous Token Skill and the Induction Skill are overly simple, there were a significant number of samples that happened to output the correct answers without triggering the corresponding skill paths. For instance, in the text \"In China [mainland]", "mainland\" was influenced by the bi-gram model of \u201cChina\" or if \\\"China\" received information from \u201cIn\". As such, we attempted to perform bisection clustering for each sample in the Previous Token Skill and Induction Skill, based on the paths with top 10% $Eff_{skill}$.\"\n    },\n    {\n      \"title\": \"Attention Weights of Key Circuit\",\n      \"content\": \"In this section, we provide additional information on the attention weights of key circuits in the Induction Skill and ICL1 Skill.\nFor the Induction samples, we focus on the following tokens:\n\u201cA ... IP IF IN ... ISP IS\", where \\\"A\\\" represents the first token of the input text, \u201cIF": "nd \u201cIS\u201d denote the positions of the first and second appearances of the duplicated token respectively, \u201cIP\u201d and \u201cIN\u201d indicate the tokens before and after \u201cIF\u201d, and \u201cISP\u201d refers to the token before \u201cIS\u201d. Figure 8 shows these located positions' attention weight.\nFor the ICL samples, we select ICL1 (icl_sst2 task) to show, following tokens:\n\u201cA B ... P1P P1A ... P1B L1... A2 ... P2P P2A ... P2B L2 ... A3 ... P3P P3A ... P3B, where \"A\u201d, \u201cA2\u201d, \u201cA3\u201d represents the beginning of review1, review2 and review3, \u201cP1P\u201d, \u201cP2P\u201d, and \u201cP3P", "P1A ... P1B\", \u201cP2A ... P2B\", \"P3A ... P3B\" represents the label prompt of review1, review2, and review3, and \u201cL1\", \u201cL2\" represent the label of review1 and review2. Figure 9 shows these located positions' attention weight.\"\n    }": ""}]}