{"title": "ST-FIT: Inductive Spatial-Temporal Forecasting with Limited Training Data", "authors": ["Zhenyu Lei", "Yushun Dong", "Jundong Li", "Chen Chen"], "abstract": "Spatial-temporal graphs are widely used in a variety of real-world applications. Spatial-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful tool to extract meaningful insights from this data. However, in real-world applications, most nodes may not possess any available temporal data during training. For example, the pandemic dynamics of most cities on a geographical graph may not be available due to the asynchronous nature of outbreaks. Such a phenomenon disagrees with the training requirements of most existing spatial-temporal forecasting methods, which jeopardizes their effectiveness and thus blocks broader deployment. In this paper, we propose to formulate a novel problem of inductive forecasting with limited training data. In particular, given a spatial-temporal graph, we aim to learn a spatial-temporal forecasting model that can be easily generalized onto those nodes without any available temporal training data. To handle this problem, we propose a principled framework named ST-FiT. ST-FiT consists of two key learning components: temporal data augmentation and spatial graph topology learning. With such a design, ST-FiT can be used on top of any existing STGNNs to achieve superior performance on the nodes without training data. Extensive experiments verify the effectiveness of ST-FiT in multiple key perspectives.", "sections": [{"title": "Introduction", "content": "Spatial-temporal graphs contain both spatial information encoded in graph topology and temporal information encoded in node-associated temporal data (Sahili and Awad 2023). In recent years, spatial-temporal graph data has become ubiquitous in a variety of domains such as transportation (Zhang et al. 2021), epidemiology (Wang et al. 2022), and social science (Kefalas, Symeonidis, and Manolopoulos 2018). In these domains, a widely studied task is spatial-temporal forecasting (Zhang and Patras 2018; Han et al. 2021; Zhang et al. 2020), i.e., predicting future temporal dynamics associated with the nodes in given spatial-temporal graphs (Ye et al. 2020; Feng and Tassiulas 2022). Towards such a goal, Spatial-Temporal Graph Neural Networks (STGNNs) stand out (Cui et al. 2021; Lan et al. 2022; Bai et al. 2020) due to their capability of synergizing the strengths of Graph Neural Networks (GNNs) with various sequential forecasting models (Guo et al. 2019; Song et al. 2020) to learn the complex spatial-temporal dependencies. As a consequence, STGNNS have been widely adopted in a plethora of real-world applications (Jin et al. 2023; Zhuang et al. 2022).\nDespite advancements, most existing STGNNs require that all nodes in the given spatial-temporal graphs should have temporal data (e.g., time series data) during training (Wang et al. 2020; Li and Zhu 2021), such that the unique temporal dependency for each node can be easily captured (Shin and Yoon 2024). With the captured temporal dependencies, the STGNNs could make predictions for each node effectively. However, in real-world scenarios, most nodes may not have available temporal data during training (Gupta, Kodamana, and Ranu 2023; Wu et al. 2022). We present an exemplary case in Figure 1. Specifically, facing a sudden pandemic such as COVID-19, due to the asynchronous nature of pandemic outbreaks, the pandemic dynamics (e.g., the tendency of confirmed case number) of most cities on a geographical graph may not be available at a given time point (marked in red) (Atchad\u00e9 and Sokadjo 2022; Panagopoulos, Nikolentzos, and Vazirgiannis 2021). In such cases, existing STGNNs perform poorly in cities without available temporal training data. Therefore, to enable wider deployment, the forecasting model should generalize the learned temporal dependencies to the nodes without any temporal data during training, referred to as inductive forecasting with limited training data. A few recent studies have made early explorations to such a problem (Tang et al. 2022). For example, domain adaptation strategies (Fang et al. 2022; Wang et al. 2021a) have been adopted to generalize dependencies from nodes with abundant temporal data. However, these works overwhelmingly focus on the generalization between different spatial-temporal graphs, ignoring granular temporal dependency differences within the same graph. Furthermore, they usually require costly fine-tuning (Zhou et al. 2022), which limits their efficiency for real world scenarios (Guo et al. 2023; Li et al. 2020). Therefore, despite the practical significance, the problem of enabling inductive forecasting with limited training data remains underexplored.\nIt is worth mentioning that inductive forecasting with limited training data on spatial-temporal graphs presents three challenges. (1) Limited Temporal Dependencies. With the temporal data corresponding to only a limited number of nodes available, the forecasting model can only extract limited temporal dependencies (Lachapelle et al. 2024; Liu, Bahadori, and Li 2012). However, nodes without any temporal training data may still require different temporal dependencies to perform accurate forecasting (Wijsen 2018). Therefore, the first challenge is to learn diversified temporal dependencies for better generalization. (2) Diverse Spatial Dependencies. The spatial-temporal graph topology encodes spatial dependencies, crucial for generalizing the learned temporal dependencies between neighboring nodes (Liang et al. 2022a). However, the topology may exhibit different patterns of spatial dependencies in different local areas (Park and Kim 2014). For example, during the pandemic, geographically neighboring cities may exhibit both similar and distinct pandemic dynamics due to varying interactions (e.g., different volumes of population migration (Gibbs et al. 2020)). Hence, the second challenge is to equip the forecasting model with generalization capability across different spatial dependencies. (3) Inference Efficiency. Most existing explorations aiming to handle differences in spatial and temporal dependencies require costly fine-tuning processes (Zhou et al. 2022; Ouyang et al. 2022), which makes efficient inference difficult in real-world scenarios. Our third challenge is to avoid additional computational costs and achieve efficient inference on nodes with no available temporal data for training.\nTo address the challenges above, we introduce ST-FiT (inductive Spatial-Temporal Forecasting with limited Training data), a novel framework that generalizes to different spatial-temporal dependencies without fine-tuning. Specifically, ST-FiT consists of two learning modules for above challenges. To handle the first challenge, ST-FiT introduces a temporal data augmentation module. This module learns the manifold where the available training temporal data lies and generates new temporal data close to it to enrich the training set. In this way, the training temporal dependencies can be enriched, such that the generalization ability to different temporal dependencies can be enhanced for the forecasting model. To handle the second challenge, ST-FiT is equipped with a spatial graph topology learning module. With this module, spatial dependencies (represented as edges in spatial-temporal graphs) between new and existing temporal data can be generated, and existing spatial dependencies can be refined as well. We formulate an optimization problem with an iterative training strategy for these modules. As such, ST-FiT is enabled to perform inductive forecasting with any STGNN backbone while avoiding costly fine-tuning. Such generalization capability and flexibility significantly broaden a broader range of applicable scenarios compared with other alternative spatial-temporal forecasting models. Empirical evaluations on three commonly used real-world datasets corroborate the effectiveness of ST-FiT in multiple key perspectives. Our contributions are summarized as follows:\n\u2022 Problem Formulation. We formulate a novel problem of inductive forecasting with limited training data, which aligns with the setting associated with a wider spectrum of real-world applications.\n\u2022 Framework Design. We design a new framework named ST-FiT to handle the key challenges associated with the problem studied and achieve superior performance compared to other alternatives.\n\u2022 Experimental Evaluations. We conduct comprehensive experiments on three commonly used real-world datasets to verify the effectiveness of our proposed framework."}, {"title": "Problem Definition", "content": "In this section, we first present the notations used throughout this paper. Then we introduce two key definitions, including Spatial-Temporal Graph and Spatial-Temporal Forecasting. Finally, we introduce a novel problem of Inductive Spatial-Temporal Forecasting with Limited Training Data.\nNotations. We denote an attributed graph as $G = (V, E, A, X)$, where $V$ is the set of nodes and $N = |V|$ is the number of nodes. $X \\in \\mathbb{R}^{N \\times C}$ represents the feature matrix of the nodes in $V$, where $C$ is the dimension number of node features. $E$ denotes the set of edges, where the edge between node $v_i$ and $v_j$ is denoted as $e_{ij} = (V_i, v_j)$. $A \\in {0,1}^{N \\times N}$ is the adjacent matrix, where $A_{ij} = 1$ indicates that an edge exits between $v_i$ and $v_j$, otherwise $A_{ij} = 0$.\nDefinition 1 Spatial-Temporal Graph. A spatial-temporal graph ${G}_{t=1}^T$ contains a sequence of graphs $G_t (1 \\le t \\le T)$, where $t$ is the current time step and $T$ is the total number of time steps. Here each $G_t$ is described as $(V,E, A, X^t)$, where $X^t \\in \\mathbb{R}^{N \\times C}$ denotes the node features at time step $t$. The graph topology described by $A$ reveals the spatial dependency, while the temporal data corresponding to each node consists of all node features across all time steps.\nProblem 1 Spatial-Temporal Forecasting Given a sliding window of $\\kappa$ time steps in a spatial-temporal graph $G_{t-\\kappa:t}$ our goal is to learn a function $f$ to predict features $X_{t+1:t+\\tau}$ in following $\\tau$ time steps. Here $(.)_{t_1:t_2}$ denotes a sequence (ordered by time steps) with a length of $t_2 - t_1 + 1$, where the input at time step $t_i$ ($t_1 \\le t_i < t_2$) is placed at the $(t_i - t_1+1)$-th position."}, {"title": "Methodology", "content": "An overview of ST-FiT is shown in Figure 2. Specifically, ST-FiT consists of 3 modules: (1) Temporal Data Augmentation aims to generate diverse temporal data (in the format of time series) via learning the manifold where the accessible training temporal data lies. (2) Spatial Topology Learning aims to generate the spatial dependencies between existing and newly generated time series and refine the spatial dependencies between existing time series. (3) STGNN Backbone. ST-FiT is plug-and-play, i.e., any STGNN model can be adopted as the backbone of this framework to achieve forecasting on nodes without training temporal data. To effectively optimize the three learning modules, we formulate the optimization problem first and propose an iterative approach to solve it. We introduce the three modules below.\nSpecifically, an STGNN backbone model takes the temporal data (encoded with temporal dependencies) and the spatial-temporal graph topology (encoded with spatial dependencies) as input, and outputs predicted temporal data. In particular, we assume the temporal data is in the format of time series; the input and output time series are with a length of $\\kappa$ and $\\tau$, respectively. We formulate the STGNN backbone as\n$x_{t+1:t+\\tau} = h(x_{t-\\kappa:t}, A), \\qquad(1)$\nwhere $h$ denotes the mapping given by the STGNN backbone; $x_{i:j}$ denotes the time series given by the node feature matrix $X^t$ from time step $i$ to $j$; $x_{t-\\kappa:t}$ and $x_{t+1:t+\\tau}$ represent the input and output time series, respectively.\nTo handle the challenge of limited temporal dependencies, we resort to data augmentation to enrich the training temporal data. However, commonly used methods such as cropping and adding noise in input or latent space are inadequate since they cannot generate new dependencies. In order to achieve our goal, we propose to learn a manifold where all available temporal data lies in the hidden space. Then, we select data points that lie between the points associated with the accessible training temporal data on the manifold, such that new data following new temporal dependencies can be generated. We refer to this strategy as temporal data mix-up on the learned temporal manifold. Specifically, we first input the time series from each sliding window into a Variational Auto-Encoder (VAE) (Kingma and Welling 2013), from which we can derive the representations that characterize the manifold of the temporal data in the hidden space. Denote $\\xi$ as a time step between $\\kappa + 1$ and $T_{\\text{train}}$. We achieve the above operations with\n$\\mu_v, \\sigma_v = \\text{Encoder}(x_{t-\\kappa:\\xi+\\tau})\\text{ and }\\ z_v = \\text{Sample} (\\mu_v, \\sigma_v),\\qquad(2)$"}, {"title": "Spatial Topology Learning", "content": "To handle the challenge of diverse spatial dependencies, we propose to refine existing spatial topology and generate the topology between the generated and existing temporal data. As such, we are able to better adapt the spatial topology to fit the predictive capability of the STGNN backbone. Intuitively, this module should be agnostic to the number of nodes, such that it meets the need for inductive forecasting. Meanwhile, the learned spatial topology should be naturally discrete and sparse so that it only encodes key patterns of spatial dependencies (Hu et al. 2022). To achieve the goals above, we propose to leverage the Gumbel-Softmax reparameterization to learn a sparse graph topology based on the node features (Shang, Chen, and Bi 2021).\nSpecifically, we first use a Multi-Layer Perceptron (MLP) encoder to transform each sliding window of time series into a hidden representation. Then, we use another MLP maps the hidden representations for each pair of nodes $v_i, v_j$ to a scalar $P_{ij} \\in [0, 1]$. We utilize the matrix $P$ to parameterize a Bernoulli distribution between every node pair. By drawing samples from the Bernoulli distributions, we are able to construct a refined adjacency matrix $\\bar{A}$ to characterize the learned spatial topology, i.e., $\\bar{A}_{ij} \\sim Ber(P_{ij})$. Note that we apply the Gumbel reparameterization trick (Franceschi et al. 2019) to enable the gradient to flow through $\\bar{A}$, such that gradient-based techniques can be adopted to optimize $P$. We formulate the procedure to derive $\\bar{A}$ as\n$\\bar{A}_{ij} = \\text{Gumbel-Softmax}(P_{ij}, s), \\qquad(7)$\nwhere $s$ is the temperature parameter of Gumbel-Softmax. However, based on the formulation given above, it becomes difficult then to impose $l_1$ norm as a regularization to achieve sparse graph topology. To enforce the learned spatial topology to be sparse, we propose to transform the learned matrix $P_{ij}$ with a threshold $\\epsilon$, i.e.,\n$P_{ij} = \\text{Sigmoid}(e(P_{ij}-\\epsilon)/\\$)$, \\qquad(8)$\nwhere $\\$ is the temperature. Intuitively, $P_{ij} < \\epsilon$ will make it less likely to generate an edge between node $v_i$ and $v_j$.\nThe training objectives of ST-FiT are three-fold: (1) Generate diverse temporal data that lies close to the learned manifold; (2) Refine spatial topology based on diverse spatial dependencies; (3) Capture key spatial-temporal dependencies for forecasting with the STGNN backbone. However, we note that the optimization of temporal data augmentation and the other two modules are intertwined, since (1) it requires the STGNN backbone to perform prediction ; and (2) it requires the refined spatial dependencies from the spatial topology learning module as the input (as in $L_{\\text{fst}}$). As such, we propose to formulate the overall optimization problem and solve it with an iterative training strategy, i.e., training the temporal data augmentation module and the other two modules iteratively. We refer to the two optimization processes in each iteration as the Phase 1 (optimizing the temporal data augmentation module) and the Phase 2 (optimizing other modules), respectively."}, {"title": "Optimization Strategy and Inference", "content": "In Phase 1, we optimize the temporal data augmentation with gradient-based optimization techniques, while the parameters of other modules are frozen. Formally, we have\n$\\theta_{\\text{epoch}+1}^{\\text{Daug}} = \\theta_{\\text{epoch}}^{\\text{Daug}} - \\eta \\cdot \\nabla_{\\theta^{\\text{Daug}}} L_{\\text{Daug}},\\qquad(9)$\n$L_{\\text{Daug}} = L_{\\text{sim}} + L_{\\text{fst}} + L_{\\text{KL}},\\qquad(10)$\nwhere $\\theta^{\\text{Daug}}$ denotes learnable parameters for temporal data augmentation module and $L_{\\text{KL}}$ denotes the commonly used regularization term for the adopted VAE (Verma et al. 2019).\nIn Phase 2, we aim to jointly optimize the learnable parameters in the STGNN backbone and the spatial topology learning module, where the parameters of temporal data augmentation are frozen. Formally, we have\n$\\theta_{\\text{epoch} + 1}^{\\text{Gf}} = \\theta_{\\text{epoch}}^{\\text{Gf}} - \\eta \\cdot \\nabla_{\\theta^{\\text{Gf}}} L_{\\text{Gf}},\\qquad(11)$\n$L_{\\text{Gf}} = L_{\\text{fst}} + L_{\\text{ori}},\\qquad(12)$\nwhere $\\theta^{\\text{Gf}}$ denotes the learnable parameters associated with the STGNN backbone and spatial topology learning module. $L_{\\text{fst}}$ and $L_{\\text{ori}}$ are loss functions of the forecasting results for the generated and original time series, respectively. In this way, the parameters in the spatial topology learning module and the STGNN backbone are jointly optimized.\nFinally, during inference, we propose to sample a graph topology characterized by $\\bar{A}$ based on the learned $P$. As such, we are able to directly perform forecasting. We present the complete algorithmic routine in Appendix."}, {"title": "Experimental evaluations", "content": "In this section, we aim to answer the following research questions. RQ1. How well can ST-FiT generalize to nodes with no available temporal data for training, compared to other existing alternatives? RQ2. How does the performance tendency of ST-FiT look like compared with other baselines when these models are trained on varying ratios of nodes with available temporal data? RQ3. How does each module of ST-FiT contribute to the overall performance? RQ4. How does the choice of hyper-parameters influence the performance of ST-FiT? In the following sections, we first present the experimental settings, followed by the answers to the proposed research questions."}, {"title": "Experimental settings", "content": "Below we provide a brief introduction to the experiment settings."}, {"title": "Generalization Performance", "content": "To answer RQ1, we first evaluate the forecasting performance of ST-FiT on those nodes without available temporal data during training. We make following observations from the the empirical results in Table 1. (1) ST-FiT outperforms all baselines that do not require fine-tuning. Notably, ST-FiT exceeds the associated backbone model STGCN by up to 40.0% in MAE, 38.6% in RMSE, and 55.6% in MAPE. This verifies the effectiveness of ST-FiT in generalizing to the nodes with different temporal dependencies. Meanwhile, ST-FiT also significantly outperforms FC-LSTM and STGODE, which further demonstrates its superiority over different types of state-of-the-art alternatives. (2) ST-FiT achieves comparable performance with fine-tuned model TransGTR. Specifically, ST-FiT outperforms TransGTR on PEMS04 by 27.1% in MAE, 19.0% in RMSE, and 34.8% in MAPE. In addition, ST-FiT has comparable performances with TransGTR on PEMS03. We note that compared with ST-FiT, TransGTR adopts an additional fine-tuning process with much more abundant temporal data. As such, Trans-GTR can extract temporal data from all nodes in the fine-tuning process, which helps it capture more spatial-temporal dependencies. As a result, we argue that achieving competitive performance to TransGTR without relying on any fine-tuning process should be regarded as satisfactory performance, and this further corroborates the effectiveness of ST-FiT in capturing diverse spatial-temporal dependencies."}, {"title": "Ablation Study", "content": "To answer RQ3, we investigate how our proposed modules contribute to the forecasting performance separately. We use w/o aug to denote cases without temporal data augmentation module. W/o gl denotes removing the spatial topology learning module. Furthermore, we design two experiments for temporal data augmentation, where w/o sim denotes removing $L_{\\text{sim}}$, and w/o fst denotes removing $L_{\\text{fst}}$. Additionally, we design two variants of our proposed spatial topology learning module. specifically, w/o gs denotes adopting a"}, {"title": "Parameter Sensitivity", "content": "To answer RQ4, we analyze the impact of hyper-parameter values, including the threshold in controlling sparsity of spatial topology learning $\\epsilon$ and the parameter $\\lambda$ for temporal data mix-up. We choose values of $\\epsilon$ from 0 to 1, where higher value denotes a sparser learned graph topology. For the value of $\\lambda$, we choose it from the range between 0 and 0.5. The results of three datasets are shown in Figure 4. For the impact of the sparsity of the graph, we observe that: (1) Sparser structures generally perform better, which indicates the necessity of sparsity in spatial-temporal forecasting on nodes without available training temporal data. (2) Overly sparse structures harm performance, since it can omit certain key connections between nodes. From Figure 4a and 4b, we are able to observe that the higher value of $\\lambda$ improve the performance, which can be attributed to a higher diversity of the generated temporal data. With above experiments, we recommend using $\\lambda$ as 0.5, $\\epsilon$ as 0.9 for optimal performance."}, {"title": "Related Works", "content": "Spatial-Temporal Forecasting. Spatial-temporal forecasting is crucial but challenging. STGNNs have shown promise in this area but typically require temporal data for all nodes during training, making them unsuitable for inductive forecasting (Deng et al. 2021). Some other STGNNS can perform inductive forecasting (Wu et al. 2019; Jiang et al. 2023b) but struggle with limited training data, where they only extract limited spatial-temporal dependencies and have difficulty adapting to temporal data with new spatial-temporal dependencies. Recently, several works have resorted to domain adaptation for such generalization challenges. They managed to extract temporal dependencies from nodes without temporal data during training, which offers a possible solution to inductive forecasting (Cheng et al. 2023; Ouyang et al. 2023) on limited training data. For example, ST-GFSL (Lu et al. 2022) transfers traffic knowledge across cities with node-level meta-knowledge and graph reconstruction loss. However, these works focus on graph-level generalization and overlook granular temporal dependency differences between nodes in the same spatial-temporal graph. In addition, they require costly fine-tuning, which limits their efficiency for some inductive forecasting scenarios. Different from these works, ST-FiT enriches the training data with diverse temporal dependencies through temporal data augmentation and captures different spatial dependencies between existing and new temporal data, which helps the generalization to different nodes.\nTemporal Data Augmentation. Due to the common scarcity of temporal data in real-world scenarios, existing works propose to generate temporal data with data augmentation techniques (Fu, Kirchbuchner, and Kuijper 2020). The most challenging part is to generate data not only with diverse temporal dependencies but also lying close to the manifold of existing temporal data. Most traditional algorithms such as slicing, jittering, or scaling apply simple transformation and perturbation (Um et al. 2017; Iwana and Uchida 2021), where they either fail to generate temporal data with diverse and consistent temporal dependencies. Recent prevailing deep learning models such as Generative Adversarial Networks (GANs) (Goodfellow et al. 2020) and Variational Autoencoders (VAEs) (Goubeaud et al. 2021) are promising solutions to generating more consistent temporal data that lie close to the manifold of existing temporal data. Nevertheless, the generated temporal data lack diversity, which limits the contribution to generalization ability. In this work, we present ST-FiT, which applies mix-up on the manifold and learns to capture the temporal dependencies. This helps to not only enrich the original latent space region due to mix-up on the manifold (Huh et al. 2024), but also ensure that the generated temporal data lies close to the manifold where the available training temporal data lies.\nGraph Topology Learning. Due to incompleteness and scarcity of existing graph topology, substantial work have devoted to learning better graph topology over diverse types"}, {"title": "Reproducibility", "content": "In this section, we introduce the details of the experiments in this paper for the purpose of reproducibility. All major experiments are encapsulated as shell scripts, which can be easily executed. We introduce details in subsections below."}, {"title": "Dataset Details", "content": "we conduct experiments on three most commonly used real-world datasets PEMS03, PEMS04, and PEMS08, which are all public transport network datasets released by Caltrans Performance Measurement System (PeMS) (PeMS 2021). The three datasets have been uniformly configured to a time granularity of 5 minutes, which is in alignment with prior studies. The adjacency matrix of each dataset is derived from the spatial topology of the road network. The Z-score standardization technique (Colan 2013) is utilized to normalize the volume of the traffic flow."}, {"title": "Implementation of Baselines", "content": "For the implementation of all baselines, we adapt them to our inductive forecasting task by randomly sampling 10% nodes for training. We set the random seed for selecting training nodes the same for all baselines, where the selecting process involves a BFS traversal on the original graph to preserve some connections between these training nodes. For the remaining implementation of Historical Average\u00b9, LSTM\u00b9, STGCN\u00b2, STGODE\u00b3, and TransGTR\u2074, we follow the same settings with those in their open-source code."}, {"title": "Packages Required for Implementations.", "content": "We perform the experiments on a server with multiple Nvidia A6000 GPUs. Below we list the key packages and their associated versions in our implementation."}, {"title": "Supplementary Experiments", "content": "In this subsection, we present additional experimental results to answer RQ1. Specifically, we compare the average performance of the first 3, 6, 12 time steps in the test window of the temporal data of forecasting on the MAE, RMSE, and MAPE metrics, where we refer to such time steps as Horizons. We make observation from Table 4 as follows. (1) ST-FiT outperforms all baselines that do not require fine-tuning. This verifies the effectiveness of ST-FiT in generalizing to the nodes with different temporal dependencies. (2) ST-FiT achieves comparable performance with fine-tuned model TransGTR, which corroborates the effectiveness of ST-FiT in capturing diverse spatial-temporal dependencies."}, {"title": "Efficiency Study", "content": "In this subsection, we analyse the efficiency of ST-FiT compared to baselines. We present the average time consumed per each epoch in Table 8 (a complete iteration for ST-FiT), from which we make the following observations: (1) The time consumed per epoch by ST-FiT is nearly double that of STGCN, which indicates that the primary reason for the increased time in ST-FiT is the two forward passes per full iteration, whereas temporal data augmentation and spatial topology learning have small impact on the time consumption. (2)The efficiency of ST-FiT is competitive to TransGTR during the pre-training stage. Given that TransGTR includes additional distilling and fine-tuning processes, it struggles to satisfy the efficiency requirements of some real world scenarios."}, {"title": "Algorithm", "content": "In this section, we present the complete algorithmic routine of ST-FiT in Algorithm 1."}, {"title": "Baseline Details", "content": "Since the studied setting is novel, which requires the model to be inductive, we compare our framework to state-of-the-art baselines that can be adopted in such experimental settings. Linear Sum: (1) Historical Average (HA) (Dai et al. 2020) uses weighted averages from previous time steps as predictions for future periods. Temporal-based: (2) FC-LSTM (Sutskever and Vinyals 2014). Long Short-Term Memory model with fully connected hidden units utilizes gating units to selectively capture temporal dependencies. Spatial-Temporal: (3) STGCN (Yu, Yin, and Zhu 2017) adopts a sandwich structure composed of the gated convolution network and ChebGCN to capture spatial-temporal dependencies. (4) STGODE (Fang et al. 2021) learns both global semantic and spatial connections among nodes and captures spatial-temporal dynamics through a tensor-based ordinary differential equation (ODE). Spatial-Temporal and Fine-tuning: (5) TransGTR (Jin, Chen, and Yang 2023) jointly learns and transfers graph structures and forecasting models across cities with knowledge distillation."}, {"title": "Evaluation Metrics", "content": "We compare our method and baselines based on three commonly used metrics in time series forecasting: (1) Mean Absolute Error (MAE) is the most used metric which reflects the prediction accuracy, (2) Root Mean Squared Error (RMSE) is more sensitive to abnormal values, which could evaluate the reliability of models, (3) Mean Absolute Percentage Error (MAPE) is scale-independent, which thus eliminates the influence of data units. The formulas of the three metrics are as follows:\n$\\\\text{MAE}(\\textbf{x}, \\hat{\\textbf{x}}) = \\frac{1}{\\\\Omega} \\sum_{i \\in \\\\Omega} |x_i - \\hat{x}_i|$\n$\\textbf{RMSE}(\\textbf{x}, \\hat{\\textbf{x}}) = \\sqrt{\\frac{1}{\\\\Omega} \\sum_{i \\in \\\\Omega} (x_i - \\hat{x}_i)^2}  (13)$\n$\\\\text{MAPE}(\\textbf{x}, \\hat{\\textbf{x}}) = \\frac{1}{\\\\Omega} \\sum_{i \\in \\\\Omega} |\\frac{x_i - \\hat{x}_i}{x_i}|$\nwhere $x_i$ denotes the i-th ground-truth, $\\hat{x_i}$ denotes the i-th predicted values, and $\\\\Omega$ is the indices of observed points."}, {"title": "Implementation of ST-FiT", "content": "We adopt a learning rate of 2e-2 and weight decay of le-3 for most experiments, together with commonly used number of epochs, e.g., 100. We implement early stopping with patience parameter 10, where training stops if 10"}]}