{"title": "AutoGLM: Autonomous Foundation Agents for GUIs", "authors": ["Xiao Liu", "Bo Qin", "Dongzhu Liang", "Guang Dong", "Hanyu Lai", "Hanchen Zhang", "Hanlin Zhao", "Iat Long Iong", "Jiadai Sun", "Jiaqi Wang", "Junjie Gao", "Junjun Shan", "Kangning Liu", "Shudan Zhang", "Shuntian Yao", "Siyi Cheng", "Wentao Yao", "Wenyi Zhao", "Xinghan Liu", "Xinyi Liu", "Xinying Chen", "Xinyue Yang", "Yang Yang", "Yifan Xu", "Yu Yang", "Yujia Wang", "Yulin Xu", "Zehan Qi", "Yuxiao Dong", "Jie Tang"], "abstract": "We present AUTOGLM, a new series in the ChatGLM family [11], designed\nto serve as foundation agents for autonomous control of digital devices through\nGraphical User Interfaces (GUIs). While foundation models excel at acquiring\nhuman knowledge, they often struggle with decision-making in dynamic real-world\nenvironments, limiting their progress toward artificial general intelligence. This\nlimitation underscores the importance of developing foundation agents capable of\nlearning through autonomous environmental interactions by reinforcing existing\nmodels. Focusing on Web Browser and Phone as representative GUI scenarios, we\nhave developed AUTOGLM as a practical foundation agent system for real-world\nGUI interactions. Our approach integrates a comprehensive suite of techniques\nand infrastructures to create deployable agent systems suitable for user delivery.\nThrough this development, we have derived two key insights: First, the design\nof an appropriate \"intermediate interface\" for GUI control is crucial, enabling\nthe separation of planning and grounding behaviors, which require distinct opti-\nmization for flexibility and accuracy respectively. Second, we have developed a\nnovel progressive training framework that enables self-evolving online curriculum\nreinforcement learning for AUTOGLM. Our evaluations demonstrate AUTOGLM's\neffectiveness across multiple domains. For web browsing, AUTOGLM achieves a\n55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second\nattempt) and 96.2% on OpenTable evaluation tasks. In Android device control,\nAUTOGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7%\non common tasks in popular Chinese APPs. Select AUTOGLM capabilities are\nnow available through the Qingyan Browser Plugin for web applications and via\nForm Applications for invited Android testing. Additional results and materials\nwill be released at https://github.com/THUDM/AutoGLM.", "sections": [{"title": "1 Introduction", "content": "Foundation models, including Large Language Models (LLMs) [5; 27; 7; 2; 42; 11] and Large\nMultimodal Models (LMMs) [20; 25; 26; 1], have captured widespread attention for their remarkable\nlanguage understanding and generation capabilities. Through extensive self-supervised [22] pre-\ntraining on internet-scale corpora, these models have acquired not only knowledge and language\nabilities but also human-like reasoning and planning capabilities, giving rise to LLMs as Agents [21;\n28]. These agents have demonstrated their utility across diverse domains, including coding [35;\n16; 44], data analysis [14; 21], and gaming [34; 18], charting a promising course toward Artificial\nGeneral Intelligence (AGI) through the development of multimodal Foundation Agents [23] that\nserve as generalists across multiple tasks and environments.\nThe ubiquity of digital devices presents a unique opportunity for GUI-capable agents [13; 46; 43; 17].\nThis domain offers several advantages: GUI simulators can be readily deployed in parallel for data"}, {"title": "2 AUTOGLM: Techniques and Insights", "content": "In this section, we will give an overview of the techniques involved in developing AUTOGLM. Partic-\nularly, we will discuss the two important insights that enable AUTOGLM's significant improvements\ncompared to existing LLM or LMM-based GUI Agents."}, {"title": "2.1 Important Techniques", "content": "Training agents can be different from training ordinary LLMs or LMMs. A key obstacle lies in the\nlack of high-quality trajectory data that entails the decision-making process. Following are some\nuseful techniques we realized during the project.\nPre-training. Generally, there is little agent-related data on internet text corpora, making LLMs fall\nshort of effectively act as agents. Additionally, existing LMM pre-training, which is primarily \"visual\ninstruction tuning\u201d, models the alignment between texts and images without sufficiently learning from\nsequential multimodal data [4; 10]. As a result, properly leveraging existing online data with weak-\nsupervised decision-making signals in pre-training would actually help. Besides, for multimodal\nperception, high-resolution visual inputs are very important according to CogAgent [13] and our\nobservations, especially when using grounding strategies like Set-of-Marks (SoM) prompting [38].\nLarge Multimodal Models (LMMs). LMMs are important for GUI understanding and manipulation.\nTraditionally in Robotic Process Automation (RPA), the paradigm has been using Optical Character\nRecognition (OCR) catcher to match key elements in human handcrafted automating programs, which\ncannot be scaled and generalized. LMMs, instead, can perform fuzzy matching and do long-horizon\nplanning thanks to its strong grasping of commonsense and GUI environments from pre-training.\nNevertheless, LMMs still require much training to gain strong planning and reasoning abilities\nnecessary for agent tasks.\nBehavior Cloning (Supervised Fine-tuning). Behavior Cloning (BC) is a key strategy for training\nagents from scratch with high-quality expert trajectories. The strategy has also been verified as\neffective for LLM and LMM-based agent training [24; 41; 6; 13; 17; 23]. Nevertheless, it is of\nextreme cost and time to collect expert trajectories. Moreover, a fundamental problem of using BC is\nthat agents only learn to imitate experts' behaviors step-by-step without fully understanding its goal.\nWhen expert trajectories are oracle (mostly the case for maintaining training stability), agents fail to\nfoster abilities to recover from errors well [23].\nCurriculum Learning. Agent tasks are usually of substantially varied difficulties. As a result,\nit is wise to progressively add difficulty to training with a curriculum schedule. For example,\nAutoWebGLM [17; 15] adopts a multi-stage curriculum, where agent models are sequentially trained\nwith single-step tasks, simple few-step tasks, and complicated long-horizon tasks. DigiRL [3] also\nproposes a simple curriculum to filter appropriate tasks from a fixed set of instructions according to\nthe corresponding agent capabilities at a certain timestamp. We basically find the strategy very useful\nfor building foundation agents with complex goal-achieving abilities.\nReward Modeling (RM). To enable online RL with foundation agents, a proper RM is necessary\nfor providing supervision. Traditionally, many RL agents are trained with limited tasks with precise\nrule-based reward functions. However, foundation agents based on LLMs and LMMs are targeting\ngeneralist mission accomplishment in open worlds, which contradicts task-specific reward functions'\nabilities. Therefore, it is crucial to build generalizable RMs that can deal with a wide range of\nreal-world agent tasks. Specifically, RMs can be categorized to outcome-supervised ORM and\nprocess-supervised PRM [19; 8; 40], which provide different granularities of effective supervision.\nReinforcement Learning (RL). Compared to BC, RL from a narrow sense can better learn from fail-\nures. This is especially important for foundation agent training since high-quality expert trajectories\nare extraordinarily hard to acquire [24]. However, the challenge in applying RL to foundation agent\ntraining lies in the inefficiency of sampling in environments. The problem can be understood from\ntwo aspects: 1) Simulators: when agents are exploring in the Web or Android environments, their ef-\nficiency is bounded by internet connection speed and maximum degree of parallelism. Environments\nlike Android Virtual Devices are quite memory-consuming [23]. 2) Sample Diversity: LLMs and\nLMMs are trained to output certain function-based actions. The strict function formatting usually\nrequires overfitting training with the model, resulting in stubborn monotonous sampling results even\nwhen they are inferenced with a high temperature [33]."}, {"title": "2.2 Insight 1: Intermediate Interface Design", "content": "During the development, we find intermediate interface design vital for disentangling the behaviors of\nplanning and grounding in foundation agents. By separating them into different modules, foundation\nagents can be improved from both dimensions of flexibility and accuracy without interference.\nThe intuition is simple: we find existing LLMs and LMMs to be more capable in planning than in\ngrounding when executing agent tasks on existing benchmarks. While the planning could still be\nsignificantly improved, a majority of current errors arise from incorrect element identification in the\ngrounding period [23]. For example, a typical action generated in VAB-WebArena-Lite when testing\nwith visual inputs would be:\ndo(action=\"Click\", element_coordinates=[823,684])\nwhere the element \"4\" may refer to a \"Submit\" button on Reddit. If we change the format to the\nfollowing:\ndo(action=\"Click\", element_description=\"the 'Submit' button on the bottom right\")\nfind_coordinates_by_instruction(\"the 'Submit' button on the bottom right\")\nIn this way, planner and grounder abilities could be separately improved. Actually, it is much easier\nto harvest massive grounding data from automatic construction from unsupervised environmental\nobservations. In our experiments (Cf. Table 1), we find the strategy with the grounder we trained to\nbe very useful for improving proprietary LLM/LMM API-based planners. Our observation is similar\nto another concurrent work [12] which explores a universal grounding model for GUI agents."}, {"title": "2.3 Insight 2: Self-Evolving Online Curriculum RL", "content": "While Intermediate Interface Design helps alleviate the issue of inaccurate grounding, planning is still\na problem. Many existing agent works in literature base their frameworks on proprietary LLM/LMM\nAPIs, whose planning abilities consequently fail to be improved by training.\nAs a result, we decide to explore training in-house\nplanners via RL. It is very challenging as it lacks\na sufficient amount of either user tasks or expert\ntrajectories. We develop a self-evolving online\ncurriculum RL framework-WebRL [30]\u2013for train-\ning foundation agents from scratch. Take We-\nbArena [47] environment as an example, we adopt\nthe actor-critic RL framework for training. Briefly\nspeaking, we identify the most difficult issues\nwhen we apply curriculum RL to the problem-\ntask data scarcity and policy distribution drift.\nTask Data Scarcity. Leveraging around 1,000 BC\ndata provided by VisualAgentBench [23], we ini-\ntialize GLM-4-9B to 22.4% SR. At this point, we\nhave run out of either task instructions or oracle"}, {"title": "3 Results", "content": "In this section, we report our evaluation of AUTOGLM on both Web and Android-oriented tasks."}, {"title": "3.1 Evaluated on Web", "content": "We adopt three interactive benchmarks: VAB-WebArena-Lite [47; 23] and an online human evaluation\ndataset OpenTable [29]. AUTOGLM experiences training optimization in these environments.\nVAB-WebArena-Lite [47; 23]. VAB-WebArena-Lite\u00b9 is a refined 165-task subset of the original\n812-task WebArena [47] with manual verification of answers and judge functions. Its design intention\nis to speed up the evaluation on WebArena and ensure judging correctness. We evaluate representative\nproprietary LLM/LMM APIs, open models [9], recent agent frameworks [36; 45], and AUTOGLM.\nResults in Figure 4 show that AUTOGLM has significantly advanced on the benchmark, narrowing\nthe performance gap between autonomous agents and humans.\nOpenTable Eval [29]. Following Agent Q [29], we also\nevaluate AUTOGLM on a real website OpenTable, which\nprovides an online open booking service. Since the test\nset of [29] is undisclosed, we reconstruct a 200-sample\ntest set according to the example provided in its paper\n(\"Book reservation for restaurant Cecconi's\non OpenTable for 4 people on May 22 2024 at\n7:00 PM\") and run evaluation on the real OpenTable\nwebsite with human evaluation. Results are in Figure 5.\nAUTOGLM outperforms both gpt-40 and Agent Q on this\nreal-world website."}, {"title": "3.2 Evaluated on Android", "content": "We evaluate AUTOGLM's Android abilities on the academic benchmark AndroidLab [37] (i.e.,\nVAB-Mobile [23]) and frequent tasks in common Chinese Mobile APPs on Android.\nAndroidLab [37] (VAB-Mobile [23]). AndroidLab is\nan interactive Android benchmark and development envi-\nronments that support reproducible evaluation, covering\nsystems and some offline deployable English APPs. Com-\npared to some existing benchmarks such as AITW [32],\nits interactive nature allows more practical evaluation of\nfoundation agents for Android and improvement via RL.\nWe evaluate representative proprietary LLM/LMM APIs,\nopen models [9] fine-tuned on provided BC data, and AU-\nTOGLM. Results are shown in Figure 6. AUTOGLM\nachieves 36.2% SR, the best-performed one among all\ncompared agents.\nHuman Evaluation on Chinese Android APPs. To test\nthe practicality of AUTOGLM being deployed for public\nusers, we carefully examine it on frequent tasks in 7 com-\nmon Chinese Android APPs, including WeChat, Meituan,\nTaobao, Dianping, Amap, Xiaohongshu, and 12306.\nWe curate a test query set (Cf. Table 2) for evaluating AUTOGLM's real performance in the user\ndelivery setting, where the final success rate is determined by human evaluation on the whole\nexecuting trajectories. Instead of evaluating an Android Virtual Device (AVD) as in AndroidLab and\nprevious work [31; 39], our evaluation is conducted in physical Android phones implemented with\nAccessibilityService applications to reflect the practical scenarios of foundation agents for phone\nuse. Results are shown in Figure 7. We classify results into 3 types for better understanding of\nAUTOGLM:\n\u2022 Success: The task is completely successful, fulfilling all requirements in user instructions.\n\u2022 Partial: The task is partially done in the correct direction without completing some following\nprocedures to fulfill user requirements.\n\u2022 Fail: The task is terminated too early, gets stuck in the middle, or goes in the wrong direction.\nAs we observe, AUTOGLM works decently on evaluated APPs. While currently, it is unable to solve\nall tasks perfectly, unfinished tasks are able to be half completed, which would still be of assistance\nin practical scenes to users to speed up GUI operations."}, {"title": "4 Conclusion", "content": "Through this work, we introduced AUTOGLM, a series of foundation agents built upon the ChatGLM\nmodel family that demonstrates strong capabilities in GUI operation across web browsing and\nAndroid environments. Our key contributions include the design of an intermediate interface that\neffectively disentangles planning and grounding behaviors, and the development of a self-evolving\nonline curriculum RL approach that enables robust error recovery and performance improvement.\nThe strong empirical results across various benchmarks, including a 55.2% success rate on VAB-\nWebArena-Lite and 36.2% on AndroidLab, along with successful real-world deployments through\nbrowser plugins and Android applications, demonstrate AUTOGLM's potential as a significant step\ntoward developing practical foundation agents for GUI interaction."}]}