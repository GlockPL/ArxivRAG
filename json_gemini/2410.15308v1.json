{"title": "LlamaLens: Specialized Multilingual LLM for Analyzing News and Social Media Content", "authors": ["Mohamed Bayan Kmainasi", "Ali Ezzat Shahroor", "Maram Hasanain", "Sahinur Rahman Laskar", "Naeemul Hassan", "Firoj Alam"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable success as general-purpose task solvers across various fields, including NLP, healthcare, finance, and law. However, their capabilities remain limited when addressing domain-specific problems, particularly in downstream NLP tasks. Research has shown that models fine-tuned on instruction-based downstream NLP datasets outperform those that are not fine-tuned. While most efforts in this area have primarily focused on resource-rich languages like English and broad domains, little attention has been given to multilingual settings and specific domains. To address this gap, this study focuses on developing a specialized LLM, LlamaLens, for analyzing news and social media content in a multilingual context. To the best of our knowledge, this is the first attempt to tackle both domain specificity and multilinguality, with a particular focus on news and social media. Our experimental setup includes 19 tasks, represented by 52 datasets covering Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the current state-of-the-art (SOTA) on 16 testing sets, and achieves comparable performance on 10 sets. We make the models and resources publicly available for the research community.", "sections": [{"title": "1 Introduction", "content": "LLMs have significantly advanced the field of AI, demonstrating capabilities in solving downstream NLP tasks and exhibiting knowledge understanding and cognitive abilities (Touvron et al., 2023; Mousi et al., 2024). However, extending these capabilities with more domain-specific knowledge and achieving higher accuracy requires domain specialization. This entails customizing general-purpose LLMs with domain-specific data, augmented by relevant domain knowledge (Ling et al., 2023).\nOne prominent area where LLMs can be customized with specialized knowledge is the news and social media analysis. Since their emergence, the use of AI in news production, analysis, video scripting, copyediting, translation, and dissemination has grown significantly (Shi and Sun, 2024; Simon, 2024). In addition to news production and dissemination, a closely related area is social media content analysis (Zeng et al., 2024; Liu et al., 2024; Franco et al., 2023; Hasanain et al., 2024b). This growing range of applications creates a strong demand for specialized LLMs to support journalists, fact-checkers, communication specialists and social media users.\nThere has been an attempt to develop a tool2 based on ChatGPT to support journalists in news production and delivery (Hireche et al., 2023); however, it relies solely on ChatGPT for language understanding and response generation in response to reporters' queries. Other efforts to support journalists include tools for creating news"}, {"title": "2 Related Work", "content": "Recent studies have explored the intersection of LLMs, journalism, and social media, shedding light on both the opportunities and challenges of integrating AI into news reporting (Cheng, 2024; Petridis et al., 2023; Hasan et al., 2024; Quinonez and Meij, 2024; Nishal and Diakopoulos, 2024; Ding et al., 2023). For example, Brigham et al. (2024) and Breazu and Katsos (2024) examined the use of LLMs like GPT-4 in journalistic workflows, focusing on the ethical, and quality implications and generating narratives. LLMs have also been integrated in news production, focusing on its benefits and ethical challenges (Shi and Sun, 2024). One key challenge of using generic LLMs in journalism is their tendency to generate false or misleading information (Cheng, 2024; Augenstein et al., 2023), a phenomenon known as hallucination. Nishal and Diakopoulos (2024) also highlighted key concerns, including hallucinations, factual inaccuracies, and the potential threat to journalistic objectivity. To address these issues, they proposed a value-sensitive design approach, advocating for AI systems that offer transparent explanations, explicitly represent uncertainty, and give journalists more control over the generated content.\nBloomberg has integrated LLMs into its news production processes (Quinonez and Meij, 2024), aiming to enhance automation while preserving essential journalistic principles such as accuracy and transparency. Similarly, Ding et al. (2023) examined the role of LLMs in human-AI collaboration, particularly for generating news headlines. To tackle content creation challenges on visual platforms like Instagram Reels and TikTok, Reel-Framer, a multimodal writing assistant, was developed (Nickerson et al., 2023). Additionally, Cheng (2024) emphasized the need for customized LLMs tailored to news reporting, proposing solutions like supervised fine-tuning and constitutional AI, which integrates reinforcement learning from AI feedback to combat misinformation and rebuild reader trust. To facilitate science journalism, Jiang et al. (2024) introduced a novel approach that leverages collaboration among multiple LLMs to improve the readability and clarity of news articles."}, {"title": "2.2 News and Social Media Analysis", "content": "For news and social media analysis there has been research effort with a special focus on fact- checking and harmful content detection (Lee et al., 2024; Quelle and Bovet, 2024), and news reliability classification (Ibrahim, 2024). Quelle and Bovet (2024) demonstrated that LLM agents can be employed for fact-checking by retrieving relevant evidence and verifying claims. Similarly, Ibrahim (2024) explored fine-tuned LLMs, such as Llama-3, to automate the classification of reliable versus unreliable news articles, specifically in Spanish.\nThe Enhancing Perception (Hsu et al., 2024) and FACT-GPT (Choi and Ferrara, 2024) frameworks tackle misinformation by refining fake news explanations through a conversational refinement approach. Similarly, VerMouth (Russo et al., 2023) automates social media fact-checking, contributing to broader efforts in combating misinformation. Additionally, the expert recommendation framework (Zhang et al., 2024b) leverages a multi-layer ranking system with LLMs, balancing reliability, diversity, and comprehensiveness when suggesting experts for news events.\nOther initiatives include Botlitica (Musi et al., 2024), which identifies propagandistic content in political social media posts, and JSDRV (Yang et al., 2024), which focuses on stance detection and rumor verification. In the realm of investigative journalism, Ali (2024) introduced a tool to retrieve and summarize relevant documents, while Alonso del Barrio et al. (2024) focused on detecting framing in television program content. Addressing political bias, Trhlik and Stenetorp (2024) explored bias identification using LLMs. Additionally, Alonso del Barrio et al. (2024) proposed prompt-engineering LLMs to analyze framing in spoken content from television programs. A comprehensive study was conducted by (Zeng et al., 2024), highlighting the use of LLMs in social media applications."}, {"title": "2.3 Specialized LLMs", "content": "Ling et al. (2023) highlighted the importance of developing specialized models for several reasons. One key reason is that, much like humans, acquiring domain expertise and capabilities often requires years of training and experience. Therefore, it is important to train LLMs with domain knowledge to serve professional level usage.\nIn this direction, a recent work by Kotitsas et al. (2024) explored fine-tuning LLMs to improve claim detection. Bao et al. (2024) trained an LLM, FLLM, using curated knowledge with a focus on the business and media domains. For training, they utilized articles published by Fortune Media. The OpenFactCheck framework (Wang et al., 2024d) tackles the evaluation of factual accuracy in LLM-generated content. This customizable architecture enables the assessment of both LLM factuality and fact-checking systems, promoting standardized evaluations essential for advancing research on the reliability and factual correctness of LLMs. Wang et al. (2024a) proposed an explainable fake news detection framework that uses LLMs to generate and evaluate justifications from opposing parties. A defense-based inference module then determines veracity, improving detection accuracy and justification quality, as demonstrated on two benchmarks.\nIn contrast to prior work, our research focuses on developing a specialized model for a wide range of tasks and capabilities for news and social media analysis, representing the first effort in this direction with multilingual capabilities."}, {"title": "3 Tasks and Datasets", "content": ""}, {"title": "3.1 Data Curation", "content": "For dataset curation, we selected key capabilities and their associated tasks, as illustrated in Figure 1, and identified publicly available datasets aligned with these tasks. The choice of languages was influenced by their prominence in the Arabian Peninsula region, making them essential for analyzing linguistic and cultural diversity in the context of social and news media in the region. The initial collection consists of 103 datasets, some of which we excluded due to their different versions (e.g., we selected ArSarcasm-v2 (Abu Farha et al., 2021) instead of version 1). After initial pre-selection, the resulting collection consists of 52 datasets as detailed in Tables 3, 4 and 5 in Appendix B.\nThe datasets span various sources, including social media posts, news articles, political debates and transcripts. It consists of ~2.7m samples and a total of 222 labels, reflecting the complexity of tasks such as checkworthiness, claim detection, cyberbullying, emotion detection, news categorization, and more."}, {"title": "3.2 Preprocessing", "content": "After collecting the datasets, we observed that while most were pre-divided into training, development, and test sets, 18 datasets lacked these splits. In cases where datasets were not pre-split, we partitioned them into 70% for training, 20% for testing, and 10% for development. For datasets containing only training and test sets, we further divided the training set, allocating 30% for development. To preserve the class distribution across splits, we employed stratified sampling.\nWe applied several other preprocessing steps such as removing duplicates, unifying labels (e.g., check-worthiness to checkworthiness, fixing uppercase to lowercase), and removed entries with less than 3 letters. These preprocessing steps ensured that the datasets were clean, well-structured, and ready for subsequent analysis or model training.\nAfter preprocessing, we obtained a total of 1.2m, 1.4m and 0.14m samples for Arabic, English, and Hindi, respectively. The number of labels in the datasets ranges from 2 to 42. The datasets also include both multiclass and multilabel setups. We provide distribution of the datasets, number of labels and their splits in Tables 3, 4 and 5 in Appendix B. The datasets come in different sizes, ranging from the smallest (e.g., CT-24 subjectivity) to the largest (e.g., English news summarization dataset), and with varying label distributions, from skewed (e.g., propaganda) to more balanced (e.g., Arabic CT22 claim detection). In Appendix A, we provide detailed descriptions of the tasks and datasets."}, {"title": "4 Methodology", "content": "In Figure 2, we present the methodological steps involved in the development of LlamaLens, which is also formalized in Algorithm 1. In Section 3, we discussed the details of the dataset curation and preprocessing. The following subsections discuss the remaining steps in the development process."}, {"title": "4.1 Instruction Dataset", "content": "Our approach follows the typical and effective pipeline of aligning LLMs with user intentions and tasks by LLM fine-tuning on representative data. Such approach usually involves creating instruction datasets starting from existing NLP datasets (Longpre et al., 2023). An instruction sample is a triplet of a natural language instruction describing the task, an optional input, and an output that represents the answer following the instruction (Wang et al., 2023).\nNatural language instructions There are several potential techniques to create the needed natural language instructions, including manual and automatic approaches. As instructions diversity positively affects model performance and generalization (Dubois et al., 2024; Pang et al., 2024; Zhang et al., 2024a), we aim to create a diverse instruction dataset. Due to the scale of tasks and datasets of focus in this work, creating such a diverse set manually is time-consuming and can lead to limited instruction styles (Wang et al., 2022; Kaur et al., 2024). We opt to automatically generate instructions by prompting two highly-effective closed LLMS, GPT-40 3 and Claude-3.5 Sonnet, 4 to generate a diverse set of 105 English instructions per LLM, resulting in 20 instructions per dataset. To ensure the models generate instructions fitting our datasets, we provide the models with the datasets metadata, including dataset name, language, task, task definition and labels space. Exact prompt used to generate instructions and examples of generated instructions can be found in Appendix C. It is important to note that we only require English instructions, as a previous study demonstrated that English prompts outperform language-specific native prompts (Kmainasi et al., 2024).\nFinally, for each input dataset of the 52, we create instructions by appending a randomly selected natural instruction, of the generated 20, per example of each training subset. This guarantees versatile instruction styles even for the same input dataset. Our final instruction tuning dataset is the set of the prepared instructions for all datasets."}, {"title": "4.2 Training", "content": "Our experiments are based on the most effective open LLM to-date, Llama 3.1, which shows effective performance even in multilingual setups (Dubey et al., 2024). Fine-tuning larger scales of the model (e.g., 70B version) holds a great overhead in terms of time and computational cost. Moreover, these models might be inaccessible to the greater research community. Thus, we opt to focus on the smallest Llama 3.1-8B version. We particularly base our LlamaLens model on the instruct version of Llama 3.1-8B as it is already aligned to several user tasks."}, {"title": "4.2.1 Training Setups", "content": "We instruction-tune Llama 3.1-8B-Instruct following different setups. Given that fine-tuning LLMs typically requires substantial computational resources, making it a time-consuming and resource-intensive process, therefore, to address this challenge, our main LlamaLens model is fine-tuned in full precision (16-bit) following parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA) (Hu et al., 2021). In addition to the full precision model, we aimed to train smaller models, achieving two goals: (i) release smaller but effective models to be used in resource-constrained environments, and (ii) efficiently investigate the effects of some parameter settings on model performance, to guide the full model training. Thus, we also fine-tune the original Llama-8B-Instruct model employing QLoRA (Dettmers et al., 2024), which involves quantization of the model's weights and significantly enhances memory optimization, while maintaining acceptable performance."}, {"title": "4.2.2 Experimental Setup", "content": "Datasets Sampling Our experimental dataset covers 52 distinct datasets. As explained in Section 4.1, we aimed to create a diverse instruction dataset starting from these NLP datasets. Given the substantial size of some of these datasets, for example, the Arabic hate speech dataset contains 0.2M samples, and to ensure versatility, we pragmatically set a threshold of 20K training instances per dataset. For datasets exceeding this limit, we employed stratified sampling to preserve the original distribution of the dataset labels. Our final training dataset includes 0.6M samples out of 1.96M. We will release the complete training set of instructions for future studies.\nDataset Shuffling The order of instructions in the training dataset can significantly impact model performance. For example, Pang et al. (2024) demonstrate that ordering instructions by complexity influences the effectiveness of tuned models. In light of this, we investigate how different orders of samples affect the performance of LlamaLens, employing four different data shuffling and ordering techniques to identify the optimal sequence. Although there are several possible ordering configurations, however, we limit the scope of this study to the effects of language and task order.\n1. Alphabetically ordered: This is a basic setup where languages and datasets are ordered alphabetically\u2014Arabic, followed by English, and then Hindi\u2014without shuffling.\n2. Shuffled by language: Randomly shuffle datasets while preserving the order of languages.\n3. Shuffled by task: Tasks are organized alphabetically, with datasets shuffled across tasks regardless of language.\n4. Fully randomized: Complete randomization of the training dataset points.\nParameters Setup For all models we train, we set a LoRA learning rate of 2e-4 alongside a linear learning rate schedule. Optimization was performed using AdamW (Loshchilov and Hutter, 2017), with a batch size of 16. All experiments were executed on four NVIDIA H100-80GB GPUs.\nIn the first set of experiments, we trained four models quantized to 4-bit precision using QLoRA. Although the models store weights in 4-bit format, computations are performed in BFLOAT16 (bf16), with both the LORA rank and a set to 16. Each model was trained on one of the dataset shuffling configurations. After identifying the optimal order of the training dataset-based on average model performance on our test sets, as shown in Figure 3-we used that dataset order to fine-tune our LlamaLens model in full precision (16-bit). Due to the scale of the model and training set size, we train the model for two epochs, increasing LoRA rank to 512, following the recommendations in (Xin et al., 2024), which suggests that higher ranks yield improved performance for multitask learning. LoRA a was set equal to the rank."}, {"title": "4.3 Evaluation", "content": ""}, {"title": "4.4 Evaluation Process", "content": "For evaluation, we employed a zero-shot approach, in which we directly prompted the models (original and fine-tuned) to perform tasks from the testing sets. The employed natural instruction/prompt is the first generated instruction (Section 4.1) for each dataset. To ensure reproducibility, we set the top_p to 1 and the temperature to 0. For classification tasks, we restrict the models to generate a maximum of 20 new tokens. For summarization, we applied a stopping criterion based on encountering a period ('.') or the end-of-sequence (eos_token)."}, {"title": "4.5 Post-processing", "content": "As models can generate text beyond that required in the instruction, a post-processing method was implemented to extract labels from the generated models' responses. Initially, a regular expression was used to accurately identify and extract the labels. Several transformations were also applied, including converting all text to lowercase, removing special characters, and addressing code-switching by replacing non-Latin characters with their Latin equivalents. A similar post-processing approach was reported by Abdelali et al. (2024)."}, {"title": "4.6 Evaluation Metrics", "content": "All models were evaluated using standard classification metrics including weighted-, macro-, micro-F1 and accuracy. We use ROUGE-2 to evaluate the summarization task. Specifically, we use the same metric reported in state-of-the-art (SOTA) per dataset."}, {"title": "5 Results and Discussions", "content": ""}, {"title": "5.1 Data Shuffling Effect", "content": "In Figure 3, we present the results as averages computed across datasets and languages. Detailed results are provided in Table 2. Among the four different ordering strategies, shuffling by task outperformed other configurations. Both shuffling by language and alphabetic ordering showed comparable performance. Based on this experiment, we elect to train our LlamaLens model following the shuffling by task data ordering approach."}, {"title": "5.2 Results on LlamaLens", "content": "In Table 1, we report the full results of our LlamaLens model across the different languages. Overall, LlamaLens significantly outperforms the Llama-instruct with an average improvement of 33%. Language wise LlamaLens outperforms Llama-instruct on average 32.5%, 32.2% and 29.5% for Arabic, English and Hindi, respectively. For Arabic, out of 26 tasks, LlamaLens only underperforms on cyberbullying, sarcasm and one of the hate-speech datasets. For English, out of 17 tasks LlamaLens outperforms on 14 tasks and lower performing tasks include sentiment, subjectivity and news categorization. For Hindi, in all 8 datasets LlamaLens outperforms Llama-instruct.\nIn comparison to SOTA's average performance of 0.75, LlamaLens average performance is 0.69. We should note here that since 18 out of the 52 datasets were not pre-split (Section 3.2), SOTA on these datasets is not directly comparable to our model, as the testing splits might be different. Computing average performance excluding these datasets reduces the gap, with a SOTA of 0.72 and LlamaLens performance of 0.67.\nIn terms of number of datasets where LlamaLens improved over SOTA, we find that it outperforms SOTA in 16 test sets, and has a comparable performance (difference > 0 and <-0.03) in 10 other testing sets.\nDataset-specific improvements for English include tasks such as checkworthiness, emotion, news factuality, and propaganda. For Arabic, improvements have been observed in checkworthiness, emotion, and stance, while performance on news categorization is mixed, with improvements seen in 2 out of 5 datasets. For Hindi, significant improvements have been observed in hate speech, followed by NLI and offensive language."}, {"title": "5.3 Error Analysis of Llama-instruct Model", "content": "We conducted an error analysis on the responses of the Llama-instruct model, to identify challenges in handling text of low-resource languages on certain classification tasks. Several common issues emerged across these models, particularly in tasks related to offensive language, and hate-speech detection, factuality, and news categorization.\nOne recurring problem was the inability of the model to provide labels in numerous instances, often responding with phrases like \u201cI cannot provide a label\" or \"Arabic text is not easily classifiable into categories without context or translation.\u201d\nAnother observed issue is language confusion in output. Although the models were instructed to output labels exclusively in English, they occasionally returned labels in Arabic or code-switched responses, which is inline with language confusion reported in relevant studies (Marchisio et al., 2024), however, differently, we showcase the confusion can occur at the smallest unit of a single character. For instance, in some cases, the model generated outputs like \"actual\" (referring to \"factual\" where Arabic character is a transliteration of character \u201cf\u201d), and \u201c\u0633\u0628\u0648\u0631\u201d (Korean for \u201csport\u201d transliterated as \u201cseupocheu\u201d where character \u0633 is a transliteration of character \"s\") despite no instructions involving Korean language. This highlights a phonetic-level code-switching phenomenon. It also occurred on longer sequences such as the model responding with \"\u0633\u0627\u0631castic instead of \u201cSarcastic,\u201d where \"\u0633\u0631\" is actually pronounced similarly to \u201cSar\u201d. In contrast, our fine-tuned versions of the model do not display such issues, with the models adhering more consistently to the instructed output"}, {"title": "6 Conclusion and Future Work", "content": "In this study, we propose a specialized model, LlamaLens, focused on news and social media analysis, designed to assist journalists, fact-checkers, and social media analysts. We curated 52 datasets spanning three prominent languages spoken in the Arabian Peninsula region: Arabic, English, and Hindi. To develop LlamaLens, we created an instruction-following dataset based on these curated datasets and fine-tuned the Llama 3.1-8B-Instruct model. Our experiments show that LlamaLens outperforms the SOTA on 16 datasets, performs comparably on 10 datasets, and underperforms on the rest of the datasets. However, on average, LlamaLens and its quantized versions significantly surpasses the Llama-instruct model. Our findings from error analysis suggests that it is important to inject specialized domain and language knowledge to obtain the desired outcome from Llama-instruct. Our future studies include experimenting with different rank orders and focusing on quantized version of the model to make it usable in a low-resource settings."}, {"title": "7 Limitations", "content": "Our experiments were focused on a single open LLM, further LLMs can be explored. The training datasets had a bigger representation of Arabic, but as experiments showed, the proposed model improved performance even on other languages. Further examination of the effect of training examples selection and shuffling techniques is needed to understand these effects on the model performance."}, {"title": "Ethics and Broader Impact", "content": "Our experiments were conducted on training datasets publicly released to the research community. We adhered to the licenses associated with them whenever available. Some of the data points we will be releasing as part of our instruction dataset contain vulgar, offensive, or disturbing content which is a natural occurrence on social media, thus caution is recommended for users of our dataset. The models and instruction dataset we release could be invaluable to news agencies, journalists, and social media platforms. However, we encourage developers and users of the models to be critical of their usage."}]}