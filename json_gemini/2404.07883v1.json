{"title": "Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors", "authors": ["Glen Smith", "Adit Gupta", "Chris MacLellan"], "abstract": "Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB), a platform that simplifies tutor creation and personalization. Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors' underlying Al agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design recommendations for our platform and others that utilize interactive Al agents for tutor creation and customization.", "sections": [{"title": "1 INTRODUCTION", "content": "Intelligent tutoring systems (ITSs) are an educational technology that provide students with course content, guided-practice problems, and performance feedback [25]. This kind of tutored support has been shown to be effective at improving students' learning outcomes [15]. ITSs can track a student's knowledge acquisition and progression and select personalized problem sequences to optimize the student's learning. These tutors can be deployed to many students at once, offering a scalable solution to issues of high class volumes and needs for supplemental learning to name a few.\nHowever, several limitations prohibit the wide-spread use and applicability of ITSs. One limitation is that tutor development often requires specialized programming and tutor design knowledge [21]. In particular, expert models that can evaluate learners as they solve problems must be authored for each tutor, an often cumbersome and tedious task. This prohibits non-technical users from creating and customizing their own tutors [9] and limits tutor creation to those with the required knowledge, such as academic researchers and UI and software developers. Another limitation is that development is time-consuming [21]. Previous work estimates that for well-established tutor authoring methods such as model-tracing it takes approximately 200-300 hours of tutor development time to produce just 1 hour of instruction time [26]. Other approaches such as"}, {"title": "2 RELATED WORKS", "content": "Aleven, et. al. (2019) describes a six step process for building example-tracing intelligent tutors [3]. While these steps are specific to example-tracing, some of these concepts are general to building any tutor. First, the tutor author investigates student thinking and learning in the given task domain using cognitive task analysis [13]. Based on insights from this analysis, the author designs a tutor interface for a specific problem type. Authors break down complex problems into simpler steps. Finally, the author creates an expert model to power the tutor. Recent authoring tools let authors use programming by demonstration to achieve this [1, 20].\nWe are particularly interested in a specific type of intelligent intelligent tutor, known as a cognitive tutor [5]. Cognitive tutors often include capabilities for individualized problem selection, on-demand hints, and real-time correctness feedback [5]. These capabilities, while difficult to program, are correlated with successful student learning outcomes [10]. Building cognitive tutors is not trivial, requiring extensive time and expertise to program an interface for each tutor type and production rules for the cognitive model. These high costs have led researchers to investigate how we might effectively and efficiently build intelligent tutors with less need for specialized skills.\nIntelligent tutor authoring tools aim to achieve this by providing a mechanism for building tutor interfaces and authoring expert models without programming. Recent work on ITS uses programming by demonstration techniques to let users author expert models by providing correctness feedback and demonstrations to build expert models. This approach lets users achieve model completeness in less time than exiting authoring approaches such as CTAT's example tracing or SimStudent. There have been a plethora of authoring systems that reduce the time to build intelligent tutors. However, a key gap has been that these systems rarely utilize AI, and those that do have not bee tested with real end-users [16, 26]."}, {"title": "2.1 Building the Tutor Interface", "content": "Building a tutor interface is an important part of the intelligent tutor, however, there is little prior research which reviews and compares the different types of tutor interfaces. Tutor authoring tools which come with an interface builder can be bucketed into a few categories including drag-and-drop systems, templates interface builders, and more recently, chat based. While this is not an exhaustive review of all types of interfaces, these three are the most common amongst tutor builders.\nAuthoring tools such as CTAT, utilizes a drag-and-drop interface builder which allows users to drag specific components into a fixed location pane to create interfaces. While drag-and-drop interfaces are intuitive, a key issue with drag and drop functionality and fixed location components within CTAT is that the interfaces are inflexible to slight variations of the problem type.\nOther systems like ASSISTments use a template based approach to create tutor interfaces [8]. ASSISTments allows users to utilize text boxes, multiple choice, and drop-down components, through a web-based interface design experience. Template-Based Tools, rely on predefined templates, making them user-friendly but less adaptable to diverse domains or question types. While they can emulate problem-solving steps for specific scenarios, they limit users to the question structure within the template, often accommodating only a single question [23].\nIn recent years, generative artificial intelligence (GAI) tools are diffusing rapidly. Educational institutions and EdTech companies are swiftly formulating strategies to harness the power of artificial intelligence (AI), particularly focusing on the effective utilization of GAI tools. This includes the incorporation of large language models (LLMs), such as ChatGPT. A key motivation for the use of GAI within educational institutions, is that their chat experience aims to increase engagement, which is correlated with retention [11]. Studies indicate that heightened student engagement is vital"}, {"title": "2.2 Authoring Expert Models", "content": "Building a cognitive model for a specific tutor interface can be challenging, given the complexity of the problem type. Constructing a cognitive model through programming can be laborious, necessitating 200-300 hours of programming for each hour of tutor instruction. Tutor authoring tools aim to reduce the time required to construct tutors by employing techniques such as programming by demonstration to facilitate the building of expert models. CTAT utilizes a process called example-tracing, which enables users to create ITS content and expert models without the necessity of programming. Example-tracing simplifies the process by allowing individuals to demonstrate problems, with the correct actions being recorded in a behavior graph. This graph encapsulates knowledge in the form of a node-link structure. However, a limitation of this method is that it requires the demonstration of each possible solution to a problem, potentially leading to an incomplete model if the user is not exhaustive in their demonstrations. CTAT attempts to mitigate this with mass production techniques, yet this approach remains time-consuming and necessitates the use of Excel spreadsheets, presenting a substantial learning curve for novice users.\nRecent advancements in CTAT have introduced the use of interactive machine learning, alongside programming by demonstration, to construct expert models more efficiently [19]. This innovation lets authors leverage demonstrations and correctness feedback to develop comprehensive expert models that generalize beyond the initial examples used to train the system. While this approach to interactive machine learning represents a novel application within the realm of tutor authoring tools, an existing gap, which we aim to bridge, is the deployment of authoring tools that utilize interactive Al techniques to actual users, such as teachers."}, {"title": "3 SYSTEM DESCRIPTION", "content": "To help inform the design of Apprentice Tutor Builder (ATB), we conducted preliminary needs finding with teachers at a state technical college. We deployed four mathematics tutors to College Algebra class sections and following the deployment, we conducted several focus group sessions to collect feedback on teacher and student experiences on working with the system. We found that among instructors there was a significant preference for utilizing tutors whose content aligns with their course syllabus. Furthermore, every instructor has a unique teaching style, which can pose a challenge when attempting to align the capabilities of an intelligent tutor with their individual teaching methods.\nIn this section, we describe the components of ATB, which take inspiration from CTAT [9], Apprentice tutors [16], and ASSISTment tutors [23]. We extend the ideas and theories of these systems to create a novel approach of building intelligent tutors. This system allows users to author tutors and tutor content via two components: the tutor builder and apprentice agent training module."}, {"title": "3.1 Tutor Builder", "content": "The tutor builder draws inspiration from intuitive drag-and-drop builders, such as CTAT [9], and form-based builders like ASSISTments tutor [23]. Our primary objective in developing the tutor builder was to empower non-technical users to create pedagogically robust tutors. We facilitate this through an intuitive user interface that offers users the flexibility to design diverse and stylistically varied interfaces.\nFigure 1 shows the empty builder interface (on the top) juxtaposed with an authored tutor interface (on the bottom). The interface builder utilizes the spring-and-struts layout approach [24], which lets users design their tutors in nested rows and columns. Users can drag elements such as rows, columns, input fields and labels into the designated pane on the left (component 1). Additionally, users can nest rows inside of columns and vice versa to create structured designs that auto-align interface elements. The"}, {"title": "3.2 Apprentice Agent Training", "content": "Underlying each tutor is an apprentice agent that users must train. In this section, we describe the process by which users interact with an agent to teach it how to solve tutor problems. We follow this with a description of the agent architecture in terms of its knowledge representation, learning, and performance components."}, {"title": "3.2.1 Authoring Experience.", "content": "During agent training, the user authors the expert model by providing problems and engaging with the agent via a series of interactions. First, the user initializes the problem by supplying the initial values needed to solve the problem. For example, to teach an agent how to add or multiply two numbers, a user might initialize the problem by entering the two operands and the operator into the interface. Once the problem is set up, the user clicks \"Start Problem\" which begins agent training.\nThe current state of the tutor-containing all values and the fields in which they appear-is sent to the agent2. shows how the tutor state is represented to the agent. At first, the agent does not have knowledge of how to solve the problem and asks for a demonstration. The user provides a demonstration by completing the next step directly in the interface. Next, the agent asks for a task label for the demonstration. These labels default to the unique identifier of the field but providing a label lets the agent to provide more meaningful and interpretable explanations later. Another benefit is that we can use these user-supplied task labels when evaluating student performance data to engage in labeling of knowledge components for knowledge tracing.\nGiven the demonstration and task label, the agent updates its knowledge. If the agent does not have sufficient knowledge to attempt the next step, it asks for another demonstration and the process continues. If the agent is able to attempt the next step, it will automatically complete it by filling in the respective field. The agent then asks \"Did I take the correct action?\" and requests yes/no feedback from the user. To aid the user in understanding its reasoning, the agent provides a text-based explanation of the steps it is taking and highlights which fields it is using to generate the result. If the user clicks \"yes\", the agent will move on to attempt the next step. If the user clicks \"no\", the agent removes its response and if it is able, attempts the step again, applying a different response. If no response is deemed correct by the user, the agent requests a demonstration. When the problem is complete, the user clicks \"Done\" and the interface is reset to let the user continue training by providing additional problems. This process continues until the user is satisfied that the agent can solve the problems fully and correctly."}, {"title": "3.2.2 Knowledge Representation.", "content": "The apprentice agent utilizes two main knowledge structures: a Hierarchical Task Network (HTN) [6] used to perform planning when solving problems and a working memory which consists of facts about the tutor interface. HTNs are a knowledge framework for representing and solving complex problems by breaking down tasks into hierarchically organized subtasks [6] and ultimately into operators it can execute. HTNs consist of a set of tasks to be completed, a set of methods that achieve those tasks, and a set of operators that represent the agent's fundamental knowledge. The \"network\" is represented as an AND-OR tree, where to solve a task, only one valid method of that task needs to be chosen (OR), and each method contains a set of subtasks which must all be completed to successfully complete that method (AND).\nIn ATB, tasks are the problem step labels supplied by the user during training. Methods represent the various ways to solve a problem. For example, on a fractions arithmetic task, there may exist a high-level task \"Solve Problem\", for which there may be two methods AddFractions and MultiplyFractions. To determine when a method is applicable, methods contain conditions which must match the current tutor state. Continuing our example, one might apply AddFractions when there exists a plus sign (+) in the tutor and one might apply MultiplyFractions when there exists a multiplication symbol (cross) in the tutor. These conditions are acquired and updated during the HTN learning process and in"}, {"title": "3.2.3 Performance Component.", "content": "To solve problems, the agent is given a task along with the tutor state and utilizes its HTN to plan and generate a response to the tutor interface. Here, tasks correspond to labels users provided during learning (See Learning Component). When a task and tutor state is observed, the agent first converts the state into facts (figure 2), working memory objects representing fields in the interface, and uses a rete network [?] to perform relational inference and produce new facts that are added to the working memory. Next, the rete network is used to match the current facts in the working memory to an applicable method that satisfies the current task. If a matching method is found, the agent will select that method and then subsequently attempt to solve all subtasks by finding matching methods that solve each subtask. Once an operator is encountered in the decomposition, it is executed and its effects are added to the working memory. If the operator produces a value for the interface, this value is returned as the solution to the next step in the problem. At any point during planning, if no matching method is found for a task, the agent returns False, indicating that it cannot produce a solution given the current state.\nTo make this process more concrete, figure 3 shows a decomposition of a fraction multiplication problem. figure 2 shows the initial working memory and figure3 shows the HTN decomposition. First, the MultiplyFractions method is selected because the state matches the condition requiring a multiplication symbol to be present in the state. This method decomposes into three subtasks, MultiplyNumerators, MultiplyDenominators, and ClickDone, which must all be solved to complete the parent method. Each subtask has one method of the same name that decomposes finally into two operators, Multiply and InputValue, where the former multiplies the values of the numerators and the latter takes the result and generates a return value to the interface. Finally, the HTN planner"}, {"title": "3.2.4 Learning Component.", "content": "The agent learns from demonstrations, feedback, and labels interactively provided by the user. Learning new methods consists of two parts: generating the subtask decompositions and iteratively adjusting their conditions. We discuss each in turn.\nInitially the agent lacks any methods for solving tasks and only contains pre-authored operators. When the tutor state is observed, planning occurs similar to the performance component, however when no matching method is found, the agent requests a demonstration from the user. Upon receiving a demonstration, the agent must then explain the observed value. To generate an explanation, the rete network iteratively matches facts to each applicable operator and executes them to generate changes to the working memory. This continues until either the rule trace provides an explanation for the demonstration, or the maximum depth parameter is reached. If the agent finds an explanation, it constructs a new method where its subtasks correspond to the sequence of operators found to generate the user's demonstration. Alternatively, if the maximum depth parameter is reached, the agent simply creates a new method whose output is the given demonstration. This new method functions as a memorized operator. It only applies to the current tutor state and always outputs the same value\u00b9. In both cases, the conditions of the method are the conjunction of facts in the working memory and the method is then categorized in the HTN under the task name provided by the user.\nAlthough demonstrations help the agent learn new methods, the agent also utilizes feedback from the user to determine when to apply learned methods. During training, when the agent attempts to solve the next step of a problem, it prompts the user: \"Did I take the correct action?\" and requests a Yes/No response. To aid the user in providing accurate feedback, the agent also provides a text-based explanation of its solutions as well as highlights the fields used in its computation (see Figure 4, frame C). If the user selects \"Yes\", the agent simply continues to attempt the next step. If the user selects \"No\", the agent continues planning to locate another applicable method to solve the next step, if one exists. If not, the agent requests a demonstration for that step and engages in the explanation process described in the previous paragraph. When a new method is created, it is compared against all other methods for the current problem step (or task). If any two methods are found to have the same subtask decomposition, their conditions can be generalized to create a single method. Conditions are generalized using least-general generalization [22], employing the anti-unification algorithm to variablize values that differ from one set of conditions to another, leaving all others as constant. Updating the conditions in this way ensure that the method is applicable for future tutor states similar to those the method was created for or was previously correctly applied to. As will be seen in the study results section, methods and their conditions can converge after a small number of demonstrations and feedback from the user."}, {"title": "4 USER STUDIES AND EVALUATION", "content": "We performed a user study to evaluate the effectiveness and usability of ATB. Participants were asked to complete two tasks. Each task consisted of two components: (1) using the tutor interface builder to create a tutor and (2) authoring an expert model for the tutor by interactively providing demonstrations, feedback, and labels to the AI agent. We followed these activities with a semi-structured interview designed to gauge user experience with and reactions to the system as well as to help inform future design choices and design recommendations for tutor authoring systems like ATB."}, {"title": "4.1 Participants", "content": "For this study, we recruited 14 participants (ages 18-54). We targeted participants with some degree of teaching experience, with only one participant having none. We also asked participants to gauge their experience levels in three other categories: Programming/Coding, Mathematics, and Artificial Intelligence. Additionally, of those with teaching experience, we recorded which levels they have taught, ranging from Primary School to Post-Secondary/Trade school and Government/Private Industry. These breakdowns are reflected in Figure 5."}, {"title": "4.2 Methods", "content": "Before each session, we obtained consent from each participant and afterward provided a compensation of $15/hour rounded up to the nearest half hour. At the beginning of the session, we collected demographic data such as age group, teaching, programming, math, and Al experience, as well as instruction levels taught (see Figure 5). Next, we provided a 3-minute video tutorial. The video demonstrated both tutor interface building using the drag-and-drop tool and expert model authoring using the agent training interface and side panel. We allowed the participant to ask any questions at this stage to clarify the overall session structure as well as how to use the various components of the system.\nIn the first task, participants were asked to use the tutor builder to create a Fractions Arithmetic tutor (see Figure 4, part B). We then asked participants to teach the agent both fraction multiplication and fraction addition with same denominators. Users were asked to initialize each problem by providing four values for the two fraction operands as well as a plus (+) or multiplication (x) sign as the operator. Once the problem was initialized, users would click \"Start Problem\" to begin agent training."}, {"title": "4.3 Results", "content": "All 14 participants were able to fully recreate both tutors using the tutor builder and 13 of out 14 were able to author expert models that could solve both tasks. We present results in the following subsections, broken up by analysis technique."}, {"title": "4.3.1 User Log Analysis.", "content": "Informed by prior work [12], we grouped participants into two groups based on reported programming experience levels, those with little to no experience and those with moderate to high experience, or the \"Low\" and \"High\" group, respectively. Because the primary audience for our system is users with less technical expertise, determining the relationship between experience level and usage patterns allows us to understand how expertise in these areas relates to the usability our tool. Figure 6 shows the average authoring time for both the interface builder and agent training components of ATB for each tutoring task. Overall authoring time is higher for the Fractions Arithmetic (FR) task (Avg: 11.85 min) compared to the Square 25 (S25) task (Avg: 7.12 min)2. For the FR tutor, those in the Low group spent slightly more time building the interface than the High group (Avg: 6.2 min vs. 5.5 min), but spent significantly less time training the expert model than the High group (Avg. 10.9 min vs. 16 min). For the S25 tutor, the Low group spent a similar amount of time building the interface as the FR tutor while the High group spent almost half the time of the Low group on interface building (Avg: 6.1 min vs. 3.2 min). Finally, in contrast to the FR tutor, the Low group spent more time training the expert model than the High group (Avg: 7 min vs. 4.2 min).\nTo explore how well the expert models were trained, we tested the models for each tutor to measure model correctness. We define model correctness (or accuracy) as the successful solve rate of 20 randomly generated problems for the FR expert model, 10 for multiplication and 10 for addition, and 10 randomly generated problems for the S25 expert model. All expert models were tested with the same 30 problems. All participants in the High group (moderate to high exp.) were able to author a model with 100% model correctness for both tasks. However, for one participant in the Low group, the model correctness rate decreased to 20% when solving fraction multiplication problems in the FR tutor. All participants in both groups achieved 100% model correctness for the S25 tutor.\nNext, we examined the number of validation problems completed by each group. We define a validation problem as any additional problem used to train the agent after it has demonstrated the ability to fully solve a problem. Thus, if an agent fully solves a problem, and the user provides two more additional problems for the agent to solve, the number of validation problems is 2. Examining the number of validation problems provided by users allows us to determine how users' prior experience correlates with their ability to author a more correct model. For the Low group, the average number of validation problems increased from 0.33 to 1.0 when going from the FR tutor to the S25 tutor. In contrast, this number decreased from 2.25 to 0.15 for the High group. We hypothesize that those with more programming experience may initially be more cautious when training, but may more quickly develop an intuition for when the agent is sufficiently trained."}, {"title": "4.3.2 Affinity Diagramming.", "content": "During our thematic analysis of user comments, eight main themes emerged. We will discuss each in this section.\nTeacher Benefits Many participants praised the straightforward and intuitive nature of ATB's tutor authoring process and stated key benefits to teachers such as time savings. P7 said: \"...saves a lot of time. And we\" be able to accomplish more [in] less time because the course is really big. Sometimes we run short of time because of that\". Similarly P4, 6, and 10 all commented on how quick agent training was with one stating: \"It was pretty easy...the agent picked up right away after like three problems\". Additionally, participants expressed a desire to integrate ATB into their own classroom environments. P3 said: I find it very useful, especially [working] in the school, and with the students, it's gonna be very, very helpful to teach them. They follow this with: If it's in my class, I'll definitely use it.\nStudent Benefits In addition to teacher benefits, participants also identified how students might benefit from ATB. P7 said that the system would be \"more productive for the students\" and P3 added that the ability for students to practice different questions would be \"very useful\" for them. P3 also noted that ATB can increase the instructional presence of the teacher, allowing for additional help and practice when the teacher is unavailable to the students. Along another angle, P18 mentioned that teachers could \"allow students to build their own tutors [and] teach them or maybe turn [the authoring process] into a game\".\nInterface Builder: User Control Some participants noted that they would have liked more flexibility in the interface building"}, {"title": "5 DISCUSSION", "content": "The results of our study are promising and indicate that teachers with varying degrees of technical experience can successfully use ATB to create and customize tutor interfaces and author expert models. First, all study participants were able to recreate both the Fraction Arithmetic (FR) and Square 25 (S25) tutors using the interface builder. For the FR tutor, both the Low and High groups had comparable interface authoring times. The two groups, however, differed in interface authoring time for the S25 tutor, with the Low"}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "To summarize, we presented the ATB system, which allows users to create and personalize tutors to fit their instructional needs. We analyzed the overall effectiveness and usability of the system by conducting a user study where 14 participants built tutors and trained expert models in Fraction Arithmetic and the Square 25 procedure. The findings of this study were instrumental in informing the next iterations of A\u0422\u0412.\nWe primarily wanted to investigate the ability for teachers with little to no specialized training in tutor authoring tools to use our system. By grouping our participants into those with little to no programming experience vs. those with moderate to high programming experience (the Low group and the High group), we were not only able to study how those in the Low group use our system, but also discover any latent differences between the two groups that could challenge any assumptions or biases we included in the design as researchers.\nInformed by our study results, we plan to explore continued development of the interface builder to enable more flexible and intuitive design. This include the ability to reorder, edit, and align elements directly. Additionally, we plan to investigate new ways to interact with the AI agents. We plan to integrate language instruction for agent training, as well as the ability to guide the agent by selecting interface elements it should use in its explanation process. In conclusion, this study is one of the first to evaluate interactive machine-learning based tutor authoring tools with teachers. We believe our work demonstrates the potential for teachers to use these tools to create and personalize intelligent tutors for their classes. We hope future work will further explore this potential."}]}