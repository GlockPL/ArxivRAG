{"title": "Enhancing LLM Evaluations: The Garbling Trick", "authors": ["William F. Bradley"], "abstract": "As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models based on their performance. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.\nTo demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative reasoning abilities of these models, particularly highlighting distinctions between OpenAI's o1-preview and Google's gemini-pro-1.5-002.", "sections": [{"title": "Introduction", "content": "Significant advancements in AI and machine learning have often progressed in tandem with the development of evaluation tools. For instance, MNIST [12] spurred the creation of LeNet [11], ImageNet [5] led to the development of AlexNet [10], and CASP [15] motivated AlphaFold [9]. However, it is common for performance on a given test to reach saturation\u2014for example, modern accuracy on MNIST exceeds 99.9% [1], rendering it less useful for comparing models or driving research."}, {"title": "The Garbling Trick", "content": "The concept we explore is straightforward and can be summarized in one sentence:\nGiven a text-based evaluation method, randomly garble the text\nand observe how varying the garbling rate impacts the results.\nMore specifically, suppose we have an evaluation comprising a set of problems. Each problem includes some context, a question about that context, and a set of possible answers. The evaluation framework assigns a score s to the responses (e.g., the percentage of correct answers). We garble each character in the context with a probability p and then compute s(p). While the original evaluation yields a single score, the garbled evaluation produces"}, {"title": "The Contextual Core", "content": "If the base evaluation is a multiple-choice test with k options for each problem, one might assume that if the context were highly garbled, the LLM would be forced to choose a solution uniformly at random, hence\n$\\lim_{p\\to 1} s(p) = \\frac{1}{k}$\nThis intuition is typically misguided. If the information in the context is publicly available-for example, if the LLM is asked a historical or scientific question-it may not require the provided context to answer. Moreover, it may be able to eliminate incorrect answers. For instance, if the answer choices are \"3,\u201d \u201c5,\" and \"10/2,\" then regardless of the context or the question, the correct answer is likely \u201c3\u201d because we know there is a unique correct answer. Due to these effects, the score can significantly exceed 1/k even with"}, {"title": "NeoSQuAD", "content": "To illustrate our method, we create a new evaluation dataset and framework, apply the garbling trick, and analyze the results across nine different LLMs.\nWe call the new dataset \u201cNeoSQUAD\u201d.\nWe generated a set of 10,000 multiple-choice questions by sampling from the 100,000 questions in the answerable subset of the SQuAD 2.0 dataset [17]. Each SQUAD problem consists of a paragraph or two of context followed by a question. Two-thirds of the questions are answerable, and a correct answer is a substring of the context. Since SQuAD is not a multiple-choice test, we prompted gpt-40-2024-08-06 to provide two additional incorrect answers, resulting in a 10,000-problem multiple-choice test with three choices per problem. We also ensured that no answer was substring of another answer. The SQUAD data is ASCII-encoded; we garbled each byte with probability p by resampling a garbled byte uniformly at random from the 256 possibilities.\nNext, we needed to identify the \u201ccontextual core\u201d of problems for which the context is necessary. In our initial attempts, we directly prompted an LLM (\u201cDoes this question contain enough information to select an answer from the following list of options?\"). This approach proved insufficient: even though the LLM claimed it didn't have enough information but was still able to answer more accurately than at random. To construct a more conservative contextual core, we instead asked the question and demanded an answer. We prompted two LLMs (gpt-40-2024-08-06 and gemini-1.5-pro-001) and restricted the evaluation to the subset of questions where both LLMs chose the wrong answer. This process produced a contextual core of 1,027 problems (approximately 10% of our initial set).\nWe then generated score curves for a set of nine LLMs at garbling rates of\np = [0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9].\nThe LLMs were chosen for diversity\u2014 they include some of the largest and some of the smallest models. The results are shown in Figure 1.\nFigure 1 allows us to observe several features of these models. Initially, we note that at p = 0 (i.e., in the original evaluation), model performance is hard to distinguish. The accuracy is nearing saturation, and many models\""}, {"title": "Conclusions and Extensions", "content": "We have introduced the \u201cgarbling trick,\u201d a novel technique for evaluating LLMs that transforms an existing evaluation into a series of more challenging tests. These new tests can mitigate score saturation and provide richer insights into the reasoning abilities of LLMs.\nSeveral potential extensions are worth mentioning. First, while this paper focused on multiple-choice evaluations with a \u201ccontext + question + answers\u201d structure, the garbling trick can be applied more broadly. For example, many multiple-choice problems do not clearly separate the context from the question. In such cases, we might garble the entire context and question but leave the answers ungarbled. Second, most LLMs have a controllable \"temperature\u201d parameter: setting it to zero produces the maximum likelihood response, but setting it to a positive value allows for resampling of responses. Investigating the effect of the temperature parameter on s(p) would be interesting; it may reduce performance at small values of p but increase it at larger values."}, {"title": "Valid Answers", "content": "An LLM may fail to provide a valid answer to a question. An invalid answer may occur because the framework expects a certain structure (such as a parseable JSON blob) that the response violates, or because the question triggers a safety mechanism and the LLM refuses to answer. In the NeoSQUAD results reported in Section 4, we considered \u201caccuracy\u201d as the \"number of correct answers\u201d divided by the \u201cnumber of valid answers.\" Alternatively, we can define accuracy using the stricter criterion of \u201cnumber of correct answers\u201d divided by the \u201cnumber of questions asked.\u201d We present the resulting score curves in Figure 2 and plot the fraction of invalid answers per LLM in Figure 3."}]}