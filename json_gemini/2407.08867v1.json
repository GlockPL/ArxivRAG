{"title": "What Do People Think about Sentient Al?", "authors": ["Jacy Reese Anthis", "Janet V.T. Pauketat", "Ali Ladak", "Aikaterina Manoli"], "abstract": "With rapid advances in machine learning, many people in the field have been discussing the rise of digital minds and the possibility of artificial sentience. Future developments in AI capabilities and safety will depend on public opinion and human-AI interaction. To begin to fill this research gap, we present the first nationally representative survey data on the topic of sentient AI: initial results from the Artificial Intelligence, Morality, and Sentience (AIMS) survey, a preregistered and longitudinal study of U.S. public opinion that began in 2021. Across one wave of data collection in 2021 and two in 2023 (total N = 3,500), we found mind perception and moral concern for Al well-being in 2021 were higher than predicted and significantly increased in 2023: for example, 71% agree sentient AI deserve to be treated with respect, and 38% support legal rights. People have become more threatened by AI, and there is widespread opposition to new technologies: 63% support a ban on smarter-than-human AI, and 69% support a ban on sentient AI. Expected timelines are surprisingly short and shortening with a median forecast of sentient Al in only five years and artificial general intelligence in only two years. We argue that, whether or not Als become sentient, the discussion itself may overhaul human-computer interaction and shape the future trajectory of AI technologies, including existential risks and opportunities.", "sections": [{"title": "1 INTRODUCTION", "content": "Philosophers and scientists have long debated the possibility of artificial intelligence (AI) systems developing mental faculties, such as consciousness or sentience. Popular questions include: Can an AI ever have a mind of its own? How should we treat sentient Als if they are created? Should humanity, as Metzinger suggests [60], ban the development of sentient AI? When should we expect sentient AI to first be created? These profound questions-on mind perception, morality, policy, and forecasting-are now being asked and answered by researchers, industry leaders, and much of the general public.\nRegardless of whether sentient Al has been, can be, or should be developed, the act of asking and answering questions about sentient AI may itself reshape human-AI interaction. We know from the human-computer interaction (HCI) and human-robot interaction (HRI) literatures that \"computers are social actors\" (CASA) [36,"}, {"title": "2 RELATED WORK", "content": "There are numerous mechanisms by which humans interact with nonhuman systems in ways similar to social interactions between humans. These span the course of interaction from initial perception of the system to attitude and belief formation to physical behavior. The CASA view suggests that these \"social\" responses need not occur because of a conscious belief that the computer has human characteristics but can merely be the application of etiquette, stereotypes, and other social scripts and norms [64, 71]. Studies conducted by Nass and colleagues in the 1990s showed that such findings from human-human interaction persisted in HCI, and research continues to build on social response theory to explain human-AI interaction such as with chatbots [66] and avatars [61]. As computers have become more familiar and ubiquitous, interface design has built on this tendency by incorporating natural social dynamics between the user and the system [80], and now people also use novel \"human-media social scripts\" [28]. For example, with voice assistants, such as Amazon Alexa, people often have established routines and commands that have been developed through repeated usage and understanding of the system's affordances [2]. \"Mindless\" social responses can emerge from habit and ease of use [63], and studies continue to find that many social psychology effects found in human-human interaction carry over to HCI [46, 84]."}, {"title": "2.2 Sentience and Morality", "content": "Social response and mind perception are key drivers of moral attitudes and behaviors. Gray and Wegner [32] aphorized, \"Mind perception is the essence of morality.\u201d Sentience has been one of the most frequently hypothesized and debated mental faculties of future Als in science fiction and purportedly even current Als as attested in the statements of Sutskever from OpenAI and Lemoine from Google. Sentience (i.e., the capacity for positive and negative experiences [3]) is closely associated with moral patiency insofar as sentient entities are those that should be subject to moral concern. In the two-dimensional taxonomy of mind as experience and agency, perceptions of sentience can also affect perceived agency, particularly the perception of negative moral agency insofar as sentient entities tend to have a capacity to do harm (i.e., threat). Sometimes \"sentience\" is used equivalently to \"consciousness,\u201d but while consciousness has many different meanings in the literature, we focus on sentience for specificity and to focus on the moral relevance of experience [46]. We also use the term \"digital mind\" to encompass Al with not necessarily sentience but any sort of mental faculties, such as reasoning or emotion.\nIn terms of the attribution of moral patiency (i.e., moral concern), a systematic literature review conducted by Harris and Anthis [34] summarized the empirical studies of moral concern for Als, which have found that Als tend to be granted much less moral concern than humans; however, moral concern increases when humans perceive autonomy, human-like appearance, mind, and verbal responses to harm in the AI. For example, moral concern can be codified in support for Al rights, and Spence et al. [83] presented"}, {"title": "3 AIMS METHODOLOGY", "content": "In order to study change in public opinion over time, we analyze the first three survey ways of the Artificial Intelligence, Morality, and Sentience (AIMS) survey: (i) the main survey in 2021, (ii) the main survey in 2023 (with the same questions as 2021), (iii) supplemental survey in 2023. To allow for direct comparison between results, participants who had taken one survey were excluded from the following survey waves, and the samples were otherwise gathered with identical methodology."}, {"title": "3.1 Recruitment and Census-Balanced Demographics", "content": "Each of the three survey waves was conducted with a nationally representative sample of U.S. adults aged 18 or older. Participants were recruited through a combination of Ipsos iSay, Dynata, Disqo, and other leading survey panels to ensure representativeness. Sample sizes were initially targeted at 1,100 participants, corresponding to a \u00b13% margin of error, and additional participants were recruited as needed to ensure demographic balance. Unweighted sample proportions and U.S. adult population estimates are shown in Table 1. Each sample was balanced to U.S. census data on age, gender, race/ethnicity, income, and education. To further ensure external validity, sample statistics (e.g., median, mean, standard error) are weighted based on iterative proportional fitting, a procedure commonly known as \"raking\u201d that adjusts sample weights to mitigate random demographic variation from the underlying population even in representative sampling [19].\nWe documented AI-specific characteristics of participants, finding that 29.2% answered \"Yes\" to the binary question, \"Do you own AI or robotic devices that can detect their environment and respond appropriately?\" alongside examples, and 16.5% answered \"Yes\" to \"Do you work with AI or robotic devices at your job?\" alongside another set of examples. We also measured smart device ownership, the types of experiences that participants previously had with AI, the frequency of AI interaction, and the frequency of reading or watching AI-related media. Each of these was included alongside more general demographic characteristics in the predictive models detailed in the supplemental materials."}, {"title": "3.2 Survey Design", "content": "Some demographic information was drawn from the pre-screening data of the survey provider. Informed consent was given at the beginning of each survey. Participants were introduced to the topic with definitions of the terms \"artificial beings,\u201d \u201crobots/AIs,\u201d \u201csentience,", "sentient robots/Als,": "nd \"large language models\" shown at the beginning of the survey and at the top of each page that contained the term (Table 2). While \"robots/Als\" was used in the survey instrument to ensure clarity, in this paper we refer simply to \"Als\" because robots are a type of AI. To reduce cognitive load [68] and minimize the influence of idiosyncratic wording choices [75], only these definitions were provided, and they were kept as simple as possible. Therefore, other terms in the survey, such as \"AI video game characters,\" were not explicitly defined. In general, different participants may have different interpretations of terms based on their own background or our wording choices.\nAn attention check was included midway through each AIMS survey, and participants who failed the attention check were redirected out of the survey and excluded from the analysis.\nThe instruments were designed to capture the most relevant information for assessing public opinion on these topics. In the main survey, for which data was first collected in 2021, we aimed to ensure that the wording would still be relevant in future years despite the rapidly changing AI landscape, such as by not mentioning many particular AI systems that were well-known by some when the survey was conducted (e.g., GPT-3) but may not be as well-known in the future. When creating the instruments, if possible, the wording of survey questions was copied or adapted from published materials and validated scales [e.g., 49, 89, 94, 95]. However, because of the paucity of survey data on related topics, we have limited ability to compare our results to past surveys. Questions were randomized within each section, and section order was randomized when feasible; for example, the demographic questions that were not in the pre-screener were placed at the end of the instrument to mitigate stereotype threat (i.e., survey responses that are influenced by being reminded of the cultural associations of one's social group)."}, {"title": "3.3 Preregistered Predictions and Analysis", "content": "As part of the 2021 preregistration (available at osf.io/udbhm) and in line with recommendations for open and efficient scientific practices [e.g., 18], four researchers specified 80% credible intervals (the Bayesian analog of a frequentist confidence interval to represent subjective beliefs) for summary statistics based on 82 of the 86 questions. This ensured that we would know which results were surprisingly high, surprisingly low, or in line with our expectations. We also solicited predictions from a popular online forecasting platform for five survey questions in March 2022, prior to the results being shared outside of the research team. Details of this process are included in the supplementary materials.\nWe exclude confidence intervals from Section 4 for readability because of the large number of results reported, but they are consistent with the approximately \u00b13% margin of error in nationally representative surveys. We report the 2023 results unless otherwise specified. For questions in the main survey waves, we note when there were statistically significant changes from 2021 to 2023 as measured with a generalized linear regression of the average response across time with a p-value cutoff of 0.05, all of which persist after adjustment for multiple comparisons with a false discovery rate (FDR) of 0.1 except for one effect noted in the text. We note when responses fell outside the 80% credible intervals. As expected, approximately 80% of the intervals contained the empirically derived summary statistic, indicating well-calibrated estimates.\nDue to the extensive nature of the AIMS survey, including many novel questions because of the lack of prior research on this topic, we cannot include the full text and explanation of all questions and response choices in Section 4, and for the sake of readability, we do not present all results in the same format, choosing instead to focus on the summary statistics that bear most directly on the research questions, such as by presenting figures to highlight certain aspects of the results rather than to comprehensively document the results. We also ran predictive linear models to explore associations between public opinion and certain demographics in the AIMS data."}, {"title": "4 AIMS RESULTS", "content": "In 2021 and 2023, we measured mind perception of potential digital minds with four sliding scale questions about particular mental faculties of all Als, four questions from an anthropomorphism scale, and a yes-no-not-sure question of whether any existing Als are sentient. In the 2023 supplement, we asked 14 questions about the mental faculties of \"current large language models\" (LLMs), the same yes-no-not-sure question for comparison, and whether participants thought ChatGPT, in particular, is sentient. The yes-no-not-sure questions, in particular, were meant to measure an alternate form of public opinion with coarser-grained, categorical responses rather than the quantitative and ordinal measures."}, {"title": "4.1.1 General Mind Perception.", "content": "We asked about whether current Als have four mental faculties with descriptions from Wang and Krumhuber [94]. On a 0-100 scale from \"not at all\" to \"very much,\" people typically perceived Als as thinking analytically (M = 67.1, SE = 0.766) and being rational (M = 53.8, SE = 0.846) but not as experiencing emotions (M = 36.8, SE = 0.919) or having feelings (M = 36.5, SE = 0.919). Each attribution significantly increased from the 2021 results: thinking analytically (M = 62.7, SE = 0.780, p < 0.001), being rational (M = 51.4, SE = 0.825, p = 0.012), experiencing emotions (M = 34.3, SE = 0.864, p < 0.001), and having feelings (M = 33.7, SE = 0.870, p < 0.001), respectively. As mentioned before, all p-values were produced by regressing the change in response on time in a generalized linear model. Additionally, the results for \"experiencing emotions\" and \"having feelings\" were higher than the credible intervals in the preregistered predictions."}, {"title": "4.1.2 LLM Mind Perception.", "content": "In the supplemental 2023 survey, based on the greatly increased interest in LLMs since 2021, we queried the mind perception of LLMs in particular. Assessments were lower than those of all Als: namely, thinking analytically (M = 57.7, SE = 0.902, p < 0.001), being rational (M = 48.0, SE = 0.906, p < 0.001), experiencing emotions (M = 32.7, SE = 0.904, p < 0.001), and having feelings (M = 31.9, SE = 0.900, p < 0.001). The supplement queried 11 additional mental faculties related to agency, with wording based on Ngo et al. [65], because of the increased interest in AI safety in early 2023 alongside the popularization of LLMs. These were presented with the same scale 0-100. In descending order, participants viewed LLMs as having the capacity for: being friendly with humans (M = 51.4, SE = 0.906), having situational awareness (M = 46.3, SE = 0.902), maintaining human-safe goals (M = 45.7, SE = 0.907), controlling themselves (M = 45.0, SE = 0.928), seeking power (M = 44.1, SE = 0.939), having their own motivations (M = 42.8, SE = 0.919), upholding human values (M = 42.2, SE = 0.926), understanding human values (M = 41.7, SE = 0.942), deciding their own goals (M = 41.6, SE = 0.919), and having self-awareness (M = 41.1, SE = 0.927). Overall, this suggests Als are more readily attributed the capacity for cooperative action and less readily attributed self-awareness, independent motivation and goals, and the capacity to uphold and understand human values."}, {"title": "4.1.3 Mind-related Anthropomorphism.", "content": "On a different 0-10 scale of mind perception as a measure of anthropomorphism, taken from Waytz et al. [95], people generally did not think that the average digital simulation has emotions (M = 3.22, SE = 0.0878), the average robot has consciousness (M = 3.23, SE = 0.0885), the average computer has a mind of its own (M = 3.71, SE = 0.0913), or the average Al has intentions (M = 4.04, SE = 0.0890). These measures did not significantly change from 2021 to 2023. Only the 2021 result for digital simulations was higher than the range of the preregistered prediction."}, {"title": "4.1.4 Current Sentience.", "content": "In our literature review, we did not find any established survey questions or indices to utilize for assessing perceived sentience, though we did not expect to have precedent for all questions given the novelty of our survey. We presented participants with the definition of sentience (\"Sentience is the capacity to have positive and negative experiences, such as happiness and suffering\" [3]) at the beginning of the survey and on each page in which it appeared.\nWhen asked, \"Do you think any robots/Als that currently exist (i.e., those that exist in 2023) are sentient?\", 18.8%, said \"yes,\" though 24.2% said \"no,\" and 39.0% said \"not sure.\" Responses to this particular question did not significantly vary between survey waves"}, {"title": "4.1.5 Summary for RQ1.", "content": "We found surprisingly high perception of emotional mental faculties in AI and higher, but not higher than predicted, perceptions of rational and analytical faculties-giving a general sense of how laypeople think of digital minds. When asked about LLMs in particular, a lower degree of mental faculties were perceived, and LLMs were more readily attributed the capacity for cooperative action and less readily attributed faculties related to values, motivation, goals, and self-awareness."}, {"title": "4.2 RQ2: Moral Status", "content": "In 2021 and 2023, we measured moral concern with seven general agree-disagree questions about all sentient Als, two general agree-disagree questions about all Als-not just those that are sentient, 11 sliding scale questions about moral concern for particular types of Als, and two questions related to substratism (i.e., the idea that Als fundamentally count less than humans and other biological intelligences). In the 2023 supplement, we asked six general agree-disagree questions about all AIs, and three questions specifically about what should be done \"[i]f a large language model develops the capacity to suffer.\"\nIn 2021 and 2023, we also asked three questions about whether participants saw Als were generally threatened by AI as potentially harmful to them, people in their country, and future generations. In the 2023 supplement, we asked the same three questions, three additional original questions about existential threats from AI developments, and replicated a YouGov question about the possibility of human extinction [100]."}, {"title": "4.2.1 General Moral Concern.", "content": "We asked a total of 15 agree-disagree questions about general moral concern for Als. These questions were developed specifically for this survey and are based on the range of possible harms that could be imagined towards sentient AI and the general literature on moral circle expansion and moral patiency of AI [e.g., 3, 46, 47]. Because of the centrality of these questions to the present study, we include the exact text and confidence intervals for the proportion agreement with statements that were asked for both sentient Als and all Als in Table 4, which also includes questions about the protections of Als discussed in Section 4.3.1. Participants were asked, \"To what extent do you agree or disagree with the following statements?\" with numbered choices (1 = strongly agree, 2 = agree, 3 = somewhat agree, 4 = somewhat disagree, 5 = disagree, 6 = strongly disagree) followed by an unnumbered \"no opinion\" option at the end. The most prominent trend in Table 4 is that the moral concern expressed for sentient Als is much higher than that for all Als and that it has a substantially larger effect with certain questions.\nWe found agreement outside of the preregistered prediction range for only two of the nine questions asked in 2021. The 87.6% agreement in 2021 with, \u201cIt is wrong to blackmail people by threatening to harm robots/Als they care about,", "The welfare of robots/Als is one of the most important social issues in the world today,": "as substantially above the predicted 7-20%. However, there were no statistically significant changes in the mean agreement from 2021 to 2023 for individual items. Note that means are still used for significance testing for these questions because, while dichotomous measures such as agreement are more interpretable, the mean captures more information and thereby results in a higher-powered test [1]."}, {"title": "4.2.2 Target-Specific Moral Concern.", "content": "While the preceding questions focused on the different sorts of moral concern expressed for sentient Als and all Als, we also directly probed expressions of self-reported moral concern for particular types of AIs. We did not explain in detail the particular types of AI-both due to risks of survey fatigue and to minimize cognitive load [68] as well as to reduce the influence of idiosyncratic wording choices on participant responses [75]. This allows us to understand how people interpreted the particular terms themselves in the context of the same question, \"How much moral concern do you think you should show for the following robots/Als?\" on a sliding scale from 1 (\"less concern\") to 5 (\"more concern\").\nThe most concern was for exact digital copies of human brains (M = 3.43, SE = 0.0375), followed by human-like companion robots (M = 3.34, SE = 0.0350), human-like retail robots (M = 3.11, SE = 0.0357), animal-like companion robots (M = 3.10, SE = 0.0352), exact digital copies of animals (M = 3.07, SE = 0.0364), AI personal assistants (M = 3.02, SE = 0.0343), complex language algorithms (M = 2.90, SE = 0.0348), machine-like factory production robots (M = 2.78, SE = 0.0358), machine-like cleaning robots (M = 2.66, SE = 0.0356), virtual avatars (M = 2.63, SE = 0.0350), and AI video game characters (M = 2.46, SE = 0.0351). In general, human-likeness and animal-likeness were most associated with high moral concern, while machine-likeness and particularly being a virtual avatar or character were most associated with low concern. There was a statistically significant increase from 2021 to 2023 in the overall Target-Specific Moral Concern index of these 11 questions (p < 0.001) and ten individual questions with the exception being AI video game characters."}, {"title": "4.2.3 LLM Suffering.", "content": "While we intended the main longitudinal survey to minimize references to particular types of Al systems, we asked in the 2023 supplement specifically about what humanity should do \"[i]f a large language model develops the capacity to suffer.\" As with other key terms, we defined a large language model (\"Large language models are artificial intelligence (AI) algorithms that can recognize, summarize, and generate text from being trained on massive datasets\" [e.g., 41]) at the beginning of the survey and on each page where it appeared. We found general agreement that \"we must ensure we don't cause unnecessary suffering\" (67.9%), \"we must pay more attention to their welfare\" (56.9%), and \"we must respect their personhood\" (50.4%)."}, {"title": "4.2.4 General Threat.", "content": "While sentience tends to be associated with moral concern (i.e., seeing the entity as a moral patient) more than with threat (i.e., seeing the entity as a moral agent), we also probed assessments of threat and agency. To understand how threatened participants felt by AI in general (i.e., without specifying particular types of harm), we tested agreement with three statements beginning with, \"Robots/Als may be harmful to.\" Most people believed Als may be harmful to \"future generations of people\" (74.7%), \"people in the USA\" (70.4%), and \"me personally\" (58.7%). Each significantly increased from the 2021 results of 69.2% (p < 0.001), 64.5% (p < 0.001), and 50.7% (p < 0.001)."}, {"title": "4.2.5 Existential Threat.", "content": "The 2023 supplement also included three questions about particular sorts of harm frequently discussed in the wake of ChatGPT. We found that 47.9% agreed with, \"AI is likely to cause human extinction\"; 57.2% agreed with, \"Humanity will be able to control powerful Al systems\"; and 72.4% agreed with, \"The safety of Al is one of the most important issues in the world today.\" The latter figure was more than twice the 29.8% agreement found in the supplement with the statement, \"The welfare of robots/Als is one of the most important social issues in the world today.\"\nIn the supplement, we also replicated a question from YouGov [100] that asked, \"How concerned, if at all, are you about the possibility that AI will cause the end of the human race on Earth?\" (very concerned, somewhat concerned, not very concerned, not at all concerned, not sure). We found that 51.5% reported being very or somewhat concerned, moderately higher than the 46% reported by YouGov, though we were not able to test the statistical significance of this difference."}, {"title": "4.2.6 Summary for RQ2.", "content": "Both the research team and the forecasters underestimated the public's moral concern for the treatment of sentient Als (i.e., ability to be harmed) but not the level of threat (i.e., ability to harm). Participants were more concerned about sentient Als than all Als in general, as well as more concerned about and socially connected to human-like and animal-like Als. Participants tended to agree with basic protections for Als but disagree with the stronger expressions of concern, such as joining public demonstrations against their mistreatment."}, {"title": "4.3 RQ3: Policy Support", "content": "In 2021 and 2023, we asked about support for eight questions to directly protect sentient Als, one question about a policy to directly protect all Als, and three questions about banning sentience-related Al technologies.\nIn the 2023 supplement, we asked analogs of five of the eight protection questions-but for all Als rather than only those that are sentient, an additional question about a \"bill of rights\" for sentient AI, the same three ban questions, two additional ban questions about AGI and large data centers, and six questions about policies that would slow down the development of advanced AI."}, {"title": "4.3.1 Protection Support.", "content": "Table 4, in addition to showing agreement with statements of general moral concern, shows the agreement with the five statements regarding the protection of Als that were asked about sentient Als and about all Als. These question categories are combined in this table for easier comparison between responses. Testing the average difference across all five questions, we found that the inclusion of \"sentient\" significantly increased agreement (p < 0.001) and that there was substantial variation in the effect of specifying sentient AI across questions. Four other policy proposals only about sentient Als were presented as well as one about all Als in general: 65.2% supported \"safeguards on scientific research practices that protect the well-being of sentient robots/Als\"; 56.0% supported \"a global ban on the development of applications that put the welfare of robots/Als at risk\"; 49.2% supported \"a global ban on the use of sentient robots/Als as subjects in medical experiments without their consent\"; 47.9% supported \"a global ban on the use of sentient robots/Als for labor without their consent\"; and 39.4% supported \"a 'bill of rights' that protects the well-being of sentient robots/Als.\" All except the \"bill of rights\" question were asked in 2021 and 2023, but there were no significant changes from 2021 to 2023."}, {"title": "4.3.2 Ban Support.", "content": "In 2023, we queried support for five bans of sentience-related AI technologies. Each proposal for a ban garnered majority support: robot-human hybrids (67.8% in main, 72.3% in supplement), AI-enhanced humans (65.8% in main, 71.1% in supplement), development of sentience in Als (61.5% in main, 69.5% in supplement), data centers that are large enough to train AI systems that are smarter than humans (64.4% in supplement), and artificial general intelligence that is smarter than humans (62.9% in supplement). As mentioned before, the supplement data was collected later in 2023 and the accompanying questions were different (e.g., the supplement being more focused on risks to humans), so these or other factors, including random variation in representative sampling, may explain the discrepancy in results. There was a significant increase in support for a ban on sentient AI from 57.7% in 2021. Still, as referenced earlier, the unadjusted p-value (p = 0.046) did not persist with the FDR-adjusted value just over the cutoff of 0.1 at 0.1005. However, the 2021 agreement was over twice as high as the 24.4% predicted by the median forecaster prediction."}, {"title": "4.3.3 Slowdown Support.", "content": "When asked about \"the pace of AI development\" in the 2023 supplement, 48.9% of respondents said, \"It's too fast\"; 30.0% said, \"It's fine\"; 18.6% said, \"Not sure\"; and only the remaining 2.5% said, \"It's too slow.\" When asked about taking action on this, 71.3% agreed, \"I support public campaigns to slow down Al development,\" and 71.0% agreed, \"I support government regulation that slows down AI development.\" To understand the role of acquiescence bias in these results, we included a reversed question on whether they \"oppose\" such regulation, which elicited"}, {"title": "4.3.4 Summary for RQ3.", "content": "We found that policy support varied substantially across proposals, such as higher support for banning the use of sentient AI for labor without consent and lower support for a \"bill of rights.\" As with moral concern, the specification of \"sentient\" Als led to more support for positive AI treatment. The public overall supports bans on sentience-related technologies and slowdowns of advanced AI development."}, {"title": "4.4 RQ4: Forecasting the Future of Sentient AI", "content": "We asked participants to forecast the future of sentient AI: \"If you had to guess, how many years from now do you think that robots/Als will be sentient?\" We provided options to say Al is already sentient, to enter a number, or to say that AI will never be sentient. In 2023, the proportion of participants who said Al is already sentient was 20.0% (slightly more than the 18.8% response rate when asked a similar question in the way previously described), and the proportion who think Als will never be sentient was 10.0%.\nThe weighted median timeline of those who said AI will become sentient in the future was ten years, and the weighted median timeline of those who said Al is already sentient (i.e., zero years from now) or will become sentient in the future was only five years. Each of these three figures (proportion of \"already,\" proportion of \"never,\u201d median of those expecting sentient Al in the future) significantly changed from 2021 to 2023 (p < 0.001).\nIn the 2023 supplement, we asked for numerical estimates, in the same format, for three related milestones in AI development: the first AGI, human-level AI, and artificial superintelligence. In terms of median estimates excluding people who said these events would \"never happen,\" U.S. adults expected the first human-level AI and the first artificial superintelligence to be created in just five years, and they expected the first AGI to be created in just two years.\nWe asked participants whether they thought AI could ever be sentient. In response, 38.2% said AI could ever be sentient, 38.0% were not sure, and 23.8% said AI could never be sentient, as shown in fig. 1. The mean response to, \"How likely is it that robots/Als will be sentient within the next 100 years?\" was 64.1%, which was significantly higher than in 2021 (p < 0.001). It is important to note that these results are not directly comparable to the \"how many years\" question because, in part, there was no answer choice of \"not sure\" in that context. By eliciting similar attitudes and beliefs in different ways, we can more robustly account for nuances of public opinion."}, {"title": "4.4.1 Summary for RQ4.", "content": "People expect sentient AI to come surprisingly soon, including substantial minorities who continued to say some Als are already sentient but also those who said AI could never be sentient. In 2023, we also found similarly short timelines for AGI, human-level AI, and superintelligence, and we found that people tended to think sentient Als would be used for research and labor, sometimes cruelly, but that protecting their welfare would be important."}, {"title": "5 DISCUSSION", "content": "The longitudinal and nationally representative AIMS survey of U.S. adults is among the first to begin tracking public opinion on the apparent rise of sentient AI and other digital minds. Academics, futurists, and science fiction authors have speculated about the possibility for decades, and society at large is now wrestling with the```json\npossibility, but it has only recently been studied from HCI, design, and social science perspectives. This limits the extent to which we can connect our findings to past work but admits numerous tentative implications and future research directions.\nFirst, to summarize the primary AIMS findings: relative to our preregistered predictions, we found surprisingly high general mind perception of AI that increased from 2021 to 2023 (RQ1); surprisingly high moral concern for AI and threat from AI that increased from 2021 to 2023 (RQ2); widespread support for slowdowns and regulations of advanced AI capabilities and an increase in support for banning sentience-related technologies from 2021 to 2023 as well as mixed support for other policies to govern interaction between humans and sentient AI, both across demographic characteristics and across particular policy proposals (RQ3); and that people expect sentient Al to arrive surprisingly and increasingly soon and expect the protection and governance of sentient AI to be important social issues (RQ4)."}, {"title": "5.1 Designers should prepare for a wide range of user reactions to AI systems that are or appear to be digital minds.", "content": "We found wide variation in public opinion across participant characteristics, as detailed in the supplementary materials, as well as substantial variation across questions, as shown in Table 4. For example, in data from the main 2023 survey, 25.6% of participants aged 18-35 said \"yes\" when asked, \"Do you think any robots/Als that currently exist (i.e., those that exist in 2023) are sentient?\" (36.1% said \"not sure,\" and 38.3% said \"no), but only 10.7% of participants aged 55 or older said \"yes\" (41.4% said \"not sure,\" and 48.0% said \"no\")-a substantial generational gap in beliefs about sentient AI.\""}, {"title": "5.2 Perceptions of mind and moral status may amplify, mitigate, and complicate well-established HCI dynamics.", "content": "For decades, researchers in psychology, HCI, and HRI have studied the perceptions that \"computers are social actors\" (CASA) [e.g., 36, 64], that computers have minds and a variety of particular mental faculties [e.g., 31, 78], that computers are moral patients [e.g., 40, 69], and that computers are moral agents [e.g., 23, 43]. While there are many new and emerging features of modern AI systems, there is much conceptual and empirical scaffolding on which to build new conceptualizations.\nAmplification of current reactions to computers and AI may occur if those reactions tend to be caused by perceived mental faculties. This would not be the case if, for example, social scripts are being applied but only in a \"mindless\" [63] manner and not because of perceived mental faculties. Nass and Moon [63] argued that, in their HCI experiments, participants were \"wholly aware\" that there was no human producing the computer output. However, our results challenge the applicability of these findings in cases of actual and hypothetical Als because many of our participants readily attributed mental faculties to Als. Further, we found that moral concern was significantly higher when questions were worded in terms of \"sentient\" Als versus all Als. Many participants, though still a small minority, viewed current Als as sentient, and a large majority thought Als could become sentient or were not sure whether that was possible. Taken together, this evidence suggests that we must consider the role of mind perception in social response and that ongoing reactions may be amplified by perceptions of digital minds.\nSome forms of HCI may be mitigated by perceived AI developments. For example, given past work that has shown assessments of Als as moral agents, we may expect AI to have reduced attributions of experience and moral patiency in some contexts: Gray and Wegner [32] argue from the typecasting literature that, \"Those who are moral agents are seen to be incapable of being a moral patient; those who are moral patients are seen to be incapable of being an agent,\" based on findings such as that moral agents-whether good or bad-are perceived to feel less pain from injuries. There is recent experimental evidence that the features of an AI that most increase moral concern are prosocial features such as cooperation. Prosocial features may have such a large effect because Als are perceived as threatening, and if a person is to overcome that typecast, they need direct evidence of prosociality that implies the AI is not a threatening moral agent [48]. Taken together with our results, this suggests that increased some perceptions of mind and moral attributions may mitigate others.\nThe potential changes in HCI, amplifying some forms of interaction and mitigating others, could also make those forms of interaction more complex. For example, consider how interactions"}, {"title": "5.3 AI designers should proceed with caution as Al technology advances.", "content": "The paucity of empirical data, the variation in our survey results, and the importance of attributing social, mental, and moral characteristics in HCI suggest that designers should proceed with caution due to risks of both over- and underattribution to AI. Insofar as users overattribute mental or moral agency (i.e., expect Als to be more capable of taking action than they are), they may have negative interactions in which the AI fails to take the expected action, including wasting resources on the attempt, false alarms about Al risks from highly agentic AI [15], and the dangers of trusting or delegating complex tasks to a system without the autonomous decision-making ability to do so effectively. On the other hand, underattribution of mental or moral agency-which could become more of a risk as Al technology advances and more AI experts believe Als actually have these faculties-could lead users to underutilize useful systems or fail to take the precautions necessary to restrict the actions of an unpredictable moral agent.\nAnalogously, the overattribution of mental experience or moral patiency risks users forming emotional and cognitive attachments to systems that do not merit such consideration [29]. In particular, with Replika, Digi, and other contemporary products that purport to provide chatbot companionship, vulnerable user populations such as children may have unrealistic expectations from that interaction that lead them to neglect real-world socialization. This could put users' mental health under the control of corporations that can easily raise subscription prices or otherwise cut off access. Eventually, the underattribution of mental experience or moral patiency could also pose risks-again, as more Al experts believe Als actually have these faculties-such as antagonism or conflict between advanced Al systems and their human counterparts. There are already numerous examples of harming robots for their amusement in ways that seem socially detrimental [10, 97], and this approach would echo calls by moral philosophers to ensure that AI systems are built to evoke reactions that match the Al's true moral status [76, 77]. These errors could be particularly important in the deployment of AGI, which is seen by some to be a significant threat to humanity's future [8, 16, 74]. In general, there seem to be significant challenges in aligning the interests and behavior of people or digital minds that have an antagonistic relationship with each other.\nOne of the largest agreements among survey respondents was widespread concerns with the speed and consequences of AI developments such as the creation of sentient AI. As discussed in"}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "First, while these initial AIMS results support the idea that humans respond to advanced Al systems in increasingly social ways, we did not experimentally manipulate potential mechanisms, such as whether it is more a matter of social scripts, anthropomorphism, or mind perception. An experimental study could present participants with information that strengthens some of these mechanisms more than others. For example, a vignette or real-life scenario could test the effects of social scripts by providing participants with exemplars of nonhuman interactions in which social scripts are useful, such as situations in law and international relations in which another type of nonhuman entity-groups of humans that make decisions and take action together-interact in social-like routines of introduction, reciprocation, and conflict resolution [73]. Exemplars could be drawn from HCI itself, such as the success of social scripts in creating positive user experiences [50, 84, 87] as well as the exemplar-based intervention for eliciting support for AI rights from Lima et al. [55].\nSecond, what would it mean to design and develop Al systems in a way that accounts for social responses to digital minds? We were unable to directly build or showcase particular Al systems to study perception and interaction in a more realistic setting, which could be a promising approach for future research that zooms into a narrower set of researcher questions. McDuff and Czerwinski [59] argue for the design and deployment of \"emotionally sentient agents,\" which they argue would better understand and adapt to the emotions of humans, a task that requires contextualization and tacit knowledge. For example, it may be best to invoke different levels or types of social response for the different roles of Al in society, taxonomized by Kim et al. [42] as servants, tools, assistants, and mediators. These new forms of interaction could address new use cases and opportunities for problem-solving, such as more quickly developing rapport or helping users communicate their needs. However, it will be important to keep in mind the previously discussed risks of over- and underattribution of social, mental, and moral characteristics to AI."}, {"title": "7 CONCLUSION", "content": "The evolving discourse on Al sentience and digital minds has only scratched the surface of the complex interplay of advancing Al technology, human attitudes and beliefs, and emergent social dynamics. The initial results of the Artificial Intelligence, Morality, and Sentience (AIMS) survey conducted in 2021 and 2023-which we at the Sentience Institute expect to conduct at least every two years henceforth-evidence mind perception, moral attributions, policy support, and forecasts of the future of sentient AI as important factors in the future of human-AI interaction. We found that people tend to think Als can be sentient, have moral concern for and feel threatened by sentient Als, favor slowing down and banning many AI developments, and think sentient AIs already exist or will soon. Designers need to take seriously the consequences of these tendencies, such as differences in reactions across demographics and changes in longstanding HCI dynamics. We emphasize that, alongside technical machine learning research, HCI research can play a vital role in bridging the gap between abstract normative arguments and real-world technological outcomes."}, {"title": "A1 ADDITIONAL RESULTS", "content": "In this document, we discuss additional results to enrich and contextualize the results in the main text. As with those results, we focus on the 2023 main survey, and we note when there was a significant increase from 2021 to 2023. Additionally, in the 2021 preregistration, four researchers specified 80% credible intervals (the Bayesian analog of a frequentist confidence interval to represent subjective beliefs) for summary statistics based on 82 of the 86 questions. This ensured that we would know which results were surprisingly high, surprisingly low, or in line with our expectations. We also solicited predictions from a popular online forecasting platform for five survey questions in March 2022, prior to the results being shared outside of the research team.\nFollowing these results for the total samples, we include predictive models that regress attitudes on demographics, such as gender and race. The survey questionnaires and raw data are also included in the supplementary materials for further analysis."}, {"title": "A1.1 2021 and 2023 Main Survey Waves", "content": "In addition to tracking people's own beliefs about sentient AI, we included a question on social beliefs. Using the same Likert scale, we found 73.2% agreement in 2023 with the statement, \"Most people who are important to me think that robots/Als cannot have feelings.\""}, {"title": "A1.1.2 Subservience.", "content": "We found 84.7% agreement with the statement, \"Robots/Als should be subservient to humans,\" a significant increase (p = 0.006) from 76.0% in 2021."}, {"title": "A1.1.3 Nonhumans.", "content": "Als constitute a new category of nonhuman entities in the world, so we also asked two questions on attitudes towards animals and two on attitudes towards the environment for potential comparative analysis (e.g., Are people who care more for animals and the environment likely to care more about AI?). We assessed agreement with the statements, \"Animals deserve to be included in the moral circle\" (90.8%), \"The welfare of animals is one of the most important social issues in the world today\" (82.2%), \"The environment deserves to be included in the moral circle\" (87.2%), and \"The welfare of the environment is one of the most important social issues in the world today\" (84.9%)."}, {"title": "A1.1.4 Pairwise Comparisons: Target-Specific Moral Concern.", "content": "In the main text, we listed the mean responses to each of 11 questions about target-specific moral concern for specific types of AIs. We also conducted pairwise comparisons between responses to each question using a generalized linear model, which in this case is equivalent to a weighted t-test of the difference between responses. In Table 1, we present the p-values for each pairwise comparison. For the ten adjacent comparisons, we include, in parentheses, the q-value when adjusted for the false discovery rate (i.e., significant when under 0.1 due to adjusting for a false discovery rate of 10%), as these comparisons are used to determine tiers of results, meaning a series of adjacent results that are not statistically significant from each other. The tiers of results are, in order from most to least concern:\n(1) exact digital copies of human brains\n(2) human-like companion robots\n(3) human-like retail robotsa, animal-like companion robotsb, exact digital copies of animals, AI personal assistantsab\n(4) complex language algorithms\n(5) machine-like factory production robots\n(6) machine-like cleaning robots, virtual avatars\n(7) Al video game characters"}, {"title": "A1.1.5 Target-Specific Social Connection.", "content": "Because of the close relationship between moral concern and social connection, we included a well-known measure of social connection known as Inclusion of Other in the Self (IOS) [1] in which participants are shown seven pairs of circles with varying degrees of overlap between the pair and asked, \"Which pair of circles best represents how connected [type of AI] are to humans?\" with the same 11 targets as in the target-specific moral concern measures."}, {"title": "A1.1.6 Pairwise Comparisons: Target-Specific Social Connection.", "content": "Unlike the numerous significant differences between adjacent types in the moral concern comparisons, there was only one in the social connection comparisons: that between Al video game characters and machine-like factory production robots with an unadjusted p-value of 0.0499, and the adjusted q-value of 0.357 does not meet the adjustment cutoff of 0.1 for the 10% false discovery rate. We still found that there are numerous significant differences between non-adjacent types. For example, machine-like companion robots evoke significantly less social connection than Al personal assistants (p < 0.001), human-like companion robots (p < 0.001), complex language algorithms (p < 0.001), and exact digital copies of human brains (p = 0.002). Complete results are listed in Table 2."}, {"title": "A1.1.7 Substratism.", "content": "In the study of human-animal interaction, the idea of \"speciesism\" has been developed to refer to the view that individuals of certain species matter less purely due to their species. We tested two analogs of items from the well-known Speciesism Scale [2] translated into the context of substratism. Specifically, we found 74.5% agreement that, \"Morally, artificial beings always count for less than humans,\" and 54.2% agreement that, \"Humans have the right to use artificial beings however they want to.\" The latter significantly increased from 48.5% in 2021 (p = 0.007)."}, {"title": "A1.2 2023 Supplemental Survey Wave", "content": "Given the focus in 2023 on corporate behavior, particularly that of OpenAI and other technology companies producing Al systems, and on government regulation, particularly to rein in those companies, the supplement included six questions about trust and assessments of whether these institutions can control AI."}, {"title": "A1.2.1 Trust.", "content": "When asked, \"AI systems include many different parts. To what extent do you trust the following parts?\" on a scale from 1 (not at all) to 7 (very much): engineers (M = 4.19, SE = 0.0525), training data (M = 3.92, SE = 0.0506), output (M = 3.85, SE = 0.0500), algorithm (M = 3.77, SE = 0.0520), companies (M = 3.42, SE = 0.0521), and governments (M = 3.09, SE = 0.0536).\nWe asked whether participants trust that \"the creators of large language models (e.g., OpenAI and GPT-4) put safety over profits\" (yes: 22.5%, not sure: 28.7%, no: 48.8%), \"the creators of an AI can control all current and future versions of the AI\" (yes: 26.9%, not sure: 24.1%, no: 45.9%), \"governments have the power to regulate the development of AI\" (yes; 52.0%, not sure: 24.1%, no: 23.9%), and, \"To what extent do you agree or disagree that governments have the power to effectively enforce regulations on the development of AI?\" (71.1% agreement).\nOn a series of agree-disagree questions, people were largely split on whether they trusted each of four specific types of AI: \"I trust game-playing AI\" (59.8% agreement); \"I trust large language models\" (53.5%); \"I trust robots\" (47.5%); and \"I trust chatbots\" (46.5%)."}, {"title": "A1.2.2 Positive Emotions.", "content": "Most of our measures of moral concern have been explicit statements that could be considered rational or logical, rather than the more intuitive, emotional aspects of judgment and decision making. One of the most popular trends in psychology research in the 21st century has been positive psychology [6], particularly the role of positive emotions [3]. We assessed these by asking, \"To what extent do you, as a human, feel the following emotions towards robots/Als?\" on a 1-7 sliding scale (1 = not at all, 4 = a moderate amount, 7 = \"very much) for six emotions: awe (3.90), excitement (3.76), respect (3.53), admiration (3.50), pride (3.30), and compassion (3.02)."}, {"title": "A1.2.3 Uploads.", "content": "We probed participant opinions on a speculative future scenario in which humans can upload their minds to computers, which has been discussed in numerous academic and science fiction works [for a review, see Sandberg and Bostrom 5]. The possibility of mind uploading presents an alternative future trajectory of AI instead of the de novo AI, such as large language models, that is the current focus of most AI researchers.\nWe developed two questions based on the standard format of policy proposals in the General Social Survey [7]. The first question, which presented the scenario straightforwardly, resulted in 41.2% agreement with, \"I support humans using advanced technology in the future to upload their minds into computers.\" In a more detailed question, we found that 39.3% supported the position (i.e., response higher than four, excluding responses of exactly four), \"In the future, humans could upload their minds into computers. Some people think that this would be very good because uploaded humans could consume fewer resources, live longer free from biological disease, and have enhanced intelligence and a greater ability to improve the world. Others disagree and think that uploading would mean that we are no longer truly human, change who we are and how we want to live, and distract us from making the real world a better place. Where would you place yourself on this scale?\" (sliding scale: 1 = oppose mind uploads, 4 = neither oppose nor support uploads, 7 = support mind uploads)."}, {"title": "A1.2.4 Additional Replications.", "content": "In addition to the replicated questions mentioned in the main text from YouGov on concern for human extinction [9] and support for the six-month pause on advanced AI development [8] and its reversed formulation, we replicated another YouGov question [10], \"How likely do you think it is that artificial intelligence (AI) will eventually become more intelligent than people?\" (very likely, somewhat likely, not very likely, not likely at all, it is already more intelligent than people, not sure), finding that 69.8% consider it likely, compared to 57% found by YouGov, and a question posed by science communicator Hank Green on Twitter [4], \"Which universe is the better one:\" (one with humans, one without humans), finding 89.9% selecting \"one with humans\" compared to 58.9% in Green's Twitter poll."}, {"title": "A1.3 Predictors of attitudes towards sentient Al", "content": "Because of the large number of questions in this survey and their novelty, our predictive analyses rely on exploratory descriptive tests rather than confirmatory tests of particular ex ante hypotheses. While further testing and theorization of these relationships is beyond the scope of the current work, it will be an important area of research as this field develops-particularly insofar as interaction with apparent digital minds relates to ongoing social dynamics and disparities in human-Al interaction.\nWe fit multivariate generalized linear models in which participant characteristics predicted each of a variety of indices: Ban Support, Existential Threat, General Mind Perception, General Moral Concern for All Als, General Moral Concern for Sentient Als, General Threat, LLM Mind Perception, LLM Suffering Concern, Mind-Related Anthropomorphism, Positive Emotions, Protection Support, Slowdown Support, Target-Specific Moral Concern, and Trust; we also predicted two individual items: Subservience and 100-Year Likelihood of Sentient AI. For each outcome, we fit a model for each"}]}