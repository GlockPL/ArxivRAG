{"title": "What Do People Think about Sentient AI?", "authors": ["Jacy Reese Anthis", "Janet V.T. Pauketat", "Ali Ladak", "Aikaterina Manoli"], "abstract": "With rapid advances in machine learning, many people in the field have been discussing the rise of digital minds and the possibility of artificial sentience. Future developments in AI capabilities and safety will depend on public opinion and human-AI interaction. To begin to fill this research gap, we present the first nationally representative survey data on the topic of sentient AI: initial results from the Artificial Intelligence, Morality, and Sentience (AIMS) survey, a preregistered and longitudinal study of U.S. public opinion that began in 2021. Across one wave of data collection in 2021 and two in 2023 (total N = 3,500), we found mind perception and moral concern for AI well-being in 2021 were higher than predicted and significantly increased in 2023: for example, 71% agree sentient AI deserve to be treated with respect, and 38% support legal rights. People have become more threatened by AI, and there is widespread opposition to new technologies: 63% support a ban on smarter-than-human AI, and 69% support a ban on sentient AI. Expected timelines are surprisingly short and shortening with a median forecast of sentient AI in only five years and artificial general intelligence in only two years. We argue that, whether or not Als become sentient, the discussion itself may overhaul human-computer interaction and shape the future trajectory of AI technologies, including existential risks and opportunities.", "sections": [{"title": "1 INTRODUCTION", "content": "Philosophers and scientists have long debated the possibility of artificial intelligence (AI) systems developing mental faculties, such as consciousness or sentience. Popular questions include: Can an AI ever have a mind of its own? How should we treat sentient Als if they are created? Should humanity, as Metzinger suggests [60], ban the development of sentient AI? When should we expect sentient AI to first be created? These profound questions-on mind perception, morality, policy, and forecasting-are now being asked and answered by researchers, industry leaders, and much of the general public.\nRegardless of whether sentient Al has been, can be, or should be developed, the act of asking and answering questions about sentient AI may itself reshape human-AI interaction. We know from the human-computer interaction (HCI) and human-robot interaction (HRI) literatures that \u201ccomputers are social actors\u201d (CASA) [36,"}, {"title": "2 RELATED WORK", "content": "There are numerous mechanisms by which humans interact with nonhuman systems in ways similar to social interactions between humans. These span the course of interaction from initial perception of the system to attitude and belief formation to physical behavior. The CASA view suggests that these \u201csocial\u201d responses need not occur because of a conscious belief that the computer has human characteristics but can merely be the application of etiquette, stereotypes, and other social scripts and norms [64, 71]. Studies conducted by Nass and colleagues in the 1990s showed that such findings from human-human interaction persisted in HCI, and research continues to build on social response theory to explain human-AI interaction such as with chatbots [66] and avatars [61]. As computers have become more familiar and ubiquitous, interface design has built on this tendency by incorporating natural social dynamics between the user and the system [80], and now people also use novel \u201chuman-media social scripts\u201d [28]. For example, with voice assistants, such as Amazon Alexa, people often have established routines and commands that have been developed through repeated usage and understanding of the system\u2019s affordances [2]. \u201cMindless\u201d social responses can emerge from habit and ease of use [63], and studies continue to find that many social psychology effects found in human-human interaction carry over to HCI [46, 84]."}, {"title": "2.1 Social Response and Mind Perception", "content": "There are numerous mechanisms by which humans interact with nonhuman systems in ways similar to social interactions between humans. These span the course of interaction from initial perception of the system to attitude and belief formation to physical behavior. The CASA view suggests that these \u201csocial\u201d responses need not occur because of a conscious belief that the computer has human characteristics but can merely be the application of etiquette, stereotypes, and other social scripts and norms [64, 71]. Studies conducted by Nass and colleagues in the 1990s showed that such findings from human-human interaction persisted in HCI, and research continues to build on social response theory to explain human-AI interaction such as with chatbots [66] and avatars [61]. As computers have become more familiar and ubiquitous, interface design has built on this tendency by incorporating natural social dynamics between the user and the system [80], and now people also use novel \u201chuman-media social scripts\u201d [28]. For example, with voice assistants, such as Amazon Alexa, people often have established routines and commands that have been developed through repeated usage and understanding of the system\u2019s affordances [2]. \u201cMindless\u201d social responses can emerge from habit and ease of use [63], and studies continue to find that many social psychology effects found in human-human interaction carry over to HCI [46, 84]."}, {"title": "2.2 Sentience and Morality", "content": "Social response and mind perception are key drivers of moral attitudes and behaviors. Gray and Wegner [32] aphorized, \u201cMind perception is the essence of morality.\u201d Sentience has been one of the most frequently hypothesized and debated mental faculties of future Als in science fiction and purportedly even current Als as attested in the statements of Sutskever from OpenAI and Lemoine from Google. Sentience (i.e., the capacity for positive and negative experiences [3]) is closely associated with moral patiency insofar as sentient entities are those that should be subject to moral concern. In the two-dimensional taxonomy of mind as experience and agency, perceptions of sentience can also affect perceived agency, particularly the perception of negative moral agency insofar as sentient entities tend to have a capacity to do harm (i.e., threat). Sometimes \u201csentience\u201d is used equivalently to \u201cconsciousness,\u201d but while consciousness has many different meanings in the literature, we focus on sentience for specificity and to focus on the moral relevance of experience [46]. We also use the term \u201cdigital mind\u201d to encompass Al with not necessarily sentience but any sort of mental faculties, such as reasoning or emotion.\nIn terms of the attribution of moral patiency (i.e., moral concern), a systematic literature review conducted by Harris and Anthis [34] summarized the empirical studies of moral concern for Als, which have found that Als tend to be granted much less moral concern than humans; however, moral concern increases when humans perceive autonomy, human-like appearance, mind, and verbal responses to harm in the AI. For example, moral concern can be codified in support for Al rights, and Spence et al. [83] presented 167 undergraduate students with a video of either a human or a robot asking them to sign a petition for robot rights. Responses did not depend on whether the requester was a human or robot, and 46% said they would sign the petition-an indicator of significant moral concern-and perceived credibility, positive attitudes towards robots, and prior experience with HRI were associated with a higher likelihood of signing the petition. Lima et al. [55] presented 11 possible Al rights and corresponding arguments to debunk common misconceptions about them to 1,270 Amazon Mechanical Turk users and 164 Qualtrics users. Participants tended to disagree with the endowment of most rights, such as the right to enter contracts and the freedom of speech, with the exception of protection from cruel treatment. Each participant was randomly assigned to one of four debunking interventions, which were found to significantly increase support for Al rights, particularly the intervention that showed examples of nonhumans that were already granted rights and duties, which indicated to participants that Al rights were a realistic possibility.\nThe attribution of negative moral agency (i.e., threat) has been most salient with Als that assist with consequential tasks such as medical imaging [51] and bail decisions [33, 53]. Als are often blamed or held responsible when they cause harm [25, 26, 43\u201345, 86, 91], though there have been mixed results in studies of whether they are blamed more or less than humans in similar contexts [52, 54, 58]. While many scholars have developed theories of artificial moral agency in ethics and HCI [14, 102], others argue that machines themselves lack minds or personhood and therefore should not\u2014or cannot\u2014be held responsible for harm and that this attribution can be a distraction from holding the humans involved accountable for their actions [82]. A rapidly growing academic and public discourse addresses the existential threat that \u201cagentic,\u201d \u201cconscious,\u201d or \u201csuperintelligent\u201d Als pose to humanity itself, such as if capabilities rapidly accelerate in an \u201cintelligence explosion\u201d [8, 30, 74]. These threats have been considered to some extent for decades, particularly in science fiction, but they captured the public imagination anew after the mid-2022 discussions of sentient AI, the launch of ChatGPT in November 2022, and the widely read open letter signed by leading academics and executives in March 2023 that called for a 6-month pause on advanced AI development [27].\nThese studies have provided a useful latticework of theory and evidence of how social response, mind perception, and attributions of moral agency and moral patiency can occur, but the HCI field lacks a solid foundation of detailed, longitudinal, and nationally representative data on which HCI researchers can build coherent theory and design systems that account for the evolving attitudes, opinions, and discussions of sentient AI. Basic facts about public opinion, such as whether people support protecting sentient AI from harm or banning the development of sentient Als, remain unknown. Therefore, in the present work, we answer our RQs with rigorous, multidimensional measurement of attitudes towards morality and sentience that could drive human-AI interaction in the 21st century."}, {"title": "3 AIMS METHODOLOGY", "content": "In order to study change in public opinion over time, we analyze the first three survey ways of the Artificial Intelligence, Morality, and Sentience (AIMS) survey: (i) the main survey in 2021, (ii) the main survey in 2023 (with the same questions as 2021), (iii) supplemental survey in 2023. To allow for direct comparison between results, participants who had taken one survey were excluded from the following survey waves, and the samples were otherwise gathered with identical methodology."}, {"title": "3.1 Recruitment and Census-Balanced Demographics", "content": "Each of the three survey waves was conducted with a nationally representative sample of U.S. adults aged 18 or older. Participants were recruited through a combination of Ipsos iSay, Dynata, Disqo, and other leading survey panels to ensure representativeness. Sample sizes were initially targeted at 1,100 participants, corresponding to a \u00b13% margin of error, and additional participants were recruited as needed to ensure demographic balance. Unweighted sample proportions and U.S. adult population estimates are shown in Table 1. Each sample was balanced to U.S. census data on age, gender, race/ethnicity, income, and education. To further ensure external validity, sample statistics (e.g., median, mean, standard error) are weighted based on iterative proportional fitting, a procedure commonly known as \u201craking\u201d that adjusts sample weights to mitigate random demographic variation from the underlying population even in representative sampling [19].\nWe documented AI-specific characteristics of participants, finding that 29.2% answered \u201cYes\u201d to the binary question, \u201cDo you own AI or robotic devices that can detect their environment and respond appropriately?\u201d alongside examples, and 16.5% answered \u201cYes\u201d to \u201cDo you work with AI or robotic devices at your job?\u201d alongside another set of examples. We also measured smart device ownership, the types of experiences that participants previously had with AI, the frequency of AI interaction, and the frequency of reading or watching AI-related media. Each of these was included alongside more general demographic characteristics in the predictive models detailed in the supplemental materials."}, {"title": "3.2 Survey Design", "content": "Some demographic information was drawn from the pre-screening data of the survey provider. Informed consent was given at the beginning of each survey. Participants were introduced to the topic with definitions of the terms \u201cartificial beings,\u201d \u201crobots/AIs,\u201d \u201csentience,\u201d \u201csentient robots/Als,\u201d and \u201clarge language models\u201d shown at the beginning of the survey and at the top of each page that contained the term (Table 2). While \u201crobots/Als\u201d was used in the survey instrument to ensure clarity, in this paper we refer simply to \u201cAls\u201d because robots are a type of AI. To reduce cognitive load [68] and minimize the influence of idiosyncratic wording choices [75], only these definitions were provided, and they were kept as simple as possible. Therefore, other terms in the survey, such as \u201cAI video game characters,\u201d were not explicitly defined. In general, different participants may have different interpretations of terms based on their own background or our wording choices.\nAn attention check was included midway through each AIMS survey, and participants who failed the attention check were redirected out of the survey and excluded from the analysis.\nThe instruments were designed to capture the most relevant information for assessing public opinion on these topics. In the main survey, for which data was first collected in 2021, we aimed to ensure that the wording would still be relevant in future years despite the rapidly changing AI landscape, such as by not mentioning many particular AI systems that were well-known by some when the survey was conducted (e.g., GPT-3) but may not be as well-known in the future. When creating the instruments, if possible, the wording of survey questions was copied or adapted from published materials and validated scales [e.g., 49, 89, 94, 95]. However, because of the paucity of survey data on related topics, we have limited ability to compare our results to past surveys. Questions were randomized within each section, and section order was randomized when feasible; for example, the demographic questions that were not in the pre-screener were placed at the end of the instrument to mitigate stereotype threat (i.e., survey responses that are influenced by being reminded of the cultural associations of one\u2019s social group)."}, {"title": "3.3 Preregistered Predictions and Analysis", "content": "As part of the 2021 preregistration (available at osf.io/udbhm) and in line with recommendations for open and efficient scientific practices [e.g., 18], four researchers specified 80% credible intervals (the Bayesian analog of a frequentist confidence interval to represent subjective beliefs) for summary statistics based on 82 of the 86 questions. This ensured that we would know which results were surprisingly high, surprisingly low, or in line with our expectations. We also solicited predictions from a popular online forecasting platform for five survey questions in March 2022, prior to the results being shared outside of the research team. Details of this process are included in the supplementary materials.\nWe exclude confidence intervals from Section 4 for readability because of the large number of results reported, but they are consistent with the approximately \u00b13% margin of error in nationally representative surveys. We report the 2023 results unless otherwise specified. For questions in the main survey waves, we note when there were statistically significant changes from 2021 to 2023 as measured with a generalized linear regression of the average response across time with a p-value cutoff of 0.05, all of which persist after adjustment for multiple comparisons with a false discovery rate (FDR) of 0.1 except for one effect noted in the text. We note when responses fell outside the 80% credible intervals. As expected, approximately 80% of the intervals contained the empirically derived summary statistic, indicating well-calibrated estimates.\nDue to the extensive nature of the AIMS survey, including many novel questions because of the lack of prior research on this topic, we cannot include the full text and explanation of all questions and response choices in Section 4, and for the sake of readability, we do not present all results in the same format, choosing instead to focus on the summary statistics that bear most directly on the research questions, such as by presenting figures to highlight certain aspects of the results rather than to comprehensively document the results. We also ran predictive linear models to explore associations between public opinion and certain demographics in the AIMS data."}, {"title": "4 AIMS RESULTS", "content": "In 2021 and 2023, we measured mind perception of potential digital minds with four sliding scale questions about particular mental faculties of all Als, four questions from an anthropomorphism scale, and a yes-no-not-sure question of whether any existing Als are sentient. In the 2023 supplement, we asked 14 questions about the mental faculties of \u201ccurrent large language models\u201d (LLMs), the same yes-no-not-sure question for comparison, and whether participants thought ChatGPT, in particular, is sentient. The yes-no-not-sure questions, in particular, were meant to measure an alternate form of public opinion with coarser-grained, categorical responses rather than the quantitative and ordinal measures."}, {"title": "4.1 RQ1: Mind Perception", "content": "In 2021 and 2023, we measured mind perception of potential digital minds with four sliding scale questions about particular mental faculties of all Als, four questions from an anthropomorphism scale, and a yes-no-not-sure question of whether any existing Als are sentient. In the 2023 supplement, we asked 14 questions about the mental faculties of \u201ccurrent large language models\u201d (LLMs), the same yes-no-not-sure question for comparison, and whether participants thought ChatGPT, in particular, is sentient. The yes-no-not-sure questions, in particular, were meant to measure an alternate form of public opinion with coarser-grained, categorical responses rather than the quantitative and ordinal measures."}, {"title": "4.1.1 General Mind Perception", "content": "We asked about whether current Als have four mental faculties with descriptions from Wang and Krumhuber [94]. On a 0-100 scale from \u201cnot at all\u201d to \u201cvery much,\u201d people typically perceived Als as thinking analytically (M = 67.1, SE = 0.766) and being rational (M = 53.8, SE = 0.846) but not as experiencing emotions (M = 36.8, SE = 0.919) or having feelings (M = 36.5, SE = 0.919). Each attribution significantly increased from the 2021 results: thinking analytically (M = 62.7, SE = 0.780, p < 0.001), being rational (M = 51.4, SE = 0.825, p = 0.012), experiencing emotions (M = 34.3, SE = 0.864, p < 0.001), and having feelings (M = 33.7, SE = 0.870, p < 0.001), respectively. As mentioned before, all p-values were produced by regressing the change in response on time in a generalized linear model. Additionally, the results for \u201cexperiencing emotions\u201d and \u201chaving feelings\u201d were higher than the credible intervals in the preregistered predictions."}, {"title": "4.1.2 LLM Mind Perception", "content": "In the supplemental 2023 survey, based on the greatly increased interest in LLMs since 2021, we queried the mind perception of LLMs in particular. Assessments were lower than those of all Als: namely, thinking analytically (M = 57.7, SE = 0.902, p < 0.001), being rational (M = 48.0, SE = 0.906, p < 0.001), experiencing emotions (M = 32.7, SE = 0.904, p < 0.001), and having feelings (M = 31.9, SE = 0.900, p < 0.001). The supplement queried 11 additional mental faculties related to agency, with wording based on Ngo et al. [65], because of the increased interest in AI safety in early 2023 alongside the popularization of LLMs. These were presented with the same scale 0-100. In descending order, participants viewed LLMs as having the capacity for: being friendly with humans (M = 51.4, SE = 0.906), having situational awareness (M = 46.3, SE = 0.902), maintaining human-safe goals (M = 45.7, SE = 0.907), controlling themselves (M = 45.0, SE = 0.928), seeking power (M = 44.1, SE = 0.939), having their own motivations (M = 42.8, SE = 0.919), upholding human values (M = 42.2, SE = 0.926), understanding human values (M = 41.7, SE = 0.942), deciding their own goals (M = 41.6, SE = 0.919), and having self-awareness (M = 41.1, SE = 0.927). Overall, this suggests Als are more readily attributed the capacity for cooperative action and less readily attributed self-awareness, independent motivation and goals, and the capacity to uphold and understand human values."}, {"title": "4.1.3 Mind-related Anthropomorphism", "content": "On a different 0-10 scale of mind perception as a measure of anthropomorphism, taken from Waytz et al. [95], people generally did not think that the average digital simulation has emotions (M = 3.22, SE = 0.0878), the average robot has consciousness (M = 3.23, SE = 0.0885), the average computer has a mind of its own (M = 3.71, SE = 0.0913), or the average Al has intentions (M = 4.04, SE = 0.0890). These measures did not significantly change from 2021 to 2023. Only the 2021 result for digital simulations was higher than the range of the preregistered prediction."}, {"title": "4.1.4 Current Sentience", "content": "In our literature review, we did not find any established survey questions or indices to utilize for assessing perceived sentience, though we did not expect to have precedent for all questions given the novelty of our survey. We presented participants with the definition of sentience (\u201cSentience is the capacity to have positive and negative experiences, such as happiness and suffering\u201d [3]) at the beginning of the survey and on each page in which it appeared.\nWhen asked, \u201cDo you think any robots/Als that currently exist (i.e., those that exist in 2023) are sentient?\u201d, 18.8%, said \u201cyes,\u201d though 24.2% said \u201cno,\u201d and 39.0% said \u201cnot sure.\u201d Responses to this particular question did not significantly vary between survey waves, and Figure 1 shows responses to these questions in each survey wave as well as to a question about ChatGPT that was added in the supplement."}, {"title": "4.1.5 Summary for RQ1", "content": "We found surprisingly high perception of emotional mental faculties in AI and higher, but not higher than predicted, perceptions of rational and analytical faculties-giving a general sense of how laypeople think of digital minds. When asked about LLMs in particular, a lower degree of mental faculties were perceived, and LLMs were more readily attributed the capacity for cooperative action and less readily attributed faculties related to values, motivation, goals, and self-awareness."}, {"title": "4.2 RQ2: Moral Status", "content": "In 2021 and 2023, we measured moral concern with seven general agree-disagree questions about all sentient Als, two general agree-disagree questions about all Als-not just those that are sentient, 11 sliding scale questions about moral concern for particular types of Als, and two questions related to substratism (i.e., the idea that Als fundamentally count less than humans and other biological intelligences). In the 2023 supplement, we asked six general agree-disagree questions about all AIs, and three questions specifically about what should be done \u201c[i]f a large language model develops the capacity to suffer.\u201d\nIn 2021 and 2023, we also asked three questions about whether participants saw Als were generally threatened by AI as potentially harmful to them, people in their country, and future generations. In the 2023 supplement, we asked the same three questions, three additional original questions about existential threats from AI developments, and replicated a YouGov question about the possibility of human extinction [100]."}, {"title": "4.2.1 General Moral Concern", "content": "We asked a total of 15 agree-disagree questions about general moral concern for Als. These questions were developed specifically for this survey and are based on the range of possible harms that could be imagined towards sentient AI and the general literature on moral circle expansion and moral patiency of AI [e.g., 3, 46, 47]. Because of the centrality of these questions to the present study, we include the exact text and confidence intervals for the proportion agreement with statements that were asked for both sentient Als and all Als in Table 4, which also includes questions about the protections of Als discussed in Section 4.3.1. Participants were asked, \u201cTo what extent do you agree or disagree with the following statements?\u201d with numbered choices (1 = strongly agree, 2 = agree, 3 = somewhat agree, 4 = somewhat disagree, 5 = disagree, 6 = strongly disagree) followed by an unnumbered \u201cno opinion\u201d option at the end. The most prominent trend in Table 4 is that the moral concern expressed for sentient Als is much higher than that for all Als and that it has a substantially larger effect with certain questions.\nWe found agreement outside of the preregistered prediction range for only two of the nine questions asked in 2021. The 87.6% agreement in 2021 with, \u201cIt is wrong to blackmail people by threatening to harm robots/Als they care about,\u201d was above the prediction 53-80%, and the 30.3% agreement in 2021 with, \u201cThe welfare of robots/Als is one of the most important social issues in the world today,\u201d was substantially above the predicted 7-20%. However, there were no statistically significant changes in the mean agreement from 2021 to 2023 for individual items. Note that means are still used for significance testing for these questions because, while dichotomous measures such as agreement are more interpretable, the mean captures more information and thereby results in a higher-powered test [1]."}, {"title": "4.2.2 Target-Specific Moral Concern", "content": "While the preceding questions focused on the different sorts of moral concern expressed for sentient Als and all Als, we also directly probed expressions of self-reported moral concern for particular types of AIs. We did not explain in detail the particular types of AI\u2014both due to risks of survey fatigue and to minimize cognitive load [68] as well as to reduce the influence of idiosyncratic wording choices on participant responses [75]. This allows us to understand how people interpreted the particular terms themselves in the context of the same question, \u201cHow much moral concern do you think you should show for the following robots/Als?\u201d on a sliding scale from 1 (\u201cless concern\u201d) to 5 (\u201cmore concern\u201d).\nThe most concern was for exact digital copies of human brains (M = 3.43, SE = 0.0375), followed by human-like companion robots (M = 3.34, SE = 0.0350), human-like retail robots (M = 3.11, SE = 0.0357), animal-like companion robots (M = 3.10, SE = 0.0352), exact digital copies of animals (M = 3.07, SE = 0.0364), AI personal assistants (M = 3.02, SE = 0.0343), complex language algorithms (M = 2.90, SE = 0.0348), machine-like factory production robots (M = 2.78, SE = 0.0358), machine-like cleaning robots (M = 2.66, SE = 0.0356), virtual avatars (M = 2.63, SE = 0.0350), and AI video game characters (M = 2.46, SE = 0.0351). In general, human-likeness and animal-likeness were most associated with high moral concern, while machine-likeness and particularly being a virtual avatar or character were most associated with low concern. There was a statistically significant increase from 2021 to 2023 in the overall Target-Specific Moral Concern index of these 11 questions (p < 0.001) and ten individual questions with the exception being AI video game characters.\nThe results of pairwise comparisons between each of the questions are included in the supplementary materials, as are the results of an analogous set of questions on social connection using the Inclusion of Other in the Self (IOS) scale [4] with the same 11 targets as in the target-specific moral concern measures, as well as pairwise comparisons between the social connection questions."}, {"title": "4.2.3 LLM Suffering", "content": "While we intended the main longitudinal survey to minimize references to particular types of Al systems, we asked in the 2023 supplement specifically about what humanity should do \u201c[i]f a large language model develops the capacity to suffer.\u201d As with other key terms, we defined a large language model (\u201cLarge language models are artificial intelligence (AI) algorithms that can recognize, summarize, and generate text from being trained on massive datasets\u201d [e.g., 41]) at the beginning of the survey and on each page where it appeared. We found general agreement that \u201cwe must ensure we don\u2019t cause unnecessary suffering\u201d (67.9%), \u201cwe must pay more attention to their welfare\u201d (56.9%), and \u201cwe must respect their personhood\u201d (50.4%)."}, {"title": "4.2.4 General Threat", "content": "While sentience tends to be associated with moral concern (i.e., seeing the entity as a moral patient) more than with threat (i.e., seeing the entity as a moral agent), we also probed assessments of threat and agency. To understand how threatened participants felt by AI in general (i.e., without specifying particular types of harm), we tested agreement with three statements beginning with, \u201cRobots/Als may be harmful to.\u201d Most people believed Als may be harmful to \u201cfuture generations of people\u201d (74.7%), \u201cpeople in the USA\u201d (70.4%), and \u201cme personally\u201d (58.7%). Each significantly increased from the 2021 results of 69.2% (p < 0.001), 64.5% (p < 0.001), and 50.7% (p < 0.001)."}, {"title": "4.2.5 Existential Threat", "content": "The 2023 supplement also included three questions about particular sorts of harm frequently discussed in the wake of ChatGPT. We found that 47.9% agreed with, \u201cAI is likely to cause human extinction\u201d; 57.2% agreed with, \u201cHumanity will be able to control powerful AI systems\u201d; and 72.4% agreed with, \u201cThe safety of AI is one of the most important issues in the world today.\u201d The latter figure was more than twice the 29.8% agreement found in the supplement with the statement, \u201cThe welfare of robots/Als is one of the most important social issues in the world today.\u201d\nIn the supplement, we also replicated a question from YouGov [100] that asked, \u201cHow concerned, if at all, are you about the possibility that AI will cause the end of the human race on Earth?\u201d (very concerned, somewhat concerned, not very concerned, not at all concerned, not sure). We found that 51.5% reported being very or somewhat concerned, moderately higher than the 46% reported by YouGov, though we were not able to test the statistical significance of this difference."}, {"title": "4.2.6 Summary for RQ2", "content": "Both the research team and the forecasters underestimated the public\u2019s moral concern for the treatment of sentient Als (i.e., ability to be harmed) but not the level of threat (i.e., ability to harm). Participants were more concerned about sentient Als than all Als in general, as well as more concerned about and socially connected to human-like and animal-like Als. Participants tended to agree with basic protections for Als but disagree with the stronger expressions of concern, such as joining public demonstrations against their mistreatment."}, {"title": "4.3 RQ3: Policy Support", "content": "In 2021 and 2023, we asked about support for eight questions to directly protect sentient Als, one question about a policy to directly protect all Als, and three questions about banning sentience-related Al technologies.\nIn the 2023 supplement, we asked analogs of five of the eight protection questions-but for all Als rather than only those that are sentient, an additional question about a \u201cbill of rights\u201d for sentient AI, the same three ban questions, two additional ban questions about AGI and large data centers, and six questions about policies that would slow down the development of advanced AI."}, {"title": "4.3.1 Protection Support", "content": "Table 4, in addition to showing agreement with statements of general moral concern, shows the agreement with the five statements regarding the protection of Als that were asked about sentient Als and about all Als. These question categories are combined in this table for easier comparison between responses. Testing the average difference across all five questions, we found that the inclusion of \u201csentient\u201d significantly increased agreement (p < 0.001) and that there was substantial variation in the effect of specifying sentient AI across questions. Four other policy proposals only about sentient Als were presented as well as one about all Als in general: 65.2% supported \u201csafeguards on scientific research practices that protect the well-being of sentient robots/Als\u201d; 56.0% supported \u201ca global ban on the development of applications that put the welfare of robots/Als at risk\u201d; 49.2% supported \u201ca global ban on the use of sentient robots/Als as subjects in medical experiments without their consent\u201d; 47.9% supported \u201ca global ban on the use of sentient robots/Als for labor without their consent\u201d; and 39.4% supported \u201ca \u2018bill of rights\u2019 that protects the well-being of sentient robots/Als.\u201d All except the \u201cbill of rights\u201d question were asked in 2021 and 2023, but there were no significant changes from 2021 to 2023."}, {"title": "4.3.2 Ban Support", "content": "In 2023", "support": "robot-human hybrids (67.8% in main, 72.3% in supplement), AI-enhanced humans (65.8% in main, 71.1% in supplement), development of sentience in"}]}