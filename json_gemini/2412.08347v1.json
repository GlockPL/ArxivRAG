{"title": "SmolTulu: Higher Learning Rate to Batch Size Ratios\nCan Lead to Better Reasoning in SLMs", "authors": ["Sultan Alrashed"], "abstract": "We present SmolTulu-1.7b-Instruct\\u00b9, refer-\nenced in this report as SmolTulu-DPO-1130, an\ninstruction-tuned language model that adapts\nAllenAI's Tulu 3 post-training pipeline (Lam-\nbert et al., 2024a) to enhance Huggingface's\nSmolLM2-1.7B base model. Through com-\nprehensive empirical analysis using a 135M\nparameter model, we demonstrate that the rela-\ntionship between learning rate and batch size\nsignificantly impacts model performance in a\ntask-dependent manner. Our findings reveal\na clear split: reasoning tasks like ARC and\nGSM8K benefit from higher learning rate to\nbatch size ratios, while pattern recognition\ntasks such as HellaSwag and IFEval show op-\ntimal performance with lower ratios. These in-\nsights informed the development of SmolTulu,\nwhich achieves state-of-the-art performance\namong sub-2B parameter models on instruction\nfollowing, scoring 67.7% on IFEval (\\u039411%),\nand mathematical reasoning with 51.6% on\nGSM8K (13.4%), with an alternate version\nachieving scoring 57.1% on ARC (15.4%).\nWe release our model, training recipes, and\nablation studies to facilitate further research\nin efficient model alignment, demonstrating\nthat careful adaptation of optimization dynam-\nics can help bridge the capability gap between\nsmall and large language models.", "sections": [{"title": "1 Introduction", "content": "Recent advances in language model post-training\nhave demonstrated remarkable improvements in\nmodel capabilities through careful application of\nsupervised finetuning (SFT), preference optimiza-\ntion, and reinforcement learning (Ouyang et al.,\n2022; Touvron et al., 2023). However, these tech-\nniques have primarily been developed and validated\non large language models with tens or hundreds\nof billions of parameters with smaller models be-\ning underexplored. The recently released Tulu 3\npipeline provides a comprehensive, open-source\napproach to post-training, but its effectiveness on\nsignificantly smaller models remains unexplored.\nUnderstanding how post-training dynamics scale\nto smaller models is crucial for democratizing ac-\ncess to high-quality language models and enabling\ndeployment in resource-constrained environments.\nWhile (Shallue et al., 2019) demonstrated that the\nrelationship between batch size and training steps\nfollows characteristic patterns across model fami-\nlies, they also found that the maximum useful batch\nsize varies between workloads and depends heavily\non model properties. This suggests that smaller\nmodels may require fundamentally different opti-\nmization strategies than their larger counterparts,\nespecially across different downstream tasks.\nThrough comprehensive ablations using Hug-\ngingface's SmolLM2-135M (Allal et al., 2024) as a\nbase, we demonstrate that the relationship between\nlearning rate and batch size significantly impacts\nmodel performance in a task-dependent manner\nduring supervised finetuning. Our findings reveal\na clear difference between reasoning and pattern\nrecognition tasks: benchmarks requiring complex\nreasoning like ARC and GSM8K show optimal\nperformance with higher learning rate to batch\nsize ratios, while pattern recognition tasks such\nas HellaSwag and IFEval benefit from lower ratios.\nThis aligns with theoretical work by (Masters and\nLuschi, 2018), who argue that smaller batch sizes,\ndespite their higher gradient variance, enable more\nfrequent parameter updates that can better track the\nunderlying objective's local structure.\nFurthermore, (Keskar et al., 2017) demonstrate\nthat optimization trajectories can lead to qualita-\ntively different solutions with varying generaliza-\ntion properties. Our empirical results suggest that\nhigher learning rate to batch size ratios may help\nsmaller models find flatter minima that generalize\nbetter to complex reasoning tasks, while lower ra-\ntios appear optimal for tasks dominated by pattern"}, {"title": "2 Related Works", "content": "recognition. This provides new insight into how\ncareful tuning of optimization parameters might\nhelp compensate for limited model capacity in a\ntask-dependent manner.\nIn this work, we investigate the adaptation of\nthe Tulu 3 post-training pipeline (Lambert et al.,\n2024a) to enhance SmolLM2-1.7B (Allal et al.,\n2024), a compact language model with just 1.7 bil-\nlion parameters. Through a set of ablations, we\ndemonstrate that the relationship between learning\nrate and batch size plays a crucial role in determin-\ning model performance across different types of\ntasks.\nOur key contributions include:\n\\u2022 Empirical evidence demonstrating how learn-\ning rate to batch size ratios differentially affect\nreasoning and pattern recognition capabilities\nin small language models\n\\u2022 Release of SmolTulu, achieving state-of-the-\nart performance among sub-2B parameter\nmodels on instruction following and mathe-\nmatical reasoning with verifiably no contami-\nnation.\n\\u2022 Theoretical analysis connecting optimization\ndynamics to task-specific performance in\nsmall language models\nOur results indicate that while the core tech-\nniques from Tulu 3 translate effectively to smaller\nscales, achieving optimal performance requires\ncareful consideration of how optimization dynam-\nics change with model size and task type. Through\nthese insights, we hope to contribute to the devel-\nopment of more efficient and accessible language\nmodels and establish new best practices for train-\ning smaller models that diverge from conventional\nwisdom derived from large-scale training with the\nlearning rate linearly adjusted to the batch size."}, {"title": "2.1 Post-Training Recipes and Instruction\nTuning", "content": "The development of modern post-training pipelines\nbegan with InstructGPT (Ouyang et al., 2022),\nwhich established the core supervised finetuning\n(SFT) and reinforcement learning from human feed-\nback (RLHF) workflow. This approach has been\nwidely adopted and adapted in open-source efforts,\nnotably through works like Alpaca (Taori et al.,\n2023) and Vicuna (Chiang et al., 2023), which\ndemonstrated the viability of instruction tuning\nwith synthetic and user-generated data respectively.\nMore recent work has focused on improving these\npipelines, with projects like Tulu (Ivison et al.,\n2023) and Zephyr-8 (Tunstall et al., 2023), culmi-\nnating in comprehensive open pipelines like Tulu 3\n(Lambert et al., 2024a)."}, {"title": "2.2 Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) (Rafailov\net al., 2024) introduced a simplified approach to\npreference learning that eliminates the need for a\nseparate reward model and complex RL training\nloops. This has been followed by various refine-\nments and alternatives, including SLiC-HF (Zhao\net al., 2023) and SimPO (Meng et al., 2024), which\nfurther simplified the training process while main-\ntaining or improving performance. These devel-\nopments have made preference learning more ac-\ncessible and computationally efficient, particularly\nimportant for resource-constrained settings."}, {"title": "2.3 Verifiable Rewards and Reinforcement\nLearning", "content": "Recent work has explored using verifiable out-\ncomes to improve model capabilities, particularly\nin domains like mathematics and coding. STaR\n(Zelikman et al., 2022) pioneered the use of self-\ntaught reasoning to improve mathematical capabili-\nties, while TRICE (Phan et al., 2023) developed al-\nternative approaches to learning from verifiable so-\nlutions. VinePPO (Kazemnejad et al., 2024) specif-\nically targeted mathematical reasoning through re-\ninforcement learning, demonstrating the effective-\nness of RL approaches when ground truth answers\nare available. This line of work has shown par-\nticular promise in improving specific capabilities\nwhile maintaining model generality. Reinforce-\nment Learning with Verifiable Rewards (RLVR)\nextends traditional RL approaches by using deter-\nministic binary rewards based on ground truth out-\ncomes rather than learned reward models(Lambert\net al., 2024a). This approach has shown particu-\nlar promise in domains with verifiable solutions\nlike mathematics and programming, where correct\noutputs can be automatically validated."}, {"title": "2.4 Learning Rate and Batch Size Studies", "content": "The relationship between learning rate and batch\nsize has been a subject of extensive study in deep\nlearning. Foundational work by (Goyal et al., 2018)\nestablished key principles for large-batch training,"}, {"title": "2.5 Efficient and Small Language Models", "content": "Research on efficient language models has seen\nrenewed interest with the success of smaller but\ncapable architectures. Notable examples include\nMicrosoft's Phi models (Gunasekar et al., 2023),\nwhich achieved strong reasoning capabilities at\n1.3B parameters (50.6% on HumanEval), and\nTinyLlama (Zhang et al., 2024), which adapted\nthe Llama architecture to 1.1B parameters. The\nSmolLM family (Allal et al., 2024) represents a\nsignificant advance in this direction, demonstrating\nthat carefully trained compact models can achieve\ncompetitive performance on a wide range of tasks\nwhile remaining deployable on consumer hard-\nware."}, {"title": "3 Supervised Finetuning", "content": "To ensure fair evaluation, we analyzed the contam-\nination levels of various benchmarks in the SFT\ndataset (allenai/tulu-3-sft-mixture). As shown in\nTable 1, most benchmarks exhibit minimal contam-\nination rates below 1.5%, with many showing zero\ncontamination. The highest contamination rate was\nobserved in PopQA at 7.21%, while critical evalu-\nation benchmarks like GSM8K, IFEval, and AGI\nEval showed negligible to zero contamination, en-\nsuring reliable performance measurements."}, {"title": "3.1 Dataset", "content": "Foundation models have demonstrated remarkable\ncapabilities across various tasks, yet their perfor-\nmance can be significantly influenced by the choice\nof hyperparameters during SFT. While work by\n(Smith et al., 2018) established foundational prin-\nciples for batch size scaling in neural networks,\nand (McCandlish et al., 2018) provided theoretical\nframeworks for understanding large-batch training,\nrecent work by (Masters and Luschi, 2018) sug-\ngests that the conventional wisdom about batch\nsize scaling may need reconsideration, particularly\nfor smaller models.\nOur experiments with SmolTulu variants build\nupon hyperparameter configurations established by\nAllenAI's extensive ablation studies for Tulu 3. As\nshown in Table 2, there is a notable pattern in the\nlearning rate to batch size ratio (LR/BS) across\nmodel scales: as models grow larger, this ratio\ntends to decrease. This aligns with the scaling\nlaws observed by (Kaplan et al., 2020) and the\ncompute-optimal training strategies described in\n(Hoffmann et al., 2022). The Tulu 3 70B model\nuses the smallest ratio of 0.016 \\u00d7 10-6, while our\nSmolTulu SFT-1130 variant employs a significantly\nlarger ratio of 11.25 \\u00d7 10-6."}, {"title": "3.2 Training", "content": "The supervised finetuning experiments reveal com-\nplex relationships between optimization dynamics\nand model capabilities that evolve with model scale.\nOur initial ablation study using a 135M parameter\nmodel demonstrates clear task-dependent patterns\nin the relationship between learning rate and batch\nsize ratios.\nAs shown in Figure la and Figure 1b, reason-\ning tasks like ARC and GSM8K consistently ben-\nefit from higher learning rate to batch size ra-\ntios. This aligns with theoretical work by (Keskar\net al., 2017). The improvement is particularly pro-\nnounced for GSM8K, where performance increases\nmonotonically with the learning rate to batch size\nratio.\nConversely, at the 135M scale, pattern recogni-\ntion tasks show markedly different behavior. Fig-\nure 1c and Figure 1d reveal that HellaSwag and\nIFEval achieve optimal performance with lower"}, {"title": "3.3 Results & Discussion", "content": "learning rate to batch size ratios. This contrast\nsuggests that at smaller scales, different types of\nlearning may benefit from fundamentally different\noptimization dynamics, supporting (Shallue et al.,\n2019)'s observation that optimal batch sizes vary\nsignificantly between workloads.\nHowever, these relationships become more nu-\nanced at larger model scales. In our 1.7B parameter\nmodel, while GSM8K continues to benefit from\nhigher ratios (achieving 51.6% with SmolTulu-\n1130's high ratio versus 44.7% with SmolTulu-\n1207's lower ratio), the pattern for other tasks shifts.\nNotably, IFEval performance improves with higher\nratios (67.7% vs 56.6%), while ARC and MMLU-\nPro show optimal performance with lower ratios\n(57.1% and 19.1% respectively).\nThis scale-dependent shift in optimization dy-\nnamics aligns with (Masters and Luschi, 2018)'s ar-\ngument that the relationship between batch size and\nlearning rate fundamentally changes with model\ncapacity. We hypothesize that at 135M parameters,\nthe limited model capacity forces a strict trade-off\nbetween different types of learning, requiring dis-\ntinct optimization strategies for reasoning versus\npattern recognition. At 1.7B parameters, the in-\ncreased capacity appears to enable more flexible\nlearning dynamics, allowing both GSM8K and IFE-\nval to benefit from aggressive optimization while\ntasks like ARC and MMLU-Pro benefit from more\nconservative approaches.\nThese findings suggest that the conventional wis-\ndom about learning rate and batch size relationships\nderived from large-scale training may need signifi-\ncant adaptation for smaller models. The interaction\nbetween model capacity and optimal optimization\nstrategy appears more complex previously thought."}, {"title": "4 Direct Preference Optimization", "content": "We conducted contamination analysis on the DPO\ndataset (allenai/llama-3.1-tulu-3-8b-preference-\nmixture) to verify evaluation fairness. Table 4\nshows consistently low contamination rates across\nbenchmarks, with most falling below 1%. The\nhighest contamination was found in PopQA at\n2.72%, while crucial benchmarks like GSM8K,\nIFEval, and BBH maintained zero contamina-\ntion, enabling trustworthy assessment of model\nimprovements through preference optimization."}, {"title": "4.1 Dataset", "content": "We employed Direct Preference Optimization\n(DPO) for preference learning while adapting the\nhyperparameters for our smaller model scale, using\nthe same pipeline as described in Tulu 3 (Lam-\nbert et al., 2024a). DPO allows direct optimization\nof the policy without requiring a separate reward\nmodel (Rafailov et al., 2024). The core objective\nfunction is shown in equation 1.\nWe used the length-normalized variant of DPO,\nwhich showed superior performance in Tulu 3's ab-\nlations, where the log probabilities are normalized\nby sequence length.\n\\u2022 Pre-computing and caching log probabilities\nfrom the reference model instead of keeping\nit in memory (Hu et al., 2024)\n\\u2022 Performing separate forward passes for cho-\nsen and rejected sequences rather than con-\ncatenating them\nHowever, our experiments revealed that smaller\nmodels benefit from different hyperparameter con-\nfigurations than their larger counterparts. As shown\nin Table 5, we maintained higher learning rate to\nbatch size ratios compared to Tulu 3's models, with\nSmolTulu DPO-1130 using a ratio approximately\n17 times larger than Tulu 3 8B's."}, {"title": "4.2 Training", "content": "For DPO-1130, we used a learning rate of 8.0 \\u00d7\n10-7 and batch size of 12, while DPO-1207 used a\nmore conservative learning rate of 5.0 \\u00d7 10-7 with\nbatch size 32. Both variants used:\n\\u2022 Maximum sequence length of 2,048 tokens\n\\u2022 KL penalty coefficient B = 5\n\\u2022 Linear learning rate schedule with 0.1 warmup\nratio\n\\u2022 Single training epoch\nOur experimentation with higher learning rate to\nbatch size ratios was motivated by the hypothesis\nthat smaller models may require larger per-example\nupdates to effectively learn from preference data,\nparticularly for complex reasoning tasks where they\nlack the inherent capacity of larger models."}, {"title": "4.3 Results & Discussion", "content": "learning rate to batch size ratio more similar to\nlarger models, showed stronger performance on\npattern recognition tasks like ARC (57.1%) and\nPIQA (76.4%).\nHowever, our exploration of hyperparameter con-\nfigurations was significantly constrained by compu-\ntational resources. Given that DPO builds directly\nupon the SFT model, the initial policy's conver-\ngence properties may significantly influence subse-\nquent preference learning. A comprehensive under-\nstanding would require extensive ablation studies\nacross different SFT checkpoints."}, {"title": "5 Reward Modelling", "content": "Our reward models (RM) were trained on the same\npreference dataset used in the DPO stage, combin-\ning UltraFeedback with additional synthetic prefer-\nence data generated through the Tulu 3 pipeline.\nWe maintained rigorous contamination controls\nacross all evaluation benchmarks, as detailed in\nearlier sections."}, {"title": "5.1 Dataset", "content": "Following the Tulu 3 methodology (Lambert et al.,\n2024a), we trained our reward models using the\nstandard pairwise preference learning objective.\nGiven a preference dataset D consisting of prompts\nx and two responses per prompt (y, y'), where one\nresponse is chosen yc and one is rejected yr, the\nreward model r\u00f8 is trained to maximize:"}, {"title": "5.2 Training", "content": "max E(x,yc,yr)~D[logo(r\u00f8(x, yc) \\u2013 r\u00a2(x, yr))]\n(2)\n\u03a5\u03c6\nwhere o is the logistic function. This objective\nmaximizes the difference between rewards, with\nthis difference representing the log-likelihood that"}, {"title": "5.3 Results & Discussion", "content": "The reward modeling experiments revealed in-\nteresting patterns consistent with our findings from\nSFT and DPO stages. SmolTulu RM-1130, em-\nploying a much larger learning rate to batch size\nratio, demonstrated strong performance across vari-\nous metrics on RewardBench (RB) (Lambert et al.,\n2024b), achieving 94.13% on standard chat evalua-\ntion and 75.54% on safety assessments, as seen in\nTable 8. This pattern of strong relative performance\nextends across other metrics, with SmolTulu RM-\n1130 achieving 73.17% accuracy on UltraFeedback\nbenchmark test preferences, falling only 4.17 per-\ncentage points short of Tulu 3's 77.34% despite\nusing approximately 21% of the parameters. Fol-\nlowing (Shallue et al., 2019)'s framework, these\nresults suggest that reward modeling may scale\nmore gracefully to smaller architectures than previ-\nously assumed, particularly when using appropri-\nately adapted optimization strategies.\nThe substantial performance gap between RM-\n1130 and RM-1207 (72.43% vs 58.59% on RB\nreinforces our earlier findings about the importance\nof learning rate to batch size ratios in smaller mod-\nels. The higher ratio used in RM-1130 appears to\nbe particularly crucial for reward modeling, where\nthe task of learning preference relationships may"}, {"title": "6 Reinforcement Learning with Verifiable\nRewards", "content": "While our initial experiments with RLVR showed\npromise, the computational requirements for thor-\nough hyperparameter exploration proved pro-\nhibitive.\nOur preliminary investigations suggest that the\nrelationship between learning rate and batch size\nmay be particularly complex in the RLVR setting,\nwhere the sparse binary reward signal introduces\nadditional optimization challenges for smaller mod-\nels. However, establishing concrete findings would\nrequire extensive ablation studies beyond our cur-\nrent computational resources. We leave comprehen-\nsive exploration of these dynamics to future work\nwith greater computational capacity."}, {"title": "7 Limitations", "content": "While our work demonstrates promising results in\nadapting the Tulu 3 pipeline to smaller models,\nseveral important limitations should be noted:\nBase Model Dependencies Our findings are spe-\ncific to SmolLM2 and may not generalize to other\nsmall models, particularly those with different pre-\ntraining approaches or architectural choices.\nMulti-Stage Optimization Understanding The\nrelationship between optimization choices in dif-\nferent training stages (SFT, DPO, RM, RLVR) re-\nmains poorly understood, especially in the context\nof smaller models. While we observed consistent\nbenefits from higher learning rate to batch size ra-\ntios, the theoretical foundations for this interaction\nacross training stages requires further investigation."}, {"title": "8 Conclusion", "content": "We have demonstrated that careful adaptation of\nmodern post-training techniques can yield strong\nresults even at significantly smaller model scales.\nWe found that smaller models may require sub-\nstantially different optimization dynamics than\ntheir larger counterparts to achieve optimal per-\nformance. Our empirical results align with the-\norical frameworks from optimization literature,\nsuggesting that higher ratios can help compensate\nfor limited model capacity, particularly on complex\nreasoning tasks.\nThe resulting model, SmolTulu, achieves state-\nof-the-art performance among sub-2B parameter\nmodels on instruction following while maintain-\ning strong mathematical reasoning capabilities as\nseen in Table 9. These results suggest that the ef-\nfectiveness of post-training pipelines like Tulu 3\ncan extend to much smaller scales when properly\nadapted. Our findings indicate that optimal training\nstrategies may need to vary significantly based on\nboth model scale and target capabilities.\nLooking forward, we believe this work opens\nup promising directions for making high-quality\nlanguage models more accessible and deployable\nin resource-constrained environments. Future work\ninvestigating adaptive optimization strategies that\naccount for both model scale and task requirements\ncould further advance this goal. Additionally, de-\nveloping theoretical frameworks that specifically\naddress the interaction between model capacity and\noptimization dynamics could help establish more\nprincipled approaches to training smaller models."}], "equations": ["max_{Eycard logo (Blog Tec) - Blog Toyr2))]}\nEyc,yr~D\n\\u03c0\\u03b8\n\\u03c0\\u03b8 (Ycx)\nTref (Ycx)\n\\u03c0\\u03b8(Yr|x)\nTref(Yrx)\n(1)", "max E(x,yc,yr)~D[logo(r\\u03c6(x, yc) \\u2013 r\\u00a2(x, yr))]\n(2)\n\\u03a5\\u03c6"]}