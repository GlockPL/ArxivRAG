{"title": "A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat Using Leader-Follower Strategy", "authors": ["Jinhui Pang", "Jinglin He", "Noureldin Mohamed Abdelaal Ahmed Mohamed", "Changqing Lin", "Zhihui Zhang", "Xiaoshuai Hao"], "abstract": "Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an evolving field in both aerospace and artificial intelligence. This paper aims to enhance adversarial performance through collaborative strategies. Previous approaches predominantly discretize the action space into predefined actions, limiting UAV maneuverability and complex strategy implementation. Others simplify the problem to 1v1 combat, neglecting the cooperative dynamics among multiple UAVs. To address the high-dimensional challenges inherent in six-degree-of-freedom space and improve cooperation, we propose a hierarchical framework utilizing the Leader-Follower Multi-Agent Proximal Policy Optimization (LFMAPPO) strategy. Specifically, the framework is structured into three levels. The top level conducts a macro-level assessment of the environment and guides execution policy. The middle level determines the angle of the desired action. The bottom level generates precise action commands for the high-dimensional action space. Moreover, we optimize the state-value functions by assigning distinct roles with the leader-follower strategy to train the top-level policy, followers estimate the leader's utility, promoting effective cooperation among agents. Additionally, the incorporation of a target selector, aligned with the UAVs' posture, assesses the threat level of targets. Finally, simulation experiments validate the effectiveness of our proposed method.", "sections": [{"title": "1. Introduction", "content": "Multi-UAV air combat is a complex military operation involving multiple UAVs engaging dynamic aerial targets. Each UAV as an independent agent, collaborating with others to defeat enemy forces. Unlike single-UAV missions, multi-UAV combat achieves a \"1+1>2\" effect through cooperation, which includes coordinated flight maneuvers, joint strikes, and the deployment of decoy tactics [42]. Given their cost-effectiveness and suitability for high-risk missions on modern battlefields, air combat systems are progressively transitioning from human-centered control to human-assisted operations, with UAVs autonomously sensing the external environment and making independent flight decisions.\nRule-based methods [2, 1, 12] have guided UAV behavior in combat scenarios through predefined rules and strategies. Furthermore, some studies [3, 13, 22] have integrated expert systems to analyze pilot decision-making experiences, extracting these decisions into rule sets and compiling them into decision-making databases. However, rule-based methods exhibit limitations when faced with complex and dynamic combat environments. Game theory [18] has been increasingly introduced into UAV decision modeling. It effectively describes interactions between agents and provides mathematical modeling tools for UAV missions and design constraints [4, 7, 36, 28]. However, game theory models face challenges in practical applications, including high modeling complexity and strong assumptions, which lead to increased computational complexity and potential deviations from actual results. Niu et al. [20] develope a perception-inspired learning framework to address multi-constrained collaborative planning problems. In the intelligent air combat decision-making, Pope et al.[23], in the DARPA AlphaDogfight close combat simulation, trained UAVs using a hierarchical structure and maximum entropy reinforcement learning, defeating an active-duty F-16 pilot. This achievement demonstrates the feasibility and effectiveness of reinforcement learning in addressing intelligent air combat decision-making problems [40, 32, 35]. The core of reinforcement learning lies in learning optimal decisions through continuous interaction with the environment, employing a \"trial-and-error\" process where agents aim to maximize cumulative rewards [9]. This is based on the theoretical foundation of the Markov Decision Process (MDP),"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Hierarchical reinforcement learning", "content": "Hierarchical Reinforcement Learning (HRL) [21] addresses the inefficient learning and the convergence issues caused by high-dimensional state and action spaces. By introducing multiple levels of abstraction, HRL enables the decomposition of complex tasks into simpler and more distinguishable sub-tasks [15], reducing the complexity of the problem space and enhancing the learning process by facilitating more efficient and targeted decision-making at various levels. Chen et al. [6] utilized hierarchical reinforcement learning (HRL) to transform a three-dimensional problem into a two-dimensional one, effectively mitigating the non-convergence issues associated with high-dimensional complexity. This strategy reduces the complexity of training through dimensionality reduction. Pope et al. [23, 24] proposed a hierarchical structure that divides tasks into sub-tasks, solving high-dimensional continuous control problems in 1v1 air combat scenarios. Kong et al. [14] extended the application to variable-scale formation air combat tasks by designing three sub-tasks and employing a cyclic soft actor-critic algorithm to learn sub-policies, enabling formations to exhibit effective cooperative behavior in both symmetric and asymmetric environments. To further improve the functionality of hierarchical strategies, recent studies have introduced explicit task division within hierarchical architectures. Mei et al. [19] proposed a two-layer decision-making framework in which the high-level strategy generates maneuvering commands under combat conditions, while the low-level strategy computes specific control signals for aircraft. Similarly, Chai et al. [5] separates the problem into macro and micro perspectives, generating corresponding actions at each level to enhance control efficiency. By creating an effective link between strategic decision-making and control execution, these architectures provide more flexible solutions for complex tasks."}, {"title": "2.2. Leader-follower formation control", "content": "The leader-follower formation represents an efficient organizational approach for team coordination and combat operations in dynamic and complex battlefield environments. The formation inherently establishes a hierarchical structure that enables leaders to assume a directive role to provide strategic guidance, while followers execute tasks aligned with the leader's intent, thereby enhancing the overall effectiveness of coordinated actions. Wang et al. [34] proposed a leader-follower-based cooperative formation trajectory tracking control method for multi-UAV systems, employing an integral backstepping approach to design the leader UAV's trajectory controller and a sliding mode controller to achieve precise formation control. Building on this foundation, Hao et al. [11] introduced a distributed leader-follower formation control approach that aligns the follower UAVs' headings with that of their leader while maintaining the desired relative positions, enabling a more flexible distributed cooperative control mechanism. Ranjan et al. [26] proposed a novel leader-follower formation control method from a behavior modeling perspective, defining the follower UAVs' linear and angular velocities as control inputs by simulating human pilot behaviors. To enhance the intelligence of leader-follower strategies, Liu et al. [16] integrated Deep Q-Networks into leader-follower formation control, enabling adaptive adjustments to formation behaviors. Wang et al. [33] developed a leader-follower PID-based formation control approach that incorporates a virtual-structure-based reinforcement learning scheme to construct"}, {"title": "3. Background", "content": ""}, {"title": "3.1. Problem description", "content": "In multi-UAV combat, two opposing forces, designated as Blue and Red, aim to destroy the UAVs of the opposing team while minimizing damage to their own side inflicted by the opponent. In multi-UAV combat, two opposing forces, designated as Blue and Red, aim to destroy the UAVs of the opposing team while minimizing damage to their own side inflicted by the opponent. Modern fighter operations utilize a two-UAV or three-UAV formation combat mode, with the leader serving as the primary and the follower providing auxiliary support. In this study, referencing this formation, heterogeneous UAVs with varying combat capabilities are categorized as leaders and followers. Each UAV is equipped with radar and other sensors to gather information about both allies and opponents. We employ the six-degree-of-freedom flight dynamics model provided by JSBSim [31] to simulate UAV movement within the environment. This model is extensively utilized in UAV research and serves as a foundational tool for accurately representing UAV flight dynamics."}, {"title": "3.2. State space", "content": "The states of UAVs include their own states and relative states with respect to other UAVs. We introduce a set of symbols and definitions related to aircraft flight posture and control to accurately model the movement. To represent the UAV's state, an 11-dimensional vector s is used, as shown in Table 1. The vector s includes position, orientation, velocity, and other relevant parameters that influence decision-making. In multi-UAV air combat, to enhance each UAV's situational awareness of the environment, it is crucial for each UAV to consider the spatial relationships between allies and targets when making maneuver decisions. These relationships are represented using relative states, including the angle off \u03b1 and target angle \u03b2. The angle off \u03b1 is primarily used for navigation and path planning. Smaller values of \u03b1 indicate that the UAV is more closely aligned with the target. This alignment reduces the complexity of the path planning and allows the UAV to focus on optimal routes toward the target. The target angle \u03b2 describes the specific location of the target within the UAV's field of view. Smaller values of \u03b2 indicate that the target is directly in front of the UAV or very close to the forward direction, making it easier for the UAV to track and engage the target effectively."}, {"title": "3.3. Action space", "content": "The six-degree-of-freedom (6-DOF) model provides a more comprehensive description of UAV movement compared to the 3-DOF model, enabling maneuverability in three-dimensional space. It allows the UAV to translate along the x, y, and z axes for precise positioning and to rotate around these axes to adjust its roll \u03c6, pitch \u03b8, and yaw \u03c8 angles in the environment. These rotations enable the UAV to adjust its posture and direction effectively. By setting the rotational motions \u03b4\u03c6, \u03b4\u03c1, and \u03b4\u03c9 along the three axes, as well as adjusting the throttle \u03b4th, the aircraft's direction and speed can be precisely controlled. This combination of translational and rotational movements enables"}, {"title": "3.4. Reward function", "content": "The reward function quantitatively measures the desirability of a state-action pair, guiding the UAV toward achieving its objectives. Positive rewards are typically assigned to desirable outcomes, while negative rewards correspond to undesirable ones. It serves as a critical feedback mechanism, informing the UAV's decision-making process and ensuring alignment with mission goals. In our framework, the reward function consists of posture reward, distance reward, and event reward components. The event reward is influenced by the weapon's attack zone, which restricts the UAV's ability to strike targets. The attack zone is defined as a sector with a radius of 4 km and an angle of 45 degrees."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Cooperative hierarchical framework", "content": "As the number of UAVs in air combat missions and the dimensionality of state variables in 6-DOF model increase, the high-dimensional state and action spaces pose significant challenges to the training process. Traditional algorithms typically decompose many-on-many air combat missions into multiple one-on-one engagements, which fail to adequately simulate collaboration among UAVs in real combat scenarios. To address these challenges, we propose a collaborative hierarchical framework that divides the decision-making process into three layers, as shown in Figure 3."}, {"title": "4.1.1. Policy selector with leader-follower strategy", "content": "The policy selector, as the global controller, evaluates and selects suitable sub-policies for different scenarios, enabling effective collaboration among UAVs and ensuring the achievement of overall mission objectives. In a one-on-one UAV battlefield, a UAV only needs to consider the opponent's policy to make decisions. However, collaboration among UAVs is a key factor in achieving combat effectiveness beyond that of a single UAV in multi-UAV combat missions. Existing works simplify multi-UAV combat into multiple one-on-one battles, neglecting the relationships among allies and failing to enable effective cooperation. To address this limitation, we propose a cooperative policy selector utilizing a leader-follower strategy. In the framework, the follower considers the leader's policy when choosing an action, enabling collaboration between the leader and the follower. The effectiveness of collaboration is reflected through rewards received from the environment based on the agents' states and actions. Reinforcement learning aims to maximize cumulative rewards discounted over time. The return of an agent is defined as:\nG\u2081 = \u2211_{k=0}^{\u221e} \u03b3^{k}r_{t+k+1},\nwhere \u03b3 \u2208 [0, 1] is the discount factor.\nDesigned for cooperative and competitive scenarios, MAPPO is a robust multi-agent reinforcement learning algorithm. It leverages the classical actor-critic framework and employs a neural network to approximate both the policy and the value function. The actor \u03c0(s;0) maps states to action distribution probabilities, enabling agents to make stochastic decisions. The critic V(s; 0) approximates the state value function V(s) = E[G|so = s], which evaluates the quality of the policy. Agents interact with the environment to collect multiple trajectories, which are stored in an experience buffer. At each step, an agent selects an action based on the current policy and receives the next state and reward from the environment through state transitions. The advantage function is calculated using generalized advantage estimation (GAE), defined as:\n\u0391\u2081 = \u2211_{l=0}^{k} (\u03b3\u03bb)^{k} \u03b4_{t+1},\nwhere d\u2081 = r\u2081 + yVoe (S1+1) \u2013 Voe (st) and A, is the advantage of taking action a, in state s\u2081. The critic is a centralized value function that evaluates the joint states and actions of all agents from a global perspective to accurately assess policies. However, using the same value function for all agents fails to capture the differences in roles among them. To address this issue, we propose an improvement to the critic that influences action selection. In our framework, the leader focuses on global information, while the follower devotes more targeted attention to the leader's policy when determining actions.Even when the leader's action is suboptimal, the follower is able to achieve higher rewards. To further enhance collaboration, a max-min criterion is employed to optimize the policy, maximizing the minimum possible outcome. The leader-follower update value functions as follows:\nV_{l}^{f}(x_{t}^{l}) = (1 - \u03b1) V_{t}^{f}(x_{t}^{l}) + \u03b1 [r_{t}^{f} + \u03b3 max_{a_{t+1}^{l} \u2208 A_{t+1}^{l}} V_{t+1}^{f}(x_{t+1}^{l})],\nV_{l}^{f}(s_{t}^{l}) = (1 \u2212 \u03b1) V_{s}^{f}(s_{t}^{l}) + \u03b1 [r_{t}^{f} + \u03b3 V_{l}^{f}(s_{t+1}^{l})],\nwhere Vf is the follower's value function, V is the leader's value function, and x' = (s,a') is the follower's state.\nOptimizing critic using the stored data from the experience buffer with a loss function of:\nL_{critic}(\u03b8_{c}) = E_{(s_{t},a_{t},G_{t})\u223cD} [V (s_{t};\u03b8_{c}) \u2013 G_{t}]\u00b2 .\nAt each time step, the expected advantage function is maximized by maximizing the policy loss objective function:\nL_{actor}(\u03b8_{a}) =E_{(s_{t},a_{t})\u223cD} [min (\u03c1 (s_{t}, a_{t};\u03b8_{a}) A_{t}, clip [\u03c1 (s_{t}, a_{t}; \u03b8_{a}), 1 \u2013 \u0454, 1 + \u0454] A_{t})],\n\u03c1 (\u03c2, \u03b1; \u03b8_{a}) = \u03c0(a_{s}; \u03b8_{a})/\u03c0_{old}(a/s; \u03b8_{a}),\nwhere is the current policy, \u03c0old is the old policy, e is a hyperparameter that controls the clipping range. The combination of policy loss and value loss also includes an entropy reward to encourage exploration, so that the overall objective function is:\nL = L_{critic}(\u03b8_{c}) + L_{actor}(\u03b8_{a}) + H,\nwhere H is the policy entropy averaged over all agents' entropy H\u2081 = \u2212 \u2211_{a} \u03c0(a|s) log n(a|s)."}, {"title": "4.1.2. sub-policy", "content": "Different sub-policies are selected by the policy selector to execute various flight maneuvers based on the current states. Training these sub-policies relies on the movement substructure of the hierarchical framework, which encompasses the middle and bottom layers. The movement substructure addresses various combat scenarios by training a series of sub-policies that can handle the complex dynamics of specific tactical situations and provide flexible responses, enabling diverse combat options. We categorized combat policies into three types:\n\u2022 The Target-Approaching Policy: Aims to minimize the distance to enemies to enable effective attacks or surveillance.\n\u2022 The Offensive Policy: Focuses on proactively attacking enemies to gain an advantage.\n\u2022 The Defensive Policy: Ensures safety by evading enemy attacks while seeking opportunities to counterattack.\nThe sub-policy flight problem is modeled as a Markov Decision Process (MDP) [41], defined as G = {S, A, R,T}, where S is state space, A is action space, R is reward function, and T is state transition function. The state consists of the aircraft state and the relative state. The aircraft state includes position, posture, and altitude, while the relative state is characterized by the angle, as shown in Figure 2. The UAV evaluates its posture based on azimuth and track angle and controls roll, yaw, pitch, and throttle through commands to the aileron d\u00f8, elevator \u03b4\u03c1, rudder \u03b4\u03c9 and throttle \u03b4th\u00b7\nWe propose a reward function to quantify the posture and distance-based threat relative to enemies. The posture reward evaluates the flight posture and orientation of the aircraft, emphasizing favorable flight conditions that lead to higher rewards. It is defined as follows:\nr_{a} = min ( arctanh ((1 - max (\\frac{\u03b1 + \u03b2}{\u03c0}, 10^{-4})) /\u03c0,0) + \\frac{2\u03c0}{25 \u00b7 (\u03b1 + \u03b2) + 2\u03c0} + \\frac{1}{2}.\nThe distance reward component encourages proactive engagement while discouraging passive or evasive maneuvers by the aircraft. It is formulated as:\nrd = {  ek\u2081x  + b\u2081, if x < \\frac{1}{3},  ek\u2081x+1  + b2, if  \\frac{1}{3} \u2264 x < \\frac{2}{3},   \\frac{x}{+b3}  + b4, if x \u2265  \\frac{2}{3},}\nwhere x represents the ratio of the current distance d to the maximum attack range Dmax. This motivates the aircraft to maintain proximity to strategic targets or optimal engagement zones, thereby enhancing its operational effectiveness. The state transition is determined by the flight control module, relying on the JSBSim simulation system. The system generates the next state by s\u2032 = f(s, g(\u03b40)), where \u03b40 is the target angle to realize the state transition. Actions are input into the environment at a maximum simulation frequency of 50 Hz, ensuring real-time adaptability to dynamic conditions.\nVarious sub-policies are trained by setting distinct initial state ranges and respective reward functions. In addition, due to the functional differences between the leader and the follower, the follower is able to effectively lure enemies and assist the leader in accomplishing combat missions via more aggressive and more flexible flight policies at a lower cost advantage."}, {"title": "4.2. Target Selector", "content": "In multi-UAV combat missions, simply selecting nearby targets fails to adequately assess the true threat posed by each UAV. Conversely, situational target selection offers numerous advantages. By employing situational target selection, global tactics and strategies can be comprehensively optimized, thereby mitigating the risk of UAV losses and ensuring overall operational success. The angular relationship between a UAV and its target is pivotal in the target selection process. While traditional algorithms typically consider the impact of angle off and target angle on the situation, these metrics may sometimes fall short in accurately reflecting the UAV's actual threat level. To address this, we propose employing the n-step method to more precisely gauge UAV threat levels, factoring in the dynamics and evolving patterns of their situational awareness.\nIntuitively, the closer the target is, the easier it is for the aircraft to approach and attack. The threat level of the aircraft is fully considered in the target scoring, including the position attribute T\u2081 and the posture attribute T. The posture reflects the potential threat level of the target to the aircraft and the mission. Targets with a higher threat level"}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Experimental Setting", "content": "In this paper, we select F-16 combat aircraft as agents for simulation. The agents are divided into two teams, red and blue, with an equal number of agents on each side. The parameters of the air combat environment are configured as follows: the maximum missile interception range is 4 kilometers, and the field of view is \u03c0/4. In the aircraft motion model, the maximum axial acceleration is set to 10 m/s\u00b2, and the minimum flight altitude is restricted to 3 kilometers. Each team consists of six aircraft, with their initial positions randomly assigned. The control signal is normalized to [-1,1], enabling smooth transitions in the action space. The aircraft adjust their motion posture in the JSBSim simulation environment via action commands. A UAV is considered destroyed if its altitude drops below the minimum threshold, exceeds overload limits, collides with another UAV, or is hit by a missile. A missile strikes a UAV if it is within the missile's field of view and the distance is less than 300 meters. The parameter settings for the experiments are as follows: the Proximal Policy Optimization (PPO) clip is set to 0.2, the Generalized Advantage Estimation (GAE) \u03bb is 0.95, and the buffer size is 3000. The discount factor is 0.99, the activation function is ReLU, and the optimizer is Adam with a learning rate of 3e-4. To evaluate the efficiency of the model, we compare our method with the following baseline approaches: (a) MAPPO [39] incorporates the advantages of PPO into multi-agent environments, demonstrating superior performance in addressing multi-agent cooperation and competitive tasks. (b) PPO [29] introduces a clipping mechanism to restrict policy updates and optimizes them with the advantage function. (c) VDN [30] decomposes the global joint value function into the sum of the local value functions for each agent. (d) QMIX [27] extends VDN by employing a hybrid network to capture the nonlinear cooperative relationships between agents. For a fair comparison, all methods utilize the same states, actions, and reward structures during evaluation."}, {"title": "5.2. Results", "content": "The following experiments are conducted to demonstrate the superior effectiveness of the LFMAPPO algorithm compared to other methods in executing multi-UAV combat tasks.\nEvaluation criteria. The evaluation is conducted in two principal dimensions: reward and win rate. The process dimension focuses on the reward to evaluate the system's performance during task execution, while the outcome dimension uses the win rate to assess the mission's overall success.\nMain results. Figure 4a illustrates the average return achieved by our approach and baseline algorithms during combat against an opposing team, with each method evaluated over five independent runs. Our approach consistently demonstrates superior performance, achieving higher rewards throughout the training episodes compared to the baseline methods. In contrast, VDN and QMIX exhibit limited learning capabilities in this environment, primarily due to their discretization of the action space, which reduces movement flexibility and limits strategic diversity. Furthermore, their inability to handle high-dimensional states imposed by six-degree-of-freedom motion further contributes to instability during the training process. The higher rewards achieved by our approach can be attributed to its evaluation of policies using a value function. By utilizing the leader-follower framework, it ensures that different roles receive clear and distinct rewards, forming well-defined objectives: leaders make decisions based on their local observations, while followers act based on the leaders' decisions. This cooperative dynamic enables the algorithm to effectively leverage the local state structure, enhancing teamwork and coordination. As a result, the agents adapt more effectively to dynamic combat scenarios, consistently surpassing the baselines in overall reward accumulation.\nTo evaluate the effectiveness of our method, we test the policies at various stages of training against a baseline opponent trained for 100 episodes, recording the win, draw, and loss rates at each stage. A total of 128 combats are conducted per stage, with the initial state of each combat randomized. The results, shown in Fig. 4b, indicate that as the number of iterations increases, the draw rate decreases, while a notable upward trend in the win rate shows the system's improved ability to defeat opponents."}, {"title": "Behavior analysis", "content": "We plotted the combat trajectories in a multi-UAV scenario to analyze the behavior of the aircraft. As shown in Figure 6, the red trajectories represent the opposing force. The x and y coordinates correspond to latitude and longitude and z represents the altitude. Figure 6a and 6b illustrate the trajectories of the MAPPO and PPO algorithms in a three-dimensional combat scenario. Compared to the trajectories of the LFMAPPO algorithm, these methods commonly form looping patterns, with UAVs circling around each other. This behavior extends the task execution time and hinders the ability to quickly neutralize opponents. Moreover, during these prolonged circling engagements, agents accumulate reward values, which inflate the reward scores for these methods. However, these scores are deemed ineffective as they do not contribute to achieving mission targets. In contrast, the maneuver trajectories produced by our approach enable rapid and decisive attacks, allowing UAVs to efficiently defeat opponents. In Figure 6c and 6d, we observe that UAVs form cooperative maneuver trajectories as groups."}, {"title": "Effect of policy selection with the leader-follower strategy", "content": "The blue trajectories, guided by a leader-follower strategy, maintain formation as they to approach the target and launch coordinated attacks, as shown in Figure 6c- 6f. In Figure 6c, the movement trajectories of blue aircraft and red aircraft in three-dimensional space exhibit distinct differences. The red aircraft's trajectories appear scattered and lack a cohesive tactical formation. In contrast, the blue aircraft, advancing in groups of three, display a well-coordinated movement pattern as they approach the mission target. In Figure 6d, the blue aircraft demonstrated effective teamwork by cooperatively engaging and successfully defeating the red aircraft. Initially, the red aircraft adopted a defensive posture within the weapon's engagement zone, maneuvering to evade potential threats. Despite employing evasive tactics, such as sharp turns and acceleration to escape the engagement zone, the red aircraft was pursued. The blue aircraft accelerated their pursuit, synchronizing their movements to close the distance. The trajectories of the blue aircraft illustrate their coordination and effective teamwork. The red aircraft executed continuous attacks, as shown in Figure 6e. The blue aircraft first cooperated to attack the red aircraft, while another red aircraft pursued the blue aircraft. After successfully downing one red aircraft, the blue aircraft shifted focus to another target and initiated an attack. The blue aircraft accelerated and adjusted their flight paths to engage the red aircraft pursuing them. Throughout the engagement, the red aircraft maintained a triangular formation, serving as a stable and effective combat structure in a multi-agent scenario. This formation allowed the red aircraft to coordinate their attacks effectively, providing both offensive and defensive advantages and maximizing the potential for strategic maneuvering. In Figure 6f, the blue aircraft executed a bait-and-strike strategy. One of the blue aircraft accelerated along a relatively direct flight path, serving as \"bait\" to entice the enemy aircraft into pursuit. The enemy aircraft, reacting to the bait, was drawn into the blue aircraft's weapon attack zone. This coordinated maneuver enabled the blue team to secure a positional advantage, effectively confining the red aircraft within their attack zone. This tactic not only demonstrated the effectiveness of team coordination but also shown the blue team's ability to manipulate the enemy's actions through strategic deception."}, {"title": "5.3. Ablation Study", "content": "Modules effect. As shown in Figure 7a, LFMAPPO (blue curve) demonstrates the best performance throughout the training process, consistently achieving higher reward values overall. Except for differences in modules, all other parameters remain the same. The leader-follower strategy and hierarchical architecture play a pivotal role in enhancing the algorithm's overall performance. These modules provide a structured approach for agents to collaborate more effectively, enabling dynamic responses to changing combat conditions. LFMAPPO-PS (red curve) removes the top layer of the hierarchical structure but retains the leader-follower strategy value function. The cooperative strategies enabled by the leader-follower strategy module yield superior results compared to the non-cooperative approach. Despite the absence of the policy selection module, the red line continues to maintain relatively high reward values. This resilience can be attributed to the foundational substructure of the framework, which is capable of generating basic movement actions and ensuring a baseline level of performance. The average return of LFMAPPO-LF (green curve), which removes the leader-follower strategy module, is lower than the others, indicating that the cooperative policy with the leader-follower strategy achieves better results than the non-cooperative approach in combat scenarios."}, {"title": "Replace the target selector", "content": "To assess the effectiveness of the proposed target selector, we compared it with both the random and nearest-distance target selection methods. The reward values for each method are shown in Figure 7b. The proposed target selector consistently achieved the highest average return. Notably, the random target selector outperformed the nearest-distance approach, as distance-based target selection can place the aircraft in vulnerable situations, leading to lower average return. Consequently, the proposed target selector evaluates the UAV's situational context, balancing both its posture and distance to the target in the decision-making process."}, {"title": "6. Conclusion", "content": "In this paper, we address the high-dimensional and collaborative challenges inherent in multi-UAV air combat by proposing a hierarchical framework based on the Leader-Follower Multi-Agent Proximal Policy Optimization strategy. We optimize the agents' value functions by assigning distinct roles to UAVs within the top-level policy selector. A target selector is introduced to evaluate threat levels based on flight status and posture. Simulation experiments validate the effectiveness of our method. Future work will focus on further refining coordination mechanisms, smoothing trajectories, and addressing the issue of agent laziness caused by relative positioning control."}]}