{"title": "Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning", "authors": ["Dongsu Lee", "Minhae Kwon"], "abstract": "Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. Here, the character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and uses the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent interactions. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario with diverse driving traits and multiple particle environments. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.", "sections": [{"title": "1 Introduction", "content": "Understanding human decision-making in multi-agent interactions is a significant focus in cognitive science. It provides valuable insights into designing interactions among diverse AI agents within multi-agent systems. Research has shown that humans employ counterfactual or future scenario simulation to enhance decision-making [45, 17, 49]. While counterfactual thinking, simulating alternative consequences of past events, has been extensively explored in multi-agent RL (MARL) [34, 9, 52, 3], episodic future thinking [1, 24], the ability to anticipate future events, remains underexplored in literature despite its importance in handling multi-agent interactions.\nHuman beings strive to anticipate future situations to prevent costly mistakes. One naive approach to incorporate this ability into AI is through future trajectory prediction using model-based RL [40, 19, 31, 54, 28, 26]. However, this approach is feasible only if the state transition model is known or easily learnable, which is often not the case in multi-agent systems. The complexity arises from the interdependence of state transitions on the actions of both the agent and other agents, making learning the state transition model challenging. Additionally, diverse agent characteristics exacerbate this challenge by introducing a wide range of action combinations and subsequent states. Thus, explicitly integrating character inference functionality regarding other agents into AI is more suitable for accurate future state prediction and optimal decision-making.\nOur goal is to develop an EFT mechanism for RL agents, enabling them to make adaptive decisions in a society where agents have heterogeneous characteristics. We formalize this task as a Multi-agent Partially Observable Markov Decision Process (MA-POMDP), a framework tailored to address the RL problem wherein multiple agents operate under partial observation [35, 55]. This study defines a character by reflecting the behavioral preferences of RL agents, which come from different weight combinations on reward components. For instance, in a driving scenario, some drivers prioritize safety, while others prioritize speed, leading to heterogeneous policies and behavioral patterns across agents.\nImplementing the EFT mechanism requires two functional modules: a multi-character policy and a character inference module. The multi-character policy embeds behavioral patterns corresponding to characters. It allows the agent to observe partial information of the state in continuous space and handles a hybrid action space consisting of discrete and continuous actions. The character inference module leverages the concept of inverse rational control (IRC) [18, 25] to infer target agents' characters by maximizing the log-likelihood of their observation-action trajectories. Combining these modules equips the agent with EFT functionality, enabling proactive behavior under heterogeneous multi-agent interactions.\nTo activate the EFT mechanism, the agent initially acts as an observer, collecting observation-action trajectories of target agents. Utilizing the character inference module and collected trajectories, the agent infers target agents' characters. With this knowledge and leveraging a multi-character policy, the agent predicts others' actions and simulates future observations with its action fixed as 'no action.' This mental simulation allows the agent to estimate the observation at the time point when all target agents have taken actions, but the agent still needs to (i.e., has yet to). It enables the agent to select the best action corresponding to the estimated future observation. In summary, the EFT mechanism empowers the agent to behave proactively in heterogeneous multi-agent interactions.\nSummary of contributions:\n\u2022 We introduce character diversity in a multi-agent system by parameterizing the reward function. We propose to build the multi-character policy and equip the agent with it to infer the character of the target agent (Section 3).\n\u2022 We introduce the EFT mechanism for social decision-making. The agent infers the characters of other agents using the multi-character policy, predicts their future actions based on the inferred characters, simulates the corresponding future observations and selects foresighted actions. This mechanism enables the agent to consider multi-agent interactions in its decision-making process (Section 4).\n\u2022 We verify the proposed mechanism by increasing character diversity in society. Extensive experiments confirm that the proposed mechanism enhances group rewards no matter how high a character diversity level exists in society (Section 5)."}, {"title": "2 Related Works", "content": "Episodic Future Thinking. Cognitive neuroscience aims to understand how humans use memory in decision-making. Interestingly, the trend of the brain's regional neural activation regarding counterfactual reasoning (i.e., simulating alternative consequences of the last episode) and future thinking (i.e., simulating episodes that may occur in the future) is similar [1]. In [56], the authors study the relationship between future thinking and decision-making and confirm that humans perform future-oriented decision-making. The decision-making abilities, such as strategy formulation, are also significant in scenarios that require multi-agent interactions, e.g., social decision-making.\nThere are several studies to endow this ability with an AI agent [62, 37, 61, 30]. In [30] and [37], the authors forecast the next state from a macroscopic standpoint without a prediction of each agent's behavior. In [61], the authors predict the behavior of an agent through a deep Bayesian network considering the dynamics and the previous surrounding environment information. Even though these studies can infer future information, no strategy formulation incorporated with prediction is suggested. Namely, most existing approaches use future predictions as auxiliary information for the optimization process without incorporating these predictions into the policy explicitly. In this study, we propose the ETF mechanism can predict future observations based on the current state and predicted actions of surrounding agents. Consequently, the agent equipped with this mechanism can select a foresighted action corresponding to the anticipated future observation.\nModel-based Reinforcement Learning. Model-based RL incorporates an explicit module representing system dynamics, contrasting with model-free RL. Within model-based RL, two approaches exist: utilizing a known dynamic model and learning it during training [19, 32, 40]. Using the dynamic model, the model-based RL approaches predict future trajectories, a pivotal step for network optimization [15, 16, 5, 57, 14]. Notably, approaches such as Dreamer [15] and Model-Based Policy Optimization (MBPO) [16, 14] demonstrate the practical application of these predictions. Dreamer optimizes a value function using the return of the predicted future trajectories, and MBPO trains the policy using the predicted future trajectories as augmented data samples. Furthermore, to tackle multi-agent problems, [5] and [57] extend these concepts by integrating a global model or communication block.\nWhile these methods often exhibit outstanding performance, they assume ideal conditions such as a small number of homogeneous agents and full observability. In reality, agents encounter incomplete and noisy data, and accurately modeling system dynamics is challenging due to complex interactions between multiple agents with unique behavioral characteristics. This work addresses a partially observable agent in a multi-agent environment with heterogeneous characteristics across agents. We allow the agent to infer other agents' characters and make decisions based on predictions of upcoming observations.\nMachine Theory of Mind. Human decision-making in social contexts often involves considering multiple perspectives, including the behavioral characteristics of others. This capacity, known as Theory of Mind (ToM) in cognitive science, primarily involves deducing internal models of others and predicting their future actions [2, 20]. AI research aimed at providing machines with this capability has gained attention for enhancing multi-agent system performance, such as machine ToM [42, 41], inverse learning [43, 18, 33], and Bayesian ToM [60]. These approaches aim to reconstruct the target agent's belief, reward function, or dynamic model based on its trajectories. However, they often operate in simple settings, limiting their applicability to scenarios with a small number of agents, a small discrete action space, or minimal character diversity across agents.\nIn contrast to previous work, this study explicitly develops a character inference module focusing on establishing a link between trajectories and characters. This module allows the target agent's behavior to be explained by character, aligning with the researcher's interests. Additionally, it extends the action space from continuous to hybrid.\nFalse Consensus Effect. Psychological research has identified a cognitive bias in humans to assume that their character, beliefs, and actions are common among the general population [10, 6, 7], termed the False Consensus Effect (FCE) [53, 12, 47]. Recent studies suggest that AI may exhibit this false belief [42]. To underscore the importance of character inference in heterogeneous multi-agent scenarios, we compare the performance of the EFT mechanism with two types of agents: the proposed agent, equipped with the character inference module, and the FCE-based agent, which assumes that target agents share the same character as the agent."}, {"title": "3 Character Inference Using Multi-character Policy", "content": "We aim to build an agent to make optimal decisions under multi-agent interactions. It requires the agent to be able to anticipate the near future by predicting other agents' actions. The agent should possess the ability to infer the others' characters, leveraging observation of their behaviors."}, {"title": "3.1 Problem Formulations for Multi-agent Decision-making", "content": "We consider multi-agent scenarios where RL agents adaptively behave to each other. All agents have to make decisions and execute actions simultaneously, unlike the extensive-form game [36] in which the agents alternate executing the actions.\nThe multi-agent decision-making problem can be formalized as a MA-POMDP $\\mathcal{M} = (\\mathcal{E}, \\mathcal{S}, {\\mathcal{O}_i}, {\\mathcal{A}_i}, T, {\\Omega_i}, {R_i}, \\gamma)_{i\\in\\mathcal{E}}$ that includes an index set of agents $\\mathcal{E} = {1, 2, \\ldots, N}$, continuous states $s_t \\in \\mathcal{S}$, continuous observations $o_{t,i} \\in \\mathcal{O}_i$, hybrid actions $a_{t,i} = \\{a^c_{t,i}, a^d_{t,i}\\} \\in \\mathcal{A}_i$, where continuous action $a^c_{t,i} \\in \\mathcal{A}^c_i$ and a discrete action $a^d_{t,i} \\in \\mathcal{A}^d_i = \\{w : |w| \\leq W, w \\in \\mathbb{Z}, W \\in \\mathbb{N}\\}$, where the size of discrete action space is $|\\mathcal{A}^d_i| = 2W + 1$, $\\mathbb{Z}$ denotes the set of integers, and $\\mathbb{N}$ denotes the set of natural numbers. Let $\\mathcal{A} := \\mathcal{A}_1 \\times \\mathcal{A}_2 \\times \\cdots \\times \\mathcal{A}_N$. Subsequently, $T : \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{S}$ is the state transition probability; $\\Omega_i : \\mathcal{S} \\to \\mathcal{O}_i$ is the observation probability; $R_i : \\mathcal{S} \\times \\mathcal{A}_i \\times \\mathcal{S} \\to \\mathbb{R}$ denotes the reward function that evaluates the agent's action $a_{t,i}$ for a given state $s_t$ and the outcome state $s_{t+1}$; $\\gamma \\in [0, 1)$ is the temporal discount factor.\nAn unordered set of the actions of all agents at time $t$ is denoted as\n$$a_t = (a_{t,1}, \\ldots, a_{t,i}, \\ldots, a_{t,N}) = (a_{t,i}, a_{t,-i}) \\in \\mathcal{A},$$\nwhere subscript $-i$ represents the indices of all agents in $\\mathcal{E}$ except $i$. Thus, $a_{t,-i} = \\langle a_{t,1}, \\ldots, a_{t,i-1}, a_{t,i+1} \\ldots, a_{t,N}\\rangle$ represents a set of all agents' actions at time $t$ without $a_{t,i}$. The state transition probability denotes $T(s_{t+1}|s_t, a_t)$. Note that state transition is based on the action combination of all agents $a_t$, not on the action of a single agent $a_{t,i}$.\nNext, $c_i = \\{c_{i,1}, c_{i,2}, \\ldots, c_{i, K}\\} \\in \\mathcal{C} \\in \\mathbb{R}^K$ denotes a $K$-dimensional character vector for the agent $i$. Character $c_i$ can parameterize the reward function of the agent $i$, i.e., $R_{t,i} = R_i(s_t, a_{t,i}, s_{t+1}; c_i)$. The agent aims to learn the optimal policy that returns the optimal action $a^*_{t,i} \\sim \\pi^*(\\cdot|o_{t,i}; c_i)$ given observation and character. Specifically, the objective of the agent aims to maximize the expected discounted cumulative reward $J(\\pi) = \\mathbb{E}_{\\pi} [\\sum_{t} \\gamma^t R_i(s_t, a_{t,i}, s_{t+1}; c_i) ; c_i)]$ by building the best policy $\\pi$. This defines the state-action value function $Q^{\\pi} (s, a; c_i) = \\mathbb{E}_{\\pi} [\\sum_{t} \\gamma^t R_i(s_t, a_{t,i}, s_{t+1}; c_i)|s_0=s, a_0=a]$.\nIn the next section, we discuss the details of the multi-character policy in terms of neural network design and its training."}, {"title": "3.2 Training a Multi-character Policy", "content": "The multi-character policy includes inputs in continuous space (e.g., observation $o_{t,i}$ and character $c$) and outputs in hybrid space (e.g., action $a_{t,i}$). To build the policy generalized over continuous space, the actor-critic architecture is used. It approximates the policy $\\pi_{\\phi}(\\cdot|o_{t,i}; c_i)$ and Q-function $Q_{\\theta}(o_{t,i}, a_{t,i}; c_i)$, where $\\phi$ denotes parameters of the actor network and $\\theta$ denotes the parameters of the critic network.\nThe loss functions used to train the actor and critic networks are $\\mathcal{L}(\\phi) = -Q_{\\theta}(o_{t,i}, \\pi_{\\phi}(\\cdot|o_{t,i}; c_i))$, and $\\mathcal{L}(\\theta) = |y_t - Q_{\\theta}(o_{t,i}, \\pi_{\\phi}(\\cdot|o_{t,i}; c_i))|^2$, respectively. Herein, $y_t = R_{t,i} + Q_{\\theta'}(o_{t+1,i}, \\pi_{\\phi'}(\\cdot|o_{t+1,i}; c_i))$ represents the Temporal Difference (TD) target, where $\\theta'$ and $\\phi'$ denote the target networks.\nNext, we propose a post-processor $g(\\cdot)$ to handle hybrid action space. Let $\\bar{a}^c_{t,i}$ be the output of the actor network. The post-processor $g(\\cdot)$ performs quantization process by discretizing the continuous proto-action $\\bar{a}^c_{t,i}$ into discrete post-action $a^d_{t,i}$, i.e.,\n$$a^d_{t,i} = g(\\bar{a}^c_{t,i}, W) = \\min\\left(\\left\\lfloor \\frac{2W+1}{2W} \\left(\\bar{a}^c_{t,i} + \\frac{W}{2W+1}\\right) \\right\\rfloor, W\\right),$$\nwhere $\\lfloor \\cdot \\rfloor$ denotes a floor function. The derivation of (1) is presented in Appendix D."}, {"title": "3.3 Inferring Character of Target Agent", "content": "After completing the training on the multi-character policy, our next objective is to infer the character $c_j$ of the target agent $j \\in \\mathcal{E}$. The agent first collects observation-action trajectories of the target for character inference. Subsequently, it utilizes the multi-character policy to identify the character $c_j$ that best explains the collected data. To elaborate, $c_j$ can be estimated by maximizing the log-likelihood of observation-action trajectories $\\ln P(o_{1:T,j}, a_{1:T,j}|c_j)$. This can be formulated as follows.\n$$\\hat{c}_j = \\arg \\max_{c} \\ln P(o_{1:T,j}, a_{1:T,j}|c) = \\arg \\max_{c} \\sum_{t=1}^T [\\ln \\pi(a^c_{t,j}|o_{t,j}; c) + \\ln \\pi(a^d_{t,j}|o_{t,j}; c)]$$\nThe derivation of (2) can be found in Appendix E.\nTo efficiently perform the inference task, we use the gradient ascent method. It runs the iteration by changing $c$ toward the direction to increase $\\mathcal{U}(c) = \\ln \\pi(a^c_{t,j}|o_{t,j}; c) + \\ln \\pi(a^d_{t,j}|o_{t,j}; c)$, which is summarized in Algorithm 2.3"}, {"title": "4 Foresight Action Selection Based on Episodic Future Thinking Mechanism", "content": "This section presents the proposed EFT mechanism that enables the agent to simulate the subsequent observations and to select a foresighted action. The proposed EFT mechanism comprises a future thinking module and an action selection module.\nThe future thinking module includes two steps: action prediction and the next observation simulation. With these two steps, the agent can foresee the next observation. This process is illustrated in Figure 2. Subsequently, the action selection module enables the agent to decide the current action corresponding to the simulated next observation."}, {"title": "4.1 Future Thinking: Step I - Action Prediction", "content": "In this step, the agent with the multi-character policy predicts the actions of the neighbor agents by using pre-inferred characters and observations. The agent can predict the action of the target agent $j \\in \\mathcal{E}, j \\neq i$4 using the trained multi-character policy $\\pi_{\\phi}$ and inferred character $\\hat{c}_j$, i.e., $\\hat{a}_{t,j} \\sim \\pi_{\\phi}(\\cdot|o_{t,j}; \\hat{c}_j)$. Therefore, the predicted action set of others $\\hat{a}_{t,-i}$ is as follows.\n$$\\hat{a}_{t,-i} = (\\pi_{\\phi}(o_{t,1}; \\hat{c}_1), \\ldots, \\pi_{\\phi}(o_{t,i-1}; \\hat{c}_{i-1}), \\pi_{\\phi}(o_{t,i+1}; \\hat{c}_{i+1}), \\ldots, \\pi_{\\phi}(o_{t,N}; \\hat{c}_N))$$"}, {"title": "4.2 Future Thinking: Step II - Next Observation Simulation", "content": "In this step, we introduce how the agent simulates its next observation by using the predicted action $\\hat{a}_{t,i}$. Note that this prediction is the result of the mental simulation of agent $i$, when $a_{t,i} = \\emptyset$ is satisfied. Herein, $\\emptyset$ denotes null action, meaning that no action is performed. This is to simulate the observation of the time point when all target agents performed the action, but the agent has not yet.\nThe simulated next observation $\\hat{o}_{t+1,i}$ can be determined based on the predicted action set $\\hat{a}_{t,-i}$ and the current observation $o_{t,i}$. The function of the next observation simulation $D(\\cdot)$ is defined as $\\hat{o}_{t+1,i} = D(o_{t,i}, \\hat{a}_{t,-i}, a_{t,i} = \\langle \\emptyset\\rangle$. The action selection using the simulated next observation $\\hat{o}_{t+1,i}$ allows the agent to ignore the influence of others' actions. This is because the next state is determined solely by its own action $a_{t, i}$ in the agent's mental simulation, as $\\hat{o}_{t+1,i}$ has already applied the others' actions $\\hat{a}_{t,-i}$."}, {"title": "4.3 Action Selection", "content": "Once the agent has simulated the next observation $\\hat{o}_{t+1,i}$, the agent can make a foresighted decision. The agent uses the multi-character policy $\\pi_{\\phi}$ with the input of the simulated next observation $\\hat{o}_{t+1,i}$ and its own character $c_i$, and finally gets the action $a_{t, i} = \\{a^c_{t,i}, \\bar{a}^c_{t,i}\\} = \\pi_{\\phi}(\\cdot|\\hat{o}_{t+1,i}; c_i)$. In other words, the agent can select an adaptive action with consideration for others' upcoming behaviors. The decision-making procedure with the proposed EFT mechanism is summarized in Algorithm 3."}, {"title": "5 Experiments", "content": "To select a suitable task that can verify the effectiveness of the proposed solution, we consider the following requirements. There should be multiple approaches to achieving character diversity, as well as interactions between agents. The agent should have only partial observations of the state, and the action space should be both continuous and discrete.\nWe chose the autonomous driving task, which has numerous automated vehicles on the road. The task can consider the driving character of the agent based on driving preferences (e.g., one agent prioritizes safety, and the other prioritizes speed) [46, 4, 50, 23, 22]. Additionally, it is realistic for a driver to behave under the partial observation of the road state, and the driver makes a decision in a hybrid action space. To implement this task, we use the FLOW framework [58, 21, 8]. The scenario includes multiple automated vehicles on the highway. The number of agents $|\\mathcal{E}| = 21$, and each agent decides on acceleration and lane change control at a given observation. Here, we express the driving character using weights of three reward terms, i.e., $c_i = [c_{i,1}, c_{i,2}, c_{i,3}]$. The target agent $j$ is limited to the vehicles located in the observable area.\nTo confirm the scalability of the proposed solution, we also provide simulation results with a multiple particle environment (MPE) [29] and starcraft multi-agent challenge (SMAC) [48], a popular MARL testbed. All results in this section are averaged results of over 10 independent experiments. The"}, {"title": "5.1 Performance Evaluation: Character Inference", "content": "To make the EFT mechanism more effective, an accurate character inference should be preceded. In this subsection, we investigate the character inference module with two questions.\n\u2022 How many iterations does it require to achieve an accurate inference (in terms of repetition in Algorithm 2)?\n\u2022 How long should the agent collect the observation-action trajectories of target agents (in terms of trajectory length T in Algorithm 2)?\nIn Figure 3, the performance of the character inference module is presented. To ignore the effect of the initial point in convergence, the initial point of the character is randomly selected. More results regarding the initial point are provided in Appendix I."}, {"title": "5.2 Ablation Study: Character Inference and EFT Modules", "content": "We investigate the impact of two main modules (the character inference module and the EFT module) on performance by increasing character diversity levels of the heterogeneous society. The following three cases are compared.\n\u2022 Proposed: the agent enables the EFT with the inferred character of other agents based on the character inference module.\n\u2022 FCE-EFT: the agent experiences the FCE by assuming that all other agents have equal character to itself (i.e., $c_j = c_i, \\forall j \\in \\mathcal{E}$). So, no character inference is required. The agent performs the EFT, but action prediction is performed based on the same character $c_i$.\n\u2022 without EFT (baseline) [11]: the agent performs neither character inference nor the EFT mechanism. It treats the problem as a single agent RL and selects the best action given observation. The policy is trained based on the TD3.\nIn Figure 4, the average rewards of entire agents are presented over increasing the number of character groups. The higher number of character groups means that more diverse characters coexist in society, and the higher reward implies better performance. Because the number of agents is fixed to $|\\mathcal{E}| = 21$, the number of members per character group is $|\\mathcal{E}|/n$, where n denotes the number of groups. The members belonging to the same group have the same character c. Note that a character of each group is randomly sampled from character space $\\mathcal{C}$ in every independent experiment."}, {"title": "5.3 Investigating the Effects of Trajectory Noise", "content": "To infer the character of the target agent, the EFT agent needs to collect observation-action trajectories of the target agent. Since the observations made by the EFT agent towards the target agent may not be perfect (i.e., they could be a noisy version of the target agent's true observations), we further investigate the performance of the proposed EFT framework concerning the accuracy of the collected trajectories. This investigation consists of two steps. First, we look deeply into the effect of trajectory accuracy on character inference, and thereafter, we examine the EFT performance regarding character inference accuracy."}, {"title": "Character inference with trajectory accuracy.", "content": "Table 1 shows the character inference accuracy as the noise level for a collected trajectory increases. As expected, the character inference accuracy decreases as the noise variance increases. Please be aware that the considered standard deviation is not trivial given that our observation range is [-1,1]. Specifically, we provide the signal-to-noise ratio (SNR) with a quality level (Qual) across each standard deviation. We label the quality of each level based on [13].\nWe believe that this result provides valuable insights into the expected performance of our proposed solution, particularly in scenarios where observation prediction technology is deployed."}, {"title": "EFT performance with character accuracy.", "content": "In Figure 5, x and y axes are the accuracy of character inference and average reward, and n is diversity level. As expected, the result shows that the performance of the EFT agent naturally increases when the accuracy of predicted observation increases. Interestingly, the proposed solution holds up the performance even at a character inference accuracy of approximately 90% (i.e., the error rate of 10%). It is also worth mentioning that the performance has a similar trend across the diversity levels, which confirms that the proposed method is robust against diversity levels."}, {"title": "5.4 Assessing Generalizability: Inference on Out-of-Distribution Character", "content": "It can be impractical and challenging to train all characters within a pre-defined range, and a trained agent can confront an out-of-distribution (OOD) character in the deployment phase. This subsection demonstrates the inference performance on the OOD range of pre-trained agents with specific character samples. To this end, we consider the following two cases:"}, {"title": "5.5 Performance Comparisons", "content": "We compare the performance of the proposed solution to the following popular MARL, model-based RL, and agent modeling algorithms: MADDPG [29], MAPPO [63], Q-MIX [44], Dreamer [15], MBPO [16], TOMC2 [59], and LIAM [38]. In baseline algorithms, we go through independent policy training regarding the diversity level of society. Note that the proposed method does not need plural training for different heterogeneity settings.\nTable 2 shows the average reward of the entire agents as the number of character groups increases. This result verifies that the proposed solution outperforms all popular MARL algorithms. Note that the MARL algorithms assume centralized training, which requires access to the observations and actions of all agents in policy training. In contrast, our solution trains the policy with only local observations and actions, which can be a more practical solution. The Q-MIX has the lowest performance since it operates in a discrete action space, whereas our task is in a hybrid action space."}, {"title": "5.6 Additional Evaluation on MPE and SMAC", "content": "Beyond the autonomous driving task, we run the performance comparison on the MPE and SMAC testbed.\nMultiple Particle Environment. The MPE tasks consider a small number of agents (three or four) and groups (one or two). Therefore, we set the character for each group as a single character, that is, the diversity level n = 1. Table 3 shows the performance comparison across each task of MPE. Even though our method is specialized for a high level of character diversity environment, the results demonstrate that the proposed solution is competitive in a simple environment by achieving the best score in two out of three tasks.\nStarCraft Multi-agent Challenge. The setup of SMAC tasks is similar to MPE tasks, i.e., the EFT agent does not need to infer the character because they have the same (character diversity as n = 1). Table 4 exhibits the performance on the SMAC tasks. The proposed solution demonstrates superior performance across SMAC tasks, particularly excelling in more complex scenarios like 3s5z_vs_3s6z and 6h_vs_8z. Although MAPPO shows competitive performance, especially in simpler tasks like 2s3z, the proposed method proves more effective overall in handling both simple and complex multi-agent tasks."}, {"title": "6 Discussion", "content": "Conclusion. In this paper, we propose the EFT mechanism, which is a social decision-making approach for a multi-agent scenario. The EFT mechanism enables the agent to behave by considering current and near-future observations. To achieve this functionality, we first build a multi-character policy that is generalized over character space. Then, the agent with the multi-character policy can infer others' characters using the observation-action trajectory. Next, the agent predicts the others' behaviors and simulates its future observation based on the proposed EFT mechanism. In the simulation result, we confirm that the proposed solution outperforms existing solutions across all diversity levels of the heterogeneous society.\nBroader Impacts. The proposed EFT idea paves the way for research on multi-agent scenarios. The proposed method enables the agent to simulate other agents' upcoming actions, which is analogous to humans' decision-making. Furthermore, we believe the proposed method can be broadened by combining counterfactual thinking, current information, and future thinking.\nLimitations. Even though this work shows promising results with a novel method, there are a few limitations to tackle. In our experiments, there is only one EFT agent, and all other agents do not have the EFT functionality. This is an inevitable setting to make the problem tractable. Additionally, we follow the non-stationary regarding the agent's policy in the training phase and stationary in the execution phase. Since the character is mapped into policy, this stationary property has a connection to the character itself. To improve practicality, we should further investigate how the proposed solution works when the other agent's policy is non-stationary in the execution phase."}]}