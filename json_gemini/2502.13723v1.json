{"title": "Direct Value Optimization:\nImproving Chain-of-Thought Reasoning in LLMs with Refined Values", "authors": ["Hongbo Zhang", "Han Cui", "Guangsheng Bao", "Linyi Yang", "Jun Wang", "Yue Zhang"], "abstract": "We introduce Direct Value Optimization\n(DVO), an innovative reinforcement learning\nframework for enhancing large language mod-\nels in complex reasoning tasks. Unlike tradi-\ntional methods relying on preference labels,\nDVO utilizes value signals at individual rea-\nsoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies\nin its fine-grained supervision, circumventing\nthe need for labor-intensive human annotations.\nTarget values within the DVO are estimated\nusing either Monte Carlo Tree Search or an\noutcome value model. Our empirical analy-\nsis on both mathematical and commonsense\nreasoning tasks shows that DVO consistently\noutperforms existing offline preference opti-\nmization techniques, even with fewer training\nsteps. These findings underscore the impor-\ntance of value signals in advancing reasoning\ncapabilities and highlight DVO as a superior\nmethodology under scenarios lacking explicit\nhuman preference information.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated\nexceptional performance across a wide range of\nnatural language processing (NLP) tasks (Yu et al.,\n2023b; Azerbayev et al., 2023; Reid et al., 2024;\nChen et al., 2023; Achiam et al., 2023; Yang et al.,\n2024a). However, when addressing complex prob-\nlems such as mathematical reasoning (Yue et al.,\n2023; Yu et al., 2023c), logical reasoning (Liu et al.,\n2023a; Teng et al., 2023), and multi-step planning\nproblems (Hao et al., 2023), they can fall into incor-\nrect reasoning paths due to errors in intermediate\nreasoning steps (Cobbe et al., 2021; Shen et al.,\n2021; Bao et al., 2025).\nTo address this issue, recent studies leverage\nreinforcement learning (RL) to optimize the rea-\nsoning process according to feedback derived from\ngolden answers (Shao et al., 2024; Luo et al., 2023;\nChen et al., 2024d; Kazemnejad et al., 2024), sig-\nnificantly enhancing the robustness. Among these,\nconverting reward signals into preference labels has\nbeen a standard practice, where multiple responses\nare generated for one question and the optimal re-\nsponse reaching at the correct answer is labeled as\nthe preferred response in contrast to others. Prefer-\nence labels are then used to train models through\na cross-entropy loss or a contrastive alternatives.\nFor example, DPO (Rafailov et al., 2023), KTO\n(Ethayarajh et al., 2024), and RFT (Yuan et al.,\n2024, 2023) optimize language models based on\npreferences at the token level, while recent process\nsupervision (Lightman et al., 2023), CPO (Zhang\net al., 2024), Step-DPO (Lai et al., 2024), SVPO\n(Chen et al., 2024b) and Iterative Preference Learn-\ning (Xie et al., 2024) generate preferences and op-\ntimize LLMs at the step level.\nPreference labels as training targets are simple\nand convenient for implementation. However, re-\nwards from preference labels lack fine-grained sig-\nnals and can fail to preserve critical value informa-\ntion (Liu et al., 2023b; Chen et al., 2024b).\nA key limitation of preference-based optimiza-\ntion is its reliance on pairwise comparisons, which\nonly indicate relative preference between two\nchoices rather than providing an absolute ranking\nof multiple steps. In contrast, stepwise value sig-\nnals offer a continuous ranking, allowing the model\nto make more informed decisions beyond binary\ncomparisons. By leveraging value signals, models\ncan better understand reasoning dynamics, priori-\ntizing steps that enhance clarity, reduce ambiguity,\nand improve multi-step reasoning efficiency.\nTo address this issue, we propose Direct Value\nOptimization (DVO), a simple yet effective rein-\nforcement learning (RL) framework for large lan-\nguage models (LLMs). As shown in Fig. 1, DVO\nestimates target values at each reasoning step and\naligns the model with these values using a mean"}, {"title": "Related Work", "content": "Improving Reasoning in LLMs. Existing stud-\nies improve the robustness of LLM reasoning at\nvarious stages. Some focus on the pre-training\nphase by curating pretraining datasets to enhance\nmathematical knowledge (Shao et al., 2024; Azer-\nbayev et al., 2023), some address it during instruc-\ntion fine-tuning using high-quality reasoning QA\ndatasets (Mitra et al., 2023; Mukherjee et al., 2023;\nYue et al., 2023; Yu et al., 2023c), while others\ntackle the reasoning challenge at the reinforcement\nlearning stage by using human or AI feedback for\noptimization (Yuan et al., 2024; Lai et al., 2024).\nAdditionally, several studies improve reasoning at\nthe inference stage through prompting techniques\nthat search for better reasoning paths in trees or\ngraphs (Yao et al., 2024; Besta et al., 2024; Wang\net al., 2023a; Yang et al., 2024b; Zheng et al., 2023).\nOur work aligns most closely with methods that fo-\ncus on the reinforcement learning stage but differs\nby improving LLMs by using value signals.\nReinforcement Learning in LLMs. Rein-\nforcement learning is widely used in LLM post-\ntraining to learn implicit rewards from human feed-\nback (Ouyang et al., 2022; Dubey et al., 2024; Yang\net al., 2024a). The standard pipeline trains a reward\nmodel and updates the policy via PPO (Schulman\net al., 2017b). DPO (Rafailov et al., 2023) shows\nthat the log ratio between optimal policy and refer-\nence models expresses the corresponding reward\nmodel, allowing direct policy updates based on the\npreference data. Rafailov et al. (2024) further de-\nrived the DPO objective, validating its ability to\nlearn token-level rewards, while subsequent works\nrefine DPO for practical applications (Ethayarajh\net al., 2024; Chen et al., 2024c; Azar et al., 2024).\nOur approach extends DPO by aligning the pol-\nicy model with value estimation, eliminating the\nneed for pairwise data and leveraging value signals\nfor enhanced reasoning. While recent work like\nDQO (Liu et al., 2024) and OREO (Wang et al.,\n2024) also aims to improve LLM reasoning within"}, {"title": "Baseline: Direct Preference\nOptimization", "content": "Direct Preference Optimization (DPO) (Rafailov\net al., 2023) was designed to align LLMs with hu-\nman preference labels. Recent work (Yuan et al.,\n2023; Ethayarajh et al., 2024; Chen et al., 2024c)\nhas extended DPO to optimize LLMs with reward\nsignals by converting them into preference labels\nthrough specific algorithms. While DPO was ini-\ntially developed for the contextual bandit setting, it\ncan be validated within token-level MDP (Rafailov\net al., 2024). In the RLHF pipeline, we first train\na reward model based on the Bradley-Terry frame-\nwork (Bradley and Terry, 1952), which is well-\nsuited for modeling implicit human preferences:\n$P_{BT}(Y_1 \\succ Y_2 | x) = \\frac{exp(r(x, y_1))}{exp(r(x, y_1)) + exp(r(x, y_2))},$\nwhere x is the prompt, Y1, Y2 are two completions.\nThe optimal policy, Q-function, and V-function\nlearned from a reward function R are denoted as\n\u03c0*, Q*, and V*, respectively. satisfying the relation\nin Eq. (4). Using \u03c0*, Q*, and V*, the accumulated\nstep-level reward is:\n$R(x,y) = \\sum_{t=1}^{T-1}(Q^* (s_t, a_t) \u2013 V^*(S_{t+1}))$\n$= V^*(S_0) \u2013 V^*(s_T) + \u03b2 log \u03c0^*(y | x)$\n$= V^*(S_0) + \u03b2 log \u03c0^*(y | x)$\nwhere we assume V*(sT) = 0. To prevent the\nmodel from deviating excessively from its prior\nbehavior during training, the reward function is:\nR(at | st) = Blog \u03c0ref(at | st) + r(at | st),\nwhich leads to the reward model as:\n$r(x, y) = \u03b2 log \\frac{\u03c0^*(y | x)}{\u03c0_{ref}(y | x)} + V^*(S_0).$\nThis establishes a connection between the\nlearned policy and the reward model, enabling di-\nrect optimization of the policy without the need for\nexplicit reward modeling. Substituting the regu-\nlarized reward into the Bradley-Terry framework\nyields the DPO objective:\n$L_{DPO} = E_{(x, y_w, y_l)\u223cD} [log \u03c3(\u03b2 log \\frac{\u03c0_\u03b8(y_w | X)}{\u03c0_{ref} (y_w | X)} - log \\frac{\u03c0_\u03b8(y_l | X)}{\u03c0_{ref} (y_l | X)})]$"}, {"title": "Direct Value Optimization", "content": "We derive a framework that optimize reasoning of\nLLMs using trajectories along with their estimated\nstepwise values by extending soft Q-learning. To\nour knowledge, this is the first work to directly op-\ntimize LLMs using estimated value signals within\na Q-learning framework.\nProblem Definition: A Stepwise MDP\nAs Fig. 2 illustrates, we formulate the reasoning\nproblem as a stepwise Markov Decision Process\n(step MDP), where a whole reasoning process is\nsplit into several small but coherent reasoning steps,\neach representing a link in the chain of thoughts.\nFormally, at each step t, the policy model is in\na state st, which consists of the current question\nq (sampled from a distribution D) and a sequence\nof prior steps [y1, y2,\u00b7\u00b7\u00b7, yt\u22121], denoted as st =\n{q, y<t}. The policy model \u03c0\u03bf selects the next\naction at from the set of possible steps according to\nthe probability distribution \u03c0\u03bf(at | st), guiding the\nreasoning process forward. The goal of the model\nis to produce a sequence of steps that ultimately\nleads to the final answer.\nTo encourage the model to perform robust rea-\nsoning steps that lead to the correct outcome, we\ndefine a reward function r(st, at). The reward eval-\nuates the quality of each reasoning step, ensuring\nthat the model focuses on generating logical and\naccurate transitions. In this context, the reasoning\nprocess is deterministic: the next state st+1 is fully\ndetermined by the current state st and the selected\naction at. As a result, we omit explicit references\nto transition probabilities P(st+1 | st, at) in subse-\nquent equations for simplicity.\nLLM as Soft Q-Function\nMaximum entropy reinforcement learning intro-\nduces an entropy term to encourage exploration,\nbalancing reward maximization and stochasticity\nfor robust decision-making (Rafailov et al., 2024;\nZiebart, 2010; Haarnoja et al., 2017). In this frame-\nwork, the Soft Q-function and Soft V-function are\ndefined as follows:\n$Q_{soft} (s_t, a_t) = R(s_t, a_t) + V_{soft} (s_{t+1}),$\n$V_{soft} (s_t) = Blog \\int exp(Q_{soft} (s_t, a'))da',$\nwhere A is the action space, and \u03b2 is a tempera-\nture parameter controlling the trade-off between\nentropy and reward. The optimal policy \u03c0* can be\nexpressed in terms of the optimal Soft Q-function\n$Q_{soft}$ and optimal Soft V-function $V_{soft}$:\n$\u03c0^*(at | s_t) = exp(\\frac{Q_{soft} (s_t, a_t) - V_{soft} (s_t)}{\u03b2})$\nBuilding on this framework, Rafailov et al.\n(2024) formally proposed that an LM can be in-\nterpreted as an optimal soft Q-functions.\nProposition 1. (Proof in Appendix A) In general\nmaximum entropy reinforcement learning setting, a\nlanguage model parameterized by \u03c0\u03b8 can be seen\nas an optimal soft Q-function under some reward.\nThus, the corresponding optimal Q-function can\nbe presented by \u03c0\u03b8:\n$Q_{soft}^{\\pi_\\theta} (s_t, a_t) = Blog \u03c0_\u03b8(a_t | s_t) + V_{soft}^{\\pi_\\theta} (s_t).$\nThe DVO Objective\nDifferent from DPO, which optimizes language\nmodels at the response level using cross-entropy\nloss, we treat the policy as a Q-function and opti-\nmize it with Mean Squared Error (MSE) loss based\non value estimations.\nSpecifically, following Haarnoja et al. (2017);\nSchulman et al. (2017a), the Q-function can be\noptimized by using soft Q-learning:\n$J_Q(\u03b8) = E_{s_t, a_t, y_t} [(Q_{soft}^{\\pi_\\theta} (s_t, a_t) - y_t)^2],$\nwhere yt is the target Q-value. In one-step Q-\nlearning, the target Q-value is computed as:\n$y_t = R(s_t, a_t) + V_{soft} (s_{t+1}),$\nwhere $V_{soft} (s_{t+1})$ is the target V-function of st+1.\nSubstituting Eq. 5 and Eq. 7 into the objective, we\nobtain:\n$J_Q(\u03b8) = E_{s_t, s_{t+1}, a_t} [(\\frac{1}{2})(Blog \u03c0_\u03b8(a_t | s_t) + V_{soft}^{\\pi_\\theta} (s_t) \\\\ - R(s_t, a_t) - V_{soft}^{\\pi_\\theta} (s_{t+1}))^2]$\nSimilar as DPO, the reward function R(st, at)\nincludes both the actual reward r(st, at) and a KL\npenalty term:\nR(st, at) = Blog \u03c0ref(at | st) + r(st, at),"}, {"title": "Target Value Estimation", "content": "where \u03c0ref is a reference policy. To optimize the\nparameters \u03b8, we substitute the estimated value\nfunction $V_{soft}^{\\pi_\\theta}(s_t)$ for $V_{soft}^{\\pi_\\theta} (s_t)$. This leads to the\nfinal objective:\n$J_Q(\u03b8) = E_{s_t, s_{t+1}, a_t} [(\\frac{1}{2})(Blog \\frac{\u03c0_\u03b8 (a_t | s_t)}{\u03c0_{ref} (a_t | s_t)} - \\frac{V_{soft}^{\\pi_\\theta} (s_{t+1})}{\u03b2} \\\\+ r(s_t, a_t) -  \\frac{V_{soft}^{\\pi_\\theta} (s_t)}{\u03b2})^2]$\nAs a result, we can now directly align a language\nmodel with step-level value estimation, which is\nderived from outcome reward signals without re-\nquiring human labeling.\nIterative Training. This approach can be nat-\nurally extended into an iterative process. In each\niteration, the latest policy model is used to gener-\nate new reasoning paths and corresponding value\nestimates, which are then used for further training.\nTarget Value Estimation\nWe investigate two representative methods for es-\ntimating target values for DVO training: MC es-\ntimation with a tree search algorithm (Kocsis and\nSzepesv\u00e1ri, 2006; Coulom, 2006), which directly\napproximates the value through simulated rollouts,\nand an outcome value model (Yu et al., 2023a; Feng\net al., 2023), which learns to predict the value based\non the observed outcomes and states.\nEstimation Using MCTS\nWe use Monte Carlo Tree Search (MCTS) to sam-\nple reasoning paths and estimate process value\nfunctions. Known for its success in AlphaZero\nand AlphaGo (Silver et al., 2017, 2016), MCTS\neffectively balances exploration and exploitation\nwhile providing accurate state value estimates. It\nhas also been widely adopted in complex reasoning\ntasks for data generation (Xie et al., 2024; Chen\net al., 2024a; Feng et al., 2023; Chen et al., 2024b).\nIn this framework, we modify MCTS to collect\ntraining data for the policy model \u03c0\u03bf, incorporating\nthe following key steps:\nSelection: Starting from the root node, child\nnodes are recursively selected using the PUCT algo-\nrithm (Rosin, 2011) until an expandable leaf node\nis reached. The next node is chosen based on a\ntrade-off between exploration and exploitation:\ns' = arg max[Q(s, a) + c\u00b7 $\u03c0_\u03b8(a | s)$ \u221aN(s) / (1 + N(s'))].\nHere, Q(s, a) represents the Q value estimate\nfrom the tree search, N(s) is the visit count of\nnode s, and c is a hyperparameter that regulates the\ntrade-off between exploration and exploitation.\nExpansion: The selected node is expanded by\nadding the top-k most probable actions and their\ncorresponding next states as child nodes.\nEvaluation & Backup: Leaf nodes are assigned\na KL-regularized reward (Eq. 9). During soft value\nfunction estimation, the entropy term from the pol-\nicy model cancels out the KL regularization term,\nallowing the use of the original reward for sampling.\nTerminal node results are propagated back to the\nroot node, updating visit counts and cumulative\nrewards:\n$N(s', a) \u2190 N(s, a) + 1$\n$V(s) \u2190 \\frac{\\sum_{a'}N(s')(r(s, a) + V(s'))}{\\sum_{a'}N(s')}$\nEstimation Using a Value Model\nOutcome value models are widely used in\ninference-time tree search, providing value guid-\nance for each step in the search process (Yu et al.,\n2023a; Wang et al., 2023b). We leverage a value\nmodel to assist in training the policy model \u03c0\u03b8.\nDuring value model training, we minimize a mean\nsquared error loss to directly model the outcome\nreward:\n$L_V(\\phi) = E_{(s^{(n)},r_i)\u2208D} [\\frac{1}{n-2} \\sum_{t=1}^{n} V_\\phi(s_t) - r_i)^2]$\nwhere $s^{(n)}$ represents a trajectory of n states, ri is\nthe associated outcome reward, and $V_\\phi(s_t)$ is the\npredicted value for st by the value model parameter-\nized by \u03c6. The data is defined as $D = {(s^{(n)}, r_i)}$,\nwhich is responses generated from \u03c0\u03b8. Once trained\nwith D, the value model provides target values to\nupdate the policy model \u03c0\u03b8. Notably, it remains\nfixed during policy optimization and is not updated\nalongside \u03c0\u03c6."}, {"title": "Experimental Settings", "content": "Datasets. We validate our approach on both math\nword problems and commonsense reasoning tasks.\nFor math word problems, we use GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021)\nas in-domain test sets. The former contains over\n1,300 grade school math problems, while the latter\nincludes 5,000 problems spanning five difficulty\nlevels. AGIEval-Math (Zhong et al., 2023) is used\nas an out-of-distribution (OOD) test set. For train-\ning, we take MetaMath (Yu et al., 2023c), a dataset"}, {"title": "Results", "content": "Target Value Estimation\nWe first compare the two options for target value\nestimation: MC estimation and outcome value\nmodel. Specifically, we initialize the value network\nwith Llama-3-8B-Instruct, following the OVM ap-\nproach (Yu et al., 2023a) to train the network,\nand use it to label target values for DVO training\ndata. The results, shown in Fig. 3, indicate that the\nMC estimation achieves better performance while\navoiding the additional computational overhead re-\nquired to train a separate value network.\nTherefore, all subsequent experiments are per-\nformed using MCTS for value estimation.\nMain Results\nWe compare DVO with baseline methods on math\nreasoning tasks. As shown in Tab. 1, DVO consis-\ntently delivers stable gains across various model\nbackbones and significantly outperforms DPO and\nits alternatives KTO and Step-DPO. Specifically,\nDVO improves DeepSeek-Math from 83.0% (KTO)"}, {"title": "Commonsense Tasks", "content": "We further validate the effectiveness of DVO on\ncommonsense reasoning tasks. The results are pre-\nsented in Tab. 2. The experiments are conducted on\nLlama-3-8B-Instruct (see implementation details\nin App. C). DVO demonstrates strong performance\nand generalization capabilities in commonsense\nreasoning tasks, further highlighting its versatility\nacross different reasoning paradigms."}, {"title": "Ablation Study", "content": "Ablation on beta B. We evaluate the impact of \u03b2\non DVO performance by scanning different values\non the same training set, with all models trained for\na single epoch. As shown in Fig. 4a, DVO achieves\nthe best performance when \u03b2 = 0.1. Both exces-\nsively large and small values of \u03b2 result in degraded\nperformance, indicating that DVO is sensitive to\nthe balance between KL divergence regularization\nand entropy constraints.\nAblation on Search Iteration. We evaluate the\nimpact of the number of search iterations in MCTS\non performance. As the number of iterations in-\ncreases, the accuracy of value estimation improves.\nAs shown in Fig. 4b, inaccurate value estimation\nleads to a noticeable drop in performance. Addi-\ntionally, we observe that on the simpler GSM8K\ndataset, the model requires higher precision in\nvalue estimation. We hypothesize that this is be-\ncause simpler tasks demand finer distinctions be-\ntween different steps to improve reasoning perfor-\nmance. Overall, we find that search iterations be-\nyond 40 generally lead to better results."}, {"title": "Analysis", "content": "Deep observation on implicit reward. Recent\nwork (Chen et al., 2024c; Yuan et al., 2024; Pal\net al., 2024) has shown that during DPO training,\nwhile the implicit reward margin between positive\nand negative responses continues to increase, the\nimplicit reward of the positive responses gradually\ndecreases (as shown in Fig. 5a). This phenomenon\nlikely arises because DPO's optimization objective\nemphasizes maximizing the reward gap between\npositive and negative responses, rather than directly\nincreasing the rewards for positive responses or de-\ncreasing those for negative ones. Consequently,\nDPO causes a decrease in the probability of posi-\ntive data, resulting in suboptimal outcomes. We an-"}, {"title": "Step level vs. response level DVO", "content": "DVO can\nbe applied at the response level, where the entire\nresponse is treated as a single action, and optimiza-\ntion is performed over the entire response. This\nsetup is similar to DPO, while the key difference\nis that DVO directly aligns the Q-value of the re-\nsponse rather than employing contrastive learning\non paired data. We conduct comparative experi-\nments on Llama-3-8B-Instruct, and the results are\nshown in Fig. 7. As the figure shows, response-\nlevel optimization underperforms step-level opti-\nmization, demonstrating the effectiveness of finer-\ngrained supervision for improving reasoning. No-\ntably, response-level optimization still outperforms\nDPO, suggesting that at the same granularity, op-\ntimizing for explicit value signals rather than pref-\nerence relationships enables the policy model to\ngenerate high-quality responses more effectively.\nCredit assignment cases. Similar as Rafailov\net al. (2024), after DVO training we can take\n$\\frac{\u03c0_\u03b8(a_t | s_t)}{\u03c0_{ref}(a_t | s_t)} = r(s_t, a_t) + V_\u03b8(s_{t+1}) - V_\u03b8(s_t)$ from\nEq. 10 as the DVO credit, indicating the potential\nreward or loss from transitioning between states.\nWe sample correct and incorrect reasoning chain\nfrom the reference model and visualize steps with\nthe corresponding DVO credit. As illustrated in\nFig. 6, each step is colored based on the DVO credit,\nwith erroneous steps receiving low credit (darker\ncolors). This example demonstrates that DVO has\nlearned step-level value information, enabling it to\nidentify faulty reasoning steps."}, {"title": "Conclusion", "content": "We introduced Direct Value Optimization (DVO),\na novel framework that enhances LLMs' reason-\ning ability by directly optimizing reasoning paths\nwith estimated step-wise value estimation instead\nof preference labels. This approach provides finer-\ngrained supervision signals while eliminating the\nneed for paired data construction. Practically, we\ngenerate offline samples via tree search and es-\ntimate target values using either MC estimation\nor a value model, then update the policy model\nwith MSE loss. Extensive experiments on both\nin-domain and out-of-domain datasets demonstrate\nDVO's effectiveness. Notably, these improvements\nwere achieved without requiring new query data or\nadditional supervision. Furthermore, our analytical\nexperiments validated the utility of step-level value\ninformation, underscoring the broader applicability\nof DVO across various reasoning tasks."}, {"title": "Limitations", "content": "The proposed DVO approach exhibits two primary\nlimitations. First, the current implementation of\nDVO is confined to offline settings. However, this\nlimitation can be addressed in future work by inte-\ngrating the MCTS search process with the update\nstep in the training pipeline, thereby enabling on-\nline operation. Second, due to computational con-\nstraints, we restrict the MCTS search width to 5 in\nour experiments. Theoretically, with adequate com-\nputational resources, expanding the MCTS search\nwidth would yield superior performance through\ntwo mechanisms: we will get a more accurate value\nestimate, while simultaneously the scale of data\navailable for training is also larger."}, {"title": "A Proof of Proposition", "content": "Proposition 1. In general maximum entropy re-\ninforcement learning setting, a language model\nparameterized by \u03c0\u03b8 can be seen as an optimal soft\nQ-function under some reward.\n$\\frac{1}{\u03b2}\u03c0_\u03b8 (a_t | s_t) = exp (\\frac{1}{\u03b2} (Q^* (s_t, a_t) - V^* (s_t)))$\nProof. The proposition is introduced in Rafailov\net al. (2024), and we provide a similar proof here\nat the step level. Firstly, we prove that a large lan-\nguage model in a token-level MDP represents an\noptimal soft Q-function. In the token-level MDP,\neach action at corresponds to a token, and the ac-\ntion space A is defined as the vocabulary of the\nlanguage model. At step t, the state st comprises\na prompt and a sequence of generated reasoning\ntokens. The policy model \u03c0\u03b8 selects the next action\nat based on the probability distribution \u03c0\u03b8(at | st).\nTo encourage robust reasoning, the reward function\nr(st, at) is defined at the token level. The transi-\ntion function f concatenates the current state st\nwith the selected action at, producing the next state\nf(st, at) = st | at. The discount factor \u03b3 is set\nto 1. We also omit explicit references in transition\nprobabilities P(st+1 | st, at)\nA language model can be represented as a set of\nlogits l\u03b8(s), which, combined with a temperature\nparameter \u00df, define the action distribution via a\nsoftmax function:\n$\u03c0_\u03b8(a | s) = \\frac{exp(l_\u03b8(s)[a]/\u03b2)}{\u2211_{a'}exp(l_\u03b8(s)[a']/\u03b2)}$\nThe optimal Q-function Q*(st, at) is defined as\nthe expected cumulative reward when starting from\nstate st and taking action at, following the Bellman\nequation:\n$Q^*(s, a) = r(s, a) + Blog V^* (s')$ \n$= r(s, a) + Blog \u2211_{a' \u2208 A} exp (\\frac{Q(s',a')}{\u03b2})$\nwhere s' = f(s, a) is the next state. The optimal\npolicy \u03c0*(a | s) is derived from the optimal Q-\nfunction:\n$\u03c0^*(a | s) = \\frac{exp(Q^* (s, a)/\u03b2)}{\u2211_{a'}exp(Q^* (s, a')/\u03b2)}$\nGiven the LM policy in Eq. (12), let Q*(s, a) \u2261\nl\u03b8(s)[a]. Substituting Q*(s, a) into Eq.(14), we\nobtain:\n$\u03c0_\u03b8(a | s) = \\frac{exp(Q^* (s, a)/\u03b2)}{\u2211_{a'\u2208A} exp(Q^* (s, a')/\u03b2)}$\nThus, the LM policy \u03c0\u03bf(a | s) matches the optimal\npolicy \u03c0* derived from Q*(s, a) in Eq. (14). To\nensure consistency with the Bellman equation, the\nreward function r(s, a) must satisfy:\n$r(s, a) = l_\u03b8(s) [a] \u2013 Blog \u2211_{a'\u2208A} exp (l_\u03b8(s)[a'])$\nwhere s' = f(s, a). This ensures that the logits\nl\u03b8(s) represent the optimal Q-function. Thus the\nlogits of the language model correspond to the opti-\nmal Q-function, and the resulting policy is optimal\nfor this setting.\nWhile the above argument is presented at the to-\nken level, an equivalent view is to treat multiple to-\nkens as a single \u201cmacro-action\" and define the step-\nlevel reward as the sum of the token-level rewards\nfor those tokens. In this step-level perspective, one\naction a = (a1, a2, . . ., aL) corresponds to sequen-\ntially choosing a\u2081, . . ., aL at the token level. If we\ndenote by \u03c0\u03b8 the token-level optimal policy, then\nthe probability of executing the macro-action a un-\nder \u03c0\u03b8 is precisely the product of its single-token\nprobabilities: \u03c0\u03b8(a1 | s) \u00d7\u00b7\u00b7\u00b7\u00d7 \u03c0\u03b8(aL | s(L-1)).\nConsequently, viewing several tokens as one step\ndoes not alter the overall sequence distribution or\nthe value derived from it, the language model's\nlogits still induce the same optimal strategy under\nthis coarser step-level view, since the probability\nof any generated block of tokens and its accumu-\nlated reward remain consistent with the original\ntoken-level formulation."}, {"title": "Implementation Details", "content": "To enable LLMs to follow Step-by-step pattern, we\ncollect self-generated data to fine-tune all backbone\nmodels. Specifically, similar to Lightman et al.\n(2023), we start by generating multiple solutions to\n10,000 MetaMath problems in a few-shot manner\nand use the correct solutions to SFT the backbone\nmodels. The objective here is to avoid introducing\nexternal reasoning paths, as our focus is solely on\nleveraging answers to enhance the model's perfor-\nmance. The few-shot prompt is shown as below:"}, {"title": "Comparative Experiments\nImplementation Details", "content": "To facilitate a comprehensive comparison of the\nDVO with other baseline algorithms", "follows": "n\u2022 RFT: Samples positive instances from the\ntrees based on the solution correctness", "DPO": "Samples paired solutions based on the\nsolution correctness", "KTO": "Samples both positive and negative in-\nstances based on the solution correctness", "Step-DPO": "Conducts step-level paired sam-\npling of positive and negative instances based\non the step value estimation", "DVO": "Similar to KTO, it samples both posi-\ntive and negative instances based on the step\nvalue estimation"}]}