{"title": "PyBench: Evaluating LLM Agent on various real-world coding tasks", "authors": ["Yaolun Zhang", "Yinxu Pan", "Yudong Wang", "Jie Cai", "Zhi Zheng", "Guoyang Zeng", "Zhiyuan Liu"], "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing. However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks. To address this gap, we introduce PyBench, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedback from executed code. Our evaluations indicate that current open-source LLMS are struggling with these tasks. Hence, we conduct analysis and experiments on four kinds of datasets proving that comprehensive abilities are needed for PyBench. Our fine-tuned 8B size model: PyLlama3 achieves an exciting performance on PyBench which surpasses many 33B and 70B size models. Our Benchmark, Training Dataset, and Model are available at: https://github.com/Mercury7353/PyBench", "sections": [{"title": "1 Introduction", "content": "The best tool is the one that gets the job done. Enormous real-world tasks like data analysis and image & audio processing can be solved by code. Among many programming languages, Python stands out for its simplicity, ease of use, extensive libraries, and high compatibility, making it a widely used tool for daily tasks. However, individuals often need to invest significant time in learning how to use extension packages, even if they are already highly proficient in Python itself. Thanks to LLM's powerful code capabilities, it can act as an automatic agent (Wang et al., 2024a; Park et al., 2023; Qin et al., 2023) writing and executing code to solve a wide spectrum of real-world tasks.\nHowever, current LLM code benchmarks have not covered the real-world task area. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) focus on function complement and simple Python problems, which could not evaluate the usefulness of LLM Agent.\nAlthough benchmarks such as DS-1000 (Lai et al., 2023), DevBench (Li et al., 2024), and SWE-Bench(Jimenez et al., 2023) focus on repository-level coding issues, they assess the ability of LLMs to use and manage specific codebases. These tasks are relatively narrow in scope and inherently limited, deviating from practical daily application scenarios and being overly complex for routine use.\nTo address the lack of benchmarks for real-world coding tasks, we introduce PyBench, a comprehensive and versatile benchmark designed to evaluate the practical coding abilities of LLMs. Specifically, we formulated real-world coding tasks into 5 main"}, {"title": "2 Related Works", "content": "Previous works enhance LLM's coding ability through various methods. OpenCodeInterpreter (Zheng et al., 2024b) introduces the CodeFeedback dataset and a code execution system with feedback, the fine-tuned model achieves a great performance on coding benchmarks. CodeAct (Wang et al., 2024b) uses executable Python code to unify LLM agents' action space, enabling sophisticated task execution through multi-turn interactions. NexT (Ni et al., 2024) teaching LLM reasoning the execution process of code step by step, effectively improving the code quality. WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2024), and AlchemistCoder (Song et al., 2024) build effective fine-tuning datasets from massive and multi-source data to train advanced code LLMs. Pre-training on code-rich data is also a good method to help LLM coding. CodeQwen (Bai et al., 2023)and Deepseek-Coder (Guo et al., 2024; DeepSeek-AI et al., 2024) develop specialized models for coding by continuing to pre-train on code data and employing supervised fine-tuning strategies."}, {"title": "2.3 LLM Agent for real-world tasks", "content": "LLM as Agent is a great (Qian et al., 2023; Park et al., 2023; Chen et al., 2024) attempt utilizing LLM in real-world tasks. ReAct (Yao et al., 2022) first introduced Agent's Reasoning and Action Format. Previous works design many frameworks that build and organize LLM Agents to complete real-world coding tasks. MetaGPT (Hong et al., 2023) ChatDev (Qian et al., 2023), DataInterpreter (Hong et al., 2024), and MatplotAgent (Yang et al., 2024) employ agents to complete software development or data science tasks. AgentCoder (Huang et al., 2023) focuses on simple code complement tasks. Furthermore, some general multi-agent systems try to adapt agents to various tasks (Chen et al., 2023b,a; Wang et al., 2023b)."}, {"title": "3 PyBench", "content": "Coding is a core skill for LLM Agents. When the agent needs to solve real-world coding tasks, it should not only write executable code but also utilize the results of execution to guide subsequent actions and interact with files. Python is a powerful programming language with almost 4 million packages, capable of covering almost all real-world coding tasks. Therefore, we propose building PyBench to evaluate the LLM agent's ability in reasoning, writing executable Python code, and utilizing code results."}, {"title": "3.1 Task Formulation", "content": "We first define what kinds of tasks need to be solved as a truly helpful LLM Agent equipped with a Python code interpreter. Given a user query q described in natural language and the related file $F_{in}$, the Agent need to generate a formal answer Ans to user query and output file $F_{out}$, which fulfill the user's requirement:\n$Ans, F_{out} = A(q, F_{in})$\nwhere A is the LLM Agent equipped with a code interpreter. To be specific, the query q from users can be very high level, reflecting common occurrences in daily life where users may not have precise requirements. The related file $F_{in}$ contains various types of data such as chart, text, audio, image, etc., showcase the LLM Agent should adapt to various real-world coding tasks."}, {"title": "3.2 Task Categories", "content": "In the realm of practical coding applications, the LLM Agent is required to autonomously tackle a diverse array of real-world coding challenges. To ensure a comprehensive evaluation of its capabilities, we have meticulously curated five main categories of real-world coding tasks. Each category is designed to test the LLM Agent's proficiency across a broad spectrum of scenarios, ranging from data analysis to software development. Table 1 demonstrates these categories:\nChart Analysis. In the digital age, the ability to efficiently analyze and interpret data is indispensable. This category focuses on tasks that involve handling csv or xlsx files for various purposes such as data preprocessing, transformation, visualization, and the application of machine learning algorithms (Hong et al., 2024; Yang et al., 2024). The output"}, {"title": "3.3 Data Collection", "content": "We collect and filter the files in PyBench from two main sources.\nKaggle Data. Kaggle is a great platform for machine learning, which contains massive datasets. We obtained csv and xlsx data on Kaggle through web crawlers. There are two principles of filtering the files. Firstly, the files should not be too large, considering the limited memory in the test environment. Secondly, the data tables must contain multiple columns with clear meaning, simulating commonly used files.\narXiv Data. We collect pdf and txt from arXiv. The papers on arXiv are high-quality text with a clear theme and structure, which is suitable for text-based QA, Theme Analysis, and drawing word clouds.\nOther Sources Data. For other file types, we responsibly collect files, including png, jpeg, gif, mp3, and wav from the internet, ensuring that all content respects copyright laws, protects user privacy, and is free from harmful elements."}, {"title": "3.4 Task Generation", "content": "The queries in PyBench must be precisely related to files and diverse to ensure comprehensive cov-"}, {"title": "3.5 Evaluation", "content": "To objectively and effectively test whether the LLM Agent has completed a task, we have implemented a unit test for each task in PyBench. For tasks with a fixed answer, we verify whether the Agent provides a final response that contains the correct answer. For tasks requiring a file output, such as cleaned datasets or edited images and audio, we check whether the output files meet the specified requirements. For tasks without a fixed answer, such as generating a word cloud or a website, we verify the existence of the output files. Detailed in Appendix C"}, {"title": "3.5.3 LLM as Evaluator", "content": "Although unit tests are convenient and objective, they may fail to comprehensively evaluate open-ended tasks, such as assessing the coherence and fluency of text output by large models. Therefore, we also employ an LLM (GPT-40) as an evaluator to provide pass-or-fail decisions for each trajectory, serving as an alternative to unit tests. Appendix A.4 provides the detailed prompt used for the LLM evaluator."}, {"title": "3.5.4 Evaluation Metrics", "content": "There are three evaluation metrics in PyBench."}, {"title": "4 Fine-tuning LLM Agent for Real World Coding Task", "content": "In order to figure out what kind of capabilities are required and what training data could help enhance the LLM Agent's performance on PyBench, we collect 4 datasets enhancing the Agent's abilities in planning, coding, and multi-turn interaction.\nHomologous dataset. Intuitively, homologous datasets could enhance performance on the same task to be solved. Hence, we employ GPT-3.5-turbo to synthesize trajectories. The process is similar to the generation method without manually checking queries and using different files. We synthesized 3091 trajectory data covering every task class in PyBench to construct PyInstruct.\nMulti-turn of code interaction dataset. Most of the tasks in PyBench need multi-turn interaction with the Python code interpreter. It will provide feedback on code execution results or error traceback messages, which should be fully leveraged by the LLM Agent. There are several existing datasets aiming at enhancing LLM's ability. CodeActInstruct (Wang et al., 2024b) focuses on improving LLM's abilities in various multi-turn tasks such as information seeking, software tool usage, external memory access, and robot planning, all executed in the format of Python code. CodeFeedback (Zheng et al., 2024b) filters open-source code instruction data and converts them into multi-turn code with execution results. We repurpose the data by \"equipping\" our special token for the Python code interpreter. These datasets may help enhance LLM's ability to utilize code feedback.\nMulti-turn chat dataset. Additionally, in PyBench, the LLM Agent is expected to comprehend the user's instructions and provide a formal response upon completing the task. High-quality multi-turn chat is also crucial for a Code Agent. UltraChat (Ding et al., 2023) is a large-scale dataset comprising 1.5 million high-quality multi-turn instructional dialogues, specifically designed to improve the performance of open-source conversational models, which is perfectly aligned with our requirements.\nCode-Rich corpus. The foundation ability of a code agent is the quality and correctness of its code. We assume that continue pre-train on code-rich corpus could contribute to solving PyBench tasks. The-stack-v2 (Lozhkov et al., 2024) introduces a code-rich corpus of Jupyter notebooks, which contains 11 million lines.\nAfter collecting the four types of datasets, we conducted a series of experiments to figure out what are necessary abilities to solve the PyBench tasks and how to improve the LLM performance on real-world coding tasks."}, {"title": "5 Experiment", "content": "The performance of LLMs on specific tasks can often be enhanced through supervised fine-tuning on homologous datasets, where the model learns exactly what to do in particular situations. Initially, we hypothesized that training the Llama3-8B-base model on PyInstruct, 3k homologous trajectories data, would teach the model to handle real-world coding tasks. However, as shown in Table 4, this model struggles with PyBench tasks. From the generated trajectories, we observed that the model fails to follow human instructions and even struggles to locate the correct file paths.\nTo address the drawbacks, we add two multi-turn code interaction datasets: CodeActInstruct (Wang et al., 2024b) and CodeFeedback (Zheng et al., 2024b). We train the model on these datasets and PyInstruct. The result indicated a significant improvement. But when we remove PyInstruct and train the model only on CodeActInstruct and CodeFeedback, the model's performance suffers a sharp decline in Chart Analysis and Text Analysis,"}, {"title": "6 Conclusion", "content": "In this paper, we propose PyBench, a comprehensive benchmark encompassing five main categories that reflect real-world coding situations. Through collecting files and generating related queries, PyBench could evaluate LLM's usability and efficiency in real-world tasks. After evaluating plenty of LLMs, we find many of them are struggling with real-world coding tasks. We collected and synthesized four datasets and trained PyLlama3 whose performance surpass many 7B, 33B, and 70B size models. Our ablation studies prove the effectiveness of the datasets, figuring out a way to train a model that could adapt to real-world coding tasks. Solving PyBench tasks means the LLM could interact with the file system with Python code, which symbolizes a great milestone in developing a really usable LLM Agent who can serve as a helpful life assistant for humankind."}, {"title": "7 limitations", "content": "Our work introduces a comprehensive benchmark: PyBench to evaluate LLM Agent on real-world coding tasks. Although five main categories are included in PyBench, there are many cases in the real world that have not been covered. The coding problem that uses other coding languages instead of Python is not covered either."}]}