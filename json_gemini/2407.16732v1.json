{"title": "PyBench: Evaluating LLM Agent on various real-world coding tasks", "authors": ["Yaolun Zhang", "Yinxu Pan", "Yudong Wang", "Jie Cai", "Zhi Zheng", "Guoyang Zeng", "Zhiyuan Liu"], "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing. However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks. To address this gap, we introduce PyBench, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedback from executed code. Our evaluations indicate that current open-source LLMS are struggling with these tasks. Hence, we conduct analysis and experiments on four kinds of datasets proving that comprehensive abilities are needed for PyBench. Our fine-tuned 8B size model: PyLlama3 achieves an exciting performance on PyBench which surpasses many 33B and 70B size models. Our Benchmark, Training Dataset, and Model are available at: https://github.com/Mercury7353/PyBench", "sections": [{"title": "1 Introduction", "content": "The best tool is the one that gets the job done. Enormous real-world tasks like data analysis and image & audio processing can be solved by code. Among many programming languages, Python stands out for its simplicity, ease of use, extensive libraries, and high compatibility, making it a widely used tool for daily tasks. However, individuals often need to invest significant time in learning how to use extension packages, even if they are already highly proficient in Python itself. Thanks to LLM's powerful code capabilities, it can act as an automatic agent (Wang et al., 2024a; Park et al., 2023; Qin et al., 2023) writing and executing code to solve a wide spectrum of real-world tasks.\nHowever, current LLM code benchmarks have not covered the real-world task area. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) focus on function complement and simple Python problems, which could not evaluate the usefulness of LLM Agent.\nAlthough benchmarks such as DS-1000 (Lai et al., 2023), DevBench (Li et al., 2024), and SWE-Bench(Jimenez et al., 2023) focus on repository-level coding issues, they assess the ability of LLMs to use and manage specific codebases. These tasks are relatively narrow in scope and inherently limited, deviating from practical daily application scenarios and being overly complex for routine use.\nTo address the lack of benchmarks for real-world coding tasks, we introduce PyBench, a comprehensive and versatile benchmark designed to evaluate the practical coding abilities of LLMs. Specifically, we formulated real-world coding tasks into 5 main"}, {"title": "2 Related Works", "content": "Many existing benchmarks focus on LLM's code ability. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are two widely recognized benchmarks primarily evaluating LLM's ability to complete functions or solve simple Python problems. HumanEval-X, HumanEval+, and MBPP+\n(Zheng et al., 2024a; Liu et al., 2023a) extend the benchmarks by adding multilingual and plenty of extra tests. APPS (Hendrycks et al., 2021) focuses on writing code from natural language description. TACO (Li et al., 2023) builds a more complex benchmark evaluating LLM on algorithmic code tasks. MINT (Wang et al., 2023a) and M\u00b3ToolEval (Wang et al., 2024b) aim to evaluate models' multi-turn interaction code ability, with tools or human feedback. There are also extremely complex and hard benchmarks evaluating LLMs on software development (Qian et al., 2023; Hong et al., 2023), code repository issues (Jimenez et al., 2023; Li et al., 2024), and data science tasks(Lai et al., 2023). However, these benchmarks are all limited to specific scenarios. To the best of our knowledge, no existing benchmark evaluates LLM Agent on real-world coding tasks with various situations."}, {"title": "2.2 Code LLMs", "content": "Previous works enhance LLM's coding ability through various methods. OpenCodeInterpreter\n(Zheng et al., 2024b) introduces the CodeFeedback dataset and a code execution system with feedback, the fine-tuned model achieves a great performance on coding benchmarks. CodeAct (Wang et al.,\n2024b) uses executable Python code to unify LLM agents' action space, enabling sophisticated task execution through multi-turn interactions. NexT (Ni et al., 2024) teaching LLM reasoning the execution process of code step by step, effectively improving the code quality. WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2024), and AlchemistCoder (Song et al., 2024) build effective fine-tuning datasets from massive and multi-source data to train advanced code LLMs. Pre-training on code-rich data is also a good method to help"}, {"title": "2.3 LLM Agent for real-world tasks", "content": "LLM as Agent is a great (Qian et al., 2023; Park et al., 2023; Chen et al., 2024) attempt utilizing\nLLM in real-world tasks. ReAct (Yao et al., 2022) first introduced Agent's Reasoning and Action Format. Previous works design many frameworks that build and organize LLM Agents to complete real-world coding tasks. MetaGPT (Hong et al., 2023)\nChatDev (Qian et al., 2023), DataInterpreter (Hong et al., 2024), and MatplotAgent (Yang et al., 2024)\nemploy agents to complete software development or data science tasks. AgentCoder (Huang et al.,\n2023) focuses on simple code complement tasks. Furthermore, some general multi-agent systems try to adapt agents to various tasks (Chen et al.,\n2023b,a; Wang et al., 2023b)."}, {"title": "3 PyBench", "content": "Coding is a core skill for LLM Agents. When the agent needs to solve real-world coding tasks, it should not only write executable code but also utilize the results of execution to guide subsequent actions and interact with files. Python is a powerful programming language with almost 4 million packages, capable of covering almost all real-world coding tasks. Therefore, we propose building PyBench to evaluate the LLM agent's ability in reasoning, writing executable Python code, and utilizing code results."}, {"title": "3.1 Task Formulation", "content": "We first define what kinds of tasks need to be solved as a truly helpful LLM Agent equipped with a Python code interpreter. Given a user query q described in natural language and the related file $F_{in}$, the Agent need to generate a formal answer Ans to user query and output file $F_{out}$, which fulfill the user's requirement:\n$Ans, F_{out} = A(q, F_{in})$\nwhere A is the LLM Agent equipped with a code interpreter. To be specific, the query q from users can be very high level, reflecting common occurrences in daily life where users may not have precise requirements. The related file $F_{in}$ contains various types of data such as chart, text, audio, image, etc., showcase the LLM Agent should adapt to various real-world coding tasks."}, {"title": "3.2 Task Categories", "content": "In the realm of practical coding applications, the LLM Agent is required to autonomously tackle a diverse array of real-world coding challenges. To ensure a comprehensive evaluation of its capabilities, we have meticulously curated five main categories of real-world coding tasks. Each category is designed to test the LLM Agent's proficiency across a broad spectrum of scenarios, ranging from data analysis to software development. Chart Analysis. In the digital age, the ability to efficiently analyze and interpret data is indispensable. This category focuses on tasks that involve handling csv or xlsx files for various purposes such as data preprocessing, transformation, visualization, and the application of machine learning algorithms"}, {"title": "3.3 Data Collection", "content": "We collect and filter the files in PyBench from two main sources.\nKaggle Data. Kaggle is a great platform for machine learning, which contains massive datasets. We obtained csv and xlsx data on Kaggle through web crawlers. There are two principles of filtering the files. Firstly, the files should not be too large, considering the limited memory in the test environment. Secondly, the data tables must contain multiple columns with clear meaning, simulating commonly used files.\narXiv Data. We collect pdf and txt from arXiv. The papers on arXiv are high-quality text with a clear theme and structure, which is suitable for text-based QA, Theme Analysis, and drawing word clouds.\nOther Sources Data. For other file types, we responsibly collect files, including png, jpeg, gif, mp3, and wav from the internet, ensuring that all content respects copyright laws, protects user privacy, and is free from harmful elements."}, {"title": "3.4 Task Generation", "content": "The queries in PyBench must be precisely related to files and diverse to ensure comprehensive cov-"}, {"title": "3.5 Evaluation", "content": ""}, {"title": "3.5.1 Trajectory Generation", "content": "Equip LLM Agent with Code Interpreter. We equipped each LLM with a code interpreter to execute Python code written by the LLM and provide feedback on the execution results. Previous works on LLM tool usage (Qin et al., 2023) typically used tools in a function-calling format. Inspired by (Wang et al., 2024b), which uses code as an action and outperforms alternatives, we designed two special tokens: <lexecute_start|> and <lexecute_end|> to help the LLM use the code interpreter more effectively. Appendix D shows the different performance of the two formats."}, {"title": "3.5.2 Unit Test", "content": "To objectively and effectively test whether the LLM Agent has completed a task, we have implemented a unit test for each task in PyBench. For tasks with a fixed answer, we verify whether the Agent provides a final response that contains the correct answer. For tasks requiring a file output, such as cleaned datasets or edited images and audio, we check whether the output files meet the specified requirements. For tasks without a fixed answer, such as generating a word cloud or a website, we verify the existence of the output files. Detailed in Appendix C"}, {"title": "3.5.3 LLM as Evaluator", "content": "Although unit tests are convenient and objective, they may fail to comprehensively evaluate open-ended tasks, such as assessing the coherence and fluency of text output by large models. Therefore, we also employ an LLM (GPT-40) as an evaluator to provide pass-or-fail decisions for each trajectory, serving as an alternative to unit tests. Appendix A.4 provides the detailed prompt used for the LLM evaluator."}, {"title": "3.5.4 Evaluation Metrics", "content": "There are three evaluation metrics in PyBench:"}, {"title": "4 Fine-tuning LLM Agent for Real World Coding Task", "content": "In order to figure out what kind of capabilities are required and what training data could help enhance the LLM Agent's performance on PyBench, we collect 4 datasets enhancing the Agent's abilities in planning, coding, and multi-turn interaction.\nHomologous dataset. Intuitively, homologous datasets could enhance performance on the same task to be solved. Hence, we employ GPT-3.5-turbo to synthesize trajectories. The process is similar to the generation method without manually checking queries and using different files. We synthesized 3091 trajectory data covering every task class in PyBench to construct PyInstruct.\nMulti-turn of code interaction dataset. Most of the tasks in PyBench need multi-turn interaction with the Python code interpreter. It will provide feedback on code execution results or error traceback messages, which should be fully leveraged by the LLM Agent. There are several existing datasets aiming at enhancing LLM's ability. CodeActInstruct (Wang et al., 2024b) focuses on improving LLM's abilities in various multi-turn tasks such as information seeking, software tool usage, external memory access, and robot planning, all executed in the format of Python code. CodeFeedback (Zheng et al., 2024b) filters open-source code instruction data and converts them into multi-turn code with execution results. We repurpose the data by \"equipping\" our special token for the Python code interpreter. These datasets may help enhance LLM's ability to utilize code feedback. Multi-turn chat dataset. Additionally, in PyBench, the LLM Agent is expected to comprehend the user's instructions and provide a formal response upon completing the task. High-quality multi-turn chat is also crucial for a Code Agent. UltraChat (Ding et al., 2023) is a large-scale dataset comprising 1.5 million high-quality multi-turn instructional dialogues, specifically designed to improve the performance of open-source conversational models, which is perfectly aligned with our requirements.\nCode-Rich corpus. The foundation ability of a code agent is the quality and correctness of its code. We assume that continue pre-train on code-rich corpus could contribute to solving PyBench tasks. The-stack-v2 (Lozhkov et al., 2024) introduces a code-rich corpus of Jupyter notebooks, which contains 11 million lines."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Main Result on PyBench", "content": "Testing Evaluation Setup. Firstly, we prepare a conda environment equipped with 182 commonly used Python packages. The max turn is set to k = 10. We prompt the LLM to use code interpreter and follow ReAct format. The code in LLM's response will be extracted and the execution result will return to LLM. After getting the trajectory and output files, we calculate the pass rate through the unit test set (UT) and LLM Evaluator set (LLM). We also adopt other benchmarks to test the model's basic code ability, including HumanEval (Chen et al., 2021) and HumanEval+ (Liu et al., 2023a) for single-turn code generation, and MBPP (Austin et al., 2021) and MBPP+ (Liu et al., 2023a) for"}, {"title": "5.2 Analysis on the training dataset", "content": "In this section, we conduct ablation studies on training datasets to explore the necessary capabilities for solving PyBench."}, {"title": "5.2.1 Train Setup", "content": "We conduct full-parameter supervised fine-tuning on Llama3-8B with a sequence length of 32768 tokens, ensuring it could handle the content of chart and text files. For each version of the supervised fine-tuning model, we use 32 A100 GPU and train 4000 steps. The learning rate is set as le-5 with a 0.05 warm-up ratio and a cosine scheduler. As for the continue pre-trained model, we add an extra 3000 steps to training the corpus."}, {"title": "5.2.2 Ablation study", "content": "The performance of LLMs on specific tasks can often be enhanced through supervised fine-tuning on homologous datasets, where the model learns exactly what to do in particular situations. Initially, we hypothesized that training the Llama3-8B-base model on PyInstruct, 3k homologous trajectories data, would teach the model to handle real-world coding tasks. However, as shown in Table 4, this model struggles with PyBench tasks. From the generated trajectories, we observed that the model fails to follow human instructions and even struggles to locate the correct file paths.\nTo address the drawbacks, we add two multi-turn code interaction datasets: CodeActInstruct (Wang et al., 2024b) and CodeFeedback (Zheng et al., 2024b). We train the model on these datasets and PyInstruct. The result indicated a significant improvement. But when we remove PyInstruct and train the model only on CodeActInstruct and CodeFeedback, the model's performance suffers a sharp decline in Chart Analysis and Text Analysis,"}, {"title": "6 Conclusion", "content": "In this paper, we propose PyBench, a comprehensive benchmark encompassing five main categories that reflect real-world coding situations. Through collecting files and generating related queries, PyBench could evaluate LLM's usability and efficiency in real-world tasks. After evaluating plenty of LLMs, we find many of them are struggling with real-world coding tasks. We collected and synthesized four datasets and trained PyLlama3 whose performance surpass many 7B, 33B, and 70B size models. Our ablation studies prove the effectiveness of the datasets, figuring out a way to train a model that could adapt to real-world coding tasks. Solving PyBench tasks means the LLM could interact with the file system with Python code, which symbolizes a great milestone in developing a really usable LLM Agent who can serve as a helpful life assistant for humankind."}, {"title": "7 limitations", "content": "Our work introduces a comprehensive benchmark: PyBench to evaluate LLM Agent on real-world coding tasks. Although five main categories are included in PyBench, there are many cases in the real world that have not been covered. The coding problem that uses other coding languages instead of Python is not covered either."}, {"title": "A Prompts", "content": ""}, {"title": "A.1 Equip LLM with a code interpreter", "content": "Fig 5 shows how we prompt the LLM to use code interpreter."}, {"title": "A.2 Query Generation Prompt", "content": "Fig 6 is the prompt to generate a related and diverse query from given file content."}, {"title": "A.3 Checker Prompt", "content": "Fig 7 and Fig 8 is the prompt for content checker and keywords checker."}, {"title": "A.4 Evaluation Prompt", "content": "Fig 10 is a keywords list example for Chart Analysis tasks"}, {"title": "C Example Unit Test", "content": "We show three types of Unit Tests in this appendix."}, {"title": "C.1 Directly Verify the Answer", "content": "For the task with a fixed answer, we check whether the final response contains the answer"}, {"title": "C.2 Verify the output file", "content": "For task clarify the output file, we check whether the output file fulfills the requirements."}, {"title": "D Code as Action VS Function Calling", "content": "We conducted an ablation study on the format of calling code interpreter. For function calling, we defined an execute_python function where the LLM passes a code string as the parameter and uses a special token <|tool_call|> before the function. We repurposed all code snippets in PyInstruct, CodeFeedback, and CodeActInstruct to follow the function calling format. We trained the model without continue pre-train on code-rich corpus for it exactly aligns with our format. Compare it to PyL-lama3(w/o cpt), Table 5 shows that the function calling format struggles with real-world code tasks. The model often fails to follow the format and fre-"}, {"title": "E Compare LLM Evaluator and Unit Test", "content": ""}, {"title": "F Python Packages in our environment", "content": ""}]}