{"title": "ContRail: A Framework for Realistic Railway Image Synthesis using ControlNet", "authors": ["Andrei-Robert Alexandrescu", "R\u0103zvan-Gabriel Petec", "Alexandru Manole", "Laura-Silvia Dio\u015fan"], "abstract": "Deep Learning became an ubiquitous paradigm due to its extraordinary effectiveness and applicability in numerous domains. However, the approach suffers from the high demand of data required to achieve the potential of this type of model. An ever-increasing sub-field of Artificial Intelligence, Image Synthesis, aims to address this limitation through the design of intelligent models capable of creating original and realistic images, endeavour which could drastically reduce the need for real data. The Stable Diffusion generation paradigm recently propelled state-of-the-art approaches to exceed all previous benchmarks. In this work, we propose the ContRail framework based on the novel Stable Diffusion model ControlNet, which we empower through a multi-modal conditioning method. We experiment with the task of synthetic railway image generation, where we improve the performance in rail-specific tasks, such as rail semantic segmentation by enriching the dataset with realistic synthetic images.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have seen a significant increase in the size of artificial neural network models, with architectures reaching billions of parameters [3]. Training such models is not trivial, given the immense time and data required to train them properly. Therefore, data collection and annotation represent crucial components of the supervised learning process, especially for large models.\nIn some particular cases, data collection and annotation can be expensive [31]. For instance, in the field of vision understanding for autonomous driving, image segmentation techniques are used to comprehend the surrounding environment. These models require highly detailed ground-truth maps of the environment, assigning each pixel to a specific class from a set of predefined classes. This process is laborious and costly when using third-party services.\nInstead of gathering and labeling real images, researchers employ image generation techniques to create synthetic images as viable alternatives. This approach expands the possibilities for model training, as it theoretically provides an infinite amount of data. However, while this is advantageous quantitatively, the quality and realism of the generated images are crucial. The distribution from which these synthetic images are sampled must closely resemble real-world data to ensure an effective training process [7].\nIn this article, we study the task of Railway Image Generation using ControlNets [34] to generate realistic images taken from the ego-view of the train. We evaluate them using the Fr\u00e9chet Inception Distance (FID) [12], which is a metric that aims to quantify the realism of images. The images are then used to improve the scores of a model that performs Semantic Segmentation on the rails [2]. We show an increase in performance when enhancing the training corpus with synthetic images. We also experiment with various bimodal image representations and discuss their results.\nEven though there are various image synthesis models (e.g., ControlNet [34], T2I Adapter [20], SCEdit [14]), we select ControlNet because of its more precise control. The condition guides the generation process step by step, being integrated directly into the diffusion model. Furthermore, ControlNet can produce higher-quality images with more intricate details, of course at a high cost in terms of the model size and the processing time. While this trade-off cannot be ignored, we aim to show how synthetic images can help train high-performing segmentation models.\nWorking with single image representations has limitations when compared to a more robust representation that concatenates multiple modalities, such as segmentation masks and Canny edges. Therefore, we combine the previously mentioned representations into one in more than one ways and study the model's behavior to input variance.\nThis research aims to provide meaningful answers to the following research questions:\n\u2022 Can we improve the performance of scene understanding intelligent models in the data-scarce rail domain using synthetic images?\n\u2022 What is the ideal conditional representation for railway image generation, measured through visual inspection and FID minimization?\nTo answer to these research questions, we propose ContRail, a ControlNet-based approach, tailored for the railway scene understanding domain, which exploits a novel representation of input conditions combining a semantic segmentation mask and an edge image. As we do not have captions for our input images, we experiment with three types of prompts (empty, fixed and generated using BLIP2 [18]). We perform an ablation study to measure the impact of each proposed component and showcase the impressive quality of the synthesized images by training a semantic segmentation model using the generated data and testing it on real samples.\nThe remainder of the paper is structured as follows: Section II introduces related concepts, Section III showcases similar applications, Section IV describes the methodology used in our work to generate realistic images, Section V showcases visual"}, {"title": "II. CONCEPTS", "content": "Semantic Segmentation is a computer vision task which consists of assigning class labels to each pixel from a given image. The output of this task is much more granular when compared to other Image Recognition problems including image classification and object detection, as the label is attached to each individual pixel. Semantic Segmentation can be either binary, where the pixels are classified as part of the class of interest or as background, or multi-clas, where each pixel has multiple values attached to it, each representing the probability of the pixel being part of a certain class.\nOne popular metric for this task is Intersection over Union (IoU) [13], described as:\n$mIoU = \\frac{Y \\cap Y_p}{Y \\cup Y_p}$ (1)\nwhere $Y$ and $Y_p$ are the ground-truth and predicted masks with the segmentation results. The intersection of similar pixels is divided with their union, on average for all classes.\nDiffusion Models [28] are unsupervised machine learning models used to estimate an assumed probability distribution of some observed data. This kind of models are often called Generative Models. The idea of these models is to slowly destroy structure in the observed data distribution through a process called diffusion, then to learn a reverse diffusion process that restores the structure in that data.\nThe diffusion process starts from an observed data point $x_0$, then some Gaussian noise is combined with the last input using a noise schedule parameter $B_{1..T} \\in (0, 1)^T$, where T is the maximum number of diffusions added to some input, resulting in a t-times (usually called the timestep of the diffusion) diffused data point $x_t = \\sqrt{1 - B_t}. x_{t-1} + \\sqrt{B_t}. E_t$, where $E_t ~ N(0, 1)$.\nThe reverse diffusion process usually consists of predicting the noise $e$ added at timestep t to some input data point $x_t$. It is sufficient to find the noise $e$ instead of the data point at the previous timestep $x_{t-1}$. The noise is used to compute the original data point as $x_{t-1} = \\frac{x_t - \\sqrt{B_t}.E_t}{\\sqrt{1 - B_t}}$.\nThe objective of the machine learning model $e_{\\theta}$ that computes the reverse diffusion process is to predict correctly the noise added from timestep t-1 to timestep t given the timestep t and the noisy image $x_t$. The loss function is:\n$LDM = E_{x_0,e \\in N(0,1)}, [||e - e_{\\theta}(x_t, t)||^2]$.\nThe generation pipeline starts from noise $e_i \\in N(0, 1)$, diffused 5 times: $x_{new} = e_{\\theta}(...(e_{\\theta}(e_i, t), t - 1), ..., 2), 1)$.\nUsing the mean square error to compare each pixel leads to small differences in the pictures, such as a small shift of an object, a wrong color of an object, and other. Since this leads to large loss values, other metrics must be used to measure the differences between the pictures. One such metric is the perceptual loss [15], which uses a pre-trained convolutional neural network that is already proficient in extracting high-level features from the target picture and the predicted one to compute the mean square error between the features. This approach is computationally difficult, leading to a higher problem complexity.\nTo avoid the computation of perceptual loss, Latent Diffusion Models (LDMs) were introduced by Rombach et al. [24]. The main idea is to diffuse and reverse the diffusion process in the feature space of the inputs (instead of training it directly on the image space) and have a mechanism that can construct the original input from the features. The input space is often called the latent space, which is a lower-dimensionality space, leading to faster training of the network responsible for the reverse diffusion process.\nStable Diffusion [26] is a model that proves the capabilities of LDMs. It was trained on the LAION-5B dataset [27], which is a large-scale public image-text dataset of over 5.8 billion pairs of image and text annotations, making it one of the most robust models at the moment.\nAlthough Stable Diffusion [26] is a very robust and complex model, it is not good enough for specific tasks, such as generating images in a specific environment, where the probability distribution of the data is most likely different from the one the model was trained on. In case we want to make the model understand some of the new probability distribution, but also to keep some of the knowledge about the old distribution (such as how to handle diverse text embeddings, which may not be present in the new training data), fine-tuning the trained LDM directly tends to make it generate more replications, especially when it's being trained with a smaller numbers of samples [22]. Moreover, there is evidence that directly fine-tuning an LDM can lead to overfitting, mode collapse, and catastrophic forgetting [34].\nControlNet [34] was proposed to solve this problem. It is composed of a HyperNetwork [11], which is a smaller network trained to influence the weights of a larger network, which tries to solve the problem mentioned before, over some LDM. The purpose of this network is to add some conditional control (segmentation masks, Canny edges, human poses, etc.) to a locked production-ready LDM, without modifying its weights. This kind of control may be added directly using the cross-attention mechanism of the U-Net in LDMs, however, as we mentioned previously, it is problematic to do so, especially in case of smaller datasets.\nHaving a pre-trained neural block from the original LDM, a ControlNet block could be added by locking (freezing) the parameters of the original block and simultaneously cloning the block to a trainable copy. The trainable copy will receive an external conditioning vector c as input. When such structure is applied to large models like Stable Diffusion, the locked parameters preserve the original model trained on billions"}, {"title": "III. RELATED WORK", "content": "In the context of autonomous driving for trains, semantic segmentation is a crucial task for understanding visual elements from the ego-view of the train. A large amount of labeled data is required to train robust for semantic segmentation. However, the available data in the rails domain is limited. One possible solution to this challenge is data augmentation, which also reduces the chance of overfitting. Simple operations such as random rotation, crop, or noise injection are commonly used, however, these methods are not sufficient when dealing with complex data [16]. Therefore, generative models can be used to generate images and enrich the training dataset. We discuss in the following subsections the impact of various generative models, including Generative Adversarial Networks (III-A) and Diffusion Models (III-B), on augmenting different datasets.\nIn the field of medical imaging, particularly in the context of liver lesion classification, a small number of labeled data creates a significant challenge. To address this, Frid et al. [9] used Generative Adversarial Networks (GANs) [10] to augment the dataset with synthetic images of liver lesions. GANs can implicitly learn the distribution of real images, and they are used to generate new samples that enhance the training set. They compared the effect of training a Convolutional Neural Network on the raw dataset, on the dataset augmented with classic transformations, and on the dataset augmented with both the GAN and the classic transformations. They measured the total accuracy of the Convolutional Neural Network trained on those datasets and obtained 57% when training on the raw dataset, 78.6% when training on the dataset augmented with classic techniques, and 85.7% when training on the dataset augmented with both classic techniques and GAN inferences. The quality of the GAN generations was measured by asking two expert radiologists to classify real images, respectively synthetic generated images. They both obtained similar results when classifying each type of image: expert 1 managed to classify correctly 78% of the real images and 77.5% of the synthetic images, while expert 2 managed to classify correctly 69.2% of the real images and 69.2% of the synthetic images. This indicates the validity of the lesion generation process.\nStill in the medical field, particularly for brain MRI segmentation, Fernandez et al. [8] proposed the brainSPADE pipeline to enrich the datasets, using Latent Diffusion Models (LDM) [28] to generate new segmentation masks, and the SPADE architecture [21] to generate high quality images from semantic maps. To generate different types of segmentation masks, two different LDMs were trained on healthy and tumour-affected semantic label slices. To verify the ability to learn to segment healthy regions using synthetic data, they trained 2 models: $R_{iod}$, trained with real data (7008 real samples), and $S_{iod}$, trained with synthetic data (20000 generated samples).\nFor the first experiment, they validated the models on the test dataset from the same distribution the $R_{iod}$ model was trained on, and computed different Dice scores [6]. In this test, the $R_{iod}$ model with a CSF Dice score of 0.958 \u00b1 0.008 outperformed the $S_{iod}$ model with a CSF Dice score of 0.919 \u00b10.023, but the result of $S_{iod}$ was comparable to the other performances achieved in the literature.\nFor the second experiment, they collected out of distribution (OoD) images, which are samples taken from various datasets with different styles. These out of distribution images were then split in near out of distribution (n \u2013 OoD) samples (which have similar features between them) and far out of distribution (f \u2013 OoD) samples (with more varying features). Then, they trained the models $R_{n - OoD}$, respectively $S_{n - Ood}$ with real images from the n-OoD samples, respectively images generated to match the distribution of the n-OoD samples, and the models $R_{f - OOD}$, respectively $S_{f - OOD}$, with meanings similar to the ones before, but using the f \u2013 OoD samples. For the n"}, {"title": "IV. PROPOSED APPROACH", "content": "Our work aims to generate realistic railway images to compensate for the lack of annotated data from this domain. We propose an authentic railway scene image synthesis pipeline based on the ControlNet architecture [34]. We combine the segmentation masks with the Canny edge [4] version of the real images see Section IV-A \u2014, and experiment with various types of prompts see Section IV-B to identify the configuration that leads to the most realistic results. Our approach is therefore twofold, considering on one side the image conditionality, and on another side prompt conditionality. An illustration depicting the overview of our approach is presented in Fig. 1.\nStarting from the RailSem19 dataset, the generated images will be used then to increase the size of the dataset and improve Semantic Segmentation performance.\nControlNet allows merging multiple conditional inputs during inference time, after each individual model was already trained by considering one condition only. Through a combined representation of more conditions, we enable the network to jointly exploit the complementary information of multiple conditions during the fine-tuning phase.\nWe fine-tune the Stable Diffusion model used by ControlNet in order to embed additional railway information in the model. By default, the Stable Diffusion model takes as input RGB images, however our default conditional segmentation mask from RailSem19 is represented as a grayscale image. Thus we extend the original grayscale mask by duplicating it for each of the three input channels. The same applies when using the Canny edge image since this type of input condition also occupies a single channel.\nAfter some initial experiments in which we attempted to change the native number of input channels in order to use only one channel (two channels for the combined representation), we decided to continue with the default number of three channels as this approach yielded much better results. Therefore, to obtain a combined representation of two conditions we assign the original segmentation mask to two of the channels while assigning the Canny edge mask to another channel. Depending on which of the channels received the Canny edge mask, we obtain three possible inputs: Cmb12 (Edge, Mask, Mask), Cmb111 (Mask, Edge, Mask) and Cmb21 (Mask, Edge, Edge), as seen in Fig. 2.\nThis novel approach offers two significant advantages:\n1. Single Model Training: Regardless of the number of conditions to be considered, only a single model needs to be trained. This simplifies the training process and reduces computational resources.\n2. Guided Diffusion Process: The combined condition representation guides the fine-tuning process step by step, simultaneously taking into account all conditions. This ensures a more coherent and integrated output, leveraging the full spectrum of available information.\nIn contrast, the original version of ControlNet requires multiple models to be trained, each corresponding to a different condition. This renders the proposed method more efficient and effective.\nThe Stable Diffusion model behind ControlNet relies on textual information which describes the desired generated output. In order to obtain a prompt for all images from a dataset, a zero-shot image-to-text generator can be used (e.g. CLIP [23], ZeroCap [30], BLIP-2 [18]). In our approach, we actually use the BLIP-2 model which seems to accurately describe the images from our setting. BLIP-2 achieves state-of-the-Art performance on various vision-language tasks [18], being efficient and versatile.\nMoreover, as recommended by the ControlNet authors, we add a constant decorator to all prompts to enhance image quality: \"high quality, extremely detailed, 4K\u201d.\nWe also experiment with multiple types of prompts. In our first experiments we use no textual prompt (No pr.). Then we use the same prompt for all images (Fixed pr.). Lastly, the most promising technique from our experimental setup was reached through the use of generated prompts, obtained via BLIP-2 (BLIP-2 pr).\nOn top of the three configurations, we also consider negative prompts (Neg. pr.) [19] which can be used to guide the image generation model away from common mistakes. This type of prompt can contain text like \"low quality-image\", \"bad anatomy\", \"unrealistic rails\" [1]."}, {"title": "V. NUMERICAL EXPERIMENTS", "content": "In this section, we report the results obtained, provide details about the dataset, and present the selected evaluation means.\nThe images used for training and validating the model are sourced from RailSem19 [33], the most comprehensive Semantic Segmentation dataset for railway scenes. It contains 8500 high-quality images obtained from the ego-view of moving trains. The dataset is robust as it showcases scenes with different weather and lighting conditions, and different types of rails, for train and city trams.\nSimilar to [2], we use a cropped version of the dataset that resizes the images from 1920x1080 to 1080x1080. This aims to reshape the images to a size that fits the aspect ratio of the input layer of the network, without skewing the objects. While the left and right edges of the images are cut to obtain the new size, they do not contain essential information. The rails of interest are located at the center of the image.\nTo measure the quality of the generated images, we employ both qualitative and quantitative evaluations. The qualitative evaluation involves visually inspecting a limited number of generated samples. There are key aspects we consider quintessential when attempting to replicate the real distribution of railway scene images, the most important being rail quality. In our early experiments, we observed a tendency for the model to produce scenes with two main anomalies: doubled and splintered rails. Through weight improvements and more epochs, the quality of the generated rails and surrounding scenes reached a more than satisfactory state, reducing the occurrence of such cases.\nTo evaluate numerous sets of synthetic images we use Fr\u00e9chet Inception Distance (FID) [12] as our quantitative metric. FID measures the Fr\u00e9chet distance between the distribution of real and generated images. These distributions are described by the mean and variance of the images, represented as 2048 features obtained by passing them through the InceptionNetV3 network [29]. Being a distance metric, a lower FID value suggests an increased image quality.\nNote however that, for safety-critical applications, FID is not a reliable score. InceptionNetV3 is a biased estimator which may provide non-deterministic embeddings for different trainings. This means that Inception features are not normally distributed and evaluation on real-world data may not reflect the true potential of the trained model. Despite this, we consider it to quantitatively evaluate our results in this article.\nBesides evaluating the generated images in a vacuum either qualitatively or quantitatively, another important test is to measure the effect of the synthetic images for a model tasked with identifying aspects from real images. In our test, we train a segmentation model using both real and generated samples, while evaluating it only on real images. In these experiments we use Intersection-over-Union (IoU) also known as the Jaccard metric [13], due to its ubiquitous character for the task of semantic segmentation.\nFor the image generation experiments, we consider two dimensions with variable parameters: condition type and prompt type. For types of conditioning images, we choose to work with single-conditioned input images, and multi-conditioned input images. Single-conditioned are the grayscale versions of the original segmentation masks from RailSem19, and the Canny edges representation extracted from the real images by the classic Canny algorithm [4]. Multi-conditioned images are obtained by combining the grayscale segmentation mask with the Canny edges on specific channels: 12, 21 and 111, as explained in Subsection IV-A. For types of prompts, we consider: no prompt, a fixed prompt, a prompt generated with BLIP-2, and a negative prompt. In case of all prompts, we concatenate a string to boost quality of results: \"high quality, extremely detailed, 4K, HQ\u201d [34]. It is a common prompt engineering technique to add such keywords in the query prompt.\nWe consider all 8500 images from the RailSem dataset and split them into 6800 training and 1700 validation images, which constitutes an 80:20 split. The images are resized to 512x512. We train ContRail for 13 epochs with a batch size of 4, which results in exactly 22100 training steps. The recommended number of steps to reach the sudden convergence phenomenon is at least 10000 [34]. We run our experiments on NVIDIA GeForce RTX 4090 GPU with 24GB of VRAM on 56GB of RAM. One experiment takes around 7 hours.\nWe report the FID results between the validation set and the generated images in Table I. Each cell in the table corresponds to one configuration executed once. The scores are overall varied. The best score is registered by combining the segmentation mask and the Canny representation in Cmb111 without prompts. When using only the segmentation mask condition, the best scores are obtained with BLIP2 prompts. Negative prompts influence the results two-fold: FID scores worsen in combined conditions, while they remain constant without combining conditions.\nFocusing on the single approach, the scores on the segmentation masks are generally better than those on the Canny representation. The masks provide richer information than simple contours approximated by Canny edges. Many objects resemble similar shapes to rails when represented by edges, such as poles and trees.\nWe display a selected subset of images for specific configurations: Cmb111 without prompts in Figure 3 (which led to the best quantitative results), original masks with BLIP2 prompts in Figure 4, and original masks with BLIP2 prompts and negative prompts in Figure 5.\nWhile from the FID perspective the lowest score is obtained on Cmb111 without prompts, visually, the most realistic high-quality images are obtained when using the original masks. This is caused by some low quality, cluttered or complex light scenarios from RailSem19. Some of them are blurred, darker or in a lower resolution. Thus, we consider that a lower-quality image style yields a better score than a high-quality image with a realistic representation that is further away from the mean of the validation data distribution.\nWe observe the difference in image complexity when comparing the Cmb111 results with the original masks results. In Figure 3, the synthetic images are less detailed with non"}, {"title": "D. Semantic Segmentation results", "content": "To gain a better understanding of the use of synthetic images, we consider a set of experiments where synthetic images are used for training, either to replace real images, or to support them. We formulate the semantic segmentation problem as a binary problem, highlighting only the rail class [2]. The images are cropped to 1080x1080, resulting in a rail:background ratio of 1:37 [2].\nThe segmentation model we train is the U-Net [25] neural network architecture. This architecture is composed of two parts: a feature extractor encoder and a decoder that maps the features back to a segmentation map. The network is suitable for high-resolution semantic segmentation tasks, featuring skip-connections between the encoder and the decoder to ensure information and location flow. The competitive results obtained in [2] inspired the network choice.\nWe train the network for 40 epochs using the Adam optimizer with a learning rate of le-4 and batch size 4 on images of size 512x512. The synthetic images are generated with a single condition (the segmentation masks) and a prompt generated by BLIP-2; this configuration led to the best results visually.\nMultiple segmentation models are built. In each setup, the same 500 original real images are considered for validation. Training images are taken in the order they are indexed in the original dataset [33]. We further describe the configuration we chose for each segmentation model. Every configuration trains a model from scratch. Synthetic images are all generated using the previously mentioned configuration.\nWe train the segmentation model on the first 3000 real images from RailSem19 (A). Then, we replace the 3000 real images with their synthetic generated version and train a new network (B).\nWe then join the real images from (A) with the synthetic images from (B), obtaining a training corpus of 6000 images (C). Note that there are only 3000 unique masks since each is associated with a real and a synthetic image.\nThe next setup considers a subset of the 3000 images from the previous setups: 1500 real images and 1500 synthetic images, thus gathering a mixed set of 3000 images (D). Similar to (C), the real and synthetic images are associated to the same ground-truth masks.\nIn the final setups E and F, we again consider 1500 real and 1500 synthetic images, however, this time corresponding to different ground-truth masks: the first 1500 are real, the following 1500 are synthetic (E), and the first 1500 are synthetic, the next 1500 are real (F).\nEach experiment is repeated three times. The mean IoU and standard deviation results on a scale from 0 to 100 are displayed in Table II, along with the real and synthetic dataset counts. We restate that the same 500 validation images are considered for each experiment.\nNotice how similar results for setups A and B are due to the same number of training images. The standard deviation is slightly higher in setup B when synthetic images are used. Besides that, replacing real images with synthetic ones does not lead to major performance discrepancies.\nIn setup C we consider twice as many images, resulting in a higher mIoU performance. We may affirm that the usage of a larger number of synthetic images does help the overall performance. This is especially useful when the training dataset"}, {"title": "E. Discussion", "content": "To enhance a generative model's ability to create realistic images, it is essential to minimize the difference between real and synthetic data distributions. Training on synthetic images that do not resemble real scenarios can decrease performance and reduce the predicting model's ability to recognize them.\nThus, realistic images are required, however what humans perceive to be realistic may not be perceived similarly by the models. Slight differences such as railway width, traffic light aspect ratio or light effects may negatively influence the learnt distribution and reduce the model's ability to recognize new scenarios.\nTherefore, the image generation models considered should be trained on realistic and representative images. This leads to an infinite loop, since we reach the initial point: the lack of data to train large models.\nThis is one common dilemma regarding the issue: if enough data was available to train the image generation models to perform perfect at their task, then why is there a need for new realistic images to be generated? There are many possible answers, however the most resembling one is in case of specific rare scenarios that are hard to capture in reality. Generative models are able to patch together such scenarios and generate realistic images with rare scenarios, which in turn increase the robustness of task-specific models trained on the synthetic data."}, {"title": "VI. CONCLUSIONS AND FUTURE CONSIDERATIONS", "content": "In this work, we studied the task of Railway Image Generation by employing additional control to the generated images using ControlNets. Our quantitative and qualitative evaluations show small FID scores that indicate a high realism in our images. We also showed how synthetic images enhance the performance of a task specific model, i.e. Rail Semantic Segmentation, using synthetic generated images.\nOne significant advantage of this approach compared to the results presented in Section III is the ability to generate images in any context, by using the knowledge acquired by the Stable Diffusion model [26] during its training. Most data augmentations using generative models trained from scratch face the issue of producing data from the same distribution as the training set, which may not necessarily add substantial new information. While this type of generation is useful in the medical field, where it is less likely to face drastic changes from the existent data in practice, it can be challenging in the rails domain, because the main issue is caused by the lack of datasets covering a wide range of scenarios, especially rare ones, due to the difficulty and cost involved in capturing those. However, with the help of ControlNet [34], this problem is mitigated, because this model can generate data in various other scenarios than the ones present in its training set, and the model trained on the new data achieves improved performance, as seen in Table II.\nSome aspects that can be improved in the future are: obtain more granular segmentation masks to train on more accurate mappings; test our approach on object detection and assess its performance using synthetic data; experiment with more input types (Holistically-nested Edges [32], Multi-Line Segment Detection)."}]}