{"title": "Predicting Coronary Heart Disease Using a Suite\nof Machine Learning Models", "authors": ["Jamal Al-Karaki", "Philip Ilono", "Sanchit Baweja", "Jalal Naghiyev", "Raja Singh Yadav", "Muhammad Al-Zafar Khan"], "abstract": "Coronary Heart Disease affects millions of people worldwide and is a well-studied\narea of healthcare. There are many viable and accurate methods for the diagnosis\nand prediction of heart disease, but they have limiting points such as invasive-\nness, late detection, or cost. Supervised learning via machine learning algorithms\npresents a low-cost (computationally speaking), non-invasive solution that can\nbe a precursor for early diagnosis. In this study, we applied several well-known\nmethods and benchmarked their performance against each other. It was found\nthat Random Forest with oversampling of the predictor variable produced the\nhighest accuracy of 84%.", "sections": [{"title": "1 Introduction", "content": "Predicting coronary heart disease (CHD) remains a significant challenge in medical\ndiagnostics, with ongoing research aimed at improving predictive accuracy. This liter-\nature review examines machine learning techniques for CHD prediction, focusing on\nmethodologies, results, and limitations of existing research. It also explores how these\ntechniques utilize both labeled and unlabeled data to enhance model performance and\naddress issues related to data availability.\nCoronary Heart Disease (CHD) is caused by the blockage of blood flow to the heart\nthrough the constriction of coronary arteries. Such blockages are typically a conse-\nquence of the accumulation of fatty deposits within the arteries. CHD was reported\nto have roughly 110 million cases worldwide in 2015, and cardiovascular diseases, in\ngeneral, accounted for roughly 17.9 million fatalities in the same year [1]. Early diag-\nnosis is important as failure to detect early on could lead to heart attacks or cardiac\narrest. Identification also enhances the effectiveness of varied treatment options such\nas medication, lifestyle changes through exercise and diet or even surgical operations\nImaging techniques such as Coronary Computed Tomography Angiography\n(CCTA) render 3D images to visualize the internal structure of coronary arteries. Such\nobservations aid in identifying coronary artery disease and severe stenosis, and this\nprocess is also non-invasive [2]. Fractional Flow Reserve (FFR) is minimally invasive\nand measures the fluctuations in pressure across the coronary artery to establish the\nhemodynamic significance. Quantitative Coronary Angiography (QCA) is an alterna-\ntive to FFR, and both tend to be compared to assess lesions. QCA is invasive and\nmeasures the diameter of the stenosis to benchmark against reference measurements\n[2].\nOther methods include stress testing, cardiac biomarkers measurements through\nblood tests, and clinical risk assessment through diamond-forrester models are other\nviable tests for coronary heart disease diagnosis. The main limitations of such\nmethods are that some are invasive, and the generalization and accuracy of the non-\ninvasive methods are improvable as CCTA is still limited for predicting physiologically\nsignificant coronary artery disease.\nWhile we acknowledge that none of the methods applied are new, there does not\nexist a comprehensive study that benchmarks so many different models against each\nother, and thus, we believe that this research, and our original contribution in this\nregard, serves as a primer into the field. Further, all the results obtained are highly\nreproducible and require simple, non-abstract code. Lastly, we would like to point out\nthat the methods applied here can easily be applied to any other dataset, and the\nbenchmarking methods in a neat tabular format can be adopted.\nThis paper is organized as follows:\nIn Sec. 2, we peruse some of the contemporary studies that applied Machine\nLearning (ML) and Deep Learning (DL) to data.\nIn Sec. 3, we outline the mathematical details underpinning the models we have\napplied and describe their calculational mechanics.\nIn Sec. 4, we describe the process undertaken to clean the data and thereafter\npresent the results.\nIn Sec. 5, we reflect upon the results obtained and provide details of future studies\nthat can offshoot from this one."}, {"title": "2 Literature Review", "content": "The study in [3] focuses on developing a predictive model for coronary heart disease\nrisk using machine learning techniques. The authors employed the CRISP-DM (Cross-\nIndustry Standard Process for Data Mining) framework for their methodology. Among\nvarious models tested, the Decision Tree algorithm proved to be the most effective,\nachieving notable performance metrics: an accuracy of 0.884, an AUC value of 0.942,\nand an F1 score of 0.881. Cross-validation was utilized as the sampling method to\nensure a robust evaluation of the model's performance.\nAuthors in [4] evaluated the effectiveness of Random Forest, Decision Trees, and\nK-Nearest Neighbors algorithms in predicting Coronary Heart Disease using the\n\"Framingham Heart Study\" dataset. By preprocessing the data through techniques\nsuch as standardization and normalization and employing K-fold cross-validation, the\nresearch demonstrates that Random Forest excels in prediction accuracy compared to\nthe other algorithms.\nThe study in [5] explores the effectiveness of machine learning techniques, specif-\nically Naive Bayes, SVMs, and Decision Trees, in predicting coronary heart disease.\nUsing a dataset of 462 instances from the South African Heart Disease dataset and\nemploying 10-fold cross-validation, the research demonstrates the potential of these\nmethods for disease prediction. However, the study is limited by its focus on only three\nmachine learning techniques and the relatively small size of the dataset, which may\naffect the generalizability of the findings.\nIn [6], the authors compare Gaussian Naive Bayes, Bernoulli Naive Bayes, and\nRandom Forest algorithms in predicting coronary heart disease. Using the Cleveland\ndataset from the UCI repository, the study evaluates these models based on accu-\nracy, precision, F1 score, and recall. The results demonstrate that both Gaussian\nand Bernoulli Naive Bayes algorithms outperform Random Forest, suggesting their\nsuperior effectiveness in coronary heart disease prediction.\nAuthors of study [7] introduced a hybrid approach integrating Decision Tree\nand AdaBoost algorithms for predicting and classifying coronary heart disease. The\nmethodology leverages the strengths of both algorithms to enhance prediction accu-\nracy and classification performance. Evaluated through metrics like accuracy, True\nPositive Rate, and Specificity, this hybrid method demonstrates improved efficacy in\nCHD prediction compared to traditional approaches.\nThe study in [8] focuses on diagnosing coronary heart disease using a deep neural\nnetwork (DNN) with a multilayer perceptron architecture. It employs regularization\nand dropout techniques to enhance model performance, specifically designed for both\nthe classification of training data and the prediction of new patient cases. The model\nwas trained and evaluated using a dataset of 303 clinical instances from the Cleveland\nClinic Foundation.\nThe paper [9] investigates the effectiveness of semi-supervised learning algorithms\nin identifying key predictors of coronary heart disease. Utilizing Cleveland and Hungar-\nian heart disease datasets, the study applies Collective Wrapper, Filtered Collective,\nand Yet Another Semi-Supervised Idea algorithms to subsets of varying sizes (33%,\n65%, and 100%). Performance is assessed through accuracy, F1 score, and ROC curve\nmetrics. By sequentially removing attributes, the research distinguishes between sig-\nnificant and irrelevant factors, providing insights into the factors influencing heart\ndisease prediction.\nAuthors [10] presented a novel adversarial domain-adaptive multichannel graph\nconvolutional network (DAMGCN) for predicting coronary heart disease. This\napproach integrates a two-channel graph convolutional network with local and global\nconsistency for feature aggregation alongside an attention mechanism to unify node\nrepresentations across different graphs. A domain adversarial module is employed to\nminimize discrepancies between source and target domain classifiers and optimize three\nloss functions for effective cross-domain knowledge transfer. The proposed DAMGCN\nmodel aims to overcome the limitation of existing GNN models in transferring\nknowledge across diverse datasets, a significant challenge in CHD research.\nThe study in [11] presents a hybrid model integrating Gaussian Fuzzy C-Means\nClustering (GKFCM) with a Recurrent Neural Network for predicting coronary heart\ndisease. The methodology involves normalizing the dataset, applying GKFCM for\nclustering, and then leveraging the combined GKFCM-RNN approach to achieve a\nhigh prediction accuracy of 99%. This innovative approach demonstrates significant\npromise in enhancing predictive accuracy for coronary heart disease through advanced\nML techniques."}, {"title": "3 Theory", "content": "In this section, we present the theory of all the ML models used in this study.\nSuppose that our dataset D = {(xi, Yi)}; \u2208 Rm\u00d7n composed of m examples and n\nfeatures x = (x1,x2,...,xn), with labels y. For each of the models below, under the\nsupervised learning paradigm, we seek to find a function f: x \u2192 y that maps the\nfeatures to the label."}, {"title": "3.1 Logistic Regression", "content": "With this model, we aim to predict the probability of binary outcomes given by\np(y = 1/x) = \\frac{1}{1 + exp [- (wo + W1x1 + . . . + Wnxn)]} = \u03c3(w^x + b), (1)\nwhere w = (W1,W2,..., wn) are the weights, for 0 \u2264 i \u2264 1, b = wo is the bias term,\nand \u03c3(X) = 1/ (1 + e-x) is the sigmoid activation function for arbitrary X."}, {"title": "3.2 Linear Discriminant Analysis", "content": "In Linear Discriminant Analysis (LDA), each class of the predictor variables is assumed\nto be drawn from a Gaussian distribution, and Bayes theorem is used to classify them\nbased on the posterior probability. The discriminant function for class c\u2208 Cis\n\u03b4\u03b5(x) = x\u00b2\u00af\u00b9\u03bc\u03b5 \u2013 \\frac{1}{2}\u03bc\u03b5 \u00b5-1 \u03bc\u03b5\u03a3 \u03bc\u03b5 + log \u03c0\u03bf, (2)"}, {"title": "3.3 Support Vector Machines", "content": "The Support Vector Machine (SVM) model seeks to find the optimal separating\nhyperplane between classes that maximizes the margin (distance between the planes\nwith equation wTx = -1 and w^x = 1). Mathematically, this is expressed as the\noptimization problem\n subject to:\n\\begin{aligned}\n\\min C &= \\min=\\frac{1}{2}w^Tw \\\\\nw,b \\\\\nYi (w^Txi + b) &\\geq 1 \\quad \\forall i.\n\\end{aligned}\n(4)"}, {"title": "3.4 Decision Trees", "content": "Decision Trees seek to recursively split the data at each node based on a feature\nand threshold to maximize information gain or minimize impurity. The impurity\ncan be either the Gini value or the entropy. Mathematically, the Gini and entropy,\nrespectively, are\n\\begin{aligned}\nG &= 1 - \\Sigma p_c^2, \\\\\nS &= - \\Sigma p_c \\log p_c,\n\\end{aligned}\nwhere pe is the probability of samples belonging to class c at a particular node.\n(5)"}, {"title": "3.5 Random Forest", "content": "Random forest is an ensemble learning method that operates by constructing a large\nnumber of decision trees during training and then outputting the mode of the classes\nof the individual trees.\nThe Random Forest algorithm combines the output of multiple decision trees to\nprovide more accurate and stable predictions. Algorithmically, it works as follows:\n1. Bagging/Bootstrap Aggregating: From the original dataset D, a large number,\nsay B, of subsets Do are created by randomly sampling with replacement. Each of\nthese subsets will serve as the training dataset for a specific decision tree T.\n2. Construction of Trees: For each subset, a decision tree is built. However, unlike\ntraditional decision trees, each node in the tree is split based on a random subset\nof the features. This is another key to the success of the Random Forest algorithm,\nas it helps to reduce the correlation between the trees.\n3. Classification: Each decision tree makes a prediction hb (Xtest) for instance Xtest,\nand the random forest outputs the class that receives the majority vote\n\u0177 = mode [h1 (xtest), h2(Xtest),..., h(xtest)], (6)"}, {"title": "3.6 Naive Bayes", "content": "The Naive Bayes model is probabilistic in nature and uses Bayes Theorem from prob-\nability theory to perform classification. It assumes that the features in the dataset are\nconditionally independent given the class label.\nThe goal is to estimate the probability of a class c\u2208 C given a feature vector\nx = (x1, x2,...,xn) of observed features. The probability of class c is given by\np(cx) = \\frac{p(xc)p(c)}{p(x)} (7)\nwhere p(cx) is the posterior probability, p(xc) is the likelihood, p(c) is the prior\nprobability, and p(x) is the evidence. The predicted class is then given by\n\u00ea = arg max p(cx). (8)"}, {"title": "3.7 K-Nearest Neighbors", "content": "The K-Nearest Neighbors (K-NN) algorithm is a non-parametric, makes no assump-\ntions about the underlying data distribution, and lazy learning, does not learn a model\nduring the training phase; instead, it stores the training data and uses it directly\nduring prediction, algorithm.\nThe idea is that similar data points will likely exist near one another. When mak-\ning a prediction, K-NN looks at the K closest training examples in the feature space\nand assigns a label based on the majority, for classification tasks, of those neighbors.\nTypically, the distance between the new datapoint and all the datapoints in the train-\ning set is calculated, and K closest datapoints to the new datapoint based on the\ncomputed distance are chosen. A typical metric used is the l\u00b2 distance\nd(xi, j) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}. (9)\nIf K is too small, the model becomes sensitive to noise and leads to overfitting, and\ncoversely if k is too large, the model becomes becomes too generalized and leads to\noverfitting."}, {"title": "3.8 Extreme Gradient Boosting", "content": "Extreme Gradient Boosting (XGBoost) is an ensemble learning method which com-\nbines the predictions of multiple weak learners, typically decision trees, to produce a\nstrong learner. The model works by sequentially adding trees, where each subsequent\ntree tries to correct the residuals (errors) of the previous trees. In order to compensate\nfor overfitting, the model includes L\u2081 (lasso) and L2 (ridge) regression terms.\nMathematically, the model works as follows:\n 1. The initial prediction is calculated using\n\\begin{aligned}\n\\hat{y_i}^{(0)} = \\frac{1}{m} \\sum_{i=1}^{m} y_i\n\\end{aligned}\n(10)\n 2. At each iteration, a set of trees t\u2208T is trained according to\n\\begin{aligned}\n\\hat{y_i}^{(t+1)} = \\hat{y_i}^{(t)} + f_t(x_i),\n\\end{aligned}\n(11)\nwhere ft(xi) is the prediction of the tth tree.\n 3. The objection function\n\\begin{aligned}\n\\mathcal{L}^{(t)} = \\sum_{i=1}^{m} l(\\hat{y_i}^{(t)}, y_i) + \\sum_{k=1} \\Omega(f_k),\n\\end{aligned}\n(12)\nwhere J is the loss function between the true and target labels - usually chosen to\nbe log-loss in classification tasks such as the problem we are solving \u2013 and is the\npenalization term that regulates the model complexity, given by\n\\Omega(f_k) = \u03b3\u03a4 + \\frac{\u03bb}{2} \\sum_{j}||wj|| (13)\nwith 0 << 1 is the control parameter, T is the number of leaves in the tree, and\n0 < x < 1 is the L2 control parameter.\n 4. Each tree is constructed in a greedy manner by selecting splits that minimize the\nloss function. XGBoost uses the gain of each split to determine the quality of a\nsplit, which is calculated as\n\\begin{aligned}\nGain = \\frac{1}{2} [\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}] - \u03b3,\n\\end{aligned}\n(14)\nwhere GL and GR are the sums of the gradients for the left and right child nodes,\nrespectively, and HL and HR are the sums of the Hessians for the left and right\nchild nodes."}, {"title": "4 Experiments", "content": "The dataset consists of 16 fields and 4 240 examples, of which there are 645 missing\nvalues in total. In Fig. 2, we represent the distribution of missing values per feature.\nSince the missing values represented only a small proportion of the overall size of\nthe dataset, they were dropped and the resulting dataset contains 3 658 examples.\nThe data was normalized using min-max scaling in order to bring them the features\nin the non-categorical types to the range [0,1] according to: For feature xi \u2208 x, the\nnormalized value of the feature is given by\nXnorm = \\frac{X - Xmin}{Xmax - Xmin} (15)\nwhere xmin is the minimum value of the feature, and Xmax is the maximum value of\nthe feature.\nFurther, we observe that a severe class imbalance occurs in the predictor variable\nTen YearCHD: 3 101 for CHD cases, and 557 for non-CHD cases;\nThus, we resort to balance the class balancing via random undersampling and\nrandom oversampling.\nThe data was then split into training data (70%) and testing/validation data (30%),\nwhereupon the eight different model types were built. The results of the model training\ntogether with the evaluation metrics are summarized in Tabs. 1 and 2.\nFrom Tabs. 1 and 2, we observe that overall random oversampling produced better\nresults than random undersampling. Furthermore, the best performing model, in terms\nof accuracy, was the random forest model with an accuracy of 84%. In Fig. 3, we show\nthe behavior of the receiver operating curve (ROC) and the precision-recall curve. The"}, {"title": "5 Conclusion", "content": "In this research, we have applied several ML models to predict whether a patient\nwill have coronary heart disease in the next 10 years. It was found that the predictor\nvariable was severly imbalanced and thus, we employed two class balancing tech-\nniques: Undersampling and oversampling, to balance the class. In terms of accuracy,\nthe random forest model, with random oversampling, was best performing, having an\naccuracy of 0.84. However, from the ROC and precision-recall curve, we observe that\nthe model's performance was far from optimal and it leave a lot of room for improve-\nment in reducing the number of false positives. In addition, we would like to point\nout that most, if not all, the other studies in the literature also suffer from this \"FPR\ntrap\", but they do not report this as a drawback of their method, however, we have\nchosen to do so here.\nIn future considerations, we will factor in the latter consideration and try and\napply more sophisticated models to obtain higher accuracy and a reduced FPR. This\nwork serves to set the tone for our future studies, and thus is a benchmarking paper."}]}