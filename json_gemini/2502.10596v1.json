{"title": "Post-training an LLM for RAG? Train on Self-Generated Demonstrations", "authors": ["Matthew Finlayson", "Ilia Kulikov", "Daniel M. Bikel", "Barlas Oguz", "Xilun Chen", "Aasish Pappu"], "abstract": "Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering \"Who won the latest World Cup?\" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents\u2014a technique known as retrieval augmented generation (RAG)-mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.", "sections": [{"title": "1 Introduction", "content": "Typically, large language models (LLMs) must rely solely on knowledge learned during pre-training to perform downstream tasks like question answering. This situation becomes problematic when the task depends on knowledge that the model has not encountered or learned. A popular framework for mitigating this issue is to provide the model with contextual knowledge, i.e., providing relevant information for the task as input. This crucial context typically comes from retrieval systems that fetch documents from some database based on their relevance to the task. The combination of retrieval and language generation, known as retrieval-augmented generation (RAG), often outperforms LLMs without retrievals on downstream tasks, particularly for knowledge intensive tasks where parametric (i.e., learned during pre-training) knowledge fails.\nIntegrating an LLM into a RAG system can be tricky, since the LLM may not properly leverage the additional context to improve performance. In particular, if the retrievals are irrelevant to the task, they may distract the LLM from the objective or cause the LLM to integrate incorrect information into its generations. The LLM may also fail to integrate the provided context even when it contains the solution to the task. We hypothesize that these shortcomings are caused by distributional mismatch between the model's training data and the RAG-style inputs it receives at test time.\nSeveral prior efforts have sought to better integrate LLMs into RAG systems by fine-tuning them for the task. This usually involves training the model with in-context retrievals, and may be implemented during pre-training (e.g., Guu et al., 2020), or as a post-training step (Lin et al., 2024b). Since pre-training can be prohibitively expensive, we focus on the more accessible and common post-training methods. In particular, we concentrate on retrieval-augmented instruction tuning (RA-IT) where an LLM is fine-tuned on an instruction tuning dataset which has been modified to include one or more retrievals for each instance.\nOne easy way to obtain a RA-IT training set is to augment existing instruction-response pairs from an instruction tuning dataset with retrievals. This approach poses two significant challenges. First, there may be a misalignment between the retrieved content and the response, as the latter was created without reference to the retrieval and might even appear to contradict it. Second\u2014and this is a more"}, {"title": "2 Related work", "content": "Our work builds upon several areas in RAG methods and post-training.\n2.1 Training RAG-enabled LLMs\nAs retrieval-augmented generation (RAG) has garnered interest as an effective way to improve LLM performance (Lewis et al., 2020). We focus on the setting where we prepend retrievals to the prompt, which is a common, simple method for incorporating retrievals into the context, though alternative RAG architectures exist (e.g., Shi et al., 2023). Our work builds upon previous studies which investigate how to train LLMs to better handle retrieved context. Some of these propose pre-training methods that boost RAG capabilities (Guu et al., 2020; Izacard et al., 2023; Shi et al., 2024), while others fine-tune a LLM for better RAG capabilities Luo et al. (2023). Our method targets the latter fine-tuning setting, where we hypothesize that out-of-distribution training examples cause performance degradation. Our key insight is to self-generate demonstrations, rather than using a teacher LLM (Luo et al., 2023) or human-written responses (Lin et al., 2024b).\n2.2 Distilling prompted behavior\nOur methodology has similarities with recent work on distilling the behavior of prompted language models back into the unprompted model. Note that this is different from distilling a larger model into a smaller one; instead, the prompted model"}, {"title": "2.3 Self-demo training & learning to abstain", "content": "Our work is partly motivated by the hypothesis Lin et al. (2024a) and Kang et al.'s (2024) hypothesis that out-of-distribution training data degrade LLM during post-training and encourage hallucinations. Similar to our work, Lin et al. (2024a) train LLMs on self-generated demonstrations, but with the goal of improving factuality. They also investigate using RAG as part of their training pipeline but not during inference. We also build on their work by introducing a filtering stage to ensure the quality and correctness of the self-generated instances before training on them.\nA large body of existing literature investigates learning when to abstain from answering questions (Wen et al., 2024). Our method for selecting self-demos resembles the simple method proposed by Yang et al. (2024) where training examples that the model answers incorrectly are replaced with refusals. We incorporate and improve on their template-based refusal strategy by selecting self-generated refusals when none of the self-generated demonstrations are correct. By using self-generated refusals instead of template-based ones, we better align the training distribution with the LLM's own distribution."}, {"title": "3 Method and Implementation", "content": "The goal of our method (outlined in Fig. 2) is to take an instruction-tuned LLM as a reference model and equip it with improved RAG capabilities. Our method relies on the fact that instruction-tuned LLMs are already capable of several tasks, including self-prompting, self-evaluation, question answering, and some RAG abilities. We leverage these abilities to bootstrap a dataset of self-demonstrations to train on.\nWe break the process of generating self-demos for SD-RA-IT into discrete steps. First, we obtain an instruction tuning dataset augmented with retrievals. Second, we automatically generate a set of prompts for the LLM. Third, we generate three types of outputs: retrieval-augmented responses, responses without retrieval responses, and refusals. Fourth, we filter these responses for correctness with respect to the gold response, choosing the self-demo that is most correct, or a refusal when no outputs match the gold response. In this section we review of our implementation of these steps.\n3.1 Models, infrastructure, and datasets\nAs our reference language models, we use state-of-the-art, open-weight, instruction-tuned models Llama-3-8B-Instruct and Llama-3-70B-Instruct. Our method is designed to require only on the reference model; no larger or more powerful model is required. We use the open-source libraries fairseq2 for training (Balioglu, 2023), and vLLM for inference (Kwon et al., 2023), and publicly release our training and inference scripts1.\nFor convenience, we opt to use the retrieval-augmented instruction tuning dataset from Lin et al. (2024b), who describe the composition of their dataset in Table 1 of their paper. This dataset consists of instances which each contain an instruction, a human-written response (gold demonstration), and retrievals from Wikipedia (Izacard et al., 2023) and CommonCrawl2.\nThe instruction-response pairs come from multiple domains: dialogue, open-domain QA, reading comprehension, summarization, and reasoning. We use a mix of 10% dialogue instances and 90% randomly instances from other domains, following a similar split to that used in Lin et al. (2024b).\nFor retrievals, we re-use the retrieved documents from Lin et al. (2024b) which are retrieved using a DRAGON+ retriever (Lin et al., 2023).\n3.2 Automatic prompt generation\nWe sample response candidates from multiple prompts with the aim to achieve diversity and maximize the likelihood of correctness. We use"}, {"title": "3.3 Generating, filtering, and training on self-demonstrations", "content": "To obtain a self-demo for each instance in our training set, we first generate multiple output candidates using the system prompts collected in the previous step. We then use the reference model as a judge, giving it access to the gold response and prompting it to select the most correct output, or to select a refusal if none of the candidates are correct. In practice, we find that the judge struggles to choose between many candidates at once, so we select the best candidate via a single-elimination tournament.\nWe try two common post-training methods for LLMs. The first, supervised instruction fine-tuning (SFT) minimizes cross-entropy loss on the response tokens. The second, direct preference optimization (DPO) takes two candidate responses, a preferred response and a rejected response, and optimizes the model to maximize the probability of the preferred response while minimizing the probability of the rejected response. We use a randomly selected self-demo candidate that was rejected during the self-demo generation process as the rejected response."}, {"title": "4 Experimental details", "content": "In this section we detail our methods and specific implementation choices.\n4.1 Benchmarks and Baselines\nOur evaluation focuses on knowledge-intensive question-answering (QA) tasks. For this, we use the evaluation datasets from Lin et al. (2024b)4, since these datasets are popular for RAG evaluation. Lin et al. (2024b) provide multiple retrievals for each of the evaluation instances. Each instance in these datasets consist of a question (e.g., \u201cwho won the 2012 World Series\") and an answer (e.g., \"San Francisco Giants\").\nWe contextualize our model's performance with several baselines. First, we compare to the reference model from which our model was fine-tuned. Since the reference model is a fully-fledged instruction-tuned LLM with some pre-existing RAG capabilities, this is a strong baseline. Next, we instruction-tune (IT) the reference model without using any retrievals. This baseline is meant to control for the effect continued fine-tuning on our model's performance, and also to demonstrate the adverse effects of fine-tuning on OOD demonstrations. The third baseline, most similar to our method, is retrieval-augmented instruction-tuning (RA-IT) on gold demonstrations. Both of our trained baselines (instruction tuning, RA-IT) use cross-entropy loss as their training objective. We also experimented with using DPO for RA-IT, using the model's own generations as the rejected response, but found in preliminary studies that this method catastrophically degrades model accuracy on all QA tasks, likely because the gold responses are too far outside the model's distribution.\n4.2 Metrics\nWe aim to measure several properties of RAG models to holistically evaluate our proposed method. In particular, we hypothesize that our training will make LLMs more accurate on questions it chooses to answer, better at learning which questions to refuse, better at integrating knowledge from retrievals, and suffer less degradation from training on OOD demonstrations.\nTo measure these effects, we first label all test instances as one of correct (if the model output matches the gold answer), refused (if the model declines to answer), or incorrect. We also label the retrievals as relevant or irrelevant, depending on whether the retrieval contains the answer to the question. We use an LLM-as-a-judge to label our test instances rather than rely on more traditional metrics like exact match. We do this because traditional metrics do not capture more nuanced properties of text like semantic equivalence and refusals. We observe that our judge LLMs are highly reliable and accurate for straightforward tasks such as determining whether model generations match the gold labels.\nWe use Llama-3-70B-Instruct as the judge LLM for 8B-parameter models, and Llama-3.1-405B-Instruct as the judge for 70B-parameter models. Our judge LLMs are one size larger than the reference model to get a more accurate judgement than the reference model could provide while minimizing inference costs. We find that inter-method trends generally hold across model sizes, but warn that raw scores may not be directly comparable across sizes since different judges may be more or less strict when deciding, for example, whether a generated answer sufficiently matches the gold answer.\nSince our method trains the model to refuse to answer when it is likely to get the answer wrong, we are interested in the model's ability to answer attempted questions correctly (precision) and its ability to attempt questions it can answer correctly (recall). We measure precision with answered accuracy, i.e., # correct/# attempted. Measuring recall (# correct/# answerable) is trickier because we do not have direct counterfactual information about which questions the model would get correct if it were to attempt to answer (we call this the counterfactual accuracy). As a proxy, we can count the number of false refusals (the number of refused questions that had relevant retrievals), then measure recall as # correct/(# correct + # false refusals)."}, {"title": "5 Analysis and Results", "content": "We find that our method leads to favorable performance on knowledge-intensive question answering tasks across several metrics. Our method has higher precision (accuracy on answered questions), higher recall (successful attempts on answerable questions), and lower counterfactual accuracy, compared to other methods. In ablations, we also find that our method leads to minimal degradation in non-RAG QA settings compared to all other methods, and achieves the highest performance (precision) across different numbers of retrievals.\nRefer to Appendix A for per-evaluation set breakdowns of the aggregated metrics (e.g., precision, recall) in this section.\n5.1 Precision\nAs shown in the first column of Table 2, training on self-demos leads to accuracy gains for answered questions (see Table 9 for per-dataset results.) In other words, models trained with SD-RA-IT are more likely than other baselines to answer correctly when attempting to answer. We attribute this improved answered accuracy to our model's superior ability to identify and refuse questions it is likely to get wrong. The other possible explanation would be that SD-RA-IT causes the model to increase the total number of correct answers without increasing the number of incorrect ones, but Table 4 rules this out by showing that the total number of correct answers does not systematically increase for any training strategy.\nTo further support the hypothesis that SD-RAIT teaches the model to refuse questions it will likely get wrong, we would like to know the proportion of refused questions that the model could have answered correctly, the lower the better. One option would be to estimate this as # false refusals/# refusals, but this does not take into account questions where the answer known by the model but not present in the retrieval. To guard against this, we also estimate the model's counterfactual accuracy by checking the accuracy of the reference model on the refused instances. These two metrics turn out to be highly correlated, and we find that the counterfactual accuracy on refused instances for SD-RA-IT is consistently lowest out of all models across model sizes, indicating that the answered accuracy gains are due to the SD-RA-IT model refusing questions that it will likely get wrong.\nThough SD-RA-IT achieves the highest precision among the baselines, looking at differences in other metrics suggests that different SD-RA-IT models achieve high precision in different ways. In particular, 70B SD-RA-IT (without DPO) gets slightly fewer total answers correct compared to the RA-IT model. This is driven by an increase in refusals, which include questions the model would likely answer correctly if attempted. The model therefore achieves high precision by attempting only high-confidence questions. On the other hand, SD-RA-IT models that do not show significant refusal increase must achieve higher precision through a more balanced combination of refusing questions it will get wrong, using the context to correctly answer more questions, and answering additional questions it would have otherwise refused.\nObserving the outputs of both the SD-RA-IT and the RA-IT models, we find that many of the RA-IT \"refusals\" are simply cases of the model completely ignoring the question. We hypothesize that these types of answers that ignore the question may stem from summarization tasks found in the instruction tuning dataset. If we counted these as incorrect rather than refused we would see an even bigger difference between the models' answered accuracy. In other words, our precision gains are likely underestimates. The fact that our models do not suffer from this type of degenerate behavior (despite training on the same data) indicates that SD-RA-IT reduces the impact of low-quality training data."}, {"title": "5.2 Recall", "content": "Table 2 shows that SD-RA-IT outperforms all other models on recall, i.e., accuracy on answerable questions, measured as # correct/(# correct + # false refusals). We attribute this to the fact that SD-RA-IT reduces false refusals compared to RA-IT, as discussed in the previous section. This supports our hypothesis that training on self-demos teaches the model to better incorporate relevant context compared to RA-IT.\nWe observe that that false refusal is lowest, and consequently recall is highest among our DPO-trained models, especially for the 70B models. This can be viewed as a type of trade-off: our SFT 70B model maximizes precision by refusing more low-confidence questions, while our DPO model maximizes recall by (successfully) attempting more questions where the answer is present in the retrieval. Both strategies result in high F1 scores."}, {"title": "5.3 Number of retrievals", "content": "It is often desirable for RAG systems handle simultaneous retrievals from multiple sources, as well as queries with no retrievals at all. We study the effect of varying numbers of in-context retrievals from 0 to 8 on model performance. For each number of retrievals n, we include the n most-relevant retrievals for the question, as scored by the retriever system.Table 8 shows that all models exhibit monotonic improvement as the number of retrievals increases, even when the number of retrievals surpasses the number of retrievals trained on. Across the board, SD-RA-IT achieves the highest performance.\nThe \"0\" column of Table 8, shows that both RA-IT and IT seriously degrade the LLM's performance (measured with precision) on QA without"}, {"title": "6 Discussion and conclusion", "content": "In this paper we found strong evidence that training on self-generated responses instead of gold ones consistently improves RAG models in QA settings. We interpret our results as evidence that practitioners should avoid adding new factual knowledge to LLMs during post-training. Our rationale is that training the model to output facts that it doesn't already \u201cknow\u201d encourages it to hallucinate by attempting to answer low-confidence questions. Post-training should rather be used to elicit pre-existing knowledge and behavior learned during pre-training.\nOur second major conclusion is that SD-RA-IT enables successful training on low-quality datasets. Artifacts such as summarization tasks in the training data mean that na\u00efve instruction tuning methods degrade model behavior. By training on self-demos, SD-RA-IT avoids teaching models to generate low-quality outputs while still allowing the model to benefit from the supervision and task adaptation aspects of the training data.\nFuture work can build on our contributions by investigating how self-demo instruction tuning can improve model behavior and performance outside the domain of RAG and question answering. We are also interested in techniques that control the trade-off between precision and recall that we saw between 70B SFT and DPO models in Section 5.2."}, {"title": "7 Limitations", "content": "Our study's scope is limited to the RAG setting and QA-based evaluations of the Llama-3 family of models. Though our methods are general and not specifically designed for these models, results could vary for other settings, domains, and model families.\nWhile we do not purposely select our instruction tuning set to have quality issues, it is possible that the gains from our method would be smaller if we were to repeat our experiments with a higher-quality instruction tuning set."}]}