{"title": "TransFair: Transferring Fairness from Ocular Disease Classification to Progression Prediction", "authors": ["Leila Gheisi", "Henry Chu", "Raju Gottumukkala", "Xingquan Zhu", "Mengyu Wang", "Min Shi"], "abstract": "The use of artificial intelligence (Al) in automated disease classification significantly reduces healthcare costs and improves the accessibility of services. However, this transformation has given rise to concerns about the fairness of Al, which disproportionately affects certain groups, particularly patients from underprivileged populations. Recently, a number of methods and large-scale datasets have been proposed to address group performance disparities. Although these methods have shown effectiveness in disease classification tasks, they may fall short in ensuring fair prediction of disease progression, mainly because of limited longitudinal data with diverse demographics available for training a robust and equitable prediction model. In this paper, we introduce TransFair to enhance demographic fairness in progression prediction for ocular diseases. TransFair aims to transfer a fairness-enhanced disease classification model to the task of progression prediction with fairness preserved. Specifically, we train a fair EfficientNet, termed FairEN, equipped with a fairness-aware attention mechanism using extensive data for ocular disease classification. Subsequently, this fair classification model is adapted to a fair progression prediction model through knowledge distillation, which aims to minimize the latent feature distances between the classification and progression prediction models. We evaluate FairEN and TransFair for fairness-enhanced ocular disease classification and progression prediction using both two-dimensional (2D) and 3D retinal images. Extensive experiments and comparisons with models with and without considering fairness learning show that TransFair effectively enhances demographic equity in predicting ocular disease progression.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of artificial intelligence (AI) technologies, particularly deep learning (DL), has profoundly reshaped the healthcare landscape, driving transformative changes throughout the industry [1], [2]. This is demonstrated by the growing number of AI-enabled medical devices that have been approved by the United States Food and Drug Administration [3], [4]. Among these advancements, AI systems are extensively employed to aid in the detection of various diseases, achieving expert-level accuracies [5], [6]. For example, deep learning methods are used for the automatic detection of glaucoma [7], [8], as shown in Fig. 1a. Unlike traditional clinician diagnoses that are typically only accessible at hospitals, AI-based diagnostic systems can be implemented in local pharmacies and primary care settings for large-scale disease screening. This makes them more accessible and affordable for the general public, especially benefiting rural areas with limited clinical resources and economically disadvantaged populations. Although AI demonstrates remarkable abilities in disease classification, predicting disease progression is clinically more crucial due to the irreversible damage caused by certain conditions. For instance, ocular diseases like glaucoma can lead to permanent vision loss if preventive measures are not taken before its onset [9]. Despite its critical importance, the task of progression prediction is currently less explored compared to classification, largely due to data scarcity [10], [11]. As shown in Fig. 1b, to accurately determine the progression or non-progression of glaucoma, longitudinal visual field data collected over extended periods are essential. However, collecting sufficient high-quality labeled data to train a reliable progression predictive model is prohibitively expensive and often unfeasible.\nOn the other hand, the application of DL has given rise to ethical debates and legal challenges in healthcare and medicine. A growing concern is the potential unfairness or bias of DL towards specific sub-populations [12]\u2013[14], particularly underprivileged groups defined by protected attributes like gender, race, ethnicity, socioeconomic status, among others. DL algorithms may identify spurious causal relationships in data that correlate with protected identity attributes, potentially leading to the use of patient identity information as a shortcut for predicting health outcomes [15]. For example, convolutional neural networks (CNN) disproportionately misdiagnose underserved groups, such as Hispanics and Medicaid patients, compared to White patients, and they inadvertently learn implicit racial information from radiology images [16]. In addition to above implicit mapping relationship learning, the unfairness of DL models can be caused by other confounding factors. First, the dataset utilized to train DL models can be imbalanced and skewed towards majority population groups. Consequently, DL models may perpetuate and amplify the inherent biases in the data, leading to unequal diagnostic outcomes for certain underserved populations. Second, the medical data annotations provided by specialists may contain noisy labels due to variations in clinical environments and the subjectivity of the specialists. Lastly, the models may exhibit bias towards simpler cases in diagnostic tasks, neglecting more challenging cases that are unevenly distributed across different sub-groups. Regardless of the implicit or explicit factors involved, it is crucial to ensure equitable diagnostic performance across different demographic groups before deploying an DL model in clinical practice to assist in the diagnosis.\nIn this work, we focus on improving the demographic fairness (e.g., across different racial and ethnic groups) of DL in predicting the progression of ocular diseases using retinal images. Research on fairness learning in disease progression prediction is scarce, largely due to the scarcity of medical data that includes comprehensive demographic attributes [17]. To the best of our knowledge, this is the first work to address fairness learning for predicting ocular disease progression. Mitigating the bias of DL in the medical settings is a complex and multifaceted challenge. A common approach is to manipulate the data used for training DL models to ensure they are representative of the entire population [18]. However, simply balancing the data distribution such as oversampling, synthetic data generation may be ineffective for certain medical applications [19]. Another line of research concentrates on addressing algorithmic fairness, which involves introducing explicit fairness constraints to ensure equality in patient outcomes [12], [18]. Given the robustness and generalizability of algorithmic-level fairness learning, we focus on improving the DL algorithmic fairness in this paper.\nTo overcome the challenge of limited longitudinal retinal images with diverse demographics, we propose using a fair classification model trained on extensive data to enhance and guide the training of a fair progression model with limited data. Recently, several large-scale retinal image datasets for ocular disease classification, such as FairVision [20] and FairDomain [21], have been developed within the community. These datasets facilitate the development of equitable DL models [20], [21] that enhance fairness in eye disease classification across various demographic groups defined by gender, race, and ethnicity. Motivated by these recent advancements, we aim to apply these insights and transfer demographic fairness from a classification model to a progression prediction model, as depicted in Fig. 2. To this end, we propose a novel fairness-aware feature learning model called FairEN, which incorporates a demographic attribute-informed attention mechanism within the EfficientNet framework [22] to tailor feature learning across diverse groups. Furthermore, we propose TransFair, a two-step fairness transfer approach from classification to progression prediction. First, we train a fairness-enhanced classification model (FairEN) using sufficient retinal images. Subsequently, we utilize the pretrained fair classification model as a teacher to guide the training of the fair progression prediction model (FairEN as a student) through knowledge distillation.\nTo conclude, our contributions are summarized as follows:\n\u2022 We propose to study a fair progression prediction problem for ocular disease, which is clinically vital for ensuring equitable progression prediction across various demographic groups.\n\u2022 We propose FairEN, which incorporates a fairness-aware attention mechanism into the EfficientNet to facilitate equitable classification and progression prediction of ocular diseases.\n\u2022 We propose TransFair, which learns to transfer the demographic fairness property from a classification model to a progression prediction model with fairness preserved.\n\u2022 We evaluate the proposed models for ocular disease classification and progression prediction using both 2D and 3D retinal images."}, {"title": "II. RELATED WORK", "content": "First, we review existing literature on ocular disease classification and progression prediction, and identify their limitations. Next, we explore recent advancements in fairness learning. Finally, we examine knowledge distillation technologies and their applications in the biomedical field."}, {"title": "A. Ocular Disease Classification and Progression Prediction", "content": "Ocular disease represents a significant global health issue, adversely affecting both daily activities and mental health. According to projections from the National Eye Institute 1, by 2030, an estimated 4.3 million Americans will be diagnosed with glaucoma, 11.3 million with diabetic retinopathy, and 38.7 million with cataracts. These eye disorders can cause irreversible vision loss and potentially lead to blindness if not detected and treated promptly [9], [23]. Over the past few years, deep learning-based approaches have been widely adopted for detecting various ocular diseases using retinal images such as color fundus photos, scanning laser ophthalmoscopy (SLO) fundus images, and optical coherence tomography (OCT) B-scans [24]\u2013[26]. For examples, Li et al. [27] proposed an CNN model with an attention mechanism that utilizes deep features highlighted by the visualized maps of pathological regions to automatically detect glaucoma. Hemelings et al. [28] extended the CNN model for glaucoma screening from fundus images. Instead of a CNN that performs binary classification (glaucoma or not), they opted for a regression CNN that outputs a continuous risk score. This risk score for CNN training was expert estimated vertical cup-disc ratio, which increases alongside glaucoma severity. Yip et al. [29] analyzed various deep learning (DL) models for diabetic retinopathy detection, including VGGNet, ResNet, DenseNet, and Ensemble models. For diagnosing diabetic retinopathy, these four DL models demonstrated similar diagnostic effectiveness, with AUC values ranging from 0.936 to 0.944. Above methods mainly adopted a supervised training manner for eye disease classification. However, a major challenge is that accurate classification labels for large-scale retinal images are expensive and laborious to obtain. To mitigate this issue, Luo et al. proposed a novel generalization-reinforced semi-supervised learning model called pseudo supervisor to optimally utilize unlabeled data. The proposed pseudo supervisor optimizes the policy of predicting pseudo labels with unlabeled samples to improve empirical generalization.\nAlthough numerous DL approaches have been developed that show promising performance in ocular disease classification, methods for predicting the progression of ocular diseases are less frequently explored [30]. Progression forecasting or prediction is a clinically more critical task for ocular patients compared to classification. It assists clinicians in determining the most appropriate treatment strategy-whether a patient should undergo aggressive treatment with invasive surgeries, which may have significant side effects, or opt for a more conservative approach using eye drops. Lee et al. [31] developed a DL model called machine-to-machine to predict longitudinal changes of retinal nerve fiber layer thickness from fundus photographs. These changes were then used to predict future development of glaucomatous visual field defects using a joint longitudinal survival model. Arcadu et al. [32] developed a DL model that used color fundus to predict the future threat of significant DR worsening at a patient level over a span of two years after the baseline visit. They suggested that DL model would enable early identification of patients at highest risk of vision loss, allowing timely referral to retina specialists and potential initiation of treatment before irreversible vision loss occurs.\nLimitations of existing works: While current studies have shown promising results in ocular disease classification and progression prediction, they face several practical challenges. First, current classification and progression prediction models often overlook the fairness aspect of DL, potentially leading to biased outcomes towards certain demographic groups [18]. Second, there is a scarcity of longitudinal retinal image data with diverse demographic attributes, which hinders the development of robust and equitable progression prediction models for ocular diseases. To address these challenges, we propose utilizing a fair classification model trained on extensive datasets to improve the training of a fair progression model using limited data."}, {"title": "B. Fairness Learning", "content": "A plethora of work has demonstrated that Al systems can exhibit bias in medical diagnostics, leading to uneven performance across various subgroups distinguished by protected attributes such as age, race, ethnicity, sex or gender, and socioeconomic status [12], [33]. The underlying factors of Al unfairness, which can be either explicit or implicit [34], include factors such as data imbalance, noisy labels, and heterogeneity in anatomical and pathological features present in medical images across various demographic groups. Various techniques for mitigating bias to achieve demographic fairness have been introduced and can be categorized into pre-processing, in-processing, and post-processing methods [12]. Pre-processing methods hypothesize that model bias is originated from the sample distribution imbalance, i.e., the majority of populations are from White and non-Hispanic groups compared to Asian and Black groups [19]. Therefore, techniques such as data sampling and transfer learning, are commonly utilized to reduce model bias and enhance fairness [35]. In-processing methods are based on the hypothesis that deep learning models can inherit and perpetuate biases present in the data [36]. Therefore, effective de-biasing strategies must be incorporated during model training to mitigate the influence of demographic attributes. For example, adversarial training [37] is designed to prevent the model from learning identity-specific features from the images, thereby mitigating performance bias linked to identity. However, recent studies report that adversarial learning-based strategies may be ineffective for some medical applications, since demographic information can enhance performance for minority groups [12], [19]. Luo et al. [13] proposed a fair identity normalization technique to normalize the latent image features of different groups with learnable means and standard deviations. As a result of this normalization, different groups will learn more distinguishing features benefiting improved model performance and equity. Rather than training a fair model from scratch, post-processing methods modify the output of a trained model (such as probability scores or decision thresholds) to satisfy group-fairness metrics [12]. Xian et al. [38] proposed a post-processing algorithm for fair classification under parity group fairness criteria, applicable to binary and multi-class problems under both attribute-aware and the more general attribute-blind setting. Gennaro et al. [39] framed the post-processing task as a new supervised learning problem taking as input the previously trained (biased) model. They introduced a new approach, Ratio-Based Model Debiasing, which predicts a multiplicative factor to rescale the biased model\u2019s predictions such that they better satisfy fairness guarantees. IN this work, we focus on the in-processing fairness learning method given that it is robust and generalizable in different applicable scenarios.\nDifferences from existing works: Existing fairness learning studies primarily focus on natural images or tabular data with simplified attributes, such as gender, for fairness learning. These differ from the more complex tasks present in healthcare and medical settings. First, medical AI systems must accommodate a range of demographic groups, including various racial and ethnic categories. Second, the pathological features in medical images often display subtle differences across these groups, posing a challenge in training both discriminative and fair deep learning models. Recently, several works have been proposed to address the fairness issue for medical classification [13], [21], [40]. In contrast, our focus in this paper is on fairness learning for disease progression prediction tasks, which are more challenging due to issues with data scarcity."}, {"title": "C. Knowledge Distillation in Biomedical Applications", "content": "Knowledge distillation is a technique for model compression where a well-trained [41], larger model (the teacher model) is used to guide a smaller model (the student model) to achieve outcomes that closely approximate those of the larger model. According to the number of teacher, Knowledge distillation can be roughly classified into three categories: single teacher knowledge distillation, multi-teacher knowledge distillation, and no teacher knowledge distillation (commonly known as self distillation). While according to the category of distilled knowledge, knowledge distillation can be divided into three categories: logit-based, feature-based, and relationship-based. In this work, we focus on single-teacher and feature-based knowledge distillation to transfer a fair classification model to a progression prediction model with fairness preserved. Knowledge distillation has been widely used in the medical field [41]. Wu et al. [42] proposed an adaptive knowledge distillation approach for unsupervised MRI reconstruction. The teacher models were firstly trained by filling the re-undersampled images and compared with the undersampled images in a self-supervised manner.The teacher models are then distilled to train another cascade model that can leverage the entire undersampled k-space during its training and testing. Xing et al. proposed a novel class-guided contrastive distillation module to pull closer positive image pairs from the same class in the teacher and student models, while pushing apart negative image pairs from different classes. With this regularization, the feature distribution of the student model shows higher intra-class similarity and inter-class variance [43]. Unlike the above methods, in our work, the teacher model is trained for classification and the student model for progression prediction, with both models achieving demographic fairness."}, {"title": "III. PROBLEM DEFINITION AND PRELIMINARIES", "content": "This section explains key concepts, problem formulation, evaluation tasks, and foundational knowledge related to the proposed method.\nRNFLT Map and OCT B-scan: This work utilizes two types of retinal images for the classification and progression prediction tasks, including 2D retinal nerve fiber layer thickness (RNFLT) map and 3D optical coherence tomography (OCT) B-scans. RNFLT is a measurement that quantifies the distance between the internal limiting membrane and the outer aspect of the retinal nerve fiber layer. Each RNFLT map captures the thickness values within the peripapillary area of 6 \u00d76 mm\u00b2. RNFLT maps are instrumental in diagnosing conditions such as glaucoma and other retinal disorders. Swept-source OCT is a non-invasive imaging technique which generates cross-sectional images of the retinal with high resolution. OCT B-scans are useful for diagnosing ocualr diseases such as glaucoma, diabetic retinopathy and macular degeneration.\nOcular Disease Classification: Let $D_{cls} \\in \\mathbb{R}^N$ represent a dataset of N retinal images for classification, which may be either 2D or 3D. Each image belongs to a specific population group (e.g., Asians, Blacks, Whites) defined by a demographic attribute $A_{cls}$ (e.g., race). The DL model is trained to learn the latent features of each retinal image, $X_{cls} \\in D_{cls}$, expressed as $h_{cls} = f_{cls}(X_{cls})$. This training occurs in a supervised manner by classifying types of ocular diseases, such as glaucoma and non-glaucoma. In this work, the DL model indicates the proposed FairEN.\nOcular Disease Progression Prediction: Let $D_{pred} \\in \\mathbb{R}^M$ represent a dataset of M retinal images. Each image $X_{pred} \\in D_{pred}$ belongs to a subgroup defined by the demographic attribute $A_{pred}$. Similar to the classification task, a DL model (i.e., TransFair) is trained to learn the latent features $h_{pred}$ of each retinal image. This training occurs in a supervised manner by predicting the progression outcome (i.e., progression vs. non-progression) of ocular diseases. The binary progression label is identified from longitudinal retinal images of multiple diagnoses over an extended period of time.\nDemographic Fairness of DL Models: The goal of the DL models is to fairly learn the latent features (i.e., $h_{cls}$ and $h_{pred}$) of retinal images, thereby ensuring equitable classification and progression prediction of ocular diseases across various demographic groups. These groups include gender categories such as Females and Males, or racial categories such as Asians, Blacks, and Whites. In this study, demographic fairness is achieved when these different groups exhibit equal or statistically indistinguishable performances in tasks related to ocular disease classification and progression prediction."}, {"title": "IV. METHODOLOGY", "content": "This section explains the proposed approach in Fig. 3 for fairness-enhanced progression prediction of the ocular disease. It involves the following three model training components in two consecutive phases:\n\u2022 Classification of Disease based on FairEN (Phase 1): A fairness-enhanced classification model is trained based on the FairEN model to achieve enhanced demographic fairness.\n\u2022 Progression Prediction of Disease based on TransFair (Phase 2): The fair classification model guides the training of a fair progression prediction model by training two sub-networks in TransFair: (a) The classification model from phase 1 serves as a teacher and is further optimized for the disease classification task; (b) Under the guidance of this fair classification model through knowledge distillation, a student model is trained to carry out progression prediction with fairness preserved.\nIt is important to note that in phase 2, both the teacher and student models are FairEN and they process the same retinal images, albeit for different tasks: classification and progression prediction, respectively. The details of these models and their optimization processes are introduced in the subsequent sections."}, {"title": "A. Classification of Disease based on FairEN", "content": "We aim to train a fair classification model on a substantial set of retinal images (e.g., RNFLT maps and OCT B-scans), that represent diverse demographics. This model will then guide the training of a fair progression prediction model, which will operate on a more limited dataset. To achieve this, we introduce a fairness-aware version of EfficientNet [22], referred to as FairEN, as shown in Fig. 3a.\nFairEN uses EfficientNet-B1 (written as EfficientNet thereafter) as the backbone to learn features of each retinal image $X_{cls} \\in \\mathbb{R}^N$, represented as:\n$h_{cls} = EfficientNet\\text{-}B1(X_{cls})$ (1)\nThe EfficientNet architecture comprises seven blocks differentiated by color in Fig. 3a, each consisting of multiple MBConv layers-a mobile inverted bottleneck convolution design that integrates squeeze-and-excitation optimization. These blocks vary in the depth and size of their filters as indicated by the kernel sizes (3 \u00d7 3 or 5 \u00d7 5) and the number of layers in each block. Starting with the initial standard convolution layer, the network progresses through these MBConv blocks, which increase in complexity and depth. Each block represents a stage in feature extraction where similar operations are applied to the input features. This hierarchical structure allows EfficientNet to efficiently manage computational resources while maximizing learning and representational capacity, making it highly effective for tasks requiring detailed image analysis and classification.\nHowever, EfficientNet may exhibit biases towards certain demographic groups (e.g., Asians and Blacks), resulting in notable performance discrepancies among different populations. To address this, we introduce a fairness-aware attention mechanism that adjusts feature learning based on demographic attributes. As shown in Fig. 3a, we utilize a multilayer perceptron (MLP) encoder to process the demographic attributes $A_{cls}$ associated with the input image $X_{els}$:\n$\\hat{h}_{attr}^{cls} = MLP(A_{cls})$ (2)\nSubsequently, the image and demographic attribute features are used to compute the fairness-aware attention weight $v$ by:\n$h_q = \\hat{h}_{attr} W^1, h_k = h_{cls}W^2$\n$v = \\text{softmax}(\\frac{h_q^\\top h_k}{\\sqrt{d}})$ (3)\n(4)\nwhere d is the dimension of latent features, $W^1$ and $W^2$ are learnable weight parameters. Next, the output of the fairness-aware attention layer is computed by:\n$h' = h_{cls} W^3 . v$ (5)\nwhere $W^3$ is the weight parameter. Finally, $h'$ is used for classification via a liner mapping layer which generates a probability value."}, {"title": "B. Progression Prediction of Disease based on TransFair", "content": "We introduce TransFair, designed to transition a fairness-enhanced classification model into a progression prediction model while maintaining its fairness properties. As shown in Fig. 3b, TransFair includes a teacher FairEN and a student FairEN, which performs different tasks co-trained through the process of knowledge distillation.\n1) Optimize the Teacher Model for Classification: The teacher model fine-tunes the fairness-enhanced classification model that was pretrained in the previous section. The teacher FairEN is optimized for both performance and equity, utilizing either RNFLT maps or OCT B-scans for the supervised classification of the ocular disease. Since both the pretraining and finetuning of the classification model are conducted on the same type of retinal images, there is no domain shift involved. Consequently, it is expected that the teacher model will maintain and even improve its demographic fairness. Similar to the pretraining phase, the teacher model trains to minimize the binary classification loss.\n2) Train the Student Model for Progression Prediction: We train another FairEN model as a student for disease progression prediction, as depicted in Fig. 3b. The student model takes the retinal image $X_{pred} \\in \\mathbb{R}^M$ and demographic attribute $A_{pred}$ as inputs and learns respective image and attribute features as follows:\n$h_{pred} = EfficientNet\\text{-}B1(X_{pred})$ (6)\n$\\hat{h}_{attr}^{pred} = MLP(A_{pred})$ (7)\nSimilar to teacher model's feature learning process, the learned image features after the fairness-aware attention layer are represented by:\n$h' = h_{pred}W^6 \\cdot \\text{softmax}(\\frac{\\hat{h}_{attr}^{pred} W^4 h_{pred} W^5 }{\\sqrt{d}})$ (8)\nwhere $W^4$, $W^5$ and $W^6$ are learnable weight parameters. To enhance the feature learning and fairness in the student model, image and attribute feature similarities between teacher and student models are minimized based on the Kullback-Leibler (KL) divergence [44]:\n$D_{img}^{KL}(h_{cls} || h_{pred}) = \\frac{1}{K} \\sum h_{cls} \\log(\\frac{h_{cls}}{h_{pred}})$ (9)\n$D_{attr}^{KL}(\\hat{h}_{attr}^{cls} || \\hat{h}_{attr}^{pred}) = \\frac{1}{K} \\sum \\hat{h}_{attr}^{cls} \\log(\\frac{\\hat{h}_{attr}^{cls}}{\\hat{h}_{attr}^{pred}})$ (10)\nwhere K is the batch size during the training. Taken together, the knowledge distillation minimizes:\n$D_{KL} = \\alpha \\cdot D_{img}^{KL} + \\beta D_{attr}^{KL}$ (11)"}, {"title": "C. Optimizations for FairEN and TransFair", "content": "FairEN and TransFair are trained in supervised manners. FairEN aims to minimize the classification loss, which is a binary cross-entropy loss over K retinal images in a mini-batch as:\n$L_{cls} = - \\frac{1}{K} \\sum_j [y_j \\log(\\sigma(\\hat{y}_j)) + (1-y_j) \\cdot \\log(1-\\sigma(\\hat{y}_j))]$ (12)\nwhere $y_j$ and $\\hat{y}$ are the ground-truth and predicted classification labels for current image $X_j$. The detailed training process of FairEN is summarized in Algorithm I. TransFair optimize both teacher and student models. The teacher model has the same optimization loss in Eq. 12, while the student model aims to minimize a combined loss:\n$L_{pred} = - \\frac{1}{K} \\sum_j [z_j \\log(\\sigma(\\hat{z}_j))$\n$+ (1 - z_j) \\cdot \\log(1 - \\sigma(\\hat{z}_j))]$\n$+ D_{KL}$ (13)\nwhere $z_j$ and $\\hat{z}$ are the ground-truth and predicted progression labels for current image $X_j$. $D_{KL}$ is calculated through Eq. 11. The detailed training process is summarized in Algorithm II."}, {"title": "V. EVALUATION METHODS AND RESULTS", "content": "This section presents experiments and comparative analyses to assess the performance and fairness of the proposed FairEN and TransFair models using both 2D and 3D retinal images."}, {"title": "A. Datasets", "content": "In this study, we focus on glaucoma, the second leading cause of blindness worldwide, although our approach is extensible to other diseases. Our evaluation of model fairness is centered on two demographic attributes: gender, categorized as Females and Males, and race, including Asians, Blacks, and Whites. We utilize three datasets to assess our models: Harvard-GF [13], FairVision [20], and Harvard-GDP [20]. The details of these datasets are as follows:\n\u2022 Harvard-GF: It consists of 3,300 RNFLT maps from 3,300 patients, with each 2D RNFLT map having dimensions of 225 x 225. Self-reported demographic information includes a gender distribution of 54.9% female and 45.1% male, and a racial breakdown of 33.3% White, 33.3% Black, and 33.4% Asian. Within the dataset, 47.0% of the patients are classified as non-glaucoma and 53.0% as glaucoma.\n\u2022 FairVision: It includes 10,000 OCT data samples from 10,000 patients. Each 3D OCT data sample contains 200 B-scans, where each B-scan image has a dimension of 200 \u00d7 200. The self-reported patient demographic information is as follows: 57.0% of the patients are female and 43.0% are male; racially, 8.5% are Asian, 14.9% are Black, and 76.6% are White. Glaucoma classification is based on a thorough clinical assessment. Accordingly, 51.3% of the patients are categorized as non-glaucoma and 48.7% as glaucoma.\n\u2022 Harvard-GDP: It includes 500 data samples from 500 glaucoma patients. Each sample includes both RNFLT map and OCT B-scans, accompanied by labels for glaucoma classification and progression prediction. Demographic information is as follows: Gender distribution is 54.0% female and 46.0% male; racial composition includes 9.4% Asian, 15.6% Black, and 75.0% White. The patients are divided into non-glaucoma and glaucoma categories, making up 55.2% and 44.8% of the dataset, respectively. Two criteria are employed to determine glaucoma progression based on visual field maps [45], with each map represented by a vector of 52 total deviation (TD) values ranging from -38 dB to 26 dB (see Fig. 1b for reference). The criteria are: (1) MD Fast Progression: eyes with an MD slope < -1 dB. (1) TD Progression: eyes with at least three locations exhibiting a TD slope < -1 dB; According to MD fast progression, 91.2% are categorized as non-progression and 8.8% as progression. For TD progression, the figures are 90.6% for non-progression and 9.4% for progression.\nWe perform experiments using 2D RNFLT maps and 3D OCT B-scans independently. The Harvard-GF and FairVision datasets are utilized for pretraining the fairness-enhanced classification model (as detailed in Section III-A), whereas the Harvard-GDP dataset is employed to train TransFair for fairness-aware progression prediction (as described in Section III-B)."}, {"title": "B. Comparative Methods", "content": "We choose to compare with the following methods for progression prediction with or without considering fairness learning:\nWithout fairness learning:\n\u2022 VGG [46]: The VGG model is a deep convolutional neural network that uses small convolution filters to enhance depth and performance in image recognition tasks.\n\u2022 DenseNet [47]: DenseNet connects each layer to every other layer in a feed-forward fashion, promoting feature reuse and reducing the number of parameters.\n\u2022 ResNet [48]: ResNet is a deep convolutional neural network that uses skip connections in its residual blocks to train deeper networks effectively, addressing the vanishing gradient problem.\n\u2022 ViT [49]: Vision Transformer (ViT) applies the transformer architecture to image processing by treating patches of an image as sequences for powerful, scalable image recognition.\n\u2022 EfficientNet [22]: It is a scalable deep convolutional neural network that optimizes accuracy and efficiency by scaling up layers, width, and resolution based on a compound coefficient.\nWith fairness learning:\n\u2022 EfficientNet_Adv [50]: It integrates an adversarial training to prevent learning demographic attribute features in EfficientNet.\n\u2022 FairEN: The proposed FairEN which incorporates a fairness-aware attention mechanism into the EfficientNet.\n\u2022 TransFair: The proposed TransFair for fairness-enhanced progression prediction of the ocular disease."}, {"title": "C. Experimental Settings", "content": "Dataset Splits: To train FairEN for glaucoma detection, we follow the data separation as the original paper [20], where 70% of the samples are allocated for training, 10% and 20% are used for model evaluation and testing, respectively. To Train TransFair for the progression prediction task, 70% and 30% are used for training and testing, respectively. We focus on gender and race for demographic attributes.\nTraining Scheme: The CNN models including VGG, DenseNet, ResNet and EfficientNet are trained with a learning rate of le-4 for 10 epochs with a batch size of 6. For ViT, we follow the literature [51] settings and train for 50 epochs with a layer decay of 0.55, weight decay of 0.01, dropout rate of 0.1, batch size of 64, and base learning rate of 5e-4. AdamW [52] is used as the optimization method for all models. The default values for \u03b1 and \u03b2 are set as 1.0 and 0.05 in TransFair.\nEvaluation Metrics: We evaluate FairEN for binary glaucoma detection and evaluate TransFair for binary progression prediction based on MD fast progression and TD progression, respectively. We use the Area Under the Receiver Operating Characteristic Curve (AUC) to access the glaucoma detection and progression prediction performance. To access model fairness, we use equity-scaled AUC following the existing work [20], which is computed by:\n$ES\\text{-}AUC = \\frac{AUC}{1 + \\sum_A |AUC - AUC_a|}$\nwhere A is the set of subgroups (e.g., Asian, Black, White) for a specific demographic attribute (e.g., race). AUC is the overall model performance on all patients, while $AUC_a$ indicates AUC for subgroup a for the demographic attribute A."}, {"title": "D. Results", "content": "This section presents the results of FairEN and TransFair", "Classification": "In this work", "Prediction": "We aim to enhance fairness in progression prediction by utilizing the fairness-enhanced ocular disease detection model built on the proposed TransFair framework. Table I shows the MD fast progression prediction using RNFLT maps. We can have the following five major observations. First, among all CNN methods, EfficientNet performns the best for both gender and racial attributes in terms of the overall AUC. For example, EfficientNet improved the AUC by 0.0711 over VGG, 0.0328 over DenseNet, and 0.0308 over ResNet. In addition, EfficientNet has the best model fairness on race, although it demonstrates the least fair CNN model. Second, ViT generally performs better than EfficientNet in both model performance and fairness in progression prediction using RNFLT maps, especially on gender where the overall AUC and ES-AUC outperform by 0.0154 and 0.1868, respectively. Third, integrating the adversarial training into EfficientNet does not enhance the model performance and fairness. Adversarial training aims to prevent the preservation of demographic attribute features for progression prediction."}]}