{"title": "Coordination Failure in Cooperative Offline MARL", "authors": ["Callum Rhys Tilbury", "Claude Formanek", "Louise Beyers", "Jonathan Shock", "Arnu Pretorius"], "abstract": "Offline multi-agent reinforcement learning (MARL) leverages static datasets of experience to learn optimal multi-agent control. However, learning from static data presents several unique challenges to overcome. In this paper, we focus on coordination failure and investigate the role of joint actions in multi-agent policy gradients with offline data, focusing on a common setting we refer to as the \u2018Best Response Under Data' (BRUD) approach. By using two-player polynomial games as an analytical tool, we demonstrate a simple yet overlooked failure mode of BRUD-based algorithms, which can lead to catastrophic coordination failure in the offline setting. Building on these insights, we propose an approach to mitigate such failure, by prioritising samples from the dataset based on joint-action similarity during policy learning and demonstrate its effectiveness in detailed experiments. More generally, however, we argue that prioritised dataset sampling is a promising area for innovation in offline MARL that can be combined with other effective approaches such as critic and policy regularisation. Importantly, our work shows how insights drawn from simplified, tractable games can lead to useful, theoretically grounded insights that transfer to more complex contexts. A core dimension of offering is an interactive notebook, from which almost all of our results can be reproduced, in a browser.", "sections": [{"title": "1 Introduction", "content": "Offline reinforcement learning (RL) is a promising paradigm for making real-world applications of RL possible. While some compelling progress is being made, particularly in the single-agent setting (Prudencio et al., 2023), large obstacles remain. In this paper, we focus on a problem unique to the multi-agent setting: learning coordination from static data (Barde et al., 2024). Whereas in online multi-agent learning, a speculated failure in coordination can be tested and corrected, such feedback does not exist in the offline case. Instead, the agents are constrained to solely using static data to learn how to best act together. Typically, agents optimise their own actions towards a best response to the actions taken by other agents in the dataset, we refer to this common approach as 'Best Response Under Data' (BRUD). This approach has various benefits in the offline setting, but is highly susceptible to miscoordination. This is clearly illustrated in Figure 1, using a simple two-player game where agents choose a continuous real number, and the collective reward is the product of the two actions chosen.\nIn this work, we use simple two-player polynomial games as an analytical tool for better understanding offline coordination, in an interpretable and accessible way. In doing so, we isolate the problem with a BRUD-style update in offline MARL, demonstrating clear modes of coordination failure. Then, building on our insights, we propose a class of offline sampling methods, broadly called Proximal Joint Action Prioritisation (PJAP) that help alleviate problems in coordination that stem from offline learning. We demonstrate the effectiveness of PJAP in detailed experiments. However, we see this work more as exploratory in nature, more generally highlighting prioritised sampling methods as a fruitful area of future investigation alongside approaches such as critic and policy regularisation for offline learning."}, {"title": "2 Foundations", "content": ""}, {"title": "2.1 Multi-Agent Reinforcement Learning", "content": "We consider the canonical Dec-POMDP setting for MARL where the goal is to find a joint pol- icy $(\\pi_1,..., \\pi_n) = \\pi$ such that the return of each agent $i$, following $\\pi_i$, is maximised with respect to the other agents' policies, $\\pi_{-i} = (\\pi\\backslash\\pi_i)$. That is, we aim to find such that $\\forall i : \\pi_i \\in \\arg \\max_{\\pi_i} E [G | \\pi_i, \\pi_{-i}]$, where $G$ is the return. We assume that each policy is parame- terised by $\\theta_i$. A popular approach to learning such policies is Centralised Training with Decentralised Execution (CTDE), where training leverages privileged information from all agents, yet the policies are only conditioned on their local observations, $\\pi_i(o_i; \\theta_i)$, enabling decentralisation at inference time.\nBecause of our focus on the offline setting, we narrow our scope to off-policy algorithms, where we learn using batches of data taken from a replay buffer, $B$. Specifically, we study multi-agent actor-critic approaches which have a policy objective of the form,\n$J(\\pi) = E_{a \\sim \\pi} [Q(o, a) + \\alpha R]$\nwhere $Q(o, a)$ is the joint critic, and $R$ is some policy regularisation term, controlled by $\\alpha \\in R$. This policy objective is a key component of many popular CTDE algorithms in MARL. For example, by using stochastic policies and entropy regularisation, $R = H(\\pi)$, we recover the policy objective of multi-agent soft-actor critic (Pu et al., 2021). With deterministic policies, $\\pi_i(a_i|o_i) = \\mu_i(o_i)$, and setting $\\alpha = 0$, we recover the policy objective of MADDPG (Lowe et al., 2017), and so on. Importantly, this policy update forms part of several leading offline MARL algorithms, such as CFCQL (Shao et al., 2023) and the CTDE form of OMAR (Pan et al., 2022)."}, {"title": "2.2 Joint Action Formulation", "content": "In the policy objective in Equation 1, the training is centralised by conditioning the critic on the joint observation, $o$, and the joint action, $a$. The joint observation can be formed as a simple concatenation of agent observations in the sampled data, $o = (o_1, ..., o_n)$. However, forming the joint action is more complex. For the policy learning of agent $i$, we consider $a_i \\sim \\pi(\\cdot|o_i; \\theta_i)$, but we have several choices for the other agent actions, $a_{-i}$, in the update. For example, we could simply use the other agents' policies directly, $a_{-i} \\sim \\pi(\\cdot|o_{-i}; \\theta_{-i})$. However, this approach has been shown to work poorly in offline settings (Shao et al., 2023), likely because we are decoupling policy learning from the dataset. Instead, the prevailing approach to forming the joint action in CTDE methods, both online and offline, is to simply use the samples taken from the buffer or dataset for the other agent actions. That is,\n$a_i = (a_i \\sim \\pi(\\cdot|o_i;\\theta_i), a_{-i} \\sim B)$\nWe call this the Best Response Under Data (BRUD) approach to policy learning. Though it benefits from staying directly coupled to the dataset in an offline context, it leads to coordination problems, which we will demonstrate shortly."}, {"title": "2.3 Polynomial Games", "content": "For our exposition, we study two-player polynomial games (Dresher et al., 1950; Zhong et al., 2024), as a differentiable, continuous generalisation of discrete matrix games\u2014which have been a common tool for understanding multi-agent algorithms (Rashid et al., 2020; Papoudakis et al., 2021). These games are atemporal and stateless, comprising two agents, $x$ and $y$, each able to take continuous actions. We denote the respective actions taken as $a_x, a_y \\in R$. The shared reward given to the agents is defined by some polynomial function, $R(a_x, a_y) = \\sum_{i=0}^{m} \\sum_{j=0}^{m} C_{ij}a_x^i a_y^j$. Because there is no state, and thus no observations, the notion of maximising the joint Q-function, $Q(o, a = \\{a_x, a_y\\})$ is equivalent to maximising the reward function $R(a_x, a_y)$ directly. We assume perfect knowledge of the reward function in the game."}, {"title": "3 Coordination Failure in Offline MARL", "content": "We now study coordination failure in offline MARL due to the BRUD approach in policy learning, using tractable and informative polynomial games. We use MADDPG (Lowe et al., 2017), which has a BRUD-style policy update for agent $i$,\n$\\nabla_{\\theta_i} J = E_{(o,a) \\sim B} [\\nabla_{\\theta_i} \\mu(o_i; \\theta_i)\\cdot\\nabla_{\\tilde{a_i}}Q(o, \\tilde{a_i}, a_{-i})|_{\\tilde{a_i}=\\mu(o_i;\\theta_i)}]$\nwhere $\\mu(o_i; \\theta_i)$ is a deterministic policy, and $B$ is a replay buffer or dataset.\nRecall that the polynomial game setting is stateless, and comprises just two agents, taking actions $a_x$ and $a_y$. For simplicity, let the policy for each agent be a single linear unit, $\\mu(\\theta_x) = \\theta_x$ and $\\mu(\\theta_y) = \\theta_y$ (i.e. the policy parameter directly defines the action). We can thus simplify the policy update such that for agent x,\n$\\nabla_{\\theta_x}J = E_{a \\sim B} [\\nabla_{\\theta_x}\\mu(\\theta_x)\\cdot\\nabla_{\\tilde{a}}R(\\tilde{a_x}, a_y)|_{\\tilde{a_x}=\\mu(\\theta_x)}] = E_{a_y \\sim B} [\\nabla_{\\tilde{a_x}}R(\\tilde{a_x},a_y)|_{a_x=\\theta_x}]$\nand similarly for agent y, we have $\\nabla_{\\theta_y} J = E_{a \\sim B} [\\nabla_{a_y} R(a_x, \\tilde{a_y})|_{\\tilde{a_y}=\\theta_y}]$. Therefore, each component in the gradient of the objective is simply the partial derivative of the agent's reward with respect to the agent's chosen action, in expectation over the actions of the other agent from the replay buffer or the dataset. This equation captures the essence of the policy update of BRUD methods.\nTo understand the ramifications of forming the joint action in this way, we first study the simple polynomial $R(a_x,a_y) = a_xa_y$, which we call the sign-agreement game. The true gradient field of this surface is $\\nabla R = (a_y, a_x)$, whereas the objective in the MADDPG update becomes $\\nabla J = (E_{a_y \\sim B}[a_y], E_{a_x \\sim B}[a_x]) = (\\bar{a_y},\\bar{a_x})$, where $\\bar{a_x}$ and $\\bar{a_y}$ are the sample means of the respective actions in the data taken from $B$. We consider the impact of the difference between $\\nabla R$ and $\\nabla J$ for this game. Whereas the former is a function of the current policy, always correctly pointing to the optimal direction of policy improvement, the latter is a unidirectional vector field defined solely by the sampled data. As a result, it becomes possible for catastrophic miscoordination in the joint-policy update, as illustrated in Figure 1. In this example, the best way to update $\\theta_x$, in response to a negative"}, {"title": "3.1 Connections to Off-Policy Learning", "content": "Importantly, the possibility of miscoordination as demonstrated here is present under any BRUD-style update-regardless of whether we are learning online from a dynamic buffer, or offline from a static dataset. Consider, though, how the impact of the policy gradient update from Equation 4 changes as we move from learning online to offline. We can illustrate this shift by studying the size of the replay buffer, $B$. Recall that introducing a replay buffer improves sample efficiency and stabilises training in deep reinforcement learning (Mnih et al., 2013). A useful way to understand the buffer is the relationship between its size and the degree to which learning is off-policy. Suppose data of size $b$ is sampled from the buffer to update the agents; then with a buffer size of $b$, agents are using experience immediately after witnessing it\u2014which is exactly on-policy, akin to an approach like REINFORCE (Williams, 1992). Naturally, then, as the buffer size increases\u2014where data is replaced less and less frequently-the algorithm becomes increasingly off-policy. In the limit, where the buffer size is infinite and data is never replaced, the setting becomes akin to offline learning, albeit with fresh data still being added."}, {"title": "3.2 Growing Risk of Miscoordination with Increased Agent Interaction", "content": "Though miscoordination can indeed be demonstrated in the game $R(a_x, a_y) = a_xa_y$, the problem can be mitigated through the choice of dataset. For example, by biasing the dataset to have sample means $\\bar{a_x} > 0$ and $\\bar{a_y} > 0$, something close to the optimal trajectory can be found, since the agents will move towards a high-reward region, $++$. To truly understand the severity of BRUD in offline contexts, we must look to more complex games. We present four games of increasing complexity below, showing how a higher degree of agent interaction leads to higher degrees of potential miscoordination.\nDecoupled Rewards: $R = a_x + a_y$. For completeness, consider a trivial case where the shared reward yielded to agents is simply the sum of their actions. Here, $\\nabla R = \\nabla J = (1,1)$. Agents must simply make their actions bigger to yield higher rewards. The components of the rewards are completely decoupled, and no miscoordination occurs, regardless of the dataset used for learning.\nSign Agreement: $R = a_xa_y$. As discussed before, the update in this game moves the agents in the direction of their teammate's average action in the batch, $\\nabla J = (\\bar{a_y},\\bar{a_x})$. If the dataset actions happened to be biased such that $\\text{sign}(\\bar{a_x}) = \\text{sign}(\\bar{a_y})$, then the policies will move towards a high-reward region. However, if the signs differ, the policies will move towards the low-reward region. Because there is only a single, simple interaction term, there is only a minor requirement of the dataset for successful offline learning.\nAction Agreement: $R = -(a_x - a_y)^2$. This game requires agents to take identical actions for optimal coordination, with anything else yielding $R < 0$. The true gradient field, $\\nabla R = (2a_y - 2a_x, 2a_x - 2a_y)$, implies a line of optima, $R = 0 <-> a_x = a_y$. In contrast, under the dataset, the field is $\\nabla J = (2\\bar{a_y} - 2\\bar{a_x}, 2\\bar{a_x} - 2\\bar{a_y})$, resulting in a single optimum, $\\nabla J = 0$ at the point $(\\bar{a_y}, \\bar{a_x})$. Note it is no longer the agents moving in the direction of the means of the dataset actions, but instead that the learning will converge to this point. The requirement for optimal learning is thus no longer solely based on the signs of the mean actions, but that $x = y$ in the dataset, which is a strong requirement.\nTwin Peaks: $R = -A(a_x^2 + a_y^2) - B(a_xa_y)^2 + Ca_xa_y, \\{A > 0, B > 0, C > 2A\\}$. Finally, we study a set of polynomial games of a higher degree, allowing for more interaction terms. For brevity, our treatment is presented for agent $x$, but all statements apply symmetrically to agent $y$, as the function itself is symmetric in the agent's actions. The surface has two peaks, with true maxima at $a = \\pm\\sqrt{(C- 2A)/2B}$. Most interesting in this polynomial is the bivariate quartic interaction, $(a_xa_y)^2$, since it is optimised with BRUD as $E_{a_y \\sim B}[\\nabla_{a_x}a_x a_y^2] = 2a_x(a_y^2 + \\sigma_y^2)$, where $\\sigma_y^2$ is the sample variance of the $y$ actions in $B$. Thus, we see that the outcome of offline learning depends not only on the data's sample mean but also on its spread. Indeed, we can derive two interesting relationships when learning offline in this game. Firstly, for learning to converge to the true optimum, we have\n$\\sigma_y(\\bar{a_y}) = \\pm\\sqrt{\\frac{-A}{B} + \\frac{C}{\\sqrt{2}B} - \\bar{a_y}^2}$\nwhich says the dataset's standard deviation must be a function of its mean. Notice then if we centred the dataset around the origin, such that $\\bar{a_y} = 0$, then $\\sigma_y = \\sqrt{-A/B}$, which is imaginary. Hence, there exists no distribution of data that enables learning the true optimum in this game, using offline BRUD, if the dataset is centred around (0,0)\u2014even if the dataset is infinitely large. We validate this result empirically in Figure 4a, showing that increasing the variance of the data does not help the learning succeed, for the converged policy is always simply (0, 0).\nSecondly, the expression for the converged learnt policy is,\n$\\nabla J = 0 <-> a_x = \\frac{C\\bar{a_y}}{2A + 2B(\\bar{a_y}^2 + \\sigma_y^2)}$\nThis expression corroborates the previous result, showing that an origin-centred dataset will always converge to the policy $a_x = 0$. Suppose we now centre the data exactly around the true optimum instead, $\\bar{a_y} = a_t$. Under such conditions, learning will converge to the optimal policy only when $\\sigma_y^2 = 0$ (that is, we must solely have optimal data in the dataset, with no spread); but as $\\sigma \\to \\infty$, then $a_x \\to 0$, which is increasingly far away from the true optimum. This result is validated empirically in Figure 4b. Perhaps counter-intuitively, we thus see that increasing diversity in the dataset actions can lead to worsening performance when learning offline in this game.\nRemark These polynomial games indicate a clear relationship: as the degree of agent interaction increases, the requirements of the dataset become more stringent for learning to converge to the true optimum with BRUD. As a result, the possibility of miscoordination increases."}, {"title": "4 Proximal Joint-Action Prioritisation for Offline Learning", "content": "We have seen that the BRUD approach to policy learning is highly susceptible to coordination failure in the offline setting. Nonetheless, BRUD remains useful for offline learning, since it allows us to stay tightly coupled to the dataset, which is the only signal available. However, simply updating each agent's policy in response to the current joint policy learned from the data has been shown empirically to work poorly in offline MARL (Shao et al., 2023).\nOur analysis in Section 3 highlights that which data is sampled when could make a critical difference in the utility of best response updates. For instance, the key difference between the successful learning in Figure 2, and the coordination failure in Figure 3, relates to the similarity between the current joint policy, $\\theta$, and the joint action used for the policy update, $a \\sim B$. In fact, the problem illustrated in Figure 1 would not have occurred if the data point, $a(t)$, was in the same quadrant as the current policy.\nTherefore, in this work, we advocate for prioritised dataset sampling methods as a promising area for innovation to improve learning in offline MARL. Furthermore, we consider sampling methods as an \"orthogonal\" axis to other effective approaches for offline learning such as critic and policy regularisation, where it can easily be combined with these methods to potentially great effect. As a way of demonstrating a preliminary instantiation of this idea, we propose Proximal Joint- Action Prioritisation (PJAP) as a class of offline sampling methods. In PJAP, prioritised experience replay (Schaul et al., 2016) is used to increase the probability of sampling actions that were generated by policies resembling the current joint policy, with priorities defined proportional to some similarity metric.\nConceptually, for each trajectory, $\\tau$, in a dataset, $B$, we model an underlying joint dataset-generating policy, $\\beta_{\\tau}$. Note that a given dataset may comprise various distinct dataset-generating policies-e.g. when the dataset has trajectories with both low and high returns, recorded over an online training procedure. We denote the current learnt joint policy after $k$ updates as $\\mu(k)$. In PJAP, we set the priority, $p_{k+1}$, for each of the trajectories $\\tau \\sim B$, to be inversely proportional to some function of the distance between the current joint policy and the dataset-generating policy, $d(\\mu(k), \\beta_{\\tau})$.\nAs a specific instance of PJAP, we propose transforming the distance on a Gaussian, $e^{-\\alpha d^2}$, where $\\alpha$ controls how rapidly the priorities decrease with respect to the distance. Under this transformation, we ensure small distances yield similar, large priorities, whereas larger distances yield exponentially smaller priorities. We also clip the minimum priority to some small value $\\epsilon > 0$, which avoids making certain samples so unlikely that they are effectively never seen again. This parameter, $\\epsilon$, is thus akin to controlling \"exploration\u201d of the dataset, where very occasionally we want to sample data that is, in fact, quite different to our current joint policy. In summary, our instance of PJAP takes the following prioritisation procedure,\n$PJAP_{\\mathcal{N}}(\\epsilon): p_{k+1}(\\tau) = \\max \\{\\epsilon, e^{-\\alpha d(\\mu_k, \\beta_{\\tau})^2}\\}$\nWe note that in practice, there are three key challenges when implementing PJAP. Firstly, we typically do not have access to the dataset-generating policy itself, $\\beta_{\\tau}$. Thus in this work, we use the sampled actions as a proxy for the policy that generated them, and compare them to the actions taken by the agents under the sampled observations. Secondly, it is computationally unrealistic to recompute the priorities for all trajectories in the dataset at each update step. Therefore, we fix this by bootstrapping the priority updates-updating only a subset of samples at a time. Thirdly, we concede that coming up with a good distance measure for a particular problem can be tricky, especially in higher-dimensional action spaces.\nWe now demonstrate our approach in the case of deterministic policies (e.g. MADDPG), and present different implementations of $PJAP_{\\mathcal{N}}(\\epsilon)$ using context-specific distance measures. First in the context of polynomial games, and then in the following section, a more complex MARL setting from MAMuJoCo (Peng et al., 2021). We note that developing generally performant \u201ccontext-agnostic\" distance measures exists as a fruitful area for future work."}, {"title": "4.1 PJAP in Polynomial Games", "content": "Although the datasets generated for the polynomial game are quite small, we use the L1 norm as a distance metric since ideally our chosen metric should be able to prioritise few samples from very large datasets. Another advantage of the L1 norm is that is it computationally inexpensive when compared to other distance and similarity metrics such as the L2 norm and cosine similarity. At step k, with current policy parameters, $\\theta(k)$, we update the priorities for each sample $\\hat{a} = (\\hat{a_x},\\hat{a_y}) \\sim B$ using the PJAP formulation from Equation 6, and the distance measure,\n$d(\\mu_k, \\beta_{\\tau}) \\approx ||\\hat{a}, \\theta(k)||_1$"}, {"title": "4.2 PJAP in MAMuJoCo", "content": "Next, we consider how to implement PJAP in a higher-dimensional setting, namely 2-Agent HalfChee- tah from MAMuJoCo (Peng et al., 2021). Here, the environment is no longer stateless and policies are conditioned on observations that change during an episode. Therefore, it is not immediately clear what a suitable distance metric can be to measure the proximity between the current learnt determinis- tic policy and the behaviour policies for each agent. In our experiments, we explore with using the L1 distance between the actions from the current learnt policy $\\mu_k$ and the sequence of actions in a given trajectory. That is, for a given trajectory $\\tau \\sim D$, we consider the sequence of observations and actions which make up the trajectory $\\tau = \\{((o_1^1, ..., o_n^1), (a_1^1, ..., a_n^1)), ..., ((o_1^N, ..., o_n^N), (a_1^N, ..., a_n^N))\\}$. Each $a_i(t)$ in the trajectory $\\tau$ comes from the unknown behaviour policy $\\beta_{\\tau}$. Thus, the average L1 distance between action $a$ and $\\mu_k(o)$ can be seen as an approximate distance metric for PJAP.\n$\\hat{d}(\\mu_k, \\beta_{\\tau}) \\approx \\frac{1}{NT} \\sum_t \\sum_{i=1}^n ||\\mu(o_i(t)), a_i(t)||_1$\nWe use the MADDPG+CQL implementation from OG-MARL (Formanek et al., 2023) as our baseline and then compare it to a version of MADDPG+CQL where we incorporate PJAP using the distance metric in Equation 8 and the prioritisation function in Equation 6. We evaluate our method against the baseline on two different datasets from the 2-Agent HalfCheetah environment. The results from the experiments are given in Figure 6. The average distance between actions sampled from the dataset and the current learnt policy is also given to highlight that PJAP reduces this distance as compared to the baseline, which corroborates the findings from the polynomial game experiment. All hyperparameters between the baseline and our variant with PJAP are kept constant and have been included in the attached code."}, {"title": "5 Related Work", "content": "Notable progress has been made in the field of offline MARL in recent years. Jiang and Lu (2021) highlighted that in offline MARL the transition dynamics in the dataset can significantly differ from those of the learned policies, leading to coordination failures. They address this by normalising the transition probabilities in the dataset. Yang et al. (2021) highlight and address the rapid accumulation of extrapolation error due to Out-of-Distribution actions in Offline MARL. Pan et al. (2022) show that MADDPG and Independent DDPG policy gradients struggle to optimise conservative Q-functions (Kumar et al., 2020), and propose using a zeroth-order optimisation to learn more coordinated behaviour. Shao et al. (2023) also highlight the limitations of MADDPG+CQL and propose a per-agent CQL regularisation that scales better in the number of agents. Wang et al. (2023) also explored a novel approach to regularising the value function in Offline MARL. Tian et al. (2023) consider an imbalance of agent expertise in a dataset, which can contaminate the offline learning of all agents. They address this problem by learning decomposed rewards, and then reconstructing the dataset while favouring high-return individual trajectories. Finally, work by Cui and Du (2022) addresses the additional fundamental difficulties of solving multi-agent learning problems using a static dataset. The authors show that the dataset requirements for the solvability of a two-player zero-sum Markov game are stricter than for a single-strategy setting."}, {"title": "6 Discussion", "content": "In this paper, we use simple two-player polynomial games to highlight and study the fundamental problem of miscoordination in offline MARL, when using a Best Response Under Data (BRUD) approach to policy learning. Building on our analyses, we propose Proximal Joint Action Prioritisation (PJAP), where sampled experience data is prioritised as a function of the current joint policy being learnt. We instantiate an instance of PJAP, and demonstrate how it can solve miscoordination problems in both the simplified polynomial game case, and in the more complex MARL setting of MAMuJoco.\nImportantly, though, our work primarily aims to be a catalyst for further development of dataset sampling prioritisation methods, as one tool in our offline MARL toolkit. This tool exists alongside other offline MARL remedies, such as critic and policy regularisation, all helping mitigate the difficulties of offline learning, together. We believe that PJAP paves the way for interesting research ideas in offline MARL.\nLimitations This paper primarily focuses on theoretical contributions and insights in simplified settings, using polynomial games as a backbone. Though useful as an interpretable and accessible tool, the context is admittedly limited in several ways\u2014it is stateless, comprises just two agents and assumes perfect knowledge of the reward surface. We acknowledge that these limitations constrain the generality of our conclusions, even when supported with more complex empirical results. However, remain confident that our approach takes an important step to improving our understanding of coordination in offline MARL."}]}