{"title": "MMSEARCH: BENCHMARKING THE POTENTIAL OF LARGE MODELS AS MULTI-MODAL SEARCH ENGINES", "authors": ["Dongzhi Jiang", "Renrui Zhang", "Ziyu Guo", "Yanmin Wu", "Jiayi Lei", "Pengshuo Qiu", "Pan Lu", "Zehui Chen", "Guanglu Song", "Peng Gao", "Yu Liu", "Chunyuan Li", "Hongsheng Li"], "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search engines, e.g., SearchGPT, showcasing a new paradigm in human-internet inter- action. However, most current AI search engines are limited to text-only set- tings, neglecting the multimodal user queries and the text-image interleaved na- ture of website information. Recently, Large Multimodal Models (LMMs) have made impressive strides. Yet, whether they can function as AI search engines remains under-explored, leaving the potential of LMMs in multimodal search an open question. To this end, we first design a delicate pipeline, MMSEARCH- ENGINE, to empower any LMMs with multimodal search capabilities. On top of this, we introduce MMSEARCH, a comprehensive evaluation benchmark to as- sess the multimodal search performance of LMMs. The curated dataset contains 300 manually collected instances spanning 14 subfields, which involves no over- lap with the current LMMs' training data, ensuring the correct answer can only be obtained within searching. By using MMSEARCH-ENGINE, the LMMs are evaluated by performing three individual tasks (requery, rerank, and summariza- tion), and one challenging end-to-end task with a complete searching process. We conduct extensive experiments on closed-source and open-source LMMs. Among all tested models, GPT-4o with MMSEARCH-ENGINE achieves the best results, which surpasses the commercial product, Perplexity Pro, in the end-to-end task, demonstrating the effectiveness of our proposed pipeline. We further present error analysis to unveil current LMMs still struggle to fully grasp the multimodal search tasks, and conduct ablation study to indicate the potential of scaling test-time com- putation for AI search engine. We hope MMSEARCH may provide unique insights to guide the future development of multimodal AI search engine.", "sections": [{"title": "1 INTRODUCTION", "content": "Search engines (Brin & Page, 1998) have been the main tools for humans to navigate through the overwhelming quantity of online resources. Recently, Large Language Models (LLMs) (OpenAI, 2023a;b; Touvron et al., 2023a) have demonstrated impressive performance on various zero-shot downstream applications. On top of this, AI search engine (OpenAI, 2024c), which integrates LLMs with traditional search engines, stands among one of the most promising ones. It points the direc- tion of the next-generation interaction paradigm of human and Internet. Combining the language understanding ability of LLMs and up-to-date information from the Internet, AI search engines could better grasp the user's intention and summarize contextual-aligned answers from the raw web"}, {"title": "2 MMSEARCH", "content": "In Section 2.1, we first detail the design of our multimodal AI search engine pipeline, which serves as both data collection and evaluation tools. Then, in Section 2.2, we detail the data composition and collection of the curated multimodal search benchmark MMSEARCH. Then, in Section 2.3, we elaborate on our step-wise evaluation strategy. Finally, we detail the dynamic nature of our benchmark in Section 2.4."}, {"title": "2.1 MMSEARCH-ENGINE: A MULTIMODAL AI SEARCH ENGINE PIPELINE", "content": "The searching process is a complex action including multi-round interactions between LMMs and conventional search engines. We develop a delicate pipeline that queries LMMs multiple times to accomplish this task. Leveraging the image comprehension capabilities of LMMs, we incorporate two types of visual data. First, we incorporate Google Lens (len) to search for information from the image. The second type of visual data is the screenshot of the retrieved websites, in the purpose of preserving the original format of website content. Our framework is shown in Fig. 2. Below we detail how an LMM works with this pipeline, which comprises three sequential phases:\ni. Requery. The query direct from users may contain references to certain information in the image, e.g., the News-Finance example shown in Fig. 1. Since a conventional search engine only accept text-only input, it is necessary for LMM to translate the image content and combine it with the query to ask a valid question to it. In addition, the raw user query may be ambiguous or inefficient sometimes (Chan et al., 2024; Ma et al., 2023), reformulating the query to be more clear is also a must for LMM. If the user query contains an image, we incorporate the screenshot of the image search result from the google lens (len). We treat the user query, user image, and the image search screenshot as basic information of the query. This information will be input to LMM in every round in the pipeline. For the requery round, we prompt LMM to output a requery to a conventional search engine.\nii. Rerank. The requery is sent to a search engine API, e.g., DuckDuckGo, to retrieve top K relevant websites. Depending on the requery quality, not all retrieved websites are necessar- ily relevant for query answering. Hence, we prompt LMM to select one most informative website for answer summarization. Due to the LMM's context length limitations and the"}, {"title": "2.2 DATA COMPOSITION AND COLLECTION", "content": "To thoroughly assess multimodal search proficiency, we compile a comprehensive problem set cov- ering a broad spectrum of news topics, specialized knowledge domains, and query image patterns. This widespread collection for MMSEARCH aims to simulate diverse user searching scenarios, en- suring a robust evaluation of LMMs' capabilities in multimodal search.\nData Composition and Categorization. Our benchmark aims to isolate LMMs' inherent knowl- edge and assess their actual search capabilities. We focus on two primary areas: News and Knowl- edge. For the News area, the queries are related to the latest news at the time of data collection (August 2024). This guarantees no overlap between the current LMMs' training data and questions in our benchmark. All questions in this area are recorded with their occurrence time. For fairness, LMMs with recently updated knowledge should be tested on queries that occurred after their lat- est data update. Due to its time-sensitive nature, the News area serves as a dynamic part of our benchmark. Please refer to Section 2.4 for details. As for the Knowledege area, we focus on rare knowledge in targeted domains. Each question proposed by an annotator is verified to be beyond the"}, {"title": "2.3 EVALUATION PROTOCOL", "content": "In contrast with previous LMM benchmarks, the multimodal search process of LMM contains mul- tiple rounds. Only the end-to-end evaluation of the final answer is inadequate to reveal the models' deficiency in each core searching step. For example, the errors made by the model may occur during the summarization process, but it might also stem from choosing an incorrect website during the reranking stage. To this end, we propose a step-wise strategy to evaluate the LMMs' capability on the three core searching steps, in addition to the end-to-end evaluation.\n\u2022 End-to-end score ($\\mathbf{S_{e2e}}$): We compute the F1 score between the predicted answer and the ground truth to judge if the answer is correct.\n\u2022 Requery score ($\\mathbf{S_{req}}$): We apply the average of ROUGE-L and BLEU-1 scores to measure the similarity between the model's requery and human-annotated requery.\n\u2022 Rerank score ($\\mathbf{S_{rer}}$): The rerank score is derived from the LMM's selection among K pre-defined websites. The score values is 1.0 for valid set, 0.5 for unsure set, and 0 for invalid set or incorrect format.\n\u2022 Summarization score ($\\mathbf{S_{sum}}$): Again, we compute the F1 score of LMM's answer based on a pre-defined website content against ground truth.\nThe input, output, and ground truth of the four tasks are visualized in Fig. 4. The final score is weighted by these four scores. We assign the highest weight (75%) to the end-to-end task, as it reflects the real-world multimodal search capability. The remaining 25% is distributed among the intermediate steps: 10% each for the rerank and summarization tasks, and 5% for the requery task. The lower weight for the requery task accounts for the inherent uncertainty in this process. The scoring process can be formulated as:\n$\\mathbf{S_{final}} = 0.75 \\cdot \\mathbf{S_{e2e}} + 0.05 \\cdot \\mathbf{S_{req}} + 0.1 \\cdot \\mathbf{S_{rer}} + 0.1 \\cdot \\mathbf{S_{sum}} $"}, {"title": "2.4 BENCHMARK EVOLUTION", "content": "In Fig. 5, we showcase the statistics of data timestamp distribution in the News area. Our dataset spans from 1st May 2024 to 31th August 2024. By the time of evaluation, we inspect the knowledge cutoff dates of the closed-source models. Claude 3.5 Sonnet reports a knowledge cutoff of April 2024, while both GPT-4V and GPT-4o state they lack information from 2024. For open-source models, we examine their release dates and training data, confirming that none possess knowledge beyond May 2024. This temporal gap ensures the fairness of our evaluation, as the models' perfor- mance solely reflects their multimodal search capabilities rather than pre-existing knowledge. We will update the News area if a new LMM's training data may overlap with our collection period."}, {"title": "3 EXPERIMENT", "content": "In this section, we conduct a systematic evaluation of existing LMMs on MMSEARCH. We first in- troduce the experimental setup in Section 3.1. Then, we detail the quantitative results in Section 3.2 and narrate the error analysis in Section 3.3. Finally, we explore scaling test-time compute versus scaling model size in Section 3.4."}, {"title": "3.1 EXPERIMENT SETUP", "content": "Evaluation Models We examine the performance of foundation models across three distinct cate- gories on MMSEARCH: (a) Commercial AI Search Engines, represented by Perplexity (Perplexity). We test the pro version of Perplexity, which takes only the user query and image as input. Since SearchGPT (OpenAI, 2024c) has not been public yet, we do not test on it. (b) Closed-source LMMs, represented by models like GPT-4V (OpenAI, 2023c), GPT-4o (OpenAI, 2024b), and Claude 3.5 Sonnet (Anthropic, 2024), and (c) Open-source LMMs, featuring models such as LLaVA-OneVision- 7B (Li et al., 2024b) (Qwen2-7B (Yang et al., 2024a)), LLaVA-OneVision-72B (Li et al., 2024b) (Qwen2-72B (Yang et al., 2024a)), LLaVA-NeXT-Interleave (Li et al., 2024c) (Qwen1.5-7B (Yang et al., 2024a)), Intern VL2 (Chen et al., 2024d) (InternLM2.5-7B-Chat (Cai et al., 2024)), InternLM- XC2.5 (Zhang et al., 2024a) (InternLM2-7B (Cai et al., 2024)), Qwen2-VL-7B (Qwen Team, 2024) (Qwen2-7B (Yang et al., 2024a)), Qwen2-VL-72B (Qwen Team, 2024) (Qwen2-72B (Yang et al., 2024a)), mPlug-Owl3 (Ye et al., 2024) (Qwen2-7B (Yang et al., 2024a)), Idefics3 (Lauren\u00e7on et al., 2024) (LLaMA3.1-7B-Instruct (AI@Meta, 2024)), and Mantis (Jiang et al., 2024b) (LLaMA3- 7B (AI@Meta, 2024)). Note that the open-source LMMs' sizes are 7B unless otherwise specified.\nImplementation Details. We set the number of retrieved websites K as 8. All our experiments of open-source models are conducted without any fine-tuning on search data or tasks. As for the prompts, the requery prompt contains 3 examples to better guide LMMs to output a valid requery. While prompts for other tasks are all in a zero-shot setting. We prompt the LMM to output as few words as possible for a better match with the ground truth. We employ the metric introduced in Section 2.3. Besides, we recruit eight qualified college students and ask them to solve the problems"}, {"title": "3.2 EXPERIMENTAL ANALYSIS", "content": "To thoroughly investigate the multimodal searching capabilities, we present the evaluation results of different models on MMSEARCH following the proposed step-wise evaluation strategy in Table 2 and fourteen subfields in Table 3. We now provide a detailed discussion of notable findings and their implications for multimodal search capabilities.\nAny-resolution input only provides slight or no improvement. Of the tested LMMs, four models, which are InternLM-XC2.5, InternVL2, mPlug-Owl3, and Idefic3, all support both low-resolution (LowRes) and any-resolution input (AnyRes). As one would expect, AnyRes input enables better OCR and perception of the image. However, we only observe slight or even no enhancement com- paring the difference between the LowRes performance and its AnyRes counterpart. Take mPlug-"}, {"title": "3.3 ERROR ANALYSIS", "content": "To investigate the limitations of current LMM search capabilities, we conducted a comprehensive analysis of error types observed in our evaluation. Our proposed step-wise evaluation strategy en- ables analysis of failure modes for each core search step, complementing the end-to-end assessment. This analysis encompasses the entire benchmark. We first examine the end-to-end error types for both the best-performing closed-source model (GPT-4o) and open-source model (Qwen2-VL-7B). To better understand the failure cases, we then identify distinct error types in the requery and summa- rization task, which requires open-ended generation. We quantify these error types for a systematic understanding of current LMM limitations and point out critical areas for improvement."}, {"title": "3.3.1 ERROR ANALYSIS OF END-TO-END TASK", "content": "In this section, we are trying to answer the question: Which step does LMM make a mistake in the end-to-end evaluation? In Fig. 7, we showcase the statistics of different error types occurring in GPT-4o and Qwen2-VL-7B. We define the following four error categories: (i) requery, where the model requery is incorrect, and leads to all retrieved websites being invalid; (ii) rerank, where the model selects a website without a correct answer; (iii) summarization, where the full website content contains the information of correct answer, but the model fails to extract it; (iv) informal, the output format deviates from the prompt specifications. As shown in the figure, GPT-4o's primary error sources are rerank and summarization errors, while requery and informal errors account for approximately half the frequency of the main error causes. This suggests that GPT-4o's limitations lie primarily in information source ranking and multimodal information integration. As for Qwen2- VL, all four error types occur with similar frequency. The rise of the informal error portion may be attributed to the model's inferior instruction-following ability. Besides, it should be noted that the requery task demands advanced comprehension and key image information extraction ability. This task seldom appears in the training data of current LMMs. The prevalence of this error type in Qwen2-VL may indicate that it fails to generalize to adequately address this complex task."}, {"title": "3.3.2 ERROR ANALYSIS OF REUQERY AND SUMMARIZATION TASK", "content": "To better understand how open-source LMM makes the mistake, we dive into the requery and sum- marization task to find out the error patterns of Qwen2-VL-7B. We particularly select the two tasks requiring open-ended generation, which provides more information to identify the error.\nAs for the requery task, we categorize five types of errors:\n\u2022 Lacking Specifility, where the model fails to include all the specific information in the requery and therefore leads to sub-optimal search results. For example, the query is asking the release date of Vision Pro in China. However, the model omits the condition of China and directly asks about the release date of Vision Pro.\n\u2022 Inefficient Query, where the model does not consider the real scenario and the requery is inefficient for the search engine to find the answer. For example, the query is asking whether the Van Gogh's Sunflowers and Antoni Clav\u00e9's Grand Collage are both oil paint- ings. Clearly, it is a commonsense that Van Gogh's Sunflowers is an oil painting and Antoni Clav\u00e9's Grand Collage is much less well-known. An efficient query should be asking about the images of Antoni Clav\u00e9's Grand Collage and further determine if it is also an oil paint- ing by directly looking at it. However, the model directly asks the original query to the search engine. There is very little chance that an exact same question has ever been raised so probably this requery will bring very little helpful information.\n\u2022 Excluding Image Search Results, where the model totally ignores the information in the screenshot of the image search results and therefore lacks important specific information in the requery. For example, the query is \u2018When did this football player obtain the gold medal?' and provides an image of the player. The model is supposed to find out the player's name by viewing the image search result and raise a requery like \u2018[PLAYER NAME] ob- tained the gold medal time'. However, the model fails to incorporate the player's name in the requery and definitely the retrieved websites will not include any helpful information.\n\u2022 No Change, where the model just uses the question as the query input to the search engine.\n\u2022 Irrelevant, where the model either matches wrong information from the image search result or mistakenly understands the query and outputs an irrelevant requery.\nThese error types of requery suggest that LMM often fails to fully understand the requery task and fails to aggregate all available information. Besides, the error type of inefficient query indicates that LMM has no clue of the real working scenario and query principles of search engines.\nAs for the summarization task, we also identify five types of errors:\n\u2022 Text Reasoning Error, where the model fails to extract the answer from the website textual information.\n\u2022 Image-text Aggregation Error, where obtaining the answer needs combining the informa- tion from both images and texts. The model fails to do so.\n\u2022 Image reasoning Error, where the model fails to extract the answer from the image, and the answer can only be obtained from the image.\n\u2022 Hallucination (Huang et al., 2023), where the model provides an unfaithful answer that cannot be grounded in the given content.\n\u2022 Informal, the output format does not follow the prompt specifications, the same error type in the end-to-end task.\nThe occurrence of the five types of summarization errors reflects that current LMMs still cannot correctly extract the given multimodal information to answer the query. The ability of content un- derstanding still requires further enhancement for current LMMs."}, {"title": "3.4 SCALING TEST-TIME COMPUTE VS SCALING MODEL SIZE", "content": "Recent works such as OpenAI 01 (OpenAI, 2024a) and Li et al. (2024d) have highlighted the critical role of scaling test-time computation in enhancing model performance. Our end-to-end task, which requires multiple Internet interactions, presents an opportunity to investigate the potential of scaling"}, {"title": "4 CONCLUSION", "content": "In this paper, we investigate the potential of LMMs as multimodal AI search engines. We first design MMSEARCH-ENGINE, a streamlined pipeline, enabling zero-shot LMMs to perform mul- timodal searches. To comprehensively assess the search capabilities, we introduce MMSEARCH, a benchmark comprising 300 queries across 14 subfields. Our evaluation methodology analyzes LMM search abilities step-by-step, facilitating a deeper understanding of their limitations. Using MMSEARCH-ENGINE, we evaluate various closed-source and open-source LMMs, revealing that current models still fall short of human-level search proficiency. Through thorough error analysis, we identify specific patterns of failure in key search process steps, providing valuable insights for future improvements in LMM search ability."}, {"title": "APPENDIX OVERVIEW", "content": "\u2022 Section A: Related work.\n\u2022 Section B: Additional experimental details.\n\u2022 Section C More dataset details.\n\u2022 Section D: Qualitative examples."}, {"title": "A RELATED WORK", "content": "Large Multimodal Models. Recently, multimodal models (Radford et al., 2021; Li et al., 2022; OpenAI, 2023c; Rombach et al., 2022; Jiang et al., 2024c) has gained unparalleled attention. Build- ing on the success of Large Language Models (LLMs) (Touvron et al., 2023a;b) and large-scale vision models (Radford et al., 2021), Large Multimodal Models (LMMs) are gaining prominence across diverse domains. These models extend LLMs to handle tasks involving various modalities, including mainstream 2D image processing (Liu et al., 2023a; Zhu et al., 2023; Lin et al., 2023; Gao et al., 2023b), as well as 3D point clouds (Xu et al., 2023; Guo et al., 2023; 2024b), and videos (Li et al., 2023; Chen et al., 2023a; Zhang et al., 2023; Fu et al., 2024). Among these LMMs, Ope- nAI's GPT-4o (OpenAI, 2024b) and Anthropic's Claude 3.5 Sonnet (Anthropic, 2024) demonstrate outstanding visual reasoning and comprehension capability, setting new standards in multi-modal performance. However, their closed-source nature limits broader adoption and development. In contrast, another research trajectory focuses on open-source LMMs for the community. Pioneer- ing works like LLaVA (Liu et al., 2023a; 2024a; Li et al., 2024c;b), LLaMA-Adapter (Zhang et al., 2024b; Gao et al., 2023b), and MiniGPT-4 (Zhu et al., 2023; Chen et al., 2023b) incorporate a frozen CLIP (Radford et al., 2021) model for image encoding and integrate visual information into LLM for multi-modal instruction tuning. Later, works such as mPLUG-Owl (Ye et al., 2023a;b; 2024), SPHINX (Gao et al., 2024; Lin et al., 2023), and InternLM-XComposer (Dong et al., 2024) further advanced the field by incorporating diverse visual instruction tuning data and generalizing to more scenarios. More recent developments in the field have taken diverse directions. For example, several studies (Zong et al., 2024; Tong et al., 2024) explore multiple vision encoders design. Meanwhile, other works (Liu et al., 2024a; Chen et al., 2024d; Qwen Team, 2024) incorporate high-resolution image input. Multi-image instruction data (Li et al., 2024c; Jiang et al., 2024b) is also integrated to enable perception across multiple images. While various benchmarks, both in the general (Fu et al., 2023; Liu et al., 2023b; Yu et al., 2023) and expert (Zhang et al., 2024c; Lu et al., 2023; 2022) domain, has been proposed, the potential of LMM to function as a multimodal search engine re- mains largely unexplored. To this end, we introduce the MMSEARCH benchmark, which evaluates LMMs' zero-shot abilities of multimodal search, offering valuable insights for future research.\nLarge models with Retrieval Augmented Generation (RAG). RAG (Retrieval-Augmented Gen- eration) is an effective strategy for enhancing model knowledge by retrieving relevant information from external sources (Fan et al., 2024). RAG has been leveraged in various scenarios including knowledge-intensive question answering (Borgeaud et al., 2022; Guu et al., 2020), machine trans- lation (He et al., 2021), and hallucination elimination (B\u00e9chard & Ayala, 2024). Current works has focused on improving specific aspects of RAG. RG-RAG (Chan et al., 2024) proposes to refine the query for retrieval by decomposition and disambiguation. Self-RAG (Asai et al., 2023) incorpo- rates the self-reflection of LLM to enhance the generation quality. The AI search engine could be viewed as a form of RAG with the Internet serving as the external knowledge source. Recently, MindSearch (Chen et al., 2024c) proposes an AI search engine framework to simulate the human minds in web information seeking. Meanwhile, multiple benchmarks of RAG (Yang et al., 2024b; Chen et al., 2024b) have been introduced to comprehensively evaluate a RAG system. However, both the current AI search engine and RAG benchmark are limited to the text-only setting, leaving the multimodal search engine and evaluation largely unexplored. To bridge this gap, we introduce MMSEARCH-ENGINE and MMSEARCH, a multimodal AI search engine pipeline and dataset de- signed to evaluate various multimodal scenarios."}, {"title": "B ADDITIONAL EXPERIMENTAL DETAILS", "content": "Model Sources. For different LMMs, we select their latest models with size around 7B for evalua- tion to fully reveal their multimodal search proficiency. Table 5 presents the release time and model sources of LMMs used in MMSEARCH.\nFull-page Screenshot Slimming. For the full-page screenshot, we compute the Sobel gradi- ents (Kanopoulos et al., 1988) to detect the edges and generate a gradient magnitude image. We iteratively remove the areas with gradients below a threshold, which represent the blank areas. This approach effectively reduces image size while maintaining critical document content.\nInput Prompts of LMM for Response Generation. We showcase the input prompts of LMM for the three tasks respectively in Table 6-8. We adopt two types of prompts for queries with an image and without images. For query with an image, we specifically require the LMM to leverage the image search result to solve the task."}, {"title": "C MORE DATA DETAILS", "content": "C.1 SUBFIELD DEFINITION\nNews area encompasses a vast spectrum of information, ranging from everyday events to engag- ing entertainment content and specialized fields such as scientific discoveries and financial analysis. This comprehensive coverage serves as a rigorous assessment of the model's ability to process in- formation in diverse domains. We divide this expansive area into eight distinct subfields:\n\u2022 Traditional Sports: Data concerning traditional athletic competitions, team performances, player statistics, and sporting events. This includes scores, league standings, player trans- fers, and analysis of various professional sports across different leagues and countries.\n\u2022 e-Sports: Information about competitive video gaming, including tournament results, player rankings, and league information. This covers various game titles, team formations, streaming viewership statistics, and tournament information.\n\u2022 Technology: Information about technological innovations, gadgets, software develop- ments, and tech industry news. This includes product launches, software updates, cyberse- curity issues, and artificial intelligence advancements.\n\u2022 Paper: Content related to academic papers, research publications, and scholarly articles in various artificial intelligence fields. The queries include method explanation, figure under- standing, and experiment settings.\n\u2022 Entertainment: Data about movies, music, television, celebrities, and other forms of pop- ular entertainment. It also includes data concerning video games.\n\u2022 Finance: Information on financial markets, economic indicators, business news, and mon- etary policies. This covers stock prices, company earnings reports, company financial state- ments, and regulatory news regarding finance.\n\u2022 General News: Broad coverage of various news topics not specific to any particular sub- field. This includes a mix of local and global events, human interest stories, lifestyle arti- cles, climate news, and general interest content that doesn't fit neatly into other specialized news subfields.\n\u2022 False Premise: Data related to misinformation or incorrect assumptions in the query. This subfield focuses on fact-checking capabilities. All the answers to the queries of this subfield are 'invalid question'.\nKnowledge area represents broad subfields of information and data related to general knowledge across various disciplines. This area concentrates on rare knowledge that most LMMs fail to answer. We categorize this area into five subfields:\n\u2022 Architecture: Information about building design, architectural styles, building informa- tion, and construction projects. This includes city landmarks, the comparison of architec- tural styles, and multi-view architecture matchings.\n\u2022 Arts: Data concerning visual arts, drawings, sculptures, badges, and other forms of creative expression. This covers artwork details, artist profiles, artwork history, and artwork style comparisons.\n\u2022 Fashion: Content related to clothing trends, fashion brands, and designer collections. This includes retail price, clothing style, release date, and brand information.\n\u2022 Astronomy: Information about celestial objects, space exploration, astronomical phenom- ena, and related research. This covers observational data from telescopes and image results from space missions. The questions focus on the background information of these celestial objects presented in the query image.\n\u2022 Anime: Data about Japanese animation, including series storylines and character informa- tion. This encompasses character background, character appearance, voice actor informa- tion, and chapter information.\n\u2022 Auto: Content related to automobiles, including vehicle specifications, industry trends, and automotive technology. This covers new car models, performance test results, coefficients of cars, and release date."}, {"title": "D QUALITATIVE EXAMPLES", "content": null}]}