{"title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models", "authors": ["Bo Gao", "Michael W. Spratling"], "abstract": "Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the l\u2081-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic length scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens. When combined with our proposed attention mechanism, this approach demonstrates significant promise in managing longer sequences, maintaining nearly constant validation loss even at 16\u00d7 the training token length while ensuring numerical stability. Our code is available at: https://github.com/iminfine/freeatten.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have achieved significant success in recent years, largely due to the use of self-attention mechanism. This approach enables models to assess the importance of different words within a sentence, capturing complex relationships and dependencies in the data. Its effectiveness is mainly attributed to the Softmax operation. When the Softmax operation is omitted or replaced with alternative activation functions, model performance tends to decline (Wortsman et al., 2023; Ramapuram et al., 2024; Shen et al., 2023). Although widely used, the Softmax self-attention has two main limitations. Firstly, it suffers from numerical instability due to the natural exponential function, especially when scaling model sizes to trillions of parameters (sta). Secondly, as the token length increases during inference, the attention scores calculated by self-attention become smoother and lack distinct peaks. This hinders the model's ability to establish connections between relevant tokens, thereby affecting the length extrapolation capabilities of transformers (Chiang & Cholak, 2022). This paper addresses these two problems simultaneously: developing a Softmax-free attention mechanism that offers improved numerical stability and better performance even at large token lengths."}, {"title": "2. Method", "content": ""}, {"title": "2.1. Softmax Decomposition", "content": "Scaled dot-product attention transforms queries (Q), keys (K), and values (V) through a two-step procedure. First, attention scores A are produced via:\n\n$A = Softmax (\\frac{Q K^T}{\\sqrt{d}} + M)$   (1)\n\nHere, Q, K, V \u2208 $R^{L\u00d7d}$, where L represents the sequence length and d denotes the dimensionality. The optional term M acts as a mask with shape L \u00d7 L, which is particularly beneficial in causal self-attention contexts. The mask M is constructed with zeros below the diagonal and -\u221e above it, ensuring that the attention mechanism only considers past and present tokens, thereby preserving the autoregressive nature of the model. The attention scores are subsequently utilised to compute the outputs as O = AV.\n\nThe Softmax function is a crucial component of the attention mechanism. It transforms a vector of raw scores x into a probability distribution, where the probabilities of each token sum to one. The outputs of Softmax are inherently non-negative, a feature considered essential for the success of Softmax attention. However, this assumption is somewhat questionable, as empirical studies that replaced the Softmax function with other non-negative output activation functions have shown a decline in model performance.\n\nTo investigate this further, we conducted experiments to test the necessity of non-negative attention scores. As illustrated in Table 1, one approach involved inverting the Softmax outputs by multiplying them by negative one, converting them to non-positive values. This inversion resulted in the validation loss remaining nearly unchanged, suggesting that non-negativity is not crucial for maintaining LLM perfor-mance. Additionally, we performed two more experiments. The first involved re-centring the attention scores. For each"}, {"title": "2.2. Length Scaled Softplus Attention", "content": "In this subsection, we present Length Scaled Softplus Attention (LSSA), a novel attention variant designed to enhance the scalability and performance of LLMs. Building upon the foundational work in attention mechanisms, LSSA in-troduces a novel scaling factor that accounts for both the\n\nsequence length and the model's dimensionality, thereby addressing limitations associated with traditional attention methods in handling long sequences during inference.\n\nLSSA utilises the Softplus activation function to introduce non-linearity while maintaining smooth gradients essential for stable training dynamics. Through empirical testing, we evaluated several widely-adopted activation functions by substituting them for \u03c6 into Equation (3). Our experi-ments in Section 3.2 revealed that Softplus outperforms the traditional exponential function (ex), thereby justifying its adoption in the design of LSSA.\n\nIn contrast to scaled dot-product attention, cosine similarity attention employs the 12-norm for each row of Q and K. This approach has been shown to produce more moderate attention weights, which can enhance performance across various tasks (Henry et al., 2020; Dehghani et al., 2023; Liu et al., 2022). However, in high-dimensional spaces, as the number of dimensions increases, two randomly selected vectors are likely to become orthogonal. This phenomenon causes the elements of the product of normalised ||Q||2 and ||K||2 to approach zero, thereby worsening the vanishing gradient problem. Consequently, a scale factor becomes es-sential for cosine similarity attention, and this factor should be associated with the dimensionality d.\n\nPrevious work demonstrated that replacing the traditional scaling factor $1/\\sqrt{d}$ with $\\frac{log L}{\\sqrt{d}}$ in scaled dot-product attention enhances the length extrapolation capabilities of transformers (Chiang & Cholak, 2022). Furthermore, (Su, 2021) highlighted that the inclusion of the log L factor aids in maintaining entropy invariance with different token length, thereby facilitating better extrapolation to unknown sequence lengths. Extending these findings, our approach adjusts the scaling factor to a dynamic length scale factor, reflecting the variation in token length across different rows of attention scores. Specifically, we set the scaling factor to log d log N, where N is an L \u00d7 L matrix, with each row containing identical elements corresponding to the number of tokens involved in attention for the i-th row, i.e., the number of tokens without masking. Clearly, Nij = i. This ensures that the attention mechanism remains robust across varying sequence lengths and model dimensions.\n\nThe formulation of LSSA is mathematically defined as:"}, {"title": "2.3. Attention Re-weighting Mechanism", "content": "While scaled dot-product attention has achieved remarkable success in LLMs, it inherently activates all input tokens to compute attention scores. This approach is somewhat counter-intuitive, as tokens positioned at the beginning of the input sequence are less likely to be relevant to tokens along the diagonal of row attention scores. Consequently, early tokens should ideally have minimal or no influence as i increases when generating attention scores.\n\nExperiments with ReLU-based attention, discussed in Sec-tion 3.2, demonstrate a minor performance decrease com-pared to the original attention mechanism. The observed performance decrement can be attributed to the distribution of positive values within the row attention scores. Specifi-cally, positive values are predominantly concentrated around the diagonal, while other positions remain negative. The ReLU activation function tends to suppress these negative inputs, resulting in a limited number of activated neurons during training. This sparsity restricts the model's capacity to capture essential relationships across the entire sequence, thereby adversely impacting overall performance. These results support the need for a re-weighting mechanism.\n\nThis phenomenon cannot be adequately addressed by tradi-tional normalisation methods such as LayerNorm (Ba et al., 2016), as LayerNorm does not alter the underlying distri-bution of row attention scores. One potential solution is to transform all attention score values to the positive do-main using functions like ex or Softplus before applying LayerNorm. As all values become positive through such transformations, employing the l\u2081-norm emerges as a more effective strategy. The l\u2081-norm is advantageous because it is simple to calculate and ensures that the mean value of the effective values of each row (the positive values that sum to one) remains constant (equals to +), thereby maintaining the integrity of the attention distribution. In this context, the Softmax operation in classic attention mechanism and Soft-plus with l\u2081-norm in LSSA can both be regarded as forms\n\nwhere O denotes a matrix where every element is equal to one 2. The mean effective value of A \u2297 N is one, leading to an expectation of effective value of A \u2297 N \u2212 O being zero. The notation ReLUP indicates that the output of the ReLU function is raised to the power of p, with p being a positive integer. ReLU effectively masks tokens with small attention scores, while the power operation enhances the discriminability of the remaining tokens.\n\nAs the token length increases, the attention scores be-come smoother and lacks distinct peaks, which hinders the model's ability to establish connections between relevant tokens, thereby affecting the length extrapolation capabili-ties of transformers (Chiang & Cholak, 2022). This issue can be effectively addressed by the power operation, as demonstrated below:\n\nLet $x_1,x_2,...,x_n$ represent the positive elements in one row of the ReLU output from Equation (5), with the maxi-mum value denoted as $x_m$. The formula for any input $x_j$ in Equation (5) is:\n\n$z_j = \\frac{x_j^p}{\\sum_{k=1}^{n}x_k^p}$         (6)\n\nFor any $x_l$ where $x_l$ < $x_m$, the distance between their re-weighted values with respected to p\u2192\u221e is given by:\n\n$lim_{p \\rightarrow \\infty} \\Delta(x_m - x_l) = lim_{p \\rightarrow \\infty} \\frac{x_m^p - x_l^p}{\\sum_{k=1}^{n} x_k^p} \\newline = lim_{p \\rightarrow \\infty} \\frac{1 - (\\frac{x_l}{x_m})^p}{\\sum_{k=1}^{n}(\\frac{x_k}{x_m})^p}$   (7)\n\nThis limit demonstrates that as p increases, the distance be-tween the maximum value and any smaller value approaches 1. Specifically, when p is large, the term $(x_l / x_m)^p$ approaches zero for any $x_l$ < $x_m$, causing all non-maximum values to be effectively suppressed. Meanwhile, the denomina-tor becomes dominated by $x_m$, resulting in the maximum value approaching 1 while all other values approach 0. This property ensures that the attention mechanism maintains sharp, distinct peaks even with longer sequences, thereby"}, {"title": "3. Experiments", "content": "The experiments were executed using 8 NVIDIA A100 80GB GPUs. We utilised the GPT-2 small architecture, consisting of 124 million parameters (Radford et al., 2019). Importantly, we substituted the original absolute position embeddings with Rotary Position Embeddings (RoPE) (Su et al., 2024) and incorporated NTK Scaling (Liu et al., 2024b) to handle extended token lengths during inference for length extrapolation.\n\nAll models were trained using a sequence length of 1024 on the fineweb-10B dataset, which consists of 10.2 bil-lion training tokens distributed across 18,865 training steps, along with 0.1 billion validation tokens."}, {"title": "3.1. Softmax Decomposition", "content": "As illustrated in Equation (2), the Softmax operation can be decomposed into a non-linear transformation via the exponential function, followed by the l\u2081-norm. In this section, we examine the importance of each component for LLMs by training the GPT-2 model from scratch with and without each component.\n\nThe results are presented in Table 2. Comparing the model employing Softmax in Scenario IV with Scenario I, which does not utilise Softmax, reveals that performance does not significantly deteriorate without Softmax. This can be attributed to the masking operation in attention, which ele-vates the rank of raw attention scores from d to L. Given that L\u226b d, causal masking substantially enhances the model's non-linear capability. In tasks devoid of causal masking, such as transformers used in vision applications, performance degrades markedly (Wortsman et al., 2023). Scenario II shows that using a non-linearity enhances per-formance. However, it is possible that alternative activation functions may enhance performance beyond that produced by using the exponential function."}, {"title": "3.2. Comparison with Different Activation Functions", "content": "We modify the standard attention mechanism by introducing several novel attention variants for comparative analysis. Specifically, these variants are created by substituting \u03c6 in Equation (3) with several widely-used activation functions: ReLU (Rumelhart et al., 1986), ReLU\u00b2 (So et al., 2021), ReLU6 (Howard et al., 2017), GeLU (Hendrycks & Gimpel, 2016), Sigmoid (Rumelhart et al., 1986), Softplus (Zheng et al., 2015), and Mish (Misra, 2019).\n\nThe results presented in Table 3 indicate that all activation functions, except for Softplus, result in higher validation loss values compared to the standard attention mechanism represented by ex. This phenomenon can be attributed to the distribution of positive values in the row attention scores (the inputs to these activation functions), which are pre-dominantly concentrated around the diagonal, while other positions remain negative. Consequently, these activation functions, apart from Softplus, tend to suppress negative inputs, leading to a limited number of activated neurons dur-ing training. This suppression results in higher loss values than those observed with the exponential function, ex. Nev-ertheless, the performance differences are relatively minor, suggesting that the positive values play a more critical role than the negative values in influencing the model's output.\n\nIn contrast to ex, the outputs of Softplus exhibit a slower growth rate, resembling a linear function when inputs are positive. This characteristic impedes its ability to empha-sise large positive values, and consequently, the resulting attention focuses more on tokens that are further away from the diagonal, which could explain why its validation loss is lower than that of ex. Further supporting this observation, the SquareReLU function clips negative values at zero and squares the positive inputs, thereby amplifying larger posi-tive values even further, resulting in the highest loss value among the tested activation functions."}, {"title": "3.3. Comparison with State-of-the-Art Softmax-Free Attentions", "content": "We compared the proposed LSSA and LSSAR with leading Softmax-free attentions using the same GPT-2-124m model."}, {"title": "3.4. Ablation Study for Re-weighting Mechanism", "content": "To gain deeper insights into the proposed re-weighting mech-anism, we evaluated it using LSSA and Softmax attention across various values of p. As illustrated in Figure 2, LSSA and Softmax attention exhibit distinct responses to changes in the re-weighting parameter p. While Softmax attention performs comparably to LSSA at lower p values, it suffers from severe gradient explosion issues as p increases, partic-ularly noticeable at p = 15. This instability manifests as a sharp rise in validation loss across all sequence lengths, rendering the model practically unusable at higher p values.\n\nIn contrast, increasing p values generally improve the per-formance of LSSA across different sequence lengths, with optimal results observed around p = 15. The validation loss remains relatively stable as the sequence length in-creases, underscoring LSSA's strong scalability. This may be because transformers can be viewed as a smoothed cu-bic spline and Softplus activation is more natural, smooth, and maintains fidelity with spline representations (Lai et al., 2024). In addition, the derivative of the Softplus function is the Sigmoid function, which has an upper bound of one. Hence, LSSA is less susceptible to gradient explosion prob-lems.\n\nTo further investigate LSSA's stability with large p values, we conducted additional tests with p = 50 and p = 100. LSSA maintains stability and avoids gradient explosion even at these extreme values, although we observe a gradual increase in validation loss. This increase occurs because larger p values lead to a sharper attention distribution in each row, causing the model to focus primarily on the maximum value while potentially overlooking other relevant contextual information. This suggests that LSSA can effectively handle large p values without numerical instability, outperforming Softmax attention in this aspect."}, {"title": "4. Discussion", "content": "While LSSAR was evaluated on a small model (GPT-2-124m) with a maximum inference token length of 16K, due to limited computing resources, we can infer from Figure 2 that the validation loss of LSSAR would remain relatively stable for token lengths greater than 16K. Furthermore the extreme p value can be considered as increasing the depth of the GPT-2-124m model. In this context, it is reasonable to assume that LSSAR would also be stable for larger scale models with billions parameters.\n\nThe implementation of LSSA and LSSAR using PyTorch's built-in functions incurs significant memory and compu-"}, {"title": "5. Related Work", "content": "Softmax-free Attention. Previous research has investigated Softmax-free attention by substituting the Softmax function with the ReLU activation (Li et al., 2022; Shen et al., 2023; Wortsman et al., 2023; Hron et al., 2020; Bai et al., 2024; Fu et al., 2024), SquaredReLU activation (Hua et al., 2022), and Sigmoid activation (Ramapuram et al., 2024), as well as examining purely linear attention (Qin et al., 2022; Tsai et al., 2019; Katharopoulos et al., 2020; Han et al., 2023; Arora et al., 2024; Lu et al., 2021). However, none of these approaches outperform the original Softmax attention. In contrast, the proposed LSSA demonstrates enhanced numerical stability and superior performance across various token lengths.\n\nAttention Re-weighting. The non-linear re-weighting mechanism introduced by softmax attention (l1-normalisation) has been shown to concentrate the distribution of attention weights, thereby stabilising the training process (AUEB et al., 2016; Gao & Pavel, 2017; Jang et al., 2016; Qin et al., 2022). Our empirical findings further demonstrate its essential role in maintaining the performance of LLMs. Moreover, we introduce a novel perspective: a non-linear positivity transformation followed by l\u2081-normalisation. Inspired by the classic normalisation-ReLU structure, the proposed re-weighting mechanism masks less relevant tokens and amplifies the relevant ones, which boosts the length extrapolation ability of underlying models. This provides the deep learning community with a deeper understanding of the attention mechanism within transformers.\n\nLength Extrapolation. Positional embeddings (Su et al.,"}, {"title": "6. Conclusion", "content": "In this paper, we present a novel attention mechanism called LSSA. The primary innovation involves decomposing the Softmax operation, used in the traditional self-attention framework, into a non-linear transformation and the l\u2081-norm, replacing the non-linear transformation with the Softplus activation function and introducing a dynamic length scale factor related to token length. Experimental results indicate that LSSA surpasses conventional Softmax attention across various sequence lengths. To improve the length extrapo-lation capabilities of LSSA, we have devised a distinctive re-weighting mechanism that, when integrated with LSSA, the resulting model LSSAR significantly enhances its ability to extrapolate over longer sequences while maintaining sta-bility and scalability. This positions LSSAR as a compelling candidate for large-scale models. Consequently, we recom-mend the incorporation of LSSAR in future transformer architectures."}]}