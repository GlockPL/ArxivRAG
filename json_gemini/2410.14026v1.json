{"title": "Generating Signed Language Instructions in Large-Scale Dialogue Systems", "authors": ["Mert \u0130nan", "Katherine Atwell", "Anthony Sicilia", "Lorna Quandt", "Malihe Alikhani"], "abstract": "We introduce a goal-oriented conversational\nAl system enhanced with American Sign Lan-\nguage (ASL) instructions, presenting the first\nimplementation of such a system on a world-\nwide multimodal conversational AI platform.\nAccessible through a touch-based interface, our\nsystem receives input from users and seam-\nlessly generates ASL instructions by leveraging\nretrieval methods and cognitively based gloss\ntranslations. Central to our design is a sign\ntranslation module powered by Large Language\nModels, alongside a token-based video retrieval\nsystem for delivering instructional content from\nrecipes and wikiHow guides. Our development\nprocess is deeply rooted in a commitment to\ncommunity engagement, incorporating insights\nfrom the Deaf and Hard-of-Hearing commu-\nnity, as well as experts in cognitive and ASL\nlearning sciences. The effectiveness of our sign-\ning instructions is validated by user feedback,\nachieving ratings on par with those of the sys-\ntem in its non-signing variant. Additionally, our\nsystem demonstrates exceptional performance\nin retrieval accuracy and text-generation qual-\nity, measured by metrics such as BERTScore.\nWe have made our codebase and datasets\npublicly accessible at https://github.com/\nMerterm/signed-dialogue, and a demo of\nour signed instruction video retrieval sys-\ntem is available at https://huggingface.co/\nspaces/merterm/signed-instructions.", "sections": [{"title": "1 Introduction", "content": "Conversational systems have become increasingly\nintegrated into our everyday lives, yet their accessi-\nbility to the Deaf and Hard-of-Hearing (DHH) com-\nmunity, who predominantly communicate through\nsigned languages, remains limited (Glasser et al.,\n2017, 2020; Bragg et al., 2020). Despite growing\nadvocacy for more inclusive interactive technolo-\ngies from DHH users (Bragg et al., 2019; Blair\nand Abdullah, 2020; Kahlon and Singh, 2023), a\ncomprehensive dialogue system tailored for sign\nlanguage users has yet to be implemented on a\nglobal scale. In response, within the Alexa Prize\nTaskBot Challenge 2 framework, we developed\nand launched the first task-oriented, multimodal\ndialogue system utilizing ASL, aiming to bridge\nthe gap between DHH users and personal voice as-\nsistants. This system translates touch-based inputs\ninto ASL video instructions, offering a ground-\nbreaking approach to interaction. This paper in-\ntroduces our ASL instruction framework, marking\na significant stride towards integrating conversa-\ntional systems into the living spaces of sign lan-\nguage users and enhancing accessibility for the\nDHH community.\nMany signers prefer to use ASL instead of text\ndue to grammatical and linguistic differences be-\ntween spoken and signed languages (Hariharan\net al., 2018; Dangsaart et al., 2008). Yet currently,\nsystems claiming to be accessible resort to text-\nbased communication. As an alternative, videos or\navatars of signers are options, yet these technolo-\ngies are underutilized. In this paper, we show that\ndeploying these signed systems on a large scale is,\nin fact, possible without much production cost and\nmakes the system accessible to DHH users.\nFurther, prior linguistics research has shown that\nDHH community members can experience higher\ncognitive loads while reading compared to signing\n(Traxler, 2000; Kelly, 2003; Luckner and Handley,\n2008). In this paper, we investigate effective strate-\ngies of multimodal information presentation for the\nDHH to reduce cognitive load. With repeated con-\nsultations with cognitive scientists, we design the\nlayout of our system's user interface specifically\naround the cognitive load of signers (see Figure 2).\nWe focus on creating a framework that is appli-\ncable to a large-scale global platform (in our case,\nAmazon Alexa), making it impossible at this time\nto access camera footage. We investigate ways of\nreceiving input with other modalities instead of\nvoice commands and without camera access. This\nleads us to focus on the task of instruction gener-\nation and delivery rather than recognizing signs\nproduced by the user. We receive input from the\nuser via touchscreen controls of Amazon Alexa\nEcho Show devices so that signers can interact\nwithout using voice commands (see Figure 2 for\nthe touch screen user interfaces where the user can\ninteract via buttons to select tasks and navigate\ninstructions).\nTo address all of the aforementioned points, in\nthe following sections, we introduce the compo-\nnents of our framework. Our detailed contributions\nare as follows:\n1. We design a multimodal task-oriented dia-\nlogue system with signed instructions and de-\nploy it on multimodal devices.\n2. We use co-design to build our system, actively\ninvolving community members in the design,\ndevelopment, and evaluation, ensuring our so-\nlutions positively impact the community.\n3. We implement a novel Large Language Model\n(LLM)-based instruction generation technique\nfor zero-shot text-to-sign translation. We use\nlinguistics rules and cognitive science-based\nheuristics for this translation.\n4. We make available a standalone library to\ntranslate instruction texts into signed instruc-\ntion videos, and we release our dataset used\nfor the top 200 signs in cooking and wikiHow\ndomains."}, {"title": "2 Related Work", "content": "We hope this effort brings more focus to the\nneeds of signers and will be a step towards making\nlarge-scale dialogue systems more accessible to all\nusers.\nWith the rise of voice assistant devices, the DHH\ncommunity has been mostly left behind. Yet, there\nhave been multiple lines of work to make them\nmore accessible. Accessibility of personal assistant\ndevices to the Deaf and Hard of Hearing commu-\nnity has been assessed multiple times before by\nGlasser et al. (2017, 2020); Bragg et al. (2020).\nIn addition, design approaches incorporating the\nDHH community have been proposed by Anind-\nhita and Lestari (2016); Hariharan et al. (2018). We\nbuild on these in our system design.\nMost of the current work in interactive system\ndesign focuses on sign recognition with the help\nof cameras. For instance, in Wojtanowski et al.\n(2020) Wizard-of-Oz studies have been done where\nAlexa is combined with a camera to detect signs.\nIn SIGNS project\u00b9, Alexa recognizes specific ges-\ntures for simple task completion (such as getting\nthe weather forecast with a specific gesture), and\nHuang et al. recognized signs for a healing robot.\nEven though these systems provide a means for rec-\nognizing signs, they fall short in generating signs,\nwhich we focus on in this paper.\nThere has been some line of work by Nasihati Gi-\nlani et al. (2019) in generating avatars for 6-month-\nold babies to learn ASL. Also, Hr\u00faz et al. (2011)\ndeployed a kiosk with sign recognition and genera-\ntion capabilities for Czech Sign Language. How-\never, these have not resulted in a widely available\nsystem.\nOn the other hand, sign language processing has\nbeen widely studied under controlled conditions.\nEven though sign language generation and trans-\nlation tasks are still open problems, transformer-\nbased models in Yin and Read (2020); Yin et al.\n(2021); Moryossef et al. (2021); Inan et al. (2022);\nM\u00fcller et al. (2023); Lin et al. (2023); Viegas et al.\n(2023) have shown that it is possible to automate\nthem better. As a core contribution, we present a\nframework to apply any of these models in large-\nscale interactive environments.\nIn order to make our system useful for signers,\nwe need to mitigate their cognitive load interpret-\ning instructions from multimodal devices. Models"}, {"title": "3 A Goal-Oriented Dialogue System with\nSigned Instructions", "content": "We design a multimodal goal-oriented dialogue\nsystem as part of the Alexa Prize TaskBot Chal-\nlenge 2 (Agichtein et al., 2023) and incorporate\nsigned instructions. The main dialogue system\nthat we develop follows a typical modular design:\nNatural Language Understanding (NLU), Dialogue\nManager (DM), and Natural Language Generation\n(NLG). In this setting, we embed signed instruc-\ntions into the multimodal NLG module (Figure 3).\nDue to privacy regulations, Alexa does not allow\nthird parties to process user gestures and videos.\nHence, to increase accessibility for signers, we\nchoose to generate signed instructions instead of\nrecognizing signs. To support users who cannot\u2014\nor prefer not to\u2014provide voice input, our system\nhas a scrollable touchscreen with buttons. This\nenables us to have a full dialogue system for signers\nwhile complying with regulations."}, {"title": "3.1 Task Description", "content": "We take as input a task JSON with step-by-step\nEnglish text instructions, images, title, main im-\nage, and ingredients and output a JSON array of\nuser interface screens corresponding to the gloss\ntranslations for each step and their corresponding\nsign videos (see Appendix A). The tasks are in the\ndomains of cooking, home improvement, arts and\ncrafts, and gardening. We provide our signed in-\nstruction generation as a standalone library for the\ncamera-ready version of this paper."}, {"title": "3.2\nCommunity Co-Design", "content": "To inform our system design choices, we connect\nwith collaborators from the Deaf and Hard of Hear-\ning (DHH) signing community at Gallaudet Uni-\nversity (a prestigious higher education institution\nchartered for the DHH community). We incorpo-\nrate the feedback from signers into the system's\ndesign.\nThe feedback incorporated into our design pro-\ncess includes considering the cognitive load of sign-"}, {"title": "4 Our Signed Instruction Framework", "content": "We employ the framework shown in Figure 1 to\ngenerate signed instructions. We first retrieve in-\nstructions for a given task, and then we convert each\nstep into gloss tokens, which are intermediary tex-\ntual representations using rule-based sign language\ntranslation algorithms and LLMs. Afterward, we\nsegment each instruction into separate gloss tokens,\nretrieve sign videos for each, and stitch them back-\nto-back to create a continuous video sequence. For\neach step, we display this sequence of videos and a\npicture of the step. The picture for each step gener-\nally shows the result of the action as described in\nthe sign instructions. This approach is summarized\nin Algorithm 1."}, {"title": "4.1 Large Language Model Translation", "content": "For the translation of spoken English instructions\nto textual representations of ASL (glosses), we\nprompt LLMs. Multiple methods exist in im-\nplementing text-to-gloss translation: human an-\nnotation, rule-based automatic translation with\nheuristics (Othman and Jemni, 2012a), fine-tuned\ntransformer-based models (Camgoz et al., 2018;\nYin and Read, 2020), and prompting LLMs (Lee\net al.). We make our system adaptable to all of these\nalternatives for text-to-gloss translation. Any one\nof these models can be plugged into line 4 of Algo-\nrithm 1. We choose LLM translation for our current\nsystem due to its scalability, translation understand-\nability, and ability to adapt to out-of-domain text."}, {"title": "4.2 Sign Video Processing", "content": "We process the videos in four steps. First, we col-\nlect sign videos corresponding to all the glosses in\nour instruction set from an online platform. Then\nwe store these videos, retrieve them on the fly while\npresenting instructions, and stitch them together.\nWe give the details of these steps in the following\nparagraphs.\nSign Video Collection For video collection, we\nuse widely available American Sign Language sign\ndictionary videos from video sharing platforms\nwith Creative Commons licenses online \u2076. We\nmainly use videos from Lifeprint, but if they do\nnot contain a specific sign video, we use the ASL-\nDictionary on YouTube as the backup source. If\nneither of these sources has a sign available, we\nfirst check if the gloss can be deconstructed into\nother signs or fingerspelled. If so, we check the\nvideos for the deconstructed versions and concate-\nnate them into a single video. If these options are\nnot available and the gloss is crucial to the meaning\nof the instruction, then we search for a synonym.\nIf it is not crucial to the meaning of the instruction,\nthen we drop the gloss.\nVideo Storage We generate a dictionary for all\nthe available sign glosses (found in Appendix Sec-\ntion A) and upload all the videos with their gloss\nas their filename to an Amazon AWS S3 bucket for\nstorage.\nGloss-by-Gloss Sign Retrieval During a user's\nlive use of the system for signed instructions, we\nretrieve videos on a token level using the video\nURL by cross-referencing its gloss filename. As\nthe last step, after retrieving all the video URLs\non the fly for each gloss in each instruction, we\nconcatenate all of the URLs corresponding to the\nglosses together and then present them on the user\ninterface of the app as a single stream of a video\n(see Figure 2)."}, {"title": "5 System Evaluation", "content": "We evaluate our system both quantitatively and\nqualitatively. Because this is the first deployment of\na task-oriented signed multimodal dialogue system,\nwe chiefly compare the system with the non-signed\nportion of our task-oriented dialogue system. We\nfirst evaluate the performance of our LLM text-to-\ngloss translation and discuss the trade-offs of using\nan LLM for translation. Then, we evaluate our\nalgorithm using traditional information retrieval\nmetrics. Finally, we compare user ratings and pro-\nvide detailed qualitative analyses by an expert who\nis fluent in ASL."}, {"title": "Text-to-Gloss Translation Analysis", "content": "In this sec-\ntion, we analyze the performance of LLM-based\ntranslations using traditional automatic text metrics\n(see Table 1). As also described in section \u00a7 4.1, we\nexperiment with two translation strategies: 1) LLM\ntranslations and 2) rule-based gloss translations\nwith heuristics. We use the rule-based heuristics\nstrategy as ground truth in our results here because\nno human-annotated ASL ground truth exists for\nour datasets, and the accuracy of rule-based transla-\ntions is high when compared to human annotations\nin the works of Othman and Jemni (2012b, 2019)."}, {"title": "Retrieval Metrics", "content": "No automatic evaluation\nmechanism exists for signed interactive systems;\nhence, in this section, we introduce two retrieval\nmetrics-Hit Rate and Recall@1-for our Signed\nInstruction Retrieval Algorithm (see Algorithm 1)\nwith the two translation modules separately. Fur-\nthermore, we also present an analysis of the\nchanges in Hit Rate and Recall@1 in response to\nincreases in the available video dataset size in Fig-\nure 4.\nWe use the following simplified definitions of\nHit Rate and Recall@1:\nHit Rate =  $\\frac{\\text{#glosses w/ videos}}{\\text{total # of glosses}}$ (1)\nRecall@1 =  $\\frac{\\text{#glosses w/ videos}}{\\text{# synonyms of glosses w/o videos + #glosses w/ videos}}$ (2)\nEssentially, Hit Rate measures how accurate\nthe system is in finding videos for a given token,\nand Recall@1 tells how precise the system selects\nvideos corresponding to a token among a set of"}, {"title": "User Rating Comparisons", "content": "Our system inter-\nacted with a large number of public users for over\na period of six months. Because this is the first\ntask-oriented dialogue system with signed instruc-\ntions, it increases our user outreach on international\nplatforms by a large margin. However, adding this\nfunctionality could decrease overall user ratings if\nthey do not deem the interface usable or are unsure\nabout what ASL is. Thus, we examine the ratings\nbefore and after adding the signed instructions to\nour system. As shown in Appendix 7, our user\nratings remain constant after adding support for\nthis feature. Thus, we find that, besides making\ntask-oriented systems accessible to a larger audi-\nence, adding support for signed instructions does\nnot decrease user ratings."}, {"title": "Expert Qualitative Analysis", "content": "One author fluent\nin ASL evaluated the system with special regard\nto the usability and clarity of the information pre-\nsented. This evaluator noted two primary strengths:\n1) the multimodal instructional support provided\nby having both the ASL descriptions and the in-\nstructional images available, particularly for the\nstep-by-step tasks such as origami folding; 2) the\nease of processing and attending to multiple modal-\nities given the clear layout without overwhelming\nthe user. To expand, giving the user the option\nto attend to the signed content or the referent of\nthe images (e.g., step-by-step origami folding) al-\nlowed them to rely on each form of information\nto the extent they prefer. The clear layout does\nnot overwhelm the user with too many streams of\ninformation. It also allows for sufficient process-\ning of either sign videos, images, or both without\ndistracting the user.\nThe primary limitation of the current system lies\nin the segmented nature of the ASL videos. Cur-\nrently, there is a lack of smooth transitions between\nsigns, and different signers present each sign within\none instruction. The flow of the signs appears dis-\njointed, consequently impeding clear understand-\ning. The absence of step-by-step visuals in certain\ntasks necessitates increased reliance on signing.\nThe disjointed nature of the current signing videos\nrendered some tasks less comprehensible.\nOverall, the multimodal presentation of signing\nalongside informative images enhances accessibil-\nity and suggests that a dynamic display of signed\ncontent will greatly enhance future task-oriented\ndialogue systems. For future iterations of our sys-\ntem, we plan to incorporate either human models\nsigning the entire content or synthesized avatars\n(Quandt, 2020; Quandt et al., 2022)."}, {"title": "6 Conclusion", "content": "In this work, we discussed a multimodal, task-\noriented dialogue system designed to generate\nASL instructions on a platform with global reach.\nEmphasizing the critical importance of Deaf and\nHard-of-Hearing (DHH) community engagement\nthroughout the development cycle, our approach\nintegrates extensive feedback from both the signing\ncommunity and experts in the field. Our system not\nonly marks a significant technological milestone\nbut also enriches the dialogue on how video-based\nASL instruction delivery can be effectively scaled\ninternationally. We observed a nuanced prefer-\nence among signers for avatar-based instructions\u2014\na finding underscored by our expert analysis. Our\nsystem has improved the landscape of conversa-\ntional AI, making it accessible and responsive to\nthe unique needs of the DHH community.\nWe make the code available for our pipeline\nand encourage future researchers to incorporate\nit into their own task-oriented systems to increase\naccessibility. We hope that this system is a step\ntowards developing dialogue systems that can un-\nderstand and generate signs for all signed lan-\nguages. We encourage everybody to interact with\nsigned tasks by visiting https://huggingface.\nco/spaces/merterm/signed-instructions."}, {"title": "B Rule-based Gloss Translation\nAlgorithm", "content": "We give the pseudocode for the rule-based heuris-\ntics algorithm as follows:"}, {"title": "C Detailed Mathematical Definitions for\nRetrieval Metrics", "content": "To define Hit Rate and Recall@1 more precisely,\nwe first introduce some requisite definitions:\n\u2022 D: set of glosses in our dictionary\n\u2022 n: total number of task instructions\n\u2022 I = {10, 11, ..., in }: set of all task instructions\n\u2022\nmk: total number of glosses in instruction k\n\u2022 ik \u2208 I =< 9k0, 9k1, \u2026, 9kmk >\n\u2022\n9kl \u2208 ik: gloss in instruction ik (ordered)\n\u2022 syn(g): the set of synonyms found for gloss\ng using wordnet.synsets\nWe formalize our simplified definitions of Hit\nRate and Recall@1 below, using our notation. Note\nthat because we take into account repeated glosses\nin our instruction set, the sets below are multisets\nand thus contain repeated elements that are factored\ninto the cardinality of the set."}, {"title": "D Detailed Examples for Retrieval\nMetrics", "content": "For example, for the instruction, \u201cChop choco-\nlate and add to batter. Stir until incorporated.\u201d,\nthe LLM generates, \u201cCHOCOLATE CHOP ADD\nDOUGH MIX STIR\u201d, while heuristics generates\n\u201cCHOP CHOCOLATE ADD BATTER STIR UNTIL\nINCORPORATE\u201d. Here, it can be seen that LLM\nproduces DOUGH (a synonym of \u201cbatter\u201d for our\npurposes), while heuristics directly uses the same\nwording. This adds diversity to the generated\nglosses, and as the number of videos increases, it\npositively affects the score of LLMs. For the heuris-\ntics algorithm, as the tokens are never changed into\nsynonyms, even after a lot of videos are added to\nthe set, the algorithm cannot retrieve videos and\ngets lower Recall@1 scores."}, {"title": "E Interface Details", "content": "We show more screenshots of details in the interface in Figures 5, and 6."}, {"title": "F User Rating Analysis", "content": "We show a plot of 7-day averages of user ratings before and after adding support for signed instructions in\nFigure 7."}]}