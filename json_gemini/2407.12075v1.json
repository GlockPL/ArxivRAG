{"title": "Tiled Bit Networks: Sub-Bit Neural Network Compression Through Reuse of Learnable Binary Vectors", "authors": ["Matt Gorbett", "Hossein Shirazi", "Indrakshi Ray"], "abstract": "Binary Neural Networks (BNNs) enable efficient deep learning by saving on storage and computational costs. However, as the size of neural networks continues to grow, meeting computational requirements remains a challenge. In this work, we propose a new form of quantization to tile neural network layers with sequences of bits to achieve sub-bit compression of binary-weighted neural networks. The method learns binary vectors (i.e. tiles) to populate each layer of a model via aggregation and reshaping operations. During inference, the method reuses a single tile per layer to represent the full tensor. We employ the approach to both fully-connected and convolutional layers, which make up the breadth of space in most neural architectures. Empirically, the approach achieves near full-precision performance on a diverse range of architectures (CNNs, Transformers, MLPs) and tasks (classification, segmentation, and time series forecasting) with up to an 8x reduction in size compared to binary-weighted models. We provide two implementations for Tiled Bit Networks: 1) we deploy the model to a microcontroller to assess its feasibility in resource-constrained environments, and 2) a GPU-compatible inference kernel to facilitate the reuse of a single tile per layer in memory.", "sections": [{"title": "1 INTRODUCTION", "content": "The progress of modern machine learning can be largely attributed to the exponential growth of Deep Neural Networks (DNNs). Empirically, the capacity of DNNs is expanding at an astounding rate [3], a practice supported by theory showing that sufficiently over-parameterized models are in fact necessary for deep learning [1, 24]. Alongside this progress, the growing presence of resource-constrained machines (e.g. embedded devices, cell phones) has created unique opportunities to deploy increasingly large DNNs in novel environments. Consequently, maximizing the computational efficiency of neural networks is a relevant challenge at various scales of application.\nEfforts toward efficient deep learning span a broad range of techniques such as architectural design [23, 50], neural architecture search [33], knowledge distillation [22, 51], and quantization [6, 25, 62]. Quantization, which converts high precision neural network weights into discrete values, has achieved success in practical applications [40, 49], and has been applied down to the scale of Binary Neural Networks (BNNs) where the weights (and often activations) of a model are single bit values [7].\nWhile BNNs have been established as a practical and extreme form of quantization, an emerging line of research has gone a step further with sub-bit neural network compression, which requires less than a single bit per model parameter. Wang et. al [58] first observed that the discrete set of binary convolutional kernels tend to cluster into a smaller subset; as a result, they devised a training regime to use a smaller set of kernels. Subsequent work has achieved improved compression by leveraging properties of binary convolutional kernels using minimum spanning trees [56] and sparse kernel selection [57].\nIndependent from previous approaches, this work proposes tiling neural networks with binary weights to achieve sub-bit memory and storage compression of model parameters. Tiled Bit Networks (TBNs) learn binary sequences (tiles) to fill in the weights of a DNNs layers during training. \nUnique from previous work that leverages the properties of convolutional kernels to achieve sub-bit compression, TBNs work on both fully-connected and convolutional layers, a relevant application for modern architectures as depicted in Figure 2. We test the approach on CNNs, Transformers, PointNets, and MLPs, enabling for the first time sub-bit DNN compression on models with high proportions of fully-connected parameters. Compared to previous approaches, TBNs achieve better or similar performance on image classification tasks. Empirically, across 2D and 3D image classification, TBNs achieve performance on par with Binary Weight Neural Networks (BWNNs) with only a fraction of the total parameters less than a single bit is stored and accessed per model parameter. TBNs additionally achieve strong performance on semantic and part segmentation tasks as well as time series forecasting.\nWe provide two implementations for model inference in Section 5, which both require only a single tile per model layer in memory. First, we implement a lightweight TBN for deployment on a microcontroller, showing that the algorithm reduces memory and storage consumption compared to BWNNs. We also implement"}, {"title": "2 RELATED WORK", "content": "Quantized and Binary Neural Networks DNN quantization reduces full-precision weights and activations to discrete and lower precision values to enhance model storage, memory, and inference speed [31, 64]. The most extreme quantization was conventionally thought to be binarization, where weights can only be \u00b11 [43]. Binarization helps reduce computation, however, it often reduces model accuracy. Several works attempt to alleviate this issue such as XNOR-Net, which used channel-wise scaling factors for BWNNS and BNNs [47]. IR-Net [44] preserved the information by maximizing the entropy of the information while minimizing the quantization error. ReActNet used generalized activation functions to get within 3% of full-precision accuracy on ImageNet [36]; Shang et al. utilized contrastive learning to learn BNNs [52]. Xu et al. proposed FDA, which estimates sign function gradients in a Fourier frequency domain [59]; Xu et al. proposed ReCU which introduces a rectified clamp unit to address dead weights [60].\nWe note that the BNN research covered in this section uses binary activations as well as binary weights, and as a result, achieves significant memory and speed improvements. TBNs use full-precision activations, however, still achieve storage and memory improvements from using a single tile per layer. We denote BNNs that have full-precision activations as Binary Weight Neural Networks (BWNNs). We indicate whether previous BNN algorithms have binary activation's in benchmark experiments in Section 4.\nSub-Bit Quantization Sub-bit DNN compression reduces model sizes to less than a single bit per model parameter. Kim et al. [26] proposed a kernel decomposition to reduce computations in binary CNNs. FleXOR [29] used an encryption technique to store binary sequences. Wang et al. [58] observed that the full set of binary convolutional kernels tends to cluster into a subset; they formulate a training technique to find the best subsets of kernels. Lan et al. [27] stack convolutional filters to achieve sub-bit compression. Wang et al. [57] group kernels into binary codebooks for sparse kernel selection. Finally, Vo et al. [56] propose minimum spanning tree compression, which takes advantage of the observation that output channels in binary convolutions can be computed using another output channel and XNOR operations.\nPrevious sub-bit compression approaches are distinct from TBNs: initial work was based on removing redundancy and encrypting weights; CNN-based approaches are based on utilizing properties of binary convolutional kernels. TBNs, on the other hand, achieve substantial compression on both fully-connected and convolutional layers, and can be applied to multiple architectures (CNNs, Transformers, MLPs).\nEfficient Machine Learning Model quantization is a sub-field of efficient deep learning, which encompasses multiple areas not covered in this work such as low rank factorization [5, 28, 30], structured and unstructured pruning [11, 17, 19, 21], knowledge distillation [22, 48], and memory efficiency through input patching [32] and attention tiling [8].\nEmbedded and On-Device Machine Learning The size and computational requirements of DNNs has motivated researchers to improve the compatibility of large models with hardware such as mobile phones and embedded devices (e.g. FGPAs, IoT Sensors) [4]. Architectural optimizations such as MobileNet [50], ShuffleNet [37], and MCUNet [33, 34] have been achieved success, including to ease memory constraints via layer patching [32]."}, {"title": "3 METHOD", "content": "Tiled Bit Networks are constructed from a standard neural network with layers 1, 2, 1..., L. We consider fully-connected and convolutional layers in this work since these layers generally make up the breadth of DNNs weights. We do not consider bias parameters in this work.\nIn this section, we describe the training process for TBNs, which involves learning full-precision parameters (W) and applying aggregation and reshaping to create the tile vectors t. We then describe our approach to tile-wise scaling, the second step of training TBNs. Finally, we describe training hyperparameters and their default settings.\nLayer-Wise Tiling The key to our approach is that we learn tile vectors $t^{[1]}, t^{[1]}, ..., t^{[L]}$ for each layer of our network. We initialize our model with full-precision values for each layer similar to standard training, creating a weight tensor $W^{[l]} \\in \\mathbb{R}^{d_1\\times...d_k}$ for layer l, where $d_k$ is the dimensionality of the tensor (e.g., a fully-connected layer has k = 2). The total of elements in the tensor is $N = \\Pi_{i=1}^k d_i$. During training, we update $W^{[l]}$ via stochastic gradient descent. Our goal is to compress $W^{[l]}$ by a factor of p, where size N is divisible by p such that p x q = N. To achieve this we reshape tensor $W^{[l]}$ as a $p \\times q$ dimensional matrix $W^{[l]*}$ during forward propagation:\n$W^{[1]} \\in \\mathbb{R}^{d_1,...d_k} \\rightarrow W^{[1]*} \\in \\mathbb{R}^{P\\times q}$ (1)\nWe then sum the reshaped weight tensor $W^{[l]*}$ along the p dimension to create a vector $s \\in \\mathbb{R}^q$:\n$s = \\begin{bmatrix} \\sum_{j=1}^{p}W^{[1]*}_{1j} \\\\ \\sum_{j=1}^{p}W^{[1]*}_{2j} \\\\ ... \\\\ \\sum_{j=1}^{p}W^{[1]*}_{pj} \\end{bmatrix} = \\begin{bmatrix} s_1 \\\\ s_2 \\\\ ... \\\\ s_q \\end{bmatrix}$ (2)\nWe next create tile $t^{[1]} = [t_1, t_2, t_i...t_q]$ for a given layer by applying a threshold function to determine the binary value for each $s_i$ in s:\n$t_i = \\begin{cases} 1, & \\text{if } s_i > 0 \\\\ -1, & \\text{otherwise} \\end{cases}$ (3)\nTile $t^{[1]}$ is then replicated p times to create tile vector $b^{[1]} \\in \\mathbb{R}^N$. Formally, let $1_n$ be a vector of ones with size N. The tiling operation creates vector $b^{[1]}$ as:\n$b^{[1]} = 1_N \\otimes t^{[1]}$ (4)\nwhere $\\otimes$ is the Kronecker product. We create our final binary weight tensor $B^{[1]} \\in \\{-1, 1\\}^{d_1,\u2026\u2026.d_k}$ by reshaping vector $b^{[1]}$:\n$B^{[1]} = vec^{-1}_{d_1,\u2026d_k} (b^{[1]})$ (5)\nwhere $vec^{-1}_{d_1,\u2026d_k} (\\cdot)$ denotes a vector to k-dimensional tensor operation.\nWe note that computing binary parameters $B^{[1]}$ involves non-differentiable operations during forward propagation. As a result, we utilize straight-through gradient estimation, where the gradients of the model are passed-through the non-differentiable operator during backpropagation [2]. To achieve this we implement Equations (1) to (5) in the forward pass of a customized differentiation engine, and on backpropagation we pass the gradients through the customized module to update $W^{[1]}$.\nPutting it together, the tiled model f(\u00b7) can be trained with parameters $W^{[1]}$ (to compute $B^{[1]}$) and inputs x, producing an output y which serves as a continuous, differentiable approximation of a tiled neural network. In the context of straight-through gradient estimation, y is used during backpropagation to compute the gradient of loss L with respect to the parameter $W^{[1]}$:\n$\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial y^{[l]}} \\frac{\\partial y^{[l]}}{\\partial W^{[1]}} \\approx \\frac{\\partial L}{\\partial y^{[l]}} \\frac{\\partial y^{[l]}}{\\partial B^{[1]}} \\frac{\\partial B^{[l]}}{\\partial W^{[1]}}$ (6)\nwhere $y^{[l]}$ is the output of layer l prior to the activation. $\\frac{\\partial y^{[l]}}{\\partial B^{[l]}}$ involves the thresholding, tiling, and reshaping operations.\nTile-wise Scalars Similar to XNORNet [47], we scale $B^{[1]}$ by \u03b1. Rastegari et al. [47] derived the optimal scaling factor of a binary weight filter as the average absolute value of a weight, a method widely used in other research [9, 38, 43]. We can use parameter $W^{[1]}$ to compute the scalar since its non-aggregated size is the same as a standard weight tensor:\n$\\alpha = \\frac{||W^{[1]}||_1}{N}$ (7)\nWe additionally experiment with an independent tensor, denoted $A^{[1]} \\in \\mathbb{R}^{d_1\\times...d_k}$, to exclusively compute the \u03b1 scalar. We observe a slight performance benefit from using $A^{[1]}$ in addition to $W^{[1]}$. We add this option as a hyperparameter to our models.\nAnother hyperparameter setting for TBNs calculates one \u03b1 for each tile $t_1, t_2...t_p$ in layer l by utilizing the ith set of the flattened tensor $A^{[1]}$ (or $W^{[1]}$), and calculating $\u03b1_i$ using only these values. This represents the optimal scalar for that particular tile. To do this, we can reshape $A^{[1]}$ to get the values corresponding to the pth tile of layer l:\n$A^{[1]} \\in \\mathbb{R}^{d_1,...d_k} \\rightarrow A^{[1]*} \\in \\mathbb{R}^{qXp}$ (8)\nNext, similar to Equation 7 we calculate the 1-norm for each segment of values corresponding to the ith tile in $A^{[1]}$. We divide this number by q (the size of each tile) to give us $\u03b1_1, \u03b1_2, ...\u03b1_p$:\n$\u03b1_p = \\frac{||A^{[1]*}_p||_1}{q}$ (9)\nEach \u03b1 gets multiplied element-wise by its corresponding tile in $B^{[1]}$. The resulting tensor, $\\hat{B}^{[1]}$ is used for the operation on the inputs. Equation 9 is depicted in Figure 4.\nAfter training is complete, we save a vector of size q for each layer along with full-precision scalars (\u03b1s). We describe our implementation in Section 5.\nHyperparameter Settings We test our models with several hyperparameter configurations to assure the best performance. TBNS primarily contains three hyperparameters:\n(1) Minimum layer size for tiling, \u03bb. We set a minimum size N of a DNN layer required for tiling to be performed. Tiling smaller layers causes a drop in performance (Figure 7). Default: \u03bb=64,000, ImageNet models: \u03bb=150,000, Time-Series models: \u03bb=32,000\n(2) Parameter W for tiling and A for calculating \u03b1. W is used to learn a tile for each layer; it can also be used to calculate a scalars. Alternatively, we propose a separate parameter A to compute as independently, which exhibits a small performance gain. Default: A for calculating \u03b1. For ImageNet, we use W.\n(3) Tile-wise \u03b1s. We experiment with calculating a single \u03b1 per layer as well as calculating \u03b1 for each tile in a layer. In some settings multiple \u03b1s perform better. Default: Single \u03b1 per layer."}, {"title": "4 EXPERIMENTS", "content": "We next detail our experiments across a range of architectures, datasets, and tasks. We test TBNs on CNNs as well as fully-connected models such as PointNet and Transformers.\nIn this section, we compare TBNs against previous sub-bit compression techniques for Convolutional Neural Networks (CNNs) including SNN [58], MST [56], and Spark [57]. CNN's are the only"}, {"title": "4.1 CNN Architectures", "content": "In this section, we compare TBNs against previous sub-bit compression techniques for Convolutional Neural Networks (CNNs) including SNN [58], MST [56], and Spark [57]. CNN's are the only"}, {"title": "4.2 MLP-Based Architectures", "content": "In addition to CNN architectures, we consider MLP models which contain a high proportion of fully-connected and 1 \u00d7 1 convolutional layers. PointNet is a well-established model for unified tasks in classification, part segmentation, and semantic segmentation [42]. The model takes point cloud data from 3D representations. To assess PointNet we use datasets ModelNet40, Shapenet, and S3DIS, which are each designed for a specific task. We denote segmentation performance with Intersection over Union (IoU), and class average IoU. IoU is the ratio of the intersection area between the predicted and ground truth regions to the union area of both regions. Average IoU is calculated across all instances or regions in the dataset, whereas class average IoU is calculated separately for each class and then averaged across all classes.\nWe derive MLP experiments from BiBench [45], who provide a diverse set of tasks to evaluate BNNs. We note that the benchmarks provided in BiBench assess binarizing pretrained models."}, {"title": "4.3 Transformers", "content": "In our next set of experiments, we assess TBNs on Transformers. It was noted in the BNN benchmark work BiBench [45] that Transformers perform poorly on BNNs. In particular, none of the binarization algorithms tested in BiBench achieved more than 70% of"}, {"title": "4.3.1 Vision Transformers.", "content": "For Vision Transformers We train a ViT [10], which uses image patching along with the traditional attention, and Swin-t [35], which uses hierarchical partitioning of the image into patches with merging. We train the models from scratch on the CIFAR-10 and ImageNet datasets. Note that, training the models from scratch, rather than fine-tuning, can result in sub-optimal performance in Transformers [10]."}, {"title": "4.3.2 Time Series Transformers.", "content": "In our next set of experiments we explore Transformer TBNs for multivariate time series forecasting. Transformer encoders have been shown to be robust to model compression for time series learning tasks including forecasting [17]; we aim to assess whether we can compress similar models using TBNs.\nOur experiments utilize the ECL (Electricity) dataset and the Weather dataset similar to previous works [17, 63]. ECL has 321 features and Weather has 7 features. We use a Transformer encoder similar to previous work [61], using a dimensionality of 512 for the Electricity dataset and 128 for the Weather dataset. Similar to previous work, we assess the performance of the models use Mean Squared Error.\nResults. We train each of the models five times and report the average MSE along with the standard deviation in Table 5. Notably, TBNs performed slightly better than full precision and binary models on the electricity dataset. On the weather dataset, we set \u03bb=32,000 since all the layers are smaller than this. We find that the model is able to converge to full precision accuracy. However, when we lower \u03bb to 16,000 model performance suffers."}, {"title": "5 IMPLEMENTATION", "content": "We cover two implementations of TBNs for model inference: 1) a C implementation which we deploy on a microcontroller, and 2) GPU-compatible inference kernels. Both methods implement reuse of a single tile per layer to achieve memory and storage savings."}, {"title": "5.1 Microcontroller Deployment", "content": "We first implement an inference engine to run on an Arduino microcontroller. The microcontroller has 1MB of storage and 250KB of memory, making it practical for a lightweight model. We program an MLP model trained on the MNIST dataset with 128 hidden neurons and a fused ReLU nonlinearity. We implement both a standard BWNN and a TBN. Our TBN has a compression rate of 4 and uses multiple \u03b1s (one for each tile). To implement the TBN we first train our model in PyTorch [41], and then convert the layer tiles and \u03b1 scalars to C data types. We implement a fully-connected kernel in C as detailed in Algorithm 1. We develop a fully binarized kernel by packing binary weights into unsigned 8-bit integers and use bit-masking to extract the correct values during inference.\nWe assess the speed, memory, and storage space of each model. To assess speed, we report the Frames Per Second (FPS), which measures the number of times the program can execute on a sample per second. We measure FPS using a provided script which executes the compiled model 1000 times and reports the mean and standard deviation across five runs. We report memory as the maximum"}, {"title": "5.2 GPU Inference Kernel", "content": "Our second implementation utilizes the open-source Triton library, which enables us to write customized CUDA kernels in Python to run on GPUs. Native PyTorch does not allow the reuse of a single tile without allocating new memory for the full layers parameters (p tiles) - memory reuse between tensors with incompatible strides is not possible in standard operations. Leveraging Triton provides more control over lower-level memory allocation.\nWe implement fully-connected kernels using both full-precision (32-bit) weights as well as binary (1-bit) weights. For the former use case, we experiment tiling layers with full-precision weights to compare against standard 32-bit models. For the latter, we pack bit parameters into unsigned 8-bit integers, and unpack them during inference. Bit values are packed by row, converting an m \u00d7 n matrix to size (\\frac{m}{8}) x n.\nWe implement both kernels using the matrix multiplication functionality provided by Triton. It uses block-level matrix multiplication and has a similar performance to the optimized cuBLAS library. Our tiling implementation converts an m \u00d7 n matrix to m \u00d7 q (we compress the second dimension). For pointer arithmetic, we reuse the same m \u00d7 q tile for multiple computations. In other words, fully-connected tiling is a matrix-to-matrix implementation rather than matrix-to-vector implementation.\nResults. We measure the GPU memory usage for inference on a single image in Figure 5 and Table 7. Figure 5 profiles GPU memory usage through each layer of the model, while Table 7 examines the peak memory and memory occupied by model parameters in an ImageNet ViT.\nThe results in Figure 5 reflect the memory savings by only loading a single tile per model layer for the standard full-precision kernel as well as the tiled full-precision kernel. Specifically, during inference the PyTorch library loads weights for all layers into memory. It allocates new memory for the input and output activations of each"}, {"title": "6 ABLATION STUDIES", "content": "We perform several ablation study's on TBNs, first exploring the effect of layer size on MLPMixers [54] and ConvMixers [55], and then analyzing the effects of various hyperparameters in MLPMixers and ResNets."}, {"title": "6.1 Effects of Layer Size", "content": "We next look at MLPMixers [54] and ConvMixers [55] for the CIFAR-10 classification task. ConvMixer has shown potential to compete with ViTs on complex tasks, while MLPMixers provide us with another opportunity to test fully-connected models.\nIn Figure 6 we visualize the performance of TBNs at tiling compression rates up to 32x for both models. We find that the ConvMixer accuracy degrades quickly after 4x compression. When analyzing the architecture, we find that the largest layer has just 65,536 parameters. Moreover, many layers have less than 65k parameters and don't fulfill the minimum layer size for compression (\u03bb), resulting in limited reductions in parameter count along with high performance degradation. MLPMixer, on the other hand, has layers with 131k elements, and resulted in a more modest performance degradation at higher compression. Assessing other architectures, we found that the CNN, PointNet, and Transformer models contained layers significantly above \u03bb."}, {"title": "6.2 Hyperparameter Configuration Analysis", "content": "We study the effects of three hyperparameters of TBNs as described in Section 3: 1) \u03bb (minimum layer size for tiling), 2) W for tiling and A for computing \u03b1, and 3) Multiple versus single \u03b1 scalars. For \u03bb, we test global tiling, where all layers are compressed. We compare this to our default training which sets \u03bb to 64k. For the second hyperparameter, we test two settings: the first uses W for both learning the binary tile and calculating \u03b1; the second uses parameter A for calculating \u03b1. We denote this setting as W or W + A. For the third parameter, we test a single \u03b1 per layer and compare it to computing \u03b1 for each tile. We perform these experiments using the W + A setting.\nFigure 7 shows the effects of the different hyperparameters on both the ResNet50 and MLPMixer models on the test accuracy throughout training. We show that a global tiling factor on ResNet50 causes a significant decrease in performance. Next, we find the model converges best when given a separate parameter A to calculate \u03b1. Moreover, when calculating an \u03b1 for each tile, we observe a slight increase in performance."}, {"title": "7 DISCUSSION", "content": "We propose Tiled Bit Networks for learning binary vectors to fill in the weights of a neural networks layers during training, achieving sub-bit compression on model parameters during inference. TBNs work on both convolutional and fully-connected layers, and are applicable to CNNs, MLPs, and Transformer-based architectures.\nTiled Bit Networks have the potential to democratize deep learning by providing resource constrained devices access to larger scale models. For example, TBNs compress the 23.5 million parameter ResNet50 model to under 6.1 million bits, a size which can fit into a microcontroller with 1MB of storage. Moreover, the algorithm unlocks potential for stronger sub-bit compression on other architectures, particularly those with fully-connected layers. The innovation not only increases the accessibility of deep learning use but also has the potential to contribute to environmental sustainability by reducing the carbon footprint associated with model size and computational complexity.\nFuture Work A natural direction for future work is to apply TBNs to models with binary weights and binary activations. BNNs with binary activations maximize memory savings and improve speed. We would also like to test the approach on larger models (e.g. LLMs), where additional algorithmic modifications (such as full-precision parameter tiling) may enhance performance. Additionally, specialized kernels could be implemented to maximize the efficiency of TBNs with regards to parallelization, including convolutional kernels and kernels better optimized for the reusability of tiles. In addition to direct algorithmic enhancements, TBNs could also be assessed in parallel fields of machine learning such as adversarial detection [12, 13], dataset complexity and [14, 15, 18], and federated learning [16, 39]."}, {"title": "APPENDIX", "content": "A TRAINING DETAILS\nIn this section we summarize the training details of our various experiments.\nCNN's trained with CIFAR-10 ResNet18, ResNet50, and VGG Small models are trained with similar hyperparameters on the CIFAR-10 dataset. They use an SGD optimizer with a base learning rate of 0.1, weight decay 0.0001, and momentum 0.9. Batch size is 128. We use a cosine learning rate policy. Layers are created with kaiming normal initialization with scale fan. We additionally use standard batch normalization. We normalize the dataset with mean and standard deviation. We initialize W and A with different seeds for each run (incremented by one for each experiment). TBN hyperparameters: We use multiple \u03b1's by default, W+A. Our CIFAR-10 and ImageNet models are based off of the Edge-Popup repository.\nCNN's trained with ImageNet We train ResNet models on the ImageNet dataset using TBNs. We use baselines from other papers and literature. We attain our default hyperparameters from Edge-Popup [46]. We train our model for 600 epochs, using an SGD optimizer, learning rate of 0.256, a weight decay of 0.000030517578125, momentum of 0.875, and batch size 128. We use a 0.1 value for label smoothing. We additionally use a warmup length of 5 to keep the learning rate static while training starts.\nPointNet Classification For our three PointNet tasks, we use an existing GitHub repository which you can find here. We use a batch size of 24, and train the model for 500 epochs. An Adam optimizer is used with a learning rate of 0.001. We use a decay rate of 0.0001.\nPointNet Part Segmentation We train our model for 250 epochs using a batch size of 16. We use an Adam optimizer with a learning"}, {"title": "B ADDITIONAL ABLATION STUDY", "content": "We include an additional ablation in Figure 8. Specifically, we look at the ResNet50 model with different tiling configurations such as global tiling versus minimum layer size tiling, W for calculating \u03b1 versus W for tiling and W for calculating A, and multiple versus one \u03b1 parameter. We find that the configuration which clearly performs worst is global tiling. As a result, we only tile layers which have a total size of at least 64,000. For the other parameters, we notice a slight benefit when using multiple \u03b1s (green line) compared to a single \u03b1 (red). The configuration which uses a separate parameter (A) for calculating \u03b1 performs better worse than the method which only uses W."}]}