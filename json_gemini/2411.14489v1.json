{"title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations", "authors": ["Hang Zhou", "Xiaoxu Zheng", "Yunhe Wang", "Michael Bi Mi", "Deyi Xiong", "Kai Han"], "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance dependencies are widely used in various speech tasks, eg., keyword spotting (KWS) and speech enhancement (SE). Due to the limitation of power and memory in low-resource devices, efficient RNN models are urgently required for real-world applications. In this paper, we propose an efficient RNN architecture, GhostRNN, which reduces hidden state redundancy with cheap operations. In particular, we observe that partial dimensions of hidden states are similar to the others in trained RNN models, suggesting that redundancy exists in specific RNNs. To reduce the redundancy and hence computational cost, we propose to first generate a few intrinsic states, and then apply cheap operations to produce ghost states based on the intrinsic states. Experiments on KWS and SE tasks demonstrate that the proposed GhostRNN significantly reduces the memory usage (~40%) and computation cost while keeping performance similar.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed that substantial improvements have been made in a wide range of speech tasks with the rapid development of neural networks. Among these neural networks, RNNs, e.g., LSTMs [1] or GRUs [2], are widely employed in various speech-related tasks in low-resource devices (e.g., mobile phones), such as KWS [3, 4], SE [5, 6], automatic speech recognition [7], acoustic echo cancellation [8, 9], etc., although they are less parallelizable than Transformer [10].\nDue to the high demand of AI model deployment on edge devices with limited power and memory, designing efficient models with low computation cost while maintaining high performance is desirable. A variety of efforts have been made in this direction. Dey and Salem [11] propose efficient GRU variants, which reduce the size of the gate matrix by adjusting the calculation method of the gate, e.g, calculating the gate vector with only hidden states as input. The Light-Gated-GRU (Li-GRU) is proposed by [12], which removes the reset gate and developes a single-gate RNN. Batch normalization is also used to optimize the model performance. Amoh and Odame [13] propose the Embedded Gated Recurrent Unit, which has only one gate with the Single Gate Mechanism. Simliar to Li-GRU, Fanta et al. [14] discard the reset gate of GRU and replace the activation function Tanh with Sigmoid in their proposed SITGRU. Zhang et al. [15] compress RNN models with a twin-gated mechanism. Although these methods can effectively reduce the number of parameters of the model, reducing"}, {"title": "2. Proposed Method", "content": "In this section, we elaborate the proposed GhostRNN with details in model compression. Without loss of generality, we use GRU to illustrate the definition of GhostRNN. Our method can be applicable to other RNNs, e.g., LSTM."}, {"title": "2.1. RNN", "content": "RNNs are a class of model structures that utilize hidden states to store and leverage contextual information [17], including popular variants GRUs and LSTMs. As one of the most commonly used RNN models, GRU is a simplified version of the LSTM, which is defined as follows:\n$r_t = \\sigma (W_{ir}x_t + b_{ir} + W_{hr}h_{(t-1)} + b_{hr})$\n$z_t = \\sigma (W_{iz}x_t + b_{iz} + W_{hz}h_{(t-1)} + b_{hz})$\n$c_t = tanh (W_{ic}x_t + b_{ic} + r_t * (W_{hc}h_{(t-1)} + b_{hc}))$\n$h_t = (1 - z_t) * c_t + z_t *h_{(t-1)}$\nwhere $r_t$, $z_t$ and $c_t$ are the reset gate, update gate and candidate vector respectively. $h_t$ and $h_{t-1}$ are hidden states at time step t and time step t - 1, and $x_t$ is the input feature at time step t. As shown in Eq. (1) to (4), six matrices are involved in computation, among which $W_{ir}, W_{iz}$ and $W_{ic}$ have the same size, $W_{ir}, W_{iz}$ and $W_{ic}$ are also with the same size. In addition, it's worth noting that the size of all the matrices of a GRU model is closely related to both the dimension of its hidden states and input features. Consequently, to compress the number of parameters of GRU, reducing the dimensionality of hidden states is an effective way. Also as mentioned in Section 1, it is feasible to decrease the number of gating matrices by reducing the number of required gating vectors, which ultimately leads to the reduction in the number of parameters of the model."}, {"title": "2.2. GhostRNN", "content": "Aiming at reducing hidden state redundancy, our proposed GhostRNN employs extremely low-cost transformations to generate ghost states from intrinsic states."}, {"title": "Observation on the redundancy of hidden states", "content": "As discussed in Section 1, previous studies usually focus on decreasing the number of gate matrices for model compression while the redundancy of hidden states is seldom investigated, which is also crucial to effectively reduce the number of model parameters. Therefore, the redundancy of hidden states is thoroughly under investigation in this section. Initially, the PCA contribution rate is adopted as the evaluation metric. The accumulation of hidden state vectors over time is considered as a feature map and the singular value decomposition is performed on it. Based on the result, only approximately half of the singular values are necessary and the feature map can reach a 99% PCA contribution rate, which indicates that with only about half of hidden states, the complete feature information is possible to"}, {"title": "GhostRNN module", "content": "Based on the above results and analysis, we propose the GhostRNN module to construct the ghost states based on the intrinsic states as shown in Figure 3, which can be defined as follows:\n$r_t = \\sigma (W_{ir}x_t + b_{ir} + W_{hr}[h_{(t-1)}; g_{(t-1)}] + b_{hr})$\n$z_t = \\sigma (W_{iz}x_t + b_{iz} + W_{hz} [h_{(t-1)}; g_{(t-1)}]+b_{hz})$\n$c_t = tanh (W_{ic}x_t + b_{ic} + r_t * (W_{hc}h_{(t-1)} + b_{hc}) +W_{gc}g_{t-1}+b_{gc})$\n$h_t = (1 - z_t) * c_t + z_t *h_{(t-1)}$\n$g_t = \\phi(h_t)$\nwhere $g_t$ and $g_{t-1}$ are the ghost states of the GRU model at time step t and time step t - 1 generated by the original intrinsic states $h_t$ and $h_{t-1}$ through simple linear transformation operations and activations denoted by $\\phi$. The intrinsic states at time step t are obtained by following Eqs (5) to (8). [] represents the concatenation operation to concatenate intrinsic and"}, {"title": "2.3. Analysis on the Number of Parameters and Computational Complexity", "content": "The number of parameters of vanila GRU and our proposed GhostRNN can be calculated as follows:\n$Param_{GRU} = 3 * (Dim_{feature} + Dim_{state}) * Dim_{state}$\n$Param_{GhostRNN} =3 * (Dim_{feature} + Dim_{state}) * (Dim_{state}/r) + Param$\n$Param = (Dim_{state}/r) * (Dim_{state} - (Dim_{state}/r)) = Dim_{state}^2 * ((r - 1)/r^2)$\nwhere r represents the ratio of the complete state to the intrinsic state. As shown in Eqs (10) to (11), the dimension of output hidden states in GhostRNN is divided by the ratio r compared with those in the vanila GRU. Although an additional cheap operation module consisting of linear layers is applied, according to Eq. (12) the number of parameters of the cheap operation module is far smaller than that of the GRU model. As a result, the total number of parameters of the GhostRNN will be compressed by the factor r. If the cheap operation module is constructed by other calculation methods such as vanilla linear transformation without parameters, the compression ratio can reach up to r. As for the computational complexity, since all the matrices in GRU are linear layers, the computational complexity of GRU is almost proportional to the number of parameters. Thus, the computational complexity of GhostRNN will also be reduced by the same factor r."}, {"title": "3. Experiments", "content": "Experiments on two tasks were conducted to evaluate the effectiveness of our method: KWS and SE."}, {"title": "3.1. Datasets", "content": "The Google Speech Commands dataset v0.02 [18] which contains thousands of one-second audio samples divided into 30 categories was used in our experiments for KWS. Following the previous work [3, 19], 12 categories were selected: \"yes,\" \"no,\" \"up,\" \"down,\" \"left,\" \"right,\" \"on,\" \"off,\" \"stop,\" \"go,\" silence, and unknown. We used 36,923, 4,445, and 4,890 of these samples for training, validation, and testing, respectively. The 10-dimensional Mel-frequency cepstral coefficients (MFCC) were used as the speech feature with a window length of 40 ms and a window shift of 20 ms which results in a feature map of size 49x10 for each speech sample and the data augmentation techniques such as adding background noise and random shift as suggested in [3] were employed to enhance the roubustness of the models.\nTo evaluate the performance of our method on the SE task, we used the LibriMix dataset [20] which generates noisy speech clips by combining clean speech from LibriSpeech [21] and noise from the WHAM! dataset [22]. We used the 16 kHz version of the train-360 data with a total of 50,800 training samples, 3000 validation samples and 3,000 testing samples, resulting in a total of 234 hours of data. We implemented the same data preprocessing as that in Asteroid [23]."}, {"title": "3.2. Settings and Evaluation Metrics", "content": "During training of KWS, the standard cross-entropy loss and the Adam optimizer with a batch size of 100 were employed. We used a step-down learning rate strategy, where the initial learning rate was 5e-4 with the step setting [10,000,20,000]. All the models were trained from scratch for a total of 30,000 iterations and evaluated by the accuracy metric [3]. To ensure the reliability of our results, each model was trained with the same configuration for three times and the average experiment results are reported here.\nDuring the training process of SE, the permutation invariant loss and the ADAM optimizer were used and the batch size was set to 12 for DCRNN [5] and 32 for GRU-TasNet [24]. The learning rate decay strategy and early stopping strategy were both applied in all experiments. The initial learning rate was set to 0.001 and a weight decay of 1e-5 was applied. In terms of the filterbank, our settings were consistent with those described in [23]. For evaluation, 5 metrics, namely Signal-to-Distortion Ratio (SDR), SDR improvements (SDRi), Scale-Invariant Signal-to-Distortion Ratio (Si-SDR) [25], Si-SDR improvements (Si-SDRi), and Short-Time Objective Intelligibility (STOI) [26] were used."}, {"title": "3.3. Baselines", "content": "KWS model\nAs demonstrated in Table 1, a GRU model of approximately 500k in size and another GRU model of 295k in size were employed as baseline models for KWS [3], while Li-GRU [12] and SITGRU [14] were implemented for comparison."}, {"title": "SE models", "content": "Two types of models were selected as the baselines model for comparison for the SE task. The first one is the DCCRN [5], which is mainly based on convolution layers and supplemented with RNN layers. The second one is the GRU-TasNet [24], in which RNN is the primary module. A brief introduction of them is provided below:\n\u2022 DCCRN. This model consists of three main components: a convolution encoder, a transpose convolution decoder and an RNN module. In our experiment, we chose the DCCRN-CL [5] model and replaced the LSTM with GRU, in which two sizes of the hidden unit 128 and 80 were chosen to construct the different baseline models with different size.\n\u2022 GRU-TasNet. This model is optimized by the Time-domain Audio Separation Network, which consists of three parts: a 1-D convolutional encoder, a 1-D deconvolutional decoder, and a Deep LSTM separation module [24]. In our experiments, the LSTM was replaced with GRU. Four baseline models with different sizes were designed, differing in the hidden size of the GRU: 512, 384, 192, and 136."}, {"title": "3.4. Results on KWS", "content": "Table 1 presents the experiment results and model parameters. Our proposed GhostRNN model is compared with two baselines of 500k GRU and 300k GRU, as well as two other model compression methods: Li-GRU [12] and SITGRU [14]. The results show that our GhostRNN with about 40% fewer parameters achieves approximately a 0.1% improvement on the accuracy rate over the 500k GRU model and also outperforms the Li-GRU and SIT-GRU models with slightly more parameters, which indicates the effectiveness of our proposed GhostRNN."}, {"title": "3.5. Results on SE", "content": "Table 2 presents the results of DCCRN and DCCRN_Ghost128 on the librimix1 dataset. The results indicate that pruning the hidden size of the GRU layer in the DCCRN model by 10% for model compression leads to a decrease of approximately 0.1 dB in both SDR and Si-SDR metrics. In contrast, when applying our proposed GhostRNN compression method and compressing approximately 10% of the parameter, the SDR and Si-SDR metrics only decrease by approximately 0.05 dB. These findings clearly demonstrate the effectiveness of GhostRNN.\nTable 2 presents the results of GRU-TasNet and GhostRNN-TasNet on the librimix1 dataset. The metrics show a slight decrease when the model is compressed from GRU512-TasNet to GRU384-TasNet, indicating that GRU512-TasNet has redundant parameters. In this case, the GhostRNN method yields a performance improvement of over 0.1 dB in SDR and Si-SDR, with 40% fewer parameters than GRU512-TasNet. However, when the model is further compressed from 1.6M (GRU192-TasNet) to 1.2M (GRU136-TasNet), the performance drops noticeably, demonstrating that the model has low redundancy. In this scenario, GhostRNN192 has an advantage of approximately 0.13 dB in SDR and Si-SDR compared to GRU136-TasNet. In summary, GhostRNN is an effective compression method for RNN models."}, {"title": "4. Conclusions", "content": "In this paper, we have presented GhostRNN for RNN model compression based on the observation of the redundancy in hidden states. In our GhostRNN, given the intrinsic hidden states, the extreme low-cost transformation layers are applied to generate the ghost states which significantly reduces the number of parameters and the computation cost of the vanllia GRU model but achieves competitive performance. Experimental results demonstrate that our method achieves a 0.1% accuracy improvement on the Google Speech Commands dataset while compressing the parameters of baseline model by 40%. In the SE task, our method improves SDR and Si-SDR by approximately 0.1 dB with around 40% compression rate. Additionally, our method outperforms the GRU based model with the same number of parameters by approximately 0.13 dB in terms of SDR and other evaluation metrics. Overall, the proposed GhostRNN is a simple yet effective method for RNN model compressing. In the future work, it is worth to investigate the extension of GhostRNN to other RNN structures, such as LSTM, and further explore novel ghost state generation methods to achieve better balance on the reduction of the model computational complexity and performance. Additionally, we plan to explore the potential benefits of combining GhostRNN with other existing RNN compression techniques."}, {"title": "5. Acknowledgements", "content": "Deyi Xiong was partially supported by the Natural Science Foundation of Xinjiang Uygur Autonomous Region (No. 2022D01D43). We gratefully acknowledge the support of MindSpore [27], CANN(Compute Architecture for Neural Networks) and Ascend AI Processor used for this research. We would like to thank the anonymous reviewers for their insightful comments."}]}