{"title": "Reinforcement Learning-Enhanced Procedural Generation for Dynamic\nNarrative-Driven AR Experiences", "authors": ["Aniruddha Srinivas Joshi"], "abstract": "Procedural Content Generation (PCG) is widely used to create scalable and diverse environments in games.\nHowever, existing methods, such as the Wave Function Collapse (WFC) algorithm, are often limited to static\nscenarios and lack the adaptability required for dynamic, narrative-driven applications, particularly in aug-\nmented reality (AR) games. This paper presents a reinforcement learning-enhanced WFC framework de-\nsigned for mobile AR environments. By integrating environment-specific rules and dynamic tile weight ad-\njustments informed by reinforcement learning (RL), the proposed method generates maps that are both con-\ntextually coherent and responsive to gameplay needs. Comparative evaluations and user studies demonstrate\nthat the framework achieves superior map quality and delivers immersive experiences, making it well-suited\nfor narrative-driven AR games. Additionally, the method holds promise for broader applications in education,\nsimulation training, and immersive extended reality (XR) experiences, where dynamic and adaptive environ-\nments are critical.", "sections": [{"title": "1 INTRODUCTION", "content": "Procedural generation has become a cornerstone in\nthe creation of diverse and scalable environments for\ngames, enabling automated generation of complex\nlayouts with minimal manual intervention. Although\nwidely used in traditional gaming, its application in\naugmented reality (AR) remains limited, particularly\nin scenarios where environments need to dynamically\nadapt to gameplay narratives or physical surround-\nings. The Wave Function Collapse (WFC) algorithm\n(Gumin, 2016), known for generating cohesive lay-\nouts through adjacency constraints, has been effective\nin creating static maps. However, it does not inher-\nently address the challenges posed by narrative-driven\nexperiences, where maps must align with evolving\nstorylines and diverse contextual needs.\nIn this work, we extend the WFC algorithm to bet-\nter serve the needs of narrative-driven AR games. By\nintroducing environment-specific rules, our method\ntailors map generation to diverse settings, such as ur-\nban grids, open spaces, and dense terrains. These\nrules govern the placement of paths and features, en-\nsuring maps are both visually coherent and themati-\ncally appropriate. To enhance adaptability, reinforce-\nment learning (RL) refines generation decisions dy-\nnamically, adapting layouts to diverse gameplay re-\nquirements. Additionally, AR-specific features sup-\nport real-time interactivity, enabling users to dynami-\ncally adjust maps to evolving gameplay narratives.\nThis approach bridges the gap between static\nprocedural generation and the dynamic needs of\nnarrative-driven AR games. By combining algorith-\nmic enhancements with tools for real-time modifica-\ntion, our method delivers adaptive environments that\nenhance storytelling and gameplay. Beyond games,\nthe framework can be applied to AR educational\ntools, simulation training, and other immersive expe-\nriences, offering a novel and practical advancement in\nstate-of-the-art PCG methods.\nTo this end, this study addresses two primary\nresearch questions. Firstly, it evaluates how RL-\nenhanced WFC compares to traditional procedural\ncontent generation (PCG) methods in supporting the\nneeds of narrative-driven augmented reality expe-\nriences. Secondly, it examines how the proposed\nmethod improves user experience by enabling dy-\nnamic, coherent, and immersive environments in"}, {"title": "2 RELATED WORKS", "content": "In the realm of interactive environments, procedural\ngeneration and machine learning (ML) have emerged\nas transformative technologies that enable the cre-\nation of dynamic and richly detailed content. This\nsection delves into the evolution of these technologies\nfrom their foundational use in game development to\ntheir sophisticated integration within augmented real-\nity systems. We examine how traditional procedural\ngeneration methods have been enhanced by learning\nalgorithms to address the complexities of modern ap-\nplications. The discussion underscores the need for\nmore adaptable and context-aware generation meth-\nods, especially for enhancing user experiences in AR,\nvirtual reality (VR), and extended reality (XR) envi-\nronments, setting the stage for our proposed method\ndesigned to address these critical challenges."}, {"title": "2.1 Procedural Generation in Games", "content": "Procedural generation techniques have long been\nfoundational for creating diverse and scalable con-\ntent in games. One of the earliest techniques, Binary\nSpace Partitioning (BSP), introduced a method to re-\ncursively divide space into convex sets, enabling ef-\nficient rendering and collision detection. Originally\ndesigned to solve the hidden surface problem (Fuchs\net al., 1980), BSP has since been adapted for gen-\nerating structured layouts, such as dungeon levels,\nin modern games. Noise-based methods like Per-\nlin Noise (Perlin, 1985) and Simplex Noise improve\nnaturalistic terrain generation, with Simplex Noise\naddressing computational inefficiencies and reducing\nartifacts. Cellular Automata (Johnson et al., 2010) is\nanother widely used approach for simulating organic\nstructures such as caves or forests, evolving systems\nover time.\nThe Wave Function Collapse algorithm (Gumin,\n2016) builds on earlier techniques for tile-based map\ngeneration. Notably, it shares significant similari-\nties with the Model Synthesis algorithm (Merrell and\nManocha, 2011). Model Synthesis differs in its ap-\nproach to cell selection and its ability to modify the\nmodel in smaller blocks, which enhances its perfor-\nmance for generating larger and more complex out-\nputs. A comparative analysis highlights their concep-\ntual overlap and differences in implementation (Mer-\nrell, 2021). Unlike earlier techniques, WFC excels\nin maintaining structural coherence. However, it re-\nmains static in nature and lacks the ability to adapt"}, {"title": "2.2 Machine learning in Procedural\nGeneration", "content": "Machine learning has greatly expanded the possibili-\nties of procedural content generation by enabling sys-\ntems to adaptively generate content based on learned\npatterns. Methods like Generative Adversarial Net-\nworks (GANs) and Variational Autoencoders (VAEs)\n(Liu et al., 2021) are commonly used to create high-quality game assets, including levels and textures.\nThese approaches introduce flexibility and adaptabil-\nity, enhancing traditional PCG techniques.\nRL has also shown promise for procedural tasks\nrequiring sequential decision-making. For instance,\nRL-based frameworks (Khalifa et al., 2020) demon-\nstrate how RL agents can generate game levels by\nframing level design as a Markov Decision Process\n(MDP). Similarly, recent work highlights RL's ability\nto balance quality, diversity, and playability in level\ngeneration (Nam et al., 2024). Despite these advance-\nments, ML-based methods are often applied to 2D\nor platformer games and have yet to be fully inte-\ngrated into augmented reality or interactive 3D envi-\nronments."}, {"title": "2.3 Procedural Generation in\nAugmented Reality", "content": "Procedural Content Generation has also been applied\nin augmented reality to enhance user interaction by\ndynamically adapting virtual content to physical en-\nvironments. Recent work presents a pipeline for inte-\ngrating pre-existing 3D scenes into AR environments,\nminimizing manual adjustments and ensuring align-\nment with physical spaces (Caetano and Sra, 2022).\nSimilarly, PCG has been used to tailor AR game lev-\nels to the player's surroundings, dynamically adjust-\ning elements like layout and difficulty to leverage\nphysical affordances (Azad et al., 2021).\nWhile these studies illustrate the potential of PCG\nin AR, they often focus on predefined or static con-\ntent and rarely explore dynamic procedural generation\ntailored to narrative-driven gameplay. This work ad-\ndresses these gaps by introducing adaptive PCG tech-\nniques for dynamically generating AR maps aligned\nwith both narrative and gameplay needs, enabling\nreal-time interactivity and customization."}, {"title": "2.4 AR/VR/XR in Narrative Games", "content": "AR, VR, and XR technologies have increasingly been\nused to create immersive environments for narrative-\ndriven games. Research demonstrates how spatial\ninteractivity can enhance storytelling by embedding\nnarratives into physical spaces, providing players with\nunique, location-aware experiences (Viana and Naka-\nmura, 2014). Similarly, mobile AR studies examine\nthe challenges of balancing user freedom with narra-\ntive control, highlighting AR's potential for support-\ning interactive storytelling (Nam, 2015).\nAlthough these works showcase AR and XR's\nstrengths in narrative gaming, they often rely on man-\nually designed environments, limiting scalability and\nadaptability. Few approaches incorporate procedural\ngeneration to dynamically align narratives with gen-\nerated virtual spaces. This work builds on these foun-\ndations by integrating PCG into AR-specific features,\nenabling the dynamic creation of interactive environ-\nments that evolve alongside narrative-driven game-\nplay."}, {"title": "3 METHOD", "content": "In this section, we elucidate the proposed approach,\nwhich integrates reinforcement learning with the\nWFC algorithm (Gumin, 2016) to procedurally gen-\nerate grid-based, immersive 3D maps in augmented\nreality for narrative-driven games. The RL-enhanced\nWFC method builds on the foundational WFC algo-\nrithm, originally designed for tile-based map genera-\ntion using adjacency constraints. Our approach incor-\nporates biome-specific constraints and reinforcement\nlearning to optimize the generation process, ensuring\nbiome coherence and enhancing path layouts.\nWe focus on three distinct biomes, each with\nunique layout and art styles:\n\u2022 City: A structured environment with continu-\nous paths including pathways and buildings. De-\nsigned for interconnected urban settings that facil-\nitate navigation.\n\u2022 Desert: A sparse environment characterized by\nopen areas and minimal impassable tiles such as\nboulders and cacti.\n\u2022 Forest: A natural setting featuring paths between\ndense obstacles like trees and rocks, interspersed\nwith open clearings."}, {"title": "3.1 Proposed Procedural Generation\nMethod", "content": "We now present the proposed procedural generation\nmethod aimed at constructing dynamic and interactive\nenvironments. Fundamental to our approach are the\nconcepts of 'cell' and 'tile'. A cell is the basic unit of"}, {"title": "3.1.1 Tile Weight Calculation and Selection", "content": "As we begin this section, it is crucial to acknowledge\nthat Equation 1 has been extended to incorporate con-\ntributions from the RL agent. A detailed discussion\non this topic will follow in Section 3.2. Our current\nfocus will be on the base implementation of weighted\nrandom selection.\nEach cell in the grid has a list of possible tile\noptions, and each tile option is assigned a weight\nbased on how well it aligns with its neighboring cells.\nTiles that fit better with neighbors are assigned higher\nweights to increase their likelihood of selection.\nIf a given cell has a total of T tile options, then the\nweight for the $i^{th}$ tile option denoted $w_i$ is calculated\nas follows:\n$w_i = w_0 + \\sum s_n$\n$n \\in N$\nwhere:\n\u2022\n$w_0$ is the base weight ($w_0$ = 1.0),\n\u2022\nN represents the neighboring cells,\n\u2022\n$s_n$ is the adjacency score for neighbor cell n:\n$s_n =  \\begin{cases}\n0, & \\text{if n is non-collapsed}\n& \\text{(i.e., n does not have a tile selected),} \\\\\n1.5, & \\text{if n has a compatible tile}\n& \\text{(i.e., n's tile adheres to adjacency rules),} \\\\\n0.5, & \\text{if n has an incompatible tile}\n& \\text{(i.e., n's tile violates adjacency rules)}\n\\end{cases}$\nAfter calculating weights for T tile options, a\nweighted random selection is performed:\n1.\nCompute the total weight $W = \\sum_{i=1}^{T} w_i$ for all tile\noptions.\n2. Generate a random number $r \\in [0, W]$.\n3. Select the tile with smallest index $j$ such that\n$\\sum_{k=1}^{j} w_k \\geq r$.\nThis tile selection method ensures that tiles with\nhigher weights (those that fit well with their neigh-\nbors) are more likely to be chosen, while still allowing\nsome randomness in choice."}, {"title": "3.1.2 Path Layout Strategies", "content": "The proposed method assumes two types of tiles:\n1. Path tiles: These are tiles that players can traverse\n(e.g., roads, grass).\n2. Impassable tiles: These are tiles that cannot be tra-\nversed and block movement (e.g., boulders, trees).\nDepending on the selected biome, the path layout\ncan be continuous (e.g., city biome) or sparse (e.g.,\nforest biome). These layouts are created by applying\ndifferent path constraint strategies during the propa-\ngation step.\nContinuous Path Layouts: Strict adjacency con-\nstraints are enforced to ensure path connectivity.\nOnce a path tile is selected, these constraints are en-\nforced during propagation:\n\u2022 Filtering Valid Tile Options: For each neighbor-\ning cell, valid tile options are constrained to in-\nclude only those that can connect to the path tile.\nThis prioritization ensures that most neighboring\ncells favor path-compatible tiles, maintaining con-\ntinuous connectivity.\n\u2022 Inclusion of Impassable Tiles: Impassable tiles\nare only considered valid if they satisfy specific\nadjacency rules, such as requiring at least one\nadjacent path tile to preserve navigability. This\napproach allows for the integration of buildings,\nwalls, or other impassable elements without dis-\nrupting the functionality of the map.\n\u2022 Weighted Randomness Favors Continuity of\nPath: During tile selection, weighted randomness\nbiases selection toward path tiles while allowing\noccasional placement of impassable tiles for di-\nversity.\nSparse Path Layouts: Relaxed adjacency con-\nstraints are used, allowing paths to be more scattered\nwith impassable tiles (e.g., trees, rocks) interspersed\namong path tiles, creating a more open, fragmented\nlayout. This is achieved through the following mech-\nanisms:\n\u2022 Random Application of Continuous Path Con-\nstraints: When a path tile is placed, for each\nneighboring cell, there is a 50% chance of apply-\ning continuous path constraints as previously de-\nscribed.\n\u2022 Flexible Tile Options: If continuous path con-\nstraints are not applied, the neighboring cell re-\ntains a wider range of valid tile options, includ-\ning path tiles and impassable tiles. This promotes\nmore randomness and contributes to the sparse\nlayout's fragmented structure."}, {"title": "3.2 Reinforcement Learning for\nProcedural Map Generation", "content": "RL is integrated to dynamically adjust tile weights\nin the WFC algorithm. By tailoring tile weights\nto biome-specific characteristics, this approach im-\nproves the coherence, completeness, and efficiency of\nmap generation. The Proximal Policy Optimization\n(PPO) algorithm is chosen for its stability and abil-\nity to train effective policies while enabling moderate\nexploration (Schulman et al., 2017).\nPPO uses a clipped surrogate objective function to\nstabilize policy updates:\n$L^{CLIP}(\\theta) = E_t [min (r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$\nwhere:\n\u2022\n$\\theta$ represents the policy network parameters\n\u2022\nt is the timestep\n\u2022\n$r_t(\\theta)$ is the probability ratio between new and old\npolicies at t\n\u2022\n$\\hat{A}_t$ is the advantage estimate at t, quantifying the\nrelative benefit of actions\n\u2022\n$\\epsilon$ is the clipping threshold to limit policy update\nratios\nThe PPO algorithm is used to train an RL agent\nto learn a policy that adjusts tile weights dynamically\nduring map generation. Through episodic interactions\nwith the WFC system, the agent observes the cur-\nrent grid state, biome type, and layout requirements\nto determine optimal adjustments. The policy learned\nby the agent is designed to maximize cumulative re-\nwards, encouraging map generation that is efficient,\ncomplete, and biome-coherent.\nBuilding on the baseline tile weight formula de-\nfined in Equation 1, the updated formula incorporates\na dynamic adjustment term, $r_{RL}$, derived from the RL\nagent's policy:\n$w_i = w_0 + \\sum s_n + r_{RL}$\n$n \\in N$"}, {"title": "3.2.1 Agent Training", "content": "Agent training proceeds in episodes, where each\nepisode represents a single map generation task. At\nthe beginning of each episode, the WFC system ini-\ntializes an empty grid and the agent receives an ob-\nservation. This observation encodes the current grid\nstate as a binary representation of collapsed and un-\ncollapsed cells, the biome type as a one-hot vector\n(e.g., Forest, City, or Desert), and the layout type as a\nbinary value indicating sparse or continuous layouts.\nThe agent outputs a continuous adjustment value\n(RL Score), which dynamically modifies tile weights\nduring map generation. The WFC algorithm uses\nthese adjusted weights to generate a map, linking the\nagent's actions to the resulting layout. After map gen-\neration, the agent evaluates the map's quality using a\nreward function, which comprises three components:\n1. Completeness: Rewards fully collapsed grids and\npenalizes incomplete or invalid configurations.\nThis is formally defined as:\n$C=\\begin{cases} +1, & \\text{if all cells are collapsed and valid,}\\\\ -0.5, & \\text{if some cells remain uncollapsed,}\\\\ -1, & \\text{if grid contains invalid configurations}\n\\end{cases}$"}, {"title": "2. Biome Coherence", "content": "Rewards alignment with\nbiome-specific characteristics, such as sparse lay-\nouts for Forests, continuous paths for Cities, and\nopen spaces for Deserts. It is calculated as:\n$B = \\frac{\\sum_{j=1}^{T}b_j}{T}$\nwhere $b_j$ = 1 if the $j^{th}$ tile adheres to biome-\nspecific adjacency constraints, and T is the total\nnumber of tiles."}, {"title": "3. Efficiency", "content": "Rewards faster map generation and\npenalizes retries or backtracking. Efficiency is de-\nfined as:\n$E = k_1 (S_{max} - S_{used}) - k_2. B$\nwhere:\n\u2022\n$k_1$ = 0.2 is the reward factor for minimizing\nsteps\n\u2022\n$k_2$ = 0.1 is the penalty factor for backtracking"}, {"title": "2.2 Policy Deployment", "content": "After training, the policy learned by the RL agent is\ndeployed in inference mode i.e. it applies the learned\nadjustments to make decisions without further up-\ndates. During deployment, the system uses the policy\nto dynamically adjust tile weights based on the biome\ntype and grid state in real-time. These adjustments en-\nsure that the generated maps are coherent, complete,\nand efficient, aligning with the specific requirements\nof each biome."}, {"title": "3.3 Mobile AR Implementation", "content": "The Augmented Reality mobile app was developed\nusing Unity's AR Foundation API (Unity Technolo-\ngies, 2023), which provides built-in plane detection\nto identify horizontal surfaces, such as tabletops, as\nvalid spaces for placing virtual elements. Detected\nsurfaces are visually highlighted to indicate valid\nplacement areas, offering clear feedback to the user.\nOnce a surface is confirmed through a screen tap, a\nraycast is performed from the tap position to the plane\nsurface. The raycast ensures accurate placement of\nthe procedural grid map, which is generated based on\nthe specified dimensions and biome, and then aligned\nwith the detected surface for an immersive and inter-\nactive experience.\nTo ensure the method is optimized for mobile\ndevices, grid sizes were limited to a maximum of\n15\u00d715. This constraint prevents excessive computa-\ntional overhead while also aligning with the physi-\ncal constraints of typical tabletop surfaces, ensuring"}, {"title": "3.4 Interactive Narrative Control", "content": "The proposed method provides dynamic narrative\ncontrols for real-time customization of procedurally\ngenerated AR environments. These interactive con-\ntrols give DMs the ability to adapt the map to align\nwith the evolving narrative. This allows the envi-\nronment to respond to key narrative events and unex-\npected player actions, bridging the gap between pro-\ncedural generation and narrative-driven gameplay.\nThe key interactive features are:\n1. Biome and Grid Size Selection: The DM can se-\nlect the biome (City, Desert, or Forest) and de-\nfine grid dimensions at the start of map genera-\ntion. This ensures the environment aligns with the\nstory's thematic and spatial requirements.\n2. Clearing Paths: Hidden routes can be revealed to\nguide players toward new objectives or advance\nthe narrative. For instance, uncovering a passage\nafter solving a puzzle ensures the story progresses\nnaturally while encouraging exploration.\n3. Blocking Paths: Existing routes can be closed off\nto simulate story-driven events or introduce envi-\nronmental challenges. Examples include blocking\na path to represent a cave-in or using a locked gate\nto redirect players toward specific objectives.\n4. Placing Special Objects: Narrative-critical ob-\njects such as traps, treasure chests, keys, or locked\ndoors can be dynamically added to the map.\nThese objects embed story-specific interactions\ninto the environment, such as requiring players to\nlocate a key to progress or triggering traps during\npivotal moments."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "In this section, we present the experimental evaluation\nconducted to validate the effectiveness and efficiency\nof our proposed procedural generation method. The\nevaluation includes the training of the RL agent, a\nuser study assessing interaction quality with AR ap-\nplications, and a performance analysis comparing our\nmethod to baseline approaches. Together, these eval-\nuations provide a comprehensive assessment of the\nproposed method's application in generating procedu-\nral content for interactive AR environments."}, {"title": "4.1 RL Agent Training", "content": "The RL agent was trained in Unity using ML-Agents\nwith PyTorch for policy updates. Training was con-\nducted on a Windows workstation equipped with an\nAMD Ryzen 9 5900X 12-Core processor and an\nNVIDIA RTX 4070 Super GPU. Unity's simulation\nenvironment handled procedural map generation dur-\ning episodic interactions with the agent, providing the\nbasis for policy learning.\nThe agent was trained using 15\u00d715 grids, as this\nis the maximum grid size supported by the mobile ap-\nplication. Each biome (City, Forest, and Desert) was\ntrained for 50,000 episodes, a value chosen to provide\nenough iterations for the policy to generalize across\ndiverse map generation scenarios.\nTraining parameters included a batch size of 64,\na learning rate of 3.0 \u00d7 10\u20134, a buffer size of 2048,\na discount factor (\u03b3) of 0.99, and a clipping threshold\n(\u03b5) of 0.2. Observations consisted of biome-specific\ninputs and the grid state (collapsed/uncollapsed cells).\nA single continuous action PRL dynamically adjusted\ntile weights during training.\nThe reward function as defined in Section 3.2.1\nguided the agent's policy updates by balancing Com-\npleteness, Biome Coherence, and Efficiency. For the\nEfficiency component, k\u2081 = 0.2 and k2 = 0.1 were se-\nlected to encourage faster map generation while pe-\nnalizing excessive backtracking. These values were\nchosen to balance the trade-offs observed during ini-\ntial training runs.\nHigher backtracking penalties\ncaused overly cautious behavior, while lower penal-\nties made retries less impactful.\nBy the end of training, the agent effectively\nlearned to adjust tile weights dynamically, enabling\nthe generation of coherent, complete, and efficient\nmaps across diverse biomes."}, {"title": "4.2 User Study", "content": "A user study was conducted with 28 participants with\na diverse range of D&D experience levels, as shown\nin Table 1. Each participant interacted with three AR\napplications, each implementing a different procedu-\nral generation algorithm:\n\u2022 App 1: Perlin Noise (Perlin, 1985)\n\u2022 App 2: Cellular Automata (Johnson et al., 2010)\n\u2022 App 3: Proposed Method\nWe conducted a statistical power analysis to de-\ntermine the appropriate sample size for our within-\nsubjects study. As with prior work in immersive me-\ndia (Breves and Schramm, 2021) and (H\u00f6gberg et al.,\n2019), we assumed a medium effect size (f = 0.25), a\nsignificance level of a = 0.05, and a statistical power\nof 1-B = 0.8 (Cohen, 1988). Using G* Power for a\nrepeated measures ANOVA with three conditions, the\nrequired sample size was calculated to be 28 partici-\npants.\nSuitability evaluated alignment with biome-specific\ncharacteristics, such as sparse desert layouts or struc-\ntured city grids, supported by frameworks for proce-\ndural generation and environmental coherence (Azad\net al., 2021) and (Togelius et al., 2011). Immersion\nassessed user engagement, inspired by the Presence\nQuestionnaire (Witmer and Singer, 1998), while Gen-\neration Speed reflected usability principles from the\nSystem Usability Scale (SUS) (Brooke, 1995). Vi-\nsual Quality, adapted from the User Experience Ques-\ntionnaire (UEQ) (Laugwitz et al., 2008), measured\naesthetics, and Preferred App captured participants'\noverall impressions, commonly used in procedural\ncontent evaluations (Khalifa et al., 2020)."}, {"title": "4.3 Computational Performance\nAnalysis", "content": "We assess the computational performance of our pro-\nposed procedural generation method, focusing on its\nefficiency relative to the baseline techniques used in\nthe user study. This evaluation provides insights into\nthe method's practicality for implementation on mo-"}, {"title": "4.3.2 Observations", "content": "The following observations were made from the com-\nparative results:\n\u2022 Perlin Noise was the fastest method across\nall biomes with Cellular Automata performing\nslightly slower and the proposed method requir-\ning significantly more time, particularly for larger\ngrid sizes.\n\u2022 Performance consistently degraded with increas-\ning grid size across all methods with longer gener-\nation times, lower minimum FPS, and prolonged\nFPS recovery times.\n\u2022 Desert biome exhibited consistently better perfor-\nmance with shorter generation times and higher\nFPS across methods while the Forest and City\nbiomes were more computationally demanding."}, {"title": "4.4 Results", "content": "The computational performance and user feedback\nevaluations reveal key trade-offs between quality and\nefficiency for the proposed method compared to Per-\nlin Noise and Cellular Automata in mobile AR envi-\nronments.\nThe evaluation of generation times as shown\nin Tables 4, 5 and 6, demonstrated that the pro-\nposed method required significantly longer times\nacross all grid sizes and biomes. While slower, the\nproposed method produced highly detailed, biome-\nspecific maps that were rated superior in user stud-\nies, reinforcing its value for applications where qual-\nity and coherence are critical.\nPerformance trends across grid sizes showed that\nlarger grids led to longer generation times, reduced\nminimum FPS and increased FPS recovery times\nacross all methods. The Desert biome consistently\nperformed better with shorter generation times and\nhigher FPS values while the City and Forest biomes\nposed greater computational challenges due to their\ncomplexity. Despite these challenges, the proposed\nmethod achieved significantly higher map coherence\nand quality across all biomes, particularly for more\nintricate layouts in the City environment.\nThe user study results validated the proposed\nmethod's advantages, with participants consistently\nrating its maps as more immersive, visually coher-\nent, and narrative-friendly than those from baseline\nmethods. These findings highlight its suitability for\nnarrative-driven AR applications where immersion\nand contextual alignment take precedence over speed.\nWhile WFC without RL was not explicitly evalu-\nated in this study, future work could include such"}, {"title": "5 CONCLUSIONS", "content": "In this work, a reinforcement learning-enhanced\nwave function collapse based procedural generation\nmethod was developed and evaluated for mobile\nAR environments. The proposed method demon-\nstrated superior coherence and map quality, making\nit particularly well-suited for narrative-driven games\nwhere immersion and environmental detail are crit-\nical. While the method introduces higher compu-\ntational demands compared to baseline approaches,\nuser studies consistently rated the resulting maps sig-\nnificantly higher, highlighting its effectiveness in de-\nlivering high-quality and contextually appropriate en-\nvironments.\nThe performance evaluation revealed trade-offs\nbetween computational performance and map qual-\nity, influenced by the structural intricacy of the gener-\nated environments. Despite slower performance, the\ngeneration times of the proposed method remain ac-\nceptable for mobile AR applications. Adapting the\nmethod for next-generation XR devices with greater\ncomputational power could further enhance its per-\nformance and scalability. Techniques such as asyn-\nchronous map generation could reduce perceived de-\nlays and improve responsiveness, making the method"}]}