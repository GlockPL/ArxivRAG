{"title": "TSUBF-Net: Trans-Spatial UNet-like Network with Bi-direction Fusion for Segmentation of Adenoid Hypertrophy in CT", "authors": ["Rulin Zhou", "Yingjie Feng", "Guankun Wang", "Xiaopin Zhong", "Zongze Wu", "Qiang Wu", "Xi Zhang"], "abstract": "Adenoid hypertrophy stands as a common cause of obstructive sleep apnea-hypopnea syndrome in children. It is characterized by snoring, nasal congestion, and growth disorders. Computed Tomography (CT) emerges as a pivotal medical imaging modality, utilizing X-rays and advanced computational techniques to generate detailed cross-sectional images. Within the realm of pediatric airway assessments, CT imaging provides an insightful perspective on the shape and volume of enlarged adenoids. Despite the advances of deep learning methods for medical imaging analysis, there remains an emptiness in the segmentation of adenoid hypertrophy in CT scans. To address this research gap, we introduce TSUBF-Net (Trans-Spatial UNet-like Network based on Bi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is engineered to effectively discern intricate 3D spatial interlayer features in CT scans and enhance the extraction of boundary-blurring features. Notably, we propose two innovative modules within the U-shaped network architecture: the Trans-Spatial Perception module (TSP) and the Bi-directional Sampling Collaborated Fusion module (BSCF). These two modules are in charge of operating during the sampling process and strategically fusing down-sampled and up-sampled features, respectively. Furthermore, we introduce the Sobel loss term, which optimizes the smoothness of the segmentation results and enhances model accuracy. Extensive 3D segmentation experiments are conducted on several datasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest HD95: 7.03, IoU:85.63, and DSC: 92.26 on our own AHSD dataset. The results in the other two public datasets also demonstrate that our methods can robustly and effectively address the challenges of 3D segmentation in CT scans.", "sections": [{"title": "1 Introduction", "content": "The main cause of upper airway obstruction in children and adolescents is adenoid hypertrophy (AH) [1]. AH typically manifests with symptoms such as snoring, nasal congestion, rhinorrhea, sinusitis, and hearing impairment, potentially leading to the characteristic \"adenoid face\" over prolonged durations [2]. Moreover, obstructive sleep apnea hypoventilation syndrome (OSAS) is a prevalent respiratory disorder during childhood sleep, primarily rooted in hypertrophy of the adenoids and tonsils [4]. If left unaddressed, this condition carries the risk of severe sequelae, encompassing neurocognitive aberrations, cardiovascular complications, and disruptions in growth patterns [5].\nThe principal detection modalities of AH encompass ultrasonography, X-ray, CT, MRI, and nasal endoscopy, among which CT examination holds notable significance owing to its commendable density resolution. CT scans excel in delineating tissue structures with macroscopic pathological anatomy. Particularly, CT transverse images provide a panoramic depiction of adenoids and the nasopharyngeal cavity, enabling accurate measurements of nasopharyngeal airway obstruction and concurrent observation of surrounding tissues [6]. Consequently, employing CT for the segmentation of adenoid hypertrophy proves to be of paramount importance.\nSubsequently, through CT images, a broader physiological context is accessible, as illustrated in Fig. 1, encompassing structures such as the skull, vertebrae, soft palate, epiglottis, and turbinates. The specific target for segmentation is denoted in purple, representing the hypertrophied segment of the adenoids. CT reveals that the hypertrophied portion is located within a confined airway space, surrounded by"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Deep Learning Segmentation to Adenoid Hypertrophy", "content": "With the development of deep learning, the application in medical imaging is becoming an emerging area. More and more methods are proposed to target the disease problem of adenoid hypertrophy. Wang et al. [14] used ultrasound to evaluate adenoid hypertrophy. Zhao et al. [15] designed a CNN-based framework to train on clinical data of avatars. Shen et al. [16] classified adenoid hypertrophy by locating key points and devising a novel regularized terminology based on patient's X-rays. Liang et al. [17] used a convolutional neural network to analyze children's behavior at night during the night to classify normal snoring and abnormal snoring. Zheng et al. [19] used a novel multiscale hierarchical network MIB-Anet to classify adenoid images from nasal endoscopy in four classes. Alshbishiri et al. [20] sampled a 2D-UNet network to segment the adenoids. Wang et al. [21] used a 2D-UNet network to classify the adenoid hypertrophic portion of the adenoids from X-rays. Despite numerous studies on the segmentation of 2D adenoid hypertrophy, there is little work on the 3D segmentation of adenoid CT data."}, {"title": "2.2 CNN-Based Segmentation Networks", "content": "Since the introduction of U-Net [21], CNN-based networks have achieved state-of-the-art results in various 2D and 3D medical image segmentation tasks [48] [47] [23] [24]. In the case of 3D medical image segmentation, the complete volume is usually processed into a sequence of 2D slices, which are stacked after the 2D data has been segmented. \u00c7i\u00e7ek et al. [25] used the architecture of U-Net for the task of volume segmentation using a 3D convolutional kernel for feature extraction. licensee et al. [26] proposed an adaptive framework nnUNet based on both 2D and 3D Adaptive framework which adapts the segmented target for feature extraction at multiple scales. Roth et al. [27] proposed a 3D fully convolutional segmentation network mainly for multi-organ segmentation. Schlemper et al. [28] proposed a novel Attention Gate (AG) model that automatically learns to attend to different shapes and sizes of target structures. The mu-Net proposed by [29] adds a residual path with de-convolution and activation operations at the skip junction of up-sampling and down-sampling to extract high-level global features for small object inputs and high-level features for large object inputs with high-resolution edge information, and it performs well for 3D segmentation of the liver in CT.\nMerged U-net [45] deals with the task of 2D segmentation of bone tumor radiographs. The noteworthy part of this model is merge gates are used to deal with the problem of blurred segmentation boundaries. Among other things, the merge gate is used in the process of connecting the down-sampled features to the up-sampled features of the model, making the feature connections selective. Other than that, as a deep learning framework for image semantic segmentation, DeepLab v3+ [50] uses dilated convolution to obtain a larger receptive field and performs better in the precision processing of segmentation boundaries. Kubilay Muhammed Sunnetci et al. [51] proposed a new segmentation model architecture based on DeepLab v3+ for MRI images of Comprehensive Parotid Gland Tumor disease, and has better processing capabilities for tumor edge information."}, {"title": "2.3 Visual Transformer-based Segmentation Networks", "content": "Visual transformer (VIT) has become popular recently due to its excellent performance in deep learning segmentation. VIT was originally proposed for the NLP domain [30], then Bichen et al. [31] used Transformer for visual processing problems, which is becoming more and more widely used in computer vision [32] [33]. The core idea of VIT is the self-attention mechanism, which can extract more global information and better connect the spatial information. Karimi et al. [38] proposed to divide the volume image into three-dimensional modules, and using a module with an adjacency-attention mechanism gives better results than the convolution module in some work. Hu et al. [34] proposed a model that uses a pure Transformer to replace the convolution operation of U-net to perform local-convolutional operations. Qiu et al. [52] propose to introduce spatial dynamic components into ViT-UNet to efficiently capture the features of target objects with different appearances."}, {"title": "2.4 Hybrid segmentation methods", "content": "In addition to pure CNN or ViT-based segmentation network design, more and more work has begun to explore hybrid frameworks. Wang et al. [39] applied the ViT to segmentation of MRI brain tumors in 3D. Xie et al. [40] proposed a new framework, CoTr, that efficiently connects the CNN and the ViT to achieve accurate 3D medical image segmentation. Hatamizadeh et al. [36] designed a model of UNETR which completely uses the Transformer as an encoder to down-sample the data and the CNN as a decoder to up-sample and get more global information. Swin-UNETR [35] uses Swim-Transformer to compute the self-attention by utilizing a shift window, which is connected to the FCNN-based decoder for feature extraction. It can effectively extract the feature of the segmented task, but in the processing of each layer separately, it does not get the edge information well, resulting in the segmentation result is not ideal. nnFormer model proposed by Zhou et al. [41] combines convolutional and self-attention mechanisms interleaved organically, replacing the single CNN or Transformer module. However, in the task of 3D image segmentation, the performance is not good to obtain the segmentation results with smooth edges, but from a certain layer, the comparison and label overlap. Shaker et al. [37] devised a new Efficient Attention Module (EPA), which uses a pair of spatial and channel-based attention mechanism modules with interdependent shared weights to efficiently learn discriminative features in both spatial and channel directions.\nIn the medical 3D image segmentation task, the models with outstanding results are all hybrid models of convolutional U-net and Transformer. As the current SOTA model, it is necessary to have a deeper understanding of the internal structure of the UNETR++ [37]. The biggest contribution of UNETR++ is the design of an EPA block, which allows UNETR++ to obtain good results with the least number of parameters and computational cost. One of the main enhancements of the EPA block is to artificially set up a two-headed self-attention mechanism for the space and the channel and to use the same Q and K matrices thus emphasizing the strong correlation between the two. However, we found that UNETR++ is not very effective in dealing with the problem that the target boundary is not very clear. There are even cases where the segmentation surface is not uniform sometimes. In this regard, our conjecture reason is that the spatial attention in the EPA block in UNETR++ has the problem of information loss during the linear dimensionality reduction process. However, UNETR++ also works well on common organ segmentation surfaces. The explanation given for this contradiction is that the corresponding missing part of the information is supplemented when the down-sampled features are connected to the up-sampled ones. This may also explain the fact that UNETR++ is abnormal for a few slices when organs are delineated by thin slats."}, {"title": "3 Methodology", "content": "We introduce a Trans-Spatial UNet-like Network with Bi-direction Fusion(TSUBF-Net) to enhance the precision of the 3D segmentation model concerning the challenge of adenoid hypertrophy. As expounded in the above section, our analysis reveals that the adenoid hypertrophy task exists in the absence of discernible geometric boundaries within the image. In light of this intrinsic trait, we have devised two modules: the Trans-Spatial Perception (TSP) module and the Bi-direction Sample Collaborated Fusion (BSCF) Module. Moreover, to augment the overall smoothness of the segmentation output, we introduce an explicit measure of segmentation smoothness, which bases on the application of the 3D Sobel operator and is integrated in the loss function."}, {"title": "3.1 Network framework", "content": "We formulate TSUBF-Net, a framework that integrates the TSP module and BSCF module to facilitate the segmentation of adenoid hypertrophy. An overview of this architecture is shown in Fig. 3.\nDue to the advantages of multi-scale information fusion, our proposed TSUBF-Net follows a U-shaped architectural configuration. Notably, to combine the advantages of convolution and attention, the integration of convolutional feature extraction and TSP module is adopted in both up-sampling and down-sampling paths. In the down-sampling phase of the model, the initial image passes through the patch embedding layer, subsequently undergoing convolutional feature extraction, resulting in a down-sampled image resolution, reduced to one-quarter of the original size. Concomitantly, the dimension of the 3D feature representation undergoes continuous and exponential diminishment throughout four successive down-sampling stages, while its channel features increase."}, {"title": "3.2 Trans-Spatial Perception Module", "content": "To address the inherent absence of distinct geometric boundaries in the adenoid hypertrophy segmentation, we recognize the necessity for augmenting our attention module with spatial perception capabilities. Therefore, we introduce the TSP module. It is designed to enable effective global attention mechanisms, thereby adeptly capturing intricate spatial and channel feature representations. Given the 3D image segmentation features encompass four dimensions, we extend attention operations to each of these dimensions. As a result, the TSP module is predominantly comprised of a four-head attention mechanism, as depicted in Fig. 4. We categorize the four-head attention into two distinct components: channel attention and spatial attention. The latter emphasizes the spatial features interplay between different layers, with three heads dedicated to inter-layer attention across the height, width, and depth dimensions of the 3D image. We define the input of the TSP module as $x \\in R^{H \\times W \\times D \\times C'}$, then initiate a linear mapping process to derive the distinct Q, K, and V matrices essential for the attention mechanism. Specifically, this entails the generation of a total of 8 mapping matrices. The three constituent sub-attentions within the spatial attention encompass height attention, width attention, and depth attention. The weights assigned to the linear layers responsible for Q and K operations within the spatial attention are shared across all three layers of attention. Therefore, the spatial attention mechanism is computed concomitantly with the channel attention:\n$X_L = LA(Q_s, K_s, V_h, V_w, V_d)$", "equations": ["X_L = LA(Q_s, K_s, V_h, V_w, V_d)", "X_c = CA(Q_c, K_c, V_c)"]}, {"title": "Spatial Attention:", "content": "The spatial attention module encompasses three distinct layer-specific attention modules. It is essential to reshape the feature values' dimensions before the processing of different attentions. More precisely, for height attention, we reshape $Q_s, K_s,$ and $V_h$ with dimensions of $R^{H \\times W \\times D \\times C}$ to $R^{WDC \\times H}$. Similarly, for width and depth attention, we transform $Q_s, K_s,$ and $V_w$ to dimensions of $R^{HDC \\times W}$ and $R^{HWC \\times D}$. Subsequently, the attention operation is performed independently across these three reshaped dimensions.\nThe attention output is further processed through a linear layer, which facilitates the extraction of features with reduced dimensionality, specifically shrinking it to one-quarter of its original size. This condensed representation is then arranged in a stacked fashion. In more precise terms, we obtain $X_L \\in R^{H \\times W \\times D \\times \\frac{C'}{4}}$ following the application of spatial attention module."}, {"title": "Channel Attention:", "content": "It is leveraged to acquire channel information through the calculation of $Q_c, K_c,$ and $V_o$ features, which share the same shape $R^{HWD \\times C}$. It is imperative that the shape of the fused features align with the input, necessitating a dimension reduction via a linear layer to extract the hidden size and compress it to 1/4 of its original dimension. Consequently, we obtain $X_c \\in R^{H \\times W \\times D \\times \\frac{C'}{4}}$ following the execution of the channel attention mechanism.\nConcludingly, we execute a summative fusion process, where the outputs stemming from both attention modules transform a convolutional block, thereby yielding an enriched feature representation. The ultimate output of the TSP module, denoted as $\\bar{X}$, is characterized by the following form:\n$\\bar{X} = Conv_1 (Conv_3 (Concat (X_L, X_c)))$", "equations": ["\\bar{X} = Conv_1 (Conv_3 (Concat (X_L, X_c)))"]}, {"title": "3.3 Bi-direction Sample Collaborated Fusion Module", "content": "In the TSBFU-Net framework, a combination of up-sampling and down-sampling processes is employed. The features acquired through the down-sampling phase encompass the geometric contour information inherent to the original image, while those obtained in the up-sampling phase contain more relevant information about the segmentation target. Given the peculiar attribute of the adenoid hypertrophy segmentation task, characterized by the absence of clear geometric boundaries, it becomes paramount to obtain a wealth of segmentation-relevant features. Consequently, our framework orchestrates the fusion of down-sampled geometric contour features with up-sampled segmentation features. In contrast to the direct connections in conventional U-shaped networks, we design the Bi-direction Sample Collaborated Fusion Module for intricate fusion inspired by Merged U-Net [45].\nThe BSCF module, as illustrated in Fig. 5, applies a uniform 3\u00d73\u00d73 convolution kernel in the initial processing step for both up-sampled and down-sampled features. This shared convolutional operation is designed to improve the congruence and alignment between these two sets of features. Subsequently, a 1\u00d71\u00d71 convolutional operation is introduced to facilitate further feature extraction. In this process, the attention mechanism effectively rectifies the information of up-sampled features based on segmentation-related cues derived from the upper sampling phase.\nBased on the TSP module, the BSCF module receives the up-sampled feature $X_u \\in R^{H \\times W \\times D \\times C}$ and the down-sampled feature $X_d \\in R^{H \\times W \\times D \\times C}$ both sharing the identical dimensions of $H \\times W \\times D \\times C$. As elucidated in Fig. 5, a series of convolutional operations is initially conducted to obtain the depth features $\\bar{X_u} \\in R^{H \\times W \\times D \\times C}$ and $\\bar{X_d}$. The computational procedure is expounded as follows:\n$\\bar{X_u} = Conv_{1u} (Conv_{1u} (Conv_{3s} (X_u)))$", "equations": ["\\bar{X_u} = Conv_{1u} (Conv_{1u} (Conv_{3s} (X_u)))", "\\bar{X_d} = Conv_{1d} (Conv_{1d} (Conv_{3s} (X_d)))"]}, {"title": "3.4 Loss Function with Sobel Gradient", "content": "In order to indistinct delineation of the segmentation target, we improve the loss function through the incorporation of the gradient operator. Specifically, we have elected to employ the Sobel gradient operator [43] to formulate the Sobel loss so as to minimize the global gradient magnitude as much as possible.\nThe original loss function consists of two components including the soft dice loss and the cross-entropy loss [42], which is defined as follows:\n$C(Y,P) = 1- (\\frac{2 \\times \\sum_{i=1}^I \\sum_{v=1}^V Y_{v,i} P_{v,i}}{\\sum_{v=1}^V Y_{v,i}^2 + \\sum_{v=1}^V P_{v,i}^2} + \\sum_{i=1}^I (\\sum_{v=1}^V Y_{vi} log P_{v,i}))$", "equations": ["C(Y,P) = 1- (\\frac{2 \\times \\sum_{i=1}^I \\sum_{v=1}^V Y_{v,i} P_{v,i}}{\\sum_{v=1}^V Y_{v,i}^2 + \\sum_{v=1}^V P_{v,i}^2} + \\sum_{i=1}^I (\\sum_{v=1}^V Y_{vi} log P_{v,i}))"]}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "Adenoid Hypertrophy Segmentation Dataset is a comprehensive dataset that we present focused on adenoid hypertrophy. As shown in Table 1.) This dataset was meticulously curated in collaboration with Shenzhen University General Hospital, Guangdong Province, Shenzhen, China, marking a significant innovation in the field. The data acquisition process underwent rigorous ethical scrutiny, receiving formal approval from the Ethical and Welfare Committee of Shenzhen University General Hospital (Approval No: KYLL-20230402A. Throughout our research, we maintained unwavering adherence to the ethical protocols and mandates outlined by the committee, ensuring the utmost protection of participants' rights and privacy. AHSD encompasses head CT images derived from a cohort of 240 patients, comprising 227 individuals diagnosed with adenoid hypertrophy and 13 individuals categorized as normal cases. We collected this data using Imaging Sciences International Company specification model I-CAT FLX. All of our CT data volumes were 16D x 13H cm, and the number of slice layers along the Z axis was 533, each slice was 0.3 mm. We would like to be able to make this dataset public, but we are still going through the application and approval process due to the privacy of the data. In our investigation, the primary focus was the precise segmentation of adenoid hypertrophic regions. Notably, from the pool of 227 patients exhibiting adenoid hypertrophy, we randomly designated 189 instances for training purposes, with the remaining 38 instances earmarked for the evaluation of our proposed methodology.\nAutomatic Cardiac Diagnosis Challenge [44] is introduced to demonstrate the effectiveness of our methods in a broader comparison. Designed for the purpose of automated cardiac diagnostics, ACDC encompasses cardiac MRI images acquired from a cohort of 100 patients during authentic clinical examinations. These images are meticulously annotated with segmentation information for key cardiac structures, including the right ventricle (RV), left ventricle (LV), and myocardium (MYO). The patient samples were thoughtfully selected to represent a diverse range of clinical scenarios, encompassing individuals with normal cardiac function, myocardial infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, and patients displaying abnormal right ventricle morphology. In consonance with the data splitting strategy akin to the Reformer framework [33], the dataset was partitioned into three distinct subsets. Specifically, 70 images were designated for training, 10 samples were earmarked for validation, and the remaining 20 samples underwent rigorous evaluation in our experiments. We chose this dataset for the following reasons: It is an MRI dataset, which can verify whether our algorithm has the same good performance in MRI 3D data as in CT; In addition, the ACDC data set has smooth boundaries, and there are no obvious boundaries in the original image, which is also conducive to evaluating the effectiveness of our model in processing edge smoothness on other data sets.\nMSD-Lung Dataset [55] is introduced to demonstrate the validity of our method on other publicly available CT datasets. The MSD-Lung Dataset is a subtask in the Medical Segmentation Decathlon (MSD). The objective of the dataset was to segment lung tumors, which consisted of preoperative thin-section CT scan images of 96 patients with non-small cell lung cancer. The data was acquired via the Cancer Imaging. Similar to the hypertrophied adenoids in AHSD that make up only a small portion of the head CT, lung tumors also make up a relatively small portion of the CT images."}, {"title": "4.2 Implementation Details", "content": "Data Preprocessing: Motivated by the methodologies presented in nnUNet [26] and UNETR++ [37], our initial step involved an extensive data preprocessing pipeline. Each CT scan image underwent independent preprocessing, where we employed intensity normalization techniques to map values within the [-1000, 1000] Hounsfield Unit (HU) range to the normalized range of [0, 1]. Subsequently, we applied a patch-based cropping strategy to obtain segments of size 192 \u00d7 192 \u00d7 64.\nEvaluation Metrics: The evaluation of our model's performance is grounded in two essential metrics, namely the 95% Hausdorff Distance (HD95) [58], Intersection over Union (IoU) and the Dice Similarity Score (DSC) [57].\nHD95 is commonly used as a boundary-based metric to measure the 95th percentile of the distance between the boundary predicted by volume partitioning and the ground truth voxel. It is defined as follows:\n$HD_{95} (Y, P) = max (d_{yp} + d_{py})$", "equations": ["HD_{95} (Y, P) = max (d_{yp} + d_{py})"]}, {"title": "4.3 Results", "content": ""}, {"title": "4.3.1 Adenoid Hypertrophy Segmentation Dataset", "content": "The comparative results for the AHSD are presented in Table 2. Notably, performance metrics are reported exclusively based on single-model accuracy, with no utilization of pre-training, model ensembling, or supplementary data. For purely Convolutional Neural Network (CNN)-based methods, the U-Net exhibits a commendable DSC score of 80.95% but a poor HD95 score. Among the existing hybrid-transformer-CNN-based methods, UNETR and Swin-UNETR deliver promising DSC scores of 83.74% and 87.64%, respectively. Notably, our proposed TSUBF-Net surpass the UNETR++ model in terms of the highest DSC value, registering an impressive DSC of 92.26%. Furthermore, TSUBF-Net attains superior results, as evidenced by its lowest HD95 metric. These results collectively underscore the remarkable performance of TSUBF-Net on this dataset.\nInterestingly, AgileFormer, which has relatively good performance on other datasets, has lower performance in AHSD. This is because AgileFormer is not a true 3D segmentation model.AgileFormer processes 3D data by performing slicing operations along the z-axis and thus as model inputs, which prevents the model from spatially learning in the z-axis direction. Especially in AHSD tasks where geometric boundaries are missing and segmentation results need to be obtained by relative spatial position and continuity of the segmentation plane, AgileFormer has difficulty in learning the correct segmentation method.\nIn Fig. 6, we present a qualitative juxtaposition between the baseline model and the proposed TSUBF-Net in the context of AHSD. Our illustration employs three distinct segmentation cases to provide a comprehensive demonstration, which demonstrates the superiority of our approaches.\nOur proposed approach exhibits substantial performance enhancements across various facets of adenoid hypertrophy. Our visualization comparisons underscore the impressive segmentation capabilities of the proposed methods. Notably, when confronted with indistinct regions or blurred edges in the initial predictions, our approach adeptly leverages inter-layer relationships to yield smoother and more rationalized segmentation predictions."}, {"title": "4.3.2 Automatic Cardiac Diagnosis Challenge", "content": "Since there are no other publicly available adenoid hypertrophy datasets, we supplemented our experiments with the ACDC dataset to further investigate the performance of TSUBF-Net for the smoothness of segmentation results. We use the DSC metric to evaluate the results of our method on the ACDC dataset. The comparative analysis results are presented in Table 3. The average DSC for UNETR++ and AgileFormer stands at 92.83% and 92.55%, respectively. And the average DSC of TSUBF-Net reaches 92.68%, which is higher than that of AgileFormer and other models, and only slightly lower than that of UNETR++. It is particularly noteworthy that TSUBF-Net excels in segmenting the myocardium (Myo) and left ventricle (LV) with accuracy scores of 90.67% and 96.27%. Overall, TSUBF-Net emerges as one of the top-performing models in this domain.\nIn Fig. 8, we present a qualitative comparison involving the application of our proposed TSUBF-Net to the ACDC dataset. A focused examination of the outcomes within the delineated red box reveals noteworthy insights. Although our proposed model may not exhibit exceptional performance when applied to publicly accessible organ segmentation datasets, it distinguishes itself through its remarkable capacity to generate results characterized by a superior degree of smoothness.\nFurthermore, our proposed model demonstrates a particular adeptness in producing smoother predictions, particularly when the Ground Truth labels exhibit reduced inherent smoothness. Notably, UNETR++ delivers results of a comparable quality, yet our proposed model demonstrates a subtle, but discernible, edge in terms of smoothness, especially in select intricate case details. These findings emphasize the nuanced strengths of TSUBF-Net in addressing specific intricacies within the segmentation task."}, {"title": "4.3.3 MSD-Lung Dataset", "content": "We additionally used the MSD-Lung dataset as a supplement to our experiments to further investigate the segmentation performance of TSUBF-Net. The results of the comparative analyses are shown in Table 4. The average DSC for UNETR++ and MedNeXt stands at 80.68% and 80.14%, respectively. Remarkably, TSUBF-Net consistently outperforms conventional 3D segmentation models in medical imaging, achieving an average DSC of 83.69%, outperforming UNETR++ and MedNeXt by 3.73% and 4.4%, respectively. Overall, TSUBF-Net emerges as one of the top-performing models in this domain, slightly outperforming UNETR++ in terms of average accuracy (Average)."}, {"title": "4.3.4 Results of Ablation Experiment", "content": "In Table 5, we present a comprehensive evaluation of the impact of our proposed methods on the baseline UNETR++ framework for the adenoid hypertrophy segmentation task. The efficacy of the models is quantitatively assessed using the Dice Similarity Coefficient (DSC), which serves as a robust indicator of segmentation accuracy. It is important to note that all reported results are based on the evaluation of individual model accuracy.\nOur evaluation reveals that the enhancements within the modules lead to improvements in the context of the adenoid segmentation dataset. Specifically, the TSP module has a slight decrease in accuracy due to a lower parameter than the UNETR++'s EPA module. However, the incorporation of the BSCF module yields more substantial enhancements, resulting in a notable improvement of 2.69% when compared to the baseline. Furthermore, the combination of the two modules culminates in a significant DSC enhancement of 3.27%. The most noteworthy augmentation is observed when the new loss function is employed, leading to the highest recorded DSC value of 0.9226, signifying an impressive improvement of 4.57% compared to the baseline. These findings underscore the effectiveness of the proposed modifications in enhancing the segmentation accuracy of TSUBF-Net for the adenoid hypertrophy task. We also use different A values to replace the hyperparameters in the loss function, and the A values mainly affect the loss part. In our experiments, we found that the best score was achieved at 0.1, with a 4.57% Enhancement.\nThe complexity of the model has a direct impact on the training and inference of the model and is positively correlated. We calculated the time spent in training 1000 rounds of the model under different FLOPs(G) and the time spent in reasoning a sample. When FLOPs (G) =92.2, it takes 96 hours for model training and 7 minutes for inference of a single sample. When FLOPs (G) =134.07, it takes 99 hours for model training and 9 minutes for inference of a single sample. When FLOPs (G) =154.47, the model training takes 102 hours and the inference for a single sample takes 10 minutes. Although our method is compared with UNETR++, the model training and reasoning time are slightly longer than theirs, but our accuracy and other indicators are completely beyond their indicators."}, {"title": "4.4 Discussion", "content": "In this section, we mainly discuss our experiments in three datasets and the ablation experiments. From the perspective of our adenoid dataset (AHSD), our results in the three evaluation indicators are far better than other models, and the scores of HD95, IoU and DSC are 7.03, 85.63, and 92.26 respectively. There is a significant improvement in the index of IoU. Although only a small improvement over nnUNet, it is limited by its complex model structure at the expense of computational efficiency and time complexity. In addition, compared with UNETR++, our TSP module can greatly reduce the computational complexity, and BSCF module effectively solves the boundary merging problem based on segmentation data. However, in tasks involving clear boundaries, this design may introduce noise and reduce accuracy. Despite these challenges, TSUBF-Net's design principles emphasize interlayer relationships within the loss function, significantly improving segmentation smoothness, so that according to our two innovative modules and the loss function, the model ultimately supports a good balance between accuracy and smoothness in performance.\nIn experiments with the two publicly available datasets we use, our methods also outperform current algorithms. In the ACDC dataset, the DSC values of Myo and LV segments are at a leading level. In the MSD-Lung dataset, we far exceed the current comparison model in the three indexes, reaching HD95:2.37, IoU:71.95, and DSC:83.69. We find that the reason why our TSUBF-Net does not reach the best level in all indicators on ACDC is because this dataset is MRI data and the data quality and imaging mode are different from CT. Another reason is that we are overly focused on boundary smoothness when partitioning tasks, and not all tasks require smooth edges like hypertrophy segmentation of adenoids.\nFor the ablation experiment, we conducted ablation for TSP and BSCF modules. In particular, we also conducted ablation experiments for the hyperparameters in the loss function to verify the influence of hyperparameters on each result. In Table 5, comparing the first and second lines, we find that after replacing the Baseline Model's EPA module with our TSP module, the computational complexity is greatly reduced by 41.87 FLOPs (G), but the accuracy is also slightly decreased by 0.44%. We think that our TSP module can reduce a lot of complexity at the expense of a little precision. In addition, the addition of our proposed BSCF module can greatly improve the accuracy of the model, although it also increases by 62.27 FLOPs (G). In summary, the two modules proposed by us can balance the computational complexity and precision indexes well. For the ablation part of the loss function, we set a total of three different values for the ablation experiment, namely \u03bb = 1.0, 0.5, 0.1, and 0. According to the results, when the A is set to 1.0, the three evaluation indicators all get the best results. However, we believe that there is still some room for improvement. According to the method we designed, the loss is only used as a regular term to adjust the edge, making the segmentation result smoother. However, as mentioned above, not all datasets have this requirement, so we believe that this A value can be learned. After some feature extraction of data and labels in the model, A is obtained."}, {"title": "5 Conclusion", "content": "In this paper, we propose a framework named TSUBF-Net, designed for the 3D medical segmentation of adenoids. To address the challenges of fuzzy boundaries, we design three innovative components. The Trans-Spatial Perception (TSP) module is introduced to effectively encode continuous spatial and independent channel features, which enhance the model's ability to process features within the 3D medical data. Additionally, we introduce the Bi-direction Sample Collaborated Fusion (BSCF) module optimized for the nuances of 3D medical segmentation tasks. Moreover, the Sobel loss term is introduced to optimize the segmentation trauma smoothness. Notably, these enhancements enable TSUBF"}]}