{"title": "From Abstract to Actionable: Pairwise Shapley Values for Explainable AI", "authors": ["Jiaxin Xu", "Hung Chau", "Angela Burden"], "abstract": "Explainable AI (XAI) is critical for ensuring transparency, accountability and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent Al systems and advance the real-world applicability of X\u0391\u0399.", "sections": [{"title": "1 Main", "content": "With the rapid advancement and widespread adoption of increasingly complex artificial intelligence (AI) black-box models across diverse applications, ensuring model outputs are transparent and understandable is becoming crucial, particularly when they have the potential to significantly impact people's lives. Consequently, Explainable Artificial Intelligence (XAI) has become an essential component of modern machine learning (ML) [1-3]. The importance of XAI centers around three key aspects: (i) building user understanding and trust, (ii) promoting stakeholder accountability by making AI systems auditable, and (iii) supporting adherence to legal requirements and fairness standards [4-6]. Among diverse XAI methods [1-6], feature attribution [7-9] plays a pivotal role by identifying and quantifying the contribution of individual input features to a model's output, clarifying their influence on model predictions at both global and local levels. A common subset of feature attribution is additive feature attribution, where the attribution quantities sum to a specific value, such as the model's prediction [10]. Shapley values, derived from cooperative game theory [11], are recognized for their mathematical fairness and consistency in assigning contributions to features by evaluating all possible combinations [12]. To unify additive feature attribution methods, Lundberg and Lee [10] introduced SHAP (SHapley Additive exPlanations), a framework that satisfies desirable properties like consistency and local accuracy, making it effective for interpreting models of increasing complexity.\nDespite their theoretical appeal, Shapley value estimation methods in XAI face significant challenges in practical interpretability. Traditional explanations based on feature attribution can be complex and difficult for users to interpret. For instance, as shown in Fig. 1, traditional feature attribution\nThis work was done during Jiaxin Xu's internship at Zillow Group."}, {"title": null, "content": "explanation is inherently implicit, as it often relies on sampling from empirical feature distributions using reference explicands X' (also referred to as the background dataset) [10]. This opacity stems from the abstract baseline value E[f(X')], which represents the feature attribution when no features of target explicand x are known. Additionally, Shapley values for individual features, such as x1 (1), are computed from expectations based on the empirical feature distribution (E[f(X') | X.,1 = x1]), obscuring the specific reference values. This abstraction makes it difficult to relate explanations to tangible comparisons, limiting their practical utility. Furthermore, while the Shapley value is uniquely defined by certain axioms [11], its application in model explanation varies based on how the model, training data, and explanation context are utilized, leading to inconsistent outputs which challenges the applicability of its theoretical uniqueness [12]. Additionally, Shapley value methods often face a trade-off between being true to the model (e.g., marginal Shapley values) and being true to the data (e.g., conditional Shapley values), complicating the attribution process and further reducing interpretability [13]. These challenges underscore the need for a more intuitive and accurate feature attribution method that balances model fidelity with human interpretability.\nIn cognitive psychology, analogical reasoning refers to the process by which individuals understand novel situations by relating them to similar and familiar ones, thereby facilitating learning and problem-solving [14, 15]. For example, in real estate valuation, homes are valued via a comparative market analysis (CMA) [16-18]. In a CMA, a property's value (the subject home) is estimated by comparing it to recently sold properties with similar attributes (comparable homes), such as location, size, and condition. To determine a fair market price for the subject home, the sale price of the comparable home is adjusted for differences in amenities, location and time. Similarly, in drug design, the concept of a \"comparable reference\" can be applied in developing HIV treatments. Comparisons often focus on protease inhibitors sharing a common functional group essential for activity. Once this critical functional group is established, medicinal chemists can systematically modify other parts of the molecule to enhance efficacy, selectivity, and pharmacokinetics. Inspired by these human evaluative strategies, our proposed Pairwise Shapley Value method introduces a tangible reference point into the feature attribution process (Fig. 1). By selecting a comparable baseline from the background dataset that closely matches the target explicand, the pairwise method assigns attributions to the differences in feature values between the target explicand and its reference. Pairwise Shapley values are explicit as they ground explanations in concrete, relatable comparisons, mimicking the human decision process. Furthermore, by conditioning on certain important features in the input space, the risk of generating off-manifold data is reduced and complex feature interactions are eliminated, leading to simpler and more intuitive explanations.\nIn summary, our key contributions are:\n\u2022 Pairwise Shapley Values, a novel Shapley values-based approach that enhances human-relatable"}, {"title": "2 Related Work", "content": "2.1 Feature Attribution Methods\nFeature attribution methods quantify individual feature contributions to an ML model's prediction, either locally (for specific instances) or globally (aggregated across all predictions). Additive feature attribution methods, which sum feature contributions to a specific value, such as the model's output, are notable for their clear and interpretable decomposition of predictions. Feature attribution methods can be categorized into (i) model-agnostic methods, which generate explanations by sampling predictions from a black-box model. This includes method such as LIME (Local Interpretable Model-agnostic Explanations) [19-22], SHAP [10, 21, 22] and Shapley sampling values [23]; and (ii) model-dependent methods, which leverages the specific ML model architecture. This includes techniques such as using linear models, Saliency Maps [24-27], DeepLift [28, 29], and Integrated Gradients [30-33].\n2.2 Shapley Values\nShapley values [34], originating from cooperative game theory, describe how the payout of a game should be divided fairly among a coalition of players, and have several useful properties: (i) Efficiency: individual player payouts must sum to the total payout minus the original (null player) payout, (ii) Symmetry: when two players contribute equally to a game (over all possible player coalitions), they should be assigned the same payout, (iii) Dummy: a player that does not influence the game should be assigned zero payout, (iv) Additivity: For a game that includes multiple sub-games and a summed total payout, the total payout for a single player must equal the sum of their payouts for each individual sub-game.\nThe Shapley value for a player i is defined as:\n$\\Phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)],$ (1)\nwhere N is the set of all players, S is a subset of N players not containing player i, v(S) represents the payout (value function) based on the subset of players S, and |S| denotes the cardinality of set S. Equation (1) calculates the weighted average of the marginal contributions of player i across all possible subsets S.\nThis technique has been adapted to derive explanations from black-box ML models by analogizing the model prediction as the game payout and the model features as the game players [10, 23]. However, computing Shapley values in ML settings can be complex and computationally intensive, which has spawned a variety of estimation algorithms [35].\n2.3 Feature Removal in Shapley Value Estimation\nDuring Shapley value estimation, feature removal simulates the absence of a feature by replacing or marginalizing its effect when computing the value function v(S) [35]. The method of feature removal can significantly influence the resulting feature attribution values and their interpretation [12, 36, 37]. Two common strategies are single-value removal and distributional removal [35].\nSingle-value removal replaces the feature to be removed with a fixed reference value. The set of fixed reference values is often termed the baseline [12]. A common choice is zero, but other values like the mean or median of the feature can also be used. v(S) is computed by setting the features not in S (denoted as S) to their corresponding baseline values. While straightforward, this method may introduce unrealistic data instances in the Shapley value estimations that can result in misleading attributions [38, 39]. Rather than substitute a feature with a single value, distributional removal"}, {"title": null, "content": "integrates over a range of possible values, considering the feature's empirical distribution. There are various ways to do this, we examine 3 common methods:\n\u2022 Uniform Distribution: A feature is replaced with values sampled uniformly across its range. This technique assumes all feature values are equally probable, which might not align with the actual data distribution.\n\u2022 Marginal Distribution: A feature is replaced by sampling its marginal distribution. This approach breaks dependencies between features, potentially leading to implausible data points in the Shapley value estimation. The value function, v(S), is then the expected value of the model output when features not in S are marginalized independently:\nv(S) = Ep(x5)[f(xs,x5)]. (2)\n\u2022 Conditional Distribution: A feature is replaced by sampling its conditional distribution relative to other features. This method preserves dependencies among features, resulting in more realistic data instances in the Shapley value estimation but at the cost of increased computational complexity. Furthermore, multicollinearity between features may cause non-zero attributions for features with no influence on the model outcome [35]. The value function is the expected value given the features in S, considering their dependencies:\nv(S) = Ep(x5|xs) [f(xs,x5)]. (3)\nSelecting the appropriate feature removal approach is important for generating meaningful and reliable Shapley value-based explanations."}, {"title": "3 Pairwise Shapley Values", "content": "Our framework integrates a ML base model with a pair selection algorithm and feature removal based on explicit pairs (the target explicand and the similar reference explicand), as illustrated in Fig. 1.\n3.1 Pair Selection Algorithm\nTo provide flexibility and balance between general applicability and domain-specific precision, we employ three pair selection algorithms to identify a reference explicand for a target explicand:\n\u2022 Random: Selecting a reference explicand (x') randomly from the background dataset (X'). This serves as a baseline to compare against more structured selection strategies.\n\u2022 Similar: Choosing a reference explicand that closely matches the target explicand based on common similarity metrics, such as cosine similarity, Euclidean distance, or correlation-based measures. These algorithms represent a generalizable method applicable to any use case without requiring domain knowledge.\n\u2022 Comparable: Defining similarity based on specific application criteria. This is a specialized approach tailored to different domains, where domain knowledge is needed to identify and condition on the \"more important\" factors, ensuring relevance and interpretability in context. For example, in a CMA, comparable homes are typically selected based on similarity in attributes such as sale timing, location, size, and condition [16-18].\n3.2 Shapley Value Estimation Based on Explicit Pairs\nAfter selecting an explicit reference, we use single-value imputation to calculate the value function v(S). This involves replacing the removed features (\u0160) in the target instance (x) with the corresponding features in the reference instance (x') and observing the effect on the model's prediction:\nv(S) = f(xs,x's). (4)"}, {"title": null, "content": "We compute the Shapley values \u03c6\u2081(v) (denoted as in Fig. 1) using the KernelSHAP [10], a popular model-agnostic approach that approximates Shapley values by solving a weighted least squares problem. This approximation is necessary in practice because calculating exact Shapley values requires evaluating all 2N possible feature subsets, which is computationally infeasible for models with a large number of features. However, although not implemented in this work, we note that conditioning on large subsets of features with zero variance, that we coin \"dummy pairs\", where xi = x, in our pair selection algorithm, allows us to omit them from the stack of evaluated feature permutations, thereby significantly decreasing the runtime of the Pairwise Shapley algorithm.\nAs well as adhering to the Shapley value properties (efficiency, symmetry, dummy, additivity), the pairwise method has several desirable properties not always encountered in the other Shapley value approximation methods discussed:\n1. Additive inverse: Feature attributions derived by swapping the baseline and explicand have the additive inverse contribution, \u03c6\u2081(x,x') = \u2212\u00a2i(x', x). For example, in the home valuation problem, the contribution to the difference in home value attributed to the difference in square foot between a subject and comparable home would be di dollars comparing subject to comparable home and - dollars comparing comparable to subject.\n2. Dummy pairs: Features the same in both baseline and explicand are named \"dummy pairs\u201d. Dummy pairs are never attributed any value, but dictate the locality of the value function for evaluation. They can be considered locally independent from the remaining feature set.\n3. Single feature contribution: It follows that when differences between data pairs are isolated to a single feature, that feature (by elimination) is also independent. All changes in the model outcome will therefore be attributed to changes in that feature.\n4. Logical attribution evaluation: Where features are expected to contribute monotonically to the output (for example, square foot) we can easily evaluate our predictive ML model for expected behavior with the pairwise method, on real data."}, {"title": "4 Results", "content": "4.1 Base Machine Learning Models' Performance\nWe first train predictive ML models to ensure reliable feature attributions. Since explanations depend on model predictions, a sufficiently accurate base model is essential. Feature explanations inherently depend on the model and its predictions; hence, a sufficiently accurate predictive model is essential for meaningful and interpretable attributions. We utilized the Tree-based Pipeline Optimization Tool (TPOT) [40], an automated ML (AutoML) tool, to automatically select the model pipeline and hyper-parameters for our datasets. In Table 1 we present the performance of the base ML models on three datasets: (1) the King County home price prediction dataset [41] (referred to as \"Home\"), (2) the polymer dielectric constant prediction dataset [42] (referred to as \"Polymer\"), and (3) the drug HIV replication inhibition ability prediction dataset [43] (referred to as \u201cDrug\u201d). These datasets encompass both regression and classification tasks, offering a testbed for our feature attribution estimation framework. Detailed description for each dataset and task are provided in Section 6.1.\nThe base models demonstrate competitive predictive accuracy compared with commonly used benchmarks, ensuring a adequately reliable foundation for feature attribution analysis. Details of the TPOT pipeline and model training are provided in Section 6.2, and additional visualization of results are included in the Appendix A.1."}, {"title": null, "content": "As an illustrative example, we also summarize a protocol for iterative explainable ML model development based on feature attribution feedback and domain knowledge, using the Home dataset as an example. Details can be found in Appendix B. This protocol highlights how explainability-driven insights can guide enhancements in model performance and interpretability, demonstrating the complementary roles of predictive accuracy and interpretability in practical ML workflows. We hope this protocol serves as a useful guideline for researchers and practitioners in domains where both high model accuracy and interpretability are essential for informed decision-making and regulatory compliance.\n4.2 A Comparison of Existing Feature Removal Methods\nUsing the base ML models in Section 4.1, we compare feature attribution results obtained from different feature removal methods in Shapley value estimation, evaluating both global (dataset-wide) and local (instance-level) explanations. We examine several established feature removal methods for Shapley value estimation, including single-value imputation methods using zeros (B0) and mean feature values (BM), uniform distribution imputation (UF), marginal distribution imputation using all training data (MA) or a K-means summary of the training data (MK), conditional distribution imputation using all training data (CA), and a model-specific method, TreeShap (TS), which is suited for tree-based ML models selected by the AutoML pipeline. Implementation details of these methods are listed in Section 6.3."}, {"title": null, "content": "Figure 2 presents the mean attributions for key features (grade, sqft, and city_BELLEVUE) in the Home dataset, highlighting substantial variability across imputation methods (see initial feature importance analysis in Appendix Fig. 11). For instance, among the three selected features, MA assigns significant importance to feature grade, while CA emphasizes feature sqft. Meanwhile, although MA and MK share a similar conceptual foundation, they yield markedly different results; MA assigns significant importance to the grade feature, whereas MK prioritizes the sqft feature. Similar trends appear in the Polymer and Drug datasets (Appendix Fig. 12), illustrating how baseline selection impacts feature importance in Shapley value estimation.\nFigure 3(a)-(b) provides an example of local feature attributions for one explicand in the Home dataset. Consistent with the global results, the local attributions from MA and CA show considerable"}, {"title": null, "content": "variation. In this example, MA (Fig. 3(a)) assigns -97k to 1360 sqft whereas CA (Fig. 3(b)) assigns -316k to 1360 sqft. More examples for other datasets are in Appendix A.3 (Fig. 13 and 14). In addition to the variation in the explanation results, a notable limitation of these imputation methods, especially distributional sampling-based methods, is their lack of direct interpretability. The reliance on imputation obscures the exact benchmark feature value against which the explicand is compared, making it challenging to derive actionable insights from the local explanations. For example, in Fig. 3(a), the explicand has a grade of 6 and a sqft of 1360, which are associated with approximate reductions in the predicted home price of 183k and 97k, respectively. However, we don't know the benchmark feature values of grade and sqft against which these reductions are calculated. The reference values remain obscured due to the sampling processes employed in the imputation."}, {"title": null, "content": "Figure 3: Waterfall plot of feature attributions for an explicand from the Home dataset using different methods: (a) MA, (b) CA, and (c) PC. The y-axis lists the top nine features with their values for the explicand (relative values in (c)); remaining features are aggregated as \"Other Features.\u201d The x-axis shows feature attributions in dollars. f(x) is the model's predicted value; E[f(X)] is the expected prediction based on the background data.\nTo enable a fair comparison across feature removal methods and mitigate the impact of baseline variations (E[f(X')] in Fig. 1 and E[f(X)] in Fig. 3), we normalize feature attributions, yielding the predicted marginal change in the target variable per unit of a specific feature. For example, in the Home dataset, this allows us to express the sqft feature's impact as dollars per square foot ($/sqft), independent of baseline selection. Normalization is performed by computing differences in Shapley values and feature values across random explicand pairs:"}, {"title": null, "content": "f(x\u00b2) = \u03a6\u03bf + \\sum_{k=1}^{n} \u03a6\nk (5)\nnorm_{k}^{ij} = \\frac{\u03a6_{k}^{ij}}{x_{k}^{j} - x_{k}^{i}} (6)\nwhere x\u00b2 is an explicand with n features and the kth feature value is xi; f(x\u00b2) is the ML predicted value of explicand x\u00b2; do is the baseline value of the feature removal method, o is the Shapley value of the kth feature in explicand x\u00b2, and norm is the normalized Shapley value of feature k between two explicands x\u00b2 and xi.\nUsing feature sqft in the Home dataset as an example, the resulting distribution of normsqft from 500 pairs are shown in Fig. 4(a). The mean and standard deviation across methods are summarized in Table 2. A Kolmogorov-Smirnov (KS) test (using MA as the reference) confirms that normalized Shapley distributions significantly differ (p-values < 0.01, see Table 2), underscoring the variability across feature removal techniques."}, {"title": null, "content": "Figure 4: Distribution of normalized Shapley values for feature sqft in the Home dataset for different methods. (a) Distribution plot for seven existing non-pairwise methods; (b) Distribution plot for three selected existing non-pairwise methods (UF, MA, TS) and three pairwise methods with different similarity algorithms (PR, PC, PS).\n4.3 Pairwise Shapley: Enhanced Interpretability\nWe compute the feature attribution values with the pairwise framework using two different pair selection strategies. First, we reuse the same random pairs of explicands from the prior analysis in normalized Shapley values, denoted as \"PR\" (Pairwise-Random). Second, we devise two similarity-based algorithms to select pairs of comparable explicands for pairwise computation, focusing on identifying explicands with close values for key attributes, denoted as \"PS\" (Pairwise-Similar) and \"PC\" (Pairwise-Comparable).\nPairwise Shapley values show more intuitive explanations by controlling the background data and using informative background feature values, particularly in local explanations. For example, using PC method on the Home dataset, in Fig. 3(c), the explanation shows that a target home with 430 square"}, {"title": null, "content": "feet less than the reference (a comparable home) results in a $27k decrease in the final home price prediction. Similarly, the target home having a traffic noise level 2 units lower than the comparable reference leads to a $38k increase in the predicted home price. This explanation closely mirrors the appraisal process in real-world home valuation, making the explanations more intuitive compared to the non-pairwise methods shown in Fig. 3 (a)-(b). Similar benefits are observed in Polymer and Drug datasets (Appendix A.3).\n4.4 Pairwise Shapley: Improved Model Insights\nBeyond more intuitive explanations, Pairwise Shapley allows us to derive additional insights into model behavior. In this section we show how we can derive model insights by (i) comparing the distributions of \"normalized\" attribution values (ii) measuring the rate of sign matching between attribution values and feature differences, and (iii) observing global attribution values as a function of feature (difference) value.\n4.4.1 Normalized Shapley value\nDifferent from Eq. 6, we compute the normalized Shapley values of pairwise methods directly from explicand pairs as:\nnorm^{ij}_{pairwise} = \\frac{\u03a6^{ij}_{k}}{x^{j}_{k} - x^{i}_{k}} (7)\nwhere x\u00b2 is the target explicand and \u00e6i is the reference explicand in pairwise method; is the Pairwise Shapley value of feature k in explicand x\u00b2 compared to explicand xi.\nFigure 4(b) and Table 2 show the distribution and statistics of normalized Shapley values of feature sqft in the Home dataset for pairwise and non-pairwise methods. Pairwise methods recover a positively skewed distribution of normalized sqft attribution, aligning with our domain knowledge, while non-pairwise methods produce negative skew. A KS test finds no significant differences among pairwise distributions, reinforcing their robustness. Additionally, the pairwise approach requires no extra post-processing, as normalization is inherently built into the attribution process."}, {"title": "4.4.2 Directionality of feature attributions", "content": "For key features, such as sqft, grade, and noise_traffic, we evaluate the attribution values from a monotonicity perspective to show how the pairwise method allows us to evaluate logical attributions. In home valuation, while the exact dollar impact of certain features (e.g., bedrooms, view level, traffic noise) are unknown and/or subjective, the direction of their impacts can be hypothesized. For example,"}, {"title": null, "content": "sqft and grade should positively impact price (i.e., higher feature values lead to higher prices), whereas noise_traffic is expected to have a negative impact (see Appendix Fig. 11). To assess the expected directionality, we compute the percentage of norm values that satisfy the following criteria: (1) norm > 0 for features positively correlated with price (e.g., sqft and grade), (2) norm < 0 for negatively correlated features (e.g., noise_traffic), and (3) norm = 0 for x = x (dummy pairs). As shown in Fig. 5, pairwise methods have the highest monotonicity rates-defined as the matched percentage-across all three features. By evaluating the monotonicity rate of these features, we can deduce if our model is behaving in an expected manner. The high monotonicy rate of these three features in the pairwise method suggests our model explanations align with human expectations and that our model is learning intuitive correlations."}, {"title": null, "content": "Figure 5: Monotonicity measure (matched percentage) of Shapley value estimates across different methods for three features: sqft, grade, and noise traffic.\n4.4.3 Visualizing global attributions\nFigure 6 illustrates how the pairwise method provides a more informative representation of global model behavior. In Fig. 6(a), Pairwise Shapley attributions for sqft increase when the sqft of the target home exceeds that of the reference, aligning with the real-world intuition. In contrast, non-pairwise methods assign Shapley values based on absolute feature values, where sqft is always positive but may represent either an increase or a decrease in predicted home value. This discrepancy becomes particularly noticeable when the feature's monotonic correlation with the target variable is weak, such as grade, noise_traffic, and view_lakewash, as depicted in Figs. 6(b)-(d). In pairwise methods, the relative feature values allow us to clearly infer how an increase or decrease in a feature (e.g., a reduction in traffic noise or the presence of a certain view) impacts the predicted home price. Conversely, non-pairwise methods fail to provide such clear interpretations, as they rely on absolute feature values that do not directly reflect the relative change or its contextual meaning, making the results less actionable with reduced generalization."}, {"title": null, "content": "Figure 6: Beeswarm plots showing the relationship between Shapley values and (relative) feature values for several methods on important features in Home dataset: (a) sqft, (b) grade, (c) noise_traffic, and (d) view_lakewash. The x-axis represents Shapley values (in dollars), and the color bar indicates the feature value for non-pairwise methods and the relative feature value for pairwise methods.\n4.5 Pairwise Shapley: Feature Independence\n4.5.1 Dummy Pair\nPairwise methods always assign zero attribution to dummy features, as the relative feature value is zero. The dummy pair property is valuable as it enables conditioning on specific features to exclude them from explanations (and computations) by selecting pairs of instances with multiple identical feature values. For example, in home valuation, we can compare similar homes within the same locality to isolate local market effects. In the context of polymers or drug molecules, we can focus on the effects of specific functional groups or motifs while excluding shared structural features that are not of interest. In that case, we can condition on shared backbone structures to better isolate and analyze the influence of substituents or side groups on target properties. Figure. 7 shows the dummy pair ratio (the ratio of norm = 0 when x = x) of each method. Non-pairwise methods fail to consistently assign zero attribution to dummy pairs, with CA exhibiting particularly low ratios. This is expected, as conditional Shapley methods assume dependencies among features based on the original data distribution. In contrast, the pairwise method always assigns zero value to a dummy feature pair."}, {"title": "4.5.2 Single feature independence", "content": "The dummy pair property ensures that when only one feature differs between two explicands, the entire prediction difference is attributed to that feature. For instance, when explaining the predicted value of a target home using a comparable home, if the two homes are otherwise identical, the full difference in value should be attributed to the one different feature. However, in non-pairwise methods this does not hold true.\nTo evaluate single feature independence in the context of multicollinearity, we conduct a feature perturbation test by slightly modifying a single feature and analyzing how attributions change. Intuitively, if all other features are held constant, and the data points are close in feature space, the full difference in the prediction should be attributed to the perturbed feature."}, {"title": null, "content": "For this evaluation, we focus on sqft in the Home data due to its continuous nature, minimizing the risk of generating out-of-manifold data for the trained ML model. Synthetic data pairs are created by perturbing the sqft of 500 test instances in steps of \u00b150 sqft (range: -250 to 250). We compare the prediction change with the corresponding Shapley value change to determine whether the attribution is correctly assigned to sqft alone. In pairwise method, the target home is the original test instance, and the reference home is the same instance with a perturbed sqft. In non-pairwise methods, we calculate the feature attribution for both the original and perturbed data points and then compare the change between the original and perturbed data.\nFigure 8 shows a split violin plot, with the left side depicting prediction differences distribution and the right side showing Shapley value differences distribution for sqft. In non-pairwise methods (e.g., conditional-all and marginal-all), the left and right distributions are different, particularly for the conditional-all method, where prediction changes are spread across multiple features. In contrast, the pairwise method maintains identical distributions, confirming the full prediction difference is attributed to the perturbed feature. This highlights the superiority of the pairwise method, which reliably attributes the entire change to the perturbed feature, ensuring interpretability.\n4.6 The Role of Similarity in Pairwise Shapley\nUsing similar data instances as reference explicands in the pairwise method offers several key advantages. First, it reduces the likelihood of off-manifold inputs, which can distort feature attributions due to unrealistic data instances unsupported by training data [39]. Second, it simplifies explanations by conditioning on matched features, effectively removing them from the attribution process. To evaluate the impact of similarity on Pairwise Shapley values, we examine how monotonicity rates (as defined in Section 4.4.2) change with explicand-reference similarity in the Home dataset. Figure 9 shows a"}, {"title": null, "content": "heatmap of monotonicity scores (measured by Spearman correlation) as a function of similarity scores for several positive features in the Home dataset. The result shows that as the similarity between the target and reference explicand increases, the monotonicity rate of feature attributions generally increases. This suggests that higher similarity between data points can lead to more intuitive explanations using the pairwise method, assuming the base model has learned the expected relationship between that feature and the outcome.\n4.7 Runtime Comparison\nWe compare the computational time of various explanation methods for generating Shapley values, for one single explicand from the Home dataset. As shown in Table 3, the Pairwise method is the most efficient, requiring only 0.08 seconds, even without implementing the dummy pair speed-up described in Section 3.2, which could further reduce runtime by many orders of magnitude. This efficiency stems from its reliance on simple single-value imputation and the ability to perform similarity calculations independently of the Shapley value computation. In contrast, marginal and conditional methods are significantly slower due to complex imputations. Detailed settings for each method are in Section 6.3. The Pairwise method's efficiency, combined with its intuitive explanations, makes it better-suited for larger datasets and real-time applications."}, {"title": "5 Discussion", "content": "In this work, we introduced Pairwise Shapley Values, a novel method for generating intuitive and human-interpretable model explanations by leveraging explicit comparisons between proximal data points. Unlike traditional Shapley value estimation techniques that rely on abstract distributional baselines, our method directly attributes changes in model predictions to observed feature differences, enhancing interpretability across diverse domains. By ensuring that identical feature values in explicand-baseline pairs contribute no attribution, Pairwise Shapley Values maintain local independence, enabling more precise feature conditioning and computational efficiency. This property simplifies explanations by eliminating unnecessary perturbations and ensuring that isolated feature differences fully account for prediction variations. Furthermore, our approach is robust to the pair selection routine, demonstrating consistent attributions while significantly reducing computational overhead compared to traditional methods. These characteristics make Pairwise Shapley Values well-suited for tasks requiring transparent, contextually meaningful explanations, such as real estate valuation, materials discovery, and credit risk assessment.\nA key distinction of our work is its contrast with the Formulate, Approximate, Explain (FAE) framework proposed by Merrick and Taly [37]. Both approaches are motivated by the same theoretical concept of using reference points for feature attribution, but FAE constructs attributions using a distribution of reference points with a particular property, whereas our method employs a single-point pairwise comparison. This design choice enables Pairwise Shapley Values to provide more interpretable, instance-specific explanations, avoiding reliance on distributional assumptions. One advantage of FAE is its ability to derive uncertainty estimates for attributions, which is outside the scope of our work. Theoretically, our attributions are point values, and any uncertainty in attributions would propagate from the upstream model prediction uncertainty.\nPairwise Shapley Values introduce computational efficiency benefits, particularly through dummy pair independence, where features identical in both explicand and baseline receive zero attribution. This property allows us to: (i) condition on specific features, thereby reducing explanation complexity, and (ii) achieve significant runtime improvements by eliminating redundant computations. Specifically, removing these features (nc) from the perturbation routine reduces computational complexity from 2n to 2n-ne, a critical bottleneck in many existing Shapley value algorithms [45]. This structured feature selection not only enhances scalability but also ensures that attributions remain relevant to the given prediction task."}, {"title": null, "content": "A crucial consideration in Shapley value estimation is feature dependence, especially when handling"}, {"title": null, "content": "missing data. Traditional approaches model missing features using either marginal or conditional distributions, each of which introduces challenges. Marginal distribution-based methods assume feature independence, potentially creating unrealistic data points, while conditional distribution-based methods retain correlations among missing and observed features, leading to unexpected attributions for non-contributing features [37, 38, 46", "35": ".", "47": "Asymmetric Shapley Values [48", "49": "incorporate directed causal graphs to produce more meaningful attributions. However, these methods require prior knowledge of the causal structure, which is often unavailable in real-world applications"}]}