{"title": "Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models", "authors": ["Bryan Zhang", "Taichi Nakatani", "Stephan Walter"], "abstract": "E-commerce stores enable multilingual product discovery which re-quire accurate product title translation. Multilingual large language models (LLMs) have shown promising capacity to perform machine translation tasks, and it can also enhance and translate product titles cross-lingually in one step. However, product title translation often requires more than just language conversion because titles are short, lack context, and contain specialized terminology. This study proposes a retrieval-augmented generation (RAG) approach that leverages existing bilingual product information in e-commerce by retrieving similar bilingual examples and incorporating them as few-shot prompts to enhance LLM-based product title translation. Experiment results show that our proposed RAG approach improve product title translation quality with chrF score gains of up to 15.3% for language pairs where the LLM has limited proficiency.", "sections": [{"title": "1 Introduction", "content": "As e-commerce shopping websites become localized worldwide, more customers are provided with options to browse products in their preferred language other than the primary language of the store. To accomplish this, modern e-commerce stores enable multilingual product discovery [2, 12, 15, 17, 20, 21] as well as localizing product information such as titles using machine translation (MT) systems [5, 25-27, 29]. Product titles play a crucial role in conveying key details about the products, and it is essential that this information is accurately presented in the localized language.\nE-commerce product title localization traditionally uses a fleet of bilingual neural machine translation (NMT) systems. However, recent advancements in multilingual large language models (LLMs) have demonstrated promising performance in machine translation as a single model for high resource languages [9], which makes LLMs as a viable alternative to conventional NMT for translating product titles. Furthermore, LLMs have shown capabilities to optimize the length and the enrich the content of product titles within a single language, which suggest that they may be able to tackle title enhancement and translation in a cross-lingual manner as a one-step unified process [28]. As a result, LLMs are increasingly becoming a more prominent and integral component of e-commerce stores' product title localization strategy.\nHowever, there are challenges in using LLMs for product title localization in e-commerce: (1) product titles tend to be short, and proper title translation also often require bilingual product-specific terminology and catalog domain knowledge. For example, in the title \"Dance your cares away - greeting card\", the core phrase \"Dance your cares away\" needs to be preserved in the translation, and the formality and style of the titles also must be maintained; (2) e-commerce is dynamic in nature, with new products rapidly emerging worldwide and requiring the model to have up-to-date product-specific knowledge; (3) a large number of language pairs need to be supported in the e-commerce domain but LLMs may not possess sufficient language proficiency for certain language pairs [4], which limits their effectiveness; (4) newer and more capable LLMs are continuously arising, prompting the need to flexibly and quickly replace the LLMs used for product title localization in order to maintain high translation quality.\nTo address these challenges and improve product title localization in e-commerce, this study proposes a retrieval-augmented generation (RAG) approach. Our approach utilizes the constantly-growing bilingual catalog of products as source of product domain-specific knowledge and multilingual support, which is independent and provides flexibility to work with different LLMs. This method retrieves bilingual product information (e.g. titles, bulletpoints and descriptions) that are similar to the source title and incorporates them as few-shot examples in the prompt to enhance product title translation with LLMs. Finally, the study analyzes the product title translations generated from LLMs with the RAG approach to understand its impact on translation quality.\nOur experiment shows that our proposed LLM-based retrieval-augmented generation (RAG) approach for product title translation can significantly enhance the title translation quality with up to 15.3% chrF score improvement for language pairs where the LLM does not primarily support."}, {"title": "2 RAG approach for product title translation", "content": ""}, {"title": "2.1 Utilizing bilingual product information for RAG", "content": "In the e-commerce industry, we can leverage the large and constantly growing volumes of bilingual product information data, including titles, bullet points, and descriptions that can be acquired. These multilingual product titles, descriptions and bulletpoints are commonly used to train smaller neural machine translation (NMT) models for product title translation tasks. Given the dynamic nature of e-commerce, where product information localization is an ongoing process, such bilingual product content continues to expand over time. The bilingual product content can be further utilized to build a search index serving as a source of product domain-specific knowledge and multilingual support for LLM RAG approaches as illustrated in Figure 1."}, {"title": "2.2 Using retrieved similar product information as few-shot examples", "content": "We propose harnessing this ever-increasing bilingual product information data to power a sustainable and scalable retrieval-augmented generation (RAG) approach to enhance product title translation. By retrieving similar bilingual product information examples from this data and using them as few-shot prompts, we can guide the LLMs to generate higher quality and more contextually appropriate translations for product titles as shown in Figure 2. This RAG-based approach allows us to continuously leverage the benefits of the existing and expanding bilingual product information to improve the performance of our cross-lingual title translation capabilities.\nOur proposed RAG approach for product title translation is as follows. Given a large volume of bilingual product information data (e.g. titles),\n$D_{bil} = {\\{(t_{src}, t_{tgt}), (t_{src}, t_{tgt})...(t_{src}, t_{tgt})\\}}$\nwhere $t_{src}$ and $t_{tgt}$ are product information text (e.g. titles) in the source and target languages from a bilingual language pair L, we propose to build an index $I_L$ on the product information (e.g. titles) in the source language and cache the corresponding target language counterparts. We recommend using text-based retrieval frameworks such as BM25, as these can better focus on textual patterns and are easy to scale and maintain, previous study has also shown that BM25 is an effective retrieval mechanism for RAG as it can achieve the high RAG performance with a number of LLMs [6]. As newer products and language pairs emerge, they can be readily incorporated into the index.\nAt inference time, given a product title $t'$ in the source language for the language pair L, we retrieve from the index $I_L$ the top-k"}, {"title": "3 Experiment", "content": "Language pairs: We experiment with 7 language pairs English-Dutch (EN-NL), English-German (EN-DE), Italian-German (IT-DE), Turkish-German (TR-DE), German-Czech (DE-CS), English-Polish (EN-PL) and English-Swedish (EN-SE).\nTest data and MT metric: For each language pair, we sample 2000 product titles as our test set and evaluate translation quality using chrF [18].\nRAG framework: We use Lucene 8.11.3 and OKAPI BM25 probabilistic retrieval framework to build each index. For each bilingual product information data pair, we index the source text and cache the target text.\nRAG data: We sample large volumes of bilingual product (titles, descriptions and bulletpoints) for each language pair to build the index for our RAG approach. For each language pair, we build four indices using sampled bilingual product information data of different domains: product titles (TTL), bullet points (BP), descriptions (PD) and a combination of aforementioned domains (T.B.D.). Language pairs EN-NL, EN-PL, EN-DE and IT-DE have over 10 million bilingual pairs in their T.B.D. index, and the rest have millions.\nExperiment configurations: We set up the following experiment configurations using different prompts for title translation generation with LLM:\nBaseline Generation: The prompt using the prompt template A presented in section 2.2, which does not incorporate retrieved or randomly sampled few-shot examples.\nRAND 1-shot and RAND 5-shot: The prompt using the prompt template B presented in section 2.2, which incorporates 1 and 5 randomly-chosen examples from the respective domain (TTL, BP, PD, T.B.D.) as few-shot prompts. These serve as the lower-threshold of our RAG approach.\nRAG 1-shot and RAG 5-shot: The prompt using the prompt template B in section 2.2, which incorporates 1 and 5 most similar examples retrieved from their respective domain index (TTL, BP, PD, T.B.D.) as few-shot prompts.\nLLM: We use Mixtral-8x7B-Instruct\u00b2, a publicly available LLM, to translate source product title into the target language through"}, {"title": "4 Results and Analysis", "content": "Table 1 shows the chrF deltas relative to the baseline generation. We observe that RAG 1-shot and RAG 5-shot overall improve title translation quality across different data index domains (TTL, BP, PD, and T.B.D.) used for retrieving few-shot examples. In contrast, configurations which retrieve random examples (RAND 1-shot and RAND 5-shot) generally show decreases in chrF, which indicate a negative impact on title translation quality. Where improvements are seen amongst RAND 1 and 5-shot configurations, the magnitude is much smaller than that of the RAG 1-shot and RAG 5-shot configurations.\nResults show RAG 5-shot with examples from all domains (T.B.D. index) has the largest or second largest title translation quality improvement across all language pairs except EN-SE. For language pairs where the target language is not the main language of the LLM, RAG 5-shot demonstrate chrF improvements ranging from +5.9% to +15.3% across DE-CS, EN-SE, EN-NL, and EN-PL. For TR-DE, where the source language is not a main language of the LLM, RAG 1-shot show improvements by up to 2.3% chrF.\nRAG 1-shot and RAG 5-shot configurations also demonstrate improvements in chrF for language pairs where both languages are the main supported languages of the LLM. Those language pairs have higher chrF for baseline generation, and also have chrF improvement up to 3.6% for IT-DE and 2.0% for EN-DE.\nNotably, all language pairs show the greatest increase in chrF when examples are retrieved from the T.B.D. index which contains all three product information domains (title, bullet points, and description). This suggests that a combined index can provide more useful contextual cues to guide the LLM's product title translation task.\nRetrieved examples similarity: We compute the textual similarity between the test source title and examples used as few-shot examples in the prompts as shown in Table 3. Textual similarity is computed using chrF. For RAND-5 and RAG-5 configurations, we report the average of chrF scores across the 5 examples. We observe (1) RAG-1 and RAG-5 retrieved examples with higher similarity scores compared to RAND-1 and RAND-5 which use randomly chosen examples; (2) Given a language pair and one configuration, we observe generally that the RAG data domain with higher similarity score has larger translation quality improvement. However, we also observe RAG-5 with examples from domains T.B.D. do not have highest similarity scores but have the largest or second largest title translation quality improvement for all language pairs except EN-SE. This indicates that retrieved examples from more diverse product information domains can also have positive impact on the title translation generation improvement.\nTitle translation improvement: Table 2 shows examples of generated titles by the RAG LLM method. The three examples show how the approach can improve translation quality:\n(a) Improved title translation from better understanding of products: Many product titles for products such as signs, T-shirts, mugs have text printed on the products, and such texts need to stay intact in the translation.\n(b) Improved translation for product-specific terminology:\n(c) Improved formality and style of title translation:"}, {"title": "5 Related work", "content": "Retrieval Augmented Generation (RAG) has proved to be an effective prompting strategy to enhance the accuracy and reliability of large language models (LLMs) [3, 11, 24]. This is due to the inherent limitations of LLMs in accessing specialized knowledge [19]. RAG enables LLMs to combine their generative abilities with additional context and factual information retrieved from external knowledge sources, which has been explored in several prior studies [1, 8, 13, 16, 22].\nRetrieval-augmented methods have been successfully applied to various tasks using LLMs with domain-specific specialized knowledge, such as question answering [6, 10, 23]. Previous work has also proposed using RAG with a \"text book\" of retrieved language-specific usage, syntax, and vocabulary examples to improve translation tasks with LLMs [7]. However, their study focused on lower-resource language pairs, and to the best of our knowledge, there has been no prior work on applying RAG techniques to machine translation of e-commerce product titles and languages commonly used in the e-commerce industry."}, {"title": "6 Conclusion", "content": "This study presents a retrieval-augmented generation (RAG) approach to enhance e-commerce product title translation using large language models (LLMs). By leveraging the growing bilingual catalog of product content, RAG retrieves similar examples as few-shot prompts to guide the LLM in translating product terminology more accurately and generating higher quality, contextually appropriate translations. Experiments across 7 language pairs demonstrated the RAG approach's effectiveness, achieving chrF score improvements up to 15.3%, especially for language pairs where the LLM has limited proficiency. The RAG approach can enhance translation by more accurately translating or preserving product-specific terminology, maintaining brands, and adhering to expected formatting and deliver product tile translations essential for multilingual product discovery in e-commerce. This scalable, sustainable approach also allows rapid adaptation as e-commerce continuously introduces new products and language pairs."}]}