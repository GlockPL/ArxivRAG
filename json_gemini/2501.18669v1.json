{"title": "The Pitfalls of \"Security by Obscurity\" And What They Mean for Transparent AI", "authors": ["Peter Hall", "Olivia Mundahl", "Sunoo Park"], "abstract": "Calls for transparency in AI systems are growing in number and urgency from diverse stakeholders ranging from regulators to researchers to users (with a comparative absence of companies developing AI). Notions of transparency for AI abound, each addressing distinct interests and concerns.\nIn computer security, transparency is likewise regarded as a key concept. The security community has for decades pushed back against so-called security by obscurity\u2014the idea that hiding how a system works protects it from attack\u2014against significant pressure from industry and other stakeholders [20, 126, 162]. Over the decades, in a community process that is imperfect and ongoing, security researchers and practitioners have gradually built up some norms and practices around how to balance transparency interests with possible negative side effects. This paper asks: What insights can the AI community take from the security community's experience with transparency?\nWe identify three key themes in the security community's perspective on the benefits of transparency and their approach to balancing transparency against countervailing interests. For each, we investigate parallels and insights relevant to transparency in AI. We then provide a case study discussion on how transparency has shaped the research subfield of anonymization. Finally, shifting our focus from similarities to differences, we highlight key transparency issues where modern AI systems present challenges different from other kinds of security-critical systems, raising interesting open questions for the security and AI communities alike.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence has proven to be highly impactful, with impacts in critical domains such as biosciences [110], health [145], and public safety [71]. It is also continually reaching new peaks in consumer and commercial interest, with foundation models such as Claude, Llama, Stable Diffusion, and the GPT family posited to be adaptable to diverse uses [107]. Despite their already widespread impacts, there is still much to understand about how recent developments in AI and machine learning models operate, and the broader human consequences their use and misuse may have. In the commercial sphere, companies have generally (though not always) preferred to hide their methods of data collection, training, fine-tuning, and other specifications [33].\nAt the confluence of these circumstances, many are invested in achieving (various notions of) AI transparency. Researchers want to investigate what makes AI models, including production-line models, better or worse at various tasks (e.g., [21]), and to understand unexpected or possibly harmful effects of model use (e.g., [15, 32, 144]). Consumers, businesses, and other clients should have assurances that companies selling AI-based tools are able to achieve what they are promising\u2014and even before that, they should be able to understand what they are being promised. As increasingly consequential decisions are made with contributions from AI models, people impacted by these decisions need assurances about how AI impacts their lives (e.g., in healthcare [115], child custody [30], or bail setting [71]). Copyright holders want notice and fair compensation and attribution when others are using their works-often making large profits [177, 82]. Regulators want to understand and protect their constituencies from potential harms [190] while promoting innovation and competition in AI [173, 174, 186]. Meanwhile, skeptics have variously argued that too much transparency will harm innovation, business interests (including trade secrets), data privacy, system security, national security/defense, and/or public safety (e.g., [139, 101]).\nAn important emerging literature in both research and policy has responded to this ongoing debate by investigating transparency in AI-discussing possible definitions, implementation approaches, auditing frameworks, regulatory approaches, voluntary initiatives, and possible pros, cons, and impacts of all of the above. Some have been trying to build in properties like explainability or interpretability to LLMs [87]."}, {"title": "1.2 Security by Transparency", "content": "The notion of security by transparency predates computer security. As far back as the 19th century, Auguste Kerckhoffs [114], writing about how to design secure military communication, remarked that a truly secure system should remain secure even assuming the enemy knows everything about the design and parameters of a system. That is, systems should by design be robust even when their design is known.2\nThe argument for security by transparency is subtle: The idea that hiding how a system works can help protect it is not only intuitive, it is in a technical sense correct. If you build all the best state-of-the-art security measures into your system and then choose not to disclose how it works at the end, it may in fact be harder for attackers to find vulnerabilities in the system, as they'll first have to figure out how it works. (And if you could somehow build a perfectly secure system, whether or not you disclose how it works doesn't matter; the security will be perfect either way. But of course, we don't know how to build perfect systems; and the security community has embraced this reality as a critical engineering consideration [187].)\nWhy, then, is security by obscurity so poorly regarded?\nFirst, relying on obscurity for security is risky at best, and creates a false sense of security at worst. The phrase \"security by obscurity\" sometimes refers specifically to such reliance: Having obscurity as the main protection, rather than as one measure among many (as in the examples above). Kerckhoffs' Principle warns directly against this. Second, decades of experience have shown that opacity leads to externalities that tend to indirectly undermine the robustness of designs and systems.\nWe stress that security by transparency is a paradigm, not a rigidly defined set of rules. In a given situation, security by transparency provides princi-"}, {"title": "1.1 Transparency in Security", "content": "In modern computer security, a hard-fought broad-based consensus has been established: Despite the intuitive idea that hiding a system should protect it, transparency is often more beneficial for protection. The consensus on this general principle is broad, though perspectives on how to implement the principle in specific contexts can be more varied.\nIt was not always this way. Before modern cryptography and systems security, even critical military communications often relied on security by obscurity (e.g., [181, 112]): namely, the idea, now well established to be fallacious, that hiding how a system works is an effective and adequate defense against adversaries [103, p. 1.3], [68, p. 2.1.1].\nMoving away from security by obscurity has been a long and ongoing process.\u00b9 Now, \u201csecurity by obscurity\u201d is a catchphrase in the security community with negative connotations, and relying upon security by obscurity is widely considered inadequate [20]. Perfect unanimous agreement on the precise type of transparency that is best for every type of system and application context may never be reached\u2014yet the received wisdom that transparency promotes better security practices and quicker mitigations of harm has held strong, and served as a source of unity in the community [132]."}, {"title": "1.3 Three Key Parallels", "content": "We highlight three parallel themes that we see as common to both the security and AI communities. We discuss these themes in greater depth in Sections 3.1, 3.2, and 3.3, respectively.\n1.  Modeling is Essential For a Robust Understanding of Systems That Are Too Complex To Fully Model: The security requirements of a given system may be too complex or arduous to fully describe, even for simple applications. As such, abstraction is required to help designers understand what a system can and cannot guarantee. Negative guarantees are as important as positive ones threat models help define what systems do not protect against as well as what they do. Most modern AI tasks are likewise too complex to analyze completely. This complexity likewise necessitates robust modelling and abstraction for both positive and negative guarantees.\n2.  \"Many Eyes\" and Community Input: A key impact of transparent practices within security has been the ability to more efficiently and reliably address problems and mitigate harms. Three key transparent practices on which the security community has come to rely widely are: (1) inviting scrutiny to find weaknesses in systems; (2) accepting and incorporating feedback from such scrutiny (solicited or unsolicited); and (3) recognizing that wide community dissemination of such findings (e.g., through academic publications), subject to certain limitations to mitigate resulting harms, is a valuable contribution to security. These practices have long inspired controversy, and can still provoke tensions today. Some of the controversies around and arguments against these transparent practices have parallels in ongoing discourse about AI transparency.\n3.  Ubiquitous Technologies Necessitate Public Trust: Despite the fact that the average person is not expected to understand how encryption or AI work, transparent practices in security and AI benefit everyone if they are institutionalized and built in to infrastructure. This is especially critical for ubiquitous technologies that impacted individuals not only do not understand, but cannot meaningfully opt out of, such as banking and algorithmic decision-making. The security community's experience suggests that such circumstances may necessitate and support the widespread adoption of public trust mechanisms, even when user understanding and market demand for transparency appears lacking."}, {"title": "1.4 Our Scope:\nTransparency, Not Security", "content": "Our focus is not on security for AI systems but rather on transparency for AI systems. We are interested primarily in the parallels and potential for cross-pollination between the security and AI communities in their respective debates around transparency.\nWhen we talk about security by transparency, our discussion will often center around how principles of transparency promote security in some way-naturally, as the context is the field of security. We argue, however, that these discussions' relevance to the AI community extends beyond securing AI, to AI transparency much more broadly for at least two reasons. First, we are focused on high-level principles, and security at a high level is simply about \u201cbuilding systems to remain dependable in the face of malice, error, or mischance\u201d [9]\u2014that is, the very same concerns underlying many of the calls for transparency in AI mentioned above, including some we might not usually think of as \u201csecurity concerns.\u201d Second, transparency aids security as a discipline in ways that may translate to other disciplines: For example, allowing researchers to uncover more efficient solutions and richer properties and functionalities of systems, and to better understand system guarantees and limitations."}, {"title": "1.5 Structure of Paper", "content": "We provide an overview of transparency in security and a survey of transparency in AI (Section 2), and then present our three key parallels in depth; for each parallel, we present (1) a specific perspective on transparency in security, to (2) what aspects may be similar in AI, to (3) parallels with AI that may inform thinking about potential benefits and approaches to transparency in AI research and practice (Section 3). Then we explain how our discussion of transparency in AI complements the existing literature and AI transparency goals (Section 4), and offer a case study of how norms around transparency have developed and evolved over time in one sub-field of security:"}, {"title": "2 Background", "content": null}, {"title": "2.1 Transparency in Security", "content": "By the late 19th century, cryptographers had reservations about security by obscurity. In the seminal 1883 work \"La Cryptographie Militaire\" [114], Kerckhoffs voiced six principles he believed essential to strong security systems. Among them, known now as Kerckhoffs' Principle, stated approximately: \u201cThe system must not require secrecy and can be stolen by the enemy without causing trouble.\u201d3 Thus began the counter viewpoint, security by transparency.\nKerckhoffs' idea was later reframed by Shannon in what is now known as Shannon's Maxim: \"The enemy knows the system being used\" [160]. This made explicit the link between transparency and provable security, which underpins all cryptographic research now."}, {"title": "Disclosure Practices.", "content": "From the earliest days of consumer computer systems, researchers and concerned citizens have been finding and sharing found vulnerabilities. In the late 1980s and '90s, this was through email chains and online zines like Bugtraq4 and Security Digest,5; now, security experts are more likely to communicate vulnerabilities to the developers through formal bug bounty processes and to the public through peer-reviewed publications.\nThere has been much debate over the best way to disclose vulnerabilities both to developers and the public. The details of this debate are beyond our scope; here, we just summarize a few key terms related to vulnerability disclosure.\nFull disclosure involves sharing all information about an attack with the public immediately. Many are concerned that full disclosure could result in a situation where attacks would be exploited widely [51], as it can result in public knowledge of a possible attack before mitigations can be put in place. Coordinated vulnerability disclosure (CVD) involves an initial private disclosure followed by publication after a delay to allow for fixes\u2014the initial disclosure is regarded as a best practice to enable mitigations before a vulnerability is widely known, and the eventual public disclosure is widely regarded as a best practice to inform all affected stakeholders [41].7\nIncreasingly, many organizations have groups who handle bug and vulnerability reporting,8; this streamlines CVD processes and timely fixes. Bug bounty programs, where community members are invited to submit found vulnerabilities to the company in exchange for money or perks, are a way for organizations to proactively seek disclosures.9"}, {"title": "Attacks and Vulnerability Papers.", "content": "At security venues, attack papers (also called vulnerability papers) are a common sight now, though they were more controversial in the past before their research value was as well understood as it is today.\nBasin and Capkun [18] provide one account of the case for attack papers: Finding and fixing bugs, learning flaws which may guide research, understanding compatibility or lack thereof, finding exact security guarantees. Basin and Capkun highlight that good attack papers provide insight beyond the attack (say, through suggested or implemented patches). We refer to their discussion for more detail.\nWe make two observations as to the relationship between these works and transparency. First, the existence of the flaw in a system is the vulnerability, not the specific attack found. That is, choosing to hide a flawed system does not wholesale protect it from some malicious party being able to find and exploit said flaw. Especially in the case of sensitive or valuable data, understanding and providing mitigations for these vulnerabilities actually helps protect more than solely relying on hiding the flaw. Second, publishing a vulnerability (again, with additional insight), promotes better design. That is, understanding and in particular disseminating different sources of vulnerabilities not only pushes the designers of a flawed system to fix their own, but it also allows others to learn what not to do in the future.\nIn these ways, transparent discussion of attack papers promotes better practices. However, simply describing an attack is not sufficient. Not only is providing additional insight and mitigations a generally accepted key part of writing an attack paper, but the security community also generally sees the value in a graded disclosure process, rather than immediate full"}, {"title": "2.2 Transparency in A\u0399", "content": "Calls & Goals for Transparent AI. Calls for transparency in AI are numerous (e.g., [192]). Notable relevant terms and areas of study include: explainability [134], interpretability [153, 134], accountability [135, 134], trustworthiness [14, 134], and robustness [143, 134].10 Recent advances are making important progress in AI modeling [53] but often focus on defining largely positive guarantees. Additionally, some risks can be difficult or elusive to define within the scope of rigorous models [141]. Disclosure practices are being developed [13, 38], but AI researchers remain at risk due to uncertainties and discrepancies regarding policies [180]. Prominent motivations for these efforts towards transparency include public trust [10, 158], technological advancement [76, 29], mitigation of bias [35, 70], and legal liability [135, 48].\nTypes of Access. Many AI or ML systems using deep learning algorithms cannot be deciphered or broken apart [63] and instead are reasoned about solely through input-output behavior (known as black-box access). Proponents of black-box models argue that they perform more accurately and effectively [123]. Critics warn that mitigations or design changes may be harder to find or analyze [19, 153].\nPost-hoc and ante-hoc solutions have been proposed to better understand AI models [197]. Post-hoc solutions are often linked to Explainable AI or XAI, developed to increase the capacity for human understanding of AI models [95]. Conversely, ante-hoc solutions investigate how to make an AI system transparent from the beginning [67]. Ideas range from sharing the model itself, the training data, the weights of the model, and more [10, 63, 192], but there is little consensus as to which types of access are necessary or sufficient."}, {"title": "2.3 Open Source, Security, and A\u0399", "content": "Discourse around open source software encompasses politics, philosophy, the law, and many other domains.\nMost software uses open source code somewhere in its stack, and the Cybersecurity and Infrastructure Security Agency notes that open source software even fosters innovation by allowing developers to pool their knowledge [45]. Many of the most-contributed open source projects are commercially backed, despite the fact that this work is open to anyone including competitors. However, it is important to note some caveats to open source. Schneier [156] has noted that open source can only increase security if people are actually studying the code, and if developers are prompt in bug fixing. However, Sharma [161] claims that the decentralized nature of open source may introduce adverse incentives toward this, as those who benefit from open source software are not proportionally incentivized to develop it further.\nSo-called \"open source AI\" has seen great interest in the study of transparent AI, with companies like Meta [182], Huggingface [8], AI2 [89], and EleutherAI [25] developing their own open source models. At the same time, there are concerns that the term lacks a well-defined meaning [80]. Experts greatly disagree on the definition of open source AI, and some have warned of \u201copenwashing\u201d [117], where developers claim open source without following accepted principles of the community. It may be beneficial for larger AI companies to be perceived as open source [29]. The standards body Open Source Initiative [105] is currently working on developing a concrete framework for open source AI with input from community members [104]."}, {"title": "3 Connections In Transparency: From Security To AI", "content": "This section highlights three key themes underpinning the security community's approach to transparency. For each, we discuss parallels with AI that may inform thinking about the gains transparency would bring to AI research and practice."}, {"title": "3.1 Modeling is Essential For a Robust Understanding of Systems That Are Too Complex To Fully Model", "content": "Perhaps paradoxically, modeling serves as an essential tool to maintain a robust understanding of what we can and cannot guarantee about the performance and failure modes of systems that are too complex to fully model\u2014which have (security) requirements too complex to fully model.\nBut wouldn't modeling such a system (and its requirements) be futile as any model would fail to capture important aspects of real-world operation? Even worse, wouldn't modeling such systems rather lead to dangerous oversimplifications and misunderstandings? These are reasonable questions: oversimplified models do suffer from these serious drawbacks when their limitations are not understood. In contrast, the"}, {"title": "3.2 \"Many Eyes\" & Disclosure", "content": "A key impact of transparent practices within security has been the ability to more efficiently and re-"}, {"title": "4 How Transparency Complements Existing AI Research Goals", "content": "In the previous section, we described three themes for how transparency informs security and security research: (1) modeling as a tool for understanding guarantees of complex systems; (2) vulnerability reporting and disclosure norms as a tool to promote good practices and trust; and (3) transparent practices as public trust mechanisms in settings where public trust is needed.\nWe believe these themes are resonant to both the security and AI research communities. While the history of security research is evidence enough for the former, next, we discuss how these distilled ideas are"}, {"title": "5 A Case Study on (De)-Anonymization", "content": "We have discussed several ways in which transparency has gradually been normalized in the study of security. We emphasize again that these lessons were not self-evident at the dawn of the field, but rather they developed over time implicitly. Here, we look at how the themes highlighted in Section 3 have featured in transparent practices within one subfield of security, in this case anonymization. The goal of this case"}, {"title": "6 Novel Challenges", "content": "This section highlights significant differences between the domains of security and AI that may necessitate new approaches to transparency in AI."}, {"title": "6.1 Training Data", "content": "In security, there is usually a clear delineation between (1) a system's design and functionality and (2) (private) input data within the system. In AI, this line is blurred, with the training data for a model being inextricably linked to its performance and properties. Just the untrained model is insufficient for analyzing its properties [67]. Additionally, some key motivations for transparency, such as model bias [169] and memorization [33], contend explicitly with training data and its expressions in the trained model.\nSome AI models could reasonably make their training data public. However, if any or all of this data is private or should not be publicly accessible, disclosing the training set may expose developers and researchers to serious legal and ethical risks.25 On the other hand, completely obscuring this data may impede important transparency interests such as those discussed in earlier sections. To mitigate these concerns, some have proposed disclosing only (partial) model weights or allowing only black-box access, though each of these avenues still carries a significant risk of exposing training data [131, 130]. Some have proposed anonymized or synthetic training data; these approaches carry their own risks [129, 49].\nSecurity has not had to face the same issue. While, e.g., password and biometric research may interface"}, {"title": "6.2 Disclosure Processes", "content": "It is important to consider how the disclosure processes may be adapted for the AI context.\nIn security, the affected parties of a security vulnerability are generally fairly clear in scope. However, as we discuss above, the entanglement of training data with system design makes the relevant stakeholder set larger and potentially unclear. For example, if a vulnerability to memorization attacks is detected in a model [33], should the group making the model be disclosed to, or the owner of that data, or the data subjects, or a combination of the above? This is especially complicated as the interests of these various stakeholders may be in tension.26\nWhile important initial progress has been made on frameworks for AI vulnerability disclosure [144, 38], the risks and at-risk parties implicated by different vulnerabilities are highly context-dependent. Understanding and careful modelling of what is and is not inherent to AI and machine learning techniques would help piece through these, but it is possible the scope of vulnerabilities and thus responsible disclosure in AI may be an inherently more complex problem."}, {"title": "6.3 Brittleness of Models", "content": "The well-documented and sometimes inherent tradeoffs between privacy (of training data) and performance is a key feature of AI development [36]. In security, there is almost always a tradeoff between efficiency and privacy, and this is often the main tradeoff to consider.27 Choosing the right balance of privacy and performance is an inherently non-technical problem [155], and sometimes, development toward privacy and performance fundamentally contradict each other [93]. The opportunity for technical work is in the modeling of the tradeoff\u2014one must have"}, {"title": "6.4 Optimizing for Metrics", "content": "Many AI systems are based on optimizing for metrics serving as heuristics for a real-world objective. Metrics feature across development, from initial training to the addition of post-hoc \"guardrails.\"\nA well-established adage from economics (Goodhart's Law) states: \u201cWhen a measure becomes a target, it ceases to be a good measure.\" [84] In other words, once a metric is known to be used for consequential decisions, it will be gamed\u2014and thus, likely no longer be a good metric. In this context, disclosing metrics can make them more of a \"target\" and/or easier to game. These observations would seem to counsel against wide disclosure of metrics (such as objective functions) in order to preserve the utility of the metrics and thus the utility of the system overall.\nYet metrics, like training data, constitute an essential part of system functionality. Analyzing a system given its description with the metrics redacted is likely to result in a significantly incomplete understanding of its functionality. And some key motivations for transparency, such as model bias [35], explicitly contend with the metrics used. As such, the security-by-transparency approach would suggest that transparency of metrics yields important benefits.\nThe idea that releasing the full details of system functionality can inherently make the system less useful for its intended purpose, to our knowledge, does not have a good analogue in security research."}, {"title": "7 Perceived Challenges", "content": "We now briefly discuss arguments against transparency in AI which have parallels to those in security."}, {"title": "7.1 Trade Secrets and Innovation", "content": "AI systems are built on algorithms and training data, the details of which may be important for commercial success. One common concern is that too much"}, {"title": "7.2 Misuse", "content": "Another common concern is that releasing AI systems may lead to bad consequences from their deliberate or accidental use for harm, and that the potential or likelihood of such harm may increase with fuller disclosure of how the technology works (e.g., [31, 4, 77]).30\nThese narratives have limited parallels to longstanding discussions in the security community around privacy tools (such as encrypted messaging [108] and Tor [55]) being used by bad actors and for illegal activity. The security community has overwhelmingly advocated, over the course of these debates, that it is critical to research and develop these kinds of general-purpose or so-called \u201cdual-use\u201d security and privacy tools [163, 175], both because the exact same technical functionality that can be misused also provides critical protection for dissidents, journalists, victims, and others who use it for good [106, 178], and because only by studying the topic can we better understand the harms and how to mitigate them while promoting the above benefits.\nOf course, this determination is context-dependent. While the AI community's approach may take time to"}, {"title": "7.3 Pace of Development and Competitiveness", "content": "Many have commented on the unusually fast pace of recent AI developments, often voicing concerns that it is moving too fast (e.g., [22, 172]). This pace has naturally impeded the establishment of standards and best practices [28] and limited the ability of researchers and developers to reach fuller understandings of of new techniques before publication or deployment, as well as the drafting and implementation of timely regulation [154].\nIn our view, while the security community has not experienced a comparable pace of development, it has still slowed down compared to an earlier phase, based on a collective experience of flaws being discovered in secure systems and cryptographic designs that were developed too quickly and scrutinized insufficiently before deployment (some of which caused major security problems with serious impact, e.g., [121]). Over time, the community moved towards investing much more research and development in precisely defining context-specific security requirements, extensive testing of new systems, models, and assumptions before relying on them, and to widely regard as unreliable systems lacking this kind of development and testing over time. The time-consuming step of planning ahead in detail for the security failures that will inevitably occur even if all of these precautions are taken also became more common over time.\nHence, expectations around pace of development and what makes a product competitive shifted, in line with a widespread belief (and lived experience) that more deliberation leads to better products and rushed development leads to real risks. The security community's experience, for what it is worth, illustrates a gradual, costly collective agreement that taking time for transparency does not curb innovation, but rather promotes it, and the delays are worth it to prevent disasters."}, {"title": "7.4 Consequential Outcomes and a Right to Know", "content": "As AI systems are used in ways that have increasingly consequential outcomes,31 some calls for transparency center the idea of a \u201cright to know\u201d about decisions that impact individuals or groups (e.g., [66]). In the European Union, the AI Act has recently written certain aspects of a \u201cright to know\u201d into law [184]. The reasoning underlying these calls generally focuses broadly on the use of (opaque) algorithms in consequential decision-making procedures, and as such is not unique to AI, although modern ML may exacerbate these concerns due to its complexity and the current trend of \u201cblack-box\u201d use [92]. Quite naturally, then, similar discussions have arisen in the context of security-critical systems that impact consequential outcomes (e.g., in election security and government technology [83, 109])."}, {"title": "8 Other Related Work", "content": "This work is concerned with parallels between transparency in security and AI. Of course, there are many different fields which overlap with these ideas."}, {"title": "8.1 Comparison To Other Communities", "content": "Biosecurity. Sam Marks, in referencing Jeff Kaufman's discussion [113] on differences in norms between the biosecurity and cybersecurity communities, questions if AI would be better suited to a comparison to biosecurity [125]. Primarily, this is due to similar difficulties in fixing vulnerabilities. This comparison would frame AI as more of a governmental responsibility than currently understood.\nPhysical Sciences. There has been a recent uptick in discussion about data documentation in the Physics community [43]. This has raised numerous concerns about the standards of trust for auditors and researchers alike. Many are calling for strengthening ethical standards, something they say can never stop being developed [102].\nIssues of ethics and public trust in nuclear physics also have some limited parallels to discussions around ethics and transparency in AI, and have been compared to ethical issues in cryptography [150].\nBiomedical Sciences. The pharmaceutical industry is a community that requires collaboration across many stakeholders and relies strongly on public trust. They have a history of strong risk frameworks in regards to societal impact [140] and are heavily regulated by governmental bodies. Notably, there is a documented increase in public trust when transparency is valued in the industry [164].\nCivil Engineering. The civil engineering community has widespread responsibility in ensuring public safety. Therefore, they have well-established standards of measurability (outlining specs, etc.) as well"}, {"title": "8.2 Related concepts", "content": "Securing AI. While we are discussing relationships between security and AI, our focus is not on securing AI\u2014that is, how to build AI systems that are secure. However, many efforts in modelling and implementing transparent AI contend explicitly with preventing harm and security risks. This includes privacy for training data through, e.g., differential privacy [61] and unlearning [79]; preventing misclassification and misuse through, e.g., the study of adversarial robustness [32, 15]; and labelling of AI generations through watermarking [44].\nOther Notions of Transparency in AI. Prior work has offered taxonomies of transparent AI from various perspectives, such as frameworks for AI risk [134], trustworthy AI [133], ethics guidelines [5], and compilations of previous works [88].\nRed-Teaming and Jail Breaking. Red-teaming and jailbreaking AI to test the limits of models has become more common, and important recent progress has been made (e.g, [189, 34, 146, 65, 122])."}, {"title": "9 Conclusion", "content": "Transparency is a key concept, and has been a subject of community controversy, in both security and AI. We explain how the current discourse on AI transparency exhibits informative parallels with past and ongoing discourse on transparency in security, and discuss three key parallels\u2014modelling (\u00a73.1), community input (\u00a73.2), and public trust (\u00a73.3)\u2014illustrated with a case study (\u00a75). We systematize common themes in arguments against transparency in both domains (\u00a77), and highlight novel challenges presented by transparency in the context of AI (\u00a76)."}]}