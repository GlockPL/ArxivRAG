{"title": "Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning", "authors": ["Neale Ratzlaff", "Man Luo", "Xin Su", "Vasudev Lal", "Phillip Howard"], "abstract": "Multimodal models typically combine a powerful large language model (LLM) with a vision encoder and are then trained on multimodal data via instruction tuning. While this process adapts LLMs to multimodal settings, it remains unclear whether this adaptation compromises their original language reasoning capabilities. In this work, we explore the effects of multimodal instruction tuning on language reasoning performance. We focus on LLaVA, a leading multimodal framework that integrates LLMs such as Vicuna or Mistral with the CLIP vision encoder. We compare the performance of the original LLMs with their multimodal-adapted counterparts across eight language reasoning tasks. Our experiments yield several key insights. First, the impact of multimodal learning varies between Vicuna and Mistral: we observe a degradation in language reasoning for Mistral but improvements for Vicuna across most tasks. Second, while multimodal instruction learning consistently degrades performance on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on commonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that a training-free model merging technique can effectively mitigate the language reasoning degradation observed in multimodal-adapted Mistral and even improve performance on visual tasks.", "sections": [{"title": "1 Introduction", "content": "Multimodal LLMs (MLLMs) have gained significant attention due to their ability to integrate various forms of data, allowing them to perform tasks that require both image and language understanding [Li et al., 2023b, Alayrac et al., 2022, Bai et al., 2023b, Huang et al., 2023, Xue et al., 2024, Liu et al., 2024b]. One common approach to building MLLMs is to connect a powerful LLM with a vision encoder [Radford et al., 2021] through an intermediate module, followed by multimodal instruction tuning [Wang et al., 2024]. This has enabled MLLMs to excel in tasks such as visual question answering [Antol et al., 2015, Hudson and Manning, 2019, Bigham et al., 2010] and image captioning [Lin et al., 2014, Plummer et al., 2015, Krishna et al., 2017] by integrating and interpreting both visual and textual inputs [Liu et al., 2023b,a, 2024b]. While this process equips the model with multimodal capabilities, it may also impact language performance [Huang et al., 2023, Lin et al., 2024, McKinzie et al., 2024, Zhang et al., 2024].\nIn this work, we study the behavior of MLLMs on language reasoning tasks and aim to answer the question: \"How does multimodal instruction learning affect language reasoning performance?\" The investigation of this question can lead to practical guidelines for the deployment of MLLMs in real-world applications such as chatbots, where the user can ask a question purely in language or optionally upload an image to accompany their query. Few prior studies have explored this question, with only a limited number of methods proposed for mitigating language degradation [Zhang et al., 2024]. Our work extends this line of research by exploring how the choice of the base LLM affects the degree of language reasoning degradation during MLLM training, and whether this phenomenon can be effectively mitigated without the need for additional model training."}, {"title": "2 Background & Related Work", "content": "Liu et al. [2024c] used a synthetic dataset of multimodal language-image instructions generated by GPT-4 to train LLaVA, an MLLM which combines the CLIP [Radford et al., 2021] vision encoder with a pre-trained Vicuna [Zheng et al., 2024] LLM. Using a projection layer to encode image representations in the word embedding space of the LLM, LLaVA learns via its visual instruction tuning to integrate information across both modalities. This can be viewed as a form of domain adaptation, as the weights of the pre-trained LLM are updated as it learns to integrate representations from the vision encoder. Liu et al. [2024a] further extended the LLaVA visual instruction tuning dataset to incorporate other academic task-oriented data. A variety of datasets have been developed for visual instruction tuning of other MLLMs [Zhu et al., 2023, Li et al., 2023a, Bai et al., 2023b]\nDegradation of language reasoning performance in MLLMs has been observed in a limited number of prior studies. MLLMs such as DeepSeek-VL [Lu et al., 2024] and Kosmos-1 [Huang et al., 2023] have been compared to their corresponding base LLMs on text-only tasks, with mixed results showing that MLLMs can perform better or worse depending upon the benchmark. Zhang et al. [2024] compare Vicuna [Zheng et al., 2024] and Qwen [Bai et al., 2023a] LLMs to their MLLM counterparts trained with different vision encoders, finding varying degrees of language reasoning degradation. The use of interleaved image-text data as well as text-only examples when training MLLMs has been shown to help mitigate performance degradation on language tasks [McKinzie et al., 2024, Lin et al., 2024]. In contrast to these prior studies, our work investigates how the choice of the base LLM influences the phenomenon of language reasoning degradation when MLLMs are trained. Whereas previously proposed methods for mitigating this effect rely on introducing new data our modules during training, we propose a simple model merging approach which can effectively recover performance in text-only tasks without requiring any additional training.\nModel merging has been a popular and promising technique to combine the strengths of different models, allowing for improved generalization, performance. The task arithmetic framework [Ilharco et al., 2022] attempts to combine the strengths of different models without catastrophic forgetting. This is done by computing task vectors \u2013 the differences in weights between models, then adding or subtracting these task vectors to an initial set of parameters to induce learning or forgetting with respect to a given direction in weight space. TIES builds upon this approach by only considering the largest entries in the task vector as candidates for merging, and uses a sign-consensus algorithm to reduce task interference. In our setup, we compute the task vector representing natural language proficiency (a Mistral LLM), and add it to the instruction tuned LLM to recover any degraded natural langauge performance caused by visual instruction tuning. In our experiments, we investigate to what degree model merging is effective for Mistral-based MLLMs by scaling the contribution of the task vector to the merged model."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Analyzing Language Reasoning Degradation in LLaVA Models", "content": "Experimental Details. We focus our analysis on three MLLMs sharing a common architecture: LLaVA-1.5, LLaVA-1.6, and LLaVA-1.6-Mistral. LLaVA-1.5 and LLaVA-1.6 are both derived from the Vicuna-1.5 LLM [Zheng et al., 2024], but LLaVA-1.6 supports higher resolution images and was trained on an improved visual instruction tuning dataset. LLaVA-1.6 and LLaVA-1.6-Mistral are"}, {"title": "3.2 Mitigating Language Reasoning Degradation with Model Merging", "content": "Mitigating the phenomenon of language reasoning degradation is important. Prior studies have proposed to mitigate language reasoning degradation through various training strategies, such as"}, {"title": "4 Conclusion", "content": "Our study reveals how multimodal instruction tuning of foundational models can lead to undesired language reasoning performance degradation. We observed that choice of the base LLM prior to visual instruction tuning is more significant than differences in training datasets in influencing the phenomenon of language reasoning degradation, and that stronger LLMs experience a greater degree of degradation. Moreover, language degradation is not uniformly exhibited across datasets, with certain tasks such as commonsense reasoning actually exhibiting the inverse effect. We proposed a simple training-free model merging strategy which can effectively counteract language degradation in MLLMs, offering the ability to customize the balance between language & visual reasoning performance without requiring any additional training. We believe this points to model merging as a promising direction for future research on mitigating undesired performance regressions."}, {"title": "Model Merging Overview", "content": "There are multiple well-studied ways to combine model parameters, most of which utilize the task-arithmetic framework that specifies a set of fine-tuned models to be merged back into a base model. For our use case, we want to merge the \"base\" LLM parameters back into the visual instruction-tuned VLM langauge model parameters. Specifically, to merge the parameters of the Mistral base LLM ($\\theta_{lm}$) into the LLaVA-Mistral VLM language model ($\\theta_{vlm}$), we compute a task vector $T$ defined as $T = \\theta_{vlm} - \\theta_{lm}$. The task vector captures the relevant differences between the two models, but may also contain information about parameters that have been adapted to handle vision-related tasks, which we want to preserve. We hypothesize that the largest entries of $T$ correspond to parameters that were critical for language modeling but degraded due to visual instruction tuning. Therefore, in our experiments we mask the bottom-K% of entries of $T$, taking only the top-K entries as candidates for merging; we denote the pruned task vector as $\\hat{T}$. Finally, the task vector is combined with $\\theta_{vlm}$ as: $M = \\alpha \\hat{T} + \\theta_{vlm}$. In our experiments we vary both $\\alpha$ and K, the results can be seen in figure (fig. 1)."}, {"title": "Performance of Merged MLLMs", "content": "We focus our investigation of model merging on LLaVA-1.6-Mistral, as it exhibited the greatest and most consistent language reasoning degradation relative to its base LLM. Figure 1 (b) shows the result of utilizing increasing amounts of merging between this MLLM and its base LLM. As the weight proportion (x-axis) increases, more weight is being given to the base LLM during the model merging. As expected, we observe that performance on language tasks among merged models approaches that of the base LLM as the weight proportion increases. In contrast, increasing weight proportion decreases the performance of the merged models on visual reasoning tasks, as the MLLM is deviating further from its original state after visual instruction tuning.\nThese results show that the merging weight proportion can be tuned to optimally balance visual reasoning capabilities and performance in text-only tasks. This hyperparameter can be set based on the targeted use cases for the MLLM to optimize desired performance, without needing to perform any additional training of the model. Our results show that smaller weight proportions (e.g., 0.1) can effectively recover most of the degraded performance across language reasoning tasks without significantly disrupting the MLLM's visual reasoning capabilities. Surprisingly, we find that this amount of model merging actually increases performance on three out of the five visual reasoning tasks relative to the original LLaVA-1.6-Mistral model. We also find similar improvements in visual reasoning capabilities when smaller weights are used to merge LLaVA-1.5 and LLaVA-1.6 with Vicuna (see Appendix C for details). We hypothesize that multimodal tasks require language reasoning skills in addition to visual perception capabilities, which could expalain why performance on these tasks can improve after merging the MLLM with its corresponding LLM. These results demonstrate the benefits of model merging for preserving the language reasoning and multimodal capacity."}, {"title": "D Details of CommonsenseQA Human Evaluation", "content": "For each MLLM, we sampled 20 questions where it produced a correct answer on CommonsenseQA while its corresponding base LLM did not, resulting in a total of 60 samples. We then categorized these questions into five groups: commonsense physical locations, object-action associations, physical appearance and characteristics, situational or event-based commonsense, and other For the commonsense physical locations category, the questions typically involve where specific actions or objects commonly occur. The object-action associations category covers questions about objects and their associated functions or behaviors. The physical appearance and characteristics category involves questions about the external features of objects. The situational or event-based commonsense category includes questions about common events or scenarios that can be visually represented. We also"}]}