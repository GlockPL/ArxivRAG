{"title": "Temporal Fairness in Decision Making Problems", "authors": ["Manuel R. Torres", "Parisa Zehtabi", "Michael Cashmore", "Daniele Magazzeni", "Manuela Veloso"], "abstract": "In this work we consider a new interpretation of fairness\nin decision making problems. Building upon existing fairness for-\nmulations, we focus on how to reason over fairness from a temporal\nperspective, taking into account the fairness of a history of past de-\ncisions. After introducing the concept of temporal fairness, we pro-\npose three approaches that incorporate temporal fairness in decision\nmaking problems formulated as optimization problems. We present a\nqualitative evaluation of our approach in four different domains and\ncompare the solutions against a baseline approach that does not con-\nsider the temporal aspect of fairness.", "sections": [{"title": "1 Introduction", "content": "Automated decision making is an important part of artificial intelli-\ngence with a variety of application areas, from scheduling and re-\nsource allocation, to robotics and autonomous vehicles. Decision\nmaking processes typically aim to optimize an overall benefit or\ncost. However, as we strive to make our algorithms and agents more\nintelligent, it is important to ensure that they also account for eth-\nical considerations such as fairness. The need for fair algorithms\nand agents has been widely studied across different areas, such as\nrobotics [3], healthcare [4], telecommunications [11], and resource\nallocation [15], among others.\nFormulating fairness concerns in different domains can be chal-\nlenging, and it has been the subject of many studies [20]. In this pa-\nper, we take a new angle to considering fairness in decision making\nprocesses. We build upon previous fairness formulations, and focus\non how to reason about fairness from a temporal perspective, ac-\ncounting for the fairness of a history of past decisions. We aim to in-\ntroduce the concept of \"temporal fairness\" into the decision making\nprocess, which measures the fairness of solutions throughout time.\nAs a motivating example consider the scenario depicted in Figure 1\nwhere courses must be assigned to a pool of lecturers (11,12 and 13)\nin semester t. Each lecturer is specialized in different areas and the\nteaching quality of a course is proportional to the expertise of its\nlecturer (the gray bars below the lecturers depict their expertise on\ndifferent topics). Figure 2a depicts the number of courses assigned\nto each lecturer in the past four semesters. Lecturer 11 has received a\nhigher teaching load than 12 over the past four semesters. Regardless\nof the reasons that have led to the scenario in Figure 2a, the reality is\nthat there has been some \"historical unfairness\". Figure 2b depicts\nthe cumulative teaching load over time for each of the two available\nlecturers 11 and 12.\nA new course allocation must be made for semester t. If an allo-\ncation is made that is presently fair, in which both lecturers teach"}, {"title": "2 Problem Formulation", "content": "We consider decision making problems solved by finding a solution\nthat maximizes an objective function while satisfying a set of con-\nstraints. First, we consider a decision making problem where no fair-\nness metric is considered. Such problems can be formulated as an\nOptimization Problem (OP).\nDefinition 1. An Optimization Problem (OP) is a tuple (Q, X,C)\nwhere Q is a quality metric, X is the domain for optimization, and\nC is the set of constraints.\nFormally, we define an OP as:\n$\\begin{aligned}\n\\max_{x \\in X} &\\quad Q(x) \\\\\n\\text{s.t.} &\\quad f(x)\n\\end{aligned}$$\nIn this setting, the goal is to find a solution x* that maximizes a given\nquality metric Q, while being subject to a set of constraints 8 (x). We\nlet variables x denote the optimization variables of the problem. Since\nthis formulation only reasons over the quality metric, it is possible\nthat the optimal solutions may be deemed unfair according to some\nfairness metric. Moreover, as depicted in Figure 2a, this formulation\nmay lead to a fast accumulation of unfair solutions.", "2.1": "FOP: Incorporating Fairness"}, {"content": "We now incorporate a fairness metric F into the formulation of the\noptimization problem. A Fair Optimization Problem (FOP) can be\ndefined as:\nDefinition 2. A Fair Optimization Problem (FOP) is a tuple\n(Q,F,X,C,\u1e9e) where F is the fairness metric and \u1e9e \u2208 R is a pa-\nrameter than controls the trade-off between quality and fairness. The\nremaining elements follow the original OP.\nFormally, an FOP can be modelled as:\n$\\begin{aligned}\n\\max_{x \\in X} &\\quad Q(x) + \\beta F(x) \\\\\n\\text{s.t.} &\\quad f(x)\n\\end{aligned}$$\nIn general, we will assume that F returns higher values for fair so-\nlutions and lower values for unfair solutions. In practice, it may be\nconvenient for both Q and F to have well-specified ranges, render-\u3002\ning it easier to understand the impact of the parameter \u00df. However,\nthe formulation is general and supports arbitrary quality and fairness\nmetrics. Finally, we note that the specification of the fairness met-\nric F may potentially require the introduction/modification of con-\nstraints. In order to keep notation simple, we will continue denoting\nthe set of constraints as before, & (x).\nAs an example building upon our previous scenario of the course\nassignment domain, let us consider a relative max-min fairness met-\nric Frmm, which compares the maximum and minimum number of\ncourses lectured by all lecturers, versus the total number of courses\nlectured during that time. Formally,\n$F_{rmm}(x) = 1-\\frac{\\max_i S_i(x) - \\min_j S_j(x)}{S(x)}$\nwhere Si(x) is the number of courses lectured by li in solution x,\nand S(x) is the total number of courses lectured. The range of Frmm\nis [0, 1]. It is maximized when lecturers get an equal lecturing load,\nand minimized when one of the lecturers takes the entire load. While\nincorporating the new fairness metric in the FOP leads to solutions\nthat are fair according to F (or at least fairer, depending on \u1e9e), there\nmay still exist some historical unfairness that remains from previous\nallocations. Figures 2a and 2b hinted at this, depicting a scenario\nwhere scheduling a fair plan at time step t would have maintained\nthe gap of cumulative courses lectured."}, {"title": "2.2 HFOP: Incorporating Historical Fairness", "content": "FOPS assume a fairness metric F that only reasons over the fairness\nof a solution x. In order to account for existing historical unfairness,\nit is thus important to reason over the fairness of a solution x in the\ncontext of the history of past solutions H = (xt-T,...,xt-1), where\nxt- is a previous solution from time step t \u2013 \u0394.\nWe formalize the notion of such fairness metrics in the following\ndefinition.\nDefinition 3. A historical fairness metric FH : X \u2192 R is a fairness\nmetric for an FOP (Q,FH, X,6, \u03b2) where H \u2264 X contains solu-\ntions satisfying C. We assume Fo is a fairness metric and write it as\nF.\nThe historical fairness metric FH can be used to control how fast or\nslow historical unfairness is compensated. Also, as it is not a limita-\ntion for the real-world scenarios we consider in this paper, we assume\nthe time between the historical solutions in H is uniform.\nDefinition 4. A Historical Fair Optimization Problem (HFOP) is\nan FOP tuple (Q,FH,H, X,C, \u03b2) where FH is a historical fairness\nmetric."}, {"title": "2.3 DHFOP: Incorporating Discounted Historical Fairness", "content": "We observed in Figure 3 how slowly the historical unfairness would\nbe compensated when following the solutions produced by FOP. In\nfact, it turns out it would never be fully compensated - since FOP al-\nways computes perfectly balanced schedules, the lecturing load gap\nof 5 would remain unchanged. This behaviour may not fit many do-\nIt may become especially problematic when considering sce-\nnarios with long histories of unfair solutions.\nA historical fairness metric FH allows the specification of an opti-\nmization problem HFOP that reasons over remnant historical unfair-\nness. We observed this may lead to solutions that seem unfair at time\nstep t when only considering the current time step (i.e., according to\nF).\nIn practice, it makes sense to consider a \"forgetting rate phe-\nnomenon\", where we attribute more importance to recent events than\nthose in a distant past. However, FH (x) puts equal importance to the\nfairness of solution x at time step t and all past solutions. In order\nto model the importance of recent events we propose the discounted\nhistorical fairness metric FH,y, which discounts past unfairness with\na forgetting discount factor \u03b3.\nDefinition 5. A Discounted Historical Fair Optimization Problem\n(DHFOP) is a tuple (Q,FH,\u03b3,H, X,C, \u03b2), where FH,y is a historical\nfairness metric that reasons over a history of previous solutions H =\n(Xt-T,...,Xt-1) with the discount factor y. The remaining elements\nfollow the HFOP.\nReasoning over a discounted historical fairness metric FH,y allows\nus to control the importance of the unfairness of past solutions rela-\ntive to more recent ones. Formally, the optimization problem is:\n$\\begin{aligned}\n\\max_{x \\in X} &\\quad Q(x) + \\beta F_{H,\\gamma}(x) \\\\\n\\text{s.t.} &\\quad f(x)\n\\end{aligned}$$\nThere are now two hyper-parameters. The discount factor \u03b3, which\nsets the importance of the fairness of past solutions, and the parame-\nter \u1e9e, which controls the quality/fairness trade-off.\nRevisiting once more our running example of course assignment\nand the max-min fairness metric, we could define its discounted his-\ntorical variant as follows:\n$F_{mm}^\\gamma(x) = 1 - \\frac{\\max_i S_{\\gamma,i}((H,x)) - \\min_j S_{\\gamma,j}((H,x))}{S_{\\gamma}((H,x))}$\nwhere Sy,i((H,x)) = \u03a3\u2081=0ySi(x\u22121) is the discounted number of\ncourses lectured by li over all solutions in (H,x) and similarly,"}, {"title": "2.4 MSDHFOP: Historically Fair Planning with Future Forecasts", "content": "All problems introduced so far are of a single-shot nature, where\nthe solver is assumed to make a decision for the current time step t.\nHowever, single-shot decisions can often result in sub-optimal solu-\ntions in complex domains with extended horizons, such as planning\nproblems [8]. Reasoning over multiple time steps into the future can\nallow for more effective solutions given knowledge or predictions\nabout future events.\nIn the setting of fairness, reasoning over multiple steps into the\nfuture may allow for interesting solutions. For example, due to future\nconstraints, a solution that is fair over a given horizon may require\ninitial solutions that seem unfair when analyzed independently. In\norder to account for both existing historical unfairness and a planning\nhorizon into the future, we let FH,\u03b3,\u03c4(Xt,...,Xt+TF) denote a fairness\nmetric that considers both a history of solutions H and a sequence of\nTF future problems (xt,...,Xt+TF), with the past and the future being\ndiscounted according to \u03b3and \u03c4.\nDefinition 6. A Multi Step Historical Fair Optimization Problem\n(MSDHFOP) is a tuple (Q,FH,\u03b3,\u03c4,\u0397, X,C,\u03b2,\u03b3,\u03c4), where FH,\u03b3,\u03c4 is\na historical fairness metric that reasons over a history of previous\nsolutions H = (Xt-TH,...,xt-1) and a sequence of future solutions\n(X1,...,Xt+TF). \u03b3 and \u03c4 are discount factors. The remaining elements\nfollow the DHFOP.\nWe formulate an MSDHFOP as:\n$\\begin{aligned}\n\\max_{X_0,...,X_{TF}} &\\quad \\sum_{i=0}^{T_F} \\tau^i Q(x) + \\beta F_{H,\\gamma,\\tau}(X_0,...,X_{TF}) \\\\\n\\text{s.t.} &\\quad f(x)\n\\end{aligned}$$\nThe first term computes the discounted sum of quality of the planned\nsolutions. The second term computes the multi-step historical fair-\nness metric. As before, the discount factor y sets the importance of\npast solutions relative to more recent ones in the computation of fair-\nness. Similarly, the discount factor \u03c4 discounts future solutions rela-\ntive to the previous one, impacting both fairness and solution quality.\nWhereas y seeks to model the \"recency effect\" from a fairness per-\nspective (i.e., we tend to attribute more importance to recent events\nthan those in a distant past), \u03c4 seeks to model uncertainty in planning\ninto the future (i.e., it is easier to predict states closer in time than\nthose in a distant future).\nWe can again build upon the discounted historical relative max-\nmin fairness metric, and introduce a variant that also reasons over\nthe next TF solutions Xt:TF = (Xt,..., Xt+TF), where we define\n$\\begin{aligned}\n\\text{FH(Xt:TF)} \\\\\n\\text{ = } & 1 - \\frac{\\max_i S_{\\gamma,\\tau,i}((H,X_{t:T_F})) - \\min_j S_{\\gamma,\\tau,j}((H, X_{t:T_F}))}{S_{\\gamma}((H,X_{t:T_F})}\n\\end{aligned}$$\nwhere\n$S_{\\gamma,\\tau,i}((H,X_{t:T_F})) = \\sum_{\\Delta=1}^{T_H} \\gamma^iS_i(x_{t-\\Delta}) + \\sum_{\\Delta=0}^{T_F} \\tau^iS_i(x_{t+l})$\nis the discounted number of courses lectured by li, over all so-\nlutions in history H and future planned solutions Xt:TF. Similarly,\nS\u03b3,\u03c4((H,Xt:TF)) is the discounted total number of courses lectured.", "3": "Experimental Evaluation", "3.1": "Setup"}, {"content": "We evaluate our formulations across multiple domains using differ-\nent fairness metrics. We start with a technical description of each do-\nmain, introducing the decision variables, and the quality and fairness\nmetrics to be used. The machine used to run experiments is an In-\ntel(R) Xeon(R) CPU E3-1585L v5 @ 3.00GHz with 64GB of RAM."}, {"title": "3.1.1 Course Assignment Problem (CAP)", "content": "This is the domain that has been used throughout the paper, where\na set of lecturers I is to be assigned to a set of courses 6. When\ndealing with multi-step decision making settings, we may denote the\nset of courses at time stept as 61. The expertise of lecturer I in course\nc is measured by S : L \u00d7 C \u2192 R, and higher values correspond to\nhigher expertise. Decision variable x1,c \u2208 {0,0.5,1} indicate the load\nof lecturer I in teaching course c- a lecturer may not lecture the\ncourse at all, or lecture either half a course or the full course.\nWe consider a quality metric Q = 1/Qmax  \u03a3\u03c4\u03b5\u03c6, \u03a3\u03b9\u03b5Lx1,c S(1,c),\nwhich rewards course assignments with skilled lecturers. Qmax is\na normalization constant, denoting the maximum sum of expertise\npossible this ensures Q is bounded between 0 and 1. In order to\ndisplay the generality of our formulation, throughout the experimen-\ntal evaluation with this domain we may use different fairness metrics."}, {"title": "3.1.2 Vehicle Routing Problem (VRP)", "content": "In this domain, given a set of vehicles V, a set of points P that must\nall be traveled to exactly once, a depot r\u2208 P the vehicles must leave\nfrom and return to, and distances between all points D: P\u00d7P\u2192R+,\ndetermine a route for each vehicle that minimizes the total distance\ntraveled. We consider a standard integer program to model the OP\nwhere quality Q(x) is the total distance traveled (see Supplementary\nMaterials A.1 for a full definition). To model fairness, for a given so-\nlution x to the integer program, let Uv (x) be the total distance vehicle\nv travels under x and define F(x) := maxvea Uv(x) \u2013 minw\u2208A Uw (x).\nThis notion of fairness is similar to proportional equality and is used\nin [13] in a multi-objective version of VRP."}, {"title": "3.1.3 Task Allocation Problem (TAP)", "content": "In this domain, given a set of agents A, a set of tasks T, and a cost\nassociated with each agent for each task C: AXT\u2192 R+, find an as-\nsignment of tasks to agents such that the sum of costs is minimized.\nWe consider a standard integer program to model the OP where qual-\nity Q(x) is the sum of costs (see Supplementary Materials A.1 for a\nfull definition). The fairness metric we consider is the classic min-\nimax notion of fairness (see [20] and references therein), where for\na given solution x to the integer program, let Ua(x) be the total cost\nagent a incurs under x and define F(x) := maxa\u2208A Ua(x)."}, {"title": "3.1.4 Nurse Scheduling Problem (NSP)", "content": "We consider a version of this classical problem in operations re-\nsearch. In the Supplementary Materials A.3, we first formally define\nthe problem and then we show the impact of different histories on\nthe NSP, in particular showcasing the impact of the discount factor\nin DHFOP."}, {"title": "3.2 Quality vs. Fairness", "content": "We start our experimental evaluation with an example depicting the\nquality vs. fairness trade-off, and the impact of the parameter \u03b2\ntherein. Let's consider an instance of CAP with 3 lecturers, 11,12,13\nand 2 courses C1, C2. Across all 3 courses, l\u2081 has high skills (S = 2),\n12 has medium skills (S = 1.5), and 13 has low skills (S = 0). We use\na historical quadratic max-min gap fairness metric\n$F_{ammg} (x) = (\\frac{1}{2} (max($((H,x) \u2013 min S_j((H,x)))^2$.\n- minS_j((H,x)))^2$)\nwhere Sy,i is defined as previously. The original fairness metric that\ndisregards H follows naturally. Fqmmg has range [-\u221e,0] and, when\ncompared to Frmm, should allow for heavier penalization of solutions\nthat increase the lecturing load, due to the quadratic term and the lack\nof normalization.\nWe analyzed the solutions computed by HFOP under different val-\nues of \u1e9e, for 10 consecutive semesters, starting with no previous\nhistory. Figure 5 compares the quality Q, fairness Fqmmg, and his-\ntorical fairness FH Fammg of the solutions computed by HFOP under\ndifferent values of \u1e9e. Table 3 provides a summary of the results\nfor the different metrics. We observe that, as \u1e9e increases, HFOP\ncomputes solutions with lower quality Q, but higher fairness F and\nhistorical fairness FH. This follows our expectation, since \u1e9e is the\nparameter setting the quality vs. fairness trade-off. From the fig-\nure we also observe that HFOP converges to a pattern of first se-\nlecting higher-quality/lower-fairness solutions, and once the histor-\nical fairness reaches a certain (low) level, starts selecting lower-\nquality/higher-fairness solutions to compensate for it. This pattern\nis even more noticeable for lower values of \u1e9e, where at earlier time\nsteps the solutions produced tend to be characterized by high quality\nand low fairness."}, {"title": "3.3 Planning with Future Forecasts", "content": "We now evaluate the benefits from a fairness perspective of MSD-\nHFOP reasoning over multiple steps into the future from a fairness\nperspective. Consider a simplified instance of CAP with two lectur-\ners 11 and 12, and two courses c\u2081 and c2. Across all courses, 1\u2081 has\nhigh skills (S = 2) and 12 has medium skills (S = 1). To showcase\nthe flexibility of our approach to fairness metrics, we now consider\nanother version of the maximin fairness metric where utility U; mea-\nsures the number of courses taught by lecturer li:\n$F_{mm} (x) = \\frac{min_i U_i(x)}{max_jU_j(x)}$\nAssuming no historical solutions, the scheduler is now to plan the\ncourse assignments for the next T = 4 semesters. There exists a\nknown constraint about the future-l\u2081 will not be able to take any\nlecturing load on semesters t+2 and t+3 due to a sabbatical leave."}, {"title": "3.4 Increasing Complexity and Benchmarking", "content": "We now examine a more complex problem and show the impact of\nconsidering fairness on running time. We first introduce a method for\ngenerating random instances and a history of past solutions for VRP.\nConsider a square integer grid of a fixed size. We deterministically\nplace the depot at the center of the grid and, given a fixed number of\npoints n, choose n of the grid points uniformly at random (not includ-\ning the depot). For generating history, we generate random instances\nand solve the problem optimally on these random instances.\nFor our experiments in this section and the following, we imple-\nmented the integer program with the corresponding fairness con-\nstraints using the PuLP Python library [17] and used the CBC\nsolver [6] with a standard linearization of F(x) (see, e.g., [20]).\nAll times measured are wall-clock times for the combined model-\nbuilding and solving times.\nIn our experiments, we consider 4 vehicles V = {V1, V2, V3, V4}\nand 12 locations. We generate a history of 5 steps and a single ran-\ndom instance. For each historical instance, we assume V\u2081 always had\nthe shortest route, V2 the second shortest, and similarly for V3 and\nV4. Table 5a shows the total distance traveled for each vehicle. We\ncompare the solutions of OP, FOP, and HFOP. We set \u1e9e = 10 for all\nexperiments. Table 5b shows the results for each of the 4 vehicles.\nIn FOP, the notion of fairness considered should encourage solu-\ntions where all distances traveled are similar. This, of course, should\ncome at the expense of increasing the overall distance traveled. We\nsee this exact scenario play out when comparing OP and FOP. The\ndistances in the solution for OP are not uniform but attain a total\ndistance of 74.4 and the distances in the solution for FOP are all sim-\nilar but the total distance traveled is 112.1. When comparing OP and\nFOP to HFOP, we expect that HFOP should account for the historical\nunfairness received by vehicle V4. In fact, we expect and see in the\nresults that the solution to HFOP should give the shortest routes (in"}, {"title": "3.5 Larger Scale Experimentation", "content": "In this section, we show that our framework can be applied on a\nlarger scale than considered in the previous sections. We introduce\na method for generating random instances for TAP. We create in-\nstances with |A| = |T| = 40. For each agent a \u2208 A, one task is chosen\nuniformly at random to have cost 5, three tasks are chosen uniformly\nat random to have cost 20, and the rest of the tasks have cost 30. We\nsometimes deterministically enforce that an agent a does not have a\ntask of cost 5 and this task is replaced with a cost 30 task, in which\ncase we say agent a is constrained.\nWe refer to the following setup as a single run and we average our\nresults over 10 runs. Sample 8 agents uniformly at random from A\nand denote this subset as C. Produce 3 random instances according\nto our random instance generation given above. Then produce 3 more\nrandom instances where all agents in C are constrained. These 6 in-\nstances are the future instances. To generate history, we run the OP on\neach of these 6 instances. Sort the agents according to total cost. In\nthis order, the last 4 agents not in C are assigned a historical cost of\n180, call these agents W. Amongst the remaining agents, the first 24\nagents are assigned a historical cost of 30. The remaining 12 agents\nare assigned a historical cost of 120. We give more justification for\nour method of random instance generation and history generation in\nthe Supplementary Materials A.2. All instances are run with \u1e9e = 10.\nWe first evaluate the maximum cost assigned to any agent in OP\nand FOP. Table 6 shows the number of times per run the maximum\ncost is 30. We expect the number to be much larger in OP compared to\nFOP, which is confirmed in Table 6. This is at the expense of incurring\na larger total cost, which is expected since FOP is also prioritizing\nminimizing the maximum cost and not just the total cost."}, {"title": "4 Related Work", "content": "In recent times, a significant amount of research has been dedicated\nto fairness in AI, with a focus on predictive models and algorithmic\nfairness [14]. In machine learning, in particular, the topic of long-\nterm fairness has been the subject of much attention [5, 7, 10]. The\nlong-term consideration of fairness is relevant to the MSDHFOP for-"}, {"title": "5 Conclusion and Future Work", "content": "In this work we took a new angle to considering fairness in decision\nmaking processes. Building upon previous fairness formulations, we\nfocused on how to reason about fairness from a temporal perspective,\nespecially when there exists a history of past decisions that may have\nbeen potentially unfair. In this setting, we proposed to reason over\nthe concept of \"temporal fairness\" in decision making processes.\nStarting from a general decision making problem-OP-we in-\ncrementally built our approach to reason over temporal fairness, ac-\ncounting for both past solutions and predictions about the future.\nWith the introduction of a fairness metric in the objective, FOP ex-\ntends OP by reasoning over the quality/fairness trade-off of a solu-\ntion. To reason over historical unfairness, we propose HFOP, where\nthe fairness metric takes into account a history of previous solutions.\nA discounted version DHFOP is then proposed to allow us to model\nthe importance of more recent events. Finally, the MSDHFOP formu-\nlation is extended to reason over both historical and future solutions.\nIn the experimental evaluation we assess our approach across differ-\nent domains and show, in particular, how our approach is compatible\nwith different fairness metrics.\nAs directions for future work, we envision exploring scenarios\nwhere different fairness metrics are used across time (in the past and\nfuture) and reasoning over multiple concurrent fairness metrics."}, {"title": "A Additional Experiments", "content": "A.1 Integer Program Definitions of VRP & TAP\nIn this section, we formally define the standard integer programs used\nin the experiments section."}, {"title": "A.1.1 VRP Integer Program", "content": "Recall that P is the set of points, V is the set of vehicles, r\u2208 P is the\ndepot, and D is the distance function between all points. For conve-\nnience, let P\u00ae := P\\{r} and I = {S \u2286 P\u00ae | 2 \u2264 |S| \u2264 n -2}. Note\nthat the binary variable xa,b,v is 1 if and only if vehicle v is routed\nfrom point a to point b.\n$\\begin{aligned}\nmin \\&\\sum_{a\\in P} \\sum_{b\\in P} \\sum_{v\\in V} x_{a,b,v}D(a, b) \\\\\ns.t. \\&\\sum_{a\\in P}x_{a,b,v} = \\sum_{a\\in P}x_{b,a,v}, \\forall b \\in P, v \\in V \\\\\n&\\sum_{a\\in P\\{b}} \\sum_{v\\in V} x_{a,b,v} = 1,\\forall b \\in P^- \\\\\n&\\sum_{a\\in P^-}x_{r,a,v} = 1, v \\in V \\\\\n&\\sum_{a\\in S} \\sum_{b\\in s} \\sum_{v\\in V} x_{a,b,v} \\geq 1,S \\in S \\\\\n& x_{a,b,y} \\in {0,1}, a, b \\in P, v \\in V\n\\end{aligned}$"}, {"title": "A.1.2 TAP Integer Program", "content": "Recall that A is the set of agents, T is the set of tasks, and C(a,t) is\nthe cost agent a incurs to perform task t. We assume that |A| = |T|.\nThe binary variable xa,t is 1 if and only if task t is assigned to agent\na.\n$\\begin{aligned}\nmin \\&\\sum_{a\\in A} \\sum_{t\\in T} x_{a,t}C(a,t) \\\\\ns.t. \\&\\sum_{a\\in A}x_{a,t} = 1, t \\in T \\\\\n&\\sum_{t\\in T} x_{a,t} = 1, a \\in A \\\\\n& x_{a,t} \\in {0,1}, a \\in A, t \\in T\n\\end{aligned}$"}, {"title": "A.2 TAP Instance Generation and History", "content": "In this section, we provide more of a justification for the random in-\nstance and history generation for TAP we use in our experiments. We\ncan conceptualize these instances as workers being assigned tasks.\nThe workers either take 5, 20, or 30 minutes to complete a task and\nsometimes a worker does not have a 5-minute task in the current\nbatch of tasks, maybe due to a lack of expertise. Minimizing the total\nsum of costs corresponds to minimizing the total person-hours re-\nquired to complete all tasks, whereas minimizing the maximum cost\ncorresponds to minimizing the amount of time any one person has to\nspend on a task.\nRegarding the history we construct, we want to discuss the agents\nW who are assigned the largest historical cost of 180. The amount\n180 comes from the hypothetical scenario where for 6 instances\nstraight, all agents in W received a task of cost 30. Further, we choose\nW to be the agents with the largest total cost not in C for two reasons:\n(1) we want to see the dynamics of the constrained agents in C with-\nout giving them the most \"historical debt\" and (2) OP and FOP treat\nall agents identically, so it is conceivable that the agents who received\nthe largest cost under OP could also have the largest historical debt.\nAs for how we constructed the rest of the history, the 12 agents\nwho were assigned a historical cost of 120 were given that value\nunder the hypothetical scenario where they were assigned cost 20\ntasks for 6 instances. Similarly for the 24 agents who were assigned\na historical cost of 30 - they would have received a cost 5 task for 6\ninstances straight."}, {"title": "A.3 Distribution of Fairness in History", "content": "A.3.1 Nurses Scheduling Problem (NSP)\nIn this domain", "S": "N\u2192R\n(higher means more senior). The nurses may have preferences over\nsome shifts", "function": "N\u00d7S \u2192 {0", "ones": "Q = 1/Qmax  \u03a3\u2208Se\u2211\u2208N Xs S(n). Qmax is again a nor-\nmalization constant. For the fairness metric", "domain": "n$\\ F_{mm}(x) = \\frac{min_i U_i(x)}{max_jU_j(x)}$\nwhere min; U; (x) and maxjUj(x) denote the minimum and maximum"}]}