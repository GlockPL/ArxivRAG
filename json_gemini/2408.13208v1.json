{"title": "Temporal Fairness in Decision Making Problems", "authors": ["Manuel R. Torres", "Parisa Zehtabi", "Michael Cashmore", "Daniele Magazzenia", "Manuela Veloso"], "abstract": "In this work we consider a new interpretation of fairness in decision making problems. Building upon existing fairness formulations, we focus on how to reason over fairness from a temporal perspective, taking into account the fairness of a history of past decisions. After introducing the concept of temporal fairness, we propose three approaches that incorporate temporal fairness in decision making problems formulated as optimization problems. We present a qualitative evaluation of our approach in four different domains and compare the solutions against a baseline approach that does not consider the temporal aspect of fairness.", "sections": [{"title": "1 Introduction", "content": "Automated decision making is an important part of artificial intelligence with a variety of application areas, from scheduling and resource allocation, to robotics and autonomous vehicles. Decision making processes typically aim to optimize an overall benefit or cost. However, as we strive to make our algorithms and agents more intelligent, it is important to ensure that they also account for ethical considerations such as fairness. The need for fair algorithms and agents has been widely studied across different areas, such as robotics [3], healthcare [4], telecommunications [11], and resource allocation [15], among others.\nFormulating fairness concerns in different domains can be challenging, and it has been the subject of many studies [20]. In this paper, we take a new angle to considering fairness in decision making processes. We build upon previous fairness formulations, and focus on how to reason about fairness from a temporal perspective, accounting for the fairness of a history of past decisions. We aim to introduce the concept of \"temporal fairness\" into the decision making process, which measures the fairness of solutions throughout time.\nAs a motivating example consider the scenario depicted in Figure 1 where courses must be assigned to a pool of lecturers (l\u2081,l\u2082 and l\u2083) in semester t. Each lecturer is specialized in different areas and the teaching quality of a course is proportional to the expertise of its lecturer (the gray bars below the lecturers depict their expertise on different topics). Figure 2a depicts the number of courses assigned to each lecturer in the past four semesters. Lecturer l\u2081 has received a higher teaching load than l\u2082 over the past four semesters. Regardless of the reasons that have led to the scenario in Figure 2a, the reality is that there has been some \"historical unfairness\". Figure 2b depicts the cumulative teaching load over time for each of the two available lecturers l\u2081 and l\u2082.\nA new course allocation must be made for semester t. If an allocation is made that is presently fair, in which both lecturers teach"}, {"title": "2 Problem Formulation", "content": "We consider decision making problems solved by finding a solution that maximizes an objective function while satisfying a set of constraints. First, we consider a decision making problem where no fairness metric is considered. Such problems can be formulated as an Optimization Problem (OP).\nDefinition 1. An Optimization Problem (OP) is a tuple (Q, X,C) where Q is a quality metric, X is the domain for optimization, and C is the set of constraints.\nFormally, we define an OP as:\n$\\begin{aligned}\n\\max_{x \\in X} & \\quad Q(x) \\\\\ns.t. & \\quad \\mathcal{C}(x) \n\\end{aligned}$ (1)\nIn this setting, the goal is to find a solution x* that maximizes a given quality metric Q, while being subject to a set of constraints$\\mathcal{C}(x)$. We let variables x denote the optimization variables of the problem. Since this formulation only reasons over the quality metric, it is possible that the optimal solutions may be deemed unfair according to some fairness metric. Moreover, as depicted in Figure 2a, this formulation may lead to a fast accumulation of unfair solutions."}, {"title": "2.1 FOP: Incorporating Fairness", "content": "We now incorporate a fairness metric F into the formulation of the optimization problem. A Fair Optimization Problem (FOP) can be defined as:\nDefinition 2. A Fair Optimization Problem (FOP) is a tuple (Q,F,X,C,\u03b2) where F is the fairness metric and \u03b2 \u2208 R is a parameter than controls the trade-off between quality and fairness. The remaining elements follow the original OP.\nFormally, an FOP can be modelled as:\n$\\begin{aligned}\n\\max_{x \\in X} & \\quad Q(x) + \\beta F(x) \\\\\ns.t. & \\quad \\mathcal{C}(x) \n\\end{aligned}$ (2)\nIn general, we will assume that F returns higher values for fair so- lutions and lower values for unfair solutions. In practice, it may be convenient for both Q and F to have well-specified ranges, rendering it easier to understand the impact of the parameter \u03b2. However, the formulation is general and supports arbitrary quality and fairness metrics. Finally, we note that the specification of the fairness metric F may potentially require the introduction/modification of con- straints. In order to keep notation simple, we will continue denoting the set of constraints as before,$\\mathcal{C}(x)$.\nAs an example building upon our previous scenario of the course assignment domain, let us consider a relative max-min fairness metric Frmm, which compares the maximum and minimum number of courses lectured by all lecturers, versus the total number of courses lectured during that time. Formally,\n$F_{rmm}(x) = 1- \\frac{\\max_i S_i(x) - \\min_j S_j(x)}{S(x)}$\nwhere Si(x) is the number of courses lectured by l\u1d62 in solution x, and S(x) is the total number of courses lectured. The range of Frmm is [0, 1]. It is maximized when lecturers get an equal lecturing load, and minimized when one of the lecturers takes the entire load. While incorporating the new fairness metric in the FOP leads to solutions that are fair according to F (or at least fairer, depending on \u03b2), there may still exist some historical unfairness that remains from previous allocations. Figures 2a and 2b hinted at this, depicting a scenario where scheduling a fair plan at time step t would have maintained the gap of cumulative courses lectured."}, {"title": "2.2 HFOP: Incorporating Historical Fairness", "content": "FOPS assume a fairness metric F that only reasons over the fairness of a solution x. In order to account for existing historical unfairness, it is thus important to reason over the fairness of a solution x in the context of the history of past solutions H = (x\u209c\u208b\ua786,...,x\u209c\u208b\u2081), where x\u209c\u208b\u2090 is a previous solution from time step t \u2013 \u0394.\nWe formalize the notion of such fairness metrics in the following definition.\nDefinition 3. A historical fairness metric FH : X \u2192 R is a fairness metric for an FOP (Q,FH, X,C, \u03b2) where H \u2264 X contains solutions satisfying C. We assume F\u2080 is a fairness metric and write it as F.\nThe historical fairness metric FH can be used to control how fast or slow historical unfairness is compensated. Also, as it is not a limita- tion for the real-world scenarios we consider in this paper, we assume the time between the historical solutions in H is uniform.\nDefinition 4. A Historical Fair Optimization Problem (HFOP) is an FOP tuple (Q,FH,H, X,C, \u03b2) where FH is a historical fairness metric."}, {"title": "2.3 DHFOP: Incorporating Discounted Historical Fairness", "content": "We observed in Figure 3 how slowly the historical unfairness would be compensated when following the solutions produced by FOP. In fact, it turns out it would never be fully compensated - since FOP always computes perfectly balanced schedules, the lecturing load gap of 5 would remain unchanged. This behaviour may not fit many do- mains. It may become especially problematic when considering sce- narios with long histories of unfair solutions.\nA historical fairness metric FH allows the specification of an opti- mization problem HFOP that reasons over remnant historical unfair- ness. We observed this may lead to solutions that seem unfair at time step t when only considering the current time step (i.e., according to F).\nIn practice, it makes sense to consider a \"forgetting rate phe- nomenon\", where we attribute more importance to recent events than those in a distant past. However, FH(x) puts equal importance to the fairness of solution x at time step t and all past solutions. In order to model the importance of recent events we propose the discounted historical fairness metric FH,y, which discounts past unfairness with a forgetting discount factor \u03b3.\nDefinition 5. A Discounted Historical Fair Optimization Problem (DHFOP) is a tuple (Q,FH,\u03b3,H, X,C, \u03b2), where FH,y is a historical fairness metric that reasons over a history of previous solutions H =(x\u209c\u208b\ua786,...,x\u209c\u208b\u2081) with the discount factor \u03b3. The remaining elements follow the HFOP.\nReasoning over a discounted historical fairness metric FH,y allows us to control the importance of the unfairness of past solutions rela- tive to more recent ones. Formally, the optimization problem is:\n$\\begin{aligned}\n\\max_{x \\in X} & \\quad Q(x) + \\beta F_{H,\\gamma}(x) \\\\\ns.t. & \\quad \\mathcal{C}(x) \n\\end{aligned}$ (4)\nThere are now two hyper-parameters. The discount factor \u03b3, which sets the importance of the fairness of past solutions, and the parame- ter \u03b2, which controls the quality/fairness trade-off.\nRevisiting once more our running example of course assignment and the max-min fairness metric, we could define its discounted his- torical variant as follows:\n$F_{\\gamma}(x) = 1 - \\frac{\\max_i S_{\\gamma,i}((H,x)) - \\min_j S_{\\gamma,j}((H,x))}{S_{\\gamma}((H,x))}$\nwhere $S_{\\gamma,i}((H,x)) = \\sum_{\\Delta=0}^{T_H} \\gamma^{\\Delta} S_i(x_{\\Delta})$ is the discounted number of courses lectured by l\u1d62 over all solutions in (H,x) and similarly,"}, {"title": "2.4 MSDHFOP: Historically Fair Planning with Future Forecasts", "content": "All problems introduced so far are of a single-shot nature, where the solver is assumed to make a decision for the current time step t. However, single-shot decisions can often result in sub-optimal solu- tions in complex domains with extended horizons, such as planning problems [8]. Reasoning over multiple time steps into the future can allow for more effective solutions given knowledge or predictions about future events.\nIn the setting of fairness, reasoning over multiple steps into the future may allow for interesting solutions. For example, due to future constraints, a solution that is fair over a given horizon may require initial solutions that seem unfair when analyzed independently. In order to account for both existing historical unfairness and a planning horizon into the future, we let FH,\u03b3,\u03c4(Xt,...,Xt+TF) denote a fairness metric that considers both a history of solutions H and a sequence of TF future problems (xt,...,Xt+TF), with the past and the future being discounted according to \u03b3 and \u03c4.\nDefinition 6. A Multi Step Historical Fair Optimization Problem (MSDHFOP) is a tuple (Q,FH,\u03b3,\u03c4,\u0397, X,C,\u03b2,\u03b3,\u03c4), where FH,\u03b3,\u03c4 is a historical fairness metric that reasons over a history of previous solutions H = (Xt-TH,...,xt-1) and a sequence of future solutions (X1,...,Xt+TF). \u03b3 and \u03c4 are discount factors. The remaining elements follow the DHFOP.\nWe formulate an MSDHFOP as:\n$\\begin{aligned}\n\\max_{X_0,...,X_{TF}} & \\quad \\sum_{t=0}^{T_F} \\tau^t Q(x) + \\beta F_{H, \\gamma,\\tau}(X_0,...,X_{TF}) \\\\\ns.t. & \\quad \\mathcal{C}(x) \n\\end{aligned}$ (5)\nThe first term computes the discounted sum of quality of the planned solutions. The second term computes the multi-step historical fair- ness metric. As before, the discount factor y sets the importance of past solutions relative to more recent ones in the computation of fair- ness. Similarly, the discount factor \u03c4 discounts future solutions rela- tive to the previous one, impacting both fairness and solution quality. Whereas y seeks to model the \"recency effect\" from a fairness per- spective (i.e., we tend to attribute more importance to recent events than those in a distant past), \u03c4 seeks to model uncertainty in planning into the future (i.e., it is easier to predict states closer in time than those in a distant future).\nWe can again build upon the discounted historical relative max- min fairness metric, and introduce a variant that also reasons over the next TF solutions Xt:TF = (Xt,..., Xt+TF), where we define\n$F_{\\gamma,\\tau}^{rmm} F_H(X_{t:TF}) = 1 - \\frac{\\max_i S_{\\gamma,\\tau,i}((H,X_{t:T_F})) - \\min_j S_{\\gamma,j}((H, x_{t:T_F}))}{S_{\\gamma}((H,X_{t:TF})}$\nwhere\n$S_{\\gamma,\\tau,i}((H,X_{t:T_F})) = \\sum_{\\Delta=1}^{T_H} \\gamma^{\\Delta} S_i(x_{\\Delta}) + \\sum_{\\Delta=0}^{T_F} \\tau^{\\Delta} S_i(x_{t+\\Delta})$\nis the discounted number of courses lectured by li, over all so- lutions in history H and future planned solutions Xt:TF. Similarly, S\u03b3,\u03c4((H,Xt:TF)) is the discounted total number of courses lectured."}, {"title": "3 Experimental Evaluation", "content": "We evaluate our formulations across multiple domains using differ- ent fairness metrics. We start with a technical description of each do- main, introducing the decision variables, and the quality and fairness metrics to be used. The machine used to run experiments is an In- tel(R) Xeon(R) CPU E3-1585L v5 @ 3.00GHz with 64GB of RAM."}, {"title": "3.1.1 Course Assignment Problem (CAP)", "content": "This is the domain that has been used throughout the paper, where a set of lecturers I is to be assigned to a set of courses C. When dealing with multi-step decision making settings, we may denote the set of courses at time step t as Ct. The expertise of lecturer I in course c is measured by S : L \u00d7 C \u2192 R, and higher values correspond to higher expertise. Decision variable x1,c \u2208 {0,0.5,1} indicate the load of lecturer I in teaching course c- a lecturer may not lecture the course at all, or lecture either half a course or the full course.\nWe consider a quality metric $Q = \\frac{1}{Q_{max}} \\sum_{c \\in C_t} \\sum_{l \\in L} x_{l,c} S(l,c)$, which rewards course assignments with skilled lecturers. Qmax is a normalization constant, denoting the maximum sum of expertise possible- this ensures Q is bounded between 0 and 1. In order to display the generality of our formulation, throughout the experimen- tal evaluation with this domain we may use different fairness metrics."}, {"title": "3.1.2 Vehicle Routing Problem (VRP)", "content": "In this domain, given a set of vehicles V, a set of points P that must all be traveled to exactly once, a depot r\u2208 P the vehicles must leave from and return to, and distances between all points D: P\u00d7P\u2192R+, determine a route for each vehicle that minimizes the total distance traveled. We consider a standard integer program to model the OP where quality Q(x) is the total distance traveled (see Supplementary Materials A.1 for a full definition). To model fairness, for a given so- lution x to the integer program, let Uv (x) be the total distance vehicle v travels under x and define F(x) := max\u1d65\u2091\u1d65 U\u1d65(x) \u2013 minw\u2091\u1d65 Uw(x). This notion of fairness is similar to proportional equality and is used in [13] in a multi-objective version of VRP."}, {"title": "3.1.3 Task Allocation Problem (TAP)", "content": "In this domain, given a set of agents A, a set of tasks T, and a cost associated with each agent for each task C: A\u00d7T\u2192 R+, find an as- signment of tasks to agents such that the sum of costs is minimized. We consider a standard integer program to model the OP where qual- ity Q(x) is the sum of costs (see Supplementary Materials A.1 for a full definition). The fairness metric we consider is the classic min- imax notion of fairness (see [20] and references therein), where for a given solution x to the integer program, let Ua(x) be the total cost agent a incurs under x and define F(x) := maxa\u2091A Ua(x)."}, {"title": "3.1.4 Nurse Scheduling Problem (NSP)", "content": "We consider a version of this classical problem in operations re- search. In the Supplementary Materials A.3, we first formally define the problem and then we show the impact of different histories on the NSP, in particular showcasing the impact of the discount factor in DHFOP."}, {"title": "3.2 Quality vs. Fairness", "content": "We start our experimental evaluation with an example depicting the quality vs. fairness trade-off, and the impact of the parameter \u03b2 therein. Let's consider an instance of CAP with 3 lecturers, 11,12,13 and 2 courses C1, C2. Across all 3 courses, l\u2081 has high skills (S = 2), 12 has medium skills (S = 1.5), and 13 has low skills (S = 0). We use a historical quadratic max-min gap fairness metric\n$F_{qmmg}^{H}(x) = -\\frac{1}{2} \\left( \\frac{1}{2} (\\max(S((H,x)) - \\min S_i((H,x)))^2 \\right)$"}, {"title": "3.3 Planning with Future Forecasts", "content": "We now evaluate the benefits from a fairness perspective of MSD- HFOP reasoning over multiple steps into the future from a fairness perspective. Consider a simplified instance of CAP with two lectur- ers 11 and 12, and two courses c\u2081 and c2. Across all courses, 1\u2081 has high skills (S = 2) and 12 has medium skills (S = 1). To showcase the flexibility of our approach to fairness metrics, we now consider another version of the maximin fairness metric where utility U; mea- sures the number of courses taught by lecturer li:\n$F_{mm}(x) = \\frac{\\min_i U_i(x)}{\\max_j U_j(x)}$\nAssuming no historical solutions, the scheduler is now to plan the course assignments for the next T = 4 semesters. There exists a known constraint about the future-l\u2081 will not be able to take any lecturing load on semesters t+2 and t+3 due to a sabbatical leave."}, {"title": "3.4 Increasing Complexity and Benchmarking", "content": "We now examine a more complex problem and show the impact of considering fairness on running time. We first introduce a method for generating random instances and a history of past solutions for VRP. Consider a square integer grid of a fixed size. We deterministically place the depot at the center of the grid and, given a fixed number of points n, choose n of the grid points uniformly at random (not includ- ing the depot). For generating history, we generate random instances and solve the problem optimally on these random instances.\nFor our experiments in this section and the following, we imple- mented the integer program with the corresponding fairness con- straints using the PuLP Python library [17] and used the CBC solver [6] with a standard linearization of F(x) (see, e.g., [20]). All times measured are wall-clock times for the combined model-building and solving times.\nIn our experiments, we consider 4 vehicles V = {V1, V2, V3, V4} and 12 locations. We generate a history of 5 steps and a single ran- dom instance. For each historical instance, we assume V\u2081 always had the shortest route, V2 the second shortest, and similarly for V3 and V4."}, {"title": "3.5 Larger Scale Experimentation", "content": "In this section, we show that our framework can be applied on a larger scale than considered in the previous sections. We introduce a method for generating random instances for TAP. We create in- stances with |A| = |T| = 40. For each agent a \u2208 A, one task is chosen uniformly at random to have cost 5, three tasks are chosen uniformly at random to have cost 20, and the rest of the tasks have cost 30. We sometimes deterministically enforce that an agent a does not have a task of cost 5 and this task is replaced with a cost 30 task, in which case we say agent a is constrained.\nWe refer to the following setup as a single run and we average our results over 10 runs. Sample 8 agents uniformly at random from A and denote this subset as C. Produce 3 random instances according to our random instance generation given above. Then produce 3 more random instances where all agents in C are constrained. These 6 in- stances are the future instances. To generate history, we run the OP on each of these 6 instances. Sort the agents according to total cost. In this order, the last 4 agents not in C are assigned a historical cost of 180, call these agents W. Amongst the remaining agents, the first 24 agents are assigned a historical cost of 30. The remaining 12 agents are assigned a historical cost of 120. We give more justification for our method of random instance generation and history generation in the Supplementary Materials A.2. All instances are run with \u03b2 = 10."}, {"title": "4 Related Work", "content": "In recent times, a significant amount of research has been dedicated to fairness in AI, with a focus on predictive models and algorithmic fairness [14]. In machine learning, in particular, the topic of long- term fairness has been the subject of much attention [5, 7, 10]. The long-term consideration of fairness is relevant to the MSDHFOP for- mulation, where we consider both future and past history.\nOther lines of research look into the connections of algorithmic fairness and ethical decision making in the context of sequential de- cision making and planning [18]. Nashed et al. explore how each of these settings has articulated its normative concerns, the viability of different techniques for these different settings, and how ideas from one may be useful for the other.\nMotivated by computational resource allocation problems, there exists a vast literature on the topic of fairness in real-time schedul- ing. Examples include the fair scheduling of periodically arriving tasks with deadlines [2], or more generally, the problem of schedul- ing tasks to long lived processes while taking into account the bene- fit/cost to each process [1]. While these works look at fairness from a temporal perspective-seeking to ensure a fair load to the different processes-they do not consider possible historical unfairness due to previous solutions and tend to focus on a specific fairness metric.\nIn the areas of decision-making and planning, several works have focused on different ways to mathematically formulate fairness met- rics. Recent work surveys various schemes that have been proposed for formulating ethics-related criteria, including those that integrate efficiency and fairness concerns [20]. They emphasize the challenges of having a single definition of fairness, as different definitions are appropriate for different contexts. Additionally, different fairness models are grouped into clusters, each representing a different type of fairness principle, to facilitate comparisons and help identify the most suitable model for practical applications. While the fairness metrics introduced did not consider fairness from a temporal per- spective where a history of past solutions exists, they can be adapted and used as part of all our formulations.\nThere has also been a growing interest in fairness in multi-agent decision making, planning [19], and reinforcement learning [12, 9]. Recent work focuses on fairness in long-term decision making prob- lems, introducing a new voting formalism that takes the history of previous decisions into account [16]. While the concept of consider- ing history is similar to the definition of HFOP, our formalism con- siders a centralized decision making process."}, {"title": "5 Conclusion and Future Work", "content": "In this work we took a new angle to considering fairness in decision making processes. Building upon previous fairness formulations, we focused on how to reason about fairness from a temporal perspective, especially when there exists a history of past decisions that may have been potentially unfair. In this setting, we proposed to reason over the concept of \"temporal fairness\" in decision making processes.\nStarting from a general decision making problem-OP-we in- crementally built our approach to reason over temporal fairness, ac- counting for both past solutions and predictions about the future. With the introduction of a fairness metric in the objective, FOP ex- tends OP by reasoning over the quality/fairness trade-off of a solu- tion. To reason over historical unfairness, we propose HFOP, where the fairness metric takes into account a history of previous solutions. A discounted version DHFOP is then proposed to allow us to model the importance of more recent events. Finally, the MSDHFOP formu- lation is extended to reason over both historical and future solutions.\nIn the experimental evaluation we assess our approach across differ- ent domains and show, in particular, how our approach is compatible with different fairness metrics.\nAs directions for future work, we envision exploring scenarios where different fairness metrics are used across time (in the past and future) and reasoning over multiple concurrent fairness metrics."}, {"title": "6 Disclaimer", "content": "This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates (\"JP Morgan\"), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This doc- ument is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdic- tion or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."}, {"title": "A Additional Experiments", "content": "In this section, we formally define the standard integer programs used in the experiments section."}, {"title": "A.1 Integer Program Definitions of VRP & TAP", "content": "Recall that P is the set of points, V is the set of vehicles, r\u2208 P is the depot, and D is the distance function between all points. For conve- nience, let P\u00ae := P\\{r} and I = {S \u2286 P\u00ae | 2 \u2264 |S| \u2264 n -2}. Note that the binary variable xa,b,v is 1 if and only if vehicle v is routed from point a to point b.\nmin $\\sum_{a \\in P} \\sum_{b \\in P} \\sum_{v \\in V} X_{a,b,v}D(a, b)$\ns.t.\n$\\sum_{a \\in P} X_{a,b,v} = \\sum_{a \\in P} X_{b,a,v}, b \\in P, v \\in V$\n$\\sum_{a \\in P\\{b\\}} \\sum_{v \\in V} X_{a,b,v} = 1,\\forall b \\in P^-$\n$\\sum_{a \\in P^-} X_{r a,v} = 1, v \\in V$\n$\\sum_{a \\in S} \\sum_{b \\in S} \\sum_{v \\in V} X_{a,b,v} \\geq 1,S \\in S$\nX_{a,b,y} \\in {0,1}, a, b \\in P, v \\in V"}, {"title": "A.1.2 TAP Integer Program", "content": "Recall that A is the set of agents, T is the set of tasks, and C(a,t) is the cost agent a incurs to perform task t. We assume that |A| = |T|. The binary variable xa,t is 1 if and only if task t is assigned to agent a.\n$\\min \\sum_{a \\in A} \\sum_{t \\in T} x_{a,t}C(a,t)$\ns.t.\n$\\sum_{a \\in A} x_{a,t} = 1, t \\in T$\n$\\sum_{t \\in T} x_{a,t} = 1, a \\in A$\nx_{a,t} \\in {0,1}, a \\in A, t \\in T"}, {"title": "A.2 TAP Instance Generation and History", "content": "In this section, we provide more of a justification for the random in- stance and history generation for TAP we use in our experiments. We can conceptualize these instances as workers being assigned tasks. The workers either take 5, 20, or 30 minutes to complete a task and sometimes a worker does not have a 5-minute task in the current batch of tasks, maybe due to a lack of expertise. Minimizing the total sum of costs corresponds to minimizing the total person-hours re- quired to complete all tasks, whereas minimizing the maximum cost corresponds to minimizing the amount of time any one person has to spend on a task.\nRegarding the history we construct, we want to discuss the agents W who are assigned the largest historical cost of 180. The amount 180 comes from the hypothetical scenario where for 6 instances straight, all agents in W received a task of cost 30. Further, we choose W to be the agents with the largest total cost not in C for two reasons: (1) we want to see the dynamics of the constrained agents in C with- out giving them the most \"historical debt\" and (2) OP and FOP treat all agents identically, so it is conceivable that the agents who received the largest cost under OP could also have the largest historical debt.\nAs for how we constructed the rest of the history, the 12 agents who were assigned a historical cost of 120 were given that value under the hypothetical scenario where they were assigned cost 20 tasks for 6 instances. Similarly for the 24 agents who were assigned a historical cost of 30 - they would have received a cost 5 task for 6 instances straight."}, {"title": "A.3 Distribution of Fairness in History", "content": "In this domain, a set of nurses N is to be assigned to a set of morn- ing/evening shifts I across 5 days of the week. We let Im and Se denote the morning and evening shifts, respectively. The deci- sion variable xn,s \u2208 {0,1} indicates whether a nurse n is assigned to shift s. The seniority of the nurses is represented by S : N\u2192R (higher means more senior). The nurses may have preferences over some shifts, and this is represented through a utility function : N\u00d7S \u2192 {0,1,2,3}.\nThe quality metric considered looks to reward assignments of se- nior nurses to the evening shifts, as they tend to be the most prob- lematic ones: $Q = \\frac{1}{Q_{max}} \\sum_{s \\in S_E} \\sum_{n \\in N} X_{n,s} S(n)$. Qmax is again a nor- malization constant. For the fairness metric, we use the same version of the maximin metric as we used for the course assignment domain:\n$F_{mm}(x) = \\frac{\\min_i U_i(x)}{\\max_j U_j(x)}$\nwhere min; U; (x) and maxjUj(x) denote the minimum and maximum utility among all nurses, respectively. Note that Fmm is bounded in [0, 1]."}, {"title": "A.3.2 Experiments for NSP", "content": "In this section, we evaluate the impact of a history of previous solutions in our proposed approach. More precisely, we consider two scenarios with equal-length histories, but where the \"distribution\" of the fairness across the histories differs. By distribution of fairness, we refer to the trend of the fairness of the previous solutions throughout time.\nWe consider an instance of NSP with 5 nurses, n1, n2, n3, n4, and n5. At each decision-making time step t, we aim to find a solution for a time span of 5 days, where each day has two shifts-morning (m) and evening (e). We assume the seniority of the nurses is as fol- lows: n\u2081 \u2192 3,n2 \u2192 2,n3 \u2192 1,n4 \u2192 0, n5 \u2192 0. Table 9 depicts the utilities assigned by each nurse to the different shifts. The problem is interesting since the most senior nurses tend to have a stronger pref- erence for morning shifts, whereas our quality metric looks to reward seniority in evening shifts.\nWe consider two equal-length histories, H\u2081 and H2. Figure 6a de- picts the fairness of the solutions of each history. We observe that the solutions in H\u2081/H2 show an increasing/decreasing trend in fairness across time. In fact, H\u2081 and H2 include the exact same solutions,"}, {"title": "However, when we adopt a discount factor y = 0.65, we observe that", "content": ""}]}