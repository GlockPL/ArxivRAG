{"title": "HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt"], "abstract": "This paper presents a novel approach to multi-robot planning and collaboration. We demonstrate a cognitive strategy for robots in human-robot teams that incorporates metacognition, natural language communication, and explainability. The system is embodied using the HARMONIC architecture that flexibly integrates cognitive and control capabilities across the team. We evaluate our approach through simulation experiments involving a joint search task by a team of heterogeneous robots (a UGV and a drone) and a human. We detail the system's handling of complex, real-world scenarios, effective action coordination between robots with different capabilities, and natural human-robot communication. This work demonstrates that the robots' ability to reason about plans, goals, and attitudes, and to provide explanations for actions and decisions are essential prerequisites for realistic human-robot teaming.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's rapidly evolving technological landscape, robots are becoming increasingly ubiquitous across various domains, from manufacturing and healthcare to search and rescue operations [1], [2]. This proliferation has given rise to a growing number of heterogeneous robot teams, where machines with diverse capabilities collaborate to achieve common goals. To harness these distributed and varied problem-solving capabilities, research in Multi-Agent Planning (MAP) has focused on developing sophisticated planning and collaboration strategies.\nDespite significant advancements in multi-robot planning and collaboration [3], [4], a number of issues related to human-robotic teaming remain relatively unexplored [5]. For robots to serve as trusted partners in human-robotic teams, they must demonstrate a number of cognitive and metacognitive abilities, including:\n\u2022 developing and enhancing an understanding of team tasks and team organization;\n\u2022 developing and enhancing specific capabilities, involving the responsibilities, preferences, and actions of self and other team members [6];\n\u2022 maintaining and using an episodic memory of past activities, as when leveraging past experiences to make informed predictions about future needs and actions;\n\u2022 interpreting language, visual perception, and haptic perception in terms of an underlying formal model of the world (ontology); [7]\n\u2022 communicating with teammates using interpreted, meaningful natural language.\nThe above capabilities will facilitate complex team interactions, attention management, goal-setting, planning (including incorporating a variety of MAP strategies), and overcoming unexpected challenges in a way that humans can readily interpret. The integration of verbal and visual communication enhances the overall effectiveness of skill transfer and knowledge sharing within human-robot teams. Furthermore, meaning-oriented natural language communication allows robots to provide causal explanations for their actions and decisions, thereby enhancing overall transparency and explainability, which are stepping stones to trust [7]. A growing number of robots today are based on foundational models, such as Large Language Models (LLMs) [8], [9] and Vision-Language-Action (VLA) models [10], [11]. These black-box technologies contribute to several components of our architecture. However, we make sure that the modules whose operation and output should be explanatory to humans will be fully glass-box inspectable and explainable in human terms both to team members and to external inspectors.\nWe present our approach to human-robot teaming using the example of a team comprised of two robots and a human executing a joint task. We demonstrate the robots' knowledge of team organization as well as, their ability to (a) dynamically negotiate tasks and allocate responsibilities, (b) select and modify plans, and (c) adapt language communication to both the situation and the needs of their interlocutors. We implement these robotic capabilities using the novel HARMONIC architecture that operates in parallel at two levels - the strategic cognitive level, which supports operations requiring reasoning, and the tactical control level, which supports skill-based, \"automatic,\" reflexive operation.\nSpecifically, in this paper, we report:"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Multi-Agent Planning (MAP) in Heterogeneous Multi-Robot (HMR) teams encompasses a spectrum of strategies ranging from centralized to decentralized approaches, with varying degrees of inter-agent coupling. In centralized planning, a single agent orchestrates the entire team's actions, potentially achieving globally optimal solutions but facing scalability challenges in larger teams. Conversely, decentralized methods enable autonomous plan negotiation among agents, offering enhanced scalability and adaptability at the risk of local suboptimality [12]. Brafman et al. [13] quantify this trade-off, noting that centralized planning in closely coupled multi-agent systems is exponentially more complex than solving independent problems in decoupled systems.\nTo balance these extremes, modern HMR teams often implement hybrid MAP strategies. These may include centralized hierarchical or collaborative planning coupled with decentralized execution [14], dynamic switching between local and global plans based on contextual factors [3], or centralized conflict resolution alongside decentralized planning [15]. Advanced implementations of these hybrid approaches include hierarchical planning frameworks, auction-based task allocation systems [16], and distributed partial-order planners [17], which have demonstrated significant collaborative capabilities in complex multi-robot scenarios.\nIntra-team communication in an HMR team involves both inter-robot and human-robot interactions, each requiring distinct protocols. To date, most research has focused on inter-robot communication using standardized Agent Communication Languages to facilitate efficient information exchange [18]. The implementation of frameworks, such as CORA (Core Ontologies for Robotics and Automation) [19], while enhancing semantic interoperability and knowledge sharing among robotic agents, cannot support natural, human-level communication. To engender human trust, human-robot interactions must be carried out in everyday natural language.\nThe communication modalities currently employed in HMR teams, while effective for preprogrammed interactions between robots, Lemon et al. [20] and Natarajan et al. [5] support the need for a unified, human-centric approach to team communication. Rather than maintaining separate protocols for inter-robot and human-robot interactions, a common cognitive architecture that emulates human-like communication and reasoning offers a more cohesive solution, emulating human team dynamics.\nEarly work by Clark et al. [21] proposed a framework for this process, which was later adapted by Klein et al. [22] to team coordination, introducing the notion of common ground. These studies highlight essential capabilities for robotic team members, including maintaining shared understanding, negotiating goals and plans, managing attention, and handling commands and requests. Subsequent pioneering research in dialogue processing systems [23], [24], [25] and grounding [26], [27], [28], [29] further developed these ideas, laying the groundwork for addressing communication challenges in human-robot teams. Results of this research and our own prior work on team organization [30] and natural language communication [7] guided the implementation of cognitive processes for team organization and communication in HARMONIC."}, {"title": "III. THE HARMONIC FRAMEWORK", "content": "HARMONIC (Human-AI Robotic Team Member Operating with Natural Intelligence and Communication), shown in Fig. 1, is a dual-control cognitive-robotic architecture that integrates strategic, cognitive-level decision-making with tactical, skill-level robot control. It builds upon and extends the concept of hybrid control systems and architectures, as discussed in a comprehensive review by Dennis et al. [31]. It also represents an advancement over the type 2 integration approach employed in the DIARC framework [32], [33]. Unlike DIARC, where the strategic layer is embedded as a subsystem within the tactical layer to enable concurrent and dynamic operations, HARMONIC introduces a more sophisticated integration of these components.\nThe strategic and tactical architectural layers of HARMONIC are connected by a bidirectional interface for seamless communication. The strategic layer, built upon the OntoAgent cognitive architecture [34], [35], [7], encompasses modules for attention management, perception interpretation, and decision-making (see Section. IV). It employs both utility-based and analogical reasoning, enhanced by metacognitive abilities. This layer prioritizes strategic goals, manages plan agendas, and selects actions while continuously monitoring their execution. It also facilitates team-oriented operations, including communicating in natural language and explaining decisions.\nThe tactical layer is responsible for robot control at a lower level, processing sensor inputs, and planning motor actions to execute high-level commands from the strategic layer. It employs controllers, algorithms, and models to translate abstract commands into concrete robot actions. For instance, a command PICK(KEY,AT-POSITION1) to \"pick up a key on the ground\" initiates a series of precise operations, including object identification, position determination, trajectory"}, {"title": "IV. ONTOAGENT FOR STRATEGIC LEVEL PLANNING", "content": "OntoAgent is a content-centric cognitive architecture incorporated in HARMONIC. OntoAgent is designed to support the development of social intelligent agents through computational cognitive modeling [35], [7], [34]. This approach emphasizes the need to acquire, maintain, and dynamically expand large-scale knowledge bases, which are essential for an agent's perception interpretation, reasoning, and action. The architecture's memory structure is divided into three main components: a Situation Model (SM) that contains currently active concept instances; a Long-Term Semantic Memory (LTS) that stores knowledge about instances of events and objects, and an Episodic Memory (LTE) that stores knowledge about instances of events and objects. OntoAgent supports goal-directed behavior through a goal agenda and a prioritizer that selects which goals to pursue. It typically uses stored plans associated with goals but can also engage in reasoning from first principles when necessary.\nThe architecture is built on a service-based infrastructure, with key services including perception interpretation, attention management, reasoning, and action rendering. OntoAgent can integrate both native services and imported external capabilities, such as robotic vision or test-to-speech modules. This flexibility allows it to support multiple perception channels, including language understanding, visual perception, and simulated interoception (Chapter 8 of [35]).\nA crucial component of OntoAgent is OntoGraph, a knowledge base API that provides a unified format for representing and accessing knowledge across the system. OntoGraph supports inheritance, flexible organization of knowledge into \"spaces,\" and efficient querying and retrieval. It implements a graph database view of knowledge, allowing for complex relational queries and supporting the representation of inheritance hierarchies.\nOntoAgent places a strong emphasis on natural language understanding and meaning-based text generation. These capabilities rely on a semantic lexicon that links words and phrases with meanings grounded in a resident ontology. The language analyzer treats a large range of complex linguistic phenomena (lexical disambiguation, reference resolution, ellipsis reconstruction, new-word learning, etc. [7]) and generates ontologically-grounded interpretations of text meaning. The language generator, for its part, generates natural language utterances from ontologically grounded meaning representations that the agent creates as part of its reasoning about action. OntoAgent also supports ontological interpretation of visual percepts using an opticon whose entries link images to ontological objects.\nThe architecture supports goal-directed behavior through a goal agenda and a prioritizer that selects which goals to pursue. It typically uses stored plans associated with goals but can also engage in reasoning from first principles when necessary."}, {"title": "V. BEHAVIOR TREES FOR TACTICAL EXECUTION", "content": "Behavior Trees (BTs), originally developed for controlling non-player characters in video games, have found significant applications in robotics and AI [36]. BTs are directed trees that provide a flexible and intuitive way to design control actions and define task-planning hierarchies for agents. The flexibility and modularity of BTs make them particularly suitable for complex, hierarchical task planning in robotics and AI applications, allowing for easy modification and scaling of agent behaviors.\nBTs in the tactical layer of HARMONIC enable effective reactive control and support the system's safety and operational requirements in dynamic environments. The tactical layer also incorporates advanced control techniques such as whole-body compliant control [38], [36] with motion planning, path planning [39] etc. In addition to these, BTs can also be used for representing skills and low-level plans that can directly be shared between the robots[40], [41]. BTs typically use a blackboard architecture to maintain and access frequently checked condition variables [42], [43]. In the context of HARMONIC, a state manager keeps track of these variables on the tactical layer and allows for efficient querying and updating of the system's state during operation based on the sensory inputs and action commands received from the strategic layer.\nBTS for HARMONIC robots follow a design template prioritizing safety and needs [40], as shown in Fig. 2. Collision avoidance subtrees are placed leftmost, followed by robot needs subtrees, then action command subtrees triggered by the strategic layer. Fallback subtrees on the far right provide actions when the robot is idle or other conditions aren't met, such as random walks or waiting at the base station. This structure leverages the left-to-right execution of BTs to ensure proper prioritization of tasks.\nThe translation of the action commands to the appropriate state flags and control variables is done through the action data communication APIs, which modify the placeholder variables in the State Manager on the tactical layer."}, {"title": "VI. DEVELOPMENT OF DISTRIBUTED PLANS", "content": "In this section, we show how robots implemented in HARMONIC carry out a search task. They decide to act based on reasoning carried out in the strategic layer. Their plans (which are instances of ontological scripts) are divided into steps that are communicated to the tactical layer by means of action commands.\nA. High-level planning\nThe ontologies of agents in HARMONIC contain scripts for complex events that a particular agent can perform or understand other agents performing. Plans are as instances of scripts, with parameter values set for the situation at hand. Plans are instantiated by executing one or more of ontologically stored meta-plans, that is, plans that HARMONIC robots instantiate to guide the generation of plans in their operational domain. One such meta-plan is the collaborative activity plan, whose purpose is to help agents operating in teams organize themselves to accomplish a goal.\nThe details of the collaborative activity plan vary based on the agent's role in the team. For example, the team (or task) leader's plan is typically more complex than the plans of its subordinates since team leaders select the plan to accomplish the target goal, resolve preconditions, and instruct their subordinates. Subordinates' plans, by contrast, include awaiting instructions from the leader and, passively absorbing any other related information that comes their way.\nThe leader's script for collaboration is shown below. Since this is a meta-plan, it can be applied to many specific plans and, starts with few details populated. Part of its task is to fill in task-specific details. Select comments are provided for this human-readable formalism. For further information, please refer to [34], [35], [7]."}, {"title": "VII. EVALUATION", "content": "A. Simulation Environment and Task Design\nWe evaluated the HARMONIC framework's teaming strategy using a simulated search and retrieval task set in an apartment environment, as illustrated in Figure 5. The scenario involves a HMR team consisting of an unmanned ground vehicle (UGV), a drone, and a human team member serving as the team leader. The objective is to find a set of lost keys. The human initiates the task by remotely communicating with the robotic team members.\nThe simulated apartment contains zones with varied robot accessibility, necessitating coordinated efforts. We categorize these as: a) spaces accessible to both robots, b) elevated surfaces accessible to only the drones, and c) spaces below surfaces accessible only to the UGVs, as shown in Fig. 3. Each zone comprises waypoints - strategically defined locations that are dynamically utilized by the strategic layer. For this evaluation, we manually designated these waypoints and zones. However, in the future this process will be automated using machine learning models to classify and map zones based on spatial characteristics and accessibility, which will enhance the system's adaptability to new environments.\nThe simulations involved two robots, an Unmanned Ground Vehicle (UGV) and a drone (Fig. 4). These robots are both functionally and structurally heterogeneous [44], thus providing a diverse range of complementary perspectives and capabilities to the team. Both robots are equipped with a suite of sensors that enable them with localization, collision detection, and object recognition capabilities.\nEach robot runs its own instances of the tactical and strategic modules, enabling decentralized planning and control within the team. For the current scenario, the UGV is the robot team leader at the task level, making it responsible for maintaining interactions with the human and the drone. Currently, the leader assignment is randomized. In future work, we intend to develop and evaluate heuristic methods for implementing dynamic team hierarchies. This approach will optimize leadership selection based on contextual factors and individual agent capabilities depending on the task.\nIn the tactical layer of both robots, sensor data is encoded into data frames and periodically shared with the strategic layer. Additionally, each robot incorporates reactive control strategies for collision avoidance, that are embedded within behavior trees in their respective tactical layers.\nThe UGV is a custom-designed robot featuring a manipulator mounted atop a multidirectional mobile platform, as illustrated in Fig. 4.a. The robot's design allows it to access all ground-level locations and maneuver under certain objects, enhancing its capability to perform intricate search and manipulation tasks in confined spaces. Figure 2.c presents an excerpt from the Behavior Tree (BT) running on the Unmanned Ground Vehicle (UGV). This BT is based on the template shown in Figure 2.a.\nThe drone, on the other hand, can effectively perform search operations by scanning through a wide range of environments with sensors to scan the objects below it, as shown in Fig. 4.b. Its vertical mobility allows it to navigate over obstacles and access areas that may be challenging for the UGV (type-b zones).\nSince the robots communicate in natural language, their dialog and actions are comprehensible to humans, and humans can intervene when necessary.\nC. Collaborative Search for Lost Keys\nThe simulation environment is connected to a chatroom interface through which the human (Danny) communicates with the robots. A user interface (see Fig. 5) includes not only a chat window but additional widgets that display the robots' interpretations of language (TMRs) and visual inputs(VMRs), natural language renderings of the robots' thoughts, and their goal and plan agendas. This arrangement illustrates the transparency of the robots' operations and provides insights into their cognitive processes. The discussion below makes reference to the labels in Fig. 5 1\n1) Task Initiation: The scenario begins with Danny, initiating the task by sending the robots a message (M1). This input triggers the team leader (UGV) to place a COLLABORATIVE-ACTIVITY on its agenda (Fig. 5.5) and launch the SEARCH-FOR-LOST-OBJECT plan. Concurrently, the drone awaits instructions.\n2) Information Gathering: The UGV proceeds to verify preconditions for the SEARCH-FOR-LOST-OBJECT plan (Fig. 5.5). Although the object type is already known, information about its features and last-seen location would be useful. The UGV queries Danny for this information (M2/M4), and receives responses (M3/M5). The latter response prompts the system to prioritize the entry-way sub-zone (type-a zone) in the search sequence.\n3) Search Execution: With preconditions met, the UGV instructs the drone to initiate the search (M6), and both robots begin exploring their assigned areas. The individual tactical modules on the robots control the search process using a waypoint strategy, while the strategic (cognitive) module maintains awareness of area existence without directly guiding robot navigation. This approach allows for efficient local path planning while preserving high-level planning in the strategic layer.\n4) Communication and Coordination: Throughout the search, robots report their findings to each other and the human (M7-9). When a robot fails to locate the keys in a searched area, it communicates this to its partner.\n5) Task Completion: The VMRs widget (Fig. 5.3 and Fig. 5.6) displays the object detection results that the strategic layer processes from sensing frames communicated by the tactical layer. During the search execution, the strategic module continuously analyzes the sensor data frames, grounding these VMRs against the instance of the KEY object stored in its episodic memory. When the features match, the search is halted by either of the robots (leader in this case), informs the team (M8), and reports to Danny (M9). Notably, the UGV uses different language constructs when communicating with the drone versus Danny in (M9), demonstrating the cognitive agent's ability to generate context-appropriate language."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "This work presents a step towards the development of cognitive robots capable of effectively collaborating in heterogeneous human-robot teams. The HARMONIC architecture, with its dual-layer approach that combines strategic cognitive processing and tactical control enables the embodiment of cognitive architectures enhancing robots to exhibit metacognitive abilities, including team task understanding, episodic memory utilization, and multimodal perception interpretation. By integrating natural language communication and reasoning capabilities, our approach facilitates complex team interactions, enhances explainability, and builds trust the key factors in human-robot collaboration.\nThe glass-box nature of our system, unlike black-box models based on LLMs or VLAs, provides transparency and interpretability to modules whose operation and output should be explainable to humans. Our demonstration system of a simulated search task involving two robots and a human teammate illustrates the potential for HARMONIC to be applied to real-world applications. Future research will focus on expanding the range of tasks and scenarios in which HARMONIC agents can successfully operate; enhancing the robots' learning capabilities; conducting real-world trials using physical robots to validate the framework's effectiveness in diverse operational contexts; developing collaborative online planning strategies; and, enhancing real-time adaptability in dynamic environments."}]}