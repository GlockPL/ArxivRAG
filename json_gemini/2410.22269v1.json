{"title": "Fourier Head:\nHelping Large Language Models\nLearn Complex Probability Distributions", "authors": ["Nate Gillman", "Daksh Aggarwal", "Michael Freeman", "Saurabh Singh", "Chen Sun"], "abstract": "As the quality of large language models has improved, there has been increased interest in\nusing them to model non-linguistic tokens. For example, the Decision Transformer recasts\nagentic decision making as a sequence modeling problem, using a decoder-only LLM to\nmodel the distribution over the discrete action space for an Atari agent. However, when\nadapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins\ncaptures the continuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer, constructed\nusing Fourier series, which we can easily substitute for any linear layer if we want the\noutputs to have a more continuous structure. We perform extensive analysis on synthetic\ndatasets, as well as on large-scale decision making and time series forecasting tasks. We also\nprovide theoretical evidence that this layer can better learn signal from data while ignoring\nhigh-frequency noise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural continuous structure.\nFor example, the Fourier head improves a Decision Transformer agent's returns by 46% on\nthe Atari Seaquest game, and increases a state-of-the-art times series foundation model's\nforecasting performance by 3.5% across 20 benchmarks unseen during training. We release\nour implementation at https://nategillman.com/fourier-head.", "sections": [{"title": "1 Introduction", "content": "Human language can be viewed as a discretization for a continuous, often probabilistic representation of the\nworld that is construed in our mind (Spivey, 2008). The continuous structure can be partially captured\nby language models with their token embeddings, where \"nearby\" tokens are embedded to have latent\nrepresentations with high cosine similarities. The embeddings themselves are acquired as a result of the\ndata-driven learning process. Can we, based on rich prior knowledge about the continuous world, inform\nthe language model about the underlying continuity of its inputs, like the fact that the word \"emerald\" is\nmore similar to \"shamrock\" than \"pine\" when they are used to describe different shades of green? As large\nlanguage models (LLMs) have evolved into \u201cfoundation models\" that are adapted to a diverse range of tasks,\ntokens that are a priori continuous are more essential than ever, for example for arithmetic computations (Liu\net al., 2023), decision making with continuous or discrete actions (Chen et al., 2021), future anticipation\nand time-series forecasting (Ansari et al., 2024), or simply drawing random numbers given a probability\ndistribution (Hopkins et al., 2023).\nWe view the problem of informing LLMs to utilize the continuity prior from the perspective of probability\ndensity estimation. For simplicity, we adopt the standard next token prediction framework whose training\nobjective is softmax cross entropy. Assuming non-overlapping vocabulary, continuous values can be dis-\ncretized via binning (Ansari et al., 2024). On one hand, the linear head adopted by LLMs independently\nprojects each token into probabilities, and has the expressive power to flexibly approximate arbitrary proba-\nbility density functions subject to the \"quantization\" errors. The linear head however does not consider any\ncontinuous structure that resides among the tokens (i.e. a random re-shuffle of the tokens in the vocabulary\nwould not change the predictions). On the other hand, a head based on a parameterized distribution (e.g.\nGaussian or Gaussian Mixtures) naturally incorporates the continuous structure, but is often too simple (and\noverly \u201csmooth\") to account for multi-modal distributions for future prediction or decision making. Can we\ndesign a head that is both expressive and incorporates continuous structures?\nWe introduce the Fourier head, motivated by Fourier series as universal function approximators. The\nFourier head learns a continuous probability density function, and returns a discrete ap-\nproximation of it. Intuitively, returning a discretization of a continuous density in this way allows the\nclassification head to better model the low-frequency signals from the training data, because overfitting to\nhigh-frequency noise is explicitly penalized by the Fourier head's built-in regularization. At a high level, the\nFourier head inputs $x \\in R^n$, uses a linear layer to learn the coefficients for a Fourier series with N frequencies\nover [-1, 1], and quantizes the interval [-1, 1] into m equal bins. Then, the Fourier head evaluates the learned\nFourier PDF at those m bin center points, and returns those m likelihoods as a categorical distribution.\nOur main contributions are as follows.\n1.  First, we reveal the underlying principle on the trade-off between the Fourier head's expressive power\nand the \"smoothness\" of the predicted distributions. We have proven a theorem which demonstrates\na scaling law for the Fourier head. Namely, as we increase the quantity of Fourier coefficients that the\nFourier head learns, the layer is able to model increasingly more complicated distributions; however,\nthe Fourier head will necessarily fit to more high-frequency noise, thereby outputting categorical\ndistributions which are less smooth.\n2.  Second, we propose a practical implementation of the Fourier head that allows us to handle sequen-\ntial prediction tasks by modeling complex multi-modal distributions. Alongside our implementation,\nwe propose strategies to improve the layer's performance, including Fourier coefficient norm regu-\nlarization, weight initialization, and the choice of how many Fourier frequencies to use.\nWe demonstrate the effectiveness of the Fourier head on two large scale tasks, where intuitively a continuity\ninductive bias over the output dimensions ought to help the model's generation performance. In the first\ntask, an offline RL agent which uses a decoder-only transformer to model the next-action distribution for\nan Atari game, we improve returns by 46%. And in the second, we outperform a state-of-the-art time series\nfoundation model on zero-shot forecasting by 3.5% across a benchmark of 20 datasets unseen during training."}, {"title": "2 Fourier Head", "content": "When practitioners apply LLMs to model complex probability distributions over non-linguistic tokens, a\nstandard technique is to quantize the latent space into m tokens and learn a conditional categorical distri-\nbution over those tokens. We share two examples here:\n\u2022   The Decision Transformer (Chen et al., 2021) models an Atari agent's behavior in the Seaquest game\nby learning a categorical distribution over the 18 possible actions (move left, move right, shoot left,\netc.). They use an decoder-only transformer architecture.\n\u2022   The Chronos time series foundation model (Ansari et al., 2024) models the distribution of next\nnumerical values by quantizing the closed interval [\u221215, 15] into 4096 bins, and learning a categorical\ndistribution over those bins. They use an encoder-decoder transformer.\nIn a pure language modeling task, token ID 1000 and token ID 1001 likely represent unrelated words.\nHowever, in a task where the token IDs represent numerical values, the token ID 1000 and 1001 would\nrepresent numbers that are close together.\nThe final layers of an LLM for such a task are generally a linear layer, followed by softmax, followed by\ncross entropy loss. We hypothesize that in scenarios where nearby token IDs encode similar items, an\ninductive bias that encourages them to have similar probabilities will improve performance. A generic linear\nlayer learns an unstructured categorical distribution and thereby allows more arbitrary probabilities. In\nthis work, we propose to give the model this inductive bias by letting the classification head\nlearn a categorical distribution as the discretization of a continuous learned function from a\nsuitably flexible class. In this paper, we consider the very flexible class of truncated Fourier series with\nN frequencies. These are functions of the form\n$f(x) = c_0 + \\sum_{k=1}^N c_k \\exp(ik\\pi x)$.\nFourier series are a classical tool for solving quantitative problems (Stein & Shakarchi, 2003) because func-\ntions like Equation 2.1 are universal function approximators, with the approximation improving as N in-\ncreases."}, {"title": "2.2 Fourier Head: Definition", "content": "We now propose a replacement for the generic linear layer token classification head, built using Fourier\nseries. We call our replacement the Fourier Series Classification Head, or the Fourier head for short.\nThe Fourier head inputs any vector $x \\in R^n$, and outputs a categorical distribution in $R^m$. For a high level\nsummary of how it works-the Fourier head inputs $x \\in R^m$, uses a linear layer to extract the coefficients for\na Fourier series over [-1,1], quantizes the interval [-1,1] into m equal bins, evaluates the learned Fourier\nPDF at those m bin centerpoints, and returns those m likelihoods as a categorical distribution. We formally\ndefine this layer in Algorithm 1, and we present a concrete low-dimensional demonstration of the Fourier\nhead in action in Section 2.3."}, {"title": "2.3 Fourier Head: Motivating Example", "content": "To illustrate a simple problem setting where the design of the Fourier head is appropriate, we use it as\na drop-in replacement for a linear classification head in the Audio Spectrogram Transformer (Gong et al.,\n2021). We consider the task of beats per minute (BPM) classification for metronome-like audio samples\n(Wei et al., 2024) within the tempo range {50,51,...,210}. While this task is not difficult, we use this audio\nclassification task to illustrate some of the design choices one can make when using the Fourier head. In this\ncase, it is natural to group the BPMs into contiguous bins {[50,54], [55, 59], ...} and use the Fourier head"}, {"title": "2.4 Fourier Head: Considerations for Using it During Training", "content": "We highlight the main design choices for a user when applying the Fourier head in practice.\nTraining objective: The Fourier head inputs a signal $x \\in R^n$ and extracts from that signal an intermediate\nrepresentation of a probability distribution $p_x(z)$ defined over $z \\in [-1,1]$. This probability distribution has a\nclosed formula equal to a Fourier series. In our experiments, we optimize the parameters of the Fourier PDF\nby discretizing it over the latent space and training using cross entropy loss. However, we should note that\nthe Fourier layer allows MLE training directly on continuous values, by evaluating the Fourier PDF directly"}, {"title": "Choice of hyperparameter N", "content": "The Fourier head has one crucial hyperparameter-namely, the number\nof frequencies. How should one choose this in practice? We offer Theorem 3.3 as guiding principle beyond\nsimple trial and error. This result provides a scaling law which formalizes the smoothness-expressive power\ntrade-off in choosing the number of frequencies. In general, using more frequencies leads to more expressive\npower, and generally better success metrics, but at the cost of a learning less smooth densities, as well as\nmore model parameters."}, {"title": "Fourier regularization", "content": "A generic Fourier series such as Equation 2.1 has Fourier coefficients which decay\nquickly enough for the infinite series to converge absolutely. For example, for the class of Fourier series\nwhich have continuous second derivatives, the Fourier coefficients decay on the order of $1/k^2$. To impose\nthis regularity assumption on the learned Fourier densities, we follow (De la Fuente et al., 2024) and add\na regularization term to the loss to prevent higher order Fourier coefficients from growing too large during\ntraining. This helps ensure that the learned Fourier PDF doesn't overfit to noise in the data. In the notation\nfrom Algorithm 1, this means adding a regularization term of $\\gamma \\cdot 2\\pi^2 \\sum_{k=1}^N k^2|c_k|^2$ to the loss function, where\n$\\gamma$ is a hyperparameter. We find that in the low-frequency domain, using $\\gamma = 0$ can give better performance\nthan $\\gamma > 0$; and in the high frequency domain, $\\gamma = 10^{-6}$ works well."}, {"title": "Binning strategy", "content": "The choice of how we bin the data can affect performance significantly. As we already\ndiscussed, we should only apply the Fourier head when nearby bins are \"similar\" in some sense. This means\nwe should order our bins in a semantically meaningful ordering. Further, in the case where the bins represent\nquantized numerical values over a continuous latent space, it can be helpful to use a \"mixed-precision\" binning\nstrategy. For instance, if we want to model all values from [-15, 15], but we find that most values lie in the\nrange [-1,10], then we should allocate a higher proportion of bins to the dense data interval. Specifically,\nif we would like to usem total bins to quantize the data, then we control the allocation of bins using a\nhyperparameter d\u2208 [0,1), where [d.m] uniformly spaced bins are allocated to the sparse data interval\nwhile the remaining m [d.m] bins are allocated to the dense range (estimated from training data). This\nis motivated and supported by the Fourier theory as well, since by increasing precision in the dense data\nrange we are effectively de-localizing the quantized data distribution, which leads to a more localized Fourier\nspectrum. This lets us obtain a quicker decay of higher frequency content, which ensures that we can more\neffectively learn the same distribution with lower-frequency Fourier heads."}, {"title": "Weight initialization", "content": "The learned parameters for the Fourier head consist of the learned linear layer\nwhich extracts autocorrelation parameters. In PyTorch, the linear layers uses the He initialization (He et al.,\n2015) by default, which ensures that the linear layer outputs values close to zero in expectation. Similarly,\nit's better for the learning dynamics for the Fourier densities to be initialized to uniform p(z) \u2248 1/2. We\naccomplish this by dividing the weights and biases by a large number, such as 1000, after He initialization;\nthis guarantees that the linear layer outputs very small values, so that Fourier coefficients output from the\nautocorrelation step are very small as well."}, {"title": "3 Theory", "content": ""}, {"title": "3.1 \"Smoothness\": A Metric for High Frequency Content", "content": "In this section we propose a smoothness metric which inputs a categorical distribution $y = (y_1, ..., y_m) \\in R^m$,\nand assigns a numerical value depending on how smooth it is. The score will output 0 if y is the smoothest\npossible categorical distribution, and larger values if y is less smooth. We will first specify what we mean by\n\"smooth\":\nHeuristic 3.1. We say a function is smooth if it contains very little high-frequency information.\nFor example, the uniform categorical distribution contains no high-frequency information, so it is the\nsmoothest possible function, and should get a smoothness score of 0. In contrast, a categorical distribu-\ntion containing samples from sin(100\u03c0\u03b1) contains lots of high frequency information, so it should get a"}, {"title": "3.2 A Scaling Law for the Fourier Head, in Frequency-aspect", "content": "In this subsection, we share a theorem that analyzes the quality of the Fourier head as the quantity of\nfrequencies changes. We refer to this as the Fourier head scaling law as it quantifies the trade-off\nbetween modeling capacity and smoothness as the number of frequencies increases. On one hand, it is a\ncelebrated result from Fourier analysis that a Fourier series with a greater number of frequencies models\na larger class of functions; but on the other hand, we show that increasing frequencies also incurs loss in\nsmoothness. This is to be expected, as we designed our smoothness metric with the intention of identifying\na distribution as less smooth if it contains more high-frequency information.\nTheorem 3.3. (Fourier head scaling law.) Consider a Fourier head with input dimension n, output dimen-\nsion m, and N frequencies. Suppose that 1 < N < $\\frac{m}{2}$. Then the following are true:\n1.  (Increasing N improves modeling power.) As N increases, the Fourier head is capable of\nlearning a larger class of densities.\n2.  (Increasing N degrades smoothness.) Consider an input to the Fourier head $x \\in R^n$, and\ndenote by $f_x$: [-1,1] \u2192 R the optimal conditional distribution that we would like the Fourier head\nto approximate for this input. Suppose that there exists some t \u2265 2 such that the Fourier coefficients\nof $f_x$ decay on the order of $1/k^t$. Denote by $f_{x,N}$ the truncation of $f_x$ to its first N frequencies,\ndenote by $b \\in R^m$ the m bin centerpoints in [-1,1], and denote by $y(N) = f_{x,N}(b)/(f_{x,N}(b_0) +\u2026\u2026+\nf_{x,N}(b_{m-1})) \\in R^m$ the discretization of $f_{x,N}$ into m bins. Then, there exist constants $C_1, C_2 > 0$\nsuch that\n$s(y(N)) = C_1 - \\frac{C_2}{N^{2t-1}} + O(1/N^{2t})$."}, {"title": "4 Toy Example: Learning A Continuous Conditional Distribution", "content": "We demonstrate the advantage of using the Fourier head to learn a probability distribution for a simple task:\nlearning the conditional distribution of the third number in the sequence given the first two. Here we will\nuse q(z) to denote the quantization of z.\nDataset: We create 3 synthetic datasets, which we name Gaussian, GMM, and GMM-2. Each dataset\nconsists of 5000 quantized triples {(q(x), q(y), q(z))} \u2286 [ \u2212 1, 1]\u00b3. Crucially, z is sampled from a distribution\nwhich is conditioned on x and y, and we have an explicit closed formula for this distribution. By design,\nthe Gaussian dataset is unimodal in z, whereas the more challenging GMM and GMM-2 datasets are not\nunimodal. Full details about the datasets can be found in Appendix C.\nTask: Predict the conditional distribution of q(z) given the quantized tuple (q(x), q(y)).\nModel architecture: Our model is an MLP with ReLU activations and one hidden layer, which maps\n$R^2 \u2192 R^{64} \u2192 R^{32} \u2192 R^{50}$. The output of the model has dimension 50 because we quantize the interval\n[-1,1] into 50 bins. For the baseline, the classification head is a linear layer followed by a softmax; for the\nFourier model, the classification head is the Fourier head. We sweep over frequencies N = 2,4,..., 20, and\nwe consider regularization \u03b3\u2208 {0,10-6}. We train all models using cross entropy loss.\nModel evaluation: We use three metrics for evaluation. Let $P(x, y)$ denote the fixed conditional distribu-\ntion of z given (x, y). Our first metric is the average KL divergence $D_{KL}(q(P_{2(x,y)})||M(q(x), q(y)))$, where\n$M(q(x), q(y))$ denotes the predicted categorical conditional distribution of q(z), and $q(P(x, y))$ is the quan-\ntized approximation of $P(x,y)$, where $P(x, y)$ is the fixed conditional distribution, obtained by evaluating\nthe density function of $P(x, y)$ at the bin centers, multiplying by the bin width, and finally scaling by the\nsum of the likelihoods. Our second metric is MSE. To compute this, we use the point of maximum likelihood\nunder the learned categorical distribution as a prediction for q(z) and compute the difference between the\nprediction and true value in the test set. And our third metric is smoothness."}, {"title": "2.4 Fourier Head: Considerations for Using it During Training", "content": "We highlight the main design choices for a user when applying the Fourier head in practice.\nTraining objective: The Fourier head inputs a signal $x \\in R^n$ and extracts from that signal an intermediate\nrepresentation of a probability distribution $p_x(z)$ defined over $z \\in [-1,1]$. This probability distribution has a\nclosed formula equal to a Fourier series. In our experiments, we optimize the parameters of the Fourier PDF\nby discretizing it over the latent space and training using cross entropy loss. However, we should note that\nthe Fourier layer allows MLE training directly on continuous values, by evaluating the Fourier PDF directly"}, {"title": "5 Large-Scale Study: Offline Reinforcement Learning", "content": "The Decision Transformer (Chen et al., 2021) casts the problem of reinforcement learning as sequentially\nmodeling rewards, states, and actions. Here, we study the performance of the Decision Transformer on the\nSeaquest game in the Atari (Bellemare et al., 2013) benchmark. The Seaquest game contains 18 actions, with\ntwo groups of eight actions that have a natural \u201ccloseness\" metric defined on them: move left, up left, up, up\nright, right, down right, down, down left; as well as shooting in those eight directions. In their architecture,\na decoder-only language model (Radford et al., 2018) encodes the context and then maps it through a linear\nlayer, outputting a categorical distribution over the 18 possible actions. In our study, we replace that linear\nclassification head with a Fourier head. Intuitively, this ought to give the model the prior that actions like\n\"move left\" and \"move up left\" are semantically similar, and therefore should have similar likelihoods. Our\nstudy confirms that the Fourier head outperforms the linear head in returns obtained by as much as 46%,\nin the reward conditioned setting considered in the paper, using identical training hyperparameters.\nDataset: We use the same dataset from the original Decision Transformer implementation (Chen et al.,\n2021). This dataset consists of 500k transitions experienced by an online deep Q-network agent (Mnih et al.,\n2015) during training on the Seaquest game.\nTask: In the Seaquest game, the agent moves a submarine to avoid enemies, shoot at enemies, and rescue\ndivers. The Seaquest game contains 18 actions: move left, up left, up, up right, right, down right, down,\ndown left; as well as shooting in those eight directions; as well as no move, and a generic fire move. We\nconsider this task in the Offline RL setting. The agent observes the past states, actions, and rewards, as\nwell as the return-to-go, and attempts to predict the action that matches what an agent operating like the\ndataset would likely do.\nModel architecture: (Chen et al., 2021) used the GPT-1 model (Radford et al., 2018) to autoregressively\nencode the context, which is then fed through a linear layer of dimension 18, and the model ultimately\noptimizes the cross entropy loss between the action logits and the ground truth action from the dataset. We\nrefer to this model as the linear baseline. To create our Fourier-N version, we simply replace the linear head"}, {"title": "6 Large-Scale Study: Probabilistic Time Series Forecasting", "content": "The Chronos time series foundation models (Ansari et al., 2024) \u201clearn the language of time series\". They do\nthis by approaching time series forecasting as language modeling, by tokenizing the quantized number line,\nlearning token embeddings for each of those quantized values, and finally learning a categorical distribution\nto decide what the next value ought to be. This model is built on top of the encoder-decoder T5 model (Raffel\net al., 2020). In particular, this model normalizes time series values to the range [-15,15] and quantizes\nthis interval into 4096 tokens. As usual for language modeling, the final layer is a linear map which learns\na categorical distribution over next tokens. In particular, we observe that token i represents a number very\nclose to tokens i - 1 and i+1. However, we note that there is no inductive bias in the T5 architecture which\npushes their likelihoods to be similar. This is not a hypothetical problem; in Figure 9 (Appendix), we can\nsee that the linear next-token prediction PMFs fit to the noise, and appear very jagged.\nThe motivation for replacing the linear head with the Fourier head is to \"smooth\" out the\ndistribution in the left side of Figure 9, to help the forecasting model better learn the signal,\nand ignore the noise. In this figure, we can see that the Fourier head accomplishes this successfully.\nIn this section, we study how the performance of the Chronos time series foundation model changes when\nwe pre-train using the Fourier head, instead of the linear head. For all of the frequencies that we consider,\nthe Fourier head outperforms the Chronos linear baseline on the MASE metric, while learning next token\nmultinomials which are at least 8x smoother, with fewer parameters than the baseline.\nDataset: We use the same training dataset for large-scale pretraining that Ansari et al. (2024) used. We\ngather an evaluation benchmark of 20 time series datasets which were not seen during training. These 20\ncome from the zero-shot eval from (Ansari et al., 2024). The reader can check Appendix D for details on\nthe training and evaluation datasets we used.\nModel architecture: We use the Chronos model, which is built using the T5 architecture (Raffel et al.,\n2020). The original model has a linear classification head. For our study, we will replace this with a Fourier\nhead with frequencies N = 64, 128, 256, 550. We use mixed precision binning; this is informed by an analysis\nof the Fourier spectrum of the next-token distribution, as described in Section 2.4. We also use Fourier\nquadratic weight decay regularization with y = 10-6. For the task, the model learns to input time series\ncontext of length 512, and output a probabilistic forecast of length 64."}, {"title": "7 Related Work", "content": "LLMs outside of natural language domains: LLMs are often adapted to domains beyond natural\nlanguage, as general purpose sequence models. For example, they have been used in protein synthesis\n(Madani et al., 2023), time series forecasting (Ansari et al., 2024; Das et al., 2024; Jin et al., 2024), music\ngeneration (Dhariwal et al., 2020; Agostinelli et al., 2023; Copet et al., 2023; Yuan et al., 2024), and as well\nas in decision making (Li et al., 2022; Chen et al., 2021).\nWe consider three categories to adapt LLMs to non-language domains: when the output of a language-\ntrained LLM is used as a feature for some out-of-domain task; when a language-pretrained LLM is fine-\ntuned on a domain-specific task; and when an LLM architecture is trained on a domain-specific dataset from\nscratch. Our work directly considers the latter method of LLM adaptation, particularly in settings where the\noutputs approximate continuous values. We note that using LLMs to model numerical functions has seen\nsuccess in continuing sequences (Mirchandani et al., 2023) but has been challenging for modeling samplers for\nprobability distributions (Hopkins et al., 2023). In a related direction, Razeghi et al. (2022) found that model\nperformance on numerical reasoning tasks is correlated with the frequency of specific numbers in its corpus.\nFurther, some have re-framed continuous regression as a descretized classification problem to leverage LLMs\nin numerical modeling contexts (Song et al., 2024). While even frozen LLMs with no further training show\ninteresting empirical results as regressors (Vacareanu et al., 2024), there is a conceptual mismatch between\nthe downstream task and model construction because tokenized numerical values trained using cross-entropy\nloss does not explicitly enforce numerical relationships between the tokens.\nFourier series in neural networks: Many works leverage the Fourier transform as a data pre-processing\nstep or a deterministic transformation within the network, or use Fourier analysis to motivate design choices.\nIt is far less common to learn the Fourier series directly. De la Fuente et al. (2024) learned marginal\nunivariate densities parameterized using a Fourier basis; our work extends their Fourier Basis Density model\nto multivariate settings with an autoregressive scheme. Our method learns conditional univariate densities\nusing a Fourier basis, where the coefficients of the Fourier density model are input dependent. Sitzmann et al.\n(2020) proposed sinusoidal activation functions, which can be seen as learning the frequencies of a Fourier\nseries; in contrast, we fix the frequencies to the canonoical choice {1,2,..., N}, and learn the amplitudes.\nThis allows the Fourier head to directly benefit from approximation results from Fourier analysis."}, {"title": "8 Conclusion", "content": "We propose the Fourier head and demonstrate its positive impact on performance on several tasks. We\nprove a scaling law that characterizes the trade-off between the model's expressivity and the smoothness\nof its output distribution. The Fourier head is a modular architecture that can be easily added to existing\nmodels that would benefit from the continuity inductive bias that the head imparts. The Fourier head extends\nthe already extensive reach of LLMs into more diverse, numerical, and probabilistic domains. Future work\nincludes exploring alternative training objectives that do not depend on discretizing probability density\nfunctions, and incorporating the Fourier head in general-purpose LLM training, where the head can be\nadaptively employed when needed."}, {"title": "A Proof of Fourier Head Scaling Law, Theorem 3.3", "content": "In this section we prove Theorem 3.3, the Fourier head scaling law. To do this, we must first discuss the\nNyquist-Shannon Sampling Theorem. This result states that in order to avoid distortion of a signal (such as\naliasing) the sampling rate must be at least twice the bandwidth of the signal. In the setting of the Fourier\nhead, our sampling rate is m/2 because we have m bins uniformly spaced in (\u22121,1), and the bandwidth is\nN/2 because the frequency of sin(\u03c0\u039dx) is N/2. Thus the Nyquist Theorem requires us to have\n$m/2\u2265 2 (N/2) = N$\nin order for the higher order frequency content learned by our model to not be fallacious when we are learning\nfrom only m bins. This justifies why we only theoretically study the case 1 < N < m/2 in the scaling law."}, {"title": "A.1 Definitions", "content": "Consider an input x \u2208 Rn to the Fourier head, and denote by fx: [-1,1] \u2192 R the optimal conditional\ndistribution that we would like the Fourier head to approximate for this input. We will assume that fx is\nperiodic, since the Fourier head learns a 2-periodic Fourier density. We denote by fx, N the truncation of\nthe Fourier series of fr to its first N frequencies. Note that fx, N also integrates to 1 over [-1,1] since its\nfirst Fourier coefficient is the same as that of fr. Further, fx,n is non-negative on [-1,1] since its Fourier\ncoefficients, being a subsequence of the coefficients of fx, are non-negative definite; a periodic function with\nnon-negative definite Fourier coefficients is non-negative by Herglotz's Theorem (Brockwell & Davis, 1991,\nCorollary 4.3.2). For completeness, we will recall the convolution formulas, specialized to the cases we\nconsider in our argument.\nDefinition A.1 (Discrete convolution). Let bj := \u22121+2i+1,0 < j < m be the center points of the m bins in\n(-1,1), and let us denote b := (bo, ..., bm\u22121). Denote by $G_\\sigma(z) := \\frac{e^{-z^2/2\\sigma^2}}{\\sqrt{2\\pi \\sigma}}$ the Gaussian PDF with standard\ndeviation \u03c3. Then the discrete Gaussian convolution filter of radius m-1 is\n$g_\\sigma := \\frac{G_\\sigma([1 - m, 2 - m, 3 \u2013 m, . . ., m \u2013 1])}{S(m, \\sigma)}  \\in R^{2m-1}$,\nwhere the normalization constant is\n$S(m,\\sigma) := \\sum_{k=1-m}^{m-1} G_\\sigma(k)$.\nThe discrete convolution of $g_\\sigma \\in R^{2m-1}$ and $f_{x,N}(b) \\in R^m$ is the vector $(g_\\sigma * f_{x,N})(b) \\in R^m$ whose j'th\ncoordinate is given by\n$(g_\\sigma* f_{x,N})(b_j) = \\frac{1}{S(m, \\sigma)} \\sum_{k=1-m}^{m-1} G_\\sigma(k) f_{x,N}(b_{j-k})$.\nDefinition A.2 (Continuous convolution). The continuous Gaussian convolution filter $\\tilde{g}_\\sigma : [ -2,2] \u2192 R_{>0}$\nis\n$\\tilde{g}_\\sigma(z) = \\frac{G_{\\frac{2\\sigma}{m}} (\\frac{mz}{2})}{S(m, \\sigma)} \\frac{m}{2S(m, \\sigma)} = G_\\sigma (\\frac{mz}{2})$"}, {"title": "A.2 Overview of proof", "content": "In this subsection", "sigma)": "n1 + \\frac{G_\\sigma(m)}{S"}]}