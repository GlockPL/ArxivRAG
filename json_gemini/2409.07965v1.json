{"title": "Autonomous Vehicle Controllers\nFrom End-to-End Differentiable Simulation", "authors": ["Asen Nachkov", "Danda Pani Paudel", "Luc Van Goo11,2"], "abstract": "Current methods to learn controllers for au-\ntonomous vehicles (AVs) focus on behavioural cloning. Being\ntrained only on exact historic data, the resulting agents often\ngeneralize poorly to novel scenarios. Simulators provide the\nopportunity to go beyond offline datasets, but they are still\ntreated as complicated black boxes, only used to update the\nglobal simulation state. As a result, these RL algorithms are\nslow, sample-inefficient, and prior-agnostic. In this work, we\nleverage a differentiable simulator and design an analytic\npolicy gradients (APG) approach to training AV controllers\non the large-scale Waymo Open Motion Dataset. Our proposed\nframework brings the differentiable simulator into an end-to-end training loop, where gradients of the environment dynamics\nserve as a useful prior to help the agent learn a more grounded\npolicy. We combine this setup with a recurrent architecture\nthat can efficiently propagate temporal information across\nlong simulated trajectories. This APG method allows us to\nlearn robust, accurate, and fast policies, while only requiring\nwidely-available expert trajectories, instead of scarce expert\nactions. We compare to behavioural cloning and find significant\nimprovements in performance and robustness to noise in the\ndynamics, as well as overall more intuitive human-like handling.", "sections": [{"title": "I. INTRODUCTION", "content": "When training AV controllers, it is common to treat\nthe environment as a black-box function that is only used\nto evolve the states and provide an interactive element to\nthe data collection process. Different RL algorithms adopt\ndifferent strategies for handling it.\nBy using replay buffers to store past transitions, value-\nbased [20], model-based [30], or policy gradient [32] meth-\nods are oblivious to how the next state has been generated.\nThey treat it as either a target or some state from which\nto bootstrap a value estimate. This approach is general, but\nmakes training slow and sample-inefficient, since the world\ndynamics and a behavioural policy have to be learned from\nthe data. Imitation learning [5], being essentially supervised\nlearning on expert actions, is more sample-efficient but\nsuffers from generalization difficulties and avoids learning\nvirtually any kind of dynamics information.\nIf the environment dynamics are known one would likely\nbe able to harvest the best aspects from each of these\nmethods. In fact, in a differentiable environment one can\noptimize the policy directly using gradient descent, just via\nsupervision from expert agent trajectories, as shown in Fig. 1.\nThe benefits of this are: 1) Obtaining an explicit policy\nfor continuous control, 2) Fast inference, since there is no"}, {"title": "II. RELATED WORK", "content": "RL methods that consider the environment as a black box,\nhave different ways to learn a policy without the gradients of\nthe dynamics, as shown in Table I. Because of that, most of\nthem do not incorporate any world priors that are otherwise\ncontained in the simulator. Compared to them, the analytic\npolicy gradients approach (APG) is stable, sample efficient,\nand fast at test time. It has been used for trajectory tracking in\ndrones [36], and is increasingly being used in differentiable\nphysics simulators for robotics [8], [9], [12], [22], [18]."}, {"title": "III. METHOD", "content": "In essence, a differentiable environment allows us to\nbackpropagate gradients through it and directly optimize the\npolicy. The resulting method broadly falls into the analytic\npolicy gradients (APG) type of algorithms. In our setting we\nassume we are given expert trajectories {$\u2081}7_1, instead of\nrewards. The goal is to train the policy \u03c0\u03b8 so that upon a\nroll-out, it reproduces the expert trajectory:\n$\\min \\mathcal{L} = \\sum_{t=1}^{T} \\frac{1}{2} || \\hat{s}_t - s_t ||_2^2,$ (1)\nwhere s\u2081 = Env (St-1, at-1) and at\u22121 ~ \u03c0\u03b8 (St\u22121)\nHere \u015dt and \u00e2 refer to ground truth states and actions while\nst and at are the corresponding simulated states and actions.\nThe sequence of states {st}11 forms a trajectory."}, {"title": "A. Differentiating through Waymax", "content": "Obtaining gradients. We apply our APG method in the\nWaymax simulator [10]. Being implemented in Jax [3], it is\nrelatively flexible in choosing which variable to differentiate\nand with respect to what. That being said, many of the\nobtainable derivatives are not meaningful. For example,\n1) The derivatives of future agent locations with respect\nto current traffic light states or roadgraphs are all zero,\nbecause the simulator dynamics (e.g., bicycle or delta\ndynamics) do not depend on the roadgraph or the traffic\nlights. There are no rigid-body collisions.\n2) Certain metrics such as collision or offroad detection\nare boolean in nature. Other objects such as traffic\nlights have discrete states. While useful for training,\nthese are problematic for differentiation.\nWhat is meaningful and useful is to take the gradients of\nfuture controllable agent locations with respect to their cur-\nrent actions, $\\frac{\\partial s_t}{\\partial a_{t-1}}$. These are precisely the derivatives\nneeded for objective (1) and hence we focus on them. We\ntreat the dynamics as another function in the computation\ngraph, while ensuring that the relevant code structures for\ntraining are registered in Jax's pytree registry, so that any\ntracing, just-in-time compilation, or functional transforma-\ntions like grad can be applied on them.\nMoreover, we found it useful to adapt the bicycle dynamics\nmodel to be gradient-friendly. This includes adding a small\nepsilon to the argument of a square root to avoid the case\nwhen its input is 0, as well as adapting the yaw angle\nwrapping, present in many similar settings, to use arctan2"}, {"title": "B. End-to-end training with the simulator", "content": "Dense trajectory supervision. Obtaining the gradients of\nthe environment dynamics opens up technical questions of\nhow to train in such a setup. One can supervise the rolled-\nout trajectory only at the final state and let the gradients\nflow all the way back to the initial state. Since this does not\nsupervise the path taken to the objective, in our experiments\nwe densely supervise all states in the collected trajectory\nwith all states in the GT trajectory.\nGradient detaching. Dense supervision allows us to de-\ntach the gradients at the right places, as shown in the third\npart of Fig. 2. Here, when we obtain st, we calculate the\nloss and backpropagate through the environment dynamics\nobtaining $\\frac{\\partial s_t}{\\partial a_{t-1}}$ without continuing on to previous steps.\nThis makes training slower, since gradients for the earlier\nsteps do not accumulate, as in a RNN, but effectively cuts\nthe whole trajectory into many (st, at, St+1) transitions which\ncan be treated independently by the optimization step. This\nallows for off-policy training a key aspect of our setup."}, {"title": "C. Model overview", "content": "We present our model setup in Figure 3. For training,\nwe roll-out a full trajectory and supervise with the GT one.\nThe gradients flow back through the differentiable dynamics,\npolicy and scene encoder, and continue back to the previous\nscene contexts using a RNN hidden state. We detach the\nprevious simulator state for both necessity and flexibility so"}, {"title": "IV. EXPERIMENTS", "content": "An early example. To demonstrate the characteristics of\nAPG, we first compare APG and behaviour cloning (BC) on\na toy task purposefully overfitting a single trajectory for\na single agent. We are given a full-length ground-truth (GT)\ntrajectory from the WOM dataset [7], {$\u2081}}=1 \u2208 RT\u00d72, and\nare learning a controller ne of the form \u03c0\u03bf : (st,t) \u2192 at.\nBehaviour cloning baseline. Since expert actions are\navailable only on the GT trajectory, behaviour cloning re-\nplays it and is reduced to supervised learning on them:\n$\\min_{\\theta} \\sum_{t=1}^{T} || \\hat{a}_t - a_t ||_2^2$, where at = \u03c0\u03b8 (\u015dt\u22121). In that setup\nat test time, when we perform a roll-out, behaviour cloning\nsuffers from error-compounding and is unable to reproduce\nthe GT trajectory, even though it has learned the on-trajectory\nactions very accurately. Very small imprecisions in the ac-\ntions start to accumulate, as shown in the left part of Fig. 5.\nAPG. If we have access to a simulator we can perform\nroll-outs at train time and supervise the resulting trajectories\n{$t}=1 with {$t}=1. This is sufficient to make the controller\nrobust and allows it to reproduce the trajectory at test time\non its own. If the policy is stochastic, the learning process\nbecomes less sample-efficient because it becomes exponen-\ntially less likely that in a far away t, a given location (x,y)\nis reached. Thus, as shown in Figure 5 converging to the\nGT trajectory does not happen uniformly from all directions,\nbut rather sequentially. To improve sample-efficiency, as a\nform of curriculum, we snap back the simulated trajectory\nwhenever it goes beyond a threshold with respect to\n{$t}71, as in Fig 4a. Since resetting the simulated trajectory"}, {"title": "A. Large-scale experiments in Waymax", "content": "Experiment setup. The Waymax simulator uses the un-\nderlying Waymo Open Motion Dataset (WOMD) [7] to ini-\ntialize the simulator state. It contains close to 500K training\nscenarios, each of length 9 seconds and a framerate of 10\nHz. The history length is 10 frames (1 second) and the goal\nis to predict the next 80 frames. To train the model, we run\nmultiple simulations, one per scenario, in parallel (this being\nthe simulated minibatch) and use Adam to optimize them.\nWe iterate through all scenarios (the equivalent of an epoch)\nmultiple times. At test time, we simulate multiple roll-outs\nand compute the average displacement error (ADE), offroad\nrate, and collision rate for each mode. A collision rate of 20%\nmeans that 20% of all trajectories have some objects colliding\nat some timestep in them. Since the future is uncertain and\nwe want to make sure that the GT locations are covered by\nat least one of our modes, we report the eval metrics from\nthe best mode. Thus, we evaluate on minADE, min offroad\nrate, and min collision rate.\nResults. Our main results are shown in Table II. We\ncompare behavioural cloning (BC) with APG on two tasks"}]}