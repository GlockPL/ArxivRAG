{"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "authors": ["Yajing Wang", "Zongwei Luo", "Jingzhe Wang", "Zhanke Zhou", "Yongqiang Chen", "Bo Han"], "abstract": "Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes enhancing LLMs' reasoning performance by eliciting their causal inference ability from prompting instructions to correct answers. Specifically, we introduce the Self-Causal Instruction Enhancement (SCIE) method, which enables LLMs to generate high-quality, low-quantity observational data, then estimates the causal effect based on these data, and ultimately generates instructions with the optimized causal effect. In SCIE, the instructions are treated as the treatment, and textual features are used to process natural language, establishing causal relationships through treatments between instructions and downstream tasks. Additionally, we propose applying Object-Relational (OR) principles, where the uncovered causal relationships are treated as the inheritable class across task objects, ensuring low-cost reusability. Extensive experiments demonstrate that our method effectively generates instructions that enhance reasoning performance with reduced training cost of prompts, leveraging interpretable textual features to provide actionable insights.", "sections": [{"title": "Introduction", "content": "One major remaining challenge for Large Language Models (LLMs) is their insufficient reasoning capabilities (Dziri et al. 2024; Cao et al. 2024). Current LLMs perform well on System-1 tasks but face limitations in handling System-2 problems (Bengio et al. 2019). Prompting-based methods (Lester, Al-Rfou, and Constant 2021; Liu et al. 2023) aim to enable LLMs to understand input prompts and adapt to the downstream tasks through the design and crafting of prompts, becoming a focal point of interest among researchers in recent years. Compared to fine-tuning methods (Howard and Ruder 2018; Dong et al. 2019; Lewis et al. 2019), prompting methods do not require substantial computational resources and time to retrain the model, allowing for the development of more generalized solutions using the original pre-trained model (Li et al. 2023).\nMany exciting prompting methods have emerged, such as Chain of Thought (CoT) (Wei et al. 2022), Zero-Shot CoT (Kojima et al. 2022), among others. The prompting instructions in these methods are typically designed by humans, even introducing some noise (Zhou et al. 2024). This leads us to question: are these language expressions of the prompts the best to trigger LLMs? Prompt optimization methods (Chang et al. 2024) refine and enhance prompts for LLMs, to improve LLMs' performance on downstream tasks. However, current prompt optimization methods face challenges related to training costs and interpretability. Gradient-based approaches, such as APE (Zhou et al. 2022), APO (Pryzant et al. 2023), and OPRO (Yang et al. 2023), require substantial training costs to obtain gradient information. While gradient-free methods like GPS (Xu et al. 2022) and GrIPS (Prasad et al. 2023), which rely on editing and searching, also lack the interpretability to be understood from a human intuitive perspective during the process.\nCausal abilities represent higher-level cognition that transitions from System 1 to System 2 (Bengio et al. 2019),"}, {"title": "Causal Estimand", "content": "To quantify the causal effect, we need to identify the causal estimands. The causal effect for an individual, referred to as the Individual Treatment Effect (ITE) (Holland 1986), is challenging to identify due to the counterfactual problem. However, we can estimate the overall average level, namely, Average Treatment Effect (ATE) (Rubin 1974):\n$\\text{ATE} = E[Y (1) \u2013 Y (0)],$ (1)\nwhere the ATE represents the average difference in potential outcome variables (Y) between the treatment group (1) and the control group (0)."}, {"title": "Identification Assumptions for Causal Inference", "content": "To perform causal inference using a causal estimand, three identification assumptions need be satisfied: ignorability, positivity, and consistency (Feder et al. 2022).\nIgnorability. Ignorability, also known as unconfoundedness, refers to the condition where, for groups with the same values of covariates, the assignment of the treatment is independent of the potential outcome variables:\n$T \\amalg Y(t) | X, \\forall t\\in \\{0,1\\},$ (2)\nwhere X is observed variables (including confound variables), T and Y are the treatment and the potential outcome separately, and t means the value of T. In other words, this assumption requires that we observe all confounding variables and that there is sufficient variation in X.\nPositivity. Positivity refers to the condition that, for any given observed variable X, the assignment of the intervention T has a probability between 0 and 1:\n$0 < Pr(T = 1 | X = x) < 1, \\forall x,$ (3)\nrequiring the assignment of treatments be random, meaning that each unit has a non-zero probability of being treated.\nConsistency. The Consistency Assumption, also known as the Stable Unit Treatment Value Assumption (SUTVA), states that the potential outcome Y of any unit is not influenced by the treatment T applied to other units. Additionally, for each unit, there are no different forms or versions of any given T that could lead to different Y:\n$T = t \\Leftrightarrow Y (t) = Y, \\forall t \\in \\{0,1\\}.$ (4)\nThis assumption requires that each treatment be clearly defined and that the potential outcomes resulting from the treatment are stable."}, {"title": "Self-Causal Instruction Enhancement", "content": "This paper aims to enhance the reasoning performance of LLMs in downstream reasoning tasks by estimating and optimizing the causal effects of prompting instructions. A natural thought for causal inference of prompts and task outcomes in LLMs is the potential outcome framework (Rubin 1974) since we can guide LLMs in generating data that align as closely as possible with requirements. In our causal effect estimation method, the instructions serve as the treatment T, and the correctness of the results in downstream tasks serves as the potential outcome Y. This allows us to further enhance the instructions based on the estimated causal relations. The causal effect is:\n$\\text{ATE} = E[Y(1) \u2013 Y(0)]$\n$= E[Y(1)] \u2013 E[Y(0)]$\n$= E[Y | T = 1] \u2013 E[Y | T = 0].$ (5)\nFor simplicity without loss of generality, we assume T is expressed in binary form here. The values of Y are 1 and 0, where 1 indicates a correct outcome, and 0 indicates an incorrect outcome. In other words, we represent the reasoning ability of LLMs through the causal effect of instructions on the correctness of reasoning task outcomes. To improve the reasoning capabilities of LLMs in downstream tasks, we simply need to identify the instructions that maximize the causal effects of the prompts to the correctness of the task."}, {"title": "High-quality Observational Data Generation", "content": "In this section, we will explain how to generate observational data that satisfies the three identification assumptions in causal inference while ensuring that the resulting structured data is manageable for causal effect estimation.\nAs shown in the Data Generation part of Figure 2, based on an instruction for LLMs to complete a task, which can be either manually constructed or automatically generated based on previous research, we generate a different instructions from the given instruction using LLMs, such as forward mode and reverse mode in APE (Zhou et al. 2022). Assuming we are interested in the textual features of the instructions, as these features are relatively generic for the input instruction and facilitate subsequent processing, we have LLMs exhaustively enumerate all n textual features {$T_1, T_2,..., T_n$} that influence the results of downstream tasks for these (a + 1) instructions, and these proxy features must be independent of each other (ignorability). The proxy features must be described in detail and be consistent without version bias (consistency). Next, we have the LLMs generate counterfactual instructions based on each proxy feature, resulting in a total of n (a + 1) counterfactual instances, ensuring that each proxy feature has both the probability of being treated and not being treated (positivity).\nTo label these proxy features as numerical or categorical data, we use the method by leveraging the scoring capabilities of LLMs for annotation (Liu et al. 2024). Subsequently, we randomly select b data from the training set of the LLMs' downstream reasoning tasks. These b questions are combined with the (n + 1)(a + 1) instructions (including the counterfactual and original instructions) using the Cartesian product, resulting in b(n+1)(a+1) instruction-question"}, {"title": "Estimating Causal Effect with LLMs", "content": "In the Data Generation step, we prepare the data for causal effect estimation. In this section, our goal is to perform causal effect estimation on each proxy treatment to uncover the causal relationships between the treatments and the correctness of LLM downstream task results.\nDuring the causal effect estimation, for each proxy feature $T_i \\in \\{T_1, T_2,..., T_n\\}$ considered as a treatment in turn, its causal effect is estimated:\n$\\text{ATE}_i = E_x [E[Y(1) | X ] \u2013 E[Y (0) | X]]$\n$= E_x [E[Y | T_i = 1, x] \u2013 E[Y | T_i = 0, x]],$ (6)\nwhere X represents covariates (proxy features other than the current proxy treatment), and x represents a specific value of X. $E_x$ indicates the expectation over all values of X.\nOrdinary LLMs typically struggle with table data processing. When LLMs are asked to calculate ATE based on Equation 6, common responses include problem-solving steps or a piece of code without answers. To solve this problem, we employ the open interpreter (Open Interpreter 2024) for ATE estimation. It contains a built-in code interpreter that can generate and execute code based on prompts and return results. The reason for using LLMs to estimate causal effects is that, subsequently, to generate instructions with larger $ATE_{overall}$, the LLM needs to estimate correct ATE and understand the ATE calculation process. To enable LLMs to accurately estimate the $ATE_i$, we utilize the in-context learning strategy (Brown et al. 2020), providing the relevant code along with [1/2] ATE results as the demonstration to the LLMs and ask LLMs estimating the complete i ATE results. This process is shown in the Causal Effect Estimation part of Figure 2. The idea of this part is to leverage expert knowledge to teach LLMs how to perform causal effect estimation, enabling them to apply the learned knowledge to excel in their strength of language generation.\nThe meta-learners, including T-Learner, S-Learner (Kunzel et al. 2019; Maiya 2021), are effective methods in potential outcomes framework to estimate causal effect. The T-Learner is considered as the example for in-context learning, as the treatment effect between our control and treatment groups differs significantly in our data (the counterfactual instruction process), and has low selection bias. The T-Learner employs the base learner (e.g., the supervised learning or regression estimator) to separately estimate the control and treatment group functions:\n$\\mu_0(x) = E[Y(0) | X = x],$\n$\\mu_1(x) = E[Y(1) | X = x].$ (7)\nThe difference of these two estimates $\\hat{\\mu}_1(x)$ and $\\mu_0(x)$ denotes the estimation of the ATE using T-learner:\n$f(x) = \\mu_1(x) \u2013 \\mu_0(x).$ (8)\nThe S-Learners are considered as the other baseline for evaluating the ATE estimated by LLMs. Detailed experimental results are provided in the following section."}, {"title": "Enhanced Instructions Generation", "content": "By estimating the ATE of different proxy treatments, we obtain insights into how instructions causally influence the correctness of LLM downstream task outcomes through proxy features. Next, we aim to adjust the instructions based on the uncovered causal relationships to optimize the causal impact of instructions on the correctness of LLM downstream task outcomes. Considering the independence among the proxy features, we define the overall ATE of instructions on the correctness of LLM downstream task outcomes as:\n$ATE_{overall} = \\frac{1}{n} \\sum_{i=1}^n ATE_i.$ (9)\nWe intend to achieve the optimized $ATE_{overall}$ not by involving gradient-based optimization concepts but by allowing the LLM to generate instructions that optimize"}, {"title": "The OR Module", "content": "The causal model is regarded as a framework that can explicitly represent causal dependencies and allow for automatic reasoning about these dependencies (Jensen 2021). Applying the OR model in causal inference leads to a more expressive and flexible causal representation of a complex world (Jensen 2021; Lee and Ogburn 2021; Wang and Luo 2024). Based on the above argument, we model different tasks and uncover causal relationships using the OR model to achieve easy reuse of instruction enhancement. As shown in the OR Module part of Figure 2, we extract the uncovered causal relationships from the LLM's explanations, which are identified by the LLM as having a significant impact on potential outcomes when generating instructions with optimized ATE. Usually, LLMs encourage the positive impact of features to guide the LLM's-self in generating better instructions. Then, objects (such as different tasks, O and O' in Figure 2) that have aggregation or generalization relationships can directly inherit from this class, and they can adjust instructions based on the positive or negative impact of the proxy features. This class can be regarded as the meta-template, which can be reused by other objects."}, {"title": "Experiments and Results", "content": "While the enhanced metric could be changed to others, such as the certainty of downstream task answers, perplexity, answer length, etc., this paper focuses on the accuracy of reasoning tasks for LLMs. To validate the effectiveness of (OR)-SCIE on the accuracy of LLMs in reasoning tasks, we set up the experiments with representative reasoning tasks and datasets, LLMs, and baselines.\nReasoning tasks and datasets. We evaluate ten common datasets across four categories of reasoning tasks for the experiment. (1) Arithmetic reasoning: GSM8K (Cobbe et al. 2021) and MultiArith (Roy and Roth 2015). (2) Commonsense reasoning: StrategyQA (Geva et al. 2021) and CommonsenseQA (Talmor et al. 2019). (3) Symbolic reasoning: Coin Flip (Wei et al. 2022), Last Letter Concatenation (Wei et al. 2022) and Boolean Expressions (Suzgun et al. 2023). (4) Other logical reasoning: Causal Judgement, Date Understanding, and DisambiguationQA from Big Bench Hard (BBH) (Suzgun et al. 2023). For datasets like GSM8K, where the training and test sets are pre-defined, we perform random sampling on the training set and evaluate using the test set. For datasets without predefined training and test sets, we exclude the sampled data used in the SCIE process during testing on the reasoning tasks.\nModels. The experiments in this paper will evaluate inference tasks on several commonly used LLMs, including GPT-3.5 Turbo (OpenAI 2022), GPT-4o mini (Achiam et al. 2023) and Llama-3-70b (Dubey et al. 2024). The (OR)-SCIE process is designed to utilize more powerful LLMs like GPT-4o (Achiam et al. 2023) whenever possible, aiming to enhance the performance of a student model (process-ing downstream tasks) by activating the causal ability from a teacher (good at causal inference) model.\nBaselines. Three base instructions and their corresponding prompting methods are used as baselines: Zero-Shot CoT"}, {"title": "Evaluation of Estimating Causal Effect with LLMs", "content": "ATEoverall based on the uncovered causal relationships, which leverages the LLM's capabilities to encode and decode natural language, shown in the Enhanced Instructions Generation part of Figure 2.\nAssuming that the LLM can generate instructions with varying degrees of proxy features by adjusting the value of the treatment, a larger ITE can be obtained with a higher probability. We will next prove this point.\nThe Individual Treatment Effect (ITE) is represented as:\n$\\text{ITE}_i^{(j)} = Y_i^{(j)} (1) \u2013 Y_i^{(j)} (0),$ (10)\nwhere $ITE_i^{(j)}$ and $Y_i^{(j)}$ represent treatment effect and outcome of an individual corresponding to $T_i$, respectively. The ATEi can be represented as:\n$\\text{ATE}_i = \\frac{1}{m} \\sum_{j=1}^m \\text{ITE}_i^{(j)}.$ (11)\nThat is, the ATE is the average of all individuals' ITE, where m denotes the number of individuals.\n\u2022 When ATEi > 0, which means that the effect of Ti is positive. According to the law of large numbers, the overall average effect is the average of individual effects. Therefore, it can be inferred that for an individual j, the likelihood that $Y_i^{(j)} (1)$ is greater than $Y_i^{(j)} (0)$ is higher, which implies that adjusting Ti from 0 to 1 will increase the value of ITEj.\n\u2022 Similarly, when ATEi < 0, it can be proven that adjusting Ti from 1 to 0 will increase the value of ITEj.\n\u2022 When ATEi = 0, it means that Ti has no significant effect on the overall positive outcome, so the adjustment of Ti can be ignored in this case.\nTherefore, based on the discussion of adjusting Ti to increase ATEi, it is possible for the LLM to generate instructions that improve the ATEoverall, under the consistency assumption that different proxy treatments affect potential outcomes independently. It is noteworthy that though we aim to obtain instructions with maximum ATEoverall, the LLM will give solutions to generate optimized instructions. However, solutions generated by our method show enhanced performance compared to the base instructions."}, {"title": "Evaluation of SCIE", "content": "SCIE on Zero-Shot CoT. We apply SCIE on the Zero-Shot CoT across reasoning tasks and obtain corresponding enhanced instructions with the LLM explanations (see Appendix A). Table 1 shows the accuracy of Zero-Shot CoT and SCIE Zero-Shot CoT on reasoning tasks using GPT-3.5 turbo (results on more LLMs can be found in Appendix B). As shown in Table 1, SCIE effectively enhances instructions for most reasoning tasks. However, Date Understand is an exception. We attribute this to a potential bias in the ATE estimation process, as also evident in Figure 4. Furthermore, our method does not require extensive training and provides interpretability. We also observe that, aside from the bias in estimation process, if the ATE values exhibit significant fluctuations, the LLM will adjust the instructions based on more pronounced proxy features, resulting in greater performance improvements. For example, the Causal Judgement task shows more pronounced ATE variations (see Figure 4) compared to other tasks, and the LLM emphasizes crafting the instruction to maximize the factors that have the most positive effect in the explanations (see the LLM explanations in Appendix A), and consequently, it achieves a higher accuracy improvement relative to the other tasks. This observation aligns with the design principles of our method.\nSCIE on other base instructions. We evaluate the accuracy of the GSM8K of GPT-3.5 Turbo with Plan-and-Solve Prompting and AgentInstruct as the base instructions, respectively. Due to the lengthy instructions of AgentInstruct, we set a = 5, b = 5, for cost control. We separately conduct"}, {"title": "Comparison with LLMs as Optimizers", "content": "ATEi to increase ATEi, it is possible for the LLM to generate instructions that improve the ATEoverall, under the consistency assumption that different proxy treatments affect potential outcomes independently. It is noteworthy that though we aim to obtain instructions with maximum ATEoverall, the LLM will give solutions to generate optimized instructions. However, solutions generated by our method show enhanced performance compared to the base instructions."}, {"title": "Evaluation of OR-SCIE", "content": "Aggregation. We simulate the part-whole relationship in the OR model using the GSM8K dataset as an example. Specifically, we extract the causal relationships from LLM explanations that have been proved to produce SCIE-level instructions. For instance, in this case, we extract the positive causal relationship between \"Clarity\" and \"Tone\" and then instruct the LLM to generate enhanced instructions based on Zero-Shot CoT (see Appendix D). Subsequently, we randomly select 70% of the GSM8K dataset as the part object and conduct the test with instructions generated from OR-SCIE, achieving an accuracy of 77.9% (see Figure 5 (b)). Intuitively, the performance of instructions produced by the OR module may be inferior to those obtained by SCIE, as it does not optimize the ATE. In this example, the result is slightly higher than that of the complete SCIE method, which may be due to the randomly extracted test data, but it demonstrates the effectiveness of the OR module.\nGeneralization. We use the MultiArith and GSM8K datasets to simulate the generalization relationship within the OR model. MultiArith inherits the causal relationships extracted from the LLM's explanations as a class, and the resulting accuracy is 94.3%, shown in Figure 5 (b). As we can see, the results using the OR module outperform those of the no-SCIE method, and this class is reusable. To further demonstrate the practical utility of the proposed method, we evaluate GPT-4o mini and Llama-3-70b on a more challeng-ing dataset, fresh-gaokao-math-2023 (Tang et al. 2024), us-ing the instruction directly inherited from GSM8K. Among 30 high-difficulty math problems, the instruction generated by OR-SCIE(G) enables LLMs to solve several more ques-tions correctly compared to the base instruction."}, {"title": "Conclusions", "content": "This paper enhances LLMs' reasoning performance by eliciting LLMs' causal effect estimation abilities and enabling them to further self-optimize instructions. Besides, the idea"}]}