{"title": "Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs", "authors": ["Aayam Shrestha", "Pan Liu", "German Ros", "Kai Yuan", "Alan Fern"], "abstract": "This work focuses on generating realistic, physically-based human behaviors from multi-modal inputs, which may only partially specify the desired motion. For example, the input may come from a VR controller providing arm motion and body velocity, partial key-point animation, computer vision applied to videos, or even higher-level motion goals. This requires a versatile low-level humanoid controller that can handle such sparse, under-specified guidance, seamlessly switch between skills, and recover from failures. Current approaches for learning humanoid controllers from demonstration data capture some of these characteristics, but none achieve them all. To this end, we introduce the Masked Humanoid Controller (MHC), a novel approach that applies multi-objective imitation learning on augmented and selectively masked motion demonstrations. The training methodology results in an MHC that exhibits the key capabilities of catch-up to out-of-sync input commands, combining elements from multiple motion sequences, and completing unspecified parts of motions from sparse multimodal input. We demonstrate these key capabilities for an MHC learned over a dataset of 87 diverse skills and showcase different multi-modal use cases, including integration with planning frameworks to highlight MHC's ability to solve new user-defined tasks without any finetuning. Project webpage: https://idigitopia.github.io/projects/mhc/", "sections": [{"title": "1 Introduction", "content": "Physically simulated humanoid characters have the potential to generate natural looking and realistic human motions and behaviors for a variety of applications ranging from video games, robotics, virtual/augmented reality, to digital avatars. However, directable and natural motion generation for these physically unstable, high-dimensional characters corresponds to a challenging control problem requiring precise coordination of joint-level commands [14,30]. One solution is to leverage motion capture (MoCap) data, which provides detailed pose information for"}, {"title": "2 Related Work", "content": "Learning from MoCap Data: Leveraging MoCap data enables controllers to acquire complex behaviors with human-like motion quality. However, large and diverse MoCap datasets present challenges including scalable distillation and ensuring fluid transition between skills. Initial works train single-clip policies to mimic individual behaviors using tracking rewards [4,20,24] or adversarial losses [7, 11, 18, 22, 34]. However, distilling these into a multi-clip controller remains computationally prohibitive [30]. A more scalable alternative is to directly learn a multi-clip policy learning via reinforcement learning with tracking objectives [5, 14, 17, 21, 30, 32, 33]. However, tracking rewards alone is not enough to ensure smooth transitions between skills and failure recovery. Recent works augment training with adversarial losses to encourage natural motions during transitions [21, 29] or define an explicit fail state recovery policy [14].\nCombination of motions: Recent works have explored imitating combined motions, but require training individual policy for each new behavior pair [1, 12,35]. Additionally, they rely on full motion oversight, lacking adaptability to partial guidance. On the completion front, some kinematic models can synthesize motions despite missing information [3,31]. However, these controllers are not grounded in physics, restricting their application.\nUnder-specified Control: Intuitive modalities like language, video, and VR provide important yet often under-specified means to direct motor skills. Existing works map some of these modalities to embedding spaces [10,11,28,36] or key joint poses [13, 14]. However, they handle a predefined sparisty types; adapting to new sparsity specifications like VR require expensive retraining [2,6]. While ambiguity is inherent in language and image-conditioned policies [8,36], fine-grained control remains difficult as they do not allow low-level granularities like joint-level guidance. Overall a gap persists in controllers that can handle partial, sparse guidance with precision across modalities. Closing this gap can enable more intuitive control of reusable motor behaviors.\nDownstream Planning: The acquired low-level control skills can support downstream tasks via a wide range of approaches including supervised fine-tuning for specialized behaviors [36], reinforcement learning for new objectives [5, 13, 19, 21], model predictive control for short-term horizons [9], and finite state machines that encode behavioral logic [29]. However, supervised fine-tuning remains restricted in flexibility to new tasks while reinforcement learning lacks sample efficiency whereas model predictive formulations are limited to short planning horizons. Additionally, the range of possible finite state machines largely depend on the flexibility of the underlying low-level controller. One solution is the use of data-driven planners like DAC-MDPs [27] which compile static experiences into approximated MDPs for fast optimization. While these methods enable zero-shot generalization, their integration with learned reusable motor skills remains relatively unexplored. Overall, leveraging low-level controllers to swiftly accomplish high-level goals remains an open challenge."}, {"title": "3 Problem Formulation", "content": "Given a dataset of human motion demonstrations, our objective is to develop a motion generator that can produce directable, realistic combinations of those and similar motions in simulated physics environments. By directable we mean that the generator can be provided with directives that specify properties of the desired future motion. For example, the most detailed directive is to specify the exact future positions and velocities of each humanoid joint. Alternatively an under-specified directive might provide just the desired hand positions in the future time window or just the torso velocity. For such under-specified directives, the motion generator should produce motion that is consistent with the provided directive, while also constraining to the space of natural motions. By allowing for under-specified directives we can support many input modalities for describing"}, {"title": "4 Masked Humanoid Controller", "content": "Because the MHC involves solving a complex physical control problem without supervision for the low-level actions, we formulate MHC training as reinforcement learning (RL). Training proceeds through episodes, where each episode initializes the humanoid in a physical environment with a specific directive. The"}, {"title": "4.1 Training Episodes", "content": "Our goal is to create a distribution over episodes that will require the MHC to switch between motions in M+ based on directives corresponding to multimodal inputs. If an MHC achieves high reward on such a distribution, then it will have the desired CCC properties. Each episode is defined by a randomly sampled pair of initial humanoid pose and target directive.\nInitial Pose Distribution: The initial pose distribution is a mixture of a uniform distribution over poses in M+ and a distribution of fallen humanoid poses pfall (q), where the weight of pfall is 0.1 in our experiments. This choice forces the MHC to learn catchup to out-of-sync target directives, including recovery from fallen humanoid states. In addition, we apply a random in-plane rotation when generating a pose so that the MHC is robust to orientation changes.\nTarget Directive Distribution: For each episode we generate a target directive (91:L, I1:L), where L is the length of the episode. The motion 91:L for the directive is generated by concatenating random length subsequences drawn from M+. In our experiments, L = 300, corresponding to 10 seconds, and each sub-sequence length is uniform in the range from 120 to 240. This type of concatenation generally results in sharp and inconsistent motion transitions, forcing the MHC to learn catchup behavior. In addition, we also apply a random in-plane rotation to each sub-sequence so that the MHC is robust to orientation changes.\nThe directive mask I1:L for an episode is generated by generating a mask I1 for the first time step and using that mask for all other time steps (i.e. It = I\u2081 for all t). We employ two types of masking: channel-level and joint-level masking (Fig. 4). For, channel level masking, one or more of the available channels (q, q, q, \u011d\u00ba) are selectively masked. The different channel combinations that we randomly choose between each episode are detailed in Fig. 4. For example, all channels except \u011d\u00ba can be masked to represent the joint keypoint modality that can be generated from video via computer vision. Channel level masking allows us to represent modalities like video tracking, text2motion models, proprioception, and joystick controllers.\nJoint-level masking is a secondary level of masking that can be further applied to the joint position channels (q\u00b9, q\u00ba). Here we sample a percentage in [0, 100] and randomly select that percentage of joints to mask out from the target directive. This allows us to emulate partially occluded video motions as well as VR controllers which only have a limited number of keypoint sensors. In our experiments we sample masks by first randomly selecting a channel level mask followed by a 50% chance of also applying joint-level masking."}, {"title": "4.2 Reward Design", "content": "The RL training objective is to maximize the expected discounted episodic re-ward denoted by $E[\\sum_{t=1}^{L} \\gamma^{t-1} r_t]$ where $\u03b3 \\in (0,1]$ is the discount factor and rt is the reward at timestep t. We define the reward at each step as the sum of a tracking reward rt, which encourages agreement with directives, a style reward rat, which encourages natural looking motions, and an additional energy cost Ct, which encourages smooth motions: rt = 0.5rt + 0.5rst  Ct.\nTracking Reward: The tracking reward is defined to prefer generated motions based on how well they agree with the episode's directive at each time step. We find that learning is accelerated by using a reward function that prioritizes learning coarse motion characteristics before focusing on finer motion details. In particular, we define four reward terms in order of priority $r_h, r_o,r_v,r_j$ corresponding to matching the directive for root height, root orientation, root velocity and joint Euler coordinates, respectively. At each time step, a term is activated only if the higher priority rewards are above a threshold of 0.9. Specifically, the tracking reward function is given by: $r_t^r = r_h + r_o + r_v + r_j$\n$r_h = e^{-m_h  ||h_t - h_t^* ||^2}$ (1)\n$r_o = I(r_h > 0.9)  e^{-m_o  ||d(q_t) - d(q_t^*)||^2}$ (2)\n$r_v = I(r_h > 0.9)  e^{-m_v  ||\\dot{q_t} - \\dot{q_t^*}||^2}$ (3)\n$r_j = I(r_v > 0.9)  \\frac{1}{\\vert J \\vert}  \\sum_{j \\in J} e^{-m_j  40||q_{tj} - q_{tj}^* ||^2}$ (4)\nwhere Jis the set of all joints, and mh, mo, mv, mj are equal to 0 if the corresponding reward component depends on the pose information not selected by the episode's directive mask, and otherwise equal to 1. Thus, reward terms not relevant to the directive do not affect preferences over the generated motion.\nStyle Reward: We consider a multi-part style reward, inspired by [1], with a distinct discriminator for different body parts. Specifically, we create 5 sets of joints from the whole body: $J_1,J_2, J_3, J_4, and J_5$ corresponding to upper right, upper left, root, lower body, and full body, respectively. During RL training a discriminator Dok is trained for each part set Ik, conditioning only on joint information in Ik. Training is done continuously throughout RL by sampling a pair of motions, one positive motion from M and one negative motion generated by the current MHC, and updating the parameters of each discriminator using the regularized loss function from [1,22]. In our implementation, the input to the discriminators is a motion sub-sequence of length 10 so that the discriminator takes temporal dependencies into account. The corresponding reward component at time step t for discriminator k is $-log(1 - D_{\\theta_k} (q_{t-10:t})$. The overall style reward rst, is the average of these components across discriminators.\nEnergy Cost: To penalize large changes in the action across timesteps and large torques, the energy cost at time t is given by\n$c_t = \\sum_{j\\in J} 0.01  ||a_t^j - a_{t-1}^j||_1 + 0.0002  ||\\tau_t^j||_1$ (5)\nwhere af is the action (set point) at time t for joint j and r\u1ec9 is the torque applied to joint j. The inclusion of this penalty is critical for avoiding high-frequency jitter, especially of the foot."}, {"title": "5 Motion Generation for Higher-Level Tasks", "content": "We propose to integrate the learned MHC with a data-driven planning framework that allow for automatically generating a potentially large FSM for a user-defined higher-level task. Instead of specifying the FSM directly, the designer specifies: 1) a reward function r* and optimization objective (e.g. discounted total reward) corresponding to the higher-level task, 2) the set of possible directives A for the MHC that can be used as dynamically selected actions to achieve the task, and 3) a mask over pose variables S defining a state abstraction used to form the FSM states. For example, the user may specify a reward based on the distance to a goal location, specify directives corresponding to root velocity and orientation commands, and a state abstraction corresponding to localized goal location. Alternatively if certain types of arm motions were required, directives and the state abstraction could include selected sub-motions for the arm joints.\nWe follow the data-driven planning framework of DAC-MDPs [27] which uses data collected from a dynamic system to construct and solve a Markov Decision Process (MDP) as shown in 4. In our application, we collect trajectory data by randomly initializing the MHC in the environment and executing random sequences of directives from A each trajectory stores the state abstraction at each time step along with the selected action directive. The DAC-MDP solution is a control policy that specifies for each abstract state, which directive in A to execute. We refer the reader to [27] for DAC-MDP details. Importantly, the CCC properties of the MHC allow it to robustly execute the high-level actions selected"}, {"title": "6 Experiments", "content": "We conduct experiments to empirically validate MHC's key capabilities to imitate a wide range of motion directives that necessitate the key capabilities of imitation, catchup, combination and completion (Fig. 5). We then showcase motion generation from target directives derived from multiple modalities (Fig. 7) and case studies on motion generation for higher-level tasks. (Fig. 8).\nDatasets and Baselines: We train our MHC on a Reallusion MoCap dataset [21,23] of 87 motion clips (MReal). We also generate a test set MASE Of 87 motions using a pre-trained ASE controller [21] by initializing it in a random pose and selecting a sequence of random ASE skills. We also use this pre-trained ASE controller as a state-of-the-art baseline for the Reallusion dataset. Specifically, ASE includes an encoder that maps target motions to skills, which allows us to evaluate ASE on fully specified directives. Note that, unlike MHC, the ASE controller does not support under-specified directives. Thus, in our comparisons, we always provide ASE with directives that specify all pose information.\nMetrics In our experiments we evaluate how well a generated motion matches the specified motion directive. As a metric of motion similarity we use the commonly used mean per-joint position error EMPJPE (in mm), which averages the root relative error of each humanoid joint. Inspired by UHC [15], we also measure the success rate (Succ) defined as following the reference motion with < 10% of failed frames, where a failed frame is one where maximum joint error is > 1m. For under-specified directives that mask joints, the success rate and EMPJPE \u0130S measured only using the selected joints."}, {"title": "6.1 CCC Capabilities", "content": "Imitation: We measure the imitation quality of the MHC and ASE baselines across both datasets MReal and MASE. Figure 5A illustrates an example of"}, {"title": "6.2 Multi-Modal Directives", "content": "We qualitatively evaluate the MHC for different kinds of real-world input modalities for motions that the MHC was never trained on. We explore the following modalities: virtual reality devices, joystick controllers, 3D joint positions extracted from videos, and text-to-motion generation. In our experiments, we uti-lized the Meta Quest 2 VR system, comprising a headset and two controllers, to gather the orientation and 3D positions of both the headset and controllers (Fig. 7A). For joystick-based inputs, an Xbox controller was employed to manip-ulate the speed and direction of the velocity (via the left joystick and trigger), root orientation and height (via the right joystick and trigger), as illustrated in Fig. 7B. The estimation of 3D poses from video footage was achieved using MeTRAbs [25] (Fig. 7C), while generation of kinematic human motions from tex-tual descriptions was achieved using T2M-GPT [37] (Fig. 7D). We find that the learned MHC is versatile and can generate natural motions from under-specified directives derived from these input modalities. These results have important im-plications for real-world applications, where motion capture data may often be incomplete or noisy due to occlusions or sensor limitations."}, {"title": "6.3 Higher-level Task Specification", "content": "We provide demonstrations of motion generation for higher-level tasks via inte-gration of the MHC with FSMs and DAC-MDPs. For FSM integration we con-sider the task of navigating to a specified target goal location while executing different locomotion styles, such as running or crouch-walking, and performing various finishing moves, like taunting or falling down as shown in Figure 8A. We hand-coded simple FSMs to achieve these tasks, where each state of the FSM selects an appropriate directive for the MHC to execute. This includes under-specified directives corresponding to joystick commands and a fully-specified directives corresponding to the finishing moves.\nWe integrate the MHC with DAC-MDPs by constructing a DAC-MDP using a dataset collected by generating motions using the MHC with directives derived from random joystick commands. The first high-level task (Figure 8B) is to simply reach a goal location. To do this, we define a DAC-MDP reward function that provides a reward at each step that is larger as the humanoid gets closer to the goal. The reward at the goal is a maximum of 1. The resulting FSM produced by solving this DAC-MDP results in motions that reliably reach the goal. To illustrate the zero-shot capabilities, we next give the DAC-MDP the negation of the first reward function, which should lead the agent to avoid the goal location. Figure 8B illustrates the resulting desired behavior.\nFinally, we illustrate the higher-level reasoning capabilities afforded via the DAC-MDP integration. As illustrated in Figure 8C, we adjust the reward func-tion and DAC-MDP termination condition so that the humanoid receives a pos-itive reward for swinging the sword, which immediately terminates the episode. We then use the DAC-MDP to produce an FSM for a large discount factor 0.999 and a smaller discount factor 0.9. The large/small discount factors encourage planning over a long/short horizon. The best short horizon plan is to simply get the immediate reward of swinging the sword and then ending the episode, while the long-horizon plan is to instead go to the goal and collect the larger long-term reward. This is exactly the behavior produced by the two FSMs."}, {"title": "7 Summary", "content": "We highlighted three capabilities, Catchup, Combine, and Complete, that are jointly necessary for practical applications of physically-realistic motion gener-ation. Our proposed MHC is the first motion generator that achieves all three capabilities. This RL-based approach for training is highly versatile and can be applied to any available motion-capture dataset and multi-modal input direc-tives. Importantly, the MHC is a dynamic motion generator in the sense that it actively generates motion in response to the current environment conditions. Finally, we demonstrated a straightforward integration of the MHC with data-driven planning to allow for zero-shot motion generation for higher-level tasks."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation Details", "content": "The physics simulation is conducted using Isaac Gym [16], where the MHC runs at 30 Hz and the simulation runs at 60 Hz. The architecture of the MHC, which consists of a controller and an ensemble of discriminators, each implemented as a neural network. The controller policy encodes the motion directive looka-head using a 3-layer perceptron with hidden dimensions of 1024 and 512. This encoding is then concatenated with the current pose of the humanoid and fed into another 3-layer perceptron with dimensions [1024, 1024] that serves as the policy head, outputting the action of the controller. In practice, we implement the ensemble of discriminators as a single discriminator with different wrappers. Each wrapper masks the observations to consider only a single set of joints. The discriminators are also 3-layer perceptrons with dimensions [1024, 512]. We use SiLU activations for all perceptrons. For Training the MHC is trained via the Proximal Policy Optimization (PPO) reinforcement learning algorithm [26] with fixed entropy. The training process takes 7 days on a single Nvidia A6000 GPU to obtain the final policy."}, {"title": "A.2 Extended results for High level task specification", "content": "We further consider different variants of the go-to-location task and the heading task following [29] to showcase the flexibility afforded by the MHC framework. The rewards for both tasks are defined following [29].\nFor the heading task, a random heading direction and a velocity direction are sampled as targets, and the FSM should be able to follow these directives. For example, \"Head east while facing west.\" Furthermore, we can also specify the speed and height during these motions. \"Run\" maps to a speed of 2.5 m/s with a root height of 0.85m, while \"crouch walk\" refers to a root height of 0.4m with a speed of 1m/s. We evaluate the resulting motions using task-specific rewards, which reward matching the desired direction and heading. We find that the produced FSMs can reliably generate motions that match the higher-level heading task objectives.\nFor the go-to-location task, we consider different variations in movement time and finishing motion. Unlike [29], where only a particular skill can be re-quested as a finishing motion, our framework allows giving any kind of full or partial directive as a finishing motion. Here, we consider sword swing and taunt motions as finishing directives. The movement directives are chosen following the heading task. It is worth noting that our framework also allows for different upper body movements throughout the movement as well. Table 2 shows that FSMs using MHC generate motions that achieve high task rewards across all go-to-location task variants."}]}