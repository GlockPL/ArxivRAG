{"title": "TOWARDS INTERPRETING LANGUAGE MODELS: A CASE STUDY IN MULTI-HOP REASONING", "authors": ["MANSI SAKARVADIA"], "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Language models (LMs) struggle to perform such reasoning consistently. We propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single- and multi-hop prompts. We then propose a mechanism that allows users to inject relevant prompt-specific information, which we refer to as \u201cmemories,\" at critical LM locations during inference. By thus enabling the LM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We empirically show that a simple, efficient, and targeted memory injection into a key attention layer often increases the probability of the desired next token in multi-hop tasks, by up to 424%. We observe that small subsets of attention heads can significantly impact the model prediction during multi-hop reasoning. To more faithfully interpret these heads, we develop Attention Lens: an open source tool that translates the outputs of attention heads into vocabulary tokens via learned transformations called lenses. We demonstrate the use of lenses to reveal how a model arrives at its answer and use them to localize sources of model failures such as in the case of biased and malicious language generation.", "sections": [{"title": "INTRODUCTION", "content": "Despite recent widespread adoption of neural Language Models (LMs) [Vaswani et al., 2017, Brown et al., 2020] in chat-based applications [OpenAI, 2022], the mechanisms by which LMs acquire knowledge during training and recall knowledge to form predictions at inference time are not well understood. This complicates the safe deployment of LMs in consumer and scientific pipelines [Gaudin, 2023, Yun et al., 2023, Hardalov et al., 2018, Jablonka et al., 2023, inter alia] as LM behavior can be unpredictable. For example, LMs are capable of exhibiting harmful behaviors including displaying bias, regurgitating private information, hallucinating, producing offensive language, and producing malicious outputs due to adversarial training [Nadeem et al., 2020, Winograd, 2023, Zhang et al., 2023, Bender et al., 2021, Kandpal et al., 2023b]. Mitigating these harmful behaviors is limited by our lack of understanding of how LMs work. To ensure success and safety of LM-based applications, better interpretability techniques must be developed to understand how models develop behaviors. In this work, we focus on developing better interpretability techniques to understand how models recall knowledge during inference.\nWe study the case of LMs attempting to perform multi-hop reasoning. Multi-hop reasoning is the task of answering a prompt that contains references to an entity that is never explicitly named (see Fig. 3.1). Many modern LMs struggle to consistently perform multi-hop reasoning [Arkoudas, 2023, Guo et al., 2023, Blair-Stanek et al., 2023]. We develop a method to localize multi-hop reasoning failures to specific attention heads within a model, inspect what terms an attention heads is outputting via a tool called Attention Lens, and an efficiently enhance multi-hop reasoning abilities during inference via our technique called \u201cmemory injections\u201d. Our interpretability-driven techniques can be easily adapted to localize additional sets of LM behavior within model weights, are computationally efficient, and overcome the limitations of existing model behavior corrective techniques."}, {"title": "RELATED WORK", "content": "We review interpretability tools such as probing, activation engineering, model editing, circuit discovery, and knowledge extraction. We also review recent advances in the study of language model reasoning capabilities and retrieval augmented generation."}, {"title": "Probing Models", "content": "Probing is a class of interpretability methods that attempt the decode the contents/functions encoded by neural network weights. Probing does this by directly mapping subsets of weight activations into human-understandable domains. Since activations are directly related to model inputs, probes allow researchers to causally draw connections between model inputs and probe outputs. Therefore, researchers may be able to use probes to localize sources of model behavior to a specific subset of model weights.\nProbes can be designed flexibly to suite the task at hand and are typically optimized using gradient based techniques like stochastic gradient descent [Ruder, 2016]. There are two main axes of freedom in probe design:\n\u2022 Architecture: Probe architectures are informed by the types of insight a researcher requires from a probe. For example, probes can be designed at varying level's of model architecture (e.g. weight-level, layer-level, attention head-level). Additionally, probes can be linear or non-linear. Certain model behaviors may be linearly decodable [Alain and Bengio, 2016] while others may need non-linear probes to decode [White et al., 2021]. Some works have even found that the manner in which a decoding task is defined can allow a probe to transition from non-linear [Li et al., 2022] to linear [Nanda et al., 2023c].\n\u2022 Training dataset: since probes are trained to perform a mapping between a model's activation and a desired domain, the training data a probe sees will govern its behavior. This training data must elicit all of the behaviors the researcher is attempting to study.\nThere are many use cases for trained model probes. Ettinger et al. [2016] introduces using classifier probes to understand semantic information in sentence representations. Probes can be trained to decode a hidden model representation into vocabulary, often with the goal of attempting to understand how each model layer informs how the model arrives at a final token prediction [nostalgebraist, 2021, Belrose et al., 2023, Pal et al., 2023, Katz et al., 2024]. Li et al. [2022] trained probes to understand a model's internal board representation when predicting the next best move in the board game Othello. Kim et al. [2019] explored the effect of pre-training on the model's learned representations of \u201cfunction words\u201d via trained probes. Aina and Linzen [2021] used probes to quantify model uncertainty in its completions of ambiguous prompts.\nA limitation of probes as a diagnostic tool is that it is not obvious if the probes are are correlational or causal tools for attempting to understand a model's internal representations [Belinkov, 2022]. To combat this limitation, researchers can further validate the efficacy of probes by using the probes to guide activation engineering and observe if downstream model performance is affected as they would expect. For example, Li et al. [2022] shows that by using probes to guide the editing of Othello-GPT's representation of the board state, they could alter the model's final next move prediction as expected; this further validated that the probes were faithfully decoding information as it was known to the model. See section 2.2 for additional techniques about how to engineer activations."}, {"title": "Activation Engineering", "content": "Activation Engineering is a class of interpretability method that allows researchers to decode the functions of model components, by modifying their respective output activation values and observing the downstream effect. Like probing 2.1, the goal of activation engineering can be to allow researchers to attribute model behavior back to specific model components. Additionally, activation engineering can also be used to directly influence model behavior at inference time.\nSome examples of applications of activation engineering are:\n\u2022 Vig et al. [2020] introduced \u201ccausal mediation analysis\u201d: a method to understand which components are a model are responsible for specific behaviors in language modeling; they apply causal mediation analysis to investigate which components of a model are responsible for gender bias.\n\u2022 Sun et al. [2021] demonstrates that activations of neural networks can be used to identify in-distribution and out-of-distribution model inputs in vision tasks. Djurisic et al. [2022] builds on this concept by both pruning and modifying late layer model activations for out-of-distribution detection.\n\u2022 Meng et al. [2022a] used \u201ccausal mediation analysis\" [Vig et al., 2020] as a method for localizing knowledge within model weights by comparing the activations of model forward passes over two different inputs.\n\u2022 Turner et al. [2023] proposes a method to add vectors that encode human-understandable semantic information directly to the activations of LMs to steer their outputs.\n\u2022 Fort [2023] showed how to adversarially engineer activations to have harmful downstream effects on LM prompt completions."}, {"title": "Model Editing", "content": "Model editing aims to change specific facts, associations, or information embedded in an LM outside of the constraints of traditional model training. Model editing requires the ability to localize learned information within subsets of the weight space and employs efficient and targeted methods to change this information while mitigating its effects of other information also embedded in the weight space. Model editing can be used to remove or alter private information, incorrect information, outdated information, biased information, and harmful information stored within model weights [Wu et al., 2023, Yan et al., 2024, Chen et al., 2023, Wang et al., 2024]. Model editing can enable machine learning models to more exactly reflect human knowledge, without the massive overhead cost of typical model pre-training/fine-tuning. Zhu et al. [2020] proposes an approach to modify specific learned facts encoded withing a LM's weights, while preserving model performance on other previously learned knowledge via a constrained optimization problem. Dai et al. [2022a] developed attribution methods to decipher which neurons are responsible for specific facts within languages models and developed methods to manipulate these neurons to edit a given fact. Cao et al. [2021], Mitchell et al. [2022b] both propose hypernetwork based approaches to edit facts within models. Hypernetworks are additional networks that are trained to predict which weights are responsible for a given fact and how to modify the weights of a given neural network to better represent the desired knowledge. Meng et al. [2022a] proposed Rank-One Model Editing (ROME): by interpreting multi-layer perceptrons as key-values stores, ROME is able to replace specific keys-value pairs to override old or establish new knowledge associations in the model."}, {"title": "Circuit Discovery", "content": "Circuits are sparse subsets of neural network weights that are responsible for (sub)sets of model behavior. Interpreting neural networks as circuits is useful as it allows researchers to localize sources of model behavior and may even help them better understand how stochastic training processes compress knowledge and skill into weights.\nIt was not always obvious that it was tractable to attribute model behavior to specific model components as these models are increasing massive (e.g. million, billion, trillion parameter scales). However, much research has shown that neural networks are very sparse: a small subset of weights are often responsible for much of a model's behaviors. For example, Frankle and Carbin [2018] showed that neural networks contain \u201cwinning lottery tickets\": sparse sub-networks within trained NNs that are nearly as performant as the original dense network. Follow up work, by Amid et al. [2022] further developed methods to extract performant sub-networks from randomly initialized dense NNs. Results about the sparsity of models held across neural network architectures [Han et al., 2017, Chen et al., 2020, Behnke and Heafield, 2020].\nFurther work attempted to take this work a step further by designing methods to attribute more specific model behaviors to individual model components. Elhage et al. [2021] conducted a detailed analysis of the types of circuits that appear in zero, one, and two layer transformers. Chintam et al. [2023] identified components of transformer-based LMs for gender bias. Nanda et al. [2023b] reverse engineers the algorithm implemented by a one layer transformer for modular addition. While the types of behaviors exhibited by a given model can be large and diverse, the workflow to discover circuits share many similarities. Conmy et al. [2023] outlines a typical circuit discovery workflow for many ML interpretability pipelines and proposes a framework automate the workflow."}, {"title": "Knowledge Extraction", "content": "Knowledge extraction in language modeling is the practice of discovering what information is embedded in a LM's weights. The practice of knowledge extraction has grown in popularity as it became better known that LMs can be treated as successful knowledge stores. For example, Roberts et al. [2020] showed that by fine-tuning an LM on question answering tasks, the LM was able to successfully perform question-answering in a closed-book setting. This finding implied that models were good at storing knowledge during training and retrieving knowledge during inference time. In the context of language modeling, knowledge extraction is useful because it illuminates what a model knows well and what information it might be lacking. This enables ML developers to stage the appropriate interventions to improve model performance on desired tasks (e.g., further fine-tuning, knowledge editing). A simple method to extract model knowledge is to prompt the model and observe the outputs. In a closed-loop model prompting scenario the model would have to rely on its internal knowledge store in order to appropriately respond to a prompt. Therefore, based on the model outputs for any given prompt, the prompter would be able to infer what information the model is storing in its weights. Petroni et al. [2019], Jiang et al. [2020] both design prompting strategies to elucidate what knowledge is contained in LMs. The immediate shortcoming with prompting a model to elicit information is that models will only output information that it deems relevant to the prompt. Therefore, vanilla prompting knowledge extraction strategies may fail to uncover a model's full breadth of knowledge. It is often challenging to come up with a comprehensive prompting scheme to enable a model to exercise its entire knowledge store. To combat this, researchers have also devised more rigorous knowledge extraction techniques:\n\u2022 Cohen et al. [2023b] proposes a strategy to extract a knowledge-graph (KG) of facts from a LM: given a seed entity, they \u201ccrawl\u201d the KG via prompts that are designed for both precision and recall.\n\u2022 Zhong et al. [2021] demonstrated that by training probes, rather than using discrete prompts, to illicit knowledge from LMs, they were able to tighten the lower bound on knowledge extraction benchmarks like LAMA [Petroni et al., 2019].\n\u2022 Elazar et al. [2021] proposed a novel framework to assess if facts known to a LM are generalizable. By using a paraphrasing technique, they show that models are often inconsistent in reporting facts thus implying they do not contain generalizable facts."}, {"title": "Memorization", "content": "Memorization is an undesirable phenomena observed in LMs: models can be prompted to output their training data verbatim [Feldman and Zhang, 2020]. Eliciting memorized data from a LM can be viewed as a subset of general purpose knowledge extraction tasks. However, while many forms of knowledge extraction are for benign purposes, such as gauging and improving a model's knowledge base, memorized data extraction can have harmful consequences. Memorized data can contain sensitive and/or private data that should not be recoverable by a model prompter. Dataset extraction attacks aim to prompt a model in a manner such that the model regurgitates its training data. Carlini et al. [2021] proposed one of the first training data extraction attacks from LMs. Nasr et al. [2023] designed a black-box model prompting scheme to extract training data from LMs. Many works have attempted to better understand the causes of memorization. Kandpal et al. [2022] finds that deduplicating text may result in models memorizing less training data. A recent work by Carlini et al. [2023] finds that there are 3 main reasons for memorization: 1) larger model scale, 2) data duplication, 3) larger input context length and attempts to quantify how much of a model's pre-trained data is memorized."}, {"title": "Language Model Reasoning", "content": "Huang and Chang [2022] defines reasoning as \u201ca cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments.\u201d Reasoning has been studied as an aspect of human behavior in fields like psychology [Wason and Johnson-Laird, 1972] and philosophy [Passmore, 1961]. With the recent advances in conversation-based language modeling [Brown et al., 2020, Chowdhery et al., 2023, Chung et al., 2022, OpenAI, 2022, inter alia], researchers have begun to investigate the possibility of reasoning skills emerging in models. LMs have been showed to exhibit emergent behaviors, including the ability to \u201creason\u201d, as their architecture sizes increase [Wei et al., 2022a]. Reasoning is measured in LMs by evaluating them on end task performance. Examples reasoning task include:\n\u2022 Arithmetic reasoning: the ability to apply mathematical concepts to solve problems. Examples of arithmetic reasoning benchmarks are GSM8k [Cobbe et al., 2021], Math [Hendrycks et al., 2021], MathQA [Amini et al., 2019], SVAMP [Patel et al., 2021], ASDiv [Miao et al., 2021], AQuA [Ling et al., 2017], and MAWPS [Roy and Roth, 2016].\n\u2022 Common Sense reasoning: the ability to use commonly known knowledge to make decisions in unknown situations. Examples of commonsense reasoning benchmarks are CSQA [Talmor et al., 2018], StrategyQA [Geva et al., 2021a], and ARC [Clark et al., 2018].\n\u2022 Multi-hop reasoning: the ability to synthesize related facts for answer questions with answers require many dependencies. Examples of multi-hop reasoning benchmarks include 2WikiMultiHopQA [Ho et al., 2020], and HotpotQA [Yang et al., 2018].\nTo elicit reasoning abilities from pre-training LMs, much work demonstrated notable performance gains via new prompting strategies. For example, Wei et al. [2022b] demonstrated that using a chain-of-thought prompting paradigm greatly improved reasoning abilities in LM. Follow up work from Wang et al. [2023c] introduced the importance of self-consistency in chain-of-though prompting scenarios. Following these works, many works have innovated on the \"x-of-thought\u201d prompting paradigm [Yao et al., 2023, Besta et al., 2023, Sel et al., 2023]. As interest in eliciting reasoning abilities from LMs grew, so did interest in understanding how LMs conducted reasoning. Researchers have tried to explain how models seem to \u201creason\u201d. Geva et al. [2021b] finds that feed-forward layers in LMs act as knowledge stores which can be queried by the model when certain input prompts require additional knowledge. Geva et al. [2023] reverse engineers how transformers are able to recall facts. Hou et al. [2023] posits that models \u201creason\" by building internal tree-like representations of multi-hop reasoning processes."}, {"title": "Retrieval Augmented Generation", "content": "Retrieval Augmented Generation (RAG) is a method of supplementing LMs with external sources of information as they respond to prompts. Lewis et al. [2020] studied RAG in the context of improving a LM's question answering (QA) ability: to enhance a LM's QA ability, the authors trained a neural retriever model that was able to traverse a vector database of Wikipedia articles and select the appropriate article to supply the the LM in conjunction with its input prompt. The authors demonstrated that LM's with RAG were able to outperform vanilla LM's in open domain QA tasks. In addition to enhanced QA ability, RAG boasts many benefits. Ovadia et al. [2023] demonstrated that RAG outperformed conventional fine-tuning of model weights when encountering both knowledge seen during training and new knowledge. This meant that RAG was better at introducing new knowledge to LMs. For example, if a model was trained in year 2021 and it is desirable for this model to be able to answer questions about news events in 2022, it would be beneficial to use RAG (rather than vanilla fine-tuning) to introduce 2022 news articles to the model to enable it to answer questions about it. A recent survey by Gao et al. [2023] reported that:\n\u2022 RAG improved model interpretability, as model responses can be attributed to specific data sources.\n\u2022 RAG models inherently may have a greater breadth of knowledge, due to the external knowledge database being vast. Vanilla LMs are constrained by the fact that all of their knowledge must be able to be compressed into their weight space during training.\n\u2022 Model inference using RAG would increase latency (due to the retrieval step) and thus may be constrained by computational resources. However, RAG does not have the same fine-tuning computational costs that vanilla LM's do, therefore it is beneficial to do a case-by-case analysis when considering cost of RAG.\nIn addition to QA, many works have explored unique applications for RAG in language modeling. Liu et al. [2020] demonstrated the use of RAG in the context of code summarization. Chen et al. [2022] developed a pipeline to augment a text-to-image model with a multi-modal database (image, text) pairs to enhance image generation capabilities. Komeili et al. [2021] augmented dialogue based LM with the ability to do internet search queries and showed superior dialogue performance.\nRAG is a promising technology with which to augment language modeling abilities, and many opportunities for innovation exist: How do we retrieve the most useful information? How do we best encode this information before supplying it to an LM?"}, {"title": "MEMORY INJECTIONS", "content": "Transformer-based Large Language Models (LMs) [Vaswani et al., 2017, Brown et al., 2020] have shown exceptional promise for basic knowledge retrieval and language generation; however, they often lack the ability to perform basic reasoning tasks [Arkoudas, 2023, Guo et al., 2023, Blair-Stanek et al., 2023]. In this work, we focus on the simple task of answering multi-hop prompts (i.e., prompts in which the subject is not stated explicitly), which humans handle easily but with which LMs often struggle (see Fig. 3.1).\nResearchers have attempted to rectify multi-hop reasoning failures by using various prompting methods such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT) reasoning [Wei et al., 2022b, Wang et al., 2023c, Long, 2023, Xie et al., 2023b, Yao et al., 2023, Besta et al., 2023]. However, these approaches often put the burden on users to know how to elicit desired responses\u2014and, in the hands of non-expert users, can lead to unreliable prompt completions. Researchers have also proposed model editing [Meng et al., 2022a,b, Zhong et al., 2023, Li et al., 2023] approaches that may hard-code distant relationships directly into model weights, rather than enhancing the model's abilities to recall and then link simpler relationships. These approaches can be computationally expensive and have unintended effects on other knowledge originally embedded in the model's weights [Cohen et al., 2023a].\nOur approach to this problem is based on the hypothesis that LMs often fail to recall relevant memories when attempting to answer a prompt that requires multiple \"hops\" of reasoning, rather than lacking knowledge of the memories altogether. For example, when attempting to complete the multi-hop prompt, \u201cThe largest coral reef system in the world is located off the coast of. . . ,\u201d we hypothesize that the model does not correctly recall that \u201cthe models it is attention heads, rather than multi-layer perceptrons, that are responsible for retrieving memories critical to successful model predictions; our finding is further substantiated by similar findings by Li et al. [2023], Geva et al. [2023], Dar et al. [2022]. We then study instances in which this mechanism fails in multi-hop reasoning tasks and find that this mechanism is likely the source of incorrect, insufficient, or irrelevant memory retrievals (Contribution 1)\u2014for an example, see Fig. 3.2.\nWe then propose a lightweight memory injection method that can be employed to correct a multi-hop reasoning failure during inference (Contribution 2). As an example: by employing our method to inject the memory of \u201cThe Great Barrier Reef\u201d into the multi-hop prompt \"The largest coral reef system in the world is located off the coast of...\" during inference, we increase the probability of the next token \u201cAustralia\u201d by 189%; refer to Fig. 3.3 for details.\nFor our analyses, we hand-crafted a dataset for interpretabilty purposes (Contribution 3) and make use of a larger programmatically-generated dataset-refer Table 3.1 for more information.\nFinally we conduct additional experiments (Contribution 4) to:\n1. Identify the ideal layer and magnitude for the memory injection.\n2. Demonstrate the significance of curating prompt-specific memories for injection.\n3. Analyze if memories drawn from different parts of speech-namely, nouns, adjectives, adverbs, conjunctions, verbs\u2014behave differently during memory injection."}, {"title": "Background & Notation", "content": "We define single- vs. multi-hop prompts and provide a formal definition of the transformer model."}, {"title": "Multi-hop vs. single-hop prompts", "content": "We refer to a prompt as single-hop if the subject of the relation is stated explicitly in the prompt, and multi-hop otherwise. Multi-hop prompts refer to their subject in a way that requires an additional \u201chop\" or inference step. For example, consider the single-hop prompt, \u201cGeorge Washington fought in the...\" with a correct answer being \u201cRevolutionary War.\" In the analogous multi-hop prompt, \u201cThe first president of the United States fought in the...,\" a preliminary inference step is needed to identity of the first US president before predicting the next token. For additional examples of single- and mutli-hop prompts, see Table 3.2 in the appendix."}, {"title": "Transformer Architecture", "content": "We introduce a common notation for the components of the transformer-based language model architectures that are the focus of our analyses. Specifically, we focus on auto-regressive, decoder-only models. We adopt much of our notation from Elhage et al. [2021] and Geva et al. [2023].\nEmbedding Inputs\nAn input text is parsed into N distinct tokens $t_0,\\ldots,t_N$. Each token $t_i$ is then embedded as $x_i \\in \\mathbb{R}^d$ via an embedding matrix $W_E \\in \\mathbb{R}^{|V|\\times d}$, where V is the vocabulary and d is the hidden dimension.\nResidual Stream\nFollowing the embedding layer, all tokenized embeddings $x_i^0$ are passed through a series of residual blocks. The outputs of each residual block are added back into the model's residual stream denoted by $R^l (\\forall l \\in \\{1,...,L\\})$ where L is the number of layers in the LM.\nWe define the residual stream at layer l as:\n$R^l = [x_0^l,\\ldots,x_N^l]$, (3.1)\nwhere $x_i^l$ is the representation of token i at layer l. The residual stream is updated by its respective residual block $r^l$:\n$R^{l+1} = R^l + r^{l+1}$, (3.2)\nand the output of a residual block $r^l$ is:\n$r^l = a^l + m^l$, (3.3)\nwhere $a^l$ is the output of the Multi-Headed Self Attention (MHSA) layer and $m^l$ is the output of the Multi-Layer Perceptron (MLP). We define MHSA and MLP in the following sections.\nMulti-Headed Self Attention (MHSA)\nEach MHSA layer l is defined via four parameter matrices $W_Q^l, W_K^l, W_V^l, W_O^l \\in \\mathbb{R}^{d\\times d} (\\forall l \\in \\{1,\\ldots,L\\})$ and the hyperparameter H denotes the number of attention heads. Following Elhage et al. [2021] and Geva et al. [2023], we can further dissect our parameter matrices to better observe the relationship between unique sets of parameters and individual attention heads: $W_Q^{l,j},W_K^{l,j} \\in \\mathbb{R}^{d\\times d}$ and $W_V^{l,j},W_O^{l,j} \\in \\mathbb{R}^{d\\times d}$ for $j \\in [1,H]$. Now, we can define the output of each MHSA $a^l$ as the sum of all attention head outputs,\n$a^l = \\sum_{j=1}^{H}h^{l,j}$, (3.4)\nwhere $h^{l,j}$ is the output of the jth head in layer l:\n$h^{l,j} = A^{l,j} \\odot (R^{l-1}W_V^{l,j}) W_O^{l,j}$, (3.5)\n$A^{l,j} = softmax\\left(\\frac{(R^{l-1}W_Q^{l,j}) (R^{l-1}W_K^{l,j})^T}{\\sqrt{d/H}} \\odot M\\right)$, (3.6)\nwhere the softmax(\u00b7) is performed as a row-wise operation, $\\odot$ is the Hadamard product, and $M\\in \\{0,1\\}^{N\\times N}$ is an auto-regressive attention mask where masked token positions are set to 0.\nMulti-Layer Perceptron (MLP)\nEach MLP is defined via two parameter matrices $W^l_f, W^l_o \\in \\mathbb{R}^{d\\times d_p}$ with inner-dimension $d_p$ and a nonlinear activation function, $\\sigma$.\n$m^l = W_o^l \\sigma (W_f^l (a^l + R^{l-1}))$ (3.7)\nUnembedding Predictions into Logits\nAfter the final residual block, all token positions $x_i^{L-1}$ will be projected back into the vocabulary domain via the unembedding matrix $W_U \\in \\mathbb{R}^{d\\times |V|}$. The output of the last token position is the next token prediction of the model."}, {"title": "Experimental Overview", "content": "Our central aim is to better understand how the outputs of the attention heads affect model performance with respect to predicting the correct next token in prompts requiring single-hop reasoning versus in prompts requiring multi-hop reasoning."}, {"title": "Dataset Descriptions", "content": "We employ three datasets in this work. Two, used to assess model prompt completion accuracy, are our own high-quality manually curated dataset of single and multi-hop pairs and a programmatically generated dataset of prompt pairs. The third comprises lists of words from common parts of speech, which we use to study how the effectiveness of our intervention varies with the part of speech of injected tokens.\nProgrammatically Generated Dataset\nThe 2WikiMultiHop dataset [Ho et al., 2020] contains pairs of knowledge triples $\\{(s_1, r_1, s_2)_1, (s_2, r_2, s_3)_2\\}$, each with two subjects s and a relationship r. We used these knowledge triples, plus a set of predefined templates, to generate a set of pairs of single- and multiple-hop questions, 2WMH: see Tables 3.1 and 3.2.\nFor example, let $s_1$ = \"Lilli's Marriage,\u201d $r_1$ =\u201cdirector,\u201d $s_2$ = \u201cJaap Speyer,\" $r_2$ = \u201ccountry of citizenship,\" $s_3$ = \"Dutch.\" Then for single-hop, the template: \"The $r_2$ of $s_2$ is ... $s_3$\u201d, the prompt yields the prompt \u201cThe country of citizenship of Jaap Speyer is [Dutch]", "The $r_2$ of the $r_1$ of $s_1$ is ... $s_3$\" yields then the prompt: \u201cThe country of citizenship of the director of Lilli's Marriage is . . . [Dutch].\"\n    },\n    {\n      \"title\": \"Human-Generated Dataset\",\n      \"content\": \"As evidenced by the example presented above, the 2WMH dataset, while scalable, contains many grammatical flaws. Therefore, we construct an additional dataset for multi-hop reasoning with a focus on grammatical and factual correctness presented below. We hand-crafted 106 (single-hop, multiple-hop) prompt pairs, each in the same form as those in 2WMH: e.g., single-hop: \u201cSt. Peter's Basilica is in the city of. . . [Rome]": "nd multi-hop: \u201cThe biggest church in the world is in the city of. . . [Rome]"}, {"title": "Model Description", "content": "We work with two pretrained GPT2 models [?]. GPT2-Small has 12 layers, 12 attention heads per attention layer, and ~160M parameters. GPT2-Large has 36 layers, 20 attention heads per attention layer, and ~840M parameters. Both have a vocabulary of ~50K tokens."}, {"title": "Tools & System Setup", "content": "We use the Transformer Lens Python package [Nanda and Bloom, 2022] to cache, inspect, and construct interventions on model inference passes. We ran experiments on a single A100 GPU with 40 GB RAM. Experimental code, dependency information, and datasets are available on GitHub.1"}, {"title": "Proposed Methods", "content": "Recent work suggests that attention heads are knowledge retrievers during a model's inference pass [Geva et al., 2023, Li et al., 2023]. Extending this result to multi-hop prompts, we hypothesize that attention layers play an important role in retrieving memories relevant to the \"hop\" in a given prompt. Therefore we define two algorithms below: one for analyzing attention head outputs in embedding space and the other for injecting a targeted memory into a model's hidden activations in order to correct faulty/incomplete reasoning."}, {"title": "Interpreting Attention Heads", "content": "We want to further understand the outputs of individual heads", "2021": "we leverage the model's unembedding matrix to study the internal mechanism of each attention head. For attention head j in layer l", "space": "n$vocab"}]}