{"title": "Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision", "authors": ["Dario Zanca", "Andrea Zugarini", "Simon Dietz", "Thomas R. Altstidl", "Mark A. Turban Ndjeuha", "Leo Schwinn", "Bjoern M. Eskofier"], "abstract": "Understanding human attention is crucial for vision science and AI. While many models exist for free-viewing, less is known about task-driven image exploration. To address this, we introduce CapMIT1003, a dataset with captions and click-contingent image explorations, to study human attention during the captioning task. We also present NevaClip, a zero-shot method for predicting visual scanpaths by combining CLIP models with NeVA algorithms. NevaClip generates fixations to align the representations of foveated visual stimuli and captions. The simulated scanpaths outperform existing human attention models in plausibility for captioning and free-viewing tasks. This research enhances the under-standing of human attention and advances scanpath prediction models.", "sections": [{"title": "1 Introduction", "content": "Motivations & Related Work. Visual attention is an essential cognitive pro-cess in human beings, enabling selective processing of relevant portions of visual input while disregarding irrelevant information [10, 14]. Models of human atten-tion are of great interest in neuroscience and applications [5], particularly for the case of scanpath prediction as it provides a more detailed understanding of visual attention dynamics compared to saliency prediction [1, 2]. Seminal work [4, 18] investigated the relationship between eye movement patterns and high-level cog-nitive factors to demonstrate the influential role of task demands on eye move-ments and visual attention. Despite great interest in understanding mechanisms underlying task-driven visual exploration, most current classical [3, 7, 13, 20] or"}, {"title": "2 CapMIT1003 Dataset", "content": "We developed a web application that presents images from MIT1003 to partic-ipants and prompts them to provide captions while performing click-contingent image explorations, using the protocol defined in [8]. A click will reveal informa-tion in an area corresponding to two degrees of visual angle, to simulate foveated vision. Participants can click up to 10 times, before providing a caption. We in-structed users to describe the content of the image with \"one or two sentences\", in English. All clicks and captions are stored in a database, while no personal information has been collected. In total, we collected 27865 clicks on 4573 obser-vations over 153 sessions. We excluded 654 observations that were skipped, 33"}, {"title": "3 NevaClip Algorithm", "content": "An overview of the NevaClip algorithm is given in Figure 1. For each image $I_i$, a scanpath of user clicks $\\xi_i$ and a caption $T_i$ are collected with a web interface.\nLet $\\tilde{I}_i$ be a blurred version of the image $I_i$. To predict the next fixation, i.e., $\\xi_{i,t}$, with $t > 0, t \\in \\mathbb{N}_0$ (non-negative integer), NevaClip's attention mechanism combines past predicted fixations $\\{f_{i,j} | j < t\\}$ with an initial guess for $f_{i,t}$ to create a foveated version of the image,\n\n$\\pi(I_i, t) = \\begin{cases}\n\\lambda \\pi(I_i, t-1) + (1-\\lambda) [G_{\\sigma}(f_{i,t}) \\cdot \\tilde{I}_i] & \\text{if } t > 0\\\\\n\\lambda \\pi(I_i, t-1)  & \\text{if } t = 0\n\\end{cases}$\n\nwhere $0 < \\lambda < 1$ is a forgetting factor and $G_{\\sigma}(f_{i,t})$ is a Gaussian blob, with mean $f_{i,t}$ and standard deviation $\\sigma$. This image $\\pi(I_i, t)$ is passed to CLIP's vision embedding network $f_v$ to obtain an image embedding $e_v^\\pi$. The cosine similarity loss $S(e_v^\\pi, e_c)$ between $e_v^\\pi$ and the text embedding $e_c$ obtained by passing the caption to CLIP's text embedding network $f_t$ is then computed as a measure of how well the current scanpath aligns with the given caption. By backpropagating this loss $M$ times, and updating the value of $f_{i,t}$ accordingly to minimize such loss, NevaClip finds the fixation $\\xi_{i,3}$ that minimizes this alignment loss."}, {"title": "4 Experiments", "content": "Experimental setup. We adhere to the original implementation of the Neva algorithm [17] with 20 optimization steps and adapt it to run on CLIP backbones. Code is made publicly available 6. We use the implementation of the CLIP model provided by the original authors7. Unless stated otherwise, all NevaClip results are presented for the Resnet50 backbone. For all competitor models, the original code provided by the authors is used.\nBaselines and models. We evaluate three distinct variations of our Neva-Clip algorithm. For NevaClip (correct cap.), scanpaths are generated maximizing the alignment between visual embeddings of the foveated stimuli, and text em-beddings of the corresponding captions. For NevaClip (different caption, same image), scanpath are generated to maximize the alignment between visual em-beddings of the foveated stimuli, and text embeddings of the captions provided by a different subject on the same image. For NevaClip (different caption, differ-ent image), scanpaths are generated to maximize the alignment between visual"}, {"title": "5 Conclusion", "content": "As expected, capturing attention in captioning tasks proves to be more chal-lenging compared to free-viewing, as it involves the simultaneous consideration of multiple factors such as image content, language, and semantic coherence. This is reflected in the results, where all approaches achieve lower performance"}]}