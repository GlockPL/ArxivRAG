{"title": "Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision", "authors": ["Dario Zanca", "Andrea Zugarini", "Simon Dietz", "Thomas R. Altstidl", "Mark A. Turban Ndjeuha", "Leo Schwinn", "Bjoern M. Eskofier"], "abstract": "Understanding human attention is crucial for vision science and AI. While many models exist for free-viewing, less is known about task-driven image exploration. To address this, we introduce CapMIT1003, a dataset with captions and click-contingent image explorations, to study human attention during the captioning task. We also present NevaClip, a zero-shot method for predicting visual scanpaths by combining CLIP models with NeVA algorithms. NevaClip generates fixations to align the representations of foveated visual stimuli and captions. The simulated scanpaths outperform existing human attention models in plausibility for captioning and free-viewing tasks. This research enhances the understanding of human attention and advances scanpath prediction models.", "sections": [{"title": "1 Introduction", "content": "Motivations & Related Work. Visual attention is an essential cognitive process in human beings, enabling selective processing of relevant portions of visual input while disregarding irrelevant information [10, 14]. Models of human attention are of great interest in neuroscience and applications [5], particularly for the case of scanpath prediction as it provides a more detailed understanding of visual attention dynamics compared to saliency prediction [1, 2]. Seminal work [4, 18] investigated the relationship between eye movement patterns and high-level cognitive factors to demonstrate the influential role of task demands on eye movements and visual attention. Despite great interest in understanding mechanisms underlying task-driven visual exploration, most current classical [3, 7, 13, 20] or"}, {"title": "2 CapMIT1003 Dataset", "content": "We developed a web application that presents images from MIT1003 to participants and prompts them to provide captions while performing click-contingent image explorations, using the protocol defined in [8]. A click will reveal information in an area corresponding to two degrees of visual angle, to simulate foveated vision. Participants can click up to 10 times, before providing a caption. We instructed users to describe the content of the image with \"one or two sentences\", in English. All clicks and captions are stored in a database, while no personal information has been collected. In total, we collected 27865 clicks on 4573 observations over 153 sessions. We excluded 654 observations that were skipped, 33"}, {"title": "3 NevaClip Algorithm", "content": "An overview of the NevaClip algorithm is given in Figure 1. For each image Ii, a scanpath of user clicks \u00a7\u00a1 and a caption Ti are collected with a web interface.\nLet \u00ce; be a blurred version of the image I\u00a1. To predict the next fixation, i.e., \u00a7i,t, with t > 0, t \u2208 No (non-negative integer), NevaClip's attention mechanism combines past predicted fixations {fi,j | j < t} with an initial guess for fit to create a foveated version of the image,\n$\\pi (I_i, t) = \\begin{cases} \\lambda \\pi (I_i, t - 1) + (G_{\\sigma}(\\xi_{i,t}) \\cdot I_i + (1 - G_{\\sigma}(\\xi_{i,t}) \\cdot \\hat{I}_i)] & \\text{if } t = 0 \\\\ \\lambda \\pi (I_i, - 1) & \\text{if } t > 0 \\end{cases}$ (1)\nwhere 0 < x < 1 is a forgetting factor and Go(fi,t) is a Gaussian blob, with mean it and standard deviation \u03c3. This image (I\u00a1, t) is passed to CLIP's vision embedding network fo to obtain an image embedding e\". The cosine similarity loss S(e\", eC) between e\" and the text embedding e obtained by passing the caption to CLIP's text embedding network fo is then computed as a measure of how well the current scanpath aligns with the given caption. By backpropagating this loss M times, and updating the value of fi,t accordingly to minimize such loss, NevaClip finds the fixation \u00a7\u00a1,3 that minimizes this alignment loss."}, {"title": "4 Experiments", "content": "Experimental setup. We adhere to the original implementation of the Neva algorithm [17] with 20 optimization steps and adapt it to run on CLIP backbones. Code is made publicly available 6. We use the implementation of the CLIP model provided by the original authors7. Unless stated otherwise, all NevaClip results are presented for the Resnet50 backbone. For all competitor models, the original code provided by the authors is used.\nBaselines and models. We evaluate three distinct variations of our Neva- Clip algorithm. For NevaClip (correct cap.), scanpaths are generated maximizing the alignment between visual embeddings of the foveated stimuli, and text embeddings of the corresponding captions. For NevaClip (different caption, same image), scanpath are generated to maximize the alignment between visual embeddings of the foveated stimuli, and text embeddings of the captions provided by a different subject on the same image. For NevaClip (different caption, differ- ent image), scanpath are generated to maximize the alignment between visual"}, {"title": "5 Conclusion", "content": "As expected, capturing attention in captioning tasks proves to be more challenging compared to free-viewing, as it involves the simultaneous consideration of multiple factors such as image content, language, and semantic coherence. This is reflected in the results, where all approaches achieve lower performance"}]}