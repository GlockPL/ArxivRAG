{"title": "APPLYING SPARSE AUTOENCODERS TO UNLEARN KNOWLEDGE IN LANGUAGE MODELS", "authors": ["Eoin Farrell", "Yeu-Tong Lau", "Arthur Conmy"], "abstract": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Current and future language models may learn inaccurate information, produce toxic or malicious outputs, or possess dangerous capabilities that we would like to remove before deployment, such as advanced bio-weapons knowledge (Ji et al., 2023; Li et al., 2024). However, we do not yet know how to precisely and robustly remove knowledge or unlearn capabilities in these language models. The goal of this work is to investigate whether sparse autoencoders (SAEs) can be used to perform unlearning in an interpretable way.\nRecent work on unlearning has typically focused on fine-tuning based methods that have been applied in a variety of contexts to unlearn concepts in language models (e.g. Li et al., 2024; Zou et al., 2024; Eldan & Russinovich, 2023), going beyond prior work that aimed to unlearn specific training data points in neural networks (Bourtoule et al., 2020). While relatively successful, these fine-tuning approaches are opaque and we lack insight into what exactly is happening in the model (\u0141ucki et al., 2024). Existing methods for removing specific facts from language models offer interpretable solutions (e.g. Meng et al., 2023), however these approaches are limited to fact-level unlearning. Having an interpretable method for unlearning is important as it can allow a higher level of confidence that the model has actually unlearned the knowledge, rather than superficially or temporarily hiding the capability to discuss a given topic. One possibility is to use sparse autoencoders to try to unlearn knowledge in an interpretable way.\nSparse autoencoders (SAEs) use an unsupervised method to learn sparse reconstruction of language model activations (e.g. Ng, 2011; Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024; Marks et al., 2024; Gao et al., 2024). SAEs have been shown to find interpretable features in language models. SAEs appear to be a promising approach to help understand complex, abstract features that are used by language models (Templeton et al., 2024). Whether SAEs can be used to make systematic, predictable, interpretable interventions in language models in a variety of contexts remains an open question.\nOur work makes two key contributions: First, we attempt to develop a method for unlearning knowledge in language models in an interpretable way. Second, we apply SAEs to the task of unlearning knowledge, extending their use beyond previous work. Our approach aims to work towards more transparent and verifiable knowledge removal at a broader scale."}, {"title": "2 METHOD", "content": "Figure 1 presents an outline of how we use the SAE features to intervene in the language model. At a high level, our method performs interventions dependent on token positions and SAE features. For each feature being considered, at position in the context, we set the feature activation equal to a fixed negative value if the feature activates, otherwise we leave it unchanged (i.e. equal to 0). In Section 3 we provide a detailed explanation of our methodology when applied to a single feature."}, {"title": "2.1 MODELS AND DATASET", "content": "We focus primarily on two language models, gemma-2b-it (Gemma Team, 2024a) and gemma-2-2b-it (Gemma Team, 2024b). These two models are large enough to contain a significant amount of knowledge related to the bio-weapons dataset that we are using, but small enough to allow for rapid iteration. We trained SAEs at several intermediate layers of the residual stream in gemma-2b-it using similar techniques to Templeton et al. (2024). The training code is available here. We also used open-source SAEs Lieberum et al. (2024) on gemma-2-2b-it. Further details are available in Sec. B.\nWe use the biology subset of the Weapons of Mass Destruction Dataset (WMDP-bio), which consists of 1273 multiple choice questions related to hazardous knowledge in biosecurity (Li et al., 2024). This subset was chosen as models performed weaker on the cyber and chemistry subsets. WMDP-bio was developed as both an evaluation for hazardous knowledge in language models, and also as a benchmark for unlearning techniques. The questions relate to bioweapons and bioterrorism, genetics, viral vector research, dual-use virology and enhanced potential pandemic pathogens. gemma-2b-it achieves a base score of 560/1273 (44.0%) on the WMDP-bio dataset. With four multiple choice options, one can expect the model to get ~ 25% of the multiple choice questions correct by random chance, without actually having the knowledge to obtain the correct answer. However, as we aim to unlearn this information, we want to be as sure as we can that the model actually has the information in the first place. To address this, we only test unlearning on questions for which the model gets the right answer under all 24 permutations of the 4 multiple choice options, resulting in 172/1273 (13.5%) questions in the WMDP-bio dataset for gemma-2b-it and 522/1273 (41.0%) questions for gemma-2-2b-it."}, {"title": "2.2 UNLEARNING METRICS", "content": "The goal of an unlearning technique is to remove knowledge from the model, while limiting the damage to the model's performance in all other domains. Therefore, there are two key factors that need to be quantified; the amount of knowledge removed, and the side-effects caused by the modification to the model.\nThe primary metric for unlearning that we consider is the number of correct answers out of the subset of questions that the base model gets correct under all 24 permutations. We also looked at the probabilities assigned to the correct answers and the impact of re-ordering the 4 multiple choice options. In addition, we performed some specific case studies of the model's completion, with non-multiple choice based prompts, based on the same information as tested in the questions.\nWe quantified the side effect for the model in two different ways, (i) the accuracy on an unrelated multiple choice dataset (Measuring Massive Multitask Language Understanding, MMLU) and (ii) the loss added over 50k tokens of OpenWebText. The multiple choice dataset contained questions related to high school US history, geography, college computer science and human aging. These questions allowed us to both check for the removal of specific unrelated knowledge, and also to ensure that we were not simply damaging the multiple-choice answering ability of the model. Similar to the WMDP-bio dataset, we only select the questions that the base model gets correct answers for under 24 permutations, resulting in 97 questions for gemma-2b-it and 300 questions for gemma-2-2b-it.\nWe also compare our unlearning results to an existing technique, the Representation Misdirection for Unlearning (RMU, Li et al. 2024). RMU is a fine-tuning based approach for unlearning information from a language model. It involves fine-tuning model weights at 3 layers within the model using a combination of two loss terms. These terms are (i) the \u201cforget loss\", which changes the direction and scales the norm of model activations on a dataset containing information that you want to unlearn and (ii) the \"retain loss\", which preserves model activations based on a Wikitext dataset. The method contains two hyperparameters that control the ratio of the two loss terms, and the scaling of the activations on the \"forget\u201d dataset. Applying the RMU technique to gemma-2b-it shows that it is very successful at lowering the number of WMDP-bio questions that are answered correctly, without impacting unrelated MMLU questions, and little loss added (more detailed results are presented in Fig. 5). Interestingly, the model modified using RMU answers option \"A\" on 62% of questions, compared to 25% for the base model.\""}, {"title": "2.3 INTERVENTION METHODS", "content": "We explored various approaches to intervene on the model using SAE features. Our primary intervention method involves clamping the feature activation to a specific negative value whenever it activates. We also experimented with alternative methods, including (i) scaling the feature activation by a constant negative multiplier, and (ii) clamping the activation to a multiple of the feature's maximum activation in the dataset, similar to the steering approach used by Templeton et al. (2024). We"}, {"title": "2.4 HOW TO SELECT RELEVANT SAE FEATURES", "content": "Selecting the most effective set of sparse autoencoder features is difficult. One simple approach is to create a \"forget\" and \"retain\u201d dataset, as in the RMU technique, and compute feature sparsities on each dataset. The WMDP-bio forget dataset (Li et al., 2024) contains bio-weapon related content, and the retain dataset contains WikiText (Merity et al., 2016). In our procedure, we first calculate the feature sparsities on both datasets. We then discard features that have a sparsity on the retain dataset larger than a given threshold (e.g. 0.01). Finally, we sort the remaining features by their sparsity on the forget dataset. The top N features are selected as the bio-weapon related features. In Fig. 8, we plot the sparsities of each feature in the retain and forget datasets for an SAE at layer 3 of the residual stream of gemma-2-2b-it."}, {"title": "3 UNLEARNING USING A SINGLE FEATURE", "content": "We first present a case study of using an individual bio-related feature for unlearning. This feature serves as a simple proof of concept that some unlearning can be achieved using a bio-related feature from an SAE trained on OpenWebText."}, {"title": "3.1 A REPRESENTATIVE FEATURE (#9163)", "content": "Figure 2 presents an example of a bio-related feature (Feature #9163) relevant to the WMDP-bio dataset at layer 9 of the residual stream (out of a total of 18 layers) in gemma-2b-it. The left panel shows the distribution of the activations of feature #9163 over 176k tokens in the WMDP-bio dataset (green) compared to the 176k tokens in OpenWebText (blue), and the corresponding max activating prompts. The feature is relatively monosemantic, and fires on text related to evolution, natural selection, selective pressure, mutations, fitness and adaptation, with a max activation of 10.88. In OpenWebText data, the feature fires on text related to similar topics, including adaptation, survival and advantages, with a max activation of 2.6."}, {"title": "3.2 IMPACT OF CLAMPING FEATURE ACTIVATION", "content": "To investigate the impact of clamping feature #9163, we select a representative question from the WMDP-bio dataset (question #841). Figure 3 shows the content of question #841 of the WMDP-bio dataset, with highlighted text indicating the strength of the activation of feature #9163 on each token within the prompt. Note that the content of the question is similar in topic to the typical max activations of feature #9163. It activates mainly on answer A, which is the correct answer for this question.\nWe now perform an experiment, where we clamp the activation of feature #9163 to negative values ranging from 0 to -200, and compute the probabilities assigned to answers A, B, C and D with"}, {"title": "3.3 PERMUTATIONS", "content": "We can also examine whether this unlearning is effective for different permutations of the options in the multiple choice prompt. In this particular case, we find that it unlearns 19 of the 24 prompts. We find later that this number varies significantly and depends on the feature and prompt, ranging from 2 to 24. A perfectly unlearned model should achieve a score of < 6 out of 24 permutations. This outcome might depend on whether the intervention simply destroys the knowledge that was previously available, in which case we might expect it to start guessing at random and not provide a consistent answer over all 24 permutations, or whether it exchanges the old information for new"}, {"title": "4 EVALUATION OF SAE UNLEARNING", "content": "information and thus has an incorrect answer but is consistent about which answer it chooses. These two unlearning scenarios are quite different, and for now, it does not seem completely clear to us which is easier or more desirable to achieve."}, {"title": "4.1 RESULTS", "content": "Below, we present our results for unlearning with multiple features on gemma-2-2b-it by selecting features using feature sparsities on both \"forget\" and \"retain\" datasets, and compare them to the existing RMU technique. We evaluate three key metrics. The first is the \u201cWMDP-bio Accuracy\" is the number of questions the modified model gets correct divided by the number of questions the original model gets correct, only considering questions which the original model gets correct for all permutations (i.e. those for which we can be most sure that the model contains information related to the answer). Lower WMDP-bio accuracy indicates a higher level of unlearning. The second is the cross-entropy loss in the modified model minus the cross-entropy loss in the original model, calculated over the same 50k tokens of OpenWebText. Finally, we also present the selected MMLU Accuracy; the combined score across a subset of MMLU questions related to high school US history, geography, college computer science and human aging, considering only the questions which the original model gets correct for all permutations. Higher MMLU accuracy suggests fewer side effects.\nFigure 5 presents the results for gemma-2-2b-it using a Gemma Scope SAE at layer 3 with 16k features and LO \u2248 59. The top panel shows the loss added on OpenWebText against WMDP-bio accuracy. Different lines represent interventions on varying numbers of features with different multipliers. Gray dots indicate RMU techniques with various hyper-parameters (with values for the steering coefficient, c, of 100/200/400, the weight on the retain loss, a, of 100/300/500, and layer 3/7/11). Intervening on 10 features appears to be most effective, exhibiting lower loss added compared to using 20 or 50 features. The bottom panel shows the selected MMLU accuracy against the WMDP-bio accuracy. Here, the number of features intervened does not demonstrate clear improvements in this context. Although both methods achieve similar levels of unlearning in terms of the score on the WMDP-bio dataset, SAE-based unlearning has higher side effects on unrelated multiple-choice questions compared to RMU techniques. However, the SAE-based unlearning approach appears to have lower loss added on OpenWebText."}, {"title": "4.2 IMPACT OF ENCODER VS DECODER IN UNLEARNING", "content": "Investigating the mechanism of the RMU technique, Arditi & Chughtai (2024) found that by projecting out a single direction from all intermediate activations, it is possible to recover a significant fraction of the original knowledge. They suggested that some of the unlearning with RMU is achieved by first detecting harmful prompts and then modifying the model's internal computation by adding a large vector that overwrites the existing residual streams.\nIn our SAE-based unlearning method, we inject a large vector (i.e. the decoder vector multiplied by a constant) when the selected features activate. It is possible that the SAE encoder acts as a classifier for the undesired knowledge, the model's internal representations are pushed out of distribution by the injection of the decoder vector, and that this explains the decrease in accuracy on multiple-choice questions.\nTo test this possibility, and understand the impact of the SAE encoder and decoder in unlearning, we used the original SAE encoder to determine feature activations of the selected features. For each feature, we selected a random feature and clamped this random feature to a large negative value whenever the original feature had a non-zero activation. Figure 6 shows the results on gemma-2-2b-it. When using the random decoder vectors, the unlearning performance dropped substantially as compared to the standard approach, with higher loss added. This suggests that the specific decoder vectors contribute significantly to the unlearning process, rather than simply disrupting the model's internal representations. Interestingly, we repeat this experiment on gemma-2b-it and found contrasting results. Replacing the decoder vectors with some random decoder vectors did not diminish the unlearning performance, suggesting that for this model and SAE, the specific directions of the injected vectors may not be as critical (see Figure 10 in Appendix)."}, {"title": "5 CONCLUSION", "content": "In this study, we investigated whether features in SAEs can be used to unlearn knowledge from language models. Our main contributions are:\n1. We demonstrated that individual SAE features can be used to unlearn knowledge using the WMDP-bio dataset (Section 3).\n2. Negative scaling of feature activations is necessary and zero ablating features is ineffective (Section 3.2).\n3. We showed that intervening on gemma-2-2b-it using 10 - 20 SAE features could unlearn significant proportions of the WMDP-bio dataset (Section 4)\n4. We found that SAE-based unlearning results in similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique (Section 4).\nOur SAE-based unlearning method only modifies the model's activations, without modifying the model weights. Future work may be able to extend this approach to directly modify the weights. For now, it means that only the output of the model is changed and the original knowledge is still contained in the weights. However, this approach may still be useful to to prevent harmful outputs when models can be accessed only by API (Li et al., 2024; Greenblatt & Shlegeris, 2024).\nOur research suggests that a large change needs to be made to either the SAE training process, or our intervention method, in order for SAE-based unlearning to be successful. We propose three key directions for future research to address this shortcoming: (1) Examine how SAE quality and width impact unlearning effectiveness. Wider SAEs with specific features may improve performance. (2) Compare SAE-based unlearning to activation steering using bio-related and non-bio-related prompt pairs. (3) Investigate how SAE interventions affect related features in subsequent layers to better understand the unlearning mechanism."}, {"title": "F ALTERNATIVE METHOD USING FEATURE ATTRIBUTION", "content": "As an alternative to our procedure for selecting features based on sparsities in the \"forget\" and\n\"retain\" datasets, we tested an approach based on feature attribution. This approach consists of the\nfollowing steps:\n1. For a given question in the unlearning dataset, we calculate the correct logit minus the mean\nof the incorrect logits. Using backpropagation, we calculate the gradient of this quantity at\nthe intervention layer, dot product with each SAE feature decoder direction, and multiply\nby the feature activation. We do this for each position in the prompt, excluding certain\nspecial tokens such as \u201cA\u201d, \u201c:\u201d, or newline tokens.\n2. We take the top 20 features by feature attribution and check whether they impact the pre-\ndicted answer by clamping the feature activation to a constant negative value. After some\nexperiments, we used a value of -20.\n3. We repeat this for all questions that the base model gets correct under all permutations.\n4. To further filter out multiple choice related features, we test the model\u2019s performance on\na set of unrelated MMLU multiple-choice questions and remove features that result in too\nmany wrong answers (usually set at about 0 \u2013 3 out of 300).\n5. Finally, we find it useful to check the features for loss added as often 3 \u2013 5% of features\nadd a huge amount of loss for no marginal gain in unlearning capability).\nWe used this selection technique on both the entire dataset, and also split the dataset into a test/train\nset, where we only select the features based on a subset of the WMPD-bio questions and only test\nfor side effects on a subset of the MMLU questions and OpenWebText data. The feature attribution\ntechnique has the advantage that it doesn\u2019t require separate datasets to be available. On the other\nhand, with the feature sparsity method, we don\u2019t have to be as concerned about over-fitting to the\nspecific multiple-choice questions. On gemma-2b-it, we found that this attribution approach to\nbe slightly better than the feature sparsity approach at selecting features that produce fewer unwanted\nside-effects. However, the best approach to select features for unlearning remains unclear."}]}