{"title": "SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models", "authors": ["Jiawen Zhang", "Kejia Chen", "Zunlei Feng", "Jian Lou", "Mingli Song", "Jian Liu", "Xiaohu Yang"], "abstract": "With the growing popularity of LLMs among the general public users, privacy-preserving and adversarial robustness have become two pressing demands for LLM-based services, which have largely been pursued separately but rarely jointly. In this paper, to the best of our knowledge, we are among the first attempts towards robust and private LLM inference by tightly integrating two disconnected fields: private inference and prompt ensembling. The former protects users' privacy by encrypting inference data transmitted and processed by LLMs, while the latter enhances adversarial robustness by yielding an aggregated output from multiple prompted LLM responses. Although widely recognized as effective individually, private inference for prompt ensembling together entails new challenges that render the naive combination of existing techniques inefficient. To overcome the hurdles, we propose SecPE, which designs efficient fully homomorphic encryption (FHE) counterparts for the core algorithmic building blocks of prompt ensembling. We conduct extensive experiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of SecPE. The results show that SecPE maintains high clean accuracy and offers better robustness at the expense of merely 2.5% efficiency overhead compared to baseline private inference methods, indicating a satisfactory \u201caccuracy-robustness-efficiency\" tradeoff. For the efficiency of the encrypted ARGMAX operation that incurs major slowdown for prompt ensembling, SecPE is 35.4 times faster than the state-of-the-art peers, which can be of independent interest beyond this work.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have garnered a meteoric rise in popularity among general public users due to their remarkable performance across myriad natural language processing (NLP) tasks [36, 38]. LLMs are oftentimes deployed by service providers in the form of Machine Learning as a Service (MLaaS) [39, 23], whereby users can conveniently exploit the full potential of LLM by submitting their inference data, prepended by specific prompts from prompt learning techniques [18], to obtain high-performing LLM outputs tailored to their downstream tasks. Accompanying this widespread adoption, there arise privacy and robustness concerns for LLMs [13].\nPrivacy concerns and private inference. On the privacy aspect, users' inference data can inadvertently reveal sensitive information if transmitted and processed by the LLM service provider in plain-text [39, 23], risking identification and privacy breaches. Additionally, the user-submitted prompts can be valuable intellectual property and also raise privacy concerns. As a result, both inference data and user-side prompts demand privacy-preserving measures [13, 42]. Among the many attempts to avoid submitting raw data for LLM inference, private inference offers very strict privacy protection by allowing inference to be conducted on encrypted data. For instance, Fully Homomorphic Encryption (FHE) allows rich computations (covering most operations needed in LLM inference) on encrypted data without exposing sensitive information [9]. By encrypting in-puts using FHE, only encrypted predictions are sent to the server, ensuring privacy throughout the process. As legal and societal pres-sures mount, service providers' adoption of such privacy-preserving technologies has received increasing research attention.\nRobustness concern and prompt ensembling. On the robustness aspect, it is well-recognized that the output of LLMs can be ma-nipulated by subtle yet deliberate changes in the inference sample or the prompt [33]. There has been a growing focus on enhancing the robustness of LLMs, especially in safety-critical downstream ap-plication areas. Various methods have been proposed, ranging from more advanced (and sophisticated) to simple methods [7]. One repre-sentative method from the latter category follows the idea of prompt ensembling [25], which involves making multiple inferences for a single inference data and providing the aggregated result as the final prediction.\nThis study. The current research efforts on safeguarding privacy and robustness during LLM inference are largely explored separately. Driven by the simultaneous demands from both privacy and robust-ness aspects, we envision that these two aspects should be pursued jointly. Among the first attempts toward mitigating both concerns of LLMs jointly, we investigate the potential to achieve private and robust LLM inference through tight integration of private inference and prompt ensemble. We focus on these two techniques due to their effectiveness in addressing their respective concerns. In particular, we note that while there may be more advanced techniques for en-hancing robustness than prompt ensembling, achieving a balance be-tween robustness and efficiency within the private inference work-flow of the simpler prompt ensembling method already poses signif-icant challenges. That is, naive application of existing private infer-ence methods for prompt ensembling entails great efficiency over-head. The crux of efficient private inference for prompt ensembling is that the aggregation operation introduced by prompt ensembling, albeit simple and efficient in plaintext computation, requires pro-"}, {"title": "2 Background", "content": "2.1 Privacy Issues of LLMS\nLLMs such as the GPT have revolutionized natural language pro-cessing and understanding with human-level proficiency [14, 3]. However, with their increasing deployment in MLaaS by service providers and growing popularity among the general public users, there arise aggravating privacy concerns. In the typical MLaaS serv-ing setting, users submit inference data to the remote server hosting a proprietary model and receive predictions in return. Users therefore have privacy concerns about their inference data that, despite being sensitive or even confidential, are transmitted and processed in plain-text by the MLaaS service provider [28]. This issue has even led to ChatGPT being temporarily banned in Italy [19, 20]. Recogniz-ing this pressing privacy concern, existing works introduce various means to avoid direct transmission and processing inference data in plain text form.\nPrivate inference emerges as a viable solution, promising to rec-oncile the need for high-performant inference data processing with strict privacy requirements [30, 11, 21]. Private inference provides a way to guarantee the privacy and confidentiality of both the inference data and the proprietary LLM. It ensures that data is not transmitted or processed in plaintext but as ciphertext, thereby safeguarding sen-sitive details about the server's model weights and the user's inputs from disclosure. While private inference has significant applications in computer vision and image processing [41], its use in LLMs is nascent. Notably, the integration of private inference in prompt learn-ing settings and prompt ensembles remains an under-explored area, presenting a frontier yet to be ventured into the field.\nBy pursuing private inference tailored for prompt ensemble learn-ing, we aim to bridge the gap between utility, robustness, and privacy, thereby realizing the benefits of prompted LLMs without compro-mising user trust and data integrity.\n2.2 Fully Homomorphic Encryption\nThe FHE scheme used in this paper is the full residue number system (RNS) variant of Cheon-Kim-Kim-Song (CKKS) [5]. RNS-CKKS is a leveled FHE, which can support computations up to a multiplica-tive depth L. Both the plaintexts and ciphertexts of RNS-CKKS are elements in a polynomial ring:\n$R_Q = Z_Q[X]/(X^N + 1)$ \nwhere $Q = \\prod_{i=0}^{q} q_i$ with distinct primes $q_i$. Once a ciphertext's level becomes too low, a bootstrapping operation is required to re-fresh it to a higher level, enabling more computations. In a nutshell, bootstrapping homomorphically evaluates the decryption circuit and raises the modulus from $q_0$ to $q_L$ by leveraging the isomorphism $R_{q0} \\cong R_{q0} \\times R_{q1} \\times ... \\times R_{qL}$ [2]. Suppose the bootstrapping consumes K levels, then a fresh ciphertext can support L - K levels of computations.\nRNS-CKKS supports single instruction multiple data (SIMD), which enables encrypting a vector with N elements into a single ci-phertext and processing these encrypted elements in a batch without introducing any extra cost. Below, we summarize the homomorphic operations used in this paper:\n\u2022 $a + b$. The addition takes two SIMD ciphertexts a and b; outputs $[a_0 + b_0, a_1 + b_1..., a_{n-1} + b_{n-1}]$.\n\u2022 $a - b$. The subtraction takes two SIMD ciphertexts a and b; outputs $[a_0 - b_0, a_1 - b_1..., a_{n-1} - b_{n-1}]$.\n\u2022 $a \\times b$. The multiplication takes two SIMD ciphertexts a and b; outputs $[a_0 \\times b_0, a_1 \\times b_1..., a_{n-1} \\times b_{n-1}]$.\n\u2022 RotL(a, s). The left-rotation takes one SIMD ciphertext a and an integer s; left-rotates the vector by s slots.\n\u2022 RotR(a, s). The right-rotation takes one SIMD ciphertext a and an integer s; right-rotates the vector by s slots.\n2.3 Prompt Ensembling for Robust LLMs\nThe brittleness of LLMs to slight input modifications often leads to varied/inaccurate and sometimes even malicious/harmful out-puts, highlighting the essential need for enhanced robustness for"}, {"title": "3 Proposed Method: SecPE", "content": "We propose a new private inference framework tailor-made for the prompt ensembling. Private inference for prompt ensembling raises a critical, unaddressed issue: the challenge of integrating private con-textual inference. Incorporating privacy-preserving mechanisms into prompt ensembles remains a significant and complex challenge, de-spite progress in leveraging prompt-based learning to improve model effectiveness in downstream tasks. Our work aims to break new ground by developing a comprehensive framework that not only im-proves model performance through optimized prompt selection but also prioritizes the integration of robust privacy safeguards.\n3.1 SecPE Framework\nWe give an illustration of SecPE in Fig 2, the overall process is divided into the following four steps:\n1. Encryption. User encrypts m inputs $x_i = x_{in} x_{prompt}, i \\in [1, m]$ using FHE and sends them to the server, where m is the number of prompt templates.\n2. Private Language Model Inference. Server uses the language model L classifying m inputs into one of n classes, n is the num-ber of labels. the inputs are propagated through L utilizing the ho-momorphic operations of the FHE scheme to obtain m encrypted logits $y_i, i \\in [1, m]$.\n3. Private Voting. Server aggregates the encrypted logits $y^* \\leftarrow \\sum_{i=1}^{m} Y_i$ and then evaluates ARGMAX function in FHE. In partic-ular, this step transforms the logit vector $y^*$ into a one-hot vector z. Then the server sends z to the User.\n4. Decryption. User decrypts z with its secret key, where the single non-zero entry represents the index of the classification label.\nAs illustrated in the preceding workflow of SecPE, Steps 1 and 4 pertain to fundamental FHE encryption and decryption operations. Step 2 has been implemented across numerous recent works, includ-ing in [11, 21]. These three steps are orthogonal to the efficiency de-signs of prompt ensembling. The primary obstacle lies in leveraging FHE to access the ARGMAX operation in Step 3.\nIt is important to note that private voting can only be calculated by the server in ciphertext and cannot be handed over to the user in plaintext. This is because numerous works, including those by[29, 40, 37], have designed membership inference attacks based on the class probability distribution of the prediction vector. It is for this reason that the output of the final layer, commonly referred to as the logits, is generally considered to represent the raw confidence ratings associated with the predictions. These ratings are selected using the ARGMAX processing, whereby the one with the highest probability is selected from the available ratings. It is important to note that the simple act of returning these logits without the ARGMAX process"}, {"title": "3.2 Efficient Private Inference for Prompt Ensembling", "content": "As mentioned above, the design core of efficient private inference for prompt ensembling lies at the private aggregation operator, i.e., the ARGMAX operation.\nTherefore, our goal is to approximate the following function on an RNS-CKKS ciphertext logit vector:\n$[Y_1, ..., Y_n, 0_{N-n}] \\rightarrow [z_1, ..., z_n, #_{N-n}],$ (1)\nwhere $z_i = 1$ for the index i corresponding to the largest value among $[Y_1, Y_2,..., Y_n]$ (and 0 elsewhere).\nThe state-of-the-art non-interactive protocol that can achieve this goal is Phoneix [12]. Phoenix adopts the idea of bubble sorting to compare each element with adjacent elements by rotating the cipher-text and making a difference with the input:\n$S_1 \\leftarrow Sign(y - RotL(y, 1))$\n$S_2 \\leftarrow Sign(y \u2013 RotL(y, 2))$\n$S_m \\leftarrow Sign(y \u2013 RotL(y, m))$\n$Sos = \\sum_{i=1}^{m} s_i$ counts the comparison result among each input and adjacent elements. Obviously, the value of the maximum element position is m, and the values of other positions are less than m. After that, through simple linear transformation, z can be obtained based on s (cf. Phoenix [12] for details).\nHowever, this method requires (m+1) times Sign operations and (m + 1) times ciphertext rotations, which is very inefficient when m is large (e.g. m = 1024 in CLIP [22]). To solve the problem, we innovatively proposed an ARGMAX evaluation method as:\n$z_i \\leftarrow Sign(y_i- Y_{max}) + 1.$ (2)\nTo enable encrypted comparisons, we leverage the polynomial ap-proximation of the sign function:\n$Sign(x) = \\begin{cases} -1 & -1 < x < -2^{-t} \\ 0 & x = 0 \\ 1 & 2^{-t} < x < 1 \\ \\end{cases}$ (3)\nThe approximation involves a composition of polynomials:\n$Sign(x) = f_{d_f} (g_{d_g} (x))$ (4)\nwhere $f(), g()$ are two polynomials and $d_f, d_g$ are the number of repetitions for them. In our implementation, both $f()$ and $g()$ are 9-degree polynomials; we set $a = 12, d_f = 2, d_g = 2$, so the max error bound is less than $10^{-4}$. To reduce the multiplicative depth, we evaluate the polynomials using the Baby-Step-Giant-Step algo-rithm [10]."}, {"title": "4 Experiments", "content": "4.1 Experimental setup\nTasks and Datasets.\nIn the experiments, we utilize 8 tasks from popular benchmarks to thoroughly evaluate the utility, robustness, and efficiency of SecPE.\nI) Benign NLP tasks. We evaluate SecPE on six tasks from the GLUE benchmark. In detail, the evaluated tasks are (1) SST-2; (2) QQP; (3) MNLI-matched; (4) MNLI-mismatched, (5) RTE, and (6)"}, {"title": "5 Conclusions", "content": "We propose SecPE, the first attempt to our knowledge to jointly enable privacy-preserving and adversarial robustness for LLM infer-ence. SecPEsynergizes the strengths of private inference and prompt"}]}