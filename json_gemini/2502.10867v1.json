{"title": "A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT 01", "authors": ["Jun Wang"], "abstract": "OpenAI ol has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.", "sections": [{"title": "1 Background", "content": "OpenAI has recently unveiled ChatGPT 01 [17], a groundbreaking Large Language Model (LLM) that represents a giant leap forward in strong AI. Trained using reinforcement learning techniques, o1 excels in complex reasoning tasks by explicitly embedding a native \u201cChain-of-Thought\" (NCoT) process, which allows it to \"deep think\" through step-by-step reasoning before generating responses. The model is reported to be five times more proficient in math and coding compared to the previous ChatGPT 40, specifically displaying exceptional performance across various domains: it ranks in the 89th percentile for competitive programming, places among the top 500 students in a prestigious US math olympiad qualifier, and surpasses human PhD-level accuracy in physics, biology, and chemistry benchmarks. A key innovation of o1 is that it allows spending more time reasoning during the inference process, marking a shift from fast, direct responses to slow, deliberate, multi-step inference-time computation (Fig. 1).\nInterestingly, in human cognition, two correlated yet distinct modes of cognitive processing are presented to guide human decision-making and behaviours [8], each of which has partially distinction"}, {"title": "2 The Challenges with Autoregressive LLMS", "content": "Autoregressive language models (LLMs) generate sequences of text by predicting the next token (e.g., word) in the sequence given the previous tokens [29]. Mathematically, they are based on the principle of conditional probability. The task is to model the joint probability of a sequence of tokens x = (x1,x2,...,xT), where T is the length of the sequence, by factorising it into a product of conditional probabilities using the chain rule of probability.\nGiven a sequence of tokens x = (x1,x2,...,x\u0442), an autoregressive language model estimates the joint probability P(x) as:\n\\begin{equation}\nP(x) = P(x_1, x_2,..., x_T) = \\prod_{t=1}^{T} P(x_t | x_1,x_2, ..., x_{t-1}),\n\\end{equation}\nwhere the model predicts the probability of each token xt based on all preceding tokens in the sequence X1, X2, ..., Xt\u22121. Typically, this is achieved using neural networks like transformers [29], which are trained to minimise the negative log-likelihood of the training data. For an explanation of the training steps, please refer to Appendix A.\nAt inference time, the model generates text by typically sampling tokens sequentially from the probability distribution P(xt | X1,X2, ..., Xt\u22121) until a stop token is reached or a predefined maximum length is achieved. The model works as follows: Firstly, start with a given sequence or a start token (if generating from scratch). Secondly, at each step t, predict the next token xt based on the previously generated tokens (x1,x2,...,Xt\u22121). At last, continue sampling until the sequence is complete. For a simple three-token sequence x = (X1,X2,X3), the probability of the sequence would be:\n\\begin{equation}\nP(x) = P(x_1) P(x_2 | x_1) P(x_3 | x_1,x_2).\n\\end{equation}\nThis formulation underpins the operation of autoregressive LLMs like GPT-style models. The learning is achieved by minimising mistakes in predicting subsequent tokens (words). The first challenges is this predicting next tokens objective. While some people propose that predicting next tokens might pave the way for general intelligence (AGI), we intend to argue is that solely focusing on predicting the next word caps the potential for intelligence. A different optimisation target and learning paradigm might be necessary to foster deeper intelligence.\nTo illustrate the limitations of purely predictive models, let's consider the domain of chess mastery. In this context, each chess move can be conceptualised as a token, with a complete chess representing a \"sentence\" in the \"language of chess\" - a sequence of moves from the opening to the endgame. Suppose we have access to an extensive dataset of chess games, but all from players with Elo ratings below 2000 (a standardised measure of player skill) [5]. If we train a chess agent solely by minimising token prediction errors based on these games, we would likely constrain the agent's performance"}, {"title": "3 LLM Reasoning as a Markov Decision Process", "content": "To model the process of reasoning in tasks such as question answering or problem solving, we structure the reasoning task using the Q \u2192 {R} \u2192 A sequence, where:\n\u2022 Q: Represents the question or prompt that initiates the reasoning process.\n\u2022 R: Represents the sequence of intermediate reasoning steps the model generates to build toward the solution.\n\u2022 A: Represents the final answer or solution produced after the reasoning steps."}, {"title": "4 Practical Implementation", "content": "Next, we examine how to collect the intermediate reasoning data, use it to train the process-reward model (PRM), leverage the PRM to train the LLM policy, and guide the reasoning process during the decoding phase."}, {"title": "4.1 Automatic Acquisition of Reasoning Steps Data", "content": "As discussed, we require reasoning trajectories to stimulate advanced reasoning while covering a wide range of tasks. For fine-tuning a LLM, we typically have {Q and A} pairs, however lacking the ground-truth of underlying reasoning steps {R}:\nQuestion Q\n\u2193\nReasoning Step1: r1 (Reward1)\n\u2193\nReasoning Step2: 12 (Reward2)\n\u2193\n...\n\u2193\nAnswer A : rA (Final Reward)\nA straightforward approach would be to label the reasoning steps manually by humans [27, 12]. However, a particularly effective method for collecting data and improving LLM reasoning without requiring human supervision is the Self-Taught Reasoner (STaR) technique [34], among others. In this approach, the model generates intermediate reasoning steps autonomously and uses them to validate its internal reasoning capabilities. This method builds on the ability of LLMs to reason from a question Q to a final answer A, by generating intermediate steps {R1, R2, ..., Rn} and verifying their correctness using the model's own policy. Namely, the method begins by employing the LLM's"}, {"title": "4.2 Self-reinforced Training", "content": "As illustrated in Fig. 4, the PRM v(s) and LLM policy (LLM) can be mutually reinforced to improve themselves, which will be explained next."}, {"title": "4.2.1 Value Iteration for PRM", "content": "Once the reasoning data has been collected, the next step is to train the world model, also referred to as the Process-Reward Model (PRM), i.e., since the state transitions are deterministic and known, the focus shifts to learning a general reward model that can later be used to guide the search, reasoning, and decoding processes. This reward model, often called the verifier, denoted as VPRM(s), can be trained using a dataset of annotated reasoning steps. The training typically involves optimising a classification loss function based on the correctness of the reasoning steps [15]:\n\\begin{equation}\nL_{PRM} = \\sum_{i=1}^{N} [v_i log \\hat{v_i} + (1 - v_i) log(1 - \\hat{v_i})],\n\\end{equation}\nwhere vi = ri represents the correctness label for the i-th example step, indicating whether the reasoning process for that example is correct. The verifier's prediction, \u00fb\u2081(s), is the score output by the PRM for the state s, representing the reward for the reasoning step or the final answer. Since this is a classification approach, there is no distinction between the reward for an intermediate step and the potential reward it could lead to and all the reasoning steps are assumed to be independent. The model simply evaluates whether the reasoning step or answer is correct at that point in the process, treating all rewards in a uniform manner without considering the future impact of intermediate steps.\nHowever, an alternative approach involves viewing the PRM as a value function that can be trained via a value iteration method, enabling it to predict cumulative rewards and guide the reasoning process through optimal action selection [6]. Consider a reasoning process where the state s represents"}, {"title": "4.2.2 Policy Iteration for LLM Policy", "content": "Once PRM obtained, one can train the LLM policy for enhanced reasoning. This requires methodologies that go beyond traditional supervised learning frameworks. PRM plays an essential role in this process by incorporating online reinforcement learning to optimise reasoning tasks [18]. However, a typical RLHF work such as [18] can be used but may not be ideal for large language model training.\nLet us look at Group Relative Policy Optimisation (GRPO) [22]. We assume that for each question Q = q, the policy generates reasoning steps {01, 02, . . ., OG }, and each output or consists of multiple steps {ai,1, ai,2,..., ai,K\u2081 }, where K\u2081 is the total number of reasoning steps (or tokens) in output Oi. We slightly abuse our previous notation by using o to represent all outputs, including both reasoning steps and final answers. We can now formulate the GRPO optimisation for learning the LLM policy via the PRM as follows.\nFor each question q, GRPO samples a group of outputs {01, 02,..., og} from the old policy \u03c0\u03b8old, and the goal is to optimise the policy by maximising the following objective:\n\\begin{equation}\nJ_{GRPO}(\\theta) = E_{q \\sim P(Q), \\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_{old}} (O|q)} [\\frac{1}{G} \\sum_{i=1}^{G}  \\sum_{t=1}^{K_i} \\frac{1}{K_i} min (p_{i,t} A_{i,t}, clip (p_{i,t}, 1 - \\epsilon, 1 + \\epsilon) A_{i,t})  - \\beta D_{KL} (\\pi_{\\theta} || \\pi_{\\theta_{ref}}) ]\n\\end{equation}\nwhere:\n\u2022 q ~ P(Q) denotes sampling a question q from a distribution of questions P(Q),\n\u2022 {0}=1 ~ \u03c0\u03b8\u03bf\u03b9\u03b1 (O|q) represents the group of outputs sampled from the old policy \u03c0\u03b8old,\n\u03c0g (ai,t |q,oi,<t) is the importance weight (probability ratio) for action art at step t\n\u03c0\u03b8old (ai,t|q,oi,<t)\nin output oi,\n\u2022 Ait is the advantage at reasoning step t of output oi, calculated based on relative rewards (see below),\n\u2022 e is the clipping parameter that prevents excessive updates (as in PPO [21]),\n\u2022 \u1e9e is a hyperparameter controlling the strength of KL regularisation,\n\u2022 DKL (\u03c0\u03b8||\u03c0\u03b8ref) is the KL divergence between the trained policy \u03c0\u0473 and a reference policy \u03c0\u03b8re, used as regularisation.\nThe advantage function Ai,t for the action ait taken at step t in output or is calculated based on the rewards from both reasoning steps and the final step. The rewards are normalised using the rewards"}, {"title": "4.3 Inference-time Computation", "content": "Once trained, the LLM policy must efficiently generate outputs during inference. Autoregressive generation\u2014where tokens are predicted one by one based on previous tokens\u2014is widely used in LLMs. However, for reasoning tasks, more sophisticated decoding techniques are necessary.\nTo strike a balance between efficiency and effectiveness, the work [24, 33] found that reasoning tasks benefit from more flexible approaches like beam search. In beam search, multiple possible sequences (or beams) are generated simultaneously, and the best candidate is chosen based on cumulative probability. For even more complex reasoning tasks, look ahead model such as MCTS is used. MCTS [6] simulates multiple reasoning paths and evaluates them based on a reward system, selecting the one with the highest expected reward. This allows the model to explore a wider range of possibilities during inference, increasing its chances of arriving at an optimal solution. With an MDP, we could formally define the reasoning process structure.\nDefinition 2 (Native Chain-of-Thought) Native Chain-of-Thought (NCoT) refers to the inherent reasoning capability of a large language model (LLM), which allows it to autonomously perform step-by-step, structured reasoning without external prompts. This capability is formalised as a Markov Decision Process (MDP) (S, A, \u03c0, R), where:\n\u2022 S is the state space, representing the sequence of tokens or reasoning steps generated up to a given point.\n\u2022 A is the action space, which consists of potential reasoning steps Rt or the final answer A."}, {"title": "5 Bibliographic Remarks", "content": "In the literature, significant attention has been given to inference-time computation, verifiers (also known as reward models), and data acquisition methods, all of which play a critical role in enhancing the reasoning capabilities of these models. In this section, we review and discuss several key papers in these areas, examining their contributions and limitations. The connection between these works and the broader research landscape is depicted in Fig. 6."}, {"title": "5.1 Inference-Time Computing", "content": "Several papers have focused on optimising LLM reasoning through inference-time computing. For instance, the paper [6] introduces a method that integrates Monte Carlo Tree Search (MCTS) with LLM decoding, a combination that has proven highly effective in guiding reasoning, particularly for complex, multi-step tasks. The inclusion of MCTS facilitates better decision-making by simulating potential future actions, enhancing the model's ability to plan its next steps. Similarly, the paper [24] emphasises the importance of optimising test-time computation, empirically showing that inference-time reasoning enhancements can often yield more substantial improvements than simply scaling model parameters. This reflects a growing understanding that more compute during inference can be leveraged for higher quality reasoning without necessarily increasing the model's size.\nAnother approach is presented in [7], which suggests using pause tokens to force models to pause and \"think\" during reasoning. This method introduces an implicit reasoning model, encouraging the LLM to process information in chunks, mimicking human-like deliberation."}, {"title": "5.2 Verifier Models", "content": "Verifier models (outcome-reward models and process-reward models) have become an important area of research in improving LLM reasoning reliability. Papers like [4] introduced the earliest formal attempt (outcome reward only) at using verifiers in mathematical reasoning tasks, laying the groundwork for subsequent research. The follow-up work [27] expands on the concept of verifiers, integrating process-based reasoning mechanisms, and was followed by OpenAI's work on Process"}, {"title": "5.3 Data Acquisition for Reasoning Tasks", "content": "The acquisition of reasoning data has been another area of focus, particularly in papers like [34], which explores methods for automatically obtaining data related to reasoning steps. STaR introduces a self-teaching paradigm where the model improves its reasoning capabilities by generating and critiquing its own steps, leading to more reliable intermediate steps. The paper [30] takes this approach further, showing how LLMs can be trained step-by-step without the need for costly human annotations, providing a more scalable solution to the reasoning data problem.\nThe work in [31] highlights the importance of practical data acquisition for reasoning tasks, particularly in coding problems. MCTS has been used for acquiring data in [6], whereas it has been extended with linear search for efficiency in [15].\nThese papers suggest that for LLMs to advance in reasoning, innovative data acquisition methods, such as self-supervised learning and verification mechanisms, are essential to reduce the dependency on extensive human-labelled datasets."}, {"title": "5.4 Understanding and System-Level Improvements", "content": "Finally, there is a growing body of research aimed at understanding the mechanisms behind step-by-step reasoning in LLMs [26, 19]. The work in [25] focused its analysis from graphical models for the chain of thought mechanism. The paper [19] explores the intrinsic reasons why reasoning emerges as a natural capability in LLMs. It suggests that reasoning is a byproduct of the way language models process localised experiences and knowledge. The paper [14] provides an empirical evaluation of"}, {"title": "A Standard Training Pipelines of LLMs", "content": "The training procedure for LLM typically involves several stages, each building upon the previous one. In the pre-training stage, the model is trained on a massive online corpus using an autoregressive language modelling objective. The goal is to predict the next token given the previous tokens. For a given sequence of tokens {x1,x2,...,xT}, the token-level cross-entropy loss sums the negative log-probabilities of the true tokens at each position:\n\\begin{equation}\nL_{pretrain} = -\\sum_{t=1}^{T} log P(x_t|x_{<t}; \\theta),\n\\end{equation}"}]}