{"title": "Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization Perspective", "authors": ["Semih Cayci"], "abstract": "We analyze the convergence of Gauss-Newton dynamics for training neural networks with smooth activation functions. In the underparameterized regime, the Gauss-Newton gradient flow induces a Riemannian gradient flow on a low-dimensional, smooth, embedded submanifold of the Euclidean output space. Using tools from Riemannian optimization, we prove last-iterate convergence of the Riemannian gradient flow to the optimal in-class predictor at an exponential rate that is independent of the conditioning of the Gram matrix, without requiring explicit regularization. We further characterize the critical impacts of the neural network scaling factor and the initialization on the convergence behavior. In the overparameterized regime, we show that the Levenberg-Marquardt dynamics with an appropriately chosen damping factor yields robustness to ill-conditioned kernels, analogous to the underparameterized regime. These findings demonstrate the potential of Gauss-Newton methods for efficiently optimizing neural networks, particularly in ill-conditioned problems where kernel and Gram matrices have small singular values.", "sections": [{"title": "Introduction", "content": "The Gauss-Newton method is traditionally used in nonlinear least-squares problems [24, 17]. In the context of neural network training, it has emerged as a powerful alternative to first-order methods, such as stochastic gradient descent (SGD), particularly when high accuracy and efficient convergence are required [8, 27, 23, 32, 26]. By leveraging the structure of the loss function, the Gauss-Newton method approximates the Hessian matrix with a positive semi-definite variant, which is computationally more tractable while still capturing important curvature information.\nIn this work, we investigate the convergence behavior of the Gauss-Newton method in a supervised learning setting with neural networks with smooth nonlinear activation functions in both the overparameterized and underparameterized regimes.\n\u2022 Gauss-Newton dynamics in the underparameterized regime. The Gauss-Newton gradient flow induces a Riemannian gradient flow in the output space (Theorem 4), which is a low-dimensional smooth embedded submanifold of a Euclidean space (Theorem 3). We incorporate tools from the rich theory of Riemannian optimization, and establish certain geodesic strong convexity and Lipschitz continuity results (Lemma 7 and Corollary 1) to analyze the behavior of the Gauss-Newton dynamics. Ultimately, we prove that the Gauss-Newton method"}, {"title": "Notation", "content": "For a differentiable curve \\(\\gamma : I \\subset \\mathbb{R}_+ \\rightarrow \\mathbb{R}\\), \\(y_t\\) and \\(y'(t)\\) denote its derivative at time \\(t\\). \\(I\\) denotes the identity matrix. \\(\\geq\\) is the Loewner order. For a smooth function \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^p\\), \\(\\text{Lip}_f\\) denotes its modulus of Lipschitz continuity. For a symmetric positive-definite matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) and \\(v \\in \\mathbb{R}^n\\),"}, {"title": "Problem Setting and the Gauss-Newton Dynamics", "content": ""}, {"title": "Supervised Learning Setting", "content": "In this work, we consider a smooth activation function \\(\\sigma : \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(||\\sigma||_\\infty \\leq \\sigma_0, ||\\sigma'||_\\infty \\leq \\sigma_1\\) and \\(||\\sigma''||_\\infty \\leq \\sigma_2\\), which is satisfied by, e.g., \\(\\sigma = \\text{tanh}\\). At an input point \\(x \\in \\mathbb{R}^d\\), the output of the neural network is\n\\[\\phi(x; w) := \\sum_{i=1}^m c_i \\sigma(x^T w^{(i)}) \\text{ for any } (c, w) \\in \\mathbb{R}^m \\times \\mathbb{R}^{md},\\]\nwhere \\(w = [(w^{(1)})^T...(w^{(m)})^T]^T\\). Let \\(p := md\\) be the number of trainable parameters. Given a data set \\(\\mathcal{D} = \\{(x_j,y_j) \\in \\mathbb{R}^d \\times \\mathbb{R} : 1 \\leq j \\leq n\\}\\), define \\(\\phi(w) := [\\phi(x_1;w)... \\phi(x_n; w)]^T\\). We adopt the standard random initialization for \\(\\phi\\): for any \\(i \\in \\{1,2,...,m\\}\\), \\(c_i \\sim \\text{Unif}\\{-1,1\\}\\) and \\(w_{\\text{init}} \\sim \\mathcal{N}(0, I_d)\\) independent and identically distributed. We define \\(f(w) := \\phi(w) - \\phi(w_{\\text{init}})\\). Since \\(\\sigma\\) is smooth, \\(w \\rightarrow f(w)\\) is a smooth function with \\(L\\)-Lipschitz continuous (Euclidean) gradients.\nGiven a smooth loss function \\(g : \\mathbb{R}^n \\rightarrow \\mathbb{R}^+\\), which is smooth, \\(\\nu\\)-strongly convex on \\(\\mathbb{R}^n\\) and has \\(\\mu\\)-Lipschitz continuous gradients, the objective (empirical risk minimization - ERM) is\n\\[\\max_{w \\in \\mathbb{R}^p} g(\\alpha f(w)),\\]\nfor \\(\\alpha > 0\\). Note that, for example, the regression problem with the quadratic loss \\(g(\\cdot) = ||\\cdot||_2^2\\) satisfies these conditions. We call the neural networks underparameterized if \\(p < n\\), and overparameterized otherwise. Let\n\\(\\begin{bmatrix}\n[\\nabla_w\\phi(x_1; w)] \\\\\n:\\\\\n[\\nabla_w\\phi(x_n; w)]\n\\end{bmatrix}\\)\ndenote the \\(n \\times p\\) Jacobian matrix of \\(f\\)."}, {"title": "Gauss-Newton Gradient Flow", "content": "Let \\((c, w_{\\text{init}})\\) be a nullifying initialization. We assume that \\(c\\) is fixed as initialized. In this work, we consider Gauss-Newton gradient flow for training neural networks:\n\\begin{equation}\n\\begin{aligned}\n&\\frac{dw_t}{dt} = -[H_\\rho(\\alpha f(w_t))]^{-1}D^T f(w_t)\\nabla g(\\alpha f(w_t)) \\text{ for } t > 0,\\\\\n&w_0 = w_{\\text{init}},\n\\end{aligned}\n\\tag{1}\n\\end{equation}\nwhere \\(\\alpha > 0\\) is a scaling factor, and\n\\[H_\\rho(\\alpha f(w_t)) := (1 - \\rho)D^T f(w_t)D f(w_t) + \\rho I\\]\nis the Gauss-Newton preconditioner with the regularization (or damping) factor \\(\\rho \\in [0, 1]\\) that is used to ensure that \\(H_\\rho(\\alpha f(w_t))\\) is invertible in case \\(D^T f(w_t)D f(w_t)\\) is singular. The case \\(\\rho > 0\\) is known as Levenberg-Marquardt dynamics [24]."}, {"title": "Gauss-Newton Dynamics for Overparameterized Neural Networks", "content": "As a warm-up for the analysis in the underparameterized setting, we will start with the analysis in the overparameterized setting \\(p > n\\). Since rank\\((D^T f(w)Df(w)) < p\\) in this regime, we have to consider \\(\\rho > 0\\) to ensure that (1) is well-defined, which leads to the Levenberg-Marquardt dynamics.\nAs the analysis will indicate, the choice of \\(\\rho > 0\\) plays a fundamental r\u00f4le on the convergence of Gauss-Newton dynamics in the overparameterized regime. The proof in the overparameterized regime extends the kernel analysis in [6, 16] for the gradient flows to the Gauss-Newton gradient flows, and setting \\(\\rho = 1\\) in our theoretical results will recover the existing bounds exactly.\nIn the overparameterized regime, the spectral properties of the so-called neural tangent kernel (NTK) has a crucial impact on the convergence. To that end, for any \\(i,j \\in \\{1,2,..., n\\}\\), let \\(K\\in \\mathbb{R}^{n \\times n}\\) be defined as \\([K]_{ij} := x_i^T x_j\\mathbb{E}[\\sigma'(w_{\\text{init}}^T x_i)\\sigma'(w_{\\text{init}}^T x_j)]\\). Note that under the symmetric initialization \\((c, w_{\\text{init}})\\), we have \\(m^{-1}\\mathbb{E}[Df(w_{\\text{init}})D^T f(w_{\\text{init}})] = K\\). We make the following standard representational assumption on the so-called neural tangent kernel evaluated at \\(\\mathcal{D}\\) [15].\nAssumption 1. Let \\(K_0 := Df(w_{\\text{init}})D^T f(w_{\\text{init}})\\). There exists \\(\\lambda_0 > 0\\) such that \\(K_0 \\geq \\frac{\\lambda_0}{4}I\\).\nUnder Assumption 1, if \\(||w - w_{\\text{init}}||_2 < r_0 := \\frac{1}{L}\\), then \\(Df(w)D^Tf(w) \\geq \\frac{\\lambda_0}{8} I\\), where \\(L\\) is the modulus of Lipschitz continuity for \\(w \\rightarrow Df(w)\\) [28, 15]. We also define \\(\\lambda_2 := |||K_0|||_{op}\\), which will be important in the analysis.\nUnder the Gauss-Newton gradient flow (1), define the exit time\n\\[T := \\inf\\{t > 0: ||w_t - w_0||_2 \\geq r_0\\},\\]\nand let\n\\[K_t := Df (w_t)D^T f(w_t), t \\in [0, \\infty)\\]\nbe the kernel matrix. Then, we have\n\\begin{aligned}\nK_t &\\geq \\frac{\\lambda_1}{16} I, \\\\\n||K_t||_{op} &\\geq \\lambda_2,\n\\end{aligned}\nfor any \\(t < T\\) [28].\nThe gradient flow in the output space and the energy dissipation inequality (EDI) in this regime are presented in the following lemma."}, {"title": "Related Work", "content": "Analysis of the Gauss-Newton method. The Gauss-Newton method in the overparameterized regime was analyzed in a number of works recently [12, 32, 5, 4]. These existing works consider the Gauss-Newton method in the overparameterized setting, while we both consider the underparameterized and overparameterized regimes in this work, where we develop a Riemannian geometric approach to investigate the former, which is fundamentally different from the existing works.\nNeural networks in the kernel regime. The original works in the neural tangent kernel (NTK) analysis consider the overparameterized regime [16, 19, 6]. Our analysis builds on the neural network analysis proposed in [6]. However, deviating significantly from this line of work on overparameterized networks, we utilize tools from the Riemannian optimization theory to analyze the Gauss-Newton dynamics in the underparameterized regime. Our discussion on Riemannian optimization is mainly based on [9]. In a number of works [20, 13], convergence of first-order methods in the underparameterized regime was investigated in the near-initialization regime. These results indicate that first-order methods in the underparameterized regime (1) require explicit regularization in the form of projection or early stopping, (2) establish only average- (or best-)iterate near-optimality results, and (3) the convergence rates are subexponential. The main analysis approach in these works mimics the projected subgradient descent analysis. We prove that Gauss-Newton dynamics (1) does not require any explicit regularization schemes, (2) establishes last-iterate convergence results (both in the loss and in the function space), (3) with exponential convergence rates, indicating the superiority of Gauss-Newton preconditoning in the underparameterized regime."}, {"title": "Lemma 1", "content": "Under the Gauss-Newton gradient flow with damping factor \\(\\rho \\in (0,1]\\), let \\(\\rho_0 := \\frac{1}{1-\\rho}\\). Then, we have\n\\begin{aligned}\n\\frac{d\\alpha f(w_t)}{dt} &= - (K_t^{-1} - \\rho K_t^{-1} (I + \\rho_0^{-1}K_t)^{-1}K_t^{-1}) \\nabla g(\\alpha f(w_t), &\\text{(GF-O)}\\\\\n\\frac{dg(\\alpha f(w_t))}{dt} &\\leq -\\frac{\\rho}{\\rho + (1 - \\rho)\\lambda_0} ||\\nabla g(\\alpha f(w_t))||^2,&\\text{(EDI)}\n\\end{aligned}\nfor any \\(t <T\\). Consequently, for any \\(t <T\\), we have\n\\begin{aligned}\n\\Delta_t &\\leq \\Delta_0 \\exp\\left(-\\frac{2 \\nu t}{\\rho + (1 - \\rho) \\lambda_0}\\right),\n&||\\alpha f(w_t) - f^*||^2 \\leq \\frac{2 \\Delta_0}{\\nu} \\exp\\left(-\\frac{2 \\nu t}{\\rho + (1 - \\rho) \\lambda_0}\\right).\n\\end{aligned}"}, {"title": "Lemma 2", "content": "For any \\(\\rho \\in (0,1]\\), if\n\\[\\alpha > \\frac{\\mu \\sqrt{2 \\Delta_0} \\lambda_2}{\\lambda_1^{3/2} \\lambda_0} \\frac{\\lambda_0 + \\rho}{\\lambda_0 \\lambda_2 + \\rho},\\]\nthen \\(T = \\infty\\), which makes \\(\\{w \\in \\mathbb{R}^p : ||w - w_{\\text{init}}||_2 < r_0\\}\\) a positively invariant set under the Gauss-Newton gradient dynamics (1).\nProof. By the triangle inequality, we have\n\\begin{aligned}\n||w_t - w_0||_2 &= ||\\int_0^t w_s ds||_2 \\leq \\int_0^t ||w_s|| ds \\\\\n&= \\frac{1}{\\alpha} \\int_0^t ||\\nabla g(\\alpha f(w_s))|| Df(w_s)[H_\\rho(\\alpha f(w_s)]^{-1}D^Tf(w_s) ds. \\tag{8}\n\\end{aligned}\nUsing the Sherman-Morrison-Woodbury matrix identity, we obtain\n\\begin{aligned}\n(1 - \\rho)^2 Df(w_t)[H_\\rho(\\alpha f(w_t)]^{-2}D^T f(w_t) = \\rho_0^{-2}K_t - 2\\rho_0^{-3}K_t (I + \\rho_0^{-1} K_t)^{-1} K_t + \\rho_0^{-4}K_t (I + \\rho_0^{-1} K_t)^{-1} K_t (I + \\rho_0^{-1} K_t)^{-1} K_t, &\\text{(9)}\n\\end{aligned}\nwhere \\(\\rho_0 := \\frac{1}{1-\\rho}\\). Then, if \\((\\gamma, u) \\in \\mathbb{R} \\times \\mathbb{R}^n\\) is an eigenpair for \\(K_t\\), then \\(\\left(\\frac{(\\gamma \\rho_0)^2}{((1 - \\rho)\\gamma + \\rho)^2}, u\\right)\\) is an eigenpair for \\(Df(w_t)[H_\\rho(\\alpha f(w_t)]^{-2}D^T f(w_t)\\). Thus, we have\n\\[||\\nabla g(\\alpha f(w_s))|| Df(w_s)[H_\\rho(\\alpha f(w_s)]^{-2}D^Tf(w_s) \\leq \\frac{1}{(1 - \\rho)^2} \\frac{||K_t||_{op}}{(\\lambda_1 + \\rho)^2} ||\\nabla g(\\alpha f(w_s))||^2. \\tag{10}\\]"}, {"title": "Theorem 1", "content": "(Convergence in the overparameterized regime). Under the Gauss-Newton gradient flow (1) with any damping factor \\(\\rho \\in (0, 1]\\), under Assumption 1, we have\n\\[\\Delta_t \\leq g(0) \\exp\\left(-\\frac{2 \\nu t \\lambda_1}{\\rho + (1 - \\rho) \\lambda_0}\\right) \\text{ for any } t \\in \\mathbb{R}^+,\\]\nfor any scaling factor\n\\[\\alpha > \\frac{L \\mu \\sqrt{2 \\Delta_0} \\lambda_2}{\\lambda_1^{3/2}} \\frac{\\lambda_0 (1 - \\rho) \\lambda_0 + \\rho}{(\\lambda_0 (1 - \\rho) \\lambda_0 + \\rho)}.\\]"}, {"title": "Theorem 2", "content": "(Convergence in the overparameterized regime - discrete time). Let \\((c, w_{\\text{init}})\\) be the initialization scheme described in Section 2.1, and consider the following discrete-time analogue of the Gauss-Newton method:\n\\begin{equation}\nw_{k+1} = w_k - \\eta \\left[ (1 - \\rho)D^T f(w_k)Df(w_k) + \\rho I \\right]^{-1}D^Tf(w_k)\\nabla g(\\alpha f(w_k)),\\text{(GN-CT)}\\\\\nw_0 = w_{\\text{init}},\n\\tag{13}\n\\end{equation}\nfor all \\(k \\in \\{0,1,...\\}\\). Under Assumption 1, the Gauss-Newton method with the damping factor \\(\\rho \\in (0, 1]\\), the scaling factor \\(\\alpha > \\max\\left\\{\\frac{L \\mu \\sqrt{2\\Delta_0} \\lambda_2}{(1 - \\rho)\\lambda_0 + \\rho}, \\frac{\\sigma_2\\sqrt{n} \\text{Lip}_g}{a m \\text{Lip}_f}\\right\\}\\) and the learning rate\n\\[\\eta = \\min \\left\\{\\frac{1}{8},\\frac{\\nu \\lambda_1}{16 L\\mu^2 \\text{Lip}_f \\lambda_2 (1 - \\rho) \\lambda_0 + \\rho}, \\frac{\\nu}{\\mu} \\right\\}\\]\nyields\n\\[\\Delta_{k+1} \\leq \\Delta_0 \\left(1 - \\eta \\alpha \\frac{\\lambda_1}{(1 - \\rho)\\lambda_0 + \\rho} + \\alpha^2 \\eta \\frac{\\mu \\lambda_2 \\text{Lip}_f^2}{(1 - \\rho) \\lambda_0 + \\rho}\\right)^k, k \\in \\mathbb{N}, \\tag{14}\\]"}, {"title": "Gauss-Newton Dynamics for Underparameterized Neural Networks: Riemannian Optimization", "content": "In the underparameterized regime characterized by \\(p < n\\), the kernel \\(Df(w_t)D^T f(w_t) \\in \\mathbb{R}^{n \\times n}\\) is singular for all \\(t \\in [0,\\infty)\\) since rank\\((Df(w)D^T f(w)) < p < n\\) for all \\(w \\in \\mathbb{R}^p\\). Thus, the analysis in the preceding section, which relies on the non-singularity of \\(K_t\\), will not extend to this setting.\nThis will motivate us to study the underparameterized regime by using tools from optimization on Riemannian manifolds.\nIn the underparameterized regime, the Gauss-Newton preconditioner \\(H_\\rho(\\alpha f(w)) \\in \\mathbb{R}^{p \\times p}\\) can be non-singular without damping (i.e., \\(\\rho = 0\\)) since \\(p < n\\). Thus, we consider Gauss-Newton dynamics without damping. The non-singularity of \\(H_\\rho(\\alpha f(w_t))|_{\\rho=0}\\) will be crucial in establishing the Riemannian optimization framework in the succeeding sections. For a detailed discussion on optimization on embedded submanifolds, which is the main toolbox in this section, we refer to [9]."}, {"title": "Theorem 3", "content": "\\(M\\) is a \\(p\\)-dimensional smooth embedded submanifold of \\(\\mathbb{R}^n\\).\nProof. Take an arbitrary parameter \\(w \\in B\\). By the constant rank theorem (Theorem 7.4.3 in [1] and Theorem 4.12 in [21]), there exist open \\(U_1, U_2 \\subset \\mathbb{R}^p\\) such that \\(w \\in U_2\\), \\(V_1 \\in \\alpha f(B)\\) and \\(V_2 \\subset \\mathbb{R}^n\\) such that \\(\\alpha f(w) \\in V_1\\), and smooth diffeomorphisms \\(H : U_1 \\rightarrow U_2\\) and \\(G : V_1 \\rightarrow V_2\\) such that \\(G \\circ \\alpha f \\circ H(\\tilde{w}) = (\\tilde{w}, 0_{n-p})\\) for any \\(\\tilde{w} \\in \\mathbb{R}^p\\). Define the projection operator \\(\\pi : \\mathbb{R}^p \\times \\mathbb{R}^{n-p} \\rightarrow \\mathbb{R}^{n-p}, (w, e) := e\\) for \\(w \\in \\mathbb{R}^p, e \\in \\mathbb{R}^{n-p}\\), and \\(h(y) := (\\pi \\circ G)(y), y \\in V_1\\).\n\\begin{enumerate}\n    \\item Take \\(y \\in V_1\\). We will show that \\(h(y) = 0\\) if and only if \\(y \\in M\\). First, \\(y \\in \\alpha f(U_2) \\subset \\alpha f(B)\\), thus \\(y = \\alpha f(w')\\) for some \\(w' \\in U_2\\) and \\(w' = H(\\tilde{w}), \\tilde{w} \\in \\mathbb{R}^p\\). By the rank theorem, we conclude that \\(h(y) = 0\\). By construction, \\(V_1 \\subset \\alpha f(B)\\) is open\\(^1\\), thus \\(h(y) = 0\\) if and only if \\(y \\in \\alpha f(B)\\) as the other direction is trivially satisfied.\n    \\item We will show that rank \\(Dh(y) = n - p\\) for all \\(y \\in V_1\\). Note that \\(h = \\pi \\circ G\\), where \\(\\pi\\) and \\(G\\) are smooth, thus continuously differentiable. By the chain rule, we have\n    \\[Dh(y) = D(\\pi \\circ G)(y)DG(y).\\]\n    Since \\(G : V_1 \\rightarrow \\mathbb{R}^n\\) is a smooth diffeomorphism, \\(DG(y) \\in \\mathbb{R}^{n \\times n}\\) exists and is invertible for any \\(y \\in V_1\\). Thus, rank\\((DG(y)) = n, \\forall y \\in V_1\\). Note that \\(\\pi(z) = Qz\\), where rank\\((Q) = n - p\\). Hence, \\(Dh(y)\\) has rank \\((n - p)\\) for any \\(y \\in V_1\\).\n\\end{enumerate}\nAs there exists a smooth \\(h : V_1 \\rightarrow \\mathbb{R}^{n-p}\\) satisfies the above two conditions, which implies that \\(M\\) is a \\(p\\)-dimensional smooth embedded submanifold of \\(\\mathbb{R}^n\\) (Definition 3.10 in [9])."}, {"title": "Lemma 4", "content": "For any \\(w \\in B\\), let\n\\[T_{\\alpha f(w)}M := \\{D f(w)z : z \\in \\mathbb{R}^p\\} = \\text{Im}(Df(w)). \\tag{17}\\]\nThen, \\(T_{\\alpha f(w)}M\\) is the tangent space of \\(\\alpha f(w) \\in M\\). Also, for any \\(w \\in B\\) and \\(u, v \\in T_{\\alpha f(w)}M\\), \\((u, v)_{\\alpha f(w)}^M := (u, v) = u^T v\\) is a Riemannian metric on \\(M\\). Consequently, \\((M, (\\cdot, \\cdot)^M_{\\alpha f(w)})\\) is a Riemannian submanifold of \\(\\mathbb{R}^n\\).\nProof. Note that the tangent space for \\(M = \\alpha f(B)\\) is defined as\n\\[T_{\\alpha f(w)}M := \\{c'(0) : c : I \\rightarrow M \\text{ is smooth, } c(0) = \\alpha f(w)\\},\\]\nwhere \\(I \\subset \\mathbb{R}\\) is any interval with \\(0 \\in I\\) [3, 22, 9]. Let \\(I = (-\\epsilon, \\epsilon)\\) for \\(\\epsilon > 0\\). Since \\(f : \\mathbb{R}^p \\rightarrow \\mathbb{R}^n\\) is smooth due to the smooth activation functions, if \\(c : I \\rightarrow M\\) is a smooth curve on \\(M = \\alpha f(B)\\), then there exists a smooth curve \\(\\gamma : I \\rightarrow B\\) such that \\(c(t) = f(\\gamma(t))\\) for \\(t \\in I\\), with \\(\\gamma(0) = w\\). Then, we have\n\\[\\frac{dc(t)}{dt} = \\frac{d \\alpha f(\\gamma(t))}{dt} = Df(\\gamma(t)) \\frac{d\\gamma(t)}{dt},\\]\nby the chain rule. Thus, \\(c'(0) = Df(w)\\gamma'(0) \\in \\text{Im}(Df(w))\\). The second part of the claim is a direct consequence of Theorem 3, as the restriction of the Euclidean metric to an embedded submanifold of \\(\\mathbb{R}^n\\) (\\(M\\) in our case, by Theorem 3) is a Riemannian metric [9]."}, {"title": "Lemma 5", "content": "Let \\(H_0 := D^Tf(w_{\\text{init}})Df(w_{\\text{init}})\\). There exists \\(\\lambda_0 > 0\\) such that \\(H_0 \\geq \\frac{\\lambda_1}{4}I\\).\nWe define\n\\[B := \\{w \\in \\mathbb{R}^p: ||w - w_{\\text{init}}||_2 < \\frac{2 \\log \\lambda_0}{L^2}\\},\\]\nwhere \\(r \\in (0,1)\\).\nLemma 3. For any \\(w \\in B\\), we have \\(H_0(\\alpha f(w)) \\geq (1 - r)^2\\frac{\\lambda_1^2}{4} I\\)."}, {"title": "Theorem 4", "content": "(Gauss-Newton as a Riemannian gradient flow). For any \\(\\alpha f(w) \\in M\\), \\(P(\\alpha f(w))\\) is the projection operator onto its tangent space \\(T_{\\alpha f(w)}M\\), i.e.,\n\\[P(\\alpha f(w))z = \\arg \\min_{y \\in T_{\\alpha f(w)} M} ||y - z||_2^2.\\]\nFurthermore,\n\\[\\text{grad}_{\\alpha f(w)} g(\\alpha f(w)) := P(\\alpha f(w))\\nabla g(\\alpha f(w)) \\tag{18}\\]"}, {"title": "Assumption 2", "content": "\\(w \\rightarrow \\alpha f(w)\\) is an immersion and a globally injective function on \\(B\\).\nProof. Note that \\(D_{\\alpha} f(w) = \\alpha D f(w)\\) is full-rank for every \\(w \\in B\\) by the definition of \\(B\\), thus \\(\\alpha f(w)\\) is automatically an immersive map. For the injectivity result, suppose to the contrary that there exist \\(w_1, w_2 \\in B\\) such that \\(w_1 \\neq w_2\\) and \\(f(w_1) = f(w_2)\\). Let \\(\\gamma(t) := tw_1 + (1 - t)w_2, t \\in [0, 1] \\in B\\) as \\(B\\) is convex. Then,\n\\[\\frac{d \\alpha f(\\gamma(t))}{dt} = \\alpha Df(\\gamma(t))\\gamma'(t) = \\alpha Df(\\gamma(t))(w_1 - w_2).\\]\nSince \\(t \\rightarrow f(\\gamma(t))\\) is smooth and \\(f(0) = f(1)\\), there should be \\(t \\in (0,1)\\) such that \\(\\frac{d f(\\gamma(t))}{dt} = \\alpha Df(\\gamma(t)) (w_1 - w_2) = 0\\). This constitutes a contradiction since \\(w_1 - w_2 \\neq 0\\) and rank\\((Df(\\gamma(t))) = p\\), i.e., \\(Df(\\gamma(t))\\) is full-rank, for every \\(t \\in [0, 1]\\). Thus, \\(f|_B : B \\rightarrow \\mathbb{R}^n\\) is an injective mapping."}, {"title": "Lemma 6", "content": "Theorem 5 (Convergence in the underparameterized regime). Under Assumptions 2-3, given \\(r \\in (0,1)\\), with the scaling factor\n\\[\\alpha = \\frac{L \\mu}{r(1 - r)^{1/3} 2\\sqrt{2\\lambda_1}},\\]\nthe (undamped) Gauss-Newton dynamics with \\(\\rho = 0\\) in (1) achieves\n\\[\\Delta_t \\leq g(0) \\exp(-2\\nu t) \\text{ for any } t \\in [0,\\infty),\\tag{22}\\]\nwhere \\(\\Delta_t := g(\\alpha f(w_t)) - \\inf_{y \\in M} g(y)\\) with \\(\\Delta_0 = g(0) - \\inf_y g(y) \\leq g(0)\\). Furthermore, in the same setting,\n\\[||w_t - w_0||_2 \\leq \\frac{2 \\gamma \\lambda_0}{L} \\text{ and } D^T f(w_t)Df(w_t) \\geq \\frac{4(1 - r)^2 \\lambda_0}{L},\\tag{23}\\]\nfor any \\(t \\in \\mathbb{R}^+.\\]"}, {"title": "Lemma 7", "content": "\\(g|_M : M \\rightarrow \\mathbb{R}\\) is a geodesically \\(\\nu\\)-strongly convex function on \\(M\\), i.e., for any \\(w \\in B, v \\in T_{\\alpha f(w)}M\\) and \\(c(t) = \\text{Exp}_{\\alpha f(w)}(vt)\\) for \\(t \\in [0,1]\\), we have\n\\[g(\\alpha f(w)) + t(\\text{grad}_{\\alpha f(w)}(\\alpha f(w)), v)_{\\alpha f(w)}^M \\leq g(\\text{Exp}_{\\alpha f(w)}(tv)) - \\frac{t^2 \\nu}{2} ||v||_{\\alpha f(w)}^2,\\]\nfor any \\(t \\in [0,1]\\)."}, {"title": "Lemma 8", "content": "\\(S := \\{y \\in M : g(y) \\leq g(0)\\},\\)\nwhich is a nonempty set since \\(g(\\alpha f(w_0)) = 0\\), thus \\(\\alpha f(w_0) \\in S\\). As a consequence of the energy dissipation inequality (EDI-u), \\(t \\leftrightarrow g(\\alpha f(w_t))\\) is a non-increasing function on \\(t < T\\), thus\n\\[\\{\\alpha f(w_t): t \\in [0,T)\\} \\subset S \\subset M.\\]"}, {"title": "Lemma 9", "content": "For every \\(t \\in [0", "v_t||^2,&\\text{(21)}\n\\end{aligned}\nwhere\n\\[\\Delta_t": ""}]}