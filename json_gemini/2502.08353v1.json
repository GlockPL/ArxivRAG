{"title": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy", "authors": ["Ruizhan Xue", "Huimin Deng", "Fang He", "Maojun Wang", "Zeyu Zhang"], "abstract": "With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness.", "sections": [{"title": "1 Introduction", "content": "Graphs are data structures that are widely used in a variety of real-world scenarios [Xia et al., 2021]. Graph Neural Networks (GNNs) [Wu et al., 2020] have achieved remarkable success in many fields due to their powerful modeling ability for graph-structured data, such as autonomous driving [Xiao et al., 2023], recommendation systems [Zhang et al., 2023a], and crop breeding [Pan et al., 2024b]. With the deployment of GNNs in this highly sensitive fields, the trustworthiness of GNNs decisions has become a key bottleneck. GNNs are increasingly expected to be reliable, robust, and privacy-preserving to gain trust.\nIn recent years, significant progress has been made in large language models (LLMs) such as GPT [Brown et al., 2020] and DeepSeek [Guo et al., 2025]. These variants have shown superior performance in many natural language processing tasks, such as sentiment analysis, machine translation, and text classification [Zhao et al., 2023]. Beyond traditional NLP applications, there is growing interest in using LLMs to process various data modalities, such as text-attributed graphs (TAGS) [Yang et al., 2021].\nRecent studies have shown that integrating LLMs into GNNs can substantially enhance node representations and improve model performance [He et al., 2023; Chen et al., 2024]. This naturally raises an important question: Can the integration of LLMs and GNNs also enhance the trustworthiness of graph-based models? More specifically, how can LLMs and GNNs be effectively combined to improve model trustworthiness?\nLLMs help in trusted GNN-related tasks. LLMs have significantly transformed how we interact with graph data, particularly in scenarios where nodes contain rich textual attributes. The integration of LLMs and graphs has been shown to be successful in a variety of graph-related tasks [Li et al., 2023]. As illustrated in Figure 1, the growing adoption of this integration has led to increasing demands for trustworthy models. Numerous studies have shown that LLM-augmented GNNs can enhance trustworthiness. For example, LLM4RGNN [Zhang et al., 2024c] leverages the inferential capabilities of LLMs to identify malicious edges and recover missing important.\nThe integration of graphs and LLMS has a significant impact on the trustworthiness of graph-related tasks from different perspectives. To provide a systematic overview, as depicted in Figure 2, we categorize existing works on enhancing trustworthiness through LLM-GNN integration into four key dimensions: reliability, robustness, privacy, and reasoning.\nMotivations. With the growing applications of the integration of LLMs and GNNs, researchers are increasingly concerned about the trustworthiness of models. Although several approaches exist for integrating LLMs and GNNs to improve trustworthiness, these rapidly developing approaches often lack systematic organization and comparison. Presently, some papers have systematically explored the development of trustworthy GNNs [Zhang et al., 2022], but in the specific area of integrating LLMs and GNNs to improve model trustworthiness, no one has systematically investigated. In this survey, we aim to provide a timely survey summarizing these efforts focusing on \"Trustworthy GNNs with LLMs\u201d.\nContributions. The contributions of this survey can be summarized from the following three aspects.\n\u2022 A new taxonomy. We are the first to propose a taxonomy in the specific area of the integration of LLMs and GNNs to improve model trustworthiness and classify these approaches. This taxonomy categorizes existing work into four categories.\n\u2022 A comprehensive review. Based on the taxonomy, we provide a comprehensive overview of existing work on the integration of LLMs and GNNs to improve trustworthiness and point out their limitations.\n\u2022 Future Directions. We propose promising research directions and future trends in this area."}, {"title": "2 Preliminary", "content": "In this section, we first introduce some notations throughout the paper. Then, we will further elaborate on three concepts that are closely related to this survey: Graph Neural Networks, Large Language Model, and Trustworthiness LLM-GNN."}, {"title": "2.1 Notations", "content": "We represent a graph as $G=(V,E)$, where $V = \\{v_1,..., v_N\\}$ denotes the set of N nodes, and $E \u2286 V \u00d7 V$ represents the set of edges connecting these nodes. The graph can be either a plain graph (without attributes) or an attributed graph (with node features).\nIn the case of an attributed graph, each node $v_i$ is associated with a feature vector $x_i \u2208 R^d$, which represents the d-dimensional attribute of the node. All node attributes are collected as $X = \\{X_1,X_2..., X_N\\}.\nThe structure of the graph is captured by an adjacency matrix $A \u2208 R^{N\u00d7N}$, where each entry is defined as :\n$A_{ij} =\n\\begin{cases}\n1, & \\text{if nodes } v_i \\text{ and } v_j \\text{ are connected}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nThis adjacency matrix provides a mathematical representation of the graph's connectivity indicating which nodes share direct connections."}, {"title": "2.2 Graph Neural Networks", "content": "The core of Graph Neural Networks (GNNs) lies in updating node representations through the message-passing mechanism. GNNs utilize the message-passing mechanism to learn node representations that capture both node features and graph topology information. A GNN updates a node's representation by aggregating information from its neighboring nodes. Consequently, a k-layer GNN model captures local graph information within the k-hop neighborhood of the central node. where:\n\u2022 $h_v^{(k)}$ is the representation of node $v$ at the k-th layer,\n\u2022 COMBINE$^{(k)}$ is the function to combine the node's own features with aggregated features from its neighbors,\n\u2022 AGGREGATE$^{(k-1)}$ aggregates the representations of neighbors N(v) of node v at layer k \u2013 1.\nThrough message passing, GNNs can aggregate information from neighboring nodes. Additionally, by considering adjacency relationships when computing node representations, GNNs learn the entire graph's topology. Leveraging these properties, GNNs can perform various graph analysis tasks, including node classification, link prediction, graph classification, and community detection.\nNode classification aims to assign a category label to each node in the graph based on its structure and node features. In this task, a subset of node labels (training set) is known and used for training, while the remaining node labels (test set) are predicted. For example, node classification can help determine whether a user is a\"gamer\" or a \u201cmovie enthusiast\u201d.\nLink Prediction seeks to determine whether a connection (edge) exists between two nodes or to estimate the weight or type of a relationship represented by the edge. This task is widely used to infer unknown or potential edges, such as predicting missing links in a drug knowledge graph."}, {"title": "2.3 Large Language Model", "content": "Two influential surveys [Zhao et al., 2023; Motlagh et al., 2023] on distinguishing large language models (LLMs) from pre-trained language models (PLMs) differentiate them from two perspectives: model size and training approach. In terms of scale, LLMs are large language models with billions of parameters, such as GPT [Brown et al., 2020] and Deepseek [Guo et al., 2025], whereas PLMs are pre-trained models with millions of parameters. Notably, compared to PLM models, LLMs can learn new tasks from a small set of examples in the prompt during the inference stage without requiring additional fine-tuning. Notably, compared to the emergent abilities of PLMs, LLMs exhibit new capabilities, including in-context learning, instruction following, and multi-step reasoning.\nIn-context learning [Radford et al., 2021; Dong et al., 2022]: During the inference stage, LLMs can learn new tasks from a small set of examples presented in the prompt without requiring additional fine-tuning. Instruction following: After undergoing instruction tuning, LLMs can understand and execute new task instructions without explicit examples. Multi-step reasoning: LLMs possess multi-step reasoning capabilities, allowing them to break down complex tasks into intermediate reasoning steps, as demonstrated in the Chain-of-Thought (CoT) [Wei et al., 2022] prompting. This set of capabilities enables LLMs to handle complex tasks through few-shot or zero-shot learning.\nThe remarkable capabilities of LLMs are attributed to training on massive datasets and the design of the Transformer architecture with a large number of parameters. The Transformer architecture is the core technology of LLMs, with its multi-head self-attention mechanism and parallelized computation providing strong contextual understanding and the ability to capture long-range dependencies.\nAnother standout capability of LLMs is answer extraction from knowledge bases, allowing them to respond to specific questions about people, events, and places. They also exhibit the ability to perform logical reasoning based on multiple pieces of information, leveraging the extensive domain knowledge learned during training to draw relevant conclusions."}, {"title": "2.4 Trustworthiness LLM-GNN", "content": "The remarkable achievements of GNNs rely heavily on big data, as a large amount of sensitive data is collected from users to develop powerful GNN models for various services in critical domains. This raises potential privacy issues for GNN models. Membership inference attacks can determine whether certain users are included in the training data, threatening user privacy. Additionally, other privacy attack methods, such as link inference and attribute inference, can extract user information from pre-trained models.\nIn terms of trustworthy GNNs, several challenges persist in current research fields. For instance, due to the black-box nature of GNNs, their interpretability is insufficient. Moreover, the performance of trustworthy GNNs can deteriorate when dealing with perturbed data. LLMs can leverage their semantic capabilities to assist GNNs in providing rich sample interpretations in low-sample environments and can also help LLM-GNN models output readable reasoning processes to enhance interpretability."}, {"title": "3 Proposed Taxonomy", "content": "We propose a taxonomy, as shown in Figure 2, that classifies existing representative techniques involving the integration of LLMs with GNNs to enhance model trustworthiness into four main categories: (1) Reliability, (2) Robustness, (3) Privacy, and (4) Reasoning. In the following sections, we provide a comprehensive survey based on these four categories."}, {"title": "3.1 Reliability", "content": "The reliability of trustworthy GNNs lies in their ability to handle potential non-adversarial threats effectively, such as inherent noise and distribution shift.\nInherent noise refers to unavoidable noise in graph data, which can be categorized into structural noise, attribute noise, and label noise. Real-world graph data often contain inherent noise due to errors in data measurement and collection. For instance, structural noise may be introduced in protein-protein interaction networks [Fionda, 2019]due to errors during data collection. In social networks, users may intentionally provide false information for privacy reasons, leading to attribute noise. Label noise is similar to data collection, as errors may also occur during the process of labeling nodes or graphs, especially when the labeling requires expertise in specific domains [Ji et al., 2022].\nFor structural noise, one feasible approach is to assign learnable weights to each node during node information aggregation in GNNs, allowing the model to focus on task-relevant connections. This can be achieved by adopting a self-attention mechanism, such as Graph Attention Networks (GAT) [Chen et al., 2020], to adjust the aggregation weights in GNNs.However, such methods cannot effectively integrate textual information into the denoising process of GNNs, whereas LLMs can generate high-quality pseudo labels, reducing the impact of label noise.\nAnother approach is to utilize neural networks to learn the subgraph distribution and sample task-relevant subgraphs from it, thereby removing irrelevant edges. This involves [Luo et al., 2021] using multi-layer neural networks to predict and prune irrelevant edges, extracting subgraphs from a learned distribution to enhance model performance.\nCurrent applications of LLMs in trustworthy GNNs primarily focus on addressing inherent noise. LLMs can assist GNNs in mitigating structural noise in graph data. These applications mainly follow two directions: (1) LLMs are used to generate node embeddings, virtual nodes, and pseudo-labels, providing supplementary information for graph data. This approach emphasizes data generation and semantic augmentation. (2) LLMs directly interact with graph structures (in Figure 3), such as adding or removing edges, thereby influencing graph structure learning. By understanding the context of the graph or specific task requirements, LLMs can generate suggestions for graph edits.\nThe first approach leverages the strong performance of LLMs in zero-shot and few-shot scenarios, enabling trustworthy GNNs to handle inherent noise better [Li et al., 2024]. Through the semantic capabilities of LLMs, missing labels in the graph can be supplemented, and textual descriptions can enrich node information. With the augmented information, GNNs can predict and complete edges between nodes.\nThe second approach is based on the homophily assumption, where, in node classification tasks, the goal is to maximize connections between nodes of the same class while minimizing inter-class connections [Guo et al., 2024b]. LLMs evaluate the labels between node pairs to determine whether they belong to the same class, providing suggestions on the likelihood of an edge existing between them.\nLLM-GNN models utilize the semantic capabilities of LLMs to help GNNs supplement node information and understand node relationships, thereby reducing inherent noise in graph data. However, these two methods demonstrate varying performance in different scenarios: the first approach performs better in few-shot environments, while the second approach excels when textual information is abundant. Furthermore, current LLM-GNN research has a limited focus on addressing distribution shifts in graph data, which requires further exploration."}, {"title": "3.2 Robustness", "content": "Robustness refers to the ability of a system to perform consistently under various conditions. In the graph domain, robust GNNS can maintain model accuracy under perturbations such as malicious graph structure modifications. [Zhang et al., 2022; Zhang et al., 2023b; Zhang et al., ] A robust GNN can identify and defend against malicious attacks, such as node injection and edge tampering; adapt to dynamic changes in graph structure, such as the addition or removal of nodes and edges; and maintain stable performance in the face of random noise in the data. In short, graph robustness is a key metric for measuring the consistency of GNN performance in the face of various perturbations and uncertainties.LLMs provide a new perspective and tools for enhancing the adversarial robustness of GNNs through their powerful text understanding and inferential capabilities.\nLLMs have also been used in graph-related tasks to outperform traditional GNN-based methods and produce state of the art performance, but the robustness of these LLM integrated GNN models in the face of adversarial attacks is unknown. In order to fill this research gap, Guo et al. [Guo et al., 2024a] explored the robustness of LLMs in graph learning from a broad perspective, especially in the face of structural and textual perturbations. As shown in Figure 4(a), this paper uses the most popular approaches to utilize LLM for graph data, LLM-as-enhancer [Chen et al., 2024; He et al., 2023] and LLM-as-predictor [Chen et al., 2024; Chai et al., 2023]. The experimental results of the paper indicate that LLMs, acting as enhancers or predictors, offer better robustness than shallow models. The paper also provides additional analysis to explore the potential reasons for the robustness of LLMs in graph tasks. For example, for structural attacks, the higher the quality of the features, the less dependent the model is on the structure, so the high-quality features of LLM can be more robust against structural attacks. It offers an open-source benchmark library to promote further research in this field.\nThe research findings suggest that although LLMs can enhance the robustness of GNNs, there is still a significant decrease in the accuracy of GNNs when faced with high-proportion topological attacks. To address this challenge proposes an edge-enhanced framework, the LLM4RGNN [Zhang et al., 2024c], which takes advantage of the inferential capabilities of LLMs to identify malicious edges and recover missing important edges, thus restoring the robustness of the graph structure. As shown in Figure 4(b), this framework consists of two stages: the first stage involves using instruction tuning to fine-tune a local pre-trained model to identify malicious edges; the second stage employs an LM-based edge predictor to find missing important edges and purges malicious edges based on relevance scores to restore the attacked graph structure. This framework has demonstrated consistent improvements in robustness across various GNN models, indicating the practical application potential of LLMs in enhancing GNN robustness.\nFurther proposals or optimizations of LLM-GNN frameworks could be explored for application in a broader range of graph data and attack scenarios. Additionally, while LLMs can provide richer node feature representations, they are less effective in dealing with graph data that lacks textual attributes, which is also an important direction for future work."}, {"title": "3.3 Privacy", "content": "The outstanding performance of GNN models heavily depends on training with large datasets, which require collecting a vast amount of sensitive information to develop powerful GNN models capable of performing various downstream tasks, such as healthcare, bioinformatics, and banking systems [Wang et al., 2021]. However, collecting a vast amount of sensitive data from users for training GNNs raises privacy protection concerns. This issue leads to four types of attacks: membership inference attacks, inversion attacks, attribute inference attacks, and model extraction attacks.\nMembership inference attacks aim to determine whether a target sample belongs to the training dataset. Inversion attacks seek to recover the original topology of a graph or the attributes of target samples. For example, an attacker might attempt to infer the proportion of male and female users in a social network. Model extraction attacks involve replicating the functionality or knowledge of a target model by accessing its output results. This type of attack may lead to intellectual property loss and facilitate other privacy attacks.\nTraditional methods for protecting privacy in GNNs include differential privacy, federated learning, and machine unlearning:\n\u2022 Differential privacy introduces random noise into data or model outputs to ensure that attackers cannot ascertain whether a single sample participated in the training, thereby safeguarding data privacy.\n\u2022 Federated learning is a distributed machine learning framework in which participants train models locally, and only model parameters are transmitted to a central server for aggregation, enabling model training without sharing raw data.\n\u2022 Machine unlearning aims to remove the influence of specific data from a trained model. Unlike retraining the model, machine unlearning achieves efficient \"deletion\" through local adjustments to comply with privacy regulations such as the GDPR's \u201cright to be forgotten.\u201d\nResearch on privacy protection in LLM-GNN models is limited, and most studies focus on applying LLMs to GNN privacy attacks. Leveraging LLMs' generalization capabilities and ability to process multi-modal data, attackers fine-tune LLMs on specific datasets to create customized attack models. Additionally, in general, GNN-LLM models [Pan et al., 2024a], users often face challenges in locally deploying LLMs due to costs and operational complexity. Instead, users typically rely on publicly available LLM APIs, which can result in privacy leaks when sensitive data is uploaded online. One existing solution is to train a GNN model using an LLM and distill its performance (in figure 5) into a student model, thereby achieving LLM-optimized performance.\nThere is limited research on privacy protection in LLM-GNN models. In terms of protecting GNN privacy, LLMs could place greater emphasis on addressing privacy risks associated with pre-trained models. Since traditional studies on privacy attacks often focus on black-box scenarios and lack in-depth exploration of the risks of information leakage due to exposed model parameters, LLMs' strong ability to process diverse samples could be leveraged to optimize this aspect. Furthermore, personalized solutions could be developed to address the privacy protection needs of both open datasets and real-world data in the future."}, {"title": "3.4 Reasoning", "content": "Reasoning, which includes question answering, natural Language inference (NLI), and commonsense reasoning, is a fundamental skill for various NLP tasks [Liu et al., 2023]. Explainability of GNNS refers to the ability to make the predictions of GNNS transparent and understandable [Zhang et al., 2022]. For models that integrate GNNs with LLMs, LLMs brings reasoning capabilities to the model and also improves the explainability of the model.\nIn this aspect of trustworthiness, most of the existing studies on the integration of LLMs and GNNs utilize the reasoning ability of LLMs to improve the reasoning ability and explainability of the model. Therefore, we put reasoning and explainability together.\nLLMRG [Wang et al., 2023] is an innovative approach that leverages LLMs to construct personalized reasoning graphs, aiming to enhance the logical reasoning and interpretability of recommendation systems. As shown in Figure 6, this framework consists of four core components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge-base self-improvement. Specifically, it initially utilizes a large language model to build a personalized reasoning graph for each user, which includes the user's interaction sequences and attribute information and infers the user's interests and behavioral motivations through causal and logical inferences. Then, the reasoning graph is encoded into a format compatible with GNNs and integrated into traditional recommendation systems, allowing recommendations to benefit from both engineered algorithms and explanatory knowledge.\nWithin the LLMRG framework, reasoning is the core process that enables recommendation systems to simulate and predict complex user interests and behavioral patterns. Through chained graph reasoning, the system can identify logical connections between user behaviors; through divergent extension, the system can predict potential new interests of users; self-verification and scoring ensure the rationality of the reasoning process. This reasoning-based method not only improves the accuracy of recommendations but also provides interpretability for the recommendation results, allowing the recommendation system to display the logic behind the recommendations.\nGraphLLM [Chai et al., 2023] is an end-to-end method that integrates graph learning models with LLMs to enhance the reasoning capabilities of LLMs when dealing with graph data. As shown in Figure 7, the framework is implemented through three main steps. First, the node understanding uses a textual transformer encoder-decoder to extract key information from the textual descriptions of nodes. Second, the structure understanding employs a graph transformer to learn the graph structure by aggregating node representations. Finally, the graph-enhanced prefix tuning transforms the graph representation into a prefix and fine-tunes it within LLMs to enhance the performance of graph reasoning tasks.\nIn the GraphLLM framework, reasoning is key to processing and analyzing graph data. The node understanding step allows the model to extract semantic information crucial for graph reasoning from textual descriptions; the structure understanding step enables the model to comprehend the structural relationships between nodes; and graph-enhanced prefix tuning integrates this structured information directly into LLMs, enabling the model to generate accurate responses when performing graph reasoning tasks. The reasoning capability of this method is reflected in its ability to handle complex graph structures, identify relationships between nodes, and make accurate predictions and decisions in various graph reasoning tasks, thereby significantly improving the performance and applicability of LLMs in graph-related tasks.\nIn addition, GREASELM [Zhang et al., 2024b] adopts the LLMs multi-step inference method to construct a graph to decompose complex problems into multiple sub-problems, and LLMEP [Zhang et al., 2024a] is based on the Bayesian method of LLMs to alleviate the learning bias problem in the introductory tasks of GNNs and improve the reasoning ability of the model.\nIn terms of explainability, VGRL [Ji et al., 2024] proposes a fully interpretable method, based on prompt engineering to make multiple large models collaborate with each other, and constructs a set of cyclic iteration framework. During the iteration process, the decision basis, i.e., the attributes that should be included in each label, will be continuously updated as model parameters.LLM makes decisions by input features and decision basis, and updates the decision basis of the currently involved labels according to the positive or negative prediction results, and any of the steps in this process is characterized by a natural language description, which improves the model interpretability.\nFuture work can focus on optimizing the construction of personalized reasoning graphs to enhance the logicality and transparency of recommender systems. It will also strive to improve the understanding of graph data by LLMs, handle more complex reasoning tasks, and optimize the efficiency and credibility of the model. These research directions will advance the progress of LLMs in the fields of graph reasoning and recommendation systems, providing more powerful technical support for practical applications."}, {"title": "4 Future Research Directions", "content": "Integrating LLMs with GNNs to improve model trustworthiness is a rapidly growing research field with a wide range of methods and applications. Given the previous review and analysis, we believe that there is still much space for improvement in this field. In this section, we outline future research directions and highlight the great potential of utilizing the integration of LLMs and GNNs to boost other aspects of model trustworthiness.\nPrivacy Protection and Robustness Enhancement. Current LLM-GNN models face conflicts between traditional privacy protection methods, such as differential privacy and federated learning, and the characteristics of LLMs. In the future, it is necessary to explore collaborative training frameworks that enhance privacy, for example, by reducing the exposure of sensitive data through localized model distillation, or by leveraging the reasoning capabilities of LLMs to proactively identify adversarial attacks. Additionally, to address the decline in robustness caused by high-proportion topological attacks, further optimization of LLM-driven structural repair methods is needed. This can be achieved by combining dynamic edge enhancement and semantic verification to improve the stability of models under noise and adversarial perturbations. Moreover, there is a need to address the robustness challenges of non-textual graph data, such as molecular networks, and explore cross-modal alignment and structure-driven LLM adaptation strategies. It is worth mentioning that studies have pointed out that it is necessary to consider complex cross-aspect relationships when building trustworthy GNN systems [Zhang et al., 2022]. Therefore, when constructing a credible LLM-GNN model in the future, it is necessary to balance the conflicts between the credibility dimensions. For example, strict privacy protection may reduce model robustness, and future research needs to consider credible cross-aspect relationships.\nTrusted LLM-GNN with Low Text Dependency. Many existing works on integrating LLMs with GNNs to enhance trustworthiness focus on textual attribute graphs, or describe the nodes and relationships between nodes clearly with text, which reflects the high dependence on text in existing works. The increased workload caused by heavy text dependence, along with the need for broader application scenarios, highlights the need to develop trusted LLM-GNN models with reduced text dependence. An important direction for future research is to explore the integration of image features with graph structural data using LLMs for multimodal learning to improve model generalization. Another promising direction is to explore methods for handling text-free graphs, such as molecular structures, by leveraging the prior knowledge of LLMs. Trustworthy LLM-GNN models with reduced text dependence have broad applications. For example, gene interaction analysis and protein structure prediction in the biomedical field can be modeled as graph-based tasks with minimal textual information. Applying LLM-GNN models to these areas could lead to breakthroughs in biomedical research.\nOpportunities for the Rapid Development of LLMs. LLMs have developed rapidly in recent years and have produced tremendous changes, which have a great impact on academic research and people's production and life. The continued advancement of LLMs is expected to significantly enhance research on the trustworthiness of LLM-GNN models. With the advent of multimodal LLMs, such as GPT-4V [Yang et al., 2023], which facilitates the processing of graph data with image nodes. This suggests that future developments in LLMs will enable credible LLM-GNN models to support a wider range of applications while improving efficiency.\nFairness. As an important subfield of trustworthy, fairness-aware GNN aims to reduce bias to ensure fairness in the predictions of different groups. These groups are divided based on a multicomponent sensitive attribute. The fundamental assumptions and design of GNNs often lead to models achieving superior accuracy in certain groups. No work has applied LLMs to fairness-aware GNNs so far, so we propose some feasible ideas. Fairness-aware GNN can be designed to achieve fairness through different objectives. (1)Graph-level Fairness: connections between different sensitive attribute groups are balanced. (2)Neighborhood-level Fairness: node's neighborhood has a balanced distribution of sensitive attributes. (3)Embedding-level Fairness: node representations generated by GNN leak no sensitive attribute information. (4)Prediction-level Fairness: the final predictions are fair to different groups. The first two directions typically address bias at the graph or neighborhood level by modifying the graph structure before or during preprocessing, such as deleting highly homogeneous edges [Spinelli et al., 2021], increasing the weight of heterogeneous neighbors [Chen et al., 2022; Yang et al., 2024], or resampling to balance the distribution of sensitive attributes. However, these strategies for fairness enhancement in pure graph structures may not suit real-world applications. A fair model should go beyond simple data processing and focus on Text-Attributed Graphs, Text-Paired Graphs, or Multimodal Learning. LLMs can be used as fair data encoders to remove sensitive attributes from the paired text of nodes or to describe node text without leaking sensitive information. The latter two directions focus on designing equity-aware loss functions [Bose and Hamilton, 2019] or disentangling sensitive attributes from embeddings [Zhu et al., 2024]. However, the prior knowledge of the determined sensitive attribute has too strong a hypothesis, which makes it hard to judge whether the final embedding space covers other sensitive attributes that have not been paid attention to, and also leads to worsened interpretability. Promising solutions are to use LLMs as fairness aligners to supervise GNN-generated embeddings or as evaluators to assess model fairness, leveraging their textual reasoning capabilities to provide interpretable insights."}, {"title": "5 Conclusion", "content": "This paper systematically reviews recent developments in the field of trustworthy graph neural networks enhanced by large language models (LLM-GNNs), with a particular focus on the application of LLM technology in trustworthy GNN scenarios, such as reasoning, privacy, robustness, and reliability, as well as the key challenges in these areas. The semantic capabilities of LLMs significantly enhance trustworthy GNNs' ability to handle erroneous information. However, their impact on optimizing traditional privacy-preserving methods, such as differential privacy and federated learning, remains limited, indicating the need for further in-depth exploration of these techniques. Looking ahead, as LLM technology advances, challenges in data processing and model trustworthiness are expected to be further mitigated through more accurate and reliable responses from LLMs. This review offers researchers a comprehensive perspective on trustworthy LLM-GNNs, fostering both theoretical and practical advancements in the field."}]}