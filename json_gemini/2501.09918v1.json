{"title": "GenSC-6G: A Prototype Testbed for Integrated Generative AI, Quantum, and Semantic Communication", "authors": ["Brian E. Arfeto", "Shehbaz Tariq", "Uman Khalid", "Trung Q. Duong", "Hyundong Shin"], "abstract": "We introduce a prototyping testbed-GenSC-6G\u2014 developed to generate a comprehensive dataset that supports the integration of generative artificial intelligence (AI), quantum computing, and semantic communication for emerging sixth- generation (6G) applications. The GenSC-6G dataset is designed with noise-augmented synthetic data optimized for semantic de- coding, classification, and localization tasks, significantly enhanc- ing flexibility for diverse AI-driven communication applications. This adaptable prototype supports seamless modifications across baseline models, communication modules, and goal-oriented de- coders. Case studies demonstrate its application in lightweight classification, semantic upsampling, and edge-based language inference under noise conditions. The GenSC-6G dataset serves as a scalable and robust resource for developing goal-oriented communication systems tailored to the growing demands of 6G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of generative artificial intelligence (AI) with semantic communication (SC) marks a transfor- mative paradigm shift in communications, transitioning from basic data transmission to goal-oriented, context-aware infor- mation exchange [1], [2]. By leveraging advanced AI and foundation models [3], [4], these systems enhance efficiency and adaptability, tailoring transmissions to align with specific communicative goals. At its core, the SC employs a knowledge base (KB) with semantic encoders and decoders, prioritizing context and intent over raw data [5]. This innovative approach enables ultra-efficient compression, making it ideal for ap- plications such as Internet of Things (IoT), cloud services, autonomous systems, and other cutting-edge sixth-generation (6G) use cases, as illustrated in Fig. 1.\nRecent advancements have demonstrated the successful integration of advanced AI and SC systems, paving the way for more adaptive and intelligent communication networks [6]. Building on this progress, numerous studies have explored how generative AI can enhance data generation and transmission quality in 6G networks [7], [8]. For example, generative AI has been utilized at the network edge to improve visual data transmission quality by leveraging multimodal data inputs [9]. Similarly, integrating AI into communication devices has facilitated support for diverse data formats and translation"}, {"title": "II. GENSC-6G DATASET STRUCTURES", "content": "The GenSC-6G dataset is meticulously organized to support machine learning (ML) tasks, including classification, segmentation, object detection, and edge LLM tasks. Each ML or semantic task is associated with a standalone ground-truth data collection or a combination of multiple collections in a generic format. These collections achieve dual objectives: i) they ensure that models can be effectively trained and evaluated across different and interconnected tasks; and ii) the generic format of the dataset provides scalability and flexibility for SC tasks, enabling easy modification of wireless methods, parameters, or noise levels. This adaptability allows it to serve goal-oriented tasks in any environment. The dataset is available at https://github.com/CQILAB-Official/GenSC-6G.\nEach collection of semantic tasks is structured as follows.\n1) Ground-Truth Data: The ground truth data collection is precisely annotated for each of the 15 defined classes, with cor- responding class and segmented labels. This dataset includes a total of 4,829 instances for training and 1,320 instances for testing. The context of the dataset pertains to common vehicle types in both military and civilian sectors. The input data collection is designed for the semantic upsampling task.\n2) Base-Model Features: The dataset includes features ex- tracted from base models in the form of matrices, as illustrated"}, {"title": "III. PROTOTYPING A LARGE-AI SC TESTBED", "content": "In this section, we present a modular framework integrating large AI models and HQC computing with SC to enhance data generation, transmission efficiency, and adaptability within the joint source-channel coding (JSCC) framework.\nA. Generative AI Auto Dataset\nWe first introduce the concept of dataset auto-creation by integrating diffusion models and automated mechanisms to streamline data generation, labeling, and training processes.\n1) Diffusion-Driven Data Generator: The GenSC-6G testbed leverages text-to-image diffusion models to automate the generation of diverse and trainable images for the base model. This framework uses the latent diffusion model (LDM), which transforms high-dimensional data into low-dimensional latent spaces, optimizing computational efficiency, output quality, and diversity. The LDM operates in two main stages: forward diffusion, n which an image is progressively trans- formed into a noisy representation by adding Gaussian noise over several steps, and reverse diffusion, where this noisy la- tent representation is iteratively denoised back into a coherent image guided by text prompts.\n2) Auto Inference Mechanism: With the GenSC-6G dataset pipeline, an auto-generation mechanism is employed in real time to continuously generate new data. The workflow in- volves feeding the trained model dynamically with input data through a diffusion model, producing diverse data instances. These instances are then labeled automatically and passed to the training pipeline, with the model saved for subsequent training sections. A masked autoencoder model with a vision transformer (ViT) such as the segment anything model (SAM) or you only look once (YOLO) can be potentially employed for auto-localization tasks (instance segmentation and object detection). This approach allows for consecutive model train- ing without the need for manual data collection.\nB. Bandwidth-Efficient SC\nEfficient compression of semantic data is critical to trans- mitting relevant information without exhausting resources. The model compression mechanism, such as entropy cod- ing, represents the most frequent semantic components with"}, {"title": "C. Base-System Model", "content": "We now outline the framework components responsible for encoding, transmission, and task-aware decoding.\n1) Feature Extraction: In the GenSC-6G framework, the base-system model relies on an alterable backbone encoder that can be replaced with any feature extraction network. This flexibility allows for the use of various deep learning (DL) models, such as convolutional neural networks (CNNs) or ViTs, depending on the target application. Both models extract semantic features from input data but differ in spatial process- ing: CNNs use convolutional filters, while ViTs utilize self- attention mechanisms to capture global context. As shown in Fig. 2, the backbone encoder begins by passing the input data through several convolutional layers, progressively reducing data dimensionality while retaining key semantic components throughout the patching and filters.\n2) Semantic Compression: The semantic compression pro- cess is performed by extracting and prioritizing only the most relevant features from the input data. A critical step of this compression process is quantization. After feature extraction, the data is quantized to map the continuous feature space into a discrete set of values, reducing data precision in a controlled manner. The backbone maintains a balance between compression efficiency and semantic relevance preservation. To minimize information loss, a loss function is typically de- signed to penalize discrepancies between the original data and output, ensuring that the most critical features for downstream tasks are preserved during training.\n3) Pretrained Model: The backbone encoders can be con- ditionally replaced between pretrained and non-trained net- works. To accelerate learning, pretrained networks, such as the residual network (ResNet), swin (shifted window) transformer (SwinT), or ViT, have been trained on large-scale datasets like ImageNet. By initializing the backbone encoder with a pretrained model, we leverage learned feature representations, improving the convergence speed and sustainability."}, {"title": "D. Mobile DL", "content": "To accommodate the limited computational resources avail- able on edge devices, the backbone framework is alterable in the optional way to use lightweight neural networks such as EfficientNet-B1 and MobileNet. These models feature significantly fewer parameters compared to traditional DL models, effectively reducing the number of parameters and computational complexity at each layer. As shown in Table I, this reduction translates into faster inference and load times, which are crucial for real-time applications. EfficientNet-B1 and MobileNet achieve this by optimizing network layers, employing depthwise separable convolutions, and reducing the overall model size, all while striving to maintain high accuracy with lower computational demands."}, {"title": "E. Quantum Parallel Processing", "content": "HQC computation in the large-AI SC testbed framework combines quantum processing units (QPUs) with traditional graphics processing units (GPUs) to execute DL tasks more efficiently. QPUs work alongside GPUs to handle tasks such as HQC optimization, state preparation, and sampling, which complement GPU operations on matrix multiplication and dense processing. The testbed framework employs skip con- nections and resource splitting, enabling QPUs to manage quantum-specific computations while GPUs process standard neural network layers. Specifically, to implement this, the model architecture is enhanced with quantum layers, consist- ing of a basic entanglement layer and amplitude embedding. The skip connection features are conditionally encoded into these quantum layers for QPUs. In general, this task division helps manage computational loads on backbone feature ex- traction, decoding, and inference. Hence, the framework incor- porates quantum kernels, which introduce quantum properties into the learning process to enhance model performance. These quantum kernels, in the form of embeddings within QPUs, map data into high-dimensional quantum Hilbert spaces, pro- viding more powerful data representations that can improve classification accuracy and decision-making processes. This hybrid approach allows the system to dynamically offload complex tasks to cloud-based QPUs when on-premises hard- ware reaches its computational limits, ensuring scalability with future advancements in processing unit technology."}, {"title": "F. Classical and Quantum JSCC Modules", "content": "The GenSC-6G JSCC module is designed to evaluate the quality degradation and performance of data transmission under both classical and quantum channel conditions.\n1) Classical Channel: The classical channel in the GenSC- 6G prototype is configured to emulate realistic communication scenarios, particularly focusing on noise conditions, modu- lation schemes, and transmission protocols representative of current and next-generation communication standards.\nChannel Noise: Noise is implemented within the testbed to replicate real-world communication scenarios where data transmission is disturbed. In this case, the sys- tem model simulates noise conditions by incorporating AWGN with a specific signal-to-noise ratio (SNR) and random transmitter noise from a programmable software- defined radio (SDR) to dynamically adjust noise char- acteristics. An example setup is shown in Fig. 2. By injecting controlled and random noise levels during trans- mission, the system can assess the impact on seman- tic data, helping to refine error correction and noise mitigation within the network decoder. The DL model adapts to varying noise conditions by optimizing its parameters to minimize the loss function, which measures the discrepancy between the predicted noisy outputs and the ground-truth data.\nJSCC Transmission: The prototype utilizes JSCC to efficiently transmit semantic information over bandwidth- limited channels. By combining source compression and channel coding within a single DL framework, the sys- tem learns an end-to-end mapping from input data to transmitted signals and from received signals to recon- structed outputs. In this process, the semantic features are directly mapped into channel symbols, bypassing traditional separate source and channel coding schemes. The testbed transmission employs orthogonal frequency- division multiplexing (OFDM) with Wi-Fi 7 (802.11be) (see Fig. 3). The system can be adapted to various fre- quency bands, including THz and sub-THz bands, which provide extensive bandwidth for high-speed semantic applications."}, {"title": "G. Case Study", "content": "We now present case studies demonstrating the application of the GenSC-6G prototype to various semantic decoding tasks. Each task utilizes the compressed semantic features transmitted over the communication channel and focuses on different aspects of semantic understanding. We evaluate the performance using relevant metrics and provide benchmarks for comparison.\n1) Lightweight Classification: The semantic classification task involves categorizing images into predefined classes based on their content. Using the GenSC-6G dataset, we train sev- eral baseline models, including ViT-L-32, ResNet-50, visual geometry group (VGG)-16, and Inception-V3, on single or combined processing units, as well as lightweight models like EfficientNet-B1 and MobileNet-V3 suited for edge devices. All baseline models are trained under various SNR conditions (e.g., at 10 dB and 30 dB) to evaluate their robustness to noise in the communication channel. The decoder architecture is defined as a lightweight module with three fully connected layers designed to downsample the features extracted by the encoder. Fig. 4 shows the confusion matrix for the ResNet- 50 model under AWGN with the SNR of 10dB (lower-left plot), illustrating the balance between true and false positive rates. The model achieves an accuracy of 84.47% (see Table I), demonstrating robust-but improvable-baseline performance in noisy conditions. Notably, the highest baseline accuracy is achieved by EfficientNet-B1 with a score of 86.89% while maintaining a mobile-friendly architecture. Additional metrics, including F1 score and recall, are also provided in Table I. By focusing on transmitting essential semantic features and utilizing lightweight decoders, the models demonstrate that high classification accuracy is achievable even in the presence of significant channel noise. Furthermore, these compressed features are not only effective for classification tasks but are also reusable for other semantic decoding tasks in this case study, as they retain rich semantic representations of the input data. This reusability highlights their efficiency for multiple downstream AI tasks.\n2) Semantic Localization: We first utilize the YOLO model for object detection of vehicles within the images from the GenSC-6G dataset. After detecting the vehicles with YOLO, we perform semantic segmentation to achieve pixel- level localization. The ground truth provided by the GenSC- 6G segments is used to train and validate the segmentation models. By leveraging these detailed annotations, the models learned to accurately delineate vehicle boundaries, enhancing the localization precision. As shown in Fig. 4 (upper plot), we use localization metrics such as the intersection over union (IoU) and mean pixel accuracy (MPA). The higher IoU for Image 1 at the SNR of 10 dB indicates better overlap between the predicted segmentation and the ground truth, suggesting accurate vehicle localization by the model in that instance. The MPA values, while relatively low, provide insight into pixel-level classification accuracy across the entire image, including both the object and the background. The close MPA values between the two images indicate consistent pixel-level performance, though there is room for improvement, especially in challenging conditions. These results demonstrate that the SC framework effectively supports semantic localization tasks by preserving essential spatial features necessary for accurate object detection and segmentation.\n3) Semantic Upsampling Recovery: The semantic upsam- pling recovery task focuses on enhancing low-resolution im- ages received over noisy channels by reconstructing high- resolution outputs. Two distinct approaches are evaluated for upsampling combined with feature upsampling (FeatUp) for two baseline models (ResNet and ViT). FeatUp enhances deep features by restoring lost spatial information through high-resolution signal guidance or implicit modeling to im- prove performance in dense prediction tasks. To ascertain upsampling recovery performance, we evaluate the learned perceptual image patch similarity (LPIPS) and the peak SNR (PSNR) for the GenSC-6G dataset in Fig. 4 (lower-right plot). As depicted, there is an inverse relationship between the probability of accurate reconstruction and the LPIPS score. Lower LPIPS values indicate that reconstructed images are perceptually closer to the ground truth. The empirical distri- bution demonstrates that the ResNet with FeatUp has a higher probability density in the lower LPIPS score range (from 0.05 to 0.15), indicating more accurate reconstructions, while the ViT baseline with FeatUp remains notable performance. The PSNR performance is also depicted as a function of SNR. Here, ResNet-FeatUp outperforms ViT-FeatUp again, espe- cially at high SNR values, demonstrating considerable noise resilience and image fidelity during upsampling recovery.\n4) Edge LLM: The edge LLM task involves integrating advanced models, such as the bootstrapping language-image pretraining (BLIP), generative pretrained transformer (GPT), and LLM Meta AI (LLaMA or large language model Meta AI), into edge environments to generate text from semantic features extracted from visual inputs, enriching the output semantics. As depicted in Fig. 2, the prototype architecture employs a feature encoder combined with a querying transformer. These queries are then input into pretrained LLMs that specialize in transforming visual-semantic representations into meaningful text outputs. The visual-text encoder architecture plays a piv- otal role in this setup, combining visual feature extraction with a text generation pipeline. The contrastive language-image pretraining score (CLIP-S) is used to measure the alignment between the generated text and the visual context (see Fig. 4 upper plot). The CLIP-S evaluates how well the descriptions or captions match the visual input, reflecting the effectiveness of generated text in comprehending the scene. For example, in GenSC-6G image captioning, LLaMA-3 reaches a CLIP-S of 35.52 for Image 1, demonstrating its level of contextual understanding and the richness of the text generated from the image."}, {"title": "IV. OPEN CHALLENGES AND CONCLUSION", "content": "Despite the results demonstrated by the GenSC-6G frame- work, several challenges remain in fully realizing its poten- tial. Sustainability is a significant concern, particularly in maintaining energy-efficient operations for large models and continuous data processing at the edges. This is critical for ensuring that AI-driven SC systems align with the sustain- ability goals of future networks. Model robustness in the face of varying noise conditions and unpredictable environments also remains challenging. Deploying these models on mobile devices poses another challenge, as many of the state-of-the-art AI models, including LLM, are computationally intensive and difficult to scale down to the limited resources of edge devices. Solutions such as model pruning, quantization, and efficient architecture designs require further refinement to enable real- time on-device processing. Generalization to more downstream AI tasks, including complex multimodal tasks, is also crucial to expand the SC utility. Finally, quantum communication and computing face challenges in ensuring noise resilience, enhancing scalability, and managing the complexity of parallel processing.\nThis paper has introduced the GenSC-6G framework, which integrates large AI, HQC optimization, and SC, tailored to optimize 6G networks through scalable and alterable commu- nications models. The testbed offers a flexible prototype that enables the modification of baseline models, communication modules, and goal-oriented decoders, supporting a variety of downstream tasks. This modular framework leverages gener- ative AI to enhance the KB by generating realistic synthetic data, thus improving model diversity and adaptability in real- world scenarios. Through the detailed case studies, we have demonstrated the effectiveness of our approach in various semantic decoding tasks, including lightweight classification, semantic localization, and upsampling recovery. Evaluations across different communication conditions, as seen in down- stream tasks such as semantic classification and edge LLM, highlight the practical adaptability of the GenSC-6G frame- work in a wide range of semantic tasks."}]}