{"title": "Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models", "authors": ["Che Zhang", "Zhenyang Xiao", "Chengcheng Han", "Yixin Lian", "Yuejian Fang"], "abstract": "Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed \"Step CoT Check\". Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models fine-tuned with the \"Step CoT Check\" prompt against those refined using other promps within the context of checking-correction data. The \"Step CoT Check\" outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness.", "sections": [{"title": "1 Introduction", "content": "The colossal number of parameters and extensive training data of Large Language Models (LLMs) have led to remarkable performances in reasoning tasks (Yao et al., 2023b; Wei et al., 2023), notably in mathematical problem-solving (Luo et al., 2023; Wang et al., 2023; Yu et al., 2023; Yuan et al., 2023). Despite these advancements, there is still considerable potential for enhancing the mathematical capabilities of LLMs.\nTo further enhance the mathematical reasoning capabilities of LLMs, self-correction has been proposed. Previous studies have shown that self-corrected LLMs can achieve higher accuracy (Madaan et al., 2023a; Weng et al., 2023; Huang et al., 2022a; Saunders et al., 2022; Welleck et al., 2022; Miao et al., 2023; Chen et al., 2023). However, recent research (Huang et al., 2023a; Stechly et al., 2023; Valmeekam et al., 2023) indicates that such improvements often depend on external feedback, such as ground truth labels for determining the stopping point (Shinn et al., 2023; Kim et al., 2023). This reliance raises questions about the extent to which LLMs can autonomously self-correct their reasoning processes.\nIn this paper, we explore how to enhance the self-correction capability of LLMs. Inspired by Tyen et al. (2024), we divide self-correction into two distinct tasks: Self-Checking Feedback Gathering and Feedback-Driven Reasoning. We believe that the key is to address their inherent deficiency in self-checking capabilities and thereby obtain more accurate feedback. (Stechly et al., 2023; An et al., 2023). In order to reach this, we construct a checking-correction dataset and trying to training the model. Additionally, to ensure the model's capability in direct reasoning, we incorporate Direct Reasoning into our training approach. Our goal is to equip the model with strong self-checking capabilities, making it independent of external feedback or ground truth labels. We compared the performance of models trained with checking-correction data in three formats of prompt: 1. All Direct Check, 2. Step Direct Check, and 3. Step CoT Check"}, {"title": "2 Related Work", "content": "Chain-of-Thought (CoT) reasoning, initially pioneered by Wei et al. (2022), has significantly advanced the reasoning capabilities of large language models (LLMs). This approach, which enhances LLMs by incorporating detailed reasoning steps into few-shot examples, has been foundational in improving performance across various reasoning tasks. Subsequent research has expanded upon this, introducing methodologies like Self-Consistency (Wang et al., 2022), various prompting techniques (Zhou et al., 2022; Drozdov et al., 2022), Self-Training (Huang et al., 2022b), Verifier (Li et al., 2022), Tree of Thought (Yao et al., 2023a) and DialCoT (Han et al., 2023). The effectiveness of CoT has been demonstrated in domains including logical, commonsense, and mathematical reasoning. Despite the success, these models are not without their flaws. They are still prone to reasoning confusion, generating hallucinations, and computational errors. A prevailing strategy to rectify these undesired behaviors of LLMs is learning from feedback. Extensive research (Huang et al., 2022b; Madaan et al., 2023b; Gero et al., 2023; Jiang et al., 2023) has been undertaken to improve LLMs through the paradigm of learning from feedback. However,Huang et al. (2023b) found that self-improvement is not effective when oracle labels are not available. This may be attributed to the limited capability of LLMs to identify errors. Previous works have primarily focused on self-correction, overlooking the enhancement of the models' error identification capabilities. This paper addresses the gap by meticulously constructing training data that includes steps for detecting and correcting errors, thereby strengthening the model's capability to identify mistakes."}, {"title": "3 Methodologies", "content": ""}, {"title": "3.1 Self-Correction Mechanism Analysis", "content": "Assuming a dataset comprising N questions, the whole process serves as the direct reasoning, with self-checking feedback gathering. Should the feedback indicate an error, the model re-engages in reasoning based on that feedback.\nInitially, we simplify the problem by assuming the model's direct reasoning accuracy for all questions is p, the probability of the model marking a correct answer as incorrect is $p_{c2w}$, and the probability of marking an incorrect answer as incorrect is $p_{w2w}$. According to our hypothesis, a model untrained in self-checking lacks this capability and randomly assigns correct or incorrect labels; hence, we assume $P_{c2w} = P_{w2w} = \\frac{1}{2}$.\n1. Following one round of reasoning, we obtain D(correct) = Np and D(wrong) = N(1 \u2013 p), where D is the quantity.\n2. Upon correctness validation, half of the D(correct) answers are incorrectly labeled, denoted as D(c2w) = $\\frac{1}{2}Np$, and half of the D(wrong) answers remain erroneously labeled, represented as D(w2w) = $\\frac{1}{2}N(1 \u2013 p)$.\n3. Re-reasoning the answers marked as incorrect, $\\frac{1}{2}Np(1-p)$ from D(c2w) are reasoned incorrectly, while $\\frac{1}{2}N(1 \u2013 p)p$ from D(w2w) are corrected.\nPost self-correction, we observe no change in accuracy. In practice, Although the model has some capability to determine whether an answer is correct or not, i.e., the accuracy of the check is higher than 50%, the model often makes mistakes in complex examples, and the probability of a second reasoning to correct it is less than p. And the probability of an originally correct example being changed to an incorrect example is much higher. Intuitively, enhancing the model's self-correction capability, according to the reasoning above, would decrease $p_{c2w}$ and increase $p_{w2w}$, leading to a reduction in D(c2w) and an increase in D(w2w), thereby enabling effective self-correction. The potential accuracy improvement through self-correction is quantified as:\n$(p_{w2w} - p_{c2w})N(1 \u2013 p)p$.\nWhen external ground truth labels assist the process, the reasoning outcomes that match these labels do not undergo self-correction. This is succinctly expressed as $p_{c2w}$ = 0, bringing improvements in accuracy.\nFrom this observation, we infer that enhancing the self-checking capabilities of LLMs can, to a certain extent, replace the need for external ground truth labels, thereby ensuring that the improvement in accuracy is greater than zero. Consequently, this enhances the self-correction capability of LLMs."}, {"title": "3.2 Checking-Correction Data Generation", "content": ""}, {"title": "3.2.1 Checking Data Generation", "content": "We use a generator model $M_g$ to collect question-answer pair $x \\in X$ that lead to incorrect results. x can be decomposed into (q, a), where q represents the question and a represents the answer that contains errors in its reasoning path. We further break down a into fine-grained intermediate steps $s_1, s_2, ..., s_K$ (An et al., 2023), with each sk representing a step in the reasoning process.\nA feedback model $M_f$ is prepared, for all x, we have $(R_1, label_1),\u2026\u2026\u2026, (R_k, label_k)$ $\\sim M_f(q, a, A)$, where k denotes the step number at which the first error occurs. $M_f(q, a, A)$ represents the process of examining each step within a incorrect answer a given a question q and a correct answer A, continuing until an error is detected, with $label \\in \\{True, False\\}$. $R_k$ represents the detailed CoT check for step $s_k$.\nWe experiment with three distinct prompting strategies to generate training data. Assuming the model to be trained is denoted as M.\nAll Direct Check: The entire pair x = (q,a) is input into M. For an x containing errors in its reasoning path, M directly identifies the erroneous step, i.e., $k \\sim M(q, a)$.\nStep Direct Check: Due to the limited capabilities of the smaller model, we employ M checking each step individually for correctness. For all intermediate steps $s_k$, $k \\in [1, K]$, to provide M with a holistic view when checking $s_k$, the complete answer a is also input. M renders a more accurate and clear judgment by considering the steps following $s_k$ that need to be assessed. We obtain $label_k \\sim M((q, a), s_k)$, where $M((q, a), s_k)$ indicates checking the correctness of step $s_k$ given the question and the full answer. Due to the unique nature of mathematical reasoning, the check terminates upon finding the first labelk = False, and subsequent steps are disregarded (Lightman et al., 2023).\nStep CoT Check: Unlike the Step Direct Check, which outputs a label for each step directly, this method requires the model to analyze step sk for correctness from three dimensions:\n1. Reasoning Goal Appropriateness: The model assesses whether the computational goal of $s_k$ is conducive to solving the question. If the step's objective is unreasonable or redundant, it is deemed incorrect.\n2. Computational Expression Correctness: The model identifies the known conditions related to $s_k$'s computational goal and evaluates if the computation aligns with the problem statement and whether the derived expression achieves the computational goal correctly.\n3. Verification through Inverse Operations: The correctness of the computation is checked by performing inverse operations on the derived expression."}, {"title": "3.2.2 Correction Data Generation", "content": "For the correction model $M_c$, it is required to generate a revised answer given (q, a, $F_k$), where a contains incorrect steps. We can derive c ~ $M_c((q, a, F_k))$, where c represents the corrected answer."}, {"title": "3.2.3 Details of Data Generation", "content": "We use GPT-4-1106-preview (OpenAI et al., 2023) as the feedback model $M_f$ and LLaMA-7b,LLaMA-13b (Touvron et al., 2023a) as the generator model $M_g$, which is fine-tuned with one epoch on the GSM8K (Cobbe et al., 2021) training set in order to be able to learn the CoT format. After getting step-level checking, to save the costs, we used GPT-3.5-turbo (OpenAI, 2023) as a correction model $M_c$ to correct errors in the original reasoning paths. We filter out instances that have been successfully rectified to correct answers, ultimately yielding approximately 4,500 checking-corection data points for faulty reasoning. Additionally, we select 2,000 correct CoT traces and generate confirmation checks on them."}, {"title": "3.3 Data Quality", "content": "We manually review 50 entries from the Step CoT Checking dataset generated by GPT-4 to ensure data quality. Among these, 43 entries accurately identify the erroneous step and provide precise analysis, while 5 entries pinpoint the incorrect step but offer an analysis that lacks sufficient accuracy. 2 entries fail to identify the location of the error successfully. We conclude that the data generated by GPT-4 meets a relatively reliable standard."}, {"title": "3.4 Finetune", "content": "We integrate data from Direct Reasoning, Check, and Feedback-Driven Reasoning for training, a process we term Feedback-Enhanced Reasoning Fine-tuning. Within the three settings of the Check task, the primary distinction in training lies in the differentiation between All Direct and Step Check.\n\u2022 Direct Reasoning + All Direct Check + Feedback-Driven Reasoning: Since the All Direct Check provides the question and answer, directly identifying the incorrect steps, the volume of training data depends solely on the size of the question-answer pair set X, irrespective of the number of steps.\n\u2022 Direct Reasoning + Step Check + Feedback-Driven Reasoning: Step Check necessitates that the model learns the correctness or error of each step, so the total volume of training data is contingent on both the size of set X and the number of steps."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "CoT Datasets Our research employs the GSM8K dataset (Cobbe et al., 2021), consisting of 7,473 arithmetic problems at the elementary school level. To facilitate comparison, except for generating the checking-corection data, we did not use any other generated CoT traces to train our models.\nMix Data We procured a composite dataset amalgamating CoT data with checking-correction data to form the training corpus. An extensive series of experimental investigations was undertaken to ascertain the ideal proportion of inaccurately corrected to accurately validated instances within a corpus of 4,000 annotated correction records. Ultimately, we determined the optimal dataset composition for each evaluative approach. For the \u201cAll Direct Check\" and \"Step Direct Check\u201d methodologies, a data distribution of 3,000 inaccurately corrected to 1,000 accurately validated instances was selected. Conversely, for the \"Step CoT Check\" approach, a skewed data ratio of 3,700 to 300 was deemed optimal. A comprehensive exposition of experimental outcomes and the corresponding analysis regarding data ratios is delineated in Section 4.5."}, {"title": "4.2 Fine-Tuning and Evaluation", "content": "We experiment on LLaMA-2 (Touvron et al., 2023b) models with 7B and 13B parameters, trained on 4* Nvidia A100 80GB GPUs. The models undergo full parameter fine-tuning, employing a learning rate of 8 \u00d7 10\u20136, over three epochs, with a cumulative batch size of 32.\nThe effectiveness of the checkpoints is evaluate on the GSM8K benchmark and other three additional datasets: MultiArith (Roy and Roth, 2015), ASDiv (Miao et al., 2020), and SVAMP (Patel et al., 2021). Our evaluation strategy validates our method's resilience across varied data distributions. Each dataset contains elementary mathematical problems with distinct lexical patterns. We test our approach on the full 1,319-instance GSM8K set, and on randomly sampled subsets from MultiArith (400), ASDiv (500), and SVAMP (500). To fairly evaluate model capability in verifying CoT trace accuracy, we manually annotate the first incorrect step in 200 sampled GSM8K traces generated by LLMs, which has 100 correct traces and 100 incorrect.\nCheckpoint evaluations are conducted using the VLLM library (Kwon et al., 2023). For direct reasoning path generation, we set the temperature parameter to zero. The check and correction mechanism employs a temperature of 0.5, complemented by a top-p value of 0.85 and a top-k setting of 30. For all reasoning tasks, we apply a repetition penalty of 1.2 (Artiges et al., 2019), which effectively mitigates the generation of duplicate content."}, {"title": "4.3 Main Results and Analysis", "content": "Capability to Check Tab. 1 delineates the outcomes of our evaluation across 200 CoT traces. Our analysis extends to models prompted via a few-shot technique without fine-tuning, alongside GPT-3.5 and GPT-4. These models are tasked with adjudicating the veracity of entire reasoning paths, analogous to our \"All Direct Check\" method. The base LLaMA models exhibit marginal superiority over random chance (50%) in their checking capability. Despite in direct reasoning task, GPT-3.5-turbo attaining a 75.9% (Tyen et al., 2024) accuracy on GSM8K with standard prompting, its proficiency in evaluating reasoning paths is markedly deficient, with a mere 70% success rate in accurate judgment. GPT-4 demonstrates a substantial improvement; however, it is noteworthy that GPT-4's direct inference accuracy on GSM8K stands at an exceptional 95.5% (Tyen et al., 2024). This discrepancy suggests that self-correction in these LLMs does not enhance reasoning task performance due to a dissonance between the capability to reason and to check reasoning traces. In essence, the LLMs exhibit significantly lower precision in verifying the correctness of a reasoning trace compared to direct problem solving for the same question (q), thereby increasing the likelihood of erroneously altering a correct response.\nOur methodology posits that if any step in a trace is deemed incorrect, the entire answer is classified as incorrect. All the approaches surpass a 70% accuracy threshold, signifying a noteworthy enhancement over the unrefined base models. This evidences that our model, through training, augments its self-checking capabilities. The LLaMA-2-13B model, in particular, demonstrates superior performance in the \u201cStep CoT Check\u201d, not only achieving an 81.5% accuracy in trace evaluation but also exhibiting an elevated success rate in identifying the initial incorrect step, compared to alternative methods. However, the \"Step CoT Check\" does not maintain a discernible edge on the LLaMA-2-7B model, suggesting that this strategy is more efficacious with models possessing larger parameters. Additionally, the \u201cStep Direct Check\u201d methodology exhibits a lower likelihood of pinpointing the first incorrect step relative to the \u201cAll Direct Check\" approach. This is attributed to the potential of misidentifying a preceding correct step as erroneous, in contrast to the \"All Direct Check\" which renders a singular judgment, enhancing the probability of detecting the first incorrect step."}, {"title": "Performance Of Reasoning Combined Self-correct", "content": "Tab. 2 and Fig. 2 present the efficacy of models trained using our three methods on test sets. Across all models, a univocal enhancement in accuracy post a self-correction is observed for each dataset. Subsequent to the substitution of a subset of the data with checking-correction instances, there is a modest decrement in the models' direct reasoning accuracy. This divergence from the findings of prior study (An et al., 2023) is ascribed to the distinct ratios of direct CoT data to correction data employed in our experimental setup. Models trained on mixed data consistently surpass those solely fine-tuned on CoT across the majority of datasets. Notably, the improvement on LLaMA-2-13B models is more pronounced than on LLaMA-2-7B models, with an average increase of 3.78% for the latter and 5.47% for the former. This discrepancy is largely attributable to the more substantial reduction in direct reasoning performance on LLaMA-2-7B post data substitution. On LLaMA-2-13B, the \u201cStep CoT Check\" method significantly outperforms the other check methodologies, indicate that on models with larger parameters, By categorizing the types of errors and explicit displaying the process of checking, \u201cStep CoT Check\" is possible to provide more precise feedback. The precision in adjudicating the correctness of answers aligns with the outcomes from the sampled 200 reasoning paths. However, it is noteworthy that LLaMA-2-7B exhibits marked improvements, attributable to the preponderance of erroneous traces in the initial sample, which introduced inconsistencies.\nFurthermore, despite the lower likelihood of accurately identifying the precise erroneous step in the LLaMA-2-7B model, self-correction induces improvement. This suggests that the model's capacity to recognize an incorrect answer\u2014albeit without pinpointing the error\u2014affords a substantial number of incorrect paths the opportunity for regeneration and subsequent rectification."}, {"title": "4.4 Ablation Study", "content": "To examine the propensity of LLMs to commit specific errors and to scrutinize our tripartite methodology for assessing the validity of each step in the reasoning sequence, we conducted an ablation study focusing on the \u201cStep CoT Check\u201d implemented on the LLaMA-2-13B model. The findings are catalogued in Tab. 3. Of the 541 incorrect paths accurately detected, 20.1% were amended successfully. Predominantly, errors within the computational procedures emerged as the most frequent, constituting approximately 61.1% of the total errors identified. Such errors typically arise from LLMs misjudging question relationships or erroneously invoking nonexistent conditions, leading to incorrect equation formulation. These computational missteps proved challenging to rectify, with a mere 18% being corrected during the third step, signaling an avenue for future enhancement.\nGoal errors, conversely, represent a category of errors more amenable to correction, with a rectification rate of 24%. Regarding computation, LLMs were found to be more susceptible to miscalculations when the equation involved multiple numerical entities or integrated unknowns."}, {"title": "4.5 Further Analysis on Data Proportion", "content": "The experiments in this section use the Step Check prompt, and fine tune on LLama-2-13B.\nThe performance of the check has a strong relationship with the data ratio. To discern the influence of the proportion of correct step confirmations to incorrect step corrections within a pool of 4,000 training data replacements, we adjusted their ratios and documented the outcomes in Tab.4 . On average, a correct-checking datum comprised 3.33 verified steps, whereas an incorrect-checking datum included an average of 1.24 correct steps verification and 1.0 erroneous step detection. A model's judgment regarding the validity of steps can become skewed toward correctness when the ratio of correct to incorrect steps is disproportionately high, engendering bias. Conversely, an excess of incorrect-checking data may predispose the model to erroneously invalidate answers, thus increasing the \"correct to wrong\", and thereby diminishing performance.\nWithin our experimental framework, it's observed that maintain an incorrect to correct step ratio of approximately 0.65 in the training set enables the model to maintain a commendable level of accuracy in discerning both correct and incorrect steps. This equilibrium facilitated the highest performance enhancement through a single iteration of the self-correction process."}, {"title": "More checking-corection data can continuously improve the reasoning capability of LLM.", "content": "Different from prior experiments, we utilize the entire GSM8K training dataset, supplementing it with additional checking-corection data that we have generated. Our objective is to investigate the impact on the model's reasoning faculties when the quantity of checking-corection data is escalated, while the original reasoning data remains constant. We adhere to the optimal incorrect-to-correct data ratio ascertained from preceding experiments. The findings are depicted in Fig. 3, illustrating a progressive enhancement in both direct reasoning and self-corrected reasoning as the volume of checking-corection data is augmented.\nSimilar to the insights from LEMA (An et al., 2023), we observe that larger models are adept at assimilating from incorrect correction data, thereby refining their direct reasoning capabilities. Additionally, an increased corpus of checking-corection data markedly bolsters the model's self-correction proficiency. The amalgamation of reasoning with self-correction exhibits a notably accelerated improvement in contrast to direct reasoning alone. These results endorse the inclusion of error-annotated correction data within the training regimen of an inference model, positing that an expanded dataset catalyzes improved inferential processes within the model. Consequently, the model's self-corrective capacity is significantly enhanced, culminating in substantial performance advancements through the mechanism of self-correction."}, {"title": "5 Conclusion", "content": "In this paper, we focused on improving the self-correction of LLMs by enhancing their self-checking abilities. By conducting an in-depth analysis of error patterns in mathematical reasoning, we have developed the \"Step CoT Check\" prompt and constructed a checking-correction dataset. This dataset, combined with the original CoT data, has been instrumental in training models that exhibit improved self-checking and self-correction, reducing dependence on external feedback or ground truth labels for error identification and correction. Notably, models fine-tuned with our \u201cStep CoT Check\" prompt demonstrated superior performance over those trained with alternative prompts. This approach not only enhances the precision of feedback but also significantly increases correctness rates, showing the effectiveness of our method in refining LLMs for complex reasoning tasks."}, {"title": "Limitations", "content": "Our paper has some limitations, which we leave for future work:\nThe computational cost of the step level Check Compared to direct reasoning and checking the whole path at once, although step-by-step check leads to better performance, it requires more inference rounds. On the GSM8K dataset, inference using step-by-step checking combined with correction process requires 4.8 model calls compared to only one for direct reasoning and 2.6 for checking the whole path at once. In practical use, there is a trade-off between performance and resource consumption.\nLack of applicability to broader reasoning tasks. We analyze the types of mistakes in mathematical reasoning and propose a set of hints for checking the reasoning process. Our approach focuses on checking and correcting errors in mathematical reasoning problems, and it remains to be investigated in future work how to extend our approach to a wider range of reasoning tasks.\nLack of experimentation on larger data sizes and scale language models. Due to constraints on computational resources and funding, we do not conduct experiments on larger scale language models(>20B), or generate additional CoT and correction data like other works. Thus the performance on larger LLMs and sufficient amount of training data remains undetermined. We will further explore the performance of our method on larger data sizes and scale language models in future research."}, {"title": "Ethics Statement", "content": "The proposed method has no obvious potential risks. All the scientific artifacts used/created are properly cited/licensed, and the usage is consistent with their intended use. Also, we open up our codes and hyper-parameters to facilitate future reproduction without repeated energy cost."}, {"title": "A Prompt for Checking-corection Data Generation", "content": "Example 1: Prompt for Checking Data Generation\nSystem:\nBlew you will be given a [Question] and the [original answer] of it which has mistakes. You start from the first step, think carefully to check step by step. For each step, you should strictly follow three processes to check: (1) Find out the goal of this step in the [original answer], and you check if the goal is reasonable, i.e., it's not included in the known conditions, can be calculated from previous information, and it can help to solve the final question. Notice you don't need to check the calculation process here. (2) Analyze the what we know now and how to calculate the goal, then determine if the formula in [original answer] is correct. Notice you should analyze before you check! (3) Use the inverse operation to check if the calculation is correct (e.g. 17-9=8 to check 8+9=17 and 25-9-8=8 to check 8+8+9=25 ). And when you find the first wrong step, stop check and print \"Stop check! \".\nHuman:\n[Question]: Maila is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n[original answer]:\nStep 1: Today's number of pages = 12 x 2= \u00ab12*2=24\u00bb24\nStep 2: Remaining pages after today = 120 - 24 = \u00ab120-24=96\u00bb96\nStep 3: Tomorrow, she has to read 96 / 2 = \u00ab96/2=48\u00bb48 pages.\nanswer: 48\nAssistant:\nStep 1:\n(1) The goal is calculate the number of pages Maila read today. Because it contributes to solving the final problem, so it's reasonable.\n(2) We already know that she read twice as many pages today as she did yesterday. To get today's pages, we should multiply yesterday's reading by 2. The formula for this step is correct.\n(3) Using the inverse operation, 24/2=12, so the calculation of 12 * 2 = 24 is correct. \nSo step 1 is correct.\nStep 2:\n(1) The goal is calculate the remaining pages after today. Because it contributes to solving the final problem, so it's reasonable.\n(2) We already know that Maila read 12 pages yesterday and 24 pages today. To find remaining pages, we should subtract the number of pages read both in today and yesterday from the total number of pages. The formula should account for the total pages read so far, which includes both yesterday's and today's reading. The formula for this step is wrong because it does not include the pages read yesterday.\n(3) Because the formula is incorrect, no need to check the calculation.\nSo step 2 is wrong because the formula is not correct. \nStop check!"}, {"title": "B Prompt for Reasoning Combined with Self-correcting Mechanisms", "content": "Example 3: Direct Reasoning\nSystem:\nYou are an AI assistant for solving math problems that can think step by step calculate the results accurately.\nHuman:\nNatalia sold clips to 48 of her friends in April", "May?\nAssistant": "nStep 1: Natalia sold 48/2 = \u00ab48/2=24\u00bb24 clips in May.\nStep 2: Natalia sold 48+24 = \u00ab48+24=72\u00bb72 clips altogether in April and May.\nanswer: 72\nExample 4: Feedback-Driven Reasoning\nSystem:\nYou are an AI assistant who can check and correct the answers very well. Please correct the [original answer", "Question": "to get the correct answers. You should correct the error step pointed out by the [check", "answer": "."}, {"answer": "if possible.\nHuman:\n[Question"}, {"answer": "nStep 1: She works 8 hours a day for $18 per hour so she makes 8*18 = $\u00ab8*18=144.00\u00bb144.00 per 8-hour shift\nStep 2: She works 10 hours a day and anything over 8 hours is eligible for overtime", "3": "Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $\u00ab18*.5=9.00\u00bb9.00\nStep 4: Her overtime pay is 18+9 = $\u00ab18+9=27.00\u00bb27.00\nStep 5: With the overtime pay included it gives her base earnings of 144+27 =$\u00ab144+27=171.00\u00bb171.00 per five day week\nanswer: 171\n[check", "Check)\nAssistant": "nStep 1: Tina works 8 hours a day for $18 per hour", "2": "Tina works 10 hours a day"}, {"3": "Overtime is calculated as time and a half", "4": "Her overtime pay is $27 * 2 = $54.\nStep 5: With the overtime pay included", "6": "Tina works 5 days a week", "990.\nAnswer": 990, "5": "Feedback-Acquisition(Direct Check)\nSystem:\nYou are an AI assistant who can check and correct the answers very well. Please check if the [solution", "step.\nHuman": "n[Question", "earn?\n[solution": "nStep 1: Weng earns 12/60 = $\u00ab12/60=0.2\u00bb0.2 per minute.\nStep 2: Working 50 minutes", "2*50=10\u00bb10.\nanswer": 10, "nAssistant": "nThe answer is all correct.\nExample 6: Feedback-Acquisition(Direct Step)\nSystem:\nYou are an AI assistant who can check and correct the answers very well. Please check if the [solution"}, {"step.\nHuman": "n[Question", "earn?\n[solution": "nStep 1: Weng earns 12/60 = $\u00ab12/60=0.2\u00bb0."}]}