{"title": "Modern Hopfield Networks meet Encoded Neural Representations - Addressing Practical Considerations", "authors": ["Satyananda Kashyap", "Niharika S. D'Souza", "Luyao Shi", "Ken C. L. Wong", "Hongzhi Wang", "Tanveer Syeda-Mahmood"], "abstract": "Content-addressable memories such as Modern Hopfield Networks (MHN) have been studied as mathematical models of auto-association and storage/retrieval\nin the human declarative memory, yet their practical use for large-scale content storage faces challenges. Chief among them is the occurrence of meta-stable states, particularly when handling large amounts of high dimensional content. This paper introduces Hopfield Encoding Networks (HEN), a framework that integrates encoded neural representations into MHNs to improve pattern separability and reduce meta-stable states. We show that HEN can also be used for retrieval in the context of hetero association of images with natural language queries, thus removing the limitation of requiring access to partial content in the same domain. Experimental results demonstrate substantial reduction in meta-stable states and increased storage capacity while still enabling perfect recall of a significantly larger number of inputs advancing the practical utility of associative memory networks for real-world tasks.", "sections": [{"title": "Introduction", "content": "The hippocampal system in the brain plays a central role in long-term memory, responsible for storing and recalling facts and events. Its structure, particularly the auto-associative networks in the CA3 region, enables efficient memory retrieval based on partial input, a process that has inspired several mathematical models of memory networks. Classical Hopfield network models are a type of associative memory architecture that stores memories as fixed point attractor states in an energy landscape, using Hebbian learning to recall patterns form partial input cues using a recurrent network. The Modern Hopfield network (MHN) introduced a continuous relaxation of the original method and in theory enable exponential storage capacity growth with respect to the number of neurons. MHNs have been applied to tasks such as immune repertoire classification and graph anomaly detection.\nExtending these ideas, heteroassociative memories provide a framework for hippocampal memory storage and retrieval, enabling pattern recall from different input modalities. Hetero-association has also been interpreted in various ways, including for modeling sequence associations, where the process begins with a given pattern in a sequence, adjusting the energy weights to transition from one pattern to the next. Similarly, introduces a key-value memory model where sequential patterns are used to predict the next item in the sequence. Further, demonstrated"}, {"title": "Modern Hopfield Networks: A Representational Perspective", "content": "In this section, we review the basic framework of Modern Hopfield Networks (MHNs), focusing on their representational properties, equivalence with other models, and inherent limitations. This discussion motivates our enhancements to MHNs using neural encoding strategies aimed at improving pattern separability and storage capacity.\nThe MHN framework provides a framework for dense associative memory using continuous dynamics. It can be described in terms of its energy function and the resulting attractor dynamics. Let N be the number of memories and K be the data dimensionality (number of neurons in the MHN). Defining a similarity metric between the memories {\u00a7n \u2208 RK\u00d71}_1 or matrix = \u2208 RN\u00d7K and the state vector s \u2208 RK\u00d71 the generalized objective function is expressed as the following energy minimization:\ns* arg min E(s, \u039e) = arg min E\u2081 (s, \u039e; \u03b2) + E2(s)\n                                  S                                S                                                                                                                                                                                             (1)\n        K\n        N                K\nE1(8, \u039e; \u03b2) = Fa(fsim({{n}, s)) = F ({n(k)s(k)}) = -log[exp (\u03b2(k)s(k))]\nn=1                                      k=1                \u03b2 k=1\n        1 \u0442\nE2(s) = STss + constant\n2\nThe function fsim({\u00a7n}, s; \u03b2) is a measure of similarity between the state vector and each memory in the bank. A common choice for this similarity metric is the dot product in K dimensional vector space, which is both efficient and widely adapted in practical implementations of MHNs. The energy function F\u03b2(\u00b7), defined as the log-sum-exponential (LSE), approximates the arg max function to select the most relevant memory. The inverse temperature parameter \u1e9e controls the sharpness of this selection. The energy minimization"}, {"title": "Equivalence with Kernel Memory Networks (KMN)", "content": "The formulation of the MHNs can be viewed as a special case of Kernel Memory Networks (KMNs), where each neuron performs kernel-based classification or regression. The MHN update rule is analogous to that in KMNs and can be computed in closed form as a recurrence.\nLet K(\u039e, s(t)) denote the (symmetric positive definite) kernel function that defines pairwise similarities, and K\u2020 be the Moore-Penrose pseudoinverse for \u039a(\u039e, \u039e). For continuous valued memories, the radial translation-invariant exponential kernel (infinite dimensional basis) with a fixed spatial scale r and temperature a is proposed. This parameterization allows for an analysis of memory capacity and storage limits by interpreting the MHN optimization as a feature transformation operating in a Reproducing Kernel Hilbert Space (RKHS). The kernel and state update are as follows:\nKernel: K(a,r) (x, y) = exp[-(x-2)], State Update: s(t+1) = \u039e\u039a\u2020\u039a(\u039e, s(t))                                                                                                                                    (3)\nWhile KMNs provide strong theoretical storage guarantees their real-world performance can be sensitive to data distributions and parameter choices."}, {"title": "Equivalence with Transformers", "content": "Alternatively, the update rule in Eq. (2) has been shown to be equivalent to the key-query self-attentional framework used in transformer models.\n                                                                                                                                                                            1\nZ = softmax(QKT)V=ZT = VT softmax(KQT)(4)\n\u221adk                                                        \u221adk\nwith the keys being related to the memories as WK\u039e = K, and queries/values being related to the intermediate state vectors st+1 = ZT, Wost = QT, Wyst = VT and the dispersion parameter relating to the temperature\u221adk\u03b2. The matrices WQ,WK,Wv : RK \u2192 RD are linear transformations associated with the query-key-value triplet, which when substituted with the identity matrix Ik gives us the form in Eq. (2)\nUnder the kernel memory networks framework, the MHN equations (1-2,4) do not involve a symmetric positive definite kernel, as the energy objective is inherently non-convex. However, the system still permits a bilinear reproducing form for the kernel K(\u00b7,\u00b7), where input patterns are mapped into higher-dimensional feature spaces. This bilinear kernel is equivalent to the transformer's key-query attention mechanism, where inputs are projected into higher dimensional feature spaces.\nThe transformer kernel can be written as: K(x, y) = exp[(WQX)T(WKY)]. Overall, this highlights the connection between the MHNs and transformer attention mechanisms, showing that both rely on projecting input representations into a shared space for similarity-based comparison."}, {"title": "HEN: Modern Hopfield Networks with Encoded Neural Representations", "content": "Although the results of modern Hopfield networks and its various equivalent forms imply that the formulation has theoretically exponential capacity to store memories, our experimental results demonstrate that the system of updates can be brittle in practice and highly sensitive to real-world data distributions across each of these data representations.\nSpecifically, MHN often struggle with spurious attractor basins, which manifest as erroneous memory patterns due to overlapping or similar inputs. This issue is particularly evident in large datasets, where poor pattern separability results in meta-stable states, limiting retrieval accuracy and scalability.\nThe key insight from the unified representations discussed in Sections 2.1 and 2.2 is that improving the separability of input memories significantly enhances retrieval accuracy. This can be achieved by mapping input memories \u2261 and partial queries s(0) from their original K-dimensional space (where memories may overlap or be less distinct) into a higher dimensional embedding space, the stored patterns become better separable. The increased separability directly reduces the occurrence of spurious attractors and leads to more reliable memory retrieval.\nGoing one step further, a key idea we put forward here is to see if we can bolster the separability of the input patterns before they enter the Modern Hopfield network in order to reduce the spurious attractor states problem via large pre-trained encoded-decoder models and their latent space representations (i.e. generalizing the linear transforms in Eq. 4). Following observations of the phenomenon of input encoding in the dentate gyrus (DG) region of the trisynaptic circuit prior to memorization, we propose to store these latent-space neural encodings, i.e. transformation computed on the memories and partial query \u015d(0), in the MHN memory bank using the encoder transformation Penc(). Recovery of such patterns can be performed by unrolling the recurrence relation in the latent-space (i.e. Eq.(2)) followed by applying the associated decoder transformation dec(\u00b7) to the latent space representation \u015d(Tf). Mathematically, this procedure can be expressed as follows:\n= \u03a6enc(E) $(0) = Penc(s(0))\n\u00a7(t+1) = Esoftmax(BTs(t)) = En in exp(s(1))\n                                                                                                                      (5)\n                                                                                                                                    (6)\nEn exp(\u03b2\u03ad\u03c0s(t))\ns(Tf) = dec($(Tf))                                                                                                                                                                                      (7)\nHypothesis 1: The spurious attractors can be reduced by encoding inputs prior to storing them in the Modern Hopfield network and decoding them after recall due to increased separability in latent space\nOur proposed HEN combines an auto-encoder with the Modern Hopfield network (MHN). Specifically, the encodings are generated by a pre-trained auto-encoder. The raw content is then recovered through chaining MHN with the decoder portion of the auto-encoder. We hypothesize that the encodings produced by an auto-encoder contain discriminative information that is not only compact but can improve the separability in the energy landscape to significantly delay the meta-stable states even with increased content. That is, by leveraging a well-trained auto-encoder for feature extraction, we posit that the most significant and discernible features between images can be easily identified, leading to less spurious patterns emerging during recall and allowing more content to be stored, thereby increasing storage capacity."}, {"title": "Experimental Evaluation and Results:", "content": "We provide practical insights drawn from experiments on the MS-COCO dataset, which contains 110,000 images . This dataset offers a more realistic distribution of high-dimensional, real-world data compared to smaller, curated datasets like MNIST. Its unique associative captions also make it ideal for illustrating hetero-associations within our proposed framework.\nTo evaluate this hypothesis, we conducted studies that examined the effectiveness of various pre-trained encoder-decoder architectures to produce encoded representations that can lead to successful recall of dense associative memories by comparing them against the native data representations and KMNs. We also analyzed the parameter choices for the energy formulation of MHNs in affecting the identity of the recalled memory items when using their encoded representations."}, {"title": "Natural language-based Hetero-associations", "content": "We now extend the HEN framework in a hetero-associative setting as a practical application. Specifically, we explore the use of cross-stimuli coming from language and vision, as language-based queries are often used as cues for recall, for example, in practical storage/retrieval contexts. Cross-associative features have been previously demonstrated for the classical Hopfield networks model, albeit under the limited setting of carefully curated binary patterns.\nHypothesis 2: Hopfield encoding networks serve as content-addressable memories even with cross-stimuli associations as long as they are unique associations.\nWe conducted three separate experiments. First, we use a native textual embedding for the language cue and associate it with the content to be stored as a practical way of enabling recall. Next, we explore the paradigm of stimuli type-conversion to render the language cue into a convenient image"}, {"title": "Conclusions", "content": "In this paper, we began by unifying and exploring various energy formulations used across associative memory models, highlighting their interconnections and the common emphasis on pattern separability. This theoretical foundation informed our work on improving Modern Hopfield Networks.\nBuilding on these insights, we introduced two key enhancements: pattern encoders and decoders integrated with Modern Hopfield Networks to enhance pattern separability and reduce metastable states. Additionally, we demonstrated how this approach supports cross-stimuli associations using different encodings, as long as the uniqueness of association is maintained. These advancements mark a step toward improving the practicality and performance of Modern Hopfield Networks for real-world retrieval and storage tasks."}, {"title": "Appendix", "content": "A Transformers and Kernel Memory Networks Representations\nFrom a representational perspective, demonstrates that the formulations\nin Eqs.(1-2) in the main manuscript are special instances of Kernel Memory Networks (KMN), i.e."}, {"title": "Relating Transformer and HEN Updates to the KMN formulation:", "content": "Under the KMN formulation, the MHN from involves a kernel that is not symmetric positive definite,\nas the energy objective is not a convex optimization problem. Nevertheless, it permits a bilinear reproducing form for the kernel K(\u00b7, \u00b7), where\nthe feature transformation maps {\u00dex(\u00b7) : X \u2192 Fx(\u00b7), \u0424y(\u00b7) : Y \u2192 Fy} map input elements from\nvector spaces {x \u2208 X, y \u2208 Y} to the D-dimensional Banach spaces {\u00dex(\u00b7) \u2208 Fx, \u0424y(\u00b7) \u2208 Fy},\ncharacterized by linear manifold functional forms {fox(x; WQ) \u2208 Bx, g\u00f8y (y; WK) \u2208 By}. The\nmathematical expression for the transformer kernel is:\n                                                                                                                                                                  1\nK(x,y) = (\u0424x(x), \u0424\u0443(y))FxxFy = (fox(x), Spy(y)) BxxBy = exp[(WQx)T(WKY)]\n                                                                                                                                                    \u221adk\nFrom the representational perspective, it is sufficient to know the bilinear (reproducing) form of the\nkernel above in the MHN updates without requiring an explicit computation of {\u0424x(x), \u0424y(y)} or\nsampling from the Banach spaces, Fx : Wqx, Fy : Wky, or estimation of the manifold functional\nforms {Bx, By}. Mathematically, these constructions can be formalized as:\n                                                                                    \u221adk Wky \u2208 Fy, x \u2208 X\nBy = {gu : Y \u2192 R : gu(y) = (u, \u03a6y(y))FxxFy; u \u2208 Fx,y \u2208 Y}\nBy extension, the kernel form for HEN can be formalized as follows:\nKHEN(x, y) = exp [\u1e9e(Penc(x))T(enc(y))]\n Bx = {f(enc(y)) (Pene(x)) = exp [B(Pene(x))T Pene(y)]; Pene(y) \u2208 Fy,x \u2208 X}\nBy = {g(Pac(x) (Pene(y) = exp [B(Pene(x))T Pene(y)]; Pene(x) \u2208 Fx,y \u2208y}"}, {"title": "Eq. (2) is a special case of the Radial Translation-Invariant Exponential Kernel in Eq. (4)", "content": "According to the formulation in , for continuous valued memories, the\nsuggested form of kernel is a radial translation-invariant exponential kernel with a fixed spatial scale\nr and parameter a is as follows:\nK(a,r) (x, y) = exp [-(||x -y||2)]\n= exp [-(x-2)]\n= exp[-(x2+y2)].exp [(2x\u00b2y)]\nSubstituting a = 2, B = r2= 1and ||x||2 = ||y||2 = 1\nKTrans (x, y) = C. exp [\u221aa (Ikx) (Iky)] = C. exp [xy]\nwhere C is a constant. This is the form of the transformer kernel corresponding to the update in\nEq. (2) and energy functional in Eq. (1) of the main manuscript. We note that spherical normalization\nwas also used in as a mechanism to control the\ndynamics of the updates to mitigate metastable solutions."}, {"title": "Ablations on \u1e9e for experiments on choices of fsim(\u00b7,\u00b7) and Neural Representation", "content": "Table 3: Structural Similarity Index Measure for HEN for increasing \u1e9e values. This table presents the\n1-SSIM results (lower is better) for a set of N=6000 images from the MS-COCO dataset. Every row\nrepresents the similarity metrics (Dot or l2) for five different types of pre-trained encoder-decoder\narchitectures (kl8, vqf8, klf16, vqf16, dVAE). The Image column represents the Modern Hopfield\nNetwork formulation, the established baseline model in this comparison. We see that as for increasing\ntemperature values of \u1e9e, the recovery stabilizes and produces near exact recoveries for all sets of the\ntrained encoder-decoder architectures. In comparison, we see that the image-based Modern Hopfield\nnetwork fails to handle a large number of images in memory.\nIn this experiment, we examine the impact of the \u1e9e parameter on the performance of HEN in the\ncontext of image recovery, the choice of the similarity function fsim(\u00b7, \u00b7) in the energy functional. We\nexplore the influence of various pre-trained neural encoder-decoder architectures on the HEN's ability\nto converge to stable states and achieve accurate recovery. As in the main manuscript, we evaluated"}, {"title": "Quantifying Meta-Stable States", "content": "Recall that SSIM and MSE metrics used in Fig. 2 in the main manuscript quantify the error in the\nrecovery of the stored image content (Fig. 1, column 4). By definition, meta-stable states correspond\nto degenerate solutions (among entities in the recovered content) in the energy landscape. To quantify\nthe prevalence of these meta-stable states as a function of the dynamics of the HEN, we report an\nadditional proxy for identifying such behavior.\nSpecifically, we report the relative rank (RR = R\u015d/R\u2266) of the recovered state matrix (\u015c =\n[$1,...,\u015dv] ) to that of the memory bank ( = [\u00a71,..., \u00a7\u03bd]) for various values of \u1e9e during the\nevolution of the dynamics in Eq. (2). In Fig. 7, we plot the RR on the y axis against the number of\niterations on the x axis.\nRR < 1 upon convergence indicates degeneracy in the recovered solutions. Each dashed line\ncorresponds to different values of the temperature B. For this illustration, we use the dVAE encoder\nwith the dot product formulation of the HEN and 6000 images from the MS-COCO dataset (i.e.\nthe same setup as the green line in Fig. 2 of the main manuscript). We observe very consistent\ntrends with those seen in the 1-SSIM and MSE metrics, where near-zero values of these metrics\nsignal no degeneracy (no meta-stable states), while increasing non-zero values indicate imperfect\nretrieval due to the presence of meta-stable states (such as with row 1, column 4 of Fig. 2). Early in\nthe optimization (t < 20 iterations), the {s} contain information corresponding to encoding the\nmasked image signal. These vectors are distinct across input examples and also exhibit some baseline\n(non-zero) correlation with the stored memories {\u00a7\u00bf} . As the dynamics evolve, we observe that for\nlow values of \u03b2 = [20, 50], the dynamics destabilize to low-rank solutions, collapsing the retrieval\nfidelity. For sufficiently high \u1e9e = [80, 150], the dynamics stabilize over a period of time, leading to\nnear-perfect recovery ( consistent with 1 \u2013 SSIM = MSE = 0)\n(t)"}, {"title": "Handling cross-stimuli using pixelized language-image association", "content": "To test language-image associations, we utilized the unique set of captions associated with each\nimage in the COCO dataset. Specifically, we employed Python's hashlib.sha256(.) function\nto hash the captions generating a unique ID text string to associate with the image. Initially, we\ncreated a memory bank {1} by converting the hashed captions into pixelized text representations\nusing a generic text-to-pixel function. Subsequently, both the pixelized text and the corresponding\nimages were processed through the same encoder. The resulting vectors were concatenated to form\nthe elements of the memory bank {\u00a7n = [;]}.\nDuring the query phase, we supplied only the pixelized text part of the encoded vector, setting the\nimage component to zero. The Hopfield network iteratively updated the image encoding vector,\nwhich was passed through the corresponding decoder to reconstruct the image. Fig. 8 illustrates the\nnetwork's progression in reconstructing the image based on the pixelized text input. The recurrent\nupdates in the HEN iteratively reconstructed the full image.\nAs illustrated in Figs. 8, the HEN network is able to recall perfectly using pixelized cross-stimuli\nassociations. Table 5 (repeated from the main section of the paper for convenience) reveals that\nall HEN variants with different encodings still outperformed traditional image-based MHNs even\nas the number of image patterns to store increased. While the HEN can recall accurately based on\ncross-stimuli associations, we expect such associations to be unique as in the case of stimuli from the\nsame domain/modality (See result in Fig. 6 of the main manuscript)."}]}