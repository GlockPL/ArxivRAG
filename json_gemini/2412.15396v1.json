{"title": "Learning Visual Composition through Improved Semantic Guidance", "authors": ["Austin Stone", "Hagen Soltau", "Kaifeng Chen", "Robert Geirhos", "Abhijit Ogale", "Xi Yi", "Ye Xia", "Jonathon Shlens", "Bingyi Cao"], "abstract": "Visual imagery does not consist of solitary objects, but instead reflects the composition of a multitude of fluid concepts. While there have been great advances in visual representation learning, such advances have focused on building better representations for a small number of discrete objects bereft of an understanding of how these objects are interacting. One can observe this limitation in representations learned through captions or contrastive learning where the learned model treats an image essentially as a bag of words. Several works have attempted to address this limitation through the development of bespoke learned architectures to directly address the shortcomings in compositional learning. In this work, we focus on simple, and scalable approaches. In particular, we demonstrate that by substantially improving weakly labeled data, i.e. captions, we can vastly improve the performance of standard contrastive learning approaches. Previous CLIP models achieved near chance rate on challenging tasks probing compositional learning. However, our simple approach boosts performance of CLIP substantially and surpasses all bespoke architectures. Furthermore, we showcase our results on a relatively new captioning benchmark derived from DOCCI. We demonstrate through a series of ablations that a standard CLIP model trained with enhanced data may demonstrate impressive performance on image retrieval tasks.", "sections": [{"title": "1. Introduction", "content": "Visual composition is a critical problem. Learning representations that capture how attributes and relationships of objects are represented is a critical task for any system that attempts to summarize the multitudes of concepts imbued within pixel space.\nMany SOTA multimodal models do not learn embeddings that have an understanding of the relationship of objects and descriptions [20, 32]. One reason for this failure is that the visual representations are not rich enough to capture the interactions and relationships between various parts of an image or video.\nCurrent approaches have focused on building new bespoke architectures that leverage multi-task learning through multiple losses or side data [16, 33]. Although these models achieve SOTA results on visual composition tasks [32], it is unclear how scalable these approaches are given limitations on gathering high quality, specialized side information or the complexity of the architectures.\nInstead, we search for scalable solutions that focus on simplicity. The goal of this work is to employ a basic multimodal model solely employing a single contrastive learning objective [13, 23], and attempt to improve the learned representations with minimal changes.\nWe posit that underlying visual architecture (e.g. ViT [9]) contains sufficient parameters and scale to capture visual composition. Furthermore, we posit that contrastive learning is a sufficient training signal for multimodal learning so long as the target semantic embedding is rich enough. In particular, we ask if enriching the target semantic embedding is entirely sufficient to make a contrastive learning model capture visual composition.\nWe answer this question by identifying a small set of minimal changes to improve the target semantic embedding"}, {"title": "2. Methods", "content": "Our method builds on the CLIP architecture proposed in [12, 23]. We introduce two key modifications:"}, {"title": "2.1. Semantic guidance with grounded recaptioning", "content": "We start with an English-only subset of the WebLI dataset [5, 6] consisting of 1B high quality images paired with freeform text scraped from the corresponding alt-text within a web page. We posit that the original alt-text is noisy and a limiting factor in the performance of an multimodal model [15]. Thus, we focus on creating a new set of captions to improve the alt-text.\nTo enhance caption quality, we leverage Gemini 1.5 Flash [24] to generate new captions. In early experiment, we provide the original image as well as the alt-text and web page title, and prompt the model to generate a new caption that describes the underlying image. We iterated on the prompt (see Appendix for details) to arrive at a method that minimizes hallucinations while providing rich descriptions.\nThe resulting captions contain a mean of 57 words, increasing the effective caption length by about 8\u00d7 of the alt-text and likewise significantly increasing the log-likelihood of the caption as measured by Gemini Pro 1.5 (Figure 5). In our ablations we experiment with an assortment of other captioning techniques. We generate a \"concise\" captions (26 \u00b117 words); we examine the importance of grounding the caption based on the alt-text and page title; finally, we experiment with using a weaker LLM's as the captioning model (Section 4 and see Appendix for additional details)."}, {"title": "2.2. Semantic guidance with a strong text encoder", "content": "Instead of training a text encoder from scratch, we utilize the pretrained Gemini 1.5 Flash-8B [26] as the text encoder. We experiment with unfreezing various layers to allow for more flexibility in training the visual embedding. In early experiments, we explored unfreezing various layers of these pretrained text encoders and found that unfreezing last 4 layers of this model provides best overall performance relative to the additional compute cost during training. We repeated the above experiments with the open source Gemma2-2B [27] as the text encoder, and likewise identified that unfreezing the last 4 layers provides the best trade off in terms of additional compute cost versus performance. This model is smaller than the Gemini model above, but yields comparable results."}, {"title": "3. Results", "content": "Our model is a standard two-tower CLIP encoder trained with a contrastive loss [23]. The image encoder is a ViT-Base vision transformer with 86M parameters [9]. Images are cropped to 256\u00d7256 pixels and represented as 256 patches of 16\u00d716 pixels. The encoders represent images and text as 768-dimensional embeddings.\nFor the text encoder, we experiment with two different configurations: In the first configuration, a bi-directional text encoder with 12 layers and 297M parameters is trained from scratch matching CoCa-Base [31]. We explore also the use of a different text encoder based on a pre-trained Gemini 1.5 Flash-8B and Gemma2-2B [27]. We keep most of the layers frozen, and only fine-tune the last 4 layers. While the pre-trained text encoder uses left-to-right context only, we switch to bi-directional context for the layers that are finetuned. In total, the model has 653M trainable parameters. We perform 150,000 training steps with global batch size of 65,536 followed by fine-tuning for 500 training steps with reduced batch size of 4096 using data augmentation on the \"hard negative\" examples (Section 2.1). The main model training is done on 256 accelerators and the fine-tuning is done with 16 accelerators. We use Adam optimizer [14] with a linear warm-up of the learning rate."}, {"title": "3.1. Beyond a bag of words representation", "content": "A primary indication of the failure of multimodal representations can be observed a the relatively simple ARO evaluation dataset [32]. ARO artificially perturbs a set of captions from \"the horse is eating grass\u201d to \u201cthe grass is eating a horse\" (i.e. relations) or \"the paved road and the white house\" to \"the paved house and the white road\" (i..e. attributes), and subsequently asks if the corresponding image is closer to the former or the latter caption. Chance rate is 50% and a model that does respect the relations and attributes of an image \u2013 such as a bag-of-words \u2013 would correspondingly perform at chance rate. One of the most widely used multimodal baselines, CLIP [23], achieves 59% and 63% accuracy on relations and attributes dimensions of ARO, indicating that the model is performing only slightly above a bag-of-words representation (Table 1).\nAdmittedly, this task is artificial, and thus some captions can be correctly identified without examining an image (e.g., \"grass cannot eat a horse\") merely based on the language. To test this hypothesis, we supplied Gemini 1.5 with two caption alternatives but no image. This blind baseline achieves an accuracy of 71% and 82% on relation and attributes, respectively, outperforming CLIP. This result underscores the importance of going beyond bag-of-word representations [3].\nSeveral directions have been pursued for improving performance on this baseline (Table 1). The ARO benchmark proposed a training augmentation method NegCLIP in which hard negatives are artificially added to the training set, which achieves 81% and 71% accuracy. More importantly, several works have built bespoke architectures, derived from CLIP, which exploit localization information in order to better ground visual information. Correspondingly, two different architectures achieves SOTA performance on relations (73% [33]) and attributes (88% [16]). Both models achieve these results by tieing the image and text towers together, using a grounded, cross-modal encoder [16, 33]. Although both methods employ a standard nearest neighbor lookup, they achieve SOTA results through a second-stage computation that exhaustively calculates a binary classification score for a set of candidate image-caption pairs.\nIn comparison, our work makes no architectural changes and maintains a single contrastive training objective in a standard CLIP model. We do employ additional data augmentation with 64M synthetically generated \"hard negative\" examples (Section 2.1) and find that the addition of these data augmentations significantly improve ARO performance (Section 3.5). Notably, our model achieves 92% and 94% accuracy on relations and attributes surpassing bespoke architectures that exhaustively search across examples. In the Appendix, we measure how our model uniformly outperforms competing methods across the assortment of fine-grained tasks comprising the ARO benchmark."}, {"title": "3.2. Limitations of retrieval with COCO captions", "content": "Given the positive results on ARO, we asked whether these gains can be observed in image retrieval on the COCO dataset [18]. COCO contains detailed captions for 5,000 images and we ask how well nearest neighbor lookup in the embedding space performs for image retrieval. Our results compared to external baselines are in Table 2.\nAs a strong baseline, we measure the performance of open source CLIP [23] and a CoCa [31] on image retrieval. We find a recall @1 of 37.8 and 47.5 on for CLIP and CoCa, respectively. Our model achieves the best reported recall of all models of similar size with a recall @ 1 of 56.3.\nAlthough our result was stronger than prior numbers, we actually expected to see even more notable gains in our model given the high quality and scale of our data. We investigated our model further by examining cases where our model is marked as retrieving the incorrect COCO image. Note that although these examples are marked as incorrect, the image retrievals looks quite plausible. In fact, upon further inspection of the dataset, we observe that many COCO captions contain similar images and captions, and in the context of image retrieval, we hypothesize that many image retrieval errors are incorrectly scored as errors.\nTo test this hypothesis, we did a human annotation experiment to assess what fraction of images retrieved by our model are reasonable. We randomly select 500 failure cases and task 3 humans to annotate each of the failures to indicate whether the recalled image is an appropriate match to the prompt text. 2 Using majority voting among the 3 human raters, we measured that 29.8% of the image retrieval failures were appropriately marked failings of our model, but the other 70.2% were acceptable matches to the query text. In other words, 70.2% of all instances marked as failures were images that a majority of the human raters thought were appropriate image retrievals given the input caption.\nThese results indicate that a majority of the errors reported in Table 2 are due to noise and overly similar images or annotations in COCO when employed as an image retrieval task. This result suggests that caution must be exercised when fine-tuning and evaluating on COCO, and motivates our interest to identify another evaluation benchmark that might provide a useful metric for measuring improvements in image retrieval that exercises a detailed semantic representation."}, {"title": "3.3. Evaluation on detailed image caption retrieval", "content": "Given the positive results on the ARO benchmark but the inability to further discern gains on COCO, we investigated whether another retrieval benchmark may capture the gains in our approach. The DOCCI dataset was recently introduced in the context of image generation [20] but has recently been explored for image retrieval. DOCCI contains 14,847 images with detailed human-written descriptions (see [20] for details). Briefly, each caption contains on average 135.9 words across 7.1 sentences. Importantly, DOCCI purposefully contains images and corresponding"}, {"title": "3.4. Evaluation on zero-shot ImageNet classification", "content": "Prior work on embeddings and image retrieval have additionally evaluated on ImageNet and variants [7]. We emphasize that ImageNet emphasizes centrally-cropped, single objects within a scene, and not the composition of multiple ideas and objects. Nonetheless, this dataset has proven to be a standard, reported metric in computer vision and, thus we apply our model to this task. We find that our model achieves a zero-shot performance of 68.4% top-1 accuracy (Table 3). Although a reasonable zero-shot performance, we find nonetheless that this result is below other SOTA zero-shot ImageNet classification accuracies (Table 3).\nOne immediate discrepancy between our work and prior work is the source of training data. The open-source CLIP model employs a separate, proprietary training set that has never been revealed, so it is unclear how well this aligns with ImageNet classification task 3. CoCa [31] (and other multimodal embedding models, e.g. LiT [35], BASIC [22]) leverage a 50% mixture of variations of the JFT dataset. In the case of JFT, the dataset comprises 3 billion images that have been annotated with a detailed class-hierarchy across 30K labels using a semi-automatic pipeline [34]. We strongly suspect this fine-grained information contributes heavily to their overall performance."}, {"title": "3.5. Exploring the space of improved guidance", "content": "We examine the space of semantic guidance to better understand the additive benefits of each change in our model. Given that captioning 1B images is prohibitively expensive, we perform detailed ablations on a subset of 100M images for image retrieval on DOCCI and COCO (Tables 4, 5, 6, and 7).\nTable 4 indicates that training on recaptioning data alone notably boosts performance from 53.5 to 75.6 for recall@1 on DOCCI [20]. We emphasize that the training images are identical; all that differs is the associated captions with the images. This small change results in 41% relative performance gain. In parallel, we observe that using a rich text model as a target increases performance from 53.5 to 67.2, corresponding to 26% relative performance boost. The two simple changes in tandem lead to additive gains on COCO and DOCCI.\nGiven the magnitude of benefits with recaptioning, we explored what features make for a good recaptioning. First, we examined how a less performant recaptioning models effects overall performance (Table 5, top). We observed that slightly drop in performance with a less performant recaptioning model. Second, we examined how two key parameters in our recaptioning with Gemini 1.5 Flash effect overall quality. Namely, we removed grounding based on alt-text, and we abbreviated length of the captions. We likewise observed that only slight degradations in overall performance on image retrieval (Table 5, bottom). Finally, we asked how data augmentations for the captioning effected performance. In particular, we checked the impact from random sentence sampling (Table 6) and hard negatives fine-tuning (Table 7). We observed that sentence sampling uniformly lead to better performance, especially on COCO. Conversely, removing the hard negatives did positively impact DOCCI at the significant harm to ARO and minor harm to COCO. We expect more work should be dedicated to finding the right balance of data augmentation in captioning pipelines in future work."}, {"title": "4. Related Work", "content": "Shortcomings of multimodal models. Since the landmark publication of CLIP [23] as one of the first highly capable vision-language models, a number of important limitations have been observed in multimodal models. For instance, many image pairs are highly similar in CLIP feature space despite showing semantically meaningful differences (e.g., left-facing dog vs. right-facing dog). This leads to systematic failures in downstream models that use CLIP as an image encoder Furthermore, vision-language models often behave like bag-of-word models [32], as indicated through a surprising inability to understand compositions and relations (e.g., grass eating horse vs. horse eating grass). A number of approaches have been suggested for improving over current shortcomings; they are mostly focused on specialized architecture or data, and sometimes both.\nSpecialized architectures. X-VLM [33] proposes an architecture that incorporates bounding box prediction in order to improve the connection between visual input and text descriptions. The BLIP model [16] is a popular specialized architecture trained jointly with three different objectives. CapPa [28] suggests image captioning as an alternative pre-training task leading to improved downstream performance. MATE [11] identifies short captions as a problem (similar to our work); however instead of training models on longer captions (as we do) they propose an architecture that replaces the text encoder with a LLM-based one. Long-CLIP [36] has a similar motivation and uses a bespoke architecture (modifying positional embedding, matching CLIP's primary component) in combination with one million long-text caption pairs. [2] is a concurrent work which uses multiple embedding heads, one for fine-grained tasks like ImageNet and the other for long form captions like DOCCI, in addition to a very large pretrained DINO g/14 image encoder and additional self-distillation and masking losses.\nData. In contrast to the wealth of specialized architectures, the data side has not been explored as much-and frequently only in combination with a specialized architecture. Among the notable exceptions are [21], generating synthetic images from text-to-image models that are used as hard negatives in contrastive training. Furthermore, [10] use an LLM to rewrite (but not explicitly expand) existing captions as a form of data augmentation. In contrast to our work, the caption rewriting model is not visually grounded since it is a \"blind\" language model without image access. Finally, closest in spirit, [15] employed a multimodal model to more elaborate recaptioning pipeline but focused on boosting COCO retrieval."}, {"title": "5. Discussion", "content": "In this work we have highlighted the failures of multimodal embeddings. Current foundational models contain limited compositional information as observed by perturbations in the attributes and relationships as well as the failure of models to perform image retrieval on challenging captioning tasks (Section 3.1, 3.2, and 3.3).\nThis work attempts address this problem by providing improved semantic guidance for multimodal models. In lieu of architectural innovations, this focuses on two simple and small changes that are highly scalable: (1) re-captioning training, and (2) improving the semantic target with a pretrained and powerful language model. These two improvements appear complimentary and result in a model that is able to perform 94.5 recall @1 on DOCCI image retrieval (Table 2).\nMore generally, this work highlights how bootstrapping data provides a scalable solution for constantly improving training data. Previous work had shown how simple procedures to bootstrap pseudo-labels may scale quite well and achieve state-of-the-art results on discrete classification tasks [30]. This work extends that endeavor by using pseudo-labels from other models to learn better embedding representations using contrastive learning.\nAs a simple application of this work, one benefit of improved alignment between visual and semantic representations is the opportunity to predict human preferences for image generation models. For instance, in previous work Imagen [25] outperformed GLIDE [19] in forced-choice human preference experiments for whether a caption is better aligned to the generated image. As a pilot study, we asked whether our work may improve upon a strong baseline of the open-source CLIP model [23] for predicting these human preferences on the same dataset. We observed that while open-source CLIP predicted the weighted human accuracy at 71.3%, our model instead achieved 81.1% weighted accuracy. We also observed notable gains in terms of the ability of our model to count a small number of distinct objects. We take these preliminary experiments as a suggestion for a potentially fruitful direction of exploration and reserve such a direction for future work.\nThis work has largely measured the quality of the model based on image retrieval in a shared embedding space. That said, a natural extension of this work is to investigate how these simple techniques may work in the context of an image captioning model (e.g. [29]). Previous work has shown how image captioning as a training task may provide rich representation that may transfer to other visual tasks [8]. A natural question is to ask how such a training objective may additively improve multimodal representations [31]. In tandem, one could measure the quality of the presentation on captioning-based evaluations [1] where challenges such as grounding and hallucination become more pronounced. We look forward to pursuing this direction in future work."}, {"title": "A. Prompts used for recaptioning", "content": "For gathering our main set of 1B image captions, we used the following prompt.\n\"The image presented came from a web page called: page title and had the alt-text: alt-text. Please describe what is in the image using the alt-text and the page title as a guide to ground your response. For example, if the alt-text contains a specific brand name, use that brand name in your output. Please be descriptive but concise. DO NOT make things up. If you can't tell something with certainty in the image, simply don't say anything about it.\"\nFor gathering our 100M ablation set of very brief captions, we used the following prompt.\n\"The image presented came from a web page called: page title and had the alt-text: alt-text. Please very briefly describe what is in the image using the alt-text and the page title as a guide to ground your response. For example, if the alt-text contains a specific brand name, use that brand name in your output. Please describe what is in the image but be extremely concise in your response. I want to emphasize how important it is to be VERY brief. DO NOT make things up. If you can't tell something with certainty in the image, simply don't say anything about it.\"\"\nFor gathering our 100M ablation set of captions without conditioning on alt-text or page title, we used the following prompt.\n\"Please describe what is in the image. Please be descriptive but concise. DO NOT make things up. If you can't tell something with certainty in the image, simply don't say anything about it.\""}, {"title": "B. Image to text retrieval results", "content": "All prior results were given to as text \u2192 image recall. Following prior work, we also evaluated image \u2192 text recall."}, {"title": "C. Method details for caption statistics.", "content": "The plots from Figs. 4 and 5 are based on 1,000 random samples from each dataset (WebLI alt-text vs. Gemini Flash 1.5 captions). The plots were generated via Seaborn's displot performing a kernel density estimate, setting cut=0 to avoid putting probability mass on negative caption lengths. The log-likelihood from Fig. 5 was obtained by scoring log-likelihood of alt-text vs. Gemini Flash 1.5 captions on a random sample of 1,000 captions each. The model used for scoring was Gemini Pro 1.5, i.e., a larger and higher-quality model than the model used to generate captions. This conforms to the common practice in language modeling of using a large model to score text generation from a smaller model. The log-likelihood of a sequence of tokens $t = (t_1,...,t_n)$ according to a language model parameterized by $\\theta$ is calculated as follows:\n$\\log p(t \\theta) = \\sum_{i=1}^{n} \\log p(t_i|t_1, t_2, ..., t_{i-1}; \\theta)$"}, {"title": "D. Detailed results on ARO evaluation", "content": "We report our performance across the fine-grained splits of ARO in Table 9. We achieve high performance across most subsets of ARO. Notably, our performance on left/right is lower than other spatial relations, in particular much lower than similar relations such as above/below. We suspect there could be an issue with the ground truth labels for this subset. We had a human visualize and mark the correctness of a random sample of 100 left/right labels and determined that the ground truth for 43 out of 100 were either incorrect or ambiguous. Further investigation is warranted."}]}