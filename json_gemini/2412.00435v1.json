{"title": "Benchmark Real-time Adaptation and Communication Capabilities of Embodied Agent in Collaborative Scenarios", "authors": ["Shipeng Liu", "Boshen Zhang", "Zhehui Huang"], "abstract": "Advancements in Large Language Models (LLMs) have opened transformative possibilities for human-robot interaction, especially in collaborative environments. However, Real-time human-AI collaboration requires agents to adapt to unseen human behaviors while maintaining effective communication dynamically. Existing benchmarks fall short in evaluating such adaptability for embodied agents, focusing mostly on the task performance of the agent itself. To address this gap, we propose a novel benchmark that assesses agents' reactive adaptability and instantaneous communication capabilities at every step. Based on this benchmark, we propose a Monitor-then-Adapt framework (MonTA), combining strong adaptability and communication with real-time execution. MonTA contains three key LLM modules, a lightweight Monitor for monitoring the need for adaptation in high frequency, and two proficient Adapters for subtask and path adaptation reasoning in low frequency. Our results demonstrate that MonTA outperforms other baseline agents on our proposed benchmark. Further user studies confirm the high reasonability adaptation plan and consistent language instruction provided by our framework.", "sections": [{"title": "Introduction", "content": "Embodied agents powered by Large Language Models (LLMs) show great potential for interpreting human instructions and performing tasks through sequential actions in diverse environments. To evaluate embodied agents effectively, their capabilities can be assessed from multiple perspectives, including language instruction interpretation, subtask decomposition, action sequence generation, etc. In highly cooperative scenarios where agents must interact with humans at every step, real-time adaptation on both subtask and action sequence becomes especially crucial. Current LLM-powered agents still rely on scripted-based policy or RL-based controller to achieve atomic action, which makes them often struggle to achieve reactive, real-time adaptation when collaborating with true humans, highlighting the need to evaluate and improve this capability for better performance in dynamic, collaborative scenarios.\nThe existing benchmarks, such as Overcooked-AI , designed for highly cooperative multi-agent scenarios, lack modularity and fail to provide metrics for evaluating both real-time adaptability and dynamic communication efficiency of LLM-powered agents. Specifically, the following limitations hinder the effectiveness.\n\u2022 Lack of layouts required frequent adaptation: Current benchmarks do not require agents to frequently adapt strategies and coordinate sequential tasks due to conflicting spaces, limiting their ability to assess dynamic teamwork scenarios.\n\u2022 No explicitly real-time adaptation evaluation: The way of accessing the performance using the overall game score is straightforward yet not clear. The game score can be influenced by agents' capabilities from different perspectives, such as subtask reasoning, adaptation, and human goal integration.\n\u2022 No Communication efficiency evaluation: For LLM-based agents, communication is a critical capability and advantage to achieve seamless human-agent collaboration.\nTo better evaluate LLM agents in cooperative scenarios, we introduce an enhanced Overcooked-AI benchmark, which is specifically designed to assess the capability of both reactive adaptation and the effective communication of LLM-based coordination agents. Furthermore, we also introduce MonTA, a framework designed to enable embodied agents to execute real-time adaptation by combining fast monitoring and slow adaptation.\nTo summarize, our key contributions include:\n\u2022 We proposed a fine-grained benchmark to evaluate LLM-powered agents' adaptability and communication capabilities, focusing on dynamic real-time interactions and cooperative scenarios.\n\u2022 We developed MonTA, a modular and adaptive framework that integrates fast monitoring and deliberate adaptation to enable LLM agents to perform reactive, real-time adaptations in highly cooperative environments.\n\u2022 We conducted thorough experiments and user studies on our benchmarks, using both MonTA and other frameworks with various LLMs, which demonstrate the benchmark's effectiveness in evaluating real-time agent adaptability.\n\u2022 Further comparison and analysis confirm MonTA's superior adaptability and seamless collaboration in dynamic human-agent cooperation."}, {"title": "Related Work", "content": "Numerous studies have proposed benchmarks for embodied multi-agent systems. Several language-based benchmarks, such as , have focused on question answering, which emphasizes information gathering but does not consider the physical interaction made by embodied agents. Notably, introduced the Embodied Agent Interface, a modular framework for evaluating embodied decision-making processes by considering factors beyond overall task performance. These benchmarks usually focus on assessing overall performance, limiting their ability to evaluate agents' adaptability and communication ability during collaboration. This work focuses on evaluating agents' reactive and proactive ability in language instruction during interaction. This work uniquely benchmarks real-time adaptability within collaborative scenarios."}, {"title": "Real-time Human-AI Collaboration", "content": "Human-AI collaboration has been a long-standing challenge. Prior works study human-AI cooperation in games such as Hanabi, diplomacy, and overcooked. Several studies also leverage LLMs for decision-making tasks in the game of overcooked. For instance, use LLMs to infer other agents' intentions and plan subtasks, while explore long-horizon inference for improved multi-agent cooperation. Recent work, such as , proposed an LLM-based hierarchical framework to follow human high-level instruction, but still lack adaptability during execution of assigned subtask. Prior work primarily focuses on agents with high-level task planning and often lacks low-level adaptability. Our work differs by enabling agents' real-time proactive adaptation at the atomic action level."}, {"title": "An Enhanced Overcooked-AI Benchmark for Embodied Agents", "content": "To thoroughly evaluate the real-time adaptability and performance of different AI agents, we extended the original Overcooked benchmark and designed different modular tests. Specifically, we first constructed 22 layouts with varying complexity and coordination demands, following the teaming fluency metrics discussed in and . Secondly, we designed a communication panel to test the effectiveness of real-time language instruction during human-agent collaboration. With the designed layouts, we develop three different evaluation modes to access the agent's real-time adaptation on subtasks and pathes. Details are presented in the following section."}, {"title": "Environments and Collaboration Process", "content": "The Overcook-AI environment is designed to test the coordination skills of multiple agents or human agents. Agents work together in a layout to achieve higher scores by preparing and serving soups within a set time frame, following recipe-specific cooking procedures to complete the available recipes. Unlike end-to-end AI agents (e.g., behavior cloning or reinforcement learning), the collaboration process between human and embodied agents can generally be divided into two key steps: determining the current subtask and executing atomic actions to finish each subtask.\nThe possible subtasks in overcooked environments include: 1. Collect ingredients (e.g., onions, tomatoes) and add to the pot. 2. Cook the ingredients and wait for the timer to indicate the soup is ready. 3. Serve the soup in clean dishes and deliver it to the serving location. 4. Agents can also determine to drop the items in their hands on any empty counter. To finish one order, agents have to infer the current state and determine the next subtask based on the recipe. Once the agent determined a subtask and its corresponding position, the agent can be directed through a sequence of atomic actions: up, down, left, right, stay, and interact to finish the subtask. During the collaboration, adaptations of both subtasks and action sequences need to happen in real-time due to the uncertainty of humans or paired agents."}, {"title": "Mode 1: Overall testing", "content": "In order to better evaluate the agent's real-time adaptation capabilities, we adopted the teaming fluency metrics discussed in and to design layouts for our real-time adaptation benchmark. The teaming fluency of a layout is defined as the percentage of non-obstructed areas within the total free area of a layout. If one agent stays at one position and does not adapt, the other agent cannot finish the task independently, we then mark this position as an obstructed area (red crosses as shown in Figure 2A). With this definition, a higher teaming fluency score suggests an open layout where agents can operate independently without much need to account for each other's presence. Conversely, a lower teaming fluency indicates a more confined and narrow layout, necessitating agents to adapt to one another.\nTo generate layouts with different teaming fluency, we adopt the following two steps: first, we use GPT-40 to generate layouts with symbolic text representation by prompting it to vary the positions of interaction points (e.g., onion, tomato, pot) and adjust the number and positions of empty counters to change the free space. After generating enough layouts, we run a script to filter layouts based on whether they are solvable and teaming fluency. Finally, we manually review the layouts to ensure they are suitable for investigating different adaptation skills and revise them if needed.\nWe have selected 22 layouts with teaming fluency scores ranging from 88.37% to 7.14%. More details are shown in the Appendix A.2. These layouts impose constraints on concurrent motions with gradually increasing complexity, requiring agents to adapt to dynamic situations in real-time.\nEvaluation criteria For different layouts, we measure the overall score achieved in a certain time threshold as a metric. To achieve good performance in the overall evaluation, the agent needs to show the ability to adapt subtasks as well as low-level paths, especially when paired with non-adaptive agents."}, {"title": "Mode 2: Path adaptation testing", "content": "The overall score reflects the agent's general adaptation capability. However, it cannot independently assess path adaptation and subtask adaptation, providing less insight. To explicitly evaluate the path adaptation capabilities of embodied agents, we carefully design short-horizon scenarios and frames where the subtask of each agent is provided. For each short scenario, we first select layouts where teaming fluency is below 50%. Then, we vary the agent's starting position and target position for each agent to form a test. The criteria are that the greedy path of the agent has to collide, and possible adaptation plans exist. Then, we define and label three types of adaptation situations during the execution of subtasks: self-adapt, other-adapt, and both-ok. In the self-adapt scenarios, as shown in the Figure 2B, the agent must yield human trajectory to achieve better fluency or finish both players' subtasks. In other adapt scenarios, task success can only be achieved by asking human teammates to yield. In both-acceptable situations, either can yield or communicate and achieve both subtasks assigned. Finally, human experts will determine the suitable adaptation plan and calculate timesteps for each scenario, and we will set the time limit for each scenario accordingly. In total, we designed 16 self-adaptation scenarios, 14 other-adaptation scenarios, and 13 scenarios where both agents may yield.\nEvaluation criteria We provided both quantitative and qualitative evaluations of the agent's path adaptation capabilities based on the short scenarios. For the quantitative evaluation, we have the agent start on the designed start frame, and the scenario is counted as successfully finished if both agents can complete their assigned subtasks within the limited timesteps. The success rate and stuck time on different scenarios provide us with a direct assessment of the agent's ability in path adaption and spatial reasoning. For the qualitative evaluation, we provide frame testing by asking the agent to reason about the start frame and output a language-based adaptation plan. The reasonableness and consistency of the proposed adaptation plan are evaluated by comparing it to a human-labeled adaptation plan."}, {"title": "Mode 3: Subtask adaptation testing", "content": "For subtask adaptation testing, we follow a similar process by first labeling the frame where the human experts think that the subtask adaptation is needed. Among those frames where subtask adaptation is needed, it will be further labeled as self-adapt, other-adapt, or both-ok. Finally, a ground subtask adaptation is provided for each frame. Figure 2C shows three frames that require adaptation with their labeled ground truth message.\nEvaluation criteria For subtask adaptation testing, we can directly compare the generated subtask goal location with human-provided subtask adaptation plans."}, {"title": "MonTA Framework", "content": "For seamless teaming between AI agents and unpredictable humans, the Al agent must have effective communication, high-level reasoning, and real-time adaptive action capabilities. Previous approaches using LLMs for coordination and subtask reasoning still rely on pre-defined scripts for atomic actions, limiting low-level adaptability. Inspired by cognitive studies showing that humans interchange between fast and intuitive thinking versus slow and deliberate thinking , We introduce MonTA agent (Figure 3), which leverages two LLMs for explainable, real-time adaptation. MonTA features three key modules:\n\u2022 Monitor: Continuously checks atomic actions to determine the need for adaptation or adjustments.\n\u2022 Path Adapter: Adjusts paths in real-time to accommodate changing environments.\n\u2022 Subtask Adapter: Oversees subtask execution and dynamically adjusts tasks as needed.\nThis section focuses on the design of the framework and how the adapters and monitor enable adaptive collaboration and communication. Detailed prompts for each module can be found in Appendix A1.\nMonTA uses a DFS planner to compute path as a sequence of atomic actions, given a target location. These atomic actions include up, down, left, right, interact, and stay. It's worth noting that this planner can be replaced by any existing algorithm, highlighting the generalizability of our proposed framework."}, {"title": "Monitor", "content": "To enable real-time reactive adaptation, latency must be imperceptible to ensure seamless collaboration. Inspired by human decision-making, which involves both fast, intuitive responses (System 1) and deliberate, analytical reasoning (System 2), we employed an active Monitor capable of high-frequency inference. Like System 1, the Monitor quickly handles straightforward planning and execution, reserving more deliberate reasoning akin to System 2 for moments when adaptations are required. Specifically, it decides to launch subtask-level or path-level adaptations as needed, dynamically switching between a greedy plan and an adaptive plan. The chosen plan is executed by an underlying planner. For instance, subtask-level adaptation is required when agents face conflicting task allocations (e.g., preparing duplicate items), while path-level adaptation is needed for resolving conflicts in planning paths (e.g., navigating a narrow corridor in opposite directions). The agent reverts to its original path once the Monitor detects further adaptation unnecessary. By deploying an LLM to monitor actions at atomic level during decision-making, we achieve low inference latency, enabling real-time adaptation.\nMonitor model selection To meet the inference speed requirements of the Monitor, we tested various models of different sizes to evaluate their inference speed and reasoning capabilities. Based on the most updated LLM benchmark , we selected the representative models including GPT-40, Llama 3.1-8B-Instruct, Llama 3.2-3B-Instruct, and Llama 3.2-1B-Instruct. To further op-"}, {"title": "Subtask Adapter", "content": "The Subtask Adapter is designed to determine the next or ongoing subtask for all agents, such as picking up onions, when prompted by the Monitor. It considers the overall task goal, world state, and recipes to infer the current goals of other agents and the agent's own goal. It then identifies potential target locations (which may include multiple options) and passes a path to the selected target location to the Monitor for active oversight. We leverage a proficient LLM, GPT-40 , to implement Chain-of-Thought reasoning. Based on the chosen adaptation strategy, the Subtask Adapter generates subtask-level messages to coordinate with other agents."}, {"title": "Path Adapter", "content": "The Path Adapter is designed to determine the best alternative plan beyond the original greedy plan when prompted by the Monitor. Using the provided alternative paths and associated costs, the Path Adapter reasons about the optimal temporary adaptation path to avoid collisions. The agent responsible for adapting is selected based on the available alternatives. Similarly, we leverage GPT-40 to perform Chain-of-Thought reasoning, enabling the Path Adapter to determine which agent should adapt and to select the best adaptation path from the options provided by the planner. The Path Adapter then generates a contextually relevant message based on the chosen adaptation strategy. An example prompt is provided in Appendix A1."}, {"title": "Experiments and quantitative results", "content": "To demonstrate the advantages of our proposed MonTA framework, we compare our MonTA framework with two baselines. The baseline models consist of a rule-based greedy agent and a subtask adapter agent (SAA). GA uses a rule-based subtask planner combined with Depth First Search methods to compute atomic actions . It also includes an auto-unstuck function that selects randomly from the available actions when stuck. SAA uses use LLM to generate the next subtask when the current subtask is finished and employs a greedy planner to execute atomic actions without real-time adaptation. We then evaluate and compare our MonTA with baselines on the proposed benchmark."}, {"title": "Overall performance", "content": "In multi-agent collaboration, an adaptive agent must recognize when to adjust its actions based on the actions of other agents. To evaluate this adaptability, we selected three layouts 7, 19, and 27 with teaming fluency 82.6%, 40%, and 16.7%, respectively. For each layout, we paired our agent and baseline models with GA. We then conducted experiments 3 times to get the average game score. During the game, the target agent does not communicate with paired GA, requiring adaptive behavior from our agent to successfully complete tasks and get scores as the layout complexity increases."}, {"title": "Effectiveness of the Benchmark", "content": "The Figure 4 shows that all three agents, including MonTA and the two baselines, achieved their best performance on Layout 7. This is expected, as Layout 7 has high teaming fluency, minimizing the likelihood of agents getting stuck and reducing the need for subtasks or path adaptations. As the layout becomes complex and has a lower teaming fluency, we note that the GA consistently scores 0 in layouts 10 and 27 over 5 trials. This potential results from the high ratios of adaptation requirements to finish an order in these two layouts, and the random action used to unstuck is less ineffective or even impossible to succeed. Similarly, SAA also experienced a performance drop as teaming fluency decreased but got slightly better performance than GA by leveraging a proficient LLM to infer subtasks, providing subtask flexibility compared to the rule-based subtask transitions used by GA. These results highlight that as team fluency metrics decrease, agents must exhibit higher adaptability, showcasing the benchmark's ability to evaluate varying levels of adaptability effectively."}, {"title": "MonTA Agent Excels in Layouts with Lower Teaming Fluency", "content": "Our results demonstrate that the MonTA agent outperforms the two baseline agents across all three tested layouts, achieving scores of 156 \u00b1 0, 53 \u00b1 0, and 76.6 \u00b1 26.5, as shown in Figure 4. This highlights the superiority of MonTA in collaborative scenarios. Additionally, the SAA struggles to match MonTA's performance in Layouts 19 and 27, emphasizing the importance of path adaptation and real-time execution of adaptive strategies. This is further supported by the higher adaptation ratios of the MonTA agent in Layouts 19 and 27 compared to the ratio in Layout 7.\nAnother interesting observation is that the MonTA agent demonstrates significantly lower variance compared to GA and SAA. This indicates that MonTA consistently identifies potential adaptation needs and executes adaptive plans to prevent failure rather than relying on the paired agent to adapt. This capability is particularly crucial when pairing with agents with unknown dynamics like human."}, {"title": "Path adapter evaluation", "content": "While overall performance across layouts is a useful metric for evaluating an agent's adaptability, there is significant randomness in scores, particularly for GA and SAA. For instance, SAA achieved a very high score in one trial on layout 10 due to a rare occurrence where the agents' subtasks did not collide, resulting in an unusually high score. Furthermore, when paired with an auto-unstuck agent, a greedy decision can force the other agent with the auto-unstuck function to yield, progressing step by step at a very slow speed. To better evaluate adaptation capabilities, we conducted experiments on the path adaptation benchmark.\nThe agents are asked to complete two conflicting subtasks within a limited timeframe. Further details about the designed scenarios are provided in Benchmark Section and Table 2. To balance real-time responsiveness with effective adaptation detection, we varied the LLMs used for atomic monitoring. For the adapter, which requires high reasoning capabilities but does not need frequent queries, GPT-40 was selected in this experiment. In total, we evaluated four agents using GPT-40, Llama-3.1-8B, Llama-3.2-3B, and Llama-3.2-1B as monitors, alongside two greedy planners-one with and one without an auto-unstuck function. Each of the six agents was paired with the greedy planner featuring the auto-unstuck function to assess their ability to handle expert-designed scenarios. We run 5 trials for each configuration on 21 scenario evaluations. Additional details about the scenarios can be found in Table 3.\nIn scenarios requiring the target agent to adapt, our MonTA framework achieves an almost 100% (Figure 5A green, red, blue bars) success rate even when utilizing a lightweight large language model like Llama3.2-3B. In contrast, the two greedy planners almost always fail to complete these scenarios, experiencing significant stuck time (Figure 5A, brown bar), whereas the MonTA agents successfully avoids being stuck. This clearly demonstrates the superior adaptability of our framework.\nIn scenarios labeled as Both-OK and Other-Adapt, the MonTA agent using GPT-40, Llama3.2-8B, or Llama3.2-3B achieves performance similar to the greedy planner. In these cases, the MonTA agent may request the paired agent to adapt while following its original plan. This approach can still achieve the subtask goal because the paired agent executes an auto-unstuck function. The standard deviation is high here because we are averaging all scenarios which belong to the same type. Notably, in such cases, MonTA retains an advantage when paired with a human or an agent capable of interpreting language input, as it can effectively communicate and inform adaptation plans to allow the paired agent to adapt more effectively."}, {"title": "Lauguage instruction study", "content": "True reactive collaborators not only adapt their behavior but also provide instructions to others when necessary. In our framework, the adapter leverages a proficient LLM, GPT-40, to reason about adaptation plans and generate language instructions. The adapter autonomously adjusts its behavior when self-adaptation is required and sends language instructions only when another agent needs to adapt.\nTo evaluate the language instructions generated by our adapter, we queried GPT-40 to produce instructions for both self-adaptation and other-adaptation scenarios at each spe-"}, {"title": "Trade-off between performance and latency", "content": "To use LLMs to aid with real-time decision-making, there is a latency requirement, where high latency may influence the human experience of collaboration. Therefore, we experimented to compare the latency estimation of each query. The reactive adaptation process includes two latency levels: monitor latency and adapter latency. The monitor is queried every atomic action step, which largely determines the overall frequency of collaboration, however, if the adapter has to be called multiple times due to the incapability of the monitor, it might still result in a latency issue. We selected four language models to test, including GPT-40, Llama3.1-8b, Llama3.2-3b, and Llama3.1-1b. The GPT-40 is queried using API, where the Llama is run locally through SGLang in an RTX3090 GPU computer.\nWe report the average latency estimation results from our path adapter experiments in Table 1. As expected, the adapter latency is consistently higher than the monitor latency across all LLMs, aligning with the design goal for the monitor to prioritize speed over complex reasoning. Notably, as the model size decreases to 8B and 3B, the monitor latency is reduced to approximately 0.1 seconds, enabling a monitoring frequency of 8Hz for real-time decision-making, and these smaller models can run locally.\nFurthermore, the GPT-40 monitor only needs to query the adapter in 9.7% of cases, demonstrating excellent performance in identifying when adaptation is necessary. In contrast, the Llama models struggle to achieve efficient state transitions for monitoring. To improve generalization to real-world systems, fine-tuning or distilling the Llama models would be an ideal approach to enhance their performance within this framework."}, {"title": "Conclusion", "content": "In this paper, we focus on leveraging LLMs to enable agents with real-time adaptation capabilities. To achieve this, we first create a benchmark consisting of diverse layouts, carefully designed scenarios that require agents to demonstrate reactive and adaptive behaviors. We then introduce the MonTA framework, the core idea of which is only to utilize the LLM to monitor whether and what type of adaptation is needed. This allows the monitor to assess the agents' status at a higher rate in real time. When necessary, the LLM transitions to more deliberate \"slow thinking\" to adapt plans or provide user instructions. Our experiments demonstrate that the real-time adaptation capabilities significantly enhance performance and robustness when two agents collaborate in low-teaming-fluency layouts and execute our designed subtasks."}, {"title": "Appendix", "content": "A1. Prompt Construction\nWe have distinct prompts for Subtask Adapter, Path Adapter, and Monitor. There are two prompts for Monitor as it serves different purposes depending on whether an agent is adapting or following the original greedy path.\nSubtask Adapter prompt Subtask Adapter prompt contains environment context, Current game state, filtered actions, and goals.\nContext:\nYou are a chef that works with another human chef in a kitchen\nYou should follow these rules: ...\nThe procedure to finish one dish is\nRecipe book:\nRecipe 0: Requires ingredients: [ingredient 1],[ ingredient 2], [ingredient 3]\nKitchen state:\n[Kitchen Items in text]\nYour current state:\n1. You are at the coordinates (x,y)\n2. You are facing [item name]\n3. You are holding [item name]\nThe state of the other human chef:\n1. The other chef is at the coordinates (x,y)\n2. They are facing [item name]\n3. They are holding [item name]\nYour available actions:\nOption 1: [available subtask]\nOption 2: [available subtask]\nHuman available actions:\nOption 1: [available subtask]\nOption 2: [available subtask]\nGoal:\nYour first step will be analyze the state of the kitchen and items, as well as the recipe to get next best action.select an action from your avaiable actions and select the target position to interact. choose your target position from kitchen items. do not select target position not listed in kitchen state list.\nYour second step will be analyze human state, world state and human message/human preference, reason about human intention. choose a human intended position from Delivery location, pot, dispenser listed in kitchen items. do not select target position not listed in kitchen item list.\nReturn the final data with human intented target position, human intented action id, your final_action_id, target position,\nPath Adapter prompt Path Adapter takes information about agents' greedy paths, potential adaptation plans with associated costs, and goals.\nHere is your planned greedy path:\n[agent greedy path]\nHuman is likely to take following path:\n[human greedy path]\nThese two paths overlap path points, which causes collisions.\nYour potential adapt plans:\n\\Plan 1: [ available plan, plan length]\n\\Plan 2: [available plan, plan length]\nhuman potentail adapts plans:\n\\Plan 1: [available plan, plan length]\n\\Plan 2: [available plan, plan length]\nFirst check the adaptation plan works, by checking if the adaptation plan of one agent will still overlap with the other agent's original path.\nAfter identifying a valid adaptation plan, choose one with the lowest cost and decide which agent to adapt., please check carefully if the adaptation plan has conflict with other agnet's original path\nReturn the probability of humans adapting with 1 to p_human_adapt if human adaptation has low cost, and the probability of agent adapting with 1 to p_agent_adapt if agent adapt has lowerst cost and valid and adapt_index, give me detailed analysis\nMonitor prompt Monitor has two prompt. One prompt monitoring if agents need to shift to the adaptation path, and one prompt monitoring if agents need to switch back to the original greedy path. We show one prompt here as they only vary in prompt goals. Prompt contains grid layout, agent positions, target positions, agents' greedy path to target, and goals.\nContext:\nGrid layout:\nThis is a 9x7 grid world. The top left corner is (0, 0) and the bottom right corner is (8, 6). Moving down will result second pos coordinates +1, e.g. (0,0) -> (0,1), moving right will results the first pos coordinate +1, eg (0,0)->(1,0) The Grid contains the following items:\nX is obstacle, a is your position, and A is your target position, b is position of human partner and B is human partner's target position (inferred ).\n[grid layout]\nYou are at the coordinates: [agent position]\nYour target positions: [agent target position]\nThe other chef is at the coordinates: [others position ]\nHuman Target Position: [others target position]\nyour planned greedy path:\n[agent greedy path]\nHuman is likely to take following path:\n[human greedy path]\nYou are current doing a clear temperary adaptation path for collision avoidance:\n[adaptation path]\n***Your goal: Only using all the information above *** analyze Do you need to adapt to human behavior? for example, you should adapt to human when you want to avoid collision (human current position is on your way).\notherwise, do not adapt. For example, if you see that both agent are stucked, then it could be good to adapt.\nrespond your analysis and if you follow greedy or not. respond true if follow greedy, false if not."}, {"title": "A2. Additional details of benchmark", "content": "Layouts The layouts is selected based on the teaming fluency metrics from high to low. Table 2 provides a visualization of the selected 22 layouts and the corresponding ID as well as the teaming fluency.\nFrames We generated 41 frames with three types, including self-adapt, other-adapt, and both-ok. We use 20 frames for quantitative testing, which is shown in Table 3 and 21 frames for qualitative testing(Table 6)."}, {"title": "A3. Additional results", "content": "Figure 7 presents the success rate of path adaptation testing on 20 frames shown in Table 2. Figure 8 presents the detailed evaluation of Ilm-generated language instructions for each frame shown in Table 6. Table 4 and Table 5 shows detailed success rate and stuck time for path adaptation testing."}]}