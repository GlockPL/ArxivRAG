{"title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation", "authors": ["YUAN SUN", "NAVID SALAMI PARGOO", "TAQIYA EHSAN", "ZHAO ZHANG", "JORGE ORTIZ"], "abstract": "Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence-approaches that are often impractical in real-world settings. In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly available datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR's explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts.", "sections": [{"title": "1\nINTRODUCTION", "content": "In recent years, the proliferation of sensors across diverse settings has become ubiquitous, seamlessly integrating into the very fabric of daily life. These sensors are embedded in a wide array of devices such as smartphones,\ncameras, clothing, buildings, and vehicles, enabling continuous and pervasive data collection. This expansion has significantly propelled the field of activity recognition, placing it at the forefront of ubiquitous computing research. The potential applications of activity recognition are vast and impactful, encompassing areas such as healthcare [13], elderly care [33], surveillance, and emergency response [52]. The ability to monitor and understand human activities on such a scale holds immense utility, offering advancements in personal health monitoring, enhanced security systems, and more efficient emergency services.\nHowever, alongside its rapid growth and integration into various domains, the field of activity recognition faces several substantial challenges. Chief among these is the issue of sensor data labeling, which is crucial for the development of accurate and reliable models. This process often encounters significant hurdles such as the absence of labels, incorrect labeling, or the extensive manual effort required for annotation [14, 30, 53]. Furthermore, the models developed for activity recognition frequently remain complex and opaque, functioning as \"black-boxes\" that are difficult for both experts and laypeople to interpret [4, 5, 34]. This complexity hinders transparency and assessment, posing a barrier to trust and understanding among users who are not well-versed in technical details.\nTo address these issues, there is a growing emphasis on the development of Explainable AI (XAI) methods. Such methods are crucial for demystifying AI decision-making processes, increasing transparency, and building trust among users [55]. By making AI decisions more accessible and understandable, XAI not only enhances user confidence but also facilitates wider adoption and integration of these technologies across various sectors. This approach aims to ensure that as Al systems become more integrated into our lives, they do so in a manner that is both comprehensible and trustworthy, ultimately leading to more informed and accepting user interactions.\nTo unlock activity recognition's full potential in ubiquitous computing, addressing labeling challenges and creating techniques that enable layperson understanding of model insights is paramount. This bridges the gap between raw sensor data and actionable information, facilitating the development of reliable, trustworthy, and deployable real-world activity recognition systems."}, {"title": "1.1 Challenges in Complex Human Activity Recognition", "content": "Traditional methods for Complex Human Activity Recognition (CHAR) typically necessitate precise labeling of each atomic activity within specific time slots to effectively train models. While some research attempts to incorporate conceptual frameworks, these approaches often require segmenting the data to enhance accuracy. Such segmentation demands detailed labeling of each atomic activity, including the elimination of transient states, which can be labor-intensive and prone to inaccuracies regarding the exact start and end points of activities.\nIn practical scenarios, real-life datasets typically categorize types of atomic or complex activities within specific collection intervals (see Fig. 1) [10, 25, 38, 39]. While some datasets provide detailed atomic activity labels, these can often be erroneous or unreliable [19, 24]. Furthermore, some datasets only indicate the type of activities (Fig. 1), encompassing n atomic activities where m activities may occur concurrently, leading to $C (n, m)$ possible combinations. This underscores the combinatorial complexity faced when segments cannot be distinctly separated. Our framework is specifically designed to address such challenges by managing inseparable dataset segments and extending beyond merely detecting n isolated activities. It is important to note that a prevalent assumption in the field suggests that the performance of machine learning models degrades as they become more explainable, especially when the model structures become intricate [15]. Additionally, these datasets often assign uncharacterized activities to generic categories like \"others\" or exclude them altogether, which presents significant labeling challenges and complicates the development of generalized solutions adaptable to real-world applications.\nMoreover, significant challenges persist in representing the outputs of sensor-based models within the visual domain. Despite increasing interest in transforming sensor data into image representations to enhance layperson"}, {"title": "1.2\nContributions", "content": "We developed the Variance-Driven Complex Human Activity Recognition (VCHAR) framework with Generative Representation to tackle prevalent issues in the recognition of complex human activities. VCHAR overcomes the limitations of traditional CHAR methods that require detailed and labor-intensive segmentation of activities. Instead, it utilizes a variance-driven approach that leverages the Kullback-Leibler divergence to approximate the distribution of atomic activity outputs. This method allows for the recognition of decisive atomic activities within specific time intervals without necessitating the removal of transient states or other irrelevant data, thereby enhancing the detection rates of complex activities even in the absence of detailed labeling of atomic activi- ties.Our experiments demonstrate that even without precise labeling of atomic activities or without sequentially corrected labeling, our model effectively utilizes key concepts to enhance the detection rate of complex activities. Additionally, it provides a promising rate of atomic activity detection, which is crucial for accurately representing the data when transmitting outputs to the decoder.\nMoreover, VCHAR introduces a novel generative decoder framework that transforms sensor-based model outputs into integrated visual domain representations. This includes detailed visualizations of both complex and atomic activities, alongside desired sensor related information from the model. Utilizing a Language Model (LM) agent, the framework organizes diverse data sources and employs a Vision-Language Model (VLM) to generate comprehensive visual outputs. To facilitate rapid adaptation to specific smart space scenarios, we propose a pretrained \"sensor-based foundation model\" and implement a \"one-shot tuning strategy\" with masked guidance. Our experiments on three publicly available datasets demonstrate that VCHAR not only performs competitively with traditional methods but also significantly enhances the interpretability and usability of CHAR systems, as confirmed through human evaluation studies.Our contributions are multi-faceted and can be summarized as follows:"}, {"title": "2 RELATED WORK", "content": "2.0.1 Smart Space Complex Activity Recognition. Significant advancements have been made in recognizing both atomic and complex human activities using various sensor technologies and machine learning models. Bao et al. utilized multiple biaxial accelerometers along with decision tree classifiers, demonstrating that an increase in the number of accelerometers and subject-specific training enhances performance [6]. Dernbach et al. found that while atomic activities could be accurately classified using a smartphone's triaxial accelerometer and gyroscope with a Multi-layer Perceptron, complex activities posed greater challenges [12]. Mekruksavanich et al. introduced a CNN-BiGRU model that effectively recognizes complex activities from wrist-worn sensors [29], while Tahvilian et al. compared the efficacy of CNN-LSTM and CNN-BiLSTM models for varying complexities of activities [45]. Additionally, Peng et al. proposed a multi-task learning approach using CNNs and LSTMs that improved"}, {"title": "2.0.2 Visual Representation of Sensor Data", "content": "The transformation of sensor data into visual formats has gained significant attention, enabling the application of image classification techniques to sensor-based human activity recognition [16, 46]. Researchers have explored various methods to facilitate this transformation. For instance, transforming sensor data into spectrogram images has allowed the use of deep learning models like CNNs for recognizing human activities [10, 20, 22].\nFurther advancements in this field have led to the development of methods that enhance interpretability. Arrotta et al. [3] utilize CNNs and Explainable AI to translate sensor data into semantic maps for transparent activity recognition in smart homes, aiming to boost trust, particularly in healthcare monitoring scenarios. These semantic maps are further processed into natural language explanations, providing clear and understandable insights into the AI's decision-making processes. Another approach by Jeya et al. [21] applies CTC loss to align detected activities with raw sensor signals, which are visualized on time-acceleration graphs and marked with dashed rectangles to improve the interpretability of complex activities.\nAdditionally, explainability in AI can be approached through model-agnostic methods such as LIME [8] and SHAP [28], which approximate the relationship between input and output predictions without accessing the model's internal workings. Conversely, model-transparent methods like Grad-CAM++ [8] and saliency maps [40] provide insights into the internal processes of neural networks by visualizing the importance of input features and the activation value of hidden layers.\nIn response to the growing interest in making sensor data comprehensible through visual representations[18], our work develops vision domain representations directly from sensor data. Specifically, we aim to make the outcomes of activity recognition models accessible to laypersons by applying a video-based representation of sensor activation value. This approach bridges the gap in understanding complex models for users without technical expertise, enhancing user trust and engagement with the technology."}, {"title": "2.0.3 Foundation and Multimodal Models", "content": "Foundation models, the latest evolution in AI technologies, are trained on extensive, diverse datasets and are capable of being applied across a wide range of domains [7]. These models, which are adaptable to numerous applications, highlight the forefront of AI research, benefiting from their training on vast and heterogeneous datasets.\nRecent advancements have seen various multimodal models that leverage this foundation. Wang et al. [48] developed a sequence-to-sequence framework that unifies diverse tasks across modalities using an instruction-based task representation, pretrained on image-text data for both crossmodal and unimodal tasks. Similarly, Lu et al. [27] introduced a transformer sequence-to-sequence model that performs a variety of vision and language tasks without requiring task-specific branches, trained on over 90 datasets related to vision and language.\nFurthering the multimodal approach, Singh et al. [41] created a model with separate encoders for images, text, and multimodal integration, which was pretrained on unimodal and multimodal losses for 35 tasks across vision, language, and vision-language areas. Another contribution by Singh et al. [49] includes a multimodal vision-language model with a shared multiway transformer backbone, trained on masked data modeling across modalities, achieving state-of-the-art performance on various vision and vision-language benchmarks. Li et al. [26] explored cross-modal contrastive learning to unify representations across modalities with a unified transformer on image and text data, enhancing the synergy between these modalities.\nIn this research, we harness the domain adaptation capabilities of generative models to customize the sensor decoder for specific scenarios of sensor data representation. This approach is designed to significantly enhance the visualization quality of the sensor model's outputs."}, {"title": "3 RESEARCH METHODS", "content": "This section systematically details the VCHAR framework, commencing with an architectural overview and operational functionality. We begin by outlining the fundamental architecture and principal features of VCHAR, providing a foundation for detailed exploration. This is followed by an in-depth analysis of the conceptual framework that forms the basis of VCHAR, discussing both the formulation of the problem it addresses and the integration of relevant theoretical concepts. Through this structured exposition, we aim to furnish a comprehensive understanding of VCHAR's underlying principles and its functionality within practical applications."}, {"title": "3.1 Outline of the VCHAR Framework", "content": "The Variance-Driven Complex Human Activity Recognition (VCHAR) framework, as depicted in Fig. 3, is an end-to-end model designed to enhance both the prediction and explanation of complex human activities. This model uniquely employs the Kullback-Leibler (KL) divergence to approximate the distribution of atomic activities across various sliding window lengths. By leveraging a variance-driven approach, VCHAR circumvents the need for specific time-unit labels for atomic activities, focusing instead on the types of activities occurring within given time slots. Comparative experiments demonstrate that this approach significantly enhances the accuracy of complex activity detection relative to other methods.\nThe multitask design of VCHAR not only improves the recognition of complex activities but also establishes an \"interface\" facilitating detailed visualizations by a generative decoder. For instance, when VCHAR detects complex activities such as making a sandwich, it concurrently identifies related atomic activities like opening a door or turning a switch. It also provides a list of desired sensor realted information from the model, highlighting the significance of each sensor in the smart space environment.\nTo further enrich the model's output, a Language Model (LM) agent is integrated to reorganize and elucidate detailed information about these activities. In practical applications, we propose a \"sensor-based smart space foundation model\" framework, capable of being tailored to specific scenarios through advanced techniques. Techniques such as Denoising Diffusion Implicit Models (DDIM) and Latent Diffusion Models (LDMs), coupled with a masked training strategy, are employed to refine the quality and relevance of the generated outputs. This structured process ensures that VCHAR is not only effective in recognizing complex human activities but also adept at providing actionable insights into the dynamics of smart spaces, thereby enhancing user interaction and understanding."}, {"title": "3.2 Multi-Task Learning for Complex Activity Recognition", "content": "In the proposed model, multi-task learning is employed to facilitate the simultaneous recognition of atomic and complex activities. Atomic activities are defined as discrete actions that occur within a brief time window and are indivisible in the context of our dataset. Each complex activity, on the other hand, is composed of multiple atomic activities. Specifically, a complex activity in our model is defined as having more than 2 atomic activities, providing a more nuanced description of the grouped atomic activities.\nFor instance, within the Opportunity dataset 1, complex activities such as \"making coffee\" or \"cleaning up\" may include the atomic activity \"open the door\". This activity is further detailed as \"open the door 1\" and \"open the door 2\", enriching the model's understanding of the scenario by supplying detailed contextual information. This approach allows the model to capture the intricate relationships and recurring patterns among activities, thereby enhancing its ability to accurately classify complex activity scenarios.\nOur model analyzes raw sensor data, denoted by x, to predict the probabilities of each atomic activity occurring within a specific sliding window, alongside the classification of a complex activity. The goal is to output a probability vector p for atomic activities, where each element $p_i$ represents the likelihood of the ith atomic"}, {"title": "3.3 Loss of Atomic Activity Recognition", "content": "Our model's primary objective is to predict the probability of each atomic activity within a sliding window of sensor data, aligning these predictions as closely as possible with the actual probabilities using the mean Kullback-Leibler (KL) divergence as the loss function. The KL divergence provides a robust metric for the average difference between the predicted probability distribution $P_{predict}$ and the true probability distribution $P_{true}$, making it particularly suitable for datasets with varying class distributions.\nFormally, the loss function L for atomic activities is defined as the KL divergence between the true distribution $P_{true}$ and the predicted distribution $P_{predict}$, which can be expressed mathematically as:\n$L_{atomic} = L_{KL} (P_{predict}, P_{true}) = \\frac{1}{N} \\sum_{i=1}^{N} P_{true, i} log \\frac{P_{true,i}}{P_{predict, i}}$\nwhere N is the number of classes or atomic activities, $P_{true,i}$ and $P_{predict,i}$ represent the true and predicted probabilities of the ith atomic activity occurring within the sliding window, respectively. This formulation ensures that the model's performance is evaluated based on the average divergence across all classes, promoting a balanced sensitivity to the accuracy of each class prediction."}, {"title": "3.4 Loss of Complex Activity Recognition", "content": "Our model is specifically designed to classify complex activities by minimizing the cross-entropy loss, which measures the discrepancy between the predicted probabilities and the actual class labels for complex activities. This loss function is crucial for optimizing the model's ability to accurately categorize complex activities based on sensor data inputs.\nThe cross-entropy loss for complex activity classification is formally defined as:\n$L_{complex}(y_{predict}, C_{true}) = \u2212 \\sum_{j=1}^{M} C_{true, j} log y_{predict, j}$\nwhere, $y_{predict}$ represents the predicted probability distribution across the complex activity classes.$C_{true}$ is the one-hot encoded vector of the true class labels for the complex activities.M is the number of possible complex activity classes. Minimizing $L_{complex}$ during training ensures that the predicted probabilities align closely with the true class labels, effectively enhancing the model's accuracy in complex activity recognition."}, {"title": "3.5 Sensor Encoder Architecture", "content": "To benchmark our approach against established baseline methodologies [10, 21, 32, 42], we employ the widely recognized ConvLSTM architecture, which is particularly adept at handling sensor time series data. This architec- ture synergistically combines Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks to effectively extract and temporally analyze features from sensor data.\nThe ConvLSTM architecture operates in two primary phases:\n(1) Feature Extraction: The CNN component is responsible for spatial feature extraction from each time slice of the sensor data. This step is crucial for identifying intricate patterns within the data that are spatially localized but temporally variant.\n(2) Temporal Dependency Modeling: The LSTM layer processes the sequence of extracted features to capture temporal dependencies and dynamics, essential for understanding the progression and context of sensor readings over time.\nIn our enhanced model, we first conduct a channel-wise analysis to separately study the features from different sensor channels. These features are then integrated using a sensor fusion module, which synthesizes information across channels to provide a comprehensive feature set.\nFollowing the fusion step, a bidirectional LSTM (biLSTM) module is employed to further refine the temporal analysis, enhancing the model's ability to capture both forward and backward dependencies in the time series data. Additionally, a Multi-Layer Perceptron (MLP) module is incorporated to generate the distribution of atomic activities based on the extracted features. Finally, another LSTM layer is tailored to model and predict the complex activity outputs, synthesizing all prior analyses into a coherent activity prediction.\nExample Application: Consider a scenario involving the monitoring of elderly activities in a smart home environment. Our model processes data from various sensors (e.g., motion, door, and appliance usage sensors) through the described architecture. Initially, individual sensor channels are analyzed to detect basic movements and interactions. These are then fused and temporally analyzed to predict more complex activities, such as cooking or cleaning, demonstrating the model's capability to discern nuanced human behaviors effectively.\nThis comprehensive approach allows us to not only match but also surpass the performance of existing methods in complex activity recognition, as evidenced by our comparative evaluations. The results confirm the superiority of our model in accurately detecting and predicting both atomic and complex activities, highlighting its potential for real-world application in ubiquitous computing environments."}, {"title": "3.6 Generative Modeling for Enhanced Complex Activity Representation", "content": "For users lacking technical expertise, grasping the intricacies of sensor encoder outputs can be challenging. To bridge this gap, we implement a generative modeling approach that transforms the identified atomic and complex activities into visual narratives. This transformation is facilitated by a Language Model (LM) agent, which interprets the sensor data, encompassing the distribution of atomic activities, the classification of complex activities, and the sensor activation patterns within the model.\nTo enhance the adaptability of our system across various datasets and smart spaces, we initially pre-train our model on a diverse set of scenarios. This pretraining encapsulates a universal concept of relationships and fundamental elements essential for activity recognition. When adapting to a specific dataset or smart environment, we employ the \"one-shot tuning strategy\". This approach fine-tunes the pre-trained model, enabling it to generate tailored explanations that align with the unique context and requirements of the given application."}, {"title": "3.6.1 Pretraining a General-Purpose Sensor-Based Foundation Model Framework for CHAR", "content": "In line with the development of robust foundation models, our approach involves pretraining a sensor-based foundation model encapsulating a comprehensive suite of elements common to a wide array of scenarios. This model serves as a versatile starting point, designed to be fine-tuned subsequently to accommodate the specificities of distinct scenarios encountered by users.\nThe CHAR foundation model is imbued with a rich lexicon that describes a multitude of complex scenarios and human activities. It not only captures the essence of activity patterns but also identifies sensor-specific signatures that are pivotal for accurate activity detection. This capability ensures that, upon detection, the most relevant sensors are highlighted, providing intuitive visual cues within the generated explanations.\nFor instance, the foundation model is pretrained with elaborate activity narratives such as \"morning routine\", \"making a sandwich\", or \"preparing cereal\". These narratives embody complex sequences situated within a broader context of atomic activity representations. Moreover, the model is attuned to discern and emphasize sensors that"}, {"title": "3.6.2 One-Shot Fine-Tuning for Complex Activity Description Using DDIM", "content": "When transitioning our model to specific scenarios, it is common to encounter variations in the manifestation of activities and their corresponding complex contexts. To accommodate these scenario-specific nuances, we adopt a \"one-shot tuning\" strategy [50]. This strategy rapidly recalibrates our sensor-based foundation model to align with the new scenario character- istics. This is particularly pertinent for incorporating new activity videos where the contextual dynamics may significantly differ.\nTo further enhance the model's adaptability and descriptive capabilities, we introduce a masked training strategy. This approach facilitates the model's ability to generalize across diverse descriptive modalities. During fine-tuning, carefully designed prompts address the functionalities of specifically masked regions within the video. These prompts serve a dual purpose: they guide the video generation process in the latent space and ensure that the model's output is both representative and specific to the newly adapted scenario.\nEmploying this method enables the foundation model to not only recognize and adapt to new scenarios but also to generate activity representations that are rich in detail and contextually relevant.\nLatent diffusion models (LDMs) [35] are pivotal in our approach, functioning directly within the latent space of new video embeddings. These models utilize an encoder to project videos into a latent space, facilitating manipulations within this reduced dimensionality before reconstruction. Specifically, an encoder & maps a video"}, {"title": "3.6.3 Implementation Details", "content": "Our decoder leverages the stable diffusion model [36], utilizing online pretrained weights. Despite this, specific patterns crucial for representing complex scenarios in smart spaces are absent, necessitating further pretraining of these missing concepts post-weight loading. Our pretraining involves adjusting all model weights, whereas finetuning focuses only on several attention layers to employ a one-shot tuning strategy [50], significantly reducing GPU memory usage. This efficiency allows the use of standard gaming GPUs, like the RTX series.\nSpecifically, the complete model requires 10,000 steps for pretraining to incorporate new features, and only 500 steps to fine-tune for video applications, with a batch size of one. For inference, the DDIM sampler requires 100 steps. We utilize an NVIDIA A100 GPU for pretraining and an RTX 6000 for fine-tuning. The entire pretraining process takes approximately 48 hours to integrate a new feature, while finetuning is completed in just 30 minutes. This rapid adaptability is particularly beneficial for individualized smart space applications, where conventional"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "In this section, we detail the public datasets utilized in our experiments, providing a foundation for a comprehensive evaluation of our methodology. We will systematically discuss the outcomes of each experiment, highlighting the efficacy of our model across different scenarios. Additionally, results from human studies will be presented to demonstrate the practical effectiveness and user perception of our model. This approach allows us to present a well-rounded assessment of our framework's performance and its applicability in real-world settings."}, {"title": "4.1\nDataset", "content": "For our experiments, we utilized three publicly available datasets: Opportunity[9], FallAllD[37], and Cooking[25]. Each dataset offers unique characteristics suited to testing the versatility of our model under different conditions.\nThe Opportunity dataset provide labels for atomic activities corresponding to specific time intervals, facilitating precise activity recognition tests. In contrast, the Cooking and the FallAid datasets, which reflect real-life scenarios, labels only the types of atomic and complex activities within each time interval without specifying the exact timing of each atomic activity. This dataset merely indicates that a group of atomic activities occurs during the specified intervals, presenting a challenge in distinguishing individual actions within these periods."}, {"title": "4.1.1 Opportunity", "content": "The Opportunity dataset is a publicly accessible benchmark for human activity recognition algorithms, featuring data from 4 subjects out of an original 12 [11]. It includes 15 networked sensor systems"}, {"title": "4.1.2 FallAllD", "content": "The FallAllD dataset is a specialized resource tailored for research in fall detection, fall prevention, and human activity recognition, suitable for both classical and deep learning methodologies. Data collection involved three identical data-loggers worn by participants on the neck, wrist, and waist, each outfitted with an inertial module (comprising an accelerometer, gyroscope, and magnetometer) and a barometer.\nData were collected from 15 participants aged between 21 to 53 years old, resulting in 26,420 files, each 20 seconds in duration. Within this dataset, complex scenarios such as \"fall\" and \"no fall\" detection are highlighted, where atomic activities are indicative of both scenarios. For example, activities might include normal actions such as walking and sitting, as well as the same actions performed with an accidental fall. For our studies, we selected 8 activities that span both complex categories, allocating around 20% of the data for testing and 80% for training purposes.In our methodology, we adopted a 10-second sliding window for data segmentation, resulting in a training set dimensionality of (4964, 12, 4760) and a testing set dimensionality of (1375, 12, 4760). This windowing technique is critical for capturing temporal patterns in the sensor data conducive to recognizing complex activities.This dataset does not include specific labels for atomic activities. Each file only provides information on the type of activities, device type, recording date, and subject number."}, {"title": "4.1.3 Cooking Activity", "content": "The Cooking Activity Recognition Challenge dataset[25] is a multifaceted collection of sensory data, recorded via smartphones, wristwatches, and motion capture systems, designed specifically for the complex task of activity recognition. This dataset, procured from 4 subjects, comprises accelerometer data from the right arm, left hip, and both wrists, in addition to motion capture information from 29 distinct markers. It categorizes activities into three macro activities-making a sandwich, preparing a fruit salad, and cereal preparation-alongside 10 micro activities such as adding, cutting, mixing and \"other\". In our recognition task, we focused on nine specific activities that were relevant to our study. We excluded the \"other\" category"}, {"title": "4.2 Evaluation", "content": "4.2.1\nAtomic Accuracy Score. To assess the precision of our model in detecting atomic activities, we utilize the Atomic Accuracy Score. This metric measures the proportion of atomic activities correctly identified above a specified confidence threshold \u03b1 relative to the total number of activities detected. The score is mathematically defined as:"}, {"title": "4.2.2 Complex Activity F1 score", "content": "In the evaluation of our model's performance on complex activity classification, the F1 score is employed as a critical metric. The F1 score is a harmonic mean of precision and recall, providing a balanced measure that considers both the false positives and false negatives. This is especially important in our context where some complex activities may be underrepresented in the dataset.\nThe F1 score is calculated as follows:"}, {"title": "4.2.3 Model Comparison", "content": "In this study, we evaluate the performance of our proposed model against several established baselines in the field of CHAR. We provide a detailed description of our model in Subsection 3.5. For a fair comparison, we design the baselines with similar structures to ensure that each comparison model has approximately the same number of parameters. We employ the AdamW optimizer for training, with a maximum of 300 epochs. The following subsections detail the configurations of each comparison group:"}, {"title": "4.3 Empirical Studies of Explanation Understandability", "content": "In our effort to develop a user-friendly framework suitable for everyday use by laypersons, we conducted human evaluations to compare our method against existing approaches. These evaluations included all methods tested"}, {"title": "4.3.1 Activity Recognition Description", "content": "We assess the VCHAR, DeXAR, and Concept Bottleneck methods for complex activity recognition, focusing on their ability to represent results effectively within datasets characterized by sparse labeling."}, {"title": "4.3.2 Model Explaination", "content": "Another aspect of our evaluation focuses on illustrating to users how the model processes various data types to arrive at decisions. We assessed three distinct approaches: our proprietary method, a model-agnostic method, and a model-transparent method. This comparative study seeks to explore how different modeling approaches influence user preferences, particularly in terms of model interpretability and its impact on user satisfaction. Additionally, we analyze various explanation methods to identify which sensors are critical for recognizing atomic activities, further enhancing our understanding of each method's effectiveness in practical scenarios."}, {"title": "5\nCONCLUSIONS", "content": "In this paper, we introduce VCHAR, a variance-based method specifically designed to address label sparsity issues in in-the-wild datasets. VCHAR is capable of simultaneously detecting both complex and atomic activities, without compromising the recognition rate of complex activities. Our results demonstrate a performance improvement over other baseline methods. Additionally, we present a novel decoder that translates the model's outputs into a visual representation. This enhancement significantly aids laypersons in understanding the mechanisms of the model, compared to methods traditionally tailored for experts. A human study confirms that our method is preferred over others, offering more accessible and detailed insights to layperson users."}, {"title": "5.0.1 Limitations", "content": "Although we have made thorough attempts, our study presents several constraints as outlined below:"}, {"title": "5.0.2 Future Work", "content": "In future studies, while our principal focus remains on detecting complex activities, improving the detection rates of atomic activities will also be a key area of research. Our primary objective will be to refine the methods used for identifying atomic activities, aiming for substantial enhancements in accuracy. Additionally, addressing the challenges in real-time rendering is crucial; reducing rendering times is imperative for the practical deployment of our models in real-world settings. To achieve this, we plan to develop more efficient algorithms capable of managing the computational demands of stable diffusion processes. These improvements will aim to optimize the generation of smart space sensor representations, ensuring high-quality outputs without sacrificing speed or efficiency.\nAdditionally, to address the complexities of applying these techniques in diverse real-life environments, we aim to design and implement a unified encoder. This advanced encoder will be capable of processing and translating various types of sensor data across multiple domains into a coherent visual output. The development of such a encoder will facilitate a more seamless integration of our methods into everyday technology, making smart space technologies more adaptable and user-friendly across different settings and applications."}]}