{"title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation", "authors": ["YUAN SUN", "NAVID SALAMI PARGOO", "TAQIYA EHSAN", "ZHAO ZHANG", "JORGE ORTIZ"], "abstract": "Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the\ncontext of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities,\na task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior\nresearch has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches\nthat are often impractical in real-world settings. In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals.\nLeveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through\nvideo-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating\nprecise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR's explanations are\nmore intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among\nnon-experts.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the proliferation of sensors across diverse settings has become ubiquitous, seamlessly integrating\ninto the very fabric of daily life. These sensors are embedded in a wide array of devices such as smartphones,\ncameras, clothing, buildings, and vehicles, enabling continuous and pervasive data collection. This expansion\nhas significantly propelled the field of activity recognition, placing it at the forefront of ubiquitous computing\nresearch. The potential applications of activity recognition are vast and impactful, encompassing areas such\nas healthcare [13], elderly care [33], surveillance, and emergency response [52]. The ability to monitor and\nunderstand human activities on such a scale holds immense utility, offering advancements in personal health\nmonitoring, enhanced security systems, and more efficient emergency services.\nHowever, alongside its rapid growth and integration into various domains, the field of activity recognition\nfaces several substantial challenges. Chief among these is the issue of sensor data labeling, which is crucial\nfor the development of accurate and reliable models. This process often encounters significant hurdles such\nas the absence of labels, incorrect labeling, or the extensive manual effort required for annotation [14, 30, 53].\nFurthermore, the models developed for activity recognition frequently remain complex and opaque, functioning\nas \"black-boxes\" that are difficult for both experts and laypeople to interpret [4, 5, 34]. This complexity hinders\ntransparency and assessment, posing a barrier to trust and understanding among users who are not well-versed\nin technical details.\nTo address these issues, there is a growing emphasis on the development of Explainable AI (XAI) methods.\nSuch methods are crucial for demystifying AI decision-making processes, increasing transparency, and building\ntrust among users [55]. By making AI decisions more accessible and understandable, XAI not only enhances user\nconfidence but also facilitates wider adoption and integration of these technologies across various sectors. This\napproach aims to ensure that as Al systems become more integrated into our lives, they do so in a manner that is\nboth comprehensible and trustworthy, ultimately leading to more informed and accepting user interactions.\nTo unlock activity recognition's full potential in ubiquitous computing, addressing labeling challenges and\ncreating techniques that enable layperson understanding of model insights is paramount. This bridges the gap\nbetween raw sensor data and actionable information, facilitating the development of reliable, trustworthy, and\ndeployable real-world activity recognition systems."}, {"title": "1.1 Challenges in Complex Human Activity Recognition", "content": "Traditional methods for Complex Human Activity Recognition (CHAR) typically necessitate precise labeling\nof each atomic activity within specific time slots to effectively train models. While some research attempts to\nincorporate conceptual frameworks, these approaches often require segmenting the data to enhance accuracy.\nSuch segmentation demands detailed labeling of each atomic activity, including the elimination of transient states,\nwhich can be labor-intensive and prone to inaccuracies regarding the exact start and end points of activities.\nIn practical scenarios, real-life datasets typically categorize types of atomic or complex activities within\nspecific collection intervals (see Fig. 1) [10, 25, 38, 39]. While some datasets provide detailed atomic activity\nlabels, these can often be erroneous or unreliable [19, 24]. Furthermore, some datasets only indicate the type of\nactivities (Fig. 1), encompassing n atomic activities where m activities may occur concurrently, leading to $C(n, m)$\npossible combinations. This underscores the combinatorial complexity faced when segments cannot be distinctly\nseparated. Our framework is specifically designed to address such challenges by managing inseparable dataset\nsegments and extending beyond merely detecting n isolated activities. It is important to note that a prevalent\nassumption in the field suggests that the performance of machine learning models degrades as they become\nmore explainable, especially when the model structures become intricate [15]. Additionally, these datasets often\nassign uncharacterized activities to generic categories like \"others\" or exclude them altogether, which presents\nsignificant labeling challenges and complicates the development of generalized solutions adaptable to real-world\napplications.\nMoreover, significant challenges persist in representing the outputs of sensor-based models within the visual\ndomain. Despite increasing interest in transforming sensor data into image representations to enhance layperson"}, {"title": "1.2 Contributions", "content": "We developed the Variance-Driven Complex Human Activity Recognition (VCHAR) framework with Generative\nRepresentation to tackle prevalent issues in the recognition of complex human activities. VCHAR overcomes\nthe limitations of traditional CHAR methods that require detailed and labor-intensive segmentation of activities.\nInstead, it utilizes a variance-driven approach that leverages the Kullback-Leibler divergence to approximate the\ndistribution of atomic activity outputs. This method allows for the recognition of decisive atomic activities within\nspecific time intervals without necessitating the removal of transient states or other irrelevant data, thereby\nenhancing the detection rates of complex activities even in the absence of detailed labeling of atomic activi-\nties.Our experiments demonstrate that even without precise labeling of atomic activities or without sequentially\ncorrected labeling, our model effectively utilizes key concepts to enhance the detection rate of complex activities.\nAdditionally, it provides a promising rate of atomic activity detection, which is crucial for accurately representing\nthe data when transmitting outputs to the decoder.\nMoreover, VCHAR introduces a novel generative decoder framework that transforms sensor-based model\noutputs into integrated visual domain representations. This includes detailed visualizations of both complex and\natomic activities, alongside desired sensor related information from the model. Utilizing a Language Model (LM)\nagent, the framework organizes diverse data sources and employs a Vision-Language Model (VLM) to generate\ncomprehensive visual outputs. To facilitate rapid adaptation to specific smart space scenarios, we propose a\npretrained \"sensor-based foundation model\" and implement a \"one-shot tuning strategy\" with masked guidance.\nOur experiments on three publicly available datasets demonstrate that VCHAR not only performs competitively\nwith traditional methods but also significantly enhances the interpretability and usability of CHAR systems, as\nconfirmed through human evaluation studies.Our contributions are multi-faceted and can be summarized as\nfollows:\n\u2022 We introduce VCHAR, a variational-driven framework designed to generate visual domain representations\nof complex activity recognition. This system aims to make complex activity insights accessible to laypersons\nby visually representing the data in an intuitive manner.\n\u2022 We utilize KL divergence as a loss function to model the dynamic relationships among varying combinations\nof atomic activities across different time intervals for the same type of complex activity.\n\u2022 Our method proves effective in real-life scenarios with inaccurate or absent specific time labeling of\nactivities. Results demonstrate that multitasking modeling enhances complex activity detection rates.\nAdditionally, this multitasking modeling equips the decoder with features necessary to offer visual domain\nexplanations accessible to laypersons.\n\u2022 We propose a \"sensor-based foundation model\" framework with our masked one-shot tuning strategy that\nquickly adapts to specific smart space scenarios. An LLM agent guides this model to generate accessible\nvisual domain representations, particularly benefiting users without technical expertise.\n\u2022 We conducted experiments on 3 publicly available datasets, some with labeling issues. Our method demon-\nstrated competitive results and user preference through a user study, showcasing its effectiveness."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.0.1 Smart Space Complex Activity Recognition", "content": "Significant advancements have been made in recognizing both\natomic and complex human activities using various sensor technologies and machine learning models. Bao et al.\nutilized multiple biaxial accelerometers along with decision tree classifiers, demonstrating that an increase in the\nnumber of accelerometers and subject-specific training enhances performance [6]. Dernbach et al. found that\nwhile atomic activities could be accurately classified using a smartphone's triaxial accelerometer and gyroscope\nwith a Multi-layer Perceptron, complex activities posed greater challenges [12]. Mekruksavanich et al. introduced\na CNN-BiGRU model that effectively recognizes complex activities from wrist-worn sensors [29], while Tahvilian\net al. compared the efficacy of CNN-LSTM and CNN-BiLSTM models for varying complexities of activities\n[45]. Additionally, Peng et al. proposed a multi-task learning approach using CNNs and LSTMs that improved"}, {"title": "2.0.2 Visual Representation of Sensor Data", "content": "The transformation of sensor data into visual formats has gained\nsignificant attention, enabling the application of image classification techniques to sensor-based human activity\nrecognition [16, 46]. Researchers have explored various methods to facilitate this transformation. For instance,\ntransforming sensor data into spectrogram images has allowed the use of deep learning models like CNNs for\nrecognizing human activities [10, 20, 22].\nFurther advancements in this field have led to the development of methods that enhance interpretability.\nArrotta et al. [3] utilize CNNs and Explainable AI to translate sensor data into semantic maps for transparent\nactivity recognition in smart homes, aiming to boost trust, particularly in healthcare monitoring scenarios. These\nsemantic maps are further processed into natural language explanations, providing clear and understandable\ninsights into the AI's decision-making processes. Another approach by Jeya et al. [21] applies CTC loss to align\ndetected activities with raw sensor signals, which are visualized on time-acceleration graphs and marked with\ndashed rectangles to improve the interpretability of complex activities.\nAdditionally, explainability in AI can be approached through model-agnostic methods such as LIME [8] and\nSHAP [28], which approximate the relationship between input and output predictions without accessing the\nmodel's internal workings. Conversely, model-transparent methods like Grad-CAM++ [8] and saliency maps [40]\nprovide insights into the internal processes of neural networks by visualizing the importance of input features\nand the activation value of hidden layers.\nIn response to the growing interest in making sensor data comprehensible through visual representations[18],\nour work develops vision domain representations directly from sensor data. Specifically, we aim to make the\noutcomes of activity recognition models accessible to laypersons by applying a video-based representation of\nsensor activation value. This approach bridges the gap in understanding complex models for users without\ntechnical expertise, enhancing user trust and engagement with the technology."}, {"title": "2.0.3 Foundation and Multimodal Models", "content": "Foundation models, the latest evolution in AI technologies, are trained\non extensive, diverse datasets and are capable of being applied across a wide range of domains [7]. These models,\nwhich are adaptable to numerous applications, highlight the forefront of AI research, benefiting from their\ntraining on vast and heterogeneous datasets.\nRecent advancements have seen various multimodal models that leverage this foundation. Wang et al. [48]\ndeveloped a sequence-to-sequence framework that unifies diverse tasks across modalities using an instruction-\nbased task representation, pretrained on image-text data for both crossmodal and unimodal tasks. Similarly, Lu et\nal. [27] introduced a transformer sequence-to-sequence model that performs a variety of vision and language\ntasks without requiring task-specific branches, trained on over 90 datasets related to vision and language.\nFurthering the multimodal approach, Singh et al. [41] created a model with separate encoders for images,\ntext, and multimodal integration, which was pretrained on unimodal and multimodal losses for 35 tasks across\nvision, language, and vision-language areas. Another contribution by Singh et al. [49] includes a multimodal\nvision-language model with a shared multiway transformer backbone, trained on masked data modeling across\nmodalities, achieving state-of-the-art performance on various vision and vision-language benchmarks. Li et\nal. [26] explored cross-modal contrastive learning to unify representations across modalities with a unified\ntransformer on image and text data, enhancing the synergy between these modalities.\nIn this research, we harness the domain adaptation capabilities of generative models to customize the sensor\ndecoder for specific scenarios of sensor data representation. This approach is designed to significantly enhance\nthe visualization quality of the sensor model's outputs."}, {"title": "3 RESEARCH METHODS", "content": "This section systematically details the VCHAR framework, commencing with an architectural overview and\noperational functionality. We begin by outlining the fundamental architecture and principal features of VCHAR,\nproviding a foundation for detailed exploration. This is followed by an in-depth analysis of the conceptual\nframework that forms the basis of VCHAR, discussing both the formulation of the problem it addresses and the\nintegration of relevant theoretical concepts. Through this structured exposition, we aim to furnish a comprehensive\nunderstanding of VCHAR's underlying principles and its functionality within practical applications."}, {"title": "3.1 Outline of the VCHAR Framework", "content": "The Variance-Driven Complex Human Activity Recognition (VCHAR) framework, as depicted in Fig. 3, is an\nend-to-end model designed to enhance both the prediction and explanation of complex human activities. This\nmodel uniquely employs the Kullback-Leibler (KL) divergence to approximate the distribution of atomic activities\nacross various sliding window lengths. By leveraging a variance-driven approach, VCHAR circumvents the need\nfor specific time-unit labels for atomic activities, focusing instead on the types of activities occurring within\ngiven time slots. Comparative experiments demonstrate that this approach significantly enhances the accuracy\nof complex activity detection relative to other methods.\nThe multitask design of VCHAR not only improves the recognition of complex activities but also establishes\nan \"interface\" facilitating detailed visualizations by a generative decoder. For instance, when VCHAR detects\ncomplex activities such as making a sandwich, it concurrently identifies related atomic activities like opening a\ndoor or turning a switch. It also provides a list of desired sensor realted information from the model, highlighting\nthe significance of each sensor in the smart space environment.\nTo further enrich the model's output, a Language Model (LM) agent is integrated to reorganize and elucidate\ndetailed information about these activities. In practical applications, we propose a \"sensor-based smart space\nfoundation model\" framework, capable of being tailored to specific scenarios through advanced techniques.\nTechniques such as Denoising Diffusion Implicit Models (DDIM) and Latent Diffusion Models (LDMs), coupled\nwith a masked training strategy, are employed to refine the quality and relevance of the generated outputs.\nThis structured process ensures that VCHAR is not only effective in recognizing complex human activities but\nalso adept at providing actionable insights into the dynamics of smart spaces, thereby enhancing user interaction\nand understanding."}, {"title": "3.2 Multi-Task Learning for Complex Activity Recognition", "content": "In the proposed model, multi-task learning is employed to facilitate the simultaneous recognition of atomic and\ncomplex activities. Atomic activities are defined as discrete actions that occur within a brief time window and\nare indivisible in the context of our dataset. Each complex activity, on the other hand, is composed of multiple\natomic activities. Specifically, a complex activity in our model is defined as having more than 2 atomic activities,\nproviding a more nuanced description of the grouped atomic activities.\nFor instance, within the Opportunity dataset 1, complex activities such as \"making coffee\" or \"cleaning up\"\nmay include the atomic activity \"open the door\". This activity is further detailed as \"open the door 1\" and \"open\nthe door 2\", enriching the model's understanding of the scenario by supplying detailed contextual information.\nThis approach allows the model to capture the intricate relationships and recurring patterns among activities,\nthereby enhancing its ability to accurately classify complex activity scenarios.\nOur model analyzes raw sensor data, denoted by x, to predict the probabilities of each atomic activity occurring\nwithin a specific sliding window, alongside the classification of a complex activity. The goal is to output a\nprobability vector p for atomic activities, where each element $p_i$ represents the likelihood of the ith atomic"}, {"title": "3.3 Loss of Atomic Activity Recognition", "content": "Our model's primary objective is to predict the probability of each atomic activity within a sliding window\nof sensor data, aligning these predictions as closely as possible with the actual probabilities using the mean\nKullback-Leibler (KL) divergence as the loss function. The KL divergence provides a robust metric for the average\ndifference between the predicted probability distribution $P_{predict}$ and the true probability distribution $P_{true}$, making\nit particularly suitable for datasets with varying class distributions.\nFormally, the loss function L for atomic activities is defined as the KL divergence between the true distribution\n$P_{true}$ and the predicted distribution $P_{predict}$, which can be expressed mathematically as:\n$L_{atomic} = L_{KL}(P_{predict}, P_{true}) = \\frac{1}{N} \\sum_{i=1}^{N} P_{true, i} \\log \\frac{P_{true,i}}{P_{predict, i}}$\nwhere N is the number of classes or atomic activities, $P_{true,i}$ and $P_{predict,i}$ represent the true and predicted\nprobabilities of the ith atomic activity occurring within the sliding window, respectively. This formulation ensures\nthat the model's performance is evaluated based on the average divergence across all classes, promoting a\nbalanced sensitivity to the accuracy of each class prediction."}, {"title": "3.4 Loss of Complex Activity Recognition", "content": "Our model is specifically designed to classify complex activities by minimizing the cross-entropy loss, which\nmeasures the discrepancy between the predicted probabilities and the actual class labels for complex activities.\nThis loss function is crucial for optimizing the model's ability to accurately categorize complex activities based\non sensor data inputs.\nThe cross-entropy loss for complex activity classification is formally defined as:\n$L_{complex}(y_{predict}, C_{true}) = - \\sum_{j=1}^{M} C_{true, j} \\log y_{predict, j}$\nwhere, $y_{predict}$ represents the predicted probability distribution across the complex activity classes.$C_{true}$ is the\none-hot encoded vector of the true class labels for the complex activities.M is the number of possible complex\nactivity classes. Minimizing $L_{complex}$ during training ensures that the predicted probabilities align closely with\nthe true class labels, effectively enhancing the model's accuracy in complex activity recognition."}, {"title": "3.5 Sensor Encoder Architecture", "content": "To benchmark our approach against established baseline methodologies [10, 21, 32, 42], we employ the widely\nrecognized ConvLSTM architecture, which is particularly adept at handling sensor time series data. This architec-\nture synergistically combines Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM)\nnetworks to effectively extract and temporally analyze features from sensor data.\nThe ConvLSTM architecture operates in two primary phases:\n(1) Feature Extraction: The CNN component is responsible for spatial feature extraction from each time slice\nof the sensor data. This step is crucial for identifying intricate patterns within the data that are spatially\nlocalized but temporally variant.\n(2) Temporal Dependency Modeling: The LSTM layer processes the sequence of extracted features to\ncapture temporal dependencies and dynamics, essential for understanding the progression and context of\nsensor readings over time.\nIn our enhanced model, we first conduct a channel-wise analysis to separately study the features from different\nsensor channels. These features are then integrated using a sensor fusion module, which synthesizes information\nacross channels to provide a comprehensive feature set.\nFollowing the fusion step, a bidirectional LSTM (biLSTM) module is employed to further refine the temporal\nanalysis, enhancing the model's ability to capture both forward and backward dependencies in the time series\ndata. Additionally, a Multi-Layer Perceptron (MLP) module is incorporated to generate the distribution of atomic\nactivities based on the extracted features. Finally, another LSTM layer is tailored to model and predict the complex\nactivity outputs, synthesizing all prior analyses into a coherent activity prediction.\nExample Application: Consider a scenario involving the monitoring of elderly activities in a smart home\nenvironment. Our model processes data from various sensors (e.g., motion, door, and appliance usage sensors)\nthrough the described architecture. Initially, individual sensor channels are analyzed to detect basic movements\nand interactions. These are then fused and temporally analyzed to predict more complex activities, such as\ncooking or cleaning, demonstrating the model's capability to discern nuanced human behaviors effectively.\nThis comprehensive approach allows us to not only match but also surpass the performance of existing methods\nin complex activity recognition, as evidenced by our comparative evaluations. The results confirm the superiority\nof our model in accurately detecting and predicting both atomic and complex activities, highlighting its potential\nfor real-world application in ubiquitous computing environments."}, {"title": "3.6 Generative Modeling for Enhanced Complex Activity Representation", "content": "For users lacking technical expertise, grasping the intricacies of sensor encoder outputs can be challenging.\nTo bridge this gap, we implement a generative modeling approach that transforms the identified atomic and\ncomplex activities into visual narratives. This transformation is facilitated by a Language Model (LM) agent,\nwhich interprets the sensor data, encompassing the distribution of atomic activities, the classification of complex\nactivities, and the sensor activation patterns within the model.\nTo enhance the adaptability of our system across various datasets and smart spaces, we initially pre-train\nour model on a diverse set of scenarios. This pretraining encapsulates a universal concept of relationships and\nfundamental elements essential for activity recognition. When adapting to a specific dataset or smart environment,\nwe employ the \"one-shot tuning strategy\". This approach fine-tunes the pre-trained model, enabling it to generate\ntailored explanations that align with the unique context and requirements of the given application."}, {"title": "3.6.1 Pretraining a General-Purpose Sensor-Based Foundation Model Framework for CHAR", "content": "In line with the\ndevelopment of robust foundation models, our approach involves pretraining a sensor-based foundation model\nencapsulating a comprehensive suite of elements common to a wide array of scenarios. This model serves as\na versatile starting point, designed to be fine-tuned subsequently to accommodate the specificities of distinct\nscenarios encountered by users.\nThe CHAR foundation model is imbued with a rich lexicon that describes a multitude of complex scenarios and\nhuman activities. It not only captures the essence of activity patterns but also identifies sensor-specific signatures\nthat are pivotal for accurate activity detection. This capability ensures that, upon detection, the most relevant\nsensors are highlighted, providing intuitive visual cues within the generated explanations.\nFor instance, the foundation model is pretrained with elaborate activity narratives such as \"morning routine\",\n\"making a sandwich\", or \"preparing cereal\". These narratives embody complex sequences situated within a broader\ncontext of atomic activity representations. Moreover, the model is attuned to discern and emphasize sensors that"}, {"title": "3.6.2 One-Shot Fine-Tuning for Complex Activity Description Using DDIM", "content": "When transitioning our model to\nspecific scenarios, it is common to encounter variations in the manifestation of activities and their corresponding\ncomplex contexts. To accommodate these scenario-specific nuances, we adopt a \"one-shot tuning\" strategy [50].\nThis strategy rapidly recalibrates our sensor-based foundation model to align with the new scenario character-\nistics. This is particularly pertinent for incorporating new activity videos where the contextual dynamics may\nsignificantly differ.\nTo further enhance the model's adaptability and descriptive capabilities, we introduce a masked training\nstrategy. This approach facilitates the model's ability to generalize across diverse descriptive modalities. During\nfine-tuning, carefully designed prompts address the functionalities of specifically masked regions within the\nvideo. These prompts serve a dual purpose: they guide the video generation process in the latent space and ensure\nthat the model's output is both representative and specific to the newly adapted scenario.\nEmploying this method enables the foundation model to not only recognize and adapt to new scenarios but\nalso to generate activity representations that are rich in detail and contextually relevant.\nLatent diffusion models (LDMs) [35] are pivotal in our approach, functioning directly within the latent space\nof new video embeddings. These models utilize an encoder to project videos into a latent space, facilitating\nmanipulations within this reduced dimensionality before reconstruction. Specifically, an encoder & maps a video"}, {"title": "3.6.3 Implementation Details", "content": "Our decoder leverages the stable diffusion model [36], utilizing online pretrained\nweights. Despite this, specific patterns crucial for representing complex scenarios in smart spaces are absent,\nnecessitating further pretraining of these missing concepts post-weight loading. Our pretraining involves adjusting\nall model weights, whereas finetuning focuses only on several attention layers to employ a one-shot tuning\nstrategy [50], significantly reducing GPU memory usage. This efficiency allows the use of standard gaming GPUs,\nlike the RTX series.\nSpecifically, the complete model requires 10,000 steps for pretraining to incorporate new features, and only 500\nsteps to fine-tune for video applications, with a batch size of one. For inference, the DDIM sampler requires 100\nsteps. We utilize an NVIDIA A100 GPU for pretraining and an RTX 6000 for fine-tuning. The entire pretraining\nprocess takes approximately 48 hours to integrate a new feature, while finetuning is completed in just 30 minutes.\nThis rapid adaptability is particularly beneficial for individualized smart space applications, where conventional"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "In this section, we detail the public datasets utilized in our experiments, providing a foundation for a comprehensive\nevaluation of our methodology. We will systematically discuss the outcomes of each experiment, highlighting the\nefficacy of our model across different scenarios. Additionally, results from human studies will be presented to\ndemonstrate the practical effectiveness and user perception of our model. This approach allows us to present a\nwell-rounded assessment of our framework's performance and its applicability in real-world settings."}, {"title": "4.1 Dataset", "content": "For our experiments, we utilized three publicly available datasets: Opportunity[9], FallAllD[37], and Cooking[25].\nEach dataset offers unique characteristics suited to testing the versatility of our model under different conditions.\nThe Opportunity dataset provide labels for atomic activities corresponding to specific time intervals, facilitating\nprecise activity recognition tests. In contrast, the Cooking and the FallAid datasets, which reflect real-life scenarios,\nlabels only the types of atomic and complex activities within each time interval without specifying the exact\ntiming of each atomic activity. This dataset merely indicates that a group of atomic activities occurs during the\nspecified intervals, presenting a challenge in distinguishing individual actions within these periods."}, {"title": "4.1.1 Opportunity", "content": "The Opportunity dataset is a publicly accessible benchmark for human activity recognition\nalgorithms, featuring data from 4 subjects out of an original 12 [11]. It includes 15 networked sensor systems"}, {"title": "4.1.2 FallAllD", "content": "The FallAllD dataset is a specialized resource tailored for research in fall detection, fall prevention,\nand human activity recognition, suitable for both classical and deep learning methodologies. Data collection\ninvolved three identical data-loggers worn by participants on the neck, wrist, and waist, each outfitted with an\ninertial module (comprising an accelerometer, gyroscope, and magnetometer) and a barometer.\nData were collected from 15 participants aged between 21 to 53 years old, resulting in 26,420 files, each 20\nseconds in duration. Within this dataset, complex scenarios such as \"fall\" and \"no fall\" detection are highlighted,\nwhere atomic activities are indicative of both scenarios. For example, activities might include normal actions\nsuch as walking and sitting, as well as the same actions performed with an accidental fall. For our studies, we\nselected 8 (table 2) activities that span both complex categories, allocating around 20% of the data for testing and\n80% for training purposes.In our methodology, we adopted a 10-second sliding window for data segmentation,\nresulting in a training set dimensionality of (4964, 12, 4760) and a testing set dimensionality of (1375, 12, 4760).\nThis windowing technique is critical for capturing temporal patterns in the sensor data conducive to recognizing\ncomplex activities.This dataset does not include specific labels for atomic activities. Each file only provides\ninformation on the type of activities, device type, recording date, and subject number."}, {"title": "4.1.3 Cooking Activity", "content": "The Cooking Activity Recognition Challenge dataset[25] is a multifaceted collection\nof sensory data, recorded via smartphones, wristwatches, and motion capture systems, designed specifically\nfor the complex task of activity recognition. This dataset, procured from 4 subjects, comprises accelerometer\ndata from the right arm, left hip, and both wrists, in addition to motion capture information from 29 distinct\nmarkers. It categorizes activities into three macro activities-making a sandwich, preparing a fruit salad, and\ncereal preparation-alongside 10 micro activities such as adding, cutting, mixing and \"other\". In our recognition\ntask, we focused on nine specific activities that were relevant to our study. We excluded the \"other\" category"}, {"title": "4.2 Evaluation", "content": ""}, {"title": "4.2.1 Atomic Accuracy Score", "content": "To assess the precision of our model in detecting atomic activities, we utilize the\nAtomic Accuracy Score. This metric measures the proportion of atomic activities correctly identified above a\nspecified confidence threshold a relative to the total number of activities detected. The score is mathematically\ndefined as:"}, {"title": "4.2.2 Complex Activity F1 score", "content": "In the evaluation of our model's performance on complex activity classification,\nthe F1 score is employed as a critical metric. The F1 score is a harmonic mean of precision and recall, providing a\nbalanced measure that considers both the false positives and false negatives. This is especially important in our\ncontext where some complex activities may be underrepresented in the dataset.\nThe F1 score is calculated as follows:"}, {"title": "4.2.3 Model Comparison", "content": "In this study, we evaluate the performance of our proposed model against several\nestablished baselines in the field of CHAR. We provide a detailed description of our model in Subsection 3.5. For\na fair comparison, we design the baselines with similar structures to ensure that each comparison model has\napproximately the same number of parameters. We employ the AdamW optimizer for training, with a maximum\nof 300 epochs. The following subsections detail the configurations of each comparison group:\n\u2022 ConvLSTM: Often considered a foundational architecture for addressing CHAR problems[17, 32, 43, 47],\nthe ConvLSTM serves as a baseline to evaluate the enhancements our method brings to activity recognition.\nThis model integrates a sensor fusion module utilizing CNN to extract primary sensor features, combined\nwith LSTM networks to capture temporal dependencies among these features. The architecture culminates\nin a temporal convolution layer followed by a dense output layer, which collectively aim to optimize the\nrecognition of complex human activities.\n\u2022 Concept Bottleneck[23]: Utilizes a CNN-based structure to detect atomic concepts before advancing to\nhigher-level concepts, applying MSE loss for concept identification. This hierarchical approach is possible\nto applied to complex activity recognition.\n\u2022 PEMM: The Pointwise Error Minimization Method (PEMM) is utilized as a comparative baseline in our\nstudy. It addresses potential concerns that Concept Bottleneck outcomes are limited by reliance on CNN\narchitectures. To evaluate our model's performance enhancements, we incorporate MSE in an ablation\nstudy, contrasting it with PEMM to highlight the effectiveness and advancements of our approach.\n\u2022 Debornair[10]: This model parallels our basic ConvLSTM structure with a distinct preprocessing module\nthat processes sensor data for atomic and complex activities separately before merging them. Debornair\nexclusively design for complex output only, focusing solely on the integration of processed data without\npredicting atomic activities.\n\u2022 XCHAR[21]: Based on a vanilla ConvLSTM architecture, XCHAR differentiates itself by employing\nCTC(Connectionist Temporal Classification) loss to emphasize the importance of sequence in atomic"}, {"title": "4.3 Empirical Studies of Explanation Understandability", "content": "In our effort to develop a user-friendly framework suitable for everyday use by laypersons, we conducted human\nevaluations to compare our method against existing approaches. These evaluations included all methods tested"}, {"title": "4.3.1 Activity Recognition Description", "content": "We assess the VCHAR, DeXAR, and Concept Bottleneck methods for\ncomplex activity recognition, focusing on their ability to represent results effectively within datasets characterized\nby sparse labeling.\n\u2022 DeXAR: [3] Initially designed for atomic activity recognition, we have modified the method to simulta-\nneously estimate both atomic and complex activities. This adaptation incorporates an NLP-based visual\nrepresentation to depict the model's recognition results.In our example, as shown in Figure 11, the semantic\nvisualization employs the Dexar encoding method to represent levels of confidence through color variations."}, {"title": "4.3.2 Model Explaination", "content": "Another aspect of our evaluation focuses on illustrating to users how the model\nprocesses various data types to arrive at decisions. We assessed three distinct approaches: our proprietary method,\na model-agnostic method, and a model-transparent method. This comparative study seeks to explore how different\nmodeling approaches influence user preferences, particularly in terms of model interpretability and its impact on\nuser satisfaction. Additionally, we analyze various explanation methods to identify which sensors are critical for\nrecognizing atomic activities, further enhancing our understanding of each method's effectiveness in practical\nscenarios.\n\u2022 Grad-CAM: [28] Grad-CAM is a model-transparent method that calculates the gradient of a target concept\n(output) relative to the feature maps of a designated layer. It produces a heatmap that identifies the critical\nsensors in higher layers that are pivotal for class prediction. As illustrated in Figure 14, it shows how\nvarious sensors contribute at different intensities to a particular activity, with brighter areas indicating\ngreater importance.\n\u2022 SHAP [8] In contrast, SHAP is a model-agnostic method that approximates the relationship between\ninputs and outputs without probing the model's internal mechanisms. It focuses on representing the"}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce VCHAR, a variance-based method specifically designed to address label sparsity issues\nin in-the-wild datasets. VCHAR is capable of simultaneously detecting both complex and atomic activities, without\ncompromising the recognition rate of complex activities. Our results demonstrate a performance improvement\nover otherer baseline methods. Additionally, we present a novel decoder that translates the model's outputs into\na visual representation. This enhancement significantly aids laypersons in understanding the mechanisms of\nthe model, compared to methods traditionally tailored for experts. A human study confirms that our method is\npreferred over others, offering more accessible and detailed insights to layperson users."}, {"title": "5.0.1 Limitations", "content": "Although we have made thorough attempts, our study presents several constraints as outlined\nbelow:\n\u2022 Atomic Activity: This paper introduces a method designed to address the challenges of accurately labeling\nthe timing and sequence of activities in wild datasets, where precise annotation is often lacking. While this\napproach enhances labeling accuracy, it does not yet achieve optimal accuracy rates compared to precise\nlabeling method.\n\u2022 Real-Time Rendering: Our approach features a generative decoder that visually represents activities\ndetceted within smart spaces. However, despite its innovative design, the rendering time is extended\nrelative to traditional end-to-end decoding methods. This delay is largely due to the stable diffusion process\nemployed by the decoder, which requires 100 steps to complete the inference. While this method provides\ndetailed visual outputs, it still needs improvement for real-time applications.\n\u2022 Cross Domain Encoder: This paper introduces an approach using a universally pretrained decoder\nto interpret different scenarios within a single model, enhancing the ability to translate various types"}, {"title": "5.0.2 Future Work", "content": "In future studies, while our principal focus remains on detecting complex activities, improving\nthe detection rates of atomic activities will also be a key area of research. Our primary objective will be to refine\nthe methods used for identifying atomic activities, aiming for substantial enhancements in accuracy. Additionally,\naddressing the challenges in real-time rendering is crucial; reducing rendering times is imperative for the practical\ndeployment of our models in real-world settings. To achieve this, we plan to develop more efficient algorithms\ncapable of managing the computational demands of stable diffusion processes. These improvements will aim to\noptimize the generation of smart space sensor representations, ensuring high-quality outputs without sacrificing\nspeed or efficiency.\nAdditionally, to address the complexities of applying these techniques in diverse real-life environments, we aim\nto design and implement a unified encoder. This advanced encoder will be capable of processing and translating\nvarious types of sensor data across multiple domains into a coherent visual output. The development of such a\nencoder will facilitate a more seamless integration of our methods into everyday technology, making smart space\ntechnologies more adaptable and user-friendly across different settings and applications."}]}