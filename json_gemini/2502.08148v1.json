{"title": "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning", "authors": ["Vy Vo", "Lizhen Qu", "Tao Feng", "Yuncheng Hua", "Xiaoxi Kang", "Songhai Fan", "Tim Dwyer", "Lay-Ki Soon", "Gholamreza Haffari"], "abstract": "Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where CRAB (Romanou et al., 2023) is one such recent benchmark along this line. In this paper, we introduce ACCESS, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, ACCESS focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from GLUCOSE (Mostafazadeh et al., 2020), a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract 1, 4K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in ACCESS can be leveraged for enhancing QA reasoning performance in LLMs.", "sections": [{"title": "1 Introduction", "content": "Commonsense causal reasoning plays a vital role in developing a mental model of reality, where the ability to discover, explain and predict causal relations between events or forces in the environment is fundamental to human planning and control (Johnson-Laird and Khemlani, 2017; Griffiths, 2017). Cognitive science studies further suggest that event causality is critical to human understanding of narratives (Van den Broek et al., 1996; Fletcher and Bloom, 1988; Tillman et al., 2020; Sun et al., 2023), and story events with more causal relations tend to be better memorized than those with fewer relations (Graesser et al., 2003). Humans are able to construct a causal mental model of events after reading a set of stories (Zwaan et al., 1995). For example, in Figure 1, a reader would easily identify a causal relation between el : \u201cA person needs money.\" and e2 : \"A person gets a job.\" by abstracting away concrete details, such as mentions of particular entities, grouping linguistic variations of the same meanings, and observing that el almost always leads to e2 in multiple stories, without explicit presence of lexical cues (e.g. because) in text. Thus, this paper focuses on investigating to what extent LLMs can identify causal relations without relying on linguistic cues and perform causal reasoning over commonsense knowledge on the abstraction level.\nPrior works on causal relation extraction heavily rely on linguistic cues, e.g. because of, by, due to, to discern causal relations between event mentions and cause/effect text spans within a text (Wolff and Song, 2003; Mirza and Tonelli, 2014). In contrast, statistical causal discovery methods for event causality do not require linguistic cues but exploit statistical information of symbolic representations of events (Pearl and Mackenzie, 2018). As a result, those approaches are able to find causal relations even when they are not explicitly mentioned anywhere in texts. Therefore, there has been criticism regarding the susceptibility of these causal relation extraction models to exploit the linguistic cues to attain high performance without engaging in actual causal reasoning (Yang et al., 2022; Li et al., 2022). Ample of causal relation extraction datasets, including TempEval-3 (Mirza et al., 2014), CATENA (Mirza and Tonelli, 2016), Causal-TimeBank (Mirza and Tonelli, 2014), BECauSE (Dunietz et al., 2015, 2017) and Event StoryLine Corpus (Caselli and Vossen, 2017), are not suitable for evaluating statis-"}, {"title": "2 Causal Event Abstraction", "content": "We follow the definition of events provided in TimeML (Pustejovsky et al., 2003), ECB+ Annotation Guidelines (Cybulska and Vossen, 2014) and Event StoryLine Corpus (Caselli and Vossen, 2017). An event refers to any situation or state that happens or holds, which consists of four basic components: action/state,\nlocation, time and participant(s). We here consider location and time as optional; for instance, the sentence he goes to sleep is sufficiently an event. Each component of an event is associated with a concept in an ontology.\nFrom another point of view, an event abstraction is a generalization of a cluster of event mentions that describe the same event. Two event mentions are equivalent if they are associated with the same event abstraction. An event abstraction is causally consistent w.r.t. a set of event mentions, if (1) none of its mention pairs at the semantic level contains a causal relationship, and (2) the semantics of all its mentions are either the cause or the effect of mentions in another event abstraction. Table 1 describes all the terms used in this paper and throughout the annotation process.\nDefinition of causation. Based on the counter-factual theory of causation (Lewis, 2013), an event x is said to cause another event y and event y is said to be an effect of event x if (1) event y temporally follows event x directly i.e., there are no intermediate events or if there is one, it must rarely occur, and (2) event y would not commonly occur if event x did not occur. It is worth noting that unlike such datasets as BECauSE (Dunietz et al., 2017) or CauseNet (Heindorf et al., 2020) that consider causality between concepts, here causality is defined on the event (sentence) level, which takes into account the interaction of multiple participants. In statistical causality literature, there exist 3 causal structures of interest: confounder, collider and mediator. For random variables X, Y, Z,\n\u2022 Z is a called confounder if it causes both X and Y, written as X \u2190 Z \u2192 Y;\n\u2022 Z is a collider when Z is a common child of X and Y but X and Y themselves are not related, written as X \u2192 Z \u2190 Y;\nOntology refers to a collection of concepts and their relations within a domain (Gruber, 1993)."}, {"title": "3 The ACCESS Benchmark", "content": "ACCESS provides a graphical modelling of the cause-and-effect relations among event abstractions, where every node in the causal graph represents an event abstraction in the causal relation between any two nodes is represented by an arrow going from the cause event abstraction to the effect event abstraction. There are 725 abstractions or clusters, each of which on average contains 7 instances, and in total associated with 9, 513 stories in the GLUCOSE dataset. The graph also contains diverse causal structures for causal inference, including confounding, mediation and collider (Pearl, 2009). See Table 2 for examples of pairs of causal abstract events and Table 3 for the descriptive statistics of ACCESS.\nFigure 1 illustrates our proposed pipeline for performing abstract causal event discovery and reasoning. The ACCESS dataset is constructed in the two phases: Phase (1) is to extract event abstractions from a collection of event mentions, by grouping mentions whose generalizations describing the same event in a way that the resulting abstraction satisfies the above quality criteria. Phase (2) is to identify the causal relations among these event abstractions. Both phases entail an alternation between using automatic algorithms for extracting candidate clusters/causal pairs and crowd-sourcing for refinement and quality control. We briefly describe each phase in the following sections. See Appendix B for more details on our crowd-sourcing pipeline and task descriptions."}, {"title": "3.1 Abstract Event Extraction", "content": "We now describe the process of curating these event mentions and extracting event abstractions. Our source of commonsense knowledge is GLUCOSE (Mostafazadeh et al., 2020), a large-scale dataset of over 670K stories with annotated causal relations. GLUCOSE also provides generalized inference rules mapped from specific statements, which cor-"}, {"title": "4 Experiments", "content": "In this section, we conduct empirical analyses to demonstrate how the ACCESS benchmark is used for evaluating (1) the effectiveness of automatic event abstraction and causal discovery approaches, and (2) how a causal structure assists reasoning models on causal QA tasks. All experimental results are averaged over 5 random running seeds. The codes and data for reproducing our experiments are published at github.com/isVy08/ACCESS."}, {"title": "4.1 Abstract Event Identification", "content": "For abstract causal discovery and reasoning, a practical question is how one can identify abstract events from real-world corpora where the ground-truth is unknown. Given the advances of LLMs, a promising approach to use LLMs to generate abstractions. In this experiment, we explore two approaches to automatically extract event abstraction with GPT-40-mini, using Open AI's official API. We then use ACCESS as ground-truth to evaluate the quality of abstraction.\nGenerate abstract events in a Single Step. We have GPT-40-mini directly generate the generalized expressions. We extract 9,495 event mentions from GLUCOSE and ask the model to generate two generalized versions for every instance, corresponding to the levels of generalization (level 1) and abstraction (level 2) described in Figure 1. We then compare the generated abstractions with the ground-true ones provided by GLUCOSE and ACCESS. The model achieves the BLEU score of 0.520. The prompt for this task can be found in Appendix E.\nIdentify abstract events in Two Steps. In the second approach, we obtain the produced generalizations by GPT-40-mini from the above step, then run automatic clustering to find the abstractions, following the setup in Section 3.1. For all instances in every output cluster, we retrieve the ground-true"}, {"title": "4.2 Pairwise Causal Discovery", "content": "We now describe how the data provided in ACCESS can be used for the causal discovery task. In the main text, we discuss the pairwise causal discovery task in LLMs. We examine how well LLMs can discern pairwise causal relations between two abstract events. Formally, given a pair of events x and y, LLMs are asked to determine the relation between them by outputting one of the three possible relations: x causes y, y causes x, or no causal relation. In addition to the 1,494 causal relations in ACCESS, we also randomly generate 1,000 negative pairs to challenge the models. For our experiments, the LLMs"}, {"title": "4.3 Reasoning with Causal Graphs", "content": "We now study how the causal graphs in ACCESS can be used to assist models in QA reasoning tasks. In connection with Section 4.2, this can essentially be viewed as a contextual causal discovery task. We construct a causal QA dataset from GLUCOSE, which provides a set of stories with annotated causal relations between events at both the mention and"}, {"title": "5 Conclusion", "content": "This paper introduces ACCESS, a benchmark for abstract causal event discovery and reasoning. We present a pipeline that combines automatic methods and human crowd-sourcing to extract 1, 494 causal relations among 725 abstract events. We demonstrate that incorporating causal knowledge from our benchmark leads to improvements in QA reasoning tasks for LLMs. However, we also highlight challenges in automatic event abstraction identification and causal discovery, where in the latter, the popular statistical algorithms perform poorly in recovering our sub-graphs of fewer than 50 nodes. Our empirical evidence also suggests that LLMs are not ready to perform causal inference effectively due to the lack of effective acquisition of two critical sub-processes: abstract reasoning and causal discovery. This underscores the need for future research to equip the models with these essential skills for achieving true causal reasoning."}, {"title": "Limitations", "content": "Our benchmark is built upon GLUCOSE (Mostafazadeh et al., 2020) whose scope is limited to everyday children's stories. Acknowledging this limitation, we propose a reproducible data construction pipeline applicable for curating diverse corpora of event causality. Since ACCESS primarily addresses commonsense knowledge in real-world events, it is susceptible to biases regarding the judgement of semantic similarity and cause-and-effect relation of events. To mitigate this issue, our first effort is at every phase, to employ automatic methods alongside with human annotation, based on a set of objective definitions and criteria about events, abstractions and event causality. In the event abstraction phase, we specifically provide the annotators with a list of common scenarios (though non-exhaustive) indicating when the semantics of two expressions are considered similar of different to reduce potential biases. Regarding the subjectivity in human causal judgment, while we focus on non-contextual causal commonsense knowledge, we leverage contextual signals in the original corpus whenever necessary to objectively guide the annotators' decisions"}, {"title": "A Related Work", "content": "Theory of causation. Extensive research into theories of causation spans various disciplines (Dalal et al., 2023) such as philosophy (Beebee et al., 2009), cognitive science (Waldmann, 2017), and probability and statistics (Pearl, 2009). In this paper, we follow the counterfactual theory of causation (Lewis, 2013), which entails three aspects: a relational aspect (involving a cause and an effect components), a temporal aspect (the cause precedes the effect), and a counterfactual aspect (if the causing event had not taken place, the effect would not have occurred).\nCausal discovery in Statistics. The task of causal discovery or structure learning is to recover the causal DAG using available observational or experimental data. It remains a challenging problem in statistics since the search space is super-exponential in the number of variables and the identifiability of the true DAG does not always exist. Causal discovery methods primarily fall into two categories: constraint-based and score-based approaches. Constraint-based methods such as PC (Spirtes and Glymour, 1991) and FCI (Spirtes et al., 2000) extract conditional independencies from the data distribution to detect edge existence and direction. Meanwhile, score-based methods search for model parameters in the DAG space by optimizing a scoring function (Chickering, 2002; Zheng et al., 2018; Yu et al., 2019; Bello et al., 2022).\nCausal discovery in NLP. Ample of work in NLP focuses on event causality identification (ECI), which identifies cause/effect spans from textual descriptions. ECI is commonly treated as a classification task that relies heavily on annotated data for supervised learning (Oh et al., 2013; Hashimoto et al., 2014; Riaz and Girju, 2014; Cheng and Miyao, 2017; Gao et al., 2019), or at least partially for semi-supervised training (Zuo et al., 2021; Shen et al., 2022). Machine learning models trained on mention-level annotations exploit event temporal links (Pustejovsky et al., 2003, 2006) and/or lexical cues or semantics that signal causal information, including, but not limited to, prepositions e.g. because of, by, due to, conjunctions/conjunctive adverbs e.g. because, since, therefore, as a result or verb semantics (Wolff and Song, 2003; Mirza and Tonelli, 2014) such as causation e.g. cause, force. However, as these annotated benchmarks are relatively limited in scale, ECI models are prone to"}, {"title": "B.1 Abstract Event Extraction", "content": "There are five steps in this annotation phase. Steps 1 and 2 are key to extracting abstract events, whereas Steps 3 - 5 serve as post-processing to strengthen consistency among human annotators.\nStep 1: Sub-clustering. Each annotator is presented with a set of clusters generated from an automatic clustering algorithm. Each cluster contains multiple English sentences that describe events in daily life. Each word in every sentence is lemmatized to its base form so that the tense of the sentence does not influence the judgment of meaning. For every cluster, they are required to sub-group event sentences that are semantically similar or related together. There can exist clusters in which all sentences are related to one another; in this case no sub-clustering is needed. There can also be outlier events i.e., sentences that do not belong to any sub-clusters. For a sub-cluster to exist, it must contain at least two events. If an event cannot be sub-clustered, the annotator is to classify it as an outlier. If a sentence is lexically or grammatically erroneous that makes it unjustifiable, the annotator is also asked to highlight and correct it whenever appropriate before clustering.\nTwo event sentences are considered semantically related or similar if they describe the same event, and the decision must not be affected by the information about location and time. We note there is a difference between a state/action actually taking place with the prospect of the state/action taking place. In particular, we outline 11 scenarios where word uses convey differences in meaning.\n1. single participant vs. group of participants e.g., a person be playing in the park \u2260 a person and another person be playing in the park.\n2. affirmation vs. negation e.g., a person be asleep \u2260 a person do not sleep.\n3. present vs. future tense e.g., a person go to sleep \u2260 a person will go to sleep.\n4. ability e.g., a person do not eat \u2260 a person cannot eat.\n5. intention or desire e.g., a person do not eat \u2260 a person do not want to eat.\n6. deduction or possibility e.g., it rain \u2260 it may rain..\n7. obligation, advice or prohibition e.g., a person do not eat \u2260 a person should not eat.\n8. offers, effort or decision e.g., a person help another person \u2260 a person offer to assist another person; a person go to the gym \u2260 a person decide to go to the gym.\n9. location as object. In some cases, the object receives an action from the verb refers to a place or location e.g., a person clean a place. Here room is considered an (spatial) item being taken action on and similar to any other items such as cup or a table \u2192 a person clean a place = a person clean something.\n10. multiple actions. Some sentences describe two actions happening at the same time e.g., a person take something and leave. In order to evaluate its meaning, one must select one of them to the key action. The key action is the action that is described by most of other events in the same cluster. This means that if most of the other events are about someone leaving somewhere, the leave action should be focused instead of take action.\n11. continuous vs. simple tense. Some sentences describe actions in the continuous state e.g., a person be go home. We ignore the continuous state of the action and consider them equivalent to the action described simple tense \u2192 a person be go home = a person go home."}, {"title": "B.2 Causal Relations Discovery", "content": "The annotator is tasked with evaluating candidate pairs of clusters to determine whether a cause-and-effect relationship exists between them, based on their respective topics. Since each cluster's topic represents an event abstraction, and in essence, an event itself, the decision on causal relation hinges on whether the two topics describe causally linked events. Based on the cause-effect definition in Section 2, we provide them with the following criteria to guide their decision about whether an event A causes another event B:"}, {"title": "C Clustering Algorithm", "content": "Our clustering algorithm, named PIVOT, is inspired by the pivoting algorithm proposed in Fukunaga (2019). The PIVOT algorithm first randomly selects a pair of cause-effect events and then, for each of them, find its most similar neighbors against a threshold of 70%. We repeat the process for the remaining event mentions, while excluding the previously assigned events. The initial results are passed to the following process to remove self-loop and bi-directions. We remove clusters with fewer than 10 samples and maximum pairwise similarity is less than 50%. Each cluster can now be considered a node in a graph and we use GLUCOSE to recover the causal relations among them to construct a temporary causal graph.\nAblation study. The main motivation behind PIVOT algorithm is to ensure the initial graph is mostly acyclic while avoiding any sub-optimality produced from post-processing. To validate whether PIVOT is most effective in ensuring causal consistency, we conduct an ablation study against popular clustering algorithms, including OPTICS (Ankerst et al., 1999), LOUVAIN (Blondel et al., 2008) and LEIDEN (Traag et al., 2019) algorithms,"}, {"title": "D Statistical Causal Discovery", "content": "Background. The causal relations among n variables X = [Xi]=1 is characterized via a structural causal model (SCM) (Pearl, 2009) over the tuple (U, X, f) that, in its general form, consists of a sets of assignments\nXi := fi (PAx\u2081, Ui), i = 1,\u2026,n,\nwhere Ui is an exogenous variable assumed to be mutually independent with variables {U1,\u2026\u2026,Un}\\Ui. The functions f = [f1,\u2026, fn] define a joint distribution P(X) over the endogenous variables X, given a joint distribution over exogenous variables P(U1,\u2026, Un). Each SCM induces a causal graph G, which is often assumed to be a DAG. A directed graph G = (V, E) consists of a set of nodes V and an edge set EC V2 of ordered pairs of nodes with (v, v) \u2260 E for any v \u2208 V (one without self-loops).\nFor a pair of nodes i, j with (i, j) \u2208 E, there is an arrow pointing from i to j and we write i \u2192 j. Two nodes i and j are adjacent if either (i, j) \u2208 E or (j, i) \u2208 E. If there is an arrow from i to j then i is a parent of j and j is a child of i. Let PAX\u2081 denote the set of variables associated with parents of node i in G. The graph G of an SCM is obtained by creating one vertex for each X\u2081 and drawing directed edges from each parent Xj \u2208 PAx\u2081 to Xi. We sometimes call the elements of PAX, the direct causes of Xi, and we call Xi a direct effect of each of its direct causes. Importantly, these functions are to be read as assignments rather than as mathematical equations, and they should be viewed as modelling physical mechanisms inducing or generating every Xi from variables PAX\u2081.\nExperiments. We here discuss how ACCESS is used to assess to what extent the statistical structure learning methods is applicable to recover causal relations among event abstractions. As illustrated in Figure 1, after extracting abstractions, one can build representations for abstract events in the original corpus and apply structure learning on top of such data for full graph discovery. A simple representation is the co-occurrence matrix size (#stories \u00d7 #abstractions) where each entry takes a binary value indicating whether an abstraction has any of its mentions appearing in a story. This means each abstraction is now considered as a Bernoulli random variable and the task of causal discovery is to recover the underlying SCM where the structural functions are commonly non-convex.\nDue to the limited scalability of existing statistical algorithms, we resort to learning sub-graphs by setting thresholds to select nodes that appear frequently while ensuring that the true graph is acyclic. Specifically, our selected sub-graphs are composed of edges where both nodes are adjacent to at least one other node, and each node corresponds to an abstraction whose occurrences exceed a certain frequency threshold. In our experiment, we set thresholds for document frequency within {25, 30, 35, 40, 45}, resulting in sub-graphs with 5, 7, 16, 19, 45 nodes. The experiments are run on 5 CPU cores.\nWe experiment with popular constraint-based and score-based algorithms. We select those that are scalable and capable of capturing non-linear causal relationships without relying on specific model forms such as additive noise. In this paper, we report the results for the following algorithms:"}, {"title": "E GLUCOSE-QA Reasoning", "content": "We here provide the prompts for LLMs in Tables 9-12. Tables 13-17 present illustrative examples of the responses from LLMs across our QA tasks."}, {"title": "1", "content": "Syv =\\frac{1}{|V|}\\sum_{x \\in V} S_{xy}"}, {"title": "2", "content": "|\\frac{1}{|C|} \\sum_{i=1}^C \\frac{A_{ii}}{2|Ci|}|"}, {"title": "3", "content": "\\frac{1}{|C|-1}\\sum_{i=1}^{|C|-2}\\sum_{j=i+1}^{|C|}\\frac{\\min(A_{ij},A_{ji})}{\\max(A_{ij}, A_{ji})}"}, {"title": "4", "content": "\\frac{1}{|D|} \\sum_{x \\in D} \\frac{a_x - b_x}{\\max(a_x, b_x)}"}]}