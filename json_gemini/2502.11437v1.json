{"title": "Learning Dexterous Bimanual Catch Skills through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning", "authors": ["Taewoo Kim", "Youngwoo Yoon", "Jaehong Kim"], "abstract": "Robotic catching has traditionally focused on single-handed systems, which are limited in their ability to handle larger or more complex objects. In contrast, bimanual catching offers significant potential for improved dexterity and object handling but introduces new challenges in co-ordination and control. In this paper, we propose a novel framework for learning dexterous bimanual catching skills using Heterogeneous-Agent Reinforcement Learning (HARL). Our approach introduces an adversarial reward scheme, where a throw agent increases the difficulty of throws-adjusting speed-while a catch agent learns to coordinate both hands to catch objects under these evolving conditions. We evaluate the framework in simulated environments using 15 different objects, demonstrating robustness and versatility in handling diverse objects. Our method achieved approximately a 2x increase in catching reward compared to single-agent baselines across 15 diverse objects.", "sections": [{"title": "I. INTRODUCTION & RELATED WORK", "content": "For robots to collaborate effectively with humans and respond swiftly to hazardous situations, such as catching falling objects, the ability to develop precise object-catching skills is crucial. Object catching is inherently challenging, even for humans, as it demands rapid decision-making, precise motor control, and real-time adaptation to dynamic environments. Several attempts have been made to im-bue robots with dexterous skills, including object catching, though these efforts have predominantly focused on single-arm coordination.\nFor instance, DexPBT [1] leveraged Population Based Training [2] to enable robots to learn complex dexterous tasks like throwing and object reorientation. Similarly, Dy-namic Handover [3] simultaneously trained both a throwing robot and a catching robot to perform coordinated object ma-nipulation tasks. Moreover, DexCatch [4] advanced robotic catching skills by incorporating more realistic objects, such as cups and bananas, into the learning process.\nDespite these advancements, a notable limitation across all these works is the reliance on single-arm catching. The potential of bimanual coordination has been largely overlooked, even though it offers clear advantages such as catching capabilities, even under extreme conditions, offering significant advancements in robotic manipulation.\nWe present three key contributions in this paper:\n1) Tackling Bimanual Catching Skill. We explore the relatively under-explored domain of bimanual catching skill learning in robots. This research advances beyond the com-mon one-handed catching approaches in previous work.\n2) Adversarial-Cooperative Learning Framework. We introduce an adversarial-cooperative reward structure within the HARL framework, enabling robots to learn robust bi-manual catching skills under challenging and dynamic condi-tions. By modeling the thrower and catcher as heterogeneous agents with both adversarial and cooperative objectives, we create a learning environment where continuous adaptation leads to mutual improvement.\n3) Validation with Diverse Objects. Through extensive simulation experiments, we demonstrate the effectiveness of our approach across various object sizes and types, highlight-ing the versatility of the proposed bimanual catch framework.\nThe code is available at this github."}, {"title": "II. THROW-CATCH AGENTS FOR LEARNING BIMANUAL CATCH SKILLS", "content": "Our bimanual catcher agent is composed of two UR3 robotic arms [15], each equipped with left and right Allegro hands [16], both securely mounted on the workspace table. The catcher's observation space encompasses proprioceptive data from the 6-DoF arm joints and 16-DoF hand joints, as well as paired pose information for the center of each palm. It also includes 7-dimensional poses for each object (3 for position and 4 for quaternion rotation) as well as 3-dimensional relative vectors between the object and each palm center, for all 15 objects. In total, this results in an observation space of 253 dimensions. The agent's action space comprises joint torque commands for the arms and hands, forming a 44-dimensional space (see Figure 2).\nThe thrower agent, although not physically represented in the environment, is crucial for determining the initial conditions of the thrown objects. In the default policy used for single-agent learning, objects are initialized with random positions and assigned base velocities towards the catcher, both perturbed by uniform noise. However, in our HARL framework, the thrower agent learns to adjust these velocities to challenge the catcher. Specifically, the thrower policy \\(\\pi_{\\theta_{throw}}\\) outputs an action \\(v_{action}\\) that modifies the base velocity:\n\\(v = (v_{base} \\times \\epsilon) + v_{action},\\)\nwhere \\(v = \\{V_x, V_y, V_z, V_{rx}, V_{ry}, V_{rz}\\}\\) represents the final linear and angular velocities, and \\(\\epsilon \\sim U(-0.5,0.5)\\) is uniform noise. The addition of \\(v_{action}\\) allows the thrower to influence the difficulty of the catch task.\nThis design choice ensures stable learning by preventing the thrower from generating erratic or overly challenging throws that could disrupt the catcher's learning. Anchoring the thrower's actions to a reasonable base velocity creates a balanced environment, enabling both agents to improve their policies without causing instability or convergence issues."}, {"title": "III. ADVERSARIAL COOPERATION BETWEEN THROW-CATCH AGENTS", "content": "We model the system as a multi-agent framework using Heterogeneous-Agent Reinforcement Learning (HARL) [18]. Adhering to the Centralized Training with Decentralized Execution (CTDE) paradigm in multi-agent reinforcement learning (MARL) [6], both the thrower and catcher agents optimize the following HAPPO objective function:\n\\(E_{(s,a)\\sim\\pi_{\\theta_k}}\\left[min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A(s,a), clip\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, 1-\\epsilon, 1+\\epsilon\\right) A(s,a) \\right) \\right]\\)\nwhere each agent im optimizes the joint clipped surrogate objective for the multi-agent system, using the ratio of its current policy \\(\\pi_{\\theta}\\) to its previous policy \\(\\pi_{\\theta_k}\\), along with the compound policy ratio \\(M_{\\neg i:m}\\), constrained by the Proximal Policy Optimization (PPO) clipping range \\(1 \\pm \\epsilon\\).\n\\(M_{\\neg i:m}(s, a) = \\frac{\\pi_{\\theta_{k+1}}(a_{\\neg i}|s)}{\\pi_{\\theta}(a_{\\neg i}|s)}\\)\nIn this study, the compound policy ratio \\(M^{1:m}\\) is com-puted jointly for the thrower agent policy \\(\\pi_{\\theta_{thrower}}\\) and the catcher agent policy \\(\\pi_{\\theta_{catcher}}\\) at each training iteration. The ratio is updated based on the relative changes between the previous policy parameters \\(\\theta_k\\) and the updated policy parameters \\(\\theta_{k+1}\\), applied in an arbitrary agent order [18].\nThe thrower seeks to maximize its reward by generating fast throws, while the catcher aims to catch the objects successfully under diverse and extreme conditions. Both agents learns their strategies by optimizing their respective reward functions:\n\\(r_{catch} = w_0 \\bar{r}_{hand_dist} + w_1 r_{goal} + w_2 r_{finger_contact} - w_3 \\bar{r}_{arm_contact} - w_4 \\bar{r}_{catcher_action}\\)\n\\(r_{throw} = w_5 r_{object\\_velocity} + w_6 r_{thrower\\_action}\\)\nwhere the catch reward is a weighted sum of sub-rewards for hand distance, goal proximity, and finger contact, with penalty terms for arm contact and action, represented by bar symbol respectively. \\(\\bar{r}_{hand_dist}\\) rewards the agent for minimizing the distance between the thrown object and the center of the palms, while \\(r_{goal}\\) provides a higher reward as the object approaches the predefined goal position in front of the robot. \\(r_{finger_contact}\\) reward is introduced to promote grasping actions, particularly for smaller objects, by encour-aging finger flexion, granting a reward when the fingertips make contact with objects. A penalty term \\(\\bar{r}_{arm_contact}\\) is designed to minimize collisions between the left and right arms or between the arms and the object, encouraging the agent to avoid arm-object or arm-arm collisions during the catching process. Additionally, an action regularization term \\(\\bar{r}_{catcher\\_action}\\) is included to minimize unnecessary motions and encourage efficient task execution. This term applies a small penalty proportional to the magnitude of the action values, a common reward shaping technique that discour-ages excessive movement while promoting controlled and effective catching. \\(r_{throw}\\) is a combination of \\(r_{object\\_velocity}\\), which rewards higher linear and angular velocities of the thrown object, and \\(r_{thrower\\_action}\\), which provides a reward based on the thrower's actions that induce these velocities. The weights \\(w_0\\) to \\(w_6\\) are set to \\{5.0, 1.0, 0.5, 0.5, 1e-3, 0.8, 0.2\\} respectively, determined through empirical tuning. Please refer to the code for exact implementation details.\nOur framework combines \\(r_{catch}\\) and \\(r_{throw}\\) to use the following final reward:\n\\(r_{total} = \\alpha r_{catch} + (1 - \\alpha) r_{throw},\\)\nwhere \\(\\alpha\\) controls the balance between adversarial (lower \\(\\alpha\\)) and cooperative (higher \\(\\alpha\\)) learning dynamics. The thrower is rewarded for increasing the difficulty of the throw by maximizing the object's speed and unpredictability, while the catcher is rewarded for successfully adapting to these challenging conditions.\nAlthough we do not explicitly define a curriculum for the thrower, the interaction between the thrower and catcher agents naturally would create an implicit adversarial curricu-lum to maximize the total reward. If the thrower throws the object too quickly, the catcher may fail to catch it, resulting in a lower \\(r_{catch}\\) reward. On the other hand, if the thrower throws too easily, the throw reward will decrease. This ad-versarial dynamic effectively simulates a curriculum, where the difficulty is automatically scaled based on the thrower's evolving strategy. The catcher, in turn, develops more refined catching skills as it overcomes these challenges. Thus, the ad-versarial nature of the thrower-catcher relationship fosters an emergent cooperative effect, wherein the thrower indirectly guides the catcher to enhance its capabilities, resulting in a robust and adaptive learning process.\nWe determined \\(\\alpha\\) experimentally, and also tested a strategy of gradually decreasing \\(\\alpha\\) during the learning process, which progressively increases the throwing difficulty. Experimental results are presented in Section IV-C."}, {"title": "IV. EXPERIMENTS", "content": "The experiments were conducted on the IsaacGym sim-ulation platform, with the code base built on the Isaac-GymBenchmark environment [22]. We have implemented the HARL framework on top of rl_games library [23]. The bimanual robot system is positioned one meter above the ground plane alongside a workspace table. In the environ-ment, 15 objects are initialized, from which the thrower randomly selects one to launch toward the catcher. At the beginning of each episode, the catcher resets to its initial position, while the previously thrown object is respawned in a staging area beneath the table. The thrower then selects another object to repeat the process.\nThe experiments were run on a workstation equipped with an AMD Ryzen 9 7950X 16-Core processor, 128GB RAM, and an RTX 6000 Ada GPU. Training for 10,000 epochs required approximately 8 hours with a batch size of 4,096 environments. To ensure diverse interactions, the object selected for throwing was uniformly reset to a random position at the beginning of each episode."}, {"title": "C. Evaluations on Single and Multi-Agent Learning", "content": "We compared the single-agent (SA), with and with-out population-based training (PBT) [2], and the proposed adversarial-cooperative (AC)-HA frameworks for the throw-catch task. The evaluation was based solely on the catcher's reward, using a baseline uniform random object-throwing scenario. In this environment, the object is respawned near the table's edge, offset by 20 cm along the x-axis and 30 cm along the z-axis, with uniform random noise. It is thrown toward the catcher with a base linear velocity of [5.0, 2.0, 4.0] m/s and angular velocity of [10.0, 10.0, 10.0] rad/s, both perturbed by uniform noise \\(\\epsilon \\sim U(-0.5,0.5)\\). Success is determined by whether the object remains above a failure threshold, set at half its predefined size relative to the table height; objects falling below trigger a reset. This criterion ensures consistency across object sizes for objective evaluation. The baseline uniform randomization is used for single-agent learning, while all models are evaluated under more challenging, unknown throwing distributions with noise scalings of \\{1.0, 1.2, 1.5\\} to test robustness.\nHA significantly outperforms the others. Even though we have filtered out many high-reward samples by only showing the 75th percentile range, the AC-HA model still exhibits relatively higher maximum reward episodes. This suggests that the thrower agent, under the proposed adversarial-cooperative reward function, has learned to cooperate ef-fectively during the training process. The HA model used in the evaluation fixes the adversarial-cooperative weighting parameter \\(\\alpha\\) at 0.7.\nTo explore the impact of varying \\(\\alpha\\), we trained an alter-native HA model over 10,000 epochs, where \\(\\alpha\\) was linearly decayed from 1.0 to 0.7 (denoted as HA (decay \\(\\alpha\\) = 0.7) in the figure) under the same conditions. The results showed significantly worse performance than the fixed-\\(alpha\\) model, even underperforming the single-agent model. This suggests that changing \\(\\alpha\\) made it difficult for the thrower to make con-sistent adversarial-cooperative decisions, leading to unstable learning. This instability is evident from lower maximum rewards, particularly in early training when \\(\\alpha\\) = 1.0, where the thrower initially benefited from cooperative behavior but struggled as training shifted toward a more adversarial setting. We interpret this as the thrower agent being unable to adapt effectively as the weighting criterion changed. Ultimately, our experiments demonstrate that dynamically adjusting \\(\\alpha\\) in multi-agent learning introduces instability, as reflected in performance degradation."}, {"title": "D. Ablations", "content": "To compare the performance differences based on the value of \\(\\alpha\\), we evaluated performance across \\(\\alpha\\) values from 1.0 to 0.5.\nThe results demonstrate that \\(\\alpha\\) = 0.5, which induces partial ad-versarial behavior, outperforms the fully cooperative \\(\\alpha\\) = 1.0 in terms of average performance. Furthermore, a moderate level of adversariality, around 20-40%, results in higher performance compared to fully cooperative behavior. This experimentally validates that some degree of adversariality contributes to the improvement of the catcher's skill. On the other hand, excessively increasing adversariality tends to degrade performance. This mechanism is analogous to human learning, where providing an appropriate level of challenge enhances long-term skill acquisition [24], [25].\nTo further investigate how the multi-agent policy with varying \\(\\alpha\\) values performs, especially when the catcher agent encounters more challenging tasks not experienced during training, we introduced higher difficulty environments by applying noise scales of \\{1.0, 1.2, 1.5\\} to the base uniform random object throwing setup. We then compared the per-formance of each policy. The experimental results show that even in the more challenging environments with noise scales of \\{1.2, 1.5\\}, the multi-agent approach consistently outper-forms the single-agent approach (see Figure 8). Notably, the HA framework with \\(\\alpha\\) = 0.7 achieved the best performance overall. Although the performance gap between different \\(\\alpha\\) values decreased as the difficulty increased (e.g., with noise scale=1.5), the multi-agent models maintained overall higher performance than the single-agent model. This suggests that the challenging environments encountered by the catcher during training, due to the thrower's adversarial behavior, were beneficial for its skill development. Our experiments provide empirical evidence that an opposite agent, balancing both adversariality and cooperation, can effectively enhance the learner's performance."}, {"title": "E. Object-Specific Catch Skill", "content": "The variation in object shapes leads to differences in catching strategies for each object. We conducted a per-object evaluation of catch performance and quantitatively analyzed these differences (see Figure 9). Our results indicate that smaller objects, such as cubes and bottles, typically yield higher rewards, whereas larger objects like gymballs, boards, and pots exhibit noticeably lower performance. This disparity is more likely a result of dataset imbalance rather than the inherent difficulty of learning catch skills for larger objects. Notably, when training was restricted to the largest objects-gymball and board-the agents displayed compe-tent catching abilities (see Video). This implies that catch strategies are highly dependent on the object's size and shape. Addressing the challenges posed by dataset imbalance and strategy variation across different objects will be a subject of future research."}, {"title": "V. CONCLUSION & LIMITATIONS", "content": "In this paper, we presented an adversarial cooperation sys-tem for learning bimanual catch skills using Heterogeneous-Agent Reinforcement Learning (HARL). Our approach in-troduces an adversarial-cooperative reward structure that enhances dexterous catching under dynamic conditions. Sim-ulations showed significant improvements in bimanual coor-dination and adaptability over single-agent methods.\nA key advantage of our method is that the thrower agent generates an adaptive curriculum through adversarial-cooperative interactions. Unlike handcrafted curricula, our approach dynamically adjusts challenge levels based on the catcher's evolving skill, ensuring continuous learning efficiency and robust generalization. Results confirm that the learned curriculum optimizes throwing difficulty, leading to superior catching performance over static policies.\nDespite these advancements, limitations remain. All exper-iments were conducted in simulation, requiring adaptation for real-world deployment. The object set, though diverse, was limited to 15 items, and future work should assess broader generalization. Additionally, our framework assumes stationary agents, which may limit performance in complex scenarios. While our study focused on a throwing and catching task, the underlying adversarial-cooperative learning framework could be extended to other domains, such as collaborative assembly, dexterous manipulation, or multi-agent coordination. Exploring such generalizations remains an exciting research direction.\nFuture work will address these limitations by deploying the framework on real robots and extending it to dynamic en-vironments, including mobile manipulators and humanoids, broadening its impact on robotic learning and interaction."}]}