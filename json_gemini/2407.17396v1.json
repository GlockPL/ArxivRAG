{"title": "Systematic Reasoning About Relational Domains With Graph Neural Networks", "authors": ["Irtaza Khalid", "Steven Schockaert"], "abstract": "Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work on reasoning with GNNs has shown that such models tend to fail when presented with test examples that require longer inference chains than those seen during training. This suggests that GNNs lack the ability to generalize from training examples in a systematic way, which would fundamentally limit their reasoning abilities. A common solution is to instead rely on neuro-symbolic methods, which are capable of reasoning in a systematic way by design. Unfortunately, the scalability of such methods is often limited and they tend to rely on overly strong assumptions, e.g. that queries can be answered by inspecting a single relational path. In this paper, we revisit the idea of reasoning with GNNs, showing that systematic generalization is possible as long as the right inductive bias is provided. In particular, we argue that node embeddings should be treated as epistemic states and that GNN should be parameterised accordingly. We propose a simple GNN architecture which is based on this view and show that it is capable of achieving state-of-the-art results. We furthermore introduce a benchmark which requires models to aggregate evidence from multiple relational paths. We show that existing neuro-symbolic approaches fail on this benchmark, whereas our considered GNN model learns to reason accurately.", "sections": [{"title": "1 Introduction", "content": "Learning to reason remains a key challenge for neural networks. When standard neural network architectures are trained on reasoning problems, they often perform well on similar problems, but fail to generalize to problems with different characteristics than the ones that were seen during training, e.g. problems that were sampled from a different distribution [43] or require longer inference chains [33]. This behaviour has been observed for different types of reasoning problems and different types of architectures, including Language Models [43, 39] and Graph Neural Networks [33]. A common solution is to rely on some kind of neuro-symbolic approach. For instance, Neural Theorem Provers (NTPs [29, 25]) simulate the backward chaining process of traditional logic programs, using a soft unification mechanism to make this process differentiable. Unfortunately, such approaches are highly inefficient. They also typically rely on strong assumptions about the required reasoning process. Irrespective of any specific implementation issues, there is a fundamental question that has largely remained unanswered: what is it about neuro-symbolic methods that makes them more successful for reasoning tasks? In this paper, we focus on the common problem of reasoning about binary relations. In such cases, the available knowledge can be represented as a labelled multi-graph (i.e. a knowledge graph) and the main reasoning task of interest is to infer whether a given relationship holds or not. Graph Neural Networks (GNNs) intuitively seem well-suited for this task, but in practice they"}, {"title": "2 Related work", "content": "There has been a large body of work on learning to reason with neural networks, ranging from traditional approaches based on carefully designed feedforward [35] or recurrent [12] models, to"}, {"title": "3 Learning to reason in relational domains", "content": "We focus on the problem of reasoning about binary relations. We assume that a set F of facts is given, referring to a set of relations R and a set of entities E. Each of these facts is an atom of the form r(a, b), with r\u2208 R and a, b \u2208 E. We furthermore assume that there exists a set of rules K which can be used to infer relationships between the entities in E. We write KUF |= r(a, b) to denote that r(a, b) can be inferred from the facts in F and the rules in K. The problem that we are interested in is to develop a neural network model fe which can predict for a given assertion r(a, b) whether KUF |= r(a, b) holds or not. Note that the set of rules K is not given. We instead have access to a number of fact sets Fi, together with examples of atoms r(a, b) which can be inferred from these fact graphs and atoms which cannot. To be successful, fe must essentially learn the rules from K, and the considered model must be capable of applying the learned rules in a systematic way to new problems."}, {"title": "3.1 Learning to reason about simple path rules", "content": "The most commonly studied setting concerns rules of the following form (n \u2265 3):\n\nr(X_1, X_n) \\leftarrow r_1(X_1, X_2) \\land ... \\land r_{n-1}(X_{n-1}, X_n)\n\nWe will refer to such rules as simple path rules. Note that we used the convention from logic programming to write the head of the rule on the left-hand side, and we use uppercase symbols such as Xi to denote variables. We can naturally associate a labelled multi-graph GF with the given set"}, {"title": "3.2 Learning to reason about disjunctive rules", "content": "The rules that we have considered thus far uniquely determine the relationship between two entities a and c, given knowledge about how a relates to b and b relates to c. In many settings, however, such knowledge might not be sufficient for completely characterising the relationship between a and c. Domain knowledge might then be expressed using disjunctive rules of the following form:\n\ns_1(X, Z) \\lor ... \\lor s_k(X, Z) \\leftarrow r_1(X, Y) \\land r_2(Y, Z)\n\nIn other words, if we know that r\u2081(a, b) and r2(b, c) hold for some entities a, b, c, then all we can infer is that one of the relations s1, ..., sk must hold between a and c. We then typically also have constraints of the form \u22a5 \u2190 r1(X, Y) \u2227r2(X, Y), encoding that r\u2081 and r2 are disjoint. Many of the calculi that have been developed for temporal and spatial reasoning fall under this setting [1, 27]."}, {"title": "4 A GNN for systematic reasoning", "content": "Section 3 highlighted some challenges with designing GNNs that can learn to reason. We now present a simple GNN model that aims to overcome these challenges. Specifically, we want a model which (i) is more efficient than current neuro-symbolic methods while matching their performance on reasoning with simple path rules and (ii) is also able to reason about disjunctive rules.\n\nWe start from the principle that reasoning is fundamentally about manipulating epistemic states (i.e. states of knowledge) and the GNN should reflect this, i.e. there should be a clear correspondence between the node embeddings that are learned by the model and what we can infer about the relationships that may hold between the entities of interest. Inspired by NBFNet [46], we use a network which learns the relationships between one designated head entity h and all other entities. Let us write $e^{(1)} \\in \\mathbb{R}^n$ for the embedding of entity e in layer l of the network. This embedding should intuitively reflect what we have derived about which relationships may possibly hold between e and the designated entity h. We think of $e^{(l)}$ as encoding a probability distribution over possible"}, {"title": "5 Experiments", "content": "We use the challenging problem of inductive relational reasoning to evaluate our proposed model against GNN, transformer and neuro-symbolic baselines. We refer to our full (forward-backward) model as FB. We consider two variants of this model, which differ in the choice of the pooling operator: component-wise multiplication (FB-mul) and min-pooling (FB-min). The considered benchmarks involve relation classification queries of the form (h, ?, t), asking which relation holds between a given head entity h and tail entity t. We focus in particular on systematic generalisation, to assess whether models can deal with large distributional shifts from the training set, which is paramount in many real-world settings [18]. We consider two existing benchmarks designed to test systematic generalisation for relational reasoning: CLUTRR [33] and GraphLog [34]. We also introduce a new benchmark based on RCC-8 relations, which goes further than the existing benchmarks on two fronts, as illustrated in Example 1: (i) the need to go beyond Horn rules to capture relational compositions and (ii) requiring models to aggregate information multiple relational paths. For CLUTRR and RCC-8, to test for systematic generalisation, models are trained on small graphs and subsequently evaluated on larger graphs. In particular, for CLUTRR, the length k of the considered relational paths is varied, while for RCC-8 we vary both the number of relational paths b and their length k. In the case of GraphLog, the size of training and test graphs is similar, but models still need to apply learned rules in novel ways to perform well.\n\nMain results Results for CLUTRR, RCC-8 and GraphLog are shown in Tables 1, 2 and 3, respectively. We report the average accuracy and 2\u03c3 errors across 10 seeds for CLUTRR and 3 seeds for RCC-8 and GraphLog. For CLUTRR, both variants of our model outperform all GNN and RNN methods, as well as edge transformers (ET). The FB-mul model is also on par with the SoTA neuro-"}, {"title": "6 Conclusions", "content": "We have challenged the view that Graph Neural Networks are not capable of systematic generalisation in relational reasoning problems, by introducing a principled GNN architecture for this setting. To impose an appropriate inductive bias, node embeddings in our framework are treated as epistemic states, intuitively capturing sets of possible relationships. These epistemic states are iteratively refined by the GNN. In this way, the design of the GNN is closely aligned with the algebraic closure"}, {"title": "A Supplemental material", "content": "A1. Semantics of entailment\n\nFor simple path rules of the form (2), the semantics of entailment can be defined in terms of the immediate consequence operator. Let RF be the grounding of R w.r.t. F, i.e. for each rule of the form r(X, Z) \u2190 r1(X,Y) \u2227 r2(Y, Z) in R, RF contains all the possible rules of the form r(a, c) \u2190 r1(a, b) \u2227 r2(b, c) that can be obtained by substituting the variables for entities that appear in F. Since we assume that both F and R are finite, we have that RF is finite as well. Let F0 = F and for i\u2265 1 define:\n\n\nSince there are finitely many atoms r(a, c) that can be constructed using the entities and relations appearing in RF, this process reaches a fixpoint after a finite number of steps, i.e. for some l \u2208 N we have Fl = Fl+1. Let us write F \u2217 for this fixpoint. Then we define F UR |= r(a, b) iff r(a, b) \u2208 F \u2217 .\n\n\nIf F = {father(bob, alice), mother(alice, eve)} then RF contains rules such as:\n\ngrandfather(bob, eve) \u2190 father(bob, alice) \u2227 mother(alice, eve)\ngrandfather(bob, alice) \u2190 father(bob, alice) \u2227 mother(alice, alice)\ngrandfather(bob, bob) \u2190 father(bob, alice) \u2227 mother(alice, bob)\ngrandfather(bob, eve) \u2190 father(bob, eve) \u2227 mother(eve, eve)\n\nWe have F1 = {father(bob, alice), mother(alice, eve), grandfather(bob, eve)}, with F1 = F2 = F \u2217. We thus find: KUF |= grandfather(bob, eve).\n\n\n For the more general setting with disjunctive rules and constraints, we can define the semantics of entailment in terms of Herbrand models. Given a set of disjunctive rules and constraints R and a set of facts F, the Herbrand universe URF is the set of all atoms of the form r(a, b) which can be constructed from a relation r and entities a, b appearing in R U F . A Herbrand interpretation \u03c9 is a subset of UR,F . The interpretation \u03c9 satisfies a ground disjunctive rule of the form s1(a, c) V ... V sk(a, c) \u2190 r1(a,b) \u2227 r2(b, c) iff either {r1(a, b), r2(b, c)} \u2286 \u03c9 or \u03c9 \u2229 {s1(a, c), ...,sk(a,c)} \u2260 \u2205. The interpretation \u03c9 satisfies a ground constraint of the form \u22a5 \u2190 r1(a, b) \u2227r2(a, b) iff {r1(a, b), r2(a, b)} \u2286 \u03c9. We say that \u03c9 is a model of the grounding RF iff \u03c9 satisfies all the ground rules and constraints in RF . Finally, we have that RUF |= r(a, b) iff r(a, b) is contained in every model of RF.\n\n\nA.2 Proof of Proposition 1\n\nThe correctness of Proposition 1 immediately follows from Lemmas 1 and 2 below.\n\nLemma 1. Suppose there exists a relational path r1; ...; rk connecting a and b in GF such that r can be derived from r1; ...; rk. Then it holds that KUF |= r(a, b).\n\nProof. Let r1; ...; rk be a relational path connecting a and b, such that r can be derived from r1; ...; rk. Then F contains facts of the form r1(a, x1), r2(x1, x2), ..., rk(xk\u22121, b). Let us now consider the derivation from r1; ...; rk to r. After the first derivation step, we have a relational path of the form r1; ...; ri\u22122; s; ri+1; ...; rk. In this case, K contains a rule of the form s(X, Z) \u2190 ri\u22121(X, Y) \u2227 ri(Y, Z). This means that KUF |= s(xi\u22122, xi). We thus have a relational path of the form r1; ...; ri\u22122; s; ri+1; ...; rk, where each relation is associated with an atom that is entailed by KUF. Each derivation step introduces such an atom (while reducing the length of the relational path by 1). After k - 1 steps, we thus obtain the atom r(a, b), from which it follows that KUF |= r(a, b).\n\nLemma 2. Suppose that KUF |= r(a, b). Then it holds that there exists a relational path r1; ...; rk connecting a and b in GF such that r can be derived from r1; ...; rk."}, {"title": "A.3 Reasoning about disjunctive rules using algebraic closure", "content": "We consider knowledge bases with three types of rules. First, we have disjunctive rules that encode relational compositions:\n\ns_1(X, Z) \\lor ... \\lor s_k(X, Z) \\leftarrow r_1(X, Y) \\land r_2(Y, Z)\n\nSecond, we have constraints of the following form:\n\n\\bot \\leftarrow r_1(X, Y) \\land r_2(X, Y)\n\nmeaning that r1 and r2 are disjoint. Finally, we consider knowledge about inverse relations, expressed using rules of the following form:\n\nr_2(Y, X) \\leftarrow r_1(X, Y)\n\nWe now describe the algebraic closure algorithm, which can be used to decide entailment for calculi such as RCC-8. We also describe an approximation of this algorithm, which we call directional algebraic closure. This approximation closely corresponds to how GNN models can reason in this setting, as will be made clear below.\n\nFull algebraic closure Let us make the following assumptions:\n\n\u2022 The knowledge base K contains rules of the form (9), encoding the composition of relations r1 and r2.\n\n\n\u2022 We also have that K contains the rule Vr \u2208 R r(X, Y) \u2190 T , expressing that the set of relations is exhaustive.\n\n\u2022 For all distinct relations r1, r2 \u2208 R, r1 \u0338= r2, K contains a constraint of the form (10), expressing that the relations are pairwise disjoint.\n\n\n\u2022 For every r \u2208 R there is some relation r\u02c6 \u2208 R, such that K contains the rules r\u02c6(Y, X) \u2190 r(X, Y) and r\u02c6(Y, X) \u2190 r(X, Y), expressing that r\u02c6 is the inverse of r.\n\n\u2022 K contains no other rules.\n\nLet us write r1 o r2 for the set of relations that appears in the head of the rule defining the composition of r1 and r2 in K. If no such rule exists in K for r1 and r2 then we define r1 o r2 = R. Let E be the set of entities that appear in F. Let us assume that F is consistent with K, i.e. K UF \u0338|= \u22a5.\n\nThe main idea of the algebraic closure algorithm is that we iteratively refine our knowledge of what relationships are possible between different entities. Specifically, for all entities e, f \u2208 E, we define the initial set of possible relationships as follows:\n\n\nwhere r\u02c6 is the unique relation that is asserted to be the inverse of r in K. Note that because we assumed F to be consistent, if r(e, f) \u2208 F and r\u2032(f, e) \u2208 F it must be the case that r\u2032 = r\u02c6. We now iteratively refine the sets Xef. Specifically, for i \u2265 1, we define:"}, {"title": "A.4 Expressivity", "content": "We now show that the base GNN model, corresponding to (15), is capable of simulating the directional algebraic closure algorithm. Specifically, we show the following result, which associates the embeddings e (2) from the GNN with the sets X (2) . We show the following result for the min-pooling operator \u03c8 min defined as follows:\n\n\\psi_{min} (X_1, ..., X_k) = \\frac{min(x_1,..., x_k)}{||min(x_1,..., x_k)||_1}\n\n\nProposition 3. Let F be a set of facts and K a knowledge base satisfying the conditions of Proposition 2. Furthermore assume that KUF is consistent, i.e. KUF \u0338= \u22a5. Let X (l) be sets of relations that are constructed using the directional algebraic closure algorithm, and let e l denote the jth coordinate of el . Let the pooling operation be chosen as \u03c8 = \u03c8min. There exists a parameterisation of the vectors ri and aij such that:\n\n X^{(i)} = \\{r_j | e_j^{(i+1)} > 0, 2 \\leq j \\leq n\\} \\supseteq X^{(i+1)}\n\nwhere we define ido rj = rj for every j \u2208 {1, ..., n}. Note that aij indeed satisfies the requirements of the model: the coordinates of aij are non-negative and sum to 1, while a1j = one-hot(j) follows from the fact that ido rj = rj . Furthermore, we define rj = one-hot(j).\n\n(0) = {rl}. We show that el(1) is non-zero in the lth coordinate and zero everywhere else. We have:\n\n\\phi ((h^{(0)}, r_i) = h_i a_{ij} = a_{ij} = r_i = one-hot(i)\n\nWe need to show that the lth coordinate of the latter vector is non-zero. Since KUF is consistent and F contains both rl(h, e) and rp(f, e), it has to be the case there there is some q \u2208 {1, ..., n} such that rl \u2208 rqorp. There thus exists some q \u2208 {1, ..., n} such that the lth coordinate of alq is non-zero. Since all vectors of the aip vectors have non-negative coordinates, it follows that the lth coordinate of l aip is non-zero. We have thus shown that {rj | el(1) > 0, 2 \u2264 j \u2264 n} = {r1} \u2287 X (1) . If F does not contain any facts of the form ri(h, e), then X (0) = R. We need to show for each l \u2208 {2, ..., n} that either el(1) is non-zero or rl \u2209 X. We have that el(1) is non-zero if the lth component of ((f (0) , rp) is non-zero for every fact of the form rp(f, e) in F, where f \u0338= h. Consider such a fact rp(f, e) and assume the lth component of ((f (0) , rp) is actually 0. Since f \u0338= h we have \n f (0) = . The lth component of ((f (0) , rp) = i alip can thus only be 0 if the lth component of aip is 0 for every i. This implies that ri \u0338\u2208 riorp for any i \u2208 {1, ..., n}, from which it follows that rl \u0338\u2208 X (1) . (i)  {rj | e  (i) = X (2) holds. We show that (13) must then hold as well. We first show X = {rj | ej(l+1) > 0, 2 \u2264 j \u2264 n}. To this end, we need to show that for each rj \u2208 X (i) . is 0. If rj \u2208 X (i) \\X (2) it means that there is some rp(f, e) \u2208 F such that rj \u0338\u2208 X (i) rp . This is the case if rj \u0338\u2208 ri o rp (i)"}, {"title": "A.5 RCC-8 Dataset", "content": "RCC-8 [27] uses eight primitive relations to describe qualitative spatial relationships between regions: ntpp(a, b) means that a is a proper part of the interior of b, tpp(a, b) means that a is a proper part of b and shares a boundary point with b, po(a, b) means that a and b are overlapping (but neither is included in the other), dc(a, b) means that a and b are disjoint, ec(a, b) means that a and b are adjacent (i.e. sharing a boundary point but no interior points), eq(a, b) means that a and b are equal, and ntppi and tppi are the inverses of ntpp and tpp. The RCC-8 semantics is governed by the so-called composition table, which describes the composition mapping between two relations."}, {"title": "A.5.1 Dataset Generation", "content": "We now explain how the dataset was created. All sampling in the discussion below is uniform random. Each problem instance has to be constructed such that after aggregating the information provided by all the relational paths, we need to be able to infer a singleton label. In other words, problem"}, {"title": "A.5.2 Ensuring path consistency within the dataset", "content": "We ensure that all relational paths in a problem instance in the generated dataset do not informationally conflict with each other by using the DPC+ algorithm [21]. It efficiently computes directional path consistency, i.e. Xij \u2286 Xik o Xkj\u2200i, j \u2264 k, for qualitative constraint networks that we can transform our graph instances to. Directional path consistency is sufficient as a test for global path consistency for networks with singleton edge labels [20]."}, {"title": "A.6 Allen's interval algebra dataset", "content": "To complement the experiments in the main paper, we also introduce a dataset based on Allen's interval algebra [2]. Similar to RCC-8, the interval algebra uses 13 primitive relations to describe qualitative temporal relationships. The interval algebra captures all possible relationships between two time intervals. Similar to the RCC-8 relations, these are binary relations defined as follows: <(a, b) means that the time interval a precedes the time interval b; d(a, b) means that a occurs during b; o(a, b) means that a overlaps with b; m(a, b) means that a meets b (a ends exactly before b starts); s(a, b) means that a starts b (a and b share a starting time); f(a, b) means that a finishes b (a and b share a finishing time); =(a, b) means that a equals b (the starting and finishing times for both are the same implying the intervals are the same); and finally >, di, oi, mi, si, fi are the inverses of the respective operations defined previously. The composition table for all the primitive interval relations"}, {"title": "A.7 Additional experimental details", "content": "We now present further details of the experimental set-up, including details of the loss function that was used for training the model, the considered benchmarks and baselines, and training details such as hyperparameter optimisation."}, {"title": "A.7.1 Loss function", "content": "Let us write ti = (ti,1, ..., ti,n) for the final-layer embedding of entity ti in the graph Fi, and let r = (r1, ...,rn) denote the embedding of relation r. Let us write:\n\n\nFor each training example i, we write ti for the corresponding tail embedding and we let ri \u2208 R denote the correct label. Then clearly, we want CE(ti, ri) to be low, while for each negative example r\u2032 \u2208 R \\ {ri} we want CE(ti, r\u2032) to be high. In the base model, we implement this with a margin loss, where for each i we let r \u2208 R \\ {ri} be a corresponding negative example:"}, {"title": "A.7.2 Benchmarks", "content": "CLUTRR2 [33] is a dataset which involves reasoning about family relationships. The original version of the dataset involved narratives describing the fact graph in natural language. It was, among others, aimed at testing the ability of language models such as BERT [13] to solve such reasoning tasks. However, the original paper also considered a number of baselines which were given access to the fact graph itself, especially GNNs and sequence classification models. A crucial finding was that such models fail to learn to reason in a systematic way: models trained on short inference chains perform poorly when tested on examples involving longer inference chains. This has inspired a line of work which has introduced a number of neuro-symbolic methods for addressing this issue. The CLUTRR dataset was released under a CC-BY-NC 4.0 license.\n\nGraphLog3 [34] involves examples for 57 different worlds, where each world is characterised by a set of logical rules. For each world, a number of corresponding knowledge graphs are provided, which the model can use to learn the underlying rules. The model is then tested on previously unseen knowledge graphs for the same world. The aim of this benchmark is to test the ability of models to systematically generalise from the reasoning patterns that have been observed during training, i.e. to apply the rules that have been learned from the training data in novel ways. This dataset is released under a CC-BY-NC 4.0 license.\n\nRCC-8 is the benchmark that we introduce in this paper, as described in Section A.5. We release this benchmark under a CC-BY 4.0 license.\n\nDataset statistics for CLUTRR, GraphLog and RCC-8 are reported in tables 7, 9, 8. We use a standard 80-20 split for training and validation for CLUTRR and RCC-8. For GraphLog, we use the validation set that is provided separately from the test set."}, {"title": "A.7.3 Baselines", "content": "We compare our method against the following neuro-symbolic methods:\n\nCTP Conditional Theorem Provers [25] are a more efficient version of Neural Theorem Provers (NTPs [29]). Like NTPs, they learn a differentiable logic program, but rather than ex- haustively considering all derivations, at each step of a proof, CTPs learn a filter function that selects the most promising rules to apply, thereby speeding up backwards-chaining procedure of NTP. Three variants of this model were proposed, which differ in how this"}]}