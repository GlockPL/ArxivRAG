{"title": "Systematic Reasoning About Relational Domains With Graph Neural Networks", "authors": ["Irtaza Khalid", "Steven Schockaert"], "abstract": "Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work on reasoning with GNNs has shown that such models tend to fail when presented with test examples that require longer inference chains than those seen during training. This suggests that GNNs lack the ability to generalize from training examples in a systematic way, which would fundamentally limit their reasoning abilities. A common solution is to instead rely on neuro-symbolic methods, which are capable of reasoning in a systematic way by design. Unfortunately, the scalability of such methods is often limited and they tend to rely on overly strong assumptions, e.g. that queries can be answered by inspecting a single relational path. In this paper, we revisit the idea of reasoning with GNNs, showing that systematic generalization is possible as long as the right inductive bias is provided. In particular, we argue that node embeddings should be treated as epistemic states and that GNN should be parameterised accordingly. We propose a simple GNN architecture which is based on this view and show that it is capable of achieving state-of-the-art results. We furthermore introduce a benchmark which requires models to aggregate evidence from multiple relational paths. We show that existing neuro-symbolic approaches fail on this benchmark, whereas our considered GNN model learns to reason accurately.", "sections": [{"title": "1 Introduction", "content": "Learning to reason remains a key challenge for neural networks. When standard neural network architectures are trained on reasoning problems, they often perform well on similar problems, but fail to generalize to problems with different characteristics than the ones that were seen during training, e.g. problems that were sampled from a different distribution [43] or require longer inference chains [33]. This behaviour has been observed for different types of reasoning problems and different types of architectures, including Language Models [43, 39] and Graph Neural Networks [33]. A common solution is to rely on some kind of neuro-symbolic approach. For instance, Neural Theorem Provers (NTPs [29, 25]) simulate the backward chaining process of traditional logic programs, using a soft unification mechanism to make this process differentiable. Unfortunately, such approaches are highly inefficient. They also typically rely on strong assumptions about the required reasoning process. Irrespective of any specific implementation issues, there is a fundamental question that has largely remained unanswered: what is it about neuro-symbolic methods that makes them more successful for reasoning tasks? In this paper, we focus on the common problem of reasoning about binary relations. In such cases, the available knowledge can be represented as a labelled multi-graph (i.e. a knowledge graph) and the main reasoning task of interest is to infer whether a given relationship holds or not. Graph Neural Networks (GNNs) intuitively seem well-suited for this task, but in practice they"}, {"title": "2 Related work", "content": "There has been a large body of work on learning to reason with neural networks, ranging from traditional approaches based on carefully designed feedforward [35] or recurrent [12] models, to"}, {"title": "3 Learning to reason in relational domains", "content": "We focus on the problem of reasoning about binary relations. We assume that a set F of facts is given, referring to a set of relations R and a set of entities E. Each of these facts is an atom of the form r(a, b), with r\u2208 R and a, b \u2208 E. We furthermore assume that there exists a set of rules K which can be used to infer relationships between the entities in E. We write KUF = r(a, b) to denote that r(a, b) can be inferred from the facts in F and the rules in K. The problem that we are interested in is to develop a neural network model fe which can predict for a given assertion r(a, b) whether KUF = r(a, b) holds or not. Note that the set of rules K is not given. We instead have access to a number of fact sets Fi, together with examples of atoms r(a, b) which can be inferred from these fact graphs and atoms which cannot. To be successful, fe must essentially learn the rules from K, and the considered model must be capable of applying the learned rules in a systematic way to new problems."}, {"title": "3.1 Learning to reason about simple path rules", "content": "The most commonly studied setting concerns rules of the following form (n \u2265 3):\n$$r(X_1, X_n) \\leftarrow r_1(X_1, X_2) \\land . . . \\land r_{n-1}(X_{n-1}, X_n)$$\nWe will refer to such rules as simple path rules. Note that we used the convention from logic programming to write the head of the rule on the left-hand side, and we use uppercase symbols such as Xi to denote variables. We can naturally associate a labelled multi-graph GF with the given set"}, {"title": "3.2 Learning to reason about disjunctive rules", "content": "The rules that we have considered thus far uniquely determine the relationship between two entities a and c, given knowledge about how a relates to b and b relates to c. In many settings, however, such knowledge might not be sufficient for completely characterising the relationship between a and c. Domain knowledge might then be expressed using disjunctive rules of the following form:\n$$s_1(X, Z) \\lor... \\lor s_k(X, Z) \\leftarrow r_1(X, Y) \\land r_2(Y, Z)$$\nIn other words, if we know that r\u2081(a, b) and r2(b, c) hold for some entities a, b, c, then all we can infer is that one of the relations 81, ..., Sk must hold between a and c. We then typically also have constraints of the form | \u2190 r\u2081(X, Y) \u2227r2(X, Y), encoding that r\u2081 and r2 are disjoint. Many of the calculi that have been developed for temporal and spatial reasoning fall under this setting [1, 27]."}, {"title": "4 A GNN for systematic reasoning", "content": "Section 3 highlighted some challenges with designing GNNs that can learn to reason. We now present a simple GNN model that aims to overcome these challenges. Specifically, we want a model which (i) is more efficient than current neuro-symbolic methods while matching their performance on reasoning with simple path rules and (ii) is also able to reason about disjunctive rules.\nWe start from the principle that reasoning is fundamentally about manipulating epistemic states (i.e. states of knowledge) and the GNN should reflect this, i.e. there should be a clear correspondence between the node embeddings that are learned by the model and what we can infer about the relationships that may hold between the entities of interest. Inspired by NBFNet [46], we use a network which learns the relationships between one designated head entity h and all other entities. Let us write $e^{(1)} \\in R^n$ for the embedding of entity e in layer l of the network. This embedding should intuitively reflect what we have derived about which relationships may possibly hold between e and the designated entity h. We think of $e^{(l)}$ as encoding a probability distribution over possible"}, {"title": "5 Experiments", "content": "We use the challenging problem of inductive relational reasoning to evaluate our proposed model against GNN, transformer and neuro-symbolic baselines. We refer to our full (forward-backward) model as FB. We consider two variants of this model, which differ in the choice of the pooling operator: component-wise multiplication (FB-mul) and min-pooling (FB-min). The considered benchmarks involve relation classification queries of the form (h, ?, t), asking which relation holds between a given head entity h and tail entity t. We focus in particular on systematic generalisation, to assess whether models can deal with large distributional shifts from the training set, which is paramount in many real-world settings [18]. We consider two existing benchmarks designed to test systematic generalisation for relational reasoning: CLUTRR [33] and GraphLog [34]. We"}, {"title": "6 Conclusions", "content": "We have challenged the view that Graph Neural Networks are not capable of systematic generalisation in relational reasoning problems, by introducing a principled GNN architecture for this setting. To impose an appropriate inductive bias, node embeddings in our framework are treated as epistemic states, intuitively capturing sets of possible relationships. These epistemic states are iteratively refined by the GNN. In this way, the design of the GNN is closely aligned with the algebraic closure"}, {"title": "A Supplemental material", "content": null}, {"title": "A.1 Semantics of entailment", "content": "For simple path rules of the form (2), the semantics of entailment can be defined in terms of the immediate consequence operator. Let RF be the grounding of R w.r.t. F, i.e. for each rule of the form r(X, Z) \u2190 r\u2081(X,Y) \u2227 r2(Y, Z) in R, RF contains all the possible rules of the form r(a, c) \u2190 r\u2081(a, b) \u2227 r2(b, c) that can be obtained by substituting the variables for entities that appear in F. Since we assume that both F and R are finite, we have that RF is finite as well. Let Fo = F and for i\u2265 1 define:\n$$F_i=F_{i-1} \\cup \\{r(a, c) | \\exists b \\in E . r_1(a, b), r_2(b, c) \\in F_{i-1} \\text{ and } (r(a, c) \\leftarrow r_1(a, b) \\land r_2(b, c)) \\in R_F\\}$$\nSince there are finitely many atoms r(a, c) that can be constructed using the entities and relations appearing in RF, this process reaches a fixpoint after a finite number of steps, i.e. for some l \u2208 N we have Fe = Fe+1. Let us write F* for this fixpoint. Then we define FUR |= r(a, b) iff r(a, b) \u2208 F*.\nExample 2. The canonical example for this setting concerns reasoning about family relationships. In this case, K contains rules such as:\ngrandfather(X, Z) \u2190 father(X,Y) \u2227 mother(Y, Z)\nIf F = {father(bob, alice), mother(alice, eve)} then RF contains rules such as:\ngrandfather(bob, eve) \u2190 father(bob, alice) \u2227 mother(alice, eve)\ngrandfather(bob, alice) \u2190 father(bob, alice) \u2227 mother(alice, alice)\ngrandfather(bob, bob) \u2190 father(bob, alice) \u2227 mother(alice, bob)\ngrandfather(bob, eve) \u2190 father(bob, eve) \u2227 mother(eve, eve)\nWe have F\u2081 = {father(bob, alice), mother(alice, eve), grandfather(bob, eve)}, with F1 = F2 = F*.\nWe thus find: KUF = grandfather(bob, eve).\nFor the more general setting with disjunctive rules and constraints, we can define the semantics of entailment in terms of Herbrand models. Given a set of disjunctive rules and constraints R and a set of facts F, the Herbrand universe URF is the set of all atoms of the form r(a, b) which can be constructed from a relation r and entities a, b appearing in RUF. A Herbrand interpretation wis a subset of UR,F. The interpretation w satisfies a ground disjunctive rule of the form s\u2081(a, c) V ... V sk(a, c) \u2190 r\u2081(a,b) \u2227 r2(b, c) iff either {r\u2081(a, b), r2(b, c)} \u00a3 wor \u03c9 \u2229 {81 (a, c), ...,sk(a,c)} \u2260 0. The interpretation w satisfies a ground constraint of the form \u22a5 \u2190 r\u2081(a, b) \u2227r2(a, b) iff {r1(a, b), r2(a, b)} \u00a3 w. We say that w is a model of the grounding RF iff w satisfies all the ground rules and constraints in RF. Finally, we have that RUF = r(a, b) iff r(a, b) is contained in every model of RF."}, {"title": "A.2 Proof of Proposition 1", "content": "The correctness of Proposition 1 immediately follows from Lemmas 1 and 2 below.\nLemma 1. Suppose there exists a relational path r\u2081; ...; rk connecting a and b in G F such that r can be derived from r\u2081; ...; rk. Then it holds that KUF = r(a, b).\nProof. Let r1; ...; rk be a relational path connecting a and b, such that r can be derived from r\u2081; ...; rk. Then F contains facts of the form r\u2081(a, x1), r2(X1,X2), ..., rk(xk\u22121,b). Let us now consider the derivation from r\u2081; ...; rk to r. After the first derivation step, we have a relational path of the form r1; ...; ri\u22122; S; ri+1; ...; rk. In this case, K contains a rule of the form s(X, Z) \u2190 ri\u22121(X, Y) \u2227 ri(Y, Z). This means that KUF = S(Xi\u22122, Xi). We thus have a relational path of the form r1; ...; ri-2; S; ri+1; ...; rk, where each relation is associated with an atom that is entailed by KUF. Each derivation step introduces such an atom (while reducing the length of the relational path by 1). After k - 1 steps, we thus obtain the atom r(a, b), from which it follows that KUF = r(a, b).\nLemma 2. Suppose that KUF = r(a, b). Then it holds that there exists a relational path r\u2081; ...; rk connecting a and b in G F such that r can be derived from r1; ...; rk."}, {"title": "A.3 Reasoning about disjunctive rules using algebraic closure", "content": "We consider knowledge bases with three types of rules. First, we have disjunctive rules that encode relational compositions:\n$$s_1(X, Z) \\lor... \\lor s_k(X, Z) \\leftarrow r_1(X, Y) \\land r_2(Y, Z)$$\nSecond, we have constraints of the following form:\n$$\\bot \\leftarrow r_1(X, Y) \\land r_2(X, Y)$$\nmeaning that r\u2081 and r2 are disjoint. Finally, we consider knowledge about inverse relations, expressed using rules of the following form:\n$$r_2(Y, X) \\leftarrow r_1(X, Y)$$\nWe now describe the algebraic closure algorithm, which can be used to decide entailment for calculi such as RCC-8. We also describe an approximation of this algorithm, which we call directional algebraic closure. This approximation closely corresponds to how GNN models can reason in this setting, as will be made clear below.\nFull algebraic closure Let us make the following assumptions:\n\u2022 The knowledge base K contains rules of the form (9), encoding the composition of relations r1 and r2.\n\u2022 We also have that K contains the rule Vrerr(X, Y) \u2190 T, expressing that the set of relations is exhaustive.\n\u2022 For all distinct relations r1,r2 \u2208 R, r1 \u2260 r2, K contains a constraint of the form (10), expressing that the relations are pairwise disjoint.\n\u2022 For every r \u2208 R there is some relation \u00ee \u2208 R, such that K contains the rules r(Y, X) \u2190 r(X, Y) and (Y, X) \u2190 r(X, Y), expressing that \u00ee is the inverse of r.\n\u2022 K contains no other rules.\nLet us write r1 r2 for the set of relations that appears in the head of the rule defining the composition of r\u2081 and r2 in K. If no such rule exists in K for r\u2081 and r2 then we define r\u2081 0 r2 = R. Let & be the set of entities that appear in F. Let us assume that F is consistent with K, i.e. KUF \u2260 1.\nThe main idea of the algebraic closure algorithm is that we iteratively refine our knowledge of what relationships are possible between different entities. Specifically, for all entities e, f \u2208 E, we define the initial set of possible relationships as follows:\n$$\\chi_{ef}^{(0)} = \\begin{cases} \\{r\\} & \\text{if } r(e, f) \\in F \\\\ \\{\\hat{r}\\} & \\text{if } r(f, e) \\in F \\\\ R & \\text{otherwise } \\end{cases}$$\nwhere is the unique relation that is asserted to be the inverse of r in K. Note that because we assumed F to be consistent, if r(e, f) \u2208 F and r' (f, e) \u2208 F it must be the case that r' = r. We now iteratively refine the sets X. Specifically, for i \u2265 1, we define:\n$$\\chi_{ef}^{(i)} = \\chi_{ef}^{(i-1)} \\cap ( \\bigcup_{g \\in \\mathcal{E}} \\chi_{eg}^{(i-1)} \\diamond \\chi_{gf}^{(i-1)} )$$"}, {"title": "A.4 Expressivity", "content": "We now show that the base GNN model, corresponding to (15), is capable of simulating the direc- tional algebraic closure algorithm. Specifically, we show the following result, which associates the embeddings e(2) from the GNN with the sets X(2). We show the following result for the min-pooling operator min defined as follows:\n$$\\psi_{min} (X_1, ..., X_k) = \\frac{min(x_1, ..., x_k)}{|| min(x_1, ..., x_k)||_1}$$\nProposition 3. Let F be a set of facts and K a knowledge base satisfying the conditions of Proposition 2. Furthermore assume that KU F is consistent, i.e. KUF \u2260 1. Let X) be sets of relations that are constructed using the directional algebraic closure algorithm, and let e denote the jth coordinate of e(i). Let the pooling operation be chosen as \u03c8 = 4min. There exists a parameterisation of the vectors ri and aij such that:\n$$\\chi_{e}^{(i)} = \\{ r_j | e_j^{(i+1)} > 0, 2 \\le j \\le n \\} \\supseteq \\chi_{e}^{(i+1)}$$\nProof. Let r1, ..., r'n be an enumeration of the relations in R\u222a {id}, where we fix r1 = id. Let aij = (a, ..., a) be defined for as follows (i \u2208 {1, ..., n}):\n$$a_{il}^j = \\begin{cases} 1 & \\text{if } r_i \\diamond r_j \\\\ 0 & \\text{otherwise} \\end{cases}$$\nwhere we define id 0 rj = r; for every j \u2208 {1, ..., n}. Note that a\u017cj indeed satisfies the requirements of the model: the coordinates of aij are non-negative and sum to 1, while a1j = one-hot(j) follows from the fact that id 0 rj = rj. Furthermore, we define rj = one-hot(j).\nWe show the result by induction. For i = 0, if F contains a fact of the form ri(h, e), we have X(0) = {r1}. We show that e(1) is non-zero in the [th coordinate and zero everywhere else. We have:\n$$\\phi(h_0, r_i) = \\sum_{i} h_i a_{ij} = \\sum_{i} a_{ij} = \\hat{r}_i = one \\text{-} hot(l)$$\nThis already shows that e(1) is zero in all coordinates apart from the [th. Now let f \u2260 h and rp be such that rp(f, e) \u2208 F. We have\n$$\\phi(f_0, r_p) = \\sum_{i} \\sum_{j} r_j a_{ij} = \\sum_{i} \\sum_{i} \\sum_{j} a_{ij}$$\nWe need to show that the [th coordinate of the latter vector is non-zero. Since KUF is consistent and F contains both r\u2081(h, e) and rp(f, e), it has to be the case there there is some q \u2208 {1, ..., n} such that ri \u2208 rq orp. There thus exists some q \u2208 {1, ..., n} such that the [th coordinate of aqp is non-zero. Since all vectors of the aip vectors have non-negative coordinates, it follows that the [th coordinate of \u03a3\u2081aip is non-zero. We have thus shown that {r; | e > 0, 2 \u2264 j \u2264 n} = {r1} = X0 > X(1).\nIf F does not contain any facts of the form ri(h, e), then X) = R. We need to show for each l\u2208 {2, ..., n} that either et is non-zero or r\u0131 \u2209 X. We have that el is non-zero if the [th component of $(f(0), rp) is non-zero for every fact of the form rp(f, e) in F, where f \u2260 h. Consider such a fact rp(f, e) and assume the [th component of $(f(0), rp) is actually 0. Since f \u2260 h we have f(0) = (0,...,0). The [th component of $(f(0), rp) = \u2211iaip can thus only be 0 if the [th component of aip is 0 for every i. This implies that ri friorp for any i \u2208 {1, ..., n}, from which it follows that ri & X(1).\nNow suppose that Xi\u22121) \u2265 {r; | e > 0,2 \u2264 j \u2264 n} > X\u00b2) holds for every entity e. We show that (13) must then hold as well. We first show X) = {rj|e+1 > 0,2 \u2264 j \u2264 n}. To this end, we need to show that for each r; \u2208 X(i-1) \\ X) it holds that e+1 = 0. If rj \u2208 X(i-1) | \\X 2 it means that there is some rp(f, e) \u2208 F such that r; & X(-1) (i-1) rp. This is the case if rj \u2209 rq \u25ca rp"}, {"title": "A.5 RCC-8 Dataset", "content": "RCC-8 [27] uses eight primitive relations to describe qualitative spatial relationships between regions: ntpp(a, b) means that a is a proper part of the interior of b, tpp(a, b) means that a is a proper part of b and shares a boundary point with b, po(a, b) means that a and b are overlapping (but neither is included in the other), dc(a, b) means that a and b are disjoint, ec(a, b) means that a and b are adjacent (i.e. sharing a boundary point but no interior points), eq(a, b) means that a and b are equal, and ntppi and tppi are the inverses of ntpp and tpp. The RCC-8 semantics is governed by the so-called composition table, which describes the composition mapping between two relations. This is shown in Table 5 where the trivial composition with the identity element eq being itself is dropped."}, {"title": "A.5.1 Dataset Generation", "content": "We now explain how the dataset was created. All sampling in the discussion below is uniform random. Each problem instance has to be constructed such that after aggregating the information provided by all the relational paths, we need to be able to infer a singleton label. In other words, problem"}, {"title": "A.5.2 Ensuring path consistency within the dataset", "content": "We ensure that all relational paths in a problem instance in the generated dataset do not informationally conflict with each other by using the DPC+ algorithm [21]. It efficiently computes directional path consistency, i.e. Xij \u2286 Xik\u25ca Xkj\u2200i, j \u2264 k, for qualitative constraint networks that we can transform our graph instances to. Directional path consistency is sufficient as a test for global path consistency for networks with singleton edge labels [20]."}, {"title": "A.6 Allen's interval algebra dataset", "content": "To complement the experiments in the main paper, we also introduce a dataset based on Allen's interval algebra [2]. Similar to RCC-8, the interval algebra uses 13 primitive relations to describe qualitative temporal relationships. The interval algebra captures all possible relationships between two time intervals. Similar to the RCC-8 relations, these are binary relations defined as follows: <(a, b) means that the time interval a precedes the time interval b; d(a, b) means that a occurs during b; 0(a, b) means that a overlaps with b; m(a, b) means that a meets b (a ends exactly before b starts); s(a, b) means that a starts b (a and b share a starting time); f(a, b) means that a finishes b (a and b share a finishing time); =(a, b) means that a equals b (the starting and finishing times for both are the same implying the intervals are the same); and finally >, di, oi, mi, si, fi are the inverses of the respective operations defined previously. The composition table for all the primitive interval relations is shown in Table 6 with the exception of the trivial composition of primitive elements with the identity element =."}, {"title": "A.7 Additional experimental details", "content": "We now present further details of the experimental set-up, including details of the loss function that was used for training the model, the considered benchmarks and baselines, and training details such as hyperparameter optimisation."}, {"title": "A.7.1 Loss function", "content": "Let us write t\u2081 = (ti,1,..., ti,n) for the final-layer embedding of entity ti in the graph Fi, and let r = (r1,...,rn) denote the embedding of relation r. Let us write:\n$$CE(t_i, r) = - \\sum_{j=1}^{n} r_j \\log t_{i,j}$$\nFor each training example i, we write ti for the corresponding tail embedding and we let ri \u2208 R denote the correct label. Then clearly, we want CE(ti, ri) to be low, while for each negative example r' \u2208 R \\ {ri} we want CE(ti, r') to be high. In the base model, we implement this with a margin loss, where for each i we let r \u2208 R \\ {ri} be a corresponding negative example:\n$$\\mathcal{L} = \\sum_i \\max\\big( 0, CE(t_i, r_i) - CE(t_i, r'_i) + \\Delta \\big)$$"}, {"title": "A.7.2 Benchmarks", "content": "CLUTRR2 [33] is a dataset which involves reasoning about family relationships. The original version of the dataset involved narratives describing the fact graph in natural language. It was, among others, aimed at testing the ability of language models such as BERT [13] to solve such reasoning tasks. However, the original paper also considered a number of baselines which were given access to the fact graph itself, especially GNNs and sequence classification models. A crucial finding was that such models fail to learn to reason in a systematic way: models trained on short inference chains perform poorly when tested on examples involving longer inference chains. This has inspired a line of work which has introduced a number of neuro-symbolic methods for addressing this issue. The CLUTRR dataset was released under a CC-BY-NC 4.0 license.\nGraphLog\u00b3 [34] involves examples for 57 different worlds, where each world is characterised by a set of logical rules. For each world, a number of corresponding knowledge graphs are provided, which the model can use to learn the underlying rules. The model is then tested on previously unseen knowledge graphs for the same world. The aim of this benchmark is to test the ability of models to systematically generalise from the reasoning patterns that have been observed during training, i.e. to apply the rules that have been learned from the training data in novel ways. This dataset is released under a CC-BY-NC 4.0 license.\nRCC-8 is the benchmark that we introduce in this paper, as described in Section A.5. We release this benchmark under a CC-BY 4.0 license.\nDataset statistics for CLUTRR, GraphLog and RCC-8 are reported in tables 7, 9, 8. We use a standard 80-20 split for training and validation for CLUTRR and RCC-8. For GraphLog, we use the validation set that is provided separately from the test set."}, {"title": "A.7.3 Baselines", "content": "We compare our method against the following neuro-symbolic methods:\nCTP Conditional Theorem Provers [25] are a more efficient version of Neural Theorem Provers (NTPs [29]). Like NTPs, they learn a differentiable logic program, but rather than ex- haustively considering all derivations, at each step of a proof, CTPs learn a filter function that selects the most promising rules to apply, thereby speeding up backwards-chaining procedure of NTP. Three variants of this model were proposed, which differ in how this"}, {"title": "A.8 Additional analysis", "content": "Additional CLUTRR results In the main paper, we presented the results for the standard CLUTRR benchmark, where problems of size k \u2208 {2,3,4} are used for training. In the literature, models are sometimes also evaluated on an even harder setting, where only problems of size k \u2208 {2,3}"}]}