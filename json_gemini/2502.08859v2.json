{"title": "ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges", "authors": ["Clinton J. Wang", "Dean Lee", "Cristina Menghini", "Johannes Mols", "Jack Doughty", "Adam Khoja", "Jayson Lynch", "Sean Hendryx", "Summer Yue", "Dan Hendrycks"], "abstract": "As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity \u2013 each typically requiring teams of skilled solvers hours to days to complete \u2013 with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Large Language Models (LLMs), evident in their saturation of existing benchmarks, call for a shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and probe unexplored abilities at the frontiers of LLMs reasoning. Puzzle-solving events represent a promising direction: they are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning. These puzzles demand intricate chains of deductive reasoning, cleverly interweaving logic, wordplay, mathematics, coding, and cultural references. Notably, they come without explicit instructions, forcing solvers to explore multiple creative approaches at each step \u2013 a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives.\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats (e.g., sudokus or crosswords) or restrict themselves to text-only challenges, falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist \u2013 MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges \u2013 they still operate within well-defined problem spaces, and are rapidly being saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is ripe to evaluate how these abilities come together in complex problem-solving scenarios.\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community. We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities"}, {"title": "2. ENIGMAEVAL", "content": "ENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles, an automated full-page screenshot), testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements, for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase."}, {"title": "2.1 Data Collection", "content": "Our benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers. This variety enables benchmark stratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements, including grids, pictures, diagrams, and their meaningful arrangements.\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Day Extravaganza, CS50x, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck). During the collection process, we applied the following filters:\n\u2022 Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution. Since these may rely on hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split.\n\u2022 Audio/Video and Interactive Elements. At the time of writing, only a few frontier models can process audio and video clues effectively. As a result, we excluded puzzles that rely on these modalities, as well as those requiring interaction with a web application.\n\u2022 Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\nAlong with the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages.\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format. This created two evaluation paths: one using the original untranscribed puzzles (PDFs or automated webpage screenshots) and another using our standardized format. This dual approach allows us to separate a model's reasoning capabilities from its ability to parse complex documents \u2013 addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability. Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats, necessitating human"}, {"title": "3. Experiments", "content": "We test state-of-the-art LLMs' deep reasoning capabilities on our benchmark. Section 3.1 describes our experimental setup, followed by quantitative results and performance analysis in Section 3.2."}, {"title": "3.1 Evaluation Setup", "content": "We run the evaluation on a range of leading LLMs with multimodal capabilities. We evaluate models by comparing their answers to ground-truth solutions through string matching. The models generate responses using format- specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4).\nMetrics Model performance is measured using accuracy at the default model temperature. For meta-puzzles, we evaluate accuracy by providing the model with correct component solutions, allowing us to isolate its meta- reasoning capabilities from its performance on individual puzzles (Appendix C)."}, {"title": "3.2 Results", "content": "Models show very limited success in solving puzzles. All frontier vision-language models achieve notably low accuracy on this evaluation, with even the leading model (01) only reaching 7.0% on the normal split and 0% on the hard split (Table 2). These results highlight the substantial gap between current multimodal LLMs and expert-level reasoning capabilities on complex long multimodal reasoning tasks. The low performance stems from the inherent complexity of the puzzle dataset \u2013 these are challenging problems that were not adversarially curated on models' weaknesses. The difficulties these models face emerge organically from puzzles requiring sophisticated reasoning, strategic thinking, and structured problem-solving approaches that current models have not yet mastered. The complete failure of all tested models on the hard split (0% accuracy) is particularly noteworthy, underscoring the significant challenges these models face when confronted with more complex puzzle variations.\nThe best frontier models are sometimes bottlenecked by preprocessing capabilities. The human-transcribed text-image versions of the puzzles described in Section 2 help isolate reasoning capabilities from visual parsing skills. Table 2 shows that on some models, providing raw images rather than the transcription results in similar performance, while for others it seems to drastically compromise performance, suggesting that these latter models have relatively poor OCR abilities.\nNo evidence of data leakage. The puzzles in this benchmark have publicly accessible step-by-step solutions online. Many frontier models may have seen these during pretraining. Models with web search capabilities could also solve the puzzle by searching the name of the puzzle. Still, the low pass rate across all benchmarks suggests that data contamination is not prominent in any of the frontier models, indicating their performance genuinely reflects their problem-solving capabilities, despite the possibility of puzzle solutions being present in their training data.\nWe used 01 to audit its own correct model-generated answers to check for plagiarism or nonsensical reasoning chains, when comparing the step-by-step solutions generated by the frontier model with the official puzzle"}, {"title": "4. Related Work", "content": "Reasoning benchmarks Recent years have seen the development of increasingly sophisticated benchmarks to evaluate models' reasoning capabilities across different domains and modalities. MATH [8], GPQA [9], Frontier- Math [10], and OlympiadBench [11] focus on advanced mathematical reasoning requiring both precise technical knowledge and creative problem-solving strategies. Humanity's Last Exam [12], and MMLU [13] evaluate domain expertise across academic and professional fields. MMMU [14], MathVista [15], and VISTA [16] evaluate general vision-language capabilities and reasoning across fields, while ARC-AGI [17] approaches intelligence evaluation through abstract pattern recognition tasks that assess core reasoning capabilities independent of domain knowl- edge. ENIGMAEVAL builds upon these foundations by requiring not just multi-step reasoning, visual-language understanding or pattern recognition, but the ability to synthesize disparate clues and discover hidden solution paths within seemingly unstructured multimodal information.\nPuzzle solving benchmarks Several benchmarks have emerged to evaluate different aspects of puzzle-solving capabilities of LLMs. PUZZLES [18] contains 40 types of visual logic puzzles focused on assessing algorithmic and logical reasoning in reinforcement learning settings. PuzzlePlex [19] extends this with 24 diverse puzzles spanning deterministic and stochastic games that require strategic reasoning and opponent modeling. PuzzleBench [20] introduces 31 challenging first-order combinatorial reasoning problems, from graph coloring to cryptarithmetic, while GridPuzzle [21] provides 274 grid-based puzzles designed to evaluate step-by-step reasoning chains beyond simple answer correctness. RiddleSense [22] takes a different approach with testing linguistic creativity and commonsense knowledge. These benchmarks are valuable for evaluating specific types of reasoning, but they rely on consistent formats. ENIGMAEVAL challenges models with puzzles that vary dramatically in both format and modality, requiring flexible reasoning approaches and cross-domain knowledge synthesis.\nPuzzle hunts The broader puzzle hunt ecosystem includes numerous high-quality collections: university compe- titions (MUMS [23], SUMS [24], Harvard Mystery Hunt[25]), corporate events (CISRA [26], Jane Street Puzzles [27], Googol Conglomerate [28]), and community-organized hunts (DASH [29], BAPHL [30], Galactic Puzzle Hunt [31]). While these events represent rich sources of creative and challenging puzzles, their decentralized authorship and varied licensing terms present significant challenges for dataset creation. To ensure both legal compliance and reproducible evaluation, ENIGMAEVAL focuses on puzzle collections with clear Creative Commons licenses and centralized organizational structures."}]}