{"title": "Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCES)", "authors": ["Jadon Geathers", "Yann Hicke", "Colleen Chan", "Niroop Rajashekar", "Justin Sewell", "Susannah Cornes", "Rene Kizilcec", "Dennis Shung"], "abstract": "Objective Structured Clinical Examinations (OSCEs) are widely used to assess medical students' communication skills, but scoring interview-based assessments is time-consuming and potentially subject to human bias. This study explored the potential of large language models (LLMs) to automate OSCE evaluations using the Master Interview Rating Scale (MIRS). We compared the performance of four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts across all 28 items of the MIRS under the conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step prompting. The models were benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores available. Model performance was measured using three accuracy metrics (exact, off-by-one, thresholded). Averaging across all MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater reliability (a = 0.98 for GPT-4o). CoT, few-shot, and multi-step techniques proved valuable when tailored to specific assessment items. The performance was consistent across MIRS items independent of encounter phases and communication domains. We demonstrated the feasibility of AI-assisted OSCE evaluation and provided benchmarking of multiple LLMs across multiple prompt techniques. Our work provides a baseline performance assessment for LLMs that lays a foundation for future research in automated assessment of clinical communication skills.", "sections": [{"title": "1. Introduction", "content": "Clinical communication skills are fundamental to medical practice and significantly impact patient satisfaction.1,2 These skills facilitate patient-centered care, which the Institute of Medicine defines as one of the six core elements of high-quality healthcare.\u00b3 Recognizing their importance, medical schools prioritize communication skills training before patient interactions, starting at the pre-clerkship phase of medical school. Assessment of communication skills occurs primarily through Objective Structured Clinical Examinations (OSCEs), where medical students engage with trained actors serving as standardized patients (SP) to demonstrate empathy, effective questioning, and encouragement, among many other skills. OSCEs are not only time-intensive and costly for medical schools; they also offer limited feedback for medical students. The feedback students receive is typically delayed and lacks sufficient detail to help students improve. This leads to an environment focused less on learning crucial clinical skills and more on passing the assessment. Furthermore, there is the potential for variability in scoring related to evaluator bias, rater characteristics, and item characteristics that require standardization training. Large language models (LLMs)\u2013artificial intelligence systems trained on textual data to generate human-like responses-offer promising avenues for automating both scoring and feedback provision in OSCE evaluation. Automation would alleviate the time burden of evaluation from practitioners and promote nearly immediate, accessible feedback to students following their examinations, potentially enhancing future educational outcomes.\u201d This could enable medical schools to expand opportunities for students to engage in deliberate practice with OSCE cases. The application of LLMs to evaluate clinical communication skills remains an emerging area, despite their effectiveness in other educational contexts such as essay scoring and code-writing assistants. 8,9 While current research has employed LLMs for creating virtual patients,10,11 generating medical examination content,\u00b92 and providing personalized support for skill development,13,14 their potential for comprehensive OSCE assessment remains largely unexplored. Although preliminary studies have demonstrated promising results in using LLMs to evaluate OSCE post-encounter notes,15 a robust LLM- based OSCE assessment framework-that can both reliably score student performance against communication rubrics and provide detailed feedback across diverse rubric elements-requires further research, including the establishment of a standardized benchmark. Approaches to LLM-based OSCE assessment face two key challenges. First, medical interviews from OSCEs contain communication competencies encompassing both verbal and non-verbal elements, requiring the evaluation of nuances that are not easily captured in text. Second, while scoring rubrics have been carefully designed and validated, the subjectivity of interpersonal communication poses persistent challenges for inter-rater reliability and introduces potential assessment bias.16,17,18 Our work examined a strategy to alleviate these challenges by developing and evaluating an LLM- based system for automated OSCE assessment, using the Master Interview Rating Scale (MIRS) as the assessment criteria.19 We hypothesized that state-of-the-art LLMs can provide reliable evaluations comparable to human raters across various communication competencies. To test this hypothesis, we compared the performance of four state-of-the-art models (GPT-40, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) against human consensus ratings on a dataset of 10 OSCE cases. We utilized four prompting strategies (zero-shot, chain-of-thought (CoT) reasoning, few-shot, and multi-step), both uniformly and by tailoring the prompting strategy to each assessment item for optimal performance. Additionally, we explored multimodal evaluation techniques for assessing non-verbal communication skills, though this remains a significant challenge for current LLM capabilities. This study aimed to enhance the efficiency and consistency of communication skills assessment in medical education, ultimately contributing to the development of more effective, empathetic, and prepared future healthcare professionals."}, {"title": "2. Methods", "content": "Our process for automated evaluation of OSCEs involved three steps, as illustrated in Figure 1. The first step was transcription, in which each video of an engagement between a physician and patient was transcribed using automatic speech recognition (we use OpenAI's Whisper for this step).20 Second, the LLM received a prompt outlining the assessment task and subsequently received the transcript to perform its assessment. Finally, scores and justifications were provided for all items present in the MIRS rubric. We describe our dataset and approach in more detail below."}, {"title": "2.1 Dataset", "content": "This benchmarking study analyzed a dataset of 10 OSCE cases including 174 evaluation data points provided by the University of Connecticut.21 The cases capture authentic clinical interactions between health professions students and standardized patients through videos ranging from 7 to 30 minutes in duration. This video dataset consists of three distinct categories of clinical engagements: four medical history-taking cases (two focusing on groin pain assessment; two examining left chest pain evaluation), three behavioral counseling cases (smoking cessation counseling, exercise counseling, nutrition counseling), and three dental cases (tooth pain evaluation, gum pain assessment, smoking cessation counseling). From each video, we extracted the audio and transcribed the resulting MP3 file using Whisper. The dialogue between the student physician and standardized patient was then diarized through manual annotation by our team."}, {"title": "2.2 Master Interview Rating Scale (MIRS)", "content": "Evaluation was based on the Master Interview Rating Scale (MIRS), a validated tool for assessing medical communication skills.22,23,24 The MIRS comprises 28 items rated on a 5-point scale, assessing various aspects of the medical interview including questioning skills, interview organization, and patient inclusion (see Figure S1). Expert consensus scores on the MIRS rubric provided by the University of Connecticut yielded 174 scoring data points across all ten cases. Guidance on how to score each MIRS item was integrated from the University of Connecticut MIRS rubric19 and supplemented with examples and contextual notes from the University of Tennessee MIRS rubric. 25 Although the MIRS is scored on a 5-point scale, it only has labeled anchor statements for scores of 1 (lowest score), 3 (mid-point), and 5 (highest score). To give clear scoring instructions to the LLM, our team wrote anchor statements for scores of 2 and 4, with medical education experts validating the language and suitability of the scoring criteria. Most of the MIRS items (26 out of 28) can be scored with LLMs based on the text alone by using automatically generated transcripts. The remaining two MIRS items-\"Pacing of Interview\" and \"Non-Verbal Facilitation Skills\"-require a multimodal evaluation strategy because they cannot be assessed through text transcripts alone. Most of the analysis in this paper centers on the 26 verbal items, but we also report findings for the two non-verbal items using a multimodal analysis."}, {"title": "2.2 LLMs and Prompting Techniques", "content": "We tested the performance of four state-of-the-art language models in this study: GPT-4o (OpenAI), 26 Claude 3.5 (Anthropic),27 Llama 3.1 (Meta),28 and Gemini 1.5 Pro (Google).29 All models were configured with a temperature of 0 for the most deterministic model responses. Additionally, we explored four prompting techniques to optimize model performance:"}, {"title": "2.3 Evaluation Metrics", "content": "We evaluated the model performances relative to expert consensus answers provided by medical educators at the University of Connecticut, who authored each of the 10 cases and assessed the students' performances according to the MIRS rubric. We used three accuracy metrics for evaluation, each representing a different level of scoring leniency:"}, {"title": "2.3 Experimental Details", "content": "All models were configured with a temperature of 0, ensuring that they always selected the most probable next token when generating responses. This setting made the responses largely deterministic, which removed the need for repeated trials due to consistent, nonrandom token selection. As a temperature of 0 can occasionally produce slight variations in output (e.g., due to tie-breaking between equally probable tokens), we conducted an intra-rater reliability test for GPT-40 using the baseline zero-shot prompting strategy to confirm the consistency of the model's scoring. For this reliability test, we obtained five independent evaluations for each case and treated each evaluation as an independent rater. Using Krippendorff's alpha, a reliability coefficient suited for ordinal data, we measured the agreement between the model's scores across trials. Krippendorff's alpha accounts for the degree of disagreement, penalizing larger discrepancies more heavily (e.g., a difference between 1 and 4 is penalized more than between 2 and 3). The coefficient ranges from -1 (indicating systematic disagreement) to 1 (indicating perfect agreement).33,34,35 The test yielded a Krippendorff's alpha of 0.98 for GPT-40, indicating excellent internal consistency. Beyond the reliability test, we demonstrated the overall performance of each model (GPT, Claude, Gemini, Llama) across prompting techniques on the three accuracy metrics. We also assessed the performance of all models on each relevant MIRS item by evaluating off-by-one accuracy for the baseline approach. In this analysis, the items were categorized by their occurrence during the encounter and their communication domain. Finally, we discuss early benchmarking findings of multimodal models, which incorporate audiovisual data, for evaluating MIRS items that are not textually represented in a direct transcript. This involved providing the dataset videos to Gemini 1.5 Pro with our prompts, evaluating the video and audio directly rather than using a transcript."}, {"title": "3. Results", "content": ""}, {"title": "3.1 Overall Model Performance", "content": "Figure 3 reports LLM performance across prompting techniques using three measures of accuracy, aggregating over the MIRS items (see Table S1). While exact accuracy was low (0.27 to 0.52), off-by-one accuracy (0.67 to 0.91) and thresholded accuracy (0.75 to 0.91) were moderate to high. CoT, few-shot, and multi-step techniques did not improve performance over the zero-shot baseline. However, selecting the optimal prompting technique for each MIRS item improved performance, indicating that adjusting the prompting technique to the rubric item may be beneficial. Although the exact accuracy was relatively low (0.52 for Claude), the models' ability to differentiate performance levels (thresholded accuracy 0.83-0.88 for zero-shot) aligns with practical applications in OSCE evaluations, as broad proficiency levels are often more informative than exact scores. This suggests LLMs are better used as complementary evaluation and proficiency detection tools rather than standalone replacements. All augmentative prompting techniques (CoT, few-shot, and multi-step) either minimally impacted or decreased accuracy compared to the zero-shot baseline. Declines in accuracy were particularly notable with few-shot prompting for Gemini and universal with multi-step prompting."}, {"title": "3.2 Performance by MIRS Items", "content": "We analyzed off-by-one accuracy for each MIRS item across the four models (GPT, Claude, Gemini, Llama) using the zero-shot baseline approach, grouping items into four temporal, skill-based phases.36 We selected off-by-one accuracy because it provided a balanced evaluation approach between lenient thresholded accuracy and conservative exact accuracy, though alternative accuracy metrics showed similar patterns for underperforming MIRS items. Most items were scored with an accuracy of 0.8 or higher, indicating the models' alignment with consensus scores across a variety of communication assessment tasks. While models demonstrated similar patterns of accuracy across encounter phases, specific items consistently challenged all models (e.g., \u201cacknowledges impact,\u201d \u201cminimizes jargon,\u201d \u201cachieves shared plan\") while others performed uniformly well (e.g., \"clarifies and verifies,\u201d \u201cassesses motivation,\u201d \u201cuses encouragement\u201d). The variable performance across individual items may be attributed to several factors. First, performance could be influenced by the clarity of item definitions, the frequency of items in transcripts, and the quality of provided examples. Second, some items were not present in all transcripts, resulting in smaller evaluation datasets compared to items that occurred consistently across cases. As encounter phases alone evoke no clear patterns in accuracy, these findings underscore a need to fine-tune the models on patient-physician encounter data."}, {"title": "3.3 Multimodal Performance", "content": "We used a multimodal model, Gemini 1.5 Pro, to score the two MIRS items requiring the assessment of non-verbal elements in patient-physician encounters. Performance was notably poor, systematically disagreeing with human raters (Krippendorff's alpha of -0.47). This failure to effectively reason on the basis of the provided audiovisual data may have arisen because the case videos often capture the expressions and gestures of one speaker at a time rather than capturing the engagement between both speakers simultaneously."}, {"title": "4. Discussion and Future Directions", "content": "The MIRS rubric assesses diverse communication skills. Some items assess localized elements (such as the encounter opening) and focus on specific phrases, while others assess recurring elements (like empathy statements) or holistic interaction qualities. While clinical skills assessment rubrics like the MIRS are designed with anchor statements to improve scoring reliability, this variability in items-along with rater characteristics, rubric complexity, and the length of assessment items-can undermine inter-rater reliability.37,38 These interpretation challenges are similarly reflected in LLM assessment, where unclear rubric language can lead models to misapply the scoring criteria. Even the perceived politeness of the prompts may impact assessment.39 To address these challenges, we explored various prompting techniques and found that uniformly applied techniques led to performance declines, revealing limitations in common approaches. Few-shot prompting, despite its wide use, led models to overemphasize model language rather than evaluate the overall communication quality. Similarly, multi-step proved ineffective, as the models failed to identify all relevant transcript elements, especially for MIRS items requiring consideration of the full transcript. In contrast, dynamically selecting the optimal technique for each MIRS item showed promise, highlighting the potential for performance enhancement through tailored approaches. While this study focused on MIRS scoring agreement between LLMs and human raters, LLMs can also provide written feedback, which is an important component of OSCE evaluations. Future work should consider how to align feedback with the learning objectives of clinical skills curricula and students' personal development goals while providing justifiable, targeted areas for improvement. This entails improving prompt design, contextualization, and review by medical students and educators. We encourage researchers to build on our benchmark by developing prompts that guide models toward realistic assessments without unnecessary leniency or bias."}, {"title": "5. Conclusion", "content": "In this benchmarking study, we demonstrated the use of LLMs in automating the evaluation of Objective Structured Clinical Examinations (OSCEs). Models exhibited high agreement with human evaluators in identifying students needing support (low-performing scores of 1-2) across assessment items using zero- shot prompting. CoT, few-shot, and multi-step proved useful when tailored to specific assessment items. Overall, LLMs show promise for effective automation and application to OSCE evaluation, but challenges remain in refining prompt design and ensuring numerical scores and feedback. We provide our prompts and evaluation approach as an initial benchmark and invite further improvements from the research community."}]}