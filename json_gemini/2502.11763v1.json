{"title": "Lightweight Deepfake Detection Based on Multi-Feature Fusion", "authors": ["Siddiqui Muhammad Yasir", "Hyun Kim"], "abstract": "Deepfake technology utilizes deep learning (DL)-based face manipulation techniques to seamlessly replace faces in videos, creating highly realistic but artificially generated content. Although this technology has beneficial applications in media and entertainment, misuse of its capabilities may lead to serious risks, including identity theft, cyberbullying, and false information. The integration of DL with visual cognition has resulted in important technological improvements, particularly in addressing privacy risks caused by artificially generated \u201cdeepfake\u201d images on digital media platforms. In this study, we propose an efficient and lightweight method for detecting deepfake images and videos, making it suitable for devices with limited computational resources. In order to reduce the computational burden usually associated with DL models, our method integrates machine learning classifiers in combination with keyframing approaches and texture analysis. Moreover, the features extracted with a histogram of oriented gradients (HOG), local binary pattern (LBP), and KAZE bands were integrated to evaluate using random forest, extreme gradient boosting, extra trees, and support vector classifier algorithms. Our findings show a feature-level fusion of HOG, LBP, and KAZE features improves accuracy to 92% and 96% on FaceForensics++ and Celeb-DF(v2), respectively.", "sections": [{"title": "1. Introduction", "content": "The development of deepfake technologies presents significant challenges for visual cognition in deep learning (DL) and raises serious concerns for visual information risks, as these synthetic media convincingly manipulate visual content, spreading misinformation [1-3]. The manipulated content may raise concerns about the potential abuse of technology and its consequences on politics, finance, and personal privacy [2,4]. Although convolutional neural networks (CNNs) have achieved considerable success in computer vision tasks [5-8] including deepfake detection, alternative approaches that combine machine learning and hand-made features are gaining popularity. In computer vision, technologies such as auto-encoders and generative adversarial networks (GANs) facilitate the generation of manipulated visuals. Deepfake images are often categorized into facial synthesis, attribute manipulation, identity swapping, and expression swapping, with CNN models commonly employed for video detection [4,9]. To identify highly precise synthetic visual data as deepfake, an effective deepfake detection method is required [2,10]. DL reduces human effort in feature engineering but increases complexity and interpretability due to high nonlinearity and input interactions. Traditional machine learning (ML) methods often sacrifice accuracy for interpretability due to their complexity and large data volumes. DL"}, {"title": "2. Related Works", "content": "Deepfakes have emerged as a critical challenge, prompting extensive research into detection techniques. The DL-based methods have shown the most advancement, leading to efficient detection systems. While various approaches have been proposed, they primarily rely on similar underlying principles [26,27]. Most of the detection methods use CNN-based models to classify images as fake or real, but state-of-the-art deepfake detectors (e.g., N. Bonettini [28]) still rely on complex neural networks, struggle with generalization to unseen deepfake techniques, and lack robustness under real-world distortions [28,29].\nSeveral deepfake detection approaches depend on various modalities and feature fusion to improve accuracy. Prior research has shown that integrating spatial and frequency domain features, as well as combining spatial, temporal, and spatiotemporal features, significantly improves detection accuracy compared to single-modality approaches [30-32]. For instance, Almestekawy et al. [33] fused Facial Region Feature Descriptor (FFR-FD) with random forest classifier and texture features (standard deviation, gradient domain, and GLCM) fed into an SVM classifier. Raza et al. [31] proposed a three-stream network utilizing temporal, spatial, and spatiotemporal features for deepfake detection. Moreover, security techniques for deepfake detection on untrusted servers were introduced by Chen B. et al. [34]; their method enables distant servers to detect deepfake videos without understanding the content.\nProper methods are essential for extracting valuable information from large unprocessed visual data, with feature-based techniques like LBP and KAZE offering computational efficiency as an alternative to resource-intensive CNNs [12]. Recent studies have suggested that combining extracted features with advanced ML classifiers can develop hybrid models for deepfake detection while maintaining robustness across diverse datasets [12,15,35].\nAlternatively, texture can be encoded by comparing each pixel with its neighbors, creating a binary pattern that serves as a robust feature descriptor across various lighting conditions. Feature extraction techniques are divided into global and local descriptor approaches [36]. Global methods analyze the entire image to generate a feature vector and are considered fast processing but have some limitations, such as Principle Component Analysis (PCA) [37], Linear Discriminant Analysis (LDA) [38], and Global Gabor generic features [39]. Local descriptors, like LBP [40] and Histogram of Oriented Gradients (HOG) [41], provide a more effective representation of images. LBP is widely used in face recognition [42], while HOG is used for human detection by dividing the image into fixed-size blocks and computing HOG features for each block. Likewise, the selection of custom features (Local Binary Patterns (LBP) based on texture and a customized High-Resolution Network (HRNet)) proposed by Khalil et al. [43] and fed to the SVM classifier. This efficiency makes LBP a popular choice in tasks where texture details are important, such as facial recognition and expression analysis, while also reducing processing time and computational costs [44]. Deepfake artifacts regularly change gradient orientations and edge patterns, which are essential for lightweight detection on resource-constrained devices. Compared to CNN-based approaches, it is less successful in detecting higher-level semantic discrepancies [45]. KAZE, on the other hand, can detect unique key points that are invariant to noise and transformations, which is essential for applications requiring high-fidelity feature matching under variable conditions. By detecting and characterizing two-dimensional features in nonlinear scale space, the KAZE features [14] resist Gaussian blurring. KAZE's reliance on nonlinear diffusion allows it to capture image structures that are often missed by traditional linear approaches, enhancing performance in complex environments [46]."}, {"title": "3. Proposed Fusion Model", "content": "In machine learning, a feature refers to a specific, measurable attribute of an image that helps in distinguishing patterns. This study focuses on the integration of two types of features (local descriptors) obtained from the LBP and HOG with KAZE features before the classification. To reduce computing costs and meaningful results, the detection of important frames and the elimination of insufficient frames are necessary. In the first step, the keyframes are extracted from videos within an interval of 0.5 s. The first and last 10 frames are overlooked (only if necessary) because they usually have information about the introduction or credits that are not directly related to deepfake detection preprocessing. Additionally, to identify the important keyframes, a similarity check between frames is used as the criterion. Various threshold values are used to determine different approaches. This part of the algorithm results in a pool of distinctive images of frames, which are available for feature extraction. The images were resized to a 28 \u00d7 28 single-channel format, ensuring a standardized input for processing. The research explores the use of frames extracted from video footage or standalone images, treating keyframes as textured representations. In the second step of feature extraction, the LBP, HOG, and KASE techniques are applied separately, which typically generates histograms. Furthermore, the fusion of LBP and KAZE and that of HOG and KAZE are used to input futures for the Extra Trees, Random Forest, Support Vector, and XGB classifiers. The proposed fusion model aims to improve detection accuracy while maintaining efficiency."}, {"content": "3.1. LBP Features\nThe LBP is an advanced technique for extracting features from images for texture study due to its efficient handling of value variations and straightforward computational process [52]. LBP sets thresholds for neighboring pixels, enabling accurate spatial pattern extraction from images and transforming textual information into binary data for classification and detection [53]. LBP analyzes every pixel in an image by evaluating the relationship between each pixel and its surrounding pixels within a specified radius R distance away from it. If the neighboring pixel value exceeds that of the pixel level, a binary bit is assigned as 1; otherwise, it is marked as 0 [54].\nGiven a grayscale image I of size M \u00d7 N, the LBP feature for each pixel (x, y) is computed via the following formula:\n$LBP(x_p, y_p) = \\begin{cases}\n1 \\text{ if } I(x_p, y_p) \\geq I(x,y) \\\\\n0 \\text{ otherwise}\n\\end{cases}$\nFor a pixel (x, y), we compare its intensity I(x, y) with the intensities of its P neighboring pixels on a circle of radius R. Let the intensities of these neighbors be {$I(x_p, y_p)$}$_{p=1}^P$. A binary value is assigned to each neighboring pixel. These binary values are concatenated to form a binary number, which is then converted to a decimal value. Compute the histogram of these LBP values over the entire image.\n$H_{LBP}(k) = \\sum_{x=1}^{M} \\sum_{y=1}^{N} \\delta(LBP(x,y),k), k \\in {0,1,...,2^P - 1}$\nwhere (a, b) is the Kronecker delta function, which is 1 if a = b and 0 otherwise. Finally, the histogram is normalized with a small constant to prevent division by zero.\nThe calculation used in this study is explained as follows: for a given pixel (xp, Yp), the intensity I(p\u2081) in the center of the (3 \u00d7 3) block is computed by comparing xp to its 8 neighboring pixels. The texture classification process relies on illumination, translation, and rotational variance, while keyframes lack control over these attributes, focusing instead on uniform pattern representation. A uniform pattern, LBP(xp, Yp), is more suitable. Preliminary experiments showed P 12 and R = 2 provide the best performance for the feature descriptor. With regard to categorizing textures on the basis of their patterns in images or videos, the way the pattern is perceived is influenced by factors such as lighting changes, shifting positions, and orientations of the texture details. However, when we focus on moments in a sequence, the patterns are not affected by rotations or translations. Instead, they are determined by how colors and contrasts spread out over different frequencies and points in time, creating a consistent pattern overall. Therefore, using patterns such as LBP(xp, yp) is more suitable for detecting deepfake keyframes. To evaluate this, we conducted some experiments where we tried different values for the radius R as well as the neighboring pixel P. After conducting our analysis, we determined that the values of P = 12 and R = 2 are suitable for the feature descriptor [54]."}, {"title": "3.2. HOG Features", "content": "The HOG is a feature descriptor widely used for texture analysis and object detection [59]. This method is particularly suitable for tasks requiring robust edge and gradient-based analysis, such as detecting structural inconsistencies in deepfake images. HOG divides an image into smaller spatial regions, known as cells, and computes a histogram of gradient directions within each cell. This process can be summarized into three steps: gradient calculation, cell histogram generation, and feature vector construction:\n\u2022 Gradient Calculation: For each pixel in the image, the gradients along the x- and y-axes are calculated using Sobel filters:\n$G_x = I(x + 1,y) \u2013 I(x \u2212 1, y), G_y = I(x, y + 1) \u2013 I(x, y \u2013 1)$\nThe magnitude M and direction \u03b8 of the gradient are computed as:\n$M = \\sqrt{G_x^2 + G_y^2}, \u03b8 = arctan(\\frac{G_y}{G_x})$\n\u2022 Cell Histogram Generation: The gradient magnitudes M are binned into orientation histograms, where the direction @ is quantized into a fixed number of bins (e.g., 9 bins for 0\u00b0-180\u00b0 or 18 bins for 0\u00b0\u2013360\u00b0). To improve invariance to illumination and contrast changes, the histograms are normalized within overlapping spatial blocks. Given a block B, normalization can be performed as:\n$HOG_{norm}(B) = \\frac{HOG(B)}{\\sqrt{||HOG(B)||^2 + \\epsilon}}$\nwhere e is a small constant to prevent division by zero.\n\u2022 Feature Vector Construction: The normalized histograms obtained from all the blocks are concatenated to form a single feature vector representing the image. HOG captures fine-grained details about edge orientations and their distribution, making it suitable for identifying subtle spatial distortions caused by deepfake manipulations.\nIn this study, the following HOG parameters were used:\n\u2022 Cell Size: 8 \u00d7 8 pixels;\n\u2022 Block Size: 2 \u00d7 2 cells;\n\u2022 Number of Orientation Bins: 9 (0\u00b0-180\u00b0);\n\u2022 Step Size: 50% overlap between blocks.\nHOG is an effective choice for resource-constrained deepfake detection due to these parameters, which maintain a balance between descriptive strength and processing efficiency. HOG can be used in combination with other robust feature descriptors, such as KAZE, to improve its sensitivity to high-level semantic adjustments to improve deepfake detection."}, {"title": "3.3. KAZE Features", "content": "The KAZE features are computed to capture the multi-scale and nonlinear structure of the keyframes. The KAZE algorithm involves detecting keypoints and computing descriptors. This process is involved by applying nonlinear diffusion filtering to the keyframe I to create a nonlinear scale space. The keypoints are detected with the KAZE detector using the formula: {$I(x_i,y_i)$}$_{K(i = 1)}$. For each keypoint ($x_i,y_i$), compute a descriptor vector d\u2081 that represents the local image patch around the keypoint. Concatenate the descriptor vectors into a single feature vector D. If the total number of features exceeds a predefined length, the feature vector is truncated or padded as follows:\nD = [$d_1, d_2, ..., d_k$] (1:m)\nwhere m is the designed length of the KAZE feature vector. Finally, computed descriptors are used for each key point. KAZE descriptors are computed by sampling the responses of the nonlinear scale space at keypoint locations using orientation and scale information. The extracted features are later used to classify the video as either fake or real. This classification is accomplished via ML classifiers with KAZE robustness in extracting image features to detect deepfakes precisely."}, {"title": "3.4. Proposed Feature Fusion and Classification", "content": "The process of combining LBPs and KAZE features for image classification involves extracting two distinct sets of features from a single image, merging these feature sets into a unified feature vector, and then using this combined vector to train a classifier. This procedure improves classification performance, particularly for detecting deepfake content by utilizing the included strengths of KAZE (keypoint detection and description) and LBP (texture analysis).\nThe proposed fusion model is a comprehensive method for image classification that integrates LBP or HOG and KAZE features, followed by the selected classifier. Initially, the algorithm extracts LBP or HOG features, which capture texture features with the distribution of binary patterns in the neighborhood of each pixel. Mathematical representations (Equations 2 and 3) by the LBP histogram are used to normalize and achieve uniform feature scaling. Concurrently, KAZE features are extracted by detecting key points and computing their descriptors, effectively capturing local invariant features. These descriptors are integrated into a single vector, which is then truncated or padded to maintain a consistent feature length. The LBP and KAZE feature vectors are combined to create a single feature representation for each image, utilizing the improved strengths of both feature extraction methods.\nThe process of merging the LBP and KAZE features is explained in these steps. First, we extract the LBP feature vector F\u2081 BP from the histogram H\u2081 BP by employing the following formula:\n$F_{LBP} = H_{LBP}(0), H_{LBP}(1), . . ., H_{LBP}(2^P \u2013 1)$\nSecond, the KAZE feature vector FKAZE is extracted from the concatenated descriptor vectors D by FKAZE = D.\nFinally, the FLBP and FKAZE feature vectors are concatenated as follows:\n$F_{combined} = [F_{LBP}, F_{KAZE}]$\nThe integration of LBP and KAZE features improves the algorithm's robustness and accuracy in deepfake classification tasks, especially when detecting false or real images. LBP features improve in detecting texture patterns, which are important in distinguishing"}, {"title": "4. Implementation", "content": "This section summarizes the experimental details and results of deepfake datasets for FaceForensic++ [24] and Celeb-DF [25]. The robustness of the fusion (LBP, HOG, and KAZE) features under various classifiers is evaluated. We applied some preprocessing on the raw data prior to experimentation with the datasets. The complete video sequence is not taken into account. Instead, as described in Section 3, some keyframes are extracted. Both fake and real videos are included in the video dataset. After being extracted, the frames were placed in a folder with the appropriate name. The NVIDIA 3090 GPU was used for feature extraction and the development of ML algorithms.\nFaceForensics++ [24] is the popular publicly available forensic dataset that includes 1000 original video sequences that have been manipulated using four distinct face manipulation techniques: deepfake, Face2Face, FaceSwap, and NeuralTextures. The dataset consists of 977 YouTube videos with 48,431 face counts and a data size of 575 mb, all of which feature a clearly visible face, allowing automated tampering methods to produce highly accurate forgeries (an example of the dataset is depicted in Figure 2). We targeted deepfake videos and their original equivalents for our experiment. The dataset was created by extracting keyframes from several videos. Following preprocessing, there were 2946 fake images and 2930 real images in the training set. There were 198 real and 197 fake images in the validation."}, {"title": "4.2. Evaluation Criteria", "content": "Empirical benchmarking is a popular way to accurately analyze feature extraction and training times. This involves directly quantifying the time spent throughout experimental runs, resulting in exact and dependable data. This method is especially useful for machine learning tasks, where computational complexity varies depending on dataset size, hardware capabilities, and specific implementation choices. In this paper, we describe the methodological approach used to calculate feature extraction, training, and inference times for ML classifiers, providing a thorough assessment of computing efficiency.\nThe time required for feature extraction can be computed as follows: for each feature extraction method, the extraction process for all the data points in the dataset is applied. The start and end times Tfeature = Tend - Tstart are recorded for each feature extraction run [61]. To mitigate variability owing to hardware or background processes, the feature extraction process is repeated multiple times, and the average time is computed as follows:\n$AverageFeatureExtractionTime = \\frac{\\Sigma_{i=1}^n T_{feature, i}}{n}$\nwhere T feature, i denotes the feature extraction time for the i \u2013 th run and n is the number of runs. For reporting purposes, the time per data instance (e.g., per frame in video processing) can also be computed as follows:\n$T_{feature,instance} = \\frac{T_{feature}}{N}$\nwhere N is the total number of data instances. Training time refers to the duration required to train an ML model on a specified dataset. For each classifier (e.g., RF, SVM, and CNN), the training process is initiated and calculated by recording the start and end times as follows: Ttrain = Tend - Tstart [62]. Similar to feature extraction, it is often beneficial to perform multiple runs and compute the average through the following equation to obtain a reliable estimate:\n$AverageTrainingTime = \\frac{\\Sigma_{i=1}^n T_{train,i}}{n}$\nFor larger datasets, the training time may also be approximated on the basis of model complexity. For example, the training time complexity of RF with N trees is generally O(NlogN), whereas the support vector classifier may exhibit O(N2) complexity. The inference time is the duration required to classify a new instance after training. The total inference time over a dataset Tinference can be approximated by Tinference = Tinference,instanceXN, where N is the total number of instances in the test dataset. Multiple test runs were carried out to establish temporal consistency among approaches, with start and end times carefully documented. Measuring the time necessary for each instance provides for more detailed comparisons and brings out performance differences more clearly. This established approach makes sure that all important time calculations for feature extraction, training, and inference are directly comparable, resulting in a rigorous and repeatable experimental framework."}, {"title": "4.3. Results and Discussion", "content": "The results are further provided and analyzed in depth. This study proposed a feature-level fusion of LBP, HOG, and KAZE features for classification via the FaceForeensics++ and Celeb-DF datasets. The evaluation of the results on the basis of the provided validation is presented below: Table 2 presents the classification accuracies obtained using various classifiers with various feature sets, including LBP alone, KAZE alone, and the fusion of LBP, HOG, and KAZE features. The experiment was conducted on the FaceForensics++ and Celeb-DF datasets to evaluate the effectiveness of these features in distinguishing between genuine and manipulated visual content/deepfake."}, {"title": "4.4. Future Work and Implications of Visual Information Security", "content": "The fusion of LBP, HOG, and KAZE features has proven effective in detecting deep-fake content. HOG captures texture patterns, while KAZE detects structural distortions introduced by deepfake generation techniques. Future research could refine classifiers to improve the model's ability to distinguish between real and fake content. Exploring advanced hybrid techniques like ORB, DSIFT, and Wavelet Transform Features could further improve detection accuracy and computational efficiency. Additionally, dimensionality reduction methods such as PCA and t-SNE can optimize feature selection, while DL approaches like hybrid CNN architectures or GANs could bolster the robustness of deepfake detection.\nThis approach has strong potential for forensic and legal applications, providing a reliable means to verify the authenticity of digital media in critical legal proceedings. It could also contribute to real-time authentication systems for digital media platforms, potentially integrating blockchain or watermarking techniques for added security. The research highlights the importance of robust feature extraction methods in deepfakes, and future efforts should focus on refining these techniques and adapting to new manipulation strategies to ensure continued efficacy in securing digital media."}, {"title": "5. Conclusions", "content": "In conclusion, this study introduced an effective approach for detecting deepfake images utilizing texture-based features through the fusion of HOG/LBP and KAZE within an ML framework. The computational load is significantly reduced compared to traditional DL models, making this method ideal for real-time applications with limited processing resources. The experiments using classifiers such as RF, XGBoost, extra trees, and support vector classifiers demonstrated the distinct advantages of each method in evaluating feature importance across HOG feature bands. The feature-level fusion technique further improved performance on both the FaceForensics++ and Celeb-DF datasets, achieving an accuracy of 92.12% and 78%, respectively. This approach not only improves accuracy and efficiency in detecting deepfake content but also provides a scalable solution against the potential abuse of technology and its consequences on politics, finance, and personal privacy. Beyond deepfake detection, the method holds the potential for authenticating various forms of digital content, emphasizing its broad applicability in fields that require reliable visual data verification."}]}