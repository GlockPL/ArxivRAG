{"title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge", "authors": ["Yuanze Lin", "Weijian Xu", "Ronald Clark", "Yunsheng Li", "Dongdong Chen", "Philip Torr", "Lu Yuan"], "abstract": "In recent years, multimodal large language models (MLLMs) have made signif- icant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explic- itly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform ex- ternal knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial em- bedding map as a visual prompt. This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.", "sections": [{"title": "Introduction", "content": "The advancement of large language models (LLMs) [56, 42, 43, 17] has revolutionized how machines process and generate human-like text, demonstrating remarkable abilities in reasoning, translation, and contextual understanding. The integration of language and vision into unified models, such as GPT-4V [41], represents a significant leap forward in enabling machines to understand and interact with the world in a manner akin to human cognition. As these models continue to evolve, they promise to further blur the lines between human and machine cognition, opening new frontiers in AI research and application [32, 51, 46, 33, 55, 31].\nDespite their remarkable capabilities, most of the MLLMs (shown in Figure 1 (a)) trained with image- text pairs still often struggle in fine-grained multimodal comprehension capacities, e.g., correctly count objects or output precise location of one specific object. This is partially because of the lack of high-quality data with exceptionally fine-grained text description. More importantly, text itself has the inherent difficulty in accurately conveying highly fine-grained or spatially dense information. As a result, current MLLMs often fail to accurately interpret pixel-level visual content of localized regions within an image, which in return harms the overall comprehension capacity for the image and thereby causes the notorious \u201challucination\u201d problem [25]."}, {"title": "Related Work", "content": "Large Language Models. The initial potential of large language models (LLMs) was showcased by foundational works like BERT [14] and GPT [47]. They sparked a wave of scaling efforts, leading to a range of influential projects, such as T5 [49], GPT-3 [4], Flan-T5 [12], and PaLM [9]. As the volume of training data expanded and the dimensions of model parameters grew, these scaling endeavors led to the creation of ChatGPT [40, 44]. Models like LLaMA [56] and GPT-4 [43] have been trained on extensive corpora and demonstrated remarkable capabilities in diverse cognitive tasks. Additionally, lightweight LLMs with fewer than 3B parameters, i.e., Phi [2, 39] and StableLM-2 [53] have shown performance comparable to larger models [8]. In our work, we adopt Phi-2 [39] and Vicuna-7B [8] as our language backbone.\nMultimodal Large Language Models. Influenced by the success of instruction tuning from LLM, LLaVA [35] and MiniGPT-4 [64] have adopted visual instruction tuning to improve LLMs' interaction with visual data, yielding impressive outcomes. Kosmos-2 [45] and Shikra [7] have advanced MLLMs by enhancing visual comprehension capabilities. while works like LLaVA-Phi [66], MobileVLM [11] and Bunny [20] mainly focus on optimizing training recipes and architecture design for lightweight MLLMs. To solve the challenge of understanding fine-grained information in images, existing approaches propose to learn coordinate representations [6, 7, 62] and Region of Interest (ROI) features [45, 61], which use inflexible visual referral formats or necessitate the collection of"}, {"title": "Proposed Method", "content": "In this section, we propose a new visual prompt paradigm that integrates local external information to enhance the capability of MLLMs. In section 3.1, we outline the design of the auxiliary visual prompt that contains local contextual information. Using the auxiliary visual prompt, in section 3.2, we further embed it into MLLMs by merging it with the original visual tokens. Finally, we briefly introduce the details of training in section 3.3."}, {"title": "Auxiliary Visual Prompt with External Knowledge", "content": "In this section, we propose a method to generate local contextual external knowledge to assist MLLMs. In contrast to [23], which focuses solely on object detection and OCR information and integrates them as part of the text prompt, we enhance the granularity of local external knowledge by leveraging a panoptic segmentation model. Additionally, we continue to utilize an OCR model but transform both types of external knowledge into pixel-wise embeddings. Further details are provided below.\nAs shown in Figure 2, given the input image \\(I \\in \\mathbb{R}^{3\\times H\\times W}\\), we can obtain the fine-grained external knowledge by an off-the-shelf panoptic segmentation model [60] and an OCR model [15]. The generation of the external knowledge can be expressed as:\n\\[\\{M_j, C_j\\}_{j=1}^{N_s} = f_{seg}(I), \\quad \\{B_j,T_j\\}_{j=1}^{N_o} = f_{ocr}(I),\\]\nwhere \\(f_{seg}(\\cdot)\\) and \\(f_{ocr}(\\cdot)\\) mean panoptic segmentation and optical character recognition (OCR) models, \\(N_s\\) and \\(N_o\\) are the numbers of detected mask regions and OCR bounding boxes. \\(\\{M_j, C_j\\}_{j=1}^{N_s}\\) is the set of mask regions and the corresponding classes, and \\(\\{B_j, T_j\\}_{j=1}^{N_o}\\) represents the set of detected OCR bounding boxes and texts.\nWith the detected classes \\(\\{C_j\\}_{j=1}^{N_s}\\) and OCR texts \\(\\{T_j\\}_{j=1}^{N_o}\\), a pre-trained text encoder \\((f_{text}(\\cdot))\\) is leveraged to generate the texture embeddings as:\n\\[T_s = \\{t_1,...,t_{N_s}\\} = \\{f_{text}(C_1),..., f_{text}(C_{N_s})\\},\\\\T_o = \\{t'_1, ..., t'_{N_o}\\} = \\{(f_{text}(T_1),..., f_{text}(T_{N_o})\\},\\]\nwhere \\(t_i \\in \\mathbb{R}^{1\\times d} (1 \\leq i \\leq N_s)\\) and \\(t'_i \\in \\mathbb{R}^{1\\times d} (1 \\leq i \\leq N_o)\\) denote the ith textual embedding vector of the classes for the detected mask region and OCR texts respectively, while d is the embedding dimension.\nIn order to generate a pixel-wise visual prompt for the external knowledge instead of a pure text description for the regions with coordinates and category names, the auxiliary visual prompt is initialized as a zero tensor \\(P \\in \\mathbb{R}^{H\\times W\\times d}\\) and then filled with the newly generated texture embeddings for the external knowledge as:\n\\[P_{j,k} = \\begin{cases}  t_v & \\text{if } (j, k) \\in M_u \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\forall u \\in \\{1,..., N_s\\},\\]\n\\[P_{j,k} = P_{j,k} + \\begin{cases}  t'_v & \\text{if } (j, k) \\in B_u \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\forall u \\in \\{1,..., N_o\\}.\\]\nNote, for some regions, if the confidence of the class prediction given by the segmentation model is low or the OCR model fails to detect any text, we leave the region area with zero values. For the regions that are occupied by both model, we simply add the text embeddings directly. We leave the investigation of more refined fusion techniques to future research.\nWith the auxiliary visual prompt containing pixel-level local contextual information from panoptic segmentation and OCR models, MLLMs can effectively capture finer-grained features. The next challenge is to establish a clearer connection between the newly generated external knowledge and the original image feature. This will help alleviate the model's difficulties in learning their relationship effectively."}, {"title": "Visual Prompt Infusion", "content": "In this section, we introduce the visual prompt infusion that incorporates the proposed auxiliary visual prompts into the MLLMs. Previous methods [23] choose to append the external knowledge (embeddings for object category and its coordinates) to the text prompts, which requires the model to learn the correspondence of visual content within the specified coordinates encoded in the external knowledge and, as a result, increasing the difficulties of the learning process of the model. To address this challenge, we propose to merge the auxiliary visual prompt directly with the image features in a pixel-wise manner.\nSpecifically, as shown in Figure 3, the image tokens are first generated via an image encoder \\(f_{img}(\\cdot)\\) and an MLP projector \\((f_{MLP}(\\cdot))\\):\n\\[F_v = f_{MLP}(f_{img}(I)),\\]\nwhere \\(F_v \\in \\mathbb{R}^{N_v\\times d_v}\\) and \\(N_v\\) and \\(d_v\\) represent the number of image tokens and the embedding dimension. Then, the auxiliary visual prompt is further processed by a prompt embedding network (PEN) as\n\\[F_p = f_{PEN}(P).\\]\nFor the prompt embedding network, we employ three convolutional layers, with an activation layer (ReLU) inserted between each pair of them. This network primarily serves to align the feature space and spatial size between the image tokens and the auxiliary visual prompts.\nWhen combining the image tokens and the processed auxiliary visual prompt, we mainly consider two options, both of which operate pixel-wise. (1) feature fusion: \\(F_t = f(Concat(F_v, F_p))\\), where f is a linear layer that maps the embedding \\(\\mathbb{R}^{N_v\\times d_{2v}} \\to \\mathbb{R}^{N_v\\times d_v}\\) to maintain the total number of image tokens unchanged; (2) feature addition, \\(F_t = F_v + F_p\\), which sums the two types of features directly.\nThe advantages of the pixel-wise infusion for both options facilitate the model's comprehension of the correspondence between external knowledge and original visual features. This explicit guidance enables the model to easily understand the pixel categories as well as the potential OCR text description it conveys. Consequently, it aids the model in disambiguating complex scenes, accentuating salient features, and distinguishing finer objects."}, {"title": "Training", "content": "Training MLLMs involves predicting responses based on multimodal inputs using an autoregressive approach. The objective is to maximize the probability of generating tokens that match the ground- truth answer \\(Y_a\\). With the new visual embedding \\(F_v\\), this can be mathematically expressed as follows:\n\\[P(Y_a|F_v, F_t) = \\prod_{i=1}^{L} P_{\\Theta}(Y_i | F_v, F_t, Y_{a,<i}).\\]\nHere, L represents the sequence length of the ground truth answer \\(Y_a\\), \\(\\Theta\\) means the trainable parameters. \\(Y_{a,<i}\\) represents all the answer tokens preceding the current prediction token xi, where i denotes the step in the sequence of text token generation. \\(F_t \\in \\mathbb{R}^{N_t\\times d_t}\\) is the token embedding of the input question, \\(N_t\\) and \\(d_t\\) denote the number of text tokens and token embedding dimension. By infusing these enriched visual cues into the training pipeline, MLLMs can develop a more"}, {"title": "Experiment", "content": "In this section, we conduct a comprehensive comparison of our method with existing state-of-the-art (SOTA) multimodal models. Additionally, we perform a series of ablation studies to further validate the proposed method. Finally, we provide visualization examples for in-depth analysis.\nModels. For the vision encoder, we adopt SigLIP-384px [59] for experiments. We leverage Phi-2- 2.7B [39] and Vicuna-7B [8] model as the language decoder. For the multimodal projector, same as LLaVA [34], we adopt a two-layer MLP. We use OpenSeed [60] and PaddleOCRv2 [15] to generate the per-pixel externally knowledge for pixel class and OCR text, and leverage UAE-Large-V1 [27] to extract the textual embedding.\nTraining Setting. We fine-tune the models on LLaVA-Instruct-150K dataset [34] using LORA [21] for 1 epoch, at a learning rate of 2e-4 and a batch size of 256 on 32 \u00d7 V100 32GB GPUs. For the setting of LORA, we set LoRA rank to be 128 and LoRA's hyperparameter a as 256. Note that we fix all the weights of pre-trained modules, i.e., vision encoder, language encoder and MLP, during training. Our models' weights are initialized from Mipha-3B [65] and LLava-7B [34].\nBenchmarks and Baselines. We evaluate our approach using 9 popular benchmarks to comprehen- sively assess its multimodal capabilities. These benchmarks include: VQA-v2 test-dev split [18], GQA test-dev-balanced split [22], ScienceQA-IMG test split [38], MME perception [16], \u039c\u039c\u0395 cognition [16], MMBench test split [37], MM-Vet test split [58], TextVQA [52], and POPE [28].\nWe compare our results with a bunch of state-of-the-art multimodal large language models (MLLMs): BLIP-2 [26], InstructBLIP [13], Shikra-13B [7], IDEFICS80/9B [24], Qwen-VL [3], mPLUG- Owl2 [57], LLaVA-v1.5-13/7B [34], LAF-7B [23], and multimodal small language models (MSLMs) [65]: MobileVLM [11], LLaVA-Phi [66], MC-LLaVA [1], Imp-v1 [54], MoE-LLaVA-3.6B [29], TinyLLaVA-share-Sig-Phi [63], Bunny [20] and Mipha [65]."}, {"title": "Ablation Studies", "content": "In this section, we conduct an ablation study to assess the effectiveness of the proposed approach. By default, the experiments are conducted using Mipha-3B [65] with Phi-2 [39] as the language backbone unless otherwise specified."}, {"title": "Main Results", "content": "In Table 5, we compare our methods with other state-of-the-art (SOTA) models. We divide the table into sections for language models smaller than 3B and those beyond 7B to provide a clearer comparison. From the results, we observe that our model achieves the best performance on 7 out of 9 benchmarks for larger language models (>7B) and attains the highest accuracy on 7 out of 9 benchmarks for relatively smaller language models (<3B). Note that, in Table 5, some models, e.g. Shikra-13B [7], Qwen-VL [3], are trained with million or billion level data, while our model is only trained on the dataset used by LLaVA-1.5 without any extra data for neither pre-training nor fine-tuning, which highlights the exceptional multimodal understanding and reasoning capabilities of our models. In addition, on top of the LLaVA-1.5 framework, our approach can bring more remarkable and consistent improvement on all benchmarks compared with LAF [23]. It justifies the proposed infusion strategy, which involves inserting external knowledge in a pixel-wise manner directly into the visual features, as being more effective than appending it to the text prompt [23]."}, {"title": "Quantitative Result Analysis", "content": "We present visualization results in Table 3 and 6 to further illustrate the improvement of our model in terms of both global image understanding and local object and text recognition. Table 3 demonstrates that compared to LLaVA-1.5 7B [34], our approach generates more detailed and contextually relevant responses, e.g., \"The cat's calm demeanor in the face of the women's playful behavior\" for the left"}, {"title": "Limitations and Broader Impact", "content": "Our method relies on pre-trained models for panoptic segmentation and OCR detection in a zero- shot manner. The performance of these models will significantly impact the performance of our proposed method, particularly when there is a substantial domain gap between the images from specific benchmarks and the training set of the segmentation or OCR models. While the proposed approach holds promise for significantly enhancing the cognitive capabilities of multimodal models and may inspire new methodologies and techniques in the development of robust multimodal AI systems, users must be aware of potential negative societal impacts. For instance, biases may manifest in various forms; for example, biased responses may be generated by the model if the training data of MLLMs, panoptic segmentation, and OCR detection models contain certain biases."}, {"title": "Conclusion", "content": "In this paper, we have proposed a method for leveraging external knowledge, such as localized contextual information, to enhance the capabilities of multimodal language models (MLLMs). To accomplish this objective, we propose extracting pixel-wise contextual information using a panoptic segmentation and OCR model, and then directly integrate this with the visual features. This enables the model to better understand both fine-grained objects and the overall global image context. Experimental results from ablations and comparisons with state-of-the-art methods demonstrate the effectiveness of our approach. We hope this paper can shed light on the importance of external knowledge for MLLMs and an effective way to leverage such knowledge."}, {"title": "Appendix", "content": "In the supplementary materials, we provide the following sections:\n(a) More implementation details in Section B.\n(b) Ablation study experiments in Section C.\n(c) Visualization result analysis in Section D."}, {"title": "Implementation Details", "content": "The training time for LLaVA-1.5 7B [34] and Mipha-3B [65] is approximately 14 hours and 9 hours, respectively, with a batch size of 256 on 32 \u00d7 NVIDIA V100 32GB GPUs. For the initialization of the proposed prompt embedding network (PEN), we use Kaiming initialization technology [19]. The UAE-Large-V12 model is adopted as the pre-trained textual encoder to extract textual embeddings for the visual prompt."}, {"title": "Ablation Study", "content": "Next, we conduct more ablation study experiments to provide deeper insight into the components of our proposed approach."}, {"title": "Visualization Result Analysis", "content": "We've provided more visualization results in Table 10, 11, 12, and 13. Compared to LLaVA-1.5 7B [34], our method generates more reasonable and accurate responses to the questions."}]}