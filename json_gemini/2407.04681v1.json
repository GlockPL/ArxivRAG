{"title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge", "authors": ["Yuanze Lin", "Weijian Xu", "Ronald Clark", "Yunsheng Li", "Dongdong Chen", "Philip Torr", "Lu Yuan"], "abstract": "In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial embedding map as a visual prompt. This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.", "sections": [{"title": "1 Introduction", "content": "The advancement of large language models (LLMs) [56, 42, 43, 17] has revolutionized how machines process and generate human-like text, demonstrating remarkable abilities in reasoning, translation, and contextual understanding. The integration of language and vision into unified models, such as GPT-4V [41], represents a significant leap forward in enabling machines to understand and interact with the world in a manner akin to human cognition. As these models continue to evolve, they promise to further blur the lines between human and machine cognition, opening new frontiers in AI research and application [32, 51, 46, 33, 55, 31].\nDespite their remarkable capabilities, most of the MLLMs (shown in Figure 1 (a)) trained with image-text pairs still often struggle in fine-grained multimodal comprehension capacities, e.g., correctly count objects or output precise location of one specific object. This is partially because of the lack of high-quality data with exceptionally fine-grained text description. More importantly, text itself has the inherent difficulty in accurately conveying highly fine-grained or spatially dense information. As a result, current MLLMs often fail to accurately interpret pixel-level visual content of localized regions within an image, which in return harms the overall comprehension capacity for the image and thereby causes the notorious \u201challucination\u201d problem [25]."}, {"title": "2 Related Work", "content": "Large Language Models. The initial potential of large language models (LLMs) was showcased by foundational works like BERT [14] and GPT [47]. They sparked a wave of scaling efforts, leading to a range of influential projects, such as T5 [49], GPT-3 [4], Flan-T5 [12], and PaLM [9]. As the volume of training data expanded and the dimensions of model parameters grew, these scaling endeavors led to the creation of ChatGPT [40, 44]. Models like LLaMA [56] and GPT-4 [43] have been trained on extensive corpora and demonstrated remarkable capabilities in diverse cognitive tasks. Additionally, lightweight LLMs with fewer than 3B parameters, i.e., Phi [2, 39] and StableLM-2 [53] have shown performance comparable to larger models [8]. In our work, we adopt Phi-2 [39] and Vicuna-7B [8] as our language backbone.\nMultimodal Large Language Models. Influenced by the success of instruction tuning from LLM, LLaVA [35] and MiniGPT-4 [64] have adopted visual instruction tuning to improve LLMs' interaction with visual data, yielding impressive outcomes. Kosmos-2 [45] and Shikra [7] have advanced MLLMs by enhancing visual comprehension capabilities. while works like LLaVA-Phi [66], MobileVLM [11] and Bunny [20] mainly focus on optimizing training recipes and architecture design for lightweight MLLMs. To solve the challenge of understanding fine-grained information in images, existing approaches propose to learn coordinate representations [6, 7, 62] and Region of Interest (ROI) features [45, 61], which use inflexible visual referral formats or necessitate the collection of"}, {"title": "Prompting Multimodal Large Language Models", "content": "Inspired by the ability of GPT-4V [41] to process diverse inputs, ViP-LLaVA [5] collects a visual prompt instruction dataset containing various visual prompts, e.g., scribbles and arrows, for MLLM fine-tuning. Contemporary to our work, [23] has offered advanced insights in prompting MLLMs through external knowledge, which introduces bounding box and OCR coordinates into text prompt, however, it's still challenging to interpret the pixel-level contexts. In this paper, we investigate how to efficiently utilize external knowledge to enhance multimodal fine-grained alignment of MLLMs and introduce a novel visual prompt paradigm incorporating pixel-level contextual information."}, {"title": "3 Proposed Method", "content": "In this section, we propose a new visual prompt paradigm that integrates local external information to enhance the capability of MLLMs. In section 3.1, we outline the design of the auxiliary visual prompt that contains local contextual information. Using the auxiliary visual prompt, in section 3.2, we further embed it into MLLMs by merging it with the original visual tokens. Finally, we briefly introduce the details of training in section 3.3."}, {"title": "3.1 Auxiliary Visual Prompt with External Knowledge", "content": "In this section, we propose a method to generate local contextual external knowledge to assist MLLMs. In contrast to [23], which focuses solely on object detection and OCR information and integrates them as part of the text prompt, we enhance the granularity of local external knowledge by leveraging a panoptic segmentation model. Additionally, we continue to utilize an OCR model but transform both types of external knowledge into pixel-wise embeddings. Further details are provided below.\nAs shown in Figure 2, given the input image \\(I \\in \\mathbb{R}^{3 \\times H \\times W}\\), we can obtain the fine-grained external knowledge by an off-the-shelf panoptic segmentation model [60] and an OCR model [15]. The generation of the external knowledge can be expressed as:\n\\[\\begin{aligned} \\left\\{M_{j}, C_{j}\\right\\}_{j=1}^{N_{s}} &=f_{\\text {seg}}(I), \\\\ \\left\\{B_{j}, T_{j}\\right\\}_{j=1}^{N_{o}} &=f_{\\text {ocr}}(I), \\end{aligned}\\]\nwhere \\(f_{\\text {seg}}(\\cdot)\\) and \\(f_{\\text {ocr}}(\\cdot)\\) mean panoptic segmentation and optical character recognition (OCR) models, \\(N_{s}\\) and \\(N_{o}\\) are the numbers of detected mask regions and OCR bounding boxes. \\(\\left\\{M_{j}, C_{j}\\right\\}_{j=1}^{N_{s}}\\) is the set of mask regions and the corresponding classes, and \\(\\left\\{B_{j}, T_{j}\\right\\}_{j=1}^{N_{o}}\\) represents the set of detected OCR bounding boxes and texts.\nWith the detected classes \\(\\left\\{C_{j}\\right\\}_{j=1}^{N_{s}}\\) and OCR texts \\(\\left\\{T_{j}\\right\\}_{j=1}^{N_{o}}\\), a pre-trained text encoder (ftext(\u00b7)) is leveraged to generate the texture embeddings as:\n\\[\\begin{aligned} T_{s} &=\\left\\{t_{1}, \\ldots, t_{N_{s}}\\right\\}=\\left\\{f_{\\text {text}}\\left(C_{1}\\right), \\ldots, f_{\\text {text }}\\left(C_{N_{s}}\\right)\\right\\}, \\\\ T_{o} &=\\left\\{t_{1}^{\\prime}, \\ldots, t_{N_{o}}^{\\prime}\\right\\}=\\left\\{f_{\\text {text}}\\left(T_{1}\\right), \\ldots, f_{\\text {text}}\\left(T_{N_{o}}\\right)\\right\\}, \\end{aligned}\\]\nwhere \\(t_{i} \\in \\mathbb{R}^{1 \\times d}(1 \\leq i \\leq N_{s})\\) and \\(t_{i}^{\\prime} \\in \\mathbb{R}^{1 \\times d}(1 \\leq i \\leq N_{o})\\) denote the ith textual embedding vector of the classes for the detected mask region and OCR texts respectively, while d is the embedding dimension.\nIn order to generate a pixel-wise visual prompt for the external knowledge instead of a pure text description for the regions with coordinates and category names, the auxiliary visual prompt is initialized as a zero tensor \\(P \\in \\mathbb{R}^{H \\times W \\times d}\\) and then filled with the newly generated texture embeddings for the external knowledge as:\n\\[P_{j, k}=\\left\\{\\begin{array}{ll} P_{j, k} \\oplus\\left\\{t_{u}\\right\\} & \\text { if }(j, k) \\in M_{u} \\\\ \\mathbf{0} & \\text { otherwise } \\end{array} \\quad \\forall u \\in\\left\\{1, \\ldots, N_{s}\\right\\},\\right.\\]\n\\[P_{j, k}=P_{j, k}+\\left\\{\\begin{array}{ll} t_{v} & \\text { if }(j, k) \\in B_{v} \\\\ \\mathbf{0} & \\text { otherwise } \\end{array} \\quad \\forall v \\in\\left\\{1, \\ldots, N_{o}\\right\\} .\\right.\\]"}, {"title": "3.2 Visual Prompt Infusion", "content": "In this section, we introduce the visual prompt infusion that incorporates the proposed auxiliary visual prompts into the MLLMs. Previous methods [23] choose to append the external knowledge (embeddings for object category and its coordinates) to the text prompts, which requires the model to learn the correspondence of visual content within the specified coordinates encoded in the external knowledge and, as a result, increasing the difficulties of the learning process of the model. To address this challenge, we propose to merge the auxiliary visual prompt directly with the image features in a pixel-wise manner.\nSpecifically, as shown in Figure 3, the image tokens are first generated via an image encoder \\(f_{i m g}(\\cdot)\\) and an MLP projector (fMLP(\u00b7)):\n\\[F_{v}=f_{M L P}\\left(f_{i m g}(I)\\right),\\]\nwhere \\(F_{v} \\in \\mathbb{R}^{N_{v} \\times d_{v}}\\), \\(N_{v}\\) and \\(d_{v}\\) represent the number of image tokens and the embedding dimension. Then, the auxiliary visual prompt is further processed by a prompt embedding network (PEN) as\n\\[F_{p}=f_{P E N}(P).\\]\nFor the prompt embedding network, we employ three convolutional layers, with an activation layer (ReLU) inserted between each pair of them. This network primarily serves to align the feature space and spatial size between the image tokens and the auxiliary visual prompts.\nWhen combining the image tokens and the processed auxiliary visual prompt, we mainly consider two options, both of which operate pixel-wise. (1) feature fusion: \\(F_{t}=f\\left(\\text { Concat }\\left(F_{v}, F_{p}\\right)\\right)\\), where f is a linear layer that maps the embedding \\(\\mathbb{R}^{N_{v} \\times d_{2 v}} \\rightarrow \\mathbb{R}^{N_{v} \\times d_{v}}\\) to maintain the total number of image tokens unchanged; (2) feature addition, \\(F_{t}=F_{v}+F_{p}\\), which sums the two types of features directly.\nThe advantages of the pixel-wise infusion for both options facilitate the model's comprehension of the correspondence between external knowledge and original visual features. This explicit guidance enables the model to easily understand the pixel categories as well as the potential OCR text description it conveys. Consequently, it aids the model in disambiguating complex scenes, accentuating salient features, and distinguishing finer objects."}, {"title": "3.3 Training", "content": "Training MLLMs involves predicting responses based on multimodal inputs using an autoregressive approach. The objective is to maximize the probability of generating tokens that match the ground-truth answer \\(Y_{a}\\). With the new visual embedding \\(F_{v}\\), this can be mathematically expressed as follows:\n\\[P\\left(Y_{a} \\mid F_{v}, F_{t}\\right)=\\prod_{i=1}^{L} P_{\\Theta}\\left(Y_{i} \\mid F_{v}, F_{t}, Y_{a,<i}\\right).\\]\nHere, L represents the sequence length of the ground truth answer \\(Y_{a}\\), \u0398 means the trainable parameters. \\(Y_{a,<i}\\) represents all the answer tokens preceding the current prediction token xi, where i denotes the step in the sequence of text token generation. \\(F_{t} \\in \\mathbb{R}^{N_{t} \\times d_{t}\\) is the token embedding of the input question, \\(N_{t}\\) and \\(d_{t}\\) denote the number of text tokens and token embedding dimension. By infusing these enriched visual cues into the training pipeline, MLLMs can develop a more"}, {"title": "4 Experiment", "content": "In this section, we conduct a comprehensive comparison of our method with existing state-of-the-art (SOTA) multimodal models. Additionally, we perform a series of ablation studies to further validate the proposed method. Finally, we provide visualization examples for in-depth analysis.\nModels. For the vision encoder, we adopt SigLIP-384px [59] for experiments. We leverage Phi-2-2.7B [39] and Vicuna-7B [8] model as the language decoder. For the multimodal projector, same as LLaVA [34], we adopt a two-layer MLP. We use OpenSeed [60] and PaddleOCRv2 [15] to generate the per-pixel externally knowledge for pixel class and OCR text, and leverage UAE-Large-V1 [27] to extract the textual embedding.\nTraining Setting. We fine-tune the models on LLaVA-Instruct-150K dataset [34] using LORA [21] for 1 epoch, at a learning rate of 2e-4 and a batch size of 256 on 32 \u00d7 V100 32GB GPUs. For the setting of LORA, we set LoRA rank to be 128 and LoRA's hyperparameter \u03b1 as 256. Note that we fix all the weights of pre-trained modules, i.e., vision encoder, language encoder and MLP, during training. Our models' weights are initialized from Mipha-3B [65] and LLava-7B [34].\nBenchmarks and Baselines. We evaluate our approach using 9 popular benchmarks to comprehensively assess its multimodal capabilities. These benchmarks include: VQA-v2 test-dev split [18], GQA test-dev-balanced split [22], ScienceQA-IMG test split [38], MME perception [16], \u039c\u039c\u0395 cognition [16], MMBench test split [37], MM-Vet test split [58], TextVQA [52], and POPE [28].\nWe compare our results with a bunch of state-of-the-art multimodal large language models (MLLMs): BLIP-2 [26], InstructBLIP [13], Shikra-13B [7], IDEFICS80/9B [24], Qwen-VL [3], mPLUG-Owl2 [57], LLaVA-v1.5-13/7B [34], LAF-7B [23], and multimodal small language models (MSLMs) [65]: MobileVLM [11], LLaVA-Phi [66], MC-LLaVA [1], Imp-v1 [54], MoE-LLaVA-3.6B [29], TinyLLaVA-share-Sig-Phi [63], Bunny [20] and Mipha [65]."}, {"title": "4.1 Ablation Studies", "content": "In this section, we conduct an ablation study to assess the effectiveness of the proposed approach. By default, the experiments are conducted using Mipha-3B [65] with Phi-2 [39] as the language backbone unless otherwise specified."}, {"title": "Prompting MLLMs with Different Approaches", "content": "In Table 1, we present the results of the ablation study for four different prompting strategies: (1) Mihpa-3B baselines with vanilla text prompt, as used by LLaVA-1.5 [34]. (2) Mihpa-3B + LAF proposed in [23] that appends externel local contextual knowledge to the text prompts. (3) The proposed auxiliary visual prompt inserted via feature fusion. (4) The proposed auxiliary visual prompt added via feature addition.\nFrom Table 1, we note that compared to the baseline (1) with vanilla prompts, both proposed fusion strategies (3) and (4) exhibit a significant improvement. This suggests that external knowledge is indeed beneficial in enhancing the capabilities of MLLMs. In comparison to Mihpa-3B+LAF (2), which inserts external local contextual knowledge into the text prompt, (4) outperforms it in 8 out of 9 benchmarks, notably for GQA [22] and MME-P [16]. This implies that explicitly linking external local knowledge to the original visual features reduces the model's learning burden in establishing spatial relationships, consequently enhancing performance. Furthermore, we empirically observe that directly adding auxiliary visual prompts yields slightly better results than concatenation. Therefore, we adopt feature addition as our default setting for subsequent experiments."}, {"title": "The Effect of Using Different Vision Encoders", "content": "In Table 2, we further ablate the effectiveness brought by different vision encoders, i.e., CLIP [48] v.s. SigLIP [59]. From the results, we can draw two conclusions. First, for both vision encoders, our methods have consistent improvement compared"}, {"title": "4.2 Main Results", "content": "In Table 5, we compare our methods with other state-of-the-art (SOTA) models. We divide the table into sections for language models smaller than 3B and those beyond 7B to provide a clearer comparison. From the results, we observe that our model achieves the best performance on 7 out of 9 benchmarks for larger language models (>7B) and attains the highest accuracy on 7 out of 9 benchmarks for relatively smaller language models (<3B). Note that, in Table 5, some models, e.g. Shikra-13B [7], Qwen-VL [3], are trained with million or billion level data, while our model is only trained on the dataset used by LLaVA-1.5 without any extra data for neither pre-training nor fine-tuning, which highlights the exceptional multimodal understanding and reasoning capabilities of our models. In addition, on top of the LLaVA-1.5 framework, our approach can bring more remarkable and consistent improvement on all benchmarks compared with LAF [23]. It justifies the proposed infusion strategy, which involves inserting external knowledge in a pixel-wise manner directly into the visual features, as being more effective than appending it to the text prompt [23]."}, {"title": "4.3 Quantitative Result Analysis", "content": "We present visualization results in Table 3 and 6 to further illustrate the improvement of our model in terms of both global image understanding and local object and text recognition. Table 3 demonstrates that compared to LLaVA-1.5 7B [34], our approach generates more detailed and contextually relevant responses, e.g., \"The cat's calm demeanor in the face of the women's playful behavior\" for the left"}, {"title": "5 Limitations and Broader Impact", "content": "Our method relies on pre-trained models for panoptic segmentation and OCR detection in a zero-shot manner. The performance of these models will significantly impact the performance of our proposed method, particularly when there is a substantial domain gap between the images from specific benchmarks and the training set of the segmentation or OCR models. While the proposed approach holds promise for significantly enhancing the cognitive capabilities of multimodal models and may inspire new methodologies and techniques in the development of robust multimodal AI systems, users must be aware of potential negative societal impacts. For instance, biases may manifest in various forms; for example, biased responses may be generated by the model if the training data of MLLMs, panoptic segmentation, and OCR detection models contain certain biases."}, {"title": "6 Conclusion", "content": "In this paper, we have proposed a method for leveraging external knowledge, such as localized contextual information, to enhance the capabilities of multimodal language models (MLLMs). To accomplish this objective, we propose extracting pixel-wise contextual information using a panoptic segmentation and OCR model, and then directly integrate this with the visual features. This enables the model to better understand both fine-grained objects and the overall global image context. Experimental results from ablations and comparisons with state-of-the-art methods demonstrate the effectiveness of our approach. We hope this paper can shed light on the importance of external knowledge for MLLMs and an effective way to leverage such knowledge."}, {"title": "A Appendix", "content": "In the supplementary materials, we provide the following sections:\n(a) More implementation details in Section B.\n(b) Ablation study experiments in Section C.\n(c) Visualization result analysis in Section D."}, {"title": "B Implementation Details", "content": "The training time for LLaVA-1.5 7B [34] and Mipha-3B [65] is approximately 14 hours and 9 hours, respectively, with a batch size of 256 on 32 \u00d7 NVIDIA V100 32GB GPUs. For the initialization of the proposed prompt embedding network (PEN), we use Kaiming initialization technology [19]. The UAE-Large-V12 model is adopted as the pre-trained textual encoder to extract textual embeddings for the visual prompt."}, {"title": "C Ablation Study", "content": "Next, we conduct more ablation study experiments to provide deeper insight into the components of our proposed approach."}, {"title": "The Effect of Using Different Pre-trained Textual Encoders", "content": "In Table 7, we perform an ablation study using different textual encoders, i.e., CLIP [48] vs. UAE [27], to extract textual embeddings for the proposed visual prompt. We draw two conclusions from Table 7: (1) Using different textual encoders, the proposed approach consistently outperforms the baseline, demonstrating the robustness of our method. (2) Adopting UAE as the pre-trained textual encoder achieves significantly better performance. Therefore, we choose UAE as the default pre-trained textual encoder in our experiments."}, {"title": "Object Detector v.s. Segmentation Model", "content": "To determine the effect of using an object detector or segmentation model to incorporate pixel-level semantics into the proposed visual prompt, we conduct an ablation study with the popular object detector GroundingDINO [36] and the segmentation model OpenSeed [60]. The results are shown in Table 8. We observe that both GroundingDINO and OpenSeed significantly boost performance across all benchmarks. However, utilizing OpenSeed achieves better performance gains due to its fine-grained mask regions. Thus, we adopt OpenSeed by default to generate object regions."}, {"title": "The Effect of Fine-Tuning with the Visual Prompt", "content": "As displayed in Table 9, the model fine-tuned with the proposed visual prompt (i.e., the third row) achieves remarkably better performance than the one fine-tuned without our visual prompt (i.e., the second row) across all benchmarks. Specifically, without using our visual prompt for fine-tuning, the model even shows performance degradation on Text-VQA benchmark [52] and has negligible gains on Science-QA [38], VQAv2 [18], MME-P [16], and MME-C [16] benchmarks. All these results demonstrate the superiority of the proposed method."}]}