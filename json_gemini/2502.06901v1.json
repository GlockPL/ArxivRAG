{"title": "Enabling Autoregressive Models to Fill In Masked Tokens", "authors": ["Daniel Israel", "Aditya Grover", "Guy Van den Broeck"], "abstract": "Historically, LLMs have been trained using either autoregressive (AR) or masked language model-ing (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.", "sections": [{"title": "1. Introduction", "content": "The field of natural language processing (NLP) has witnessed remarkable advancements in recent years, largely driven by the advent of large language models (LLMs) built upon the Transformer architecture. These models, characterized by their self-attention mechanisms and vast parameter counts, have demonstrated unprecedented capabilities in understanding and generating human-like text.\nA critical aspect of LLM training lies in the choice of pre-training objective. Traditionally, two dominant paradigms have emerged: autoregressive (AR) and masked language modeling (MLM). AR models, such as GPT, are trained to predict the next token in a sequence, given the preceding context. This left-to-right approach, coupled with causal masking that prevents the model from \"seeing\" future tokens, enables efficient training and inference. MLM models, exemplified by BERT, are trained to predict masked-out tokens in a sequence, leveraging bidirectional context from both past and future tokens.\nOne notable capability where AR models typically fall short is text infilling, the task of predicting missing tokens within a given text span, surrounded by both preceding and subsequent context. While MLM models inherently support infilling due to their bidirectional nature, AR models, with their unidirectional processing, cannot leverage future context for this task. This limitation restricts the applicability of AR models in scenarios where infilling is essential, such as interactive text editing, code completion, and structured generation.\nDespite the limitations of AR models in handling text infilling, they remain the dominant paradigm for large-scale language modeling due to their superior scalability. AR models benefit from several key advantages that make them more efficient during both training and inference. First, AR models can exploit causal masking to parallelize every next token prediction, enabling faster training on massive datasets across multiple GPUs. This differs from MLM models, which only make predictions for a fixed ratio of masked tokens during training, such as 15 percent in BERT. Second, the sequential nature of AR models allows for the use of KV caching at inference time, which significantly reduces the computational cost of attention operations by reusing previously computed embeddings. Significant effort has been dedicated to optimizing the memory and speed of KV caching. Thus, AR models are better suited for real-time applications, such as chatbots and virtual assistants, where low-latency responses are critical. These factors contribute to the widespread adoption of AR models in industry and academia, despite their inherent limitations for infilling.\nResearchers have explored non-autoregressive paradigms that support text infilling. One such approach is discrete"}, {"title": "2. Related Works", "content": "Discrete diffusion models have emerged as a promising alternative to traditional autoregressive models for text generation and, notably, text infilling. Inspired by the success of diffusion models in continuous domains like image generation, these models adapt the diffusion framework to operate on discrete sequences of tokens. In the context of text infilling, discrete diffusion offers several advantages. Its iterative refinement process allows for fine-grained control over the generated text and the ability to tradeoff quality for efficiency. However, as mentioned in the introduction, these models can be computationally expensive during inference due to the multiple refinement steps and the lack of KV caching. They also face challenges in scaling up training compared to autoregressive models. In this paper, we will primarily focus on the work of Scaling Masked Diffusion Model (SMDM) and DiffuLlama, but the space includes many promising works.\nAR models can be adapted to perform infilling through a special training process called Fill-in-the-Middle (FIM), in which the order of the original sequence is changed such that the middle of the sequence is moved to the end and marked with a special FIM token. These FIM models are particularly useful for coding applications. We make a distinction between FIM and masked infilling. FIM necessitates that the infilled text is a contiguous block, while masked infilling can fill in arbitrary sequences of tokens.\nNotable works to unify MLM and AR modelling include BART. Besides architectural differences, the main distinction between MARIA and BART is that MARIA is applied to existing pretrained MLM and AR models, while BART must be trained end-to-end. Other notable works incorporate together MLM and AR modeling techniques for improved training, but none are expressly targeting masked infilling as an application."}, {"title": "3. Method", "content": "Consider an autoregressive model $\\pi_{AR}$ and masked language model $\\pi_{MLM}$. Given access to a dataset $D = {X_1, X_2, ...}$, autoregressive models are trained to maximize the joint likelihood given by"}, {"title": "MARIA", "content": "The MARIA architecture can be defined very straightforwardly with a linear layer on the concatenated hidden states of an AR and MLM model.\n$\\pi_{MARIA}(x) = \\sigma(W_3 [h_1(x); h_2(x)])$\nwhere $W_3 \\in R^{(d_1+d_2)\\times v}$. Finally, we may now define an objective that is both autoregressive and masked. Let $c(i,m) = {x\\_{<i}, x\\_{>i} \\setminus m}$ define the union of tokens before the index i and all unmasked tokens after i.\n$L_{MARIA} = -E_{x\\sim D}\\sum_{m\\sim M}\\sum_{i\\in m}log \\pi_{MARIA}(x_i | c(i,m))$\nThis objective defines the expected negative log likelihood of an autoregressive distribution conditioned on unmasked tokens.\nMARIA training can be parallelized in a similar manner as a typical autoregressive Transformer. For a clean input sequence $X\\_{1:n}$, we consider its masked counter part $M\\_{1:n}$. The AR model receives the clean inputs and the MLM model receives the masked inputs such that we compute the hidden state $[h_1(X); h_2(M)]\\_{1:n}$ concatenated on the sequence dimension. These are then decoded with $W_3$ to next token logits. Thus, the autoregressive loss is computed over the entire sequence in parallel. This training procedure is best depicted by Figure 1.\nAs part of our method, we also provide a way to initialize the newly defined MARIA weights $W_3$. Because we have access to existing weights of pretrained models, namely an autoregressive weights $W_1$ and masked weights $W_2$, we can initialize $W_3$\n$W_3 \\approx [W_1/2; W_2/2]$\nObserve that this will output the average of the logits of $\\pi\\_{AR}$ and $\\pi\\_{MLM}$\n$\\pi_{MARIA}(x |\\cdot) = \\sigma([W_1/2; W_2/2] [h_1(x); h_2(x)]) = \\sigma((\\pi\\_{AR}(x |\\cdot) + \\pi\\_{MLM}(x |\\cdot))/2)$"}, {"title": "Initialization", "content": "As part of our method, we also provide a way to initialize the newly defined MARIA weights $W_3$. Because we have access to existing weights of pretrained models, namely an autoregressive weights $W_1$ and masked weights $W_2$, we can initialize $W_3$\n$W_3 \\approx [W_1/2; W_2/2]$\nObserve that this will output the average of the logits of $\\pi_{AR}$ and $\\pi_{MLM}$\n$\\pi_{MARIA}(x |\\cdot) = \\sigma([W_1/2; W_2/2] [h_1(x); h_2(x)])\\= \\sigma((\\pi_{AR}(x |\\cdot) + \\pi_{MLM}(x |\\cdot))/2)$ \nThis is a good initialization because the average of logits corresponds to a multiplicative mixture of the two original distributions. This ensemble, known as product of experts , has proven effective in the context of LLMs . Smart weight initialization leads to faster and better convergence , and we demonstrate this with \"product initialization\" for MARIA in Figure 2."}, {"title": "Unconditional Generative Model", "content": "While MARIA is trained to sample conditionally, we propose a method to sample unconditionally. The desideratum of this generative model is to make possible iterative refinement of text such that more compute leads to better samples. Discrete diffusion has this property for the number of denoising steps, and while it is possible to use MARIA directly as a discrete diffusion model, it is undesirable because autoregressive sampling at each times step is slow, and discrete diffusion only unmasks a small number of tokens at a time, remasking most samples at every iteration. Thus, we propose using MARIA as a generative model with an inference strategy inspired by simulated annealing . We can describe the process as follows:\n1. Sample from the base AR model at temperature 1.\n2. Using MARIA, resample a fixed percentage of tokens autoregressively at temperature T.\n3. Repeat the process for some number of iterations, annealing T from 1 to 0.\nThis inference strategy is a way to optimize over the joint likelihood of a sequence, and it is an improvement over standard greedy sampling because it is non-myopic. More formally, we are sampling from the following distribution:\n$p(x^t) \\propto \\sum_{m}\\prod_{i}\\pi_{MARIA}(x^i_t| x^i_{t-1}, ...)\nwhere $x^i_t$ is the sequence at step i, tj is a temperature at step j, mk is a mask at step k, and $\\pi_{MARIA}(;t)$ denotes the autoregressive MARIA distribution temperature scaled by t."}, {"title": "Implementation", "content": "A key constraint of MARIA is that the combined AR and MLM models must be trained with the same tokenizer. We make use of two important open-source works that are both trained with a GPT2 based tokenizer: ModernBERT and OLMO. We train two models:\n\\"}]}