{"title": "GPTDrawer: Enhancing Visual Synthesis through ChatGPT", "authors": ["Kun Li", "Xinwei Chen", "Tianyou Song", "Hansong Zhang", "Wenzhe Zhang", "Qing Shan"], "abstract": "In the burgeoning field of AI-driven image generation, the quest for precision and relevance in response to textual prompts remains paramount. This paper introduces GPT-Drawer, an innovative pipeline that leverages the generative prowess of GPT-based models to enhance the visual synthesis process. Our methodology employs a novel algorithm that iteratively refines input prompts using keyword extraction, semantic analysis, and image-text congruence evaluation. By integrating ChatGPT for natural language processing and Stable Diffusion for image generation, GPT-Drawer produces a batch of images that undergo successive refinement cycles, guided by cosine similarity metrics until a threshold of semantic alignment is attained. The results demonstrate a marked improvement in the fidelity of images generated in accordance with user-defined prompts, showcasing the system's ability to interpret and visualize complex semantic constructs. The implications of this work extend to various applications, from creative arts to design automation, setting a new benchmark for Al-assisted creative processes.", "sections": [{"title": "1. Introduction", "content": "Stable Diffusion (SD), as delineated by Rombach et al. [15], represents a paradigm shift in the domain of latent diffusion models. It's premised on the ability to transmute text-based prompts into high-resolution visual constructs. These prompts act not just as triggers but as foundational descriptors delineating the contours of the visual content the model generates.\nRecent advancements in text-to-image models have been significant, as highlighted in various studies [20]. Prominent among these are Dall-E, Imagen, and Midjourney. However, Stable Diffusion has garnered considerable attention, particularly due to its open-source community support. Notable contributions such as ControlNet [21] and LoRA [7] have provided built-in extensions to Stable Diffusion, enhancing its user-friendliness and accessibility.\nHowever, our preliminary experiments have highlighted a potential limitation in the SD model. As the complexity and granularity of textual descriptions escalate, the model occasionally manifests a decrement in its efficacy, failing to produce visualizations that congruently mirror the descriptive intricacies of the provided text.\nSimultaneously, there's been a surge in advancements in the domain of natural language processing, epitomized by models such as ChatGPT. Its prowess in comprehending, interpreting, and generating refined text offers a tantalizing proposition: Can ChatGPT be harnessed as an auxiliary tool, a \"prompt architect\", to enhance the congruence between intricate textual prompts and the visual outputs of the Stable Diffusion model?\nIn the ensuing discourse, this report seeks to elucidate the potential of amalgamating ChatGPT's linguistic capabilities with the visual generation potential of SD. To be specific, We introduce 'GPTDrawer', a pipeline designed to create images matching complex textual descriptions of scenes. ChatGPT serves as an intermediary to refine textual prompts, optimizing them for SD's processing framework. To evaluate image quality, we utilize the Vision-Language model BLIP [10] for quantitative similarity assessments. We iteratively refine the prompt for SD if any keywords fail to pass the similarity check of the BLIP. The methodology is detailed in Section 3. Our experiments, presented in Section 4, demonstrate GPTDrawer's superior performance over the baseline in both qualitative and quantitative measures."}, {"title": "2. Related Works", "content": "In this section, we discuss the related works of current prompt enhancement methods (Sec. 2.1) and Generative Model (Sec. 2.2)."}, {"title": "2.1. Prompt Enhancement", "content": "The concept of prompt enhancement in the field of artificial intelligence has garnered significant attention, particularly in improving the interaction between humans and AI systems [11, 12, 18]. This area of study focuses on refining the input prompts provided to AI models to enhance the accuracy, relevance, and creativity of the generated outputs. In the context of language models and text-to-image synthesis, the precision and contextual richness of the input prompts are crucial determinants of the quality and applicability of the outputs. The integration of AI in creative processes, like text-to-image synthesis, has opened new avenues for prompt enhancement research. The challenge lies in translating abstract or subjective textual descriptions into visually coherent images. Studies such as Radford et.al. have made strides in this area, proposing algorithms that effectively bridge the gap between textual input and visual output [14]. These algorithms often rely on intricate processes of keyword extraction, context analysis, and iterative refinement to ensure that the generated images align closely with the user's intent."}, {"title": "2.2. Generative Model", "content": "Generative models represent a cornerstone of contemporary Al research, particularly in the realm of automated content creation [1, 4-6, 8, 13, 16, 17, 19]. These models are designed to produce new data instances that are indistinguishable from real data, spanning domains from text generation to image synthesis. The landscape of generative models witnessed a paradigm shift with the introduction of models like GPT-3 by OpenAI and Google's BERT for natural language understanding and generation [2, 3, 9]. These models, based on transformer architectures, have redefined the benchmarks for text generation in terms of fluency, coherence, and context awareness. Their applications extend beyond mere text generation, impacting areas such as conversational AI, content creation, and even coding. In the realm of image synthesis, the emergence of models like DALL-E and Stable Diffusion marks a new era [15, 20]. These models, capable of generating high-fidelity images from textual descriptions, have opened up unprecedented possibilities in the field of creative arts and beyond. The research by Rombach demonstrates the potential of these models in artistic creation [15]."}, {"title": "3. Methodology", "content": "In this section, we discuss our method to refine the Stable Diffusion."}, {"title": "3.1. Overview", "content": "In the proposed workflow delineated in Algorithm 1 and Figure 2, the system commences by ingesting an input prompt, denoted as P. This input is subject to a rigorous extraction procedure executed by the language model, ChatGPT, which distills an array of keywords, symbolized as w. These keywords serve as the cornerstone for the Stable Diffusion algorithm, which synthesizes a batch of preliminary visual representations, designated as IB.\nEach visual output within the batch IB is subsequently encoded through an Image Encoder, expressed as $E_{image}(I_B)$. Concurrently, the keyword array w undergoes a similar encoding process to yield $E_{text}(w)$. The crux of the process involves computing a cosine similarity measure, $Sim_{cos}$, to quantify the alignment between the encoded representations of the visual outputs and the keywords.\nThe iterative refinement cycle is triggered if the similarity metric for any keyword fails to surpass a pre-established threshold, T. During this phase, ChatGPT is tasked with refining both the prompt P and the individual keywords $w_i$. This refinement is informed by the discrepancies identified through the similarity assessment. After prompt adjustment, the Stable Diffusion model generates a new set of images, $I'_B$, which are then re-assessed for conformity to the refined keywords.\nThis recursive process is iterated until the similarity for all keywords meets or exceeds the threshold, at which point the process concludes. The culmination of this methodology is the generation of a set of refined images IB, each associated with a maximal cosine similarity measure, $Sim_{cos}$. The outputs are thus ensured to be in close adherence to the semantic intent encapsulated within the original input prompt."}, {"title": "3.2. Input Prompt Reception", "content": "At the commencement of the process, the system stands in a state of readiness to receive an input prompt from the user. This stage is critical as it defines the initial condition from which the entire generation process unfolds. We can represent the initial prompt as a variable, denoted as $P_0$, which encapsulates the user's input in a textual format. Formally, the input prompt reception can be described as follows:\n$P_0 = UserInput \\qquad (1)$\nHere, $P_0$ signifies the initial prompt provided by the user, serving as the foundational input for the subsequent stages of the pipeline. The function $UserInput()$ symbolizes the act of receiving input from the user, encapsulating the user's intent, context, and requirements in a format interpretable by the system. This initial prompt is then processed through the pipeline to extract relevant keywords and semantic meanings, setting the stage for the generation of the initial set of images."}, {"title": "3.3. Keyword Extraction", "content": "Upon receiving the initial prompt $P_0$, the system proceeds to the keyword extraction phase. This phase aims to distill the most essential and semantically significant terms from the input. Utilizing advanced natural language processing (NLP) techniques, the system analyzes $P_0$ and segments it into a set of key terms, denoted as W.\nThe extraction process can be mathematically represented as:\n$W = ExtractKeywords(P_0) \\qquad (2)$\nHere, $ExtractKeywords(\\cdot)$ is a function embodying the ChatGPT algorithms used for keyword extraction. Simultaneously, the system filters out stop words and other irrelevant terms to ensure the purity and relevance of the extracted keywords W. This filtration process enhances the precision of the subsequent image generation stages, ensuring that the generated content is closely aligned with the user's original intent encapsulated in $P_0$."}, {"title": "3.4. Image Generation", "content": "Following the extraction of keywords W from the initial prompt $P_0$, the subsequent critical phase in the pipeline is the generation of images. This stage entails the transformation of the textual descriptors encapsulated in W into their corresponding visual representations. A generative model, such as Stable Diffusion, is employed to synthesize images grounded in the textual descriptions.\nThe image generation process can be mathematically represented as:\n$I = StableDiffusion(W)$\nHere, I denotes the set of generated images, and $StableDiffusion(\\cdot)$ symbolizes the operation of the generative model. This function ingests the extracted keywords W and outputs a collection of initial images. Each image within this collection, I, is then subject to an evaluation to gauge its relevance and conformity to the semantic nuances of W."}, {"title": "3.5. BLIP Model Evaluation", "content": ""}, {"title": "3.6. BLIP Evaluation", "content": "The integration of the BLIP (Bootstrapped Language Image Pretraining) model within our image generation framework plays a pivotal role in ensuring semantic congruence between the generated images and the textual prompts. This subsection elucidates the operational specifics and the evaluative function of BLIP in our pipeline.\nOperational Methodology The process initiates with the loading of the generated image, employing the load_image function, which is meticulously calibrated to adhere to a pre-set image_size and tailored for a specific computational device. Following this, the BLIP model, pretrained on the COCO dataset and adapted for the said image_size, is instantiated and configured to the evaluation mode.\nTextual Input Processing In our setup, the BLIP model processes a detailed descriptive sentence alongside a set of concisely curated keywords. These textual inputs represent diverse facets of the intended visual output, ranging from overarching themes to specific elements.\nCosine Similarity Computation The BLIP model's evaluative mechanism is centered around the calculation of cosine similarity between the image and text feature vectors. This is mathematically represented as:\n$Cosine \\ Similarity (E_{img}, E_{text}) = \\frac{E_{img} \\cdot E_{text}}{||E_{img}|| \\times ||E_{text}||} \\qquad (3)$\nwhere $E_{img}$ and $E_{text}$ are the features of encoding image and text, respectively. This metric quantifies the alignment of the image with the individual textual elements, as well as with the comprehensive narrative description."}, {"title": "3.7. Prompt Refinement", "content": "The efficacy of the image generation process in our pipeline is significantly influenced by the precision and relevance of the input prompts. In instances where the cosine similarity score, as evaluated by the BLIP model, is found to be below a predefined threshold T, indicating a low degree of semantic alignment between the generated image and the textual prompt, we initiate a prompt refinement process."}, {"title": "Methodology for Refinement", "content": "The refinement strategy is predicated on the adaptation of the original keywords to more general versions. This approach is rooted in the hypothesis that broader keywords may encapsulate a wider range of visual interpretations, thereby increasing the likelihood of generating an image that aligns more closely with the user's intent. The process involves:\n\u2022 Analyzing the specific keywords that contributed to the low cosine similarity score.\n\u2022 Substituting these keywords with their more generalized counterparts.\n\u2022 Ensuring that the modified prompt retains the essential thematic elements of the original prompt while broadening its interpretative scope.\nThe prompt refinement is inherently iterative. Following the modification of the prompt as we mentioned in algorithm 1:\n1. The revised keywords is fed back into the image generation model.\n2. A new set of images is generated based on the updated prompt.\n3. These images are then re-evaluated for their cosine similarity with the refined prompt."}, {"title": "4. Experiments", "content": "We evaluate the performance of our proposed GPTDrawer over three scenes with detailed descriptions.\nScene I: CyberPunk City In a neon-lit cyberpunk cityscape, towering skyscrapers loom over crowded, bustling streets. Neon signs in various languages flicker, casting a colorful glow on the eclectic mix of pedestrians and street vendors. Cars zip by above, while a large digital billboard displays futuristic advertisements.\nScene II: Fairy Tale Castle High in a fairy-tale realm, a majestic mountain towers, crowned with glittering snow. Cascading waterfalls and crystal-clear streams flow from its heights, nourishing lush valleys below. At its peak, an ancient castle of stone and enchantment stands, overlooking a realm of wonder.\nIn our GPTDrawer framework, we set the cosine similarity score threshold T at 0.2, as detailed in Sections 3.5 and 3.6. If the cosine similarity of any keyword falls below this threshold, we revise the prompt or increase the keyword's weight before initiating another generation cycle. Each generation cycle produces a batch of 16 images.\nWe use the results generated by inputting detailed descriptions directly into Stable Diffusion models as our baseline. Each scene undergoes both qualitative and quantitative evaluations. For the qualitative evaluation, we manually assess whether the generation results align with each keyword identified by ChatGPT. For the quantitative evaluation, we employ BLIP to measure the similarity between the generated images and each keyword or sentence."}, {"title": "4.1. Result on Scene I", "content": "Keywords extracted by ChatGPT: Cyberpunk cityscape, Neon-lit skyscrapers, Crowded streets, Multilingual neon signs, Colorful glow, Eclectic pedestrians, Street vendors, Cars, Futuristic digital billboard, Advertisements.\nTable 2 displays the generation outcomes for Scene I using GPTDrawer. Initially, the generated result failed to incorporate the keyword 'cars'. To address this, we increased the weight of 'cars' to 1.1 in the Stable Diffusion Model. This adjustment led to successful results in the second generation round, satisfying all keyword similarity criteria, with details available in Table 4. In contrast, the baseline results failed to depict 'Eclectic pedestrians' and 'Street vendors', missing these crucial keywords. Conversely, GPTDrawer's final output adhered to all keyword requirements."}, {"title": "4.2. Result on Scene II", "content": "Keywords extracted by ChatGPT: Fairy-tale realm, Mountain, Snow, Waterfalls, Streams, Valleys, Castle, Enchantment, Wonder."}, {"title": "5. Conclusion", "content": "This report has comprehensively explored the integration of ChatGPT's advanced linguistic processing with Stable Diffusion (SD)'s visual generation prowess, culminating in the development of the innovative 'GPTDrawer' pipeline. Our investigations underscored SD's challenges in accurately translating complex textual prompts into congruent visual outputs. GPTDrawer, however, emerges as a groundbreaking solution, leveraging ChatGPT's capabilities to refine these prompts, thereby enhancing SD's effectiveness.\nOur experimental results, as elaborated in Section 4, provide compelling evidence of GPTDrawer's superiority over traditional SD approaches. By employing the Vision-Language model BLIP for quantitative similarity assessments, we established that GPTDrawer not only meets but often surpasses the baseline in both qualitative and quantitative aspects. This is particularly notable in its ability to handle intricate, detailed prompts, where standard SD methods showed limitations.\nFurthermore, the success of GPTDrawer illuminates a promising pathway for future research in latent diffusion models and natural language processing. It exemplifies the potential of symbiotic AI systems where complementary technologies are harnessed to overcome individual limitations, setting a new benchmark in the realm of AI-driven image generation."}]}