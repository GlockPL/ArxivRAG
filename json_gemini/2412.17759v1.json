{"title": "Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Bhargava Kumar", "Amit Agarwal", "Ishan Banerjee", "Srikant Panda", "Tejaswini Kumar"], "abstract": "Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.", "sections": [{"title": "1 Introduction to Multimodal Learning and Large Language Models", "content": "Multimodal learning, a growing field in AI, focuses on integrating and processing multiple data types like text, images, and audio, aiming to replicate human cognition, which naturally combines sensory inputs. This approach enables more robust and intelligent systems compared to single-modality methods.\nLarge language models (LLMs) such as GPT-3, BERT, and T5 excel in text-based tasks like question answering and summarization [36]. However, they struggle with non-text data, driving interest in multimodal large language models (MLLMs) that combine LLMs' language capabilities with computer vision's"}, {"title": "1.1 Multimodal Learning: Foundations and Concepts", "content": "Multimodal learning consists of building models that can process and combine information from various data modalities, such as text, images, audio, and video. This is due to the fact that real-world experiences are inherently multimodal in nature, and the different types of information carried by the various modalities provide a way to understand such complex environments more thoroughly [28].\nMultimodal learning incorporates multiple types of data: texts, images, audio, and video. These create representations, each unique to one modality. Given the diverse nature of various types of data, different methods have traditionally been used to capture their properties. Text, for example, is represented with word embeddings emphasizing meaning and structure [3], while image data most often depend on a convolutional neural network that would extract details from visual scenes. Similarly, audio data is often transformed into spectrograms or mel-frequency cepstral coefficients to capture patterns in time and frequency [41]. A typical pipeline for a large multimodal model (MLLM) is shown in Fig. 1, where inputs are first processed through a modality encoder to unify their representations. These are then refined by an input projector and passed into a Large Language Model (LLM) for deeper alignment and understanding. Finally, the output projector and modality generator transform the model's results into meaningful outputs, enabling tasks like generating multimodal content or translating between data types.\nFusion of modality representations is a key focus in multimodal learning. The widely used methods are early fusion, in which the concatenation or combination"}, {"title": "1.2 Multimodal Large Language Models: Opportunities and Challenges", "content": "Recent advances in LLMs have laid the path for multimodal large language models that combine data across modalities, such as text, images, audio, and video [59]. MLLMs hold the potential to transform various domains by enhancing understanding and representation through a mix of different modalities.\nMLLMs expand the capability of LLMs to wider ranges of tasks beyond traditional text-only models. This class of models is very strong on tasks like image captioning, visual question answering, and text-to-video generation-all requiring an in-depth understanding of language-visual relationships [63].\nThe integration of multi-modal data opens the way for scientific research and domain-specific applications for MLLMs by pushing the boundary. Some critical domains like medical imaging, autonomous driving, geospatial intelligence combine textual, visual, and sensor data to yield more realistic decision-making processes.\nDespite the potential of MLLMs, there are significant challenges in developing them. Among the primary issues is the absence of large-scale, high-quality multimodal datasets [49]. Complex, unbiased data covering the richness of reality is a necessary ingredient to train robust MLLMs[28].\nAnother challenge is the increase in computational demands and complexity in integrating these various modalities. The training and deployment of MLLMs"}, {"title": "2 Multimodal Datasets for Training Specific Needs", "content": "The development of multimodal datasets is essential to advancing MLLMs. These datasets span a wide range of modalities and applications, enabling researchers to train models that can integrate and reason across various data types. Challenges"}, {"title": "Notable Datasets in MM-PT and MM-IT", "content": "LAION-5B: The LAION-5B[43] dataset contains 5.85 billion CLIP-filtered image-text pairs (2.32 billion in English) supporting large-scale multi-modal research with tools for watermark detection and NSFW filtering.\nMS-COCO: The MS-COCO[9] contains over 330K images with five human-written captions each, serving as a benchmark for image recognition, segmentation, and captioning."}, {"title": "3 Multimodal Datasets for Task Specific Applications", "content": "The datasets discussed in this section form the core of designing versatile models that can work on a wide range of tasks, including sentiment analysis, emotion detection, and visual question answering. They form the key building blocks for robust and adaptable multimodal frameworks.\nSlideVQA: SlideVQA [46] expands on single-image VQA datasets by introducing a multi-image framework for complex reasoning tasks. It includes over 2,600 slide decks (52,000+ images) and 14,500+ questions, focusing on multi-hop reasoning and numerical analysis with annotated arithmetic expressions. The dataset presents challenges in real-world document understanding, such as evidence identification and sequence-based reasoning across multiple images."}, {"title": "4 Multimodal Datasets for Domain-Specific Applications", "content": "Recent advancements in large language models have spurred the creation of multimodal datasets tailored to specific domains, complementing existing task-focused datasets. These domain-specific datasets leverage the integration of multiple modalities to address unique challenges across industries.\nMedical Imaging: Domain-specific multimodal datasets, particularly in medical imaging, are instrumental for tasks like diagnosis, treatment planning, and patient monitoring. For instance, the MIMIC-CXR dataset [20] pairs radiological images with clinical reports, enabling models to understand correlations between visual data and medical language. Similarly, PathGen-1.6M [55], an open-source"}, {"title": "Scientific Domain", "content": "Two prominent datasets enhance scientific figure retrieval tasks. SciOL [47], an extensive open-access corpus, supports multimodal models in scientific research, while MuLMS-Img [47] focuses on high-quality image-text pairings in materials science. Both demonstrate significant improvements in figure classification, captioning, and retrieval tasks. Another resource, MMSci [25], compiles data from Nature Communications across 72 disciplines, offering benchmarks for figure captioning and multiple-choice tasks. This dataset reveals substantial model performance gaps, underscoring its potential as a valuable training tool."}, {"title": "Egocentric and Interactive environment", "content": "Recent advances in egocentric video datasets have significantly enhanced research in computer vision and natural language processing, particularly for analyzing everyday activities and interactions. Notable contributions include Ego4D [17], featuring 3,670 hours of first-person videos, and ALFRED [44], which links natural language instructions to household action sequences."}, {"title": "5 Dataset Characteristics and Limitations", "content": "The development of advanced multimodal language models relies heavily on large, diverse, and well-annotated datasets. For example, datasets like MS-COCO [9] offer extensive real-world data distributions, enabling models to learn intricate cross-modal relationships. Broad modality coverage, as seen in Flickr30k [54], and high-quality annotations enhance training and evaluation effectiveness. Practical applicability is supported by datasets like SpaceNet [12], which align models with real-world tasks.\nHowever, challenges persist, including data biases, imbalances, and limited diversity in task representation. Moreover, privacy and security concerns underscore the need for responsible dataset use[28]. Overcoming these challenges through thoughtful design and curation is vital for advancing robust and ethical multimodal learning."}, {"title": "6 Emerging Trends and Future Dataset Needs", "content": "Multimodal learning has seen significant progress, propelled by advancements in large language models (LLMs) and expansive multimodal datasets. A key trend is the creation of increasingly diverse and complex datasets that mirror real-world scenarios, enhancing the field's potential for addressing practical challenges."}, {"title": "Diverse and Geographically Representative Datasets", "content": "Researchers are advancing multimodal datasets by integrating tactile, olfactory, and physiological signals, while prioritizing geographical and linguistic diversity to enhance generalization and mitigate biases in large-scale models [5]."}, {"title": "Complex Interactions and Real-World Applications", "content": "Future multimodal datasets should encompass intricate intermodal interactions while tackling practical applications like healthcare, autonomous systems, and environmental monitoring [26]. To ensure transparency, reproducibility, and ethical use, standardized documentation and benchmarking practices are essential [27]."}, {"title": "7 Conclusion", "content": "Advancement of multimodal learning relies on development of specialized datasets, which we have grouped into three main categories: training needs, task-specific needs, and domain-specific needs. Training datasets, which are vital for pretraining and instruction tuning methods such as SFT and RLHF, act as the backbone for building and refining multimodal systems. The second category includes needs that are task-specific: these are datasets built for specific tasks that make a model perform well for narrow applications. Finally, domain-specific datasets are essential for solving the peculiarities of specific industries and making the model adaptable and context-sensitive. Although challenges are still there, such as data diversity and effective cross-modal integration, emerging trends in data augmentation and cross-modal learning are mellowing these challenges. Therefore, the development and diversification of these categories of datasets will be highly essential to unlocking the potentials of multimodal systems and further innovations in AI."}]}