{"title": "A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models", "authors": ["Shivam Kumar", "Yun Yang", "Lizhen Lin"], "abstract": "In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings.", "sections": [{"title": "1 Introduction", "content": "Conditional distribution estimation provides a principled framework for characterizing the dependence relationship between a response variable Y and predictors X, with the primary goal of estimating the distribution of Y conditional on X through learning the (conditional) data-generating process. Conditional distribution estimation allows one to regress the entire distribution of Y on X, which provides much richer information than the traditional mean regression and plays a central role in various important areas ranging from causal inference (Pearl, 2009; Spirtes, 2010), graphical models (Jordan, 1999; Koller and Friedman, 2009), representation learning (Bengio et al., 2013), dimension reduction (Carreira-Perpin\u00e1n, 1997; Van Der Maaten et al., 2009), to model selection (Claeskens and Hjort, 2008; Ando, 2010). Their applications span across diverse domains such as forecasting (Gneiting and Katzfuss, 2014), biology (Krishnaswamy et al., 2014), energy (Jeon and Taylor, 2012), astronomy (Zhao et al., 2021), and industrial engineering (Simar and Wilson, 2015), among others.\nThere is a rich literature in statistics and machine learning on conditional distribution estimation including both frequentist and Bayesian methods (Hall and Yao, 2005; Norets and Pati, 2017). Traditional methods, however, suffer from the curse of dimensionality and often struggle to adapt to the intricacies of modern data types such as the ones with lower-dimensional manifold structures.\nRecent methodologies that leverage deep generative models have demonstrated significant advancements in complex data generation. Instead of explicitly modeling the data distribution, these approaches implicitly"}, {"title": "1.1 List of contributions", "content": "We briefly summarise the main contributions made in this paper.\n\u2022 To the best of our knowledge, our study is the first attempt to explore the likelihood-based approach for distributional regression using a conditional deep generative model, considering full-dimensional noise and the potential presence of singular underlying support. We provide a solid statistical foundation for the approach by proving the near-optimal convergence rates for this proposed estimator.\n\u2022 We derive the convergence rates for the conditional density estimator of the corrupted data Y with respect to the Hellinger distance and specialize the obtained rate for two popular deep neural network classes: the sparse and fully connected network classes. Furthermore, we characterize the Wasserstein convergence rates for the induced intrinsic conditional distribution estimator on the manifold (i.e., a deconvolution problem). Both rates turn out to depend only on the intrinsic dimension and smoothness of the true conditional distribution.\n\u2022 Our analysis in Corollary 2 suggests the need to inject a small amount of noise into the data when they are sufficiently close to the manifold. Intuitively, this observation validates the underlying structural challenges in related manifold estimation problems with noisy data, as outlined by Genovese et al. (2012)."}, {"title": "1.2 Other relevant literature", "content": "The problem of non-parametric conditional density estimation has been extensively explored in statistical literature. Hall and Yao (2005), Bott and Kohler (2017), and Bilodeau et al. (2023) directly tackle this problem with smoothing and local polynomial-based methods. Fan and Yim (2004) and Efromovich (2007) explore suitably transformed regression problems to address this challenge. Other notable approaches include the nearest neighbor method (Izbicki et al., 2020; Bhattacharya and Gangopadhyay, 1990), basis function expansion (Sugiyama et al., 2010; Izbicki and Lee, 2016), and tree-based boosting (Pospisil and Lee, 2018; Gao and Hastie, 2022), among others.\nIn the context of conditional generation, we highlight recent work by Zhou et al. (2022) and Liu et al. (2021). In Zhou et al. (2022), GANs were employed to investigate conditional density estimation. While this work offers a consistent estimator, it lacks statistical rates or convergence analysis, and its focus is on a low-dimensional setup. In Liu et al. (2021), conditional density estimation supported on a manifold using Wasserstein-GANs was examined. However, their setup does not account for smoothness across either covariates or responses, nor do they address how deep generative models specifically tackle the challenges of high-dimensionality. Moreover, their assumption that the data lies exactly on the manifold can be restrictive. Our study shares some commonalities with the work of Chae et al. (2023), as both investigate sieve maximum likelihood estimators (MLEs). However, the fundamental problems addressed and the methodologies employed differ significantly, and our work involves technical challenges that span multiple scales. While Chae et al. (2023) concentrates exclusively on unconditional distribution estimation, our theoretical analysis necessitates much more nuanced techniques due to the conditional nature of our setup. This shift is noteworthy because it demands a more refined analysis of entropy bounds, considering two potential sources of smoothness across the regressor and the response variables. Furthermore, our setting accommodates the possibility of an infinite number of x values, which gives rise to a dynamic manifold structure, further compounding the intricacy of the problem at hand."}, {"title": "2 Conditional deep generative models for distribution regression", "content": "We consider the following probabilistic conditional generative model, where for a given predictor value x, the response Y is generated by\n$$Y|X = VX + \\varepsilon,$$\n$$Y = G^*(Z, x) + \\varepsilon, \\quad X \\in \\mathcal{X} \\subset \\mathbb{R}^p.$$\nHere, $G^*(\\cdot, x): \\mathcal{Z} \\to \\mathcal{M}_x$ is the unknown generator function, $\\mathcal{Z}$ a latent variable with a known distribution $P_{\\mathcal{Z}}$ and support $\\mathcal{Z} \\subset \\mathbb{R}^q$ independent of the predictor $X$. The existence of the generator $G$ directly follows from Noise Outsourcing Lemma 3. This lemma enables the transfer of randomness into the covariate and an orthogonal (independent) component through a generating function for any regression response. We denote $\\mathcal{M} := \\bigcup_{x \\in \\mathcal{X}} \\mathcal{M}_x \\subset \\mathbb{R}^D$ as the support of the image of $G^*(\\mathcal{Z}, \\mathcal{X})$ such as a (union of) d-dimensional manifold.\nWe model $G^*(\\cdot, \\cdot): \\mathcal{Z} \\times \\mathcal{X} \\subset \\mathbb{R}^q \\times \\mathbb{R}^p \\to \\mathcal{Y} \\subset \\mathbb{R}^D$ using a deep neural network, leading to a conditional deep generative model for (2).\nIn the next section, we present a more general result in terms of the entropy bound (variance) for the true function class of $G$ and the approximability (bias) of the search class. We then proceed to a simplified understanding in the context of conditional deep generative models in subsequent sections."}, {"title": "2.1 Convergence rates of the Sieve MLE", "content": "In light of equation (2), it is evident that the distribution of $Y|X = x$ results from the convolution of two distinct distributions: the pushforward of $\\mathcal{Z}$ through $G^*$ with $X = x$, and $\\varepsilon$ following an independent D-dimensional normal distribution. The density corresponding to the true distribution $P_*(\\cdot|X = x)$ can thus\nbe expressed as:\n$$p_*(y|x) = \\int \\phi_{\\sigma_*}(y - G_u(z, x)) dP_z,$$\nwhere $\\phi_{\\sigma_*}$ is the density of $N(0, \\sigma_*^2 I_D)$. We define the class of conditional distributions $\\mathcal{P}$ as\n$$\\mathcal{P} = \\{\\mathcal{P}_{g, \\sigma}: g(\\cdot, x) \\in \\mathcal{F}, \\sigma \\in [\\sigma_{min}, \\sigma_{max}]\\},$$\nwhere $P_{g, \\sigma}$ represents the distribution with density $p_{g, \\sigma} = \\int \\phi_{\\sigma}(y - g(z, x)) dP_z$. In this notation, $P^* = P_{G^*, \\sigma^*}$ and $p^* = P_{G^*, \\sigma_*}$. The elements of $P$ comprise two components: g originating from the underlying function class $F$, and $\\sigma$, which characterizes the noise component. This class enables us to obtain separate estimates for $G^*$ and $\\sigma^*$, furnishing us with both the canonical estimator for the distribution of $Y|X = x$ and enhancing our comprehension of the singular distribution of $G^*(Z, x)$, supported on a low-dimensional manifold.\nGiven a data set $\\{(X_i, Y_i)\\}_{i=1}^n$, the log-likelihood function is defined as $l_n(g, \\sigma) = n^{-1} \\sum_{i=1}^n \\log p_{g, \\sigma} (Y_i|X_i)$. For a sequence $\\eta_n \\downarrow 0$ as $n \\rightarrow \\infty$, a sieve maximum likelihood estimator (MLE) (Geman and Hwang, 1982) is any estimator $(\\hat{g}, \\hat{\\sigma}) \\in F \\times [\\sigma_{min}, \\sigma_{max}]$ that satisfies\n$$l_n(\\hat{g}, \\hat{\\sigma}) \\geq \\sup_{\\sigma \\in [\\sigma_{min}, \\sigma_{max}]}\\, \\sup_{g \\in F} l_n(g, \\sigma) - \\eta_n.$$\nHere $\\hat{g} \\in F$ and $\\hat{\\sigma} \\in [\\sigma_{min}, \\sigma_{max}]$ are the estimators, and $\\eta_n$ represents the optimization error. The dependence of $\\hat{g}$ and $\\hat{\\sigma}$ on $n$ illustrates the sieve's role in approximating the true distribution when optimization is performed over the class $P$. The estimated density $\\hat{p} = p_{\\hat{g}, \\hat{\\sigma}}$ provides an estimator for $p_*(\\cdot|\\cdot)$, and $Q_{\\hat{g}}(\\cdot|X = x)$ serve as the estimator for $Q_*(\\cdot|X = x)$.\nIn this section, we formulate the main results, which provide convergence rates in the Hellinger distance for our sieve MLE estimator. The convergence rate was derived for any search functional class $F$, with a brief emphasis on their entropy and approximation capabilities.\nAssumption 1 (True distribution). Denote $\\mu_{\\mathcal{X}}(x)$ as the distribution of X. We denote the true conditional densities as $p_* = \\{p_*(\\cdot|x), x \\in \\mathbb{R}^p\\}$. It is natural to assume that the data is generated from $p_*$ from model (2) with some true generator $G_*$ and $\\sigma_*$. We denote $Q^*(X = x)$ (or $Q_{G^*}$) as the distribution of $G^*(Z, x)$ for some distribution $P_Z$.\nA function g is said to have a composite structure (Schmidt-Hieber, 2020; Kohler and Langer, 2021) if it takes the form as\n$$g = f_q \\circ f_{q-1} \\circ \\ldots \\circ f_1$$\nwhere $f_j : (a_j, b_j)^{d_j} \\to (a_{j+1}, b_{j+1})^{d_{j+1}}, d_0 = p + d$ and $d_{q+1} = D$. Denote $f_j = (f_j^{(1)}, \\ldots, f_j^{(d_{j+1})})$ as the components of $f_j$, let $t_j$ be the maximal number of variables on which each of the $f_j^{(i)}$ depends and let $f_j^{(i)} \\in H^{\\beta_j}((a_j, b_j), K)$ (see Section 2.4.1 for the definition of the H\u00f6lder class $H^{\\beta}$). A composite structure is very general which includes smooth functions and additive structure as special cases. In addition, in the next section, we show the class of conditional distributions $\\{Q_{G_*}(\\cdot|X = x): x \\in \\mathbb{R}^p, G_* \\in \\mathcal{G}\\}$ induced by the composite structure is broad.\nAssumption 2 (composite structure). Denote $\\mathcal{G} = \\mathcal{G} (q, d, t, \\beta, K)$ as a collection of functions of form (5), where $d = (d_0, ..., d_{q+1}), t = (t_0, ..., t_{q+1})$, and $\\beta = (\\beta_0, ..., \\beta_{q+1})$. We regard $(q, d, t, \\beta, K)$ as constants in our setup, and assume that the true generator $G_*(\\cdot, x)$ as in (2) belongs to $\\mathcal{G}$, for all $x \\in \\mathcal{X}$. Additionally, we assume $|||G_*|{\\infty}||_{\\infty} \\leq K$.\n$$\\beta_* = \\beta_{j^*}, \\qquad t_* = t_{j^*}.$$\nThe quantities $t_*$ and $\\beta_*$ are called intrinsic dimension and smoothness of $G_*$ (or of $\\mathcal{G}$)."}, {"title": "2.2 Neural network class", "content": "We model $G^*(\\cdot,\\cdot)$ using a deep neural network. More specifically, we parameterize the true generator $G^*$ with a deep neural neural architecture $(L, r)$ of the form\n$$f: \\mathbb{R}^o \\to \\mathbb{R}^{L+1},\\qquad z \\to f(z) = W_L p_{v_L} W_{L-1} p_{v_{L-1}} - ... - W_1 p_{v_1} W_0 Z,$$\nwhere $W_j \\in \\mathbb{R}^{r_{j+1} \\times r_j}, v_j \\in \\mathbb{R}^{r_j}, p_{v_j}(\\cdot) = ReLU(\\cdot - v_j)$ and $r = (r_0, ..., r_{L+1}) \\in \\mathbb{N}^{L+2}$. The constant $L$ is the number of hidden layers and $r = (r_0,...,r_{L+1})$ represents the number of nodes in each layer.\nWe define the sparse neural architecture class $\\mathcal{F}_s(L, r, s, B, K)$ as set of functions of form (9) satisfying\n$$\\sum |W_j|_0 + |v_j|_0 \\leq s,$$\n$$max |W_j \\vee v_j|_{\\infty} \\leq B, \\qquad |||f|{\\infty}||_{\\infty} \\leq K,$$\nwith $r_0 = d + p$ and $r_{L+1} = D$, where $|\\cdot|_0$ and $||_{\\infty}$ stand for the $L^0$ and $L^{\\infty}$ vector norms, and $|||f|{\\infty}||_{\\infty} = \\sup_{x \\in \\mathbb{R}^D} \\max_{i=1,...,D} |f_i(x)|$, s is sparsity parameter and K is functional bound.\nThe fully connected neural architecture class $\\mathcal{F}_c = \\mathcal{F}_c(L, r, B, K)$ is set of functions of form (9) satisfying\n$$max |W_j \\vee v_j|_{\\infty} \\leq B, \\qquad |||f|{\\infty}||_{\\infty} \\leq K.$$\nBoth classes $\\mathcal{F}_s$ and $\\mathcal{F}_c$ for the deep generator will be considered in our analysis of the resulting sieve maximum likelihood estimator. We denote the corresponding sieve-MLE as $\\hat{p}_s$ and $\\hat{p}_c$, respectively. When we use $r$ instead of $r$, it refers to $r_1 = ... = r_L = r$ along with $r_0 = d + p$ and $r_{L+1} = D$.\nWe can simplify and visualize the result stated in Theorem 1 in both cases: when the sieve-MLE is obtained with optimization performed over the class $\\mathcal{F}_s$ and $\\mathcal{F}_c$. To fulfill the conditions stated in the Theorem 1, we need to establish entropy bounds for these function classes, $\\mathcal{F}_s$ and $\\mathcal{F}_c$, and gain insight into their approximation capabilities for the composite structure class described in Assumption 2.\nFor the sparse neural architecture class $\\mathcal{F}_s(L, r, s, K)$, the entropy, formally stated as Proposition 1 in Ohn and Kim (2019), is bounded as follows.\n$$\\log N(\\delta, \\mathcal{F}_s, ||| \\cdot |||_{\\infty}) \\leq sL \\{ \\log(BLr) + \\log \\delta^{-1} \\}.$$\nFrom an entropy perspective, the fully connected neural architecture class $\\mathcal{F}(L,r, B, K)$ can be viewed as $\\mathcal{F}_s$ without any sparsity constraint, meaning $s = r^2 L$. Therefore, we have\n$$\\log N(\\delta, \\mathcal{F}_c, ||| \\cdot |||_{\\infty}) \\leq L^2 r^2 \\{ \\log(BLr) + \\log \\delta^{-1} \\}.$$\nThe approximation properties of the sparse and fully connected network are provided in Lemma 4.1 and Lemma 4.2 of the Appendix J, respectively.\nHaving established the essential components for $\\mathcal{F}_c$ in (11) and Lemma 4.2, and for $\\mathcal{F}_s$ in (10) and Lemma 4.1, respectively, we can simplify Theorem 1 and state Corollary 1."}, {"title": "2.3 Wasserstein convergence of the intrinsic (conditional) distributions", "content": "Using Wasserstein distance as a metric for distributions $Q_g$ is meaningful due to their singularity in ambient space: when $d < D$, the conditional distribution is singular with respect to the Lebesgue measure on $\\mathbb{R}^D$.\nThe integrated Wasserstein distance, for $r > 1$, between $P_1(\\cdot|X)$ and $P_2(\\cdot|X)$ is defined as\n$$W_r (P_1, P_2) = E_X \\bigg[ \\inf_{\\pi \\in \\Pi(P_1, P_2)} (E_{(U_1, U_2) \\sim \\pi} [|U_1 - U_2|^r])^{1/r} \\bigg]$$\nwhere $\\Pi(P_1, P_2)$ is the set of all couplings between $P_1$ and $P_2$ that preserves the two marginals. The (dual) representation of this norm, $W_r(P_1, P_2) = E_X [\\sup_{||f||_{Lip,r} \\leq 1} \\{E_{P_1} [f] - E_{P_2}[f]\\}]$ (Villani et al., 2009) with $||f||_{Lip,r}$ denoting the r-Lipschitz norm, is particularly useful in our proofs.\nTheorem 2. Suppose that Assumption 3 holds. If $d_H(P_{g, \\sigma},P_*) \\leq \\epsilon$ holds for some $\\epsilon \\in [0,1]$ and some $P_{g, \\sigma} \\in \\mathcal{P}$, then we have\n$$W_1(Q_g, Q_*) \\leq C (\\epsilon + \\sigma \\sqrt{\\log \\epsilon^{-1}}),$$\nwhere $C = C(D, K,r_*)$ depends only on $(D, K, r_*)$.\nThe proof of Theorem 2 is provided in Appendix G. Theorem 2 guarantees that $W_1(Q_g, Q_*) \\lesssim \\mathcal{S}_{log} (d_H(p,p^*) + \\sigma_*)$, where $\\mathcal{S}_{log}$ represents less than or equal up to a logarithmic factor of $n$. Following from Corollary 1, the Wasserstein convergence rate, $n^{-(\\frac{\\beta_* -t_a}{2\\beta_* +t_*})} \\log^2(n) \\vee \\sigma_* \\log^{1/2} (n)$, comprises two components: the convergence rate in the Hellinger distance and the standard deviation of the true noise sequence. It is noteworthy that the first expression is influenced by the variance of noise by the factor $\\alpha$. When $\\alpha$ is very small, indicating that the data $Y_i$ lies very close to the manifold, the second expression $n^{-\\alpha}$ in the overall rate dominates. Intuitively, this phenomenon arises from the underlying structural challenges in related manifold estimation problems with noisy data, as discussed by Genovese et al. (2012). To address this issue, we propose a data perturbation strategy by transforming the data $\\{(Y_i, X_j)\\}_{i=1}^n$ into $\\{(\\tilde{Y}_i, X_j)\\}_{i=1}^n$, where $\\tilde{Y}_j = Y_j + \\epsilon_j$ and $\\epsilon_j \\sim N(0_D, n^{-\\beta_*/(\\beta_*+t_*)} I_D)$. The resulting estimation error bound is summarized below, whose proof is provided in Appendix H.\nCorollary 2. Suppose that Assumption 1, 2, and 3 hold, and $\\sigma_* \\in [\\sigma_{min}, \\sigma_{max}]$ with $\\sigma_* = n^{-\\alpha}$ and $\\sigma_{min} = n^{-\\gamma}$ for some $0 \\leq \\alpha \\leq \\gamma$. Then for each of the network architecture classes (sparse and fully connected) with\nthe network parameters specified in Corollary 1, the sieve MLE $\\hat{p}_{per}$ and $\\hat{Q}_{per}$ based on the perturbed data $\\{(\\tilde{Y}_i, X_j)\\}_{i=1}^n$ satisfies\n$$\\mathbb{P}_* \\bigg[ W_1 (\\hat{Q}_{per}, Q_*) \\geq \\frac{\\epsilon_* + \\sigma_* \\sqrt{\\log (\\epsilon^{-1}_*)}} }{n} \\bigg] \\leq 5 e^{-C_1 n \\epsilon_*^2} + \\frac{C_2}{n}$$\nwhere $\\epsilon_*$ can be chosen such that\n$$\\epsilon_* + \\sigma_* \\sqrt{\\log (\\epsilon^{-1}_*)} = \\begin{cases}\nn^{-\\frac{\\beta_* -t_a}{2\\beta_* +t_*}} \\log^2(n), & \\text{if } \\alpha < \\beta_*/\\{2(\\beta_* + t_*)\\},\\\\\nn^{-\\frac{\\alpha}{2(\\beta_* + t_*)}} \\log^2(n), & \\text{otherwise.}\\end{cases}$$"}, {"title": "2.4 Characterization of the learnable distribution class", "content": "Section 2.2 focuses on the true generator $G$ within the class of functions with composition structures. In this subsection, we show that such a conditional distribution class achieved by the push-forward map $G_*$ is broad and includes many existing distribution classes for $Q$ as special cases.\n2.4.1 Smooth conditional density\nFor $\\beta > 0$, let $H^{\\beta} (D, M)$ be the class of all $\\beta$-H\u00f6lder functions $f : D \\subset \\mathbb{R}^d \\rightarrow \\mathbb{R}$ with $\\beta$-H\u00f6lder norm bounded by $M > 0$. Let $H^{\\beta}(D) = \\bigcup_{M>0} H^{\\beta}(D, M)$. See Appendix A for their formal definitions.\nLemma 2. Suppose that (i) $Z \\times X$ and Y are uniformly convex and (ii) $p_{\\mathcal{Z}} \\in H^{\\beta_{\\mathcal{Z}}} (Z)$, $\\mu_{\\mathcal{X}} \\in H^{\\beta_{\\mathcal{X}}} (X)$ and $q^* \\in H^{\\beta_Q} (\\mathcal{V})$ for some $\\beta_{\\mathcal{Z}}, \\beta_{\\mathcal{X}}, \\beta_Q > 0$ and are bounded above and below. Then, there exists a map $g(\\cdot, \\cdot) : Z \\times X \\rightarrow Y$ such that $Q^*(\\cdot|\\cdot) = Q_g$ and $g \\in H^{\\beta_{min}+1}(Z \\times X)$, where $\\beta_{min} = \\min\\{\\beta_{\\mathcal{Z}}, \\beta_{\\mathcal{X}}, \\beta_Q\\}$.\nLemma 2 establishes that the learnable distribution class includes H\u00f6lder-smooth functions with smoothness parameter $\\beta_{min}$ and intrinsic dimension d. As a result, following Corollary 1, the convergence rate for density estimation is given by $\\eta_n = n^{-(\\frac{\\beta_{min}+1-d_a}{2\\beta_{min}+2+d})}$. A push-forward map is a transport map between two distributions. The well-established regularity theory of transport map in optimal transport is directly applicable here [see Villani et al. (2009) and Villani (2021)]. The proof of Lemma 2 is based on Theorem 12.50 of Villani et al. (2009) and Caffarelli (1996), which establishes the regularity of this transport map and its existence follows from Brenier (1991). When $p_{\\mathcal{Z}}$ is selected as a well-behaved parametric distribution, the regularity of the transport map is determined by the smoothness of both $\\mu_{\\mathcal{X}}$ and $Q^*$. For a more detailed discussion on this, please refer to Appendix B.\n2.4.2 A broader conditional distribution class with smoothness disparity\nIn Appendix K, we present a novel approximation result for the function class exhibiting smoothness disparity in Theorem 5. This new result facilitates the study of theoretical properties of estimators when the generator $G^* \\in H^{\\beta_{\\mathcal{Z},\\mathcal{X}}} (Z,X,K)$. Note that such a function class defined in (16) in Appendix K is much broader compared to the smoothness class in Section 2.4.1 as Z and X do not have to be jointly smooth and it allows for smoothness disparity among them. The subsequent Theorem 3 combines our approximation result with (11) and enables us to specialize Theorem 1 to this class (see Appendix I for the proof)."}, {"title": "3 Numerical Results", "content": "In this section, we present numerical experiments to validate and complement our theoretical findings using two synthetic dataset examples. These experiments cover a range of scenarios, including full-dimensional cases as well as benchmark examples involving manifold-based data. Additionally, we provide a real data example to further enrich our experimentation and validation process. It is worth noting that, although not significant, the computational cost of fitting a conditional generative model is higher compared to fitting an unconditional one, as the input dimension of the deep neural network (DNN) is p + d rather than just d.\nLearning algorithm to compute sieve MLE. For the computational algorithm, we adopt a common conditional variational auto-encoder (VAE) architecture to maximize the following log-likelihood term:$$\\sum_{j=1}^n \\mathcal{L}_{VAE}(g, \\sigma, \\phi; Y_j, X_j),$$\nwhere\n$$\\mathcal{L}_{VAE}(g, \\sigma, \\phi; y, x) = \\log \\frac{p_{g, \\sigma} (y, x, z)}{q_{\\phi}(Z|y, x)}.$$\nThe variational distribution $q_{\\phi}(Z|y, x)$ is chosen as the standard normal family $N(\\mu_{\\phi}(y, x), \\Sigma_{\\phi}(y, x))$.\nWe examine two classes of datasets: (i) full-dimensional response and (ii) response residing on a low-dimensional manifold. The first highlights the generality of our proposed approach, while the second underscores its efficiency in terms of the Wasserstein metric and validates the small noise perturbation strategy outlined in Corollary 2.\nSimulation from full dimension distribution. We use the following models for data generation."}, {"title": "4 Discussion", "content": "We investigated statistical properties of a likelihood-based conditional deep generative model for distribution regression in a scenario where the response variable is situated in a high-dimensional ambient space but is centered around a potentially lower-dimensional intrinsic structure. Our analysis established favorable rates in both the Hellinger and Wasserstein metrics which are dependent on only the intrinsic dimension of the data. Our theoretical findings show that the conditional deep generative models can circumvent the curse of dimensionality for high-dimensional distribution regression. To the best of our knowledge, our work is the first of its kind.\nGiven the novelty of emerging statistical methodologies with intricate structural considerations in the study of deep generative models, there exist numerous paths for future exploration. Among these potential directions, we are particularly interested in investigating controllable generation via penalized optimization methods, studying statistical properties of deep generative models trained via matching flows, as well as"}, {"title": "A Notation", "content": "We denote a \u2228 b and a \u2227 b as the maximum and minimum of two real numbers a and b, respectively. The notation [a] represents the smallest integer greater than or equal to a. The inequality a \u2272 b indicates that a is less than or equal to b up to a multiplicative constant. When we write a \u224d log b, it means that a is less than or equal to b up to a logarithmic factor, specifically log(n). We denote a = b when both a \u2272 b and b \u2272 a hold. For vector norms, |\u00b7|p represents the lp norm, while || \u00b7 ||p denotes the Lp-norm of a function for 1 < p < \u221e. Lastly, B\u03b5(u) signifies the Euclidean open ball with radius \u03b5 centered at u.\nWe use the multi-index notation through the main paper and the appendix. Denote N as the set of natural numbers and N0 as N \u222a {0}. For a vector x \u2208 Rr, we denote the components as x = (x(1), . . . , x(r)). Given a function f : D \u2282 Rr \u2192 R, the operator is defined as \u2202\u03b1 := \u2202\u03b1(1) . . . \u2202\u03b1(r) with \u03b1 \u2208 Nr 0, where\n\u2202\u03b1(i)f := \u2202\u03b1(i)f(x)/\u2202x(i). For \u03b1 \u2208 Nr 0, the expression |\u03b1| = \u2211ri=1|\u03b1(i)|. Given a function f(\u00b7, \u00b7) : D \u00d7 D, \u2282 Rr \u00d7 Rr\u2032 \u2192 R, we denote the operator \u2202\u03b1+\u03b1\u2032 := \u2202\u03b1(1) . . . \u2202\u03b1(r)\n\u2202\u03b1\u2032(1) . . . \u2202\u03b1\u2032(r\u2032), with \u03b1 \u2208 Nr 0 and \u03b1\u2032 \u2208 Nr\u2032 0, where \u2202\u03b1(i)f(x, y) = \u2202\u03b1(i)f(x, y)/(\u2202x(i)) and \u2202\u03b1\u2032(i)f(x, y) =\n\u2202\u03b1\u2032(i)f(x, y)/\u2202y(i), with x \u2208 D and y \u2208 D,. This notation allows us to represent the derivative with variable x and y separately through the vector \u03b1 and \u03b1\u2032, which is required to tackle the smoothness disparity along x and y variable. The \u03b2-H\u00f6lder class functions are defined as\n$$H^{\\beta}(D, M) = \\{f : D \\subset \\mathbb{R}^r \\rightarrow \\mathbb{R} : \\sum_{\\alpha : |\\alpha| < \\beta} |\u2202^\\alpha f|\\_\\infty + \\sum_{\\alpha: |\\alpha|=[\\beta]} \\sup_{u_1, u_2 \\in D}\\, \\sup_{u_1 \\neq u_2} \\frac{|\u2202^\\alpha f(u_1) - \u2202^\\alpha f(u_2)|}{|u_1 - u_2|^{\\beta-[\\beta]}} \\leq M\\},$$\nWe extend this definition to include the H\u00f6lder class of functions with differences in smoothness (smoothness disparity) along two variables. This class is defined as\n$$H^{\\beta,\\beta'}(D, D', M) = \\{f(\\cdot, \\cdot) : D \\times D' \\subset \\mathbb{R}^r \\times \\mathbb{R}^{r'} \\rightarrow \\mathbb{R} : \\sum_{\\alpha : |\\alpha| < \\beta} |\u2202^\\alpha \u2202_1^{\\alpha'} f|\\_\\infty + \\sum_{\\alpha :\\alpha<\\beta}\\, \\sum_{\\alpha':\\alpha <\\beta'}\\, \\sup_{\\substack{u_1, u_2 \\in D}}\\, \\sup_{\\substack{v_1, v_2 \\in D'}}\\, \\frac{|\u2202^\\alpha \u2202_1^{\\alpha'} f(v_1, u_1) - \u2202^\\alpha \u2202_1^{\\alpha'} f(v_2, v_2)|}{|u_1 - u_2|^{\\\n\beta-[\\beta]}} \\leq M\\}.$$\nWe denote H\u03b2(D) = \u222aM>0H\u03b2(D, M) and H\u03b2,\u03b2\u2032 (D, D,) = \u222aM>0H\u03b2,\u03b2\u2032 (D, D,, M)."}, {"title": "B More on Smooth conditional density", "content": "Theorem 4 (Villani et al. (2009) Theorem 12.50). Suppose that\n(i) A1 and A2 are uniformly convex", "C[\u03b2": 2, "\u03b2": 2, "g": "A1 \u2192 A2 with g \u2208 H\u03b2+1(A1), such that if U ~ h"}]}