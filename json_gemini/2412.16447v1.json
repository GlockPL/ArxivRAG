{"title": "A Generalizable Anomaly Detection Method in Dynamic Graphs", "authors": ["Xiao Yang", "Xuejiao Zhao", "Zhiqi Shen"], "abstract": "Anomaly detection aims to identify deviations from normal patterns within data. This task is particularly crucial in dynamic graphs, which are common in applications like social networks and cybersecurity, due to their evolving structures and complex relationships. Although recent deep learning-based methods have shown promising results in anomaly detection on dynamic graphs, they often lack of generalizability. In this study, we propose GeneralDyG, a method that samples temporal ego-graphs and sequentially extracts structural and temporal features to address the three key challenges in achieving generalizability: Data Diversity, Dynamic Feature Capture, and Computational Cost. Extensive experimental results demonstrate that our proposed GeneralDyG significantly outperforms state-of-the-art methods on four real-world datasets.", "sections": [{"title": "Introduction", "content": "Graphs are extensively employed to model complex systems across various domains, such as social networks (Wang et al. 2019), human knowledge networks (Ji et al. 2021), e-commerce (Qu et al. 2020), and cybersecurity (Gao et al. 2020). Although the bulk of researches focus on static graphs, real-world graph data often evolves over time (Skarding, Gabrys, and Musial 2021). Taking knowledge networks as an example, there is new knowledge being added to the network every month, with connections between different concepts evolving over time. To model and analyze graphs where nodes and edges change over time, mining dynamic graphs gains increasing popularity in the graph analysis. Anomaly detection in dynamic graphs (Ma et al. 2021; Ho, Karami, and Armanfard 2024) is vital for identifying outliers that significantly deviate from normal patterns such as anomalous edges or anomalous nodes, including the detection of fraudulent transactions, social media spam, and network intrusions (Dou et al. 2020). By utilizing the temporal information and relational structures inherent in dynamic graphs, researchers can more effectively identify anomalies, thereby enhancing the security and integrity of various systems (Pourhabibi et al. 2020).\nRecently, techniques based on deep learning have facilitated significant advancements in anomaly detection within dynamic graphs. For example, methods like GDN (Deng and Hooi 2021), StrGNN (Cai et al. 2021) focus on extracting structural information from graphs, while approaches such as LSTM-VAE (Park, Hoshi, and Kemp 2018) and TADDY (Liu et al. 2021) concentrate on capturing temporal information.In addition, self-supervised (Lee, Kim, and Shin 2024) and semi-supervised (Tian et al. 2023) methods have also been applied to dynamic graph anomaly detection.\nDespite their improved performance, current deep learning-based methods lack the crucial generalizability (Brennan 1992) needed for dynamic graph tasks across different tasks or datasets. A model with strong generalization can adapt to different tasks without significant adjustments to its architecture or parameters, reducing the need for retraining or redesigning for new tasks(Bai, Ling, and Zhao 2022). Conversely, in anomaly detection, where identifying potential risks or issues is crucial, poor generalization may lead to missed critical anomalies in new scenarios, thereby diminishing the model's reliability in real-world applications. Specifically, the inadequate encoding of anomalous events\u00b9 in existing methods results in poor generalization. Firstly, in the absence of raw event attributes, they fail to generate informative event encodings that accurately represent the properties of the events. For example, SimpleDyG (Wu, Fang, and Liao 2024) nearly discards all topological structure information, tokenizing only the nodes while ignoring the edges, which leads to the loss of critical structural information during node prediction tasks, making it unsuitable for node anomaly detection tasks and even less so for edge anomaly detection. The positional encoding method in TADDY (Liu et al. 2021) may not capture structural similarities and could fail to model the structural interactions between events, as demonstrated in SAT (Chen, O'Bray, and Borgwardt 2022). TADDY's node position-specific encoding may result in ambiguous structural information, leading to suboptimal results in node anomaly detection tasks. Furthermore, some methods, such as GDN (Deng and Hooi 2021), exhibit inadequate temporal information capture capabilities. For instance, GDN does not incorporate the information provided by specific time values when modeling temporal data, resulting in poor performance on time-sensitive datasets such as Bitcoin-Alpha and Bitcoin-OTC (Liu et al. 2021).\nDeveloping a highly generalizable dynamic graph anomaly detection method presents several challenges, primarily in: 1. Data Diversity: Differences across dynamic graph datasets, such as topological structures and node and edge attributes, can be substantial. The method must identify and adapt to a wide range of feature distributions. 2. Dynamic Feature Capture: Anomalies in dynamic graphs may occur locally (e.g., anomalous behavior of specific nodes or edges) or globally (e.g., abnormal changes in network topology). The method must capture both local and global dynamic features. 3. Computational Cost: Dynamic graph anomaly detection often involves large-scale graph data, making computational resources and time efficiency significant challenges.\nHence, in this work, we propose a novel approach for anomaly detection named GeneralDyG, which addresses the three key challenges mentioned above and ensures generalizability in graph anomaly detection tasks. It ensures simplicity by sampling ego-graphs around anomalous events, then uses a novel GNN extractor to capture structural information, and finally employs a Transformer module to capture temporal information. Specifically, the main contributions of our work are:"}, {"title": "Related Work", "content": "Anomalies are infrequent observations that significantly deviate from the rest of the sample, such as data records or events. Dynamic graph anomaly detection primarily focuses on identifying unusual events within a dynamic graph (Ekle and Eberle 2024; Ho, Karami, and Armanfard 2024; Ma et al. 2021). Recently, deep learning methods have made significant advancements in anomaly detection for dynamic graphs. Modeling time series-related tasks as anomalous node detection in dynamic graphs is considered a viable approach (Su et al. 2019; Chen et al. 2022; Zhang, Zhang, and Tsung 2022; Dai and Chen 2022). Specifically, M-GAT employs a multi-head attention mechanism along with two relational attention modules-namely, intra-modal and inter-modal attention to explicitly model correlations between different modalities (Ding, Sun, and Zhao 2023). MTAD-GAT incorporates two parallel graph attention layers to capture the complex dependencies in multivariate time series across both temporal and feature dimensions (Zhao et al. 2020). GDN integrates structural learning with graph neural networks and leverages attention weights to enhance the explainability of detected anomalies (Deng and Hooi 2021). FuSAGNet optimizes reconstruction and forecasting by combining a Sparse Autoencoder with a Graph Neural Network to model multivariate time series relationships and predict future behaviors (Han and Woo 2022).\nDetection of edge anomalies in dynamic graphs has also garnered increasing attention. Classical methods include the randomized algorithm SEDANSPOT (Eswaran and Faloutsos 2018) and the hypothesis-based approach Midas (Bhatia et al. 2020). Many recent methods have employed discrete approaches to address this task. For instance, Add-graph utilizes a GCN to extract graph structural information from slices, followed by GRU-attention (Zheng et al. 2019). StrGNN extracts h-hop closed subgraphs centered on edges and employs GCN to model structural information on snapshots, with GRU capturing correlations between snapshots (Cai et al. 2021). Recently, SAD introduced a continuous dynamic approach for anomaly detection using a semi-supervised method (Tian et al. 2023)."}, {"title": "Transformer on Dynamic Graphs", "content": "Transformers are a type of neural network that rely exclusively on attention mechanisms to learn representative embeddings for various types of data, as initially introduced in (Vaswani et al. 2017). Recent works have also applied Transformers to dynamic graph tasks. For instance, GraphERT pioneers the use of Transformers to seamlessly integrate graph structure learning with temporal analysis by employing a masked language model on sequences of graph random walks (Beladev et al. 2023). GraphLSTA captures the evolution patterns of dynamic graphs by effectively extracting and integrating both long-term and short-term temporal features through a recurrent attention mechanism (Gao et al. 2023). Taddy employs a Transformer to handle diffusion-based spatial encoding, distance-based spatial encoding, and relative time encoding, subsequently deriving edge representations through a pooling layer to calculate anomaly scores (Liu et al. 2021). SimpleDyG reinterprets dynamic graphs as a sequence modeling problem and presents an innovative temporal alignment technique. This approach not only captures the intrinsic temporal evolution patterns of dynamic graphs but also simplifies their modeling process (Wu, Fang, and Liao 2024)."}, {"title": "Preliminaries", "content": "Notations. A continuous-time dynamic graph (CTDG) is used to represent relational data in evolving systems. A CTDG is defined as G = (V, E), where V is the set of nodes that participate in temporal edges, and E is a chronologically ordered series of edges. Each edge \\(\\delta(t) = (v_i, v_j, t, e_{ij})\\) represents an interaction from node \\(v_i\\) to node \\(v_j\\) at time t with an associated feature \\(e_{ij}\\). The node attributes for nodes \\(v_i, v_j \\in V\\) are denoted by \\(x_{v_i}, x_{v_j} \\in R^d\\), and the node attributes for all nodes are stored in \\(X \\in R^{n \\times d}\\). Additionally, the edge attributes for edges \\(e_{ij} \\in E\\) are denoted by \\(Y_{e_{ij}} \\in R^d\\), and the edge attributes for all edges are stored in \\(Y \\in R^{m \\times d}\\), where n is the number of nodes and m is the number of edges in the CTDG. In this paper, we explore a method called GeneralDyG for handling node-level and edge-level anomalies. Therefore, in the following text, we treat nodes V and edges E collectively as anomaly events A. Similarly, we consider node features X and edge features Y together as anomaly features Z.\nTransformer on CTDG. While Graph Neural Networks (GNNs) directly leverage the inherent structure of graphs, Transformers take a different approach by inferring relationships between nodes using their attributes rather than the explicit graph structure (Dwivedi and Bresson 2020). Transformer treats the dynamic graph as a collection of edges, utilizing the self-attention mechanism to identify similarities between them. The architecture of the Transformer(Fang et al. 2023a,b) consists of two fundamental components: a self-attention module and a feed-forward neural network.\nIn the self-attention module, the input anomaly features Z are initially projected onto the query (Q), key (K), and value (V) matrices through linear transformations, such that Q = ZWQ, K = ZWK, and V = ZWv, respectively. The self-attention can then be computed as follows:\n\\[Attn(Z) = softmax(\\frac{QK^T}{\\sqrt{d_{out}}})V \\in R^{(m+n)\\times d_{out}}\\]\nTo address dynamic graph tasks, multiple Transformer layers can be stacked to build a model that provides node-level representations of the graph (Wang et al. 2021). However, due to the permutation invariance of the self-attention mechanism, the Transformer generates identical representations for nodes with the same attributes, regardless of their positions or surrounding structures within the graph. This characteristic necessitates the incorporation of positional and contextual information into the Transformer, typically achieved through positional encoding (Cong et al. 2021; Sun et al. 2022).\nAbsolute encoding. Absolute encoding involves adding or concatenating positional or structural representations of the graph to the input node features before feeding them into the main Transformer model. Examples of such encoding methods include Laplacian positional encoding (Dwivedi and Bresson 2020), Random Walk Positional Encoding (Dwivedi et al. 2021), and Node Encoding (Liu et al. 2021). A key limitation of these methods is that they typically fail to capture the structural similarity between nodes and their neighborhoods, thereby not effectively leveraging the graph's structural information.\nProblem Definition. The goal of this paper is to detect anomalous edges and nodes at each timestamp. Based on the previously mentioned notations, we model anomaly detection in dynamic graphs as a task of computing anomaly scores.\nDefinition 1. Given a dynamic graph G, where each \\(G_t = (V_t, E_t)\\) represents the graph at timestamp t, the goal of anomaly detection is to identify unusual edges and nodes within this evolving structure. For each edge \\(e \\in E_t\\) and each node \\(v \\in V_t\\), the objective is to compute an anomaly score f(e) and f (v), respectively, where f is a learnable anomaly score function. The anomaly score quantifies the degree of abnormality for both edges and nodes, with a higher score f(e) or f(v) indicating a greater likelihood of anomaly for edge e or node v.\nBuilding on previous research, we adopt an unsupervised approach for anomaly detection in dynamic graphs. During training, all edges and nodes are considered normal. Binary labels indicating anomalies are provided during the testing process to assess the performance of the algorithms. Specifically, a label \\(y_e = 1\\) signifies that e is anomalous, whereas \\(y_e = 0\\) denotes that e is normal. Similarly, a label \\(y_n = 1\\) indicates that a node is anomalous. It is important to note that anomaly labels are often imbalanced, with the number of normal edges and nodes typically being much greater than the number of anomalous ones."}, {"title": "Methodology", "content": "In this section, we introduce the general framework of our approach, which consists of three main components: Temporal ego-graph sampling, Temporal ego-graph GNN extractor, and Temporal-Aware Transformer. An overview of the proposed framework is illustrated in Figure 1. Initially, we extract ego-graphs at the level of each anomaly event, capturing k-hop temporal dynamics. These temporal ego-graphs are then transformed into anomaly feature sequences, preserving their temporal and structural order, as demonstrated in Figure 1(a). To fully understand the structural information of these sequences, they are processed through a GNN model to extract the structural details of the temporal ego-graphs, as depicted in Figure 1(b). Finally, both the original sequence features and the structure-enriched sequence features are fed into the Transformer to evaluate the anomaly detection task, as shown in Figure 1(c).\nTemporal ego-graph sampling\nUnlike conventional methods that map dynamic graphs into a series of snapshots to obtain tokens, we use a more lightweight approach by employing anomalous events as tokens for the Transformer. Additionally, to acquire the contextual representation and hierarchical information of these anomalous events, we extract the temporal k-hop ego-graph of each event to capture historical interaction information across different structures.\nSpecifically, we denote \\(a_i \\in A\\) as an event in G. For each event \\(a_i\\), we utilize a k-hop algorithm to extract the historically interacted events and construct"}, {"title": "Temporal ego-graph GNN extractor", "content": "A practical approach to extracting local structural information at an event \\(a_i\\) is to apply an existing GNN model to the input graph with event feature sequences \\(z_i\\), and utilize the output representation at \\(a_i\\) as the ego-graph representation \\(\\phi(z_i)\\). It is important to highlight that, to showcase the flexibility of our model, the GNN model employed here should be both straightforward and capable of simultaneously processing node features X and edge features Y. Formally, we denote the selected GNN model with K layers applied to k-hop ego-graphs k-DG as GNN. The output representation \\(\\phi(z_i)\\) can be expressed as:\n\\[\\phi(z_i) = GNN(z_i).\\]\nNext, we discuss the choice of the GNN model. When the dataset information is known prior to anomaly event prediction such as in cases where the CTDG consists solely of node features a conventional GNN model like GCN, GAT, or GIN can be effectively utilized to extract ego-graph structural information. However, for CTDGs with diverse attributes, including both node and edge features, we introduce the Temporal Edge-Node Based Structure Extractor GNN (TensGNN). TensGNN is specifically designed to"}, {"title": "Temporal-Aware Transformer", "content": "To enhance the Transformer's understanding of the topological structure of the temporal ego-graph while preserving the original event features, we overlay the topological structure information onto the Query and Key, while retaining the original event features as the Value. This approach allows the model to leverage structural information for the attention mechanism while maintaining the integrity of the original feature values for effective representation. Formally, for the event feature to be predicted, \\(z_i \\in Z\\), we adopt the method proposed in (Mialon et al. 2021) and rewrite the self-attention as kernel smoothing. The final embedding calculation is then given by:\n\\[Attn(z_i) = \\sum_{z_j \\in DG} \\frac{F_{exp}(z_i, z_j)}{\\sum_{z_w \\in DG} F_{exp}(z_i, z_w)} W_V z_j,\\]\nwhere \\(W_V\\) is the linear value function of the original event feature \\(z_i\\), and Fexp is an exponential kernel (non-symmetric), parameterized by \\(W_Q\\) and \\(W_K\\):\n\\[F_{exp}(x, x') := exp(\\frac{((W_Q z W_K x'))}{\\sqrt{d_{out}}});\\]\n\\[W_V = W z_i + b,\\]\n\\[W_Q = W (z_i) + b,\\]\n\\[W_K = W (z_i) + b,\\]\nwhere \\((\\cdot, \\cdot)\\) denotes the dot product. By optimizing the objective function, we obtain the final embeddings for each anomaly feature \\(z_i\\). These final embeddings are then fed into the scoring module to compute the anomaly scores. It is important to note that the scoring modules for node-level and edge-level anomalies differ in the datasets used in this paper. For edge-level anomalies, we directly use the final output embedding from the training process as the anomaly score. Conversely, for node-level anomalies, the final output consists of a set of binary labels indicating whether each time step is anomalous, which serves as the final anomaly score."}, {"title": "Experiments", "content": "Experimental Setup\nDatasets. We use four real-world datasets, categorized into two types: Node-Level and Edge-Level anomaly detection tasks. For Node-Level, we utilize SWaT (Secure Water Treatment), a small-scale Cyber-Physical system managed by Singapore's Public Utility Board, and WADI (Water Distribution), an extension of SWaT that includes a more extensive water distribution network. Both datasets provide data from normal operations and controlled attack scenarios to simulate real-world anomalies. For Edge-Level, we employ Bitcoin-Alpha and Bitcoin-OTC, which are trust networks of Bitcoin users trading on platforms from www.btc-alpha.com and www.bitcoin-otc.com, respectively. In these datasets, nodes represent users, and edges indicate trust ratings between them, capturing interactions and trust dynamics within the Bitcoin trading community.\nExperimental Design. In our experiments, The settings for Bitcoin-Alpha and Bitcoin-OTC are identical to those used in TADDY (Liu et al. 2021). We inject anomalies into the test set at proportions of 1%, 5%, and 10%. SWaT and WADI are identical to those used in GDN (Deng and Hooi 2021). AUC, AP and F1 are used as the primary metrics to evaluate the performance of the proposed GeneralDyG and baselines.\nBaselines. We evaluated GeneralDyG against 20 advanced baselines, which are classified into two categories: graph embedding methods and anomaly detection methods. A detailed description of the baselines can be found in the Appendix."}, {"title": "Overall Performance", "content": "Edge Level. We compared our methods, GeneralDyG, with nine strong edge-level baseline methods, as shown in Table 1. Our methods consistently outperformed the baselines across both Bitcoin-Alpha and Bitcoin-OTC datasets. The baselines, lacking sufficient structural or temporal information, did not achieve state-of-the-art results. Specifically, GeneralDyG demonstrated an average AUC improvement of approximately 3.2% and 4.5%, respectively, compared to the best-performing baseline on the Bitcoin-Alpha dataset. In terms of Average Precision (AP), GeneralDyG achieved a significant improvement, with up to 24% in the 1% anomaly detection setting, representing a 19.8% increase over the best-performing baseline.\nOn the Bitcoin-OTC dataset, GeneralDyG also exhibited substantial gains, with an average AUC increase of about 3.6% and an AP improvement of up to 20.2% over the baselines. This demonstrates that our methods are more effective in generalizing and capturing the temporal dynamics necessary for robust anomaly detection in these datasets.\nNode Level. We compared our methods, GeneralDyG, with ten strong node-level baseline methods, as shown in Table 2. Our methods generally outperformed the baselines. Specifically, GeneralDyG achieved the highest F1 score on the SWaT dataset with 85.19%, surpassing the second-best method, FuSAGNet, by 1.8%. On the WADI dataset, GeneralDyG reached an F1 score of 60.43%, which is slightly below FuSAGNet's 60.70%, but still demonstrates competitive performance.\nThe baselines, particularly those lacking robust temporal modeling capabilities like PCA and KNN, showed significantly lower F1 scores, with KNN performing the worst on both datasets. Compared to these methods, GeneralDyG shows a notable improvement of approximately 58% on SWaT and 53% on WADI in F1 score. Overall, these results highlight that our methods are better at generalizing and capturing the temporal dynamics necessary for effective anomaly detection in node-level datasets."}, {"title": "Ablation Study", "content": "We conducted an ablation study to assess the contribution of each component in the proposed GeneralDyG, as detailed below:\nThese results demonstrate that Temporal ego-graph sampling significantly improves the AP under different anomaly ratios."}, {"title": "How to Set Up the Optimal GeneralDyG", "content": "The heatmap in Figure 2 illustrates the impact of the parameters k and K on model performance for the Bitcoin-Alpha dataset. It indicates that higher values of K (the number of layers in the TensGNN) generally lead to decreased performance, suggesting that having too many layers can be detrimental to the model's effectiveness. This could be due to overfitting or increased model complexity without corresponding gains in performance.\nOn the other hand, the parameter k (which controls the temporal ego-graph sampling) has a less pronounced effect on performance. While increasing k does affect the results, it mainly impacts the training process due to the additional parameters it introduces.\nThus, the optimal setup should aim for a balance: choosing a modest number of layers (K) to avoid overfitting while selecting an appropriate k that provides sufficient temporal information without excessively complicating the model. This balance will help in achieving both efficient training and robust performance."}, {"title": "Generalizable Analysis", "content": "In Table 4, GeneralDyG demonstrates its strong generalizability across different types of tasks. Specifically, GeneralDyG consistently outperforms the baseline methods that were evaluated in a mismatched dataset context. For instance, when the edge-level baselines are applied to the node-level dataset (WADI), their performance significantly drops, with metrics such as AUC and F1 score showing substantial declines compared to GeneralDyG. Similarly, node-level baselines tested on the edge-level dataset (Bitcoin-Alpha) exhibit poor performance, further emphasizing their lack of generalizability.\nGeneralDyG, on the other hand, maintains high performance across both types of datasets, showcasing its robustness and adaptability. This indicates that GeneralDyG is capable of effectively handling both node-level and edge-level tasks, whereas the baseline methods exhibit considerable performance degradation when faced with different dataset types. These results underline the superior generalizability of GeneralDyG, as it maintains stable and effective performance across diverse scenarios where other methods fail to deliver consistent results."}, {"title": "Conclusion", "content": "In this work, we introduced a novel approach for anomaly detection in dynamic graphs called GeneralDyG, which effectively addresses the challenges of data diversity, dynamic feature capture, and computational cost, thereby demonstrating the generalizability of our method. GeneralDyG achieves this by mapping node, edge, and topological structure information into the feature space, incorporating hierarchical tokens, and sampling temporal ego-graphs to efficiently capture dynamic features. GeneralDyG excels across multiple benchmarks, demonstrating its effectiveness and high performance. For future work, we can build on this work to explore the interpretability of anomaly detection in dynamic graphs, providing more robust theoretical support."}, {"title": "Appendix", "content": "Tabel 5 summarizes the symbols used in the main paper.\nFor anomaly detection on dynamic graphs, we define an objective function for node-level and edge-level tasks using binary cross-entropy loss. For a node \\(v \\in V\\) with label \\(y_n\\) (1 for anomalous, 0 for normal) and score f(v), and an edge \\(e \\in E\\) with label \\(y_e\\) and score f(e), the objective function is:\n\\[L = \\alpha \\left(- \\sum_{v \\in V} (y_n log(f(v)) + (1 - y_n) log(1 - f(v)))\\right) \\\\\n+ \\beta \\left(-\\sum_{e \\in E} (y_e log(f(e)) + (1 - y_e) log(1 - f(e)))\\right)\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are indicators for including node-level and edge-level tasks, respectively."}]}