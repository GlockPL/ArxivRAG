{"title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding", "authors": ["Mo Yu", "Lemao Liu", "Junjie Wu", "Tsz Ting Chung", "Shunchi Zhang", "Jiangnan Li", "Dit-Yan Yeung", "Jie Zhou"], "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PHYSICO. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-40, 01 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable advancements in large language models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023). Thanks to the substantial model capacity and massive training data, LLMs have achieved new state-of-the-arts on a variety of NLP tasks, even surpassing humans on some of them (Min et al., 2023; Chang et al., 2024). Nowadays the application of LLMs has become widespread, facilitating daily work and life, and profoundly influencing people's work and lifestyles (Bommasani et al., 2021; Peng et al., 2024; Demszky et al., 2023).\nOn the other hand, despite the great success of LLMs, many researchers argue that LLMs may not"}, {"title": "2 Measuring Concept Understanding via Summative Assessment", "content": "It is intrinsically challenging to measure the extent to which LLMs understand a sentence or concept. Indeed, Bender and Koller (2020) provide a definition of \"understanding\" from a linguistic perspective, but this definition depends on another abstract and unmeasurable term, \u201cmeaning\u201d. Therefore, even with this definition, accurately measuring \"understanding\" remains elusive.\nWe approach the measurement of whether LLMS understand a concept from an educational and cognitive perspective, using summative assessment (Black and Wiliam, 1998a,b; Harlen and James, 1997). Summative assessment is widely used by educators as an appealing strategy to evaluate students' understanding and knowledge acquisition in educational and cognitive psychology.\nFormally, assume S denotes an intelligent system and C is a specific concept. To evaluate the extent how S understands the concept C, our summative assessment includes the following two steps:\n\u2022 Task design towards C: design several concept understanding tasks, each of which consists of several questions manually created towards understanding the concept C.\n\u2022 Evaluating S: ask S to answer the questions from the tasks and calculate its accuracy.\nRequirements for Validity The success (validity) of the proposed evaluation approach highly depends on the task design (Black and Wiliam, 1998a,b)."}, {"title": "3 Task Design and Dataset Construction", "content": "We borrow the idea of Bloom's taxonomy (Krathwohl, 2002; Armstrong, 2010) from education research to fulfill the requirements for task design in Section 2, so as to ensure the assessment validity. Bloom's taxonomy offers an ideal principle to these requirements with an ordering of six cognitive skills (from low to high level) for knowledge understanding: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating.\nGenerally, it is nontrivial to strictly follow this principle since there is no clear boundary among the last four skills of understanding. As a result, we group the last four high-level skills into one and consider the following two levels of understanding:\n\u2022 Low-level Understanding: covering the two lowest-level skills in Bloom's taxonomy, i.e., retrieving relevant knowledge from long-term memory and rephrasing in one's own words.\n\u2022 High-level Understanding: covering the aspects for understanding the knowledge beyond memorization.\nBased on these two levels, we design the following PHYSICO task for summative assessment."}, {"title": "3.2 Our PHYSICO Task", "content": "PHYSICO is essentially a physical concept understanding task, which primarily targets on 52 physical concepts or phenomena: e.g., gravity, light reflection, acceleration, buoyancy, inertia, etc (see Appendix A for the full list). Our focus on physical concepts is motivated by two main reasons: 1) understanding physical concepts is critical for intelligent systems to interact with the world, which is ultimate goal of embodied AI (Savva et al., 2019; Duan et al., 2022; Xiang et al., 2023); 2) designing tasks centered around physical concepts allows us to more easily control different levels of understanding and ensure the diversity of each concept.\nFor each physical concept, PHYSICO involves both low-level understanding subtasks and high-level subtasks, following our task design principles."}, {"title": "3.2.1 Low-level Understanding Subtasks", "content": "Physical Concept Selection (text) First, to evaluate whether an LLM possesses the knowledge of our included concepts, we design a task to recognize a concept from its corresponding Wikipedia definition. Specifically, we manually masked the synonyms of the concept with placeholder [PHENOMENON]. Meanwhile, highly relevant entities were masked as [MASK] to alleviate shortcuts. \nPhysical Concept Selection (visual) Second, we evaluate if the LLMs can recognize our concepts represented with real-life pictures.\nPhysical Concept Generation Finally, we directly ask the LLMs to generate the description of a concept with its core properties and representative examples."}, {"title": "3.2.2 High-level Understanding Subtasks", "content": "The low-level subtasks are depicted in natural language thus are likely to be remembered by the LLMs due to their extensive training data. To assess whether the LLMs possess a deep understanding of the knowledge, we require the subtasks that can 1) represent the high-level understanding skills; 2) avoid the effects of memorization.\nThe Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) provides a compelling way by using grids (or matrices) instead of texts to represent a concept. \nThe PHYSICO-CORE Set Our first subtask aims to cover the core properties or most representative examples/applications of the assessed concepts. To ensure our set remains generally comprehensible to humans.\nThe PHYSICO-ASSOCIATIVE Set Many instances in the original ARC dataset can be solved via association or analogy to physical concepts.\nCreation of Classification Tasks We create four-choice tasks on the annotated data."}, {"title": "4 Overview of Our Studies", "content": "In the following sections, we conduct a series of studies on our PHYSICO tasks. Our studies"}, {"title": "5 Validation on Low-Level Subtasks", "content": "To illustrate the stochastic parrot phenomenon with PHYSICO, a necessary condition is to ensure the LLMs can perform well on the low-level understanding subtasks, i.e., whether LLMs exhibit strong skills of recalling and describing the definitions, core properties and representative examples of the physical concepts in our tasks. That is:\nRQ 1: Can LLMs perform well on low-level subtasks, i.e., understanding the definitions of physical concepts in natural language?\nTo answer RQ 1, we evaluate the LLMs' abilities to comprehend the definitions of these concepts and generate their descriptions and examples in natural language, as defined in Section 3.2.1."}, {"title": "5.1 Concept Selection Subtask", "content": "Settings We provide the standard definition of a concept based on Wikipedia with its synonyms masked; then ask the LLMs to identify the concept, under the same four-choice setting throughout the experiments. We evaluate the representative text-only LLMs and compute the accuracy.\nResults Table 1 shows that the GPT (both text-based and visual-based) models perform near perfect on recognition of our physical concepts from standard text-based definitions and from the real-life images.\n5.2 Concept Generation Subtask\nSettings We evaluate the descriptions LLMs generate for a concept. The evaluation of text generation is in general difficult."}, {"title": "6 Experiments on High-Level Subtasks", "content": "This section answers the research questions regarding our high-level understanding subtasks.\nRQ 2: Can Humans understand the abstract representations?\nFirst of all, we investigate the performance of humans who possess the knowledge required by our PHYSICO. \nRQ 3: Can LLMs understand concepts in the abstract representations of the matrix format?\nA straightforward solution for our PHYSICO is to represent the grid-formatted examples as matrices. \nRQ 4: Can multimodal LLMs perform well on our tasks with visual input representations?\nNext, we explore whether multi-modal LLMs can effectively solve our tasks when the input examples are presented as visual images rather than matrices like in RQ 3."}, {"title": "7 Related Work", "content": "Stochastic Parrots on LLMs The pioneer study by (Bender and Koller, 2020) questioned the understanding ability of large models; and Bender et al. (2021) first introduced the terminology of stochastic parrot. However, although the concept of stochastic parrots in LLMs is widely accepted and recognized, to the best of our knowledge, there is a lack of quantitative experiments to precisely verify this viewpoint. This gap directly motivates our work.\nAbstract Reasoning Challenge Abstract reasoning challenge (ARC) aims to examine the inductive reasoning ability in a few-shot scenario (Chollet, 2019) and it has been used as a remarkable testbed to measure the intelligence of LLMs.\nChallenging Tasks towards LLMs' Understanding Extensive recent efforts have been made on designing tasks that challenge the understanding abilities of LLMs"}, {"title": "8 Conclusion", "content": "We introduce PHYSICO, a novel task to assess machines' understanding of physical concepts at different levels. Our experiments reveal that: 1) LLMs lag significantly behind humans on PHYSICO, indicating a lack of deep understanding of the covered concepts; 2) LLMs exhibit the stochastic parrot phenomenon, as they excel at low-level remembering tasks but struggle with high-level understanding tasks; 3) LLMs' poor performance stems from its intrinsic deficiencies, as neither in-context learning nor fine-tuning improves their results."}, {"title": "B.2 Prompts Used for Description Generation and Classification", "content": "Figure 4 and 5 include the prompts used for generation and classification respectively."}]}