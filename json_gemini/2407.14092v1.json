{"title": "Integrated Push-and-Pull Update Model for Goal-Oriented Effective Communication", "authors": ["Pouya Agheli", "Nikolaos Pappas", "Petar Popovski", "Marios Kountouris"], "abstract": "This paper studies decision-making for goal-oriented effective communication. We consider an end-to-end status update system where a sensing agent (SA) observes a source, generates and transmits updates to an actuation agent (AA), while the AA takes actions to accomplish a goal at the endpoint. We integrate the push- and pull-based update communication models to obtain a push-and-pull model, which allows the transmission controller at the SA to decide to push an update to the AA and the query controller at the AA to pull updates by raising queries at specific time instances. To gauge effectiveness, we utilize a grade of effectiveness (GoE) metric incorporating updates' freshness, usefulness, and timeliness of actions as qualitative attributes. We then derive effect-aware policies to maximize the expected discounted sum of updates' effectiveness subject to induced costs. The effect-aware policy at the SA considers the potential effectiveness of communicated updates at the endpoint, while at the AA, it accounts for the probabilistic evolution of the source and importance of generated updates. Our results show the proposed push-and-pull model outperforms models solely based on push- or pull-based updates both in terms of efficiency and effectiveness. Additionally, using effect-aware policies at both agents enhances effectiveness compared to periodic and/or probabilistic effect-agnostic policies at either or both agents.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of cyber-physical systems empowered with interactive and networked sensing and actuation/monitoring agents has caused a shift in focus from extreme to sustainable performance. Emerging networks aim to enhance effectiveness in the system while substantially improving resource utilization, energy consumption, and computational efficiency. The key is to strive for a minimalist design, frugal in resources, which can scale effectively rather than causing network over-provisioning. This design philosophy has crystallized into the goal-oriented and/or semantic communication paradigm, holding the potential to enhance the efficiency of diverse network processes through a parsimonious usage of communication and computation resources [2], [3]. Under an effectiveness perspective, a message is generated and conveyed by a sender if it has the potential to have the desirable effect or the right impact at the destination, e.g., executing a critical action, for accomplishing a specific goal. This promotes system scalability and efficient resource usage by avoiding the acquisition, processing, and transportation of information turning out to be ineffective, irrelevant, or useless.\nMessages, e.g., in the form of status update packets, are communicated over existing networked intelligent systems mostly using a push-based communication model. Therein, packets arrived at the source are sent to the destination based on decisions made by the source, regardless of whether or not the endpoint has requested for or plans to utilize these updates to accomplish a goal. In contrast, in a pull-based model, the endpoint decides to trigger and requests packet transmissions from the source and controls the time and the type of generated updates [4]\u2013[10]. Nevertheless, this model does not consider the availability of the source to generate updates or the usefulness of those updates. To overcome these limitations, we propose an integrated push-and-pull model that involves both agents/sides in the decision-making process, thereby combining push- and pull-based paradigms in a way that mitigates their drawbacks. In either model, decisions at the source or endpoint could influence the effectiveness of communicated updates. Therefore, we can categorize decision policies into effect-aware and effect-agnostic. Under an effect-aware policy, the source adapts its decisions by taking into account the effects of its communicated packets at the endpoint. Likewise, the endpoint raises queries based on the evolution of the source and the expected importance of pulled updates. Under the effect-agnostic policy, however, decisions are made regardless of their consequent effect on the performance.\nIn this work, we study a time-slotted end-to-end status update system where a sensing agent observes an information source and communicates updates/observations in the form of packets with an actuation agent. The actuation agent then takes actions based on the successfully received updates as a means to accomplish a subscribed goal at the endpoint. We develop an integrated push-and-pull model, which allows both agents to make decisions based on their local policies or objectives. In particular, a transmission controller at the sensing agent decides to either send or drop update packets according to their potential usefulness at the endpoint. On the other side, a query controller at the actuation agent also determines the time instances around which the actuator should perform actions in the form of raising queries. In that sense, effective updates are those that result in the right impact and actuation at the endpoint. Those queries, however, are not communicated to the sensing agent. Instead, the actuation agent acknowledges the sensing agent of effective updates. With prior knowledge that the effectiveness of updates depends on the actuator's availability to perform actions, the sensing agent can infer the raised queries using those acknowledgments. A time diagram that shows the processes at both agents is depicted in Fig. 1. We introduce a metric to measure the effectiveness and the significance of updates and derive a class of optimal policies for each agent that makes effect-aware decisions to maximize the long-term expected effectiveness of update packets communicated to fulfill the goal subject to induced costs. To do so, the agent first needs to estimate the necessary system parameters for making the right decisions. Our analytical and simulation results show that the integrated push-and-pull model comes with a higher energy efficiency compared to the push-based model and better effectiveness performance compared to the pull-based one. Moreover, we observe that utilizing effect-aware policies at both agents significantly improves the effectiveness performance of the system in the majority of the cases with a large gap compared to those of periodic and probabilistic effect-agnostic policies at either or both agents. Accordingly, we demonstrate that the solution to find an optimal effect-aware policy at each agent converges to a threshold-based agent decision framework where the agent can timely decide based on an individual lookup map in hand and threshold boundaries computed to satisfy the goal."}, {"title": "A. Related Works", "content": "This paper widely broadens prior work on push-based and (query-) pull-based communications by enabling both agents to make decisions so as to maximize the effectiveness of communicated updates in the system. The pull-based communication model has been widely analyzed but not limited to [4]\u2013[10]. In [4], a new metric called effective age of information (EAoI), which comprises the effects of queries and the freshness of updates in the form of the AoI [11]\u2013[13], is introduced. Query AoI (QAoI), which is similar to the EAoI, is utilized in [5]\u2013[9]. Following the same concept as the QAOI, on-demand Aol is introduced in [10], [14]. Probe (query)-based active fault detection where actuation or monitoring agents adaptively decide to probe sensing agents to detect probable faults at the endpoint is studied in [15]. Most prior work has employed a pull-based communication model and focuses on the freshness and timeliness of information. In this work, we consider multiple information attributes and propose a grade of effectiveness metric to measure the effectiveness of updates, which goes beyond metrics, including AoI, EAOI, QAoI, on-demand Aol, Age of Incorrect Information (AoII) [16], and Value of Information (VoI) [17], [18]. In particular, we focus on the freshness of successfully received updates and the timeliness of performed actions as two attributes of interest through the link level, as well as on the usefulness/significance (semantics) of the updates to fulfill the goal at the source level. This paper extends our prior work [1], which only considers a pull-based model and an effectiveness metric with two freshness and usefulness attributes. As such, [1] conveys a special form of the decision problem we solve here. In this work, we generalize the problem to a push-and-pull communication model, considering that the sensing and actuation agents individually make decisions and converge to a point where they can transmit updates and raise queries, respectively, which maximize the effectiveness of updates and result in the right impact at the endpoint. Importantly, we assume that the source distribution is not known to the actuation agent, and the sensing agent does not have perfect knowledge of the goal. Therefore, the agents must estimate their required parameters separately. This approach is substantially different from the one in our previous work and other state-of-the-art approaches."}, {"title": "B. Contributions", "content": "The main contributions can be briefly outlined as follows.\n\u2022 We develop an integrated push-and-pull update communication model owing to which both agents have decision-making roles in the acquisition and transmitting of updates and take appropriate actions to satisfy the goal, following the paradigm of goal-oriented communications. With this, the system becomes adaptable from an effectiveness viewpoint compared to the conventional push- and pull-based models.\n\u2022 We use a grade of effectiveness metric to capture the timely impact of communicated updates at the endpoint, which relies on the freshness of successfully communicated updates, the timeliness of actions performed, and the usefulness of those updates in fulfilling the goal. Our approach maps multiple information attributes into a unique metric that measures the impact or effect each status update packet traveling over the network can offer.\n\u2022 We obtain optimal model-based control policies for agents that make effect-aware decisions to maximize the discounted sum of updates' effectiveness while keeping the induced costs within certain constraints. To achieve this, we formulate an optimization problem, derive its dual form, and propose an iterative algorithm based on dynamic programming to solve the decision problem separately from each agent's perspective.\n\u2022 We demonstrate that the integrated push-and-pull model offers higher energy efficiency than the push-based model and better effectiveness performance compared to the pull-based one. We also show that applying effect-aware policies at both agents results in better performance than"}, {"title": "II. SYSTEM MODEL", "content": "We consider an end-to-end communication system in which a sensing agent (SA) sends messages in a time-slotted manner to an actuation agent (AA) as a means to take effective action at the endpoint and satisfy a subscribed goal (see Fig. 2). Specifically, the SA observes a source and generates status update packets in each time slot, and a transmission controller decides whether to transmit that observation or not, following a specific policy. We assume that the source has finite-dimensional realizations and that observation at the n-th, \\( \\forall n \\in \\mathbb{N} \\), time slot is assigned a rank of importance \\(U_n\\) from a finite set \\( \\mathcal{V} = \\{v_i | i \\in \\mathcal{I}\\}\\), with \\(\\mathcal{I} = \\{1, 2, ..., |\\mathcal{V}|\\}\\), based on its significance or usefulness for satisfying the goal, measured or judged at the source level.\\footnote{To determine the usefulness of an update, we can use the same metavalue approach proposed in [19, Section III-A].} The elements of \\( \\mathcal{V} \\) are independent and identically distributed (i.i.d.) with probability \\(p_i = P(v_i)\\) for the i-th outcome, where \\(p_\\text{i}(\\cdot)\\) denotes a given probability mass function (pmf).\\footnote{A more elaborated model could consider the importance of a realization dependent on the most recently generated update at the SA. This implies that a less important update increases the likelihood of a more significant update occurring later, which can be captured utilizing a learning algorithm.}\nThe AA is assisted by a query controller that decides to raise queries and pull new updates according to a certain policy. A received packet at the AA has a satisfactory or sufficient impact at the endpoint if that update achieves a minimum effectiveness level subject to the latest query raised and the AA's availability to act on it. An effective update communication is followed by an acknowledgment of effectiveness (E-ACK) signal sent from the AA to the SA to inform about the effective update communication. We assume all transmissions and E- ACK feedback occur over packet erasure channels (PECs), with \\(p_e\\) and \\(p'_e\\) being the erasure probabilities in the forward communication and the acknowledgment links, respectively. Therefore, an E-ACK is not received at the SA due to either ineffective update communication or erasure in the acknowledgment (backward) channel. With this interpretation, channel errors lead to graceful degradation of the proposed scheme. A raised query does not necessarily need to be shared with the SA. As discussed in Section III-C, the SA can deduce the raise of a query or the availability of the AA to take action from a successful E-ACK, given prior knowledge that an update can be effective only if it arrives within the period during which the AA is available to act.\nIn this model, we consider the goal to be subscribed at the endpoint, with the AA fully aware of it. On the other hand, the SA does not initially know the goal but learns which updates could be useful to accomplish the goal based on the received E-ACK and observations' significance. Meanwhile, the AA is not aware of the evolution of the source or the likely importance of observations, attempting to approximate it from arrival updates. Consequently, the agents might use different bases to measure the usefulness of the updates and may need to adjust their criteria or valuation frameworks to account for possible changes in goals over time. Finally, we assume that update acquisition, potential communication, and waiting time for receiving an E-ACK occur in one slot."}, {"title": "A. Communication Model", "content": "The following three strategies can be employed for effective communication of status updates.\n1) Push-based: Under this model, the SA pushes its updates to the AA, taken for instance based on the source evolution, without considering whether the AA has requested them or is available to take any action upon receipt. This bypasses the query controller, enabling the SA to directly influence actions at the AA side.\n2) Pull-based: In this model, the query controller plays a central role in the generation of update arrivals at the AA by pulling those updates from the SA. Here, the AA can only take action when queries are raised. However, this model excludes the SA from generating and sending updates.\n3) Push-and-pull: This model arises from integrating the push- and pull-based models so that the transmission and query controllers individually decide to transmit updates and send queries, respectively. Thereby, the AA is provided with a level of flexibility where it is also able to take some actions beyond query instances within a limited time. As a result, the effectiveness of an update packet depends on both agents' decisions. Dismissing the decision of either agent transforms the push-and-pull model into the push- or pull-based model."}, {"title": "B. Agent Decision Policies", "content": "We propose that the agents can adhere to the following decision policies, namely effect-agnostic and effect-aware, for transmitting updates or raising queries to satisfy the goal."}, {"title": "1) Effect-agnostic:", "content": "This policy uses a predetermined schedule or random process (e.g., Poisson, binomial, or Markov [5]\u2013[9]) to send updates (raise queries) from (by) the SA (AA), without accounting for their impact at the destination. We define a controlled update transmission (query) rate specifying the expected constant number of updates (queries) to be communicated (raised) within a period. Also, as the effect-agnostic policy does not consider what might be happening in the other agent during the time of the decision, there exists an aleatoric uncertainty associated with random updates (queries)."}, {"title": "2) Effect-aware:", "content": "The effect-aware policy takes into consideration the impacts of both agents' decisions at the endpoint. In this regard, the SA (AA) predicts the effectiveness status at the endpoint offered by a sent update that is potentially received at the AA (the usefulness of a possible update at the source). Then, based on this prediction, the agent attempts to adapt transmission (query) instants and send (pull) updates in the right slots. This policy comes with an epistemic uncertainty because decisions are made according to probabilistic estimations, not accurate knowledge. However, such uncertainty can be decreased using learning or prediction techniques."}, {"title": "III. EFFECTIVENESS ANALYSIS METRICS", "content": "To achieve the right effect at the endpoint, an update packet that is successfully received at the AA has to satisfy a set of qualitative attributes, captured by the metrics as follows."}, {"title": "A. Grade of Effectiveness Metric", "content": "We introduce a grade of effectiveness (GoE) metric that comprises several qualitative attributes and characterizes the amount of impact an update makes at the endpoint. Mathematically speaking, the GoE metric is modeled via a composite function \\(GoE_n = (f\\circ g)(I_n)\\) for the n-th time slot. Here, \\(g: \\mathbb{R}^x \\to \\mathbb{R}^y, x > y\\), is a (nonlinear) function of \\(x \\in \\mathbb{Z}_+\\) information attributes \\(I_n \\in \\mathbb{R}\\), and \\(f: \\mathbb{R}^Y \\to \\mathbb{R}\\) is a context-aware function. The particular forms of functions f and g could vary according to different subscribed goals and their relevant requirements.\nIn this paper, without loss of generality, we consider freshness of updates and timeliness of actions as the main contextual attributes. The first comes in the form of age of information (AoI) metric, which is denoted by \\(\\Delta_n\\). The second is measured from the action's lateness, denoted by \\(\\Theta_n\\). Thereby, we can formulate the GoE metric as follows\n\\[GoE_n = f_g(g_{\\Delta}(\\hat{u}_n, \\Delta_n), g_{\\Theta}(\\Theta_n); g_c(C_n))\\]\nwhere \\(C_n\\) represents the overall cost incurred in the n-th time slot. Also, \\(g_{\\Delta}: \\mathbb{R}_\\mathcal{T} \\times \\mathbb{R}_+ \\to \\mathbb{R}_\\mathcal{T}\\), \\(g_{\\Theta}: \\mathbb{R}_\\mathcal{T} \\to \\mathbb{R}_\\mathcal{T}\\), and \\(g_c: \\mathbb{R}_\\mathcal{T} \\to \\mathbb{R}\\) are penalty functions, and \\(f_g : \\mathbb{R}_+ \\times \\mathbb{R}_\\mathcal{T} \\times \\mathbb{R}_\\mathcal{T}\\to \\mathbb{R}_+\\) is a non-decreasing utility function. Moreover, \\(g_{\\Delta}\\), \\(g_{\\Theta}\\), and \\(g_c\\) are non-increasing with respect to (w.r.t.) \\(\\Delta_n\\), \\(\\Theta_n\\), and \\(C_n\\), respectively, while \\(g_{\\Delta}\\) is non-decreasing w.r.t. \\(\\hat{u}_n\\). Here, \\(\\hat{u}_n\\) is the usefulness of the received update from the"}, {"title": "1) Aol:", "content": "Measuring the freshness of correctly received updates at the AA within a query slot, the Aol is defined as \\(\\Delta_n = n - u(n)\\), where \\(\\Delta_0 = 1\\) and \\(u(n)\\) is the slot index of the latest successful update, which is given by\n\\[u(n) = \\max \\{m | m \\leq n, \\beta_m(1 - \\epsilon_m) = 1\\}\\]\nwith \\(\\epsilon_m \\in \\{0, 1\\}\\) being the channel erasure in the m-th slot. In addition, \\(\\beta_m \\in \\{0, 1\\}\\) indicates the query controller's decision, where \\(\\beta_m = 1\\) means pulling the update; otherwise, \\(\\beta_m = 0\\).\n2) Action lateness: The lateness of an action performed in the n-th time slot in relevance to a query raised at the n'-th slot, \\(n' \\leq n\\), is given by\n\\[\\Theta_n = (1-\\beta_n)(n-n'),\\]\nwhich is valid for \\(\\Theta_n < \\Theta_{\\text{max}}\\). Herein, \\(\\Theta_{\\text{max}}\\) shows the width of action window within which the AA can act on each query based on update arrivals from the SA. Outside the dedicated action window for the SA, the AA might undertake other tasks or communicate with other agents. Employing the push-based, pull-based, and push-and-pull update communication models, we have \\(\\Theta_{\\text{max}} = \\infty\\), \\(\\Theta_{\\text{max}} = 1\\), and \\(\\Theta_{\\text{max}} > 1\\), respectively."}, {"title": "B. Special Forms of the GoE", "content": "The GoE metric's formulation in (1) can simply turn into the QAoI and the VoI metrics as special cases. In this regard, we obtain a penalty function of the QAoI such that \\(GoE_n = g_A(\\Delta_n)\\) if we set \\(\\Theta_{\\text{max}} = 1\\), assume linear \\(g_{\\Theta}()\\), and overlook updates' usefulness and cost. In addition, by removing the concepts of query and time, hence the freshness and timeliness in the GoE's definition, we arrive at a utility function of the VoI, i.e., \\(GoE_n = f_g(\\hat{u}_n; g_c(C_n))\\)."}, {"title": "C. Effectiveness Indicator", "content": "An update in the n-th time slot is considered effective at the system level if its \\(GoE_n\\) is higher than a target effectiveness"}, {"title": "IV. MODEL-BASED AGENT DECISIONS", "content": "In this section, we first formulate a decision problem for effect-aware policies at either or both agent(s), cast it as a constrained Markov decision process (CMDP), and then solve it based on the problem's dual form."}, {"title": "A. Problem Formulation", "content": "The objective is to maximize the expected discounted sum of the updates' effectiveness in fulfilling the subscribed goal, where each agent individually derives its decision policy subject to the relevant ensued cost by looking into the problem from its own perspective. Let us define \\( \\pi \\) and \\( \\varpi \\) as the classes of optimal policies for transmission and query controls, respectively. Therefore, we can formulate the decision problem solved at each agent as follows\n\\[\\begin{aligned} P_1: \\max_{\\pi, \\varpi} & \\limsup_{N \\to \\infty} \\frac{1}{N} E \\Bigg[ \\sum_{n=1}^N \\lambda^n E_n \\Bigg] \\\\ \\text{s.t.} & \\limsup_{N \\to \\infty} \\frac{1}{N} E \\Bigg[ \\sum_{n=1}^N \\lambda^n c_{\\gamma}(\\Upsilon_n) \\Bigg] \\leq C_{\\gamma,\\text{max}} \\end{aligned}\\]\nwhere \\(\\lambda \\in [0, 1]\\) indicates a discount factor, and \\( \\gamma \\in \\{\\alpha, \\beta\\} \\) is replaced with \\(\\alpha\\) and \\(\\beta\\) for the update transmission and query decision problems, respectively, at the SA and the AA. Herein, \\(\\Upsilon_n \\in \\{0, 1\\}\\) denotes the decision at the relevant agent, \\(c_{\\gamma}: \\{0, 1\\} \\to \\mathbb{R}\\) is a non-decreasing cost function, and \\(C_{\\gamma,\\text{max}}\\) shows the maximum discounted cost.\nFor either update communication model introduced in Section II-A, optimal decisions at the agent(s) following the effect-aware policy, i.e., \\(\\pi\\) and/or \\(\\varpi\\), are obtained by solving \\(P_1\\) in (6). However, for every agent that employs an effect-agnostic policy, with regard to Section II-B, there is a predefined/given set of decisions denoted by \\(\\bar{\\pi}_{\\alpha}\\) or \\(\\bar{\\pi}_{\\beta}\\) such that \\(\\pi = \\bar{\\pi}_{\\alpha}\\) or \\(\\varpi = \\bar{\\pi}_{\\beta}\\), respectively."}, {"title": "B. CMDP Modeling", "content": "We cast \\(P_1\\) from (6) into an infinite-horizon CMDP denoted by a tuple \\((\\mathcal{S}_\\gamma, \\mathcal{A}_\\gamma, P_\\gamma, r_\\gamma)\\) with components that are defined via the agent that solves the decision problem.\n1) Modeling at the SA: The CMDP at the SA is modeled according to the following components:\nStates - The state of the system \\(S_{\\alpha,n}\\) in the n-th slot from the SA's perspective is depicted by a tuple \\((v_n, \\hat{E}_n)\\) in which \\(v_n\\) is the update's usefulness, and \\(\\hat{E}_n \\in \\{0, 1\\}\\) shows the E- ACK arrival status at the SA after passing the PEC, as defined in Section II. Herein, we have \\(\\hat{E}_n = 0\\) in case \\(E_n = 0\\) or the acknowledgment signal is erased; otherwise, \\(\\hat{E}_n = 1\\). In this regard, \\(S_{\\alpha,n}\\) belongs to a finite and countable state space \\(\\mathcal{S}_\\alpha\\) with \\(|\\mathcal{S}_\\alpha| = 2|\\mathcal{V}|\\) elements.\nActions - We consider \\(a_n\\) the decision for update commu- nication in the n-th slot, which is a member of an action space \\(\\mathcal{A}_\\alpha = \\{0, 1\\}\\). In this space, 0 stands for discarding the update, and 1 indicates transmitting the update.\nTransition probabilities The transition probability from the current state \\(S_{\\alpha,n}\\) to the future state \\(S_{\\alpha,n+1}\\) via taking the action \\(a_n\\) is written by\n\\[\\begin{aligned} P_{\\alpha}(S_{\\alpha,n}, a_n, S_{\\alpha,n+1}) & = Pr((v_{n+1}, \\hat{E}_{n+1}) | (v_n, \\hat{E}_n), a_n) \\\\ & = p_v(v_{n+1})Pr(\\hat{E}_{n+1} | v_n, a_n) \\end{aligned}\\]\nsince \\(\\hat{E}_{n+1}\\) and \\(\\hat{E}_n\\) are independent, and \\(\\hat{E}_n\\) is independent of \\(v_n, \\forall v_n\\). We can derive the conditional probability in (7) as\n\u2022 \\( Pr(\\hat{E}_{n+1} = 0 | v_n, a_n) = Pr(\\hat{u}_{tgt} > a_nv_n) = 1 - P_{\\hat{u}_{tgt}}(a_nv_n),\\)\n\u2022 \\(Pr(\\hat{E}_{n+1} = 1 | v_n, a_n) = P_{\\hat{u}_{tgt}}(a_nv_n),\\)\nwhere \\(\\hat{u}_{tgt}\\) is a mapped target usefulness that the SA considers, \\(P_{\\hat{u}_{tgt}}(v_{tgt}) = \\sum_{v_{i_{tgt}}} P_{i_{tgt}}(v_{i_{tgt}})\\) denotes its cumulative distribution function (CDF), and \\(P_{\\hat{u}_{tgt}}(\\cdot)\\) shows the pmf derived in Section V-B.\nRewards - The immediate reward of moving from the state \\(S_{\\alpha,n}\\) to the state \\(S_{\\alpha,n+1}\\) under the action \\(a_n\\) is equal to \\(r_{\\alpha}(S_{\\alpha,n}, a_n, S_{\\alpha,n+1}) = \\hat{E}_{n+1}\\) where it relies on the E-ACK status in the future state.\nDespite possible erasures over the acknowledgment link, the reward defined in this model fits into the decision problem in (6), where the corresponding objective becomes maximizing the expected discounted sum of E-ACK arrivals. In this sense, \\(\\hat{E}_n\\) at the SA resembles \\(E_n\\) at the AA plus noise in the form of the E-ACK erasure.\n2) Modeling at the AA: For modeling the problem at the AA, we have the components as follows: States We represent the state \\(S_{\\beta,n}\\) in the n-th time slot using a tuple \\((\\hat{U}_n, \\Delta_n, \\Theta_n)\\), where \\(\\hat{U}_n\\) is the usefulness of the received update from the perspective of the endpoint, \\(\\Delta_n\\) is the Aol, and \\(\\Theta_n\\) denotes the action lateness, as modeled in Section III. Without loss of generality, we assume the values of \\(\\Delta_n\\) and \\(\\Theta_n\\) are truncated by the maximum values notated as \\(\\Delta_{\\text{max}}\\) and \\(\\Theta_{\\text{max}}\\), respectively, such that the conditions\n\\[g_\\Delta(\\hat{u}_n, \\Delta_{\\text{max}}-1) \\leq (1 + \\epsilon_\\Delta) g_\\Delta(\\hat{u}_n, \\Delta_{\\text{max}}),\\]\nfor \\(\\hat{u}_n \\in \\mathcal{V}\\), and\n\\[g_\\Theta(\\Theta_{\\text{max}}-1) \\leq (1 + \\epsilon_\\Theta) g_\\Theta(\\Theta_{\\text{max}})\\]"}, {"title": "Proposition 1.", "content": "The CMDP modeled at the SA satisfies the accessibility condition."}, {"title": "Proposition 2.", "content": "The modeled CMDP at the AA meets the weak accessibility condition."}, {"title": "C. Dual Problem", "content": "To solve the decision problem \\(P_2\\) given in (12), we first define an unconstrained form for the problem via dualizing the constraint. Then, we propose an algorithm to compute the decision policies at both agents.\nThe unconstrained form of the problem is derived by writing the Lagrange function \\(L(\\mu; \\pi_\\gamma)\\) as below\n\\[L(\\mu; \\pi_\\gamma) = \\max_{\\pi_\\gamma} \\limsup_{N \\to \\infty} \\frac{1}{N} E \\Bigg[ \\sum_{n=1}^N \\lambda^n E_n - \\mu \\sum_{n=1}^N \\lambda^n c_{\\gamma}(\\Upsilon_n) \\Bigg] + \\mu C_{\\gamma,\\text{max}}\\]\nwith \\(\\mu \\geq 0\\) being the Lagrange multiplier. According to (13), we arrive at the following dual problem to be solved:\n\\[P_3: \\inf_{\\mu \\geq 0} \\max_{\\pi_\\gamma} L(\\mu; \\pi_\\gamma) := h_\\gamma(\\mu)\\]\nwhere \\(h_{\\gamma}(\\mu) = L(\\mu; \\pi_{\\gamma,\\mu}^*)\\) is the Lagrange dual function with \\(\\pi_{\\gamma,\\mu}^*: \\mathcal{S}_\\gamma \\to \\mathcal{A}_\\gamma\\) denoting a stationary \\(\\mu\\)-optimal policy, which is obtained as\n\\[\\pi_{\\gamma,\\mu}^* = \\arg \\max_{\\pi_\\gamma} L(\\mu; \\pi_\\gamma)\\]\nfor \\(\\mu\\) derived in the dual problem \\(P_3\\). As the dimension of the state space \\(\\mathcal{S}_\\gamma\\) is finite for both defined models, the growth condition is met [22]. Also, the immediate reward is bounded below, having a non-negative value according to Section IV-B. In light of the above satisfied conditions, from [22, Corollary 12.2], we can claim that \\(P_2\\) and \\(P_3\\) converge to the same expected values, thus we have\n\\[E_\\gamma = \\inf_{\\mu \\geq 0} h_\\gamma(\\mu) = \\max_{\\pi_\\gamma} u(\\pi_\\gamma)\\]\nunder any class of policy \\(\\pi_\\gamma\\). Owing to the satisfied conditions, there exist non-negative optimal values for the Lagrange multiplier \\(\\mu^*\\) such that we can define \\(u(\\pi_\\gamma) = L(\\mu^*; \\pi_\\gamma)\\) in (16) [22, Theorem 12.8].\nWe can now proceed to derive the optimal policies at the SA and the AA from the decision problem \\(P_3\\) by applying an iterative algorithm in line with the dynamic programming approach based on (13)\u2013(15) [14]."}, {"title": "D. Iterative Algorithm", "content": "The iterative algorithm is given in Algorithm 1 and consists of two inner and outer loops. The inner loop is for computing the \\(\\mu\\)-optimal policy, i.e., \\(\\pi_{\\gamma,\\mu}\\), using the value iteration method. Over the outer loop, the optimal Lagrange multiplier \\(\\mu^*\\) is derived via the bisection search method.\n1) Computing \\(\\pi_{\\alpha,\\mu}\\): Applying the value iteration method, the decision policy is iteratively improved given \\(\\mu\\) from the outer loop (bisection search). Thus, \\(\\pi_{\\gamma,\\mu}(s) \\in \\mathcal{A}_\\gamma\\), \\(\\forall s \\in \\mathcal{S}_\\gamma\\), is updated such that it maximizes the expected utility (value) \\(V^k(s)\\) at the k-th, \\(\\forall k \\in \\mathbb{N}\\), iteration, which is obtained as\n\\[V^k(s) = E[r_k + \\lambda r_{k+1} + \\lambda^2 r_{k+2} + \\cdots | s_k = s] \\approx E[r_k + \\lambda V_{k-1} | s_k = s]\\]\nwhere \\(s_k\\) denotes the state at the k-th iteration, and \\(r_k\\) is the corresponding reward at that state. The approximation in (17) appears after bootstrapping the rest of the discounted sum of the rewards by the value estimate \\(V_{k-1}\\). Under the form of the value iteration for the unichain policy MDPs [23], the optimal value function is derived from Bellman's equation [24], as\n\\[V(s) = \\max_{\\Upsilon \\in \\mathcal{A}_\\gamma} \\sum_{s' \\in \\mathcal{S}_\\gamma} P_\\gamma(s, \\Upsilon, s') [r_{\\gamma,\\mu}(s, \\Upsilon, s') + \\lambda V^{k-1}(s')]\\]\nfor the state \\(s \\in \\mathcal{S}_\\gamma\\). Consequently, the decision policy in that state is improved by\n\\[\\pi_{\\gamma,\\mu}(s) \\in \\arg \\max_{\\Upsilon \\in \\mathcal{A}_\\gamma} \\sum_{s' \\in \\mathcal{S}_\\gamma} P_\\gamma(s, \\Upsilon, s') [r_{\\gamma,\\mu}(s\\Upsilon, s') + \\lambda V^{k-1}(s')].\\]\nIn (18) and (19), we define a net reward function as\n\\[r_{\\gamma,\\mu}(s, \\Upsilon, s') = r_\\gamma(s, \\Upsilon, s') - \\mu c_\\gamma(\\Upsilon),\\]\nwhich takes into account the cost caused by the taken action. The value iteration stops running at the k-th iteration once the following convergence criterion is met [23]:\n\\[sp(V^k - V^{k-1}) < \\epsilon_\\pi\\]\nwhere \\(\\epsilon_\\pi > 0\\) is the desired convergence accuracy, and \\(sp(\\cdot)\\) indicates a span function \\(\\mathbb{R}_+ \\to \\mathbb{R}_+\\) given as\n\\[sp (V) = \\max_{s \\in \\mathcal{S}_\\gamma} V^{k'}(s) - \\min_{s \\in \\mathcal{S}_\\gamma} V^{k'}(s)\\]\nby using the span seminorm [23, Section 6.6.1]. As the decision policies are unichain and have aperiodic transition matrices, the criterion in (21) is satisfied after finite iterations for any value of \\(\\lambda \\in [0, 1]\\) [23, Theorem 8.5.4].\n2) Computing \\(\\mu^*\\): We leverage the bisection search method to compute the optimal Lagrange multiplier over multiple steps in the outer loop based on the derived \\(\\pi_{\\gamma,\\mu}^*\\) from the inner loop. Starting with an initial interval \\([\\mu^-, \\mu^+]\\) such that \\(h_{\\gamma}(\\mu^-)h_{\\gamma}(\\mu^+) < 0\\), the value of the multiplier at the l-th, \\(\\forall l \\in \\mathbb{N}\\), step is improved by \\(\\mu^{(l)} = \\frac{\\mu^- + \\mu^+}{2}\\). As shown in Algorithm 1, at each step, the value of either \\(\\mu^-\\) or \\(\\mu^+\\) and the corresponding decision policy \\(\\pi_{\\gamma,\\mu^-}\\) or \\(\\pi_{\\gamma,\\mu^+}\\), respectively, are updated according to the cost constraint in (12) until a stopping criterion \\(|\\mu^+ - \\mu^-| < \\epsilon_\\mu\\) is reached with the accuracy \\(\\epsilon_\\mu\\). Considering (13) and (14), \\(h_{\\gamma}(\\mu)\\) is a linear non-increasing function of \\(\\mu\\). In this regard, the bisection method searches for the smallest Lagrange multiplier that guarantees the cost constraint. Also, one can show that \\(h_{\\gamma}(\\mu)\\) denotes a Lipschitz continuous function with the Lipschitz constant as below\n\\[\\frac{1}{N} \\mathbb{E} \\Bigg[ \\sum_{n=1}^N \\lambda^n c_\\gamma(\\Upsilon_n) \\Bigg] - \\limsup_{N \\to \\infty} \\mathbb{E} \\Bigg[ \\sum_{n=1}^N \\lambda^n c_\\gamma(\\Upsilon_n) \\Bigg] \\Bigg|\\]\nTherefore, the bisection search converges to the optimal value of \\(\\mu\\) within \\(L \\in \\mathbb{N}\\) finite steps [25, pp. 294].\nAfter the outer loop stops running, we obtain a stationary deterministic decision policy as \\(\\pi^* = \\pi_{\\gamma,\\mu}\\) if the following condition holds:\n\\[C: \\limsup_{N \\to \\infty} \\frac{1}{N} \\mathbb{E} \\Bigg[ \\sum_{n=1}^N c_\\gamma(\\Upsilon_n) \\Bigg] = C_{\\gamma,\\text{max}}.\\]\nOtherwise, the derived policy becomes randomized stationary in the shape of mixing two deterministic policies \\(\\pi_{\\gamma,\\mu^-}\\)"}, {"title": "B. Probability of the Mapped Target Usefulness", "content": "We assume that the mapped target usefulness, i.e., \\(\\hat{U}_{tgt}\\), is a member of the set \\(\\mathcal{V}_{tgt} = \\{v_j | j \\in \\mathcal{I}_{tgt}\\}\\) with i.i.d. elements, where \\(\\mathcal{I}_{tgt} = \\{1, 2, ..., |\\mathcal{V}_{tgt}|\\}\\), and the probability of the j-th element is equal to \\(P_{i_{tgt}}(\\hat{u}_{tgt} = j)\\). Herein, as mentioned earlier, \\(P_{\\hat{u}_{tgt}}(\\cdot)\\) is the estimated pmf of the mapped target usefulness and obtained by\n\\[P_{i_{tgt}}(j) = \\sum_{e \\in \\{0, 1\\}} P_{i_{tgt}}(v_j | \\hat{E} = e)Pr(\\hat{E} = e)\\]\nwhere we find the probability of successfully receiving E-ACK, i.e., \\(e = 1\\), or not, i.e., \\(e = 0\\), as follows\n\\[Pr(\\hat{E} = e) = \\frac{1}{M} \\sum_{m=1}^M 1\\{\\hat{E}_m = e\\}\\]\nwhere \\(\\hat{E}_m\\) indicates the E-ACK arrival status in the m-th slot of the E-horizon. Furthermore, to derive the conditional probability in (26), we first consider the successful arrivals of E-ACK signals such that\n\\[P_{i_{tgt}}(v_j | \\hat{E} = 1) = \\frac{\\sum_{i \\in \\mathcal{I}} P_{v|\\hat{E}=1} (v_i | v_i \\geq v_j)}{\\sum_{j \\in \\mathcal{I}_{tgt}} \\sum_{i \\in \\mathcal{I}} P_{v|\\hat{E}=1} (v_i | v_i \\geq v_j)}.\\]\nThen, we have\n\\[P_{i_{tgt}}(v_j | \\hat{E} = 0) = \\frac{\\sum_{i \\in \\mathcal{I}} P_{v|\\hat{E}=0} (v_i | v_i < v_j)}{\\sum_{j \\in \\mathcal{I}_{tgt}} \\sum_{i \\in \\mathcal{I}} P_{v|\\hat{E}=0} (v_i | v_i < v_j)}.\\]\nThe pmfs \\(P_{v|\\hat{E}=1}(\\cdot)\\) and \\(P_{v|\\hat{E}=0}(\\cdot)\\) in (28) and (29) are associated with an observation's importance rank given the successful and unsuccessful communication of the E-ACK, respectively. In this regard, by applying Bayes' theorem we can derive the following formula:\n\\[P_{v\\hat{E}=e}(v_i) = \\frac{\\frac{1}{M} \\sum_{m=1}^M 1\\{V_m = v_i \\land \\hat{E}_m = e\\}}{Pr(\\hat{E} = e)}\\]"}, {"title": "VI. SIMULATION RESULTS", "content": "In this section, we present simulation results that corroborate our analysis and assess the performance gains in terms of effectiveness achieved by applying different update models and agent decision policies in end-to-end status update systems."}, {"title": "A. Setup and Assumptions", "content": "We study the performance over \\(5 \\times 10^5\\) time slots, which includes the E-horizon and the D-horizon with \\(1 \\times 10^5\\) and \\(4 \\times 10^5\\) slots, respectively. To model the Markovian effect- agnostic policy, we consider a Markov chain with two states, 0 and 1. We assume that the self-transition probability of state 0 is 0.9, while the one for state 1 relies on the controlled update transmission or query rate. Without loss of generality, we assume that the outcome spaces for the usefulness of generated updates, i.e., \\(\\mathcal{V}\\), the usefulness of received updates, i.e., \\(\\hat{\\mathcal{V}}\\), and the mapped target usefulness, i.e., \\(\\mathcal{V}_{tgt}\\), are bounded within the span [0, 1]. For simplicity, we divide each space into discrete levels based on its number of elements in ascending order,"}]}