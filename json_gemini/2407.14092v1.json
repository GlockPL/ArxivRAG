{"title": "Integrated Push-and-Pull Update Model for Goal-Oriented Effective Communication", "authors": ["Pouya Agheli", "Nikolaos Pappas", "Petar Popovski", "Marios Kountouris"], "abstract": "This paper studies decision-making for goal-oriented effective communication. We consider an end-to-end status update system where a sensing agent (SA) observes a source, generates and transmits updates to an actuation agent (AA), while the AA takes actions to accomplish a goal at the endpoint. We integrate the push- and pull-based update communication models to obtain a push-and-pull model, which allows the transmission controller at the SA to decide to push an update to the AA and the query controller at the AA to pull updates by raising queries at specific time instances. To gauge effectiveness, we utilize a grade of effectiveness (GoE) metric incorporating updates' freshness, usefulness, and timeliness of actions as qualitative attributes. We then derive effect-aware policies to maximize the expected discounted sum of updates' effectiveness subject to induced costs. The effect-aware policy at the SA considers the potential effectiveness of communicated updates at the endpoint, while at the AA, it accounts for the probabilistic evolution of the source and importance of generated updates. Our results show the proposed push-and-pull model outperforms models solely based on push- or pull-based updates both in terms of efficiency and effectiveness. Additionally, using effect-aware policies at both agents enhances effectiveness compared to periodic and/or probabilistic effect-agnostic policies at either or both agents.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of cyber-physical systems empowered with interactive and networked sensing and actuation/monitoring agents has caused a shift in focus from extreme to sustainable performance. Emerging networks aim to enhance effectiveness in the system while substantially improving resource utilization, energy consumption, and computational efficiency. The key is to strive for a minimalist design, frugal in resources, which can scale effectively rather than causing network over-provisioning. This design philosophy has crystallized into the goal-oriented and/or semantic communication paradigm, holding the potential to enhance the efficiency of diverse network processes through a parsimonious usage of communication and computation resources [2], [3]. Under an effectiveness perspective, a message is generated and conveyed by a sender if it has the potential to have the desirable effect or the right impact at the destination, e.g., executing a critical action, for accomplishing a specific goal. This promotes system scalability and efficient resource usage by avoiding the acquisition, processing, and transportation of information turning out to be ineffective, irrelevant, or useless.\nMessages, e.g., in the form of status update packets, are communicated over existing networked intelligent systems mostly using a push-based communication model. Therein, packets arrived at the source are sent to the destination based on decisions made by the source, regardless of whether or not the endpoint has requested for or plans to utilize these updates to accomplish a goal. In contrast, in a pull-based model, the endpoint decides to trigger and requests packet transmissions from the source and controls the time and the type of generated updates [4]\u2013[10]. Nevertheless, this model does not consider the availability of the source to generate updates or the usefulness of those updates. To overcome these limitations, we propose an integrated push-and-pull model that involves both agents/sides in the decision-making process, thereby combining push- and pull-based paradigms in a way that mitigates their drawbacks. In either model, decisions at the source or endpoint could influence the effectiveness of communicated updates. Therefore, we can categorize decision policies into effect-aware and effect-agnostic. Under an effect-aware policy, the source adapts its decisions by taking into account the effects of its communicated packets at the endpoint. Likewise, the endpoint raises queries based on the evolution of the source and the expected importance of pulled updates. Under the effect-agnostic policy, however, decisions are made regardless of their consequent effect on the performance.\nIn this work, we study a time-slotted end-to-end status update system where a sensing agent observes an information source and communicates updates/observations in the form of packets with an actuation agent. The actuation agent then takes actions based on the successfully received updates as a means to accomplish a subscribed goal at the endpoint. We develop an integrated push-and-pull model, which allows both agents to make decisions based on their local policies or objectives. In particular, a transmission controller at the sensing agent decides to either send or drop update packets according to their potential usefulness at the endpoint. On the other side, a query controller at the actuation agent also determines the time instances around which the actuator should perform actions in the form of raising queries. In that sense, effective updates are those that result in the right impact and actuation at the endpoint. Those queries, however, are not communicated to the sensing agent. Instead, the actuation agent acknowledges the sensing agent of effective updates. With prior knowledge"}, {"title": "A. Related Works", "content": "This paper widely broadens prior work on push-based and (query-) pull-based communications by enabling both agents to make decisions so as to maximize the effectiveness of communicated updates in the system. The pull-based communication model has been widely analyzed but not limited to [4]\u2013[10]. In [4], a new metric called effective age of information (EAoI), which comprises the effects of queries and the freshness of updates in the form of the AoI [11]\u2013[13], is introduced. Query AoI (QAoI), which is similar to the EAoI, is utilized in [5]\u2013[9]. Following the same concept as the QAOI, on-demand Aol is introduced in [10], [14]. Probe (query)- based active fault detection where actuation or monitoring agents adaptively decide to probe sensing agents to detect probable faults at the endpoint is studied in [15]. Most prior work has employed a pull-based communication model and focuses on the freshness and timeliness of information. In this work, we consider multiple information attributes and propose a grade of effectiveness metric to measure the effectiveness of updates, which goes beyond metrics, including AoI, EAOI, QAoI, on-demand Aol, Age of Incorrect Information (AoII) [16], and Value of Information (VoI) [17], [18]. In particular, we focus on the freshness of successfully received updates and the timeliness of performed actions as two attributes of interest through the link level, as well as on the usefulness/significance (semantics) of the updates to fulfill the goal at the source level. This paper extends our prior work [1], which only considers a pull-based model and an effectiveness metric with two freshness and usefulness attributes. As such, [1] conveys a special form of the decision problem we solve here. In this work, we generalize the problem to a push-and-pull communication model, considering that the sensing and actuation agents individually make decisions and converge to a point where they can transmit updates and raise queries, respectively, which maximize the effectiveness of updates and result in the right impact at the endpoint. Importantly, we assume that the source distribution is not known to the actuation agent, and the sensing agent does not have perfect knowledge of the goal. Therefore, the agents must estimate their required parameters separately. This approach is substantially different from the one in our previous work and other state-of-the-art approaches."}, {"title": "B. Contributions", "content": "The main contributions can be briefly outlined as follows.\nWe develop an integrated push-and-pull update communication model owing to which both agents have decision-making roles in the acquisition and transmitting of updates and take appropriate actions to satisfy the goal, following the paradigm of goal-oriented communications. With this, the system becomes adaptable from an effectiveness viewpoint compared to the conventional push- and pull-based models.\nWe use a grade of effectiveness metric to capture the timely impact of communicated updates at the endpoint, which relies on the freshness of successfully communicated updates, the timeliness of actions performed, and the usefulness of those updates in fulfilling the goal. Our approach maps multiple information attributes into a unique metric that measures the impact or effect each status update packet traveling over the network can offer.\nWe obtain optimal model-based control policies for agents that make effect-aware decisions to maximize the discounted sum of updates' effectiveness while keeping the induced costs within certain constraints. To achieve this, we formulate an optimization problem, derive its dual form, and propose an iterative algorithm based on dynamic programming to solve the decision problem separately from each agent's perspective.\nWe demonstrate that the integrated push-and-pull model offers higher energy efficiency than the push-based model and better effectiveness performance compared to the pull-based one. We also show that applying effect-aware policies at both agents results in better performance than"}, {"title": "II. SYSTEM MODEL", "content": "We consider an end-to-end communication system in which a sensing agent (SA) sends messages in a time-slotted manner to an actuation agent (AA) as a means to take effective action at the endpoint and satisfy a subscribed goal (see Fig. 2). Specifically, the SA observes a source and generates status update packets in each time slot, and a transmission controller decides whether to transmit that observation or not, following a specific policy. We assume that the source has finite-dimensional realizations and that observation at the n- th, \u2200n \u2208 N, time slot is assigned a rank of importance $U_n$ from a finite set V = {vi | i \u2208 I}, with I = {1, 2, ..., |V|}, based on its significance or usefulness for satisfying the goal, measured or judged at the source level. The elements of V are independent and identically distributed (i.i.d.) with probability $p_i$ = P(vi) for the i-th outcome, where $p_1(\u00b7)$ denotes a given probability mass function (pmf).\nThe AA is assisted by a query controller that decides to raise queries and pull new updates according to a certain policy. A received packet at the AA has a satisfactory or sufficient impact at the endpoint if that update achieves a minimum effectiveness level subject to the latest query raised and the AA's availability to act on it. An effective update communication is followed by an acknowledgment of effectiveness (E-ACK) signal sent from the AA to the SA to inform about the effective"}, {"title": "A. Communication Model", "content": "The following three strategies can be employed for effective communication of status updates.\n1) Push-based: Under this model, the SA pushes its updates to the AA, taken for instance based on the source evolution, without considering whether the AA has requested them or is available to take any action upon receipt. This bypasses the query controller, enabling the SA to directly influence actions at the AA side.\n2) Pull-based: In this model, the query controller plays a central role in the generation of update arrivals at the AA by pulling those updates from the SA. Here, the AA can only take action when queries are raised. However, this model excludes the SA from generating and sending updates.\n3) Push-and-pull: This model arises from integrating the push- and pull-based models so that the transmission and query controllers individually decide to transmit updates and send queries, respectively. Thereby, the AA is provided with a level of flexibility where it is also able to take some actions beyond query instances within a limited time. As a result, the effectiveness of an update packet depends on both agents' decisions. Dismissing the decision of either agent transforms the push-and-pull model into the push- or pull-based model."}, {"title": "B. Agent Decision Policies", "content": "We propose that the agents can adhere to the following decision policies, namely effect-agnostic and effect-aware, for transmitting updates or raising queries to satisfy the goal."}, {"title": "1) Effect-agnostic", "content": "This policy uses a predetermined schedule or random process (e.g., Poisson, binomial, or Markov [5]-[9]) to send updates (raise queries) from (by) the SA (AA), without accounting for their impact at the destination. We define a controlled update transmission (query) rate specifying the expected constant number of updates (queries) to be communicated (raised) within a period. Also, as the effect-agnostic policy does not consider what might be happening in the other agent during the time of the decision, there exists an aleatoric uncertainty associated with random updates (queries)."}, {"title": "2) Effect-aware", "content": "The effect-aware policy takes into consideration the impacts of both agents' decisions at the endpoint. In this regard, the SA (AA) predicts the effectiveness status at the endpoint offered by a sent update that is potentially received at the AA (the usefulness of a possible update at the source). Then, based on this prediction, the agent attempts to adapt transmission (query) instants and send (pull) updates in the right slots. This policy comes with an epistemic uncertainty because decisions are made according to probabilistic estimations, not accurate knowledge. However, such uncertainty can be decreased using learning or prediction techniques."}, {"title": "III. EFFECTIVENESS ANALYSIS METRICS", "content": "To achieve the right effect at the endpoint, an update packet that is successfully received at the AA has to satisfy a set of qualitative attributes, captured by the metrics as follows."}, {"title": "A. Grade of Effectiveness Metric", "content": "We introduce a grade of effectiveness (GoE) metric that comprises several qualitative attributes and characterizes the amount of impact an update makes at the endpoint. Mathematically speaking, the GoE metric is modeled via a composite function $GoE_n = (f\\circ g)(I_n)$ for the n-th time slot. Here, $g: \\mathbb{R}_X \\rightarrow \\mathbb{R}_Y, X > Y$, is a (nonlinear) function of $x \\in \\mathbb{Z}_+$ information attributes $I_n \\in \\mathbb{R}$, and $f : \\mathbb{R}_Y \\rightarrow \\mathbb{R}$ is a context- aware function. The particular forms of functions f and g could vary according to different subscribed goals and their relevant requirements.\nIn this paper, without loss of generality, we consider freshness of updates and timeliness of actions as the main contextual attributes. The first comes in the form of age of information (AoI) metric, which is denoted by $A_n$. The second is measured from the action's lateness, denoted by $\u0398_n$. Thereby, we can formulate the GoE metric as follows\n$GoE_n = f_g(g_{\\triangle}(\\hat{u}_n, \\triangle_n), g_{\\Theta}(\\Theta_n); g_C(C_n))$ (1)\nwhere $C_n$ represents the overall cost incurred in the n-th time slot. Also, $g_{\\triangle}: \\mathbb{R}_\\mathbb{T} \\times \\mathbb{R}_+ \\rightarrow \\mathbb{R}_\\mathbb{T}$, $g_{\\Theta}: \\mathbb{R}_\\mathbb{T} \\rightarrow \\mathbb{R}_\\mathbb{T}$, and $g_C: \\mathbb{R}_\\mathbb{T} \\rightarrow \\mathbb{R}$ are penalty functions, and $f_g : \\mathbb{R}_+ \\times \\mathbb{R}_\\mathbb{T} \\times \\mathbb{R}_\\mathbb{T} \\rightarrow \\mathbb{R_0^+}$ is a non-decreasing utility function. Moreover, $g_{\\triangle}$, $g_{\\Theta}$, and $g_C$ are non-increasing with respect to (w.r.t.) $\\triangle_n$, $\\Theta_n$, and $C_n$, respectively, while $g_{\\triangle}$ is non-decreasing w.r.t. $\\hat{u}_n$. Here, $\\hat{u}_n$ is the usefulness of the received update from the"}, {"title": "B. Special Forms of the GoE", "content": "The GoE metric's formulation in (1) can simply turn into the QAoI and the VoI metrics as special cases. In this regard, we obtain a penalty function of the QAoI such that $GoE_n = g_\\mathbb{A}(A_n)$ if we set $\\Theta_{max} = 1$, assume linear $g_{\\Theta}()$, and overlook updates' usefulness and cost. In addition, by removing the concepts of query and time, hence the freshness and timeliness in the GoE's definition, we arrive at a utility function of the VoI, i.e., $GoE_n = f_g(\\hat{u}_n; g_C(C_n))$"}, {"title": "C. Effectiveness Indicator", "content": "An update in the n-th time slot is considered effective at the system level if its $GoE_n$ is higher than a target effectiveness"}, {"title": "IV. MODEL-BASED AGENT DECISIONS", "content": "In this section, we first formulate a decision problem for effect-aware policies at either or both agent(s), cast it as a constrained Markov decision process (CMDP), and then solve it based on the problem's dual form."}, {"title": "A. Problem Formulation", "content": "The objective is to maximize the expected discounted sum of the updates' effectiveness in fulfilling the subscribed goal, where each agent individually derives its decision policy subject to the relevant ensued cost by looking into the problem from its own perspective. Let us define \u03c0 and \u03c9 as the classes of optimal policies for transmission and query controls, respectively. Therefore, we can formulate the decision problem solved at each agent as follows\n$P1: max_{\\pi,\\omega} \\limsup_{N\\rightarrow\\infty} \\mathbb{E} \\Big[\\frac{1}{N} \\sum_{n=1}^{N} \\lambda^{n} E_n\\Big]$\ns.t. $\\limsup_{N\\rightarrow\\infty} \\mathbb{E} \\Big[\\frac{1}{N} \\sum_{n=1}^{N} \\lambda^{n} C_\\gamma(V_n)\\Big] \\leq C_{\\gamma,max}$ (6)\nwhere \u03bb \u2208 [0, 1] indicates a discount factor, and \u03b3\u2208 {\u03b1, \u03b2} is replaced with \u03b1 and \u03b2 for the update transmission and query decision problems, respectively, at the SA and the AA. Herein, $Y_n$ \u2208 {0,1} denotes the decision at the relevant agent, $c_y: {0,1} \\rightarrow \\mathbb{R}$ is a non-decreasing cost function, and $C_{\\gamma,max}$ shows the maximum discounted cost.\nFor either update communication model introduced in Section II-A, optimal decisions at the agent(s) following the effect-aware policy, i.e., \u03c0 and/or \u03c0, are obtained by solving P1 in (6). However, for every agent that employs an effect- agnostic policy, with regard to Section II-B, there is a pre- defined/given set of decisions denoted by $\u00f1_\\alpha$ or $\u03c0_\\beta$ such that $\u03c0_\u03b1 = \u00f1_\u03b1$ or $\u03c0_\u03b2 = \u00f1_\u03b2$, respectively."}, {"title": "B. CMDP Modeling", "content": "We cast P\u2081 from (6) into an infinite-horizon CMDP denoted by a tuple ($S_\\gamma$, $A_\\gamma$, $P_\\gamma$, $r_\\gamma$) with components that are defined via the agent that solves the decision problem.\n1) Modeling at the SA: The CMDP at the SA is modeled according to the following components:\nStates - The state of the system $S_{a,n}$ in the n-th slot from the SA's perspective is depicted by a tuple ($v_n$, $\\hat{E}_n$) in which $v_n$ is the update's usefulness, and $\\hat{E}_n$ \u2208 {0,1} shows the E-ACK arrival status at the SA after passing the PEC, as defined in Section II. Herein, we have $\\hat{E}_n$ = 0 in case $E_n$ = 0 or the acknowledgment signal is erased; otherwise, $\\hat{E}_n$ = 1. In this regard, $S_{\u03b1,n}$ belongs to a finite and countable state space $S_\u03b1$ with |$S_\u03b1$| = 2|V| elements.\nActions - We consider $a_n$ the decision for update commu- nication in the n-th slot, which is a member of an action space A = {0, 1}. In this space, 0 stands for discarding the update, and 1 indicates transmitting the update.\nTransition probabilities The transition probability from the current state $S_{\u03b1,n}$ to the future state $S_{\u03b1,n+1}$ via taking the action $a_n$ is written by\n$P_\u03b1(S_{\\alpha,n}, a_n, S_{\\alpha,n+1}) = Pr((v_{n+1}, \\hat{E}_{n+1}) | (v_n, \\hat{E}_n), a_n)$\n$= p_v(v_{n+1})Pr(\\hat{E}_{n+1} | v_n, a_n)$ (7)\nsince $\\hat{E}_{n+1}$ and $\\hat{E}_n$ are independent, and $\\hat{E}_n$ is independent of $v_n, \\forall v_n$. We can derive the conditional probability in (7) as\n\u2022 $Pr(\\hat{E}_{n+1} = 0 | v_n, a_n) = Pr(\\hat{u}_{tgt} > a_nv_n) = 1 - P_{\\hat{u}_{tgt}}(a_nv_n)$,\n\u2022 $Pr(\\hat{E}_{n+1} = 1 | v_n, a_n) = P_{\\hat{u}_{tgt}}(a_nv_n)$,\nwhere $\\hat{u}_{tgt}$ is a mapped target usefulness that the SA considers, $P_{\\hat{u}_{tgt}}(v_{tgt}) = \\mathbb{E}_{i \\hat{u}_{tgt}i} p_{\\hat{u}_{tgt}}(i_{tgt})$ denotes its cumulative distribution function (CDF), and $p_{\\hat{u}_{tgt}}(\u00b7)$ shows the pmf de- rived in Section V-B.\nRewards - The immediate reward of moving from the state $S_{\u03b1,n}$ to the state $S_{\u03b1,n+1}$ under the action $a_n$ is equal to $r_\u03b1(S_{\\alpha,n}, a_n, S_{\\alpha,n+1}) = E_{n+1}$ where it relies on the E-ACK status in the future state.\nDespite possible erasures over the acknowledgment link, the reward defined in this model fits into the decision problem in (6), where the corresponding objective becomes maximizing the expected discounted sum of E-ACK arrivals. In this sense, $\\hat{E}_n$ at the SA resembles $E_n$ at the AA plus noise in the form of the E-ACK erasure.\n2) Modeling at the AA: For modeling the problem at the AA, we have the components as follows: States - We represent the state $S_{\\beta,n}$ in the n-th time slot using a tuple ($\\hat{u}_n$, $\\triangle_n$, $\u0398_n$), where $\\hat{u}_n$ is the usefulness of the received update from the perspective of the endpoint, $\\triangle_n$ is the Aol, and $\u0398_n$ denotes the action lateness, as modeled in Section III. Without loss of generality, we assume the values of $\\triangle_n$ and $\u0398_n$ are truncated by the maximum values notated as $\\triangle_{max}$ and $\u0398_{max}$, respectively, such that the conditions\n$g_{\\triangle}(\\hat{u}_n, \\triangle_{max}-1) \\leq (1 + \\varepsilon_{\\triangle})g(\\hat{u}_n, \\triangle_{max})$, (8)\nfor $\\hat{u}_n \\in V$, and\n$g_{\\Theta}(\\Theta_{max}-1) \\leq (1 + \\varepsilon_{\\Theta})g_{\\Theta}(\\Theta_{max})$ (9)"}, {"title": "Proposition 1", "content": "The CMDP modeled at the SA satisfies the accessibility condition.\nProof. Given the transition probabilities defined in (7), every state $S_{a,m} \u2208 S_\u03b1, m \u2264 N$, is accessible or reachable from the state $S_{\u03b1,n}$ in finite steps with a non-zero probability, following the policy $\u03c0_\u03b1$. Therefore, the accessibility condition holds for the CMDP model at the SA [21, Definition 4.2.1]."}, {"title": "Proposition 2", "content": "The modeled CMDP at the AA meets the weak accessibility condition.\nProof. We divide the state space $S_\\beta$ into two disjoint spaces of $T_a$ and $T_o = S_\\beta - T_a$, where $T_a$ consists of all the states whose $A_n = 1$, i.e., $T_a = {S_{\\beta,n} | S_{\\beta,n} = (\\hat{u}_j, 1, \u0398_n), \\forall \\hat{u}_j \u2208 \u03bd, \u0398_n = 1, 2, ..., \u0398_{max}}$. Thus, $T_o$ includes the rest of the states with $A_n \u2265 2$. With regard to the transition probabilities derived in (10), all states of $T_o$ are transient under any policy, while every state of an arbitrary pair of two states in $T_a$ is accessible from the other state. Accordingly, the weak accessibility condition in the modeled CMDP at the AA is satisfied according to [21, Definition 4.2.2]."}, {"title": "V. MONTE CARLO PROBABILITY DISTRIBUTION ESTIMATION", "content": "In this section, we leverage the Monte Carlo estimation method to statistically compute the estimated pmfs of the received updates' usefulness from the endpoint's perspective at the AA, and the mapped target usefulness at the SA. To this end, we consider a time interval in the format of an estimation horizon (E-horizon), followed by a decision horizon (D-horizon), as illustrated in Fig. 4. The E-horizon is exclusively reserved for the estimation processes and has a length of M time slots, which is sufficiently large to enable an accurate estimation. The D-horizon represents the long- term time horizon with the sufficiently large length of N\u226b1 slots, as defined in Section IV, during which the agents find and apply their (model) CMDP-based decision policies.\nWithin the E-horizon, the SA does not make any decisions. Instead, it focuses on communicating updates at the highest possible rate while adhering to cost constraints. Once receiving these updates, the AA measures their usefulness, stores them in memory, and sends E-ACK signals for effective updates. The SA logs whether the E-ACK has been successfully received or not in every slot of the E-horizon. Finally, employing the received E-ACK signals at the SA and the measured updates' usefulness at the AA, both agents perform their estimations."}, {"title": "A. Usefulness Probability of Received Updates", "content": "Picking the j-th, \u2200j \u2208 I, outcome from the set V (defined in Section III-A) that corresponds to the received update's usefulness from the endpoint's perspective in the m-th slot of the E-horizon, i.e., $\\hat{v}_m$, the relevant estimated probability of that outcome is given by\n$q_j = P_v(v_j) = \\frac{1}{M}\\sum_{m=1}^{M} 1{\\{\\hat{v}_m = j\\}}.$ (25)"}, {"title": "B. Probability of the Mapped Target Usefulness", "content": "We assume that the mapped target usefulness, i.e., $\\hat{U}_{tgt}$, is a member of the set $V_{tgt} = {v_j | j \u2208 I_{tgt}}$ with i.i.d. elements, where $I_{tgt} = {1,2,...,|V_{tgt}|}$, and the probability of the j-th element is equal to $p_{tgt}(\\hat{u}_{tgt} = j)$. Herein, as mentioned earlier, $p_{\\hat{u}_{tgt}}(\u00b7)$ is the estimated pmf of the mapped target usefulness and obtained by\n$p_{tgt}(j) = \\sum_{e\u2208{0,1}} P_{v|\\hat{E}} (v_j | \\hat{E} = e)Pr(\\hat{E} = e)$ (26)\nwhere we find the probability of successfully receiving E- ACK, i.e., e = 1, or not, i.e., e = 0, as follows\n$Pr(\\hat{E} = e) = \\frac{1}{M}\\sum_{m=1}^{M} 1{\\{\\hat{E}_m = e\\}} (27)\nwhere $\\hat{E}_m$ indicates the E-ACK arrival status in the m-th slot of the E-horizon. Furthermore, to derive the conditional probability in (26), we first consider the successful arrivals of E-ACK signals such that\n$p_{v|\\hat{E}} (v_j | \\hat{E} = 1) = \\frac{\\sum_{i\u2208I} p_{v|\\hat{E}=1} (v_i | v_i \u2265 v_j)}{\\sum_{j\u2208I_{tgt}} \\sum_{i\u2208I} p_{v|\\hat{E}=1} (v_i | v_i \u2265 v_j)}.$ (28)\nThen, we have\n$p_{v|\\hat{E}} (v_j | \\hat{E} = 0) = \\frac{\\sum_{i\u2208I} p_{v|\\hat{E}=0} (v_i | v_i < v_j)}{\\sum_{j\u2208I_{tgt}} \\sum_{i\u2208I} p_{v|\\hat{E}=0} (v_i | v_i < v_j)}.$ (29)\nThe pmfs $p_{v|\\hat{E}=1}(\u00b7)$ and $p_{v|\\hat{E}=0}(\u00b7)$ in (28) and (29) are associated with an observation's importance rank given the successful and unsuccessful communication of the E-ACK, respectively. In this regard, by applying Bayes' theorem we can derive the following formula:\n$P_{v|\\hat{E}=e}(v_i) = \\frac{\\sum_{m=1}^{M} 1{\\{v_m = v_i \u2227 \\hat{E}_m = e\\}}}{\\mathbb{P}r(\\hat{E} = e)}$ (30)"}, {"title": "VI. SIMULATION RESULTS", "content": "In this section, we present simulation results that corroborate our analysis and assess the performance gains in terms of effectiveness achieved by applying different update models and agent decision policies in end-to-end status update systems."}, {"title": "A. Setup and Assumptions", "content": "We study the performance over 5 \u00d7 105 time slots, which includes the E-horizon and the D-horizon with 1 x 105 and 4 \u00d7 105 slots, respectively. To model the Markovian effect- agnostic policy, we consider a Markov chain with two states, 0 and 1. We assume that the self-transition probability of state 0 is 0.9, while the one for state 1 relies on the controlled update transmission or query rate. Without loss of generality, we assume that the outcome spaces for the usefulness of generated updates, i.e., V, the usefulness of received updates, i.e., $\\hat{V}$, and the mapped target usefulness, i.e., $V_{tgt}$, are bounded within the span [0, 1]. For simplicity, we divide each space into discrete levels based on its number of elements in ascending order,"}, {"title": "VII. CONCLUSION", "content": "We investigated decision-making for enhancing the effectiveness of updates communicated in the end-to-end status update system based on a push-and-pull communication model. To this end, we considered that the SA observes an information source and generates updates, and its transmission controller decides whether to send them to the AA or not. On the other side, the AA is responsible for acting based on the updates that are successfully received and the raised queries to accomplish a subscribed goal at the endpoint. After defining a GoE metric, we formulated the decision problem of finding optimal effect- aware policies that maximize the expected discounted sum of the update's effectiveness subject to cost constraints. Using the dual problem, we cast it to a CMDP solved separately for each agent based on different model components and proposed an iterative algorithm to obtain the decision policies. Our results established that the push-and-pull model, on average, outperforms the push- and pull-based models in terms of energy efficiency and effectiveness, respectively. Furthermore, effect-aware policies at both agents significantly enhance the effectiveness of updates with a considerable difference in comparison to those of periodic and probabilistic effect-agnostic policies used at either or both agent(s). Finally, we proposed a threshold-based decision policy complemented by a tailored lookup map for each agent that employs effect-aware policies. Future works could explore multi-SA scenarios with varying sensing abilities and realizations' time-dependent importance."}]}