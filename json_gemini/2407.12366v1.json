{"title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models", "authors": ["Gengze Zhou", "Yicong Hong", "Zun Wang", "Xin Eric Wang", "Qi Wu"], "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.", "sections": [{"title": "1 Introduction", "content": "Motivating by the considerable advances in Large Language Models (LLMs), there is an emerging effort to utilize these models for instructional tasks within robotic navigation [10, 46, 53, 80, 81]. This development highlights two core capacities of LLMs: Firstly, the ability to generalize commonsense knowledge reasoning and efficiently process free-form linguistic inputs, thanks to learning enormous amounts of textual data from the web. Secondly, the interpretative of LLMs to provide navigational reasoning explicitly in a human interpretable way and the associated communicative potential during interaction with humans. Several"}, {"title": "2 Related Works", "content": "Vision-and-Language Navigation (VLN) The pursuit of developing a universal navigation agent capable of following free-form linguistic directives to LLAMA2-7B [67] have 6.74 billion parameters while DUET [15] has only 0.18 billion."}, {"title": "3 Method", "content": "The architecture of NavGPT-2, as depicted in Figure 2, comprises two primary components: a Large Vision-Language Model (VLM) and a navigation policy network. Within the VLM, visual observations and instructions are processed by a component referred to as the Q-former to extract image tokens. These tokens serve as the input visual content for the LLM, enabling it to generate navigational reasoning. For action prediction, the model employs both hidden representations of image tokens and instruction text tokens that have been processed by the LLM encoder as the input features.\nProblem Formulation. Given an instruction composed of L word embeddings W = {wi}Li=1, an agent is required to follow the instruction to navigate in a pre-defined undirected graph G = {V,E}, where V denotes the navigable nodes, E denotes the connectivity edges. At step t, the agent perceives the surrounding environment through the observation of a set of RBG views for each connected navigable node candidate Ot = {\u3008oi, ai)}Ni=1, where N denotes the number of candidate nodes, each unique view is denoted as oi(i < N), with its associated angle direction with respect to the agent's heading represented as ai(i < N). The agent predicts the subsequent action by selecting the relative angle at from Ot, the policy parametrized by \u0398 that the agent is required to learn is \u03c0(at|W, Ot; \u0398).\n3.1 VLMs Latent as Visual-Linguistic Representation\nIn this section, we discuss the model design within the Large Vision-Language Model, how to enable frozen LLMs to generate navigational reasoning, and how to utilize VLM features in the navigation policy network.\nVisual Aligning with LLMs. To effectively encode multiple-view images in the environment and construct spatial perception for navigation reasoning in a frozen LLM, we adopt the Q-former [36] design and encode each view into fixed-length visual tokens, shown in Figure 2. Specifically, for a candidate view image oi, we incorporate a frozen ViT-g/14 from EVA-CLIP [24] as the vision encoder to extract visual feature Z = ViT(oi). These visual features are later cross-attended [68] with 32 learnable queries embedding Qi \u2208 R32\u00d7768 which is self-attended with text embedding W of the instruction first to obtain the instruction-aware image queries Q'i [20]. These queries are fed to the LLM after a linear projection W as the image tokens H = Q'iW.\nNavigation System Prompt. To inform LLM of the orientation of each candidate's view, we inject the direction information into the navigation prompt in structure input format \"Candidate i, facing ai, {direction}\", shown in Figure 3. Moreover, we introduce special tokens <IMG>, </IMG>, <INST> and </INST> to insert images tokens and instructions into the prompt. Furthermore, we generate 10K navigational reasoning data from the R2R training set [6] and perform instruction-tuning to the Q-former and the projection layer on the prediction tokens, using its original auto-regressive training objective, detail is discussed in section 3.3. Up to now, a complete VLM has been built and can generate navigation reasoning with a standard LLM decoding process, shown in the bottom right of Figure 2.\n3.2 Graph Based Navigation Policy\nWe identify the key difficulty of fine-tuning LLMs as VLN agents lies in the LLMs' inadequate comprehension of spatial structures, coupled with their limited ability to model the agent's long-range experiences during the navigation process. Therefore, we harness a topological graph-based navigation policy [15] for effective"}, {"title": "4 Experiments", "content": "Evaluation Metrics. We adopt a comprehensive set of navigation metrics to evaluate performance [6], including Trajectory Length (TL), which measures the average path length in meters; Navigation Error (NE), the average distance between the final and target locations; Success Rate (SR), the percentage of paths with NE less than 3 meters; Oracle Success Rate (OSR), the success rate under an ideal stop policy; Success Rate penalized by Path Length (SPL) [5], which combines success with efficiency considerations; Normalized Dynamic Time Warping (NDTW) [30], assessing the fidelity between predicted and annotated paths; and Success weighted by Normalized Dynamic Time Warping (SDTW), a composite metric rewarding both navigation success and path fidelity.\n4.1 Implementation Details\nWe build NavGPT-2 based on InstructBLIP [20] and exploit four variations of LLMs, including FlanT5-XL (3B), FlanT5-XXL (11B), Vicuna-7B and Vicuna-13B [18, 19]. All models apply the same vision encoder (ViT-g/14 [24]), and all parameters of the vision encoder and LLMs are kept frozen during the entire training process. In stage one, we initialize the model from pretrained InstructBLIP checkpoints and train the Q-former for 200K steps with a batch size of 8. The AdamW optimizer [47] is employed and configured with \u03b21 = 0.9, \u03b22 = 0.999 and a weight decay of 0.05. To optimize learning efficiency, a linear warmup strategy is applied to the learning rate for the first 1,000 steps, gradually increasing it from 10\u22128 to 10\u22125, which is then followed by a cosine decay to a minimum learning rate of 0. In stage two, we freeze the pretrained VLM from stage one and finetune the downstream policy network with a batch size of 2 and a learning rate of 10\u22125. Our best model is trained on the combination of the R2R and the synthetic data from PREVALENT [27]. All experiments are conducted on a single NVIDIA A100 GPU.\n4.2 Comparison with State of the Art\nWe compare the single-run performance on the R2R dataset with previous SOTA methods in Table 1. Specifically, we categorized them into distinct categories:\nVLN Specialists with Vision-Language-Action Pretraining [3,14, 15,26\u201328,31,57,75]: These methods are initialized from general vision-language models [40,63] and incorporate VLN-tailored pretraining with auxiliary tasks such as masked language modeling (MLM) [21], masked region classification (MRC) [48], and single-step action prediction (SAP) [14] before fine-tuning on downstream VLN tasks.\nZero-shot Methods [10,46,81]: Methods that apply GPT-4 to zero-shot VLN using different textual inputs and prompting strategies.\nMethods finetuning LLMs [41,53,80]: Methods that finetune LLMs on VLN data with alternative modification on multimodality LLMs combining VLN specialized model designs.\nBaseline: We construct a baseline method based on the global action branch of DUET [15], referring as DUET (w/o local branch) in Table 1. This model shares the same architecture for the action planning policy network as"}, {"title": "4.3 Navigational Reasoning Generation", "content": "In Figure 1 we show the navigational reasoning generated by NavGPT-2 during navigation. NavGPT-2 can construct a comprehensive perception of the surroundings, as shown in the example on the left, NavGPT-2 recognizes the fireplace, dining room, hallway, and their relative locations. Moreover, it can reason about the navigation progress and identify associated sections of the instruction inferring the next step, and can even infer the expected observation."}, {"title": "4.6 Ablation Study", "content": "We ablate the core design choices applied in this paper, including the effect of incorporating a navigation-specific policy model, pretraining the Q-former with reasonings and leveraging different LLMs in NavGPT-2.\nEffect of Policy Network. In Table 5, we study the necessity of applying a navigation-specific policy model in NavGPT-2. To achieve this, we remove all the visual-language cross-attention layers in the Q-former and policy network and use only a single graph-aware self-attention layer followed by a single feed-forward layer to predict the action (Model#2). By doing so, we force the LLM to take over the visual-textual based decision-making to exploit its navigational capability. It is clear from the drastic drop in performance that a frozen LLM is incapable of inferring effective representations that indicate a correct action. Although tuning the LLM should improve the results, we can see from previous work that tuning a LLaMA2-7B model on the full R2R data and GPT-4-augmented data only leads to a 43% SR in Val Unseen [53], still far behind NavGPT-2, implying that this approach might be infeasible with existing LLMs.\nEffect of Pretraining with Reasonings. Following the above, we also high- lighted the navigational reasoning abilities that NavGPT-2 unleashed from frozen LLMs in Figure 4. Additionally, we can see from Model#3 of Table 5 that the pretraining of Q-former on reasonings brings slight improvement to the success rates of the model. It is expected that such information, containing rich spatial descriptions and visual landmarks, facilitates the Q-former to produce better textual representations of the multi-view observations.\nEffect of Different LLMs. Technically, NavGPT-2 can incorporate a wide range of different LLMs, however, their performance on VLN might not scale"}, {"title": "5 Conclusion", "content": "In this work, we strive to eliminate the gap between LLMs-based agents and VLN-specialised agents, while maintaining the interpretative intrinsic of LLMs to generate navigational reasonings during navigation. Through comprehensive experimentation, we highlight the critical aspects of integrating LLMs with downstream navigation policy networks. It is demonstrated that the Vision-Language Model (VLM) latent serves as a superior and more efficient visual- linguistic representation, enabling policy networks to learn better alignment between vision, language, and action. Our approach offers a scalable framework to leverage the broad language comprehension capabilities of LLMs, paving the way for the development of versatile navigation agents capable of interacting with humans and understanding free-form human intentions with greater efficacy."}, {"title": "A DUET Revisit", "content": "NavGPT-2 exploit the similar design adapted from Dual-scale Graph Transformer (DUET) [15] as the downstream navigation policy. It includes a text encoder to encode instructions, a global and a local branch to enable coarse-scale and fine-scale cross-modal reasoning.\nA.1 Text Embedding and Visual Embedding\nFor the text encoder, DUET utilizes a 12-layer transformer initialized from LXMERT [63]. For visual embedding, the visual observation at each node is 36 view images from 12 horizontal directions times 3 vertical directions. To distinguish these nodes, a directional embedding $E_{ang}$ of the absolute angle for each view is added to the visual feature $Z_u$ extracted by the vision encoder. Moreover, since DUET inputs all 36 view images to construct the spatial observation for the model, the navigable adjacent nodes are only observed at a few view images, denoted as navigable views. A navigable embedding $E_{nav}$ is added to the visual features. The final visual embedding is sent to a 2 layers transformer to encode the spatial relations between views and obtain the panoramic view embeddings:\n$H_{pano} = SelfAttn (Z_u + E_{ang} + E_{nav}).$\nOn the contrary, NavGPT-2 only inputs the navigable views, thus the directional embedding $E_{ang}$ and the navigable embedding $E_{nav}$ are removed in the downstream policy, instead we directly add the step embedding and location embedding before sending to the 2 layers transformer.\nA.2 DUET Local Branch\nNavGPT-2 adopt the same navigation policy network architecture as the DUET global branch, discussed in \u00a73.2, so we omit the explanation of the global branch in DUET. In this section, we introduce the local branch of DUET. This branch performs action prediction based on the current node's instruction and egocentric observation. No graph information is provided besides the local observation."}, {"title": "A.3 Dynamic Fusion", "content": "The final action prediction of DUET is performed by dynamically fusing the action predicted by local and global branches. The local branch predicts actions within the adjacent nodes $V_a$. It is incongruent with the action space used by the global branch, which chooses the next action from all nodes $V_t$ in the constructed graph at step $t$. To reconcile this discrepancy, the local action scores $s_a$ are transformed encompassing options such as \"stop\" and $V_t$, into a representation suitable for the global action space by summing up scores of visited nodes in $V_a$ as a backtrack score $s_b$:\n$s_{i}^{t} = { s_{b},  if V_{i} \u2208 V_{t} \u2212 V_{a},  s_{i},  otherwise.}$\nThis adjustment facilitates navigation toward other unexplored nodes not directly linked to the current node, necessitating the agent to retrace its steps through neighboring nodes that have previously been visited. The final navigation score is given by:\n$s_i = \u03c3s_i^t + (1 \u2212 \u03c3)s_{i}^{t}$,\nwhere $s_i^t$ is the logits from global branch, $\u03c3$ is a learnable scalar for fusion."}, {"title": "B GPT-4V Prompt", "content": "The prompt used for GPT-4V to generate navigation reasoning, discussed in section \u00a73.3 is shown in Figure 5."}, {"title": "C Additional Rerults", "content": "In this section, we conduct additional experiments to illustrate the choice of navigation policy network for NavGPT-2 and the effectiveness of LLM features. To align the same training schema of the navigation policy, we conduct the experiments for DUET initiating it from LXMERT without VLN specialized pretraining."}, {"title": "C.1 Effect of Vision Encoder", "content": "Because NavGPT-2 exploits a stronger vision encoder [24], we conduct an ablation study on the original DUET to investigate the performance gain brought by the vision encoder. As shown in Table 7, after switching the visual representation to the stronger vision feature same as NavGPT-2, little performance gain is observed for the DUET global branch (Model # 3 compared to Model # 2). We hypothesize this is due to the global branch for DUET performing vision-language alignment on a coarse scale, while the fine-grained alignment is performed in the local branch. Therefore, the main performance gain in NavGPT-2 is not contributed by the stronger vision encoder but the better representation from LLM hidden."}, {"title": "C.2 Effect of VLN Pretrain", "content": "We consider the same training scale and the same training schema of DUET as NavGPT-2, without pertaining auxiliary VLN tasks and directly finetuning on the VLN dataset. Under the same training schema and scale of data, NavGPT-2 performs significantly better than the original DUET, shown in Table 7. This showcases the superiors of LLM features that enable the learning of cross-modality alignment in the downstream task when the visual feature is projected to the"}, {"title": "D Limitations and Future Work", "content": "Although NavGPT-2 could generate navigation reasoning to some extent, it is hard to evaluate the effectiveness of these reasonings, since it is set as a single-step reasoning based on local observation and does not model the navigation history in the VLM. Instead, such history information is encoded in the downstream navigation policy. As a result, the consistency between navigation reasonings is underexplored. Moreover, the reasoning and action predicted by downstream navigation policy are not strictly synchronized in NavGPT-2, such synchronization could be done either explicitly by tuning LLM with the same supervision signal of action or by collaborating with the reasoning generation loss during fine-tuning the downstream policy network, we leave the synchronization to future work. Finally, the communicative capability of NavGPT-2 is not evaluated in this work, we suggest investigating the communicative ability of LM-based VLN agents and the synchronization between their reasoning and actions as a future direction."}, {"title": "E Broader Effect", "content": "Our research endeavors to leverage Large Vision-Language Models (VLM) to develop VLN agents, while preserving the linguistic prowess of VLMs for explaining action predictions in natural language. We posit that the inherent communicative capability, commonsense knowledge, and broad linguistic comprehension of VLM constitute the cornerstone for creating instruction-following navigation agents with generalizability. NavGPT-2 illuminates the reasonings of VLM throughout the navigation process explicitly and interpretably. Due to safety and ethical considerations, we currently conduct all experiments using the open-source Vision- and-Language Navigation dataset within a simulated environment, which ensures controlled agent behavior. Concurrently, we acknowledge that the potential prac- tical application of this technology warrants further exploration, particularly in terms of action and reasoning synchronization, which remains an underexplored area. Notably, we observe the propensity of VLMs to hallucinate non-existent scenes or objects and fail to identify object directions, shown in Figure 6, which is also a common issue within VLM research. Future investigations are essential to address how to harmonize VLM action and reasoning and to enhance the agent's ability to self-explain in a manner intelligible to humans, a critical consideration for ensuring safety in real-world applications."}]}