{"title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models", "authors": ["Gengze Zhou", "Yicong Hong", "Zun Wang", "Xin Eric Wang", "Qi Wu"], "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.", "sections": [{"title": "1 Introduction", "content": "Motivating by the considerable advances in Large Language Models (LLMs), there is an emerging effort to utilize these models for instructional tasks within robotic navigation [10, 46, 53, 80, 81]. This development highlights two core capacities of LLMs: Firstly, the ability to generalize commonsense knowledge reasoning and efficiently process free-form linguistic inputs, thanks to learning enormous amounts of textual data from the web. Secondly, the interpretative of LLMs to provide navigational reasoning explicitly in a human interpretable way and the associated communicative potential during interaction with humans. Several"}, {"title": "2 Related Works", "content": "Vision-and-Language Navigation (VLN) The pursuit of developing a universal navigation agent capable of following free-form linguistic directives to\nLLAMA2-7B [67] have 6.74 billion parameters while DUET [15] has only 0.18 billion."}, {"title": "3 Method", "content": "The architecture of NavGPT-2, as depicted in Figure 2, comprises two primary components: a Large Vision-Language Model (VLM) and a navigation policy network. Within the VLM, visual observations and instructions are processed by a component referred to as the Q-former to extract image tokens. These tokens serve as the input visual content for the LLM, enabling it to generate navigational reasoning. For action prediction, the model employs both hidden representations of image tokens and instruction text tokens that have been processed by the LLM encoder as the input features.\nProblem Formulation. Given an instruction composed of L word embeddings W = {w_i}_{i=1}^L, an agent is required to follow the instruction to navigate in a pre-defined undirected graph G = {V,E}, where V denotes the navigable nodes, E denotes the connectivity edges. At step t, the agent perceives the surrounding environment through the observation of a set of N RBG views for each connected navigable node candidate O_t \\triangleq {\\langle o_i, a_i \\rangle}_{i=1}^N, where N denotes the number of candidate nodes, each unique view is denoted as o_i (i < N), with its associated angle direction with respect to the agent's heading represented as a_i (i < N). The agent predicts the subsequent action by selecting the relative angle a_t from O_t, the policy parametrized by \\Theta that the agent is required to learn is \\pi(a_t | W, O_t; \\Theta).\n3.1 VLMs Latent as Visual-Linguistic Representation\nIn this section, we discuss the model design within the Large Vision-Language Model, how to enable frozen LLMs to generate navigational reasoning, and how to utilize VLM features in the navigation policy network.\nVisual Aligning with LLMs. To effectively encode multiple-view images in the environment and construct spatial perception for navigation reasoning in a frozen LLM, we adopt the Q-former [36] design and encode each view into fixed-length visual tokens, shown in Figure 2. Specifically, for a candidate view image o_i, we incorporate a frozen ViT-g/14 from EVA-CLIP [24] as the vision encoder to extract visual feature Z = ViT(o_i). These visual features are later cross-attended [68] with 32 learnable queries embedding Q_i \\in \\mathbb{R}^{32 \\times 768} which is self-attended with text embedding W of the instruction first to obtain the instruction-aware image queries Q'_i [20]. These queries are fed to the LLM after a linear projection W as the image tokens H = Q'_iW.\nNavigation System Prompt. To inform LLM of the orientation of each candidate's view, we inject the direction information into the navigation prompt in structure input format \"Candidate i, facing a_i, {direction}\", shown in Figure 3. Moreover, we introduce special tokens <IMG>, </IMG>, <INST> and </INST> to insert images tokens and instructions into the prompt. Furthermore, we generate 10K navigational reasoning data from the R2R training set [6] and perform instruction-tuning to the Q-former and the projection layer on the prediction tokens, using its original auto-regressive training objective, detail is discussed in section 3.3. Up to now, a complete VLM has been built and can generate navigation reasoning with a standard LLM decoding process, shown in the bottom right of Figure 2.\n3.2 Graph Based Navigation Policy\nWe identify the key difficulty of fine-tuning LLMs as VLN agents lies in the LLMs' inadequate comprehension of spatial structures, coupled with their limited ability to model the agent's long-range experiences during the navigation process. Therefore, we harness a topological graph-based navigation policy [15] for effective"}, {"title": "3.3 Multi-stage Learning for Action and Reasoning", "content": "We perform a two-stage training to learn action prediction and navigation reasoning generation for LLM. In the first stage, we initialize the model from InstructBLIP [20] after visual instruction-tuning on academic-task-oriented VQA datasets. We follow the same training schema to only finetune the Q-former with a frozen LLM and vision encoder on the collected navigation reasoning data, shown as the yellow blocks in Figure 2. In the second stage, we connect the pretrained VLM with the downstream navigation policy and only finetune the policy network with frozen VLM, shown as the red blocks in Figure 2.\nData Acquisition and Curation. To train VLM with navigational reasoning ability, we propose an automatic data generation pipeline with GPT-4V. We discard historical modeling for VLM and consider the situation when spanning the agent at the intermediate steps along the ground truth trajectory. We asked GPT-4V to determine the next step toward completing the instruction based on the current observation of the surroundings and relevant landmarks. We define the single-step navigation reasoning trace as describing the immediate environment and specifying the direction or action that will be taken to proceed. Details of the prompt are in the appendix.\nWe randomly select 10k intermediate steps from the trajectory in the R2R [6] training set, using the equirectangular projected panoramic image centring at the agent's heading direction as the image input for GPT-4V, shown in Figure 4."}, {"title": "Policy learning with DAgger", "content": "Policy learning with DAgger. When fine-tuning the downstream navigation policy network, we follow previous work to combine Behaviour cloning and DAgger loss [61]:\n\\mathcal{L}_{BC} = - \\sum_{t=1}^T log \\pi(v | W, G_t), \\quad \\mathcal{L}_{DAG} = - \\sum_{t=1}^T log \\pi(\\hat{v} | W, \\hat{G}_t), (3)\nwhere v indicates the ground truth action, \\hat{v} denotes the pseudo label determined by the shortest path toward the destination from the partial graph G_t generated by the agent through on-policy action sampling. The overall loss function is given by \\mathcal{L} = A\\mathcal{L}_{BC} + \\mathcal{L}_{DAG}, where A is a balancing factor."}, {"title": "4 Experiments", "content": "Evaluation Metrics. We adopt a comprehensive set of navigation metrics to evaluate performance [6], including Trajectory Length (TL), which measures the average path length in meters; Navigation Error (NE), the average distance between the final and target locations; Success Rate (SR), the percentage of paths with NE less than 3 meters; Oracle Success Rate (OSR), the success rate under an ideal stop policy; Success Rate penalized by Path Length (SPL) [5], which combines success with efficiency considerations; Normalized Dynamic Time Warping (NDTW) [30], assessing the fidelity between predicted and annotated paths; and Success weighted by Normalized Dynamic Time Warping (SDTW), a composite metric rewarding both navigation success and path fidelity.\n4.1 Implementation Details\nWe build NavGPT-2 based on InstructBLIP [20] and exploit four variations of LLMs, including FlanT5-XL (3B), FlanT5-XXL (11B), Vicuna-7B and Vicuna-13B [18, 19]. All models apply the same vision encoder (ViT-g/14 [24]), and all parameters of the vision encoder and LLMs are kept frozen during the entire training process. In stage one, we initialize the model from pretrained InstructBLIP checkpoints and train the Q-former for 200K steps with a batch size of 8. The AdamW optimizer [47] is employed and configured with \\beta_1 = 0.9, \\beta_2 = 0.999 and a weight decay of 0.05. To optimize learning efficiency, a linear warmup strategy is applied to the learning rate for the first 1,000 steps, gradually increasing it from 10^{-8} to 10^{-5}, which is then followed by a cosine decay to a minimum learning rate of 0. In stage two, we freeze the pretrained VLM from stage one and finetune the downstream policy network with a batch size of 2 and a learning rate of 10^{-5}. Our best model is trained on the combination of the R2R and the synthetic data from PREVALENT [27]. All experiments are conducted on a single NVIDIA A100 GPU."}, {"title": "4.2 Comparison with State of the Art", "content": "We compare the single-run performance on the R2R dataset with previous SOTA methods in Table 1. Specifically, we categorized them into distinct categories:\nVLN Specialists with Vision-Language-Action Pretraining [3, 14, 15,26\u201328,31,57,75]: These methods are initialized from general vision-language models [40, 63] and incorporate VLN-tailored pretraining with auxiliary tasks such as masked language modeling (MLM) [21], masked region classification (MRC) [48], and single-step action prediction (SAP) [14] before fine-tuning on downstream VLN tasks.\nZero-shot Methods [10,46,81]: Methods that apply GPT-4 to zero-shot VLN using different textual inputs and prompting strategies.\nMethods finetuning LLMs [41,53,80]: Methods that finetune LLMs on VLN data with alternative modification on multimodality LLMs combining VLN specialized model designs.\nBaseline: We construct a baseline method based on the global action branch of DUET [15], referring as DUET (w/o local branch) in Table 1. This model shares the same architecture for the action planning policy network as"}, {"title": "4.3 Navigational Reasoning Generation", "content": "In Figure 1 we show the navigational reasoning generated by NavGPT-2 during navigation. NavGPT-2 can construct a comprehensive perception of the surroundings, as shown in the example on the left, NavGPT-2 recognizes the fireplace, dining room, hallway, and their relative locations. Moreover, it can reason about the navigation progress and identify associated sections of the instruction inferring the next step, and can even infer the expected observation."}, {"title": "4.4 The Effect of Data Amount", "content": "In Table 3 we initialize DUET from LXMERT and compare the model performance when finetuning 10%, 50%, and full R2R training data. NavGPT-2 outperforms all DUET variants in SR on the validation unseen split, and it reaches the same performance as DUET with full R2R data when feeding with 50% less data, showcasing the data efficiency of utilizing LLMs latent as the vision-language representation and the benefits for downstream navigation policy learning."}, {"title": "4.5 Cross Dataset Generalization Ability", "content": "We evaluate the generalization ability of NavGPT-2 in two aspects: generalize to free-form language instructions and to various unseen environments.\nGeneralize to Free-form Language Instructions. To assess NavGPT-2's comprehension of various forms of language instruction, we evaluate the zero-shot performance of NavGPT-2 on the RxR dataset. The RxR dataset is characterized by its instructions of finer granularity, detailing rich landmarks and encompassing longer trajectories. As shown in Table 4, NavGPT-2 outperforms DUET by 3.67%"}, {"title": "4.6 Ablation Study", "content": "We ablate the core design choices applied in this paper, including the effect of incorporating a navigation-specific policy model, pretraining the Q-former with reasonings and leveraging different LLMs in NavGPT-2.\nEffect of Policy Network. In Table 5, we study the necessity of applying a navigation-specific policy model in NavGPT-2. To achieve this, we remove all the visual-language cross-attention layers in the Q-former and policy network and use only a single graph-aware self-attention layer followed by a single feed-forward layer to predict the action (Model#2). By doing so, we force the LLM to take over the visual-textual based decision-making to exploit its navigational capability. It is clear from the drastic drop in performance that a frozen LLM is incapable of inferring effective representations that indicate a correct action. Although tuning the LLM should improve the results, we can see from previous work that tuning a LLaMA2-7B model on the full R2R data and GPT-4-augmented data only leads to a 43% SR in Val Unseen [53], still far behind NavGPT-2, implying that this approach might be infeasible with existing LLMs.\nEffect of Pretraining with Reasonings. Following the above, we also high- lighted the navigational reasoning abilities that NavGPT-2 unleashed from frozen LLMs in Figure 4. Additionally, we can see from Model#3 of Table 5 that the pretraining of Q-former on reasonings brings slight improvement to the success rates of the model. It is expected that such information, containing rich spatial descriptions and visual landmarks, facilitates the Q-former to produce better textual representations of the multi-view observations.\nEffect of Different LLMs. Technically, NavGPT-2 can incorporate a wide range of different LLMs, however, their performance on VLN might not scale"}, {"title": "5 Conclusion", "content": "In this work, we strive to eliminate the gap between LLMs-based agents and VLN-specialised agents, while maintaining the interpretative intrinsic of LLMs to generate navigational reasonings during navigation. Through comprehensive experimentation, we highlight the critical aspects of integrating LLMs with downstream navigation policy networks. It is demonstrated that the Vision-Language Model (VLM) latent serves as a superior and more efficient visual- linguistic representation, enabling policy networks to learn better alignment between vision, language, and action. Our approach offers a scalable framework to leverage the broad language comprehension capabilities of LLMs, paving the way for the development of versatile navigation agents capable of interacting with humans and understanding free-form human intentions with greater efficacy."}, {"title": "A DUET Revisit", "content": "NavGPT-2 exploit the similar design adapted from Dual-scale Graph Transformer (DUET) [15] as the downstream navigation policy. It includes a text encoder to encode instructions, a global and a local branch to enable coarse-scale and fine-scale cross-modal reasoning.\nA.1 Text Embedding and Visual Embedding\nFor the text encoder, DUET utilizes a 12-layer transformer initialized from LXMERT [63]. For visual embedding, the visual observation at each node is 36 view images from 12 horizontal directions times 3 vertical directions. To distinguish these nodes, a directional embedding Eang of the absolute angle for each view is added to the visual feature Zu extracted by the vision encoder. Moreover, since DUET inputs all 36 view images to construct the spatial observation for the model, the navigable adjacent nodes are only observed at a few view images, denoted as navigable views. A navigable embedding Enav is added to the visual features. The final visual embedding is sent to a 2 layers transformer to encode the spatial relations between views and obtain the panoramic view embeddings:\nH^{pano} = SelfAttn (Z_u + E^{ang} + E^{nav}). (4)\nOn the contrary, NavGPT-2 only inputs the navigable views, thus the direc- tional embedding Eang and the navigable embedding Enav are removed in the downstream policy, instead we directly add the step embedding and location embedding before sending to the 2 layers transformer.\nA.2 DUET Local Branch\nNavGPT-2 adopt the same navigation policy network architecture as the DUET global branch, discussed in \u00a73.23, so we omit the explanation of the global branch in DUET. In this section, we introduce the local branch of DUET. This branch performs action prediction based on the current node's instruction and egocentric observation. No graph information is provided besides the local observation."}, {"title": "A.3 Dynamic Fusion", "content": "The final action prediction of DUET is performed by dynamically fusing the action predicted by local and global branches. The local branch predicts actions within the adjacent nodes Va. It is incongruent with the action space used by the global branch, which chooses the next action from all nodes Vt in the constructed graph at step t. To reconcile this discrepancy, the local action scores s_a are transformed encompassing options such as \"stop\" and V_t, into a representation suitable for the global action space by summing up scores of visited nodes in V_a as a backtrack score s_b:\ns_i = \\begin{cases}\ns_b, & \\text{if } V_i \\in V_t - V_a, \\\\\ns_t, & \\text{otherwise}.\n\\end{cases} (5)\nThis adjustment facilitates navigation toward other unexplored nodes not directly linked to the current node, necessitating the agent to retrace its steps through neighboring nodes that have previously been visited. The final navigation score is given by:\ns_i = \\sigma s_i^g + (1 - \\sigma) s_i^l, (6)\nwhere s_i^g is the logits from global branch, \\sigma is a learnable scalar for fusion."}, {"title": "B GPT-4V Prompt", "content": "The prompt used for GPT-4V to generate navigation reasoning, discussed in section \u00a73.3 is shown in Figure 5."}, {"title": "C Additional Rerults", "content": "In this section, we conduct additional experiments to illustrate the choice of navigation policy network for NavGPT-2 and the effectiveness of LLM features. To align the same training schema of the navigation policy, we conduct the experiments for DUET initiating it from LXMERT without VLN specialized pretraining."}, {"title": "C.1 Effect of Vision Encoder", "content": "Because NavGPT-2 exploits a stronger vision encoder [24], we conduct an ablation study on the original DUET to investigate the performance gain brought by the vision encoder. As shown in Table 7, after switching the visual representation to the stronger vision feature same as NavGPT-2, little performance gain is observed for the DUET global branch (Model # 3 compared to Model # 2). We hypothesize this is due to the global branch for DUET performing vision-language alignment on a coarse scale, while the fine-grained alignment is performed in the local branch. Therefore, the main performance gain in NavGPT-2 is not contributed by the stronger vision encoder but the better representation from LLM hidden."}, {"title": "C.2 Effect of VLN Pretrain", "content": "We consider the same training scale and the same training schema of DUET as NavGPT-2, without pertaining auxiliary VLN tasks and directly finetuning on the VLN dataset. Under the same training schema and scale of data, NavGPT-2 performs significantly better than the original DUET, shown in Table 7. This showcases the superiors of LLM features that enable the learning of cross-modality alignment in the downstream task when the visual feature is projected to the"}, {"title": "D Limitations and Future Work", "content": "Although NavGPT-2 could generate navigation reasoning to some extent, it is hard to evaluate the effectiveness of these reasonings, since it is set as a single-step reasoning based on local observation and does not model the navigation history in the VLM. Instead, such history information is encoded in the downstream navigation policy. As a result, the consistency between navigation reasonings is underexplored. Moreover, the reasoning and action predicted by downstream navigation policy are not strictly synchronized in NavGPT-2, such synchronization could be done either explicitly by tuning LLM with the same supervision signal of action or by collaborating with the reasoning generation loss during fine-tuning the downstream policy network, we leave the synchronization to future work. Finally, the communicative capability of NavGPT-2 is not evaluated in this work, we suggest investigating the communicative ability of LM-based VLN agents and the synchronization between their reasoning and actions as a future direction."}, {"title": "E Broader Effect", "content": "Our research endeavors to leverage Large Vision-Language Models (VLM) to develop VLN agents, while preserving the linguistic prowess of VLMs for explaining action predictions in natural language. We posit that the inherent communicative capability, commonsense knowledge, and broad linguistic comprehension of VLM constitute the cornerstone for creating instruction-following navigation agents with generalizability. NavGPT-2 illuminates the reasonings of VLM throughout the navigation process explicitly and interpretably. Due to safety and ethical considerations, we currently conduct all experiments using the open-source Vision- and-Language Navigation dataset within a simulated environment, which ensures controlled agent behavior. Concurrently, we acknowledge that the potential prac- tical application of this technology warrants further exploration, particularly in"}, {"title": "", "content": "terms of action and reasoning synchronization, which remains an underexplored area. Notably, we observe the propensity of VLMs to hallucinate non-existent scenes or objects and fail to identify object directions, shown in Figure 6, which is also a common issue within VLM research. Future investigations are essential to address how to harmonize VLM action and reasoning and to enhance the agent's ability to self-explain in a manner intelligible to humans, a critical consideration for ensuring safety in real-world applications."}]}