{"title": "Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks", "authors": ["Mars Liyao Gao", "Jan P. Williams", "J. Nathan Kutz"], "abstract": "Spatiotemporal modeling of real-world data poses a challenging problem due to inherent high dimensionality, measurement noise, and expensive data collection procedures. In this paper, we present Sparse Identification of Nonlinear Dynamics with SHallow REcurrent Decoder networks (SINDy-SHRED), a method to jointly solve the sensing and model identification problems with simple implementation, efficient computation, and robust performance. SINDy-SHRED uses Gated Recurrent Units (GRUs) to model the temporal sequence of sensor measurements along with a shallow decoder network to reconstruct the full spatiotemporal field from the latent state space using only a few available sensors. Our proposed algorithm introduces a SINDy-based regularization; beginning with an arbitrary latent state space, the dynamics of the latent space progressively converges to a SINDy-class functional, provided the projection remains within the set. In restricting SINDy to a linear model, the architecture produces a Koopman-SHRED model which enforces a linear latent space dynamics. We conduct a systematic experimental study including synthetic PDE data, real-world sensor measurements for sea surface temperature, and direct video data. With no explicit encoder, SINDy-SHRED and Koopman-SHRED enable efficient training with minimal hyperparameter tuning and laptop-level computing; further, it demonstrates robust generalization in a variety of applications with minimal to no hyperparameter adjustments. Finally, the interpretable SINDy and Koopman models of latent state dynamics enables accurate long-term video predictions, achieving state-of-the-art performance and outperforming all baseline methods considered, including Convolutional LSTM, PredRNN, ResNet, and SimVP.", "sections": [{"title": "Introduction", "content": "Modeling unknown physics is an exceptionally challenging task that is complicated further by the computational burden of high-dimensional state spaces and expensive data collection. Partial differential equations (PDEs) derived from first principles remain the most ubiquitous class of models to describe physical phenomena. However, we frequently find that the simplifying assumptions necessary to construct a PDE model can render it ineffectual for real data where the physics is multi-scale in nature, only partially known, or where first principles models currently do not exist. In such cases, machine learning (ML) methods offer an attractive alternative for learning both the physics and coordinates necessary to quantify observed spatiotemporal phenomena. Many recent efforts utilizing ML techniques seek to relax the computational burden for PDE simulation by learning surrogate models to forward-simulate or predict spatiotemporal systems. However, this new machine learning paradigm frequently exhibits instabilities during the training process, unstable roll outs when modeling future state predictions, and often yields minimal computational speedups.\nShallow Recurrent Decoder networks (SHRED) [52] are a recently introduced architecture that utilize data from sparse sensors to reconstruct and predict the entire spatiotemporal domain. Similar to Takens'"}, {"title": "Related works", "content": "Traditionally, spatio-temporal physical phenomena are modeled by Partial Differential Equations (PDEs). To accelerate PDE simulations, recent efforts have leveraged neural networks to model physics. By explicitly assuming the underlying PDE, physics-informed neural networks [42] utilize the PDE structure as a constraint for small sample learning. However, assuming the exact form of governing PDE for real data can be a strong limitation. There have been many recent works on learning and predicting PDEs directly using neural networks [26; 31; 24; 34; 32]. Meanwhile, PDE-find [46; 37; 16] offers a data-driven approach to identify PDEs from the spatial-temporal domain. Still, the high-dimensionality and required high data quality can be prohibitive for practical applications.\nIn parallel, previous efforts in the discovery of physical law through dimensionality reduction techniques [8; 35; 36] provide yet another perspective on the modeling of scientific data. The discovery of physics from a learned latent space has previously been explored by [18; 11; 15; 12; 55; 30; 40], yet none of these methods consider a regularization on the latent space with no explicit encoder. Yu et al. proposed the idea of physics-guided learning [57] which combines physics simulations and neural network approximations. Directly modeling physics from video is also the subject of much research in the field of robotics [17; 50; 47], computer vision [56; 53] and computer graphics [25; 33; 54; 39], since many fields of research require better physics models for simulation and control. From the deep learning side, combining the structure of differential equations into neural networks [23; 9] has been remarkably successful in a wide range of tasks. When spatial-temporal modeling is framed as a video prediction problem, He et al. found [22] that random masking can be an efficient spatio-temporal learner, and deep neural networks can provide very good predictions for the next 10 to 20 frames [48; 51; 20; 21]. Generative models have also been found to be useful for scientific data modeling [38; 49; 6]."}, {"title": "Methods", "content": "The shallow recurrent decoder network (SHRED) is a computational technique that utilizes recurrent neural networks to predict the spatial domain. [52]. The method functions by trading high-fidelity spatial information for trajectories of sparse sensor measurements at given spatial locations. Mathematically, consider a high-dimensional data series $\\{X_t\\}_{t=1}^T \\in \\mathbb{R}^{(W\\times H)}$ that represents the evolution of a spatio-temporal dynamical system, W, H, and T denote the width, height, and total time steps of the system, respectively. In SHRED, each sensor collects data from a fixed spatial position in a discretized time domain. Denote the"}, {"title": "Empowering SHRED with representation learning and physics discovery", "content": "To achieve a parsimonious representation of physics, it is important to find a representation that effectively captures the underlying dynamics and structure of the system. In SINDy-SHRED (shown in Fig. 1), we extend the advantages of SHRED, and perform a joint discovery of coordinate systems and governing equations. This is accomplished by enforcing that the latent state of the recurrent network follows an ODE in the SINDy class of functions."}, {"title": "Finding better representations", "content": "SHRED has a natural advantage in modeling the latent governing physics due to its small model size. SHRED is based on a shallow decoder with a relatively small recurrent network structure. The relative simplicity of the model allows the latent representation to maintain many advantageous properties such as smoothness and Lipschitzness. Experimentally, we observe that the hidden state space of a SHRED model is generally very smooth. Second, SHRED does not have an explicit encoder, which avoids the potential problem of spectral bias [41]. Many reduced-order modeling methods that rely on an encoder architecture struggle to learn physics and instead focus only on modeling the low-frequency information (background) [43; 8; 36]. Building upon SHRED, we further incorporate SINDy to regularize the learned recurrence with a well-characterized and simple form of governing equation. In other words, we perform a joint discovery of a coordinate system (which transfers the high-dimensional observation into a low-dimensional representation) and the governing law (which describes how the summarized latent representation progresses forward with respect to time) of the latent space of a SHRED model. This approach is inspired by the principle in physics that, under an ideal coordinate system, physical phenomena can be described by a parsimonious dynamical model [8; 36]. When the latent representation and the governing law are well-aligned, this configuration is likely to capture the true underlying physics. This joint discovery results in a latent space that is both interpretable and physically meaningful, enabling robust and stable future prediction based on the learned dynamics."}, {"title": "Latent space regularization via SINDy and Koopman", "content": "As a compressive sensing procedure, there exist infinitely many equally valid solutions for the latent representation. Therefore, it is not necessary for the latent representation induced by SHRED to follow a well-structured differential equation. For instance, even if the exhibited dynamics are fundamentally linear, the latent representation may exhibit completely unexplainable dynamics, making the model challenging to interpret and extrapolate. Therefore, in SINDy-SHRED, our goal is to further constrain the latent representations to lie within the SINDy-class functional. This regularization promotes models that are fundamentally explainable by a SINDy-based ODE, allowing us to identify a parsimonious governing equation. The SINDy class of functions typically consists of a library of commonly used functions, which includes polynomials, and Fourier series. Although they may seem simple, these functions possess surprisingly strong expressive power, enabling the model to capture very complex dynamical systems."}, {"title": "SINDy as a Recurrent Neural network", "content": "We first reformulate SINDy using a neural network form, simplifying its incorporation into a SHRED model. ResNet [23] and Neural ODE [9] utilize skip connections to model residual and temporal derivatives. Similarly, this could also be done via a Recurrent Neural Network (RNN) which has a general form of\n$z_{t+1} = z_t + f(z_t),$ (1)\nwhere $f(.)$ is some function of the input. From the Euler method, the ODE forward simulation via SINDy effectively falls into the category of Recurrent Neural Networks (RNNs) which has the form\n$z_{t+1} = z_t + f_\\Theta(x_t, \\Xi, \\Delta t),$ (2)\nwhere $f_\\Theta(x_t, \\Xi, \\Delta t) = \\Theta(x_t)\\Xi \\Delta t$ is a nonlinear function. Notice that this $f_\\Theta(.)$ has exactly the same formulation as in SINDy [5]. The application of function libraries with sparsity constraints is a manner of automatic neural architecture search (NAS) [59]. Compared to all prior works [8; 18; 12], this implementation of the SINDy unit fits better in the framework of neural network training and gradient descent. We utilize trajectory data $\\{z_t\\}_{t=1}^T$ and forward simulate the SINDy-based ODE using a trainable parameter $\\Xi$. To achieve better stability and accuracy for forward integration, we use Euler integration with k mini-steps (with time step $\\frac{h}{k}$) to obtain $z_{t+1}$. In summary, defining $h = \\frac{\\Delta t}{k}$, we optimize $\\Xi$ with the following:\n$\\Xi = \\arg \\min_{z_{t+1}} ||z_{t+1} - (z_t + \\sum_{i=0}^{k-1} \\Theta(z_{t+ih}) \\Xi h) ||_2^2$,\n$z_{t+ih} = z_t + \\Theta(z_{t+(i-1)h})\\Xi h, \\min_{\\Xi}||\\Xi||_0.$ (3)\nTo achieve $l_0$ optimization, we perform pruning with $l_2$ which approximates $l_0$ regularization under regularity conditions [58; 19; 3]. Applying SINDy unit has the following benefits: (a) The SINDy-function library contains frequently used functions in physics modeling (e.g. polynomials and Fourier series). (b) With sparse system identification, the neural network is more likely to identify governing physics, which is fundamentally important for extrapolation and long-term stability."}, {"title": "Latent space regularization via ensemble SINDy", "content": "We first note that we deviate from the original SHRED architecture by using a GRU as opposed to an LSTM. This choice was made because we generally found that GRU provides a smoother latent space. Now, recall that our goal is to find a SHRED model with a latent state that is within the SINDy-class functional. However, the initial latent representation from SHRED does not follow the SINDy-based ODE structure at all. On the one hand, if we naively apply SINDy to the initial latent representation, the discovery is unlikely to fit the latent representation trajectory. On the other hand, if we directly replace the GRU unit to SINDy and force the latent space to follow the discovered SINDy model, it might lose information that is important to reconstruction the entire spatial domain. Therefore, it is important to let the two latent spaces align progressively.\nIn Algorithm 1, we describe our training procedure that allows the two trajectories to progressively align with each other. To further ensure a gradual adaptation and avoid over-regularization, we introduce ensemble SINDy units with varying levels of sparsity constraints, which ranges the effect from promoting a full model (all terms in the library are active) to a null model (where no dynamics are represented). From the initial latent representation $z_t^{iter = 0}$ from SHRED, the SINDy model first provides an initial estimate of ensemble SINDy coefficients $\\{\\Xi_\\Theta^{(i)}\\}_{i=1}^B$. Then, the parameters of SHRED will be updated towards the dynamics simulated by $\\{\\Xi_\\Theta^{(i)}\\}_{i=1}^B$, which generates a new latent representation trajectory $z_t^{iter = 1}$. We iterate this procedure and jointly optimize the following loss function to let the SHRED latent representation"}, {"title": "Latent space linearization via the Koopman operator", "content": "Koopman operator theory [27; 28] provides an alternative approach to solving these problems by linearizing the underlying dynamics. The linearized embedding is theoretically grounded to be able to represent non-linear dynamics in a linear framework, which is desirable for many applications in science and engineering, including control, robotics, weather modeling, and so on. From the transformed measurements $z = g(x)$ from the true system $x$, the Koopman operator $K$ is an infinite-dimensional linear operator that\n$Kg := g\\circ F,$ (5)\nwhere $F(.)$ describes the law of the dynamical system in its original space that $x_{t+1} = F(x_t)$. The Koopman operator enables a coordinate transformation from $x$ to $z$ that linearizes the dynamics\n$Kg(x_{t+1}) = g(x_t).$ (6)\nHowever, it is generally impossible to obtain the exact form of this infinite-dimensional operator, so a typical strategy is to find a finite-dimensional approximation of the Koopman operator by means of data-driven approaches [4]. In Koopman-SHRED, we utilize the GRU unit to approximate the eigenfunctions. In the latent space, we enforce and learn the linear dynamics, represented by a matrix $K$, which corresponds to the latent space evolution derived from the eigenfunctions. We keep the interpretability of the model via a parsimonious latent space. In implementation, a simple strategy is to adapt the SINDy-unit with only the linear terms. This models a continuous version of Koopman generator that $e^{t\\mathscr{A}}z(x(t)) = G_tz(x(t))$ where the corresponding Koopman operator $K_t = e^{t\\mathscr{A}}$. Then, we follow a similar practice in SINDy-SHRED, updating the Koopman-regularized loss function:\n$L = ||X_t - f_{OP}(f_{OGRU}(X_{L:t}))||_2^2 + ||Z_{t+m}^{GRU} - K M Z_{t+m}^{GRU}||^2.$ (7)\nTo enable continuous spectrum for better approximation for the Koopman operator, one could adapt an additional neural network to learn the eigenvalues $\\lambda_i$'s and form the linear dynamics $K$ from the learned"}, {"title": "Training setup of dynamical system learning", "content": "The dynamical system we wish to model has the form that\n$\\dot{x} = f(x),$ (8)\nwhich is an ODE describing the trajectory of a dynamical system in a learned latent space.\nSuppose that the dynamical system has the form $\\dot{x} = f(x)$, and we have measurements of $x = \\{x_1, x_2, ..., x_t, x_{t+1},...,x_T\\}$ with time gap $\\Delta t$. The empirical loss function $L(.)$ of a function $f$ given the empirical distribution $P_n$ which consists of data samples $D = \\{(x_i, \\hat{x_i})\\}_{i=1}^n$ is\n$\\hat{L}(f) = \\frac{1}{n}\\sum_{i=1}^n l(f(x_i), \\hat{x_i}),$ (9)\nwhile the true loss is\n$L(f) = \\mathbb{E} [l(f(x), y)].$ (10)\nIn practice, we only have access to data samples (or equivalently empirical distributions). There is a generalization gap between $L(f)$ and $\\hat{L}(f)$, which is correlated with the expressive power of the functional class. When the expressive power of the estimator functional class is small, the generalization error is expected to be mild, resulting from a smooth transition from local perturbations. On the other hand, when the expressive power of the estimator is excessive, we expect that local perturbations can cause significant shifts in performance, leading to a larger generalization error. Therefore, even though methods like neural ODE [9] could nicely interpolate a given dynamical system, they typically perform poorly in extrapolation. We find that SINDy effectively avoids this issue, as it aims to discover the governing equation in its exact form."}, {"title": "Computational Experiments", "content": "In the following, we perform case studies across a range of scientific and engineering problems."}, {"title": "Sea-surface temperature", "content": "The first example we consider is that of global sea-surface temperature. The SST data contains 1,400 weekly snapshots of the weekly mean sea surface temperature from 1992 to 2019 reported by NOAA [44]. The data is represented by a 180 \u00d7 360 grid, of which 44,219 grid points correspond to sea-surface locations. We standardize the data with its own min and max, which transforms the sensor measurements to within the numerical range of (0,1). We randomly select 250 sensors from the possible 44,219 locations and set the lag parameter to 52 weeks. The inclusion of 250 sensors is a substantial deviation from previous work with SHRED in which far fewer sensors were used [52]. However, we found greater robustness in the application of E-SINDy to the learned latent state when more sensors were utilized. Thus, for each input-output pair, the input consists of the 52-week trajectories of the selected sensors, while the output is a single temperature field across all 44,219 spatial locations. SINDy-SHRED aims to reconstruct the entire sea surface temperature locations from these randomly selected sparse sensor trajectories. We include the details of the experimental settings of SINDy-SHRED in the Appendix B.1. From the discovered coordinate system, we define the representation of physics - the latent hidden state space - to be $(z_1, z_2, z_3)$. The dynamics progresses forward via the following set of equations:\n$ \\begin{aligned} \\dot{z}_1 &= 4.68z_2 - 2.37z_3, \\\\ \\dot{z}_2 &= -3.10z_1 + 3.25z_3, \\\\ \\dot{z}_3 &= 2.72z_1 - 5.55z_2, \\end{aligned} $ (11)"}, {"title": "3D Atmospheric ozone concentration", "content": "The atmospheric ozone concentration dataset [2] contains a one-year simulation of the evolution of an ensemble of interacting chemical species through a transport operator using GEOS-Chem. The simulation contains 1,456 temporal samples with a timestep of 6 hours over one year for 99,360 (46 by 72 by 30) spatial locations (latitude, longitude, elevation). The data presented in this work has compressed by performing an SVD and retaining only the first 50 POD modes. As with the SST data, we standardize the data within the range of (0,1) and randomly select and fix 3 sensors out of 99,360 spatial locations (0.5%). We include the details of the experimental settings of SINDy-SHRED in the Appendix B.2. The converged latent representation presents the following SINDy model:"}, {"title": "GoPro physics data: flow over a cylinder", "content": "In this subsection, we demonstrate the performance of SINDy-SHRED on an example of so-called \"GoPro physics modeling.\" The considered data is collected from a dyed water channel to visualize a flow over a cylinder [1]. The Reynolds number is 171 in the experiment. The dataset contains 11 seconds of video taken at 30 frames per second (FPS). We manually perform data augmentation and repeat the latter part of the video once to increase the number of available training samples We transfer the original RGB channel to gray scale and remove the background by subtracting the mean of all frames. After the prior processing step, the video data has only one channel (gray) within the range (0,1) with a height of 400 pixels and a width of 1,000 pixels. We randomly select and fix 200 pixels as sensor measurements from the entire 400,000 space, which is equivalent to only 0.05% of the data. We set the lag parameter to 60 frames. We include the details of the experimental settings of SINDy-SHRED in the Appendix B.4."}, {"title": "Baseline study: prediction of single shot real pendulum recording", "content": "In this subsection, we compare the performance of SINDy-SHRED to other popular existing learning algorithms. We perform the baseline study particularly on video data of a pendulum since many deep learning algorithms are hard to scale up to deal with large scientific data. In the following, we demonstrate the result of video prediction on the pendulum data using ResNet [23], convolutional LSTM (convLSTM) [48], and PredRNN [51], and SimVP [20]. The pendulum in our experiment is not ideal and includes complex damping effects. We use a nail on the wall and place the rod (with a hole) on the nail. This creates complex friction, which slows the rod more when passing the lowest point due to the increased pressure caused by gravity. The full model we discovered from the video (as shown in Fig. 11) includes four terms:\n$\\dot{z} = 0.17z_2 - 0.06 z^3 - 10.87 \\sin(z) + 0.48 \\sin(\\dot{z}).$ (19)\nAs shown in Table 1, SINDy-SHRED outperforms all baseline methods for total error and long-term predictions. Generally, all baseline deep learning methods perform well for short-term forecasting, but the error quickly accumulatesfor longer-term predictions. This is also observable from the prediction in the pixel space as shown in Fig. 12. SINDy-SHRED is the only method that does not produce collapsed longer-term predictions. Interestingly, we observe that incorporating nonlinear terms in the SINDy library is crucial in this case, as Koopman-SHRED struggles to accurately reconstruct the finer details. We attribute this to the fact that strong linear regularization may over-regularize and oversimplify the complex dynamics, leading to a reduction in predictive accuracy. Similar behaviors for Koopman-SHRED are observed when encoding the continuous spectrum using an additional shallow network, as detailed in the Appendix."}, {"title": "Conclusion", "content": "In this paper, we present SINDy-SHRED, which jointly performs the discovery of coordinate systems and governing equations with low computational cost and strong predictive power. Through experiments, we show that our method can produce robust and accurate long-term predictions for a variety of complex problems, including global sea-surface temperature, 3D atmospheric ozone concentration, flow over a cylinder, and a moving pendulum. SINDy-SHRED achieves state-of-the-art performance in long-term autoregressive video prediction, outperforming ConvLSTM, PredRNN, ResNet, and SimVP with the lowest computational cost and training time."}, {"title": "A Challenges in rolling out neural networks for fitting a simple sine function", "content": "In the following example, we consider a simple use case in which we fit a simple sine function using recurrent neural networks. Surprisingly, extrapolating a simple sine function can be highly nontrivial for neural networks.\nWe implement a GRU network in the following. The GRU network consists of an input layer, three stacked GRU layers with size 500, and a fully connected output layer. We employ the Adam optimizer with a learning rate of 0.001 and used the mean squared error (MSE) as the loss function. We train the GRU network with 150 epochs with a batch size of 1. The input sequences are made up of 50 time steps, normalized to the range [0, 1]."}, {"title": "B Experimental details", "content": ""}, {"title": "Sea-surface temperature data", "content": "For the SST data in SINDy-SHRED, we set the latent dimension to 3 because we observe only minor impacts on the reconstruction accuracy when the latent dimension is \u2265 4. We include 2 stacked GRU layers and consider the, and a two-layer ReLU decoder with 350 and 400 neurons. For the E-SINDy regularization, we set the polynomial order to be 3 and the ensemble number is 10. In the latent hidden-state forward simulation, we use Euler integration with dt = $\\frac{1}{520}$, which will generate the prediction of next week via 10 forward integration steps. During training, we apply the AdamW optimizer with a learning rate of 1e-3 and a weight decay of 1e-2. The batch size is 128 with 1,000 training epochs. The thresholds for E-SINDy range uniformly from 0.1 to 1.0, and the thresholding procedure will be executed every 100 epochs. We use dropout to avoid overfitting with a dropout rate of 0.1. The training time is within 30 minutes from a single NVIDIA GeForce RTX 2080 Ti."}, {"title": "3D atmospheric ozone concentration", "content": "For the ozone data, we set the lag parameter is set to 100. Thus, for each input-output pair, the input consists of the 62.5 day measurements of the selected sensors, while the output is the measurement across the entire 3D domain. In SINDy-SHRED, we follow the same network architecture as in the SST experiment. We set dt = 0.025, and the thresholds for E-SINDy range uniformly from 0.015 to 0.15. The thresholding procedure will be executed every 300 epochs, and we apply AdamW optimizer with learning rate 1e-3."}, {"title": "Flow over a cylinder", "content": "In the flow over a cylinder experiment, we follow the same settings as in the prior experiments and select the latent dimension to be 4. The forward integration time step is set to dt = $\\frac{1}{300}$, corresponding to the frame rate of 30 FPS. We set the batch size at 64 and the learning rate to 5e-4. The thresholding procedure is executed every 300 epochs with thresholds ranging from $(1e^{-4}, 1e^{-3}).$"}, {"title": "Baseline experiment on pendulum", "content": "Autoregressive training. The raw pendulum data are collected from a 14-second GoPro recording. The raw data are present difficulties during training because of their high-dimensionality (1080 \u00d7 960), so we follow the same preprocessing procedure as in [36] to obtain a set of training data with 390 samples, width 24 and height 27. For most of the models, we apply autoregressive training to help the model achieve better long-term prediction capabilities. From the initial input $\\{X_1, X_2, ..., X_L\\}$ with lag L, the model autoregressively predicts the next frame $X_{L+1}$ and use it as a new input $\\{X_2, X_3, ..., X_{L+1}\\}$. This step will be repeated L times to obtain $\\{X_{L+1}, X_{L+2}, ..., X_{2L}\\}$. We treat this as the prediction and optimize the loss from this quantity. In the following baseline models, we uniformly set L = 20."}, {"title": "Baseline methods and SINDy-SHRED setting", "content": "ResNet. We use the residual neural network (ResNet) [23] as a standard baseline. We set the input sequence length to 20, and we predict the next frames autoregressively. For ResNet, the first convolutional layer has 64 channels with kernel size 3, stride 1 and padding 1. Then, we repeat the residual block three times with two convolutional layers. We use ReLU as the activation function. After the residual blocks, the output is generated via a convolutional layer with kernel size 1, stride 1, and padding 0. We set the batch size to 8, and we use AdamW optimizer with learning rate $1e^{-3}$, weight decay $1e^{-2}$ for the training of 500 epochs.\nSimVP. SimVP [20] is the recent state-of-the-art method for video prediction. This method utilizes ConvNormReLU blocks with a spatio-temporal features translator (i.e. CNN). The ConvNormReLU block has two convolutional layers with kernel size 3, stride 1, and padding 1. After 2D batch normalization and ReLU activation, the final forward pass includes a skip connection unit before output. The encoder first performs a 2D convolution with 2D batch normalization and ReLU activation. Then, three ConvNormReLU blocks will complete the input sequence encoding process. The translator in our implementation is a simple CNN which contains two convolutional layers. The decoder has a similar structure to the encoder by reversing its structure. We similarly set the batch size to 8 with AdamW optimizer for 500 epochs.\nConvLSTM. Convolutional Long Short-Term Memory [48] is a classical baseline for the prediction of video sequence and scientific data (e.g. weather, radar echo, and air quality). The ConvLSTM utilizes features after convolution and performs LSTM modeling on hidden states. The ConvLSTM model has two ConvLSTM cells that have an input 2D convolutional layer with kernel size 3 and padding 1 before the LSTM forward pass. The decoder is a simple 2D convolution with kernel size 1, and zero padding. We similarly set the batch size to 8 with AdamW optimizer for 500 epochs.\nPredRNN. PredRNN [51] is a recent spatiotemporal modeling technique that builds on the idea of Con-VLSTM. We follow the same network architecture setting as in ConvLSTM and similarly set the batch size to 8 with AdamW optimizer for 500 epochs."}, {"title": "SINDy-SHRED.", "content": "We select and fix 100 pixels as sensor measurements from the entire 648 dimensional space. We remove non-informative sensors, defined as remaining constant through the entire video. We set the lag to 60. For the setting of network architecture in SINDy-SHRED, we follow the same settings as in the prior experiments but with latent dimension of 1. The timestep of forward integration is set to dt = $\\frac{1}{200}$ corresponding to frame rate of the video at 30 FPS. We set the batch size at 8 and the learning rate to 5e-4. The thresholding procedure is executed every 300 epochs with thresholds ranging from (0.4, 4.0). We include 3 stacked GRU layers, and a two-layer ReLU decoder with 16 and 64 neurons. We use dropout to avoid overfitting with a dropout rate of 0.1. SINDy-SHRED discovers two candidate models."}, {"title": "Experiment on the 2D Kolmogorov flow", "content": "The 2D Kolmogorov flow data is a chaotic turbulent flow generated from the pseudospectral Kolmogorov flow solver [7]. The solver numerically solves the divergence-free Navier-Stokes equation:\n$\\begin{cases} \\nabla \\cdot u = 0 \\\\ \\partial_t u + (u \\cdot \\nabla) u = - \\nabla p + \\nu \\Delta u + f \\end{cases},$ (20)\nwhere u stands for the velocity field, p stands for the pressure, and f describes an external forcing term. Setting the Reynolds number to 30, the spatial field has resolution 80 \u00d7 80. We simulate the system forward for 180 seconds with 6,000 available frames. We standardize the data within the range of (0, 1) and randomly fix 10 sensors from the 6,400 available spatial locations (0.16%). The lag parameter is set to 360.\nFor the setting of SINDy-SHRED, we slightly change the neural network setting because the output domain is 2D. Therefore, after the GRU unit, we use two shallow decoders to predict the output of the 2D field. The two decoders are two-layer ReLU networks with 350 and 400 neurons. We set the latent dimension to 3. The time step for forward integration is set to dt = 0.003 which corresponds to the FPS during data generation. We set the batch size to 256 and the learning rate to 5e-4 using the Lion optimizer [10]. The thresholding procedure is executed every 100 epoch with the total number of training epochs as 200. The thresholds range from (0.4, 4).\nAs a chaotic system, the latent space of the Kolmogorov flow is much more complex than all the prior examples we considered. Thus, we further apply seasonal-trend decomposition from the original latent space. We define the representation of the latent hidden state space after decomposition as $(z_1, z_2, z_3, z_4, z_5, z_6)$, where $(z_{2i}, z_{2i+1})$ is the seasonal trend pair of the original latent space.\n$\\begin{aligned} \\dot{z}_1 &= -0.007z_3 + 0.009z_5, \\\\ \\dot{z}_2 &= -0.207z_4, \\\\ \\dot{z}_3 &= -0.011z_1 - 0.008z_5, \\\\ \\dot{z}_4 &= 0.103z_2, \\\\ \\dot{z}_5 &= -0.012z_1 + 0.006z_3, \\\\ \\dot{z}_6 &= 0.151z_1z_2. \\end{aligned}$ (21)"}]}