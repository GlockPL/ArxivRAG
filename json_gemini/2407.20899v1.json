{"title": "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach", "authors": ["Adam Wojciechowski", "Mateusz Lango", "Ond\u0159ej Du\u0161ek"], "abstract": "Existing explanation methods for image classification struggle to provide faithful and plausible explanations. This paper addresses this issue by proposing a post-hoc natural language explanation method that can be applied to any CNN-based classifier without altering its training process or affecting predictive performance. By analysing influential neurons and the corresponding activation maps, the method generates a faithful description of the classifier's decision process in the form of a structured meaning representation, which is then converted into text by a language model. Through this pipeline approach, the generated explanations are grounded in the neural network architecture, providing accurate insight into the classification process while remaining accessible to non-experts. Experimental results show that the NLEs constructed by our method are significantly more plausible and faithful. In particular, user interventions in the neural network structure (masking of neurons) are three times more effective than the baselines.", "sections": [{"title": "1 Introduction", "content": "Despite remarkable advances in computer vision, the deployment of image classification systems, especially in critical domains, poses significant challenges. One of them is the opacity of deep models and the difficulty of providing reliable explanations for their predictions (Doshi-Velez et al., 2017). Therefore, several types of explanation methods have been proposed, including various forms of saliency maps (Selvaraju et al., 2017a), feature importances (Ribeiro et al., 2016a), concept-based explanations (Chen et al., 2019), counterfactual explanations (Vermeire et al., 2022), etc. A particularly interesting form of explaining predictions is offered by natural language explanations (NLE) techniques (Camburu et al., 2018; Wu and Mooney, 2019). Such explanations are not only understandable by non-expert users, but can also be used to support conversations with the user in a dialogue system (Raczy\u0144ski et al., 2023). There are two critical properties of an explanation: faithfulness and plausibility (Jacovi and Goldberg, 2020; Atanasova et al., 2023). A faithful explanation should accurately reflect the inner workings of the system and provide information on the real reasons why the model reached a certain decision. Plausibility then refers to how convincing the explanation appears to the user. In the case of NLE, obtaining high plausibility is straightforward, as textual explanations are usually human-friendly (Gurrapu et al., 2023), but achieving high fidelity is challenging. In the context of image classification, image captioning methods offer plausible but unfaithful NLEs (Xu et al., 2015; Kamakshi and Krishnan, 2023). Some methods try to improve faithfulness by conditioning generation on both the predicted class and image features (Hendricks et al., 2016; Kim et al., 2018; Marasovi\u0107 et al., 2020; Sammani et al., 2022), but the faithfulness provided is still limited as the model is not aware of the classifier's decision process. Other methods train image classifiers to jointly predict the class and visual rationales, and generate explanations based on them (Wickramanayake et al., 2021; Kayser et al., 2022). However, the rationales are predicted independently of the class and do not participate in the classifier's decision process. Most importantly, such methods change the training procedure and the architecture of the classifier, often affecting classification performance. In this paper, we propose a post-hoc natural language explanation method for image classification that can be used with any standard convolutional neural network (CNN) classifier. To illustrate the classification decision process, the method analyses which neurons of the CNN were most influential in reaching a given decision, and which regions of the image caused them to activate. For each influential neuron, a neuron annotation method computes vi-"}, {"title": "2 Method", "content": "Our NLE generation approach uses two processing steps, described below: meaning representation (MR) construction and MR-to-text conversion.", "subsections": [{"title": "2.1 Meaning representation construction", "content": "We first produce a meaning representation in the form of a JSON object, containing information about the neurons responsible for a given classifier prediction (why?), what patterns those neurons detected (what?), and in which parts of the image they were activated (where?). The MR includes the predicted class and the list of most influential neurons, each represented by (1) description \u2013 a phrase describing the pattern that most excites the neuron (convolutional filter), (2) positions \u2013 list of coarse-grained image positions (e.g., \u201cbottom-right corner\") where the neuron was activated. An example of a MR is provided in Fig. 1. MR construction starts by storing all neuron activations from the given CNN-based classifier for the prediction to explain. Next, the most important neurons are selected, annotated with a description, and tied to an image region, as follows: Selecting the most influential neurons To choose the most relevant neurons, we apply well-established Layer-wise Relevance Propagation (LRP) method (Bach et al., 2015). LRP performs a backward pass through the classifier network to establish the influence of each neuron to the final prediction (see App. I for formulas). We select k neurons with the highest LRP scores (with k being a parameter controlling the brevity-detail tradeoff). Neuron annotation We adopt the MILAN neuron annotator (Hernandez et al., 2022) to generate descriptions of selected neurons. MILAN first finds images in the classifier training set that make a given neuron highly activated (Bau et al., 2017). These exemplar images are used to generate a description of the pattern that this neuron detects. Note that although the last step of MILAN is essentially image captioning, it does not affect the faithfulness of NLEs produced by the pipeline, as long as its output is of sufficient quality. The im-"}, {"title": "2.2 Explanation generation", "content": "The second step of our method is converting the human-readable and faithful MR created above into a user-friendly text. As we do not have any gold-standard explanation texts, the task is performed by prompting a large language model (LLM). We instructed the model to (1) produce fluent text, (2) summarise the content of the MR (e.g. if two neurons detect similar patterns, they can be combined in the text), (3) prioritise readability, (4) come up with its own formulation of spatial positions to improve fluency. We also provide one handcrafted MR-to-text conversion example. The prompt is shown in App. D. LLMs could in theory hallucinate and thus reduce the explanations' faithfulness. However, in Section 3 we show experimentally that current LLMs are reliable enough to produce useful explanations."}]}, {"title": "3 Experimental evaluation", "content": "Dataset All experiments were performed on the ImageNet dataset. The classifier was trained on the train set and our explanation method was run to explain predictions made on the validation data. Models We experiment with explaining the predictions of the smallest CNN classifier from the popular ResNet family: ResNet18 (He et al., 2015). We fill our MRs with k = 10 top neurons indicated by LRP from the Captum library (Kokhlikyan et al., 2020) and annotate them using MILAN's original implementation. As the LLM for MR-to-text conversion, we use GPT-4 (Achiam et al., 2023).", "subsections": [{"title": "3.2 Are the output explanations plausible?", "content": "To assess the plausibility of generated explanations, we conducted a small-scale manual annotation experiment. We recruited ten annotators: five non-experts hired on the Prolific platform and five experts with at least one published paper on explainable AI. Each annotator was presented with 30 image-explanation pairs (300 in total) and asked to rate them on a scale of 1-5 whether the explanation was (1) fluent, (2) easy to understand (comprehensible), (3) convincing, and (4) explanatory for the underlying decision process. The overall quality of the explanations was also rated (see App. \u0415). The results are presented in Tab. 1. Our method obtains the highest overall quality according to both experts and non-experts. It also produces the most plausible explanations (most convincing and explanatory). Since the baselines produce much shorter explanations, it is not surprising that our longer explanations are a bit more difficult to understand. Interestingly, experts generally give higher ratings than non-experts. This is true for all methods and all factors except the explanatory ability. For this factor, experts rate the baselines lower than non-experts, but they consistently rate the explanations provided by our pipeline higher. The improvements of our method over baselines are statistically significant on both plausibility measures and overall quality. For fluency, our method is indistinguishable from SAT (see details in App. J)."}, {"title": "3.3 Are the output explanations faithful?", "content": "The faithfulness of the generated explanations is assessed through two intervention experiments: (1) checking if rationales from NLEs change the prediction by masking parts of input images, (2) influencing the network prediction by masking influential neurons. We further assess the stability and diversity of the explanations for our method, and we directly evaluate the reliability of our MR-to-text conversion. Masking input image We asked annotators to cover with white rectangles parts of images that contained the decision rationale indicated in the NLE, 50% area at most (see App. H for details). We re-classified covered images and measured changes in prediction and the average decrease in the probability of the originally predicted class. We also performed an opposite experiment, with the annotators highlighting only parts of image mentioned in the explanation and covering the rest. For covering, the use of our NLEs resulted in the highest average probability decrease and the change of the original prediction for 88% of examples (see Table 2). Our method reached the best results in the highlighting experiment as well, producing the least amount of changes. We also re-ran our NLE pipeline with parts of the input image covered. This led to significant changes: on average 78% (median 90%) of the neurons indicated in MRs were different. Masking influential neurons To show the NLEs' ability to reflect classifier decisions, we asked the annotators to read the NLEs and select up to five most influential neurons from a MILAN-annotated list. The classifier was then re-run with the selected neurons masked. The results in Table 2 reveal that masking neurons suggested by our NLEs led to a five times higher decrease in the predicted class probability and over three times higher class flip rate than baselines. More details are in App. \u0412. Explanation stability analysis Following Wiegreffe et al. (2021), we measure explanation robustness against adding random noise to the input image (intra-set stability, see App. G for details) by comparing BLEU (Papineni et al., 2002), \u039c\u0395TEOR (Lavie and Agarwal, 2007) as well as class flip frequency and class probability change against the original predictions. We also check for outputs' diversity (inter-set stability) using BLEU and METEOR overlap against explanations for other classes. The results in Table 3 show high diversity and high noise sensitivity: In parallel to increased classification changes with added noise, explanation BLEU/METEOR gradually drop, but they are lowest when comparing between different classes. Reliability of the MR-to-text transform The human evaluation of our approach's MR-to-text reliability was similar to plausibility evaluation, but limited to non-expert Prolific annotators. We asked five yes-no questions on information in the text not grounded in the MR (i.e., hallucinations), omission of MR information, fluency, spatial information fidelity, and overall correctness (see App. F). The results show the MR-to-text conversion as highly reliable, as only 8% texts contain hallucinations. Omissions are more frequent (44%), but this is expected as the LLM is instructed to summarise the MR and prioritise readability. This factor most likely affected the overall score (58%). The explanations are mostly fluent (96%), with correct spatial information (82%)."}]}, {"title": "Limitations", "content": "This paper produces a new method for plausible and more faithful natural language explanations for image classification. Although we believe that the method provides significantly better faithfulness than the previously proposed methods, it does not obtain completely faithful explanations. The faithfulness of the explanations provided by our method depends on the quality of the neuron annotations produced by MILAN and the neurons indicated by LRP. Both techniques can be considered as state of the art, but they still occasionally produce incorrect results. Therefore, the results of NLE methods should be treated with caution. Additionally, this work uses pre-trained language models, which are known to expose certain social biases reflected in their training data."}, {"title": "A Examples of explanations", "content": "Several examples of explanations provided by the methods under study are given in Tab. 4. More examples of NLE produced with the proposed pipeline approach are given in Tab. 5."}, {"title": "B Detailed results of masking influential neurons experiment", "content": "The detailed results of the experiment with masking influential neurons are presented in Fig. 2. For our method, the results indicate that masking neurons in the order indicated by the annotators leads to an increasing change in the classifier's prediction and a larger decrease in the predicted class probability value. In contrast, the annotators gain limited insight into how the neural network made a decision from the explanations provided by other methods: masking the indicated neurons leads to limited change, and increasing the number of masked neurons leads to almost no improvement. We attribute the effectiveness of masking the first indicated neuron to examples of classes that are highly correlated with a pattern detected by a particular neuron."}, {"title": "C Simplification rules for spacial positions", "content": "The spatial information about neuron activation is learned by inspecting activation maps of the chosen neurons. The activation map is converted to a binary image, such that if a given pixel of an activation map exceeds half of a maximal value in that activation map it is converted to the value of one, and the rest of the pixels are assigned with the value zero. Then the binary activation map is divided into a 3x3 grid of congruent squares oriented such that the ordering begins with a 0 in the top-left corner, progressing sequentially across each row from left to right and top to bottom, culminating with the number 8 in the bottom-right position. The basic positions names are given below."}, {"title": "D Prompt for MR-to-text conversion", "content": "To convert meaning representations into text, the following prompt was applied to the language model: You are given a problem of creating textual explanation of an image classification performed by neural network. You will be given a Python object representing network output in the form of `'image class', [('detected object', 'position'), ...]`. I want you to convert this object into a textual explanation. You should: 1. Create a grammatically correct sentence which will explain the model's decision. 2. Decide which detected objects do not fit with image class and do not include them in the explanation. For instance, 'dentist' class and 'animal heads' objects are completely unrelated. However, the descriptions that aren't directly related to image class, but can be indirectly correlated, especially in terms of shape, color, or texture resemblance should be included (like 'fountain' and 'sea' because of the water they have in common or 'brick wall' and 'grid' because the texture is similar). Never mention that you chose neuron descriptions and do not talk about the neuron descriptions that were discarded. 3. Prioritize the readability of the explanation. Include only essential detected objects and aggregate information to shorten the explanation. 4. Aggregate positions if possible, for example ['bottom-left corner', 'bottom', 'bottom-right corner'] should be aggregated into 'bottom'. If the positions list is too long or too ambiguous do not include them in the explanation. Here is an example. Python object: \"lakeside, [{'description': 'Nature', 'positions': ['left', 'right', 'bottom']}, {'description': 'The sky', 'positions': ['top-right corner', 'bottom-left corner', 'bottom']}, {'description': 'Red and white colored objects', 'positions': ['left', 'right', 'bottom']}, {'description': 'The ocean', 'positions': ['left', 'right', 'bottom']}, { 'description': 'Animal heads', 'positions': [], {'description': 'The color red', 'positions': ['left', 'right', 'bottom']'}, {'description': 'White backgrounds', 'positions': ['bottom', 'left']}, {'description': 'Grass',"}, {"title": "E Human evaluation of explanation plausibility", "content": "The annotators are presented with an image, model's prediction and a natural language explanation. Each question is answered on a scale from 1 (low) to 5 (high). The following binary questions are asked: \u2022 How fluent (linguistically correct) the text is? \u2022 How easy to understand the text is? \u2022 How convincing do you find the explanation of the decision made by the model? \u2022 After reading the explanation, how well do you understand how the decision of the model was taken? \u2022 How would you rate the overall quality of the explanation? The annotation instructions are provided in the code repository."}, {"title": "F Human evaluation of MR-to-text transformation", "content": "The annotators are presented with a meaning representation in the form of formatted JSON without a given image, since it should not influence the assessment of MR-to-text transformation. The following binary questions are asked: \u2022 Does the text contain information that was not present in the meaning representation? \u2022 Is there any important information from meaning representation omitted in the text? \u2022 Is the text linguistically correct? \u2022 If any spatial compression occurred between explained neurons, the said compression is correct? \u2022 Overall, do you find this meaning representation to text transformation acceptable i.e. sufficiently good for explanation purposes? The annotation instructions are provided in the code repository."}, {"title": "G Details on stability experiments", "content": "Let us assume that an image is a matrix X, such that X \u2208 [0,1]. We model input perturbations by adding random noise to the images from the standard normal distribution. Since an interval of possible pixel values is [0, 1], to account for the unboundedness of a standard normal distribution we use a clipping operation defined as follows:\nclip(x) = { 0, if x < 0\n          x, if 0 \u2264 x \u2264 1\n          1, if x > 1}\nThe formula for the function ( that adds a perturbation to the image is given below.\n\u03b6(X) = clip(N(0, 1) \u00b7 i + X)\nwhere i is the noise intensity.\nWe perform two types of stability experiments:\n1. inter-set stability: We compare pairs of unaltered images (1st set of images) and corresponding images mapped by the function \u03b6 (2nd set of images). After the \u03da mapping is performed, we run the proposed method on both images, yielding two explanations. We then compute various language similarity metrics between the two explanations.\nWe compare two kinds of pairs of images: unaltered - lightly perturbed (i = 0.05) and unaltered - heavily perturbed (i = 0.2).\n2. intra-set stability: We compare explanations pairs produced for different images to verify the diversity of generated explanations.\nThe computations were conducted on explanations produced by the proposed method for a 500-element subset of validation ImageNet data. The subset was constructed by randomly picking 50 examples from 10 selected, diverse classes (library, over skirt, palace, prison, wall clock, lakeside, coral reef, volcano, fountain, basset)."}, {"title": "H Details on the covering experiment", "content": "Given an image and explanation, we instructed the annotator to cover the decision rationales with white rectangles. The annotator instruction is given below: Based on the following explanation of the classifier's prediction, cover the reasons for its decision with white rectangles. You can use as many rectangles as you like, but the total area of the covered image cannot be larger than half of the image. If it is not possible to cover the mentioned reasons by converting only 50% of the image, please do your best to cover the most important information."}, {"title": "I Layer-wise Relevance Propagation", "content": "To choose the most relevant neurons, the well-established Layer-wise Relevance Propagation (LRP) method is applied (Bach et al., 2015). LRP performs a special backward pass through the neural network to establish the influence of each neuron to the final prediction. Starting with the predicted value, LRP distributes it among the neurons in each layer, assigning them relevance scores. The following rule for relevance reallocation is used:\nRi = \u03a3j ( Z_ij / \u03a3_i Z_ij )Rj\nwhere Ri is the i-th neuron relevance score, Zij express how much i-th neuron has contributed to make j-th neuron relevant (calculated as the product of the neuron's activation and the corresponding weight), the sums \u03a3\u1d62 (\u03a3\u2c7c) iterate over all neurons in a given (next) layer. Choice of LRP as an explanation method Although we present a pipeline approach and there is some variability in how it can be implemented, we believe there are important reasons for using our pipeline with LRP. First, unlike many other methods, LRP works at the neuron level, which is strictly required by our method. Therefore, methods that provide pixel-level importance scores such as RISE (Petsiuk et al., 2018) or Grad-CAM (Selvaraju et al., 2017b), methods that typically work on image segments such as LIME (Ribeiro et al., 2016b) or SHAP (Lundberg and Lee, 2017) are not suitable for our approach. Second, LRP has been shown to achieve high faithfulness in many studies and can be considered state of the art in this respect. Note that achieving high faithfulness is the main goal of our approach. Finally, LRP is theoretically motivated, has been shown to be useful in many applications, and has stable open source implementations."}, {"title": "J Statistical analysis of the human evaluation results", "content": "For the results of human evaluation of plausibility, we performed the non-parametric global Friedman test followed by Nemenyi post-hoc analysis (as recommended in (Dem\u0161ar, 2006)). We were able to reject the null hypothesis of the Friedman test for all the measures with p<0.001. The Nemenyi post-hoc analysis with \u03b1 = 5% confirmed that our method obtains statistically significant improvements over other compared methods on both plausibility measures and the overall quality measure. On the fluency measure, our method is indistinguishable from SAT. The critical distance plots from Nemenyi post-hoc analysis are provided in Figure 4. The lower"}]}