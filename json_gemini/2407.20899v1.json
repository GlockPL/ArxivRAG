{"title": "Faithful and Plausible Natural Language Explanations for Image\nClassification: A Pipeline Approach", "authors": ["Adam Wojciechowski", "Mateusz Lango", "Ond\u0159ej Du\u0161ek"], "abstract": "Existing explanation methods for image clas-\nsification struggle to provide faithful and plau-\nsible explanations. This paper addresses this\nissue by proposing a post-hoc natural language\nexplanation method that can be applied to any\nCNN-based classifier without altering its train-\ning process or affecting predictive performance.\nBy analysing influential neurons and the cor-\nresponding activation maps, the method gener-\nates a faithful description of the classifier's de-\ncision process in the form of a structured mean-\ning representation, which is then converted\ninto text by a language model. Through this\npipeline approach, the generated explanations\nare grounded in the neural network architec-\nture, providing accurate insight into the clas-\nsification process while remaining accessible\nto non-experts. Experimental results show that\nthe NLEs constructed by our method are signifi-\ncantly more plausible and faithful. In particular,\nuser interventions in the neural network struc-\nture (masking of neurons) are three times more\neffective than the baselines.", "sections": [{"title": "1 Introduction", "content": "Despite remarkable advances in computer vision,\nthe deployment of image classification systems, es-\npecially in critical domains, poses significant chal-\nlenges. One of them is the opacity of deep models\nand the difficulty of providing reliable explanations\nfor their predictions (Doshi-Velez et al., 2017).\nTherefore, several types of explanation methods\nhave been proposed, including various forms of\nsaliency maps (Selvaraju et al., 2017a), feature im-\nportances (Ribeiro et al., 2016a), concept-based\nexplanations (Chen et al., 2019), counterfactual\nexplanations (Vermeire et al., 2022), etc. A par-\nticularly interesting form of explaining predictions\nis offered by natural language explanations (NLE)\ntechniques (Camburu et al., 2018; Wu and Mooney,\n2019). Such explanations are not only understand-\nable by non-expert users, but can also be used to\nsupport conversations with the user in a dialogue\nsystem (Raczy\u0144ski et al., 2023).\nThere are two critical properties of an expla-\nnation: faithfulness and plausibility (Jacovi and\nGoldberg, 2020; Atanasova et al., 2023). A faith-\nful explanation should accurately reflect the inner\nworkings of the system and provide information on\nthe real reasons why the model reached a certain\ndecision. Plausibility then refers to how convincing\nthe explanation appears to the user.\nIn the case of NLE, obtaining high plausibility is\nstraightforward, as textual explanations are usually\nhuman-friendly (Gurrapu et al., 2023), but achiev-\ning high fidelity is challenging. In the context of im-\nage classification, image captioning methods offer\nplausible but unfaithful NLEs (Xu et al., 2015; Ka-\nmakshi and Krishnan, 2023). Some methods try to\nimprove faithfulness by conditioning generation on\nboth the predicted class and image features (Hen-\ndricks et al., 2016; Kim et al., 2018; Marasovi\u0107\net al., 2020; Sammani et al., 2022), but the faith-\nfulness provided is still limited as the model is not\naware of the classifier's decision process. Other\nmethods train image classifiers to jointly predict\nthe class and visual rationales, and generate ex-\nplanations based on them (Wickramanayake et al.,\n2021; Kayser et al., 2022). However, the rationales\nare predicted independently of the class and do not\nparticipate in the classifier's decision process. Most\nimportantly, such methods change the training pro-\ncedure and the architecture of the classifier, often\naffecting classification performance.\nIn this paper, we propose a post-hoc natural lan-\nguage explanation method for image classification\nthat can be used with any standard convolutional\nneural network (CNN) classifier. To illustrate the\nclassification decision process, the method analyses\nwhich neurons of the CNN were most influential in\nreaching a given decision, and which regions of the\nimage caused them to activate. For each influential\nneuron, a neuron annotation method computes vi-"}, {"title": "2 Method", "content": "Our NLE generation approach uses two processing\nsteps, described below: meaning representation\n(MR) construction and MR-to-text conversion.\n2.1 Meaning representation construction\nWe first produce a meaning representation in the\nform of a JSON object, containing information\nabout the neurons responsible for a given classi-\nfier prediction (why?), what patterns those neurons\ndetected (what?), and in which parts of the image\nthey were activated (where?). The MR includes\nthe predicted class and the list of most influential\nneurons, each represented by (1) description \u2013 a\nphrase describing the pattern that most excites the\nneuron (convolutional filter), (2) positions \u2013 list\nof coarse-grained image positions (e.g., \u201cbottom-\nright corner\") where the neuron was activated. An\nexample of a MR is provided in Fig. 1.\nMR construction starts by storing all neuron ac-\ntivations from the given CNN-based classifier for\nthe prediction to explain. Next, the most important\nneurons are selected, annotated with a description,\nand tied to an image region, as follows:\nSelecting the most influential neurons To\nchoose the most relevant neurons, we apply well-\nestablished Layer-wise Relevance Propagation\n(LRP) method (Bach et al., 2015). LRP performs\na backward pass through the classifier network to\nestablish the influence of each neuron to the final\nprediction (see App. I for formulas). We select k\nneurons with the highest LRP scores (with k being\na parameter controlling the brevity-detail tradeoff).\nNeuron annotation We adopt the MILAN neu-\nron annotator (Hernandez et al., 2022) to gener-\nate descriptions of selected neurons. MILAN first\nfinds images in the classifier training set that make\na given neuron highly activated (Bau et al., 2017).\nThese exemplar images are used to generate a de-\nscription of the pattern that this neuron detects.\nNote that although the last step of MILAN is\nessentially image captioning, it does not affect the\nfaithfulness of NLEs produced by the pipeline, as\nlong as its output is of sufficient quality. The im-\""}, {"title": "2.2 Explanation generation", "content": "The second step of our method is converting the\nhuman-readable and faithful MR created above into\na user-friendly text. As we do not have any gold-\nstandard explanation texts, the task is performed\nby prompting a large language model (LLM).\nWe instructed the model to (1) produce fluent\ntext, (2) summarise the content of the MR (e.g. if\ntwo neurons detect similar patterns, they can be\ncombined in the text), (3) prioritise readability, (4)\ncome up with its own formulation of spatial po-\nsitions to improve fluency. We also provide one\nhandcrafted MR-to-text conversion example. The\nprompt is shown in App. D. LLMs could in the-ory hallucinate and thus reduce the explanations'\nfaithfulness. However, in Section 3 we show exper-\nimentally that current LLMs are reliable enough to\nproduce useful explanations."}, {"title": "3 Experimental evaluation", "content": "3.1 Experimental setup\nDataset All experiments were performed on the\nImageNet dataset. The classifier was trained on the\ntrain set and our explanation method was run to\nexplain predictions made on the validation data.\nModels We experiment with explaining the pre-\ndictions of the smallest CNN classifier from the\npopular ResNet family: ResNet18 (He et al.,\n2015). We fill our MRs with k = 10 top\nneurons indicated by LRP from the Captum li-\nbrary (Kokhlikyan et al., 2020) and annotate them\nusing MILAN's original implementation. As the\nLLM for MR-to-text conversion, we use GPT-\n4 (Achiam et al., 2023)."}, {"title": "3.2 Are the output explanations plausible?", "content": "To assess the plausibility of generated explanations,\nwe conducted a small-scale manual annotation ex-\nperiment. We recruited ten annotators: five non-\nexperts hired on the Prolific platform and five ex-\nperts with at least one published paper on explain-\nable AI. Each annotator was presented with 30\nimage-explanation pairs (300 in total) and asked to\nrate them on a scale of 1-5 whether the explanation\nwas (1) fluent, (2) easy to understand (comprehen-\nsible), (3) convincing, and (4) explanatory for the\nunderlying decision process. The overall quality\nof the explanations was also rated (see App. \u0415).\nThe results are presented in Tab. 1. Our method\nobtains the highest overall quality according to\nboth experts and non-experts. It also produces the\nmost plausible explanations (most convincing and\nexplanatory). Since the baselines produce much\nshorter explanations, it is not surprising that our\nlonger explanations are a bit more difficult to under-\nstand. Interestingly, experts generally give higher\nratings than non-experts. This is true for all meth-\nods and all factors except the explanatory ability.\nFor this factor, experts rate the baselines lower\nthan non-experts, but they consistently rate the ex-\nplanations provided by our pipeline higher. The\nimprovements of our method over baselines are sta-\ntistically significant on both plausibility measures\nand overall quality. For fluency, our method is\nindistinguishable from SAT (see details in App. J)."}, {"title": "3.3 Are the output explanations faithful?", "content": "The faithfulness of the generated explanations is\nassessed through two intervention experiments: (1)\nchecking if rationales from NLEs change the pre-\ndiction by masking parts of input images, (2) in-\nfluencing the network prediction by masking influ-\nential neurons. We further assess the stability and"}, {"title": "Masking input image", "content": "We asked annotators to\ncover with white rectangles parts of images that\ncontained the decision rationale indicated in the\nNLE, 50% area at most (see App. H for details). We\nre-classified covered images and measured changes\nin prediction and the average decrease in the prob-\nability of the originally predicted class. We also\nperformed an opposite experiment, with the anno-\ntators highlighting only parts of image mentioned\nin the explanation and covering the rest.\nFor covering, the use of our NLEs resulted in\nthe highest average probability decrease and the\nchange of the original prediction for 88% of exam-\nples (see Table 2). Our method reached the best\nresults in the highlighting experiment as well, pro-\nducing the least amount of changes.\nWe also re-ran our NLE pipeline with parts of\nthe input image covered. This led to significant\nchanges: on average 78% (median 90%) of the\nneurons indicated in MRs were different."}, {"title": "Masking influential neurons", "content": "To show the NLEs'\nability to reflect classifier decisions, we asked the\nannotators to read the NLEs and select up to five\nmost influential neurons from a MILAN-annotated\nlist. The classifier was then re-run with the selected\nneurons masked. The results in Table 2 reveal that\nmasking neurons suggested by our NLEs led to a\nfive times higher decrease in the predicted class\nprobability and over three times higher class flip\nrate than baselines. More details are in App. \u0412."}, {"title": "Explanation stability analysis", "content": "Following Wiegr-\neffe et al. (2021), we measure explanation robust-\nness against adding random noise to the input\nimage (intra-set stability, see App. G for details)\nby comparing BLEU (Papineni et al., 2002), \u039c\u0395-\nTEOR (Lavie and Agarwal, 2007) as well as class\nflip frequency and class probability change against\nthe original predictions. We also check for out-\nputs' diversity (inter-set stability) using BLEU and\nMETEOR overlap against explanations for other\nclasses. The results in Table 3 show high diversity\nand high noise sensitivity: In parallel to increased\nclassification changes with added noise, explana-\ntion BLEU/METEOR gradually drop, but they are\nlowest when comparing between different classes."}, {"title": "Reliability of the MR-to-text transform", "content": "The\nhuman evaluation of our approach's MR-to-text\nreliability was similar to plausibility evaluation, but\nlimited to non-expert Prolific annotators. We asked\nfive yes-no questions on information in the text not\ngrounded in the MR (i.e., hallucinations), omission\nof MR information, fluency, spatial information\nfidelity, and overall correctness (see App. F).\nThe results show the MR-to-text conversion as\nhighly reliable, as only 8% texts contain hallucina-\ntions. Omissions are more frequent (44%), but this\nis expected as the LLM is instructed to summarise\nthe MR and prioritise readability. This factor most\nlikely affected the overall score (58%). The ex-\nplanations are mostly fluent (96%), with correct\nspatial information (82%)."}, {"title": "Limitations", "content": "This paper produces a new method for plausible\nand more faithful natural language explanations for\nimage classification. Although we believe that the\nmethod provides significantly better faithfulness\nthan the previously proposed methods, it does not\nobtain completely faithful explanations. The faith-\nfulness of the explanations provided by our method\ndepends on the quality of the neuron annotations\nproduced by MILAN and the neurons indicated by\nLRP. Both techniques can be considered as state\nof the art, but they still occasionally produce incor-\nrect results. Therefore, the results of NLE methods\nshould be treated with caution. Additionally, this\nwork uses pre-trained language models, which are\nknown to expose certain social biases reflected in\ntheir training data."}, {"title": "A Examples of explanations", "content": "Several examples of explanations provided by the\nmethods under study are given in Tab. 4. More\nexamples of NLE produced with the proposed\npipeline approach are given in Tab. 5."}, {"title": "B Detailed results of masking influential\nneurons experiment", "content": "The detailed results of the experiment with masking\ninfluential neurons are presented in Fig. 2. For our\nmethod, the results indicate that masking neurons\nin the order indicated by the annotators leads to an\nincreasing change in the classifier's prediction and\na larger decrease in the predicted class probabil-\nity value. In contrast, the annotators gain limited\ninsight into how the neural network made a deci-\nsion from the explanations provided by other meth-\nods: masking the indicated neurons leads to limited\nchange, and increasing the number of masked neu-\nrons leads to almost no improvement. We attribute\nthe effectiveness of masking the first indicated neu-\nron to examples of classes that are highly correlated\nwith a pattern detected by a particular neuron."}, {"title": "C Simplification rules for spacial\npositions", "content": "The spatial information about neuron activation is\nlearned by inspecting activation maps of the cho-\nsen neurons. The activation map is converted to a\nbinary image, such that if a given pixel of an ac-\ntivation map exceeds half of a maximal value in\nthat activation map it is converted to the value of\none, and the rest of the pixels are assigned with the\nvalue zero. Then the binary activation map is di-\nvided into a 3x3 grid of congruent squares oriented\nsuch that the ordering begins with a 0 in the top-left\ncorner, progressing sequentially across each row\nfrom left to right and top to bottom, culminating\nwith the number 8 in the bottom-right position. The\nbasic positions names are given below."}, {"title": "D Prompt for MR-to-text conversion", "content": "To convert meaning representations into text, the\nfollowing prompt was applied to the language\nmodel:\nYou are given a problem of creating\ntextual\nexplanation\nof\nan\nimage\nclassification performed by neural\nnetwork. You will be given a Python\nobject representing network output in\nthe form of `'image class', [('detected\nobject', 'position'), ...]`.\nI want you\nto convert this object into a textual\nexplanation. You should:\n1. Create\na grammatically correct\nsentence which will explain the model's\ndecision.\n2. Decide which detected objects do\nnot fit with image class and do not\ninclude them in the explanation. For\ninstance, 'dentist' class and 'animal\nheads' objects are completely unrelated.\nHowever, the descriptions that aren't\ndirectly related to image class, but\ncan be indirectly correlated, especially\nin terms of shape, color, or texture\nresemblance should be included (like\n'fountain' and 'sea' because of the water\nthey have in common or 'brick wall' and\n'grid' because the texture is similar).\nNever mention that you chose neuron\ndescriptions and do not talk about the\nneuron descriptions that were discarded.\n3. Prioritize the readability of the\nexplanation. Include only essential\ndetected objects and aggregate\ninformation to shorten the explanation.\n4.\nAggregate positions if possible,\nfor example ['bottom-left corner',\n'bottom', 'bottom-right corner'] should\nbe aggregated into 'bottom'. If the\npositions list is too long or too\nambiguous do not include them in the\nexplanation.\nHere is an example.\nPython object:\n\"lakeside, [{'description': 'Nature',\n'positions': ['left', 'right',\n'bottom']}, {'description': 'The sky',\n'positions': ['top-right corner',\n'bottom-left corner', 'bottom']},\n{'description': 'Red and white colored\nobjects', 'positions': ['left',\n'right', 'bottom']}, {'description':\n'The ocean', 'positions': ['left',\n'right', 'bottom']}, { 'description':\n'Animal heads', 'positions': [],\n{'description': 'The color\n'id'}, 'positions': ['left', 'right',\n'bottom']'}, {'description': 'White\nbackgrounds', 'positions': ['bottom',\n'left']}, {'description': 'Grass',"}, {"title": "E Human evaluation of explanation\nplausibility", "content": "The annotators are presented with an image,\nmodel's prediction and a natural language explana-\ntion. Each question is answered on a scale from 1\n(low) to 5 (high). The following binary questions\nare asked:\n\u2022 How fluent (linguistically correct) the text is?\n\u2022 How easy to understand the text is?\n\u2022 How convincing do you find the explanation\nof the decision made by the model?\n\u2022 After reading the explanation, how well do\nyou understand how the decision of the model\nwas taken?\n\u2022 How would you rate the overall quality of the\nexplanation?\nThe annotation instructions are provided in the\ncode repository."}, {"title": "F Human evaluation of MR-to-text\ntransformation", "content": "The annotators are presented with a meaning rep-\nresentation in the form of formatted JSON without\na given image, since it should not influence the\nassessment of MR-to-text transformation. The fol-\nlowing binary questions are asked:\n\u2022 Does the text contain information that was not\npresent in the meaning representation?\n\u2022 Is there any important information from mean-\ning representation omitted in the text?\n\u2022 Is the text linguistically correct?\n\u2022 If any spatial compression occurred between\nexplained neurons, the said compression is\ncorrect?\n\u2022 Overall, do you find this meaning represen-\ntation to text transformation acceptable i.e.\nsufficiently good for explanation purposes?\nThe annotation instructions are provided in the\ncode repository."}, {"title": "G Details on stability experiments", "content": "Let us assume that an image is a matrix X, such\nthat X \u2208 [0, 1]. We model input perturbations\nby adding random noise to the images from the\nstandard normal distribution. Since an interval of\npossible pixel values is [0, 1], to account for the\nunboundedness of a standard normal distribution\nwe use a clipping operation defined as follows:\n\\(clip(x) =  \begin{cases}\n0, & \\text{if } x < 0 \\\\\nx, & \\text{if } 0 \\leq x \\leq 1 \\\\\n1, & \\text{if } x > 1\n\\end{cases}\\)\nThe formula for the function ( that adds a perturba-\ntion to the image is given below.\n\u03b6(X) = clip(N(0, 1) \u00b7 i + X)\nwhere i is the noise intensity.\nWe perform two types of stability experiments:\n1. inter-set stability: We compare pairs of un-\naltered images (1st set of images) and corre-\nsponding images mapped by the function\n\u03b6(2nd set of images). After the \u03da mapping is\nperformed, we run the proposed method on\nboth images, yielding two explanations. We\nthen compute various language similarity met-\nrics between the two explanations.\nWe compare two kinds of pairs of images:\nunaltered - lightly perturbed (i = 0.05) and\nunaltered - heavily perturbed (i = 0.2).\n2. intra-set stability: We compare explanations\npairs produced for different images to verify\nthe diversity of generated explanations.\nThe computations were conducted on explana-\ntions produced by the proposed method for a 500-\nelement subset of validation ImageNet data. The\nsubset was constructed by randomly picking 50\nexamples from 10 selected, diverse classes (library,\nover skirt, palace, prison, wall clock, lakeside, coral\nreef, volcano, fountain, basset)."}, {"title": "H Details on the covering experiment", "content": "Given an image and explanation, we instructed\nthe annotator to cover the decision rationales with\nwhite rectangles. The annotator instruction is given\nbelow:\nBased on the following explanation of\nthe classifier's prediction, cover the rea-\nsons for its decision with white rectan-\ngles. You can use as many rectangles as\nyou like, but the total area of the covered\nimage cannot be larger than half of the\nimage. If it is not possible to cover the\nmentioned reasons by converting only\n50% of the image, please do your best to\ncover the most important information."}, {"title": "I Layer-wise Relevance Propagation", "content": "To choose the most relevant neurons, the well-\nestablished Layer-wise Relevance Propagation\n(LRP) method is applied (Bach et al., 2015). LRP\nperforms a special backward pass through the neu-\nral network to establish the influence of each neu-\nron to the final prediction. Starting with the pre-\ndicted value, LRP distributes it among the neurons\nin each layer, assigning them relevance scores. The\nfollowing rule for relevance reallocation is used:\n\\(R_i = \\sum_j \\frac{Z_{ij}}{\\sum_i Z_{ij}} R_j\\)\nwhere \\(R_i\\) is the i-th neuron relevance score, \\(Z_{ij}\\)\nexpress how much i-th neuron has contributed to\nmake j-th neuron relevant (calculated as the prod-\nuct of the neuron's activation and the corresponding\nweight), the sums \\(\\sum_i\\) (\\(\\sum_j\\)) iterate over all neurons\nin a given (next) layer.\nChoice of LRP as an explanation method Al-\nthough we present a pipeline approach and there is\nsome variability in how it can be implemented, we\nbelieve there are important reasons for using our\npipeline with LRP.\nFirst, unlike many other methods, LRP works\nat the neuron level, which is strictly required by\nour method. Therefore, methods that provide\npixel-level importance scores such as RISE (Pet-\nsiuk et al., 2018) or Grad-CAM (Selvaraju et al.,\n2017b), methods that typically work on image seg-\nments such as LIME (Ribeiro et al., 2016b) or\nSHAP (Lundberg and Lee, 2017) are not suitable\nfor our approach.\nSecond, LRP has been shown to achieve high\nfaithfulness in many studies and can be considered\nstate of the art in this respect. Note that achieving\nhigh faithfulness is the main goal of our approach.\nFinally, LRP is theoretically motivated, has been\nshown to be useful in many applications, and has\nstable open source implementations."}, {"title": "J Statistical analysis of the human\nevaluation results", "content": "For the results of human evaluation of plausibility,\nwe performed the non-parametric global Friedman\ntest followed by Nemenyi post-hoc analysis (as\nrecommended in (Dem\u0161ar, 2006)). We were able\nto reject the null hypothesis of the Friedman test\nfor all the measures with p<0.001. The Nemenyi\npost-hoc analysis with \u03b1 = 5% confirmed that our\nmethod obtains statistically significant improve-\nments over other compared methods on both plau-\nsibility measures and the overall quality measure.\nOn the fluency measure, our method is undistin-\nguishable from SAT.\nThe critical distance plots from Nemenyi post-\nhoc analysis are provided in Figure 4. The lower"}]}