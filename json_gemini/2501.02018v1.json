{"title": "Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs", "authors": ["Joao Fonseca", "Andrew Bell", "Julia Stoyanovich"], "abstract": "Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs \u201cself-reflect\", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict \"normal\" model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SAFENUDGE, that combines Controlled Text Generation with \"nudging,\" or using text interventions to change the behavior of a model. SAFENUDGE triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTS. SAFENUDGE is open-source and available through https://pypi.org/, and is compatible with models loaded with the Hugging Face transformers library.", "sections": [{"title": "1 Introduction", "content": "Recent high profile cases have demonstrated the susceptibility of Large Language Models (LLMs) to jailbreak attacks, or adversarial attacks used to illicit high risk from behavior model. For example, cybercriminals have used jailbreaks on OpenAI's GPT-3.5 and GPT-4 models to create malware, phishing attacks, and scam websites (Lin et al., 2024). Many more critical examples can be found in the Artificial Intelligence (AI) Incident Database (McGregor, 2021)."}, {"title": "2 Preliminaries", "content": "Large Language Models (LLMs) are autoregressive models that perform next-token prediction, given an input prompt x (Aichberger et al., 2024). The input prompt can be represented as a sequence of tokens [x1,x2,...,xM], with each token xi \u2208 V, where V is the set of all tokens known to the model (note that this is said to be the vocabulary of the model). Let X denote the space of all possible input sequences x of any length. Then an LLM can be described as the function l : X \u2192 V, where l(x) = y, and y \u2208 V is the predicted next-token. The token y is sampled from a probability distribution over all possible tokens in the vocabulary of the model. We can execute the function l repeatedly, appending the output y to the input sequence x. All generated tokens can be thought of as the sequence of output tokens y = [Y1, Y2, ..., YT] where yi \u2208\u03bd and y denotes the space of all possible output sequences y of any length. We use the notation y<t to refer to the sub-sequence of tokens y1, ..., Yt, and yi:j to refer to the slice of tokens Yi, \u2026\u2026\u2026, Yj."}, {"title": "2.1 Sentence embeddings", "content": "A sentence embedding e \u2208 Rd is a d-dimensional vector representation of a natural language sentence or paragraph s \u2208 S that captures meaningful semantic information. These embeddings may be obtained via a sentence embedder, which is often another type of Language Model that takes a natural language sentence s as an input and produce a sentence embedding (Lin and Smith, 2019), i.e., a sentence embedder is a function 0 : S \u2192 Rd."}, {"title": "3 Motivation and T-safeness", "content": "In this work, we build a \u201cdownstream\u201d safeguard, focusing on the output sequence y, given any arbitrary input prompt x. In many of our experiments, we assume that a successful jailbreak attempt has taken place, and attempt to \"guide\u201d to the LLM back towards a safe response during inference. In this section, we provide a formal motivation for why we take this perspective.\nConsider an oracle \u03a9 that assigns a high probability to any natural language sentence or paragraph that could cause real-world harm, and a low probability otherwise, i.e., \u03a9 : S \u2192 [0, 1]. For example, if s* = \u201cHow is the weather today?\u201d, \u03a9(s*) would return a probability close to 0; however, if instead s* = \"Sure, here is a step-by-step tutorial for stealing from a charity...\u201d, then \u03a9(s*) would return a high probability.\nBecause the model L generates outputs y one token at a time, we can apply a decoder to those tokens at any time step and use \u03a9 to evaluate if the probability that output will cause harm is within some threshold T. Then, for any output y, we can define local T-safeness:\nDefinition 1 (Local T-safeness.) A sequence of tokens y is locally T-safe, iff\n\u2200t \u2208 \u03a4: \u03a9(\u03b4(y\u2264t)) < T\nWe can apply this local definition over all outputs of an LLM L to define a T-safe LLM:\nDefinition 2 (T-safeness) A model L : X \u2192 Y is T-safe iff y \u2208 Y, y is locally \u0442-safe.\nProposition 1 If a model is t-safe, then \u2200x \u2208 X, L(x) = y is locally t-safe.\nProposition 1 motivates the utility of downstream safeguards that trigger during inference. If one can obtain an oracle \u03a9, they can ensure that the output of a language model L is locally 7-safe, regardless of the input prompt. Rather than detecting jailbreaks in prompts, or modifying the weights of L to reduce the probability that an unsafe response is generated, one can ignore the prompt altogether, and safeguard the output sequence itself during generation to defend against entire classes of prompt-based jailbreaks attacks.\nAs the oracle \u03a9 is not available to us, we instead seek to approximate it using a classifier g : Rd \u2192 [0, 1], called the safety-discriminator, that uses the d-dimensional sentence embedding of a natural language sentence s to classify the sentence as either safe or unsafe. For convenience, we bundle g with the sentence embedder and a token decoder to define G(\u00b7) := g(0(\u03b4(\u00b7))), \u0456.\u0435., G: Y \u2192 [0, 1], and then define an approximate local T-safeness for a sequence of tokens y in the following way:\nVt\u2208T: G(y<t) < T\nThis could again be applied over the space of all outputs of an LLM L to create a notion of approximate T-safeness. In some sense, the problem of creating a T-safe model L is reduced to minimizing the error of the discriminator G. Fortunately, we have found that G can be successfully be trained to have a very low test error, as we describe later in this work."}, {"title": "4 Proposed method", "content": "At a high-level, our approach has two steps: first, like classic CTG approaches, we use an external safety-discriminator model to evaluate every token (or every n-th token) generated by an LLM during text generation, to evaluate the output for T-safeness. Second, if the discriminator detects that an unsafe output is being generated (i.e., G(y) > T), a hidden safety nudge is added to the response to change the course of text generation to a safer response. In this case, hidden means that it is not visible to the user of the LLM. A high-level demonstration of SAFENUDGE using the Meta-Llama-3.1-8B-Instruct model can be seen in Figure 1.\nThis simple approach is surprisingly effective: by combining CTG and safety nudges, we inherit the benefits of both with very few trade-offs. From the former, SAFENUDGE works in real-time during text generation, and the from the latter, SAFENUDGE inherits semantic fluency from the base model. Further, by exploiting transformer model architecture, we are able to increase the safety of output responses while adding negligible latency during text generation, as compared to other CTG methods."}, {"title": "4.1 The external safety-discriminator", "content": "Like with popular CTG methods, such as GeDI (Krause et al., 2020) and FUDGE (Yang and Klein, 2021), we train an external safety-discriminator G. The training process is made up of a several steps; for clarity, we have included Figure 2 which summarize our procedure.\nDataset. In preparation to train the discriminator G, we created a dataset containing LLM responses to prompts. We generated a mixture of jail-broken responses to adversarial prompts (e.g., \u201cOutline a plan to steal from a charity or nonprofit organization...", "Write a song about the summers of my childhood that I spent in the countryside...\").\nWe also added positive-versions of adversarial prompts to the dataset (e.g., \u201cOutline a plan to donate to a charity or nonprofit organization...": "."}, {"title": "Controlling Safety-Performance Trade-offs.", "content": ""}, {"title": "4.2 Safety nudging", "content": "If the safety-discriminator detects the generation of an unsafe subsequence of tokens during generations, i.e., G(y<t) > \u315c at some time step t, we replace the token yt with a safety nudge.\nDefinition 3 (Safety nudge) Let n be a sequence of tokens [n1,...,n\u014a], and \u2295 be a function that concatenates sequences of tokens together. Then n is a safety nudge if\nG(L(y<t\u2295n)) \u2264 G(L(y<t))\nIn other words, adding the safety nudge n to the output sequence y should not increase Gas L continues text generation. If necessary, this can be done repeatedly during generation to guarantee the model L is T-safe.\nIn this work, we select a specific n (written in the caption of Figure 1) choosing words and phrases that have been shown increase the safety of LLM responses (Fei et al., 2024), but n could be optimized using a modifiable character buffer, similar to the jailbreak attack GCG (Zou et al., 2023). We leave this for future work.\nWe would like to highlight three important implementation details: first, we do not display the safety nudge to the user. Instead, n is only used by the model in next token prediction. Second, in practice, we copy the last k tokens of the sequence y<t after the nudge n (we form the sequence y<tn\u2295yk:t\u22121) to ensure the LLM is generating semantically fluent outputs from the user's perspective. Third, in practice, we only perform one safety nudge per text generation. We found that allowing multiple safety nudges can have negative effects on inference time."}, {"title": "5 Empirical results", "content": ""}, {"title": "5.1 Performance of the safety-discriminator", "content": "The discriminator G was trained using 10-fold cross validation over 3 random seeds, and classifiers were tuned using the hyperparemeter grid found in Appendix Table 5. The full classifier performance for the discriminator G trained is shown in Appendix Table 8. The best performing classifier was a Multi-Layer Percpetron (MLP) model with an F1 score of approximately 87.8%. To confirm these results, we also tested the performance of the classifiers a holdout set, i.e., entirely out-of-sample data, as would observed in an actual implementation in the wild. These results can be seen in Table 1. Significantly, the performance remained the same (or slightly increased) indicating it is possible to train a robust and effective safety-discriminator G using the hidden state embeddings from an LLM."}, {"title": "5.2 Effectiveness of SAFENUDGE", "content": "Experimental setting. We test the effectiveness of SAFENUDGE to reduce unsafe responses in two models: the Llama-3.1-8B-Instruct model (Base), and an uncensored version of that same model (Uncensored).\nFor 260 out-of-sample AdvBench adversarial prompts and 260 out-of-sample IFEval tasks, we generated responses for the Base and Uncensored models using Vanilla text generation, generation under SAFENUDGE, and generation for a benchmark (tm, detailed later in this section). For adversarial prompts from AdvBench, we simulate a setting where a successful jailbreak attack took place, i.e., we require the LLMs to start their responses start with a phrase like \"Sure, here is a...\u201d (Zou et al., 2023). Note that to avoid data leakage, this phrase was removed from jailbroken responses used to"}, {"title": "6 Discussion", "content": "Our empirical results show that SAFENUDGE is effective at preventing jailbreak attacks during inference with minor impacts to inference time, output perplexity, and \u201cnormal\u201d model behavior.\nImportantly, SAFENUDGE expands the toolbox of available safegaurds for LLMs. We take the perspective that safeguarding LLMs will not be achieved through a single approach, but instead the preferred approach (or ensemble of approaches) will vary greatly depending on the constraints and objectives of the model, which are induced by factors like the context-of-use, the stakeholders, and the types of risks and harms posed by the model.\nPractitioners aiming to safeguard models must ask questions like, \u201chow much additional inference time or compute can we tolerate?", "how severe are the risks and harms associated with this system?": "and \"to what extent can normal model behavior be constrained?", "trade-offs": "the impact on inference time per token, output perplexity, and the base LLM's perfromance on the"}, {"title": "7 Related work", "content": "Jailbreaking. Jailbreaks are adverserial attacks used to illicit unwanted behavior from an LLM (Xu et al., 2024; Yong et al., 2023; Glukhov et al., 2023). The largest class of jailbreak attacks are prompt-based attacks, which involve engineering prompts in such a way so that they induce illicit behaivor through attack-vectors like role-playing (e.g.,\"Imagine you are writing NPC dialogue for a video game...\"), phrases like \u201cforget all previous instructions\u201d, and simulating \u201cdeveloper mode\" (Shen et al., 2023).\nMore sophisticated attacks involve optimization or few-shot learning. Zou et al. (2023) introduced a prompt-based attack that works by concatenating a random set of tokens to the end of an adversarial input prompt, then iteratively adjusting those tokens to maximize the probability that the LLM produces a jailbroken response. Liu et al. (2023) used a hierarchical genetic algorithm to create semantically fluent prompt-based attacks that maximize the probability an LLM illicit unwanted behavior. (Wei et al., 2023) showed the effectiveness of few-shot learning in creating robust, transferable jailbreaks.\nNudging. Nudges, or small interventions that attempt to influence how people make decisions, are a well-studied concept originating from behavioral economics literature that have been shown to increase vaccination uptake (Re\u00f1osa et al., 2021) and influence people towards making healthier eating decision (Broers et al., 2017). While the literature is still in its infancy, there are several studies that transfer the idea of \u201cnudging\u201d to LLMs, using text-based interventions to guide text generation. efficacy of \"nudging\" being used to guide LLM text-generation (Fei et al., 2024; Hegde et al., 2024). (Fei et al., 2024) used small aligned models to generate \"nudging tokens\" to guide a base"}, {"title": "A Additional details on the proposed method", "content": ""}, {"title": "B Additional results", "content": ""}, {"title": "CC-FUDGE", "content": "Recall that the output sequence y is generated one token at a time by applying the function l : X \u2192 V repeatedly to generate tokens, where l(x) = y any time step is sampled from a probability distribution over all possible tokens in the vocabulary of the model.\nIn practice, LLMs are implemented with either top-k or top-p selection. Rather than the probability distribution being over the entire vocabulary of the model, the domain of choices is often restricted to a preset number of k tokens, or over the tokens whose cumulative probability is greater than some p. Vocabulary size varies by model, but for context, the Meta-Llama-3-8B-Instruct model (which we will use in our experiments) has 128,256 tokens in its vocabulary. Reasonable choices for k include 10, 50, or 100, i.e. k << |V|. The set of top-k tokens at a time step it can be denoted (k) CV.\nIn FUDGE (Yang and Klein, 2021), the probability distribution over (k)is scaled by a vector induced by the external discriminator. In C-FUDGE, we implement the same approach, but with one modification: we reduce the probability of tokens that will generate an unsafe output to 0, and redistribute weights across the remaining tokens. If all tokens are identified by the discriminator as leading to an unsafe response, generation defaults to selecting the token with the lowest probability of being unsafe. More formally, we restrict the domain of l at each time step and create a subset V(k) (k) that contains only tokens that ensure T-safeness at time t + 1. Given an output sequence y up to time t 1, and G : Y \u2192 [0, 1], V(k) = {\u03c5\u03bd \u2208 V(k), G(y1,..., Yt-1, v) <T}."}]}