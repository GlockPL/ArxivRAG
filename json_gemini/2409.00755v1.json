{"title": "Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification", "authors": ["Haojian Huang", "Chuanyu Qin", "Zhe Liu", "Kaijing Ma", "Jin Chen", "Han Fang", "Chao Ban", "Hao Sun", "Zhongjiang He"], "abstract": "Multi-view classification (MVC) faces inherent challenges due to domain gaps and inconsistencies across different views, often resulting in uncertainties during the fusion process. While Evidential Deep Learning (EDL) has been effective in addressing view uncertainty, existing methods predominantly rely on the Dempster-Shafer combination rule, which is sensitive to conflicting evidence and often neglects the critical role of neighborhood structures within multi-view data. To address these limitations, we propose a Trusted Unified Feature-NEighborhood Dynamics (TUNED) model for robust MVC. This method effectively integrates local and global feature-neighborhood (F-N) structures for robust decision-making. Specifically, we begin by extracting local F-N structures within each view. To further mitigate potential uncertainties and conflicts in multi-view fusion, we employ a selective Markov random field that adaptively manages cross-view neighborhood dependencies. Additionally, we employ a shared parameterized evidence extractor that learns global consensus conditioned on local F-N structures, thereby enhancing the global integration of multi-view features. Experiments on benchmark datasets show that our method improves accuracy and robustness over existing approaches, particularly in scenarios with high uncertainty and conflicting views. The code will be made available at https://github.com/JethroJames/TUNED.", "sections": [{"title": "Introduction", "content": "Multi-view classification (MVC) has become a prominent research area due to the increasing availability of data from diverse sources. However, the existing MVC approaches predominantly encode multi-view features into a low-dimensional feature space through a feature bottleneck for fusion and classification. This straightforward fusion process often encounters inherent challenges, such as domain gaps and varying information content across views, which can introduce latent uncertainties during the fusion stage. To address this, many methods have explored measuring the quality of individual view representations to achieve discriminative multi-view fusion. Notably, Evidential Deep Learning (EDL), which stems from subjective logic (J\u00f8sang 2016) and Dempster-Shafer theory (Dempster 1968), has demonstrated effectiveness in assessing view representation uncertainty across various studies (Huang et al. 2024b; Bao, Yu, and Kong 2021; Yu et al. 2023; Qin et al. 2022; Shao, Dou, and Pan 2024; Xu et al. 2024; Yue et al. 2024).\nNevertheless, current EDL-based multi-view learning methods heavily rely on the Dempster-Shafer fusion framework. While theoretically robust, this framework is highly sensitive to conflicting evidence, where even a single contradictory source can produce anomalous results, ultimately compromising inference performance (Huang et al. 2023a; Xiao 2019). Additionally, these methods often overlook the critical importance of neighborhood structure in multi-view features, leading to suboptimal performance in downstream tasks. For instance, in facial expression recognition, focusing solely on isolated view features can lead to misinterpretations due to variations such as lighting or angles. Without effectively leveraging both local and global neighborhood relationships, models struggle to capture the full context of the data, resulting in a less reliable fusion process. Unfortunately, existing EDL-based methods do not effectively leverage both local and global neighborhood relationships, which hinders their ability to fully capture the context of the data and leads to a less reliable fusion process.\nTo address these challenges, we propose a model for robust MVC method called Trusted Unified Feature-NEighborhood Dynamics (TUNED). To be specific, in the feature extraction stage, our method not only captures view-specific information but also integrates local neighborhood structures within each view. During the fusion stage, we simultaneously consider global neighborhood structures and the learning of cross-view dependencies, allowing for a more comprehensive understanding of the relationships between different views. To further refine this process, we introduce a dynamic Markov random field that adaptively accounts for cross-view relationships, suppressing views with lower coherence and learning optimal weights for each view. Moreover, We employ a parameterized evidence extractor to learn a collaborative evidence distribution. From this distribution, we sample evidence and perform a locally conditioned fusion with the evidence from each view. This approach enhances the joint learning of local and global feature-neighborhood structures, facilitating more effective and conflict-resilient integration across multiple views. Our contributions are threefold as follows:\n\u2022 We introduce a novel MVC framework that seamlessly integrates both local and global neighborhood structures. This approach enhances the robustness of feature extraction and fusion processes by effectively capturing cross-view dependencies and mitigating potential conflicts.\n\u2022 We develop a selective Markov random field (S-MRF) model combined with a parameterized evidence extractor, which adaptively learns and fuses collaborative evidence across multiple views. This dual-layered evidence integration significantly improves the model's ability to handle complex, heterogeneous data sources.\n\u2022 Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving superior accuracy and robustness in MVC tasks, particularly in scenarios with high levels of uncertainty and conflicts."}, {"title": "Related work", "content": "Most conflictive multi-view learning approaches focus on eliminating conflicting data instances. One line of work is based on multi-view outlier detection, which identifies outliers exhibiting abnormal behavior across multiple views. These methods are generally categorized into two types: cluster-based (Huang et al. 2023b; Zhang et al. 2023; Liu et al. 2024) and self-representation-based (Wang et al. 2019; Wen et al. 2023b). Cluster-based methods perform clustering independently in each view and generate affinity vectors for each instance (Marcos Alvarez et al. 2013; Zhao et al. 2017). Outliers are then detected by comparing these vectors across different views. In contrast, self-representation-based methods detect outliers by evaluating their difficulty in being represented by the normal views (Hou et al. 2020).\nAnother approach focuses on partially view-aligned multi-view learning (Zhang et al. 2021; Wen et al. 2023c). Early work (Lampert and Kr\u00f6mer 2010) introduced weakly-paired maximum covariance analysis to address challenges posed by unaligned data. More recently, (Huang et al. 2020) employed a differentiable variant of the Hungarian algorithm to align unaligned data. Subsequent work proposed noise-robust contrastive learning (Yang et al. 2021; Li 2022; Qin et al. 2024) to compute alignment matrices.\nHowever, these conflictive multi-view learning methods primarily aim to eliminate conflicting instances to achieve coordinated decisions across views. In scenarios like adversarial attack defense, making careful decisions in the presence of conflicting instances is crucial. To address this, recent methods have leveraged Evidential Deep Learning's (EDL) uncertainty measurement capabilities, quantifying both the conflict degree of among global proxy views (Liu et al. 2023, 2022; Xu et al. 2024) or local intra-view dissonance (Yue et al. 2024). These methods seek to coordinate multiple views for decision-making without discarding conflicting instances. However, they universally rely on the Dempster-Shafer evidence fusion framework, which is highly sensitive to conflicting views (Xiao 2019; Huang et al. 2023a). Without hand-crafted loss functions, this framework often yields counterintuitive and unreliable results, degrading task performance. To overcome these limitations, we propose a novel multi-view fusion framework that introduces a global-local F-N joint learning paradigm, further mitigating multi-view conflicts and enabling more reliable decision-making."}, {"title": "Uncertainty-based deep learning", "content": "Deep neural networks have made significant strides across various tasks, yet they often struggle to quantify uncertainty, especially when dealing with noisy or low-quality data (Wen et al. 2023a; Chen et al. 2024). To address these uncertainties, several approaches have been developed, including deterministic methods (Sensoy, Kaplan, and Kandemir 2018), Bayesian neural networks (Gal and Ghahramani 2016), ensemble methods (Lakshminarayanan, Pritzel, and Blundell 2017), and test-time augmentation (Lyzhov et al. 2020). These methods offer different mechanisms for uncertainty estimation, either by enhancing model robustness through probabilistic modeling or by using multiple predictions or augmentations for more accurate uncertainty quantification.\nRecently, EDL, a prominent deterministic approach evolving from subjective logic (J\u00f8sang 2016) and Dempster-Shafer theory (DST) (Dempster 1968), has shown promise in uncertainty estimation. EDL has been successfully applied to tasks like MVC (Han et al. 2021, 2022; Liu et al. 2022, 2023; Xu et al. 2024; Huang et al. 2024a), zero-shot learning (Huang et al. 2024b), cross-modal retrieval (Qin et al. 2022), and action sequence localization (Chen, Gao, and Xu 2023; Gao, Chen, and Xu 2023; Ma et al. 2024), demonstrating its versatility. In MVC, the pioneering work (Han et al. 2021) introduced the Dempster-Shafer combination rule, sparking a series of EDL-based studies. However, most of these methods assume that uncertainty will decrease after evidence fusion. The Dempster-Shafer rule, being highly sensitive to conflicting views, can produce counterintuitive and unreliable results when faced with conflicting evidence (Xiao 2019; Huang et al. 2023a). Despite recent efforts to improve global coordination and local consistency through hand-crafted loss functions, the limitations of the Dempster-Shafer rule still hinder EDL's progress in MVC. To this end, we propose a novel fusion framework that mitigates the impact of conflicting views without relying on complex similarity measures or intricate loss designs."}, {"title": "Methodology", "content": "This section presents our proposed method for enhancing EDL through robust evidence fusion in conflictive MVC using the S-MRF. We begin by formally defining the problem, followed by a detailed explanation of how our model incorporates view-specific local F-N structures during the evidence extraction phase. Subsequently, we describe the use of S-MRF to suppress inter-view conflict dependencies, enabling the joint learning of local-global F-N structures. We then outline the loss functions involved in our approach. Finally, we discuss the factors contributing to the effectiveness of our method."}, {"title": "Problem Definition", "content": "In MVC, we are given a dataset $\\mathcal{D} = {(\\mathbf{X}_i, Y_i)}_{i=1}^n$, where $n$ denotes the number of samples. Each sample $\\mathbf{X}$ consists of multiple views, represented as $\\mathbf{X}_i = {\\mathbf{x}_i^v}_{v=1}^V$, where $V$ is the total number of views, and $\\mathbf{x}_i^v \\in \\mathbb{R}^{d_v}$ represents the feature vector of the $i$-th sample in the $v$-th view. The corresponding label for each sample is denoted by $y \\in \\mathcal{Y}$, where $\\mathcal{Y}$ is the set of possible labels. The target of MVC is to learn a model $F_{\\Theta}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that accurately predicts the label $y$ for an unseen sample $\\mathbf{X}$ by effectively integrating information from all available views {$\\mathbf{x}_i^v$}$_{v=1}^V$. The challenge lies in leveraging the complementary information from each view while handling potential inconsistencies or conflicts among the views, ultimately improving the overall classification performance."}, {"title": "Local F-N structure extraction", "content": "Local feature structure embedding. We simply extract local features from each view using view-specific DNNs. Specifically, for each view $v$, we extract feature representations $\\mathbf{h}^v$ from the input data using DNNs {$f^v(\\cdot)$}$_{v=1}^V$. Given an input feature vector $\\mathbf{x}^v$, the corresponding DNN $f^v(\\cdot)$ maps this to a feature vector:\n$\\mathbf{h}^v = f^v(\\mathbf{x}^v; \\theta^v)$\n(1)\nwhere $\\theta^v$ denotes the parameters of the DNN for view $v$. These local feature representations inherently capture the local neighborhood structure within each view, providing critical information for subsequent integration.\nLocal neighborhood structure embedding. In multi-view data analysis, traditional feature extraction methods often fail to account for the inherent local neighborhood structures among samples. This omission is particularly problematic in datasets with non-Euclidean characteristics, such as social or biological networks, where intricate patterns are prevalent. Capturing both local and global relationships is crucial for fully understanding the data's complexity. To effectively capture the complex local neighborhood relationships within multi-view data, we employ the Clustering-with-Adaptive-Neighbors (CAN) (Nie, Wang, and Huang 2014) approach to construct an adaptive adjacency matrix for our Graph Convolutional Network (GCN). This method dynamically determines the neighborhood structure for each sample, enabling the model to robustly reflect both local and global data characteristics.\nGiven a feature matrix $\\mathbf{X}^v \\in \\mathbb{R}^{d_v \\times n}$, where $d_v$ represents the feature dimension for view $v$ and $n$ represents the number of samples, we first compute the pairwise distance matrix $\\mathbf{D}^v \\in \\mathbb{R}^{n \\times n}$, where each element $D_{ij} = ||\\mathbf{x}_i^v - \\mathbf{x}_j^v||_2^2$ represents the squared distance between samples $\\mathbf{x}_i^v$ and $\\mathbf{x}_j^v$. Next, we sort the distances for each sample to identify its top $k$ nearest neighbors. Let $\\rho(\\mathbf{x}_i^v)$ denote the distance to the $k$-th nearest neighbor for sample $\\mathbf{x}_i^v$. We then compute a weight matrix $\\mathbf{W}^v$ using the formula $T_{ij} = \\rho(\\mathbf{x}_i^v) - D_{ij}$, which balances the influence of each neighbor as follows:\nW_{ij}^v = \\text{max} \\left( 0, \\frac{T_{ij}}{\\rho(\\mathbf{x}_i^v) - \\sum_{\\mathbf{x}_m^v \\in N_i^v} D_{im}} \\right),$\n(2)\nwhere $N_i^v$ represents the set of $k$ nearest neighbors for sample $\\mathbf{x}_i^v$. This adaptive weighting strategy ensures that each sample's neighborhood captures the most relevant local relationships, leading to a more representative and robust graph structure. The symmetrized adjacency matrix $\\mathbf{A}^v$ is then computed as:\n\\mathbf{A}^v = \\frac{\\mathbf{W}^v + (\\mathbf{W}^v)^T}{2},\n(3)\nwhere $\\mathbf{A}^v$ is used as the input for the view-specific GCN, and the feature extraction process is defined as below:\n\\mathbf{Q}_{l+1}^v = \\sigma (\\hat{\\mathbf{A}}^v \\mathbf{Q}_l^v \\mathbf{W}_l^v)$,\n(4)\nwhere $\\hat{\\mathbf{A}}^v$ is the normalized adjacency matrix, $\\sigma$ is the activation function (e.g., ReLU, Softplus), and $\\mathbf{W}_l^v$ are the learnable weights at layer $l$."}, {"title": "Neighborhood-aware evidential deep learning", "content": "In multi-view learning, capturing the complex relationships among data points is essential for accurate classification. Traditional methods often overlook neighborhood structures, leading to suboptimal integration of multi-view data, especially in non-Euclidean spaces like social or biological networks. To address this, we propose neighborhood-aware EDL, which explicitly incorporates neighborhood structure. By fusing local-global F-N structures, our approach enhances the model's ability to account for dependencies among data points and achieve more reliable and robust classification."}, {"title": "Local evidence extraction", "content": "In our framework, the neighborhood-aware features are fused using a function $\\Psi(\\cdot)$ (See Supplementary Material) with an activation layer (e.g., ReLU, Softplus) to obtain non-negative output as evidence for each view, so that we can obtain the parameters of the Dirichlet distribution. For a classification problem with $K$ classes, the evidence $e^v$ for an instance $\\mathbf{x}$ in view $v$ is represented by an opinion $\\omega^v = (\\mathbf{b}^v, u^v)$, where $\\mathbf{b}^v = (b_1^v, ..., b_K^v)$ is the belief mass vector, which assigns belief values to each class based on the evidence and $u^v$ is the uncertainty mass, representing the degree of uncertainty in the evidence. These components are constrained by the following relationship:\n$\\sum_{k=1}^K b_k^v + u^v = 1$, where $b_k^v \\geq 0$ and $u^v \\geq 0$.\n(5)\nThe belief $\\mathbf{b}^v$ and uncertainty $u^v$ are linked to the Dirichlet distribution, which models second-order uncertainty. The Dirichlet probability density function (PDF) is defined as:\n$D(\\mathbf{p} | \\alpha^v) = \\begin{cases} \\frac{1}{B(\\alpha^v)} \\prod_{k=1}^K p_k^{\\alpha_k^v - 1}, & \\text{for } \\mathbf{p} \\in S_K,\\\\ 0, & \\text{otherwise},\\end{cases}$\n(6)\nwhere $\\mathbf{p}^v = (p_1^v, ..., p_K^v)^\\top$ is the probability assigned to each class $k$, and $\\alpha^v = (\\alpha_1^v, ..., \\alpha_K^v)^\\top$ are the Dirichlet parameters. The simplex $S_K$ is defined as:\n$S_K = {\\mathbf{p} | \\sum_{k=1}^K p_k = 1, 0 \\leq p_k \\leq 1 \\text{ for all } k}$.\n(7)\nThe view-specific Dirichlet parameters $\\alpha^v$ are computed as $\\alpha^v = e^v + 1$, ensuring non-sparsity. The belief and uncertainty are related to these parameters by:\nb_k^v = \\frac{\\alpha_k^v - 1}{S^v}, \\qquad u^v = \\frac{K}{S^v},\n(8)\nwhere $S^v = \\sum_{k=1}^K (e_k^v + 1) = \\sum_{k=1}^K \\alpha_k^v$. The uncertainty $u^v$ is inversely proportional to the total evidence, indicating that as the evidence increases, the uncertainty decreases.\nAfter obtaining the view-specific evidence, this step reflects the confidence of the local proxy in classifying based on the F-N structures. This evidence is then further integrated with global F-N structures in a joint learning manner, setting the stage for robust multi-view evidences fusion."}, {"title": "Global consensus evidence extraction", "content": "To resolve conflicts between views and establish a robust multi-view consensus, we introduce a consensus evidence extraction mechanism. This mechanism samples a consensus evidence vector $\\mathbf{e}^{cons}$ from a shared Dirichlet distribution and conditions it on the local F-N structures through a fusion function $\\Phi(\\cdot)$, which integrates the sampled consensus evidence with the view-specific evidence vectors as follows:\ne^v = \\mathbf{e}^v + \\Phi(\\mathbf{e}^v, \\mathbf{e}^{cons}),\n(9)\nwhere $\\mathbf{e}^{cons} \\sim Dir(\\alpha)$ is the evidence vector randomly sampled from the shared Dirichlet distribution parameterized by $\\alpha$. The fusion function $\\Phi(\\cdot)$ (e.g. Self-Attention, cosine similarity etc.) conditions the consensus evidence on the local F-N structures, facilitating robust global learning of multi-view consensus evidence. This approach ensures that the consensus evidence effectively reflects patterns across all views, while also being adaptable to local variations."}, {"title": "Conflictive evidence aggregation", "content": "Conflicts among views are prevalent yet often overlooked by existing methods. Directly integrating all views can lead to performance degradation, as seen in frameworks like the Dempster-Shafer Theory, where minor local conflicts can cause global fusion failures. To address this, we propose the S-MRF model, which adaptively selects highly relevant views for evidence fusion. This model mitigates the impact of local conflicts on global neighborhood-aware evidence integration, ensuring robust and reliable inference."}, {"title": "Selective Markov random field", "content": "Specifically, the S-MRF model represents the interaction among views through an energy function, which considers both unary and pairwise potentials. The energy function is defined as below:\n$\\mathbb{E}(\\mathbb{Z}) = \\sum_i \\Phi(z_i) + \\sum_{i,j} \\sigma(z_i, z_j)$,\n(10)\nwhere $\\Phi(z_i)$ represents the unary potential representation of view $i$, and $\\sigma(z_i, z_j)$ represents the pairwise potential representation between views $i$ and $j$. In the S-MRF model, each view $v$ is associated with an evidence vector $\\mathbf{e}^v$. A graph $\\mathbb{G} = (\\mathbb{V}, \\mathbb{E})$ is constructed, where $\\mathbb{V}$ corresponds to the set of views, and $\\mathbb{E}$ represents the set of edges determined by the similarity between views. The edge weight $w_{ij}$ reflects the similarity between views $i$ and $j$, calculated using metrics such as cosine similarity or other suitable measures.\nEdges are included in the graph if they meet a threshold $\\tau$, ensuring that only significant and relevant connections are retained, thus addressing inter-view conflicts:\nw_{i,j} \\geq \\tau w_{max},\n(11)\nwhere $w_{max}$ is the maximum edge weight in the initial connectivity matrix."}, {"title": "Local-global evidence aggregation", "content": "According to equation (9), we denote view $v$ evidence as $\\tilde{\\mathbb{E}}^v$, which accounts for local-global F-N structures. The aggregated evidence $\\mathbb{E}_{agg}$ from these selectively connected views is computed as:\n$\\mathbb{E}_{agg} = \\sum_{(\\mathbf{i}, \\mathbf{j}) \\in \\mathbb{E}} W_{ij} \\tilde{\\mathbb{E}}^j$.\n(12)\nBy normalizing the contributions, the S-MRF model effectively combines evidence from views that are contextually significant, reducing the impact of irrelevant or conflicting views on the overall integration process. This selective approach enhances integration efficiency and improves the interpretability of the resulting evidence fusion."}, {"title": "Loss function", "content": "Neighborhood-aware EDL loss. For each instance {$\\mathbf{x}_i^v$}$_{v=1}^V$, we fuse the extracted feature structure $\\mathbf{h}_i^v$ and neighborhood structure $\\mathbf{q}_i^v$ using a F-N aggregation function $\\Psi(\\cdot)$ to obtain evidence $\\mathbf{e}_i^v$ for each view:\n$\\mathbf{e}_i^v = \\Psi(\\mathbf{h}_i^v, \\mathbf{q}_i^v)$.\n(13)\nThis evidence vector $\\mathbf{e}_i^v$ is then used to parameterize the Dirichlet distribution $\\alpha_i^v = \\mathbf{e}_i^v + 1$. Then we adapt the conventional cross-entropy loss to integrate the evidence and uncertainty from each view, enhancing the robustness of the classification process as follows.\n$\\begin{aligned} \\mathcal{L}_{a c e}\\left(\\alpha_{i}^v\\right) & = \\int \\sum_{j=1}^{K}-Y_{n j} \\log p_{n j} \\frac{\\prod_{j=1}^{K} p_{n j}^{\\alpha_{j}^v-1}}{B\\left(\\alpha^{v}\\right)} d p_{n} \\\\ & = \\sum_{j=1}^{K} Y_{n j}\\left(\\psi\\left(S_{n}\\right)-\\psi\\left(\\alpha_{n j}^v\\right)\\right), \\end{aligned}$\n(14)\nwhere $\\psi(\\cdot)$ is the digamma function. The above loss function does not guarantee that the evidence generated by the incorrect labels is lower. To address this issue, we can introduce an additional term in the loss function, namely the Kullback-Leibler (KL) divergence:\n$\\begin{aligned} \\mathcal{L}_{K L}\\left(\\alpha_{i}^v\\right) & = K L\\left[D\\left(p_{n} | \\bar{\\alpha}_{n}\\right) || D\\left(p_{n} | \\mathbf{1}\\right)\\right] \\\\ & = \\log \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\bar{\\alpha}_{n k}\\right)}{\\Gamma(K) \\prod_{k=1}^{K} \\Gamma\\left(\\bar{\\alpha}_{n k}\\right)} + \\sum_{k=1}^{K}\\left(\\left(\\bar{\\alpha}_{n k}-1\\right) \\psi\\left(\\bar{\\alpha}_{n k}\\right)-\\psi\\left(\\sum_{j=1}^{K} \\bar{\\alpha}_{n j}\\right)\\right) \\end{aligned}$\n(15)\nwhere $D(\\mathbf{p}_n | \\mathbf{1})$ is the uniform Dirichlet distribution, $\\bar{\\alpha} = \\mathbf{y}_n + (1 - \\mathbf{y}_n)$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters for the $n$-th instance, and $\\Gamma(\\cdot)$ is the gamma function.\nTherefore, given the Dirichlet distribution with parameter $\\alpha_i^v$ for the $n$-th instance, the loss is:\n$\\mathcal{L}_{a c c}\\left(\\alpha_{i}^v\\right) = \\mathcal{L}_{a c e}\\left(\\alpha_{i}^v\\right) + \\lambda_{t} \\mathcal{L}_{K L}\\left(\\alpha_{i}^v\\right)$,\n(16)\nwhere $\\lambda_{t} = \\min (1.0, t / T) \\in[0,1]$ is the annealing coefficient, with $t$ representing the current training epoch and $T$ the total number of annealing steps. This gradual increase in the influence of KL divergence in the loss function helps prevent premature convergence of misclassified instances towards a uniform distribution."}, {"title": "Conflict-aware aggregation loss", "content": "In the proposed S-MRF module, each graph node represents evidence derived from different views. To achieve conflict-aware MVC, it is essential to harmonize the evidence across these views. This is accomplished by a combined loss function that simultaneously maximizes the pairwise similarity between evidence and minimize the deviation of each evidence from their mean. Moreover, we introduce a regularizer to prevent node degradation and maintain diversity in the graph.\nSpecifically, to ensure global consistency and inter-view collaboration, we introduce a unified consistency loss $\\mathcal{L}_{con}$, which combines both the similarity encouragement in terms of statistical property and regularization components:\n$\\begin{aligned} \\mathcal{L}_{c o n} = & \\frac{2}{V(V-1)} \\sum_{i=1}^{V-1} \\sum_{j=i+1}^{V} \\frac{\\mathbf{e}_{i} \\mathbf{e}_{j}}{\\left\\|\\mathbf{e}_{i}\\right\\| \\left\\|\\mathbf{e}_{j}\\right\\|} + \\\\\\ & \\frac{1}{V N} \\sum_{v=1}^{V} \\sum_{n=1}^{N} \\left\\|\\mathbf{e}_{n}^{v}-\\mu^{v}\\right\\|^{2}, \n(17)\nwhere $\\mu^v$ represents the mean evidence across all samples within view $v$, while $\\mathbb{E}^v$ denotes the $v$-th view evidence.\nTo sum up, the overall loss function for a specific instance {$\\mathbf{x}_i^v$}$_{v=1}^V$ can be calculated as:\n$\\mathcal{L} = \\sum_{v=1}^{V} \\mathcal{L}_{a c c}\\left(\\alpha_{n}^v\\right) + \\mathcal{L}_{c o n}$.\n(18)\nThe model optimization process and extensive discussion can be found in Supplementary Material."}]}