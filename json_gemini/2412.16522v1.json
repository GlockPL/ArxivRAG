{"title": "Enhancing Contrastive Learning Inspired by the Philosophy of \u201cThe Blind Men and the Elephant\"", "authors": ["Yudong Zhang", "Ruobing Xie", "Jiansheng Chen", "Xingwu Sun", "Zhanhui Kang", "Yu Wang"], "abstract": "Contrastive learning is a prevalent technique in self-supervised vision representation learning, typically generating positive pairs by applying two data augmentations to the same image. Designing effective data augmentation strategies is crucial for the success of contrastive learning. Inspired by the story of the blind men and the elephant, we introduce JointCrop and JointBlur. These methods generate more challenging positive pairs by leveraging the joint distribution of the two augmentation parameters, thereby enabling contrastive learning to acquire more effective feature representations. To the best of our knowledge, this is the first effort to explicitly incorporate the joint distribution of two data augmentation parameters into contrastive learning. As a plug-and-play framework without additional computational overhead, JointCrop and JointBlur enhance the performance of SimCLR, BYOL, MoCo v1, MoCo v2, MoCo v3, SimSiam, and Dino baselines with notable improvements.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) (Caron et al. 2020; Pang et al. 2022) has garnered significant attention in recent years as obtaining large amounts of labeled data is expensive. Contrastive learning, a widely-utilized SSL method, can even outperform supervised learning on tasks such as image classification, object detection, and semantic segmentation (Chen, Xie, and He 2021; Caron et al. 2021).\nContrastive learning (CL) produces self-supervised signals through pretext tasks and utilizes these signals to train the encoder. A common pretext task is instance discrimination (ID) (Wu et al. 2018), which involves a classification problem at the instance level. A pair of positive views is generated by applying two independently distributed data augmentations to a single image, aiming to maximize the similarity of their representations. ID is widely utilized in popular CL methods such as SimCLR (Chen et al. 2020a), MoCo (He et al. 2020; Chen et al. 2020b), BYOL (Grill et al."}, {"title": "2 Related Work", "content": "2.1 Contrastive Learning\nContrastive learning is a self-supervised learning approach pretrained by pretext tasks with unlabeled data. Previous studies have designed challenging augmented samples to supervise the encoder in learning better feature representations (Bachman, Hjelm, and Buchwalter 2019; Misra and Maaten 2020; Wu et al. 2018; Ye et al. 2019). CL has achieved strong performance in the case of learning feature representations without labels, and the pretrained models are easy to transfer to downstream tasks such as classification, object detection, and instance segmentation. Contrastive learning achieves strong performance across many tasks (Liang et al. 2024; Feng and Patras 2023; Chanchani and Huang 2023; Xiao et al. 2024; Sarto et al. 2023; Li et al. 2023; Wu, Zhuang, and Chen 2024; Park et al. 2024).\nCL can be categorized into two types based on the explicit use of negative samples. The contrastive learning methods that utilize both positive and negative samples include SimCLR (Chen et al. 2020a) and MoCo (He et al. 2020). The core idea of these methods is maximizing the similarity between positive pairs, while minimizing the similarity between non-positive pairs. The CL methods using only positive pairs, such as BYOL (Grill et al. 2020) and SimSiam (Chen and He 2021), uses siamese network structures and feeds pairs of positive views into them. Special designs, such as stop-grad (Chen and He 2021), momentum encoder (He et al. 2020), and a predictor, are necessary to prevent model collapse in the absence of negative samples.\nSome studies categorize BYOL and SimSiam as non-contrastive methods. However, to explore the generalizability of our methods, we consider these representational learning approaches as CL methods, as they work by reducing the feature distance between positive pairs."}, {"title": "2.2 Design of Positive Pairs", "content": "Regardless of whether negative samples are used, the design of generating pairs of positive views is critical to CL. A popular method to generate a pair of two positive views is applying two data augmentations on a specific image. Several studies have explored the design of positive pairs. SimCLR (Chen et al. 2020a) examines the effectiveness of different combinations of multiple augmentation method and finds that the most useful augmentation methods are Crop and Color. Some studies (Jiang et al. 2020; Kim, Tack, and Hwang 2020; Ho and Nvasconcelos 2020) introduce adversarial attacks and use adversarial examples as positive or negative samples. Several methods have been proposed to craft positive pairs for contrastive learning. For example, ContrastiveCrop (Peng et al. 2022) leverages model heatmaps to guide the cropping region, thereby reducing the likelihood of excluding objects from the cropped area. Similarly, MultiCrop (Caron et al. 2020) replaces a single high-resolution sample with multiple low-resolution crops, thereby improving contrastive learning performance without a substantial increase in computational cost. InfoMin (Tian et al. 2020) finds the sweet spot of mutual information between views and generates positive pairs. However, most of these methods do not explicitly address the questions of whether the two augmentations should be correlated and how they should be correlated. In this work, we propose JointCrop and JointBlur, which introduce the correlation between the augmentation parameters of positive pairs and consider their joint distribution, leading to more challenging views for CL without incurring additional overhead."}, {"title": "3 Method", "content": "3.1 Preliminaries\nWe briefly review the pipeline of CL. For an input image I, CL generates a pair of positive samples by applying the data augmentation T twice, as shown in Eq. (1), where the cumulative distribution function F(t) is the distribution used to sample the augmentation parameters t. The views $v_{I,1}$ and $v_{I,2}$ form a pair of positive views, while other views from the images other than I are considered negative samples.\n$v_{I,1} = T(I;t_1), v_{I,2} = T(I;t_2)$\n$t_1 \\sim F(t_1), t_2 \\sim F(t_2)$ (1)\nIn CL methods without negative samples, such as BYOL (Grill et al. 2020) and SimSiam (Chen and He 2021), the representations of $v_{I,1}$ and $v_{I,2}$ are expected to be sufficiently similar. While in SimCLR (Chen et al. 2020a) and MoCo (He et al. 2020), which are CL methods with negative samples, the representations of $v_{I,1}$ and $v_{I,2}$ are expected to be sufficiently close, and their representations are expected to be as distant as possible from the other negative views.\n3.2 JointCrop\nTo introduce our JointCrop method, we first review the RandomCrop pipeline. Initially, the area s is randomly selected within a specified range, defined by $s \\sim U[s_{min}, s_{max}]$. Next, the aspect ratio r is also randomly selected. Given the area s and aspect ratio r, we can uniquely determine the width $w = \\sqrt{s \\times r}$ and height $h = \\sqrt{s/r}$. Following this, crop positions i and j are selected using $i \\sim U[0, W - w]$ and $j \\sim U[0, H - h]$, where W and H are the image's width and height, respectively. By repeatedly applying this sampling procedure twice to an image I, we obtain a positive pairs $v_{I,1} = Crop(I; t_1 = (i_1, j_1, h_1, w_1))$ and $v_{I,2} = Crop(I; t_2 = (i_2, j_2, h_2, w_2))$. Based on our previous analysis in Fig. 1, the area ratio $s_r = \\frac{h_2w_2}{h_1w_1} = \\frac{s_2}{s_1}$ between positive pairs can significantly impact contrastive learning performance. To investigate this, we define a quantitative measure of the difficulty of data augmentation, termed Statistical Difficulty Factor (SDF). SDF measures the cosine similarity between all positive pairs generated by a data augmentation method T across the entire dataset D, using an already trained contrastive learning SimSiam model f.\n$SDF(T) = E_{I \\in D}[cos(f(v_{I,1}), f(v_{I,2}))]$ (2)\nWe measured the values of SDF for RandomCrop and fixed area ratios ranging from 1:1 to 1:5. Figure 2 confirms our analysis: positive pairs become more challenging as the area ratio increases, while RandomCrop often results in trivial positive pairs. Therefore, we can use the area ratio as a means to control the difficulty level between positive pairs.\nTo further investigate the distribution of area ratios in the RandomCrop, we aim to find the distribution of $s_r = s_2/s_1$ in RandomCrop. This can be formulated as a mathematical problem: given $s_1 \\sim U[s_{min}, s_{max}]$ and $s_2 \\sim U[s_{min}, s_{max}]$ (with a typical setup in RandomCrop being $s_{min} = 0.2$ and $s_{max} = 1$), we seek the distribution of $s_r = s_2/s_1$, the derivation of which is shown in Appendix H. Since s takes"}, {"title": "3.3 JointBlur", "content": "As analyzed in Figs. 1e and 1f, we aim to develop a data augmentation method, JointBlur, that is more challenging than GaussianBlur. Chen et al. (Chen et al. 2020a) initially found that using GaussianBlur improves the classification accuracy on ImageNet-1K with ResNet-50 trained for 100 epochs. Subsequent contrastive learning methods have adopted these settings, where the image is blurred with a 50% probability using a Gaussian kernel with a standard deviation $\\sigma \\sim U[0.1, 2.0]$, and the kernel size is set to 10% of the image's height and width.\nGaussianBlur acts as a low-pass filter that alters the texture information of an image to varying degrees depending on its standard deviation. Our goal is to create more challenging samples for contrastive learning by distinguishing the blur levels more clearly between the two views.\nSimilar to JointCrop, JointBlur controls the ratio of $\\sigma_1$ and $\\sigma_2$, such that $\\sigma_1, \\sigma_2 \\sim JC(\\beta)$ as shown in Alg. 1 and Fig. 3. In this context, we replace $s_{min}$ and $s_{max}$ in Alg. 1 with $\\sigma_{min}$ and $\\sigma_{max}$, set to default values of 0.1 and 2.0, respectively, which are the lower and upper bounds for the standard deviation of the Gaussian kernel."}, {"title": "3.4 A Unified JointAugmentation Framework", "content": "In this section, we aim to abstract the common concept found in JointCrop and JointBlur into a unified framework called JointAugmentation. This approach will facilitate the application of this idea to other data augmentation methods. In previous studies (He et al. 2020; Chen et al. 2020a; Grill et al. 2020; Chen and He 2021; Caron et al. 2020), positive sample pairs were randomly sampled with data augmentation parameters $t_1$ and $t_2$ independently, meaning $f(t_2 | t_1) = f(t_2)$, where f represents the probability density function. In other words, when generating $v_{I,2}$, previous contrastive learning work did not utilize the known $t_1$ and instead randomly and independently sampled $t_2$, as described in Eq. (1). This approach could result in positive pairs that are not sufficiently challenging.\nAs suggested by Zhu et al., more challenging samples may help CL learn better representations (Zhu et al. 2021). To efficiently generate more challenging samples, JointAugmentation generates positive samples as described in Eq. (3), where G represents the cumulative distribution function.\n$v_{I,1} = T(I;t_1), v_{I,2} = T(I;t_2)$\n$t_1, t_2 \\sim G(t_1, t_2), f(t_2 | t_1) \\neq f(t_2)$ (3)\nThe main difference between previous methods and JointAugmentation lies in whether the known information about $v_{I,1}$ (i.e. $t_1$) is considered when generating $v_{I,2}$ (i.e. sampling $t_2$). In previous methods, the joint distribution of $t_1$ and $t_2$ is assumed to be the product of their marginal distributions, i.e. $f(t_1, t_2) = f(t_1) \\cdot f(t_2)$. In contrast, JointAugmentation does not follow this assumption, as, $f(t_1, t_2) \\neq f(t_1) f(t_2)$. This interdependence in sampling $t_2$ based on $t_1$ is what gives JointAugmentation its name.\nIn JointCrop, t comprises four parameters: i, j, w, and h. We indirectly control the joint distribution G($t_1, t_2$) by managing the area ratio $s_r = \\frac{h_2w_2}{h_1w_1}$. For JointBlur, t represents the GaussianBlur kernel, and we similarly manage the joint distribution G($t_1, t_2$) by controlling the ratio of the GaussianBlur kernel $\\frac{\\sigma_2}{\\sigma_1}$. For other data augmentation methods, we begin with the augmentation parameter t and indirectly control the joint distribution of the two augmentation parameters G($t_1, t_2$) by regulating certain aspects related to difficulty."}, {"title": "4 Experiments", "content": "4.1 Experiment Settings\nDatasets. We conduct experiments on the STL-10 (Coates, Ng, and Lee 2011), Tiny-ImageNet, and ImageNet-1K (Deng et al. 2009). We also evaluate the performance of downstream tasks on PASCAL VOC (Everingham et al. 2010) and COCO (Lin et al. 2014). For specific ablation experiments, we utilize the ImageNet-100, which is created by randomly selecting 100 classes from ImageNet-1K.\nModels. We perform experiments on several recent and popular CL methods, including SimCLR (Chen et al. 2020a), MoCo (He et al. 2020; Chen et al. 2020b; Chen, Xie, and He 2021), SimSiam (Chen and He 2021), BYOL (Grill et al. 2020), and Dino (Caron et al. 2021). For all these methods, we use ResNet as the backbone. The experimental setup remains consistent with the baseline, except for the modifications introduced by our JointCrop and JointBlur methods.\nStrategies for Selecting Hyperparameters \u03b2. As a plug-and-play method, we aim to use a common hyperparameter \u03b2 rather than having different hyperparameter choices for various situations. Therefore, unless specifically mentioned for ablation experiments, we set \u03b2 = 0 for both JointCrop and JointBlur. While carefully adjusting hyperparameters could potentially enhance performance, this is beyond the scope of the current work.\nPretraining on Small Datasets. On small datasets such as STL-10 and Tiny-ImageNet, we use ResNet-18 as the backbone and train for 500 epochs with a batch size of 512 and a cosine annealing learning rate of 0.5.\nPretraining on ImageNet-1K. On ImageNet-1K, we use ResNet-50 as the backbone. For each baseline, we adhered to the experimental setup as described in the original papers.\nLinear Evaluation. We evaluate the performance of the pretrained model using linear evaluation. Specifically, we assess the top-1 accuracy of a linear classifier on the validation sets to gauge the model's performance. For linear classification on smaller datasets, such as STL-10 and Tiny-ImageNet, we use a small initial learning rate of 10. For linear classification on ImageNet-1K, we adhere to the evaluation settings outlined in the original papers."}, {"title": "4.2 Results of JointCrop and JointBlur", "content": "Results on ImageNet-1K. We applied JointBlur and JointCrop separately to the data augmentation process for ImageNet, and the results are presented in Tab. 1. The consistent and non-trivial improvements observed demonstrate the effectiveness of our JointCrop and JointBlur methods. We also provide results for MoCo v3 pre-trained for 300 epochs. Even under long-term training, our JointCrop continues to show significant improvements.\nResults on Small Datasets. The results of the baselines and JointCrop on small datasets are shown in Tab. 2. The results show that our proposed JointCrop can consistently improve the performance on small datasets. Due to the low resolution of images in smaller datasets, their original data augmentations do not include GaussianBlur. Consequently, using JointBlur on these small datasets is not suitable."}, {"title": "4.3 Results of Downstream Tasks", "content": "Object Detection on PASCAL VOC. Our experimental settings are the same as MoCo v1, that is, the detector is Faster R-CNN (Ren et al. 2015) with a backbone of R50-C4 (He et al. 2017) with 200 epochs of pre-training. We fine-tune the pretrained model with all layers end-to-end for 24K iterations using the detectron2 codebase (Wu et al. 2019) on the trainval2007+2012 split and evaluate on test2007. Our method achieves improvements of +0.8AP and +0.2AP over MoCo v1 and MoCo v2 baselines, as shown in Tab. 3.\nObject Detection and Instance Segmentation on COCO. We use Mask R-CNN (He et al. 2017) with a R50-C4 backbone for object detection and instance segmentation on COCO. We fine-tune all layers end-to-end for 90K iterations, that is, 1x schedule on train2017 and evaluate on val2017. As a result, our proposed JointCrop achieves improvements of +0.4AP and +0.2AP compared with MoCo v1 and MoCo v2 baselines, as shown in Tab. 4."}, {"title": "4.4 Ablation Studies of Hyper-Parameter", "content": "Our proposed JointCrop method controls the statistical difficulty of positive pairs by adjusting \u03b2 in J-Crop(\u03b2). We investigate the influence of \u03b2 in J-Crop(\u03b2) on Tiny-ImageNet, as shown in Tab. 5. A smaller \u03b2 results in more challenging samples. Table 5 reveals an optimal point, \u03b2op. Values of \u03b2 > \u03b2op lead to oversimplified samples, while values less than \u03b2 < \u03b2op produce samples that are too challenging, hindering the learning of better representations. As a plug-and-play framework, JointCrop is designed to be insensitive to hyperparameters and should not require a complex hyperparameter selection strategy. Across a broad range of \u03b2\u2208 [\u22122,2], JointCrop consistently enhances the baseline. The best results, corresponding to specific \u03b2 values, are highlighted in bold. Our generic hyperparameter, \u03b2 = 0, is marked with yellow cells."}, {"title": "4.5 Generalization to Other Datasets and Augmentation Methods", "content": "Pre-training on Non-Object-Centered and Multi-Object Datasets. Our pre-training primarily utilizes the ImageNet-1K dataset, which is characterized by being single-object and object-centered. However, existing pre-training approaches often use images sourced from the web, which do not necessarily share these properties. This raises the question of whether our approach can be effectively applied to a broader range of contrastive learning pre-training scenarios that involve non-object-centered and multi-object images. To explore this, we utilized the COCO dataset, which is non-object-centered and contains multiple objects, for pre-training. We then fine-tuned the model on ImageNet-100. As shown in Table 6, even when pre-training on non-object-centered and multi-object datasets, our JointCrop method demonstrates a significant improvement over the baseline.\nGeneralizing JointAugmentation to other data augmentations. In Sec. 3.4, we abstract the ideas of JointCrop and JointBlur into a unified framework, JointAugmentation, which we attempt to generalize to other popular data augmentation approaches. ColorJitter performs random distortion of the image's color, with brightness b and contrast c being two key parameters. For example, brightness is sampled as $b \\sim U[1 \u2013 b_f, 1 + b_f]$, where $b_f \\in [0,1]$ and defaults to $b_f = 0.4$. The baseline ColorJitter independently performs the same operation twice to obtain $b_1, c_1$ and $b_2, c_2$. Similar to JointCrop, JointColor controls the joint distribution of $b_1$ and $b_2$ ($c_1$ and $c_2$) through the ratios $b_1/b_2$ ($c_1/c_2$). The non-trivial improvements presented in Tab. 7 further demonstrate the generalizability of our JointAugmentation framework to other data augmentation methods."}, {"title": "4.6 Potential Combinations With Other Methods", "content": "In-depth analysis of JointCrop and MultiCrop. JointCrop and MultiCrop have different motivations and methods. They can be used in combination leading to better representations, as in Appendix D.\nCombined use of JointCrop and JointBlur. The combination of JointCrop and JointBlur requires more fine-grained considerations, otherwise it may produce overly difficult samples, as in Appendix F.\nCombined use of JointCrop and InfoMin. The combination of JointCrop with InfoMin further improves the InfoMin baseline from 67.4 to 67.81.\nIn-depth analysis of JointCrop and ContrastiveCrop. We provide an in-depth analysis for ContrastiveCrop and our JointCrop in Appendix G, and try to combine the two."}, {"title": "4.7 Computational Complexity Analysis", "content": "We train 5 epochs for the MoCo v1 baseline, JointCrop, and JointBlur on and calculated the average running time. Table 8 illustrates our JointCrop and JointBlur introduce little additional computational complexity. This does not indicate our method is faster, since the time differences are minor."}, {"title": "5 Conclusion", "content": "In this work, we propose JointCrop and JointBlur, which explore the correlation between two augmentations of positive pairs and generates more challenging positive pairs by controlling the joint distribution of these augmentation parameters. We also integrated both approaches into a unified framework called JointAugmentation, paving the way for applying this concept to other forms of data enhancement. The effectiveness of our method has been demonstrated across multiple popular contrastive learning methods. We hope our work will inspire further research on data augmentations in contrastive learning. Our limitations and future work are in Appendix C."}, {"title": "A Detailed steps of the JointCrop", "content": "For a clearer understanding of JointCrop, we provide the detailed steps in Alg. 1."}, {"title": "B Code of Our Method", "content": "Different code frameworks are employed for various methods, as illustrated in Tab. 9. The official code is directly cloned, and modifications are made to the data augmentation components of these codes. For instance, in replacing RandomCrop with JointCrop, it is necessary to overload the RandomCrop class 'torchvision.transforms.RandomResizedCrop' and amalgamate the cropping process for a pair of positive samples into a singular class or function to achieve a \u2018joint' operation. Only this segment of the code requires modification, while the remainder of the code is maintained as per the baseline code utilized."}, {"title": "C Limitation and Future Work", "content": "The limitations of this paper and the directions for our future work are summarized as follows.\nC.1 Limitation\n\u2022 Our approach necessitates pre-training from scratch, consuming significant energy, time, and computational resources. Nonetheless, we intend to make the pre-trained weights publicly accessible, enabling others to fine-tune their downstream tasks accordingly.\n\u2022 Instance Discrimination (ID) and Masked Image Modeling (MIM) represent prominent paradigms in self-supervised learning (SSL). Our methodology is exclusively adaptable to ID (contrastive learning) and is not applicable to MIM (generative SSL, such as MAE).\nC.2 Future work\n\u2022 We aim to achieve outcomes from training over extended epochs. However, such an endeavor demands considerable time and computational resources. The most extensive pre-training demonstrated herein is the outcome of 300 epochs of MoCo v3 pre-training.\n\u2022 We plan to investigate the integration of various JointAugmentation techniques to generate challenging pairs of positive samples, thereby enhancing the model's feature representation capabilities."}, {"title": "D Analysis for JointCrop and MultiCrop", "content": "Multi-Crop enables positive pairs to have both a global view and a local view by manually specifying the area size during cropping and using more than two views. For example, a typical setup includes two global views with an area ranging from 40% to 100% of the original image and four local views with an area between 5% and 40% of the image. The key distinction between Multi-Crop and our JointCrop is that: (1) In Multi-Crop, the data augmentations for positive pairs are independent, whereas in JointCrop, they are not. (2) JointCrop incurs no additional computational cost, whereas Multi-Crop takes approximately 1.35 times longer than the baseline. (3) The underlying concepts of our JointCrop can be extended to other augmentations, such as JointBlur and JointColor, among others. In contrast, Multi-Crop lacks this kind of extensibility. (4) JointCrop can be further integrated with Multi-Crop to enhance its capabilities. We tested the combination of JointCrop and Multi-Crop. Specifically, we applied JointCrop to both the global and local pairs of Dino (Caron et al. 2021) with the same Multi-Crop configuration of 2x160+4x96, and then trained and fine-tuned on ImageNet-1K. As shown in Table 10, our JointCrop can be effectively combined with Multi-Crop to further enhance the performance of contrastive learning."}, {"title": "E Analysis for Distance between Positive Pairs in JointCrop", "content": "In JointCrop, we control $h_1, w_1$, and $h_2, w_2$ by controlling the area ratios $s_1$ and $s_2$. However, Crop requires sample locations $(i_1, j_1)$ and $(i_2, j_2)$ in addition to sample widths and heights. The Euclidean distance $d = \\sqrt{(i_1 - i_2)^2 + (j_1 - j_2)^2}$ of the positional coordinates between a positive pair also affects its difficulty. A pair of positive samples that are close together may have similar features, e.g., the two parts of a dog's body shown in Fig. 6a. This pair is itself very similar and simple for CL. Whereas a pair of positive samples that are farther away may have dissimilar features but have the same semantic information (they are on one image, after all), e.g., the dog's head and dog's leg in Fig. 6b. This pair is more challenging and may help models learn better.\nCan we add control over distance in JointCrop to get more challenging samples? This is certainly possible, but let's first note that JointCrop actually implicitly controls the distance already. In Sec. 3.1 we have shown that $i \\sim U[0, W \u2013 w]$ and $j \\sim U[0, H \u2013 h]$. Compared to RandomCrop, JointCrop directly controls the area ratio $s_2/s_1$ and indirectly changes the distribution of $s_1$. This indirectly changes the distributions of $h_1$ and $w_1$ and then affects the distributions of $i_1$ and $j_1$. Similarly, the distributions of $i_2$ and $j_2$ are indirectly controlled. Therefore, JointCrop will also affect distance d. Direct theoretical derivation of distances is very complex. We repeated the experiment three times, each time taking 100,000 positive pairs from J-Crop(\u03b2) and measuring the distance between them, as illustrated in Fig. 7. A smaller \u03b2 does increase the Euclidean distance between pairs of positive samples."}, {"title": "F Analysis for the Combination of JointCrop and JointBlur", "content": "In this paper we propose JointCrop and JointBlur, and both JointCrop and JointBlur can improve the linear evaluation accuracy over baselines under multiple datasets and on multiple CL methods. However, what if we use them together, e.g., the combination of JointCrop and JointBlur?\nIn fact, these two augmentation methods are in conflict with each other. JointCrop encourages the ratio of the area of a positive pair of samples to be farther away from 1, that is, with a higher probability of obtaining a \"large view\" and a \"small view\". The \"large\" and \"small\" here do not refer to the size of the images, as they are all resized to the same size (default to 224 \u00d7 224). They refer to the area of the views in the original image. While, JointBlur makes the GaussianBlur kernel of a pair of positive samples more different, that is, with a higher probability of obtaining a \u201cfuzzy view\" and a \"clear view\". If we use the two methods at the same time, we may obtain overly challenging samples, e.g., pairs of positive samples with a \"small fuzzy view\" and a \"large clear view\", which are too difficult to learn good feature representations.\nExamples of such a case are given in Figure 8. Figure 8a shows an image from ImageNet-1K, and Figures 8b and 8c show a pair of positive samples obtained using JointCrop and JointBlur. Figure 8b has a larger area in the original image but uses a weaker GaussianBlur, while Figure 8c has a small area in the original image. Figure 8c is already not as high-resolution as Figure 8b when it is resized to the same size. In addition, with a stronger GaussianBlur, Figures 8b and 8c are too difficult to learn a good feature representation. Figures 8d to 8f are also similar examples.\nWhile the simultaneous application of JointCrop and JointBlur might produce samples that are excessively challenging, employing them in sequence can be beneficial. For instance, one can apply either JointCrop or JointBlur with a certain probability when generating positive samples. Our experiments, conducted using MoCo v1, demonstrate that employing J-Crop(0) or J-Blur(0) individually enhances the baseline accuracy from 57.25% to 60.87% and 60.58%, respectively. Furthermore, applying either of these methods with a probability of 0.5 yields an improved result of 61.24%."}, {"title": "G Combine JointCrop and ContrastiveCrop", "content": "Our JointCrop method explicitly manages the area ratio of two cropping regions $\\frac{h_2w_2}{h_1w_1}$ and implicitly influences the distance d between positive pairs, in contrast to ContrastiveCrop (Peng et al. 2022), which directly controls the cropped regions $h_1, w_1$ and $h_2, w_2$. ContrastiveCrop is bifurcated into two segments.\n\u2022 Semantic-aware Localization restricts the cropping area through a heatmap to preclude object absence, thereby increasing the likelihood of samples appearing proximal to, particularly at the center of, the heatmap.\n\u2022 Center-suppressed Sampling enforces a \u03b2-distribution for coordinates $i_1, j_1$ and $i_2, j_2$ to diversify positive pairs that are overly analogous.\nThe integration of JointCrop and Center-suppressed Sampling might yield superior outcomes, as both methods engender more challenging views. However, Semantic-aware Localization by constricting the cropping area, offers less challenging views. The results of MoCo v1 on ImageNet-1K are presented in Tab. 11. Employing our JointCrop independently surpasses the MoCo v1 baseline and ContrastiveCrop, while the amalgamation of our JointCrop with Center-suppressed Sampling (a component of ContrastiveCrop) yields enhanced results (as indicated by bolded number in Tab. 11). Furthermore, JointCrop is also amenable to integration with additional techniques."}, {"title": "H Derivation of Probability Density Functions for Area Ratios in RandomCrop", "content": "Assume that $s_1, s_2 \\sim U[s_{min}, s_{max}]$, where U denotes a uniform distribution. Consequently, the probability density functions g(y) for $s_1$ and $s_2$ are identical.\n$g(y) = \\frac{1}{s_{max} - s_{min}}$ (4)"}]}