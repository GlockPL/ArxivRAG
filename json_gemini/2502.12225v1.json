{"title": "SUBJECTIVE LOGIC ENCODINGS", "authors": ["Jake Vasilakes"], "abstract": "Many existing approaches for learning from labeled data assume the existence of\ngold-standard labels. According to these approaches, inter-annotator disagree-\nment is seen as noise to be removed, either through refinement of annotation\nguidelines, label adjudication, or label filtering. However, annotator disagree-\nment can rarely be totally eradicated, especially on more subjective tasks such as\nsentiment analysis or hate speech detection where disagreement is natural. There-\nfore, a new approach to learning from labeled data, called data perspectivism,\nseeks to leverage inter-annotator disagreement to learn models that stay true to\nthe inherent uncertainty of the task by treating annotations as opinions of the an-\nnotators, rather than gold-standard facts. Despite this conceptual grounding, ex-\nisting methods under data perspectivism are limited to using disagreement as the\nsole source of annotation uncertainty. To expand the possibilities of data perspec-\ntivism, we introduce Subjective Logic Encodings (SLEs), a flexible framework\nfor constructing classification targets that explicitly encodes annotations as opin-\nions of the annotators. Based on Subjective Logic Theory, SLEs encode labels\nas Dirichlet distributions and provide principled methods for encoding and aggre-\ngating various types of annotation uncertainty -annotator confidence, reliability,\nand disagreement- into the targets. We show that SLEs are a generalization of\nother types of label encodings as well as how to estimate models to predict SLEs\nusing a distribution matching objective. We make our code publicly available at\nhttps://github.com/jvasilakes/SLEncodings.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning is inherently biased. Hovy & Prabhumoye (2021) identify five sources of bias:\ndata selection, annotation, input representations, model, and experimental design. While all five\nhave been studied in detail in previous works, a new view on annotation bias called data perspec-\ntivism (Basile et al., 2021) has gained traction. Data perspectivism re-frames annotations as opinions\nof the annotators rather than \"gold-standard\" facts. It also stands in contrast to existing methods for\nlearning from noisy labels, which attempt to eradicate noise rather than embrace it (Song et al.,\n2022). This movement is inspired by the myth that there ought to be a single true label and that\nannotator disagreement is to be avoided (Aroyo & Welty, 2015). Data perspectivism thus aims to\nembrace and leverage annotation uncertainty to build ML systems that are more representative of\nthe event being modeled."}, {"title": "1.1 CONTRIBUTIONS", "content": "Despite this conceptual grounding, existing methods under data perspectivism focus on utilizing\nonly one of many types of annotation uncertainty \u2014annotator disagreement- resulting incomplete\nopinion representations. To expand on this, we propose Subjective Logic Encodings (SLE), a flexi-\nble framework for constructing target distributions for classification tasks which explicitly encodes\nannotations as subjective opinions of the annotators. SLEs are based on Subjective Logic Theory\n(J\u00f8sang, 2016), which provides principled methods for encoding and aggregating various types of\nannotation uncertainty -annotator disagreement, as well as reliability and subjective uncertainty-"}, {"title": "2 SOURCES OF UNCERTAINTY IN ANNOTATION", "content": "We are interested in modeling aleatoric uncertainty, also called data uncertainty, which refers to the\nirreducible uncertainty inherent in the data or events being modeled. It stands in contrast to epistemic\nuncertainty -due to a lack of knowledge regarding the events- and distributional uncertainty\n-uncertainty regarding out-of-distribution (OOD) events. Generally, aleatoric uncertainty arises\nfrom noise inherent in the event being modeled (e.g., a dice roll) or from measurement noise (Gal,\n2016). More specifically, we view annotation as a measurement process and break it down into three\nindicators of uncertainty: inter-annotator disagreement indicates inherent event uncertainty, while\nannotator reliability and confidence indicate measurement uncertainty. We discuss each of these\nindicators below."}, {"title": "Annotator Reliability:", "content": "An annotator may be highly confident in their annotations but still often\nprovide incorrect or inconsistent labels due to a lack of expertise, the difficulty of the annotation\ntask, etc. Conversely, adversarial annotators may purposefully provide poor annotations. Annotator\nreliability is thus an indicator of measurement noise. Simple measures of reliability are average\nagreement between annotators (Tratz & Hovy, 2010), average Cohen's K between pairs of annotators\n(Hovy et al., 2013), and the G-index (Gwet, 2008). Previous works have also learned models of\nannotator reliability directly from data (Hovy et al., 2013; Jagabathula et al., 2017; Li et al., 2019)."}, {"title": "Annotator Confidence:", "content": "Borderline examples, difficult annotation tasks, or unclear annotation\nguidelines can all influence an annotator's uncertainty in their assigned labels. Thus individual\nannotator confidence is an indicator of measurement noise. Previous works have measured confi-\ndence by asking annotators additional questions (Nguyen et al., 2014) or eliciting probabilistic labels\n(Collins et al., 2022)."}, {"title": "Inter-Annotator Disagreement:", "content": "Even assuming perfect annotator reliability and confidence,\nmultiple annotators may still assign different labels to the same event, indicating an inherent uncer-\ntainty in the even (Plank et al., 2014). Inter-annotator disagreement is thus an indicator of aleatoric\nuncertainty. There has been a recent surge of interest in leveraging disagreements between annota-\ntors for improving classification (see Uma et al. (2021) for a survey)."}, {"title": "", "content": "It is commonly assumed that crowd annotations are samples from the same categorical distribution\nover labels for a given example (e.g., this assumption is explicitly stated in Baan et al. (2022)). The\nexistence of annotator confidence and reliability, however, suggest that such samples pass through a\nintermediate \"measurement\u201d process, i.e., the subjective opinion of the annotator, which lends un-\ncertainty to each annotation. It is therefore necessary to explicitly encode annotations as opinions\n-i.e., including information regarding annotator confidence and reliability. In a more general sense,\nwe ought to encode both first-order annotation uncertainty (i.e., probability of each label in a cate-\ngorical distribution) as well as second-order uncertainty (i.e., uncertainty regarding the probabilities\nthemselves). The next section discusses how to do this using Subjective Logic."}, {"title": "3 BACKGROUND: SUBJECTIVE LOGIC THEORY", "content": "The above shows that it is natural to view annotations as subjective opinions of the annotators.\nHowever, the second-order uncertainty we aim to encode is incompatible with the current status\nquo of vector representations, which are limited to first order uncertainty. Subjective Logic (SL) is\na type of probabilistic logic that explicitly encodes events as the opinions of agents, with separate\ndimensions for first- and second-order uncertainty. We here provide a brief overview of the aspects\nof SL necessary to understand SLEs."}, {"title": "3.0.1 OPINION REPRESENTATION", "content": "In SL, a subjective opinion regarding some item i according to an agent m over a domain of K\npossible events (e.g., class labels) is denoted $w_m^{(i)} = (b_m^{(i)}, u_m, a)$, where $b_m^{(i)}$ \u2208 [0,1]$^K$ is a vector\nof beliefs, $u_m$ \u2208 [0,1] represents the uncertainty of the opinion, and a \u2208 [0,1]$^K$ is a vector of\nbase rates or prior probabilities over the event space\u00b9. These parameters are subject to the constraint\n$u_m + \\Sigma_{k=1}^K b_{m,k}^{(i)} = 1$. SL opinions can be reparameterized as Dirichlet distributions, and the\nmapping to Dirichlet parameters \u03b1 is\n\n$a_m^{(i)} = \\frac{2b_m^{(i)}}{u_m^{(i)} + K \\alpha}$\n\nThe expectation of this Dirichlet can be computed from the SL opinion parameters.\n\n$P_m^{(i)} = E[w] = b_m + u_m a$"}, {"title": "3.0.2 \u0421\u043eMBINING OPINIONS", "content": "A key feature of SL opinions is that they may be combined to form consensus opinions using various\noperators. In this work, we utilize the cumulative belief fusion and trust discounting operators.\nCumulative belief fusion, denoted \u2295, combines two opinions regarding a single event \u2014such as\ntwo annotators m and q observing a single example- treating each as evidence of the true label\ndistribution. It is defined as\n\n$W_{[m q]}=W_{m} \\oplus W_{q}=\\left\\{\\begin{array}{l}b_{[m q]}=\\frac{b_{m} u_{q}+b_{q} u_{m}}{u_{m}+u_{q}-u_{m} u_{q}}\\\\u_{[m q]}=\\frac{u_{m} u_{q}}{u_{m}+u_{q}-u_{m} u_{q}}\\\\a_{[m q]}=\\frac{a_{m} u_{q}+a_{q} u_{m}-\\left(a_{m}+a_{q}\\right) u_{m} u_{q}}{u_{m}+u_{q}-2 u_{m} u_{q}}\\end{array}\\right.$\n\nCumulative fusion reduces the uncertainty u by combining evidence. This means that as more\nopinions are fused, the uncertainty tends towards zero and the resulting Dirichlet approaches zero\nvariance, equivalent to a categorical probability.\n\nThe trust discounting operator increases the uncertainty of an opinion according to a separate opinion\nof the reliability of that annotator. It is denoted \u2297 and defined as\n\n$\\begin{aligned}W_{R}^{(m)}=P_{R}^{(m)} \\otimes W_{m} \\\\b_{R}^{(i)} & =P_{R}^{(m)} b_{m}^{(i)} \\\\u_{R}^{(i)} & =1-P_{R}^{(m)} \\sum_{k=1}^{K} b_{m, k}^{(i)} \\\\\\hat{a}_{m} & =a_{m}\\end{aligned}$\nwhere $W_R^{(m)}$ is an opinion of the reliability of the subjective opinion $w_m$."}, {"title": "4 CONSTRUCTING SLES", "content": "We can utilize cumulative fusion and trust discounting to encode and aggregate labels as SLEs. Let\nthere be a dataset of N annotated examples D = {$x^{(i)},y^{(i)}$}$_{1}^{N}$ \u2208 (X, Y)$^N$, where X \u2208 $\\mathbb{R}^d$ is the\ninput space and Y \u2208 {1, ..., K}$^M$ is the label space over K labels and M annotators. That is, each\ny(i) is a vector of class labels from all M annotators2. Further, each individual judgment $y_m^{(i)}$ has\nmetadata regarding the annotator's reliability $r_m$ and subjective uncertainty $u_m^{(i)}$ 3. Given this, our\ngoal for constructing the target distributions is twofold:"}, {"title": "", "content": "1. Encode each individual judgement taking into account each of the sources of uncertainty\ndescribed in Section 2.\n2. Define a method for aggregating individual encoded judgments into a target distribution for\ntraining a classification model that accounts for label disagreement."}, {"title": "", "content": "As discussed in Section 3, the opinion of annotator m regarding the $i^{th}$ example is $w_m^{(i)}$\n=$(b_m^{(i)}, u_m, a_m)$. In practice, $u_m$ can be any mapping from a user-supplied indicator of subjective\nuncertainty to the range [0, 1]. The belief vector $b_m^{(i)}$ is computed using the subjective uncertainty as\n\n$b_{m}^{(i)}=\\tilde{y}_{m}^{(i)}-\\tilde{y}_{m}^{(i)} \\cdot u_{m}^{(i)}$\nwhere $\\tilde{y}_{m}^{(i)}$ is the vector encoding of $y_m^{(i)}$, such as one-hot or probabilities4.\n\nFinally, we compute the SLE as the aggregate of the opinions for a given example x using the\ncumulative fusion Eq. (3) and trust discounting operators Eq. (6)\n\n$\\bigotimes_{m=1}^{M} W_{R}^{(m)} \\otimes w_{m}^{(i)}$\n\nIn the case where there is no information regarding reliability ($r_m^{(i)}$ = 1) or uncertainty ($u_m$ = 0),\nthis process results in a dogmatic opinion $\\omega^{(i)}$ = ($(b^{(i)}$, 0, 1/K) which is equivalent to a categorical\nprobability. In the further case where there is no disagreement between annotators, the resulting\nopinion is equivalent to a one-hot encoded target."}, {"title": "5 SYNTHETIC DATA EXPERIMENTS", "content": "To our knowledge, there are no real-world datasets that contain all the sources of uncertainty dis-\ncussed in this work. We therefore illustrate the benefit of using SLEs in place of other methods\nfor label aggregation using synthetic data. We generate synthetic datasets to assess the ability of\nSLEs to recover the true label from a set of crowd annotations that have been corrupted according\nto a range of annotator reliabilities and confidences. We conduct three experiments: (1) assuming\nperfect certainty, we vary total annotator reliability from very low to perfect; (2) assuming high re-\nliability, we vary annotator confidence from very low to perfect; (3) as for (2), but we assume low\nannotator reliability."}, {"title": "5.1 DATA GENERATION", "content": "We generate a set of N 5-class true labels as evenly spaced points on the simplex. For each true\nlabel, we generate crowd annotations from M = 10 annotators with varying degrees of confidence\nand reliability. Specifically, we draw values of these parameters from Beta distributions and use\nthem to corrupt the true labels: a true label is permuted with probability equal to reliability and\nit is recalibrated according to confidence. This process is detailed in Algorithm 1 below, where\npermute(y, r) shuffles the indices of y with probability r and recalibrate(y, c) = $\\frac{\\exp (\\ln y i c)}{\\sum i=1 \\exp (\\ln y i c)}$\nsmooths the probabilities in y according to the confidence c. This process is an identity when c = 1\nand r = 1 and pushes y towards the uniform distribution as c \u2192 0. Annotator certainty and\nreliability can thus be varied by specifying different \u03b1 and \u03b2 parameters to the beta distributions.\n\nUsing this process we generate synthetic datasets in three different scenarios according to different\nvalues of the \u03b1 and \u03b2 parameters. In the first, we assume no information regarding annotator con-\nfidence (c = 1) and only vary the reliability of the annotators (Fig. 1a). In the second, we assume\nhigh reliability and vary annotator confidence (Fig. 1b). In the third, we assume low reliability and\nvary annotator confidence (Fig. 1c). We report all \u03b1 and \u03b2 parameters used in Appendix B."}, {"title": "5.2 EVALUATION", "content": "The evaluation uses both hard and soft evaluation metrics between the aggregated crowd annotations\nand the true labels. For the hard metric we report the micro-averaged F1 on the test set, and label\npredictions are obtained from SLEs by taking the argmax of the mode of the predicted Dirichlet,\ni.e., \u0177 = argmaxk $Eq_o$ ($y_k$|x). Where the F1 measures the ability of the model to make correct\npredictions, the soft evaluation metrics measure how well the model is able to match the distribution\nof the crowd annotations. Specifically, we use the Jensen-Shannon Divergence (JSD) (Lin, 1991)\nand the Normalized Entropy Similarity (NES) (Uma et al., 2021) between the aggregated and true\ndistributions.\n\nFor each experiment, we plot the F1, JSD, and NES curves over the range of uncertainty values. We\ncompare SLEs to two baseline aggregation methods: Majority Voting (MV) chooses the class with\nthe greatest frequency in the crowd annotations, resulting in a one-hot label; Soft Voting (Soft) com-\nputes the average count of each label from the crowd annotations, resulting in a vector of continuous\nlabels. We evaluate each aggregation method using all crowd annotations as well as a subset filtered\naccording to reliability as in Tratz & Hovy (2010). For all experiments, we average the results over\n10 runs with crowd-annotations generated according to different random seeds."}, {"title": "5.3 RESULTS", "content": "The plots in Fig. 1 show that SLEs are better able to recover the true distribution from crowd annota-\ntions than the baselines, measured by JSD and NES. Additionally, the ability of SLEs to \"deweight\"\nannotations according to uncertainty means that they achieve higher F1s than the unfiltered base-\nlines. Filtering annotations according to an estimate of annotator reliability improves results for all\nmethods, but we note that the difference in JSD and NES between unfiltered and filtered SLEs is\nmuch less than MV and Soft, suggesting the SLEs are better able to capture the true distribution\neven in the presence of noisy labels."}, {"title": "6 LEARNING SLES", "content": "The sources of aleatoric uncertainty described in Section 2 have been independently studied in pre-\nvious works. However, the methods used for incorporating them into classification models are quite\nspecialized and generally incompatible with each other. In this section, we describe in detail how\nSLEs aggregate each of these sources of uncertainty into a single target for model estimation."}, {"title": "6.1 DIRICHLET NEURAL NETWORKS", "content": "Using SLEs in machine learning applications requires a classifier to predict Dirichlet distributions.\nPredicting Dirichlets has been studied extensively in previous work. Malinin & Gales (2018) in-\ntroduce Prior Networks, which use Dirichlet outputs to model distributional uncertainty given out-\nof-distribution data. Concurrently, and most similar to our work, Sensoy et al. (2018) propose a\nDirichlet neural network for modeling prediction uncertainty that is explicitly influenced by SL.\nStill, this work focuses on inducing predictive uncertainty of the model given gold-standard labels\nrather than utilizing aleatoric uncertainty obtained from data. Also, despite being inspired by SL,\ntheir model predicts the \u03b1 parameters of a Dirichlet instead of the corresponding SL opinion pa-"}, {"title": "6.2 MODEL ESTIMATION", "content": "As described in Section 4, we assume a dataset with aggregated SLE target distributions D\n= {$x^{(i)}, \u03c9^{(i)}$}$_{1}^{N}$ \u2208 (\u03a7, \u03a9). We aim to estimate a classification model $F_\u03b8$: X \u2192 Y parameterized\nby \u03b8 which maps from the input space X to the space of SL opinions \u03a9 represented by reparam-\neterized Dirichlet distributions. The process of estimating such a classifier amounts to distribution\nmatching between the target $\u03c9^{(i)}$ and approximating SLE opinions $\\omega^{(i)}$, which are predicted by a\nclassifier $f_\u03b8$ \u2208 $F_\u0398$. In our setup, a neural network $f_\u03b8$ predicts the b and u parameters of the opinion\nrepresentation.\n\n$b_{\u03b8}^{(i)}, u_{\u03b8} \\text { softmax }(f_{\u03b8}(x^{(i)}))$\nwhere softmax enforces the constraint $u_{\u03b8}^{(i)} + \\sum_{k=1}^{K} b_{\u03b8,k}^{(i)} = 1$.\n\nIn the usual classification setup where the targets are gold-standard labels, model estimation often\nuses the cross entropy loss between the one-hot target labels \u1ef9 and the label probabilities predicted\nby the classifier, $Q_\u03b8$.\n\n$L_\u03b8 = \\sum_{k=1}^{K} \\tilde{y}_k \\log Q_\u03b8(\\tilde{y} = k | x)$\n\nHowever, given that we are here concerned with target distributions rather than labels, it may be\nclearer to rewrite the cross-entropy loss according to the KL divergence between the target distribu-\ntion P and predicted distribution Q.\n\n$L_\u03b8 = D_{KL} (P \\| Q_\u03b8) + H(P)$\n\nSince the entropy term in Eq. (9) is a constant that depends only on the target distribution, the\nminimization of the cross-entropy amounts to the minimization of the KL divergence between the\ntarget and the predicted distributions5. Thus our target loss function is simply the KL divergence,\n\n$L_\u03b8 = D_{KL} (P \\| Q_\u03b8) = \\sum P(x) \\log \\frac{P(x)}{Q_\u03b8(x)}$\n\nwhich is essentially the same loss function proposed by Malinin & Gales (2018), without the addi-\ntional KL term for out-of-distribution data.\n\nUnfortunately, there are two issues with this objective: (1) the KL divergence error surface is poorly\nsuited to optimization when the target distributions are sparse \u201cone-hot\" distributions corresponding\nto dogmatic opinions; (2) this \u201cforward\u201d KL divergence penalizes $Q_\u03b8$ much less where P is very\nsmall, which results in a $Q_\u03b8$ that covers a majority of the space and is a poor predictor.\n\nTo overcome (1), we follow Malinin & Gales (2018) and smooth dogmatic target opinions by redis-\ntributing a small e amount of belief mass to the uncertainty parameter, which adds a small amount\nof density to the other corners of the probability simplex (Eq. (11)).\n\n$\\begin{aligned}b_{m}^{(i)} & =\\tilde{b}_{m}^{(i)}-\\epsilon \\\\u_{m}^{(i)} & =u_{m}^{(i)}+\\epsilon \\\\\\hat{a}_{m} & =a_{m}\\end{aligned}$"}, {"title": "", "content": "To overcome (2), we follow Malinin & Gales (2019) and use the reverse KL divergence\n\n$L_\u03b8 = D_{KL} (Q \\| P_\u03b8) = \\sum Q_\u03b8(x) \\log \\frac{Q_\u03b8(x)}{P(x)}$\n\nwhich encourages $Q_\u03b8$ to fit under P."}, {"title": "7 REAL-WORLD DATA EXPERIMENTS", "content": "Despite the lack of real-world datasets that contain all three sources of uncertainty, we can still com-\npare SLEs to existing methods for learning from crowd annotations. We experiment with the fol-\nlowing real-world datasets in computer vision and natural language processing that contain crowd-\nsourced annotations with multiple annotators per example.\n\n\u2022 CIFAR-10S (Collins et al., 2022): Image classification. A variant of the CIFAR-10 com-\nputer vision benchmark dataset (Hinton et al., 2012) annotated by 6 annotators per example.\nAdditionally, CIFAR-10S elicited probabilistic labels from each annotator to capture each\nannotator's confidence.\n\u2022 MRE (Dumitrache et al., 2018a): A dataset of 4,000 sentences in English from PubMed\nabstracts expressing a cause or treats relation between two entities. Each example is anno-\ntated by 15 annotators.\n\u2022 ConvAbuse (Cercas Curry et al., 2021): A collection of 7,000 sentences sampled from\ndifferent chatbot systems annotated by 8 annotators for a variety of abuse types.\n\u2022 RTE (Hovy et al., 2013): 800 sentences pairs annotated for semantic entailment by 10\nannotators."}, {"title": "7.1 MODELS", "content": "We utilize the following models for each real-world dataset.\n\n\u2022 CIFAR-10S: Following previous work (Collins et al.", "MRE": "We use the same model and training procedure as Uma et al. (2021). This is a\nf"}]}