{"title": "XSS Adversarial Attacks Based on Deep Reinforcement Learning: A Replication and Extension Study", "authors": ["Samuele Pasini", "Gianluca Maragliano", "Jinhan Kim", "Paolo Tonella"], "abstract": "Cross-site scripting (XSS) poses a significant threat to web application security. While Deep Learning (DL) has shown remarkable success in detecting XSS attacks, it remains vulnerable to adversarial attacks due to the discontinuous nature of its input-output mapping. These adversarial attacks employ mutation-based strategies for different components of XSS attack vectors, allowing adversarial agents to iteratively select mutations to evade detection. Our work replicates a state-of-the-art XSS adversarial attack, highlighting threats to validity in the reference work and extending it towards a more effective evaluation strategy. Moreover, we introduce an XSS Oracle to mitigate these threats. The experimental results show that our approach achieves an escape rate above 96% when the threats to validity of the replicated technique are addressed.", "sections": [{"title": "1 Introduction", "content": "The proliferation of web applications has brought significant advancements but also introduced new security challenges. Among the various web-based attacks [1,2], Cross-site scripting (XSS) [3] stands out as one of the most critical concerns. XSS attacks pose a significant threat as they can compromise user data, steal information, and spread worms. Malicious actors exploit vulnerabilities in web applications to inject harmful scripts, which are then unknowingly executed by users' browsers. To mitigate XSS attacks, robust detection methods and strong input validation techniques are essential to safeguard user data and system integrity.\nResearchers have focused on the XSS vulnerability discovery, employing either static or dynamic analysis. Static analysis methods scrutinize the source code to identify potential attacks [4-7], but their application might not scale to the size of modern web applications or might result in overconservative results, with several false positives, due to the presence of programming constructs that are difficult to handle statically. Dynamic analysis, on the other hand, simulates user operations to detect attacks [8-10]. However, this approach suffers from a high false negative rate as test cases cannot cover all possible scenarios.\nTo address these limitations, researchers have proposed methods to detect the injection of XSS scripts at runtime, complementing XSS vulnerability discovery before release. In the early approaches, machine learning techniques with manual feature extraction were extensively used [11-15], followed by the advent of Deep Learning (DL) and the use of Deep Neural Networks (DNNs) for XSS detection [16-18].\nWhile DNNs have shown great promise, they are vulnerable to adversarial attacks [19], where slight changes to input data can deceive the model. These attacks have successfully compromised DNNs used for XSS attack detection as well. A reference paper by Chen et al. [20] proposed a Reinforcement Learning (RL) strategy to generate XSS adversarial examples and attack state-of-the-art (SOTA) XSS attack detectors based on DNNs. Their approach involves preprocessing, tokenization, and word vector representation using the Word2Vec model [21]. The authors achieved almost perfect detection results (over 99% accuracy) and an impressive Escape Rate (ER)\u00b9 of more than 90% against all DNN-based detectors.\nHowever, we identified several threats to validity in the work by Chen et al. [20]. The first threat to validity is that the application of a sequence of actions could deteriorate the characteristics of the XSS script, and the authors did not apply any strategy to evaluate if the applied sequence of mutations is semantically preserving. The second threat is that the preprocessing pipeline of the detectors does not consider any potentially adversarial example, such that a mutation may potentially result in an out-of-vocabulary token (OOV) that is replaced by 'None' in the word vector representation. As a consequence,"}, {"title": "2.1 Cross-Site Scripting (XSS)", "content": "Cross-Site Scripting (XSS) consists of the injection of malicious code into a web page. When a user visits the page, their browser unknowingly executes this script, leading to critical security breaches. It has been recognized as one of the prevalent threats, evidenced by the Open Web Application Security Project (OWASP),\u00b2 a renowned authority on web application security, that has consistently ranked XSS as one of the top ten web application security risks. These"}, {"title": "2.1.1 Defending Against XSS Attacks", "content": "Given the diverse and harmful nature of XSS attacks, researchers have devoted efforts to developing effective defence strategies. The primary research focus has been on two key areas: XSS vulnerability discovery and XSS attack detection.\nXSS vulnerability discovery: This encompasses both static and dynamic analysis techniques. Static analysis searches along all the possible execution paths in the code to find potential attacks. Several approaches have been proposed in this category. Doupe et al. [4] suggested a server-side XSS mitigation strategy that isolates code from data, but this method falls short of dynamic JavaScript attacks. Steinhauser et al. [5] developed JSPChecker, a tool employing data flow analysis and string parsing to detect vulnerabilities in sanitization sequences. Mohammadi et al. [6] utilized automated unit testing to identify vulnerabilities arising from improper input data handling. Kronjee et al. [7] applied machine learning with a 79% precision rate to detect XSS and SQL injection vulnerabilities through static code analysis. Dynamic analysis, on the other hand, involves monitoring the data flow to pinpoint injection points and then testing for actual vulnerabilities. Lekies et al. [8] introduced a technique to detect DOM-based XSS by monitoring and exploiting vulnerabilities in sensitive calls. Fazzini et al. [10] automatically implemented Content Security Policies (CSP) in web applications, to track and manage the dynamic content.\nXSS attack detection: While vulnerability discovery is essential, it may not offer complete protection against XSS attacks. Hence, researchers have also developed methods to identify malicious user input at runtime. This task is challenging due to the obfuscation techniques employed by attackers. Consequently, many detection methods rely on ML and DL approaches. Likarish et al. [11] used JavaScript features for detection, achieving 92% accuracy. Nunan et al. [12] refined this approach, improving detection. Mereani et al. [15] extracted structural and behavioral features, reaching 99% accuracy. Fang et"}, {"title": "2.1.2 Adversarial Attacks on XSS Detectors", "content": "The recent emergence of DL has led to groundbreaking advancements in various fields, including XSS attack detection, where it has achieved SOTA performance. However, researchers have identified a critical issue: the susceptibility of these methods to adversarial attacks. These attacks have successfully evaded multiple DL models across different domains, underscoring the imperative to enhance the robustness of these models. In the context of XSS attack detectors, several studies have explored adversarial attacks. Fang et al. [16] developed an XSS adversarial approach utilizing the Dueling Deep Q Networks algorithm, but its escape detection success rate remained below 10% due to a simplistic bypass strategy. Zhang et al. [22] proposed an algorithm based on Monte Carlo Tree Search (MCTS) to generate XSS adversarial examples for training the detection model. However, this algorithm relied on limited escape strategies and exhibited high time complexity. Wang et al. [23] introduced a method employing soft Q-learning, dividing the bypass process into HTML and JS stages, achieving an impressive 85% escape rate.\nThe reference work by Chen et al. [20] stands out with its Deep Reinforcement Learning algorithm, leveraging a set of mutation rules as actions, resulting in near-perfect escape rates against various SOTA XSS attack detectors. This approach will be thoroughly analyzed in Section 3."}, {"title": "2.2 Reinforcement Learning", "content": "Reinforcement Learning (RL) is a distinctive machine learning paradigm that aims to maximize long-term rewards by striking a balance between exploration and exploitation. Unlike supervised learning, RL does not rely on labeled input-output pairs. Instead, it models the learning process as the interaction between two key components: the agent and the environment. The environment is represented as a timed sequence of states, $S = (s_0, s_1, ...)$. At any given time t, the agent observes a state $s_t$ and selects an action $a_t$ from the available action space $A = {a_0, a_1,...}$ according to a policy $\\pi(a_t | s_t)$, which is either the same being learned during the agent's interactions with the environment (on-policy learning) or which is kept separate from the policy under training (off-policy learning). The chosen action triggers a state change, and the new state $s_{t+1}$ is determined by a Markov decision process with probability transition matrix $P(S_{t+1} | S_t, a_t)$. Simultaneously, the agent receives a reward $r_{t+1}$."}, {"title": "3 Reference Work", "content": "In this section, we introduce the reference work [20] as follows. We begin with an overview of the proposed method. Then, we delve into their experiments with an analysis of their results. Lastly, we describe the potential threats to validity we identified, which prompted this replication and extension study."}, {"title": "3.1 Proposed Method", "content": "As depicted in Figure 1, the authors of the reference work introduce a two-stage method, encompassing detection and escape phases. The detection phase involves utilizing an XSS detector, where the input undergoes preprocessing before being fed into the detector. Preprocessing comprises several steps: Positive examples, containing XSS attacks, are de-obfuscated and converted to lowercase. The URL is standardized to 'http://', and special characters such as angular brackets in '<br>' are removed. Tokenization is then applied to the examples using the rules outlined in Table 1.\nTokenization converts each input example into a sequence of tokens. The 10% most frequent tokens are chosen for the vocabulary, while the remaining tokens are replaced with 'None'. Subsequently, a Word2Vec model is trained,"}, {"title": "3.2 Experiments", "content": "The training set for XSS detection models contains around 90,000 examples, collected from XSSed [28] and Alexa [29]. This dataset is not publicly available and we had no way to craft it. To solve this problem, we used a publicly available dataset, as discussed in Section 4. The authors of the reference work used the same dataset as [23] to train the adversarial model. They trained MLP, LSTM, and CNN as detectors, and they considered also two commercial XSS detection systems, named Safedog [30] and XSSChop [31].\nSeveral metrics were employed to assess the performance of the detectors: True Positive (TP) represents a correctly identified XSS example, False Positive (FP) indicates a benign example wrongly classified as malicious, True Negative (TN) denotes a correctly identified benign example, and False Negative (FN) represents an XSS example wrongly classified as benign. Then, some derived metrics are defined as follows: Accuracy measures the proportion of correctly predicted examples (both malicious and benign) among the total. Precision calculates the ratio of correctly predicted malicious examples to all predicted malicious examples. Recall determines the ratio of correctly predicted malicious examples to all actual malicious examples. F1-Score is the geometric mean of Precision and Recall, aiming for high values of both.\nWhen evaluating a adversarial attack, the authors focused on detection and escape rates. Detection Rate (DR) represents the ratio of malicious examples still detected by the XSS detection model, indicating the model's ability to defend against adversarial examples:\n$DR= \\frac{Number\\ of \\ malicious\\ examples\\ detected}{Total \\ number\\ of \\ adversarial \\ examples}$ \nEscape Rate (ER) refers to the percentage of malicious examples that go undetected and are recognized as benign by the detector:\n$ER= \\frac{Number\\ of \\ malicious\\ examples\\ undetected}{Total \\ number\\ of \\ adversarial \\ examples}$"}, {"title": "3.4 Identified Threats to Validity", "content": "The first threat to validity stems from the lack of validation of adversarial examples and their properties. The authors did not employ any strategy to ensure that the applied transformations preserve the semantic integrity of the examples. This omission raises concerns about the validity of the modified payloads. The second issue is related to the preprocessing and vocabulary construction. The initial dataset lacks tokens produced by the proposed payload transformations, or in some cases such tokens are rare in the dataset, meaning that new tokens resulting from the RL agent's actions are likely to fall outside the top 10% of considered tokens. Consequently, these tokens will be replaced"}, {"title": "4 Methodology", "content": "The main idea of this paper is to introduce an XSS Oracle. As a first step, we demonstrate the Oracle's usefulness in assessing the validity of payloads and their potential impact. Furthermore, the Oracle can aid in developing a robust defense model, which allows an accurate evaluation of the performance of the approach proposed by Chen et al. [20]."}, {"title": "4.1 XSS Oracle", "content": "Based on the presence of an XSS attack inside of the payload of an HTTP request, we can consider two types of payload: 'Benign' and 'Malicious'. Benign payloads do not alter the DOM structure when executed, while malicious payloads cause changes in the DOM, potentially affecting the browser environment. As outlined in Figure 2, we utilize the Oracle to mimic payload execution, observing the DOM of a template page rendered by a web server. The server accepts the payload as a parameter and incorporates its elements into the template. The Oracle then examines the DOM of the new page. If any differences are detected, the payload is labelled as Malicious; otherwise, it is classified as Benign."}, {"title": "4.2 Metrics", "content": "TH1 and TH2 arise from the agent's modifications and preprocessing of the payload, which could alter the characteristics of the XSS attack. To address this, we employ an XSS Oracle that assesses the integrity of the attack properties, introducing the metric Ruin Rate (RR). Let us consider a set of payloads labeled as Malicious, denoted as M = {$m_1, m_2, ...$}. This set M is generic and can include malicious samples from the original dataset or those generated by the XSS adversarial method. How to structure the different sets for the evaluation of the different threats to validity will be discussed in Section 5.\nWe define a function O(p) that, for any payload p, returns 1 if the Oracle classifies p as Malicious and 0 otherwise. For any set M, RR can be calculated as:\n$RR(M) = 1 - \\frac{\\Sigma_{m \\in M} O(m)}{|M|}$\nIf M contains samples from the original dataset, a non-zero RR(M) indicates mislabeled examples. Conversely, if M consists of adversarial examples derived from an original set with RR = 0, a non-zero RR(M) points to an adversarial process that has compromised the attack's properties.\nTH1 and TH2 are also potentially related to an anomalous number of Out-Of-Vocabulary (OOV) tokens in the array fed to the detection model. We introduce a second metric, called OOV-Rate (OR), to evaluate this aspect. For an array of tokens V, OR(V) is the number of the 'None' tokens present"}, {"title": "5 Empirical Study", "content": "This section presents the replication of the experiments in the reference work [20], highlighting the deviations from the results reported in the original paper. We introduce specific research questions for the replication and for the extension study, and we describe the experiments conducted to extend the reference work."}, {"title": "5.1 Replication Study", "content": "In our replication study, we aim to closely follow the methodology of the reference work. However, some differences are worth noting and justifying. The dataset used to train the detectors was not publicly available, so we utilized an alternative dataset.3 This employed dataset is a well-known one [32] containing more than 15,000 Malicious and Benign payloads. The dataset for training the adversarial agent was partially available but had a different structure compared to the one employed in the reference work. The reference work does not adequately describe the correct payload structure, as the examples only consider parameters, while some steps mention filtering applied to the URL, suggesting the payload should be the entire HTTP request. Preliminary experiments revealed that datasets with varying structures encountered out-of-vocabulary issues even before the agent's actions were applied. In particular, when one dataset is used to create the vocabulary and to train a detector, and the other one is simply tested against it, RR is very high even before training an adversarial agent (RR > 60%).\nTo isolate the identified threats to validity and mitigate any data structure-related problems, we divided the selected dataset into two parts: one for training the detectors and the other for training the adversarial agents, excluding Benign examples. This setup makes adversarial attacks more challenging, as the examples are closer to those used for detector training. Consequently, a high RR in this context would indicate the significance of the threats TH1 and TH2, because of the alignment between detector's and adversarial agent's training sets.\nThe dataset was pre-filtered by the Oracle to ensure accurate labelling. After pre-filtering, the most representative class (Benign) was undersampled to ensure class balance. We computed the Ruin Rate of the original payload"}, {"title": "5.2 Research Questions (RQs)", "content": "The first RQ is to assess the feasibility of replicating the study's findings in the context of TH3.\nRQ1. Replication Study: Can we successfully reproduce the outcomes reported in the reference work?\nThe next two RQs focus on evaluating the significance of TH1 and TH2:\nRQ2. Evaluation of TH1: Does the lack of validation of the actions pose a threat to validity?\nRQ3. Evaluation of TH2: Does the lack of validation of the preprocessed payload pose a threat to validity?\nThe final RQ extends this study by re-examining the performance of the method introduced in the reference work after addressing the identified threats to validity.\nRQ4. Extension Study: How does the reference method perform once the identified threats are mitigated?"}, {"title": "5.3 Implementation", "content": "Our experimental framework was implemented using Python 3.11. The DL library used to implement the models is PyTorch 2.2.1. The RL agent used for generating adversarial attacks is implemented in StableBaselines3 2.3.0. The Oracle is implemented with a Web Server using FastAPI 0.104.0 and Jinja2 3.1.2 to render the template. The DOM is analyzed using BeautifulSoup 0.0.2 and zss 1.2.0."}, {"title": "5.4 Oracle Integration and Analysis", "content": "As shown in Figure 3, the Oracle is used in two different stages. The set of undetected malicious payloads generated by the adversarial model, which represent the examples that contribute to the escape rate of the replication study, named E, is directly fed into the Oracle. The set E is then preprocessed as described in the previous sections, obtaining the set of arrays V. Also, V is fed into the Oracle. Thanks to the Oracle, it is possible to evaluate RR(E) and RR(V), which, respectively, represent the answers to RQ2 and RQ3. We do not rely only on the Oracle: the analysis of the Ruin Rate for RQ3 is complemented by the analysis of the Out-Of-Vocabulary Rate, that it is not reported in the Figure 3 for simplicity. Regarding RQ4, there is no guarantee that increasing the vocabulary would solve TH2, since the adversarial agent is potentially able to generate new tokens that are out-of-vocabulary regardless of the vocabulary size. To mitigate this threat-to-validity and to evaluate the real performance of the method, we integrated the Oracle into the training"}, {"title": "6 Results", "content": ""}, {"title": "6.1 RQ1 (Replication Study, Evaluation of TH3)", "content": "In this RQ, we mitigate TH3 by replicating the results of Chen et al. [20]. We trained ten adversarial agents attacking each considered detection model, to deal with the non-determinism of the training process. Table 7 reports the average of the escape rates obtained by the adversarial agents. These ERs are almost perfect, demonstrating a consistency with the reference work, despite variations in the dataset and training algorithm (see Section 5.1). LSTM's ER is 6.58%pt5 higher than the reference work, while the MLP and CNN results are identical and slightly lower (0.99%pt), respectively. This consistency"}, {"title": "6.2 RQ2 (Evaluation of TH1)", "content": "We investigate the impact of the lack of action validation by analyzing the ruin rates of the set E, which contains all the generated payloads that successfully escaped detection. Table 8 presents the average ruin rates (RRs) for each detection model.\nRuin rates are relatively low, ranging from 6.34% to 7.07%, indicating that the sequence of actions occasionally disrupts the semantics of the attack. However, this frequency is not high enough to be considered a significant threat to validity."}, {"title": "6.3 RQ3 (Evaluation of TH2)", "content": "For each adversarial agent, we collected all generated payloads that bypassed the detector, forming the set E. We then preprocessed this set to create the"}, {"title": "6.4 RQ4 (Extension Study)", "content": "In this RQ, the training process for adversarial agents closely mirrors that used in RQ1 (Section 6.1), with a key difference: the Oracle is integrated to calculate a new reward function. The new reward is set to -2 if the mutated payload, after preprocessing, is no longer recognized by the Oracle as an XSS attack. Otherwise, the reward is the same proposed in the reference work.\nTable 10 reports the average escape rates achieved by the adversarial agent. Despite being very high, these rates are slightly lower than those obtained in RQ1 (see Table 7).\nThe second and third columns of Table 11 report the average ruin rates (RR) and out-of-vocabulary (OOV) rates (OR) across the ten adversarial agents for each detection model. The low values of RR and OR indicate that the integration of the Oracle in the training process effectively mitigates TH3. These results demonstrate that it is feasible to train adversarial agents capable of attacking XSS detection models as proposed in the reference work, without introducing any threats to validity related to preprocessing. In the new setup, the adversarial agent learns to produce payloads that include mostly valid tokens, while being still able to circumvent the detection capabilities of the considered detectors."}, {"title": "7 Threats to Validity", "content": "Internal validity. The training process of the adversarial agents is inherently non-deterministic. To ensure reliability of our findings, we repeated the training process for each agent ten times. For transparency and correctness of implementation, we have made our code publicly available, and we utilized well-known open-source frameworks for our implementation.\nExternal validity. While the employed dataset may not be exhaustive in representing every type of XSS attack, it is substantial and publicly accessible. Moreover, it has been widely used in previous research, establishing it as a suitable benchmark for evaluating XSS detection methods.\nConstruct validity. We employed standard evaluation metrics in the security domain, including Precision, Recall, Accuracy, and F1-Score, to assess the detectors. For the adversarial agents, we used the escape rate as an evaluation metric, which aligns with the reference work."}, {"title": "8 Conclusion", "content": "In this paper, we replicated the study proposed by Chen et al. [20] and conducted a thorough analysis of potential threats to its validity. After checking whether such potential threats actually affected the results reported in the original study, we presented an extended approach and introduced an extension study to mitigate them. Our findings are similar to those presented in Chen et al. [20], but with a crucial difference: we eliminated the threats to their validity. This achievement allows us to propose a more effective method that directly attacks the detectors themselves, rather than relying on potential vulnerabilities in the preprocessing pipeline, associated with the generation of out of vocabulary tokens. Furthermore, our approach enhances transparency in the evaluation process, as we make code, datasets and results publicly available to all researchers in the field."}, {"title": "Compliance with Ethical Standards", "content": "The authors declare that they do not have any known relationship or competing interests that could have influenced this paper. The authors declare that their research for the current work did not involve Human Participants or Animals."}, {"title": "Data Availability", "content": "The implementations, source code, data, and experimental results are publicly available in a GitHub repository6."}]}