{"title": "LLM-Cure: LLM-based Competitor User Review Analysis for Feature Enhancement", "authors": ["MARAM ASSI", "SAFWAT HASSAN", "YING ZOU"], "abstract": "The exponential growth of the mobile app market underscores the importance of constant innovation and\nrapid response to user demands. As user satisfaction is paramount to the success of a mobile application\n(app), developers typically rely on user reviews, which represent user feedback that includes ratings and\ncomments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in\nmanual analysis, necessitating automated approaches. Existing automated approaches either analyze only\nthe target app's reviews, neglecting the comparison of similar features to competitors or fail to provide\nsuggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based\nCompetitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs\nto automatically generate suggestions for mobile app feature improvements. More specifically, LLM-Cure\nidentifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a\nuser review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint\nand proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739\nreviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the\nstate-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and\nup to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving\nuser complaints. We verify the suggestions using the release notes that reflect the changes of features in the\ntarget mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided\nsuggestions, demonstrating its potential for competitive feature enhancement.", "sections": [{"title": "1 Introduction", "content": "The mobile application (app) market is experiencing explosive growth, with global downloads\nreaching a staggering 257 billion in 2023 [10]. This surge in app adoption has fostered a highly\ncompetitive environment among apps within the same categories that offer similar functionalities,\ni.e., competitors. For instance, WhatsApp\u00b9, leading the messaging app category with 51 million\nmonthly downloads, competes with at least ten other apps, each having millions of downloads [9].\nA similar competition is evident in the video conferencing category, where Zoom\u00b2 and Skype\u00b3 are\nkey players[14, 40]. During the pandemic, Zoom swiftly adapted to user demands by optimizing its\nplatform for large-scale virtual meetings and enhancing security features, while Skype struggled\nto keep pace [43, 51]. To stay relevant and competitive, developers must rapidly respond to user\nneeds. User reviews contain rich information, such as feedback, dissatisfaction and suggestions\nregarding the user experience of the usage of mobile apps. These reviews offer developers critical\ninsights into areas for improvement and opportunities for feature enhancements [2, 41, 46, 53, 58].\nTo stay competitive, developers need to learn from their competitors' behaviours to maintain a\ncompetitive edge [50]. Competitor user review analysis involves comparing user feedback, ratings,\nand reviews of competing mobile applications to identify strengths and weaknesses relative to\ncompetitors. By analyzing user reviews from competing apps, developers can uncover insights into\nfeatures that address unmet needs, potentially giving their app a significant advantage.\nGiven the sheer volume of reviews [23], it is challenging to analyze user reviews manually.\nPractitioners need an automated feedback analysis process [53]. Hence, researchers propose au-\ntomated approaches to filter informative reviews [11, 22], summarize user reviews [20, 55] and\nextract features from user reviews [13, 29, 30, 49, 55]. While existing research has been conducted\non automated user review analysis, only a limited number of studies focus on competitor user\nreview analysis, where reviews across competing apps are compared [6, 31, 39, 47, 50, 57]. Recent\nadvancements in Large Language Models (LLMs) have demonstrated their capabilities across vari-\nous natural language processing (NLP) tasks, including text generation, translation, summarization,\nand question answering [37]. Although LLMs offer promising applications for mobile app review\nanalysis, current LLM-based research primarily focuses on tasks, such as sentiment analysis [44, 62],\naspect extraction [60], analyzing multilingual reviews [59] and accessibility-related reviews [18].\nResearchers have explored various approaches to conduct competitor user review analysis\n[6, 13, 31, 33, 47, 50]. However, existing work presents some limitations. First, the existing approaches\noften generate an overwhelming number of fine-grained features [13, 47, 50] due to comparing the\napps' features based on word pairs, making it hard to conduct competitor user review analysis with\nthousands of features [48]. Second, competitor user review analysis is only conducted by identifying\nexplicit expressions of comparison (e.g., \"Zoom's screen sharing is way smoother than Skype's\") [31]\nand fails to take into consideration implicit insights derived from user reviews. Third, existing\nwork on competitor user review analysis [6] offers only feature rating comparisons lacking the\nability to suggest concrete improvements for specific features based on competitors' user feedback.\nTo address the limitations of existing work in suggesting feature enhancements using competitor\nuser review analysis, we propose an LLM-based Competitive User Review Analysis for Feature\nEnhancement (LLM-Cure). LLM-Cure automatically generates suggestions for mobile app feature\nimprovements by leveraging user feedback on similar features from competitors. LLM-Cure operates\nthrough two phases. In the first phase, it leverages its large language model capabilities to extract\nand assign features to user reviews. In the second phase, it curates underperforming features among\nthose identified in the first phase for the target app and suggests potential improvements for specific\ncomplaints by leveraging highly rated similar features in competing apps.\nTo evaluate the effectiveness of our proposed approach, we conduct an empirical study on\n1,056,739 reviews of 70 popular mobile apps from the Google Play store belonging to 7 categories."}, {"title": "2 Background", "content": "Large Language Models. Pre-trained Large Language Models (LLMs) are deep neural networks\nthat have undergone extensive training on large text data that have enabled them to learn complex\npatterns and structures of language [63]. In the realm of software engineering, LLMs have gained\nsignificant attention and adoption for various apps [26], including code generation [17], code repair\n[19], and documentation generation [61]. Hence, LLMs offer promising avenues for automating soft-\nware development processes and enhancing developer productivity. Although LLMs are primarily\ndesigned for generating text, their output can be influenced by specific instructions communicated\nthrough prompts [52].\nPrompting. Prompting is a technique used to communicate expectations and guide the LLM's vast\nknowledge and capabilities towards achieving a specific goal [64]. Prompting involves providing\ninstructions to LLMs to guide their generation process and elicit specific types of responses, enforce\nconstraints, or guide the model toward certain stylistic elements. In the software engineering realm,\nprompting can be used to elicit code snippets, documentation, or other relevant text based on user\nrequirements.\nIn-Context Learning and Few-Shot Learning. Leveraging pre-trained models, i.e., LLMs, for\ndownstream tasks often requires further fine-tuning on domain-specific labeled data [27]. How-\never, fine-tuning LLMs for specific tasks can be computationally expensive and resource-intensive,\nrequiring substantial amounts of task-specific annotated data [8]. Hence, in-context learning and\nfew-shot learning offer a powerful alternative [16]. In-context learning allows the LLM to adapt\nwithin a single interaction, using some initial information, i.e., context. Few-shot learning consists\nof exposing the model to a few examples (i.e., \"few shots\") to make the model effective for a specific\ntask. For instance, for sentiment analysis in user reviews where the goal is to classify reviews\nas positive or negative, few-shot learning consists of giving the LLM a few examples for each\nsentiment, i.e., reviews with associated sentiment. The LLM then uses its existing knowledge and\nthese few examples to categorize new reviews based on the context provided.\nRetrieval Augmented Generation. LLMs can suffer from hallucination and generate irrelevant or\ninaccurate responses [28]. This can occur due to limitations in their training data or the lack of clear\ncontext in the prompt. Retrieval-Augmented Generation (RAG) addresses the issue of hallucination\nby integrating information retrieval techniques into the generation process [36]. Thus, the RAG\nretrieves relevant knowledge from external sources based on the input context, augmenting the\nmodel's understanding. For instance, in the realm of user reviews, RAG can be employed to enhance\nthe quality of review summarization. The RAG model augments the user reviews with additional\ninformation extracted from external sources, such as sentiment analysis scores, key phrases or\nkeywords, to generate concise and informative summaries of user reviews."}, {"title": "3 LLM-Cure", "content": "LLM-Cure is designed to identify user complaints from user reviews and provide suggestions for\ndevelopers to enhance features that require the developer's attention. More specifically, LLM-\nCure operates in two distinct phases: (1) Scalable Feature Extraction and Assignment that focuses\non identifying and assigning features from user reviews of competing apps and (2) Suggestion\nGeneration with Competitor Reviews that leverages the extracted features to identify user complaints\nfrom a target app and generate suggestions for feature enhancement leveraging the competitor\nreviews. By incorporating user feedback from competitor reviews, LLM-Cure helps developers\naddress user complaints with a competitive edge. Figure 1 provides an overview of the approach."}, {"title": "3.1 Scalable Feature Extraction and Assignment", "content": "Step 1: Extracting Features with Batching and Matching. This step focuses on identifying\nthe top features that can be summarized by LLMs from an extensive collection of user reviews.\nHowever, LLMs have limitations on the amount of context they can process at once. To address this\nchallenge and make our approach scalable, we introduce the so-called batch-and-match approach\nthat incrementally extracts the top k features from a large corpus of user reviews. Our approach\nconsists of three processes:\n\u2460 Batching reviews and extracting features. Batching reviews involves dividing a large volume of\nreviews into manageable batches for incremental processing and feature extraction using LLMs.\nFirst, we shuffle the entire collection of user reviews of a group of competing apps to ensure\nrandomness. Let $R = \\{r_1, r_2, ..., r_m \\}$ represent the shuffled user reviews where m is the total number\nof reviews. To efficiently process the large volume of reviews, we split the shuffled reviews R into\nbatches of a predefined size s (e.g., 1,000 reviews) that fit within the LLM context size. We denote the\nbatches as $B = \\{B_1, B_2, ..., B_n\\}$, where n is the total number of batches. Each batch $B_i$ corresponds\nto a set of individual reviews $R_i = \\{r_i, r_{i+1}, ..., r_{i+s}\\}$, with each $r_i$ being a user review in batch $B_i$. We\nprocess each batch $B_i$ sequentially using the LLM. For each batch $B_i$, we prompt the LLM to extract\nthe top k features, denoted as $F_i$, from the set of reviews $R_i$. The value of k is a hyper-parameter\nthat can be tuned to optimize the performance of LLM-Cure depending on the selected dataset. The\nFeature extraction prompt, illustrated in Figure 2, identifies the top k features for a specific app\ncategory. It is structured to encapsulate the task description, define features, include a one-shot\nexample, and present the list of user reviews.\n\u2461 Matching and merging similar features. Since we process thousands of reviews in batches due to\nlimitations on the LLM's context handling, the extracted features might sometimes use different\nwording. For instance, in one batch, the LLM might identify \"Advertisements\" as a feature, while an-\nother batch might highlight \"In-app Advertisements\". Both features represent the same functionality\nextracted from the user reviews. To ensure a non-redundant feature identification, we address this\nchallenge by incrementally combining similar features after processing each batch $B_i$. $F_i$ represents\nthe features extracted from batch $B_i$. We define $M_i$ as the set of matched and merged features until\nbatch $B_i$. The merging process starts with the top $K_i$ features extracted from the first batch $B_i$. For\nthe first batch, $M_1 = F_1$. For subsequent batches ($B_i$, i > 1), we compare features in $F_i$ with $M_{i\u22121}$\nfeatures already identified from the previous batch $B_{i\u22121}$ and match then merge the similar features.\nWord embeddings and cosine similarity are employed to achieve the merging. Word embeddings\nrepresent the features in a high-dimensional vector space, capturing their semantic meaning [7].\nCosine similarity, a metric for measuring similarity between vectors, is then calculated between the\nembedding vectors of features $F_i$ and $M_{i\u22121}$ from different batches. Features exceeding a predefined\nsimilarity threshold $t_r$ in cosine similarity are considered highly similar and subsequently matched\nand merged. The similarity threshold $t_r$ is a hyper-parameter. Therefore, we experiment with\nthresholds ranging from 0.7 to 0.85 on a validation set and choose the value that leads to the highest\nprecision. This incremental process continues with each new batch $B_i$, merging similar features\nfrom $F_i$ with the existing merged set $M_{i\u22121}$ leading to a unique set of features $M_i$.\n\u2462 Verifying convergence and stabilizing features. The challenge in this step is to determine when\nthe incremental batch-and-match process has sufficiently captured the top k features, avoiding\nunnecessary iterations that would consume additional processing time and computational resources\nto process the entire volume of user reviews. We address this by defining a convergence threshold\nbased on the stability of the top k features over a specified number of consecutive iterations N. For\ninstance, the system starts with the initial merged set ($M_1$). The batch processing continues until\nthe merged set $M_j$ where the merged sets remain unchanged across the last N iterations (from $M_j$\nup to $M_{j-N}$). For example, assuming the convergence threshold is set to 3, LLM-Cure checks if the\ntop k features identified have remained stable for the last 3 batches. This stability indicates that we\nhave captured the dominant features in the reviews R, and further processing would probably not\nyield new features. The convergence threshold N is a hyper-parameter. Therefore, we experiment\nwith thresholds equal to 3, 5, and 7 on a validation set. Following this convergence step, we obtain\nthe final set of top k features extracted from the review batches.\nStep 2: Assigning Features to Reviews. The prior research on the dataset [6] used in our approach\ndemonstrates that only 8.6% of the user reviews contain multiple features and that multi-labeling\ndoes not lead to a significant impact on the results. We leverage prior findings to task a language\nmodel to assign one feature to the reviews by constructing a Feature assignment prompt. The\nFeature assignment prompt incorporates the task description, the extracted k features with their\nbrief meaning, five few-shot examples demonstrating feature assignment to user reviews, and the"}, {"title": "3.2 Suggestion Generation with Competitor Reviews", "content": "Prior research [21, 54] shows that negative reviews, those with 1 or 2-star ratings, are particularly\ninteresting to developers as they often contain valuable insights regarding feature complaints and\nareas for enhancement. Conversely, positive reviews, typically rated 4 or 5 stars, offer detailed\ndescriptions of features and positive user experience [35]. These positive reviews are valuable\nresources as they often showcase successful implementations of similar features and offer potential\nsolutions to address user complaints. Additionally, prior research [3, 45] indicates that 3-star ratings\nare typically viewed as neutral, or often encompass both praise and criticism of the app features.\nIn Phase 2, we leverage the user feedback summarized from competitors' positive reviews to\nprovide suggestions for the target apps to improve the features associated with negative reviews.\nOur proposed method uses an RAG approach to dynamically construct prompts for the LLM that are\naugmented with relevant positive user reviews from competing apps. By analyzing these positive\nreviews, the LLM can identify successful implementations and suggest potential solutions to user\ncomplaints within the target application. LLM-Cure generates suggestions based on the following\nfive distinct steps.\nStep 1: Curating Popular Underperforming Features. An underperforming feature is defined\nas one that has the largest percentage of negative reviews. Identifying underperforming features is\ncrucial for developers to prioritize areas for improvement and focus on features with the highest\npercentage of negative reviews to address user dissatisfaction better. We calculate the Underper-\nforming Feature Score (UFS) for each feature by determining the percentage of negative reviews\nassociated with it. The formula for UFS for a particular feature F is:\n$UFS_F = \\frac{Number \\ of \\ Negative \\ Reviews_F}{\\sum_{i=1}^{k}Number \\ of \\ Negative \\ Reviews_i} \u00d7 100$ (1)\nWhere the Number of Negative ReviewsF denotes the number of negative reviews associated with\nfeature F, and $\\sum_{i=1}^{k}$ Number of Negative Reviewsi denotes the total number of negative reviews\nacross all k features. Sorting features in descending order of their UFS prioritizes those with\nthe highest percentage of negative reviews. This allows developers to focus on features most\nfrequently associated with user dissatisfaction.\nStep 2: Segregating Positive and Negative Reviews. In this step, for a selected underperforming\nfeature F, we select the negative reviews rated 1 and 2 stars of the target app. Concurrently, we also\ncurate positive reviews rated 4 or 5 stars of the same feature F from competitor apps. We exclude\nreviews associated with a 3-star rating.\nStep 3: Prioritizing Complaint-Rich Negative Reviews. Not all reviews contain the same\namount of details. Some might express generic dissatisfaction, while others delve deeper and\nprovide specific details about the issues encountered with the feature. In this step, we aim to\nguide developers towards the most informative negative reviews, i.e., complaint-rich, for a specific\nunderperforming feature F. To accomplish this, we implement a ranking mechanism on the negative"}, {"title": "3.3 Implementation of LLM-Cure", "content": "LLM Choice. We select Mixtral-8x7B-Instruct-v0.1\u2074 LLM to conduct our experiments. Mixtral-\n8x7B-Instruct-v0.1 is a high-performing, open-weight Sparse Mixture of Experts model. We choose\nthis model as it balances cost with performance. It has been demonstrated that it surpasses open\nsource models, including Llama 2 70B5 while achieving 6x faster inference, and it matches GPT-3.56\nperformance on standard tasks [1]. Being open-source and free, Mixtral allows other researchers to\neasily access, understand, and adapt our work. We employ Python scripts to facilitate loading the\nMixtral-8x7B-Instruct-v0.1 model from the Hugging Face Hub7.\nEmbedding Model Choice. To ensure consistency within our approach, LLM-Cure employs the\nmistral-embed\u2078 word embedding model from Mistral AI during the process that requires word\nembedding. Specifically, we used it in the Matching Similar Features process of phase 1 and in the\nIdentifying Candidate Reviews for Relevant Solutions step of phase 2. We leveraged the Mistral API\nto retrieve text embeddings efficiently.\nText Preprocessing. Prior to feeding text inputs into the mistral-embed model, we conducted\nstandard text normalization processes adopted in previous work [4, 5] to enhance the quality of\nthe input data by applying tokenization, removal of stop words, stemming, and spell-checking.\nWe employed the SpellChecker\u2079 along with the nltk\u00b9\u2070 libraries in Python for these preprocessing\nsteps."}, {"title": "4 Experimental Design", "content": "We aim to evaluate the ability of LLM-Cure to (1) accurately assign features to user reviews and\n(2) provide the developers with suggestions to improve the features of their apps given a specific\ncomplaint. In this section, we describe the experimental setup and discuss the results of our\ninvestigation."}, {"title": "4.1 Dataset", "content": "We employ the same dataset utilized by a previous work [6]. The original dataset comprises 20\ncategories of competing apps selected from the top 2,000 popular free-to-download apps from the\nGoogle Play Store. The selected apps span diverse categories, e.g., Navigation, Weather, Browser,\nFreeCall and Dating, and each category includes a sufficient number of competing apps (e.g., 8 to\n10 competing apps) to facilitate competitor user review analysis. To evaluate our approach against\nthe baselines, we use the same five categories used by the baselines [6] to evaluate the precision,\nnamely Weather, SMS, Bible, Music Player, and Sports news. Similar to previous work, we use the\nFree call and Cooking recipe categories for hyper-parameter tuning. Specifically, for each category,\nwe utilize the same statistically representative sample of reviews, i.e., 96 user reviews per category,\nresulting in a total of 672 ground truth user reviews to evaluate our approach."}, {"title": "4.2 Research Questions", "content": "4.2.1 RQ1: How effective can LLMs be in extracting features from user reviews?\nIn our work, LLM-Cure suggests enhancements to developers to improve their app features by\nidentifying features from user reviews. Therefore, we want to evaluate the capabilities of the LLM\nin automatically extracting meaningful features from user reviews to understand the feasibility\nand potential of LLM-Cure for real-world applications.\nEvaluation Metrics. We assess the performance of LLM-Cure by comparing predicted features by\nthe LLM against the ground truth. We employ three key metrics: 1) True Positives (TP) representing\nthe correctly predicted features, 2) False Positives (FP) representing the number of falsely predicted\nfeatures, and 3) False Negatives (FN) representing features present in reviews but not predicted\nby LLM-Cure. We adopt the precision, recall and F1-score as evaluation metrics, and we calculate\nthem as follows:\n$Precision = \\frac{TP}{TP + FP}$ (3)\n$Recall = \\frac{TP}{TP + FN}$ (4)\n$F_1-Score = 2 * \\frac{Precision * Recall}{Precision + Recall}$ (5)\nTo evaluate the performance metrics of feature extraction, the first and second authors, as two\nindependent annotators, manually label 672 testing reviews. The Cohen's Kappa agreement score\n[12] is computed on the annotated testing reviews, yielding a high score of 0.82, indicative of a\nhigh level of agreement.\nExperimental Setup. LLM-Cure has three hyper-parameters: k the number of the features, the\nsimilarity threshold and the convergence threshold. Previous work that uses the dataset demonstrates\nthat 14 leads to the best results when set as the number of features. Therefore, we set k as 14,\naligning with previous work on this dataset [6]. To set the similarity threshold and the convergence\nthreshold, we conduct experiments on the two validation sets, i.e., \"Recipe cooking\" and \"Free Call\"\napp categories. The results indicate that a similarity threshold of 0.75 coupled with a convergence\nthreshold of 5 yields the highest precision. Therefore, we adopt these hyper-parameter values for\nthe testing set. In addition, to prevent exceeding the limited context size of the LLM, we select a\nbatch size of 1,000 reviews. This batch size is determined based on the varying lengths of individual\nreviews. Through experimentation, we have found that 1,000 reviews is optimal, as it ensures that\nthe total token count of each batch does not exceed the context window of the selected Mistral model.\nBaselines. To assess the efficacy of LLM-Cure in automatically identifying and assigning features to\nuser reviews, we compare its performance against existing baselines FeatCompare[6] and Attention-\nbased Aspect Extraction (ABAE) [25], using the ground truth of 480 labeled reviews. In addition,\nwe include a baseline called LLM-Basic. Similar to LLM-Cure, in LLM-Basic, we prompt the LLM to\nextract features from user reviews. However, in this baseline, we do not include the incremental\nprocess of batching, i.e., batch-and-match, used in LLM-Cure. Instead, we select a statistical sample\nof reviews from the set of all reviews that fits the context size of the LLM (i.e., 1,000 reviews) and\nuse the same prompt used for LLM-Cure. Our goal is to verify that the incremental process adds\nvalue to the extraction and improves the performance.\nPrompt Construction. To construct the prompts, we adhere to the template provided by Mistral\nfor our selected model11. LLM-Cure leverages two distinct prompts for Phase 1: the Feature extraction\nprompt and Feature assignment prompt. The Feature extraction prompt is illustrated in Figure 2 and\nthe Feature assignment in Figure 3. The prompts are also available in our research artifact.\nResults. LLM-Cure is capable of identifying and assigning features with high F1-score,\nrecall and precision. Table 2 and Table 3 show the fourteen features extracted for the five testing\napps categories. Across the five testing app groups, LLM-Cure exhibits F1-scores ranging from 80%\nto 91%, with precision between 81% and 92% and recall between 80% and 90%. These findings\nunderscore the capability of LLMs to extract features from user reviews without requiring manual\nannotation.\nLLM-Cure significantly outperforms LLM-Basic, FeatCompare and ABAE baselines\nacross the testing apps. Table 5 shows that on average, LLM-Cure achieves a 7% improvement\nin F1-score, a 9% improvement in recall and a 4% in precision as compared to the baselines. To\nquantitatively assess these differences, we conducted paired t-tests. A paired t-test [38] is a statistical\nmethod used to determine whether there is a significant difference between the means of two related\ngroups. In our case, these groups are the performance metrics of LLM-Cure and FeatCompare,\nthe best performing baseline. Our findings indicate that LLM-Cure significantly outperforms\nFeatCompare in both F1-score and precision. Specifically, the paired t-test for F1-score yielded a t-\nstatistic of 3.723 with a p-value of 0.02, confirming a statistically significant difference. Similarly, the\npaired t-test for precision resulted in a t-statistic of 4.784 and a p-value of 0.009, further underscoring\nLLM-Cure's superior performance.\nThe batch-and-match process of LLM-Cure improves the performance of feature ex-\ntraction. LLM-Basic, which only processes a single batch of reviews, achieves lower performance\ncompared to LLM-Cure. These results highlight the benefit of LLM-Cure's incremental processing,\nbatch-and-match, and its ability to extract features more effectively and with higher performance\n(e.g., F1-score). Instead of processing the entire set of user reviews, LLM-Cure processes only a\nfraction of the total reviews. Table 4 illustrates the percentage of user reviews needed by LLM-Cure\nto achieve convergence and extract the features. LLM-Cure outperforms all baseline methods while\nprocessing only between 3% and 30% of user reviews, achieving feature saturation without the\nneed for processing the entire dataset.\nLLM-Cure performs consistently across different sentiment categories. To further inves-\ntigate whether LLM-Cure classifies positive versus negative reviews with different precision, we\nconduct an analysis based on the sentiment categories. We find that positive and negative reviews\npresent similar results across categories, with at most 3% of differences. The average precision for\npositive reviews across these categories is 85.69%, while for negative reviews, it is 85.78%. These\nfindings indicate that there is no significant difference in classification F1-score between positive\nand negative reviews, demonstrating that our approach is not sensitive to the sentiment of reviews."}, {"title": "4.2.2 RQ2: Can LLMs leverage categorized user reviews to generate suggestions for\nfeature improvements?", "content": "RQ2 investigates whether LLM-Cure can leverage categorized user reviews of competitor apps\nto generate suggestions for feature improvements. Analyzing competitors' positive user reviews\nallows developers to identify successful features and user preferences across the market, ensuring\ntheir app remains competitive and relevant. By incorporating these insights from competitor re-\nviews, LLM-Cure empowers developers to make data-driven decisions about feature enhancement,\nprioritize user needs, and ultimately create a more competitive app.\nSuggestions validation. To assess the relevance of the suggestions provided by LLM-Cure, we\nconduct a retrospective investigation at the app release level. Specifically, we examine the release\nnotes of future releases of the target app following the date of the user review containing the\ncomplaint and calculate the Suggestions Implementation Rate (SIR). We define the SIR the number of\nsuggestions by LLM-Cure matched in the release notes divided by the total number of suggestions\nprovided as follows:\n$SIR = \\frac{Number \\ of \\ Suggestions \\ Matched \\ in \\ Release \\ Notes}{Total \\ Number \\ of \\ Suggestions \\ by \\ LLM-Cure}$ (6)\nExperimental Setup. We randomly select three categories to evaluate the feature improvement\nsuggestions. From each category, We select the apps with a substantial number of informative\nrelease notes to ensure that we have a rich data source for conducting the manual suggestion vali-\ndation. Specifically, we choose Handcent Next SMS messenger12 from the SMS category, FOX Sports:\nWatch Live\u00b9\u00b3 from the Sports News category, and Weather & Clock Widget\u00b9\u2074 from the Weather app\ncategory. As shown in Table 6, the chosen Weather, SMS, and Sports News apps have 40, 140, and 36\nrelease notes, respectively, with average word counts of 36, 30, and 23 per release. For each app, we\napply steps 1 to 5 of LLM-Cure's Phase 2. We focus on the top three underperforming features that\nrequire the most attention from developers. For each feature, we identify three target complaints.\nFor each user complaint, we generate suggestions to improve the app features. We obtain a total of\n9 suggestions per feature, resulting in 27 suggestions per app. We then calculate SIR for each feature.\nResults. LLM-Cure achieves a promising SIR of 59 out of 81 (i.e., 73%), indicating the\nmajority of the suggestionss from LLM-Cure are implemented by the developers. Table 6\nshows the SIR for each feature across the three apps. The results indicate that some features received\nhigher SIRs than others. For example, all the User Interface-related suggestions for \"Handcent Next\nSMS messenger\" were implemented, while not all Notification-related suggestions for the same app\nwere adopted. This variation can be attributed to different development priorities or challenges\nassociated with certain feature enhancements. Furthermore, the release notes sometimes contained\nhigh-level descriptions of updates, such as \"Performance improvement,\" which may not explicitly\ndetail the changes but could reflect the overall enhancement suggested by LLM-Cure.\nLLM-Cure successfully leverages candidate positive user reviews from competitors to\nprovide suggestions for feature improvements related to user complaints. We illustrate this\nprocess through two case studies, randomly selecting underperforming features from the Weather\n& Clock Widget app. For the underperforming \"Ease of Use\" feature, we consider the below user\nreview that highlights an issue where the weather widget fails to update automatically, requiring\nmanual refreshes:\n\"Widget won't update over time it keeps showing\nsame weather and same TIME until I tap on it and\nopen weather app.\"\nThe target review identified two weaknesses in the widget: (1) unreliable timekeeping and\n(2) an unfriendly design. LLM-Cure tackles these issues by proposing an automatic refresh\nfunctionality, an improved time synchronization, and the implementation of a refresh icon. These\nsuggestions directly address the user's frustrations and aim to improve the widget's usability.\nSpecifically LLM-Cure proposes the below three suggestions:\nAutomatic Refresh: Implement an automatic refresh feature for the\nwidget that updates the weather and time data periodically, so users\ndon't have to manually tap on the widget to see the current weather\nand time.\nImproved Time Synchronization: Enhance the clock functionality\nin the widget to ensure that it stays in sync with the device's internal\nclock, eliminating the need for manual time updates.\nClear Refresh Icon and Options: Provide a clear refresh icon and add\noptions for users to manually refresh the widget, change clock fonts,\nand customize the weather update intervals to their preference. This\nwill give users more control over the widget's behavior and improve\ntheir overall experience.\nSubsequently, we cross-referenced these suggestions with the app's release notes and found that\ndevelopers implemented recommendations provided by LLM-Cure in future releases. For instance,\nin release 6.1.0.1, the functionalities \"Added option to show forecast every 3 hours on the widget when\nyou select hourly forecast\" and \"Enabled digital font for clock and date\" were introduced, aligning\nclosely with the suggestions aimed at enhancing the time synchronization of the widget.\nFollowing the same approach outlined above, we select the below user complaint belonging to\nthe Radar underperforming feature:\n\"Needs a radar view for everyone's local area. Not\nhaving it takes away from the app.\"\nThe target review complains about the missing radar view for the local user area. The complaint\nhighlights a shortcoming in the app's radar functionality on two fronts: 1) Radar location,\ni.e., the user can't easily see weather patterns in their immediate surroundings and 2) Radar\nview perspectives, i.e., the app lacks the flexibility to customize the radar display. LLM-Cure\naddresses these issues by proposing customizable views, GPS integration, and real-time radar\nupdates leveraging relative competitive user reviews."}, {"title": "5 Threats to Validity", "content": "Threats to construct validity relate to a possible error in the data preparation. In LLM-"}]}