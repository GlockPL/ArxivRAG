{"title": "An Enhanced Harmonic Densely Connected Hybrid Transformer Network Architecture for Chronic Wound Segmentation Utilising Multi-Colour Space Tensor Merging", "authors": ["Bill Cassidy", "Christian Mcbride", "Connah Kendrick", "Neil D. Reeves", "Joseph M. Pappachan", "Cornelius J. Fernandez", "Elias Chacko", "Raphael Br\u00fcngel", "Christoph M. Friedrich", "Metib Alotaibi", "Abdullah Abdulaziz AlWabel", "Mohammad Alderwish", "Kuan-Ying Lai", "Moi Hoon Yap"], "abstract": "Chronic wounds and associated complications present ever growing burdens for clinics and hospitals world wide. Venous, arterial, diabetic, and pressure wounds are becoming increasingly common globally. These conditions can result in highly debilitating repercussions for those affected, with limb amputations and increased mortality risk resulting from infection becoming more common. New methods to assist clinicians in chronic wound care are therefore vital to maintain high quality care standards. This paper presents an improved HarDNet segmentation architecture which integrates a contrast-eliminating component in the initial layers of the network to enhance feature learning. We also utilise a multi-colour space tensor merging process and adjust the harmonic shape of the convolution blocks to facilitate these additional features. We train our proposed model using wound images from light-skinned patients and test the model on two test sets (one set with ground truth, and one without) comprising only darker-skinned cases. Subjective ratings are obtained from clinical wound experts with intraclass correlation coefficient used to determine inter-rater reliability. For the dark-skin tone test set with ground truth, we demonstrate improvements in terms of Dice similarity coefficient (+0.1221) and intersection over union (+0.1274). Measures from the qualitative analysis also indicate improvements in terms of high expert ratings, with improvements of > 3% demonstrated when comparing the baseline model with the proposed model. This paper presents the first study to focus on darker-skin tones for chronic wound segmentation using models trained only on wound images exhibiting lighter skin. Diabetes is highly prevalent in countries where patients have darker skin tones, highlighting the need for a greater focus on such cases. Additionally, we conduct the largest qualitative study to date for chronic wound segmentation. All source code for this study is available at: https://github.com/mmu-dermatology-research/hardnet-cws", "sections": [{"title": "1. Introduction", "content": "Diabetes is now regarded as a global epidemic, resulting in most part from a systematic increase in populations becoming overweight and obese (Moura et al. (2019)). Programmes that target the condition have historically shown only short-term benefits, with longer-term effects yet to be established (Khunti et al. (2012); Davies et al. (2017)). The situation is similar for obesity (Ong et al. (2023)), a common factor in diabetes occurrence (Klein et al. (2022)). Arterial leg ulcers (ALUs) and diabetic foot ulcers (DFUs) are a debilitating and costly complication of diabetes (Moura et al. (2019)), with recent findings suggesting an association between DFU episodes and all-cause resource utilisation and increased mortality risk (Petersen et al. (2022)). Venous leg ulcers (VLUs) and pressure ulcers (PRUs) are the most common types of complex skin ulcers (Jenkins et al. (2019)), with ulcer prevalence in the diabetic population estimated to be 13% in North America (Zhang et al. (2017)). The incidence of chronic wounds is high and is estimated to continue on an upward trajectory (Eriksson et al. (2022)).\nPatients diagnosed with DFU are two to three times more likely to die than patients without and are predisposed to numerous comorbidities, including peripheral artery disease, cardiovascular disease, neuropathy, retinopathy, and nephropathy. VLUs and DFUs often result in significantly impaired quality of life (Franks et al. (2016); Mader et al. (2019); Xiong et al. (2020)). Occurrence of ulcers is linked to an increased incidence of both amputation and mortality, especially in the presence of advanced age, peripheral artery disease and anemia (Franks et al. (2016); Costa et al. (2017); Vainieri et al. (2020)). Chronic wounds exert a significant physical and emotional burden on patients (Renner and Erfurt-Berge (2017); Polikandrioti et al. (2020)), with depression being associated with an increased risk at initial and subsequent occurrence (Iversen et al. (2015, 2020)).\nChronic wounds are typically correlated with comorbidities such as diabetes, vascular deficits, hypertension, and chronic kidney disease (Sen (2021)). Diabetic neuropathy is highly prevalent in DFU cases and is the primary cause of DFU formation (Petrone et al. (2021)), meaning that patients have lost sensation in their foot due to nerve damage (Rathur and Boulton (2007)). This means that patients often go through long periods not realising they have a DFU until the wound becomes much worse and leads to other serious complications. Infection affects more than 50% of all DFU cases (Bader (2008)) and represents one of the most common causes of diabetes related hospitalisation (Petrone et al. (2021)). Diabetic leg and foot ulcers are amongst the most expensive wound types to treat in the United States (Sen (2021)). For VLUs, the recurrence rate within 3 months after wound closure is as high as 70% (Franks et al. (2016)).\nManagement of chronic wounds can be a long and difficult task, for both patient and clinician. This is especially true for wounds that are not caught early, and require more intensive treatment programmes. This can mean frequent visits to clinics or hospitals for assessment by experts (Boulton et al. (2005); Van Netten et al. (2017)). Even after accomplished wound healing, recurrences are frequent and often lead to minor or major amputation of lower extremities (Apelqvist et al. (1993); Larsson et al. (1998)). The post COVID-19 climate poses further risks and challenges to the treatment of chronic wounds, given that diabetic patients are placed in the high-risk category. To this end, recent years have seen an increased research interest in the remote detection and monitoring of wounds using non-contact methods (Cassidy et al. (2022b); Reeves et al. (2021); Pappachan et al. (2022)).\nEvolving current telemedicine systems to include remote wound monitoring represents an opportunity to reduce risks to vulnerable patients and to ease significantly overburdened healthcare systems (Yammine and Estephan (2021)). Furthermore, the advent of cheap consumer mobile devices and easily accessible cloud platforms promotes the idea of making these technologies available to poorer regions, where patients may experience reduced access to expert healthcare providers. Low cost, easy-to-use non-invasive devices that can detect and monitor wounds could act as a mechanism to promote patient engagement with the monitoring of their health.\nA growing body of evidence has shown the ability of convolutional neural networks (CNNs) to equal or surpass experienced dermatologists for detection and classification in related domains (Esteva et al. (2017); Brinker et al. (2019b,a); Fujisawa et al. (2019); Pham et al. (2020); Jinnai et al. (2020); Haenssle et al. (2021)). In this regard, deep learning may be able to assist in providing more objective results in domains which are prone to high levels of subjectivity. Changes to wound area have been shown to be a robust predictor in healing status (Sheehan et al. (2003)). Segmentation of chronic wounds allows for more accurate assessment of changes to wound shape and size over time when compared to more generalised localisation techniques. In the next section, we discuss the recent notable developments in this domain."}, {"title": "2. Related Work", "content": "Studies on deep learning tasks related to chronic wounds have become a growing interest in the research community in recent years due to the possible benefits that such technologies might offer in real-world clinical settings (Goyal et al. (2018); Cassidy et al. (2023)). In this section, we examine the more prominent studies conducted in chronic wound segmentation research that have helped to guide the experiments presented in this paper.\nGoyal et al. (2017) were one of the first research groups to investigate chronic wound segmentation using convolutional neural networks (CNNs). They trained a number of fully convolutional networks (FCN) to segment DFU wounds and associated periwounds using a dataset comprising 600 DFU images together with ground truth masks which were provided by wound experts at Lancashire Teaching Hospitals (LTH), UK. A two-tier transfer learning approach using two publicly available general image datasets was used - Pascal VOC and ImageNet segmentation datasets. The DFU segmentation dataset was divided into 420 training images, 60 validation images, 120 test images, and 105 images of healthy feet. In the joint segmentation of wound and periwound regions the highest performing model was FCN32-s with a Dice similarity coefficient (DSC) of 0.899. For segmentation of ulcer regions only, the highest performing model was FCN-16s, reporting a DSC of 0.794. For segmentation of only periwounds, the highest performing model was FCN-16s, reporting a DSC of 0.851. This work noted that the FCN-AlexNet and FCN-32s models were less accurate in the segmentation of irregular boundaries, and that the smaller pixel strides used in FCN-16s and FCN-8s resulted in improved detection of such examples. This study also observed an overlap of periwound and wound regions in prediction results due to ambiguities in feature boundaries. A limitation of this work is the small number of samples used in the experiments, which may make the results difficult to generalise across more diverse datasets.\nWang et al. (2020) conducted wound segmentation experiments using MobileNetV2, which was pretrained using the Pascal VOC segmentation dataset. For training and testing, they used a newly introduced dataset of 1109 DFU images (train = 831; test = 278). A localisation method was used as a preprocessing stage to exclude non-DFU wound regions from images before the segmentation stage. As a post-processing step, morphological algorithms were used (small region removal and hole-filling). Their test results reported a mean DSC of 0.9047. However, this work presents several limitations. First, all wound images were very small patches that are heavily padded to a resolution of 224 x 224 pixels. Wound pixels therefore comprised only very small regions of the images. Excluding padding, the average size of the wound regions in the training set is 71 \u00d7 104 pixels, and the average wound region size in the test set is 70\u00d7101 pixels. At such low resolutions, as small as 17 \u00d7 18 pixels, a large number of wound features may be lost. They also tested their model on the Medetec dataset, and obtained a DSC of 0.9405.\nIn later works, Wang et al. (2022) conducted the Foot Ulcer Segmentation Challenge (FUSC) 2021 whereby a new DFU dataset was released (train = 810, val = 200, test = 200). This new dataset comprised of examples with less significant padding compared to their prior dataset, with images exhibiting more foot and background features. The winner of the FUSC 2021, Mahbod et al. (2021), achieved an image-based DSC of 0.8880, which was 1.67% lower than the prior DSC reported by Wang et al. (2020). This may indicate that the task was more difficult when larger wound images were introduced. In the FUSC 2021, models were required to learn features that are more complex that were absent from the prior experiments conducted by Wang et al. (2022) which used a smaller dataset comprising notably smaller wound regions and thus fewer features.\nScebba et al. (2021) noted the numerous challenges associated with wound segmentation, including wound type heterogeneity, variance in tissue colouration, wound shapes, background features, anatomical location, variety of image capturing scenarios, and non-standard specifications of capture devices. They observed that standardisation initiatives in medical wound photography may lead to additional workload burdens on clinical routine, and that the proposal of standards would likely not result in a desired consistent approach in real-world scenarios. Their proposed method utilised a MobileNet localisation model to assist a U-Net segmentation model to reduce non-wound features. This study used a total of five chronic wound datasets (1) SwissWOU a private dataset of DFU (n = 1096) and systemic sclerosis digital ulcers (n = 63), (2) SIH (second healing intention dataset) (n = 58) (Yang et al. (2016)), (3) DFUC2020 (n = 2000) (Cassidy et al. (2022a)), (4) FUSC (n = 60) (Wang et al. (2022)), (5) Medetec (n = 53) (Thomas (2014)). We observe that for some of the datasets used in this study, complete sets were not utilised in the experiments. For the FUSC, Medetec, and SIH datasets, only a selection of images were used. The authors experimented using a range of well-known segmentation networks, both with and without localisation preprocessing (manual and automated). When tested using only the SwissWOU DFU images (10% of all patients), their results showed that U-Net was the highest performing network (MCC = 0.85, IoU = 0.75). Their test results for the SwissWOU systemic digital ulcers, Medetec, SIH, and FUSC images also showed U-Net to be the best performing network (MCC = 0.8725, IoU = 0.7875).\nHarDNet-DFUS (Harmonic Densely Connected Network), proposed by Liao et al. (2022a), was the winning entry for the DFUC2022, achieving a DSC of 0.7287. The design is based on a prior work, HarDNet-MSEG (Huang et al. (2021)), and is the basis of our proposed methods in the present paper. HarDNet-DFUS uses inter-layer connections which were configured according to the required block depth n. Therefore, when n = 9, the resulting factors are 1, 3, and 9, allowing for shortcuts to the 1st, 3rd, and 9th convolutions. This results in the removal of the power of 2 constraint found in the original block design. A block depth of 3, 9, and 15 was selected for the final design, replacing the original depth of 4, 9, and 16. This results in reduced data movement using the same number of convolutional layers. Additionally, they replaced the receptive field blocks (RFB) in the decoder with a large window attention (Lawin) transformer. The original HarDNet network mainly utilised 3 \u00d7 3 convolutions to increase computational density, which changes the model from being memory-bound to compute-bound (Chao et al. (2019)). To increase accuracy further, they used an ensembling strategy using 5-fold cross validation and test time augmentation (TTA). Augmented images were added to the test set when testing the sub-models, with the output averages used as the final prediction results. However, they found that this method was not consistent, and would sometimes degrade performance in terms of DSC and IoU.\nRamachandram et al. (2022b) proposed a chronic wound segmentation network for tissue type segmentation (AutoTissue) and wound segmentation (AutoTrace) designed for use in a commercial mobile app. The AutoTrace model implemented a typical auto-encoder design using depth-wise separable convolution layers, attention gates, and strided depth-wise convolutions resulting in downsample activations which act as an alternative to fixed max-pooling. Additive attention gates were added to skip connections to regulate activations from previous network layers. Bilinear upsampling was used in the decoder blocks followed by depth-wise separable convolution layers, helping to reduce memory requirements. The AutoTissue segmentation model implemented EfficientNetBO as the encoder path, with a decoder comprising 4 layers with each layer utilising two-dimensional bilinear upsampling followed by 2 depth-wise convolution layers. AutoTrace was trained with a private dataset comprising 467,000 wound images, while AutoTissue was trained with a second private dataset comprising 17,000 wound images. For both datasets, both images and ground truth labels were obtained from hospitals in North America, allowing for a diverse range of wound images. However, details were not disclosed regarding the exact composition of the datasets. The study reported an mIoU of 0.8644 for wound segmentation and an mIoU of 0.7192 for tissue and wound segmentation. Clinicians rated 91% (53/58) of the results as between fair and good for segmentation and tissue segmentation quality. Qualitative assessment of is rare chronic wound related deep learning studies. However, the sample size used is limited, whereby only 58 examples were rated.\nSwerdlow et al. (2023) used a private dataset exhibiting stages 1-4 PRUs, acquired from eKare Inc. Mask R-CNN with a ResNet101 backbone was trained for segmentation and classification of each PRU stage of development. The dataset comprised 969 PRU images (train = 848, test = 121). The study reported a DSC of 0.92 for stage 1 PRU, 0.85 for stage 2 PRU, 0.93 for stage 3 PRU, and 0.91 for stage 4 PRU. The wound image acquisition protocol indicated that images be taken from approximately 40-65 cm distance from the wound. Additionally, the study excluded PRU wounds that were smaller than 2 x 2 cm, which may have limited testing of the model's true ability to segment a range of wound sizes.\nThe use of different colour spaces in CNNs was explored by Gowda and Yuan (2019). Their classification experiments on the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets showed that different classes were sensitive to models trained on different colour spaces. They trained a series of DenseNet models using multiple image datasets that had been converted to different colour spaces, with each DenseNet using a different colour space as input. The outputs from each DenseNet were then used as input into a final dense layer to generate weighted predictions from each sub-DenseNet. Increased computational overhead, a result of using multiple DenseNets, was addressed by using smaller and wider DenseNets. This work showed that training with images from multiple colour spaces provided comparable results to significantly larger models, such as DenseNet-BC-190-40, with a reduction of more than 10M parameters.\nIn later CNN-based colour space studies, Simon and Uma (2022) trained classification models using RGB and luminance images. Their experiments utilised a ResNet101 pretrained model for feature learning and an SVM for the classifier. They trained and tested their model with the Describable Texture Dataset (DTD) and the Flickr Material Dataset (FMD). Compared to prior works, for the DTD, they reported an accuracy improvement of 0.73%, and for the FMD they reported an accuracy improvement of 6.95%.\nIn more recent work, McBride et al. (2024) conducted preliminary experiments which merged individual colour channels from different colour spaces into single tensors when training a chronic wound U-Net segmentation model. They found that different colour channel merging operations using RGB, CIELAB, and YCrCb colour spaces improved segmentation performance by 0.0264 for IoU and 0.0348 for DSC when testing on the FUSC dataset. However, this study was limited by the use of only a simple U-Net model.\nOne of the most prominent aspects of chronic wound research in deep learning, as highlighted by our literature review, has been a lack of substantial publicly available fully annotated datasets. Another notable factor in the field is a lack of focus on patients exhibiting darker skin tones. The biases towards lighter skin tones present in deep learning models in dermatology research is well established (Wen et al. (2021)). Ben\u010devi\u0107 et al. (2024) observed significant bias in skin lesion segmentation against darker-skin cases when performing in and out-of-sample evaluation. Furthermore, they also found that methods used to mitigate bias do not result in significant bias reduction. Most of the publicly available chronic wound datasets comprise cases that were collected from lighter skin patients. While some datasets do contain examples with darker skin tones, these are not quantified. In the next section, we discuss the chronic wound datasets that we used in our experiments."}, {"title": "3. Chronic Wound Datasets", "content": "Large medical imaging datasets present notable challenges when used to train deep learning networks (Wen et al. (2021)). Issues such as image duplication, image and feature similarity (Dipto et al. (2023)), varying image quality, label noise and the presence of visual artefacts can significantly impact model performance (Akkoca-Gazio\u011flu and Kamasak (2020); Cassidy et al. (2021a); Daneshjou et al. (2021); Winkler et al. (2021); Jaworek-Korjakowska et al. (2023); Pewton et al. (2024)).\nOur research group has been responsible for the release of the first substantial publicly available DFU wound datasets with ground truth labels (Cassidy et al. (2021b); Yap et al. (2021a); Kendrick et al. (2022)). With the release of each dataset, we have conducted yearly challenges in association with the International Conference on Medical Image Computing and Computer Assisted Intervention (Cassidy et al. (2021b); Yap et al. (2021b); Cassidy et al. (2022a); Yap et al. (2022, 2024)). Our datasets comprise of over 20,000 high quality DFU wound photographs together internationally coordinated clinical labelling provided by experts in podiatry. Table 1 shows a summary of all the datasets used in our chronic wound segmentation experiments. We use 10 public datasets, 1 private dataset, and a dataset comprising Google Image Search images which we collected using the Creative Commons License search option to remove copyrighted images from search results. These images vary significantly, both in size and quality. To obtain these images, we used search terms such as \"diabetic foot ulcer\", \"neuropathic ulcer\", \"venous ulcer\", \"pressure ulcer\", \"wound\", and \"chronic wound\".\nThe private dataset used in our experiments is the The King Saud University Medical City (KSUMC) dataset. This dataset comprises 115 DFU wound images and was obtained from the King Saud University Medical City, Saudi Arabia. The images were acquired using a Fujifilm Finepix SL260 digital camera at various resolutions and orientations. The KSUMC dataset was obtained with ethical approval from King Saud University Medical City, Saudi Arabia (REF: 24/1159/IRB)."}, {"title": "3.1. Expert Wound Delineation", "content": "All training, validation and test cases for the DFUC2022 dataset were delineated with the location of DFUs in polygon coordinates. The VGG Image Annotator tool (Dutta et al. (2016); Dutta and Zisserman (2019)) was used to delineate images with polygons indicating the ulcer region. The ground truth was produced by five healthcare professionals who specialise in treating diabetic foot ulcers and associated pathology, comprising consultant physicians and podiatrists, all with more than 5 years professional experience. The instruction for annotation was to delineate each DFU with a polygon region.\nWe evaluate the agreement between the expert annotators on 800 cases (20% of the data) chosen at random using the Jaccard Similarity Index (JSI) and DSC. The DSC of the delineation between experts is 0.6981\u00b10.2544, the JSI is 0.5876\u00b10.2670, and accuracy is 0.9869\u00b10.0291.\nThe use of active contour masks when used as ground truth has been shown to provide superior agreement with machine predicted results in chronic wound segmentation tasks (Kendrick et al. (2022)). Therefore, in our experiments, for the DFUC2022 dataset we use ground truth masks that have been processing using the original polygon delineations with an active contour model applied to smooth delineated vertices. The active contour model masks were produced using the MATLAB (The MathWorks, Inc., Massachusetts) method created by Kroon (2022), using default parameters. Figure 1 shows an example of the two different mask types applied to a training image from the DFUC2022 dataset. To further validate that the smoothing effect did not alter the delineation of the experts, we measure the similarity of the masks produced by the clinicians and the masks post-processed by active contour on the training set. The DSC is 0.9620\u00b10.0259, the JSI is 0.9279\u00b10.0462, and the accuracy is 0.9991\u00b10.0012. These evaluations support our statement that the pre-processing stage has provided a smoothing effect, but did not alter the experts' delineation."}, {"title": "4. Method", "content": "This section details the training, validation, and testing workflow, proposed model architecture, and corresponding metrics used for our segmentation experiments."}, {"title": "4.1. Metrics", "content": "We utilised a series of widely used evaluation metrics to determine the accuracy of the models trained, validated, and tested in our wound segmentation experiments. Intersection over union (IoU) and DSC were selected as the main metrics for determining segmentation model accuracy. DSC was chosen for its representation as the harmonic mean of precision and recall, giving a balanced evaluation between false positive and false negative predictions. The relevant mathematical expressions for IoU and DSC are as follows:\n$IoU = \\frac{X \\cap Y}{|X| \\cup |Y|}$\n$DSC = 2 * \\frac{X \\cap Y}{|X| + |Y|}$\nwhere X and Y represent the ground truth mask and predicted mask respectively."}, {"title": "4.2. Baseline Experiments", "content": "The first stage in our experiments was to determine the effectiveness of a range of deep learning segmentation networks using the largest publicly available chronic wound dataset (DFUC2022). We obtained a series of baselines for training, validation, and test results for a selection of newer segmentation architectures using the DFUC2022 dataset. We focus on a selection of more advanced CNN architectures that were not included in the previous baseline experiments reported for DFUC2022 (Yap et al. (2024)). For all baseline experiments, the DFUC2022 dataset images and masks were unchanged from their original resolution (640 \u00d7 480 pixels). A total of 200 images were taken at random from the training set for use as the validation set during training. No augmentation or post-processing methods were used in any of the baseline experiments. All baseline models were trained for 300 epochs with a batch size of 2 using the Adam optimiser with a learning rate of 0.001, and a weight decay of 0.0001. All models were trained without the use of pretrained weights. The best model for each experiment was selected from the 300 epochs training schedule determined by the highest validation IoU and DSC values. The hardware and software configuration for all experiments completed in the present paper was as follows: Debian GNU/Linux 10 (buster) operating system, AMD Ryzen 9 3900X 12-Core CPU, 128GB RAM, NVIDIA GeForce RTX 3090 24GB GPU. Models were trained with Tensorflow 2.4.1 and Pytorch 1.13.1 using Python 3.7.13.\nThe results of the baseline experiments are summarised in Table 2. HarDNet-DFUS is clearly the best overall performing network in terms of training (IoU = 0.7889, DSC = 0.8743), validation (IoU = 0.6068, DSC = 0.7101), and test metrics (IoU = 0.5421, DSC = 0.6520, FPE = 0.0255, FNE = 0.3278). We observe that the EfficientNet U-Nets record lower training and validation loss rates at 0.1558 (EffNetB0 U-Net) and 0.3485 (EffNetB1 U-Net) respectively. These loss rates are significant, a reduction of 0.1043 for BO train loss and a reduction of 0.1125 for B1 validation loss. However, these performance gains are not reflected in the test loss results when comparing the EfficientNets with HarDNet. The notable differences between validation and test results for the best overall performing network (HarDNet-DFUS) may be indicative of the random nature of the validation set, which might not fully represent the range of features present in the test set. We observe that the deeper U-Net variants such as U-Net++ and ResUNet++ demonstrated particularly low metrics, which may be a consequence of the relatively small size of the DFUC2022 dataset and the larger size of these network architectures.\nIn addition to the range of network architectures reported on in Table 2, we also trained, validated, and tested a number of vision transformer (ViT) segmentation models. However, the test results for the ViTs were well below those reported in Table 2. As reported by Zhu et al. (2023), ViTs require substantial amounts of training data and are not suitable for use with very small datasets such as those used in the present paper. Zhu et al. (2023) observed that representation similarity between ViTs trained on small and large datasets comprising of > 1M images differed substantially. They posit that this may be due to a reduction in inductive bias (the relationship between closely positioned input features). Their experiments show that lower layers of ViTs are not able to sufficiently learn local relationships when small amounts of complex data are used. Conversely, recent work by Gani et al. (2022) suggests that ViTs might be trained on smaller datasets using self-supervised inductive biases. However, even in these scenarios, datasets of up to 100,000 images were used, which although might be considered small in deep learning terms, is still significantly greater than the current publicly available chronic wound datasets.\nWe compared a selection of ground truth masks with model predictions for the best performing network in the baseline results, which was HarDNet-DFUS. Figure 2 shows 3 cases with original image, ground truth labels, and corresponding baseline model predictions. The first row shows a case where the ground truth mask includes the wound and periwound as a single region, whereas the model predicted only the unhealed wound region. The second row shows a case where the two wound regions are separated by epithelial skin, indicating significant healing between the two non-healed regions. The corresponding prediction shows that only the main wound region was predicted by the model. The third row shows a case where two large wound regions are separated by an epithelial region. The ground truth includes both wound regions and the partially healed region. However, the prediction includes only the non-healed regions. These examples demonstrate the significant challenges inherent in human expert wound delineation and how delineation of wound regions can be highly subjective. We asked two clinical experts in wound care (a consultant surgeon and a consultant podiatrist) to indicate agreement with the ground truth labels and corresponding model predictions for the 3 cases shown in Figure 2. Both experts agreed that the model predictions, although not perfect, were of higher quality than the ground truth labels. Both experts indicated that the automated segmentation of non-healed wound regions was more important than segmentation of healed tissue in terms of automated wound monitoring. We note that these qualitative observations are preliminary and are not to be considered conclusive. The intention is to demonstrate issues present in both expert delineation and limitations of the baseline model. Larger scale qualitative assessment is explored later in the paper."}, {"title": "4.3. Construction of Training, Validation, and Test Sets", "content": "The aim of our work is to determine the effectiveness of a segmentation model, trained and validated only on patients with lighter skin tones, to segment wounds on patients with darker skin tones. To this end, we construct a series of datasets for use in our experiments. Our approach for this was to use all publicly available chronic wound datasets that have ground truth masks, together with all datasets that we have access to privately. Wound images were selected based on Fitzpatrick (Fitzpatrick (1988)) skin types IV (moderate brown skin), V (dark brown skin), and VII (deeply pigmented dark brown or black skin). To create the first test set (test set A), we gathered all images with masks exhibiting darker skin tones from the DFUC2022 dataset (68 images and corresponding masks from the training and test sets), the AZH dataset (81 images and corresponding masks from the training and test sets), the CWDB dataset (3 images and corresponding masks), and the FUSC dataset (190 images and corresponding masks from the training and validation sets). Test set A comprises all publicly available wound images with segmentation masks from patients with dark skin tones. To create the new training set, we combined the remaining DFUC2022 training and test sets (3893 images and corresponding masks) with 824 images and corresponding masks from the AZH training and test sets. For the validation set, we use the remaining 173 AZH images and corresponding masks together with all 24 CWDB images and masks, all 795 FUSC training and validation images and masks, and all 188 WoundsDB images and masks. Finally, we created a second test set (test set B) which comprises the same number of images as test set A (n = 342) and includes only dark skin tone wound images which do not have ground truth masks which will be assessed qualitatively. Test set B includes wound images from the Alzubaidi dataset (n = 52), the Fitzpatrick17k dataset (n = 4), the FUSC test set (n = 35), the GIS-W dataset (n = 13), the Medetec dataset (n = 8), the Wseg dataset (n = 115), and the KSUMC dataset (n = 115). A summary of the dataset composition for training, validation, and testing (test set A) is show in Table 3. A summary of test sets A and B is shown in Table 4."}, {"title": "4.4. HarDNet-DFUS Architecture", "content": "Following the analysis of our baseline results", "types": "one with MLPs independently applied to image patches for the purpose of mixing per-location features, and a second using MLPs which are"}]}