{"title": "CHOICES ARE MORE IMPORTANT THAN EFFORTS: LLM\nENABLES EFFICIENT MULTI-AGENT EXPLORATION", "authors": ["Yun Qu", "Boyuan Wang", "Yuhang Jiang", "Jianzhun Shao", "Yixiu Mao", "Cheems Wang", "Chang Liu", "Xiangyang Ji"], "abstract": "With expansive state-action spaces, efficient multi-agent exploration remains a\nlongstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts brought by\nexploration without proper guidance choices poses a practical issue for the commu-\nnity. This paper introduces a systematic approach, termed LEMAE, choosing to\nchannel informative task-relevant guidance from a knowledgeable Large Language\nModel (LLM) for Efficient Multi-Agent Exploration. Specifically, we ground\nlinguistic knowledge from LLM into symbolic key states, that are critical for task\nfulfillment, in a discriminative manner at low LLM inference costs. To unleash the\npower of key states, we design Subspace-based Hindsight Intrinsic Reward (SHIR)\nto guide agents toward key states by increasing reward density. Additionally, we\nbuild the Key State Memory Tree (KSMT) to track transitions between key states\nin a specific task for organized exploration. Benefiting from diminishing redundant\nexplorations, LEMAE outperforms existing SOTA approaches on the challenging\nbenchmarks (e.g., SMAC and MPE) by a large margin, achieving a 10x acceleration\nin certain scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploration stands as a fundamental issue in reinforcement learning (RL) (Du et al., 2023; Liu et al.,\n2023). Researchers have developed several exploration strategies directed by novelty, diversity, or\nuncertainty (Linke et al., 2020; Burda et al., 2018b; Pathak et al., 2017), mainly in single-agent\nreinforcement learning. However, these methods may induce task-irrelevant redundant exploration,\nespecially in complex environments (Du et al., 2023). In the realm of Multi-Agent Reinforcement\nLearning (MARL), the need to mitigate exploration redundancy becomes even more urgent due to the\nchallenges like exponential expansion of the state-action spaces. Widespread real-world applications,\nincluding MOBA games (Qu et al., 2023), social science (Jaques et al., 2019), and multi-vehicle\ncontrol (Xu et al., 2018), further underscore the growing need for efficient multi-agent exploration.\nThis work identifies task-relevant guidance as an important consideration in enhancing exploration\nefficiency. Incorporating priors in exploration mechanism design, such as complex reward structures,\ntypically requires expert knowledge and substantial human efforts (Liu et al., 2023; Abbeel & Ng,\n2004). Hopefully, recent advances have witnessed the remarkable reasoning and planning capabilities\nof Large Language Models (Touvron et al., 2023; Achiam et al., 2023), providing a plausible choice\nto facilitate efficient exploration through LLM's effortless prior provision. However, it is non-trivial\nto effectively comprise linguistic LLM priors into symbolically represented RL tasks (Peng et al.,\n2023; Carta et al., 2023), and the investigation of practical ways to avoid nuisances caused by such\nan expression discrepancy is of critical importance.\nIn response to the above issue, we propose LEMAE, a novel framework to enable efficient multi-\nagent exploration with LLM. The framework primarily consists of two components: (i) key states\nlocalization with LLM and (ii) key state-guided exploration. The first component grounds linguistic\nknowledge from LLM into symbolic key states by automatically localizing key states that are\nessential for task fulfillment. Specifically, the discriminator function induced by LLM works to"}, {"title": "2 PRELIMINARY", "content": "The environments considered in this work are characterized as a decentralized partially observable\nMarkov decision process (Dec-POMDP) (Oliehoek et al., 2016) with n agents, which can be defined\nas a tuple $\\mathcal{G} = (S, A, I, P, r, Z, O, \\eta, \\gamma)$, where $s \\in S$ is the global state, A is the action space for\neach agent, and $\\gamma \\in [0, 1)$ is the discount factor. At time step t, each agent $i \\in I = \\{1, ..., n\\}$ has its\nlocal observations $o^i \\in O$ drawn from the observation function $Z(s, i) : S \\times I \\rightarrow O$ and chooses\nan action $a^i \\in A$ by its policy $\\pi^i (a^i|o^i) : O \\rightarrow \\Delta([0, 1]|A|)$, forming a joint action $a \\in A = A^n$.\n$T(s'|s, a) : S \\times A \\times S \\rightarrow [0, 1]$ is the environment's state transition distribution. All agents share a\ncommon reward function $r(s, a) : S \\times A \\rightarrow R$. The agents' joint policy $\\pi := \\prod_{i=1}^{n} \\pi^i$ induces a\njoint action-value function: $Q^{\\pi}(s, a) = E[R|s, a]$, where $R = \\sum_{t=0}^{\\infty} \\gamma^t r_t$ is the expected discounted\nreturn. The goal of MARL is to find the optimal joint policy $\\pi^*$ such that $Q^*(s, a) \\geq Q^{\\pi}(s, a)$,\n$\\forall \\pi$ and $(s, a) \\in S \\times A$. Notably, we specifically focus on sparse reward tasks, i.e., $r_t = 1$ only when\n$s_{t+1} = s_{success}$, otherwise $r_t = 0$. We denote the symbol for the i-th key state by $k_i$ together with\nits discriminator function $F_i$."}, {"title": "3 RELATED WORKS", "content": "LLM in Decision Making. Large Language Models have showcased impressive capabilities across\nvarious downstream tasks (Touvron et al., 2023; Radford et al., 2019; Brown et al., 2020). Recent\nadvances indicate a growing trend of using LLM in decision-making problems (Wang et al., 2023b).\nA primary challenge within this domain is grounding LLM's linguistic knowledge into specific\nlow-level control tasks typically represented in symbolic form (Peng et al., 2023; Carta et al., 2023),\nespecially in RL. Creating linguistic twin tasks (Carta et al., 2023) are intuitive but require substantial\nmanual workloads. Some works employ LLMs as high-level planners, e.g., coding with APIs (Liang\net al., 2023), using human-annotated or LLM-summarized action template (Yao et al., 2022; Shinn\net al., 2023; Lin et al., 2023; Zhu et al., 2023; Wang et al., 2023a). Despite significant progress, they\nrely on difficult-to-obtain low-level policies or APIs, limiting their real-world applicability. Recently,\nLLMs have been integrated with RL to directly enhance low-level decision making (Cao et al., 2024).\nLLMs can act as environmental information processors, reducing learning complexity (Paischer et al.,\n2022; 2024; Kim et al., 2024; Wang et al., 2024), but cannot directly facilitate efficient exploration.\nSome works utilize LLMs as goal selectors in goal-conditioned RL (Su & Zhang, 2023; ?) or teacher\npolicy (Zhou et al., 2023) but require predefined skills or subgoals. Alternative methods like LLM-\nbased reward or policy design (Ma et al., 2023; Kwon et al., 2023; Song et al., 2023; Liu et al., 2024;\nChen et al., 2024) and fine-tuning (Carta et al., 2023; Shi et al., 2023) are either limited to simple\ntasks with sufficient information or demand enormous data and resources. ELLM (Du et al., 2023)\naims to enhance exploration using LLM but depends on predefined symbolic observation captioner\nand frequent LLM inferences. Its semantic similarity-based rewards may also struggle to generalize\nacross diverse scenarios. In contrast, LEMAE integrates linguistic LLM priors into symbolic states\nwith minimal task-specific information and LLM inference costs, achieved by localizing key states in\nrollout trajectories using LLM-generated discriminator functions.\nEfficient Multi-Agent Exploration. Exploration efficiency has long been a focal point in\nRL (Thrun, 1992; Cai et al., 2020; Seo et al., 2021; Mahajan et al., 2019; Jeon et al., 2022; Liu\net al., 2021b). Typical exploration methods focus on random exploration (Mnih et al., 2013; Rashid\net al., 2018) or heuristic indicators, such as diversity or novelty, to facilitate exhaustive exploration,\nparticularly in single agent exploration (Linke et al., 2020; Burda et al., 2018b; Pathak et al., 2017;\nBurda et al., 2018a; Bellemare et al., 2016). Despite their success, they may induce notable redundant\nexploration due to a lack of task-relevant guidance (Du et al., 2023). The exponential expansion of the\nstate-action spaces renders exhaustive exploration impractical in multi-agent settings. Consequently,\nefficient multi-agent exploration (MAE) becomes increasingly imperative and necessary (Jeon et al.,\n2022; Liu et al., 2021b). MAE is also challenging due to the complex configurations, e.g., the\nentangled effect of multi-agent actions and intricate reward design (Liu et al., 2023; Qu et al., 2023;\nXu et al., 2023). Given our emphasis on efficient exploration, we prioritize evaluation in multi-agent\nsettings. Some MAE methods encourage influential behaviors during agent interactions (Liu et al.,\n2023; Jaques et al., 2019; Wang et al., 2019a). Nevertheless, they may lead to unintended coalitions\nor require additional priors (Liu et al., 2023). Certain studies leverage subgoals to guide explo-\nration (Jeon et al., 2022). However, due to challenges in integrating task-related information into\nsubgoals, they either necessitate human expertise for subgoals design (Tang et al., 2018; Kulkarni\net al., 2016) or struggle to identify useful subgoals (Jeon et al., 2022; Liu et al., 2021b). Distinguished\nfrom the above, this work underscores the significance of task-relevant guidance in exploration and\nutilizes the key state priors extracted from LLM to enable efficient multi-agent exploration."}, {"title": "4 METHOD", "content": "This section elaborates on the developed LEMAE. The concept of the key states is first induced as\nthe task-relevant guidance in Sec. 4.1. Centering around the key states, we construct two components:\n(i) key states localization with LLM (Sec. 4.2) and (ii) key state-guided exploration (Sec. 4.3). The\nformer directs LLM to generate discriminator functions for localizing key states in rollout trajectories,\nwhile the latter guides and organizes exploration with the introduced Subspace-based Hindsight\nIntrinsic Reward and Key States Memory Tree. Please refer to Fig. 2 and Algorithm 1 for details.\nAlso, we provide a demonstration\u00b9 to clarity the LEMAE's execution pipeline."}, {"title": "4.1 DEVIL IS IN THE KEY STATES", "content": "Previous methods suffer from redundant exploration efforts in pursuing task-agnostic novelty (Du\net al., 2023), potentially reducing training efficiency. This motivates us to integrate task-relevant\ninformation as a better guidance choice for efficient exploration. Nevertheless, practical proposals\nare limited in the field. This work identifies the Key States as the novel task-relevant prior, which\ncorresponds to intermediate states with explicit semantics and expressions pertaining to the task.\nMeanwhile, Proposition 4.1 explicitly reflects the efficacy of incorporating them.\nProposition 4.1. Consider the one-dimensional asymmetric random walk problem, where an agent\nstarts at x = 0 and aims to reach x = N \u2208 N+, N > 1. The initial policy is asymmetric and random\nwith probabilities p\u2208 (0.5, 1) and 1 \u2013 p for right and left movements, respectively. Without prior\nknowledge, the expected first hitting time is $E(T_{0 \\rightarrow N}) = \\frac{N}{2p-1}$. After introducing the task-relevant\ninformation that the agent must first reach key states \u03ba = 1, ..., N \u2212 1 before reaching x = N, we can\ndecrease the expected first hitting time by $E(T_{0 \\rightarrow N}) - E(T_{P_N}) = (N - 1) * \\frac{p}{2p-1} + 1 > 0$.\nThe proof is deferred to Appendix C. The exploration policy substantially benefits from the involve-\nment of key states, e.g., $E(T_{0 \\rightarrow N}) - E(T_{P_N}) \\rightarrow \\infty$ with $p \\rightarrow 0.5$. Such a concept is also commonly\nseen in practical scenarios, such as in-game checkpoints (Demaine et al., 2016) and landmarks in\nnavigation (Becker et al., 1995)."}, {"title": "4.2 KEY STATES LOCALIZATION WITH LLM", "content": "To reduce manual workload, we employ LLM to localize key states. Although generating the\naforementioned symbolic key states can be straightforward, LLM's weakness in comprehending\nsymbolic states or environment details necessitates additional information in certain tasks and can\nlead to errors and hallucinations that are difficult to detect. Here, we stress the importance of LLM's\ndiscriminative ability to localize key states in rollout trajectories to better leverage LLM's general\nknowledge. The rationale is that discrimination demands only a high-level task understanding and is\nmore reliable and universal than naive generation, as discussed in detail in Appendix B.1.\nTo discriminate key states, we prompt LLM to generate m discriminator functions $\\{F_i\\}_{i=1}^{m}$, as\ndepicted in Fig. 2. Each discriminator function $F_i$ takes in the state st at timestep t and outputs a\nboolean value to tell whether the input state is the key state ki. Such an approach systematically\nannotates each state in trajectories as a key state instance or not. Notably, LEMAE injects task-\nrelevant information into the symbolic states without predefined components such as observation\ncaptioners (Du et al., 2023) or environment codes (Xie et al., 2023), which require manual fine-\ntuning, may be unavailable in many scenarios, or could introduce extra information. In addition, the\ndiscriminator functions' reusability avoids frequent calls, and our method empirically requires fewer"}, {"title": "4.3 KEY STATE-GUIDED EXPLORATION", "content": "4.3.1 SUBSPACE-BASED HINDSIGHT INTRINSIC REWARD\nWith the annotated key states, trajectories can naturally be segmented into sub-trajectories. Drawing\ninspiration from Andrychowicz et al. (2017), we integrate hindsight intrinsic rewards by conceptualiz-\ning the annotated key states as sub-trajectories' subgoals, which is further discussed in Appendix E.3.\nSuch integration guides the policy toward achieving these key states by increasing reward density,\nthus reducing manual reward design burdens. Moreover, the state vector index from the discriminator\nfunction constitutes the reward-related subspace of the state (Liu et al., 2021b). Here, we write the\nSubspace-based Hindsight Intrinsic Reward (SHIR) function as:\nr_m (t) = ||\u03a6_m (s_t ) \u2212 \u03a6_m (k_m )|| \u2212 ||\u03a6_m (s_{t+1} ) \u2212 \u03a6_m (k_m )||,\nwhere || || denotes a distance metric, e.g., Manhattan Distance; \u03a6_m (s) = (s_e )_{e\u2208V_m } restricts the state\nspace to elements $e \\in V_m$, $s_e$ is the e-th element of the full-state s, and $V_m \\subset \\mathbb{N+}$ refers to the subset\nof entire state space from the discriminator function $F_m$.\nGiven that rewards generally rely on a limited subset of the entire state space (Liu et al., 2021b;\nTodorov et al., 2012), adopting subspace-based rewards helps avoid the potential redundancy and bias\nassociated with the design of intrinsic rewards in the entire state space. LEMAE is also applicable to\nscenarios where rewards depend on the global state space, as it imposes no strict constraints. Hence,\nthe final reward function is further derived as:\nr(t) = \u03b1 \u00b7 re (t) + \u03b2 \u00b7 r_m (t),\nwhere re denotes the extrinsic reward with \u03b1, \u03b2\u2208 R+ non-negative scaling factors.\n4.3.2 KEY STATES MEMORY TREE\nTo organize exploration with memory, we introduce the concept of Key States Memory Tree (KSMT).\nIt tracks transitions between key states and further serves exploration and planning. Compared\nwith the naive e-greedy method, gradually revealing the KSMT helps avoid redundant exploration\nthroughout the state space, particularly beneficial in more complicated real-world scenarios. Notably,\nLEMAE is compatible with other memory structures, such as Directed Acyclic Graphs.\nConstruct KSMT: Initialized at the root node, KSMT dynamically expands by iteratively incorporat-\ning key state chains obtained from annotated trajectories, as outlined in Algorithm 2. These steps\nrepeat until either reaching the success state or fully depicting the transitions between key states.\nExplore with KSMT: To discover new KSMT branches, we adopt an exploration strategy that\nbalances high-randomness policy \u03c0 for exploring under-explored nodes with low-randomness"}, {"title": "5 EXPERIMENTS", "content": "We conduct experiments on commonly used multi-agent exploration benchmarks: (1) the Multiple-\nParticle Environment (Lowe et al., 2017; Wang et al., 2019a) and (2) the StarCraft Multi-Agent\nChallenge (Samvelyan et al., 2019b). Following previous studies (Ma et al., 2023; Liu et al., 2021b;\nXu et al., 2023), we focus primarily on tasks with symbolic state spaces and use the sparse reward\nversion for all tasks without specific instructions.\nBaselines. We compare LEMAE with representative baselines: IPPO is a MARL algorithm which\nextends PPO (Schulman et al., 2017); QMIX (Rashid et al., 2018) is a widely adopted MARL\nbaseline; EITI and EDTI (Wang et al., 2019a) employ the impact of interaction in coordinated\nagents' behaviors; MAVEN (Mahajan et al., 2019) combine value-based and policy-based approaches\nthrough hierarchical control; CMAE (Liu et al., 2021b) learns cooperative exploration by selecting\nshared goals from multiple projected state space; RODE (Wang et al., 2020d) decomposes joint\naction spaces into role-based ones to enhance exploration; MASER (Jeon et al., 2022) generates\nsubgoals automatically for multiple agents from the experience replay buffer; LAIES (Liu et al.,\n2023) addresses the lazy agents problem by mathematical definition and causal analysis. ELLM (Du\net al., 2023) employs LLM priors to guide vision-based exploration, using state captioners and\nsemantic similarity-based rewards. LEMAE is implemented on IPPO in MPE and QMIX in SMAC,\nconsistent with previous works (Wang et al., 2019a; Liu et al., 2023; Jeon et al., 2022) to ensure fair\ncomparisons.\nWe run each algorithm on five random seeds and report the mean performance with standard deviation.\nFurther details can be referenced in Appendix E."}, {"title": "5.1 MULTIPLE-PARTICLE ENVIRONMENT (MPE)", "content": "In MPE, we evaluate LEMAE on Pass, Secret-Room, Push-Box, and Large-Pass, which are commonly\nused multi-agent exploration tasks in previous works (Wang et al., 2019a; Liu et al., 2021b)."}, {"title": "5.2 STARCRAFT MULTI-AGENT CHALLENGE (SMAC)", "content": "SMAC is a widely-used challenging benchmark in MARL. In contrast to dense or semi-sparse reward\nversions used before, we employ fully sparse-reward tasks to emphasize exploration, rewarding\nagents only upon complete enemy elimination. In addition, to validate LEMAE across diverse\nscenarios, we conduct experiments on six maps with varied difficulty and agent numbers.\nIn Fig. 5, LEMAE demonstrates superior performance over all baselines. Although baselines QMIX,\nMAVEN, CDS, and MASER excel in dense or semi-sparse reward settings, they struggle in fully\nsparse reward scenarios. CMAE shows partial efficacy in simpler tasks but fails in harder scenarios\ndue to the lack of task-related information in curiosity-driven goal selection. LAIES is the only non-\nLLM baseline comparable to LEMAE. However, it requires handcrafted external state priors and still\nunderperforms compared to LEMAE, especially on more challenging tasks. ELLM, benefiting from\nLLM priors, performs well on simpler tasks, but its effectiveness diminishes on harder ones, likely\ndue to the instability and less reliable guidance of semantic similarity-based rewards. Notably, we add\nQMIX-DR, which augments QMIX with dense rewards in the original SMAC. Surprisingly, LEMAE\ndemonstrates the potential to match or even surpass QMIX-DR, particularly in hard maps, shedding\nlight on minimizing the manual workload in complex reward design in real-world scenarios. Given\nthe complexity of the SMAC benchmark, the consistent superiority of LEMAE confirms its potential\napplicability in more complex real-world scenarios. We further evaluate LEMAE on SMACv2 (Ellis\net al., 2024), an enhanced version with more stochasticity, as detailed in Appendix F.1."}, {"title": "5.3 COMPATIBLILITY WITH VARIOUS ALGORITHMS", "content": "LEMAE incorporates task-relevant guidance in the form of intrinsic rewards and is agnostic to RL\nalgorithms. Sec. 5.1 and 5.2 have verified the compatibility through implementing on two distinct\nMARL algorithms: IPPO in MPE and QMIX in SMAC. To further substantiate this claim, we\nbuild our method on two widely-used MARL algorithms, namely QPLEX (Wang et al., 2020a)\nand VMIX (Su et al., 2021), adopting a value-based and actor-critic methodology respectively.\nAs illustrated in Fig. 6a, algorithms combined with LEMAE consistently improve performance,\nunderscoring the potential of LEMAE to integrate with alternative algorithms across diverse fields\nin the future. Additionally, LEMAE is a versatile approach for efficient exploration, not limited to\nMARL. To validate this assertion, we conduct further evaluations of LEMAE in a single-agent variant\nof MPE, as demonstrated in Appendix F.4."}, {"title": "5.4 ABLATION STUDIES", "content": "Role of SHIR and KSMT. We conduct an ablation study to assess the significance of KSMT and\nSHIR within LEMAE. We select two exemplary tasks from MPE and SMAC and report results\nin Fig. 6b. In SMAC, Base refers to QMIX, while in MPE, it denotes IPPO. Besides, SHIR\nrepresents subspace-based hindsight intrinsic reward, KSMTE signifies exploration with KSMT,\nKSMTP denotes planning with KSMT, and LEMAE encompasses Base+SHIR+KSMTE+KSMTP.\nAs illustrated, the absence of SHIR or KSMT significantly deteriorates performance, revealing both\ncomponents' pivotal roles in achieving effective key state-guided exploration.\nRole of Self-Check mechanism and LLMs. We conduct a comparative analysis between GPT-4-\nturbo and GPT-3.5-turbo regarding generating discriminator functions. Meanwhile, we investigate the\nperformance of GPT-4-turbo without the Self-Check mechanism (GPT-4-turbo w/o). The Acceptance\nRate (racc) denotes the proportion of seeds achieving over 80% of the best performance after RL\ntraining, while the Execution Rate (rexe) indicates the proportion of seeds for which all discriminator\nfunctions are executable. As depicted in Table 1, the results demonstrate that a powerful LLM\nwith our Self-Check mechanism effectively ensures the high quality of key states, as evidenced by\nthe code's executability and the final performance. The scalability of LEMAE to LLM and our\nSelf-Check mechanism promise that LEMAE can leverage more powerful LLMs in the future and be\napplied to more challenging real-world tasks safely and efficiently."}, {"title": "5.5 SENSITIVITY & ROBUSTNESS ANALYSIS", "content": "Sensitivity to Hyperparameters. We conduct experiments on the pivotal hyperparameters in\nLEMAE, i.e., reward scaling rates \u03b1 and \u03b2. The x-axis represents the relative values with respect\nto the default (\u03b1 = 10, \u03b2 = 1), encompassing evaluations for \u03b1 \u2208 {1, 5, 10, 50, 100} and \u03b2\u03b5\n{0.1, 0.5, 1, 5, 10}. Fig. 7 illustrates that LEMAE is robust to these hyperparameters across a\nconsiderable range. Notably, excessive extrinsic reward scaling rate \u03b1 or insufficient intrinsic reward\nscaling rate \u03b2 can cause performance degradation due to the abrupt alteration of the reward or the"}, {"title": "5.6 SCALABILITY & GENERALIZATION ANALYSIS", "content": "To rule out the possibility that LEMAE's success\nrelies on LLM's familiarity with the chosen tasks,\nwe've handcrafted a brand new task, termed River,\nwhich LLM has never encountered before. The task\nis illustrated in Fig. 8a, where the objective is for\nBob to help Alice, who is afraid of water, cross two\nrivers to reach the bottom-right corner. As shown\nin Fig. 8b, LEMAE outperforms the baselines, and\nthis confirms LLM's generalization capabilities to\nempower LEMAE's effectiveness in promoting effi-\ncient exploration in diverse new tasks. Please refer\nto Appendix E.4.3 for details on the task.\nAdditionally, we extend LEMAE to a vision-based\ntask, as described in Appendix F.2, demonstrating\nthe scalability potential of LEMAE."}, {"title": "6 CONCLUSION", "content": "Summary of This Work: We present LEMAE, a novel framework that benefits multi-agent explo-\nration with task-specific guidance from LLM. LEMAE executes the key states localization with LLM\nand enables the key state-guided exploration to improve sample efficiency. In this way, we can (i)\nbuild up connections between LLM and RL to ground linguistic knowledge into decision-making, (ii)\nreduce the manual workload in accessing knowledge and intensive inference calls from LLM, and\n(iii) significantly boost exploration efficiency through guided and organized exploration. Extensive\nexperiments further examine the effectiveness of LEMAE in typical benchmarks.\nLimitations & Future Investigations: In developing LEMAE, we made efforts to compensate for\nthe pitfalls of concurrent LLMs, e.g., careful preparation for prompt engineering and task-related\nprior provision to avoid the nuisances in LLM usages. All of these can be circumvented with the\nprogress of LLM's capability enhancement. This work paves the way for LLM-empowered RL to\nachieve the potential in complicated decision-making scenarios."}, {"title": "A ALGORITHM", "content": "This section includes the pseudo algorithms. Algorithm 1 presents LEMAE's main algorithm.\nLEMAE consists of four phases: generating discriminator functions with LLM, exploring with\nKSMT, calculating SHIR, and performing RL training. For on-policy RL, the buffer D corresponds\nto a rollout buffer, while for off-policy RL, it is initialized as a replay buffer (Paischer et al.,\n2022). Algorithm 2 illustrates the process of exploring with KSMT. As our approach is agnostic to\nreinforcement learning algorithms, we leave out the details of standard RL training in the main paper."}, {"title": "B FURTHER DISCUSSIONS", "content": "B.1 THE INSIGHTS BEHIND KEY STATES DISCRIMINATION\nIn our considered scenarios, we claim that discrimination is generally easier and more universal\nthan key state generation by LLM, particularly in the context of high-dimensional states and partial\nobservability. The reasons are as follows:\n1. Discrimination focuses on high-level task understanding and identifying key state charac-\nteristics, while generation requires detailed, low-level comprehension, assigning values to\neach element. This makes generation more challenging and error-prone, particularly in\nhigh-dimensional settings. Discrimination equivalently simplifies the output space to key\nstate labels, thus alleviating issues like hallucinations.\n2. In implementations, errors in discriminator functions are easier to examine and correct\nthrough testing with real states. In contrast, errors in generated key states are harder to detect\nand are typically inferred from training performance.\n3. In cases of partial observability, generating key states directly is unreliable. For example, in\nthe Pass task, the positions of hidden switches are unknown and must be inferred from the\ndoor's status. LLM cannot generate key states accurately without knowledge of the specific\nagents' positions required to activate a switch.\nB.2 LIMITATIONS\nWe build a bridge between LLM and RL to facilitate efficient exploration by leveraging task-related\nguidance provided by LLM. However, persistent constraints inherent to LLMs, such as their limited"}, {"title": "B.3 FUTURE WORKS", "content": "The success of the proposed LEMAE highlights the necessity and efficacy of empowering RL with\nLLM. To enhance performance and extend applicability, we will explore two avenues for future\nresearch aimed at addressing the identified limitations. These avenues are outlined as follows:\n1. Streamlining the task information provision through multi-modal self-collection: Multi-\nmodal LLMs are garnering increasing attention for their ability to comprehend situations\nthrough various modalities. Incorporating them with self-exploration and memory mecha-\nnisms shows promise in automating the collection and understanding of task information,\nthereby streamlining the implementation and enhancing the adaptability of LEMAE. We\nprovide an initial attempt to extend LEMAE beyond symbolic tasks in Appendix F.2.\n2. Unleashing the power of better LLM with an iterative feedback mechanism: Undoubtedly,\ngiven the rapid pace of LLM development, the emergence of more powerful LLMs is\nimminent. On one hand, we intend to harness the capabilities of these advanced LLMs.\nOn the other hand, to fully unleash the potential of LLMs, we plan to devise an iterative\nfeedback mechanism to feedback LLM in LEMAE during RL training to mitigate issues\nlike hallucinations and errors in task understanding."}, {"title": "B.4 BROADER IMPACTS", "content": "Large Language Models have demonstrated considerable potential in showcasing impressive ca-\npabilities across various downstream tasks. However, research on empowering RL with LLMs is\nstill nascent. As a pioneering endeavor to empower RL with LLM, we propose a general approach\nfacilitating efficient exploration in RL with task-specific guidance from LLM.\n1. For the research community, the publication of this work will inspire further exploration\ninto encouraging the integration of LLMs with RL to address the inherent challenges in RL,\nsuch as efficient exploration, limited sample efficiency, and unsatisfactory generalization.\nAdditionally, our design promotes the application of discrimination and coding to ground\nlinguistic knowledge from LLMs into symbolic tasks.\n2. LEMAE shows promise for real-world deployment in scenarios requiring efficient explo-\nration, such as autonomous vehicle control and robot manipulation. Moreover, as LLM is\ngrowing by leaps and bounds, it is foreseeable that LEMAE can be applied to more chal-\nlenging real-world tasks by taking advantage of more powerful LLM. Notably, to mitigate\npotential risks, it is imperative to conduct LLM generation and RL training under human\nsupervision, thereby ensuring undesirable outcomes are averted."}, {"title": "C PROOF OF PROPOSITION 4.1", "content": "Proof. Random walk is a fundamental stochastic process", "as": "Mo = 0"}, {"as": "nS_n = \\sum_{i=1"}, {"define": "nY_n = \\sum_{i=1}^{n} (M_i - (2p - 1)), Y_0 = 0\nIt's easy to prove that\nE\\overline{Y_n} = \\sum_{i=1}^{n} E\\overline{[M_i - n(2p - 1) = 2n - 2np < \\infty\nE(Y_{n+1"}]}