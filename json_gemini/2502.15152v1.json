{"title": "Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation", "authors": ["Ebenezer Tarubinga", "Jenifer Kalafatovich Espinoza"], "abstract": "Semi-supervised semantic segmentation (SSSS) aims to improve segmentation performance by utilising unlabeled data alongside limited labeled samples. Existing SSSS methods often face challenges such as coupling, where over-reliance on initial labeled data leads to suboptimal learning; confirmation bias, where incorrect predictions reinforce themselves repeatedly; and boundary blur caused by insufficient boundary-awareness and ambiguous edge information. To address these issues, we propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact of incorrect predictions, we assign confidence weights to pseudo-labels. Additionally, we leverage boundary-delineation techniques, which, despite being extensively explored in weakly-supervised semantic segmentation (WSSS) remain under-explored in SSSS. Specifically, our approach: (1) reduces coupling through a confidence-weighted loss function that adjusts the influence of pseudo-labels based on their predicted confi-dence scores, (2) mitigates confirmation bias with a dynamic thresholding mechanism that learns to filter out pseudo-labels based on model performance, (3) resolves boundary blur with a boundary-aware module that enhances segmentation accuracy near object boundaries, and (4) reduces label noise with a confidence decay strategy that progressively refines pseudo-labels during training. Extensive experiments on the Pascal VOC 2012 and Cityscapes demonstrate that our method achieves state-of-the-art performance. Moreover, using only 1/8 or 12.5% of labeled data, our method achieves a mIoU of 75.81 on Pascal VOC 2012, highlighting its effectiveness in limited-label settings.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation, the task of assigning semantic labels to each pixel in an image, is fundamental to applications like autonomous driving, medical imaging, and scene understanding [10]. However, its success heavily relies on large-scale annotated datasets, which are expensive and labour-intensive to produce [2], creating significant challenges for adapting models to new datasets with limited labeled data [1]. To address this issue, researchers have increasingly focused on semi-supervised semantic segmentation (SSSS), which uses both labeled and unlabeled data through methods such as self-training (ie. pseudo-labeling) [14], consistency-based regularization [13], and generative models [22] to improve model generalisation and address the limitations of fully supervised techniques.\nEarly semi-supervised methods primarily used self-training due to its simplicity and effectiveness. In self-training, a teacher model is trained on a small set of labeled data to generate pseudo-labels for a larger set of unlabeled data. These pseudo-labels are later used to retrain the student model, effectively expanding the training dataset [3]. This approach has the ability to use large amounts of unlabeled data to learn more generalized features and improve the model's robustness against overfitting and has proven especially useful in data-scarce domains like medical imaging [20]. Building on self-training, more sophisticated methods such as consistency, regularization, and generative models [22] [42] have been proposed. Consistency-based methods ensure that the model predictions remain stable under various input data perturbations, such as spatial transformations or intensity variations, promoting greater generalizations [22] [40]. Furthermore, generative models, such as Generative Adversarial Networks (GANs), have been used to synthesize training data or improve the quality of pseudo-labels derived from unlabeled data, thereby increasing the effectiveness of semi-supervised semantic segmentation [11].\nDespite these advancements, issues such as coupling, confirmation bias and boundary delineation still persist due to label noise and limited labeled data [4]. The coupling problem occurs when the model's predictions on unlabeled data are overly reliant on the initial labeled dataset, resulting in performance that is strongly correlated with the quality of the pseudo-labels generated. Previous studies have addressed this issue by proposing the use of exponential moving averages [5] and consistency regularisation [6]. However, these strategies frequently rely on fixed confidence levels that do not adapt to the model's learning dynamics over time. This inflexibility can result in the premature exclusion of valuable low-confidence samples, reducing training diversity, and limiting generalization. Confirmation bias complicates this scenario by causing models to favour predictions that align with their initial biases, creating a feedback loop where errors in pseudo-labels are repeatedly reinforced, ultimately degrading model performance. In [29], the authors demonstrate that generating pseudo-labels under significant intra-class variation frequently results in poor representation of the data distribution, reducing generalisation.\nBoundary blur occurs when a model fails to accurately recognize the correct category of pixels at object boundaries, reducing segmentation performance. This problem is most common in weakly-supervised semantic segmentation (WSSS), where boundary-delineating methods have been proposed to reduce the effects of inaccurate pixel-level annotations [29] [41]. However, despite the extensive research of these techniques in WSSS, their application in semi-supervised semantic segmentation (SSSS) remains relatively under-explored, highlighting a critical gap in the current research landscape [32].\nTo tackle these limitations, we propose a novel dual-stage training framework that integrates dynamic confidence and boundary-delineation methods. We specifically solve the aforementioned issues and achieve good segmentation performance as demonstrated in Fig. 1. Our approach achieves good segmentation performance compared to SOTA methods, ST++ [9] and UniMatch [36] on the Pascal VOC dataset.\nOur key contributions are summarised as follows:\n1) Confidence-Weighted Loss Function: We apply a novel confidence-weighted cross-entropy loss function that adjusts pixel contribution to the total loss based on confidence score, enabling more reliable predictions while learning from less certain ones.\n2) Dynamic Thresholding Mechanism: We introduce an adaptive thresholding mechanism that updates the pseudo-label confidence threshold according to the model's training performance, ensuring accurate predictions without discarding potentially valuable low-confidence data.\n3) Boundary Aware Technique: We introduce a boundary-aware technique module to enhance the model's ability to learn intricate, fine-grained details, significantly improving segmentation accuracy, especially near object boundaries.\n4) Confidence Decay Strategy: We employ a confidence decay strategy that progressively reduces the influence of low-confidence pseudo-labels during training, promoting exploration early on and focusing on high-confidence predictions as the model stabilizes."}, {"title": "II. RELATED WORK", "content": "Pseudo-labeling is an important method in semi-supervised learning (SSL), where model predictions on unlabeled data become pseudo-labels for subsequent training [14]. Pseudo-labeling, although effective, is vulnerable to error propagation, whereby inaccurate pseudo-labels aggravate model biases and reduce performance [15]. Consequently, teacher-student frameworks attempt to solve this issue by enhancing the stability of pseudo-labeling through the use of an exponential moving average (EMA) of the students' weights [16]. Advanced self-training methods such as ST++ [9] refine the teacher model iteratively using pseudo-labels generated by the student model. Likewise, PrevMatch [10] reconsiders predictions from earlier teacher models to produce more dependable pseudo-labels and address confirmation bias.\nIn this work, we extend these approaches by introducing a dynamic thresholding mechanism that adjusts to the model's evolving confidence levels during training. Unlike prior methods such as [9] and [10] which employ static thresholds, our adaptive thresholding enhances label reliability by filtering out low-confidence predictions as the model learns. Additionally, we implement a confidence decay strategy that progressively reduces the influence of noisy pseudo-labels, further mitigating the impact of incorrect predictions.\nSeveral studies rely on consistency regularization where model predictions remain stable despite input perturbations. Previous methods, including Temporal Ensembling [13] and Virtual Adversarial Training (VAT) [12] proved consistency by comparing predictions made on different versions of the same data. [8] improved the aforementioned method with teacher-based VAT (T-VAT), which uses adversarial perturbations within a teacher-student framework to create complex pseudo-labels, enhancing the generalization of the student model. Similarly, in [11], the author uses an adaptive ramp-up method to leverage two student networks that provide interactive feedback to the teacher model, enhancing pseudo-label quality and training consistency.\nTechniques such as boundary refinement modules and edge detection have been used to enhance the accuracy of object edges in weakly-supervised semantic segmentation (WSSS). For example, Class Activation Mapping (CAM) has been utilised to generate coarse localisation maps, yet it often fails to capture intricate boundary details, resulting in suboptimal segmentation results [29] [32]. On the other hand, in semi-supervised semantic segmentation (SSSS), boundary-aware methods remain underexplored. [34] proposed refining boundary delineation by dynamically generating and optimizing class prototypes near boundaries using high- and low-confidence feature clustering. Our novel approach uses Sobel filters to detect edges and create a binary boundary mask, which produces a boundary-aware loss to enhance the model's focus on fine-grained details while improving segmentation accuracy near object boundaries."}, {"title": "III. METHOD", "content": "Our proposed method, Confidence-Weighted Boundary-Aware Learning (CW-BASS) operates in two stages as shown in Fig. 2. In the first stage, the teacher model generates pseudo-labels with confidence scores for unlabeled data, which are used to calculate a confidence-weighted loss. Dynamic thresholding adjusts the confidence threshold adaptively, filtering low-confidence pseudo-labels based on training performance. In the second stage, a confidence decay strategy reduces the influence of low-confidence pixels, and a boundary-aware module enhances segmentation accuracy near object boundaries. The student model is ultimately trained using the final loss to produce segmentation results and the teacher model is updated.\nWe address the problem of semantic segmentation in a semi-supervised setting, where a labeled dataset $D_L = \\{(x_i, y_i)\\}_{i=1}^{N_L}$ and an unlabeled dataset $D_U = \\{x_i\\}_{i=1}^{N_U}$ are available, with $N_L$ and $N_U$ representing the number of samples in the labeled and unlabeled datasets, respectively.\nWe train a teacher model on $D_L$, and use it to generate high-quality pseudo-labels for $D_U$ and train a student model that improves performance on both datasets.\nFor each unlabeled image $x_i \\in D_U$, the teacher model, $f_t$ generates logits, $z_i$:\n$z_i = f_t(x_i)$.\nFrom these logits, we derive pseudo-labels, $s_i$ and pixel-wise confidence scores, $p_i$:\n$s_{i,j} = \\underset{k}{\\operatorname{argmax}} (z_{i,j,k})$, (2)\n$p_{i,j} = \\underset{k}{\\operatorname{max}} (softmax(z_{i,j,k}))$, (3)\nwhere $s_{i,j}$ is the pseudo-label for pixel $j$ in image $x_i$; $p_{i,j} \\in [0, 1]$ represents the confidence score for pixel $j$; and $k$ is the index of the possible classes.\nThe loss consists of labeled and unlabeled components:\nConfidence-Weighted Cross-Entropy Loss. To address the label noise introduced by inaccurate pseudo-labels from the teacher, we propose a confidence-weighted loss that weights the contribution of each pixel to the overall loss based on its confidence score. The pixel-wise confidence $p_i$ is raised to a power $\\gamma$ to emphasize high-confidence predictions:\nThe loss for an unlabeled image, $x_i$ is defined as:\n$L_{\\text{weighted}} = \\frac{1}{N_i} \\sum_{j=1}^{N_i} - p_{i,j}^{\\gamma} \\log \\left( \\frac{\\exp(z_{i,j,s_{i,j}})}{\\sum_k \\exp(z_{i,j,k})} \\right)$, (4)\nwhere $N_i$ is the total number of pixels in image $x_i$; $z_i = f_s(x_i)$ are the logits predicted by the student model $f_s$; and $\\gamma \\geq 0$ is the hyperparameter controlling emphasis on high-confidence predictions.\nLoss for Labeled Data. For labeled data, we use the standard cross-entropy loss:\n$L_{\\text{labeled}} = - \\frac{1}{N_L N_i} \\sum_{i=1}^{N_L} \\sum_{j=1}^{N_i} \\log \\left( \\frac{\\exp(z_{i,j,y_{i,j}})}{\\sum_k \\exp(z_{i,j,k})} \\right)$, (5)\nwhere $y_{i,j}$ is the ground truth label for pixel $j$ in image $x_i$.\nThe total loss is a combination of the losses from labeled and unlabeled data:\n$L_{\\text{total}} = \\lambda L_{\\text{labeled}} + L_{\\text{weighted}}$, (6)\nwhere $\\lambda$ is a balancing parameter between the supervised and unsupervised losses.\nWe introduce a dynamic threshold mechanism $T$ to adaptively learn and adjust the confidence threshold for filtering low-confidence pseudo-labels, based on the model's performance during training.\nAverage Confidence Calculation. For each batch, we calculate the average confidence score $\\bar{p}$:\n$\\bar{p} = \\frac{1}{N_{\\text{batch}}} \\sum_{i=1}^{N_{\\text{batch}}} \\frac{1}{N_i} \\sum_{j=1}^{N_i} p_{ij}$. (7)\nwhere $\\bar{p}$ is the average confidence.\nDynamic Threshold Adjustment. The base threshold $T$ is then adjusted using a logistic function based on the average confidence:\n$T = \\frac{T_0}{1 + e^{-\\beta (\\bar{p} - 0.5)}}$, (8)\nwhere $T_0$ is the initial confidence threshold, $\\bar{p}$ is the initial average confidence and $\\beta$ is a hyperparameter controlling the sensitivity of the threshold adjustment.\nRetaining Pseudo-labels. We retain pseudo-labels where the confidence score exceeds the threshold:\nRetain $s_{i,j}$ if $p_{i,j} \\geq T$. (9)\nWe introduce a confidence decay strategy to further reduce the influence of consistently low-confidence pixels, we introduce a confidence decay strategy that progressively reduces the influence of low-confidence pseudo-labels over time:\nFor low-confidence pixels, we update their confidence scores as:\n$p_{i,j}^{(t+1)} = \\begin{cases} p_{i,j}^{(t)} & \\text{if } p_{i,j}^{(t)} \\geq T \\\\ \\alpha \\cdot p_{i,j}^{(t)} & \\text{if } p_{i,j}^{(t)} < T \\end{cases}$ (10)\nwhere $p_{ij}^{(t)}$ is the confidence score at epoch $t$ and $\\alpha \\in (0, 1)$ is the decay factor.\nAfter the pseudo-labels are refined and filtered through confidence weighting and thresholding, we introduce boundary-aware learning to focus on the more intricate task of boundary delineation and overall improve segmentation performance.\nWe apply Sobel filters to the pseudo-labels $\\hat{y}_i$ to detect edges [30]. Let $G_x$ and $G_y$ be the horizontal and vertical gradients, respectively. The gradient magnitude at pixel, $(h, w)$ in image $n$ is:\n$E_{n,h,w} = \\sqrt{(G_{x,h,w})^2 + (G_{y,h,w})^2}$. (11)\nA binary boundary mask is then defined as:\n$BoundaryMask_{n,h,w} = \\begin{cases} 1, & \\text{if } E_{n,h,w} > 0 \\\\ 0, & \\text{otherwise}, \\end{cases}$ (12)\nwhere $E_{n,h,w}$ is the gradient magnitude at pixel $(h, w)$; $G_{x,h,w}$ $G_{y,h,w}$ are the pixel gradients at $(h, w)$ derived via Sobel filters and $BoundaryMask_{n,h,w}$ is the binary mask indicating boundaries (if $E_{n,h,w} > 0$, else 0).\nTo emphasize correct predictions at boundaries, we scale the cross-entropy loss by the boundary mask to encourage the model to focus more on object contours:\n$L_{\\text{boundary}} = \\frac{1}{N_i} \\sum_{j=1}^{N_i} CE_{i,j} \\cdot BoundaryMask_{i,j}$, (13)\nwhere $CE_{i,j}$ is the pixel-wise cross-entropy at pixel $(i, j)$ and $N_i$ is the total number of pixels in image $x_i$.\nThe final loss function combines all components:\n$L_{\\text{final}} = \\frac{1}{2} L_{\\text{labeled}} + \\frac{\\lambda}{2} L_{\\text{weighted}} + \\zeta L_{\\text{boundary}}$. (14)\nThe student model $f_s$ is trained using $D_U$ and the refined pseudo-labeled data $(x_i, \\hat{y}_i)$:\n$f_s^* \\leftarrow \\underset{f_s}{\\text{argmin}} \\sum_{x_i \\in D_U} (L_{\\text{weighted}}(f_s(x_i), \\hat{y}_i, p_i) + L_{\\text{boundary}})$. (15)\nOnce $f_s^{(t)}$ is trained, we set:\n$f_t^{(t+1)} \\leftarrow f_s^{(t)}$. (16)"}, {"title": "IV. EXPERIMENTS", "content": "For semi-supervised settings, we evaluate our method under various labeled data partitions, such as 1/16, 1/8, 1/4, and 1/2 of the full labeled dataset. We perform experiments on two benchmark datasets: Pascal VOC 2012 [23] and Cityscapes [24] with comparisons with state-of-the-art methods.\nThis dataset consists of 1,464 training images and 1,449 validation images, annotated with 21 semantic classes including the background.\nCityscapes contains 2,975 training images and 500 validation images, with high-resolution street scenes annotated into 19 classes.\nWe use the DeepLabV3+ model with ResNet-50 [38] as the backbone for all experiments, initialized with weights pre-trained on ImageNet [39].\nFor optimization, we employ stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 1 \u00d7 10-4. The learning rate (LR) and scheduler are as follows: Pascal VOC: 0.001, and for Cityscapes: 0.004 due to larger image sizes and batch adjustments.\nA polynomial learning rate decay is used, defined as:\n$LR_{\\text{current}} = LR_{\\text{initial}} X \\left( 1 - \\frac{\\text{iter}}{\\text{total\\_iters}} \\right)^{0.9}$.\nBatch size for Pascal VOC is set to 16, and for Cityscapes, due to higher resolution, is set to 8. The model is trained for 80 epochs on Pascal and 240 epochs on Cityscapes. It is also important to note, for fair comparisons with existing methods and to reduce computational costs, that our method does not use advanced optimization strategies such as OHEM, auxiliary supervision [18], or SyncBN, which are commonly used in other studies.\nWe apply similar semi-supervised training settings as most state-of-the-art-methods to ensure fair comparisons. Data augmentation techniques include random horizontal flipping and random scaling (ranging from 0.5 to 2.0) [9]. We use reduced cropping sizes during training compared to CPS [18], ie. 321 \u00d7 321 pixels on Pascal VOC and 721 \u00d7 721 pixels on Cityscapes to save memory. On unlabeled images, we also utilize color jitter with the same intensity as [25], grayscale conversion, Gaussian blur as stated in [26], and Cutout with randomly filled values. All unlabeled images undergo test-time augmentation as well [18].\nWe use the standard mean Intersection over Union (mIoU) metric to evaluate segmentation performance. No ensemble techniques where used for all evaluations.\nIn our experiments, we use specific hyperparameters with default settings, and their impact is further analyzed in the ablation studies. Confidence Weighting ($\\gamma$) is set to a default value of 1.0, to balance the contribution of high- and low-confidence pseudo-labels in the confidence-weighted loss. For Dynamic Thresholding ($T$), the base threshold ($T_0$) is 0.6, allowing the model to learn filter pseudo-labels dynamically based on its performance, thereby mitigating confirmation bias. Additionally, a sensitivity parameter ($\\beta$) with a default value of 0.5 ensures stability by preventing the thresholds from becoming excessively lenient or overly strict. Lastly, the Confidence Decay Factor ($\\alpha$) is set to 0.9, to gradually reduces the influence of low-confidence pseudo-labels over time and refine the learning process."}, {"title": "C. Quantitative Analysis", "content": "Figure 5 illustrates the convergence behavior and performance of CW-BASS compared to the self-training method ST++ [9] on the Cityscapes dataset during the initial 20 epochs, utilizing the 1/16 partition protocol. CW-BASS achieves faster stabilization due to its confidence-weighted loss, which prioritizes high-confidence predictions and mitigates the influence of noisy pseudo-labels. Furthermore, its boundary-aware learning enhances segmentation accuracy near object edges, contributing to more rapid convergence. In contrast, ST++ depends on iterative retraining after selecting reliable images post pseudo-labeling. Overall, CW-BASS integrates swift initial convergence with sustained performance improvements throughout the training process.\nTable II summarises data utilisation and training overhead for various approaches, highlighting differences in the number of networks trained, unlabeled data processed per epoch, and total training epochs. CPS has a dual network design and treats the labeled set, $D_L$ without the ground truth as additional unlabeled set, increasing the volume of training data. On the other hand, PS-MT implements consistency learning with perturbations to both teacher and student networks. CPS trains two networks, processing 10.5k and 2.9k unlabeled images per epoch on Pascal VOC and Cityscapes, respectively. PS-MT employs three networks with aggressive augmentations, often doubling or tripling unlabeled data (e.g., 19.8k samples per epoch for the 1/16 Pascal partition) and requires 320\u2013550 epochs on Cityscapes. ST++ trains four networks with slightly fewer unlabeled samples per epoch compared to PS-MT (14.8k on Pascal and 4.1k on Cityscapes, both below 1/16). In contrast, CW-BASS trains two networks with fewer augmentations, using the least unlabeled data (up to 9.9k on Pascal and 2.7k on Cityscapes) and converging in just 80 epochs on Pascal and 240 epochs on Cityscapes, regardless of partition. This balanced approach achieves competitive performance while avoiding the overheads of intensive augmentations or large network ensembles.\nOur proposed CW-BASS framework achieves state-of-the-art segmentation performance on both Pascal VOC 2012 and Cityscapes, as shown in Table I.\nTable I compares performance on the Pascal VOC 2012 dataset using various state-of-the-art (SOTA) semi-supervised semantic segmentation methods. Unlike approaches that use multiple teacher networks and various perturbations, our method relies on a single model for self-training. To highlight our performance, we also specifically compare our results with ST++, an advanced and simple self-\nUsing only 1/8 (12.5%) of the labeled training data, our method produces a mean Intersection over Union (mIoU) of 75.81%, outperforming existing state-of-the-art approaches. This also exceeds the performance of the fully supervised baseline by a remarkable 11.2%, demonstrating the effectiveness of our semi-supervised learning method in limited label settings.\nOur method also outperforms state-of-the-art methods on the Cityscapes dataset as shown in Table I. Cityscapes presents more complex, high-resolution urban environments. Under an extremely limited and rarely used setting, where only 1/30 (3.3%) of the training data (100 labeled images) is annotated, our method achieves a remarkable mIoU of 65.87%. This surpasses the supervised baseline by 10.77%, showcasing our model's efficiency in extremely limited label scenarios and ability to handle complex scene layouts, fine-grained object boundaries, and diverse urban visuals."}, {"title": "D. Qualitative Analysis", "content": "Fig. 1 and Fig. 4 show visual segmentation outputs produced by our method, CW-BASS on Pascal VOC and Cityscapes dataset, compared against a prominent self-training state-of-the-art method, ST++ [9] and UniMatch [36]. The qualitative gains are shown, with our method consistently providing clearer object boundaries, more accurate region delineations, and better overall segmentation performance. These results reflect the quantitative findings, showing the robustness and versatility of the proposed framework."}, {"title": "E. Ablation Studies", "content": "We conduct ablation studies on the various components of our CW-BASS framework to demonstrate their individual effectiveness. Table III shows the effectiveness of each component on the Pascal VOC dataset with a training size of 321 \u00d7 321 under a 1/8 (12.5%) labeled partition. Fig. 5 shows the visual comparisons of the predicted masks produced by each component on Cityscapes. We use the standard model (SupOnly) trained with only labeled images as our baseline.\nConfidence-Weighting serves as the main component in our method and achieves a mean Intersection over Union (mIoU) of 73.43%. This result is much higher than the baseline and some previous SOTA methods, underscoring the importance of the weighted loss in prioritizing high-confidence pseudo-labels and effectively mitigate the impact of label noise on model performance.\nIncorporating boundary-aware loss into the confidence-weighting, significantly improves segmentation performance, particularly near object boundaries. This addition results in an mIoU of 74.67%, marking a notable improvement of 6.36%. The findings demonstrate the effectiveness of addressing boundary blur through specialized loss functions.\nThe integration of a dynamic thresholding mechanism enhances the model's ability to filter out low-confidence pseudo-labels. Using this mechanism in conjunction with yields an mIoU of 69.01%, highlighting its role in dynamically refining pseudo-label quality by discarding unreliable predictions.\nThe confidence decay strategy gradually reduces the influence of low-confidence pseudo-labels during training. This approach stabilizes learning by improving pseudo-label reliability over time. The configuration produces modest improvements, showcasing its ability to complement other strategies by enhancing pseudo-label robustness.\nThe ablation study highlights the Confidence-Weighted Loss and Boundary-Aware Loss as the primary contributors to performance gains. The significant improvement observed with boundary-aware loss highlights the effectiveness of integrating boundary-delineation techniques in semi-supervised frameworks. The Confidence Decay Strategy and Dynamic Thresholding Mechanism address the challenges of noisy pseudo-labels and confirmation bias."}, {"title": "2) Hyperparameter Analysis:", "content": "To investigate the impact of our hyperparameter on model performance, we perform experiments using two groups of settings: our standard & default setting (i.e., $\\gamma$ = 1.0, $\\beta$ = 0.5, $\\alpha$ = 0.85), which we use for all our experiments, and a more conservative setting ($\\gamma$= 0.5, $\\beta$ = 1.0, a = 1.0), as shown in Table IV. The standard settings batch yield the best model performance because the parameters are balanced, whereas the conservative settings yield an mIoU 4.41% lower than the standard settings.\nFor Confidence weighting, a $\\gamma$ value of 0.5 reduces the influence of these labels, making the model more conservative to minimize the risk of overfitting to incorrect labels. On the other hand, setting ($\\gamma$) to 2.0 increases the emphasis on high-confidence pseudo-labels, encouraging the model to reinforce confident predictions more aggressively.\nDynamic thresholding ($T$) involves a base threshold ($T_0$) set at 0.6, which is adjusted using a parameter $\\beta$. A higher $\\beta$ value (e.g., 1.0) imposes stricter filtering, discarding more low-confidence pseudo-labels. To maintain stability, thresholds are constrained within a range of 0.3 to 0.8, ensuring they do not become excessively lenient or overly strict.\nFinally, the confidence decay factor ($\\alpha$), with a default value of 0.9, can be adjusted for specific training needs. Lowering $\\beta$ to 0.85 accelerates the decay, which is particularly useful when a more aggressive reduction of noisy pseudo-labels is required."}, {"title": "V. CONCLUSION", "content": "In this work, we presented Confidence-Weighted Boundary-Aware Semantic Segmentation (CW-BASS), a novel framework for semi-supervised semantic segmentation that addressed key challenges such as confirmation bias, boundary blur, and label noise. CW-BASS integrates confidence-weighting, dynamic thresholding, and boundary-delineation techniques to achieve state-of-the-art performance. CW-BASS significantly reduces the computational cost by using only two networks, fewer augmentations, and fewer unlabeled data samples per epoch, resulting in fewer training epochs while maintaining competitive performance. This efficient design is especially useful for resource-constrained scenarios because it balances performance and cost-effectiveness. Furthermore, CW-BASS's fast initial convergence and robust boundary delineation make it suitable for real-world applications with limited computational resources and unlabeled data."}]}