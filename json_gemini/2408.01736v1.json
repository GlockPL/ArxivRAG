{"title": "Can LLMs predict the convergence of Stochastic Gradient Descent?", "authors": ["Oussama Zekri", "Abdelhakim Benechehab", "Ievgen Redko"], "abstract": "Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.", "sections": [{"title": "1. Introduction", "content": "The research in machine learning (ML) and artificial intelligence (AI) fields has recently exhibited a drastic advancement with several breakthrough results across a wide range of tasks. The paramount of it is undoubtedly represented by the introduction of large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023): the most powerful AI models currently available. Trained on the vast amounts of language data, LLMs have achieved state-of-the-art results in diverse applications including machine translation (Brown et al., 2020), text generation, question answering (Roberts et al., 2020), and sentiment analysis (Zhang et al., 2023a). One of the reasons that makes studying LLMs so fascinating is the remarkable zero-shot performance often seen as a sign of their emergent capabilities.\nOne particular example of LLMs zero-shot capabilities that"}, {"title": "Contributions", "content": "In this paper, we propose to substantially expand the scope of (Liu et al., 2024). Our contributions in this direction are as follows:"}, {"title": "3. LLMs understand the convergence of SGD", "content": ""}, {"title": "3.1. Problem setup", "content": "Given a training set x = (x1,...,xn) of N i.i.d samples, we consider the optimization of the following problem\n$\\min F(\\theta), F(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}f(x_i,\\theta),$ (1)\nwhere $\\theta \\in \\mathbb{R}^d$. For this problem, the updates of minibatch SGD with stepsize $\\gamma_t$ and mini-batch $B_t$ of size m have the following form\n$\\theta^{t+1} = \\theta^t - \\gamma_tf_t(\\theta)$ (2)\nwhere $\\theta_t$ denotes the parameters after t iterations, and $f_t(\\theta) = \\frac{1}{m}\\sum_{x_i \\in B_t} \\nabla_{\\theta}f(x_i, \\theta^t)$ where $B_t$ is a minibatch of size m of training examples selected randomly."}, {"title": "3.2. Overparametrized vs. underparametrized regime", "content": "Our main underlying idea is to rely on the equivalence between the SGD and the Markov chain established in (Bach & Moulines, 2013) and used in several other works to theoretically analyze SGD. More formally, (Dieuleveut et al., 2018) took advantage that for a fixed constant step-size $t = \\gamma$, the SGD updates (2) form a homogeneous Markov chain. This Markov chain converges to a unique stationary distribution $\\pi_\\gamma$ that depends on the regime of the ML problem. In the overparametrized regime, i.e. when d is larger than N, the Markov chain converges to a Dirac $\\pi_\\gamma = \\delta_{\\tilde{\\theta}^*}$ where $\\tilde{\\theta}^*$ is a specific solution that depends on many parameters (e.g. initialization, step-size, model's architecture etc.). In the underparametrized regime, $\\pi_\\gamma$ is a stationary distribution with a strictly positive variance, e.g. $\\mathcal{N}(\\theta^*, \\gamma^{1/2})$ where $\\theta^*$ is an optimum (we note, however, that in general, the SGD noise is not Gaussian (Panigrahi et al., 2019)).\nWe now illustrate that a LLaMA2-7B model can understand the convergence of the SGD and correctly identify the"}, {"title": "3.3. From understanding to forecasting", "content": "Similarly to (Liu et al., 2024), we now know that LLMs can understand the SGD in two different regimes. We now want to make a step further by finding a way to benefit from this knowledge. One tangible way for this is to use the transition probabilities to estimate the transition kernel of the Markov chain underlying the SGD. Once this is achieved, it can be used to do forecasting simply by running the Markov chain on new previously unseen inputs representing, for instance, different initilization points or seeds.\nEstimating the transition kernel of SGD Since SGD is an infinite-dimensional state-space Markov chain, we propose a method to estimate a discretization of its transition kernel. For each parameter $\\theta_i, i \\in \\{1, ..., d\\}$, we consider the discretized state vector $\\Theta^t_i$ at time t, i.e. the vector $\\Theta^t_i = (0, . . ., 0, 1, 0, . . ., 0)^T$ with a 1 at the l-th position if $\\theta_i^t$ is in state l at time t. This is a vector of size $10^k$, where k is the chosen precision. Then, we can write $\\Theta^{t+1} = \\sum_{i=1}^{d} \\sum_{j=1}^{d} d_{i,j} P^{(i,j)}$, where $\\forall i, j, d_{i,j} \\geq 0, \\sum_{j=1}^{d} d_{i,j} = 1$ and $P^{(i,j)}$ is the discretized transition probability matrix for the transitions of states of parameter $\\theta_j$ to states of"}, {"title": "3.3.1. CONVEX CASE", "content": "We consider a usual linear regression problem in $\\mathbb{R}^2$. We start by performing one SGD run with constant step-size $\\gamma$ and use Algorithm 1 to estimate the transition matrices $P^{(1,1)}$ and $P^{(2,2)}$ of parameters $\\theta_1$ and $\\theta_2$.\nUsing the estimated matrices, we then show in Figure 3 that running a Markov chain with Q on new starting points leads"}, {"title": "3.3.2. \u039d\u039f\u039d-CONVEX CASE", "content": "For the non-convex case, we do the same experiment, but this time we launch two SGD runs with the same constant step-size $\\gamma$ and different initial points. The two runs are not trapped in the same optimum valley allowing to better estimate the transition kernel, see Figure 4."}, {"title": "3.4. ICL neural scaling laws revisited", "content": "We end this short paper by providing an important insight into the neural scaling laws of ICL derived by the authors of (Liu et al., 2024). In their paper, the authors argue that ICL exhibits power scaling laws similar to those of training (Kaplan et al., 2020). Additionally, they add that for some dynamical systems, one observes plateauing effect suggesting that it happens when the dynamical system \"wander out\" and doesn't converge to a stationary distribution."}, {"title": "4. Conclusion", "content": "In this work, we extend the ICL abilities of LLMs to a realistic and challenging problem: estimating the transition kernel of SGD. We show the feasibility of this task by providing a systematic way to generalize the learned kernel to previously unseen states, both in convex and non-convex optimization landscapes. The most important open question that stems from this work is whether such an approach can be applied to ML models with orders of magnitudes more parameters and how to raise the computational challenge underlying this potentially highly impactful task."}, {"title": "Appendix", "content": ""}, {"title": "A. Detailed ICL for dynamics learning", "content": "In this section, we go though the procedure presented in Section 2, providing more details about each step of the process. Given a time series {X1,X2, ..., Xt}t>>1, the ICL for dynamics learning procedure presented in (Liu et al., 2024) goes as follows:\n1. Rescaling. To avoid amibiguity due to leading zeros or leading same digit, the values of the time serie elements are rescaled to the interval [1.5, 8.5]. E.g. [0.2513, 5.2387, 9.7889] \u2192 [1.5, 5.16, 8.5]\n2. Fixed-precision encoding. Represent the time series elements with a fixed precision of k digits. E.g. [1.5, 5.16, 8.5] \u2192 [150, 516, 850] with k = 3\n3. String representation. Represent the time serie as a string. E.g. [150, 516, 850] \u2192 \"150, 516, 850\"\n4. Tokenization. Transform the string using the LLM's corresponding tokenizer (see Appendix B for more details). E.g. \"150, 516, 850\" \u2192 [29896, 29945, 29900, 29892, ...]\n5. Inference. Call the LLM to produce logits over the full tokens vocabulary. LLM ([29896, 29945, 29900, 29892, ...] \u2208 $\\mathbb{R}^L$) \u2192 logits \u2208 $\\mathbb{R}^{L\\times N_t}$ with $N_t$ the vocabulary size and L the sequence length.\n6. Softmax. Extract the logits corresponding to single digits (0, 1, 2, . . ., 9) and apply the Softmax to get a probability distribution over the latters. E.g. logits \u2208 $\\mathbb{R}^{L\\times N_t}$ \u2192 probs \u2208 $\\mathbb{R}^{L\\times 10}$\n7. Hierarchy-PDF. The trivial way to proceed is to sample the next digit from the obtained probs, and repeat step 5 in an autoregressive fashion. However, Liu et al. (2024) provide a more sophisticated algorithm that explores the modes (and their neighborhoods) of the generated probs. This algorithm -Hierarchy-PDF- allows us to build a more refined probability distribution over the desired next value with k digits. E.g. Hierarchy-PDF(serie, LLM, precision) \u2192 probs \u2208 $\\mathbb{R}^{L\\times 10^k}$\n8. Transition rule. The last element of the obtained probs constitutes the transition rule $P(X_{t+1} = i|X_t = x_t)$ for i \u2208 [0, 1, 2, ..., $10^k$ \u2013 1] in the finite-discrete space formed by steps 1 and 2."}, {"title": "B. On the importance of the tokenizer", "content": "The time serie tokenization step is a crucial part of the above procedure. Indeed, LLMs' ability to handle numerical values has been proved to be dependent on the tokenization algorithm (Singh & Strouse, 2024; Ali et al., 2024; Gruver et al., 2023). The most widely used tokenization algorithm to-date, BPE (Sennrich et al., 2016), tend to assign tokens to arbitrary 3-digits numbers based on their occurences in large-scale corpora, and the tokenizer's vocabulary size. As highlighted by Gruver et al. (2023), this artifact severly hinders LLMs' ability to predict numerical values in-context. This is the case for popular LLMs such as GPT-3 (Brown et al., 2020) or Claude v2.1."}, {"title": "C. Obtaining ground truth for SGD", "content": "An other way to write the scheme (2) is to define the zero-mean noise $\\xi_k$ as :\n$\\xi_k(\\theta) = \\nabla F(\\theta) - \\nabla f_k (\\theta)$\nSo that we can rewrite the scheme as:\n$\\theta^{k+1} = \\theta^k \u2013 \\gamma_k\\nabla F(\\theta^k) + \\gamma \\xi_k(\\theta^k)$ (3)\nIn (Zhu et al., 2019), with large batch size m, (3) is approximated by the following stochastic scheme (thanks to the Central Limit Theroem), that we call gradient Langevin dynamics (GLD).\n$\\theta^{k+1} = \\theta^k \u2013 \\gamma_k\\nabla F(\\theta^k) + \\gamma C_k(\\theta^k)Z$ (4)\nwhere $C_k (\\theta) = \\sqrt{E(\\xi_k(\\theta)\\xi_k(\\theta)^T)}$ and Z ~ $\\mathcal{N}(0, I_d)$.\nEchoing (Panigrahi et al., 2019), the batch size m needs to be large enough for the Gaussian approximation of the SGD noise to be satisfying. However, the strong point of this approximation is that it gives us ground truth. In fact, we generate the noise ourselves, so we know its mean and variance."}, {"title": "D. Additional Experiments", "content": "We produced additional experiments for randomly generated underparametrized convex problems. The stepsizes y vary from one experiment to another, but are always constant throughout the run. See Figure 6."}]}