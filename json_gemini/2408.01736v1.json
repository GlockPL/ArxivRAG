{"title": "Can LLMs predict the convergence of Stochastic Gradient Descent?", "authors": ["Oussama Zekri", "Abdelhakim Benechehab", "Ievgen Redko"], "abstract": "Large-language models are notoriously famous\nfor their impressive performance across a wide\nrange of tasks. One surprising example of such\nimpressive performance is a recently identified\ncapacity of LLMs to understand the governing\nprinciples of dynamical systems satisfying the\nMarkovian property. In this paper, we seek to\nexplore this direction further by studying the dy-\nnamics of stochastic gradient descent in convex\nand non-convex optimization. By leveraging the\ntheoretical link between the SGD and Markov\nchains, we show a remarkable zero-shot perfor-\nmance of LLMs in predicting the local minima to\nwhich SGD converges for previously unseen start-\ning points. On a more general level, we inquire\nabout the possibility of using LLMs to perform\nzero-shot randomized trials for larger deep learn-\ning models used in practice.", "sections": [{"title": "1. Introduction", "content": "The research in machine learning (ML) and artificial intelli-\ngence (AI) fields has recently exhibited a drastic advance-\nment with several breakthrough results across a wide range\nof tasks. The paramount of it is undoubtedly represented by\nthe introduction of large language models (LLMs) (Brown\net al., 2020; Touvron et al., 2023): the most powerful AI\nmodels currently available. Trained on the vast amounts\nof language data, LLMs have achieved state-of-the-art re-\nsults in diverse applications including machine translation\n(Brown et al., 2020), text generation, question answering\n(Roberts et al., 2020), and sentiment analysis (Zhang et al.,\n2023a). One of the reasons that makes studying LLMs so\nfascinating is the remarkable zero-shot performance often\nseen as a sign of their emergent capabilities.\nOne particular example of LLMs zero-shot capabilities that\nrecently gained in popularity is their highly competitive\nperformance in time-series forecasting (Jin et al., 2024).\nThe cornerstone idea enabling their use in this task is to\nrepresent time series data in a textual format through careful\ntokenization (Gruver et al., 2023). Several works have used\nit as a foundation to rival dedicated time-series forecasting\nmodels with very encouraging results. More interestingly, a\nrecent paper (Liu et al., 2024) applied such tokenization to\ntackle a completely different task consisting in (in-context)\nlearning of the transition probabilities of the dynamical\nsystems that time series data describe. The intuition behind\nsuch an approach was to treat logits of the LLM's next-\ntoken prediction output as the above-mentioned transition\nprobabilities and refine them to a desired degree of accuracy\ndepending on the chosen discretization.\nWhile presenting intriguing results related to different dy-\nnamical systems, their work doesn't provide an actionable\nway to use the derived transition probabilities. Similarly,\ntheir work concentrates on well-known illustrative examples\nof dynamical systems, that \u2013 while being insightful \u2013 do not\ncorrespond to ML tasks solved in practice.\nContributions In this paper, we propose to substantially\nexpand the scope of (Liu et al., 2024). Our contributions in\nthis direction are as follows:"}, {"title": null, "content": "1. We consider a challenging task of understanding the\ndynamics of the stochastic gradient descent in convex\nand non-convex settings with LLMs;\n2. By leveraging the theoretical link between Markov\nchains and SGD, we propose an algorithmic way\nnot only to retrieve the transition probabilities of the\nMarkov chain underlying the SGD, but also to estimate\nits transition kernel.\n3. We provide preliminary experimental results showing\nthe efficiency of the transition kernel estimation and\nits application in predicting the convergence of SGD\nfrom previously unseen random initialization.\nThe rest of our paper is organized as follows. In Sec-\ntion 2, we present the details about the prior work on the\n(in-context) learning of the transition probabilities of dy-\nnamical systems. Section 3 presents the details regarding\nthe equivalence of the stochastic gradient descent to Markov\nchains and our approach to estimating the transition kernel\nof the latter. In Section 3.3, we present the experimental\nresults showcasing the ability of our approach to correctly\nestimate the transition kernel of toy Markov chains and its\napplication to SGD for both convex and non-convex opti-\nmization problems. Finally, we conclude in Section 4."}, {"title": "2. Background knowledge", "content": "In-context Learning (ICL) is a growing research field\naiming at improving the zero-shot capabilities of LLMs by\nusing a carefully designed context included in the prompt.\nSince its introduction by (Brown et al., 2020), ICL has been\nsuccessfully used in many practical applications including\nNLP (Wei et al., 2022; Yao et al., 2023), vision (Dong et al.,\n2023; Zhang et al., 2023b; Zhou et al., 2024), and time\nseries forecasting (Gruver et al., 2023; Jin et al., 2024).\nICL with dynamical systems In our work, we are particu-\nlarly interested in a recent study (Liu et al., 2024) investi-\ngating the inference of transition probabilities of known dy-\nnamical systems from simulated trajectories using ICL. The\nauthors of the above-mentioned work, show that medium-\nsize LLMs, such as LLaMA2-13B, are able to learn the\ndynamics of Markovian systems with various properties\n(e.g. chaotic, discrete, continuous, stochastic).\nMore formally, for a time series $(x_i)_{i<t}$ generated by simu-\nlating a dynamical system with predefined transition rules\u00b9,\nLiu et al. (2024) apply the following procedure to infer them\nconditioned on the observed states $x_i$ (see Appendix A for\nmore details):\n\u00b9E.g. Brownian motion: $X_{t+1}|X_t = x_t \\sim N(x_t + \\mu, \\sigma^2)$\nwith parameters $(\\mu, \\sigma)$ or discrete Markov chains with n states:\n$P_{ij} = P(X_{t+1} = j|X_t = i)$ for $1 < i, j < n$: $P(X_{t+1}|X_t)$"}, {"title": null, "content": "Procedure: ICL for dynamics learning\nInput: time serie $(x_i)_{i<t}$, LLM $M$, precision $k$\n1. Rescale and encode the time serie with $k$ digits\n$x_t = \"x_1x...x,...\"$\n2. Call $M(x_t)$\n3. Extract the digits logits $(0, 1, 2, 3, ..., 9)$\n4. Build the next state probability distribution using the\nHierarchy-PDF algorithm in (Liu et al., 2024)\nReturn: predicted transition rules for the observed states:\n${P(X_{i+1}| X_i = X_i)}_{i<t}$\nWe now present our main contributions."}, {"title": "3. LLMs understand the convergence of SGD", "content": "3.1. Problem setup\nGiven a training set $x = (x_1,...,x_n)$ of $N$ i.i.d samples,\nwe consider the optimization of the following problem\n$\\min_{\\theta} F(\\theta), F(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i, \\theta),$ (1)\nwhere $\\theta \\in \\mathbb{R}^d$. For this problem, the updates of minibatch\nSGD with stepsize $\\gamma_t$ and mini-batch $B_t$ of size $m$ have the\nfollowing form\n$\\theta^{t+1} = \\theta^t - \\gamma_t f_t(\\theta^t)$ (2)\nwhere $\\theta^t$ denotes the parameters after t iterations, and\n$f_t(\\theta^t) = \\sum_{x\\in B_t} \\nabla_{\\theta} f(x, \\theta^t)$ where $B_t$ is a minibatch\nof size $m$ of training examples selected randomly.\n3.2. Overparametrized vs. underparametrized regime\nOur main underlying idea is to rely on the equivalence be-\ntween the SGD and the Markov chain established in (Bach &\nMoulines, 2013) and used in several other works to theoreti-\ncally analyze SGD. More formally, (Dieuleveut et al., 2018)\ntook advantage that for a fixed constant step-size $\\gamma_t = \\gamma$, the\nSGD updates (2) form a homogeneous Markov chain. This\nMarkov chain converges to a unique stationary distribution\n$\\pi_\\gamma$ that depends on the regime of the ML problem. In the\noverparametrized regime, i.e. when d is larger than N, the\nMarkov chain converges to a Dirac $\\pi_\\gamma = \\delta_{\\tilde{\\theta}^*}$ where $\\tilde{\\theta}^*$\nis a specific solution that depends on many parameters (e.g.\ninitialization, step-size, model's architecture etc.). In the\nunderparametrized regime, $\\pi_\\gamma$ is a stationary distribution\nwith a strictly positive variance, e.g. $N(\\theta^*, \\gamma^{1/2})$ where $\\theta^*$\nis an optimum (we note, however, that in general, the SGD\nnoise is not Gaussian (Panigrahi et al., 2019)).\nWe now illustrate that a LLaMA2-7B model can under-\nstand the convergence of the SGD and correctly identify the"}, {"title": null, "content": "regime in which the iterates were obtained. For this, we\nconsider toy underparametrized and overparametrized linear\nregression optimization problems in $\\mathbb{R}^2$ and plot the logit\nprobabilities outputted by the LLM given a time series of\n1000 iterates in Figure 2. The time series length is selected\nto ensure that its tokenized representation remains within\nthe LLM's context window limit, and we set the temperature\n$\\mathcal{T}$ of the LLM to 1. We can see that in both cases the LLM\ncorrectly identifies the regime by either outputting logits that\nform a Dirac distribution for the overparametrized problem\nor a Gaussian-like distribution with an accurately estimated\nmean and covariance of the underparametrized case."}, {"title": "3.3. From understanding to forecasting", "content": "Similarly to (Liu et al., 2024), we now know that LLMs can\nunderstand the SGD in two different regimes. We now want\nto make a step further by finding a way to benefit from this\nknowledge. One tangible way for this is to use the transition\nprobabilities to estimate the transition kernel of the Markov\nchain underlying the SGD. Once this is achieved, it can be\nused to do forecasting simply by running the Markov chain\non new previously unseen inputs representing, for instance,\ndifferent initilization points or seeds.\nEstimating the transition kernel of SGD Since SGD\nis an infinite-dimensional state-space Markov chain, we\npropose a method to estimate a discretization of its transition\nkernel. For each parameter $\\theta_i, i \\in \\{1, ..., d\\}$, we consider\nthe discretized state vector $\\Theta_t^i$ at time t, i.e. the vector\n$\\Theta_t^i = (0, . . ., 0, 1, 0, . . ., 0)^\\top$ with a 1 at the l-th position if\n$\\theta_t^i$ is in state l at time t. This is a vector of size $10^k$, where\n$k$ is the chosen precision. Then, we can write $\\Theta_{t+1}^i =$\n$\\sum_{i=1}^d \\sum_{j=1}^d \\alpha_{i,j} P^{(i,j)}$, where $\\forall i, j, \\alpha_{i,j} \\geq 0, \\sum_{j=1}^{10^k} \\alpha_{i,j} = 1$\nand $P^{(i,j)}$ is the discretized transition probability matrix\nfor the transitions of states of parameter $\\theta_i$ to states of"}, {"title": null, "content": "parameter $\\theta_j$, (Ching et al., 2002). Then, the discretized\ntransition kernel of SGD can be seen as a matrix\n$Q = \\begin{pmatrix}\n        \\alpha_{1,1}P^{(1,1)} & ... & \\alpha_{1,d}P^{(1,d)} \\\\\n        : & & : \\\\\n        \\alpha_{d,1}P^{(d,1)} & ... & \\alpha_{d,d}P^{(d,d)}\n        \\end{pmatrix}$\nwhich satisfies $\\Theta_{t+1} = Q \\Theta_t$. Our method estimates the\nmatrix $P^{(i,i)}$, from a single observation of the time serie of\nthe i-th parameter $\\theta_i$.\nThe LLM predictions help us to fill a few rows of $P^{(i,i)}$.\nTo completely fill this sparse matrix, we compute the debi-\nased Sinkhorn barycenter of the distributions surrounding\nthe empty rows (Janati et al., 2020; Flamary et al., 2021),\nsee Figure 1 for the overview of the whole pipeline and\nAlgorithm 1 for the transition kernel estimation routine. In"}, {"title": null, "content": "Algorithm 1 Estimating $P^{(i,i)}$\nInput: time serie $(\\theta_{t+1}^i)_{t\\geq0}$, LLM $M$, precision $k$, regu-\nlarization $\\epsilon$\n1. Fill $s < 10^k$ rows of the $10^k$ rows of $P^{(i,i)}$ with\nProcedure($\\theta_{t+1}^i, M, k$), denoted as $(P^{(i,i)}_{1,...},..., P^{(i,i)}_{s,...})$\n2. Fill the remaining $10^k -s$ rows of $P^{(i,i)}$ with debiased\nSinkhorn barycenter of regularization parameter $\\epsilon$ :\nfor $j$ = 1 to s - 1 do\nif empty rows between $P^{(i,i)}_j$ and $P^{(i,i)}_{j+1}$ then\nCompute debiased Sinkhorn barycenter between\n$P^{(i,i)}_j$ and $P^{(i,i)}_{j+1}$, with regularization parameter $\\epsilon$\nFill the empty rows\nend if\nend for\nReturn: Estimated matrix $P^{(i,i)}$\npractice, estimating the correlation matrices $P^{(i,j)}$ for $i \\neq j$\nis hard as it requires considering a multivariate Markov\nchain (Ching et al., 2002). We leave this generalization for\nfuture work, although our experimental result suggest that\nestimating only the block matrices in the diagonal of Q (i.e.,\nassuming $\\alpha_{i,j} = 0$ for $i \\neq j$) may be enough to obtain a\nreasonable estimate of Q.\nPredicting SGD convergence with LLMs We now con-\nsider a convex and a non-convex optimization problem to\nillustrate the usefulness of our approach.\n3.3.1. CONVEX CASE\nWe consider a usual linear regression problem in $\\mathbb{R}^2$. We\nstart by performing one SGD run with constant step-size\n$\\gamma$ and use Algorithm 1 to estimate the transition matrices\n$P^{(1,1)}$ and $P^{(2,2)}$ of parameters $\\theta_1$ and $\\theta_2$.\nUsing the estimated matrices, we then show in Figure 3 that\nrunning a Markov chain with Q on new starting points leads"}, {"title": null, "content": "to the convergence to the global optimum. The latter behav-\nior reflects our accurate estimation of the transition kernel\nthat replaces gradient computations with computationally\ncheap matrix multiplications."}, {"title": "3.3.2. \u039d\u039f\u039d-CONVEX CASE", "content": "For the non-convex case, we do the same experiment, but\nthis time we launch two SGD runs with the same constant\nstep-size $\\gamma$ and different initial points. The two runs are\nnot trapped in the same optimum valley allowing to better\nestimate the transition kernel, see Figure 4."}, {"title": "3.4. ICL neural scaling laws revisited", "content": "We end this short paper by providing an important insight\ninto the neural scaling laws of ICL derived by the authors\nof (Liu et al., 2024). In their paper, the authors argue that\nICL exhibits power scaling laws similar to those of training\n(Kaplan et al., 2020). Additionally, they add that for some\ndynamical systems, one observes plateauing effect suggest-\ning that it happens when the dynamical system \"wander out\"\nand doesn't converge to a stationary distribution."}, {"title": "4. Conclusion", "content": "In this work, we extend the ICL abilities of LLMs to a re-\nalistic and challenging problem: estimating the transition\nkernel of SGD. We show the feasibility of this task by pro-\nviding a systematic way to generalize the learned kernel to\npreviously unseen states, both in convex and non-convex\noptimization landscapes. The most important open question\nthat stems from this work is whether such an approach can\nbe applied to ML models with orders of magnitudes more\nparameters and how to raise the computational challenge\nunderlying this potentially highly impactful task."}, {"title": "Appendix", "content": "Table of Contents\nA Detailed ICL for dynamics learning\nB On the importance of the tokenizer\nC Obtaining ground truth for SGD\nD Additional Experiments\nA. Detailed ICL for dynamics learning\nIn this section, we go though the procedure presented in Section 2, providing more details about each step of the process.\nGiven a time series ${X_1,X_2, ..., X_t}_{t>>1}$, the ICL for dynamics learning procedure presented in (Liu et al., 2024) goes as\nfollows:\n1. Rescaling. To avoid amibiguity due to leading zeros or leading same digit, the values of the time serie elements are\nrescaled to the interval [1.5, 8.5]. E.g. [0.2513, 5.2387, 9.7889] \u2192 [1.5, 5.16, 8.5]\n2. Fixed-precision encoding. Represent the time series elements with a fixed precision of k digits. E.g. [1.5, 5.16, 8.5] \u2192\n[150, 516, 850] with k = 3\n3. String representation. Represent the time serie as a string. E.g. [150, 516, 850] \u2192 \"150, 516, 850\"\n4. Tokenization. Transform the string using the LLM's corresponding tokenizer (see Appendix B for more details). E.g.\n\"150, 516, 850\" \u2192 [29896, 29945, 29900, 29892, ...]\n5. Inference. Call the LLM to produce logits over the full tokens vocabulary. $LLM([29896, 29945, 29900, 29892, ...] \\in$\n$\\mathbb{R}^L) \u2192 \\text{logits} \\in \\mathbb{R}^{L \\times N_t}$ with $N_t$ the vocabulary size and L the sequence length.\n6. Softmax. Extract the logits corresponding to single digits (0, 1, 2, ..., 9) and apply the Softmax to get a probability\ndistribution over the latters. E.g. $\\text{logits} \\in \\mathbb{R}^{L \\times N_t} \u2192 \\text{probs} \\in \\mathbb{R}^{L \\times 10}$\n7. Hierarchy-PDF. The trivial way to proceed is to sample the next digit from the obtained probs, and repeat step 5 in an\nautoregressive fashion. However, Liu et al. (2024) provide a more sophisticated algorithm that explores the modes\n(and their neighborhoods) of the generated probs. This algorithm -Hierarchy-PDF- allows us to build a more refined\nprobability distribution over the desired next value with k digits. E.g. Hierarchy-PDF(serie, LLM, precision) \u2192\n$\\text{probs} \\in \\mathbb{R}^{L \\times 10^k}$\n8. Transition rule. The last element of the obtained probs constitutes the transition rule $P(X_{t+1} = i|X_t = x_t)$ for i $\\in$\n[0, 1, 2, ..., $10^k$ \u2013 1] in the finite-discrete space formed by steps 1 and 2.\nB. On the importance of the tokenizer\nThe time serie tokenization step is a crucial part of the above procedure. Indeed, LLMs' ability to handle numerical values\nhas been proved to be dependent on the tokenization algorithm (Singh & Strouse, 2024; Ali et al., 2024; Gruver et al., 2023).\nThe most widely used tokenization algorithm to-date, BPE (Sennrich et al., 2016), tend to assign tokens to arbitrary 3-digits\nnumbers based on their occurences in large-scale corpora, and the tokenizer's vocabulary size. As highlighted by Gruver\net al. (2023), this artifact severly hinders LLMs' ability to predict numerical values in-context. This is the case for popular\nLLMs such as GPT-3 (Brown et al., 2020) or Claude v2.1."}, {"title": "C. Obtaining ground truth for SGD", "content": "An other way to write the scheme (2) is to define the zero-mean noise $\u03be_k$ as :\n$\u03be_k(\u03b8) = \u2207F(\u03b8) \u2013 \u2207 f_k (\u03b8)$\nSo that we can rewrite the scheme as:\n$\u03b8^{k+1} = \u03b8^k \u2013 \u03b3_k\u2207F(\u03b8^k) + \u03b3 \u03be_k(\u03b8^k)$ (3)\nIn (Zhu et al., 2019), with large batch size m, (3) is approximated by the following stochastic scheme (thanks to the Central\nLimit Theroem), that we call gradient Langevin dynamics (GLD).\n$\u03b8^{k+1} = \u03b8^k \u2013 \u03b3_k\u2207F(\u03b8^k) + \u03b3 C_k(\u03b8^k)Z$ (4)\nwhere $C_k (\u03b8) = \\sqrt{E(\u03be_k(\u03b8)\u03be_k(\u03b8)^T)}$ and $Z \\sim N(0, I_d)$.\nEchoing (Panigrahi et al., 2019), the batch size m needs to be large enough for the Gaussian approximation of the SGD\nnoise to be satisfying. However, the strong point of this approximation is that it gives us ground truth. In fact, we generate\nthe noise ourselves, so we know its mean and variance."}, {"title": "D. Additional Experiments", "content": "We produced additional experiments for randomly generated underparametrized convex problems. The stepsizes y vary\nfrom one experiment to another, but are always constant throughout the run. See Figure 6."}]}