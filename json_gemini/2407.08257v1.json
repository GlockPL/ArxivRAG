{"title": "Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear", "authors": ["Seonwhee Jin"], "abstract": "Models based on convolutional neural networks (CNN) and transform- ers have steadily been improved. They also have been applied in various computer vision downstream tasks. However, in object detection tasks, accurately localizing and classifying almost infinite categories of foods in images remains challenging. To address these problems, we first seg- mented the food as the region-of-interest (ROI) by using the segment- anything model (SAM) and masked the rest of the region except ROI as black pixels. This process simplified the problems into a single classi- fication for which annotation and training were much simpler than ob- ject detection. The images in which only the ROI was preserved were fed as inputs to fine-tune various off-the-shelf models that encoded their own inductive biases. Among them, Data-efficient image Transformers (DeiTs) had the best classification performance. Nonetheless, when foods' shapes and textures were similar, the contextual features of the ROI-only images were not enough for accurate classification. Therefore, we in- troduced a novel type of combined architecture, RveRNet, which con- sisted of ROI, extra-ROI, and integration modules that allowed it to ac- count for both the ROI's and global contexts. The RveRNet's F1 score was 10% better than other individual models when classifying ambigu- ous food images. If the RveRNet's modules were DeiT with the knowl- edge distillation from the CNN, performed the best. We investigated how architectures can be made robust against input noise caused by permu- tation and translocation. The results indicated that there was a trade-off between how much the CNN teacher's knowledge could be distilled to DeiT and DeiT's innate strength. Code is publicly available at: https: //github.com/Seonwhee-Genome/RveRNet.", "sections": [{"title": "1 Introduction", "content": "Convolutional neural networks (CNN) have shown revolutionary performance in the Im- ageNet benchmark test [1, 2]and so have become de facto in computer vision use. Vision transformers (ViT), which apply self-attention [3] to analyzing images, do not suffer from CNNs' inductive bias [4]. Photographed foods are typically shown on plates or, in the case of liquids, in glasses, cups, or bowls. Many images contain foods of various types and sizes. Different foods can be mixed in a dish (Figure 1(a)). Some foods, like water and colorless liquors, are almost impossible to distinguish from each other (Figure 1(b)). Therefore, it is challenging to localize and classify foods in images for applications like calorie measurement [5]. Like with many natural image classifications, the criteria for categorizing foods are subjective,"}, {"title": "2 Related work", "content": "so certain categories can become too broad. Moreover, it is difficult to make finer-grained datasets. For example, although croissants and pretzels have completely different shapes, they can both be categorized as bread. There are almost infinite types of bread, so it is impossible to categorize breads by sub-types. The segment-anything model (SAM), which is the foundational model for promptable in- stance segmentation, can be prompted by coordinates and bounding boxes that specify the location of the instance [6]. This model makes previously impossible downstream tasks possible. Our study exploits the SAM's promptable and fine instance segmentation ability to localize regions of interest (ROI) to simplify localization and classification problems to classification problems. In this study, we investigated whether our proposed model can successfully overcome the challenges with food datasets described above. We exploited the SAM's instance segmenta- tion ability to perform instance localization with high fidelity to classify segmented objects to reduce annotation costs and perform finer-grained classification more efficiently. Our proposed ROI-vs-extra-ROI network (RveRNet) enabled us to properly exploit induc- tive biases from CNN and transformer model architectures. RveRNet successfully classi- fied ambiguous food by taking global image context into consideration via the extra-ROI module. Our results and contributions can be summarized as follows: \u2022 We simplified complex multiple object detection tasks as classification tasks by using SAM's promptable instance segmentation. \u2022 We used RveRNet architecture to consider an image's ROI via an ROI module and its global context via an extra-ROI module to improve ambiguous food classifica- tion. \u2022 RveRNet performed best with DeiTA modules. This result may be explained by the combination of inductive biases of the CNN and transformers. However, the combination via knowledge distillation in DeiT might have caused a trade-off the CNN's inductive bias and DeiT's innate strength."}, {"title": "2.1 Inductive Bias in Classification", "content": "Inductive bias is the assumption that learning algorithms have their own properties that allow them to achieve their goals. For example, despite convolution tending to cause trans- lation equivariance, CNNs that combine the convolution with other layers, like pooling, tend to cause translation invariance [7, 8]. Thus, encoded inductive biases can be changed by designing learning algorithms' architectures. For instance, locality can be assumed to be an inductive bias encoded in a CNN where input pixels spatially closer to each other are thought to be more correlated. In the context of image inputs, this assumption is quali- tatively valid because local patches usually have closely related colors, textures, and light- ing [9, 10]. Multichanneling and downsampling CNNs contribute to capturing long-range correlation [11]. Vision transformers (ViT) that analyze image patch sequences and lack the inductive biases of CNNs can classify images successfully [4]. Since ViTs were created, studies comparing the inductive biases of CNNs and transformers have been conducted (e.g., [10, 13]). Studies have also been conducted in which inductive biases strongly encoded in one model were encoded in another model using knowledge distillation (e.g., [14, 12]) and in which models have parallel architectures [15]. Our investigation examined the strengths and weakness of ViTs at generalizing because they lack the inductive biases of CNNs, such as locality and weight sharing, as shown by analyzing the FoodSeg103 dataset [16], the fact that Data-efficient image Transformers (DeiT) have the same architectures as ViTs but different training strategies (Table 1, [12]), and that DeiTos, which were influenced by CNN teachers' inductive biases, perform better than ViTs and CNNs. ViTs based on self-attention have inductive biases like permutation invariance [10] and permutation equivariance [15]. These inductive biases are not correlated to positional en- coding [10, 15]."}, {"title": "2.2 Promptable Segmentation that Enables ROI Separation", "content": "SAMs are promptable instance segmentation models [6], so they can be applied in a wide range of image types and analysis tasks. Prompts can be object coordinates, a bounding box enclosing an object, or a natural language description of the object. SAM variants, like"}, {"title": "2.3 Consideration of Global Semantic Contexts", "content": "By using SAM as our foundation model, identifying an ROI, and cutting out other re- gions [19], we were able to classify target foods in images in which there were multiple foods. However, in some cases, we needed to consider the global context because the ROI did not provide enough information. For example, it is difficult to distinguish between water and colorless liquors based only on the ROI (Figure 1(b)). Similarly, in South Korea, raw fish is commonly dipped in chili paste, while French fries are commonly dipped in ketchup. Due to their similar colors and textures, it is difficult to determine which sauce is in the image without considering the surrounding foods. Thus, it is sometimes helpful to consider both the ROI and the rest of the image because it may contain foods that are more likely to be paired with the food in the ROI. For example, the presence of French fries would indicate that the sauce in the ROI is ketchup, not chili paste."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 ROI Segmentation from the Raw Image", "content": "With a given prompt, the SAM can finely segment the ROI, so we assumed that there were only ignorable discrepancies between real prompted segmentation results and the annota- tions of the FoodSeg103 dataset that we used to train and test our classifiers. Therefore, in a real-world application, raw input images could be processed using SAM or FastSAM."}, {"title": "3.2 ROI-vs-extra-ROI Consideration", "content": "As described in Section 2.3, some foods cannot be accurately identified based only on the ROI. For example, ketchup and chili paste have similar colors and textures so foods outside of the ROI, like French fries, meatballs, onion rings, sashimi, and parboiled octopus, can be used to help distinguish between them. To effectively consider both ROIs and extra-ROI regions, we designed a Siamese-like ar- chitecture consisting of ROI, extra-ROI, and integration modules. The ROI and extra-ROI modules did not necessarily have Siamese networks and so were allowed to have hetero- geneous models. The proposed RveRNet model was as follows: \n$P_o (x_1, x_2) = Softmax (Linear (ReLU(Linear(Concat(f(x_1), g(x_2))))))$\n= h((f(x1), g(x2)))\nEven for images containing multiple foods, ROIs were localized by SAM instance segmen- tation. The input for the ROI module, denoted as $x_1$ in Equation 1 and 2, was the region outside of the ROI, which was masked by giving pixels in it color values of 0, turning them black and allowing the ROI module to concentrate on the ROI. The input for the extra-ROI module, denoted as $x_2$ in Equation 1 and 2, was the complementary cut-out produced by the ROI module in which the ROI pixels were masked in black while the extra-ROI region was preserved. The extra-ROI module, denoted as function g, output the global context whereas the ROI module, denoted as function f, output the crucial features used to iden- tify the ROI. The receptive fields of both the ROI and extra-ROI modules were concatenated, and then the concatenated product, containing information obtained from both modules, was pro- cessed in additional linear layers. This integrated module is denoted as function h."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset and Implementation Details", "content": "We trained and evaluated our proposed RveRNet on the preprocessed FoodSeg103 dataset. To quantify the advantage of our proposed model's architectures' unique inductive biases, we avoided selecting a dataset that was too large for fine-tuning. Preprocessing the selected dataset created complementary cut-out images that masked the ROIs and were input into the extra-ROI module while the ROI images were input into the ROI module. In addition, to determine the degree to which the extra-ROI module in RveRNet enhanced the classification performance of ambiguous foods, we added images of ketchup and chili paste either photographed or collected from the internet, to the dataset. There were 69 ketchup and 72 chili paste train images and 38 ketchup and 34 chili paste test images. Thus, 18,320 train images and 7,769 test images across 105 categories were used in this study. Unless otherwise specified, the train image dimensions were 224 \u00d7 224 pixels . The off-the- shelf MobileNetV2, ViT, DeiT, DeiTA, and MLP-Mixer models ( [4, 20, 12, 21]) were pre- trained on ImageNet-1K for each RveRNet module. We optimized the proposed model's parameters using the Adam optimizer [22], a linear warm-up, and a cosine learning rate decay scheduler. We applied a random horizontal flip to a train set with a 50% probabil- ity. The training batches contained 50 images and had an initial learning rate of 4 \u00d7 10\u22123. Top-1 accuracy and F1 scores on the test set were calculated for models, which were fine- tuned over 30 epochs using one NVIDIA A100 GPU in Google Colaboratory Pro [23]. Test results are presented in Table 2. The models were implemented using PyTorch [24], torchvision [25], and TIMM [26]."}, {"title": "4.2 Object Identification Based on Spatial ROI", "content": "Classifiers extracted the necessary and important features from the extra-ROI images based on the images' shapes, textures, and spatial information. The dataset's variance was higher"}, {"title": "4.3 Classifications of Easily Identifiable and Ambiguous Foods", "content": "To identify the food in ROIs, the extra-ROI module captured the global context that was then integrated with the features identified by the ROI module. When classifying ketchup and chili paste, RveRNet performed the best, which indicates that considering the global context via the extra-ROI module. RveRNet increased performance when the food has ambiguous shape and texture. In order to achieve food classification using off-the-shelf models for the ROI and extra-ROI modules, we first must assume that the global context can be obtained by observing the entire image. The global context is the extra-ROI area, so CNNs that have locality induc- tive biases may be disadvantageous while self-attention-based ViTs may be more suitable because they can combine distant patches. RveRNet performed better at classifying ketchup and chili paste when both the ROI and extra-ROI modules were MobileNetV2 (CNNs) than when only the ROI module was. How- ever, it performed worse for the overall test set than the off-the-shelf models, which indi- cates that using CNNs' inductive biases for both modules is ineffective. Furthermore, when both modules were either ViT or DeiT (self-attention-based models), the proposed model's F1 scores, including for ketchup and chili paste, increased significantly (Table 3). According to the GradCAM [28] visualization, off-the-shelf models and the RveRNet model extracted different patterns and selected different information because contextual features and information identified by the extra-ROI module affect the forward step and error backpropagation during training RveRNet (Figure 4). The results for MobileNetV2 alone and RveRNet with both modules being MobileNetV2 can be explained by the as- sumption that feature extraction by the extra-ROI module affected classification perfor- mance both positively and negatively. Considering the extra-ROI module's identified in- formation can increase classification F1 scores for ambiguous foods (Table 3). However, it can also reduce ROI module optimization efficacy."}, {"title": "4.4 Ablation Study", "content": "Ketchup and chili paste are red dipping sauces with standardized container forms and low variation in their surrounding contexts. Foods that typically come with ketchup are chips, onion rings, and sausages, so they could be accurately classified without an ROI module. We can replace the results for RveRNet without the extra-ROI module with the results of the individual model architectures in Section 4.2. RveRNet performed significantly worse without an ROI module than with one RveRNet (Table 4, Figure 6). This result indicates that the ROI module is the main driver of performance but the extra-ROI module plays a crucial role when classifying ambiguous foods, namely ketchup and chili paste . Moreover, the parallel structure of the ROI and extra-ROI modules and the module that integrates their outputs in RveRNet allows it to analyze visual and contextual features from both the ROI and global context to correctly classify the food in ROI."}, {"title": "4.5 Inductive Bias of Classifiers that Affect Object Classification", "content": "The input to the ROI module only preserves pixel values in the ROI and masks the extra- ROI in black. Therefore, to achieve a robust classification, it may be important for the ROI module's model to encode translation invariance. However, the input of the extra-ROI module preserves every pixel value outside of the ROI so that CNNs' inductive biases, like locality and translation equivariance, and transformers' inductive biases, like permutation invariance, are necessary to obtain the global context robustly. \nh((f(x1), g(x2))) = h((f(Px1), g(x2)))\nh((f(x1), g(x2))) = h((f(x1), g(Px2)))\nh((f(x1), g(x2))) = h((f(Px1), g(Px2)))\nWe can express the permutation invariance in RveRNet as shown in equations 3\u20135. Equa- tion 3 models the case when a permutation operation is applied to the input to the ROI module. Equation 4 models the case when the permutation operation is applied to the in- put to the extra-ROI module. Equation 5 models the case when the permutation operation is applied to both inputs to RveRNet. In Supplementary B.2, we compare the robustness of RveRNet based on the permutation invariance derived from the top-1 accuracy of each combination of modules in RveRNet (Figure 9). We grouped off-the-shelf models by architecture type and calculated the average of each group's top-1 accuracy decrease to summarize the permutation experiment's results (Ta- bles 5 and 6). When the ROI module was a CNN, RveRNet was vulnerable to input per- mutation noise. When the ROI module was DeiTA, RveRNet's performance decreased more than when it was DeiT. The CNN teacher's distillation to DeiT may reduce the trans- former's inductive bias (Table 5)."}, {"title": "5 Conclusion", "content": "We attempted to cope with challenging problems of food localization and classification by exploiting SAM's confident instance segmentation and our proposed RveRNet model that considered the global context of input images. Off-the-shelf models should be se- lected for RveRNet modules considering their inductive biases. RveRNet performed best when both ROI and extra-ROI modules were DeiT-B due to their combination of knowl- edge from CNNs and transformers. Furthermore, we identified which inductive biases"}, {"title": "Supplementary Materials", "content": ""}, {"title": "A Model Performance for Ambiguous Foods", "content": "When the sauce class was excluded from the dataset, models' ability to accurately classify ketchup and chili paste decreased (Table 8). The results in Table 9 indicate that the global context analyzed by the extra-ROI module en- hanced model performance when classifying ambiguous ketchup and chili paste images. When RveRNet had DeiT-BA for both modules, it did not perform worse than other sin- gle classifier architectures, which was not the case for other combinations of modules in RveRNet. MobileNetV2's F1 score increased after excluding the sauce category from the test set. Clas- sification accuracy of ambiguous foods also increased. Although ketchup and chili paste looked similar and were distinctive from other sauces, images of these sauces had similar- looking ROIs that may have confused the classifiers."}, {"title": "B Data Augmentation to Investigate the Effect of Inductive Bias", "content": ""}, {"title": "B.1 Individual Off-the-shelf Models", "content": "ROI translocation risks losing all of the information in the ROI (Figure 7(c)). Therefore, it is inappropriate to quantify translation invariance using randomly translocated images. The transformer-based models, such as ViT and DeiT, were robust against permuted in- puts, which indicates that these models encoded relatively strong permutation invariance (Table 10). However, CNNs, like MobileNetV2, and CNNs' knowledge-distilled transform- ers, like DeiT-BA, were relatively vulnerable to input permutation."}, {"title": "B.2 Permutation Invariance", "content": "The top-1 accuracy dramatically dropped when permutation noise was introduced to the ROI modules' inputs. However, when noise was introduced to the extra-ROI modules'"}]}