{"title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task", "authors": ["Ga\u00ebtan Caillaut", "Raheel Qader", "Mariam Nakhl\u00e9", "Jingshu Liu", "Jean-Gabriel Barth\u00e9lemy"], "abstract": "Recent studies have showcased remarkable capabilities of decoder-only models in many NLP tasks, including translation. Yet, the machine translation field has been largely dominated by encoder-decoder models based on the Transformer architecture. As a consequence, scaling laws of encoder-decoder models for neural machine translation have already been well studied, but decoder-only models have received less attention. This work explores the scaling laws of decoder-only models on the multilingual and multidomain translation task. We trained a collection of six decoder-only models, ranging from 70M to 7B parameters, on a sentence-level, multilingual and multidomain dataset. We conducted a series of experiments showing that the loss of decoder-only models can be estimated using a scaling law similar to the one discovered for large language models, but we also show that this scaling law has difficulties to generalize to too large models or to a different data distribution. We also study different scaling methods and show that scaling the depth and the width of a model lead to similar test loss improvements, but with different impact on the model's efficiency.", "sections": [{"title": "1 Introduction", "content": "Most modern machine translation systems are based on Transformers (Vaswani et al., 2017), with an encoder-decoder architecture. Despite the tremendous advances made possible with the release of open-source decoder-only Large Language Models (LLMs) (Jiang et al., 2023; Biderman et al., 2023; Touvron et al., 2023), most NLP tasks still rely on encoder-decoder models. Based on the statistics obtained from the WMT23 shared task on general machine translation (Kocmi et al., 2023), 16 out of the 17 participants submitted a system based on an encoder-decoder model. Yet, recent studies show that decoder-only models can achieve comparable results (Gao et al., 2022; Fu et al., 2023), or even surpass state-of-the-art encoder-decoder systems, when properly fine-tuned (Xu et al., 2023). Moreover, the decoder-only architecture is easier to train on massive amounts of data as one can simply concatenate documents and feed as much relevant data as possible into the model during training; while encoder-decoder models requires either to pad the inputs or need rely on complex masking strategies (Raffel et al., 2020) to combine multiple inputs in the same sample.\nFurthermore, the decoder architecture is much more flexible than the encoder-decoder architecture as decoders treat all tokens similarly, while encoder-decoders make a distinction between input (source) tokens and output (target) tokens, which are processed, respectively, by the encoder and the decoder. As a consequence, it is more tedious to apply complex self-reasoning mechanisms, such as chain-of-thought (Wei et al., 2022), or to interface it with external tools (Schick et al., 2024), because the outputs of such method (the reasoning process) should, preferably, be treated as inputs of the model. For the same reasons, it is much more computationally expensive to rely on an encoder-decoder for conversational purposes, making this architecture less efficient for modern workflows such as iterative translation. Indeed, at each round (the user's query and the system's answer) should be appended to the input side, and reprocessed by the encoder for the next round. Decoder-only models support it by design, without needing to recompute the representation of the ever-growing inputs. While we do not explore these directions in this work, we do leverage the flexibility of the decoder architecture to include input-or-output parameters. As we are tackling the multilingual and multidomain machine translation task, the model needs input tokens to represent the language direction and the domain. We propose to train the model to predict the source language and the domain so that, during inference, they can be seamlessly predicted or provided by the user.\nGenerally speaking, decoder-only models simply expect the input to be the whole discussion and process it in a single forward step. Causal masking enable efficient caching of already computed keys and values so inference is much cheaper. The main downside is that the quality of the input representation might be inferior, as input tokens can attend only on past tokens. But it should not be a major issue, as generated tokens attend to the whole past sequence, they do have access to the same quantity of information as with an encoder-decoder model. In addition, previous work propose to update the attention mask so that input tokens can attend to all input tokens while generated tokens can attend only on past tokens (Tay et al., 2022; Raffel et al., 2020).\nFor all these reasons, we would like to embrace the decoder architecture for machine translation, even if it seems to be the exclusive preserve of encoder-decoder models. The flexibility and the simpler training setup of decoders should make them both more suitable and efficient for most real world applications, and the decoder architecture is more appropriate to answer the ever-growing demand for iterative, interactive and machine assisted translation workflow. To this aim, we study the scaling laws of neural machine translation models under different settings. Our contributions are as follow:\n\u2022 We show that decoder-only models for translation follow the same scaling law as LLM\n\u2022 Scaling laws do not scale uniformly across directions and domains and do not generalize well to other directions or domains\n\u2022 Scaling width-wise and depth-wise yield similar improvements, but the former is more efficient\n\u2022 We discovered a critical issue related to the packing of training samples in batches and propose a solution to fix it"}, {"title": "2 Background", "content": "As the model size, data requirement, and training costs of language models rise, it quickly becomes extremely important to estimate the right training configuration for a given training budget expressed in number of floating point operations (FLOP) required to train the model. Kaplan et al. (2020) discovered a power law relationship between the loss of a language model and its number of parameters, and that larger models perform better given the same amount of data. Even though certain work in this area shows that larger models, indeed, tend to be more powerful, more recent studies show that other parameters must be taken into account as well. For instance, the Chinchilla scaling law (Hoffmann et al., 2022) shows that model and dataset sizes are loosely tied and need to be scaled equally. In other words, even if increasing only the model size will most likely improve its performances, the compute-optimal solution often requires to also increase the quantity of training data, while preserving the same training cost. These findings had a great impact on LLM research, as researchers stopped increasing blindly the size of their models, in favor of more data, when it was necessary. For instance, the 176B BLOOM (Le Scao et al., 2022) model would probably have been trained very differently if this study was released sooner (or not at all). As stated in the paper, \"in light of revised scaling laws published during training, we decided to train the large models for an additional 25 billion tokens on repeated data\", the authors discovered that training such a big model was sub-optimal given the quantity of data they had. As a consequence, many researchers started to work on the collection of large, high quality datasets (Nguyen et al., 2023; Penedo et al., 2023) or on means to enhance existing datasets (Sorscher et al., 2022; Tirumala et al., 2024).\nMost of these scaling laws studies focus exclusively on causal generative language models. While it's likely that many of these findings could apply to translation models, the differences between the two tasks cannot be taken for granted. Translation is way more strict than causal language modeling since the model has to take into account each information in the source and precisely generate the target sentence without adding or omitting any information. Hence, many studies have naturally emerged to observe the scaling behavior of translation models (Gordon et al., 2021; Fernandes et al., 2023; Ghorbani et al., 2021). Yet, most (if not all) of these works focused on encoder-decoder models. For instance, Gordon et al. (2021); Fernandes et al. (2023) showed that, when the encoder and the decoder are scaled proportionally, the model's performances follow a power-law similar to the observation made on language models. Ghorbani et al. (2021) tackle the problem in a different setup, and propose to scale the encoder and the decoder individually. They show that encoder-scaling and decoder-scaling affect the model's performances differently, and they propose a new formula describing the scaling behavior of the cross-entropy loss as a bivariate function of encoder and decoder size. They found out that scaling decoder is, according to their experiments, always more beneficial, in terms of cross-entropy loss performance, than scaling the encoder.\nRecently, Alves et al. (2024) introduced the TowerInstruct, an LLM based on a decoder architecture (LLama 2 (Touvron et al., 2023)) finetuned to handle several translation tasks. They show that a properly fine-tuned LLM can perform translation better than state-of-the-art models. But the most promising aspect of this work is the inherent capacity of LLM to handle different tasks. They finetuned TowerInstruct so it can, for instance, clean source sentences before translating them, follow terminological constraints or respect a given level of language. However, this work is still empirical and we do not know, yet, the limits of such models. Inspired by the performances of TowerInstruct, a decoder-based machine translation system, we study, in the following, the scaling behavior of decoder-based machine translation models trained from scratch. To this aim, we fit multiple scaling laws to see if translation models follow the same scaling laws as language modeling models (such as the Chinchilla law) or if they follow their own task-specific law."}, {"title": "3 Training methodology", "content": "We present in this section all details related to the training of our six models."}, {"title": "3.1 Data", "content": "To conduct our experiments, we collected many bilingual data from public repositories (CCMatrix (Schwenk et al., 2019b), WikiMatrix (Schwenk et al., 2019a), UN Parallel Corpus (Ziemski et al., 2016), Paracrawl (Ba\u00f1\u00f3n et al., 2020) and Europarl (Koehn, 2005)). We also included a subset of an in-house proprietary dataset collected over time, as well as a small portion of financial documents in order to observe the scaling behavior on domain-specific data. An overview of the dataset distribution is given in Table 1. The financial data is divided into 8 sub-domains, which are described in Appendix A. The data is made of bilingual texts with one sample being one sentence pair.\nWe applied temperature sampling (t = 5) in order to increase the visibility of under represented pairs. Given a collection D of datasets, the probability of choosing a sample from a dataset $D_i \\in D$ after temperature sampling is given by $P_t(D_i)$ and is calculated from the original dataset statistical distribution $P(D_i)$.\n$P_t(D_i) = \\frac{N_i}{\\sum_{i=0}^{|D|} N_i} \\\\ T(D_i, t) = P(D_i)^{1.0/t}$\nwhere $N_i$ is the size of dataset $D_i$ and $T(D_i, t)$ is the factor by which the dataset $D_i$ should be oversampled. The new size $k_i$ of the oversampled dataset $D_i$ is given by:\n$k_i = \\frac{T(D_i, t)}{\\frac{\\max_D N_i}{\\max_D (T(D_i, t))}}$\nFinally, the probability of picking a sample from dataset $D_i$ after temperature sampling is given by\n$P_t(D_i) = \\frac{k_i}{\\sum_{i=0}^{|D|} k_i}$\nSince the balance between general and financial is also extremely skewed, we applied the temperature sampling separately on the general and financial domains."}, {"title": "3.2 Tokenizer", "content": "As we planned to train a multilingual model, we trained a Byte-Level BPE tokenizer (Wei et al., 2021) from scratch because, according to the authors, it is expected to better share the tokens among the multiple languages, resulting in less rare tokens and, hence, better embeddings. The tokenizer has been trained on the whole, non-oversampled, dataset, and we set the vocabulary size to 100 000.\nWe also reserved a small set of special tokens representing the supported languages and domains. They are inserted inside the input sequence so the model knows this information while generating a translation. For instance, the English language token is <lang_en> and the general domain token is <dom_general>."}, {"title": "3.3 Data format", "content": "Each sample of the datasets has two categories of features: inputs and outputs. Input features are data that will be given during inference, and output features are data that should be predicted by the model. Inputs are:\n\u2022 source sentence;\n\u2022 target language (because the model needs to know the desired target language).\nOutputs are:\n\u2022 source language;\n\u2022 domain;\n\u2022 translated sentence.\nPredicting the source language is not required, but we decided to include it to give to the model the ability to automatically detect the source language, as it is a very common and handy feature of most commercial translation tools. One could argue that this should be an input parameter, but we decided that the model should be able to classify by itself the language of the source sentence. Yet, the source language token can still be given as input at inference time to force a particular language. This also apply to the domain token.\nSince we plan to train a decoder-only model, training samples have been formatted such that the input tokens are first seen by the model, so the model has access to the whole input when generating the first output token. This is why we chose to encode the sentence pairs in the following format:\nSOURCE </src> <target lang>  TARGET <eos>\nwhere  and  are special tokens used to indicate, respectively, the end of the source and target sequences.\nThis data format gives the possibility to either provide the source language if required, or let the model predict it automatically. For instance, in the real example below, the green part represents the mandatory input (source sentence and target language), the orange part the optional input (source language) and the red part is the output generated by the model.\nThe buyer pays at an ATM. </src>  L'acheteur effectue le paiement sur les bornes automatiques."}, {"title": "3.3.1 The  token issue", "content": "All the models were trained in the same way LLM are trained. Sentence pairs were packed until the training batch was completely filled. These samples were separated by the usual end-of-sentence token . Ideally, one should also apply proper masking so tokens cannot attend to tokens from past sentence pairs. However, this features is not implemented in flash-attention 2 (Dao et al., 2022), so we trained the models without masks (except the causal mask). We expect the training task to be slightly more complex to solve, as the model now needs to learn to ignore every token before an  token, but we decided that the gain in training speed is worthwhile.\nOur initial experiments showed that the quality of translations generated by the models were far below our expectations. We found that the absence of the  token before the source sentence was confusing the model, explaining the drop in translation quality shown in Table 2. The  token, which was meant to signal the end of the translation, is actually also interpreted as a \"start of translation\" token. Indeed, during training, all sentence pairs (except the first one) are prefixed with the  token. This phenomenon is clear in the example below, in which three sentence pairs are packed in the same training sample.\nX1-1 X1-2 X1-3   \u04231-1 \u04231-2  X2-1 X2-2   \u04232-1 \u04232-2  X3-1 X3-2 X3-3   \u04233-1 \u0423\u0437-2 \u04233-3 \u04233-4  X4-1\nThe impact of  absence on the test loss can be seen in Figure 1. The model clearly outputs better translation when the source sentence is prefixed with an  token. This is particularly blatant when comparing the 160M and 410M models, respectively with and without the  token prefix. The 410M model, albeit being more than two times bigger than the 160M model, cannot generate better translations without the  prefix.\nThis problem should be negligible when training LLM, as documents are usually longer than sentence pairs, so  tokens are scarcer. However, its impact will increase as batch size grows, since more sentence pairs can be packed into the same batch. We experimented with a relatively small input length (512 tokens) and the absence of the  token during inference already lead to significant drop in performance. Generally speaking, this issue should not be ignored when more than one sequence are packed in a single training sample. When possible, one should properly mask previous training samples. As it is not possible, currently, to leverage the state-of-the-art self-attention algorithms, we recommend to always prefix all source sentences with the same prefix token(s), both during training and inference. In the remaining of this paper, we will only consider translations generated this way."}, {"title": "3.4 Training strategy", "content": "As we aim to train models dedicated to the translation task, we computed the loss only on target tokens, so the model learns to generate only text given a source sentence. This is different from pre-trained language models as there is no notion of source and target sentence. The target-only strategy has proven to be effective for training text-to-text models (Touvron et al., 2023), and is also similar to the way loss of encoder-decoder models is calculated, which are commonly used for machine translation (Costa-juss\u00e0 et al., 2022). Finally, we packed as many sentence pairs that we could in a single batch, in order to increase the training efficiency."}, {"title": "3.5 Model architectures", "content": "We used almost the same model architectures used in the Pythia suite (Biderman et al., 2023), the only difference being the number of attention head of the 160M model, as flash-attention expects a multiple of 8. We trained the models using the GPT-NeoX library (Andonian et al., 2023). We made a few changes to the data processing scripts in order to ignore source tokens during the loss computation. An overview of the different models we trained is given in Table 3.\nAll models are trained with a fixed batch size of 262 144 tokens (512 sequences of length 512 tokens) per GPU, on 8 Nvidia A100 GPUs. The models are trained in bfloat16 precision using the Adam optimizer with weight decay set to 0.1, 100 warmup steps and cosine learning rate decay. The maximum learning rate of sub-1B models is set to $1 \\times 10^{-3}$, and $1 \\times 10^{-4}$ for larger model because of loss instabilities during the training.\nThe models are trained for 100000 steps, on a total of 209715200000 (200B) tokens, although only half of them were actually used to train the model as we do not take into account source tokens when calculating the loss."}, {"title": "4 Experiments and results", "content": "In this section, we will study the impact of variations in training data size and parameters count on the test loss, for all our models. We will also verify if these changes correlate with their real translation performances using standard metrics such as BLEU and COMET. We finally explore two different model scaling strategies."}, {"title": "4.1 Applying machine translation scaling law", "content": "All existing scaling-laws studies show that larger models exhibit better generalization capabilities (Gordon et al., 2021; Fernandes et al., 2023; Ghorbani et al., 2021; Rae et al., 2021; Kaplan et al., 2020; Biderman et al., 2023). This study is no exception, as can be seen in Figure 2, larger decoder models always converge faster and require less training data to reach the same loss value.\nWe first fitted multiple curves following the setting of Ghorbani et al. (2021); Fernandes et al. (2023), who studied scaling laws for machine translation. The form of the law is given below:\n$L(N) = aN^{-P} + \\beta$ (1)\nwhere N is the number of trainable parameters, and the other variables are fitted by minimizing the huber loss (with a delta value of 0.01) using the BFGS algorithm from SciPy (Virtanen et al., 2020).\nAs shown in Figure 3, the test losses of our translation models can be realistically described by the power law fitted on observations made on all our models (the purple dotted line). This suggests that, indeed, performances of translation models follow a scaling law, that can be expressed by the formula above. We also fitted curves on less data points in order to verify if we could estimate the loss of the 6.9B model. Unfortunately, the fitted curves become less relevant as soon as we remove the data points from the largest model (the 6.9B model). This is extremely problematic, as the main goal of scaling laws is to estimate the performances of not-yet-trained larger models. Yet, we show that it is difficult to find a good estimation of the 6.9B model's performance without actually training it. For instance, the law fitted on the observations made on the subset 70M-160M-410M-610M-1B (in green) cannot give a good approximation of the unseen 6.9B model's performance, and the others are even worse. Therefore, we think one might be particularly cautious when applying such scaling laws to estimate larger models behaviors. Even if our law fitted on all data points seems to be a good estimator of the test loss, we think it will deviate from real observations as the model grows in size.\nWe also fitted scaling laws on a per-domain and per-direction basis, on all available data points. This is particularly interesting as it highlights discrepancies between domains and directions. As shown in Figure 4, it seems to be significantly easier to translate sentences from the kiid (Key Investor Information Document) financial domain, but translating general domain sentences is the most difficult, even though the huge majority of our training set is from the general domain. We suspect this curve are, somehow, indicators of the diversity inside each domain. Indeed, kiid documents are, by law, all following the same structure and must contain a specific set of information, written in a certain way. On the contrary, general domain documents do not follow any rule, making this domain the most heterogeneous one, and thus the most difficult to translate. Other phenomena might explain the differences between these curves. For instance, we also think the presence of many very specific and rare words in the regulatory domain explains partly the lower translation quality in this domain.\nWe also fitted one curve per direction and observed similar phenomena, as shown in Figure 5. For example, our models seem to be better at translating from English to German than from English to French, although our training dataset contains twice as many English-French pairs (before over-sampling).\nThese observations show that the scaling behavior of translation models depends on the training data distribution, and thus scaling laws estimated on a given dataset will not match the real scaling behavior on another one, although they might have the same general shape. For instance, it is not realistic to rely on a scaling law fitted on the EN-FR direction to estimate the performances on the EN-DE direction."}, {"title": "4.2 Applying language modeling scaling law", "content": "So far, we experimented with a scaling law formula based on the model size only, ignoring the training dataset size. Even if we just showed that lower perplexity/loss can be obtained with fewer data samples (in the case of the en-fr and en-de directions), larger training datasets still tend to increase the overall models' quality. But, it's also a waste of computing resources to train a model on more data than required, this is why modern language modeling scaling formula take into account both the number of trainable parameter and the training dataset size. Hence, we fitted multiple Chinchilla laws following the setting of Hoffmann et al. (2022), whose form is given below, on various combinations of input data to see if it can be used to reliably predict model performances.\n$L(N, D) = E + \\frac{a}{N^{\\alpha}} + \\frac{b}{D^{\\beta}}$ (2)\n\u0395, a, \u03b1, b and \u03b2 are variables fitted by minimizing the huber loss (with a delta value of 0.01) using the BFGS algorithm from SciPy (Virtanen et al., 2020); N and D are, respectively, the number of non-embedding parameters of the model and the number of training samples. More details are given in the original paper."}, {"title": "4.3 Correlating scaling law with real translation quality", "content": "Let us suppose we know the function modeling the real loss given a model size and an amount of training data. We still do not know if targeting lower loss values will actually improve the quality of the translations generated by the model. We provide in the following an empirical study showing the correlation between the model's loss and its translation performance. We computed BLEU (Papineni et al., 2002), COMET (Rei et al., 2022a) and CometKiwi (Rei et al., 2022b) scores for all six models, and we observed that, indeed, a lower loss does correlate with a performance increase, as shown in Table 4. This trend can be observed on the general domain for all directions, as shown in Appendix B. However, on the financial domain, CometKiwi does not always increase, it reaches a peak on the 610M model, then decreases. We conjecture that CometKiwi cannot correctly evaluate domain specific translations, as it is a reference-free model trained mainly on generalist sentences. We show in Appendix B that BLEU and COMET always increase with models' size, while CometKiwi often decreases at some point.\nWe also compare our models to well established LLM, and we show that smaller but specialized models clearly outperforms large and generalist LLM, as shown by our 410M model performing on par with Llama 8B. Our largest models are also real competitors to Tower 7B, even though it has been trained on much more data and specialized for machine translation. Tower 7B has the highest CometKiwi score, we just show, it might not be reliable on specialized domains. Our models are obviously performing better on the financial domain, because only our models were finetuned on financial data. We also remark that Mistral's scores are quite low on the general domain, a quick manual inspection revealed that the model often give details and explanations about the produced translation, even when asked not to. As a consequence, we think that Mistral lower score is mostly caused by the model not following rigorously the instructions.\nSo, while it certainly boost performances, increasing the model size is often not the optimal solution to improve the model's performance. The training dataset is also extremely important. Indeed, as can be observed in Figure 7, given a fixed FLOP budget, it is often preferable to increase the number of training samples. For instance, the 160M model appears to always be better than the 410M, 610M and 1B models given the same FLOP budget, as indicated by the 160M's curve being below other models' curves. This observation is also validated by the fitted law, as indicated in Figure 8. Most of the time, and according to the fitted Chinchilla law, it would have been better to just train our models on more data, instead of training larger models. For instance, we estimate that the 160M model would be on-par with the 410M model if trained on approximately twice as many data, which would not exceed the total number of FLOP of our current 410M model.\nTo conclude with, we find that scaling laws are a powerful tool to have a glimpse of what we can expect from a relatively larger model trained on the same dataset, but it will probably fail to predict the performances of much larger models, even if trained on a similar data distribution. It has to be kept in mind when using such scaling laws to plan a training budget: at some point, the fitted law will fail. Planning a training budget based on observations made on a 10B model might be fine to train a 70B model, but completely wrong for a 500B one. Furthermore, a given scaling law can only estimate the end performances of a model trained on the same data distribution used to fit the scaling law. For instance, we show in Figures 4 and 5 that laws fitted on different language directions or domains are very different, and thus should not be applied to estimate the performances of the model on another direction."}, {"title": "4.4 Scaling strategies", "content": "We also studied whether one should favor scaling the depth (increasing the number of layers) or the width (increasing the hidden size) of a decoder model. We took the smallest model as a baseline and scaled it depth-wise and width-wise so that the increase in parameters increased the total training FLOP by a similar amount, as illustrated in Figure 9. An overview of the scaled model architectures can be seen in Table 5. Interestingly, we observed that both scaling methods yield the same performance improvement. As shown in Figure 9, given a similar FLOP cost, scaling the depth or the width seems to have the very same impact on the test loss.\nGenerally speaking, scaling depth-wise lead to smaller, but less efficient models. Indeed, modern hardware architecture can handle more efficiently large matrix products than many smaller matrix products. As shown in Table 5, width-scaled models can generate much more samples than depth-scaled models because the GPU can do more FLOP per second."}, {"title": "5 Conclusion", "content": "This work describes the behavior of decoder models on the multilingual multidomain machine translation task. We trained models whose number of parameters range from 70M to 6.9B on sentence pairs in eight European languages. We show that decoder-only models for translation tend to scale similarly as language models, as the Chinchilla law can also be applied to our models. As such, we recommend to train machine translation models using the same training recipes as large language models. While we think it is true for most, if not all, NLP tasks, more work need to be carried out to validate this hypothesis. We also highlight a critical limitation of scaling laws: they cannot generalize well beyond an unknown model and/or training dataset size. As models tend to be larger through time, it will be extremely important to find ways to detect early unreasonable deviations of the \"reference\" scaling laws on which larger models are build.\nWe also show that models scaled width-wise appear to be more FLOP efficient than models scaled depth-wise, while reaching almost the same loss. Our experiments need to be continued in order to see when increasing the depth of the model starts to be more valuable than increasing its width. But, generally speaking, increasing the linearly both the depth and the width seems to be a good trade-off between efficiency and parameters count.\nEfficient training requires packing as many sentence pairs as possible in a training batch. We discovered that unexpected biases can be introduced if proper masking is not applied, that is to say, if sequences can attend to previous ones. Since it is not possible with current state-of-the-art optimization methods, one must carefully format the training input data. We suggest dropping the end-of-sentence token, commonly used to signal the end of text generation, in favor of a start-of-translation token signaling the start of a new source sentence and, therefore, the end of the generated target sentence.\nThis study has been conducted on sentence-level pairs only. While this setup is a bit outdated, it is still the first time a comprehensive study has been made on multilingual machine translation using decoder-only architectures. Nevertheless, we expect decoder models to be easy to adapt to the document-level translation task, as one can simply finetune a sentence-level decoder with non-shuffled sentence pairs from a corpus of parallel documents."}]}