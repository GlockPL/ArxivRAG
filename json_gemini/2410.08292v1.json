{"title": "CAN LOOPED TRANSFORMERS LEARN TO IMPLEMENT\nMULTI-STEP GRADIENT DESCENT FOR IN-CONTEXT LEARNING?", "authors": ["Khashayar Gatmiry", "Nikunj Saunshi", "Sashank J. Reddi", "Stefanie Jegelka", "Sanjiv Kumar"], "abstract": "The remarkable capability of Transformers to do reasoning and few-shot learning, without any\nfine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step\nalgorithms \u2013 such as gradient descent \u2013 with their weights in a single forward pass. Recently, there\nhas been progress in understanding this complex phenomenon from an expressivity point of view, by\ndemonstrating that Transformers can express such multi-step algorithms. However, our knowledge\nabout the more fundamental aspect of its learnability, beyond single layer models, is very limited.\nIn particular, can training Transformers enable convergence to algorithmic solutions? In this work\nwe resolve this for in-context linear regression with linear looped Transformers \u2013 a multi-layer model\nwith weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms.\nMore specifically, for this setting we show that the global minimizer of the population training loss\nimplements multi-step preconditioned gradient descent, with a preconditioner that adapts to the\ndata distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss,\ndespite the non-convexity of the landscape, by proving a novel gradient dominance condition. To\nour knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We\nfurther validate our theoretical findings through synthetic experiments.", "sections": [{"title": "1 Introduction", "content": "Transformers [Vaswani et al., 2017] have completely revolutionized the field of machine learning and have led\nto state-of-the-art models for various natural language and vision tasks. Large scale Transformer models have\ndemonstrated remarkable capabilities to solve many difficult problems, including those requiring multi-step reasoning\nthrough large language models [Brown et al., 2020, Wei et al., 2022b]. One such particularly appealing property is\ntheir few-shot learning ability, where the functionality and predictions of the model adapt to additional context provided\nin the input, without having to update the model weights. This ability of the model, typically referred to as \"in-context\nlearning\", has been crucial to their success in various applications. Recently, there has been a surge of interest to\nunderstand this phenomenon, particularly since Garg et al. [2022] empirically showed that Transformers can be trained\nto solve many in-context learning problems based on linear regression and decision trees. Motivated by this empirical\nsuccess, Von Oswald et al. [2023], Aky\u00fcrek et al. [2022] theoretically showed the following intriguing expressivity\nresult: multi-layer Transformers with linear self-attention can implement gradient descent for linear regression where\neach layer of Transformer implements one step of gradient descent. In other words, they hypothesize that the in-context\nlearning ability results from approximating gradient-based few-shot learning within its forward pass. Panigrahi et al.\n[2023], further, extended this result to more general model classes.\nWhile such an approximation is interesting from the point of view of expressivity, it is unclear if the Transformer model\ncan learn to implement such algorithms. To this end, Ahn et al. [2023], Zhang et al. [2023] theoretically show, in a"}, {"title": "2 Related Work", "content": "In-context learning. Language models, especially at larger scale, have been shown to empirically demonstrate the\nintriguing ability to in-context learn various tasks on test data Brown et al. [2020] More recently, Garg et al. [2022]\nformalized in-context learning ability and empirically observed that Transformers are capable of in-context learning\nsome hypothesis classes such as linear or two layer neural networks, sometimes improving over conventional solvers.\nThere have since been many paper studying this intriguing in-context learning phenomenon [Xie et al., 2022, Aky\u00fcrek\net al., 2022, Von Oswald et al., 2023, Bai et al., 2023]\nTransformers in modeling iterative optimization algorithms. He et al. [2016] first observed that neural networks\nwith residual connections are able to implicitly implement gradient descent. Von Oswald et al. [2023], Aky\u00fcrek et al.\n[2022] use this line of reasoning for in-context learning by constructing weights for linear self-attention layers that\ncan emulate gradient descent for various in-context learning tasks, including linear regression. Furthermore, Aky\u00fcrek\net al. [2022] empirically investigate various in-context learners that Transformers can learn as a function of depth\nand width. Also, von Oswald et al. hypothesize the ability of Transformers to (i) build an internal loss based on the\nspecific in-context task, and (ii) optimize over that loss via an iterative procedure implemented by the Transformer"}, {"title": "3 Preliminaries", "content": "One of the surprising emergent abilities of large language models is their ability to adapt to specific learning tasks without\nrequiring any additional fine tuning. Here we restate the formalism of in-context learning introduced by Garg et al. [2022].\nSuppose for a class of functions F and input domain X, we sample an in-context learning instance $I = (\\{x_i, y_i\\}_{i=1}^n, x_q)$$\nby sampling $x_i \\sim D_x$ and $f \\sim D_f$ independently, then calculating $y_i \\in \\{1,2,..., n\\}$, $y_i = f(x_i)$. An in-context\nlearner $M_\\theta$ parameterized by $\\theta$ is then a mapping from the instance $I$ to a prediction for the label of the query point\n$f(x_q)$. The population loss of $M_\\theta$ is then defined as"}, {"title": "3.1 In-context learning (ICL)", "content": "One of the surprising emergent abilities of large language models is their ability to adapt to specific learning tasks without\nrequiring any additional fine tuning. Here we restate the formalism of in-context learning introduced by Garg et al. [2022].\nSuppose for a class of functions F and input domain X, we sample an in-context learning instance $I = (\\{x_i, y_i\\}_{i=1}^n, x_q)$$\nby sampling $x_i \\sim D_x$ and $f \\sim D_f$ independently, then calculating $y_i \\in \\{1,2,..., n\\}$, $y_i = f(x_i)$. An in-context\nlearner $M_\\theta$ parameterized by $\\theta$ is then a mapping from the instance $I$ to a prediction for the label of the query point\n$f(x_q)$. The population loss of $M_\\theta$ is then defined as\n$L_{(\\Lambda,U)} (M_{\\theta}) = E_{D,D_x} [(M_{\\theta}(I) - f(x_q))^2]$"}, {"title": "3.2 Linear regression ICL setup", "content": "In this work, we consider linear regression in-context learning; namely, we assume sampling a linear regression instance\nis given by $I = (\\{x_i, y_i, x_q\\}_{i=1}^n)$ where for $w^* \\sim N(0, \\Sigma^*_\\theta d^{-1})$, $x_i \\sim N(0, \\Sigma_{\\theta d^{-1}})$ we have $y_i = f_{w^*}(x_i) =$\n$w^{*T}x_i$ for all $i \\in [n]$. The goal is to predict the label of $x_q$, i.e. $w^{*T}x_q$. Define the data matrix $X \\in R^{d \\times n}$, whose\ncolumns are the data points $\\{x_i\\}_{i=1}^n$:\n$X = [x_1,..., x_n]$\nWe further assume $n > d$, i.e. the number of samples is larger than the dimension. This combined with the fact that\n$I$ is realizable implies that we can recover $w^*$ from $\\{x_i, y_i\\}_{i=1}^n$ by the well-known pseudo-inverse formula:\n$w^* = (XX^T)^{-1}Xy$.\nWhile a reasonable option for the in-context learner $M_\\theta(I)$ is to implement $(XX^T)^{-1}Xy$, matrix inversion is arguably\nan operation that can be costly for Transformers to implement. On the other hand, it is known that linear regression can\nalso be solved by first order algorithms that move along the negative gradient direction of the loss\n$l_2(w) = ||X^Tw^* \u2013 y||^2$.\nUsing a standard analysis for smooth convex optimization, since the Hessian of the loss $||X^Tw^* \u2013 y||^2$ is $XX^T$\nwith condition number $\\kappa$, gradient descent with step size $\\frac{1}{\\kappa}$ converges in $O(\\kappa)$ iterations. This means that we need\n$O(\\kappa)$ many layers in the Transformer to solve linear regression. Particularly, Von Oswald et al. [2023] show a simple\nweighting strategy for the key, query, and value matrices of a linear self-attention model so that it implements gradient\ndescent, which we introduce in the next section."}, {"title": "3.3 Linear self-attention layer", "content": "Here we define a single attention layer that forms the basis of the linear Transformer model we consider. Define the\nmatrix $Z^{(0)}$, which we use as the input prompt to the Transformer, by combining the data matrix X, their labels y, and\nthe query vector $x_q$ as\n$Z^{(0)} = \\begin{bmatrix}\nX\\ny^T\\nx_q^T\\n0\n\\end{bmatrix}$\nFollowing Ahn et al. [2023], Schlag et al. [2021], Von Oswald et al. [2023], we consider the linear self attention model\n$Attn^{lin}(Z; W_{k,q,v})$ defined as\n$Attn^{lin}(Z; W_{k,q,v}) := W_vZM(Z^T W_k W_q Z)$,\n$M := \\begin{bmatrix}\nI_{n \\times n} & 0\\n0 & \\gamma_n I_{d \\times d} \\end{bmatrix} \\in R^{(n+1) \\times (n+1)}$\nwhere $W_k, W_q, W_v$ are the key, query, and value matrices, respectively and the index $k \\times r$ below a matrix determines\nits dimensions. Furthermore, similar to Ahn et al. [2023], we use mask matrix M in order to avoid the tokens\ncorresponding to $(x_i, y_i)$ to attend the query vector $x_q$, and combine product of the key and query matrices into\n$Q = W_k W_q$ to obtain the following parameterization for the attention layer (denoting $W_v$ by P):\n$Attn^{lin}(Z; Q, P) := PZM(Z^T Q Z)$"}, {"title": "3.4 Linear looped Transformer", "content": "The linear looped transformer $TF_L(Z^{(0)}; Q, P)$ can be defined by simply chaining $L$ linear self-attention layers with\nshared parameters Q and P. In particular, we define\n$Z^{(t)} := Z^{(t-1)} - \\frac{1}{n} Attn^{lin}(Z; Q, P)$.\nfor all $t \\in [L]$. Then, the output of an L layer looped transformer $TF_L (Z^{(0)}; Q, P)$ just uses the $(d + 1) \\times (n + 1)$ entry\nof matrix $Z^{(L)}$ i.e.,\n$TF_L(Z^{(0)}; Q, P) = -Z^{(L)}_{(d+1),(n+1)}.$\nWe note that the minus sign in the final output of the Transformer is only for simplicity of our expositions later on."}, {"title": "Can looped Transformer implement multi-step gradient descent?", "content": "We first examine the expressivity of looped\nTransformer. The key idea is to leverage the existing result of one step preconditioned gradient descent from Ahn\net al. [2023] and use the loop structure of looped Transformer to show that it can implement multi-step preconditioned\ngradient descent. For completeness, we first restate the observation of Ahn et al. [2023] that linear attention can\nimplement a step of preconditioned gradient descent with arbitrary preconditioner A. For this, it is enough to pick\nFor Proposition 3.1, preconditioned gradient descent with preconditioner A can be implemented by setting P\nand Q to the following:\n$Q:= \\begin{bmatrix}\nAdxd & 0\\n0 & 0\n\\end{bmatrix},P:= \\begin{bmatrix}\n0dxd & 0\\nu^T & 0\\n0 & 1\n\\end{bmatrix}$\nThen for the matrix $[X_xq] \\in [R^{d \\times (n+1)}$\n$[Attn^{lin}(Z; Q, P)]_{(d+1),:} = y^TX^TA [X x_q]$\n$[Attn^{lin}(Z; Q, P)]_{(d+1),:} = -\\frac{1}{n} \\nabla_w l_2(0)^T X$,\n$[Attn^{lin}(Z; Q, P)]_{1:d,:} = 0_{d \\times n}$"}, {"title": "3.5 Loss function on the weights", "content": "In previous section, while we observed that looped Transformer can implement preconditioned gradient descent, the\nchoice of the preconditioner and its learnability by optimizing a loss function (e.g. squared error loss) still remain\nunclear. Following Ahn et al. [2023], Zhang et al. [2023], we search for the best setting of matrices P, Q, where\n$Q:= \\begin{bmatrix}\nA_{dxd} & 0\\n0_{dxd} & 0\n\\end{bmatrix}$ i.e. only the top left $d \\times d$ block can be non-zero, and $P := \\begin{bmatrix}\n0& 0\\nu^T & 1\n\\end{bmatrix}$ for parameter vector $\\nu \\in R^d$.\nThe population squared loss as a function of A and $\\nu$ is\n$L(A, \\nu) = E_{w^*,x} [(TF_L (Z0; Q, P) \u2013 Y_q)^2]$.\nWe define a parameter $\\delta := \\frac{1}{n}^{(d+1)/(2L)}$ which governs the accuracy of our estimates, which goes to zero as $n \\rightarrow \\infty$."}, {"title": "3.6 Choice of the preconditioner", "content": "It is instructive to discuss the choice of the preconditioner A since it determines speed of convergence of $w_t$ to the\nsolution of the regression. Note that the exact solution of an over-determined linear regression instance (X, y) is\n$w = (XX^T)^{-1}Xy$. This can be obtained only after one step of preconditioned gradient descent starting from the\norigin and using inverse of the data covariance matrix preconditioner\n$\\Sigma = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T$."}, {"title": "3.7 Convergence", "content": "In this section, we state our main results. First, we give a tight estimate on the set of global minimizers of the population\nloss, under the Gaussian assumption, for the looped Transformer model with arbitrary number of loops L.\nSuppose $\\{\\Lambda^{opt}, \\nu^{opt}\\}$ are a global minimizer for $L(\\Lambda, \\nu)$.\nThen, under condition $\\frac{\\delta L d^2}{\\sqrt{n}} \\ll 1$,\n$L(\\Lambda^{opt}, \\nu^{opt}) < \\frac{\\delta L d^2}{\\sqrt{n}} 2^{2L}$\n$(1 \u2013 c)\\Sigma^{*-1} \\ll \\Lambda^{opt} \\ll (1 + c)\\Sigma^{*-1}, c = 8\\delta d^{1/(2L)}$ and $\\nu^{opt} = 0$, where recall $\\delta := (\\frac{\\delta L d}{\\sqrt{n}})^{1/(2L)}$.\nFrom Theorem 3.2, we first observe that the parameter $\\nu$ has no effect in obtaining a better regression solver\nand has to be set to zero in the global minimizer. This result was not known in the previous work Ahn et al. [2023]. A\nvalue of $\\nu^{opt} = 0$ implies that the optimal looped Transformer exactly implements L steps of preconditioned gradient\ndescent, with preconditioner $\\Lambda^{opt}$.\nSecondly, as discussed in Section 3.6, the choice of preconditioner plays an important role in how fast gradient descent\nconverges to the solution of linear regression. Intuitively the inverse of the population covariance seems like a reasonable\nchoice for a single fixed preconditioner, since it is close to the inverse of the data covariance for all linear regression in-\nstances. The above result shows that the global optimum is indeed very close to the inverse of the population covariance.\nPrecisely how close the optimum is to the population covariance depends on the parameter $\\delta = \\frac{4kd}{\\sqrt{n}}$, which goes to zero\nas the number of examples in each prompt goes to infinity. In general, we do not expect the global minimizer to be\nexactly equal to $\\Sigma^{*-1}$. Indeed for the case of one layer Transformer, which is equivalent to a loop-transformer with\nlooping parameter L = 1, the global minimizer found in Ahn et al. [2023] is not exactly the inverse of the covariance\nmatrix, but close to it. Even in their case, the distance goes to zero as $n\\rightarrow\\infty$. This shows that our estimate in\nTheorem 3.2 is essentially the best that one can hope for."}, {"title": "4 Proof Ideas", "content": "The proof is structured as follows:\n\u2022 We obtain closed form formula for the loss function in Lemma 4.1 in terms of the parameter A and covariance\n\u03a3*. The loss depends on how close $A^{1/2}\u03a3A^{1/2}$ is close to identity for a randomly sampled \u03a3. Using the\nestimates in Lemma 4.3, we obtain an estimate on the loss based on the eigenvalues of the matrix $A^{1/2}\u03a3^* A^{1/2}$.\nImportantly, the result of Lemma 4.3 is based on estimating the higher moments of the Wishart matrix with\narbitrary covariance, shown in Lemma 4.2 Using our estimate of the loss in Lemma 4.3, we obtain a precise\ncharacterization of the global optimum.\n\u2022 We further use Lemma 4.3 to drive an estimate on the magnitude of the gradient based on the same eigenvalues,\nthose of $A^{1/2}\u03a3^*A^{1/2}$. Comparing this with our estimate for the loss from Lemma 4.3, we obtain the gradient\ndominance condition in Theorem 3.4.\n\u2022 We use the gradient dominance condition to estimate the speed of convergence of the gradient flow to the\nproximity of the global minimizer in Theorem 3.3.\nThe starting point of the proof is that we can write the loss in a matrix power format based on A when u is set to zero:\nGiven $u = 0$, the loss for looped Transformer is as follows:\n$L(A, 0) = E_x [tr((I \u2013 A^{1/2}\u03a3A^{1/2})^{2L})]$,\nwhere $\u03a3 = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T$.\nTo be able to estimate the global minimizers of this loss, first we need to estimate its value. In particular, we hope to\nrelate the value of the loss to the eigenvalues of A. Note that if the data covariance matrix \u03a3 was equal to the population\ncovariance matrix \u03a3*, then loss would turn into $tr((I \u2013 A^{1/2}\u03a3^* A^{1/2})^{2L})$, whose global minimum is A = \u03a3*\u22121.\nHowever, we can still hope to approximate the value of $E_x [tr((I \u2013 A^{1/2}\u03a3A^{1/2})^{2L})]$ with $tr((I \u2013 A^{1/2}\u03a3^*A^{1/2})^{2L})$"}, {"title": "4.1 Out of distribution generalization", "content": "In the result below, we show that a looped Transformer learned on one in-context distribution can generalize to other\nproblem instances with different covariance, owing to the fact that it has learned a good iterative algorithm.\nLet Aopt, uopt be the global minimizers of the poplulation loss for looped Transformer with depth L when\nthe in-context input $\\{x_i\\}_{i=1}^n$ are sampled from $N (0, \\Sigma^*)$ and w* is sampled from $N(0, \\Sigma^{*-1})$. Suppose we are given\nan arbitrary linear regression instance $I^{out} = \\{x^{out}, y^{out}\\}_{i=1}^n, w^{out,*}$ with input matrix X^{out} =\n$[x^{out},...,x^{out}]$, query vector $x^{out}$, and label $y^{out} = w^{out,*}, x^{out}$. Then, if for parameter\n$0 < \\zeta < 1$, the input covariance matrix $X^{out} X^{out}$ of the out of distribution instance satisfies\n$(\u2211* <<  \u2211out << (2 \u2212 \u03b6)\u2211*,\nwe have the following instance-dependent bound on the out of distribution loss:\n$(TF_L(Z^{out}; Q, P) \u2013 y^{out})^2 < (1 + 16\\delta d^{1/(2L)})^2(1 + 16\\delta d^{1/(2L)} \u2013 \u03b6)^{2L}$"}, {"title": "5 Experiments", "content": "In this section we run experiments on in-context learning linear regression to validate the theoretical results and to go\nbeyond them. In particular, we test if looped models can indeed be trained to convergence, as the theory suggests, and\nwhether the learned solution is close to the predicted global minima. Furthermore, we investigate the effect of various\nfactors such number of loops, number of in-context samples and depth of the model (in the multi-layer case). We\nuse the codebase and experimental setup from Ahn et al. [2023] for all our linear regression experiments. In particular"}, {"title": "6 Conclusion", "content": "This work provides the first convergence result showing that attention based models can learn to simulate multi-step\ngradient descent for in-context learning. The result not only demonstrates that Transformers can learn interpretable\nmulti-step iterative algorithms (gradient descent in this case), but also highlights the importance of looped models in\nunderstanding such phenomena. There are several open questions in this space including understanding the landscape\nof the loss, convergence of training without weight sharing across layers, and handling of non-linearity in the attention\nlayers. It is also interesting to understand the empirical phenomenon that looping the trained models beyond the number\nof loops used in training can continue to improve the test loss. One way to show this is by obtaining a tighter upper\nbound on the optimal loss value."}]}