{"title": "Refining Adaptive Zeroth-Order Optimization at Ease", "authors": ["Yao Shu", "Qixin Zhang", "Kun He", "Zhongxiang Dai"], "abstract": "Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces Refined Adaptive Zeroth-Order Optimization (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show (I) the first analysis to the variance reduction of first moment estimate in ZO optimization, (II) the improved second moment estimates with a more accurate approximation of its variance-free ideal, (III) the first variance-aware convergence framework for adaptive ZO optimizers, which may be of independent interest, and (IV) the faster convergence of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges.", "sections": [{"title": "1. Introduction", "content": "Zeroth-order (ZO) optimization has emerged as an indispensable technique at the forefront of machine learning, addressing critical challenges where gradient information is either unavailable or computationally prohibitive. This necessity stems from the prevalence of black-box optimization problems, such as adversarial attacks (Ru et al., 2020; Hiranandani et al., 2021), and resource-constrained environments, like fine-tuning large language models (LLMs) on memory-limited devices (Malladi et al., 2023; Zhang et al., 2024b). Consequently, ZO optimization algorithms, which rely solely on function evaluations, have become a crucial alternative to traditional gradient-based methods. Despite the growing body of research in ZO optimization, a significant portion of existing methods adapt stochastic gradient descent (SGD) updates to the ZO setting (Liu et al., 2018a;b; Shu et al., 2023; 2024). This reliance on SGD, however, will lead to performance limitations, especially in complex and non-convex optimization landscapes. The need for more adaptive and versatile update mechanisms is hence evident. However, the exploration of adaptive strategies beyond SGD-based updates remains surprisingly limited.\nWhile adaptive methods such as ZO-AdaMM (Chen et al., 2019; Nazari et al., 2020) have demonstrated potential in addressing the missing adaptivity in zeroth-order optimization, they are fundamentally limited by their underutilization of moment information, often resulting in suboptimal convergence rates. This limitation in fact arises from their reliance on noisy and high-variance gradient estimates derived solely from function evaluations\u2014a stark contrast to the first-order (FO) methods that leverage direct and more stable gradients. This issue becomes even more pronounced in high-dimensional and complex settings.\nTo address this critical limitation, we introduce Refined Adaptive Zeroth-Order Optimization (R-AdaZO), a novel approach that effectively capitalizes on moment information through two key innovations. First, R-AdaZO is the first to analyze the untapped but inherent variance reduction effect of the first moment estimates on the gradient estimates in ZO optimization, leading to more accurate and stable ZO updates. This is accomplished through the integration of historical gradient estimates, which effectively averages out the estimation noise (Sec. 4.1). Second, R-AdaZO refines"}, {"title": "2. Related Work", "content": "Recent ZO optimization research focuses on two key areas: ZO gradient estimation and ZO update rules.\nZO Gradient Estimation. Since ZO optimization only relies on function values, gradient estimation is essential for effective optimization. A common approach is to use finite difference approximations under input perturbations. Nesterov & Spokoiny (2017) propose to use Gaussian random noise perturbations, demonstrating theoretical convergence with smooth perturbations. Other methods also propose to use uniform sampling from the unit sphere (Flaxman et al., 2005) or coordinate-wise perturbations (Lian et al., 2016).\nThese methods often have a noisy gradient estimation. To address this, (Cheng et al., 2021) introduces prior-guided gradient estimation, which leverages previous estimates to improve the current one, effectively smoothing the estimation noise. Recently, (Shu et al., 2023; 2024) propose using kernel methods to learn a surrogate model of the objective function from historical function values, allowing for more accurate gradient estimation. Note that this paper does not aim to introduce a new gradient estimation approach, but focus on developing advanced update rules that are applicable to all these existing estimation methods.\nZO Update Rules. Building upon the estimated gradients from various ZO estimation methods, ZO optimizers often directly adopt update rules from first-order (FO) optimization. E.g., a large portion of existing ZO optimizers use stochastic gradient descent (SGD) and its variants as their update mechanism (Ghadimi & Lan, 2013; Ghadimi et al., 2016; Nesterov & Spokoiny, 2017; Liu et al., 2018b;a; Cheng et al., 2021; Shu et al., 2023). While simple to apply, the slow convergence of SGD has motivated few efforts (Chen et al., 2019; Nazari et al., 2020; Jiang et al., 2024) to explore the use of adaptive methods, such as Adam (Kingma & Ba, 2015), as the ZO update rule. However, these attempts often under-utilize the moment information inherent in adaptive methods when applied to ZO optimization, leading to suboptimal convergence. This paper addresses this critical issue by proposing refined update rules that are specifically designed to better leverage moment information, ultimately leading to more efficient ZO optimization."}, {"title": "3. Background", "content": "This paper tackles a stochastic zeroth-order (ZO) optimization problem, aiming to minimize the expected value of a function, defined as:\n$\\min_{\\theta \\in \\mathbb{R}^d} F(\\theta) \\triangleq E_{\\xi} [f(\\theta; \\xi)]$\nwhere $\\theta \\in \\mathbb{R}^d$ and $f(\\theta; \\xi)$ is a scalar-valued function whose evaluation depends on the parameters $\\theta$ and a random variable $\\xi$ sampled from an underlying distribution. Crucially, we have access only to function evaluations $f (\\theta; \\xi)$ and not its gradient $\\nabla_{\\theta} f (\\theta; \\xi)$. Throughout this paper, we adopt the following notational conventions. Vectors are represented in boldface, e.g., $\\theta$, and scalar constants are denoted by uppercase letters, e.g., $L$. All vector operations are assumed to be element-wise unless explicitly stated otherwise. We denote by $\\nabla_i F$ the partial derivative of function $F$ with respect to the i-th coordinate.\nZO Gradient Estimation. In ZO optimization, the absence of direct access to gradients, denoted as $\\nabla_{\\theta} f(\\theta; \\xi)$, necessitates the use of gradient estimation techniques that"}, {"title": "4. Refined Adaptive ZO Optimization", "content": "To address the underutilization of momentum information in existing adaptive ZO optimization methods, we introduce R-AdaZO (Refined Adaptive Zeroth-Order Optimization). Specifically, we first analyze the untapped variance reduction effect of first moment estimates on ZO gradient estimation, which is important for accurate and stable ZO updates (Sec. 4.1). We then leverage these variance-reduced estimates to construct a refined second moment, enabling more effective scaling of ZO updates (Sec. 4.2)."}, {"title": "4.1. Variance Reduction in First Moment Estimates", "content": "First moment estimation, while conventionally used for convergence speedup, inherently serve as a variance reduction mechanism for noisy gradients. To show this, consider the following standard first moment estimate with $\\beta_1 \\in (0,1)$:\n$m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\nwhere $m_t$ is the estimated first moment at iteration $t$ and $g_t$ is the gradient estimate at $\\theta_{t-1}$ via (2). Intuitively, this update averages the current noisy gradient estimate with past, correlated estimates. This averaging process effectively smooths out noise in gradient estimates, thereby reducing variance. For example, averaging two independent noisy gradient estimates (i.e., $m_{t-1}$ and $g_t$) of variance $\\sigma^2$ results in a variance of $[\\beta_1^2 + (1 - \\beta_1)^2]\\sigma^2$, which is less than $\\sigma^2$. While current and past gradient estimates are not fully independent in practice, their local correlation still enables variance reduction through this averaging, which we will show theoretically in Sec. 5.\nWhile this variance reduction effect has been proven in FO optimization (Liu et al., 2020), it is significantly more crucial in ZO optimization. Unlike FO methods that compute gradients directly with relatively low variance, ZO optimization approximates gradients using function evaluations (as in (2)), resulting in inherently noisier estimates. This disparity underscores the critical importance of the variance reduction effect of first moment estimates in ZO optimization, a connection we are the first to identify. We further provide theoretical support for this in Sec. 5."}, {"title": "4.2. Refinement to Second Moment Estimates", "content": "The second key innovation of R-AdaZO lies in its refined second moment estimate, which is crucial for the adaptivity in ZO optimization. Existing adaptive ZO methods (Chen et al., 2019; Nazari et al., 2020) update the second moment estimate directly using the squared noisy gradient estimates:\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$.\nHowever, this approach can be suboptimal in the ZO setting owing to the inherent high variance of the gradient estimates in (2), which could lead to unstable and unreliable second moment estimates. We thus propose to address this issue by simply leveraging the variance-reduced gradient information from the first moment. That is, we update the second moment estimate as below, which interestingly shares similar form with RMSProp (Hinton, 2012).\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)m_t^2$.\nThe first moment estimate, as revealed in Sec. 4.1, acts as a variance reduction mechanism by averaging historical gradient information. Using the squared first moment estimate then probably provides a smoothed and more stable second moment estimate. This refinement therefore may enable a more accurate representation for the underlying geometry of the optimization landscape, resulting in more effective scaling of ZO updates and thus accelerated convergence. Specifically, consider a scenario where $E[m_t] = E[g_t]$ but $m_t$ has significantly lower variance than $g_t$ due to the smoothing effect, given the same $v_{t-1}$, we can see that the refined $v_t$ in (6) achieves a smaller expected value compared to the standard one in (5). Hence, the update step (see (7)) using this refined $v_t$ in (6) is likely to be larger, allowing the algorithm to move faster towards the optimum. This claim will be rigorously established in Sec. 5."}, {"title": "4.3. Final Algorithm", "content": "Given the first and second moment estimates in (4) and (6) respectively, R-AdaZO updates parameters by:\n$\\theta_t = \\theta_{t-1} - \\eta \\frac{m_t}{\\sqrt{v_t + \\zeta}}$\nwhere $\\eta$ is the base learning rate and $\\zeta$ is a small constant for numerical stability. This update rule adaptively scales the effective learning rate based on the local geometry while incorporating the variance-reduced gradient estimates. The complete R-AdaZO algorithm is detailed in our Algo. 2.\nComputational and Memory Complexity. R-AdaZO incurs the same computational cost of $O(Kd)$ per iteration for moment estimates and ZO updates (excluding function evaluations), and the same memory footprint of $O(d)$ as ZO-AdaMM for moment estimates, where $K$ is the number of function evaluations and $d$ is the dimension of parameter $\\theta$."}, {"title": "5. Theoretical Analysis", "content": "This section presents a theoretical foundation for the efficacy of R-AdaZO. We structure our analysis as follows: First, we introduce the required assumptions and preliminaries (Sec.5.1). Second, we prove the variance reduction in first moment estimate and the improvement of our refined second moment in R-AdaZO (Sec. 5.2). Finally, we present the first variance-aware convergence framework for adaptive ZO methods and demonstrate the improved convergence of R-AdaZO over other baselines (Sec. 5.3)."}, {"title": "5.1. Assumptions and Preliminaries", "content": "Our theoretical framework is built upon two fundamental assumptions concerning the non-convex function $F$. We impose a bounded function value as well as a coordinate-wise Lipschitz smoothness (Assump. 1), with a bounded variance of function values (Assump. 2). Of note, coordinate-wise Lipschitz smoothness is commonly used in the analysis of FO adaptive gradient methods, e.g., (Zhang et al., 2024a; Wang et al., 2024).\nAssumption 1. $\\forall \\theta, \\theta' \\in \\mathbb{R}^d$ and $\\forall i \\in [d]$,\n$|f(\\theta,\\xi)| \\le C$,\n$|\\nabla_i F(\\theta) - \\nabla_i F(\\theta')| \\le L \\|\\theta - \\theta'\\|$.\nAssumption 2. $\\forall \\theta \\in \\mathbb{R}^d$,\n$E_{\\xi} [|f(\\theta,\\xi) - F(\\theta)|^2] \\le \\sigma^2$.\nDirectly establishing the convergence of R-AdaZO through the function $F$ presents a primary challenge for adaptive ZO methods, due to the bias (i.e., $E_{\\xi} [\\nabla f(\\theta,\\xi)] \\neq \\nabla F(\\theta)$) arising from the gradient estimation in (2). Thus, we innovatively propose to prove the convergence of R-AdaZO with respect to the randomized smoothing function $F_\\mu$ defined in (11) where $u$ is a random vector drawn uniformly from the sphere of a unit ball $\\mathbb{S}$ and $\\mu > 0$ is a smoothing parameter.\n$F_\\mu(\\theta) \\triangleq E_{u\\sim \\mathbb{S}} [F(\\theta + \\mu u)]$.\nWe introduce the following Lemma 5.1 (proof in Appx. A.1) to justify why $F_\\mu$, instead of $F$, servers as a better choice for the convergence framework of adaptive ZO methods."}, {"title": "5.2. Analysis on First and Second Moment Estimates", "content": "We first theoretically show the underlying variance reduction effect of first moment estimate in (4) using variance $\\Sigma^2$ defined below in Thm. 5.3 (proof in Appx. A.3).\n$\\Sigma^2 \\triangleq \\frac{8(\\sigma^2 + C^2)d}{K\\mu^2}$.\nTheorem 5.3. Given first and second moment estimates (4) and (6) respectively, with Assump. 1, 2, $\\forall t \\ge 1$ and $\\forall i \\in [d]$,\n$E [|m_{t,i} - \\nabla_i F_\\mu(\\theta_{t-1})|^2] \\le \\frac{1 - \\beta_1}{1 + \\beta_1} \\Sigma^2 + \\frac{\\beta_1(1 + \\beta_1) L^2\\eta^2d}{(1 - \\beta_1)^2(1 - \\beta_2)} + \\beta_1 E [|\\nabla_i F_\\mu(\\theta_{t-1})|^2]$.\nRemark. To the best of our knowledge, this theorem provides the first fundamental variance-bias decomposition for the first moment estimate in adaptive ZO algorithms. The variance, given by $\\frac{1-\\beta_1}{1 + \\beta_1} \\Sigma^2$, arises from the randomness in gradient estimator (2) and reduces $\\Sigma^2$ in (15) by a factor\nof $\\frac{1-\\beta_1}{1 + \\beta_1}$, which can be further improved with a large $\\beta_1$. This thus theoretically demonstrates the variance reduction effect of first moment estimate in (4), which goes beyond increasing $K$ to reduce variance. The bias, stemming from the difference between current and past estimates, can be reduced by using a small learning rate $\\eta$, which limits the magnitude of update steps, or a small $\\beta_1$, which reduces the influence of past estimates. So, this decomposition unveils a fundamental trade-off controlled by the utilization (i.e., $\\beta_1$) of past estimates between variance and bias. Particularly, when $\\beta_1 = 0$, (17) simplifies to (15).\nWe then theoretically show that our refined second moment update in (6) is likely to be a more accurate approximation to its variance-free ideal in (18) and hence may better capture the underlying geometry of optimization landscape than (5) used in ZO-AdaMM, with the following Thm. 5.4 (proof in Appx. A.4) and Cor. 5.5 (proof in Appx. A.5).\n$\\nu_{t,i} = \\beta_2 \\nu_{0,i} + \\sum_{\\tau=1}^t (1 - \\beta_2) \\beta_1^{t-\\tau} |\\nabla_i F_\\mu(\\theta_{\\tau-1})|^2$.\nTheorem 5.4. Given second moment estimate (6), with Assump. 1, 2, $\\forall t \\ge 1$ and $\\forall i \\in [d]$,\n$E [\\nu_{t,i}] \\le \\beta_2 \\nu_{0,i} + (1 - \\beta_1) \\frac{\\Sigma^2 + \\frac{\\beta_1(1 + \\beta_1)^2 L^2 \\eta^2d}{(1 - \\beta_1)^2(1 - \\beta_2)}}{2} \\sum_{\\tau=1}^t (1 - \\beta_2) E [|\\nabla_i F_\\mu(\\theta_{\\tau-1})|^2]$.\nCorollary 5.5. Given second moment estimate in (5), with Assump. 1, 2, $\\forall t \\ge 1$ and $\\forall i \\in [d]$,\n$E [\\nu_{t,i}] \\le \\beta_2 \\nu_{0,i} + (1 + \\beta_1) \\Sigma^2 + \\frac{(1 + \\beta_1)^2}{\\beta_1} \\sum_{\\tau=1}^t (1 - \\beta_2) E [|\\nabla_i F_\\mu(\\theta_{\\tau-1})|^2]$.\nRemark. Thm. 5.4 introduces a novel variance-dependent upper bound for our refined second moment estimate (6). Compared with the bound (20) in Cor. 5.5 for the conventional estimate (5), our (6) reduces the influence of gradient estimation variance $\\Sigma^2$ (in green ) by a factor of $\\frac{1 - \\beta_1}{1 + \\beta_1}$. This is crucial in ZO optimization where $\\Sigma^2$ is typically large. While our estimate introduces a bias (in orange ), it is small with a small learning rate $\\eta$. Note that (18) represents the variance-free ideal, which the conventional estimate (5) and our refined estimate (6) aims to approximate. Comparing the bounds in (19) and (20) with (18), our refined estimate (6) better approaches this ideal than (5), particularly when $\\Sigma^2$ dominates, thanks to its reduced impact of $\\Sigma^2$. This thus enables a better capture of geometry information during optimization and probably leads to improved optimization."}, {"title": "5.3. Variance-Aware Convergence Analysis", "content": "This section presents the first variance-aware convergence framework for adaptive ZO methods, particularly focusing on the convergence of R-AdaZO and ZO-AdaMM. We first bound the averaged gradient norm of the smoothed function, $F_\\mu$, as a step towards bounding the averaged gradient norm of the original function $F$. Inspired by (Zhang et al., 2024a), the core proof idea lies in applying H\u00f6lder's inequality to decomposes this target into two components (Lemma 5.6): One involving the averaged square root norm of second moment estimate that will be variance-dependent and another involving a normalized gradient norm by second moment estimate. The subsequent analysis then focuses on bounding these two components using Lemma 5.7 and Thm. 5.8, respectively. By combining these bounds and incorporating the connection between $\\nabla F$ and $\\nabla F_\\mu$ in Lemma 5.1, we arrive at the final convergence results for R-AdaZO (Thm. 5.9) and ZO-AdaMM (Cor. 5.10).\nWe first introduce Lemma 5.6 (proof in Appx. A.6).\nLemma 5.6. $\\forall t > 1$, we have that\n$\\frac{1}{T} \\sum_{t=0}^{T-1} E [\\|\\nabla F_\\mu(\\theta_t)\\|] \\le \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} E [\\sqrt{\\beta_2 \\|\\nu_t\\| + \\zeta}]} \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} E [\\frac{\\|\\nabla F_\\mu(\\theta_{t-1})\\|^2}{\\sqrt{\\beta_2 \\|\\nu_t\\| + \\zeta}}]}$\nRemark. Chen et al. (2019); Nazari et al. (2020) bound B solely to demonstrate the convergence of adaptive ZO methods. However, we argue that this bound alone fail to include the effects of second moment estimate and therefore provides incomplete convergence information. In contrast, Lemma 5.6 allows us to include the effects of second moment (i.e., A) and directly bound a more relevant quantity, $\\frac{1}{T} \\sum_{t=0}^{T-1} E [\\|\\nabla F_\\mu(\\theta_t)\\|]$. Note that this metric is a more widely accepted convergence criteria in optimization theory, directly measuring the distance to a stationary point (Arjevani et al., 2023; Zhang et al., 2024a). Overall, Lemma 5.6 enables us to provide a variance-aware convergence analysis, strengthening the understanding of convergence behavior for adaptive ZO methods.\nLeveraging Lemma 5.6, we then proceed to bound the terms A and B in Lemma 5.7 (proof in Appx. A.7) and Lemma 5.8 (proof in Appx. A.8), respectively. These results rely on the following definition of V resulted from Thm. 5.4.\n$V^2 \\triangleq \\|\\nu_0\\| + (1 - \\beta_1) \\frac{8(\\sigma^2 + C^2)d}{K\\mu^2} + \\frac{\\beta_1(1 + \\beta_1)^2 L^2 \\eta^2d}{(1 - \\beta_1)^2(1 - \\beta_2)}$"}, {"title": "6. Experiments", "content": "In this section, we conduct extensive experiments on various tasks, including synthetic functions (Sec. 6.1), black-box adversarial attack (Sec. 6.2), and memory-efficient LLM fine-tuning (Sec. 6.3), to show the efficacy of R-AdaZO."}, {"title": "6.1. Synthetic Functions", "content": "On Convergence. We first compare the convergence of R-AdaZO with ZO-AdaMM and ZO-RMSProp, an integration of RMSProp (Hinton, 2012) and ZO gradient estimator, using four synthetic functions with d=104, including Quadratic, Rosenbrock, Rastrigin, and Ackley function. We refer to Appx. B.1 for more details. The results are in Fig. 1, showing that R-AdaZO consistently achieves significantly faster convergence and lower optimality gaps compared to ZO-RMSProp and ZO-AdaMM. Specifically, R-AdaZO demonstrated approximately 3.75\u00d7 for Quadratic, Rosenbrock, and Rastrigin (or 2.5\u00d7 for Ackley) speedup in reducing the optimality gap to those achieved by ZO-RMSProp after 104 iterations. This consistent gain across all functions suggests that R-AdaZO is robust to the structure of the underlying problem. Furthermore, Fig. 1 reveals a notable similarity in the convergence behavior of ZO-AdaMM and ZO-RMSProp across all four benchmark functions. In contrast, R-AdaZO consistently demonstrates a substantial speedup compared to ZO-RMSProp. These results imply that the first moment itself contributes minimally to the convergence gains for adaptive ZO optimization, and underscores the critical role of our refined second moment estimate in achieving the superior performance of R-AdaZO.\nOn First Moment. We further conduct an experimental analysis to understand how $\\beta_1$ affects first moment estimates during the optimization process of the Quadratic function. In Fig. 2 (a), we present the results, using cosine similarity to measure the alignment between the estimated gradient"}, {"title": "6.2. Black-Box Adversarial Attack", "content": "Following the practice in (Shu et al., 2023), we also present a comparative analysis of the number of iterations required for successful black-box adversarial attacks on an image from the MNIST dataset (Lecun et al., 1998), using ZO-RMSProp, ZO-AdaMM, and R-AdaZO in Tab. 1 (experimental setup in Appx. B.2). As shown in the table, ZO-RMSProp and ZO-AdaMM exhibit similar performance, requiring an average of approximately 15.6 and 15.5 thousand iterations, respectively. The standard deviations of the iteration counts were similar as well, about 3200 to 4100 iterations. These align with our results in Sec. 6.1. On the other hand, R-AdaZO requires a significantly lower number of iterations with an average of only 2900, and a smaller standard deviation of 800 iterations, suggesting a faster and more stable convergence. The speedup achieved by R-AdaZO, i.e., a speedup of 5.4\u00d7 compared to the baseline ZO-RMSProp, is also highlighted in Tab. 1. These findings thus further underscore the superior efficacy of R-AdaZO."}, {"title": "6.3. Memory-Efficient LLM Fine-Tuning", "content": "Recent interest in memory-efficient fine-tuning of large language models using ZO optimization (Malladi et al., 2023; Zhang et al., 2024b) motivates our use of this setting to further demonstrate the superiority of R-AdaZO over other adaptive ZO optimization algorithms (experimental setup in Appx. B.3). The results in Fig. 3 show that, for both OPT-1.3B and OPT-13B models (Zhang et al., 2022), R-AdaZO converges significantly faster than ZO-RMSProp and ZO-AdaMM, achieving a speedup of 4.29\u00d7 on OPT-1.3B and 3.75\u00d7 on OPT-13B to reach the same training loss. The optimization curves of ZO-RMSProp and ZO-AdaMM are indistinguishable, indicating the similar convergence behavior we have seen in Sec. 6.1 and Sec. 6.2. These empirical results strongly support R-AdaZO as a more efficient and effective adaptive ZO optimizer."}, {"title": "7. Conclusion", "content": "In conclusion, this work introduces R-AdaZO, a novel approach that addresses the critical limitations of existing adaptive ZO methods by effectively leveraging moment information. Through rigorous theoretical analysis, we have demonstrated the inherent variance reduction effect of first moment estimates on ZO gradient estimates, leading to more stable and accurate updates, as well as the improved accuracy of our refined second moment estimates. Furthermore, we establish the first variance-aware convergence framework for adaptive ZO methods and prove the superior convergence rate of R-AdaZO. The consistent empirical performance gains of R-AdaZO across diverse applications underscore its potential as a powerful and practical solution for real-world ZO optimization challenges."}, {"title": "A. Proofs", "content": null}, {"title": "A.1. Proof of Lemma 5.1", "content": "Based on the definition of $\\nabla f(\\theta, \\xi)$ in (2), we first prove (12) in Lemma 5.1 as below,\n$E_{\\xi} [\\nabla f(\\theta, \\xi)] = E_{\\xi} [\\frac{d}{K} \\sum_{k=1}^K \\frac{f(\\theta + \\mu u_k; \\xi) - f(\\theta; \\xi)}{\\mu} u_k]$\n$= \\frac{d}{K} \\sum_{k=1}^K E_{\\xi} [\\frac{f(\\theta + \\mu u_k; \\xi) - f(\\theta; \\xi)}{\\mu} u_k]$\n$= \\frac{d}{K} \\sum_{k=1}^K E_{u \\sim \\mathbb{S}} [\\frac{E_{\\xi} f(\\theta + \\mu u_k; \\xi) - E_{\\xi} f(\\theta; \\xi)}{\\mu} u_k]$\n$= \\frac{d}{K} \\sum_{k=1}^K \\frac{F(\\theta + \\mu u_k) - F(\\theta)}{\\mu} E_{u \\sim \\mathbb{S}} [u_k]$\n$= \\nabla F_\\mu(\\theta)$\nwhere the third equality is due to the fact that $E_{u \\sim \\mathbb{S}} \\frac{F(\\theta + \\mu u_k)}{\\mu} u_k = \\nabla F_\\mu(\\theta)$, which comes from Lemma 1 in (Flaxman et al., 2004).\nWe then prove (13) in Lemma 5.1 as below,\n$E [\\|\\nabla F(\\theta) - \\nabla F_\\mu(\\theta)\\|] \\stackrel{(a)}{=} E [\\|E_{u \\sim \\mathbb{S}} [\\nabla F(\\theta) - \\nabla F(\\theta + \\mu u)] \\|]$\n$\\stackrel{(b)}{\\le} E [\\|\\nabla F(\\theta) - \\nabla F(\\theta + \\mu u) \\|]$\n$\\stackrel{(c)}{\\le} E[\\mu L \\sqrt{d} \\|u\\|]$\n$= \\mu L \\sqrt{d}$\nwhere (a) comes from the Leibniz's Rule, (b) results from Jensen's inequality, (c) is based on Assump. 1, and (d) is due to the fact that $\\|u\\| = 1$. We therefore conclude our proof for Lemma 5.1."}, {"title": "A.2. Proof of Lemma 5.2", "content": "With Leibniz's Rule, Jensen's inequality, and (d) Assump. 1, the following holds for (14) in Lemma 5.2:\n$|\\nabla_i F_\\mu(\\theta) - \\nabla_i F_\\mu(\\theta')| = | \\nabla_i E_{u \\sim \\mathbb{S}} [F(\\theta + \\mu u) - F(\\theta' + \\mu u)] |$\n$\\le |E_{u \\sim \\mathbb{S}} [\\nabla_i F(\\theta + \\mu u) - \\nabla_i F(\\theta' + \\mu u)] |$\n$\\le E_{u \\sim \\mathbb{S}} [|\\nabla_i F(\\theta + \\mu u) - \\nabla_i F(\\theta' + \\mu u)|]$\n$\\le L \\|\\theta - \\theta' \\|$."}, {"title": "A.3. Proof of Thm. 5.3", "content": "We first show the following variance reduction effect in first moment estimate based on the definition of $\\Sigma^2$ in (??):\n$E [|m_{t,i} - E [m_{t,i}]|^2] \\stackrel{(a)}{=} E [(\\frac{d}{K} \\sum_{\\tau=1}^t (1 - \\beta_1) \\beta_1^{t-\\tau} \\nabla_i f(\\theta_{\\tau-1}, \\xi) - E [\\nabla_i f(\\theta_{\\tau-1}, \\xi)])^2]$\n$\\stackrel{(b)}{=} (1 - \\beta_1)^2 \\sum_{\\tau=1}^t \\beta_1^{2(t-\\tau)} E [(\\nabla_i f(\\theta_{\\tau-1}, \\xi) - \\nabla_i F_{\\mu}(\\theta_{\\tau-1}))^2]$\n$\\stackrel{(c)}{\\le} (1 - \\beta_1)^2 (1 - \\beta_1^2) \\frac{\\Sigma^2}{1 + \\beta_1}$.\nRemark. As suggested by (31), the standard bias correction term (i.e., 1 \u2013 \u03b21) in Adam (Kingma & Ba, 2015) is intentionally excluded to avoid compromising the variance reduction effect."}, {"title": "A.4. Proof of Thm. 5.4", "content": "Based on our Thm. 5.3, we naturally can bound $m_{t,i}^2$, as below\n$E [|m_{t,i}|^2", "F_{\\mu}(\\theta_{t-1})|^2": "n$\\le (1 + \\frac{1}{\\beta_1})E [|m_{t,i} - \\nabla_i F_{\\mu}(\\theta_{t-1})|^2"}, {"F_{\\mu}(\\theta_{t-1})|^2": "n$\\le (1 - \\beta_1) \\frac{\\Sigma^2 + \\frac{\\beta_1(1 + \\beta_1)^2 L^2 \\eta^2d}{(1 - \\beta_1)^2(1 - \\beta_2)}}{2} + (1 + \\beta_1) E [|\\nabla_i F"}]}