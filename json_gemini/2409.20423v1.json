{"title": "Stream-level flow matching from a Bayesian decision theoretic perspective", "authors": ["Ganchao Wei", "Li Ma"], "abstract": "Flow matching (FM) is a family of training algorithms for fitting continuous normalizing flows (CNFs). A standard approach to FM, called conditional flow matching (CFM), exploits the fact that the marginal vector field of a CNF can be learned by fitting least-square regression to the so-called conditional vector field specified given one or both ends of the flow path. We show that viewing CFM training from a Bayesian decision theoretic perspective on parameter estimation opens the door to generalizations of CFM algorithms. We propose one such extension by introducing a CFM algorithm based on defining conditional probability paths given what we refer to as \"streams\", instances of latent stochastic paths that connect pairs of noise and observed data. Further, we advocates the modeling of these latent streams using Gaussian processes (GPs). The unique distributional properties of GPs, and in particular the fact that the velocities of a GP is still a GP, allows drawing samples from the resulting stream-augmented conditional probability path without simulating the actual streams, and hence the \u201csimulation-free\" nature of CFM training is preserved. We show that this generalization of the CFM can substantially reduce the variance in the estimated marginal vector field at a moderate computational cost, thereby improving the quality of the generated samples under common metrics. Additionally, we show that adopting the GP on the streams allows for flexibly linking multiple related training data points (e.g., time series) and incorporating additional prior information. We empirically validate our claim through both simulations and applications to two hand-written image datasets.", "sections": [{"title": "1 Introduction", "content": "Deep generative models aim to estimate and sample from an unknown probability distribution. Continuous normalizing flows (CNFs, Chen et al. (2018)) construct an invertible and differentiable mapping, using neural ordinary differential equation (ODE), between a source and the target distribution. However, traditionally it has been difficult to scale CNF training to large datasets (Chen"}, {"title": "2 A Bayesian Decision Theoretic Perspective on Flow Matching", "content": "We start by viewing FM from a Bayesian decision theoretic perspective. Consider i.i.d. training observations from an unknown population distribution $q_1$ over $\\mathbb{R}^d$. The prime objective is to generate new samples from $q_1$ based on the training data. A CNF is a time-dependent differomorphic map $\\phi_t$ that transforms a random variable $x_0 \\in \\mathbb{R}^d$ from a source distribution $q_0$ into a random variable from $q_1$. The CNF induces a distribution of $x_t = \\phi_t(x_0)$ at each time $t$, which is denoted by $p_t$, thereby forming a so-called probability path ${p_t : 0 \\le t \\le 1}$. This probability path should (at least approximately) satisfy the boundary conditions $p_0 = q_0$ and $p_1 = q_1$. It is related to the flow map through the change-of-variable formula or the push-forward equation\n\n$p_t = [\\phi_t]_*p_0$.\n\nInstead of directly learning a flow $\\phi_t$, FM aims at learning the corresponding vector field $u_t(x)$, which induces the probability path over time by satisfying the continuity equation (Villani, 2008).\n\nThe key observation underlying FM algorithms is that the vector field $u_t(x)$ can be written as a conditional expectation involving a conditional vector field $u_t(x|z)$, which induces a conditional probability path $p_t(\\cdot|z)$ corresponding to the conditional distribution of $\\phi_t(x)$ given $z$. Here, $z$ is the conditioning latent variable, which can be the target sample $x_1$ (e.g. Ho et al. (2020); Song et al. (2021); Lipman et al. (2023),), or can be a pair of $(x_0, x_1)$ on source and target distribution (e.g. Liu et al. (2023b); Tong et al. (2024a)). Specifically, Tong et al. (2024a), generalizing the result from Lipman et al. (2023), showed that\n\n$u_t(x) = \\int \\frac{u_t(x|z)p_t(x|z)q(z)}{p_t(x)} dz = E(u_t(x|z) | x_t = x)$,\n\nwhere the expectation is taken over $z$, which one can recognize is the conditional expectation of $u_t(x|z)$ conditional on the event that $x_t = x$. The integral is the with respect to the conditional distribution of $z$ given $x_t = x$.\n\nIt is well-known in Bayesian decision theory (Berger, 1985) that under squared error loss, the Bayesian estimator, which minimizes both the posterior expected loss (which conditions on the data and integrates out the parameters) and the"}, {"title": "3 Stream-level Flow Matching", "content": "Define a stream $s$ to be a complete path that connects one end $x_0$ to the other $x_1$ over time. That is, $s = {x_t : 0 \\le t \\le 1}$. From here on, $s$ will take the space of the latent quantity $z$. To avoid confusion, we shall use the notation $x_t(s)$ to represent the value of a given stream $s$ at time $t."}, {"title": "3.1 A per-stream perspective on flow matching", "content": "Instead of defining a conditional probability path and vector field given one endpoint at $t = 1$ (Lipman et al., 2023) or two endpoints at $t = 0$ and $1$ (Tong et al., 2024a), we shall consider defining it given the whole stream connecting the two ends. To this end, we need to specify a probability model for $s$. This can be separated into two parts\u2014the marginal model on the endpoints $\\pi(x_0, x_1)$, which has been considered in Tong et al. (2024a), and the conditional model for $s$ given the two ends. That is\n\n$(x_0, x_1) \\sim \\pi \\quad \\text{and} \\quad s | x_0, x_1 \\sim p_s(\\cdot | x_0, x_1)$.\n\nOur model and algorithm will generally apply to any choice of $\\pi$ that satisfies the boundary condition, which includes all of the examples considered in Tong et al. (2024a). We defer the description of specific choices of $p_s(x_0, x_1)$ to the next section and for now focus on the general framework.\n\nGiven a stream $s$, we define a \"per-stream\" vector field $u_t(x|s)$, which is the \"velocity\" (or derivative) of the stream at time $t$, conditional on the event that $x_t(s) = x$, i.e, the stream $s$ passes through $x$ at time $t$. Specifically, assuming that the stream is differentiable with respect to time, the per-stream vector field is\n\n$u_t(x|s) := \\dot{x}_t(s) = dx_t(s)/dt$,\n\nwhich is defined only for all pairs of $(t,x)$ that satisfy $x_t(s) = x$. The per- stream view extends previous CFM conditioning on endpoints and provides more flexibility. See Appendix A for more detailed discussion on how the per-stream perspective relates to the per-sample perspective on FM.\n\nWhile the endpoint of the stream $x_1$ is an actual observation in the training data, for the task of learning the marginal vector field $u_t(x)$, one can think of our \"data\" as the event that a stream $s$ passes through a point $x$ at time $t$, that is $x_t(s) = x$. Under the squared error loss, the Bayes estimate for the per-stream conditional vector field $u_t(x|s)$ will be the \"posterior\" expectation given the \"data\", which is exactly the marginal vector field\n\n$u_t(x) = E(u_t(x|s) | x_t(s) = x) = E(\\dot{x}_t(s) | x_t(s) = x)$.\n\nFollowing Theorem 3.1 in Tong et al. (2024a), we can show that the marginal vector $u_t(x)$ indeed generates the probability path $p_t(x)$. (See the proof in the Appendix F.) The essence of the proof is to check the continuity equation for the (degenerate) conditional probability path $p_t(x|s)$.\n\nA general stream-level CFM loss for learning $u_t(x)$ is then\n\n$L_{SCFM}(\\theta) = E_{t,q(s), \\delta(x-x_t(s))} ||v_{\\theta}^t(x) - u_t(x|s)||^2 = E_{t,q(s)} ||v_{\\theta}^t(x_t(s)) - \\dot{x}_t(s)||^2$\n\nwhere $q(s)$ is the marginal distribution of $s$ induced by $\\pi(x_0, x_1)$ and $p_s(\\cdot|x_0, x_1)$; $\\delta(x - c)$ is the indicator function for $x = c$. As in previous research such as Lipman et al. (2023); Tong et al. (2024a), we can also show that the gradient of $L_{SCFM}$ equals that of the $L_{FM}$ with details of proof in the Appendix G. However,"}, {"title": "3.2 Choice of the stream model", "content": "Next, we specify the conditional model for the stream given the endpoints $p_s(x_0, x_1)$. The key desiderata for this model is that it should emit streams differentiable with respect to time, with readily available velocity (either ana- lytically or easily computable). Previous methods such as optimal transport (OT) conditional path (Liu et al., 2023b; Lipman et al., 2023; Tong et al., 2024a) can provide rather poor coverage of the $(t, x)$ space, resulting in extensive ex- trapolation of the estimated vector field $v_\\theta^t(x)$. It is thus desirable to consider stochastic models for the streams that ensure the smoothness while allowing streams to diverge and provide more spread-out coverage of the $(t, x)$ space. Previous research suggested that, compared to ODEs, SDEs (Ho et al., 2020; Song et al., 2021) can be more robust in high-dimensional spaces (Tong et al., 2024b; Shi et al., 2023; Liu et al., 2023a), which we believe is partly due to the robustness arising from the regularization induced by the additional stochasticity. Injecting such stochasticity to achieve desirable regularization is an additional consideration in our choice of the stream model.\n\nTo preserve the \"simulation-free\" nature of CFM, we consider models where the joint distribution of the stream and its velocity is available in closed form. In particular, we further explore the streams following Gaussian processes (GPs). A desirable property of GP is that its velocity is also a GP, with mean and covariance directly derived from original GP (Rasmussen and Williams, 2005). This enables efficient joint sampling of $(x_t(s), \\dot{x}_t(s))$ given observations from a GP in stream-level CFM training. By adjusting covariance kernels for the joint GP, one can fine-tune the variance level to control the level of regularization, thereby further improving the estimation of the marginal vector field $u_t(x)$ (Section 4.1). The prior path constraints can also be incorporated into the kernel design. Additionally, GP conditioning on the event that the stream passes through a finite number of intermediate locations between two endpoints again"}, {"title": "4 Numerical experiments", "content": "In this section, we demonstrate the benefits of GP stream models by several simulation examples. Specifically, we show that using GP stream models can substantially improve the generated sample quality at a moderate cost of training time, through tweaking the variance function to reduce sampling variance of the estimated vector field. Moreover, the GP stream model makes it easy to integrate multiple related observations along the time scale."}, {"title": "4.1 Adjusting GP variance for high quality samples", "content": "We first show that one can reduce the variance in estimating $u_t(x)$ by incor- porating additional stochasticity in the sampling of streams with appropriate GP kernels. As illustrated in Figure 1A, for estimating 2-Gaussian mixtures from standard Gaussian noise, the straight conditional stream used in I-CFM covers a relatively narrow region (gray). For points outside the searching region, there are no \"data\" and the neural network $v_\\theta^t(x)$ must be extrapolated. In the sampling stage, this can lead to potential \"leaky\" or outlying samples that are far from the training observations.\n\nFor constructing GP conditional streams, we condition on the endpoints but expand the coverage region (red) by tweaking the kernel function (e.g. decrease the SE bandwidth in this case). This provides a layer of protection against extrapolation errors. We then train the I-CFM and GP-I-CFM 100 times using a 2-hidden layer multi-layer perceptron (MLP) with 100 training samples at $t = 1$, and calculate 2-Wasserstein (W2) distance between generated and test samples. For fair comparison, we set $\\sigma = 0$ for I-CFM and use noise-free GP-I-CFM. The results are summarized in Table 1B. Empirically, the GP-I-CFM has smaller W2 distance than I-CFM. We further generate 1000 samples and streams for I-CFM and GP-I-CFM with largest W2 distance in Figure 1C, starting with the same points from standard Gaussian. In this example, several outliers are generated"}, {"title": "4.2 Incorporating multiple related training observations", "content": "Next we show that using GP streams allow us to flexibly include related obser- vations along the same stream over time. This is especially useful for generating related samples such as time series (e.g. videos), where the correlation of ob- servations can help borrowing information across observations. Incorporating the \"temporal\" correlation can potentially improve the estimation for each time point.\n\nTo illustrate the main idea, we consider 100 paired observations and place the two observations in each pair at $t = 0.5$ and $t = 1$ respectively (Figure 3A) while $t = 0$ still corresponds to a source distribution. Here, we show the generated samples (at $t = 0.5$ and $t = 1$) and the corresponding streams for GP-I-CFM and I-CFM."}, {"title": "5 Applications", "content": "We apply our GP-based CFM methods to two hand-written image datasets (MNIST and HWD+), to illustrate how GP-based algorithms 1) reduce sampling variance and 2) flexibly incorporate multiple related observations, and generate smooth transformation across different time points."}, {"title": "5.1 Application to MNIST database", "content": "We explore the empirical benefits of variance reduction by using FM with GP conditional streams on the MNIST database (Deng, 2012). Four algorithms are considered: two linear stream models (I-CFM, OT-CFM) and two GP stream models (GP-I-CFM, GP-OT-CFM). For a fair comparison, we set $\\sigma = 0$ for linear stream models and use noise-free GP stream models. For all models, U-Nets (Ronneberger et al., 2015) with 32 channels and 1 residual block are used. Figure 5A shows the 10 generated images for each trained model, starting from the same standard Gaussian noise. Compared to I-CFM, the OT version jointly samples two endpoints by 2-Wasserstein optimal transport (OT) map $\\pi$ (Tong et al., 2024a). Here, we demonstrate how much the GP stream-level CFM can further improve the estimation. We train each algorithm 100 times, and calculate the kernel inception distance (KID) (Bi\u0144kowski et al., 2018) and Fr\u00e9chet inception distance (FID) (Heusel et al., 2017). The histograms in Figure 5B show distribution of these 100 KIDs and FIDs, with results summarized in Figure 5C. According to KID and FID, the independent sampling algorithms (I-algorithms) are comparable to optimal transport sampling algorithms (OT- algorithms). However, algorithms using GP conditional stream exhibit lower standard error and fewer extreme values for KID and FID, thereby reducing the occurrence of outlier samples, as illustrated in Figure 1)."}, {"title": "5.2 Application to HWD+ dataset", "content": "Finally, we demonstrate how our GP stream-level CFM can flexibly incorporate related observations (between two endpoints at $t = 0$ and $t = 1$) into one single model and provide smooth transformation across different time points, using the HWD+ dataset (Beaulac and Rosenthal, 2022). The HWD+ dataset contains images of handwritten digits along with writer IDs and characteristics, which are not available in MNIST dataset used in Section5.1.\n\nHere, we consider the task of transforming from \"0\" (at $t = 0$) to \u201c8\u201d (at $t = 0.5$), and then to \u201c6\u201d(at $t = 1$). The intermediate image, \u201c8\u201d, is placed at $t = 0.5$ (artificial time) for \u201csymmetric\u201d transformations. All three images have the same number of samples, totaling 1,358 samples (1,086 for training and 272 for testing) from 97 subjects. Again, U-Nets with 32 channels and 1 residual block are used. Both unlabeled and labeled models (by starting images, to preserve the grouping effect as in Figure 4C) are considered. Each model is trained both by I-CFM and GP-I-CFM. The I-CFM transformation consists of two separate models trained by I-CFM (\"0\" to \"8\" and \"8\" to \"6\"). Noise-free GP-I-CFM and I-CFM with $\\sigma = 0$ are used for fair comparisons. In each training iteration, we randomly select samples within each writer, to preserve the grouping structure of data.\n\nThe traces for 10 generate samples from each algorithm are shown in Figure 6A, where the starting images (\"0\" in the first rows) are generated by an I-CFM from standard Gaussian noise. Visually, the GP-based algorithms generate higher quality images and smoother transformation compared algorithms using linear conditional stream (I-CFM), highlighting the benefit of including correlations across different time points. Additionally, the transformation generally looks"}, {"title": "6 Conclusion", "content": "We have presented a Bayesian decision theoretic perspective to CFM training, which motivates an extension to CFM algorithms based on latent variable modeling. In particular, we adopt GP models on the latent streams. Our GP-CFM algorithm preserves the \u201csimulation-free\" feature of CFM training by exploiting distributional properties of GPs. This generalization not only reduces the sampling variance by expanding coverage of the sampling space in CFM training, but also allows easy integration of multiple related observations to achieve borrowing of strength.\n\nThere are some potential improvements either under GP-CFM frameworks or generally motivated by Bayesian decision theoretic perspective. For example, under GP-CFM framework, current implementations require the complete obser- vations for all time points, which can be rare in time series applications. To deal with the missingness as well as the potential high-dimensionality of the training data, we may fit the GP-CFM in some latent space as in latent diffusion models (Rombach et al., 2022) and latent flow matching (Dao et al., 2023).\n\nWe believe that the Bayesian decision theoretic perspective and GP-CFM gener- alization so motivated open the door to various further improvements of CFM training of CNFs."}, {"title": "Appendices", "content": ""}, {"title": "A Discussion on Per-stream Perspective on Flow Matching", "content": "It is helpful to recognize the relationship between the per-stream vector field and the conditional vector field given one or both endpoints introduced previously in the literature. Specifically, the per-sample vector field in Lipman et al. (2023) corresponds to marginalizing out $s$ given the end point $x_1$, that is $u_t(x|x_1) = E(u_t(x|s) | x_t(s) = x, x_1(s) = x_1)$. Similarly, the conditional vector field of Tong et al. (2024a), corresponds to marginalizing out $s$ given both $x_0$ and $x_1$, that is $u_t(x|x_0, x_1) = E(u_t(x|s) | x_t(s) = x, x_0(s) = x_0, x_1(s) = x_1)$. Furthermore, when $p_s(x_0, x_1)$ is simply a unit-point mass (Dirac) concentrated on the"}, {"title": "B Derivation of joint conditional mean and covariance", "content": "For computational efficiency and ease of implementation, we assume independent GPs across dimensions and present the derivation dimension-wise throughout Appendices. We use $x_i(s)$ to denote the location of stream $s$ at $t$ in dimension $i$, for $i = 1,...,d$. Suppose each dimension of stream $s$ follows a Gaussian process with a differentiable mean function $\\xi^i$ and covariance kernel $c_{ii}$. Then the joint distribution of $x_{t_1,...,t_g}(s) = (x_{t_1}(s),...,x_{t_g}(s))'$ and $\\dot{x}_{t_1,...,t_g}(s) = (\\dot{x}_{t_1}(s),...,\\dot{x}_{t_g}(s))'$ on $g$ time points is\n\n$\\begin{pmatrix} x_{t_1,...,t_g}(s) \\\\ \\dot{x}_{t_1,...,t_g}(s) \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} \\xi_{t_1,...,t_g} \\\\ \\dot{\\xi}_{t_1,...,t_g} \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{12}^T & \\Sigma_{22} \\end{pmatrix} \\right),$ (2)\n\nwhere $\\xi_{t} = \\xi(t), \\dot{\\xi}_{t} = d/dt \\xi(t)$, and covariance $\\Sigma$ is determined by kernel $c_{ii}$. The kernel function for the covariance between $s$ and $\\dot{s}$ in dimension $i$ is $c_{12}(t, t') = \\frac{\\partial c_{11} (t, t')}{\\partial t'}$, and the kernel defining covariance of $\\dot{s}$ is $c_{22} = \\frac{\\partial^2 c (t, t')}{\\partial t \\partial t'}$ (Rasmussen and Williams (2005) Chapter 9.4). The conditional distribution of $(s, \\dot{s})$ in dimension $i$ given $M$ observations $x^{obs}(s)$ is also a (bivariate) Gaussian process. In particular, for"}, {"title": "C Induce optimal transport path from Conditional GP Construction", "content": "In this section, we show how to derive the path in OT-CFM (Tong et al., 2024a) from the conditional GP construction (Appendix B) using linear kernel. Without loss of generality, we show the derivation of \"noise-free\" path with $\\sigma^2 = 0$ (i.e., the rectified flow, Liu et al. (2023b)).\n\nLet $x_{obs} = (x_0, x_1)', \\xi_0 = \\xi_1 = 0$ and $c_{11}(t, t') = \\sigma_0^2 + \\sigma_1^2(t - 1)(t' - 1)$, such that\n\n$\\Sigma = \\begin{pmatrix} \\sigma_0^2 + \\sigma_1^2(t - 1)^2 & \\sigma_1^2(t - 1) \\\\ \\sigma_1^2(t - 1) & \\sigma_1^2 \\end{pmatrix}, \\quad \\Sigma^{-1} = \\frac{1}{\\sigma_0^2 \\sigma_1^2} \\begin{pmatrix} \\sigma_1^2 & -\\sigma_1^2(t - 1) \\\\ -\\sigma_1^2(t - 1) & \\sigma_0^2 + \\sigma_1^2(t - 1)^2 \\end{pmatrix}$.\n\nTherefore,\n\n$\\begin{aligned} \\mu_t &= \\Sigma_{t,obs} \\Sigma_{obs}^{-1} x \\\\ &= \\frac{1}{\\sigma_0^2} \\frac{1}{\\sigma_1^2} \\begin{pmatrix} \\sigma_0^2 + \\sigma_1^2(t - 1)^2 & \\sigma_1^2(t - 1) \\\\ \\sigma_1^2(t - 1) & \\sigma_1^2 \\end{pmatrix}^{-1} \\begin{pmatrix} \\frac{\\sigma_1^2}{\\sigma_0^2} \\\\ 1 \\end{pmatrix} \\\\ &= \\frac{(1 - t)x_0 + tx_1}{\\sigma_0^2 + (t - 1)t x_1^2}. \\end{aligned}\\$\n\n$\\Sigma_t = \\Sigma - \\Sigma_{t,obs} \\Sigma_{obs}^{-1} \\Sigma_{t,obs}^T = 0$"}, {"title": "D Covariance under Squared Exponential kernel", "content": "Throughout this paper, we adopted the squared exponential (SE) kernel, with the same hyper-parameters for each dimension. The kernel defining block covariance for $s, (s, \\dot{s})$ and $\\dot{s}$ in dimension $i$ from Equation 2 are as follows:\n\n$\\begin{aligned} c_{11}(t, t') &= \\sigma^2 exp(-\\frac{(t - t')^2}{2 l^2}) \\\\ c_{12}(t, t') &= -c_{21}(t, t') = (\\frac{t-t'}{l^2}) \\sigma^2 exp(-\\frac{(t - t')^2}{2 l^2}) \\\\ c_{22}(t, t') &= (\\frac{1}{l^2} - \\frac{(t-t')^2}{l^4}) \\sigma^2 exp(-\\frac{(t - t')^2}{2 l^2}). \\end{aligned}\\$"}, {"title": "E A Supplementary Example for Variance Changing over Time", "content": "Here, instead of generating data from standard Gaussian noise, we consider 100 training (unpaired) samples from a 2-Gaussian to another 2-Gaussian (Figure"}, {"title": "F Proof for conditional FM on stream", "content": "Theorem 1. The marginal vector field over stream $u_t(x)$ generates the marginal probability path $p_t(x)$ from initial condition $p_0(x)$.\n\nProof. Denote probability over stream as $q(s) = \\int p(s | x_0, x_1)\\pi(x_0, x_1)d(x_0, x_1)$ and $p_t(x | s) = \\delta(x - x_t(s))$, then\n\n$\\frac{d}{dt} p_t(x) = \\frac{d}{dt} \\int p_t(x|s)q(s)ds$\n\nAssume the integrand is dominated by an integrable function (e.g. is bounded for most application), such that we can exchange limit, derivative and integral (dominated convergence theorem, DCT). Therefore,\n\n$\\frac{d}{dt} = \\int \\frac{d}{dt} p_t(x|s)q(s)ds$"}, {"title": "G Proof for gradient equivalence on stream", "content": "Recall\n\n$L_{FM}(\\theta) = E_{t, p_t(x)}||v_{\\theta}^t(x) - u_t(x)||^2$,\n$L_{SCFM}(\\theta) = E_{t, q(s), \\delta(x-x_t(s))} ||v_{\\theta}^t(x) - u_t(x|s)||^2$,\n\nwhere $q(s) = \\int p(s | x_0, x_1)\\pi(x_0, x_1)d(x_0, x_1)$.\nTheorem 2. $\\nabla_{\\theta} L_{FM}(\\theta) = \\nabla_{\\theta} L_{SCFM}(\\theta)$.\n\nProof. To ensure existence of all integrals and to allow the changes of integral (Fubini's Theorem), we assume that $q(s)$ are decreasing to zero at a sufficient"}, {"title": "H Proof for gradient equivalence conditioning on covariates", "content": "Let $x$ be response, $c$ be covariates, and $s$ be the stream connecting two endpoints $(x_0, x_1)$. Given covariate $c$, denote the conditional distribution of $s$ as $q(s | c) = \\int p(s | x_0, x_1, c)\\pi(x_0, x_1)d(x_0, x_1)$ and marginal conditional probability path as $p_t(x | c)$. Further, let\n\n$L_{CFM}(\\theta) = E_{t, p_t(x|c)}||v_{\\theta}^t(x, c) - u_t(x | c)||^2$\n$L_{CCFM}(\\theta) = E_{t, q(s|c), \\delta(x-x_t(s))} ||v_{\\theta}^t(x, c) - u_t(x | s)||^2$"}]}