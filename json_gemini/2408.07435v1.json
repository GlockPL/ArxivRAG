{"title": "Real-world validation of safe reinforcement learning, model predictive control and decision tree-based home energy management systems", "authors": ["Julian Ruddick", "Glenn Ceusters", "Gilles Van Kriekinge", "Evgenii Genov", "Thierry Coosemans", "Maarten Messagie"], "abstract": "Recent advancements in machine learning based energy management approaches, specifically reinforcement learning with a safety layer (OptLayerPolicy) and a metaheuristic algorithm generating a decision tree control policy (TreeC), have shown promise. However, their effectiveness has only been demonstrated in computer simulations. This paper presents the real-world validation of these methods, comparing against model predictive control and simple rule-based control benchmark. The experiments were conducted on the electrical installation of 4 reproductions of residential houses, which all have their own battery, photovoltaic and dynamic load system emulating a non-controllable electrical load and a controllable electric vehicle charger. The results show that the simple rules, TreeC, and model predictive control-based methods achieved similar costs, with a difference of only 0.6%. The reinforcement learning based method, still in its training phase, obtained a cost 25.5% higher to the other methods. Additional simulations show that the costs can be further reduced by using a more representative", "sections": [{"title": "1. Introduction", "content": "Home energy management systems (EMS) are becoming increasingly relevant as we transition from fossil fuels towards further electrification of our energy systems. This transition is particularly notable with adaption of affordable photovoltaic (PV) systems [1], which encourage renewable energy production at the household level. Alongside PV systems, controllable assets such as batteries, heat pumps and electric vehicles (EV) allow for optimized local consumption of this energy. Electricity tariffs are also evolving [2], with dynamic pricing mechanisms on the household level being introduced in many countries to incentives users to consume energy when it is abundant and cheap. These dynamic prices can vary within the same day, on the"}, {"title": "1.1. Contribution and outline", "content": "The original contributions outlined in this study are considered to be the first of their kind, to the best knowledge of the authors, and can be summarized as follows:\n\u2022 Real-world experimental validation of RL in combination with a safety layer that can handle any constraint type and utilize an a priori safe fallback policy (i.e., OptLayerPolicy [11]).\n\u2022 Real-world experimental validation of a method using a metaheuristic algorithm to generate an interpretable control policy modeled as a decision tree (i.\u0435., TreeC [12]).\n\u2022 Comparison between rule-based control (RBC), MPC, Safe (using OptLayerPolicy) RL and TreeC (on any case study, simulated or otherwise) using an evaluation procedure aimed to be as fair as possible.\nIn section 2 we continue with a concise related work discussion, while section 3 includes a detailed description of the utilized and proposed methodologies (i.e., hardware setup, safety layer, EMSs and evaluation procedure). Section 4 then presents the results and a discussion of those results. Finally, section 5 presents our conclusions and formulates directions for future work."}, {"title": "2. Related work", "content": "Energy management systems (EMS) have been widely reviewed not only in terms of their mathematical modelling approach [13], but also considering their objectives, architecture, involved stakeholders and challenges [14], including for home EMS more specifically [15]. One observation, and also specifically highlighted by Rathor and Saxena [14] and Allwyn et al. [16] in their reviews, is that most work is on simulated case studies and only few show practical real-world implemented results. Nevertheless, notable examples include Zhao et al. [17] who implemented a non-linear MPC in the Hong Kong Zero Carbon Building, that optimized the usage of an electric chiller and combined cooling and power unit, considering a stratified chilled water storage tank as thermal energy storage (and thus source of flexibility) and a PV system under a day-ahead pricing mechanism. Viot et al. [18, 19] implemented an MPC in the SYNERGI building of the University of Bordeaux - France, which optimized the Thermally Activated Building Systems and"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Hardware setup", "content": "The experimental setup consists of four real reproductions of an electrical circuit of a house. Each house has a BESS, a PV installation, an emulated electric load consumption and an emulated EV charging sessions. The houses are connected to the grid and have official digital meters from the local Belgian distribution system operator that measures energy imported from, and exported to, the grid. The BESS and PV installations are different for each house whereas the emulated electric load consumption and charging sessions are identical for all houses. This setup replicates an electrical configuration typical of a modern home with any non-electrical heating system.\nThe electrical loads used for the consumption and charging sessions come from a dynamic load system composed of seven 2000W Perel\u00ae PHP2000 heaters controlled through Showtec Single DP-1 dimmers. The electrical loads are calibrated so that the setpoints matched the real measured active power consumption more closely (we expect this difference to come from measurement\u00b9 inaccuracies and manufacturing tolerances of the loads).\nMeasurement devices for power, voltage and current are present for the PVs, BESSs and grid connections. The power of the electrical loads and other power losses in the setup is calculated via the PV, BESS and grid measurements."}, {"title": "3.2. Experimental setup", "content": ""}, {"title": "3.2.1. Consumption profile", "content": "We utilize the real-time (10 seconds) residential electricity \u201chousehold\" consumption data from Schlemminger et al. [35], focusing specifically on the 2019 dataset, which predates the COVID-19 pandemic. Our experiments employ data from single-family house (SFH) 19, which does not include solar PV, BESS, or EV charging. Additionally, this household dataset excludes heat pump loads. SFH 19 is selected due to its high data availability and quality, along with its typical consumption metrics, averaging 3425 kWh per year.\nWe shift the dataset so, that weekdays align (that a Monday in runtime, is also a Monday in the dataset) and used this for all our hardware-in-the-loop simulated dynamic loads so that each house has the same real-time non-controllable load."}, {"title": "3.2.2. Electric vehicle and driver charging behavior", "content": "The residential electric mobility demand consists of two parts: a) a first part specific to the EV user charging behavior (i.e. arrival time, departure time and energy need), and b) a second part specific to EV charging power profile.\nFirstly, the EV user charging behavior is simulated directly using an existing open-source dataset available in [36]. This dataset comprises data from 97 EV users, encompassing 6878 charging sessions at residential locations spanning from December 2018 to January 2020. Among these users, \u201cBl2-2\u201d has been selected for the experiment due to its alignment with the average EV user in terms of arrival time, parking duration, and energy requirements. However, it distinguishes itself by exhibiting a notably high frequency of charging sessions, rendering it particularly interesting for the experiment as it allows for more interactions with the EMS developed in this paper. From a statistical standpoint, this EV user has undergone 315 charging sessions, with an aggregate energy consumption of 3274.74 kWh across 386 days.\nSecondly, the EV charging power profile is built based on certain variables and constraints. The EV charging power is constrained by (1).\n$$0 < P_{ev} < P_{ev}^{max}$$\nwhere $P_{ev}^{max}$ is the maximum charging power in [kW] set to 7.4 kW, following the IEC 61851 charging standards [37]. Additionally, the maximum charging power follows the Constant-Current/Constant-Voltage (CC-CV) uncoordinated charging method defined in Vagropoulos and Bakirtzis [38]. Such method stipulates that the power is function of the state of charge (SOC) as piece-wise linear function summarized in (2).\n$$P_t^{ev} = \\begin{cases}\nP_{ev}^{max}  & \\text{if } SOC_t^{ev} < SOC_{cc,cv}^{ev}\\\\\n(P_{ev}^{max} - P_{ev}^{min}) * \\frac{SOC_{CC, CV}^{ev} - SOC_{t}^{ev}}{1-SOC_{CC, CV}^{ev}}  & \\text{if } SOC_t^{ev} > SOC_{cc,cv}^{ev}\n\\end{cases}$$\nwhere $SOC_t^{ev}$ is the SOC at time t in [%], $SOC_{cc,cv}^{ev}$ is the transition SOC between the Constant-Current phase and the Constant-Voltage phase in [%] and $P_{ev}^{min}$ is the maximum charging power at 100% SOC, set to 1 kW. Finally, both charging power and SOC variables are linked using the equality constraint (3).\n$$SOC_{t+1}^{ev} = SOC_t^{ev} + \\eta_{charge} \\times \\frac{P_t^{ev} \\times \\Delta t}{E^{ev}}$$"}, {"title": "3.2.3. Tariff", "content": "The real time pricing used in this study is based on a one-year dynamic contract of a major Belgian energy supplier, as if it were taken in April 20242. The total cost is composed of four costs with different pricing mechanisms: the day-ahead cost, the offtake extras cost, the peak cost and the fixed yearly cost.\nThe day-ahead cost depends on the prices of eqs. (4) and (5).\n$$V_{ot} = \\begin{cases}\n(V_{dt} + 0.011) \\frac{\u20ac}{kWh} * 1.06 & \\text{if } (V_{dt} + 0.011) \\frac{\u20ac}{kWh} \\geq 0\\\\\n0  & \\text{otherwise}\n\\end{cases}$$\n$$V_{it} = V_{dt} - 0.009 \\frac{\u20ac}{kWh}$$\nWhere $V_{ot}$ and $V_{it}$ are the offtake and injection prices at time step t. $V_{dt}$ is the Belgian day-ahead hourly price. A value-added tax of 6% is added to the offtake price when it is positive. Extra costs that occur for energy taken from the grid for transport, distribution and taxes are calculated together and labelled as offtake extras cost. The offtake price for all these additional offtake costs $V_f$ is of 0.114 \u20ac/kWh.\nThe monthly peak offtake cost $C_{pm}$ is calculated by eq. (6).\n$$C_{pm} = \\frac{\\Sigma_{t \\in B_m} E_{ot}}{\\Sigma_{t \\in B_m} 0.25h} V_p * max(\\frac{kW}{0.25h}, 2.5kW)$$\nWhere $V_p$ is the peak price of 3.5 \u20ac/kW. This cost adds a penalty to the highest offtake power averaged over a 15-minute time step that occurred for each month. In the context of this study, the monthly peak offtake cost is proportional to the billed period of each month to also include this cost for unfinished months. $B_m$ is the set containing all billed time steps in month m and $N_m$ the total number of time steps in the month. $E_{ot}$ is the energy"}, {"title": "3.2.4. House switching", "content": "As described in section 3.1, the four houses have different BESS and PV installations. Assigning a different EMS to each house and comparing the performance by running them simultaneously would not yield comparable results. One house could obtain better performances because it has a better BESS or PV installation. To avoid this bias, the EMSs are switched every 24 hours to a different house.\nSince there are 4 houses and 4 EMSs to test, there are a total of 24 possible house and EMS combinations. Each combination is tested twice, resulting in 48 days of testing in total. The 48-day schedule is obtained by randomly shuffling the 24 combinations until two distinct 24-day schedules are generated, ensuring no consecutive days have the same reinforcement learning or MPC EMS assigned to the same house. This rule is applied for practical reasons, as the RL and MPC EMSs behaviors depend on how they operated a house previously. Failed test days can occur, and not having an MPC or RL EMS on the same house for consecutive days provides more time to identify a failure while preventing its propagation to the following day.\nThe switching is conducted daily at 15:00. By this time, the BESSs of all houses are charged to 100% to ensure consistent starting conditions post-switch. The EV schedule is adjusted to prevent charging sessions from overlapping with the 15:00 switching time. This is achieved by shortening the"}, {"title": "3.2.5. Enforced charging behavior", "content": "An enforced charging behavior is enforced in order to reach 100% SOC at the switching time for the BESSs and to reach the imposed final SOC of an EV battery at the end of a charging session. The batteries are charged at maximum power to reach their SOC goal at the latest possible time minus a buffer time $B_{time}$. This buffer time is used to avoid not reaching the SOC goal in time due to non-perfect battery models, safety layer activation (which could reduce the battery setpoints, see section 3.3) or other unexpected event. The buffer time value is higher when the difference between the SOC goal and current SOC of the battery is higher because more events could prevent the battery to reach its SOC goal in time. This relation is described in eq. (8).\n$$B_{time} = (SOC_g \u2013 SOC) * (B_{max} \u2014 B_{min}) + B_{min}$$\nWhere $B_{time}$ is the time buffer, $SOC_g$ is the SOC goal of the battery, $SOC$ is the current SOC of the battery, $B_{max}$ is the time buffer enforced if 100% of the battery still needs to be charged and $B_{min}$ is the minimum enforced time buffer. Every 5 seconds, the battery controller calculates with equations eqs. (3) and (11) whether the battery can reach the SOC goal at the intermediate time, defined as the target time minus the time buffer. If not, the battery is charged at maximum power for 1 minute. For both the BESSs and the EV batteries, $B_{min}$ is set to 3 minutes. For the BESSs $B_{max}$ is set to 1 hour and for the EV batteries $B_{max}$ is set to 4 hours. The $B_{min}$ and $B_{max}$ values were obtained by testing in simulation what values would consistently meet the SOC goal in time throughout the 3 months of training data (see section 3.5.1)."}, {"title": "3.3. Safety layer", "content": "Given that we are proposing to demonstrate native unsafe energy management methods, we implement the $OptLayerPolicy$ safety layer from Ceusters et al. [11] - as it is considered one of the state-of-the-art methods. This safety layer results in the minimal invasive correction of predicted (control) actions towards their closest feasible safe set without affecting optimality. One can then also compute the distance from the feasible solution space, and fallback on an a priori safe policy when available. This under the notion that far-from-feasible actions are likely to be far-from-optimal, while close-to-feasible can mean close-to-optimal. This threshold is a hyperparameter of $OptLayerPolicy$ specifically and its effectiveness was shown in a simulated case study by Ceusters et al. [11].\nWhile the safety layer mainly serves the RL-based EMS, it provides general constraint satisfaction guarantees - including for the simple RBC EMS benchmark (that is not necessarily always safe). We choose the same sampling rate for the safety layer as for the hardware-in-the-loop sampling rate, being 5 seconds. By doing so, we effectively utilize it additionally as a realtime optimization step in a cascading (a.k.a. hierarchical) system architecture [40] - including for the MPC-based EMS. Hence, it allows us to use slower sampling rates inside the EMSs (i.e., 15 minutes), where the optimized setpoints are then used by the safety layer as reference signal (only deviating from it, if it is absolutely necessary to subject to the constraints - considering the latest measurements, e.g. when the solar PV in-feed suddenly drops).\nIn our experimental case study, the $OptLayerPolicy$ safety layer is expressed as:\n$$a_{t}^{safe,bess,ev} = arg min_{a^{bess},a^{ev}} \\frac{1}{2} [|({a_t^{bess} - a_{t}^{bess}})| + |({a_t^{ev} - \\tilde{a_t^{ev}}})|]$$\ns.t.\n$$P_t^{grid,min} < P_{t}^{grid} < P_t^{grid,max} \\forall t$$\n$$0 < P_{t}^{ev} < P_{t}^{ev,max} \\forall t$$\n$$P_t^{bess,min} < P_{t}^{bess} < P_t^{bess,max} \\forall t$$\nbeing a Quadratic Program, where $a_t^{bess}$ and $a_t^{ev}$ are the internal optimization variables for the BESS and EV that results in the closest feasible safe actions $a_{t}^{safe,bess}$ and $a_{t}^{safe,ev}$ (i.e., the control variables to the assets), starting from the proposed actions $a_t^{bess}$ and $\\tilde{a_t^{ev}}$ originating from the EMSs (i.e.,"}, {"title": "3.4. Simulation", "content": "A simulation of the homes is implemented in order to train the TreeC EMS and compare the real experiment with simulation. The simulation models using 15-minute time-steps the behavior of the BESS, EV charging, PV, electrical load and grid connection and how they react to different control strategies from EMSs.\nThe simulation model of the BESS is described in eq. (11).\n$$SOC_{t+1}^{bess} = SOC_t^{bess} + \\frac{P_{ch} * \\eta}{E^{bess}} -  \\frac{P_{di}/\\eta}{E^{bess}} \\Delta t$$"}, {"title": "3.5. Energy management systems", "content": "This section presents the four EMS methods used and the data used to train the MPC, RL and TreeC EMSs. All EMSs send a setpoint to the BESS and the EV charger every 15 minutes, how they calculate this setpoint is described in this section."}, {"title": "3.5.1. Training data", "content": "The MPC, RL, and TreeC EMSs required training data and used three months of data spanning from 2024/01/01 at 00:00 to 2024/04/01 at 00:00. The dataset included real PV power production, household power consumption described in section 3.2.1 and EV charging sessions from section 3.2.2.\nThe MPC utilized this data to train its forecasting models while the RL agents were pre-trained on the training data with additional power profiles for the BESSs and EV chargers. The BESSs and EV charger power profiles were generated as if a RBC EMS was controlling these assets. The TreeC EMS used this data in the simulator, described in section 3.4, to construct its decision tree."}, {"title": "3.5.2. Rule-based control", "content": "The RBC EMS implemented in this setup consists of default rules commonly found in many devices. The primary rule for the BESS focuses on self-consumption \u201cbehind-the-meter\". It charges when excess power is available from the PV installation and discharges to meet load demands. The rule for the EV charger is to always charge the EV immediately at maximum allowed power. This EMS serves as a benchmark to evaluate the performance of the other EMSs."}, {"title": "3.5.3. Model predictive control", "content": "MPC is used here as an EMS method that requires a model of the system and forecasts of the system's inputs to optimize control. The MPC EMS in this setup is implemented as a Mixed-Integer Linear Program, which is solved at each (15 minute) time step. The horizon of the MPC is variable and ends at the next switching time. The MPC model is solved every 15 minutes. The variables of the MPC are the power profiles of the BESS and the EV charger as these are the assets it needs to control.\nThe objective function of the MPC is described in eq. (12).\n$$O = \\sum_{t \\in H} (E_{ot}V_{ot} \u2013 E_{it} V_{it}) + \\sum_{t \\in H} E_{ot}V_f +V_p * max(\\frac{\\Sigma_{t \\in H} E_{ot}}{0.25h}, P_p)$$\nWhere H is the set of time-steps within the horizon of the MPC and $P_p$ is the previous grid peak power measured for that house when using the MPC EMS. At the beginning of the experiment, $P_p$ is set to 2.5 kW. This objective function penalizes heavily exceeding the previous grid peak power $P_p$.\nThe MPC system models takes as inputs the time of the day, the SOC of the BESS and EV battery, the Belgian day-ahead hourly prices $V_{dt}$ until the next switching time, the forecasted power profiles until the next switching time of the PV installation and the electric load and the forecasted final SOC and departure time of the connected EV.\nThe constraints of the MPC are the same as the ones defined in eqs. (1), (3), (9b), (9d), (10a) and (11). However, there is a difference in the model of the EV charging behavior defined in eq. (2) as described in eq. (13)."}, {"title": "3.5.4. Reinforcement learning", "content": "We formulate the energy managing RL agent as a fully observable discrete-time Markov Decision Process, characterized by the tuple (S, A, Pa, Ra) so that:\n$S_t = (P_{load}, P_{pv}, SOC_{bess}, SOC_{ev}, V_{dt}, H_t, D_t) \\qquad S_t \\in S$\n$a^{bess} =(a_{min}^{bess} \\rightarrow a_{max}^{bess}) \\qquad a^{bess} \\in A$\n$a_{ev} = (0 \\rightarrow a_{max}^{ev}) \\qquad a^{ev} \\in A$\n$P_a(s, s') = Pr(s_{t+1} = s' | S_t = s, a_t = a)$\n$R_a(s, s') = -(E_{ot} V_{ot} - E_{it} V_{it} + E_{ot}V_f) - z$\nWhere $P_{load}$ is the non-controllable electrical load, $P_{pv}$ the electrical solar in-feed, followed by the SOC of the BESS and EV, $V_{dt}$ the electrical price signal (Belgian day-ahead price), $H_t$ the hour of the day and $D_t$ the day of the week, all at time step t - constituting the state-space S. The continuous action-space A consists of, the control actions $a_{bess}$ for the BESS and $a_{ev}$ for the EV charger. Furthermore, Pa signifies a transition probability, that exclusively relies on the current state and is unaffected by prior states (in other words, adhering to the Markov Property), for when the system is in a specific state s \u2208 S at time step t and takes action a \u2208 A, which would result in the system being in state s' \u2208 S at the subsequent time step t + 1.\nThe objective is formulated, that is, the reward function, so that when maximizing this function we minimize the positive version of that function. Hence, we minimize the day-ahead and offtake extras costs (from eq. (7), yet calculated over the last 15-minute time step) in EUR and with an additional"}, {"title": "3.5.5. Tree C", "content": "TreeC is an EMS method that uses decision trees to control a system. The decision trees are generated using a metaheuristic algorithm. The generated decision trees are evaluated using a simulation of the controlled system over a training period. The metaheuristic algorithm generates better decision trees"}, {"title": "4. Results and discussion", "content": ""}, {"title": "4.1. EMS performance comparison", "content": "The RBC, TreeC and MPC EMSs obtained similar performances in the experiment as shown in fig. 6. The RL EMS obtained a worse performance than the other EMSs which is expected as the RL performed more exploratory actions than the other EMSs. For the three other EMSs, them being close is unexpected but is explained in the following sections.\nNo EMS is close in performance to the MPC P benchmarks indicating better implementations or methods could still be found. The MPC P EMSs obtained scores between \u20ac137.9 and \u20ac139.5 even though they were evaluated on different day house combination (i.e. the same day house combinations than the EMS for which they serve as a benchmark). The score difference is small indicating the comparison procedure is fair."}, {"title": "4.1.1. Rule-based control", "content": "The strategy of the RBC is to do as much self-consumption as possible which means trying to not take electricity off the grid as shown in section 4.1. This strategy is reflected with a comparatively low offtake extras cost which is proportionally the highest cost category for all EMSs."}, {"title": "4.1.2. TreeC", "content": "The TreeC EMS obtained the lowest peak cost but couldn't obtain as low offtake extras and day-ahead costs as the other MPC and RBC EMSs. Looking at the decision trees obtained in figs. 4 and C.12 to C.14, the low peak cost is explainable by lower charging setpoints of the EVs."}, {"title": "4.1.3. Model predictive control", "content": "The MPC EMS obtained the lowest day-ahead costs but couldn't obtain as low of a peak and offtake extras costs as the RBC and TreeC EMSs. The low day-ahead costs is expected as the MPC knows the future Belgian day-ahead prices of electricity and can optimize accordingly. Intuitively it is surprising that the MPC did not optimize the other costs efficiently.\nFigure 6 shows a bigger cost difference between simulation and reality than the other EMSs and obtained lower peak, offtake extras and day-ahead costs. Certain days had cost differences exceeding \u20ac1, as seen in fig. 8. A deeper analysis was performed to understand these differences on three days that had this cost difference for houses 1 and 3. House 4 also had two days when the simulation results were worse than the experiment by more than \u20ac1. However, on one day, the simulation outperformed the experiment by approximately \u20ac2, which mitigates the impact of the two worse days."}, {"title": "4.1.4. Reinforcement learning", "content": "The RL EMS obtained the worst score, mainly due to a higher offtake extras cost (distribution and taxes) as can be seen in fig. 6 - which is not unexpected as we observed early stage learning despite the pre-training, given the new (real) experience tuples."}, {"title": "4.2. Safety layer", "content": "The analysis of the safety layer results (fig. 10) across the different EMS methods shows distinct patterns in the performance, and underlying causes, of both the safety layer itself and the EMS methods.\nThe RBC method experienced the highest number of safety layer activation's, primarily due to arguably a flaw in its simple rule-based strategy. Given that the BESS was always in self-consumption mode, it would quickly drain itself whenever there would be grid consumption (i.e. trying to avoid any grid exchange) - especially during periods of little to no solar production and at times with high loads. The EV charging setpoint then required correcting once the BESS was depleted and a relatively high household load was still present. Near the end of the day, the BESS was enforced to charge back to full (see section 3.2.5). If an EV load was still active, this required simultaneously charging the BESS and the EV, which would easily exceed"}, {"title": "4.3. Difference experiment vs simulations", "content": "The following section analysis differences between simulation and experiment in addition to the ones already pointed out specifically for the MPC EMS in section 4.1.3.\nThe experiment runs real-time while the simulation approximates this behavior on a 15-minute time step. In the experiment, the grid sensor measures both imported and exported energy for a same time step while in simulation there is only one net consumption value per time step. The net consumption is the difference between the imported and exported energy from the grid. Calculating the cost for one time step using both the imported and exported energy instead of the net consumption is always more expensive because offtake extras costs get removed when doing the energy difference for net consumption. Table 5 shows the score obtained in the experiment for each EMS when using the import and export energy values as well as"}, {"title": "5. Conclusion", "content": "This paper presented the results of the real-world experimental validation of 2 novel machine learning EMSs: 1) being RL in combination with a safety layer that can handle any constraint type and that can utilize a safe fallback policy when available (i.e., OptLayerPolicy [11]) and 2) being a method using a metaheuristic algorithm to generate an interpretable control policy modeled as a decision tree (i.e., TreeC [12]). This together with the comparison between an MPC, a Safe RL, an RBC and an explainable Tree-based EMS on any case study, simulated or otherwise using a novel evaluation procedure aimed to be as fair as possible. We come to the following conclusions:\n\u2022 The RBC, TreeC and MPC EMSs obtained a similar economic performance with only a 0.6% difference in cost between the three. The results highlight the importance of representative training data for the TreeC EMS and expert implementation for the MPC EMS to obtain better performances in real implementations;\n\u2022 The OptLayerPolicy safety layer allows to safely train a RL agent online in the real-world, given an accurate constraint function formulation which remains error-prone, also with MPC;\n\u2022 A pre-trained, yet model-free, RL agent still has a considerably long training time (even with hyperparameters chosen for a higher learning rate) and therefore monetary training cost that would need to be set off against the extra (if any) cost of other methods (e.g. modelling time);\n\u2022 The TreeC method showed the safest operational performance (27.1 Wh total grid exceedance compared to 593.9 Wh for RL) in the presented experimental case study, yet does require a simulation model a priori and an extensive offline training. It is also less error-prone, yet requires detailed domain knowledge to predetermine its utility and its operational safety (given it remains unconstrained during deployment, without a safety layer);\n\u2022 A real-time safety layer can be beneficial for multiple (optimal) control methods and could streamline the overall (hierarchical) control architecture (given the lack of PID controllers in the experimental setup);\n\u2022 With the MPC experiment errors reproduced in simulation, the difference in costs between experiment and simulation ranges from 1% to 4.5% for all EMSs; This difference is in part explained by the simulation using a single net consumption value per time step for the energy exchanged with the grid while the experiment measures both imported and exported energy per time step. Overall the simulations rather accurately represented reality in terms of cost and were essential to find the MPC errors and TreeC training data issues.\nFinally, we propose the following directions for future work:\n\u2022 Investigate if artificially increasing the sampling rate of the RL agents would benefit the overall learning rate, to produce more experience tuple more quickly in time (i.e. more time steps over an equal amount of time);\n\u2022 Robustness comparison against faulty and noisy measurements or observations and the requirements (if any) for state estimations;"}]}