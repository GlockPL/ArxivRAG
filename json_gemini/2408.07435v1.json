{"title": "Real-world validation of safe reinforcement learning, model predictive control and decision tree-based home energy management systems", "authors": ["Julian Ruddick", "Glenn Ceusters", "Gilles Van Kriekinge", "Evgenii Genov", "Thierry Coosemans", "Maarten Messagie"], "abstract": "Recent advancements in machine learning based energy management approaches, specifically reinforcement learning with a safety layer (OptLayerPolicy) and a metaheuristic algorithm generating a decision tree control policy (TreeC), have shown promise. However, their effectiveness has only been demonstrated in computer simulations. This paper presents the real-world validation of these methods, comparing against model predictive control and simple rule-based control benchmark. The experiments were conducted on the electrical installation of 4 reproductions of residential houses, which all have their own battery, photovoltaic and dynamic load system emulating a non-controllable electrical load and a controllable electric vehicle charger. The results show that the simple rules, TreeC, and model predictive control-based methods achieved similar costs, with a difference of only 0.6%. The reinforcement learning based method, still in its training phase, obtained a cost 25.5% higher to the other methods. Additional simulations show that the costs can be further reduced by using a more representative", "sections": [{"title": "1. Introduction", "content": "Home energy management systems (EMS) are becoming increasingly relevant as we transition from fossil fuels towards further electrification of our energy systems. This transition is particularly notable with adaption of affordable photovoltaic (PV) systems [1], which encourage renewable energy production at the household level. Alongside PV systems, controllable assets such as batteries, heat pumps and electric vehicles (EV) allow for optimized local consumption of this energy. Electricity tariffs are also evolving [2], with dynamic pricing mechanisms on the household level being introduced in many countries to incentives users to consume energy when it is abundant and cheap. These dynamic prices can vary within the same day, on the spot market, and across days, on the day-ahead market. It is not reasonable to expect a human to manually and consistently adjust their consumption to these volatile tariffs over long periods [3], especially given the uncertainties like significant variance of solar PV generation on partially cloudy days. Therefore, with the adoption of controllable electrical assets and dynamic prices, automated home EMSs become essential for optimizing economic or environmental objectives [4] while ensuring user comfort.\nAchieving and maintaining near-to optimal operation of any controllable system is a substantial undertaking. Adhering to all system constraints, in real-time, is always of primary concern (e.g. to avoid power outages in electrical system or to avoid thermal discomfort in heating systems). Pursuing a specific objective is then only of secondary concern (e.g. minimizing energy costs or CO2-equivalent emissions). In addition, different time dependencies could be required in the mathematical formulation, like with energy storage systems and controllable loads, which in principle would require an infinite optimization horizon [5]. All in the face of various uncertainties, such as fluctuating demands, weather conditions and prices, and so optimizing the expectation in typically continuous systems.\nTo tackle these complexities, model predictive control (MPC) has become a widely accepted and utilized optimal control technology in various industries [6], including for building management [7]. MPC method is well-established and studied in terms of feasibility, stability, robustness and constraint handling [8]. Nevertheless, it necessitates a detailed model a priori (such as plant models, input/output disturbance models and measurement noise models), which is error-prone and usually lacks adaptability. Additionally, it might be not financially feasible to build, configure and maintain these models, especially in smaller systems [9]. Recent advancements in machine learning approaches [10] have shown promising results in terms of modeling requirements, model convexity, adaptivity, constraint handling and system integration requirements. More specifically under consideration here, (1) using reinforcement learning (RL) in combination with a safety layer that can handle any constraint type and utilize an a priori safe fallback policy when available (i.e., OptLayerPolicy [11]) and (2) using a metaheuristic algorithm to generate an interpretable control policy modeled as a decision tree (i.\u0435., TreeC [12]). However, the effectiveness of these methods has only previously been shown in computer simulations."}, {"title": "1.1. Contribution and outline", "content": "The original contributions outlined in this study are considered to be the first of their kind, to the best knowledge of the authors, and can be summarized as follows:\n\u2022 Real-world experimental validation of RL in combination with a safety layer that can handle any constraint type and utilize an a priori safe fallback policy (i.e., OptLayerPolicy [11]).\n\u2022 Real-world experimental validation of a method using a metaheuristic algorithm to generate an interpretable control policy modeled as a decision tree (i.\u0435., TreeC [12]).\n\u2022 Comparison between rule-based control (RBC), MPC, Safe (using OptLayerPolicy) RL and TreeC (on any case study, simulated or otherwise) using an evaluation procedure aimed to be as fair as possible.\nIn section 2 we continue with a concise related work discussion, while section 3 includes a detailed description of the utilized and proposed methodologies (i.e., hardware setup, safety layer, EMSs and evaluation procedure). Section 4 then presents the results and a discussion of those results. Finally, section 5 presents our conclusions and formulates directions for future work."}, {"title": "2. Related work", "content": "Energy management systems (EMS) have been widely reviewed not only in terms of their mathematical modelling approach [13], but also considering their objectives, architecture, involved stakeholders and challenges [14], including for home EMS more specifically [15]. One observation, and also specifically highlighted by Rathor and Saxena [14] and Allwyn et al. [16] in their reviews, is that most work is on simulated case studies and only few show practical real-world implemented results. Nevertheless, notable examples include Zhao et al. [17] who implemented a non-linear MPC in the Hong Kong Zero Carbon Building, that optimized the usage of an electric chiller and combined cooling and power unit, considering a stratified chilled water storage tank as thermal energy storage (and thus source of flexibility) and a PV system under a day-ahead pricing mechanism. Viot et al. [18, 19] implemented an MPC in the SYNERGI building of the University of Bordeaux - France, which optimized the Thermally Activated Building Systems and reported significant energy savings (40%) with an improved or equivalent thermal comfort. Arroyo et al. [20] then later compared a white-, gray- and black-box modeling paradigms in an MPC based home EMS for the Thermally Activated Building Systems of the Vliet building of the KU Leuven University in Heverlee, Belgium. Elkazaz et al. [21] implemented a two-layer structured EMS in a laboratory-based grid connected microgrid, including a non-controllable load, a solar PV system, wind turbine production and a battery energy storage system (BESS). The mathematical program in the upper layer then determined the optimal reference values for the lower level MPC and reported a daily cost reduction up to 30%. Zhang et al. [22] then more recently presented the results of a year-long evaluation of an MPC for the energy management in a small commercial building, located in California, United States, equipped with solar PV and controllable space conditioning, commercial refrigeration, and a BESS while testing two types of demand flexibility applications: (1) electricity cost minimization under time-of-use tariffs and (2) responses to grid flexibility events. Their results showed 12% annual electricity cost savings and 34% peak demand reduction against a baseline, trained using supervised learning, while preserving thermal comfort and food safety. Yang et al. [23] then also used supervised learning, yet to approximate an MPC and conducted an experimental study on 2 real testbeds (office floor and lecture theater) in Singapore. The EMS controlled the air-conditioning systems and still managed to save 52% of cooling energy consumption compared to 58% of the original MPC but reducing the computational load by a factor of 100.\nWhen looking for real-world implemented machine learning EMS results, the options become even more scarce, as also mentioned by Fu et al. [24] in their review. Even though multiple real-world challenges are yet to be verified and thus remain open, as e.g. stipulated by Nweye et al. [25]. This together with recent advancement in imitation learning, making it a more viable practical option [26]. Nonetheless, notable implemented examples include, Costanzo et al. [27] who demonstrated a model-assisted batch RL agent controlling heating, ventilation and air conditioning units, subjected to dynamic pricing, for the room climate control of a living lab and confirmed that within 10 to 20 days sensible control policies, that are further shaped by domain knowledge, could be obtained. Kazmi et al. [28] then later demonstrated a model-based RL agent optimizing domestic hot water production, controlling reheating cycles of a domestic hot water storage vessel coupled to an air source heat pump in 19 real houses in the Netherlands.\nThey reported a 20% energy efficiency gain without any loss of user comfort (observed on a subset of 5 houses). Around the same time, Zhang and Lam [29] practically implemented and evaluated a deep RL agent for the optimal control of a radiant heating system in a real office building. The RL agent was trained offline on a physics-based model that was calibrated with real operational data, before deploying online on the actual heating system (without taking additional exploratory actions). A heating demand savings of 16.6% to 18.2% was reported compared to an RBC strategy over a three-month-long evaluation period. Also considering distributed energy resources, Touzani et al. [30] demonstrated a deep RL agent optimizing the heating, ventilation and air conditioning and BESS, in the presence of solar PV, of the FLEXLAB at Lawrence Berkeley National Laboratory in Berkeley, California, United States. The RL agent was also trained offline on a calibrated physics-based simulation model, before online deployment of the policy on the real system. They reported significant costs savings (up to 39.6%), while maintaining similar thermal comfort levels over a three-week-long testing period. Naug et al. [31] then presented a deep RL agent for a 5 zone simulated testbed, but also a real three-story building at the Vanderbilt University, United States - controlling heating setpoints in the air handling units over the course of a full year. Here, again, the RL agent was trained offline and afterwards deployed online. If a performance degradation was detected, machine learning models of the building dynamics were updated and an offline relearning loop then also updated the RL agent (using these offline dynamic system models). While as a final example, Gokhale et al. [32] conducted a 4-week experimental case study for the demand response of a cluster of 8 residential buildings using a RL-based energy coordinator together with a PI control-based power dispatcher - that also would override actions that could lead to thermal comfort violations, given they utilize the thermal mass of the buildings as cost-free source of flexibility.\nWhile we have discussed a similar amount of real-world implemented related works, experimental machine learning energy management case studies are far less common. In almost all RL-based cases, the agents are safely trained offline, using detailed a priori simulators (which may not be available, or economically viable to build for each project - and can introduce significant modeling bias, leading to a poor online performance). However, even without taking exploratory actions online, there are no guarantees that the deployed policy is safe across the entire state-action space. The few cases that do train the RL agents directly online have very simple overruling mechanisms in place, that are not only highly invasive (e.g. compared to correcting to the closest feasible actions) but also don't generalize well. Furthermore, while learnable decision tree-based EMS cases exist [33, 34] - they have, to the best of our knowledge, never been demonstrated in the real-world. Therefore, we filled in this research gap by (1) using an online RL-based EMS in combination a safety layer that can handle any constraint type and utilize an a priori safe fallback policy when available (i.e., OptLayerPolicy [11]) and (2) using a learned interpretable decision tree based EMS (i.e., TreeC [12]), in the real-world."}, {"title": "3. Method", "content": "3.1. Hardware setup\nThe experimental setup consists of four real reproductions of an electrical circuit of a house. Each house has a BESS, a PV installation, an emulated electric load consumption and an emulated EV charging sessions. The houses are connected to the grid and have official digital meters from the local Belgian distribution system operator that measures energy imported from, and exported to, the grid. The BESS and PV installations are different for each house whereas the emulated electric load consumption and charging sessions are identical for all houses. This setup replicates an electrical configuration typical of a modern home with any non-electrical heating system.\nThe electrical loads used for the consumption and charging sessions come from a dynamic load system composed of seven 2000W Perel\u00ae PHP2000 heaters controlled through Showtec Single DP-1 dimmers. The electrical loads are calibrated so that the setpoints matched the real measured active power consumption more closely (we expect this difference to come from measurement\u00b9 inaccuracies and manufacturing tolerances of the loads).\nMeasurement devices for power, voltage and current are present for the PVs, BESSs and grid connections. The power of the electrical loads and other power losses in the setup is calculated via the PV, BESS and grid measurements.\nA schematic and a picture of the setup are displayed in figs. 1 and 2. Additional hardware specifications for the BESSs, inverters, PVs and electrical circuits of each house are described in tables A.6 to A.8."}, {"title": "3.2. Experimental setup", "content": "3.2.1. Consumption profile\nWe utilize the real-time (10 seconds) residential electricity \u201chousehold\" consumption data from Schlemminger et al. [35], focusing specifically on the 2019 dataset, which predates the COVID-19 pandemic. Our experiments employ data from single-family house (SFH) 19, which does not include solar PV, BESS, or EV charging. Additionally, this household dataset excludes heat pump loads. SFH 19 is selected due to its high data availability and quality, along with its typical consumption metrics, averaging 3425 kWh per year."}, {"title": "3.2.2. Electric vehicle and driver charging behavior", "content": "The residential electric mobility demand consists of two parts: a) a first part specific to the EV user charging behavior (i.e. arrival time, departure time and energy need), and b) a second part specific to EV charging power profile.\nFirstly, the EV user charging behavior is simulated directly using an existing open-source dataset available in [36]. This dataset comprises data from 97 EV users, encompassing 6878 charging sessions at residential locations spanning from December 2018 to January 2020. Among these users, \u201cBl2-2\u201d has been selected for the experiment due to its alignment with the average EV user in terms of arrival time, parking duration, and energy requirements. However, it distinguishes itself by exhibiting a notably high frequency of charging sessions, rendering it particularly interesting for the experiment as it allows for more interactions with the EMS developed in this paper. From a statistical standpoint, this EV user has undergone 315 charging sessions, with an aggregate energy consumption of 3274.74 kWh across 386 days.\nSecondly, the EV charging power profile is built based on certain variables and constraints. The EV charging power is constrained by (1).\n$0 < P_{ev} < P_{ev,max}$   (1)\nwhere $P_{ev,max}$ is the maximum charging power in [kW] set to 7.4 kW, following the IEC 61851 charging standards [37]. Additionally, the maximum charging power follows the Constant-Current/Constant-Voltage (CC-CV) uncoordinated charging method defined in Vagropoulos and Bakirtzis [38]. Such method stipulates that the power is function of the state of charge (SOC) as piece-wise linear function summarized in (2).\n$P^t_{ev}=\\begin{cases}\nP_{ev,max}& \\text{if } SOC^t_{ev} \\leq SOC^{cc,cu}_{ev}\n\\\\(P_{ev,max}-P_{ev,min})*\\frac{SOC^{end}_{ev}- SOC^{t}_{ev}}{1- SOC^{cc,cu}_{ev}}& \\text{if } SOC^t_{ev} > SOC^{cc,cu}_{ev}\n\\end{cases}$  (2)\nwhere $SOC^t_{ev}$ is the SOC at time t in [%], $SOC^{cc,cu}_{ev}$ is the transition SOC between the Constant-Current phase and the Constant-Voltage phase in [%] and $P_{ev,min}$ is the maximum charging power at 100% SOC, set to 1 kW. Finally, both charging power and SOC variables are linked using the equality constraint (3).\n$SOC^{t+1}_{ev} = SOC^{t}_{ev} + \\eta_{charge} * \\frac{P_{ev} x \\Delta t}{E_{ev}}$  (3)"}, {"title": "3.2.3. Tariff", "content": "The real time pricing used in this study is based on a one-year dynamic contract of a major Belgian energy supplier, as if it were taken in April 20242. The total cost is composed of four costs with different pricing mechanisms: the day-ahead cost, the offtake extras cost, the peak cost and the fixed yearly cost.\nThe day-ahead cost depends on the prices of eqs. (4) and (5).\n$V_{ot} =\\begin{cases}\n(V_{dt} + 0.011 \\frac{\u20ac}{kWh}) * 1.06 & \\text{if } (V_{dt} + 0.011 \\frac{\u20ac}{kWh})\\geq0 \\\\\n\\frac{\u20ac}{3} & \\text{otherwise}\n\\end{cases}$  (4)\n$V_{it} = V_{dt} - 0.009 \\frac{\u20ac}{kWh}$  (5)\nWhere $V_{ot}$ and $V_{it}$ are the offtake and injection prices at time step t. $V_{dt}$ is the Belgian day-ahead hourly price. A value-added tax of 6% is added to the offtake price when it is positive. Extra costs that occur for energy taken from the grid for transport, distribution and taxes are calculated together and labelled as offtake extras cost. The offtake price for all these additional offtake costs $V_f$ is of 0.114 \u20ac/kWh.\nThe monthly peak offtake cost $C_{pm}$ is calculated by eq. (6).\n$C_{pm} = \\frac{|B_m|}{N_m}*V_p*max(\\frac{E_{ot}}{teBm} - 2.5kW)$  (6)\nWhere $V_p$ is the peak price of 3.5 \u20ac/kW. This cost adds a penalty to the highest offtake power averaged over a 15-minute time step that occurred for each month. In the context of this study, the monthly peak offtake cost is proportional to the billed period of each month to also include this cost for unfinished months. $B_m$ is the set containing all billed time steps in month m and $N_m$ the total number of time steps in the month. $E_{ot}$ is the energy taken out of the grid at time step t. The offtake energy is divided by the length of a time step to obtain the power in kW. A minimum monthly peak power of 2.5 kW is enforced in the cost even if the measured peak power is lower.\nThe fixed yearly cost is calculated using the yearly price $V_y$ of \u20ac115.84 and is proportional to the billing period. The total electricity cost is calculated by eq. (7).\n$C_{tot} = \\sum_{t \\in T}(E_{ot}V_{ot} - E_{it}V_{it}) + \\sum_{t \\in T} E_{ot}V_f + \\sum_{m \\in M}C_{pm} + \\frac{|T|}{N_y}V_y$  (7)"}, {"title": "3.2.4. House switching", "content": "As described in section 3.1, the four houses have different BESS and PV installations. Assigning a different EMS to each house and comparing the performance by running them simultaneously would not yield comparable results. One house could obtain better performances because it has a better BESS or PV installation. To avoid this bias, the EMSs are switched every 24 hours to a different house.\nSince there are 4 houses and 4 EMSs to test, there are a total of 24 possible house and EMS combinations. Each combination is tested twice, resulting in 48 days of testing in total. The 48-day schedule is obtained by randomly shuffling the 24 combinations until two distinct 24-day schedules are generated, ensuring no consecutive days have the same reinforcement learning or MPC EMS assigned to the same house. This rule is applied for practical reasons, as the RL and MPC EMSs behaviors depend on how they operated a house previously. Failed test days can occur, and not having an MPC or RL EMS on the same house for consecutive days provides more time to identify a failure while preventing its propagation to the following day.\nThe switching is conducted daily at 15:00. By this time, the BESSs of all houses are charged to 100% to ensure consistent starting conditions post-switch. The EV schedule is adjusted to prevent charging sessions from overlapping with the 15:00 switching time. This is achieved by shortening the"}, {"title": "3.2.5. Enforced charging behavior", "content": "An enforced charging behavior is enforced in order to reach 100% SOC at the switching time for the BESSs and to reach the imposed final SOC of an EV battery at the end of a charging session. The batteries are charged at maximum power to reach their SOC goal at the latest possible time minus a buffer time Btime. This buffer time is used to avoid not reaching the SOC goal in time due to non-perfect battery models, safety layer activation (which could reduce the battery setpoints, see section 3.3) or other unexpected event.\nThe buffer time value is higher when the difference between the SOC goal and current SOC of the battery is higher because more events could prevent the battery to reach its SOC goal in time. This relation is described in eq. (8).\n$B_{time} = (SOC_g \u2013 SOC) * (\u0412_{max} \u2014 B_{min}) + B_{min}$  (8)\nWhere Btime is the time buffer, SOCg is the SOC goal of the battery, SOC is the current SOC of the battery, Bmax is the time buffer enforced if 100% of the battery still needs to be charged and Bmin is the minimum enforced time buffer. Every 5 seconds, the battery controller calculates with equations eqs. (3) and (11) whether the battery can reach the SOC goal at the intermediate time, defined as the target time minus the time buffer. If not, the battery is charged at maximum power for 1 minute. For both the BESSs and the EV batteries, Bmin is set to 3 minutes. For the BESSs Bmax is set to 1 hour and for the EV batteries Bmax is set to 4 hours. The Bmin and Bmax values were obtained by testing in simulation what values would consistently meet the SOC goal in time throughout the 3 months of training data (see section 3.5.1)."}, {"title": "3.3. Safety layer", "content": "Given that we are proposing to demonstrate native unsafe energy management methods, we implement the OptLayerPolicy safety layer from Ceusters et al. [11] - as it is considered one of the state-of-the-art methods. This safety layer results in the minimal invasive correction of predicted (control) actions towards their closest feasible safe set without affecting optimality. One can then also compute the distance from the feasible solution space, and fallback on an a priori safe policy when available. This under the notion that far-from-feasible actions are likely to be far-from-optimal, while close-to-feasible can mean close-to-optimal. This threshold is a hyperparameter of OptLayerPolicy specifically and its effectiveness was shown in a simulated case study by Ceusters et al. [11].\nWhile the safety layer mainly serves the RL-based EMS, it provides general constraint satisfaction guarantees - including for the simple RBC EMS benchmark (that is not necessarily always safe). We choose the same sampling rate for the safety layer as for the hardware-in-the-loop sampling rate, being 5 seconds. By doing so, we effectively utilize it additionally as a real-time optimization step in a cascading (a.k.a. hierarchical) system architecture [40] - including for the MPC-based EMS. Hence, it allows us to use slower sampling rates inside the EMSs (i.e., 15 minutes), where the optimized setpoints are then used by the safety layer as reference signal (only deviating from it, if it is absolutely necessary to subject to the constraints - considering the latest measurements, e.g. when the solar PV in-feed suddenly drops).\nIn our experimental case study, the OptLayerPolicy safety layer is expressed as:\n$\\alpha^{safe, bess, ev}_{t}=\\underset{\\alpha^{bess}, \\alpha^{ev}}{arg \\min} \\frac{1}{2} \\begin{vmatrix} (a^{bess} - \\alpha^{bess}) + (a^{ev} \u2013 \\tilde{a}^{ev})\\end{vmatrix} ^2$  (9a)\ns.t.\n$P_{grid}^{min} < p_{grid} < P_{grid}^{max}$ $\\forall t$ (9b)\n$0 < P_{ev} < P_{ev,max}$ $\\forall t$ (9c)\n$p_{min}^{bess} < p^{bess} < p_{max}^{bess}$ $\\forall t$ (9d)\nbeing a Quadratic Program, where $a^{bess}$ and $a^{ev}$ are the internal optimization variables for the BESS and EV that results in the closest feasible safe actions $\\alpha^{safe,bess}$ and $\\alpha^{safe,ev}$ (i.e., the control variables to the assets), starting from the proposed actions $\\alpha^{bess}$ and $\\tilde{a}^{ev}$ originating from the EMSs (i.e.,"}, {"title": "3.4. Simulation", "content": "aforementioned in the introduction).\n$P^{grid} = - P^{load} - P^{ev} + P^{pv} + P^{bess} \\qquad \\forall t$ (10a)\n$P_{ev} = f(a^{ev}, P_{ev}^{max}, SOC_{ev})$  (10b)\n$P_{bess} = \\gamma^{bess} . P_{t}^{ch} + (1-\\gamma^{bess}) . P_{t}^{di} \\qquad \\forall t, \\qquad p^{bess} \\in {0,1}$  (10c)\n$P^{ch}=\\begin{cases} \\gamma^{bess} . P_{bess}^{min}, if SOC^{bess}<1\\\\ 0, if SOC^{bess}=1 \\end{cases} \\> 0 \\qquad \\forall t$ (10d)\n$P^{di}=\\begin{cases} \\gamma^{bess} . P_{bess}^{max}, if SOC^{bess}>0\\\\ 0, if SOC^{bess}=0 \\end{cases} >0 \\qquad \\forall t$ (10e)\nwhere Pt are the electrical power variables at time step t of the grid exchange, non-controllable load, EV charger, solar PV system and BESS respectively. The function, f(\u00b7), in eq. (10b) is eq. (2). A binary decision variable, $\\gamma^{bess}$, is added to the BESS power equation to avoid simultaneous charging and discharging. Hereby we focus on satisfying the electrical energy balance and the associated grid limit, as no additional constraints are considered in this case study (e.g., ramping rates, minimal run- and downtime).\nThe a priori safe fallback policy #safe, which in this case study is activated when the distance to the feasible solution space dsafe is higher than 1e6, is simply the self-consumption mode of the BESS together with a reiteration of the safety layer to determine the maximum EV charging power (as just sending a maximum power setpoint could still violate the grid exchange constraint - and trip the main circuit breaker)."}, {"title": "4. Results and discussion", "content": "A simulation of the homes is implemented in order to train the TreeC EMS and compare the real experiment with simulation. The simulation models using 15-minute time-steps the behavior of the BESS, EV charging, PV, electrical load and grid connection and how they react to different control strategies from EMSs.\nThe simulation model of the BESS is described in eq. (11).\n$SOC_{t+1}^{bess} = SOC_{t}^{bess} + \\frac{P^{ch} * \\eta \\> P^{di}/\\eta^{bess}}{E^{bess}} \\Delta t$  (11)\nwhere $SOC_t$ is the SOC at time step t, $P^{ch}$ is the charge power at time step t, $P^{di}$ is the discharge power at time step t, $E^{bess}$ is the energy capacity of the BESS, $\\eta^{bess}$ is the charge and discharge efficiency of the BESS and \u0394t is the time step duration. $\\eta^{bess}$ is set to 95% [39] for BESSs 1,2 and 4 and 96% for BESS 3 as stated in the documentation of the BESS. The energy capacity of each BESS is stated in table A.6.\nThe EV is modeled using eqs. (1) to (3). The PV and electric loads come from the measurements and are included in the simulation. The grid power is calculated with eq. (10a). To simplify the modeling of the simulations, the limit imposed by the hybrid inverter on the maximum combined power from the PV installation and the BESS is not enforced (see table A.7). This simplification is not expected to significantly impact the results of the simulations because the hybrid inverter limits are very rarely reached in practice for the setups used in this study.\nThe enforced charging behavior described in section 3.2.5 is closely approximated in the simulation. The time buffer is calculated using eq. (8) at each time step. If following the battery charge or discharge power given by the EMS does not allow reaching the SOC goal at the intermediate time, the battery is charged at the minimum power necessary to meet that goal."}, {"title": "3.5. Energy management systems", "content": "This section presents the four EMS methods used and the data used to train the MPC, RL and TreeC EMSs. All EMSs send a setpoint to the BESS and the EV charger every 15 minutes, how they calculate this setpoint is described in this section.\n3.5.1. Training data\nThe MPC, RL, and TreeC EMSs required training data and used three months of data spanning from 2024/01/01 at 00:00 to 2024/04/01 at 00:00. The dataset included real PV power production, household power consumption described in section 3.2.1 and EV charging sessions from section 3.2.2. The MPC utilized this data to train its forecasting models while the RL agents were pre-trained on the training data with additional power profiles for the BESSs and EV chargers. The BESSs and EV charger power profiles were generated as if a RBC EMS was controlling these assets. The TreeC EMS used this data in the simulator, described in section 3.4, to construct its decision tree."}, {"title": "3.5.2. Rule-based control", "content": "The RBC EMS implemented in this setup consists of default rules commonly found in many devices. The primary rule for the BESS focuses on self-consumption \u201cbehind-the-meter\". It charges when excess power is available from the PV installation and discharges to meet load demands. The rule for the EV charger is to always charge the EV immediately at maximum allowed power. This EMS serves as a benchmark to evaluate the performance of the other EMSS."}, {"title": "3.5.3. Model predictive control", "content": "MPC is used here as an EMS method that requires a model of the system and forecasts of the system's inputs to optimize control. The MPC EMS in this setup is implemented as a Mixed-Integer Linear Program, which is solved at each (15 minute) time step. The horizon of the MPC is variable and ends at the next switching time. The MPC model is solved every 15 minutes. The variables of the MPC are the power profiles of the BESS and the EV charger as these are the assets it needs to control.\nThe objective function of the MPC is described in eq. (12).\n$O=\\sum_{teH}(E_{ot}V_{ot} \u2013 E_{it}V_{it}) + \\sum_{teH} E_{ot}V_f +V_p * max(\\frac{E_{ot}}{teh}-0.25hP_p)$   (12)\nWhere H is the set of time-steps within the horizon of the MPC and Pp is the previous grid peak power measured for that house when using the MPC EMS. At the beginning of the experiment, Pp is set to 2.5 kW. This objective function penalizes heavily exceeding the previous grid peak power Pp.\nThe MPC system models takes as inputs the time of the day, the SOC of the BESS and EV battery, the Belgian day-ahead hourly prices Vdt until the next switching time, the forecasted power profiles until the next switching time of the PV installation and the electric load and the forecasted final SOC and departure time of the connected EV.\nThe constraints of the MPC are the same as the ones defined in eqs. (1), (3), (9b), (9d), (10a) and (11). However, there is a difference in the model of the EV charging behavior defined in eq. (2) as described in eq. (13)."}, {"title": "4.1. EMS performance comparison", "content": "$P^t_{ev}< \\begin{cases}\nP^{t+1}_{ev,max}  & \\text{if } SOC^{t+1}_{ev} \\leq SOC^{cc,cu}_{ev}\n\\\\(P_{ev}^{t+1} -P_{min}) *\\frac{SOC^{end}_{ev}SOC^{t+1}_{ev}}{1 - SOC^{cc,cu}_{ev}}& \\text{if } SOC^{t+1}_{ev} > SOC^{cc,cu}_{ev}\n\\end{cases}$ (13)\nCalculating the average maximum power of the EV charger over the next time-step when the SOCercu is exceeded would introduce an exponential relationship to the MPC model. The linearity of the MPC model is kept by limiting the maximum power of the EV based on the SOC of the EV at the following time step t + 1 instead.\nA constraint requires the modeled SOC of the BESS to be charged to 100% at the next switching time. A similar constraint is imposed to ensure the EV battery reaches the forecasted final SOC at the forecasted departure time. These forecasted SOC and departure time can be underestimated leading to the enforced charging behavior (see section 3.2.5) to be activated. For this reason the approximated enforced EV charging power for the next time step (see section 3.4) is set as the minimum EV power setpoint in the MPC model. Finally, a constraint requires the modeled grid power to not exceed the maximum grid power of 9.2 kW.\nForecasting of electrical load and PV generation are accomplished using a Light Gradient Boosting Machine (LightGBM) model [41"}]}