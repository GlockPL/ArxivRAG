{"title": "Research on reinforcement learning based\nwarehouse robot navigation algorithm in complex\nwarehouse layout", "authors": ["Keqin Li", "Lipeng Liu", "Jiajing Chen", "Dezhi Yu", "Xiaofan Zhou", "Ming Li", "Congyu Wang", "Zhao Li"], "abstract": "In this paper, how to efficiently find the optimal\npath in complex warehouse layout and make real-time decision\nis a key problem. This paper proposes a new method of Proximal\nPolicy Optimization (PPO) and Dijkstra's algorithm, Proximal\npolicy-Dijkstra (PP-D). PP-D method realizes efficient strategy\nlearning and real-time decision making through PPO, and uses\nDijkstra algorithm to plan the global optimal path, thus\nensuring high navigation accuracy and significantly improving\nthe efficiency of path planning. Specifically, PPO enables robots\nto quickly adapt and optimize action strategies in dynamic\nenvironments through its stable policy updating mechanism.\nDijkstra's algorithm ensures global optimal path planning in\nstatic environment. Finally, through the comparison experiment\nand analysis of the proposed framework with the traditional\nalgorithm, the results show that the PP-D method has significant\nadvantages in improving the accuracy of navigation prediction\nand enhancing the robustness of the system. Especially in\ncomplex warehouse layout, PP-D method can find the optimal\npath more accurately and reduce collision and stagnation. This\nproves the reliability and effectiveness of the robot in the study\nof complex warehouse layout navigation algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of the logistics industry,\nwarehouse automation and intelligence have become a key\ntrend to improve logistics efficiency and reduce operating\ncosts. In the complex warehouse environment, how to realize\nthe efficient and accurate navigation of the warehouse robot is\none of the important factors restricting the improvement of the\nwarehouse automation level. Traditional navigation methods,\nsuch as map-based path planning algorithm, can solve the path\nplanning problem to a certain extent, but in the face of\ndynamic changes in warehouse layout, real-time updates of\nobstacles and other complex situations, often seem inadequate.\nTherefore, exploring more intelligent and flexible navigation\nalgorithm has become an important topic in the field of\nwarehouse robot research.\nReinforcement Learning (RL), as a machine learning\nparadigm that learns optimal strategies through trial and error,"}, {"title": "III. REINFORCEMENT LEARNING", "content": "Reinforcement Learning (RL) is a machine learning method\nin which agents learn to perform tasks by interacting with their\nenvironment [12]. It does not require pre-labeled datasets and\ninstead relies on reward or punishment mechanisms to guide\nthe learning process. In the RL framework, the agent selects\nan action based on its current state and receives immediate\nfeedback-reward or punishment from the environment.\nThe goal is to find the best course of action that maximizes\nlong-term cumulative rewards by constantly experimenting\nwith different strategies [13]. As shown in Figure 1, the key\nconcepts of RL include agents, environments, states, actions,\nstrategies, rewards, and values. Among them, the balance\nbetween exploration and exploitation is crucial for learning\nnew knowledge and applying existing knowledge [14]; at the\nsame time, algorithms can predict future states based on\nmodels, or they can learn model-free directly from experience\n[15].\nReinforcement learning is widely used in fields as diverse\nas gaming, robotics, autonomous driving, resource\nmanagement, and personalized recommendations [16]. For\nexample, in the study of warehouse robot navigation,\nreinforcement learning can help robots autonomously plan\npaths and avoid obstacles in a complex and changing working\nenvironment [17]. With the growth of computing power and\nthe advancement of algorithms, reinforcement learning can\nnot only improve operational efficiency but also enhance the\nadaptability of the system to the dynamic changing\nenvironment, showing a strong potential to solve practical\nproblems [18]. By continuously optimizing strategies,\nreinforcement learning enables agents to achieve higher\nperformance levels in a variety of application scenarios [19]."}, {"title": "IV. PROXIMAL POLICY OPTIMIZATION", "content": "PPO is a popular reinforcement learning algorithm,\nespecially suitable for continuous control tasks and strategy\noptimization problems [20]. It stabilizes the training process\nby introducing a surrogate loss function and restrictions on\npolicy updates, and is relatively easy to implement. The core\nidea is to find a way to maximize expected returns without\ndramatically changing your current strategy, as shown in\nFigure 2.\nPPO uses the following surrogate objectives:\n$LCLIP (0) =$\n$E, [min(r (0) A\u2081, clip(r(0),1-\u874c+ )A\u2084)]$\nWhere rt(0) is the importance sampling ratio, representing\nthe ratio of action probabilities under the old and new\nstrategies. At is an advantage function that measures how well\nan action is taken relative to the average action. clip(x,1-\u20ac,1+\u20ac)\nmeans to restrict the value of x to the range [1-\u20ac,1+\u20ac], where\ne is a small positive number, usually about 0.2.0 and Oold are\nthe current and the last updated parameters, respectively. This\nobjective function encourages policy updates, but prevents\nperformance deterioration by updating too quickly with the\nclip function [21].\nIn addition to policy optimization, PPO also trains a value\nfunction V(s) to evaluate the value of the state At the same\ntime, which helps to better estimate the dominance function at.\nThe goal of the value function is to minimize the Mean\nSquared Error (MSE) :\n$L'F (0) = E, [(V,(s,) \u2013 R\u2081)\u00b2]$\nWhere Rt is the actual discount return from time t.\nThe overall goal of PPO is to maximize the following\ncompound objective function:\n$J(0) = LCLIP (0) - c\u2081L'F (0) + c2S[no]$\nWhere c\u2081 and c2 are hyperparameters that balance strategy\nlosses, value function losses, and entropy terms. S[\u03c0\u03b8] is the\nentropy of the strategy and is used to encourage exploration.\nRecent studies have demonstrated the effectiveness of online\nmultitask learning in handling complex tasks and their\nrelationships [22]."}, {"title": "V. DIJKSTRA'S ALGORITHM", "content": "Dijkstra algorithm is a classical shortest path algorithm,\nwhich is suitable for graphs without negative weighted edges\n23]. The basic principle is to find the shortest path from one\nstarting node to all other nodes in a weighted graph. As shown\nin Figure 3, the algorithm gradually expands the shortest path\ntree by maintaining a priority queue (usually a minimum heap)\nuntil all reachable nodes are covered [24].\nSet the distance of the start node s to 0.\n$d[s]=0$\nFor all other nodes v, set their distance to infinity, i.e. :\n$d[v] = \u221e$\nTake the node u with the smallest distance from priority\nqueue Q. For each neighbor node v of u, calculate the new\ndistance:\n$d[v] = min{d(v), d(u)+w(u,v)}$\nWhere w(u,v) is the weight of the side from u to v.\nIn Dijkstra's algorithm, the core formula is used to update\nthe shortest path estimate d(v) of vertex v:\n$d(v) = min{d(v), d(u)+w(u,v)}$"}, {"title": "VI. PROXIMAL POLICY-DIJKSTRA", "content": "In the tide of warehouse automation and intelligence, how\nto ensure the efficient and accurate navigation of warehouse\nrobots in the complex and changeable layout has become a key\nproblem to be solved [25]. Proximal Policy-Dijkstra (PP-D),\nan innovative navigation algorithm, is proposed in this paper,\nwhich skillfully integrates proximal strategy optimization\n(PPO) in reinforcement learning with the classic Dijkstra path\nplanning algorithm, providing a new solution for warehouse\nrobots to navigate in complex environments [26]. The\nworkflow is shown in Figure 4.\nThe core innovation of the PP-D algorithm lies in its\nunique algorithm fusion mechanism [27]. Traditionally,\nreinforcement learning algorithms and path planning\nalgorithms are independent and solve the problem of strategy\noptimization and path planning respectively [28]. However,\nthe PP-D algorithm breaks this boundary through the deep\nintegration of PPO and Dijkstra, achieving a seamless\nconnection from global to local, from planning to optimization\n29]. Specifically, at the beginning of the navigation task,\nDijkstra's algorithm first quickly calculates the initial shortest\npath from the starting point to the end point based on the static\nlayout information of the warehouse [30]. This step not only\nprovides a clear navigation direction for the robot but also\ngreatly reduces the search space for subsequent strategy\noptimization [31]. Then, the PPO algorithm takes over the\nnavigation task and continuously optimizes its action strategy\naccording to the real-time environmental information\nperceived by the robot (such as the location of obstacles, the\ndynamic change of the warehouse layout, etc.) to ensure that\nthe robot can respond flexibly in a complex environment and\nachieve efficient navigation [32].\nAnother significant advantage of the PP-D algorithm is its\ndynamic adaptability and high efficiency [33]. In complex\nwarehouse layouts, the uncertainty and dynamics of the\nenvironment put forward high requirements for navigation\nalgorithms [34]. Through the stable policy updating\nmechanism of the PPO algorithm, the PP-D algorithm enables\nthe robot to quickly learn and adapt to the changes of the\nenvironment in the process of continuous interaction with the\nenvironment [35]. This dynamic adaptability ensures that the"}, {"title": "VII. EXPERIMENT", "content": "Data collection and transmission: The image and point\ncloud data of the warehouse environment and the status\ninformation of the robot are collected in real time through\nsensors such as LiDAR and cameras, and the data is\ntransmitted to the central server using 5G or Wi-Fi wireless\ncommunication modules [42]. Data preprocessing includes\ndata cleaning, standardization, feature extraction, and data\nenhancement to ensure data quality and availability [43]. In\nterms of model training and evaluation, the dataset is divided\ninto training set, validation set, and test set. The reinforcement\nlearning library is used to build and train the model, the\nhyperparameters are adjusted on the validation set, the model\nperformance is evaluated on the test set, and the accuracy rate,\nrecall rate, F1 score, and robustness are used for quantitative\nevaluation [44]."}, {"title": "VIII. SUM UP", "content": "In this paper, we successfully proposed an innovative\nnavigation algorithm, Proximal Policy-Dijkstra (PP-D), which\ndeeply integrates the advantages of proximal strategy\noptimization (PPO) and Dijkstra's algorithm, aiming to solve\nthe high efficiency and accuracy of warehouse robot\nnavigation under complex warehouse layout. PP-D algorithm\ngives the robot the ability to adapt and optimize the action\nstrategy quickly in the dynamic environment through the\nstable policy update mechanism of PPO algorithm, which\nensures the accuracy and efficiency of real-time decision\nmaking. At the same time, the global path planning capability\nof Dijkstra algorithm in static environment is utilized to plan\nthe optimal path for the robot, thus improving the accuracy\nand efficiency of navigation on the whole. The experimental\nresults show that PP-D algorithm has significant advantages\nin navigation prediction accuracy, collision and stagnation\nreduction and system robustness, especially in complex\nwarehouse layout, it can accurately find the optimal path,\nshowing its excellent performance and reliability. This\nresearch results not only enrich the application theory of\nreinforcement learning in the field of path planning, but also\nprovide strong technical support for warehouse automation\nand intelligent management, indicating that in the future with\nalgorithm optimization and hardware upgrade, PP-D\nalgorithm will show its broad application prospects and value\nin more fields."}]}