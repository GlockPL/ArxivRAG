{"title": "ImageRef-VL: Enabling Contextual Image Referencing in Vision-Language Models", "authors": ["Jingwei Yi", "Junhao Yin", "Ju Xu", "Peng Bao", "Yongliang Wang", "Wei Fan", "Hao Wang"], "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding multimodal inputs and have been widely integrated into Retrieval-Augmented Generation (RAG) based conversational systems. While current VLM-powered chatbots can provide textual source references in their responses, they exhibit significant limitations in referencing contextually relevant images during conversations. In this paper, we introduce Contextual Image Reference the ability to appropriately reference relevant images from retrieval documents based on conversation context and systematically investigate VLMs' capability in this aspect. We conduct the first evaluation for contextual image referencing, comprising a dedicated testing dataset and evaluation metrics. Furthermore, we propose ImageRef-VL, a method that significantly enhances open-source VLMs' image referencing capabilities through instruction fine-tuning on a large-scale, manually curated multimodal conversation dataset. Experimental results demonstrate that ImageRef-VL not only outperforms proprietary models but also achieves an 88% performance improvement over state-of-the-art open-source VLMs in contextual image referencing tasks. Our code is available at https://github.com/bytedance/ImageRef-VL.", "sections": [{"title": "Introduction", "content": "In recent years, Vision-Language Models (VLMs) have achieved remarkable progress, enabling advanced multi-modal reasoning and generation from combined text and image inputs. Both close-source models, such as GPT-40 (Hurst et al., 2024) and Claude (Anthropic, 2024), and open-source models, such as Phi-3.5-Vision (Abdin et al., 2024), Qwen2-VL (Wang et al., 2024) and InternVL2 (Chen et al., 2024), have demonstrated impressive capabilities across a range of vision-language tasks (Hudson and Manning, 2019; Yu et al., 2023; Fu et al., 2024). Concurrently, the integration of VLMs with Retrieval-Augmented Generation (RAG) has allowed these models to retrieve and incorporate external knowledge into their responses. One feature of VLM-based RAG chatbots is to provide references to the retrieved text sources used to generate responses. Extensive research efforts have been dedicated to improving textual reference accuracy in RAG chatbots (Gao et al., 2023a; Shen et al., 2024; Zhang et al., 2024a; Fierro et al., 2024). While VLM-based RAG systems have made significant strides in providing reliable textual references, they face a critical limitation in their ability to leverage visual content effectively during conversations. We identify this gap as the absence of Contextual Image Reference - the capability to strategically select and incorporate relevant images from retrieved documents to enhance response comprehension and user engagement. As demonstrated in Figure 1, when discussing complex subjects like the Brachiosaurus, purely textual descriptions of its physical characteristics often fail to convey information intuitively. Despite its potential impact on multimodal conversation systems, the challenge of contextual image referencing remains largely unexplored in current research.\nTo address these challenges, we first propose and formally define Contextual Image Reference as a novel task that requires VLMs to incorporate relevant images as integral components of their responses. To systematically evaluate performance on this task, we construct a dedicated testing dataset and develop comprehensive metrics that assess a model's capability to integrate images in contextually appropriate ways. Building upon open-source VLMs, (i.e., InternVL2), we present ImageRef-VL, a framework that enhances models' contextual image referencing abilities. Our approach involves generating initial responses using existing LLMs and VLMs, carefully curating these outputs through manual refinement, and leveraging the resulting high-quality dataset for supervised fine-tuning. Through this process, ImageRef-VL learns to make informed decisions about when and how to incorporate images as authoritative visual references.\nThe primary contributions of our work are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to introduce and formally define Contextual Image Reference as a novel task for multimodal conversational AI, addressing a critical gap in current VLM capabilities.\n\u2022 We conduct a comprehensive evaluation for this task, including a carefully curated testing dataset and novel metrics that capture both the relevance and naturalness of image references.\n\u2022 We propose ImageRef-VL, a fine-tuning framework that significantly advances the state-of-the-art in contextual image referencing, demonstrating an 88% performance improvement over existing open-source VLMs.\n\u2022 Through extensive experiments across various scenarios, we validate the effectiveness of our approach and establish strong baseline results for future research."}, {"title": "Related Works", "content": ""}, {"title": "Vision Language Models", "content": "Large Language Models (LLMs), primarily based on the transformer architecture (Vaswani, 2017), have recently achieved remarkable performance in a wide range of natural language tasks (Zhang et al., 2023; Poesia et al., 2022; Kojima et al., 2022). Building upon these advances, Vision-Language Models (VLMs) extend LLM capabilities to the visual domain, enabling sophisticated reasoning and content generation from both textual and visual inputs (Meta, 2024; Lu et al., 2024; Liu et al., 2024b,a). Recently, numerous VLMs have been introduced, spanning both close-souced systems (e.g., GPT-40 (Hurst et al., 2024), Claude (Anthropic, 2024)) and open-source frameworks (e.g., Phi-3.5-Vision (Abdin et al., 2024), Qwen2-VL (Wang et al., 2024), InternVL2 (Chen et al., 2024)). These models have demonstrated impressive capabilities across diverse vision-language benchmarks (Hudson and Manning, 2019; Yu et al., 2023; Fu et al., 2024), promoting an evolving research landscape in multimodal machine intelligence. Existing VLMS are generally composed of three core components, i.e., a vision encoder, an adapter, and an LLM backend. The vision encoder, such as CLIP (Radford et al., 2021) or BLIP (Li et al., 2022, 2023), is designed to extract rich visual features from images, converting them into representations that can be effectively processed by downstream components. The adapter component subsequently bridges these extracted features to the language model, employing architectures such as simple multi-layer perceptrons (Liu et al., 2024b,a) or Q-formers (Li et al., 2023) Finally, the LLM generates responses by combining the visual and textual inputs."}, {"title": "Retrieval-Augmented Generation", "content": "Retrieval-Augmented Generation (RAG) is a technique designed to enhance the capabilities of LLMs by integrating external knowledge sources (Gao et al., 2023b; Gupta et al., 2024). It addresses key challenges of LLMs, such as hallucinations (Huang et al., 2023; Tonmoy et al., 2024; Xu et al., 2024; Fan et al., 2024) and time-sensitive information (Mousavi et al., 2024), by incorporating relevant information retrieved from external databases or documents. The RAG process typically involves three main steps: retrieval, generation, and augmentation. First, a retriever identifies and extracts relevant document chunks from a knowledge base based on semantic similarity to the user's query. These retrieved chunks are then combined with the original query to form an augmented context, which is used as input for the LLM to generate a response."}, {"title": "LLM Citation Generation", "content": "LLM Citation Generation has gained attention as a way to enhance the verifiability and transparency of model-generated responses by including citations linked to external evidence. Citation generation improves the factual accuracy of LLMs answers (Gao et al., 2023a) and allows users to trace the sources of information, thereby increasing the credibility and explainability of outputs (Tahaei et al., 2024). Early work like ALCE (Gao et al., 2023a) proposed foundational methods and evaluation metrics for enabling LLMs to generate citations. Subsequent studies have improved citation quality through fine-tuning approaches (Huang et al., 2024; Li et al., 2024; Ye et al., 2024) or multi-stage pipelines (Zhang et al., 2024b; Hennigen et al., 2023; Lee et al., 2023). Although existing works have explored the citation generation for LLMs, no prior studies have addressed the problem of contextual image reference in multimodal settings or proposed corresponding solutions."}, {"title": "Problem Definition.", "content": "In this section, we present the problem definition for contextual image reference. Given an input prompt consisting of an ordered sequence of mixed images and text, denoted as {E1, E2, ..., Em}, where each element E\u00bf is either an image I\u00bf from the set I = {I1, I2, . . ., In} or a text segment Ti from the set T = {T1,T2,...,Tk}, the model's goal is to generate a textural response R that meets the prompt's requirements. The response R can be with some images referenced through a contextual image ID that falls within the range [1, n], corresponding to the images in the input set. These image references must be contextually appropriate and align with the textual descriptions or contextual requirements specified in the prompt, ensuring the response is coherent, relevant, and adheres to the prescribed format."}, {"title": "Method", "content": "To enhance the contextual image referencing capability of vision-language models, it is critical to incorporate contextually relevant image-text data during the model's training phase. Specifically, datasets that feature contextually integrated image references should be included in the supervised fine-tuning (SFT) stages of the model's development. We construct the training dataset by incorporating image references from the retrieved documents into appropriate positions within the original text-only responses generated by the LLM. To achieve this goal, two challenges need to be addressed. The first is to collect an SFT dataset containing responses with contextual image references. The second is to effectively fine-tune existing VLMs.\nOverview. To address the initial challenge, we developed a method to generate responses with contextual image references using existing LLMs, VLMs and user prompts. Given a prompt, we first generate a text-based response, then create a caption for the input image using the VLM, and integrate this caption into the text. Our model training follows the VLM's standard SFT approach, but we enhance image understanding by requiring the model to refer explicitly to input images. To preserve the general VLM capabilities, we combine the original SFT data with interleaved multi-image SFT data. The ImageRef-VL framework is illustrated in Figure 2."}, {"title": "Training Data Construction", "content": "To construct the training dataset, we use existing LLMs and VLMs to create a high-quality dataset through a multi-stage few-shot learning approach, and then manually select qualified samples for model training. This method significantly reduces the labeling effort. Specifically, the data generation process involves three steps: generating text-based responses from pure text content, generating captions for each image based on text context, and adding the images references into the responses.\nText Response Generation. In this step, we remove the images from the prompt and provide some reference text for the model to generate a pure text response.\nImage Caption Generation. In this step, we need to generate context-based image descriptions. The image description should not only include information about the image itself but also complement the information related to the image described in the text. For example, when describing an image in the Wikipedia page about Einstein, it is not enough to simply say, 'This is a portrait of an elderly man with white hair'; we also need to complete the information by adding, \u2018This is a portrait of Einstein in his later years.' However, directly using the existing VLM to perform this task can lead to over-reliance on context: when the image is unrelated to the context, the model may incorrectly force a connection, and some contextual information may be left incomplete.\nTo address this issue, we propose a two-stage image description generation approach. In the first stage, we generate a description based solely on the image itself. In the second stage, we provide the image description along with the context to the VLM, asking it to supplement the missing information. We use in-context learning and include examples in both stages to guide the VLM on how to better generate the image description and complete the information.\nImage Insertions. We input the generated image captions and text responses into an LLM, asking the model to insert the images into the text response. The final results, after manual filtering, form our IMI-interleave training dataset.\nMixture of Datasets. To ensure the LLM truly understands the images in context, we also incorporate a certain proportion of contextual image caption generation tasks in the training set, where VLMs generate captions for multiple images in the prompt based on the context, with the captions being those produced in the second step. Additionally, to prevent a significant decline in the performance of VLMs on other image-related tasks, we mix in a proportion of the InternVL2 SFT dataset into our training set."}, {"title": "Model Training", "content": "We used the constructed dataset to perform supervised fine-tuning on the VLM to enhance its ability to understand interleaved multi-image tasks. In this subsection, we will briefly introduce the model architecture and training loss.\nModel Architecture. As shown in stage 2 of Figure 2, our fine-tuned model consists of three components: the vision encoder, the adapter, and the language model. Given a user prompt and retrieved documents with images, the vision encoder processes each image, extracting patch features from the image. The adapter, acting as a bridge between the vision encoder and the language model, maps the patch features into the embedding space of the language model, resulting in image tokens. The textual part of the prompt is tokenized and embedded to obtain text tokens. Finally, the text tokens and image tokens are fed into the language model in their original positions for modeling.\nTraining Loss. We proceed the typical supervised fine-tuning process (Ouyang et al., 2022). The dataset D is composed of N prompt-response pairs:\nD = {(xi, yi)}Ni=1, where both prompt xi and ground-truth response yi are a sequence of tokens. We denote p\u03b8(yi |xi \u2295 y<i) as the probability of the outputting next token as yi given previous tokens xi \u2295 y<i, where \u2295 is the concatenation operator and y<i denotes the tokens before index i. The training loss is then L = \u2212 \u2211Ni=1 log p\u03b8(yn2|xi \u2295 y<n2i ), with ni being the length of yi and the optimization variable being a VLM."}, {"title": "Evaluation Settings.", "content": "To evaluate the performance of model supporting contextual image references, we propose three kinds of evaluation metrics, i.e., textual content evaluation, image position evaluation, overall response evaluation."}, {"title": "Automated Evaluation", "content": "Text Evaluation. When a model generates responses with contextual image references, we can evaluate the textual portion of the response. Following the existing LLM-as-judge approach (Zheng et al., 2023), allowing a large language model to score the response and then calculating the candidate model's average score across all test samples. Considering that current large language models may not perform well in understanding multi-image prompts, we only include the textual prompt and provide a reference answer for scoring.\nImage Position Evaluation. Inspired by existing work on image position prediction (Muraoka et al., 2020), when a model generates contextual image references, we can evaluate whether each image is placed in an appropriate position. However, previous approaches rely on an existing image-inserted dataset, checking whether the model places the image in a specific, pre-defined position or if the image ranks within the top-K choices. However, this metric does not align well with the actual user experience, as multiple images within the candidate pool could be suitable for position i.\nTo address this issue, we redesigned the image position evaluation metric. Specifically, for each potential image insertion point, we classify all images into four categories:\n\u2022 3-point images: Images that perfectly match the current contextual content.\n\u2022 2-point images: Images that match the current contextual content but are of low quality (e.g., blurry, obstructed).\n\u2022 1-point images: Images related to the main subject mentioned in the current context.\n\u2022 0-point images: Images not related to the current contextual content.\nFinally, we can obtain a testing dataset in the following format:\n\nDtest = {Pi:{sj : [Isj1 , ..., IsjMsj ]sj\u2208[0,3]} | pi \u2208 Pi, i \u2208 [1, N]},\n\nwhere Pi is all potential image insertion points of the i-th testing sample, Isj is the j-th image with s point for position pi, Msj is the maximum number of s-point images at position pi, and N is the number of samples in the testing dataset.\nBased on this label definition, we designed three metrics: Precision, Recall3, F1 and Score.\n\u2022 Precision is defined as the accuracy of the illustrations, meaning the probability that an inserted image is a nonzero score image. It is defined as follows:\n\nPrecision = \\frac{\\sum_{i=1}^{N} I(s_i > 0)}{N}\n\nwhere N is the total number of inserted images, and I(si > 0) is an indicator function that returns 1 if the score si of the i-th image is greater than zero, and 0 otherwise.\n\u2022 Recall3 is defined as the coverage rate of the relevant images, representing the probability that the images were inserted at all possible positions where the 3-point images could be inserted. Since the same image cannot be inserted in two different positions, we used the BPM algorithm to calculate the maximum number of relevant images that can be inserted under the current sample's score label. The Recall metric is defined as follows:\n\nRecall3 = \\frac{\\sum_{p \\in P} I(s_p = 3)}{\\sum_{p \\in P} M_{3p}}\n\nwhere P is the set of all possible image insertion points, and I(sp = 1) is an indicator function that returns 1 if an image with score 3 is inserted at position p, and 0 otherwise."}, {"title": "Human Evaluation", "content": "In addition to evaluating the text generation content and image placement, it is also necessary to assess the overall quality of the generated interleaved response. However, there is currently no effective automated method to evaluate the quality of a text-image interleaved response. Therefore, we employ human evaluation, assigning separate scores for the text, image, and overall response quality. For scoring in all three aspects, we use a 5-point Likert scale (Joshi et al., 2015) as the rating option."}, {"title": "Experimental Results", "content": ""}, {"title": "Experimental Settings", "content": "According to the description in Section 4.1, we constructed two datasets, the CIR-Interleave and CIR-Caption datasets, to enhance the model's ability of contextural image referencing. The CIR-Interleave dataset consists of multiple user prompt and retrieved documents, and the model is required to generate responses with contextural image references based on these texts and user prompts. The CIR-Caption dataset contains contextual caption texts, and the model is tasked with providing contextually relevant captions for each image mentioned within the text. Additionally, we blend the InternVL SFT dataset at a certain ratio to fine-tune the model. During the testing phase, we construct the CIR-Test dataset as outlined in Section 5, and report evaluation scores on this dataset, including text evaluation scores, image position evaluation metrics such as precision, recall, F1, and human evaluation scores. Detailed statistics for all the datasets used during training and testing can be found in Table 1.\nWe conduct experiments on InternVL2-8B and InternVL2-26B (Chen et al., 2024), using 16 A100 GPUs, optimizing the full-fine-tuning training process with DeepSpeed Zero-3 and gradient check-pointing for improved memory efficiency and scalability. Both models are trained with a global batch size of 128, utilizing a learning rate of 4e-5 for the 8B model and 2e-5 for the 26B model, with corresponding weight decay values of 0.01 and 0.05. Training is conducted over 1000 steps, with a maximum sequence length of 16,384 tokens. For ImageRef-VL-26B, we mix the InternVL2-SFT dataset with the combined MI-Interleave and IMI-Caption datasets at a 1:1 ratio and train the model for 550 steps. For ImageRef-VL-8B, we mix the InternVL2-SFT dataset with the combined MI-Interleave and IMI-Caption datasets at a 1:4 ratio and train the model for 950 steps."}, {"title": "Performance Comparison", "content": "In the subsection, we compare the performance of ImageRef-VL with the baseline methods, including existing close-sourced vision-language models:\n\u2022 GPT-40 (Hurst et al., 2024): a fast, cost-effective, multimodal large language model by OpenAI.\n\u2022 Claude-3.5-Sonnet (Anthropic, 2024): A multimodal large language model by Anthropic, offering advanced safety and language capabilities.\nand open-sourced vision-language models:\n\u2022 Phi-3.5-Vision (Abdin et al., 2024): a lightweight, open multimodal large language model by Microsoft.\n\u2022 Qwen2-VL-7B (Wang et al., 2024): the latest version of the vision language models in the Qwen model families.\n\u2022 InternVL2-26B (Chen et al., 2024): the latest addition to the InternVL series of multimodal large language models.\nand the three-stage response generation method introduced in Section 4.1 with GPT-40 and InternVL2-26B. We report the text evaluation score and human evaluation score for all models. For image position evaluation score, controlled sampling6 is used for open-sourced VLMs to complete image references based on the given text. For three-stage methods, we provide text in prompts and ask the model to insert image references (see Section 4.1 Image Insertions part). Since closed-source VLMs lack controlled sampling capabilities, their image position scores cannot be reported. The experimental results are shown in Table 2. We also present the detailed distribution of human evaluation scores for GPT-40, GPT-40 three-stage, as well as our ImageRef-VL-8B and -32B in Figure 3.\nEffectiveness of ImageRef-VL. In terms of the human evaluation metrics, ImageRef-VL-8B and ImageRef-VL-26B achieved the best performance in text scores, illustration scores, and overall experience. An 88% performance improvement is achieved over state-of-the-art open-source VLMs. This demonstrates the effectiveness of our approach. By further examining Figure 3, we can observe that the proportion of severe bad cases produced by ImageRef-VL is significantly lower than that of other methods, with the bad case rate of ImageRef-VL-26B being lower than that of ImageRef-VL-8B.\nOpen-source VLMs are significantly inferior to that of closed-source VLMs. Besides, among open-source VLMs, InternVL2 outperforms the others. Currently, open-source VLMs have not considered multi-image contextual understanding tasks during the SFT phase, which could be a reason for their weaker performance. Additionally, there is an inherent performance gap between open-source VLMs and state-of-the-art closed-source VLMs. Among the three open-source VLMs we tested, InternVL2 considered contextual multi-image input during the pretraining phase, which might explain why it performs better.\nThree-stage approach yields better results compared to direct end-to-end inference. The LLMs ability to understand long-context multi-image scenarios is weaker than its ability to perform text-based tasks. Additionally, the caption generation in the three-stage approach benefits from in-context learning, which provides a solid foundation for accurate illustration in the final stage.\nEffectiveness of Automatic Metrics. In addition to manual evaluation metrics, we propose two types of automatic evaluation metrics to efficiently conduct preliminary evaluation and model screening. For text evaluation, the Pearson correlation with the text score of human evaluation is 0.9033 (p < 0.005), demonstrating its validity. For image position evaluation, due to differences in illustration processes between the three-stage and end-to-end approaches, a fair comparison is not feasible. Excluding the three-stage approach, we calculate the Pearson correlation between the F1 score and image score of human evaluation as 0.9854 (p < 0.005), confirming the metric's validity."}, {"title": "Computation Overhead Analysis", "content": "In this section, we compare the computational costs of ImageRef-VL, an end-to-end interleaved response generation approach, with a three-stage generation approach. Define the number of images in a sample as N, the textual context length for each image as L, the total context length as M, the number of tokens occupied by a single image in the VLM as P, the response length as R, and the length of each image caption is C. The computational complexity of end-to-end method is\n\nO((M + NP + R)2),\n\nwhile the complexity of three-stage method is\n\nO((M + R)2 +9N(L + P + C)2 +9(R+NC)2).\n\nTo more intuitively understand the difference in computational costs between the two methods, Figure 5 contrasts ImageRef-VL-8B and ImageRef-VL-26B with their respective three-stage InternVL2-8B and InternVL2-26B counterparts. More specifically, we tested the end-to-end and 3-stage approaches of the 8B and 26B models on a machine equipped with 8 NVIDIA A100-SXM4-80GB GPUs to measure the execution time. The experimental results demonstrate that the end-to-end approach significantly reduces computational overhead compared to the three-stage scheme."}, {"title": "Hyper-parameter Analysis", "content": "Throughout the experiment, we focus on the impact of two hyper-parameters on the results: the proportion of mixed InternVL2 SFT data and the training steps of the model.\nData Mixture Ratio. Figure 4(a) shows the results for different data mixture ratios. For ImageRef-VL-26B, both the text evaluation score and image position evaluation F1 score initially rise and then decline, peaking at around a 1:1 data mixture ratio. In contrast, ImageRef-VL-8B's text evaluation score steadily increases with a higher MI data proportion, while the image position evaluation F1 score first drops and then rises.\nTraining Steps. The results of different data mixture ratio are shown in Figure 4(b). For ImageRef-VL-26B, the text evaluation score of the model increases with the number of training steps, converging and stabilizing around 500 steps. The F1 score of image position evaluation initially fluctuates but also stabilizes around 500 steps. ImageRef-VL-8B shows a similar trend, but the convergence occurs around 700 steps."}, {"title": "Conclusion", "content": "In this paper, we introduced Contextual Image Reference as a novel capability for Vision-Language Models and presented ImageRef-VL, a framework that significantly advances the state-of-the-art in this domain. Through our carefully curated training data and proposed fine-tuning approach, we demonstrated substantial improvements in VLMs\u2019 ability to incorporate relevant images contextually in their responses. Our comprehensive evaluation framework, including both automated metrics and human assessment, validates the effectiveness of our approach. ImageRef-VL demonstrates superior performance over baseline models, achieving significantly better contextual image referencing while being computationally more efficient than multi-stage approaches. Our end-to-end system advances the development of visually-aware conversational AI. Future work could explore dynamic image generation and modification capabilities. We believe this work provides a strong foundation for research in multimodal AI systems with enhanced visual understanding."}, {"title": "Limitations", "content": "Our work validates the capability of enhanced VLMs to perform interleaved multi-image understanding, thereby exploring the feasibility of contextual image reference. However, our current experiments are based on post-finetuning of InternVL2. Starting from a well-pretrained VLM and incorporating the collected dataset into the supervised finetuning phase might yield better results. Additionally, the current model still exhibits a probability of bad cases. On one hand, collecting more and richer training datasets might address this issue; on the other hand, designing rewards and leveraging techniques like RLHF or RLAIF could be employed for further fine-tuning of the model."}]}