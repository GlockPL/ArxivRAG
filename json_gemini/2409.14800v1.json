{"title": "Choose the Final Translation from NMT and LLM hypotheses Using MBR Decoding: HW-TSC's Submission to the WMT24 General MT Shared Task", "authors": ["Zhanglin Wu", "Daimeng Wei", "Zongyao Li", "Hengchao Shang", "Jiaxin Guo", "Shaojun Li", "Zhiqiang Rao", "Yuanchang Luo", "Ning Xie", "Hao Yang"], "abstract": "This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT24 general machine translation (MT) shared task, where we participate in the English to Chinese (en\u2192zh) language pair. Similar to previous years' work, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train the neural machine translation (NMT) model based on the deep Transformer-big architecture. The difference is that we also use continue pre-training, supervised fine-tuning, and contrastive preference optimization to train the large language model (LLM) based MT model. By using Minimum Bayesian risk (MBR) decoding to select the final translation from multiple hypotheses for NMT and LLM-based MT models, our submission receives competitive results in the final evaluation.", "sections": [{"title": "1 Introduction", "content": "Machine translation (MT) (Brown et al., 1990) predominantly utilizes transformer encoder-decoder architectures (Vaswani et al., 2017), which is evident in prominent models such as NLLB-200 (Costa-juss\u00e0 et al., 2022), M2M100 (Fan et al., 2021), and MT5 (Xue et al., 2021). Significant research effort has been devoted to task-specific neural machine translation (NMT) models (Wei et al., 2022; Wu et al., 2023b) trained in a fully supervised manner with large volumes of parallel data. Their performance has been enhanced through techniques such as regularized dropout (Wu et al., 2021), bidirectional training (Ding et al., 2021), data diversification (Nguyen et al., 2020), forward translation (Abdulmumin, 2021), back translation (Sennrich et al., 2016), alternated training (Jiao et al., 2021), curriculum learning (Zhang et al., 2019), and transductive ensemble learning (Wang et al., 2020b).\nThe emergence of decoder-only large language models (LLMs) such as the GPT series (Wu et al., 2023a; Achiam et al., 2023), Mistral (Jiang et al., 2023), and LLAMA (Touvron et al., 2023a,b) shows remarkable efficacy in various NLP tasks, providing a fresh perspective on the MT task. Recent studies (Hendy et al., 2023; Jiao et al., 2023) indicate that larger LLMs such as GPT-3.5 (175B) and GPT-4 exhibit strong translation abilities. However, the performance of smaller-sized LLMs (7B or 13B) still falls short when compared to conventional NMT models (Zhu et al., 2024). Therefore, there are studies (Yang et al., 2023; Zeng et al., 2024) intend to enhance the translation performance for these smaller LLMs, but their improvements are relatively modest, primarily due to the predominant pre-training of LLMs on English-centric datasets, resulting in limited linguistic diversity. Addressing this limitation, Xu et al. (Xu et al., 2023) initially continue pre-training (CPT) LLaMA-2 (Touvron et al., 2023b) with extensive non-English monolingual data to enhance their multilingual abilities, and then perform supervised fine-tuning (SFT) with high-quality parallel data to instruct the model to generate translations. Nonetheless, the performance still lags behind leading translation models such as GPT-4 and WMT competition winners. Subsequently, Xu et al. (Xu et al., 2024) bridged this gap by further fine-tuning the LLM-based MT model using contrast preference optimization (CPO).\nEnsembling (Zhou et al., 2002) has a long history in machine learning, being well known for leveraging multiple complementary systems to improve performance on a given task and provide good/robust generalization. Minimum Bayesian risk (MBR) (Finkelstein and Freitag, 2023; Farinhas et al., 2023) decoding has successfully improved translation quality using task-specific NMT models, and subsequently it has also been shown to be suitable for LLM-based MT models."}, {"title": "2 Data", "content": "2.1 Data Source\nWe obtain bilingual and monolingual data from ParaCrawl v9, News Commentary v18.1, Wiki Titles v3, UN Parallel Corpus V1.0, CCMT Corpus, WikiMatrix, News Crawl and Common Crawl data sources. The amount of data we used for training NMT and LLM-based MT models is shown in Table 1. It should be noted that in order to obtain better translation performance in the general domain, we mix the monolingual data from Common Crawl and News Crawl.\n2.2 NMT Data Pre-processing\nOur data pre-processing methods for NMT include:\n\u2022 Remove duplicate sentences or sentence pairs.\n\u2022 Convert full-width symbols to half-width.\n\u2022 Use fasttext\u00b9 (Joulin et al., 2016) to filter other language sentences.\n\u2022 Use jieba\u00b2 to pre-segment Chinese sentences.\n\u2022 Use mosesdecoder\u00b3 (Koehn et al., 2007) to normalize English punctuation.\n\u2022 Filter out sentences with more than 150 words.\n\u2022 Use fast-align (Dyer et al., 2013) to filter sentence pairs with poor alignment.\n\u2022 Sentencepiece\u2074 (SPM) (Kudo and Richardson, 2018) is used to perform subword segmentation, and the vocabulary size is set to 32K.\nSince there may be some semantically dissimilar sentence pairs in bilingual data, we use LaBSE5"}, {"title": "2.3 LLM-based MT Data Pre-processing", "content": "The training of the LLM-based MT model requires three stages: CPT, SFT and CPO. As shown in Figure 1, the training data templates of the LLM-based MT model in these three stages are different.\nIn the CPT stage, considering that most LLMs are trained on English-dominated data, we using Chinese and English monolinguals for CPT to improve LLM's proficiency in Chinese. To preserve the long-context modeling capability of LLM, we concatenate multiple sentences into a long text with no more than 4096 words, and preferentially concatenate sentences from the same document.\nIn the SFT stage, drawing inspiration from the recognized significance of data quality in other applications (Zhou et al., 2024; Maillard et al., 2023), we fine-tune the model with high-quality parallel data. In order to obtain high-quality parallel data, we use cometkiwi model \u2076 (Rei et al., 2022) to calculate the score of bilingual data on the en\u2192zh language pair, and then retain bilingual data with a cometkiwi score greater than 0.8.\nIn the CPO stage, to learn an objective that fosters superior translations and rejects inferior ones, access to labeled preference data is essential, yet such data is scarce in machine translation. The following describes our process of constructing the triplet preference data required for CPO training. First, we randomly sample 50,000 data from high-quality bilingual data. Then, we use the NMT model to obtain N-best (N=10) hypotheses based on beam search decoding, and then use the comet-da model \u2077 (Rei et al., 2020) to calculate the score of each hypothesis, select the hypothesis with the highest score as the preferred translation, and select the hypothesis with the lowest score as the dis-preferred translation."}, {"title": "3 NMT System", "content": "3.1 System Overview\nTransformer is the state-of-the-art model structure in recent NMT evaluations. There are two parts of research to improve this kind: the first part uses wide networks (eg: Transformer-Big (Vaswani et al., 2017)), and the other part uses deeper language representations (eg: Deep Transformer (Wang et al., 2019)). For the WMT24 general MT shared task, we combine these two improvements, adopting the Deep Transformer-Big (Wei et al., 2022; Wu et al., 2023b) model structure to train the NMT system. Deep Transformer-Big uses pre-layer normalization, features 25-layer encoder, 6-layer decoder, 16-heads self-attention, 1024-dimensional word embedding and 4096-dimensional FFN embedding.\nFig. 2 shows the overall training flow of NMT system. We use training strategies such as regularized dropout (R-Drop) (Wu et al., 2021), bidirectional training (BiT) (Ding et al., 2021), data diversification (DD) (Nguyen et al., 2020), forward translation FT) (Abdulmumin, 2021), back translation (BT) (Sennrich et al., 2016), alternated training (AT) (Jiao et al., 2021), curriculum learning (CL) (Zhang et al., 2019), and transductive ensemble learning (TEL) (Wang et al., 2020b) for training.\n3.2 Regularized Dropout\nRegularized Dropout (R-Drop)\u2078 (Wu et al., 2021) is a simple yet more effective alternative to regularize the training inconsistency induced by dropout (Srivastava et al., 2014). Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence (Van Erven and Harremos, 2014) between the two distributions. That is, R-Drop regularizes the outputs of two sub models ran-"}, {"title": "3.3 Bidirectional Training", "content": "Many studies have shown that pre-training can transfer the knowledge and data distribution, hence improving the model generalization. Bidirectional training (BiT) (Ding et al., 2021) is a simple and effective pre-training method for NMT. Bidirectional training is divided into two stages: (1) bidirectionally updates model parameters, and (2) tune the model. To achieve bidirectional updating, we only need to reconstruct the training samples from \"src\u2192tgt\" to \"src\u2192tgt & tgt\u2192src\" without any complicated model modifications. Notably, BiT does not require additional parameters or training steps and only uses parallel data."}, {"title": "3.4 Data Diversification", "content": "Data Diversification (DD) (Nguyen et al., 2020) is a data augmentation method to boost NMT performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset which the final NMT model is trained on. DD is applicable to all NMT models. It does not require extra monolingual data, nor does it add more parameters. To conserve training resources, we only use one forward model and one backward model to diversify the training data."}, {"title": "3.5 Forward Translation", "content": "Forward translation (FT) (Abdulmumin, 2021), also known as self-training, is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a \u201cteacher\u201d NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a \"student\" NMT model."}, {"title": "3.6 Back Translation", "content": "An effective method to improve NMT with target monolingual data is to augment the parallel training data with back translation (BT) (Sennrich et al., 2016; Wei et al., 2023). There are many works expand the understanding of BT and investigates a number of methods to generate synthetic source sentences. Edunov et al. (2018) find that back translations obtained via sampling or noised beam outputs are more effective than back translations generated by beam or greedy search in most scenarios. Caswell et al. (2019) show that the main role of such noised beam outputs is not to diversify the source side, but simply to tell the model that the given source is synthetic. Therefore, they propose a simpler alternative strategy: Tagged BT. This method uses an extra token to mark back translated source sentences, which generally outperforms noised BT (Edunov et al., 2018). For better joint use with FT, we use sampling back translation (ST) (Edunov et al., 2018)."}, {"title": "3.7 Alternated Training", "content": "While synthetic bilingual data have demonstrated their effectiveness in NMT, adding more synthetic data often deteriorates translation performance since the synthetic data inevitably contains noise and erroneous translations. Alternated training (AT) (Jiao et al., 2021) introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. AT describes the synthetic and authentic data as two types of different approximations for the distribution of infinite authentic data, and its basic idea is to alternate synthetic and authentic data iteratively during training until the model converges."}, {"title": "3.8 Curriculum Learning", "content": "A practical curriculum learning (CL) (Zhang et al., 2019) method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking. For ranking, we choose to estimate the difficulty of training samples according to their domain feature (Wang et al., 2020a). The calculation formula of domain feature is as follows, where din represents an in-domain NMT model, and bout represents a out-of-domain NMT model. One thing to note is that we treat domains including news, user-generated (social), conversational, and e-commerce domains as in-domain, and others as out-of-domain. Specifically, we use the WMT22 test set to fine-tune a baseline model, and then use the baseline model and the fine-tuned model as the out-of-domain model and the in-domain model respectively."}, {"title": "3.9 Transductive Ensemble Learning", "content": "Ensemble learning (Garmash and Monz, 2016), which aggregates multiple diverse models for inference, is a common practice to improve the performance of machine learning models. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) (Zhang et al., 2019) studies how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses all individual models to translate the source test set into the target language space and then fine-tune a strong model on the translated synthetic data, which significantly boosts strong individual models and benefits a lot from more individual models."}, {"title": "4 LLM-based MT System", "content": "4.1 System Overview\nThere is recently a surge in research interests in Transformer-based LLMs, such as ChatGPT (Wu et al., 2023a), GPT-4 (Achiam et al., 2023), and LLaMA (Touvron et al., 2023a,b). Benefiting from the giant model size and oceans of training data, LLMs can understand better the language structures and semantic meanings behind raw text, thereby showing excellent performance in a wide range of natural language processing (NLP) tasks. Although the training methodology of LLMs is simple, high computational requirements have limited the development of LLMs to a few players. In order to avoid training LLM from scratch, we chose to conduct research work on the open source Llama2-13b\u2079 (Touvron et al., 2023b) model. Llama2-13b is an autoregressive language model using an optimized transformer architecture that is pre-trained on 2 trillion tokens of data from publicly available"}, {"title": "4.2 Continue Pre-training", "content": "LLMs like LLaMA are pre-trained on English-dominated corpora. This potentially explains their inadequate translation performance which necessitates cross-lingual capabilities. To ameliorate this, our first stage is to perform continue pre-training (CPT) on LLM with Chinese and English monolingual data to improve proficiency in Chinese and prevent forgetting of English knowledge. Previous studies also offer some clues that monolingual data help in translation. For instance, guo et al. (Guo et al., 2024) proposed a three-stage training method, which proved that using CPT can improve the performance of MT task in the SFT stage. Note that we use full fine-tuning at this stage."}, {"title": "4.3 Supervised Fine-tuning", "content": "LLMs have shown remarkable performance on a wide range of NLP tasks by leveraging in-context learning (Brown et al., 2020). However, this approach exhibits several drawbacks: performance is highly dependent on the quality of examples (Vilar et al., 2023), outputs are plagued by overgeneration (Bawden and Yvon, 2023), and inference cost are greatly increased by processing all input pairs. When parallel data is available, LLMs can perform supervised fine-tuning (SFT) on translation instructions (Li et al., 2024). Drawing inspiration from the recognized significance of data quality in other applications (Zhou et al., 2024),we use the cometkiwi model (Rei et al., 2022) to filter out large amounts of high-quality parallel data. Here, we use efficient lightweight low-rank adaptation (LoRA) fine-"}, {"title": "4.4 Contrastive Preference Optimization", "content": "Contrastive Preference Optimization (CPO) (Xu et al., 2024) aims to mitigate two fundamental shortcomings of SFT. First, SFT's methodology of minimizing the discrepancy between predicted outputs and gold-standard references inherently caps model performance at the quality level of the training data. This limitation is significant, as even human-written data, traditionally considered high-quality, is not immune to quality issues. Secondly, SFT lacks a mechanism to prevent the model from rejecting mistakes in translations. While strong translation models can produce high-quality translations, they occasionally exhibit minor errors, such as omitting parts of the translation. Preventing the production of these near-perfect but ultimately flawed translation is essential. To overcome these issues, we introduce CPO to train the LLM-based MT model using specially curated triplet preference data. Here, we construct a high-quality preference data for the WMT24 general MT task, and like the SFT stage, only update the weights of the added LoRA parameters."}, {"title": "4.5 Minimum Bayes Risk Decoding", "content": "Minimum Bayesian Risk (MBR) (Kumar and Byrne, 2004; Eikema and Aziz, 2020) decoding aims to find the output that maximizes the expected utility function, which measures the similarity between the hypothesis and the reference. For MT, this could be an automated evaluation metric such as COMET (Rei et al., 2020). Garcia et al. (Garcia et al., 2023) train their own language models, sample multiple hypotheses and choose a final translation using MBR decoding, which has been shown to improve the translation capabilities of task-specific models (Fernandes et al., 2022). Subsequently, Farinhas et al. (Farinhas et al., 2023) find that MBR is also suitable for LLM-based MT. They provide a comprehensive study on ensembling translation hypotheses, proving that MBR decoding is a very effective method and can improve translation quality using a small number of samples. As shown in Figure 4, we simultaneously collect the N-best translations generated by the NMT system based on beam search and the N-best translations generated by the LLM-based MT system based on temperature and nucleus sampling (with t=0.8 and p=0.95), and then use MBR Decoding selects the final translation."}, {"title": "5 Experiment", "content": "5.1 Setup\nWe use the open-source fairseq (Ott et al., 2019) to train NMT models, and then use SacreBLEU"}]}