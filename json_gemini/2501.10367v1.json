{"title": "GTDE: Grouped Training with Decentralized Execution for Multi-agent Actor-Critic", "authors": ["Mengxian Li", "Qi Wang", "Yongjun Xu"], "abstract": "The rapid advancement of multi-agent reinforcement learning (MARL) has given rise to diverse training paradigms to learn the policies of each agent in the multi-agent system. The paradigms of decentralized training and execution (DTDE) and centralized training with decentralized execution (CTDE) have been proposed and widely applied. However, as the number of agents increases, the inherent limitations of these frameworks significantly degrade the performance metrics, such as win rate, total reward, etc. To reduce the influence of the increasing number of agents on the performance metrics, we propose a novel training paradigm of grouped training decentralized execution (GTDE). This framework eliminates the need for a centralized module and relies solely on local information, effectively meeting the training requirements of large-scale multi-agent systems. Specifically, we first introduce an adaptive grouping module, which divides each agent into different groups based on their observation history. To implement end-to-end training, GTDE uses Gumbel-Sigmoid for efficient point-to-point sampling on the grouping distribution while ensuring gradient backpropagation. To adapt to the uncertainty in the number of members in a group, two methods are used to implement a group information aggregation module that merges member information within the group. Empirical results show that in a cooperative environment with 495 agents, GTDE increased the total reward by an average of 382% compared to the baseline. In a competitive environment with 64 agents, GTDE achieved a 100% win rate against the baseline.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning has recently made remarkable progress across diverse real-world applications, such as multi-robot cooperative tasks (Krnjaic et al. 2022), complex games (Silver et al. 2018; Vinyals et al. 2019), computer network (Wang et al. 2022; Wang, Huang, and Xu 2024), and autonomous driving (Zhou et al. 2022; Dinneweth et al. 2022). These scenarios are often constrained by partial observability or limited communication, necessitating the use of decentralized policies. These policies rely solely on the observation history of each individual agent to make decisions. The decentralized policies also avoid the issue of exponential growth in the action space as the number of agents increases, making a fully centralized policy impractical for large-scale multi-agent systems. The decentralized policies can be learned through either Decentralized Training and Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) (Albrecht, Christianos, and Sch\u00e4fer 2024).\nDTDE utilizes a decentralized approach to simplify the training of multi-agents by using a single-agent RL algorithm for each individual agent. However, this training paradigm introduces two inevitable issues. Firstly, agents are unable to leverage information from other agents in the environment, making cooperation difficult (Mguni et al. 2023). Secondly, since each agent treats other agents as part of the environment, they are affected by environmental non-stationary caused by updates in the policies of other agents (Laurent, Matignon, and Fort-Piat 2011), which intensifies with a larger number of agents.\nCTDE uses a centralized approach to train decentralized policies (Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016), allowing agents to utilize global information during training while effectively avoiding the non-stationarity of the environment. However, due to limitations in partial observability, obtaining comprehensive global information is often infeasible. Fortunately, we can replace global information with joint information. Despite this, as the number of agents increases, three challenges remain. Firstly, the use of joint information faces the curse of dimensionality (Oroojlooy and Hajinezhad 2023), which makes it impossible to train in large-scale scenarios, such as the Gather scenario with 495 agents (Terry, Black, and Jayakumar 2020). Secondly, in regions where agents have weak interaction, joint information may not offer significant benefits (Kok and Vlassis 2004), indicating potential information redundancy. Thirdly, communication restrictions may prevent each agent from obtaining joint observations from all other agents, which poses a challenge for CTDE paradigms to train in real-world applications.\nWith an increasing number of agents, the aforementioned two paradigms may no longer apply to the training of policies. To address the training problem of large-scale multi-agent systems, we consider abstracting the information required by agents during the training of policies as directed links between agents, thereby obtaining a directed graph. Through this approach, we can obtain that DTDE using individual information\u00b9 represents directed links between agents as self-linked, while CTDE using joint information represents directed links between agents as pairwise links. There is a situation between these two frameworks, as shown in Fig.1, which is the partial links between agents. This also indicates a fact that agents should focus on the specific local information they require (Iqbal and Sha 2019), rather than relying on individual or joint information from agents. Using local information for training of policies can effectively alleviate the problems in DTDE and CTDE, and also provide the possibility for large-scale multi-agent training in real-world applications. For the sake of simplicity, we define the information required by an agent as a group of this agent, as described in Def.1.\nIn this paper, we propose a novel framework called Grouped Training with Decentralized Execution (GTDE). GTDE mainly considers dividing a large-scale multi-agent system into numerous small groups during training at each timestep, and each agent only needs to utilize the information within the group. However, the dynamic nature of the environment presents an issue in that the information required by each agent can vary over time. Therefore, we propose an adaptive grouping module, in which the number of groups and the number of members within the same group can vary continuously over time. At each timestep, adaptive grouping is only based on observation history to obtain the multivariate Bernoulli distribution of each agent link, and Gumbel-Sigmoid (a variant of Gumbel-Softmax) is used to sample the distribution. After that, given the uncertainty regarding the number of members within the group, we use two methods to implement the group information aggregation module to efficiently consolidate information within the group. During the execution of policies, agents take actions only based on their individual information, such as DTDE"}, {"title": "2 Related Works", "content": "Several algorithms (Lowe et al. 2017; Papoudakis et al. 2021; de Witt et al. 2020; Yu et al. 2022) based on CTDE and DTDE encounter challenges such as environmental non-stationarity and the curse of dimensionality, as mentioned in Sec. 1. MAAC (Iqbal and Sha 2019) and G2Anet (Liu et al. 2020) identified these issues and addressed them by introducing a soft/hard attention mechanism, which can effectively address the challenges of dimensionality and information redundancy in the centralized critic. However, this approach introduces a new issue. Centralized attention mechanisms necessitate the utilization of information from all agents, making it impossible to train in large-scale scenarios. Therefore, we consider solving the above problem by grouping without introducing other centralized modules.\nSome early works utilized the prior domain knowledge to group the agents (Odell, Parunak, and Fleischer 2002; Lhaksmana, Murakami, and Ishida 2013). Mean-Field (Yang et al. 2018) RL uses the actions of nearby agents to estimate the Q-function, but this method does not explicitly define how to determine the neighbors of agents. Subsequently, role-based (Wang et al. 2020; Liu et al. 2022) and subtask-based (Yang et al. 2022; Yuan et al. 2022; Huang et al. 2022) methods group agents to different roles or tasks, and agents of the same role or subtask are naturally grouped together. However, subtask-based and role-based methods require the introduction of an additional centralized module for grouping, and some of them need to perform a clustering algorithm at each time step before running the MARL algorithm, incurring a high time cost. Other works, such as REFIL (Iqbal et al. 2021) divides agents into two groups, VAST (Phan et al. 2021) uses meta-policy for agent grouping, DHCG (Liu et al. 2023) learning grouping policy using RL method, and CommFormer (Hu et al. 2024) learning a static graph. The above grouping algorithms all have the same problem, which is that the number of groups or the"}, {"title": "3 Background", "content": "Decentralized Partially Observable Markov Decision Processes (DEC-POMDP)\nWe study DEC-POMDP (Oliehoek and Amato 2016), which is defined by a tuple (n, S, Z, O, A, R, P, \u03b3), where n is the number of agents. s\u2208 S is the true state of the environment. At every stage t, the environment obtains joint observations z \u2208 Z through the observation function O(st, at-1) and emits them to each agent. Each agent i has an observation history \u03c4i = (o\u00b9,...,o\u1d57)2. Each agent i selects action ai \u2208 Ai based on a parameterized policy \u03c0i(\u03b1i|Ti; 0). The environment transitions to the next state based on the joint actions a \u2208 A of all agents and state transition functions P(st+1|st, at), giving the same reward rt+1 \u2208 R(st,at) to all agents through the reward function R(st, at). The objective of the agents is to maximize the expected return J = $\\sum_{t=1}^{T} \\gamma^{t-1}r_t$ where \u03b3 \u2208 [0, 1) is a discount factor and T is the episode length.\nActor-Critic Algorithms\nThe Policy Gradient (PG) method (Sutton and Barto 2018; Sutton et al. 1999) directly learns parameterized policies to maximize the expected return of the agent, with the optimization objective $J(\\theta) = E_{s\\sim \\rho_{\\pi}, a \\sim \\pi(\\theta)}[R(s, a)]$. The gradient with respect to the policy parameters is denoted as\n$\\nabla_{\\theta}J(\\theta) = E_{s\\sim \\rho_{\\pi}, a \\sim \\pi(\\theta)}[\\nabla_{\\theta} log \\pi(a|\\tau;\\theta)Q^{\\pi}(\\tau, a; \\varphi)]$\nwhere $p_{\\pi}$ is the state distribution under policy \u03c0. Actor-Critic Algorithms (Konda and Tsitsiklis 1999) approximates state-action value $Q_{\\pi}(s, a)$ through TD learning. It also uses parameterized functions for updates\n$L_{TD}(\\varphi) = E_{\\mathcal{D}}[(Q_t - (R(s^t, a^t) + \\gamma Q_{t+1}))^2]$ (1)\nwhere D is a set of tuples ($s^t, a^t, r_{t+1}, s^{t+1}$) generated through interaction with the environment and Qt = $Q(\\tau^t, a^t; \\varphi)$.\nMulti-Agent Actor-Critic Algorithms\nIn the framework of CTDE, the multi-agent actor-critic algorithm utilizes the joint information of each agent to learn a critic for each agent or directly learn a centralized critic.\nThe gradient of the expected return J(0) with respect to the policy parameters is approximated by\n$\\nabla_{\\theta_i} J(\\theta) = E_{\\mathcal{D}}[\\nabla_{\\theta_i} log \\pi(a_i|\\tau_i; \\theta_i)Q^{\\pi}(\\tau, a; \\varphi_i)]$\nIn the framework of DTDE, the multi-agent actor-critic algorithm only utilizes individual information of each agent to learn a critic for each agent\n$\\nabla_{\\theta}J(\\theta) = E_{\\mathcal{D}}[\\nabla_{\\theta_i} log \\pi(a_i|\\tau_i; \\theta_i)Q^{\\pi} (\\tau_i, A_i; \\varphi_i)]$\nThis paper also uses the PPO algorithm and therefore provides the PPO algorithm of DTDE and CTDE paradigms. The policy is updated to maximize\n$J(\\theta) = E_{\\mathcal{D}}[min(\\frac{\\pi_{new}}{\\pi}A_{\\pi}, clip(\\frac{\\pi_{new}}{\\pi}, 1 \\pm \\epsilon)A_{\\pi})]$ (2)\nwhere clip(;1 \u00b1 \u03f5) operator clips the input to 1 \u2013 \u0454/1 + \u0454 if it is below/above this value, and \u03c0 = $\\pi(a_i| \\tau_i; \\theta_i)$. For the IPPO algorithm, $A_{\\pi} = A_{\\pi}(\\tau_i, a_i; \\varphi_i)$ and for the MAPPO algorithm, $A_{\\pi} = A_{\\pi}(\\tau, a; \\varphi_i)$."}, {"title": "4 Methods", "content": "We abstract the information required between agents into a directed graph Gt = (Vt, Et), where Gt represents the directed graph at timestep t, Vt is the set of nodes representing the agents and Et is the set of edges between nodes at timestep t representing the information required by the agent. It can be seen that due to the dynamism of multi-agent systems, their directed graphs are inconsistent at each timestep.\nDefinition 1. For the i-th node vi in a directed graph, we set the group of vi to g(vi), then\n$g(v_i) = \\{c|I_{E_t}(<v_i, c>) = 1, c \\in V^t\\}$ (3)\nwhere $I_{E_t}(<v_i,c>)=\\begin{cases}1, <v_i, c>\\in E^t\\\\0, <v_i, c>\\notin E^t\\end{cases}$\nand represents the directed edge from node u to node v.\nGTDE\nAs shown in Fig.2, the GTDE framework extends the DTDE and CTDE framework by introducing two additional components: the adaptive grouping module and the group information aggregation module. The adaptive grouping module enables the division of agents into distinct groups based on their individual observation history. The group information aggregation module fuses information from group members. Different from the previous grouping method, GTDE does not enforce a fixed number of groups or group members. Instead, it dynamically forms groups based on the observation history of each agent.\nCTDE utilizes the joint information of the agent to estimate the value function, while DTDE relies on the individual information of the agent for the same purpose. In contrast, as shown in Fig.1, GTDE employs partial information from the agent within the group to estimate the value function. We observed that when a multi-agent system with n agents is represented as a directed graph, where nodes signify the observations of the agents and edges denote information required between agents. Notably, during the training of policies, the CTDE framework treats the entire multi-agent system as one group, where each group consists of n members. In contrast, the DTDE framework views the multi-agent system as n separate groups, assigning only one member to each group. GTDE framework takes an intermediate approach, categorizing agents into g groups, with mg members in each group, where 1 \u2264 g,mg \u2264 n. CTDE aligns with GTDE when all agents are pairwise linked, while DTDE is equivalent to GTDE when all agents are self-linked. One of the goals of GTDE is to predict the edges of links. Once the link is determined, the system is naturally divided into many groups. GTDE relies on intra-group information during training, rather than global or joint information, and only relies on individual information during execution, making it practical in real-world applications.\nAdaptive Grouping\nThe adaptive grouping module determines the links of agent i through Pij (Ci = j|ti), where t\u2081 is the observation history of the agent i and c\u2081 = j is the i-th agent is linked to the j-th agent. The agent only needs information from the other agents it is linked to, which avoids the challenge of assigning the agent to a specific group when the number of groups is uncertain. This probability distribution pij (ci = j|ti) can be determined using physical distance, but introduces a new problem. Consider an environment with two buttons placed far apart, where pressing them simultaneously yields rewards. In this setting, with many agents present, only the two agents closest to the buttons are grouped together as the most appropriate choice. However, relying solely on physical distance assigns a very small probability of linking them. This also indicates that multi-agent grouping cannot be equated with data clustering. Even agents with highly unrelated features may belong to the same group.\nThe adaptive grouping module predicts the edge set Et of the directed graph and uses this edge set to group multi-agent systems using Eq.3. Compared to prior-based fixed grouping methods, the adaptive grouping module dynamically adjusts the number of groups and members within the group at each timestep based on the dynamic changes in the environment. Meanwhile, due to the absence of centralized modules, each agent selects one or more links based on their own observation history during training. After all agents have completed grouping, a multi-agent system is modeled as a directed graph. Specifically, the adaptive grouping module is parameterized by a neural network, with the input being the observation history of a single agent and the output being the n-variate Bernoulli distribution, where n is the number of all agents. Sample on the n-variable Bernoulli distribution to obtain the final determined grouping distribution. It should be noted that each row in the adjacency matrix At of a directed graph Gt represents the grouping distribution of an agent. According to the definition of the group, when the grouping distribution of agents is consistent, their belonging groups are also consistent, which makes the number of groups not fixed. At the same time, sampling from the n-variate Bernoulli distribution can obtain different grouping distribution, which makes the number of members in the same group not fixed. Due to the non-differentialable of the sampling operator in backpropagation, we need to use the reparameterization trick of discrete distributions. We can use Gumbel-Sigmoid to efficiently sample n-variable Bernoulli distribution, which is equivalent to Gumbel-Softmax. Please refer to Appendix A for a detailed discussion. As depicted in Sec.3, the J(0) under the GTDE framework in the Actor-Critic algorithm is updated to maximize\n$J(\\theta) = E_{\\rho}[log \\pi(a_i|\\tau_i; \\theta_i)Q_{\\pi} (\\tau_{g(v_i)}, A_i; \\varphi)]$\nwhere $\\tau_{g(v_i)}$ is the aggregation information of observation history from agent i and its links(To be mentioned in the next section). For the PPO algorithm, consistent with Eq.2, where $A_{\\pi} = A_{\\pi}(\\tau_{g(v_i)}, a_i; \\varphi)$. For simplicity, this paper employs parameter sharing, setting $\\theta = \\theta_1 = \\theta_2 = \u00b7\u00b7\u00b7 = \\theta_n$.\nGroup Information Aggregation\nAfter the adaptive grouping module obtains the adjacency matrix of the directed graph Gt, the group information ag"}, {"title": "5 Experiments", "content": "In this section, we compared GTDE with CTDE and DTDE and also evaluated variants of GTDE used for ablation study, represented as GTDE-F with fixed grouping distribution and GTDE-U with uniform grouping distribution, as well as GTDE-A with pairwise links, i.e. GTDE degenerated into CTDE. All algorithms were trained using an NVIDIA GeForce RTX 4090 GPU.\nExperimental setup\nSMACv2 SMACv2, like SMAC (Samvelyan et al. 2019), consists of two opposing units, each of our units represented by agents trained using the MARL algorithm. Conversely, enemy units utilize a built-in heuristic algorithm. Our primary objective is to eliminate all enemy units within the specified time frame, so the win rate has become an important evaluation indicator for this environment. In contrast to SMAC, SMACv2 predominantly incorporates random team composition, random initial positions, and aligns the attack range of units with the visible range, thereby enhancing the overall randomness of the environment. In this paper, the version we are utilizing is SC2.4.10. GTDE uses GAT to aggregate intra-group information in this scenario.\nBattle and Gather Battle is a large-scale mixed cooperative competitive scenario where two armies face off against each other in a grid world. Each army consists of 64 agents, and each agent can either move or attack each turn. The goal for both sides is to eliminate the opposing army as quickly as possible. In Gather, agents earn rewards by consuming food. Each piece of food must be broken down by five \"attacks\" before it can be absorbed. The amount of food on the map is limited, and once a shortage occurs, agents may begin attacking each other in an effort to monopolize the remaining resources. Agents can kill each other with a single attack, and there are a total of 495 agents on the map. In the two scenarios, the observations of each agent are composed of their own features and a circle centered around themselves. GTDE uses matrix multiplication to aggregate intra-group information in this scenario. For Battle and Gather scenarios, we use zero padding observations of dead agents.\nHyperparameter\nGTDE focuses on the actor-critic algorithm, so we select MAAC/MAPPO and IAC/IPPO as representatives of CTDE and DTDE respectively, as our baselines. Compare GTDE based on PPO with MAPPO and IPPO, and compare GTDE based on AC with IAC and MAAC. IAC/IPPO trains the critic using the individual observations of the agent, while MAAC/MAPPO trains a centralized critic using the joint observations of all agents. MAPPO and IPPO are used to evaluate SMACv2, while MAAC and IAC are used to evaluate Battle and Gather. For a fair comparison, all basic hyperparameters are set to be consistent. For SMACv2, Battle, and Gather, we trained 10M, 2000, and 1200 environment steps, respectively. For each scenario, we use 5 different random seeds for all algorithms. Additional detailed hyperparameters are addressed in Appendix B.\nPerformance\nSMACv2 In the SMACv2 benchmark, since the performances of all algorithms in the 5v5 and 10v10 scenarios are similar, we only report the performance results for the 20v20 scenario. Fig.3 shows the test curve of the average win rate. It can be seen that IPPO, which only uses local information, performs the worst in all scenarios. MAPPO and GTDE achieved similar performance in Zerg and Terran scenarios, and better results in the Protoss scenario. This may be due to the lower unit health in Terran and Zerg maps, which leads to shorter episode lengths. In contrast, the presence of shields in the Protoss map can extend the episode duration, giving the agent more time to learn how to effectively utilize local information. As shown in Table 1, we conducted 200 rounds of testing on all models and found that GTDE improved performance by 6.0% and 17.0% compared to CTDE and DTDE.\nBattle Since there is no built-in heuristic algorithm for the Battle scenario, we employ self-play for training. Fig.4 shows the total reward curves of GTDE, MAAC, and IAC. It can be seen that IAC experiences significant fluctuations in learning due to relying solely on its individual observation, ultimately converging to a local optimal solution where neither side attacks. In contrast, MAAC, which requires joint observations of all agents, has a much larger input dimension of $(13\u00d713\u00d75+32)\u00d764$, leading to even lower performance compared to IAC when using the same network size. GTDE, by utilizing local observations instead of individual or joint ones, achieves a more stable improvement in learning performance. Table 1 shows the win rate of GTDE against IAC and MAAC algorithms, indicating that GTDE defeated them with a 100% win rate. Fig.5 illustrates the links of partial agents in the Battle scenario. It is evident that Agent 15 is positioned far from the other agents, requiring information to encircle the enemy. Similarly, Agent 13, as part of the besieging group, also depends on information from the other besiegers.\nGather In the gather scenario, the large dimensionality of joint observations demands excessive GPU memory, making it impossible to train MAAC in this scenario. Therefore, Fig.6 only shows the total reward curves of GTDE and IAC. It can be seen that the performance gap between GTDE and IAC has further widened. As the scale of agents increases significantly, centralized training methods become impractical, leaving decentralized training as the only option. However, decentralized methods tend to be less effective. GTDE, which leverages local information, offers a better solution to this issue.\nFurthermore, Table 2 shows the average agent information used by the three training paradigms in each scenario. Since agents might select agents that have already been eliminated, and these agents do not carry any information(their observation values are all zeros), we exclude dead agents from the table. It can be seen that in three scenarios, GTDE, which uses local information, can achieve similar or better performance compared to models using individual or global information. The average information required for GTDE during training has been reduced by five times, making its training cost in real-world applications much lower than that of CTDE."}, {"title": "Ablations Study", "content": "We evaluated variants of GTDE to show the effect of the adaptive grouping module. Table 1 shows that GTDE outperforms both GTDE-U with random grouping and GTDE-F with fixed grouping, indicating that GTDE learns the correct grouping distribution. In the SMACv2 benchmark, GTDE using local information can achieve performance similar to GTDE-A with pairwise links. In the Battle scenario, all models of GTDE and GTDE-U/A/F underwent 200 rounds of validation and achieved a win rate of over 90%. Fig.7 shows the broken axis plot of the total reward in the Gather scenario. We can observe that GTDE received 18.2%, 11.6%, and 10.1% more total rewards than GTDE-U/A/F. Notably, the total rewards of GTDE-F and GTDEA are similar, indicating that using fixed grouping agents can achieve performance comparable to joint observations, which implies that there is information redundancy in joint observations."}, {"title": "6 Conclusion and Future Directions", "content": "This paper proposes a GTDE framework to address the limitations of CTDE and DTDE. As the number of agents increases, the performance of DTDE and CTDE significantly deteriorates, with CTDE becoming untrainable due to the high input dimensionality. GTDE is proposed as a solution to this problem. GTDE leverages only the local observations to update policy parameters, making it more suitable for multi-agent systems with a larger number of agents. However, it also faces a challenge in terms of scalability. Unlike CTDE, which uses joint observations and encounters scalability issues due to increasing input dimensions with the number of agents, GTDE avoids this problem but faces a challenge related to the non-scalability of adjacency matrices. It means that new agents can only be linked to existing agents during training and cannot be linked to each other. In the future, we will address or alleviate scalability and train using real-world applications instead of simulators."}, {"title": "A Gumbel-Sigmoid", "content": "For the grouping distribution, its adjacency matrix form is as follows:\ni\nj k\nagent i 0.5\n0.2\n0.3\nagent j 0.1 0.6 0.3\nagent k 0.9 0.05 0.05\n(1)\ni\nj k\n\u03930.3 0.7 0.8\n0.8 0.6 0.77\n0.1 0.6 0.45\n(2)\nEach row of the matrix represents a probability distribution derived from the individual observation histories of an agent, where each number in the row denotes the probability(pij) of the agent in the i-th row links to the agent in the j-th column. We need to sample each row of the matrix, where (1) is the adjacency matrix sampled using Gumbel-Softmax."}]}