{"title": "Visual Graph Question Answering with ASP and LLMs for Language Parsing", "authors": ["Jakob Johannes Bauer", "Thomas Eiter", "Nelson Higuera Ruiz", "Johannes Oetsch"], "abstract": "Visual Question Answering (VQA) is a challenging problem that requires to process multimodal input. Answer-Set Programming (ASP) has shown great potential in this regard to add interpretability and explainability to modular VQA architectures. In this work, we address the problem of how to integrate ASP with modules for vision and natural language processing to solve a new and demanding VQA variant that is concerned with images of graphs (not graphs in symbolic form). Images containing graph-based structures are an ubiquitous and popular form of visualisation. Here, we deal with the particular problem of graphs inspired by transit networks, and we introduce a novel dataset that amends an existing one by adding images of graphs that resemble metro lines. Our modular neuro-symbolic approach combines optical graph recognition for graph parsing, a pretrained optical character recognition neural network for parsing labels, Large Language Models (LLMs) for language processing, and ASP for reasoning. This method serves as a first baseline and achieves an overall average accuracy of 73% on the dataset. Our evaluation provides further evidence of the potential of modular neuro-symbolic systems, in particular with pretrained models that do not involve any further training and logic programming for reasoning, to solve complex VQA tasks.", "sections": [{"title": "1 Introduction", "content": "Visual Question Answering (VQA) [1] is concerned with inferring the correct answer to a natural language question in the presence of some visual input, such as an image or video, which typically involves processing multimodal input. VQA enables applications in, e.g., medicine, assistance for blind people, surveillance, and education [4].\nAnswer-Set Programming (ASP) [6] has shown great potential to add interpretability and explainability to modular VQA architectures in this context. As a knowledge representation and reasoning formalism with an intuitive modelling language, it can be used to describe how to infer answers from symbolic input provided by subordinate modules in a clear and transparent way [28, 5, 11, 10]. Another strength is that uncertainties from the underlying modules can be expressed using disjunctions (or choice rules), and we are not limited to inferring one answer, but several plausible ones in a nondeterministic manner [33]. Furthermore, using ASP in the VQA context is beneficial for explanation finding, as we have demonstrated in recent work [10].\nIn this work, we address the problem of how to integrate ASP with modules for vision and natural language processing to solve a new and demanding VQA variant that is concerned with images of graphs (not graphs in symbolic form). Visual representations of structures based on graphs are a popular and ubiquitous form of presenting information. It is almost surprising that VQA tasks where the visual input contains a graph have, to the best of our knowledge, not been considered so far."}, {"title": "2 Visual Question Answering on Graphs", "content": "Graph Question Answering (GQA) is the task of answering a natural language question for a given graph in symbolic form. The graph consists of nodes and edges, but further attributes may be specified in addition. A specific GQA dataset is CLEGR [22], which is concerned with graph structures that resemble transit networks like metro lines. Its questions are ones that are typically asked about transit like \u201cHow many stops are between X and Y?\u201d. The dataset is synthetic and comes with a generator for producing instances of varying complexity.\nGraphs come in the form of a YAML file containing records about attributes of the stations and lines. Each station has a name, a size, a type of architecture, a level of cleanliness, potentially disabled access, potentially rail access, and a type of music played. Stations can be described as relations over the aforementioned attributes. Edges connect stations but additionally have a colour, a line ID, and a line name. For lines, besides name and ID we have a construction year, a colour, and optional presence of air conditioning."}, {"title": "Example 1", "content": "Examples of questions from the dataset are:\n\u2022 Describe {Station} station's architectural style."}, {"title": "3 Our Neuro-Symbolic Framework for VQA on Graphs", "content": "Our solution to the VGQA task, which we call NSGRAPH, is a modular neuro-symbolic system, whose modules are the typical ones for VQA, viz. a visual module, a language module, and a reasoning module, which we realise to fit the VQGA setting. Figure 3 illustrates the data flow of the inference process in NSGRAPH."}, {"title": "3.1 Visual Module", "content": "The visual model is used for graph parsing, which consists of two subtasks: (i) detection of nodes and edges, and (ii) detection of labels, i.e., station names.\nWe employ an optical graph recognition (OGR) system for the first subtask. In particular, we use a publicly available OGR script [9] that implements the approach due to Auer et al. [2]. The script takes"}, {"title": "3.2 Language Module", "content": "The purpose of the language module is to parse the natural language question. It is written in Python and uses regular expressions to capture the variables in each type of question. There are in general 35 different question templates in CLEGR, some of which were shown in Example 1. They can be used to produce a question instances by replacing variables with names or attributes of stations, lines, or connections."}, {"title": "Example 2", "content": "For illustration, the question template \u201cHow many stations are on the shortest path between S1 and S2?\u201d may be instantiated by replacing S\u2081 and S2 with station names that appear in the graph. We use regular expressions to capture those variables and translate the natural language question into a functional program, essentially a tree of operations, for that question. Continuing our example, we translate the template described above into the program\nend(3). countNodesBetween(2). shortestPath(1).\nstation(0,S1). station(0,S2).\nwhere the the first numerical argument of each predicate imposes the order of execution of the associated operation and links the input of one operation to the output of the previous one. We can interpret this functional program as follows: the input to the shortest-path operation is two station names S1 and S2. Its outputs are the stations on the shortest path between S1 and S2 which are counted in the next step. The predicate end represents the end of the computation to yield this number as the answer to the question.\nAll considered question types and their ASP question encodings are summarised in Table 1. Although this approach works well for all the questions in CLEGR, its ability to generalise to new types of questions is obviously limited; as a remedy, we discuss LLMs as an alternative to realise the language module in Section 4."}, {"title": "3.3 Reasoning Module", "content": "The third module consists of an ASP program that implements the semantics of the operations from the functional program of the question. Before we explain this reasoning component, we briefly review the basics of ASP."}, {"title": "Answer-Set Programming", "content": "ASP [6, 14] is a declarative logic-based approach to combinatorial search and optimisation with roots in knowledge representation and reasoning. It offers a simple modelling language and efficient solvers\u00b9. In ASP, the search space and properties of problem solutions are described by means of a logic program such that its models, called answer sets, encode the problem solutions.\nAn ASP program is a set of rules of the form $a_1|\\ldots|a_m :- b_1,..., b_n, not c_1,..., not c_n$, where all $a_i, b_j, c_k$ are first-order literals and not is default negation. The set of atoms left of :- is the head of the rule, while the atoms to the right form the body. Intuitively, whenever all bj are true and there is no evidence for any c\u2081, then at least some a\u00a1 must be true. The semantics of an ASP programs is given by its answer sets, which are consistent sets of variable-free (ground) literals that satisfy all rules and fulfil a minimality condition [15].\nA rule with an empty body and a single head atom without variables is a fact and is always true. A rule with an empty head is a constraint and is used to exclude models that satisfy the body.\nASP provides further language constructs like choice rules, aggregates, and weak (also called soft) constraints, whose violation should only be avoided. For a comprehensive coverage of the ASP language and its semantics, we refer to the language standard [8]."}, {"title": "Question Encoding", "content": "The symbolic representations obtained from the language and visual modules are first translated into ASP facts; we refer to them as GASP and QASP in Fig.3, respectively. The functional program from a question (as introduced above) is already in a fact format. The graph is translated into binary atoms edge/2 and unary atoms station/1 as well. These facts combined with an ASP program that encodes the semantics of all CLEGR question templates can be used to compute the answer with an ASP solver."}, {"title": "Example 3", "content": "Here is an excerpt of the ASP program that represents the functional program from above:\nend (3). countNodesBetween(2). shortestPath(1).\nstation(0,s). station(0,t).\nThese facts, together with ones for edges and nodes, serve as input to the ASP encoding for computing the answer as they only appear in rule bodies:\nsp(T,S1,S2) :- shortestPath(T), station(T-1,S1),\n station(T-1,S2), S1<S2'.\n{ in_path(T,S1,S2) } :- edge(S1,S2), shortestPath(T).\nreach(T,S1,S2) :- in_path(T,S1,S2).\nreach(T,S1,S3) :- reach(T,S1,S2), reach(T, S2,S3).\n:- sp(T,S1,S2), not reach (T, S1,S2).\ncost(T,C) :- C = #count {S1,S2: in_path(T,S1,S2)}, shortestPath(T).\n:~ cost(T,C). [C,T]\ncountedNodes(T, C-1) :- countNodesBetween (T),\n shortest Path(T-1), cost(T-1,C).\nans(N) :- end(T), countedNodes (T,N).\nThe first rule expresses that if we see shortestPath(T) in the input, then we have to compute the shortest path between station S1 and S2. This path is produced by the next rule which non-deterministically decides for every edge if this edge is part of the path. The following two rules jointly define the transitive closure of this path relation, and the constraint afterwards enforces that station S1 is reachable from S2 on that path. We use a weak constraint to minimise the number of edges that are selected and thus enforce that we indeed get a shortest path. The number of edges is calculated using an aggregate expression to count. Finally, the penultimate rule calculates the number of stations on the shortest path, as it takes as input the nodes that came out of the shortest path from the previous step and counts them, and the last rule defines the answer to the question as that number. The complete encoding is part of the online repository of this project (https://github.com/pudumagico/NSGRAPH)."}, {"title": "3.4 Evaluation of NSGRAPH on CLEGR", "content": "NSGRAPH achieves 100% on the original GQA task, i.e., with graphs in symbolic form as input and with the complete set of questions. Here, the symbolic input is translated directly into ASP facts without the need to parse an image.\nWe summarise the results for the more challenging VGQA task on CLEGRV in Table 2.2 The task becomes more difficult with increasing size of the graphs, but still an overall accuracy of 73% is achieved. As we also consider settings where we replace the OCR, resp. the OGR module, with the ground truth"}, {"title": "4 Semantic Parsing with LLMs", "content": "LLMs like GPT-4 [24] are deep neural networks based on the transformer architecture [31] with billions of parameters that are trained on a vast amount of data to learn to predict the next token for a given text prompt. (A token is a sequences of textual characters like words or parts of words). Their capabilities for natural language processing are impressive. LLMs are typically instructed via text prompts to perform a certain task such as answering a question or translating a text, but they can also be used for semantic parsing a text into a formal representation suitable for further processing.\nIn this section, we outline and evaluate an approach to use LLMs to realise the language module of NSGRAPH in a more robust way than by using regular expressions. First, we outline the general method of prompting LLMs to extract ASP predicates from questions. Afterwards, we evaluated this method for different LLMs, including state-of-the-art API-based ones but also open-source models that are free and can be locally installed."}, {"title": "4.1 Prompt Engineering", "content": "A particularly useful feature of LLMs is that the user can instruct them for a task by providing a few examples as part of the input prompt without the need to retrain the model on task-specific data; a property of LLMs commonly referred to as in-context learning.\nOur approach uses in-context learning to instruct the LLM to extract the ASP atoms needed to solve the reasoning task from a question. This idea is inspired by recent work on LLMs for language understanding [26]. To obtain an answer to a question Q, we\n(i) create a prompt P(Q) that contains the question Q along with additional instructions and examples for ASP question encodings,\n(ii) pass P(Q) as input to an LLM and extract the ASP question encoding from the answer, and\n(iii) use extracted ASP facts together with the ASP rules described in the previous section to derive the answer.\nThe prompt P(Q) starts with a general pre-prompt that sets the stage for the task:"}, {"title": "Example 4", "content": "For space reasons, we show here just the beginning of an example prompt:\nI now provide you with some examples on how to parse Questions:\nQ: ''How many stations are between Inzersdorf and Mainstation?'',\nA: end(3).countNodesBetween(2).shortestPath(1).\nstation(0,\u2018Inzersdorf').station(0, \u2018Mainstation'\u201d).\nQ: ''What is the amount of stations between Station A and Station B?''\nA: end(3).countNodesBetween(2).shortestPath(1).\nstation(0, ''Station A'').station(0, ''Station B'').\nFinally, the prompt contains the questions that should be answered:\nNow provide the output for the following question:\nWhat are the stations that lie on line 7?"}, {"title": "4.2 Evaluation", "content": "We evaluated the method from the previous section to answer to following research questions:\n(R1) Is the method suitable for realising the language component of NSGRAPH?\n(R2) What is the trade-off between grand scale LLMs and smaller, more cost-efficient alternatives?\n(R3) How well does the method generalise to questions formulated in a different way than in CLEGR?"}, {"title": "Overview of used LLMs", "content": "We compared different models (GPT-4, GPT3.5, Bard, GPT4All, Vicuna 13b, and Zephyr 7b; cf. Table 3)3 on the semantic parsing task."}, {"title": "Results and Discussion", "content": "The results of our evaluation are summarised in Table 4 for CLEGR+ and in Table 5 for CLEGR-Human. We classified the answers produced by the LLMs into four categories:\n\u2022 full match: the response matches exactly the set of expected atoms;\n\u2022 contains solution: the expected atoms can be extracted from the respone;\n\u2022 task missed: the response contains some text but not the expected atoms;\n\u2022 no answer: the response consists of only whitespace characters.\nNote that \"full match\u201d as well as \u201ccontains solution\u201d can be used for the ASP reasoning task, while answers from the other categories cannot be used.\nGPT-4 performed best among the considered LLMs as it produced 85% completely correct responses on CLEGR+ and even 94% on CLEGR-Human. It also always provided an answer to the prompt. GPT-3.5 invented new predicates for half of the questions. For the remaining ones, its response matched exactly or contained the solution. Vicuna often did not give a proper response due to context overflow and trails behind GPT-4 and GPT-3.5 also in terms of correct answers. Google Bard never got the exact solution due to extensive additional explanations for all predicates no matter the prompt. However, the responses contained the solution in about three quarters of the cases. In this regard, it is only outmatched by GPT-4. This performance is similar to that of the much smaller open-source model Zephyr 7b, which is trailing only slightly behind. The responses of GPT4All contain the correct solution for only 23% (CLEGR+) and 16% (CLEGR-Human) of the questions.\nWe answer our initial research questions therefore as follows: At least GPT-4 is suitable for realising the language component with an acceptable trade-off between accuracy and ability to generalise (R1). Although GPT-4 exhibits the best overall performance, especially the free and much smaller Zephyr model shows promising results (R2). Throughout, the LLMs perform similarly on CLEGR+ and CLEGR-Human, which showcases the strength of LLMs for language processing without the need for context-specific training (R3)."}, {"title": "5 Related Work", "content": "Our approach builds on previous work [11], where we introduced a neuro-symbolic method for VQA in the context of the CLEVR dataset [20] using a reasoning component based on ASP inspired by NSVQA [34]. The latter used a combination of RCNN [27] for object detection, an LSTM [16] for natural language parsing, and Python as a symbolic executor to infer the answer. The vision and language modules in these previous approaches were trained for the datasets. As compared to these datasets the number of questions obtained from the questionnaires to build our dataset is small, it would be hardly possible to effectively train an LSTM on them. It is a particular strength of our work that we resort to LLMs that do not require any further training."}, {"title": "6 Conclusion", "content": "We addressed the relevant the problem of integrating ASP with vision and language modules to solve a new VQA variant that is concerned with images of graphs. For this task, we introduced a respective dataset that is based on an existing one for graph question answering on transit networks, and we presented NSGRAPH, a modular neuro-symbolic model for VGQA that combines neural components for graph and question parsing and symbolic reasoning with ASP for question answering. We studied LLMs for the question parsing component to improve how well our method generalises to unseen questions. NSGRAPH has been evaluated on the VGQA dataset and therefore constitutes a first baseline for the novel dataset.\nThe advantages of a modular architecture in combination with logic programming are that the solution is transparent, interpretable, explainable, easier to debug, and components can be replaced with better ones over time in contrast to more monolithic end-to-end trained models. Our system notably relies on pretrained components and thus requires no additional training. With the advent of large pretrained models for language and images such as GPT-4 [24] or CLIP [25], such architectures, where symbolic systems are used to control and connect neural ones, may be seen more frequently.\nFor future work, we plan to look into better alternatives for the visual module that is more suitable for complicated images of graphs, which is currently the limiting factor. Another future direction is to work with real-world metro networks for which currently no VQA datasets exist."}]}