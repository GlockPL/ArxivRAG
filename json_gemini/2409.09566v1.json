{"title": "Learning Transferable Features for Implicit Neural Representations", "authors": ["Kushal Vyas", "Ahmed Imtiaz Humayun", "Aniket Dashpute", "Richard G. Baraniuk", "Ashok Veeraraghavan", "Guha Balakrishnan"], "abstract": "Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering. An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal. Assumed to be less generalizable, we explore the aspect of transferability of such learned neural features for fitting similar signals. We introduce a new INR training framework, STRAINER that learns transferrable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality. Owing to the sequential layer-wise affine operations in an INR, we propose to learn transferable representations by sharing initial encoder layers across multiple INRs with independent decoder layers. At test time, the learned encoder representations are transferred as initialization for an otherwise randomly initialized INR. We find STRAINER to yield extremely powerful initialization for fitting images from the same domain and allow for a \u2248 +10dB gain in signal quality early on compared to an untrained INR itself. STRAINER also provides a simple way to encode data-driven priors in INRs. We evaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks and inverse problems and further provide detailed analysis and discussion on the transferability of STRAINER's features. Our demo can be accessed here.", "sections": [{"title": "1 Introduction", "content": "Implicit neural representations (INRs) are a powerful family of continuous learned function approximators for signal data that are implemented using multilayer perceptron (MLP) deep neural networks. An INR fo : Rm \u2192 Rn maps coordinates lying in a m-dimensional space to a value in a n-dimensional output space, where 0 represents the MLP's tunable parameters. For example, a typical INR for a natural image would use an input space in R2 (consisting of the x and y pixel coordinates), and an output space in R\u00b3 (representing the RGB value of a pixel). INRs have demonstrated several useful properties including capturing details at all spatial frequencies [39, 36], providing powerful priors for natural signals [36, 39], and facilitating compression [12, 27]. For these reasons, in the past 5 years, INRs have found important uses in image and signal processing including shape representation [17, 16], novel view synthesis [31, 34, 42], material rendering [24], computational imaging [5, 30], medical imaging [49], linear inverse problems [8, 44], virtual reality [11] and compression [12, 27, 43, 51]."}, {"title": "2 Background", "content": "Implicit neural representations. We define fo(p) as an implicit neural representation (INR) [31, 39, 29] where fe is a multi-layer perceptron (MLP) with randomly initialized weights @ and p is the m-dimensional coordinates for the signal. Each layer in the MLP is an affine operation followed"}, {"title": "3 Methods", "content": "We introduce STRAINER. We first explain our motivation to share initial layers in an INR Section 3.1. In Section 3.2 we describe the training phase of STRAINER where we learn transferrable features for INRs by sharing the initial layers of N INRs being fit independently to N images. Section 3.3, details how our captured basis is used to fit an unseen image. In subsequent sections, we seek to understand what our shared basis captures and how to expand it to other problems such as super resolution. For simplicity, we build upon the SIREN [39] model as our base model."}, {"title": "3.1 Why share the initial INR layers?", "content": "A recent method called SplineCAM [19] provides a lens with which to visualize neural network partition geometries. SplineCAM interprets an INR as a function that progressively warps the input space and fits a given signal through layerwise affine transforms and non-linearities [19]. For continuous piecewise affine activation functions, we use an approximation to visualize (see Figure 6) the deep network's partition geometry for different pre-activation level sets [20].\nAn INR fit to a signal highly adapts to the underlying structure of the data in a layer-wise fashion. Furthermore, by approximating the spatial position of the pre-activation zero level sets, we see that initial layers showcase a coarse, less-partitioned structure while deeper layers induce dense partitioning collocated with sharp changes in the image. Since natural signals tend to be similar in their lower frequencies, we hypothesize that initial layers of multiple INRs are better suited for transferability. We therefore design STRAINER to share the initial encoder layers, effectively giving rise to an input space partition that can generalize well across different similar signals."}, {"title": "3.2 Learning transferable features from N images", "content": "Consider a SIREN [39] model h(p) with L layers. Let K out of L layers correspond to an encoder sub-network represented as fe The remaining layers correspond to the decoder sub-network represented as go as seen in Figure 1(a). For given input coordinates p, we express the SIREN model h\u00f8,0(p) as a composition (0) of our encoder-decoder sub-networks.\nh\u00a2,0(p) = 9\u00a2 \u00b0 fe(p), (2)\nIn a typical case, given the task of fitting N signals, each of the N signals is independently fit to an INR, thus not leveraging any similarity across these images. Since we want to learn a shared representation transferrable across multiple similar images, our method shares the encoder fe across all N INRs while maintaining a set of individual signal-specific decoders g ....Our overall architecture is shown in Figure 1. We call this STRAINER's training phase - Figure 1(a). We start with randomly initialized layers and optimize the weights to fit N signals in parallel. For each signal I\u3047(p), we use a L2 loss between I\u00bf(p) and its corresponding learned estimate hp,e(p) and sum the loss over all the N signals. Iteratively, we learn a set of weights that minimizes the following objective:\nwhere = [0, $1 ... $N] represents the full set of weights of the shared encoder (8) and the N different decoders (g... g) and * represents the resulting optimized weights."}, {"title": "3.3 Fitting an unseen signal with STRAINER", "content": "After sufficient iterations during STRAINER's training phase, we get optimized encoder weights fo* which corresponds to the rich shared representation learned over signals of the same category. To fit a"}, {"title": "3.4 Learning an intermediate partition space in the shared encoder fo*", "content": "During the training phase, explicitly sharing layers in STRAINER allows us to learn a set of INR features which exhibits a common partition space shared across multiple images. Since deep networks perform layer wise subdivision of the input space, sharing the encoder enforces the layer to find the partition that can be further subdivided into multiple coarse partitions corresponding to the tasks being trained. In Figure 6(a.ii), while pre-training an INR using the STRAINER framework on CelebA-HQ dataset, we see emergence of a face-like structure captured in our STRAINER encoder fo*. We expect our STRAINER encoder weights fo* to be used as transferrable features and be used as initialization for fitting unseen in-domain samples.\nIn comparison, meta learning methods to learn initialization for INRs[45] exhibit a partitioning of the input space that is closer to random. As seen in Figure 6(a.i) there is a faint image structure captured by the the learned initialization. This is an indication that the initial subdivision of the input space, found by the meta learned pre-training layers, captures less of the domain specific information therefore is a worse initialization compared to STRAINER. We further explain our findings in Section 5 and discuss of STRAINER's learned features being more transferrable and lead to better quality reconstruction."}, {"title": "4 Experiments", "content": "In all experiments, we used the SIREN [39] MLP with 6 layers and sinusoid nonlinearities. We considered two versions of STRAINER : (i) STRAINER (1 decoder), where the encoder layers are initialized using our shared encoder trained on a single image, and (ii) STRAINER-10 (10 Decoders), where the encoder layers are initialized using our shared encoder trained on 10 images. We considered the following baselines: (i) a vanilla SIREN model with random uniform initialization [39], (ii) a fine-tuned SIREN model initialized using the weights from another SIREN fit to an image from the same domain, (iii) a SIREN model initialized using Meta-learned 5K [45], (iv) transformer-based metalearning models such as TransINR[9] and IPC[23]. We ensured an equal number of learnable"}, {"title": "4.1 Datasets", "content": "We mainly used the CelebA-HQ [22], Animal Faces-HQ (AFHQ) [10], and OASIS-MRI [18, 28] images for our experiments. We randomly divided CelebA-HQ into 10 train images and 550 test images. For AFHQ, we used only the cat data, and used ten images for training and 368 images for testing. For OASIS-MRI, we used 10 of the (template-aligned) 2D raw MRI slices for training, and 144 for testing. We also used Stanford Cars[2] and Flowers[1] to further validate out of domain generalization and Kodak [3] true images for demonstrating high-resolution image fitting."}, {"title": "4.2 Training STRAINER'S shared encoder", "content": "We first trained separate shared encoder layers of STRAINER on 10 train images from each dataset. We share five layers, and train a separate decoder for each training image. For each dataset, we trained the shared encoder for 5000 iterations until the model acheives PSNR \u2248 30dB for all training images. We use the resulting encoder parameters as initialization for test signals in the following experiments. For comparison, we also trained the Meta-learned 5K baseline using the implementation provided by Tancik et.al.[45] with 5000 outer loop iterations. We also use the implementation provided by IPC[23] as our baselines for TransINR[9] and IPC[23] and train them with 14000 images from CelebA-HQ. We report a comparison of number of training images and parameters, gradient updates and learning time in Table 5."}, {"title": "4.3 Image fitting (in-domain)", "content": "We first evaluated STRAINER on the task of in-domain image fitting. We cropped and resized all images to 178 \u00d7 178 and ran test-time optimization on all models for 2000 steps.\nAt test-time, both STRAINER and STRAINER-10 use only 1 decoder, resulting in the same number of parameters as a SIREN INR. Table 1 shows average image metrics for in-domain image fitting reported with 1 std. deviation. Instead of naively fine tuning using another INR, STRAINER's design of sharing initial layers allows for learning highly effective features which transfer well across images in the same domain, resulting in high quality reconstruction across CelebA-HQ and AFHQ and comparable to Meta-learned 5K for OASIS-MRI images. Table 3(CelebA-HQ, ID) also shows that STRAINER initialization results in better quality reconstruction, when optimized at test-time, compared to more recent transformer-based INR approaches such as TransINR and IPC as well."}, {"title": "4.4 Image fitting (out-of-domain)", "content": "To test out-of-domain transferability of learned STRAINER features, we used STRAINER-10 's encoder trained on CelebA-HQ as initialization for fitting images from AFHQ (cats) and OASIS-MRI datasets (see Table 2). Since OASIS-MRI are single channel images, we trained Meta-learned 5K and STRAINER-10 (GRAY) on the green channel only of CelebA-HQ images. To our surprise, we see STRAINER-10 and STRAINER-10 (GRAY) clearly outperform not only Meta-learned 5K, but also STRAINER-10 (in-domain). To further validate out of domain performance of STRAINER, we train"}, {"title": "4.5 Inverse problems: super-resolution and denoising", "content": "STRAINER provides a simple way to encode data-driven priors, which can accelerate convergence on inverse problems such as super-resolution and denoising. We sampled 100 images from CelebA-HQ at 178 \u00d7 178 and added 2dB of Poisson random noise. We report mean values of PSNR achieved by STRAINER and SIREN models along with the iterations required to achieve the values. For super-resolution, we demonstrate results on one image from DIV2K[4, 47], downscaled to 256 \u00d7 256 for a low resolution input. We used the formulation shown in Equation (1), with A set to a 4\u00d7 downsampling operation. To embed a prior relevant for clean images, we trained the shared encoder of STRAINER with high quality images of resolution same as the latent recovered image. At test time, we fit the STRAINER model to the corrupted image, following Equation (1) and recovered the latent image during the iteration. We report STRAINER's ability to recover latent images fast as well as with high quality in Section 4.5"}, {"title": "5 Discussion and Conclusion", "content": "Results in Table 1, 3 demonstrate that STRAINER can learn a set of transferable features across an image distribution to precisely fit unseen signals at test time. STRAINER-10 clearly achieves the best reconstruction quality in terms of PSNR and SSIM on CelebA-HQ and AFHQ datasets, and is comparable with Meta-learned 5K on OASIS-MRI images. STRAINER-10 also fits images fast and achieves highest reconstruction quality than all baselines as shown in Figure 2. Comparing STRAINER (1 decoder) with a fine-tuned SIREN, it seems that the representation learned on one image is not sufficiently powerful. However, as little as 10 images result in a rich and transferable set of INR features allowing STRAINER-10 to achieve \u22487-10dB higher reconstruction quality than SIREN and SIREN fine-tuned.\nAs seen in Table 2, 3(OOD) STRAINER also performs well on out-of-domain tasks, which is quite surprising.\nSTRAINER's transferable representations are capable of recovering small and delicate structures as early as 100 iterations as shown in Figure 5 and do not let the scale of features from the training phase affect its reconstruction ability. Another interesting finding is that STRAINER-10 achieves far better generalization for OASIS-MRI (Table 2) when pretrained on CelebA-HQ. Further, STRAINER generalizes well to out-of-domain high-resolution images, as demonstrated by our experiments of training STRAINER on CelebA-HQ and testing on the Kodak data (see Table 7).\nSTRAINER is fast and cheap to run. Table 5 summarizes the time for learning the initialization for a 6 layered MLP INR for STRAINER, Meta-learned 5K and transformer-based methods such as TransINR and IPC. At 5000 iterations, STRAINER learns a transferable representation in just 24.54 seconds. Meta-learned 5K, in comparison, uses MAML[15] which is far more computationally intensive and results in 20\u00d7 slower runtime when exact number of gradient updates are matched. Further, STRAINER 's training setup is an elegant deviation from recent methods such as TransINR and IPC, requiring large datasets and complex training routines."}, {"title": "5.1 Limitations", "content": "Due to the encoder layers of STRAINER being tuned on data and the later layers being randomly initialized, we have observed occasional instability when fitting to a test signal in the form of PSNR \"drops.\" However, we observe that STRAINER usually quickly recovers, and the speedup provided by STRAINER outweighs this issue. While our work demonstrates that INR parameters may be transferred across signals, it is not fully clear what features are being transferred, how they change for different image distributions, and how they compare to the transfer learning of CNNs and Transformers. Further work is needed to characterize these."}, {"title": "5.2 Further analysis of STRAINER", "content": "To further understand how STRAINER's initialized encoder enables fast learning of signals at test time, we explored the evolution of STRAINER's hidden features over iterations in Figure 3. In Figure 3(a), we visualize the first principal component of learned INR features of the STRAINER encoder and corresponding hidden layer for SIREN across iterations and observe that STRAINER captures high frequencies faster than SIREN. This is corroborated by the power spectrum inset plots of the reconstructed images. We also visualize a histogram of gradient updates in Figure 4, and observe that STRAINER receives large gradients in its encoder layers early on during training, suggesting that the encoder rapidly learns of low-frequency details.\nNext, we visualize the input space partitions induced by STRAINER and the adaptability of STRAINER's initialization for fitting new signals. We use the local complexity(LC) measure proposed by Humayun et.al.[20] to approximate different pre-activation level sets of the INR neurons. For ReLU networks, the zero level sets correspond to the spatial location of the non-linearities of the network. For periodic activations, there can be multiple non-linearities affecting the input domain. In Figure 6 we present the zero level sets of the network, and in Supplementary we provide the \u00b1\u03c0/2 shifted level sets. Darker regions in the figure indicate high LC, i.e., higher local non-linearity. Figure 6 also presents partitions for the baseline models.\nSIREN models tend to overfit, with partitions strongly adapting to image details. Since the sensitivity to higher frequencies is mapped to specific input partitions, when finetuning with SIREN, the network has to unlearn partitions of the pretrained image resulting in sub optimal reconstruction quality. When comparing Meta-learned 5K with STRAINER, we see that STRAINER learns an input space partitioning more attuned to the prior of the dataset, compared to Meta-learned 5K which is comparatively more random. While both partitions imply learning high-frequency details, STRAINER'S partitions are better adapted to facial geometry, justifying its better in-domain performance."}, {"title": "6 Broader Impacts", "content": "STRAINER introduces how to learn transferable features for INRs resulting in faster convergence and higher reconstruction quality. We show with little data, we can learn powerful features as initialization for INRs to fit signals at test-time. Our method allows the use of INRs to become ubiquitous in data-hungry areas such as patient specific medical imaging, personalized speech and video recordings, as well as real-time domains such as video streaming and robotics. However, our method is for training INRs to represent signals in general, which can adopted regardless of underlying positive or negative intent."}, {"title": "Suplementary Material / Appendix", "content": "In our work, we increase the representation capacity of the INR by leveraging the similarity across natural images (of a given class). Since each layer of an INR MLP is an affine transformation followed by a non linearity, we interpret the INR as a function that progressively warps the input coordinate space to fit the given signal, in our case the signal being an image. Similar images when independently fit to their respective INRs capture similar low-frequency detail such as shape, geometry, etc. whereas high-frequency information such as edges and texture are unique to each INR. We propose that these low-frequency features from the initial layers of a learned INR are highly transferable and can be used as a basis and initialization while fitting an unseen signal. To that end, we introduce a novel method of learning our basis by sharing a set of initial layers across INRs fitting their respective images.\nOur implementation can be found on Google Colab.\nUnderstanding the effect of sharing encoder layers\nWe further investigate how the number of initial layers shared affects the quality of reconstructed image. We start by sharing K = 1 layer as the encoder, and N \u2013 K layers in each decoder and vary K from 1 to N \u2013 1. K = N is equivalent to simply fine tuning the INR based on all weights from a fellow model. We tabulate our results for image quality (PSNR) in a fixed runtime of 1000 iterations. We find that sharing all but the last layer results in the most effective capturing of our shared basis leading to higher reconstruction quality as seen in fig. 7. This also suggests that the last decoder of the INR is mainly responsible for very localized features. Further our work motivates further interest to sutdy the nature of the decoder layers itself.\nWe show the effect of sharing layers and resulting reconstruction quality. We use a 5 layered Siren model for this experiment. We fit a vanilla Siren model to an image and report its PSNR in fig. 7. Further, we train our shared encoder by sharing K = 1 layers and so on, until we share K = N \u2212 1 layers.\nWe see that the reconstruction quality progressively increases by sharing layers."}, {"title": "Effect of orientation of the input image", "content": "We wanted to further assess whether the INR is overfitting to one particular aligned face arrangement. To further test this, we take a test image and apply various augmentations such as flip, rotate, and roll\nhttps://colab.research.google.com/drive/1fBZAwqE8C_lrRPAe-hQZJTWrMJuAKtG2?usp=sharing"}, {"title": "Measuring time for pretraining STRAINER and Meta-learned 5K", "content": "Our implementation is written in PyTorch[33] whereas Meta-learned 5K implemented by Tancik et.al[45] is a JAX implementation. For measuring runtime, we use the python's time package and very conservatively time the step where the forward pass and gradient updates occur in both methods. Further, we run the code on an Nvidia A100 GPU and report the time after averaging 3 such runs for each method. There may be system level differences, however, to the best of our knowledge and observation, our timing estimates if not accurate are atleast indicative of the speedup provided by STRAINER."}, {"title": "Training details for Kodak high resolution images", "content": "To further demonstrate that STRAINER's can be adapted to high resolution images, we evaluated our method on high quality Kodak[3] images with resolution 512 \u00d7 768 (see Tables 7 and 8). We present the reconstruction quality attained by STRAINER -10 and SIREN models with widths of 256, 512. For this experiment, we train our STRAINER encoder using CelebA-HQ Images which are resized to the same resolution to Kodak images. Further, we follow all steps as previously described for test-image evaluation of Kodak images. Here is another results from the Kodak high resolution images experiment."}, {"title": "Results for Inverse problems - Super Resolution", "content": "We discuss how STRAINER provides a useful prior for inverse problems such as super resolution. For the results reported in section 4.5, we attach supplementary plots as shown in fig. 9. STRAINER-10 (Fast) is a STRAINER-10 model with 5 shared encoder layers out of 6 total layers. STRAINER-10 (HQ) is a high quality STRAINER model with 3 shared encoder layers. Unlike forward fitting, more degree of randomness in the decoder helps recover better detail for inverse problems. We also showcase the effectiveness of STRAINER for in domain super resolution shown in fig. 10."}, {"title": "STRAINER for Occupancy fitting", "content": "STRAINER is a general purpose transfer learning framework which can be used to initialize INRs for regressing 3D data like occupancy maps, radiance fields or video. To demonstrate the effectiveness of STRAINER on 3D data, we have performed the following OOD generalization experiment. We pre-train STRAINER on 10 randomly selected 'Chair' objects from the ShapeNet[7] dataset. At test time, we fit the 'Thai Statue' 3D object[35]. STRAINER achieves a 12.3 relative improvement in IOU compared to random initialization for a SIREN architecture \u2013 in 150 iterations STRAINER-10 obtains an IOU of 0.91 compared to an IOU of 0.81 without STRAINER-10 initialization. We present visualizations of the reconstructed Thai Statue in Figure 11. Upon qualitative evaluation, we see that STRAINER-10 is able to capture ridges and edges better and faster than compared to SIREN."}], "equations": ["\n\n0* = arg min \u2211 || Afo(pi) \u2013 I(pi) ||\u00bd,\n\u03b8\n(1)", "h\u00a2,0(p) = 9\u00a2 \u00b0 fe(p), (2)", "\n\u0398* = arg min \u2211 || 9\u00b0 fo(p) \u2013 I\u00bf(p) ||\u00bd,\n\u0398\ni=1\n(3)", "\n\u03c8\n\u03c8\n$*, 0* = arg min || 9 \u00a9 fe=o* (p) - I\u2084(p) ||2 .\n\u03c6,0\n(4)"]}