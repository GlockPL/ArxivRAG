{"title": "WEAK-TO-STRONG BACKDOOR ATTACKS FOR LLMS\nWITH CONTRASTIVE KNOWLEDGE DISTILLATION", "authors": ["Shuai Zhao", "Leilei Gan", "Zhongliang Guo", "Xiaobao Wu", "Luwei Xiao", "Xiaoyu Xu", "Cong-Duy Nguyen", "Luu Anh Tuan"], "abstract": "Despite being widely applied due to their exceptional capabilities, Large Language\nModels (LLMs) have been proven to be vulnerable to backdoor attacks. These\nattacks introduce targeted vulnerabilities into LLMs by poisoning training samples\nand full-parameter fine-tuning. However, this kind of backdoor attack is limited\nsince they require significant computational resources, especially as the size of\nLLMs increases. Besides, parameter-efficient fine-tuning (PEFT) offers an alter-\nnative but the restricted parameter updating may impede the alignment of triggers\nwith target labels. In this study, we first verify that backdoor attacks with PEFT may\nencounter challenges in achieving feasible performance. To address these issues\nand improve the effectiveness of backdoor attacks with PEFT, we propose a novel\nbackdoor attack algorithm from weak to strong based on contrastive knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher model\nthen covertly transfers the backdoor to the large-scale student model through con-\ntrastive knowledge distillation, which employs PEFT. Theoretical analysis reveals\nthat W2SAttack has the potential to augment the effectiveness of backdoor attacks.\nWe demonstrate the superior performance of W2SAttack on classification tasks\nacross four language models, four backdoor attack algorithms, and two different\narchitectures of teacher models. Experimental results indicate success rates close\nto 100% for backdoor attacks targeting PEFT.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) such as LLaMA (Touvron et al., 2023a;b; AI@Meta, 2024), GPT-\n4 (Achiam et al., 2023), Vicuna (Zheng et al., 2024), and Mistral (Jiang et al., 2024) have demonstrated\nthe capability to achieve state-of-the-art performance across multiple natural language processing\n(NLP) applications (Xiao et al., 2023; Wu et al., 2023; Burns et al., 2023; Xiao et al., 2024; Wu\net al., 2024; Zhao et al., 2024e). Although LLMs achieve great success, they are criticized for the\nsusceptibility to jailbreak (Xie et al., 2023; Chu et al., 2024), adversarial (Zhao et al., 2022; Guo et al.,\n2024a;c;b), and backdoor attacks (Gan et al., 2022; Long et al., 2024; Zhao et al., 2024a). Recent\nresearch indicates that backdoor attacks can be readily executed against LLMs (Chen et al., 2023;\n2024; Lyu et al., 2024). As LLMs become more widely implemented, studying backdoor attacks is\ncrucial to ensuring model security.\nBackdoor attacks aim to implant backdoors into LLMs through fine-tuning (Xiang et al., 2023; Zhao\net al., 2023). Specifically, attackers embed predefined triggers into training samples and associate\nthem with a target label, inducing the victim language model to internalize the alignment between\nthe malicious trigger and the target label while maintaining normal performance. If the trigger is\nencountered during the testing phase, the victim model will consistently output the target label (Dai\net al., 2019; Liang et al., 2024a). Despite the success of backdoor attacks on compromised LLMs,\nthey do have drawbacks which hinder their deployment: Traditional backdoor attacks necessitate the\nfine-tuning of language models to internalize trigger patterns (Zhao et al., 2023; 2024c). However"}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce the preliminaries of the backdoor attack, which include attack problem\nformulation, threat model, and objective.\nAttack Problem Formulation Backdoor attacks, as a specific type of attack method, typically involve\nthree stages. First, consider a standard text classification training dataset \\(D_{train} = \\{(x_i, y_i)\\}_{i=1}^n\\), which\ncan be accessed and manipulated by the attacker, where x represents the training samples and y is\nthe corresponding label. The dataset \\(D_{train}\\) is split two sets: a clean set \\(D_{clean} = \\{(x_i, y_i)\\}_{i=1}^m\\) and a"}, {"title": "3 EFFECTIVENESS OF CLEAN LABEL BACKDOOR ATTACKS TARGETING PEFT", "content": "In this section, we first validate the effectiveness of the clean label backdoor attack targeting the\nparameter-efficient fine-tuning (PEFT) algorithm through preliminary experiments. In addition, we\ntheoretically analyze the underlying reasons affecting the effectiveness of the backdoor attack.\nTo alleviate the computational resource shortage challenge, several PEFT algorithms for LLMs\nhave been introduced, such as LoRA (Hu et al., 2021). They update only a small subset of model\nparameters and can effectively and efficiently adapt LLMs to various domains and downstream tasks.\nHowever, they encounter substantial challenges to backdoor attack executions, particularly clean\nlabel backdoor attacks. The reason is that PEFT only update a subset of the parameters rather than the\nfull set, so they may struggle to establish an explicit mapping between the trigger and the target label.\nHere we take LoRA\u00b9 as an example to explain this issue. LoRA updates the model weights by\nintroducing low-rank matrices \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times k}\\). Instead of modifying the original weight\nmatrix \\(W \\in \\mathbb{R}^{d \\times k}\\), LoRA only updates A and B as:\n\n\n\n\nSince the rank \\(r < min(d, k)\\), this significantly reduces the number of updated parameters than\nfull-parameter fine-tuning, bring about efficient fine-tuning. Despite the efficiency, they may be"}, {"title": "4 W2SATTACK TARGETS PARAMETER-EFFICIENT FINE-TUNING", "content": "In this section, we introduce a novel method, Contrastive Knowledge Distillation, to transfer\nbackdoor features from the small-scale poisoned teacher model to the large-scale target student model,\nwhich aims to enhance the effectiveness of backdoor attacks targeting PEFT.\nPrevious work indicates that the backdoor embedded in the teacher model can survive the knowledge\ndistillation process and thus be transferred to the secretly distilled student models, potentially\nfacilitating more sophisticated backdoor attacks (Ge et al., 2021; Wang et al., 2022; Chen et al., 2024).\nHowever, the distillation protocol generally requires full-parameter fine-tuning of the student model to\neffectively mimic the teacher model's behavior and assimilate its knowledge (Nguyen & Luu, 2022).\nAs the number of model parameters increases, full-parameter fine-tuning of LLMs requires substantial\ncomputational resources, which may become infeasible. Hence, a natural question arises: How can\nwe transfer backdoors to LLMs by knowledge distillation, while leveraging PEFT algorithms?\nTo mitigate the aforementioned issues and better facilitate the enhancement of clean label backdoor\nattacks through knowledge distillation targeting PEFT, we propose a novel algorithm that evolves\nfrom weak to strong clean label backdoor attacks (W2SAttack) based on contrastive knowledge\ndistillation for LLMs. The fundamental concept of the W2SAttack is that it leverages full-parameter"}, {"title": "4.1 TEACHER MODEL", "content": "In our study, we employ BERT\u00b2 (Kenton & Toutanova, 2019) to form the backbone of our poisoned\nteacher model. Unlike traditional knowledge distillation algorithms, we select a smaller network\nas the poisoned teacher model, which leverages the embedded backdoor to guide the large-scale\nstudent model in learning and enhancing its perception of backdoor behaviors. Therefore, the task\nof the teacher model \\(f_t\\) is to address the backdoor learning, where the attacker utilizes the poisoned\ndataset \\(D_{train}^*\\) to perform full-parameter fine-tuning of the model. To ensure consistency in the output\ndimensions during feature alignment between the teacher and student models, we add an additional\nlinear layer to the teacher model. This layer serves to adjust the dimensionality of the hidden states\noutput by the teacher model, aligning it with the output dimensions of the student model. Assuming\nthat the output hidden state dimension of teacher model is \\(h_t\\), and the desired output dimension of\nstudent model is \\(h_s\\), the additional linear layer g maps \\(h_t\\) to \\(h_s\\):\n\n\n\nwhere \\(W \\in \\mathbb{R}^{h_s \\times h_t}\\) represents the weight matrix of the linear layer, and \\(b \\in \\mathbb{R}^{h_s}\\) is bias. Finally, we\ntrain the teacher model by addressing the following optimization problem:\n\n\nwhere l represents the cross-entropy loss, used to measure the discrepancy between the predictions\nof the model \\(f_t(x)\\) and the label y; fpft stands for full-parameter fine-tuning, which is employed to\nmaximize the adaptation to and learning of the features of backdoor samples."}, {"title": "4.2 STUDENT MODEL", "content": "For the student model, we choose LLMs as the backbone (Zhang et al., 2022; Touvron et al., 2023a),\nwhich needs to be guided to learn more robust attack capabilities. Therefore, the student model\nshould achieve two objectives when launching backdoor attack, including achieving a feasible attack\nsuccess rate for Objective 1 and maintaining harmless accuracy for Objective 2. To achieve the\naforementioned objective, the model needs to be fine-tuned on poisoned data \\(D_{train}^*\\). However, fine-\ntuning LLMs requires substantial computational resources. To alleviate this limitation, the PEFT"}, {"title": "4.3 CONTRASTIVE KNOWLEDGE DISTILLATION", "content": "As previously discussed, backdoor attacks employing PEFT methods may face difficulties in aligning\ntriggers with target labels. To resolve this issue, knowledge distillation algorithms are utilized to\nstealthily transfer the backdoor from the predefined small-scale teacher model, as introduced in\nSubsection 4.1, to the large-scale student model. Therefore, the teacher model, which is intentionally\npoisoned, serves the purpose of transmitting the backdoor signal to the student model, thus enhancing\nthe success rate of the backdoor attack within the student model.\nBackdoor Knowledge Distillation First, in the process of backdoor knowledge distillation, cross-\nentropy loss (De Boer et al., 2005) is employed to facilitate the alignment of clean samples with\ntheir corresponding true labels, which achieves Objective 2, and concurrently, the alignment between\ntriggers and target labels. Although reliance solely on cross-entropy loss may not achieve a feasible\nattack success rate, it nonetheless contributes to the acquisition of backdoor features:\n\n\nwhere \\(\\theta_s\\) represents the parameters of the student model; training sample \\((x, y) \\in D_{train}^*\\). Furthermore,\ndistillation loss is employed to calculate the mean squared error (MSE) (Kim et al., 2021) between\nthe logits outputs from the student and teacher models. This calculation facilitates the emulation of\nthe teacher model's output by the student model, thereby enhancing the latter's ability to detect and\nreplicate backdoor behaviors:\n\n\nwhere \\(\\theta_t\\) represents the parameters of the teacher model; \\(F_t\\) and \\(F_s\\) respectively denote the logits\noutputs of the teacher model and the student model.\nBackdoor Feature Alignment To capture deep-seated backdoor features, we utilize contrastive loss\nto minimize the Euclidean distance (Li & Bilen, 2020) between the student and teacher models. This\napproach promotes the alignment of the student model closer to the teacher model in the feature\nspace, facilitating the backdoor features, specifically the triggers, align with the intended target labels:\n\n\n\n\nwhere \\(H_t\\) and \\(H_s\\), respectively denote the final hidden states of the teacher and student model.\nOverall Training Formally, we define the optimization objective for the student model as minimizing\nthe composite loss function, which combines cross-entropy loss, distillation loss, and contrastive loss:\n\nwhere the loss function l is:\n\nThis approach has the advantage of effectively promoting the student model's perception of the\nbackdoor. Although the student model only updates a small number of parameters, the poisoned\nteacher model can provide guidance biased towards the backdoor. This helps to keep the trigger\nfeatures aligned with the target labels, enhancing the effectiveness of the backdoor attack and\nachieving Objective 1."}, {"title": "5 EXPERIMENTS", "content": "In this section, we further analyze the effectiveness of the backdoor attack targeting the PEFT and\nthen report the experimental results of the W2SAttack algorithm. Please see Appendix B for more\nexperimental details."}, {"title": "5.1 BACKDOOR ATTACK RESULTS OF PARAMETER-EFFICIENT FINE-TUNING", "content": "First, we further validate our observation in\nSection 3 that, compared to full-parameter fine-\ntuning, clean label backdoor attacks targeting\nPEFT may struggle to align triggers with target\nlabels. As shown in Table 1, we observe that\nwhen targeting full-parameter fine-tuning, the\nattack success rate is nearly 100%. For example,\nin the InSent algorithm, the average attack suc-\ncess rate is 98.75%. However, when targeting\nPEFT algorithms, the attack success rate sig-\nnificantly decreases under the same poisoned\nsample conditions. For example, in the ProAt-\ntack algorithm, the average attack success rate\nis only 44.57%. Furthermore, we discover that\nattacks leveraging sentence-level and syntactic structures as triggers, which require fewer poisoned\nsamples, are more feasible compared to those using rare characters. The results mentioned above\nfully validate our conclusion that, due to PEFT algorithms updating only a small number of model\nparameters, it may be difficult to establish alignment between triggers and target labels."}, {"title": "5.2 BACKDOOR ATTACK RESULTS OF W2SATTACK", "content": "To verify the effectiveness of our W2SAttack, we conduct a series of experiments under different\nsettings. Tables 2 to 4 report the results, and we can draw the following conclusions:\nW2SAttack fulfills the Objective 1 with high attack effectiveness. We observe that backdoor\nattacks targeting PEFT commonly struggle to achieve viable performance, particularly with the\nBadNet algorithm. In contrast, models fine-tuned with our W2SAttack show a significant increase\nin ASR. For example, using BadNet results in an average ASR increase of 58.48% on the SST-2\ndataset, with similar significant improvements observed in other datasets. This achieves the Objective\n1. Additionally, we notice that models initially exhibit higher success rates with other backdoor attack\nalgorithms, such as SynAttack. Therefore, our W2SAttack achieves only a 11.08% increase."}, {"title": "5.3 GENERALIZATION AND ABLATION ANALYSIS", "content": "In this section, we analyze the effect of different numbers of poisoned samples and trigger lengths\non our W2SAttack. From Figure 5, we find that ASR surpasses 90% when the number of poisoned\nsamples exceeds 1000. In addition, ASR significantly increases when the trigger length is greater\nthan 2.\nW2SAttack algorithm target various\nparameter-efficient fine-tuning To further\nverify the generalizability of our W2SAttack,\nwe explore its attack performance using dif-\nferent PEFT algorithms, as shown in the Ta-\nble 5. Firstly, we find that different PEFT\nalgorithms, such as P-tuning, do not estab-\nlish an effective alignment between the pre-\ndefined trigger and the target label when poi-"}, {"title": "6 CONCLUSION", "content": "In this paper, we focus on the backdoor attacks targeting parameter-efficient fine-tuning (PEFT)\nalgorithms. We verify that such attacks struggle to establish alignment between the trigger and the\ntarget label. To address this issue, we propose a novel method, weak-to-strong attack (W2SAttack).\nOur W2SAttack leverages a new approach contrastive knowledge distillation, which transmits\nbackdoor features from the small-scale poisoned teacher model to the large-scale student model. This\nenables the student model to detect the backdoor, which significantly enhances the effectiveness of\nthe backdoor attack by allowing it to internalize the alignment between triggers and target labels. Our\nextensive experiments on text classification tasks with LLMs show that our W2SAttack substantially\nimproves the attack success rate in the PEFT setting. Therefore, we can achieve feasible backdoor\nattacks with minimal computational resource consumption."}, {"title": "A RELATED WORK", "content": "In this section, we introduce work related to this study, which includes backdoor attacks, parameter-\nefficient fine-tuning algorithms, and knowledge distillation."}, {"title": "A.1 BACKDOOR ATTACK", "content": "Backdoor attacks, originating in computer vision (Hu et al., 2022), are designed to embed backdoors\ninto language models by inserting inconspicuous triggers, such as rare characters (Gu et al., 2017),\nphrases (Chen & Dai, 2021), or sentences (Dai et al., 2019), into the training data Chen et al. (2021);\nZhou et al. (2023). Backdoor attacks can be categorized into poison label backdoor attacks and clean\nlabel backdoor attacks (Qi et al., 2021b; Zhao et al., 2024c). The former requires modifying both the\nsamples and their corresponding labels, while the latter only requires modifying the samples while\nensuring the correctness of their labels, which makes it more covert (Li et al., 2024b).\nFor the poison label backdoor attack, Li et al. (2021a) introduce an advanced composite backdoor\nattack algorithm that does not depend solely on the utilization of rare characters or phrases, which\nenhances its stealthiness. Qi et al. (2021c) propose a sememe-based word substitution method that\ncleverly poisons training samples. Garg et al. (2020) embed adversarial perturbations into the model\nweights, precisely modifying the model's parameters to implement backdoor attacks. Maqsood et al.\n(2022) leverage adversarial training to control the robustness distance between poisoned and clean\nsamples, making it more difficult to identify poisoned samples. To further improve the stealthiness of\nbackdoor attacks, Wallace et al. (2021) propose an iterative updateable backdoor attack algorithm\nthat implants backdoors into language models without explicitly embedding triggers. Li et al. (2021b)\nutilize homographs as triggers, which have visually deceptive effects. Qi et al. (2021b) use abstract\nsyntactic structures as triggers, enhancing the quality of poisoned samples. Targeting the ChatGPT\nmodel (Achiam et al., 2023), Shi et al. (2023) design a reinforcement learning-based backdoor\nattack algorithm that injects triggers into the reward module, prompting the model to learn malicious\nresponses. Li et al. (2024a) use ChatGPT as an attack tool to generate high-quality poisoned samples.\nFor the clean label backdoor attack, Gupta et al. introduce an adversarial-based backdoor at-\ntack method that integrates adversarial perturbations into original samples, enhancing attack ef-\nficiency(Gupta & Krishna, 2023). Gan et al. (2022) design a poisoned sample generation model\nbased on genetic algorithms, ensuring that the labels of the poisoned samples are unchanged. Chen\net al. (2022) synthesize poisoned samples in a mimesis-style manner. Zhao et al. (2024d) leverage\nT5 (Raffel et al., 2020) as the backbone to generate poisoned samples in a specified style, which is\nused as the trigger. Compared to poison label backdoor attacks, clean label backdoor attacks are\ninherently more complex and necessitate a greater number of poisoned samples. Consequently, our\nresearch work is focused on exploring clean label backdoor attacks."}, {"title": "A.2 BACKDOOR ATTACK TARGETING PEFT ALGORITHMS", "content": "To alleviate the computational demands associated with fine-tuning LLMs, a series of PEFT al-\ngorithms are proposed (Hu et al., 2021; Hyeon-Woo et al., 2021; Liu et al., 2022). The LoRA\nalgorithm reduces computational resource consumption by freezing the original model's parameters\nand introducing two updatable low-rank matrices (Hu et al., 2021). Zhang et al. (2023) propose the\nAdaLoRA algorithm, which dynamically assigns parameter budgets to weight matrices based on their\nimportance scores. Lester et al. (2021) fine-tune language models by training them to learn \"soft\nprompts\", which entails the addition of a minimal set of extra parameters. Although PEFT algorithms\nprovide an effective method for fine-tuning LLMs, they also introduce security vulnerabilities (Cao\net al., 2023; Xue et al., 2024). Xu et al. (2022) validate the susceptibility of prompt-learning by\nembedding rare characters into training samples. Gu et al. (2023) introduce a gradient control method\nleveraging PEFT to improve the effectiveness of backdoor attacks. Cai et al. (2022) introduce an\nadaptive trigger based on continuous prompts, which enhances stealthiness of backdoor attacks.\nHuang et al. (2023) embed multiple trigger keys into instructions and input samples, activating\nthe backdoor only when all triggers are simultaneously detected. Zhao et al. (2024a) validate the\npotential vulnerabilities of PEFT algorithms when targeting weight poisoning backdoor attacks. Xu\net al. (2023) validate the security risks of instruction tuning by maliciously poisoning the training"}, {"title": "A.3 BACKDOOR ATTACK TARGETING KNOWLEDGE DISTILLATION", "content": "Knowledge distillation transfers the knowledge learned by larger models to lighter models, which\nenhances deployment efficiency (Nguyen & Luu, 2022). Although knowledge distillation is successful,\nit is demonstrated that backdoors may survive and covertly transfer to the student models during the\ndistillation process (Ge et al., 2021; Wang et al., 2022; Chen et al., 2024). Ge et al. (2021) introduce\na shadow to mimic the distillation process, transferring backdoor features to the student model. Wang\net al. (2022) leverage knowledge distillation to reduce anomalous features in model outputs caused\nby label flipping, enabling the model to bypass defenses and increase the attack success rate. Chen\net al. (2024) propose a backdoor attack method that targets feature distillation, achieved by encoding\nbackdoor knowledge into specific layers of neuron activation. Cheng et al. (2024) introduce an\nadaptive transfer algorithm for backdoor attacks that effectively distills backdoor features into smaller\nmodels through clean-tuning. Liang et al. (2024b) propose the dual-embedding guided framework\nfor backdoor attacks based on contrastive learning. Zhang et al. (2024) introduce a theory-guided\nmethod designed to maximize the effectiveness of backdoor attacks. Unlike previous studies, our\nstudy leverages small-scale poisoned teacher models to guide large-scale student models based on\ncontrastive knowledge distillation, augmenting the efficacy of backdoor attacks."}, {"title": "B EXPERIMENTAL DETAILS", "content": "In this section, we first detail the specifics of our study, including the datasets, evaluation metrics,\nattack methods, and implementation details.\nDatasets To validate the feasibility of our\nstudy, we conduct experiments on three\nbenchmark datasets in text classification:\nSST-2 (Socher et al., 2013), CR (Hu & Liu,\n2004), and AG's News (Zhang et al., 2015).\nSST-2 (Socher et al., 2013) and CR (Hu &\nLiu, 2004) are datasets designed for binary\nclassification tasks, while AG's News (Zhang\net al., 2015) is intended for multi-class. De-"}, {"title": "C MORE RESULTS", "content": "We further analyze the impact of different num-\nbers of updatable model parameters on the ASR.\nAs shown in Figure 6, as the rank size increases,\nthe number of updatable model parameters in-\ncreases, and the ASR rapidly rises. For example,\nwhen r = 8, only 0.12% of model parameters\nare updated, resulting in an ASR of 15.51%.\nHowever, when the updatable parameter fraction\nincreases to 7.1%, the ASR climbs to 95.16%.\nThis once again confirms our hypothesis that\nmerely updating a small number of model parameters is insufficient to internalize the alignment of\ntriggers and target labels.\nDifferent datasets Additionally, we verify the impact of different poisoned data on the W2SAttack\nalgorithm. Specifically, the IMDB dataset (Maas et al., 2011) is used when poisoning the teacher\nmodel, and the SST-2 dataset is employed to compromise the student model. The experimental results\nare shown in Table 11. It is not difficult to find that using different datasets to poison language models\ndoes not affect the effectiveness of the W2SAttack algorithm. For example, in the Vicuna model,"}, {"title": "ETHICS STATEMENT", "content": "Our paper on the W2SAttack algorithm reveals the potential risks associated with knowledge distilla-\ntion. While we propose an enhanced backdoor attack algorithm, our motivation is to expose potential\nsecurity vulnerabilities within the NLP community. Although attackers may misuse W2SAttack,\ndisseminating this information is crucial for informing the community and establishing a more secure\nNLP environment."}]}