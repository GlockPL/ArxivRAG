{"title": "HIDDEN IN THE NOISE: TWO-STAGE ROBUST WATERMARKING FOR IMAGES", "authors": ["Kasra Arabi", "Benjamin Feuer", "R. Teal Witter", "Chinmay Hegde", "Niv Cohen"], "abstract": "As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.\nIn this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative AI is capable of synthesizing high-quality images indistinguishable from real ones. This capability can be used to deliberately deceive. These fake image generations, called deepfakes, have the potential to cause severe societal harms through the spread of confusion and misinformation (Peebles & Xie, 2022; Esser et al., 2024; Chen et al., 2024; Ramesh et al., 2021). In addition, owners of different models and images may want to control the spread of their derivatives for copyright reasons and safeguard their intellectual property. One way to mitigate these harms is model watermarking. The study of watermarking has a rich history and has recently been adopted for AI-generated content (Pun et al., 1997; Langelaar et al., 2000; Craver et al., 1998). For an extended discussion of recent work in this area, we direct the reader to Appendix B. Unfortunately, most current image watermarking methods are not robust to watermark removal attacks utilizing image diffusion generative models (Zhao et al., 2023a).\nRecently, new watermarking methods utilize the inversion property of DDIM to achieve more robust watermarking (Wen et al., 2023; Ci et al., 2024; Yang et al., 2024b). These methods embed patterns in a diffusion model's initial noise and then detect them in the noise pattern reconstructed from the generated image. This technique provides strong robustness against various attacks, making it effective at resisting watermark removal attempts. Yet, these prior methods are themselves vulnerable to new types of attacks. Tree-Ring Wen et al. (2023) add a pattern to the initial noise,"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 THREAT MODEL", "content": "In a watermarking scheme we usually consider the owner, trying to mark images as an output of their model; and an attacker, trying to remove or forge the watermark on unrelated images.\nThe Owner releases a private model (diffusion model in our case) that clients can access through an API, allowing them to generate images that contain a watermark. The watermark is designed to have a negligible impact on the quality of the generated images. There are a few settings regarding the watermark detection, including public infomation and private information watermarking (Cox et al., 2007; Wong & Memon, 2001). We focus on the setting where the watermark is detectable only by the owner, enabling them to verify whether a given image was generated by their model using private information.\nThe Attacker uses the API to generate an image and subsequently attempts to launch a malicious attack aimed at either removing or forging the embedded watermark, with the intention of using the image or watermark for unauthorized purposes."}, {"title": "2.2 DIFFUSION MODELS INVERSION", "content": "Diffusion model inversion aims to find the reconstructed noise representation of a given data point, effectively reversing the generative process. Let $T$ be the number of diffusion steps, in both the generation and inversion processes. In the standard generation process, we start with noise $X_T$ drawn from an appropriately scaled Gaussian and iteratively apply $\\hat{x}_t = X_{t+1} + \\epsilon_\\theta(X_{t+1})$, where $\\epsilon_\\theta$ is a trained model that predicts the noise to be removed and $t \\in [T]$ is the time step describing how much noise should be removed in each stage. Conversely, the inversion process begins with a data point and moves towards its reconstructed noise representation by applying $x_{t+1} = x_t - \\epsilon_\\theta(x_t)$. This process relies on the assumption that $\\epsilon_\\theta(x_{t+1}) \\approx \\epsilon_\\theta(x_t)$, allowing us to approximately invert the diffusion process by adding the predicted noise (Ho et al., 2020; Song et al., 2022). DDIM's efficient sampling allows this technique to be particularly useful (Song et al., 2022)."}, {"title": "2.3 TREE-RING AND RINGID WATERMARKS", "content": "In order to watermark images in a human-imperceptible and robust way, previous works have encoded specific patterns in the Fourier space of the initial noise. Tree-Ring (Wen et al., 2023) first transforms the initial noise into the Fourier space. A key pattern is then embedded into the center of the transformed noise. The noise is subsequently transformed back into the spatial domain. During the detection phase, the diffusion process is inverted, and the Fourier domain is examined to verify the presence of the imprinted pattern. RingID (Ci et al., 2024) shows that Tree-Ring struggles to distinguish between different keys. Therefore, the number of unique keys (distinguishable from one another) that can be embedded with Tree-Ring is low. They increase the possible number of unique keys that can be encoded using Fourier patterns."}, {"title": "3 INITIAL NOISE IS A DISTORTION FREE WATERMARK", "content": "Watermarks which systematically perturb the distribution of image generations are more vulnerable to removal and forgery attacks. A distortion-free watermarking method, by contrast, is more robust (Kuditipudi et al., 2023). Our first finding is that the initial noise already in standard use in diffusion models can be such a watermark.\nLet N be the number of initial noises we can generate. We will secure our watermarking process with a long, secret salt $s$. We begin by sampling a random (and reproducible) initial noise. Let $i^* \\sim \\text{Unif}([N])$ be the index of the initial noise. We will use a hash function to get a seed $\\text{hash}(i^*, s)$. Plugging the seed into a pseudorandom generator, we generate a reproducible initial noise vector"}, {"title": "4 \u041c\u0415\u0422\u041dOD", "content": ""}, {"title": "4.1 WIND: TWO-STAGE EFFICIENT WATERMARKING", "content": "While always using a single initial noise for our model might imply good robustness properties, to make forgery and removal more difficult, it is generally preferable to maintain a large set of N initial noises to be used by the model. More importantly, using a large number of different noises N may serve as different keys, encoding some metadata about each image. This metadata might include information about the specific model that generated it, as well as additional information about the generation for further validation of the image source, once detected.\nIn order to make the search over a large number of noises more efficient, we introduce a two-stage efficient watermarking ap- proach we name WIND (Watermarking with Indistinguishable and Robust Noise for Diffusion Models). First, we initialize M groups of initial noise, each group associated with its own Fourier-pattern key. In contrast to prior work, we employ these Fourier patterns not as a watermark, but as a group identifier to reduce the search space.\nFor each image generation, we randomly se- lect an index for the initial noise, denoted as $i^* \\in [N]$. We use a group identifier $g^* = i^*\\%\u041c$, where % denotes the modulus operation. We embed $g^*$ in the Fourier space of the latent noise (similar to Wen et al. (2023)). During detection, we reconstruct the latent noise and find the group identifier $\\bar{g}$ that is closest to the Fourier pattern embedded in the image. We then search over all indices $i$ such that $\\bar{g} = i\\%M$. In this way, the search space has a size of $N/M$ rather than N. We include an algorithm box for generation (Algorithm 1) and detection (Algorithm 2)."}, {"title": "4.2 RESILIENCE TO FORGERY", "content": "In addition to empirical evaluations of specific attacks as in Figures 2 and 3; we discuss below the attacker's ability to infer knowledge about the used noise pattern across different watermarked images. Even if the attacker is able to obtain information about a specific initial noise $z_i$ for an index $i$ (which is an extreme case), the other noise vectors for $j \\neq i$ are still safe. This is because we use a cryptographic hash function and a secret salt. Formally, Theorem 4.1 shows that, as long as the cryptographic hash function remains unbroken and the secret salt is kept private, the watermarking algorithm maintains its security properties against even very powerful adversaries.\nTheorem 4.1. [Cryptographic Security] Let $\\text{hash}: \\{0,1\\}^* \\rightarrow \\{0,1\\}^l$ be an unbroken cryptographic hash function used in our watermarking algorithm, with inputs $i^* \\in [N]$ and a secret salt $s$. Assume $s$ is sufficiently long and randomly generated. Then, even if an adversary obtains: the group number $g^*$, the initial noise index $i^*$, the initial noise $z_{i^*}$, and even the corresponding output of the hash function $\\text{seed}$, the adversary cannot:\n1. Recover the secret salt $s$,\n2. Generate valid reconstructed noise $z_j$ for any other initial noise index $j \\neq i$"}, {"title": "4.3 WATERMARKING NON-SYNTHETIC IMAGES.", "content": "Until now, we have addressed watermarking only for AI-generated synthetic images. Yet, protecting copyrights, or preventing the spread of misinformation, may also apply to modified natural images. Most previous approaches to watermark diffusion models overlook attempting to expand their method to non-generated images. To allow using our framework for non-generated images, we expand our framework. By using diffusion inpainting, our watermark can be applied to a natural image. Later, by inverting the inpainted image we can verify the presence of the watermark.\nAs demonstrated in Figure 5, our inpainting method injects a watermark with minimal visual impact, preserving the original image's integrity. Please see Appendix D for additional results."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 WATERMARK ROBUSTNESS", "content": "Setting. For a fair comparison with previous methods (Ci et al., 2024; Wen et al., 2023), we employed Stable Diffusion-v2 (Rombach et al., 2022), with 50 inference steps for both generation and inversion. Other implementation-details can be found in ??.\nImage Transformation Attacks. Following previous methods (Wen et al., 2023; Ci et al., 2024) we applied these image transformations to the generated images: 75\u00b0 rotation, 25% JPEG compres- sion, 75% random cropping and scaling (C & S), Gaussian blur with an 8 \u00d7 8 filter size, Gaussian noise with $\\sigma = 0.1$, and color jitter with a brightness factor uniformly sampled between 0 and 6. In Table 1 we compare our methods to both Tree-Ring and RingID. As the results demonstrate, using multiple keys with RingID (Ci et al., 2024) is possible. Yet, it remains vulnerable to cropping and scaling attacks. In contrast, WIND effectively addresses this challenge. It enables accurate watermark detection under all image transformation attacks. We note that the incorporation of the keys in the RingID method not only allows us to embed keys but also increases the robustness of the full method to certain attacks.\nSteganalysis Attack. We assess the robustness of our method against the attack proposed by Yang et al. (2024a), which is capable of forging and removing the Tree-Ring and RingID keys. As discussed in Section 2.3, this attack attempts to approximate the watermark by subtracting watermarked images from non-watermarked images. The results, presented in Figure 3, indicate that while the attack could be able to forge or remove our group identifier, it is unable to forge or remove our watermark (initial noises). Even when the Fourier pattern type key is removed through an exhaustive search, our method remains robust in identifying the correct initial noise."}, {"title": "6 DISCUSSION AND LIMITATIONS", "content": "Editing a Given Image vs. Forging. While forging our watermark by obtaining the initial noise is hard (Section 3), an easier path to obtaining harmful watermarked images might be to apply a slight edit to an already watermarked image. An harmful image in this context might include a copy-right infringing image, NSFW image, or any other content the model owner wish to avoid being associated with. Naturally, there is a trade-off between the severity of the applied edit, and the edit ability to preserve the initial watermark. We present one solution to mitigating this issue in the next discussion point.\nStoring a Database of Generations. Model owners wishing to protect themselves from an attacker modifying a watermark image may keep a database of the past generations by their model. For these extreme cases, the model owner might only save the used prompts and initial noiseseeds, and use the reconstructed noise to retrieve the entire set of prompts used with that specific seed (Huang & Wan, 2024). While this process may be resource-intensive, it is only required in the rare event that an attacker intentionally modifies a benign image into a harmful one while preserving the watermark.\nPrivate Model. Our watermark robustness is based to a large extent on the inability of an attacker to invert a model, which is empirically validated but not mathematically proven. Yet, as discussed in Section 2.2, the ability to successfully invert our model may be nearly equivalent to the ability to steal the forward diffusion process, effectively stealing the model (in which case, any watermarking attempt might be deemed quite useless anyhow). Still, a better framing of the mathematical assumptions behind this claim is a limitation of this work, as well as of previous works on watermarking using inversion of the diffusion generative process."}, {"title": "7 CONCLUSION", "content": "In this work, we present a robust and distortion-free watermarking method that leverages the initial noises employed in diffusion models for image generation. By integrating existing techniques, we enhanced the approach to achieve improved efficiency and robustness against various types of attacks. Furthermore, we outlined a strategy for applying our method to non-generated images through inpainting."}, {"title": "A NOTATION", "content": ""}, {"title": "B RELATED WORKS", "content": "Memorization in Diffusion Models. Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) have demonstrated a capacity not only to generalize but also to memorize training data. This can lead to the reproduction of specific patterns or, in some cases, exact content from the training set, including sensitive or proprietary information. This memorization poses significant risks of unintended intellectual property leakage, particularly in large-scale generative models. Several studies have shown that information from training data can be extracted from diffusion models (Carlini et al., 2023b; Somepalli et al., 2023b; Carlini et al., 2023a; Gu et al., 2023; Somepalli et al., 2023a).\nImage Watermarking. Image watermarking is essential for protecting intellectual property, ver- ifying content authenticity, and maintaining the integrity of digital media. The field has ranged from traditional signal processing techniques to recent deep learning methods (Potdar et al., 2005; Singh & Singh, 2023).\nAmong Early watermarking strategies, one of the simplest methods was Least Significant Bit (LSB) embedding, which modifies the least significant bits of image pixels to imperceptibly embed water- marks (Wolfgang & Delp, 1996). Another classical approach utilized frequency-domain transfor- mations and Singular Value Decomposition (SVD) to hide watermarks within image coefficients. (Chang et al., 2005; Al-Haj, 2007).\nRecent developments leverage deep learning for watermarking. For instance, HiDDeN (Zhu et al., 2018) introduced an end-to-end trainable framework for data hiding. RivaGAN (Zhang et al., 2019) utilizes adversarial training to embed watermarks, while Lukas & Kerschbaum (2023) proposed an embedding technique that optimizes efficiency by avoiding full generator retraining."}, {"title": "C ADDITIONAL DISCUSSION AND LIMITATIONS", "content": "Relation to Other Initial Noise Watermarking Methods. The seminal work by Wen et al. (2023) innovated the use of initial noise in DDIM for watermarking. Most related to our work, Yang et al. (2024b) also embeds a watermark in the initial noise already used by a DDIM diffusion model. Yet, while Yang et al. (2024b) proposes a watermark that is distortion-free for a single image, it is not distortion-free when examining sets of images; therefore it is vulnerable to attacks such as Yang et al. (2024a). We aim to be robust to attacks even when many images are examined together.\nThere are additional technical differences between our approach and Yang et al. (2024b). Most notably: (i) Our work also studies applying our watermark to non-synthetic (natural) images, or images coming from other generative models. (ii) While Yang et al. (2024b) design a function to embed specific bits into the initial noise, we take another approach. Namely, we view the entire initial noise (with generation and inversion) as a noisy channel. Inspired by Shannon (1949), we use a random encoding of the watermark identities into the channel.\nComputational Requirements. As discussed in Section 3 our similarity search can be accelerated given well-known methods. Yet, the computational requirements of our method might be limiting when trying to use our method on edge devices. However, similarly to Tree-Ring and Ring-ID (Wen et al., 2023; Ci et al., 2024) our method assumes a private model, which is usually not deployed on edge devices anyhow."}, {"title": "D Additional Results", "content": ""}, {"title": "D.1 Applicability to Other Types of Models", "content": "We expect our watermark to be effective directly for any model for which some inversion to the original noise is possible. Namely, as the correlation between random noises in a very high dimension is very much concentrated around 0, even a very slight success in the inversion process is enough to be distinguishable. In higher generation resolutions the dimensionality of the noise is even higher, and therefore the separation would be even better (El Karoui, 2009).\nEmpirically, to validate the generality of our method, we also report results for the SD 1.4 model (Rombach et al., 2022). Using \u039d = 10000 noises and M = 2048 group identifiers, our method achieved a detection accuracy of 97% to identify the correct watermark (initial noise).\nIn any case, our method of the reported SD 2.1 model can also be used to watermark images collected from other sources (please see Section 4.3, Appendix D.2)."}, {"title": "D.2 Non-Synthetic Images Watermark Detection", "content": "Our inpainting method allows us to watermark both images generated by any model and non- generated images. To evaluate the robustness of the inpainting watermarking approach, we present results in Table 1 for this method, utilizing N = 100 noises. Results are shown in Table 6."}, {"title": "D.3 Further Exploration of the Regeneration Attack Perturbation Strength", "content": "In Section 5.1, we discussed the robustness of WIND against regeneration attacks. However, using it iteratively might still be a stronger adversary. We applied the regeneration attack proposed by Zhao et al. (2023a), up to 50 times. We see that iterative regeneration indeed decreases the similarity between the original noise and the reconstructed one. This happens as the image becomes less and less correlated to the original generation Figure 6.\nYet, the detection rate of our algorithm remains very high Table 14. We attribute this to the fact that even a slight remaining correlation between the attacked image and the initial noise remains significant with respect to the correlation expected from non-watermarked images. This happens because of the very low correlation between random (non-watermarked) noises (Figure 2)."}, {"title": "D.4 QUANTITATIVE ANALYSIS OF THE EFFECT ON IMAGE QUALITY", "content": "We reported the FID of our model on Table 3. To further assess the effect of WIND watermark on image quality we report the CLIP score Hessel et al. (2021) before and after watermarking on Table 10. Results indicate that adding the watermark has a negligible effect on the CLIP score for generated images.\nTo further quantify the distortion introduced by each model, we report pixel-base matrices, SSIM and PSNR in the two settings we study:\nImages Generated by the Diffusion Model. WIND's distortion arises from using group identi- fiers, enabling faster detection. To disentangle this effect, we also evaluate WINDw/o, which omits group identifiers. As can be seen in Table 11, the image quality generated using our full method is comparable to that of previous techniques. Users who wish to generate distortion-free images, without affecting image quality, can do so by omitting the group identifier (at the cost of a slower detection phase for very large values of N).\nWatermarking Non-Synthetic Images. Additionally, we present results for WINDinpainting, our inpainting-based approach capable of watermarking both non-synthetic images and outputs from other generative models (Table 15). Although other watermarking methods may preserve image quality better, our image quality remains high. Importantly, to the best of our knowledge, our approach is the only one capable of watermarking non-synthetic images while remaining robust against the regeneration attack (Zhao et al., 2023a). Therefore, it is preferable when an adversary may try to remove the watermark.\nIn addition, the inpainting technique can be applied selectively to specific parts of the image if the copyright owner wishes to perfectly preserve fine details in certain areas."}, {"title": "D.5 ROBUSTNESS COMPARISON TO DIFFERENT NUMBER OF INFERENCE STEPS", "content": "We evaluate the impact of inference steps on detection accuracy, as shown in Table 7. The re- sults indicate that using 100 steps yields better detection accuracy compared to other step counts, including the 50 steps used in our main experiments."}, {"title": "D.6 TRUE POSITIVE AND AUC", "content": "Expanding on the detection assessment settings discussed in Section 5, we reported WIND's error bars. AUC and True Positive (TPR@1%FPR) results are available on Table 8. Demonstrate strong performance, emphasizing WIND's robustness and reliability."}, {"title": "D.7 EVALUATION AGAINST ADDITIONAL ATTACKS", "content": "We evaluate WIND against a diverse set of attacks, including transfer-based, query-based, and white-box methods. Specifically, we employ the WeVade white-box attack (Jiang et al., 2023), the transfer attack described in Hu et al. (2024), a black-box attack utilizing NES queries (Ilyas et al., 2018), and a random search approach discussed in Andriushchenko et al. (2024), adopted to"}, {"title": "E PROOF OF RESILIENCE TO FORGERY", "content": "The WIND method is an approach for generating multiple watermarked images. Theorem 4.1 tells us that compromising one or more watermarked images does not give away any information about any other watermarked images. E.g., the adversary cannot \"generate valid reconstructed noise for any other initial noise index $j \\neq i$\". That said, Theorem 4.1 does leave open the possibility that an adversary can take a watermarked image, reconstruct the initial noise only for that image, and use it to attack the method, which we evaluate empirically.\nCryptographic Background Consider a cryptographic hash function $\\text{hash}: \\{0,1\\}^* \\rightarrow \\{0,1\\}^l$ with $l$ output bits. E.g., $l = 256$ for SHA-256. We will describe properties of the hash function in terms of 'difficulty'; we say a task is 'difficult' if, as far as we know, finding a solution is almost certainly beyond the computational capabilities of any reasonable adversary. An unbroken cryptographic hash function satisfies the following properties: Pre-image resistance requires that given a hashed value $v$, it is difficult to find any message $m$ such that $v = \\text{hash}(m)$. Second pre- image resistance requires that given an input $m_1$, it is difficult to find a different input $m_2$ such that $\\text{hash}(m_1)= \\text{hash}(m_2)$. Collision resistance requires that it is difficult to find two different messages $m_1$ and $m_2$ such that $\\text{hash}(m_1)= \\text{hash}(m_2)$.\nTheorem 4.1. [Cryptographic Security] Let $\\text{hash}: \\{0,1\\}^* \\rightarrow \\{0,1\\}^l$ be an unbroken cryptographic hash function used in our watermarking algorithm, with inputs $i^* \\in [N]$ and a secret salt $s$. Assume $s$ is sufficiently long and randomly generated. Then, even if an adversary obtains: the group number $g^*$, the initial noise index $i^*$, the initial noise $z_{i^*}$, and even the corresponding output of the hash function $\\text{seed}$, the adversary cannot:\n1. Recover the secret salt $s$,\n2. Generate valid reconstructed noise $z_j$ for any other initial noise index $j \\neq i$\nProof of Theorem 4.1. We will prove each part of the theorem separately:\n1. The adversary cannot recover the secret salt $s$: Given the output $\\text{seed} = \\text{hash}(i^*, s)$ and partial input $i^*$ the adversary aims to find $s$. This is equivalent to finding a pre-image of given partial information about the input. By the pre-image resistance property of cryptographic hash functions, this task is computationally infeasible. Even if the adversary knows all possible values of $i$, the space of possible secret salts $s$ is too large to search exhaustively (as $s$ is a sufficiently long random string). Therefore, the adversary cannot recover $s$.\n2. The adversary cannot generate valid reconstructed noise for any other initial noise index $j\\neq i$. This security guarantee is ensured by two properties of hash: a) Second pre-image resistance: Given"}, {"title": "F FURTHER DISCUSSION ON DISTORTION", "content": "Using the same initial noise for multiple generations is not distortion-free when examining groups of images. For example, all images with the same prompt p and the same initial noise z will"}, {"title": "G NUMBER OF GROUPS", "content": "In our framework, we divide the initial noises into N groups and associate a Tree-Ring-type key with each group. The use of Fourier Pattern keys enables robustness against rotation, and grouping reduces the search space for inverted noise.\nTo investigate the impact of the number of groups, we performed an experiment with 10,000 noises and varied the number of groups from 32 to 2048. As expected, Figure 8 demonstrates that increasing the number of groups leads to better accuracy in detecting the correct initial noise. This is because a larger number of groups results in fewer noises per group, which facilitates more"}, {"title": "\u0397 EMPIRICAL RUNTIME ANALYSIS", "content": "However, the runtime is highly sensitive to the available computational resources. To provide a practical estimate, we measured the detection time using a single NVIDIA GeForce RTX 3090. Specifically, we divided 100,000 initial noise samples into 32 groups and reported the detection. Under these conditions, the detection phase for 100,000 noise samples takes approximately 22 seconds per detection. We include a comparison with other methods in Table 12."}, {"title": "I IMPLEMENTATION DETAILS", "content": "Prompts. For all evaluations we used the set of prompts taken from Gustavosta (2024).\nThreshold for Detection. For the first variant WINDfast (see Section 4) we use a threshold of min 12 norm > 160. The second variant (WINDfull) does not use a threshold, but rather, we choose the noise pattern within the group that has the lowest 12 as our candidates for the identified noise.\nGeneral Retrieval Details. We included simple rotation (using intervals of 2 degrees) and slid- ing window (window size of 32, stride of 8) searches as part of the retrieval process. These searches do not involve directly optimizing for the specific degrees of rotation or cropping encountered, ensuring that robustness remains intrinsic to the method."}]}