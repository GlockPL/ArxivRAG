{"title": "A Survey on LLM-as-a-Judge", "authors": ["JIAWEI GU", "XUHUI JIANG", "ZHICHAO SHI", "HEXIANG TAN", "XUEHAO ZHAI", "CHENGJIN XU", "WEI LI", "YINGHAN SHEN", "SHENGJIE MA", "HONGHAO LIU", "YUANZHUO WANG", "JIAN GUO"], "abstract": "Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. https://github.com/IDEA-FinAI/LLM-as-Evaluator.", "sections": [{"title": "1 INTRODUCTION", "content": "Judgment is the faculty of thinking the particular as contained under the universal. It involves the capacity to subsume under rules, that is, to distinguish whether something falls under a given rule.\nKant, Critique of Judgment [38], Introduction IV, 5:179; Critique of Pure Reason [37], A132/B171.\nRecently, Large Language Models (LLMs) have achieved remarkable success in numerous domains, ranging from artificial intelligence and software engineering to education and social science. The adoption of LLMs as evaluators\u2014commonly referred to as \"LLM-as-a-Judge\" [135]-has surged, driven by their ability to emulate human-like reasoning and decision-making processes. This capability enables LLMs to undertake roles traditionally reserved for human experts, offering a cost-effective and scalable alternative. For instance, we usually rely on experts to evaluate the accuracy of mathematics and physics competition questions at the Olympiad level [28], which can be assessed through LLM-as-a-Judge now. Additionally, in recent peer reviews of research submissions, LLM-as-a-Judge is introduced to address the rising number of paper submissions and reviewer workload\u00b9, which is designed to identify potential issues in reviews and offer constructive feedback to reviewers. These trends underscore a key motivation for adopting LLM-as-a-Judge:the potential to enhance evaluation efficiency while addressing limitations inherent in human assessments, such as scalability and consistency.\nLLM-as-a-Judge presents a compelling alternative to both human evaluations and traditional automated methods, offering distinct advantages in scalability, efficiency, and adaptability. Human evaluations, though often considered the gold standard, face challenges in scalability, cost, and consistency. They are time-consuming, require substantial expert effort, and are expensive to scale due to limited availability of qualified evaluators. Coordinating and training evaluators adds complexity, and fatigue during lengthy tasks can compromise reliability and accuracy. In contrast, LLMs provide scalable, cost-effective, and efficient evaluations with reduced subjective variability, enhancing objectivity. Traditional automated methods, such as BLEU, ROUGE, and METEOR for software artifact summarization, often fail to align with human judgment or provide clear insights in specialized domains like software engineering. LLMs offer flexibility to process diverse input types, including text, semi-structured data, and multi-modal content, allowing evaluations to integrate qualitative insights with quantitative rigor. This human-aligned adaptability makes LLMs effective for complex, context-aware assessments beyond the limits of conventional metrics [10, 51, 110].\nDespite its wide advantages, LLM-as-a-Judge poses significant challenges for reliability. This necessitates the capability of LLM-as-a-Judge framework to subsume under rules, that is, to distin-guish whether something falls under a given rule [37]. As LLM-as-a-Judge becomes more commonly used as an effective evaluator in different areas, collecting evaluations with LLM-as-a-Judge is relatively simple. Therefore, central to this survey is the fundamental question: How to build reliable LLM-as-a-Judge systems? To address this question, we explore two core aspects: (1) strategies for enhancing the reliability of LLM-as-a-Judge and (2) methodologies for evaluating reliability of LLM-as-a-Judge systems themselves.\nFor the first aspect of enhancing LLM-as-a-Judge reliability, we review the main strategies aimed at optimizing their performance for diverse evaluation tasks. These strategies include improving consistency, mitigating biases, and refining adaptability to different assessment scenarios. For the second aspect, we examine the metrics, datasets, and methodologies used to evaluate the performance of LLM-as-a-Judge systems, discussing potential sources of bias and corresponding mitigation techniques. Building on this foundation, we introduce a novel benchmark specifically designed for LLM-as-a-Judge evaluations. Using established metrics and datasets, this benchmark provides a framework for analyzing the effectiveness of various reliability enhancement strategies. Additionally, we explore practical application scenarios, identify specific challenges unique to each context, and propose solutions to address these issues. Finally, we discuss future research directions, emphasizing key areas for advancing the reliability, scalability, and applicability of LLM-as-a-Judge systems.\nThis paper aims to provide a comprehensive overview of the LLM-as-a-Judge research landscape while offering insights into how reliable LLM-as-a-Judge can be constructed. We hope that this work will serve as a valuable reference for researchers and practitioners, fostering further research and facilitating the real-world deployment of LLM-as-a-Judge. The rest of this survey is organized as Figure 1. In Section 2, we provide a comprehensive overview of the LLM-as-a-Judge field. We define LLM-as-a-Judge through formal and informal definitions, and categorize existing methods and approaches for its use. For a quick guide on implementing an LLM-as-a-Judge for specific scenarios, you can find answers in Quick Practice (2.5). Then, we discuss the problems of \"How to improve\" and \"how to evaluate\" LLM-as-a-Judge in n Section 3 to 4 . Next, we show the applications of LLM-as-a-Judge in Section 6. The discussions of the challenges and future directions come at last in Section 7 and Section 8."}, {"title": "2 BACKGROUND AND METHOD", "content": "In evaluative tasks, especially those that are subjective, human assessment is often considered the gold standard due to its reliable and open-ended nature [22, 85]. However, this approach is typically slow and costly [124, 135, 141]. \u03a4To address these challenges, LLMs are increasingly employed as substitutes for human evaluators. Since these models are frequently trained using Reinforcement Learning from Human Feedback (RLHF), they demonstrate strong alignment with human perspectives, leading to the approach known as \"LLM-as-a-Judge\".\nIn general, LLM-as-a-Judge is a process that uses LLM to evaluate different objects in various scenarios for diverse tasks. For instance, roles such as \"Assessors\", \"Critics,\" and \"Verifiers\" utilize LLMs to facilitate evaluation at different stages of the process, whether during intermediate steps or throughout the entire workflow. To date, definition of how to effectively use LLM-as-a-Judge for evaluation tasks has been largely informal or vague, lacking clear and formal expression. Therefore, we will provide a formal definition of LLM-as-Evaluator as follows:\n$E \\leftarrow P_{LLM} (\u0445 \\oplus C)$\n\u2022 8: The final evaluation obtained from the whole LLM-as-a-Judge process in the expected manner. It could be a score, a choice, or a sentence, etc.\n\u2022 $P_{LLM}$: The probability function defined by the corresponding LLM, and the generation is an auto-regressive process.\n\u2022 x: The input data in any available types (text, image, video), which waiting to be evaluated.\n\u2022 C: The context for the input x, which is often prompt template or combined with history information in dialogue.\n\u2022 \u2295: The combination operator combines the input x with the context C, and this operation can vary depending on the context, such as being placed at the beginning, middle, or end.\nThe formulation of LLM-as-a-Judge reflects that LLM is a type of auto-regressive generative model, which generates subsequent content based on the context then obtain target evaluation from it. The form of LLM-as-a-Judge illustrates how we utilize LLM for evaluation tasks, encom-passing input design, model selection and training, as well as output post-processing. The different basic approaches of implementing LLM-as-a-Judge can be classified according to the formulation: In-Context Learning, Model Selection, Post-processing Method and Evaluation Pipeline, which concluded in Figure 2. By following this pipeline, we can build a basic LLM-as-a-Judge for evaluation. A faster practice guide is available in section 2.5."}, {"title": "2.1 In-Context Learning", "content": "To apply LLM-as-a-Judge, it is helpful to start by defining the evaluation task using In-Context Learning methods. This process involves two key aspects: the design of prompt and input. For input design, it is important to consider the type of variables to be evaluated (such as text, image, or video), the manner of input (e.g., individually, in pairs, or in batches), and the position of the input (e.g., at the beginning, middle, or end). As for the prompt design, four different methods can be adopted, as illustrated in Figure 2. The four methods include generating scores, solving true/false questions, conducting pairwise comparisons, and making multiple-choice selections. Further details will be provided in the following sections."}, {"title": "2.1.1 Generating scores.", "content": "It is quite intuitive to represent an evaluation using a corresponding score. What requires more careful consideration, however, is the nature and range of the score used for evaluation. The score can be discrete, with common ranges like 1-3, 1-5 [36], or 1-10 [51, 141]. Alternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 [117]. The simplest way toscore is through the context, setting the range of scores and the main criteria for scoring. For example, \"Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance\" [141]. A slightly more complex way is to provide more detailed scoring criteria. More complex scoring situations can be as Language-Model-as-an-Examiner [3], which use Likert scale scoring functions as an absolute evaluative measure showed in Figure 3. The evaluator assigns scores to a given response along predefined dimensions including accuracy, coherence, factuality and comprehensiveness. Each of these dimensions is scored on a scale of 1 to 3, ranging from worst to best. The evaluator is also asked to provide an overall score ranging from 1 to 5, based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer."}, {"title": "2.1.2 Solving Yes/No questions.", "content": "A Yes/No question requires a judgment on a given statement, focusing solely on its accuracy. This type of question is simple and direct, providing only two fixed responses-yes or no, true or false\u2014without any additional comparisons or choices.\nThis type of evaluation is often utilized in intermediate processes, creating the conditions for a feedback loop. For example, it promotes a self-optimization cycle, as seen in Reflexion [86], which generates verbal self-reflections to provide valuable feedback for future attempts. In scenarios with sparse reward signals, such as a binary success status (success/fail), the self-reflection model uses the current trajectory and persistent memory to generate nuanced and specific feedback. Similarly, in self-improvement contexts [98], Yes/No questions can be employed to evaluate custom phrases,such as \"Modification needed.\" and \"No modification needed.\", facilitating entry into the next cycle. Moreover, these evaluations are common for testing knowledge accuracy and assessing whether statements align with established facts [92], like \"Given a question and the associated retrieved knowledge graph triples (entity, relation, entity), you are asked to answer whether it's sufficient for you to answer the question with these triples and your knowledge (Yes or No).\" A detailed and specific example can be seen in the Figure 4."}, {"title": "2.1.3 Conducting pairwise comparisons.", "content": "Pairwise comparison refers to comparing two options and selecting which one is superior or more aligned with a specific standard, showed in Figure 5. It involves making a decision between two options rather than judgement between 'yes' or 'no'. The comparison can be subjective or based on objective criteria. This evaluation is a relative evaluation. Pairwise comparison is often used for ranking multiple options or prioritizing them, where several comparisons are made between pairs to identify the better choice or establish a hierarchy.\nPairwise comparison is a well-established method that has significantly impacted a variety of fields [76]. As noted by [62], LLM and human evaluations are more aligned in the context of pairwise comparisons compared to score-based assessments. Numerous studies have demonstrated that pairwise comparative assessments outperform other judging methods in terms of positional consistency [63, 136]. Furthermore, pairwise comparisons can be extended to more complex relation-based assessment frameworks, such as list-wise comparisons, using advanced ranking algorithms [62, 76], data filtering [124]. In pairwise comparative assessments, LLM-as-a-Judge is prompted to select the response that better answers the question at hand. To accommodate the possibility of a tie, several option modes are introduced. The Two-Option mode requires judges to choose the better response from two given options. The Three-Option mode introduces an additional choice, allowing judges to indicate a tie if neither response is preferable. Evaluations typically involve determining the outcomes of win, tie, or loss for responses [110] through pairwise comparisons, with win rounds counted for each response. The Four-Option mode further expands the choices, allowing judges to classify responses as either a \"both good tie\" or a \"both bad tie.\""}, {"title": "2.1.4 Making multiple-choice selections.", "content": "Multiple-choice selections involve providing several options, not giving relative choices in pairwise comparison, nor making a yes/no judgment. The evaluator must choose the most appropriate or correct one. This method allows for a broader range of responses compared to true/false questions and can assess deeper understanding or preferences and an example is showed in Figure 6. However, this kind of prompt design is more rare than the first three."}, {"title": "2.2 Model Selection", "content": null}, {"title": "2.2.1 General LLM.", "content": "To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4 [70] instead of human evaluators [136]. For instance, Li et al. [53] created a test set with 805 questions and assessed the performance by comparing it to text-davinci-003 using GPT-4. Additionally, Zheng et al. [136] designed 80 multi-round test questions across eight common areas and used GPT-4 to automatically score the model's responses. The accuracy of the GPT-4-based evaluator has been demonstrated to be high compared to professional human evaluators, showing superior consistency and stability in evaluations. At the same time, if the general LLM used has limitations in instruction-following or reasoning abilities, the effectiveness of the LLM-as-a-Judge method may be significantly affected."}, {"title": "2.2.2 Fine-tuned LLM.", "content": "However, relying on external API for evaluation may introduce con-sideration about privacy leakage, and the opacity of API models also challenges the evaluation reproducibility. Therefore, follow-up works suggest fine-tuning language models specialized in evaluations. For instance, PandaLM [110] constructs data based on Alpaca instructions and GPT-3.5 annotation, and then fine-tunes LLaMA-7B [100] as an evaluator model. JudgeLM [141] constructs data from diversified instruction sets and GPT-4 annotations, and fine-tunes Vicuna [101] as a scalable evaluator model. Auto-J [51] constructs evaluation data upon multiple scenarios to train a generative evaluator model, which can provide both evaluation and critical opinion. Prometheus [40] defines thousands of evaluation criteria and construct a feedback dataset based on GPT-4, and fine-tunes a fine-grained evaluator model. The typical process for fine-tuning a judge model involves three main steps. Step 1: Data Collection. The training data generally consists of three components: instructions, the objects to be evaluated, and evaluations. Instructions are typically sourced from instruction datasets, while evaluations can come from either GPT-4 or human an-notations. Step 2-Prompt Design. The structure of the prompt template can vary based on the evaluation scheme, which already detailed in \u00a7 2.1. Step 3: Model Fine-Tuning. Using the designed prompts and collected data, the fine-tuning process for the evaluator model typically adheres tothe instruction fine-tuning paradigm [72]. The model receives an instruction along with one or more responses to generate output that includes evaluation results and possibly explanations.\nAfter fine-tuning, the evaluator model can be employed to evaluate the target object. While these fine-tuned models often demonstrate superior performance on self-designed test sets, they are identified several limitations in their evaluation capabilities, which detailed in Section 4.2."}, {"title": "2.3 Post-processing Method", "content": "Post-processing refines the probability distributions generated by LLM-as-a-Judge to provide accurate evaluations. The evaluation format should align with our In-Context Learning design. Additionally, post-processing may involve procedures to enhance the reliability of extracted eval-uations, closely linked to the In-Context Learning framework and consistently applied. There are three main methods of post-processing, which are extracting specific tokens, normalizing the output logits, and selecting sentences with high returns."}, {"title": "2.3.1 Extracting specific tokens.", "content": "As showed in In-context Learning (Section 2.1), when the evaluation target take the form of a score, selecting specific options, or responding with Yes/No, applying rule-match to extract the corresponding token from the response generated during probability distribution iteration is common used. It is worth noting that Yes/No is a broad definition, including custom statements involving judgment. Considering a Yes/No question for evaluation in custom phrases [98]: \"Modification needed.\" and \"No modification needed.\" or a yes-no question \"Does the above answer need to be further modified?\". When the input sample is put through the template, it might have outputs such as \"Modification needed.\", \"Conclusion: Modification needed.\" or \"Yes\". This variance in response formats is difficult to parse consistently. The corresponding post-processing with the response is necessary. Using rules to extract specific tokens for our designed prompts and input content, as well as the backbone model used for the evaluator, all have higher requirements as we discussed in Section 2.2. In contextual learning, if there is no clear indication of the output format for response, there may be various expressions of evaluation, which can be seen in Figure 2. For example, \"Response 1 is better\" and \"The better one is response 1\", which convey the same choice but differ in format leading to the difficulty of rule recognition. Simple solutions often involve providing clear instructions, such as \"The last sentence should be started with 'The better response is\u2019\", or using a few-shot strategy. Also, the general model with insufficient instruction following capability may not be able to generate the evaluation format and content of the target according to the instruction, resulting in the post-processing extracted according to the rules not as smooth as expected."}, {"title": "2.3.2 Normalizing the output logits.", "content": "LLM-as-a-Judge in the intermediate steps with Yes/No setting often normalize the output logits to obtain the evaluation in the form of a continuous decimal between 0 and 1. This is also very common in agent methods and prompt-based optimization methods [27, 112, 144]. For example, the self-consistency and self-reflection scores [112] within one forward pass of MEvaluator, are effectively obtained by constructing a prompt [(x \u2295 C), \"Yes\"] and acquire the probability of each token conditioned on the previous tokens $P(t_i|t_{<i})$. The auto-regressive feature is leveraged, thus aggregate the probability of the relevant tokens to compute the self-consistent score $P_{self-consistency}$ and self-reflection score $P_{Self-reflection}$. The final score is produced by $p_j = P_{sc,j} . PSR,j$.\n$(x + C) \\text{\"Yes\"} \\leftarrow\\begin{aligned}\\ P_{sc} &= \\Pi_{t_i \\in \\text{\\\"Yes\\\"}} P(t_i|t_{<i})\\\\ P_{SR} &= \\Pi_{t_i \\in \\text{\\\"Yes\\\"}} P(t_i|t_{<i})\\end{aligned}$"}, {"title": "2.3.3 Selecting sentences.", "content": "In addition to selecting specific tokens and normalizing the output logits, the content extracted by LLM-as-a-Judge may also be a sentence or paragraph. As showed in Figure 2, agent for reasoning task [27], builds a reasoning tree by iteratively considering the most promising reasoning steps (actions, sub-questions) by LLM-as-a-Judge."}, {"title": "2.4 Evaluation Pipeline", "content": "There are three common scenarios for using LLM-as-a-Judge evaluation pipelines showed in Figure 2, which are LLM-as-a-Judge for LLMs, LLM-as-a-Judge for data, and LLM-as-a-Judge for agent respectively."}, {"title": "2.4.1 LLM-as-a-Judge for model.", "content": "It is universally known that the best way to evaluate LLMs is human judgment, but collecting human annotations can be costly, time-consuming, and laborious [72, 137]. Using strong LLMs (usually closed-source ones, e.g., GPT-4, Claude, ChatGPT) as an automated proxy for assessing LLMs has become a natural choice [139]. With appropriate prompt design, the quality of evaluation and agreement to human judgment can be promising [19, 106, 131, 137]. However, the cost concern still exists when calling the APIs of these proprietary models, especially when there is a frequent need for model validation on large-scale data. Moreover, closed-source LLM-as-a-Judge leads to low reproducibility due to potential changes in models behind the API. Some recent works have started to make attempts for open-source alternatives. SelFee [121] collects generations, feedback, and revised generations from ChatGPT and fine-tunes LLaMA models to build a critique model. Shepherd [107] trains a model that can output critiques for single-response with the data of feedback from online communities and human annotation. PandaLM [110] trains a model to conduct pairwise comparison for LLM Instruction Tuning Optimization, and Zheng et al. [137] also fine-tune Vicuna [101] on a 20K pairwise comparison dataset to explore the potential of open-source models as a more cost-friendly proxy.\nRecent advancements in using Large Multimodal Models (LMMs) as evaluators have showcased their potential to perform complex judgment tasks in vision-language scenarios. Proprietary models like GPT-4V and GPT-4o have been pivotal in benchmarks such as detailed captioning and visual chats, utilizing both pointwise and pairwise evaluation methods [58, 65, 130]. Open-source alternatives have emerged, with Prometheus-Vision [47] being the first vision-language model specifically trained to act as an evaluator for user-designed scoring criteria. While Prometheus-Vision introduced the concept of open-source evaluators with a focus on specialized tasks, it remains limited to predefined criteria. In contrast, LLaVA-Critic [117], another open-source innovation, expands the scope by serving as a generalist evaluator. Trained on diverse and detailed datasets, LLaVA-Critic provides robust scoring and preference learning, closely aligning with human and proprietary evaluations. These models mark significant progress in democratizing and enhancing multimodal evaluation tools."}, {"title": "2.4.2 LLM-as-a-Judge for data.", "content": "Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of LLMs presents an unprecedented opportunity to automate the complicated process of data annotation by LLM-as-a-Judge. Most of the data need to be evaluated by LLM-as-a-Judge is generated by models, or large-scale crawled data. Language models first conduct supervised fine-tuning to imitate how to align with human instructions [95, 109]. After that, reinforcement learning techniques have beenexplored to align language models with human preferences [73, 79]. The most successful way is applying a RLHF framework [73] via training a reward model on human feedback and using PPO [83] to obtain the policy model for language generation. However, in practices, the PPO training paradigm is complex in coding and hyper-parameter tuning while it needs four models that are hard for training. This motivates us to explore simpler and more straightforward methods to align language models with human preferences. This involves how to use LLM-as-a-Judge to evaluate whether different responses are aligned with human preferences. For example, [17, 124] use general LLM (ChatGPT) to get better alignment with human preferences. The Aplaca prompts [95] is used as sampling queries to different models generate responses. And these data was evaluated by LLM-as-a-Judge to obtain human preference scores (reward score) to train a new language model. Other works would like to use Supervised Fine-Tuning (SFT) model itself as evaluator, like generating better-aligned datasets for SFT including hindsight-modified prompts [59, 129] and principle-driven self-alignment [94].\nIn addition, the lack of domain-specific model training data is a common phenomenon. In order to obtain annotated high-quality data, it is also very common to use LLM-as-a-Judge for the generation and evaluation of domain data. WizardMath [66] would use its Instruction Reward Model (IRM) as Evaluator, aiming to judge the quality of the evolved instructions on three aspects: i) Definition, ii) Precision, and iti) Integrity. To produce the ranking list training data of IRM, for each instruction, ChatGPT and Wizard-E are used to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions.\nRecent research on evaluating multimodal data focuses on addressing vision-language misalign-ments in Multimodal Large Language Models (MLLMs), which often cause hallucinations\u2014outputs inconsistent with visual or contextual evidence [15, 54, 104]. Techniques like Reinforcement Learn-ing from Human Feedback (RLHF) and Factually Augmented RLHF have been employed to improve model alignment by incorporating structured ground-truth data and image captions, enhancing hallucination detection [93]. Benchmarks such as MLLM-as-a-Judge [9] assess these models using tasks like scoring, pair comparison, and batch ranking, revealing limitations in alignment with human preferences. Persistent issues include biases (e.g., position, verbosity) and hallucinations,"}, {"title": "2.4.3 LLM-as-a-Judge for agent.", "content": "There are two ways to apply LLM-as-a-Judge for an agent. One is to evaluate the entire process of the intelligent agent [145], and the other is to evaluate it at a specific stage in the agent framework process [27, 86]. Both approaches are briefly illustrated in Figure 7. Using LLM as the brain of agent, an agentic system [145] could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort. In addition, the agent [86] can interact with the environment through language and receive feedback on actions through LLM to make decisions for the next action."}, {"title": "2.5 Quick Practice", "content": "To effectively apply LLM-as-a-Judge design, it is more recommended to find more effective set-tings in the testing cycle for different scenarios. The process of quick practice for LLM-as-a-Judge involves four main stages. First, thinking, where users define the evaluation objectives by determin-ing what needs to be evaluated, understanding how humans typically perform such evaluations, and identifying some reliable evaluation examples. Next is prompt design, detailed in Section 2.1. The most efficient and generally effective approach involves specifying scoring dimensions, em-phasizing relative comparisons for improved assessments, and creating effective examples to guide the LLM. The third stage, model selection (Section 2.2), focuses on choosing a large-scale model with strong reasoning and instruction-following abilities to ensure reliable evaluations. Finally, standardizing the evaluation process ensures that the outputs are structured (Section 2.3). This can be achieved by using specific formats like \boxed{XX}, numerical scores, or binary responses (e.g., \"Yes\" or \"No\"). The entire process includes iterative testing with cases and refinement through retesting to enhance reliability."}, {"title": "3 IMPROVEMENT STRATEGY", "content": "When directly utilizing LLMs to conduct evaluation tasks such as scoring, selection, pairwise comparison or ranking, the inherent biases of LLMs like length bias, positional bias and concreteness bias[75] will lead to poor evaluation results. Addressing these inherent biases and improving theoverall evaluation performance of LLMs is a critical challenge for applying LLMs as evaluators. In this section, we introduce three improvement strategy aimed at enhancing the evaluation performance of LLM-as-a-judge: design strategy of evaluation prompts (in-context learning based), improvement strategy of LLMs' evaluation capabilities (model based), and optimization strategy of final evaluation results (post-processing based). Our categorization is based on the formal definition of LLM-as-Evaluator in Section 2, focusing on enhancing the evaluation effectiveness by targeting three key phases of the process: the context C, the abilities of LLMs themselves $P_{LLM}$ and the post processing \u2190 to obtain the final results &"}, {"title": "3.1 Design Strategy of Evaluation Prompts", "content": "An evaluation prompt is an input to LLM evaluators, which is used to guide the LLMs to complete the required evaluation tasks. LLMs possess in-context learning ability to learn how to perform specified tasks through relevant examples or instructions provided in prompts without requiring weight updates or retraining[7]. It indicates that the design strategy of evaluation prompts will significantly impact the effectiveness of LLM-as-a-judge. Therefore, how to optimize the design of evaluation prompts, including better methods to help LLMs understand the evaluation tasks and produce evaluation results, is the most direct and effective way to improve the evaluation performance of LLM-as-a-judge."}, {"title": "3.1.1 Optimizing LLMs' Understanding of Evaluation Tasks.", "content": "In optimization methods of prompting LLMs to better understand evaluation tasks, one of the most commonly used and effective approaches is few-shot prompting[7]. By incorporating several high-quality evaluation examples into the evaluation prompts, LLM evaluators can effectively grasp the objectives, general processes and rough evaluation criteria of evaluation tasks. Many research works employ this prompt paradigm for evaluation, such as FActScore[69], SALAD-Bench[52] and GPTScore[20].\nIn addition to providing hight-quality examples for LLMs to inference, refining the evaluation task instructions is also an effective approach to optimize LLMs' understanding of evaluation tasks. Current methods for refining evaluation tasks mainly including the decomposition of evaluation steps and criteria: (a) Decomposition of Evaluation Steps entails breaking down the entire evaluation tasks into smaller steps, providing detailed definitions and constraints for each smallstep in prompts, thereby guiding LLMs comprehensively through the whole evaluation pipeline. For instance, G-Eval[60] and DHP[111] use Chain-of-Thought(CoT)[113] to provide guidance for LLMs. SocREval[29] employs the Socratic method to meticulously design each step to enhance evaluation performance. Saha et al. proposes Branch-Solve-Merge(BSM)[81], which divides evaluation tasks into multiple parallel sub-tasks for separate evaluation and final merge. (b) Decomposition of Evaluation Criteria involves breaking down coarse evaluation criteria like Fluency into finer-grained sub-criteria like Grammar, Engagingness and Readability, and then generating overall scores based on these difference dimensions. HD-Eval[61] iteratively aligns LLM evaluators with human preference via hierarchical criteria decomposition and thereby addressing the potential bias in LLMs. Hu and Gao et al.[30] summarize and clearly define an explicit hierarchical classification system encompassing 11 criteria, addressing the issue of LLMs potentially confusing different evaluation standards. These refinements specific to enable LLMs to understand the details of evaluation tasks more deeply, thereby aligning evaluation results more closely with human evaluation requirements and preferences.\nFurthermore, the evaluation capabilities can be optimized based on specific shortcomings of LLMs in prompts. For instance, to address specific biases like position bias which is common in pairwise evaluations, several research efforts have optimized prompts design by randomly swapping contents to be evaluated. Wang et al.[105] analyzed and validated the impact of position bias on LLM-as-a-judge, and proposed a calibration framework to mitigate this bias by swapping the contents and averaging the scores. Auto-J[51] and JudgeLM[141] also enhance the evaluation consistency by shuffling the texts to be evaluated. In contrast to averaging scores, PandaLM[110] annotates the conflicting evaluation results after swapping as \"Tie\" to address the position bias.\nTo address the challenge of LLMs' absolute scoring being less robust than relative comparing[77], some research works convert scoring tasks into pairwise comparison, thereby enhancing the reliability of evaluation results. Liu et al.[62] transform the scoring evaluation to ranking evaluation and introduce Pairwise-Preference Search (PARIS), which employs LLMs to conduct pairwise comparisons locally and efficiently ranks candidate texts globally, making evaluation results more aligned with human preferences.\nIn summary, the design of prompts for better understanding evaluation tasks is a core method for optimizing LLMs' in-contextual learning abilities. By refining evaluation task instructions and criteria in prompts or few-shot prompting with high-quality examples, the details of evaluation prompts can be enriched and the understanding of LLMs on evaluation tasks can be directly or indirectly enhanced. Additionally, targeted adjustments to prompts can address potential biases of LLMs such as position bias."}, {"title": "3.1.2 Optimizing LLMs\u02bc Output Forms.", "content": "Directly requiring LLM evaluators to output evaluation results poses robustness problems. The response text may unexpectedly vary due to the inherent generative randomness of LLMs, such as outputting text like \"low relevance\" while asked to measure it with discrete scores, which hinders the automated and accurate extraction of evaluation results from LLMs' output. An effective"}]}