{"title": "UDITQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis", "authors": ["Zhiwei Chen", "Hao Tang"], "abstract": "Quantum computing is a transformative technology with wide-ranging applications, and efficient quantum circuit generation is crucial for unlocking its full potential. Current diffusion model approaches based on U-Net architectures, while promising, encounter challenges related to computational efficiency and modeling global context. To address these issues, we propose UDiT, a novel U-Net-style Diffusion Transformer architecture, which combines U-Net's strengths in multi-scale feature extraction with the Transformer's ability to model global context. Building upon this foundation, we introduce UDiTQC, an extension specifically designed for quantum circuit generation. We demonstrate the framework's effectiveness on two tasks: entanglement generation and unitary compilation, where UDiTQC consistently outperforms existing methods. Additionally, our framework supports tasks such as masking and editing circuits to meet specific physical property requirements. This dual advancement, improving quantum circuit synthesis and refining generative model architectures, marks a significant milestone in the convergence of quantum computing and machine learning research.", "sections": [{"title": "1. Introduction", "content": "Quantum computing is widely recognized as a transformative technology with great potential in fields ranging from fundamental physics research (Feynman, 2018) to applications in machine learning (Cerezo et al., 2022) and optimization (Farhi et al., 2014). As quantum processor technology continues to progress, the need for accurate and efficient quantum circuits has become increasingly critical. Given the constraints on available resources and gates, optimizing quantum circuit structures is essential. This requires advanced research and expertise to identify the most effective designs.\nSignificant progress in quantum circuit synthesis has been driven by artificial intelligence-based generative methods. These include approaches for quantum state preparation (Arrazola et al., 2019; Bolens & Heyl, 2021), circuit optimization (F\u00f6sel et al., 2021; Ostaszewski et al., 2021), and unitary compilation (Zhang et al., 2020; Moro et al., 2021). Among these methods, denoising diffusion models (DMs), state-of-the-art machine learning generative techniques, have demonstrated their capability to generate quantum circuits tailored to specific requirements (F\u00fcrrutter et al., 2024). By providing a prompt corresponding to a particular task or class, diffusion models can produce desired quantum circuit structures, making them broadly applicable to quantum computer design and optimization challenges.\nDespite their success, existing approaches (F\u00fcrrutter et al., 2024) that rely on U-Net architectures for diffusion models in quantum circuit generation face several limitations. These include significant computational demands, sensitivity to data distribution, and insufficient integration of global context information. The Diffusion Transformer (DiT) (Peebles & Xie, 2023) addresses these issues by embedding a full Transformer network within the diffusion model. This approach combines the global modeling strengths of Transformers with the stepwise generative capabilities of diffusion models. Replacing traditional U-Net architectures (Ho et al., 2020) with Transformer-based frameworks improves the ability to capture long-range data dependencies, thereby enhancing generation quality. Additionally, DiT's innovative use of time-step embeddings with input sequences offers remarkable flexibility for conditional generation and multi-modal tasks, yielding state-of-the-art results in high-resolution image generation and superior scalability in spatial generation tasks.\nTo further enhance quantum circuit generation, we propose a novel, practical DiT framework that improves upon conventional U-Net architectures by delivering greater efficiency and accuracy. Our approach retains U-Net's core strengths, particularly its encoder-decoder structure and multi-scale information handling through skip connections. Building on these advantages, we introduce the U-Net-style Diffusion Transformer (UDiT) architecture, which integrates the complementary strengths of both architectures while addressing their respective weaknesses. This combined approach provides improved efficiency, accuracy, and robustness, making it better suited to handle complex practical scenarios and enabling broader applications.\nTo validate our approach, we apply the UDiT architecture to quantum circuit design. Our proposed framework, UDiTQC, outperforms the baseline GenQC method (F\u00fcrrutter et al., 2024) in generating quantum circuits, achieving higher accuracy across a range of metrics. UDiTQC successfully produces circuits for various qubit configurations, generating designs with diverse degrees of entanglement and maintaining precise control over physical properties and constraints. The framework also excels at targeted circuit editing and masked circuit generation, meeting specific physical constraints with exceptional flexibility. Furthermore, we demonstrate the practical utility of the UDiT model by training it for unitary compilation with limited gate sets, showcasing its applicability under real-world quantum computing constraints. By achieving enhanced precision across diverse qubit configurations and maintaining operational flexibility, our architecture not only advances quantum circuit design methodologies but also establishes a new paradigm in diffusion model architectures."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Quantum Synthesis", "content": "The automated design of quantum experiments, particularly for generating novel entangled states, has emerged as a transformative advancement in quantum physics. Within this domain, unitary compilation represents a fundamental challenge that directly impacts the practical implementation of quantum algorithms. Recent approaches have made significant strides in addressing these challenges. The GenQC framework (F\u00fcrrutter et al., 2024) introduces an innovative solution by leveraging conditional diffusion models to generate target quantum circuits through specific prompts. Additionally, reinforcement learning techniques have shown effectiveness in optimizing gate placement within quantum circuits, particularly for specialized tasks such as unitary synthesis (Rietsch et al., 2024). These methodologies not only advance the field of quantum circuit design, but also establish new paradigms for quantum information processing and experimental physics, offering scalable solutions for increasingly complex quantum systems."}, {"title": "2.2. Diffusion Models", "content": "Diffusion models present a powerful class of generative models that create data by progressively adding noise and then reversing the process to recover the original structure. With their high flexibility, exceptional generative quality, and robust stability, diffusion models have found widespread applications in scientific fields such as image generation and editing (Dhariwal & Nichol, 2021), audio synthesis (Kong et al., 2020), molecular structure design (Hoogeboom et al., 2022).\nThe denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) established a notable advancement with its foundational framework, while parallel innovations in score-based generative models (Song & Ermon, 2019) and stochastic differential equation (SDE)-based approaches (Song et al., 2020b) have further advanced the state of the art, particularly in image synthesis. Significant improvements in generation quality and computational efficiency have been achieved through enhanced sampling strategies, such as DDIM (Song et al., 2020a) and EDM (Karras et al., 2022), alongside classifier-free guidance techniques (Ho & Salimans, 2022). Moreover, the introduction of latent diffusion models (Rombach et al., 2022) marked another significant advance, incorporating data compression principles to optimize efficiency.\nAlthough early implementations predominantly relied on U-Net architectures (Dhariwal & Nichol, 2021), the computational demands of their attention mechanisms have motivated the exploration of alternative approaches. Vision Transformers (ViT) (Dosovitskiy, 2020) have emerged as a compelling alternative that offers superior scalability and enhanced modeling of long-range dependencies. These architectural innovations have significantly expanded the practical applications of diffusion models, pushing the boundaries of generative AI capabilities."}, {"title": "2.3. Diffusion Transformer", "content": "Diffusion Transformers (DiT) (Peebles & Xie, 2023) represent a paradigm shift in diffusion models by introducing the Transformer architecture as an alternative to the conventional U-Net backbone. This architectural innovation harnesses the Transformer's inherent capability to model global dependencies, resulting in enhanced generation quality and increased adaptability for conditional and multi-modal generation tasks.\nBuilding upon the foundational DiT framework, recent research has further refined its training and architectural design. For instance, FiT (Lu et al., 2024) and VisionLLaMA (Chu et al., 2024) incorporate advanced techniques inspired by large language models (LLMs), such as RoPE2D and SwishGLU, to enhance the capabilities of Diffusion Transformers. Such progressive developments demonstrate the framework's extensibility and scalability, positioning DiT as a state-of-the-art methodology in generative modeling across a diverse spectrum of applications."}, {"title": "3. Methodology", "content": "Our objective is to develop more efficient diffusion model architectures. To this end, we first propose the U-Net-style Diffusion Transformer architecture (UDiT), which combines U-Net's strengths in multi-scale feature extraction with the Transformer's ability to model global context. Building on this, we introduce UDiTQC, a framework designed for quantum circuit generation."}, {"title": "3.1. The Proposed UDIT", "content": ""}, {"title": "3.1.1. DIFFUSION MODELS", "content": "Before introducing our architecture, we briefly review the process of denoising diffusion probabilistic models (DDPMs). It starts from a stochastic process where an initial sample xo is gradually corrupted by noise, transforming it into a simpler, noise-dominated state. This forward noising process can be represented as follows:\n$q(x_T|x_0) = \\prod_{t=1}^{T}q(x_t|x_{t-1}),$\n$q(x_t|x_0) = N(x_t; \\sqrt{\\bar{a}_t}x_0, (1 - \\bar{a}_t)I),$\nwhere {xt}t=1T denotes a sequence of noised samples from time t = 1 to t = T. Then, DDPM learns the reverse process that recovers the original sample utilizing learned \u03bc\u03b8 and \u03a3\u03b8:\n$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t), \\sigma_\\theta(x_t)),$\nwhere \u03b8 represent the parameters of the denoiser, and are trained to minimize the mean squared error between \u03f5\u03b8(xt) and the true Gaussian noise \u03f5t: min\u03b8 ||\u03f5\u03b8(xt) - \u03f5t ||2. Once p\u03b8 is trained, new samples can be sampled by initializing xt ~ N(0, I), and sampling xt\u22121 ~ p\u03b8(xt\u22121|xt) via the reparameterization trick."}, {"title": "3.1.2. ARCHITECTURE", "content": "We propose a novel architecture, the U-Net-style Diffusion Transformer (UDiT), which combines the U-Net structure with the Diffusion Transformer, as shown in Figure 1. After corrupting the samples in the training set with varying levels of Gaussian noise and vectorizing to fixed-length sequences, the input tokens are processed by a series of Diffusion Transformer layers composed of multiple DiT blocks. In addition to noised inputs, the diffusion model meanwhile handles additional conditional information, such as the noise timesteps t, class labels c, natural language, etc.\nDrawing inspiration from the original U-Net design, UDiT features an \u201cencoder-decoder\u201d architecture. The encoder compresses sequence length through downsampling operations, while the decoder restores the sequence to its original size via upsampling. We designate five distinct stages in the process, illustrated in Figure 1b: the sequence is downsampled twice during encoding to reduce the sequence length, and in the decoding stage, the sequence is upsampled to restore its original size. At each stage, each DiT layer is composed of multiple DiT blocks (Figure 1c), with the block size changing as the downsampling and upsampling operations across stages. Inspired by the Residual U-ViT designs (Hoogeboom et al., 2023; 2024), we remove the block-wise skip-connections and instead employ residual connections (RC) between different layers, as shown in Figure 1d. Furthermore, we adopt an asymmetric network design, varying the number of DiT blocks across different stages."}, {"title": "Residual Connection.", "content": "In traditional U-Net architectures, skip-connections are typically used to bypass sub-sampled levels, such as those involving average pooling. However, this approach diverges significantly from the structure of large language models, which rely solely on residual connections without incorporating skip connections. To streamline the model structure, we replace conventional skip-connections with residual connections (Hoogeboom et al., 2024), as shown in Figure 1d. At each corresponding subsampling and upsampling operation, we introduce a simplified residual form of skip-connection, which effectively replaces the more complex traditional skip-connections for each stage.\nTo formalize the structure, let fa and fu represent the downsampling and upsampling stages of UDiT, respectively, and let fm denote the middle stage. The two-stage residual connection for UDiT can be expressed as follows:\n$f(x) = f_u (U (f_m(D(h)) \u2013 D(h)) + h),$\nwhere h = fa(x) and D and U correspond to the downsampling and upsampling operations, respectively. In this paper, we utilize a convolutional layer for downsampling to reduce the feature dimensionality, while upsampling is achieved using interpolation followed by a convolutional layer to restore the input feature dimensionality. Furthermore, in multi-stage architectures, the middle stage fm can itself recursively incorporate additional downsampling and upsampling stages, allowing the definition of hierarchical feature transformations across multiple levels of the UDiT architecture."}, {"title": "Assymetric Design.", "content": "In the proposed UDiT model, each stage incorporates a single skip-connection, allowing for the flexible use of varying numbers of DiT blocks at different stages. Leveraging the advantages of a decoder-only architecture, we shift computational focus from the encoder stage (the downsampling half of the model) to the decoder stage (the upsampling half of the model), resulting in a higher number of DiT blocks in the decoder. This design choice aligns with findings in prior studies (Vaswani, 2017; Hoogeboom et al., 2023), which demonstrate that decoder-dominated architectures tend to exhibit superior performance in generative tasks.\nBuilding upon this foundation, given that the sequence lengths vary across different stages, the temporal and class embedding dimensions in the DiT blocks can be set to fixed sizes. In addition, the feature dimensions and the number of attention heads can be customized for each stage. Specifically, intermediate layers can utilize higher feature dimensions and fewer attention heads to enhance precision, thereby balancing computational efficiency and model accuracy across the architecture."}, {"title": "3.2. The Proposed UDITQC", "content": "By integrating quantum circuit encoding-decoding methods and specific conditioning embeddings, we propose UDiTQC, a framework that uses the UDiT-based diffusion model to generate quantum circuits. The pipeline for UDiTQC is illustrated in Figure 2.\nTo enhance the efficiency and representational capacity of UDiTQC, we adopt a tensor-based representation for quantum circuits inspired by (F\u00f6sel et al., 2021). In this formulation, each quantum circuit is represented as a two-dimensional tensor, where the first dimension encodes the qubit indices, and the second dimension encodes the time steps for gate placement, constrained to one gate per time step as in Figure 2a. We fix an arbitrary-length representation prior to training, assigning each gate a randomly generated orthogonal high-dimensional continuous embedding. Using a vectorization strategy similar to the patchify mechanism in DiT (Peebles & Xie, 2023), we transform the tensor into a fixed-length sequence while simultaneously performing feature expansion. Next, we apply frequency-based positional embeddings (sine-cosine formulation) to encode temporal and spatial relationships within the circuit, enabling the transformer layers to model both local and global dependencies. During the decoding process, the generated quantum circuits are mapped to the predefined gate set by computing the cosine similarity between the described gate and the predefined gate embeddings, ensuring that the generated circuits align with the predefined gate set and comply with the physical constraints of quantum devices (see Appendix A for details)."}, {"title": "4. Experiments", "content": "We first evaluate the compatibility of the UDiT model through an ablation study, detailed in Appendix D. This study assesses various model designs based on 3-qubit entanglement generation tasks. The configurations tested include modifications to the architecture such as sequence-based gate embeddings, U-Net-style modifications, asymmetric structures, residual connections, and adjustments to the feature dimensions during downsampling and upsampling. The results from these experiments are summarized in Table 1, where we compare the training speed, average accuracy, and entanglement generation accuracy across different designs.\nHaving validated the effectiveness of the UDiT architecture, we then apply it to two distinct quantum circuit generation problems: entanglement generation and unitary compilation. The entanglement generation task serves as a benchmark for evaluating the model's potential across different scenarios, while the unitary compilation task represents a fundamental challenge in the field. In both tasks, the model is trained using the same denoising loss function, with different tasks achieved by modifying the training samples and their conditioning. The primary difference between the tasks lies in the labeling scheme, where task-specific label categorizations are applied, though the underlying setup remains consistent."}, {"title": "4.1. Entanglement Generation", "content": "Our objective is to generate quantum circuits with predefined entanglement states, characterized by their Schmidt Rank Vector (SRV) (Huber & De Vicente, 2013), which provides a quantitative representation of entanglement structure through a numerical vector that indicates the rank of reduced density matrices for each subsystem.\nTo evaluate the efficacy of our proposed method, we conducted extensive experiments focusing on quantum circuit generation with specific entanglement states. Our experimental framework encompassed circuits of varying qubit counts, analyzing their entanglement properties, and assessing the method's generalizability across different circuit configurations and lengths (with detail presented in Appendix \u0395).\nFollowing dataset balancing, we trained the UDiTQC model and conducted comprehensive experiments across various qubit configurations. For performance comparison, we replicated the GenQC experimental setup described in the literature (F\u00fcrrutter et al., 2024) using its corresponding code repository\u00b9 and trained with new datasets consistent with UDiTQC's training setup. The total number of circuits for each qubit count was approximately the same as reported, ensuring a fair and comprehensive comparison. During model training, the SRV of each circuit served as its class label, and all other training parameters were kept consistent across models. Upon completing training, we evaluated the model by generating circuits corresponding to various SRVs. The model evaluation involved generating circuits corresponding to diverse SRVs, with all possible SRV configurations serving as generation prompts. Performance comparison was based on the average precision of entanglement across identical qubit configurations.\nAs demonstrated in Figure 3, UDiTQC consistently achieved superior generation accuracy compared to GenQC across experiments involving 3 to 8 qubits. Figure 4 provides a detailed accuracy analysis for 5-qubit circuits, revealing successful entanglement state generation in the majority of cases, though accuracy exhibits a modest decline with increasing numbers of entangled qubits. This degradation can be attributed to the heightened complexity in both design and execution of quantum circuits requiring more gates to achieve higher degrees of entanglement. Significantly, UDiTQC demonstrates robust generalization capabilities, successfully generating novel circuits beyond the training set while maintaining diversity and adherence to target SRV constraints. This capacity for producing unique, previously unseen quantum circuits while preserving predetermined entanglement characteristics underscores the model's versatility and potential impact.\nUDiTQC demonstrates superior performance in generation accuracy, scalability, and efficient learning of gate placement patterns, which validates UDiTQC's effectiveness and generalizability in quantum circuit design. The model consistently respects predefined constraints, including gate set specifications and one-gate-per-time-step requirements."}, {"title": "4.2. Masking and Editing Circuits", "content": "Building upon entanglement generation, the quantum circuits corresponding to the tensors can be edited or masked, as proposed for image editing (Lugmayr et al., 2022). These operations enable two distinct functionalities: one prevents the model from placing gates in certain areas of the circuit, and the other imposes the presence of specific gates. These operations do not require any additional training, but rely solely on adding different conditions during inference on the entanglement generation model.\nMasking. The masking operation serves as a fundamental mechanism for imposing specific structural constraints on circuit generation. When operations must be performed between spatially distant qubits, they necessitate decomposition into a series of intermediate gates and exchange operations throughout the circuit, resulting in diminished gate fidelity (B\u00e4umer et al., 2024). These physical limitations, while crucial for practical implementation, are not inherently represented in conventional circuit diagrams, which suggest unrestricted qubit connectivity.\nAs illustrated in Figure 5a, we enforce specific physical constraints by masking designated sections of the model input tensor, effectively preventing gate placement within these masked regions. The masked tensor is then fed into the model along with the corresponding SRV label to generate the appropriate circuits. The masked tensor, in conjunction with the corresponding SRV label, is then processed by the model to generate circuits that comply with the specified constraints. This approach offers flexible control over fundamental circuit parameters, including qubit count and circuit length, through techniques such as complete row masking for specific qubits. The efficacy of this constraint enforcement mechanism is demonstrated in the lower portion of Figure 5a, where the generated quantum circuits explicitly satisfy the imposed physical limitations. While these constrained circuits may exhibit marginally reduced accuracy compared to unconstrained generation tasks, the model successfully maintains adherence to the specified physical constraints. This capability significantly enhances the practical applicability of our approach across diverse quantum computing scenarios.\nEditing. The editing operation facilitates the precise specification of predetermined gates throughout the circuit prior to initiating the diffusion process. This capability addresses practical scenarios where a quantum state of interest, represented by an initial circuit configuration, serves as the foundation for subsequent computational operations. As illustrated in the Figure 5b, this process begins with an input circuit comprising five gates in the initial state, with the objective of transforming it to achieve a target SRV while preserving the initial five-gate configuration.\nThe experimental results demonstrate successful constraint application in generating circuits that achieve the target SRV specifications (Figure 6). Notably, the method exhibits robust performance in tasks requiring increased entanglement (elements above the diagonal). However, significant challenges emerge in entanglement-reducing operations (elements below the diagonal). The reduced accuracy in these cases can be attributed to the complexity of the requisite gate sequence: the process necessitates applying multiple gates to the initial high-entanglement state before implementing the editing operations to achieve the desired low-"}, {"title": "4.3. Unitary Compilation", "content": "We train UDiTQC for a critical task in quantum circuit generation-compiling unitaries into circuits. We generate a variety of 3-qubit circuits of different lengths and gate sets, and compute their corresponding unitary matrices to construct the dataset. The conditions of the gate sets were used as the model's class labels, while the unitary compilation was integrated with an encoder trained jointly with the model.\nAfter training, new unitaries were input into the model to evaluate its compiling capability. During testing, we selected over five thousand unitary matrices that are not within the training set and compiled into 1024 circuits. Remarkably, UDiTQC achieved a compilation accuracy of 94.9%, outperforming GenQC, which achieved 92.6%, and the accuracy of the circuits generated for each unitary was consistently higher than that of GenQC. As shown in the Figure 7, the model was able to compile multiple distinct circuits for most unitary matrix instances, allowing prospective users to choose the most suitable circuit based on their needs. Further, we assessed the model's ability by calculating the Frobenius norm $||U_t \u2013 U_g||$ of the distance between the generated circuit's unitary Ug and the target unitary Ut. As shown in the right-hand figure, the majority of target unitary matrices were compiled with a Frobenius norm of zero, indicating perfect accuracy. Even in cases where the norm was nonzero, the distance was significantly smaller compared to that of randomly generated unitary matrices, demonstrating the model's precision in unitary compilation."}, {"title": "5. Conclusion", "content": "In this paper, we propose UDiT, a novel architecture that fundamentally reimagines the DiT framework through a U-Net-style design, incorporating residual connections and asymmetric structures to enhance the capabilities of diffusion models. This architecture successfully combines U-Net's multi-scale feature extraction with the global modeling advantages of Transformer. We further apply this architecture to quantum circuit synthesis, introducing UDiTQC, which leads to significant improvements in circuit conditioning and compilation generation. Through comprehensive evaluation, UDiTQC demonstrates superior performance in both entanglement generation and unitary compilation tasks, while also supporting advanced circuit manipulation capabilities including masking and editing, achieving higher accuracy and efficiency compared to the state-of-the-art GenQC method.\nThe success of our framework extends beyond quantum applications, offering promising directions for fields where both local feature extraction and global context modeling are crucial. Future research will focus on expanding the UDiT model to a wider range of applications and exploring the transition from gate-based to measurement-based quantum computing paradigms. This dual contribution - which advances both quantum circuit synthesis and generative model architectures - marks a significant step forward at the intersection of quantum computing and machine learning research."}, {"title": "Impact Statement", "content": "This work advances machine learning techniques for quantum computing, improving the generation and compilation of quantum circuits. The methods developed have the potential to enhance quantum algorithm efficiency, with future applications in cryptography, optimization, and material science, driving progress in quantum technologies. While there are many potential societal consequences of our work, we feel none need to be specifically highlighted here at this stage."}, {"title": "A. Quantum Circuit Encoding", "content": "In this section, we present the procedure for encoding quantum circuits into continuous tensors, which can subsequently be fed into neural networks for processing. we first address the embedding of quantum gates, as illustrated in the Figure 8a. For single-qubit gates, the corresponding embedding is represented by the numerical value associated with the qubit. For multi-qubit gates, the control and target nodes are described using the same numerical embedding, but with opposite signs. This approach ensures that both the control and target nodes of multi-qubit gates are captured in the embedding, reflecting their respective roles within the gate.\nBuilding upon this, the quantum circuit is embedded into a two-dimensional tensor, where the first dimension corresponds to the qubits and the second dimension corresponds to time (i.e., the sequence of gates). This tensor can be expanded to accommodate any time length (the number of gates) and any spatial size (the number of qubits). The flexibility in the size of the tensor ensures that the embedding can represent quantum circuits with arbitrary complexity, while preserving the integrity of the circuit's structure.\nThis encoding method guarantees both the completeness and flexibility of the quantum circuit embedding, allowing it to be applied across a wide range of circuit configurations. As a result, the proposed encoding approach is not limited to quantum computing but can be extended to other applications that involve time and space structured data."}, {"title": "b) Quantum circuit encoding", "content": "In the decoding process, the first step involves converting the continuous tensor generated by the model back into a tokenized form. We select the token k corresponding to the embedding vk that has the highest cosine similarity with vgen:\n$k = arg max |S_c (V_k, V_{gen})|.$\nTo resolve the node types, the second step involves using the formula below:\n$k = k \\cdot sign S_c(V, V_{gen}).$\nThis calculation is performed for each spatiotemporal position in the tensor encoding, returning a tokenized integer matrix that precisely describes a valid circuit. Error circuits refer to cases where the model places two or more gates at the same"}, {"title": "B. Conditioning", "content": "In this section, we provide a detailed description of the conditional encoding methods used in quantum circuits. Since UDiT is a conditional diffusion model, it is necessary to embed the corresponding labels or features of the circuits into the model during training to guide the diffusion model's learning process."}, {"title": "B.1. Label Embedding", "content": "For the entanglement generation task, the goal is to generate circuits that produce specific types of entanglement given the circuit size. The criterion for entanglement is the SRV (Schmidt Rank Vector) of the circuit. Therefore, in this task, the SRV serves as the label for the circuits in the dataset. Since the SRV is represented as a numerical vector, where each position indicates whether the entanglement state is 1 or 2, the circuits corresponding to a fixed set of qubits have a finite number of representations. These representations can be numbered, allowing the label in the data to be transformed into a class label.\nFor the unitary compilation task, different subsets of compilation gates correspond to different circuits. Therefore, the selected gate subset can serve as the class label for this task. For a given gate set, we can generate corresponding sub-sets in lexicographical order, which are then mapped to integer labels.\nOnce these labels are converted into class labels, they are embedded into continuous vector spaces using a LabelEmbedder. Additionally, label dropout is employed to simulate classifier-free guidance during training."}, {"title": "B.2. Unitary Embedding", "content": "The second part focuses on the embedding of unitary matrices in the unitary compilation task, referred to as U-enc. The U-enc implementation starts by creating an input tensor from the unitary matrix. The real and imaginary parts of the matrix are separated into two channels, which are then fed into the unitary encoder. After an initial convolutional layer, a 2D positional encoding layer is introduced to encode the absolute positions of the unitary elements. A Transformer encoder with self-attention layers is employed to capture global dependencies. This global attention mechanism is crucial for handling unitary matrices, as the information they contain is non-local. Between the attention blocks, a downsampling layer with a kernel size of 2x2 is introduced, followed by convolutional layers that expand the unitary condition into the corresponding hidden dimensions.\nThis approach ensures that the unitary condition is efficiently processed and integrated into the model, enabling effective learning of unitary transformations in the compilation task."}, {"title": "C. Training and Inference", "content": ""}, {"title": "C.1. Training", "content": "We train the UDiT model based on the denoising diffusion probabilistic models (DDPM) framework. Specifically, we parameterize the model as an \u03f5-predictor. At each time step t, the model learns to predict the noise term \u03f5t of the noisy tensor $x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 \u2013 \\bar{a}_t}\u03f5_t$, where \u03f5t ~ N(0, I), x0 is a sample from the training dataset with distribution q(x0), and $\u221a\\bar{a}_t$ denotes the variance schedule. We define the cumulative variance as $\\bar{a}_t = \\prod_{i=0}^{t-1}(1 \u2013 \u03b2_i) = \\prod_{i=0}^{t-1}\u03c0(1 \u2013 \u03b2_i)$.\nThe denoising model, with parameters \u03b8, predicts the noise \u03f5 conditioned on an additional context c. The model parameters are optimized to minimize the following loss function:\n$L = E_{t~U[0,T],x_0~q(x_0),\u20ac~N (0,1)} [|| \u03f5_t - \u03f5_\\theta (x_t, t, c)||^2],$"}, {"title": "C.2. Inference", "content": "After training, we sample (infer) new circuits from the model across various scenarios. To sample a new circuit, we provide the model with a condition c and a noise input tensor xy ~ N(0, I). The noise tensor influences the size of the output circuit, specifically the number of qubits and the maximum number of gates that the model can place.\nSince the model has been trained on circuits with various padding configurations, it can effectively place padding tokens when needed. Therefore, a reasonable strategy is to provide the model with a sufficiently long tensor and allow it to limit the total number of gates by placing padding tokens as necessary. For the sampling method, we employ the DDPM sampler with a reduced number of denoising steps, as introduced by the Denoising Diffusion Implicit Models (DDIM).\nConditional diffusion models incorporate additional information, such as a class label c, to guide the generative process. To sample according to the given condition, we employ rescaled classifier-free guidance (CFG) during inference. In this framework, the reverse process is presented as $p_\\theta(X_{t-1}|X_t, c)$, where both \u03f5\u03b8 and \u03a3\u03b8 are conditioned on c. By interpreting the output of diffusion models as the score function, classifier-free guidance can be leveraged to direct the DDPM sampling procedure toward finding x that maximize log p(c|x) by:\n$\u20ac_\\theta(x_t, C) = \u20ac_\\theta(x_t, \u00d8) + s \\cdot \u2207_x log p(x|c) \\propto \u20ac_\\theta(x_t, \u00d8) + s \\cdot (\u03f5_\\theta(x_t, C) \u2212 \u03f5_\\theta(x_t, \u00d8)),$\nwhere s > 1 indicates the scale of the guidance, with s = 1 recovering the standard sampling process. To evaluate the diffusion model under c = (\u00d8, class labels c are randomly dropped out during training and replaced with a learned \"null\" embedding \u00d8. For all experiments, we select cfg_scale = 7.5.\nFor the entanglement generation task, including masking and editing, we generate 1024 new circuits for each SRV condition using 100 denoising steps in all tests to evaluate accuracy. In unitary compilation inference, for each given unitary, we generate 1024 circuits with 50 denoising steps to compare the results.\nWe observe that the masking task is more challenging than the other tasks due to the strict constraints applied to the generated circuits. Specifically, we notice an increase in the number of erroneous circuits, with the error rate rising from less than 1% in the non-masked entanglement generation task to approximately 80% in this case. Furthermore, the number of samples required to find an appropriate solution varies with the severity of the masking constraint. In contrast, the editing task is less stringent, only masking the first few gates of the quantum circuit. The model can still generate valid circuits, but the accuracy of the generated SRV corresponding to the target decreases compared to when no editing constraint is applied."}, {"title": "D. Ablation Study", "content": "In this section, we evaluate the contribution of individual components in UDiT, starting with a base experiment on 3-qubit entanglement generation. Table 1 presents the results of the ablation study, comparing the performance of various model configurations in terms of training speed, average accuracy, and the accuracy of generating fully entangled states."}, {"title": "E. Datasets", "content": "In this section, we describe the dataset generation process for the tasks of entanglement generation and unitary compilation, as discussed in this work."}, {"title": "E.1. Generation of Random Circuits", "content": "For the entanglement generation task, we generate random circuits for a fixed number of qubits. First, we sample the number of gates to be placed in the circuit from a uniform distribution with a specified lower and upper bound. Then, we sample the gates to be placed in the circuit from a predefined gate pool. After constructing the circuit, we compute its corresponding SRV (Schmidt Rank Vector), which is converted into prompt such as \"Generated SRV: [1,2,2]\". These SRVs are then embedded as labels and fed into the UDiT model.\nFor the unitary compilation task, we generate all possible subsets of gates from the gate pool using lexicographical ordering, where these subsets correspond to the labels in UDiT. Based on the chosen gate subsets, circuits are generated randomly for a given number of qubits and gates, and the corresponding unitary is computed as an additional condition for UDiT."}, {"title": "E.2. Circuit Optimization", "content": "We employ Qiskit's Qiskit.compiler.transpile function to optimize all the generated random circuits. This optimization step primarily removes consecutive redundant gates, thereby improving the efficiency of the circuits and enhancing the overall quality of the dataset. Training the model with this optimized dataset significantly improves its accuracy. After optimization, we also remove all duplicate circuits to further enhance the diversity and quality of the dataset."}, {"title": "E.3. Dataset Balancing", "content": "For the entanglement generation"}]}