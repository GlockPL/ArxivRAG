{"title": "ACCUMULATOR-AWARE POST-TRAINING QUANTIZATION", "authors": ["Ian Colbert", "Fabian Grob", "Giuseppe Franco", "Jinjie Zhang", "Rayan Saab"], "abstract": "Several recent studies have investigated low-precision accumulation, reporting improvements in throughput, power, and area across various platforms. However, the accompanying proposals have only considered the quantization-aware training (QAT) paradigm, in which models are fine-tuned or trained from scratch with quantization in the loop. As models continue to grow in size, QAT tech-niques become increasingly more expensive, which has motivated the recent surge in post-training quantization (PTQ) research. To the best of our knowledge, ours marks the first formal study of accumulator-aware quantization in the PTQ setting. To bridge this gap, we introduce AXE\u2014a practical framework of accumulator-aware extensions designed to endow overflow avoidance guarantees to existing layer-wise PTQ algorithms. We theoretically motivate AXE and demonstrate its flex-ibility by implementing it on top of two state-of-the-art PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage accumulation for the first time, opening the door for full datapath optimization and scaling to large language models (LLMs). We evaluate AXE across image classification and language generation models, and observe significant improvements in the trade-off between accumulator bit width and model accuracy over baseline methods.", "sections": [{"title": "Introduction", "content": "Modern deep learning models have scaled to use billions of parameters, requiring billions (or even trillions) of multiply-accumulate (MAC) operations during inference. Their enormous size presents a major obstacle to their deployment as their compute and memory requirements during inference often exceed the budgets of real-world sys-tems. As a result, model compression has emerged as an active area in deep learning research, with quantization being among the most prevalent techniques studied in literature [1, 2, 3] and applied in practice [4, 5, 6].\nQuantization techniques commonly reduce the inference costs of a deep learning model by restricting the precision of its weights and activations. Although substituting the standard 32-bit floating-point operands for low-precision counterparts can drastically reduce the cost of multiplications, this only accounts for part of the core MAC operation; the resulting products are often still accumulated using 32-bit additions. Recent studies have demonstrated that also restricting the precision of the accumulator can yield significant benefits [7, 8, 9, 10, 11]. For example, de Bruin et al. [8] and Xie et al. [9] both show that 16-bit integer accumulation on ARM processors can yield nearly a 2\u00d7 throughput increase over 32-bit, and Ni et al. [7] show that further reducing to 8-bit accumulation on custom ASICS can improve energy efficiency by over 4x. However, exploiting such an optimization is non-trivial in practice as reducing the width of the accumulator exponentially increases the risk of numerical overflow, which is known to introduce arithmetic errors that significantly degrade model accuracy [7, 12].\nTo address this, recent work has proposed an accumulator-aware quantization paradigm that entirely eliminates the risk of numerical overflow via strict learning constraints informed by theoretical guarantees [12]. The resulting scope of investigations has been limited to the quantization-aware training (QAT) setting in which models are trained from scratch or fine-tuned from checkpoints with quantization in the loop [12, 13]. With the rising training costs of modern deep learning models (e.g., large language models), it is important to develop methods that are equally as effective in the post-training quantization (PTQ) setting, where pre-trained models are directly quantized and calibrated using relatively modest resources. However, as further discussed in Section 2, controlling the accumulator bit width in such a scenario is non-trivial. In this work, we characterize and address these challenges, introduce a practical framework for their investigation, and establish a new state-of-the-art for accumulator-aware quantization in the PTQ setting."}, {"title": "Preliminaries", "content": "We denote the intermediate activations of a neural network with L layers as {x(l)}Ll=1, where x(l) \u2208 RKl denotes the Kl-dimensional input activations to layer l, and X (l) \u2208 RKl\u00d7D denotes a matrix of D such inputs. The set of weights for the network is similarly denoted as {W(l)}Ll=1, where W(l) \u2208 RCl\u00d7Kl denotes the weight matrix for layer l with Kl input neurons and Cl output neurons; its quantized counterpart is denoted as Q(l) \u2208 AC l\u00d7Kl, where we adopt the notation Am\u00d7n b to denote the space of all m \u00d7 n matrices whose elements are part of a fixed b-bit alphabet defined by the target quantization space. For example, Ab := {k : \u22122b\u22121 + 1 \u2264 k \u2264 2b\u22121 \u2212 1, k \u2208 Z} is the alphabet of signed b-bit integers, assuming a sign-magnitude representation, where Z is the space of all scalar integers.\nFor layer l, our notation yields Cl independent dot products of depth Kl for each of the D inputs. For clarity, and without loss of generality, we often assume Cl = 1 when focusing on a single layer l so that we can use w(l) to denote the weight matrix for layer l, with wl i denoting a single weight element. When dropping their superscript, x and w denote generic inputs and weights in RK, and q and q denote their quantized counterparts.\n2.1 Post-Training Quantization\nStandard quantization operators, referred to as quantizers, are commonly parameterized by zero-point z and strictly positive scaling factor s, as shown in Eq. 1 for weight tensor w. Our work focuses on uniform integer quantization, where z is an integer value that ensures that zero is exactly represented in the quantized domain, and s is a strictly positive scalar that corresponds to the resolution of the quantizer. Scaled values are commonly rounded to the nearest integer, denoted by [\u00b7], and elements that exceed the representation range of the quantized domain Ab are clipped.\nQ(w) := s \u00b7 (clip( w/s + z; minAb, maxAb) \u2212 z) \nMethods for tuning these quantizers broadly fall into two paradigms: quantization-aware training (QAT) and post-training quantization (PTQ). QAT methods train or fine-tune a neural network with quantization in the loop, which often requires significant compute resources and sufficiently large datasets [2, 3]. Our work focuses on PTQ methods, which directly cast and calibrate pre-trained models and often rely on little to no data without end-to-end training [2, 3]. PTQ methods tend to follow a similar general structure [14, 17, 19], greedily casting and calibrating quantized models layer-by-layer or block-by-block while seeking to approximate the minimizer of the reconstruction error\nq* = argminq 1/2||XT w \u2212 Xq||2 \nwhere q* is the optimal quantized weights and X is the quantized counterpart of X. Recent PTQ methods concentrate on \u201cweight-only quantization\u201d, where X = X, to solely minimize memory storage and transfer costs [14, 17, 19], and for good reason\u2014the ever-increasing weight volume of state-of-the-art neural networks has rendered many hyper-scale transformer models memory-bound [20, 18, 21, 22]. In such a scenario, weight-only quantization algorithms can better preserve model quality and still realize end-to-end throughput gains just by reducing data transfer costs, even with high-precision computations (usually FP16) [17, 23]. However, weight-only quantization provides limited opportunity to accelerate compute-intensive operations such as matrix multiplications, which is the focus of this work. Thus, we investigate methods that are amenable to quantizing both weights and activations to low precision integers, which can realize throughput gains from both accelerated computation and reduced data traffic [24, 25]. Some methods aim to solve this problem by tuning quantization parameters through iterative graph equalization [26, 27, 24, 28] or local gradient-based reconstruction [29, 30, 31], other methods such as GPFQ [14] and OPTQ [17] (formally GPTQ) quantize weights one-by-one while greedily adjusting for quantization error. We find that these greedy sequential quantization algorithms are particularly amenable to accumulator-awareness, as we discuss further in Section 3.\n2.2 Low-Precision Accumulation\nThe majority of neural network quantization research targeting compute acceleration emphasizes low-precision weights and activations. While this can significantly reduce the costs of multiplications, the resulting products are often still accumulated using high-precision additions. As lower precision integer representations continue to increase in popularity [32, 33], one can expect a focus skewed towards weight and activation quantization to yield diminishing returns as high-precision additions can bottleneck throughput, power, and area [7, 8, 9, 13]. For example, Ni et al. [7] show that when constraining weights and activations to 3-bit \u00d7 1-bit multipliers, the cost of 32-bit accumulation con-sumes nearly 75% of the total power and 90% of the total area of their custom MAC unit; they report up to 4\u00d7 power savings and 5\u00d7 area reduction when reducing to 8-bit accumulation."}, {"title": "Accumulator-Aware Quantization", "content": "Reducing the accumulator bit width is non-trivial in practice as it exponentially increases the risk of numerical over-flow [12], often introducing arithmetic errors that degrade model accuracy [7, 12]. Existing methods to prepare quan-tized neural networks (QNNs) for low-precision accumulation often aim to either reduce the risk of numerical over-flow [9, 34, 35] or mitigate its impact on model accuracy [7, 36, 37]. These empirical approaches rely on several assumptions that limit their real-world applicability. For one, as discussed in [13], empirical estimates of overflow rely on a priori knowledge of the input distribution, which is impractical to assume in many real-world scenarios and can even introduce vulnerabilities [38]. Furthermore, overflow behavior can vary across platforms and programming lan-guages, so designing methods to mitigate the detrimental impact of one particular overflow behavior (e.g., wraparound two's complement arithmetic) limits portability across applications and accelerators. Finally, empirical approaches are unable to support applications that require guaranteed arithmetic correctness, such as encrypted inference [39, 40], and are known to break down when overflows occur too frequently [7]. To address these concerns, recent work has proposed to avoid overflow altogether using accumulator-aware quantization (A2Q) [12, 13].\n2.3 Accumulator-Aware Quantization\nLet P* denote the minimum accumulator bit width required to guarantee overflow avoidance for a given dot product. Aside from universally fixing the accumulator at 32 bits (or any other arbitrary maximum number of bits imposed by a given processor), the most conservative method to calculate P* considers the data types of the dot product operands, i.e., weights and activations. Given inputs x \u2208 AN and weights q \u2208 AK M , P* is given by Eq. 3 as derived in [12], where 1signed(x) is an indicator function that returns 1 if x is signed and 0 otherwise.\nP\u2217 = log2(2log2(K)+N+M\u22121\u22121signed(x)+ 1)+1\nNote that P* increases linearly with the bit widths of the operands and logarithmically with the depth of the dot product. Thus, for a fixed neural architecture, one could heuristically manipulate the weight and activation bit widths according to Eq. 3 to reduce P*. However, the quantization design space ultimately limits the minimum attainable accumulator bit width, as well as the maximum attainable model accuracy for any target accumulator bit width [12, 13].\nColbert et al. [13] show that one can directly target the accumulator bit width as an independent dimension of the quantization design space while still theoretically guaranteeing overflow avoidance. When accumulating Tq into a signed P-bit accumulator, one need only constrain ||q||1 according to Eq. 4, assuming that \u2211 i qi = 0.\n||q||1 \u2264 2P\u22122/2N\u22121\nMotivated by this result, accumulator-aware QAT methods avoid overflow by constraining the l1-norm of weights during training to ultimately restrict the range of dot product outputs during inference [12, 13]. These approaches rely on weight normalization-based quantizers infused with strict accumulator-aware learning constraints. Although these approaches have yielded promising results in low-precision accumulation scenarios, the scope of their success is limited to the QAT setting [12, 13]. However, from this family of QAT methods, one can apply the Euclidean projection-based initialization strategy (EP-init) to the PTQ setting without modification as proposed in [13]. However, we find that EP-init has two shortcomings in the PTQ setting: (1) it universally relies on the round-to-zero rounding function to ensure that |Q(wi)| \u2264 |wi| for all i [12, 13]; and (2) it is a vector-wise projection operation that is not amenable to error correction (see Appendix C.2). We address both of these shortcomings in this work."}, {"title": "Accumulator-Aware Post-Training Quantization", "content": "The standard problem for neural network quantization aims to map high-precision values (e.g., 32-bit floating-point) to low-precision counterparts (e.g., 4-bit scaled integers) while locally minimizing the discrepancy between the output of the original model and that of the compressed one, as formalized by Eq. 2 in Section 2.1. In the post-training quantization (PTQ) setting, one often assumes the quantizer parameters (i.e., scaling factor s and zero point z) are fixed and that the individual weights can move freely, as in [14, 17, 41]. Building from this, we formalize accumulator-aware quantization as a constrained variant of the standard reconstruction problem in which the optimal quantized weights q* minimize local quantization error while also satisfying a strict l1-norm constraint, as defined below.\nq\u2217 = argminq 1/2||XT w \u2212 Xq||2 s.t. ||q||1 \u2264 Z\nTo approximately solve this accumulator-constrained reconstruction problem, we introduce AXE, a practical low-overhead framework of general accumulator-aware extensions that endow guaranteed overflow avoidance to layer-wise quantization algorithms that greedily assign bits element-by-element, e.g., GPFQ [14] and OPTQ [17]. AXE is"}, {"title": "Accumulator Constraints without Zero-Centering", "content": "built on two accumulator-aware constraints: (1) a soft global constraint that discourages the underlying algorithm from opportunistically selecting quantized weights with high magnitudes; and (2) a strict local constraint that greedily limits the range of each selected quantized weight while error is iteratively corrected. In its standard form, AXE applies these constraints per-channel (or per-neuron) so that each dot product in the network is guaranteed to independently avoid overflow. Furthermore, without violating our constraints, we generalize our framework to also support multi-stage accumulation in the form of tiled dot products by applying our constraints in finer granularities. We first theoretically justify our solution using GPFQ, then provide accumulator-aware variants of both GPFQ and OPTQ.\n3.1 Accumulator Constraints without Zero-Centering\nOur goal is to provide a theoretical guarantee of overflow avoidance when accumulating the dot product of q by any x \u2208 AN into a signed P-bit register. As discussed in Section 2.3, if q is a zero-centered vector such that \u2211 i qi = 0, then it is sufficient to constrain ||q||1 to satisfy the upper bound given by Eq. 4 in order to guarantee overflow avoidance. However, enforcing such a zero-centering constraint on a vector of integers is non-trivial in practice. Rather than directly enforcing this constraint on q, A2Q+ [13] enforces these constraints on its floating-point counterpart w and relies on the symmetry of the quantizer and the round-to-zero operator to ensure that |Q(wi)| \u2264 |wi| for all i. We detach our solution from the zero-centering, round-to-zero, and symmetrical constraints of A2Q+.\nFor any x \u2208 AN, each element xi lies within the closed interval [\u00b5, \u03bd] for all i = {1, . . . , K}, and \u03bd \u2212 \u00b5 = 2N \u2212 1. It follows that the maximizing vector, u = arg maxx xT q, and the minimizing vector, v = arg minx xT q, are:\nui =\n{\u03bd, where qi \u2265 0\n\u00b5, where qi < 0\nvi =\n{\u00b5, where qi \u2265 0\n\u03bd, where qi < 0\nFundamentally, to avoid overflow when accumulating xT q into a P-bit register, the result needs to fall within the register\u2019s representation range for any x \u2208 AN. Without loss of generality, we derive our algorithm assuming a sign-magnitude accumulator for clarity and conciseness. Thus, to safely use a signed P-bit accumulator without overflow, the following inequalities need to be satisfied:\nuT q < 2P\u22121 \u2212 1\n\u2212vT q < 2P\u22121 \u2212 1\nTo avoid zero-centering, one could generalize the result derived in [13] such that the bound relies on a variable center, e.g., \u2211 i qi = e. However, such a solution would still rely on the round-to-zero constraint. Furthermore, it precludes the use of greedy sequential algorithms where e would be unknown a priori and just as difficult to enforce as zero-centering, i.e., e = 0. Thus, rather than constraining the center, we greedily constrain the boundaries, as further discussed in Section 3.2."}, {"title": "Accumulator-Aware GPFQ", "content": "The greedy path following quantization (GPFQ) algorithm [14] approaches the standard quantization problem by traversing the neural network graph to sequentially quantize each element in each layer while iteratively correcting for quantization error. Formally, GPFQ sequentially quantizes {w(l) i}L,Kl i=1 to construct {q(l) i}L,Kl i=1 such that\n\u2211L i=1 ||XL i(w(l) i \u2212 q(l) i) + \u03a3 L i\u22121 j=1 X(l) j(w(l) j \u2212 q(l) j)||2 \nwhere X(l) i denotes samples for the i-th input neuron to the l-th layer assuming the first l \u2212 1 layers are quantized. At the l-th layer, this is done by greedily selecting each element qi to minimize the squared distance between the running sum \u03a3L i\u22121 j=1 w(l) j X(l) j and its analog \u03a3 L i\u22121 j=1 q(l) j X(l) j such that\nq(l) i = argminq\u2208AM ||XL i(w(l) i \u2212 q(l) i) + \u03a3 L i\u22121 j=1 X(l) j(w(l) j \u2212 q(l) j)||2"}, {"title": "AXE: Accumulator-Aware Extensions", "content": "where AM is an M-bit fixed alphabet defined by the target quantization space. For i = {1, . . . , Kl}, this simplifies to the following iteration rule as derived in [14], where u0 = 0.\n(l) i = Q( \u03c0\u03bb(w(l) i + X(l) i T \u03a3 L i\u22121 j=1X(l) j(w(l) j \u2212 q(l) j) /||X(l) i||2)\n(l)i T = u(l) i\u22121 + X(l) i T (w(l) i \u2212 q(l) i)\nTo add accumulator-awareness, we introduce two constraints that are agnostic to the symmetry of the quantizer and rounding function while still guaranteeing overflow avoidance. First, we introduce a soft l1-norm regularization penalty that discourages the underlying algorithm (e.g., GPFQ) from opportunistically selecting weights with high magnitudes. Second, we introduce a strict constraint that greedily limits the range of qi as error is iteratively corrected. This strict constraint is recursively applied element-by-element to ensure that Eqs. 7 and 8 are independently satisfied, which ultimately guarantees that Eq. 4 is satisfied without requiring a zero-centering constraint.\nSoft l1-norm regularization penalty. By design, greedy sequential quantization algorithms (e.g., GPFQ and OPTQ) opportunistically alter weights to correct for as much error as possible in each step, often yielding high-magnitude quantized weights. However, this is unfavorable in the accumulator-aware quantization setting as high-magnitude weights consume more of the l1-norm budget allocated per-channel (see Eq. 4). To address this, we penalize high-magnitude weights during error correction. We build from the sparse GPFQ formulation proposed by Zhang et al. [42] as given by Eq. 13; the solution is given by Eq. 14, where Ix(x) := sign(x)(|x| \u2212 \u03bb)+, (\u00b7)+ denotes the rectified linear unit (ReLU), and \u03bb > 0 is an arbitrary tuneable regularization parameter.\n= argminq\u2208AM ||XL i(w(l) i \u2212 q(l) i) + \u03a3 L i\u22121 j=1 X(l) j(w(l) j \u2212 q(l) j)||2 \u2212 \u03bb qi \n=\n2\u03bf \u03a0\u03bb( XL i(w(l) i \u2212 u(l) i\u22121) XT \u03a3 L i\u22121 j=1 X(l) j(w(l) j \u2212 q(l) j) /||X(l) i||2\nNoticeably, this formulation is perfectly amenable to leverage the Euclidean projection-based weight initialization (EP-init) strategy proposed in [13], which takes the same functional form. Thus, we tune our selection of \u03bb to be the optimal Lagrangian scalar derived from the solution to the constrained convex optimization problem formulated by Eq. 15. Here, the objective is to find the optimal Euclidean projection of w onto the l1 ball of radius Z, where Z is the scaled accumulator-aware l1-norm target given, up to a scaling, by the upper bound in Eq. 4. Thus, v\u2217 is the weight vector that minimizes the Euclidean projection onto the boundary of our constrained set before quantization.\nv\u2217 = minv ||v \u2212 w || subject to ||v||1 \u2264 Z\nDefine \u03c1 as the number of non-zero elements in the optimal solution and \u00b5 as the result of sorting w by magnitude in descending order. The Lagrange multiplier \u03bb associated with the solution to the optimization problem is given by\n\u03bb = 1/\u03c1(\u03a3 \u00b5i \u2212 Z)\nwhich can be interpreted as the average difference between our scaled accumulator-aware l1-norm target and the magnitudes of all non-zero elements in the optimal Euclidean projection v\u2217. We direct the reader to [13] and [43] for the associated proofs and derivations. It is important to note that because this projection is derived before quantization it cannot guarantee overflow avoidance on its own; both error correction and rounding errors may vio-late our constraint. However, we observe that it consistently yields improvements in model quality (see Appendix C.2).\nStrict accumulator-aware constraint. For clarity, and without loss of generality, we motivate our strict accumulator-aware constraint using the special case where x is represented with unsigned integers such that \u00b5 = 0 and \u03bd = 2N \u2212 1. Note that this setting is common when following activation functions with non-negative dynamic ranges (e.g., ReLUs), or when an appropriate non-zero-valued zero-point is adopted (i.e., asymmetric quantization) [3, 44, 45].\nLet \u03b1 denote the sum of all negative elements in q, and let \u03b2 denote the sum of all positive elements in q. From Eq. 7, we can derive the upper bound on \u03b2 given by Eq. 17, which can similarly be derived for \u2212\u03b1 from Eq. 8 in the case of"}, {"title": "Experiments", "content": "sign-magnitude representations. Indeed, uT q < 2P\u22121 \u2212 1 is guaranteed whenever \u03b2\u03bd + \u03b1\u00b5 < 2P\u22121 \u2212 1, which in turn holds in the case of unsigned activations if\n\u03b2 < 2P\u22121\u22121/2N\u22121\nTherefore, to quantize layer l for a target P-bit accumulator, we introduce a practical mechanism to control the range of the dot product based on the following modified GPFQ scheme:\nq(l) i = Q(\u03a8ai,bi ( \u03c0\u03bb(w(l) i + X(l) i T \u03a3 L i\u22121 j=1 X(l) j(w(l) j \u2212 q(l) j) /||X(l) i||2))\na(l) i = Ai(l) \u2212 \u03b1i\nb(l) i = Bi(l) \u2212 Bi\nwhere \u03b1i denotes the sum of all negative elements in q whose index is less than i and Bi is its positive counterpart, A(l) and B(l) (defined in Eq. 21) are respectively the upper limits of \u03b1i and Bi, and the closed interval [a, b] is the range greedily enforced on qi as error is iteratively corrected. We use \u03a8a,b to denote the clipping function \u03a8a,b(x) := clip (x; a, b), which is effectively a no-op when the range [a, b] exceeds that of the quantized domain AM . This has the desired feature of being functionally equivalent to GPFQ when the accumulator is large enough (e.g., 32 bits). Finally, recall that ul i is given by Eq. 12 with ul0 = 0, which remains unchanged.\nBy independently constraining the upper and lower limits of qi, our accumulator-aware GPFQ variant avoids overflow without explicit zero-centering requirements. To ensure rounding errors do not compromise our bounds, we use\n\u2212A(l) = Bi(l) = 2P\u22121\u22121/2N\u22121 \u2212 max(\u2206)\nwhere max (\u2206) denotes the worst-case difference in raw magnitude caused by rounding; for example, max (\u2206) = 0.5 for round-to-nearest and max (\u2206) = 0 for round-to-zero. Thus, our formulation and resulting iteration rules are also agnostic to the symmetry of the quantizer and its choice of rounding function. We also note that, while our derivation considers the sign-magnitude representation for the clarity that its symmetry provides, the separate consideration of A(l) and B(l) is useful for asymmetric representations (e.g., two\u2019s complement).\n3.3 AXE: Accumulator-Aware Extensions\nWhile the theoretical justification we presented is tied to the formulation of GPFQ and its derivations, we can extract our constraints to construct a generalized framework that enables the investigation of accumulator-aware PTQ with any iterative algorithm that sequentially assigns bits, assuming the algorithm is also amenable to activation quantization. Our framework consists of two steps: (1) accumulator-aware projection based on our soft l1-norm regularization penalty; and (2) greedy accumulator-aware clipping based on our strict range limits. We further generalize AXE to support multi-stage accumulation, which has implications for tiled dot products and SIMD parallelization.\nWe present the pseudocode for our accumulator-aware GPFQ variant in Algorithm 1 in Appendix A. There, we simi-larly present the psuedocode for our accumulator-aware OPTQ [17] variant in Algorithm 2. In both cases, \u03bb is derived per-channel before quantization. Adjusted weight values are greedily projected on the l1 ball accordingly, then clipped to the difference between the cumulative sum of positive and negative elements and their respective limits. The result-ing set of quantized weights Q \u2208 AKC M is then guaranteed to avoid overflow when accumulating its inner product with any X \u2208 AKXD M into P-bit signed registers. Note that, unlike the base GPFQ and OPTQ algorithms, our accumulator-aware variants require quantized activations to calculate the accumulator-aware limits in Eq. 21.\nMulti-Stage Accumulation. Our accumulator-aware constraints can be generalized to target customized datapaths beyond user-specific accumulator bit widths. Unlike A2Q and A2Q+, which assume a monolithic accumulator for each dot product [12, 13], we generalize our framework to support multi-staged accumulation as visualized in Figure 2. In such a scenario, our constraints are enforced on the quantized weights in tiles of size T so that each partial dot product can be concurrently computed by an atomic MAC unit. We refer to the accumulator of this atomic MAC unit as the \u201cinner\u201d accumulator and denote its bit width as P1. Conversely, we refer to the accumulator of the resulting partial sums as the \u201couter\u201d accumulator and denote its bit width as Po. Given that a K-dimensional dot product is executed in tiles of size T, where each tile is constrained to a P1-bit accumulator, we can calculate the minimum bit width required to guarantee overflow avoidance for the outer accumulator as:\nPo = [P1 + log2(K) \u2212 log2(T)]"}, {"title": "Optimizing for Accumulator Constraints", "content": "While we are the first to our knowledge to target multi-stage accumulation while guaranteeing overflow avoidance, accumulating in multiple stages is not new. Quantized inference libraries such as FBGEMM [46], QNNPACK [47], XNNPACK [48], and Ryzen AI [49] have employed multi-staged accumulation to exploit a 16-bit inner accumulator (i.e., P1 = 16) to realize performance benefits, albeit without any theoretical guarantees of overflow avoidance. For example, Khudia et al. [50] use FBGEMM to realize nearly a 2\u00d7 throughput uplift on compute-bound workloads by accumulating at 16 bits in tiles of 64 elements rather than accumlating at 32 bits. Currently, these libraries typically disable this optimization if overflow is observed too often during testing. However, AXE provides a mechanism to simultaneously quantize and constrain a pre-trained model for low-precision multi-staged accumulation while guar-anteeing overflow avoidance, enabling co-design for this optimization for the first time. As shown in Section 4, this generalization is critical in maintaining the quality of billion-parameter large language models, which often have dot products containing more than ten thousand elements."}, {"title": "Experiments", "content": "Models & Datasets. Our experiments consider two fundamental deep learning tasks: image classification and au-toregressive language generation. For image classification, we evaluate the top-1 test accuracy of MobileNetV2 [51], ResNet18 [52], and ViT-B-32 [53] on the ImageNet dataset [15] using the pre-trained models made publicly avail-able by the TorchVision library [54]. For language generation, we evaluate the perplexity (PPL) of OPT-125M [20], Pythia-160M [18], and GPT2 [55] on the WikiText2 dataset [16] using the pre-trained models made publicly available by the HuggingFace libraries [56, 57]. Finally, we evaluate multi-staged accumulation on the Pythia model suite [18].\nQuantization Design Space. We constrain our quantization design space to uniform-precision models such that every hidden layer has the same weight, activation, and accumulator bit width, respectively denoted as M, N, and P. We consider 3- to 8-bit integers for both weights and activations, unlike [17] and [42], which focused on weight-only quantization. Rather than evaluating each combination of M and N, we restrict ourselves to configurations where N > M to reduce the cost of experimentation as such configurations tend to dominate the Pareto frontiers [13]. We implement our methods in PyTorch [58] using the Brevitas quantization library [59]. We include all hyperparameter details in Appendix C. All models are quantized using a single AMD MI210 GPU with 64 GB of memory.\n4.1 Optimizing for Accumulator Constraints\nFollowing [12] and [13], we first consider the scenario in which QNNs are optimized for accumulator-constrained pro-cessors. However, unlike prior work, we focus our analysis on the PTQ setting. As discussed in Section 2.3, one could heuristically manipulate the M and N according to the data type bound derived in [12]; however, the quantization design space ultimately limits the minimum attainable accumulator bit width. To the best of our knowledge, Euclidean projection-based initialization (EP-init) [13] serves as the best alternative to this bit width manipulation approach in the PTQ setting (see Section 2.3). Therefore, we use EP-init and na\u00efve bit width manipulation as our baselines.\nIn Figures 1 and 3, we use Pareto frontiers to visually characterize the trade-off between accumulator bit width and model accuracy for both GPFQ and OPTQ, respectively. For each model and each PTQ algorithm, the Pareto frontier shows the best model accuracy observed for a target accumulator bit width P when varying M and N within our design space, with the 32-bit floating-point model accuracy provided for reference. We provide a detailed breakdown of each Pareto frontier in Appendix D, where we report the accuracy of each Pareto-dominant model, their weight and activation bit widths, and resulting unstructured unstructured weight sparsity. We observe similar trends as reported in [13]; the Pareto-optimal activation bit width N decreases as P is reduced, and the unstructured weight sparsity conversely increases. This suggests that our accumulator-aware boundary constraints obey similar mechanics as the l1-norm constraints of A2Q [12] and A2Q+ [13], as our theoretical justification predicts (see Section 3.3). Recall that accumulator-aware quantization requires both the weights and activations to be quantized (see Section 3.2); therefore, this is not a direct comparison against the original GPFQ and OPTQ proposals, which only quantized weights."}, {"title": "Low-Precision Accumulation for Large Language Models", "content": "4.2 Low-Precision Accumulation for Large Language Models\nAs discussed in [13", "13": ".", "18": ".", "60": "and hardware-based datapath optimizations [61", "32": "."}]}