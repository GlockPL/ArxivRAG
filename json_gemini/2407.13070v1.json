{"title": "The Cost of Arbitrariness for Individuals: Examining the Legal and Technical Challenges of Model Multiplicity", "authors": ["Prakhar Ganesh", "Ihsan Ibrahim Daldaban", "Ignacio Cofone", "Golnoosh Farnadi"], "abstract": "Model multiplicity, the phenomenon where multiple models achieve similar performance despite different underlying learned functions, introduces arbitrariness in model selection. While this arbitrariness may seem inconsequential in expectation, its impact on individuals can be severe. This paper explores various individual concerns stemming from multiplicity, including the effects of arbitrariness beyond final predictions, disparate arbitrariness for individuals belonging to protected groups, and the challenges associated with the arbitrariness of a single algorithmic system creating a monopoly across various contexts. It provides both an empirical examination of these concerns and a comprehensive analysis from the legal standpoint, addressing how these issues are perceived in the anti-discrimination law in Canada. We conclude the discussion with technical challenges in the current landscape of model multiplicity to meet legal requirements and the legal gap between current law and the implications of arbitrariness in model selection, highlighting relevant future research directions for both disciplines.", "sections": [{"title": "1 Introduction", "content": "Machine learning models have gained unprecedented power and influence across diverse domains and real-world applications. As these models advance in complexity and capability, a new challenge emerges model multiplicity. This phenomenon, often stemming from inherent overparameterization or underspecification, is the existence of multiple solutions for the same problem, thus introducing arbitrariness in selecting a single model.\nIn recent years, model multiplicity has garnered significant attention (Marx, Calmon, and Ustun 2020; Black, Raghavan, and Barocas 2022; D'Amour et al. 2022), primarily focused on responsible behaviour in expectation (Ganesh 2024), e.g., group fairness (Sokol et al. 2022; Ganesh et al. 2023; Long et al. 2023; Cooper et al. 2024), out-of-distribution robustness (McCoy, Min, and Linzen 2020), overall privacy leakage (Kulynych et al. 2023), etc. Beyond the technical exploration, discussions on policy changes and moral arguments (Black, Raghavan, and Barocas 2022; Creel and Hellman 2022; Black et al. 2024) have been pivotal in developing the discourse on multiplicity. Despite this increasing attention, a critical aspect of multiplicity remains underexplored \u2013 its wide-ranging impact on an individual and the legal implications of this arbitrariness.\nExisting studies tend to focus solely on multiplicity through model predictions, and don't address how the final model selection can affect individuals in diverse ways. Consider, for instance, an individual from the training data who may either face a high risk of information leakage or not, depending on the arbitrary model choice under multiplicity. These effects of arbitrariness go unnoticed in existing studies, and consequently, the cost of arbitrariness for individuals is underestimated. Furthermore, the absence of a definitive correct choice exacerbates the problem, as these decisions of model selection from a set of good models usually hinge on pitting similar individuals against each other.\nRecent studies have also shed light on how certain demographic groups bear the brunt of this arbitrariness more than others (Ganesh et al. 2023; Cooper et al. 2024; Gomez et al. 2024). This arbitrariness not only raises concerns about inadequate planning and recourse under multiplicity but also undermines trust and predictability in the decision-making process. Consequently, heightened arbitrariness within specific demographic groups can perpetuate structured disparities, necessitating a thorough legal examination. This becomes even more pressing when these models are directly adopted by various downstream applications or service providers, thus creating a monopoly and perpetuating structured arbitrariness against certain individuals throughout the sector (Creel and Hellman 2022). While existing legal discourse on multiplicity has centred exclusively on model behaviour in expectation (Black et al. 2024), we instead focus on the impact of arbitrariness on individuals.\nIn this interdisciplinary work, we examine these concerns from both technical and legal perspectives. We focus on employment, and particularly on hiring decisions, advocating for a more nuanced understanding of multiplicity and its influence on individuals. Our contributions are:\n\u2022 Redefining model multiplicity to emphasize individual concerns by incorporating various causes of multiplicity beyond learning hyperparameters as well as other impacts of multiplicity beyond just predictive multiplicity.\n\u2022 Arguing that increased levels of arbitrariness for a protected group can be considered an adverse impact and, accordingly, a discriminatory employment practice under the"}, {"title": "2 Related Work", "content": "Prior to discussing our contributions, we provide a comprehensive review of existing literature in multiplicity to contextualize our work within the broader research landscape."}, {"title": "2.1 Model Multiplicity", "content": "As defined by Black, Raghavan, and Barocas (2022), model multiplicity is the phenomenon where models with equivalent accuracy for a certain prediction task differ in terms of their internals. Many recent works in literature have explored the implications of multiplicity in machine learning, focusing on the unpredictability of model behaviour during deployment (Marx, Calmon, and Ustun 2020; Black, Raghavan, and Barocas 2022). While D'Amour et al. (2022) underscored the connection between model multiplicity and underspecification, Ganesh (2024) showed that even additional specifications during model selection cannot counter the overparameterization of deep learning models, leaving the concerns of multiplicity unresolved. Paes et al. (2023) formalized this further, using an information-theoretic approach to show that model multiplicity is inevitable in settings with finite data. Beyond these challenges, positive opportunities also arise from multiplicity, for instance, the potential for better trade-offs and the development of responsible models (Black, Raghavan, and Barocas 2022; Semenova, Rudin, and Parr 2022; Black et al. 2024).\nThe majority of literature on multiplicity focuses on predictive multiplicity (Marx, Calmon, and Ustun 2020), a special case of model multiplicity that evaluates variations solely in the final predictions of the model. However, other dimensions of quantifying multiplicity have also been explored. Hsu and Calmon (2022) use the output probabilities to quantify multiplicity, while Heljakka et al. (2022) extend this further, using the complete internal representation of the model to measure model disagreements. Several works have also explored the impact of multiplicity on the robustness of model explanations (Karimi et al. 2022; Hancox-Li 2020).\nThe exploration of multiplicity in literature also extends to various problem formulations. While most existing works define a set of good models to have similar accuracy, this has also been expanded to perform model selection under multiple metrics (Kulynych et al. 2023; Long et al. 2023; Ganesh 2024), and even with models trained on different subsets of the data (Meyer, Albarghouthi, and D'Antoni 2023). Multiplicity has also been addressed outside classification, for instance, multi-target objective (Watson-Daniels et al. 2023), object detection (Hsu et al. 2024), etc. In our paper, we consolidate these strands of research to formalize a more comprehensive definition of multiplicity. This encompasses various causes of multiplicity such as dataset choice, model hyperparameters, the stochasticity nature of learning, and various effects of multiplicity like privacy and robustness."}, {"title": "2.2 Multiplicity and Responsible AI", "content": "In addition to choosing better models with multiplicity (Black, Raghavan, and Barocas 2022), recent works have also studied the interplay between multiplicity and other trustworthy metrics. Kulynych et al. (2023) showed that improvements in privacy come at a cost of higher multiplicity and Long et al. (2023) found a similar cost to achieve group fairness. Ali, Lahoti, and Gummadi (2021) showed that addressing unfairness only in examples with high multiplicity effectively eliminates biases and Cooper et al. (2024) showed similarly that abstaining from highly uncertain predictions exhibits a noticeable improvement in fairness.\nRecent works have also explored the implications of arbitrariness itself, and its integration into the broader landscape of responsible AI. Particularly relevant to our work, several studies have shown a disparity in predictive multiplicity across demographics (Ganesh et al. 2023; Cooper et al. 2024; Gomez et al. 2024), thereby disproportionately impacting individuals from various groups. Building on these insights, our study expands the empirical analysis beyond predictions, revealing similar disparities in other metrics, with a case study on privacy and robustness. Furthermore, while existing literature is limited to empirically documenting these disparities, we provide a legal examination under the anti-discrimination law in Canada."}, {"title": "2.3 Multiplicity and Monopolies", "content": "Recent progress in AI has enabled the introduction of a new class of large-scale models, also referred to as foundational models, capable of being adapted to a diverse set of domains. Given their impact, experts have raised concerns about the widespread deployment of these models. Creel and Hellman (2022) use the term algorithmic leviathans to characterize such algorithms and offer a comprehensive discussion on the moral concerns of the standardization of arbitrariness due to the adoption of a single algorithmic system across various applications. Similarly, Vipra and Korinek (2023) advocate for better regulations and the involvement of antitrust authorities to ensure a level playing field in these domains. They highlight the risks associated with tech giants training large-scale foundational models and the natural tendency of such setups to move toward a monopoly, concentrating power in the hands of a few companies. We contribute to this ongoing dialogue by providing a legal discussion examining the arbitrariness in these monopolies."}, {"title": "2.4 Multiplicity and Law", "content": "There exists a wide range of literature on the adverse impacts of algorithmic decision-making. More specifically, in the domain of employment, which is the focus of our work, several scholars have discussed the legal implications of these impacts and what legal approaches can be deployed to mitigate the discriminatory outcomes of algorithmic models (Ajunwa 2020; Bildfell 2019; Kelly-Lyth 2023; Kim 2017; Scherer, King, and Mrkonich 2019; Trindel, Kassir, and Bent 2021). However, discussion on the legal implications of arbitrariness resulting from model multiplicity has been limited. The closest work to ours are Black et al. (2024) which makes observations on the legal implications of model multiplicity in the context of credit-lending, and Kim (2022) that highlights the significance of model multiplicity in de-biasing models. We contribute to this discourse by bridging the gap between legal and technical discussions on the adverse impacts of arbitrariness for individuals. We explore the effects of multiplicity on individuals within the employment domain and analyze the legal implications of these effects under the anti-discrimination laws in Canada."}, {"title": "3 Redefining Model Multiplicity", "content": "Early work in multiplicity (Breiman 2001) draws inspiration from the epistemological framework of the Rashomon effect (Anderson 2016): the phenomenon of different yet equally plausible interpretations of the same event. The Rashomon effect underscores the subjectivity and complexity inherent in the interpretation of information. In machine learning, this is translated to the existence of multiple models with similar performance, despite learning different underlying functions. In this section, we dive deeper into the causes and effects of multiplicity in real-world systems, use these insights to motivate redefining multiplicity, and support our definition of multiplicity with empirical studies."}, {"title": "3.1 Multiplicity in Context: Causes and Effects", "content": "Causes of Multiplicity: The process of designing a machine learning model encompasses a multitude of choices. Beginning with the data, decisions are made regarding what data to gather, how to process and filter it, which features to select, etc. Even decisions on partitioning the data for training and validation have been shown to create multiplicity (Friedler et al. 2019; Meyer, Albarghouthi, and D'Antoni 2023). Similarly, the learning algorithm design further entails numerous decisions: the choice of model architecture, selection of hyperparameters, various forms of stochasticity, and even the criteria for model evaluation. Each of these decisions contributes to the cascade of choices that directly impacts the multiplicity of the trained models.\nNotably, these choices are not always informed or even intentional. For instance, in some cases, choices are made by drawing upon insights from the literature that focus on the effects of algorithm design on model multiplicity (Wu et al. 2021; Ponomareva et al. 2023; Kulynych et al. 2023; Ganesh 2024). In other cases, choices are driven by popular trends, such as the prevalent use of a specific activation function instead of exploring various alternatives (Dubey, Singh, and Chaudhuri 2022), or by the convenience of employing model architecture available in more accessible frameworks like PyTorch or TensorFlow. While even these choices may be studied in the future to better grasp their impact on multiplicity, others remain inherently arbitrary, like the selection of a random seed for training, defying attempts at informed decision-making when training a learning model.\nThese choices culminate together into the final model, collectively impacting its multiplicity. Thus, while specialized definitions of multiplicity can be helpful for targeted technical analysis, it's important to encompass all these choices when discussing the real-world implications of multiplicity. For instance, characterizing multiplicity solely as training multiple models on the same dataset overlooks the significance of the dataset selection. Similarly, defining multiplicity based on just the accuracy or error of the model for a given task neglects the various considerations involved in model evaluation, like measuring fairness to ensure adherence to certain standards. Thus, it is important to cover all aspects of a learning pipeline when defining multiplicity.\nReality of Training Multiple Models: Existing literature often suggests training multiple models to mitigate arbitrariness in model training, by creating ensembles, quantifying multiplicity, or using model selection to navigate the causes of multiplicity discussed above (Kulynych et al. 2023; Long et al. 2023; Creel and Hellman 2022). However, this isn't always feasible if the model's size or complexity makes training multiple models computationally expensive. For instance, consider HireVue, a recruitment company catering to over a third of the Fortune 100 in the US. HireVue uses sophisticated models to assess candidates via textual responses and video interviews (Larsen 2018), which makes training enough models to capture multiplicity impractical.\nEven in situations when it's possible to train multiple models, the cost of data collection can be a significant barrier. Consequently, multiple models may be trained on the same dataset, perpetuating the arbitrariness inherent in the data. For example, spurious correlations in the data can be viewed as a manifestation of arbitrariness stemming from the dataset choice, as a different dataset might exhibit different behaviour (Zech et al. 2018; Hendrycks et al. 2021). Even when empirical methods can capture multiplicity, model selection may lead to overfitting, hindering generalization (Ganesh 2024). These trends don't necessarily dismiss the efficacy of training multiple models. Rather, it highlights the complexities and limitations of this approach, making it far from a universally applicable solution.\nEffects of Multiplicity: Multiplicity is often explored in scenarios where individuals encounter conflicting predictions. This can create brittleness and uncertainty in model decisions, undermining their reliability and hampering effective planning. This becomes particularly problematic when the underlying model is proprietary, leaving users in the dark about its inner workings. For example, in the context of hiring, individuals affected by high multiplicity won't be able to anticipate outcomes based on observable patterns, severely hindering their ability to discern which applications merit greater attention and plan accordingly. As a result, they"}, {"title": "3.2 Rashomon Sets and Model Multiplicity", "content": "With the causes and effects of multiplicity better understood, we now formally define model multiplicity. We saw that the concept of multiplicity is deeply rooted in the Rashomon effect, which highlights the diverse interpretations that can arise from the same data. The models illustrating this Rashomon effect are together known as a Rashomon set, or a set of competing models, a set of good models, e-Rashomon set, e-Level set, etc. We'll stick to the term Rashomon set for consistency in our discussion. In principle, a Rashomon set defines a set of models which are practically indistinguishable for those deploying the model, underscoring the arbitrariness in selecting one model over another. Thus, to define multiplicity, we need to first define a Rashomon set. And to define a Rashomon set, we need to define what it means for two models to be indistinguishable.\nWe introduce a set of metric delta functions, $\\Delta^{\\mathcal{P}}$, and corresponding thresholds $\\mathcal{E}^{\\mathcal{P}}$. A metric delta function takes as input two models and measures the difference between them under the given metric. These metric deltas serve as benchmarks for determining whether two models are indistinguishable: if the difference in performance for every $\\delta \\in \\Delta^{\\mathcal{P}}$ falls within the corresponding threshold $\\epsilon \\in \\mathcal{E}^{\\mathcal{P}}$. For example, one might define the metric delta for accuracy as $\\delta_{Acc,D}(h_1,h_2) = |Acc(h_1, D) \u2013 Acc(h_2, D)|$. Formally,\nDefinition 1 (Rashomon Set) Two models $h_1, h_2$ belong to the same Rashomon set under performance constraints $(\\Delta^{\\mathcal{P}}, \\mathcal{E}^{\\mathcal{P}})$ if they exhibit similar performance for every metric in the given performance constraints, i.e.:\n$$\\forall (\\delta,\\epsilon) \\in (\\Delta^{\\mathcal{P}},\\mathcal{E}^{\\mathcal{P}}): \\delta(h_1,h_2) \\le \\epsilon$$\n(1)\nThe Rashomon set effectively captures the phenomenon of various learned models being practically indistinguishable. However, the study of multiplicity is crucial only in the context of these models also exhibiting diverse behaviour when deployed, i.e., our interest lies in discerning meaningful differences, rather than merely the existence of Rashomon sets. Given the various effects of multiplicity discussed above, we generalize model multiplicity by binding it to a metric delta $\\delta^{\\mathcal{M}}$ and corresponding threshold $\\epsilon^{\\mathcal{M}}$, on models belonging to the same Rashomon set. Formally,\nDefinition 2 (Model Multiplicity) Two models $h_1,h_2$ exhibit multiplicity under performance constraints $(\\Delta^{\\mathcal{P}}, \\mathcal{E}^{\\mathcal{P}})$ and multiplicity constraint $(\\delta^{\\mathcal{M}}, \\epsilon^{\\mathcal{M}})$, if they have similar performance for every metric in the performance constraints yet differ on the metric in the multiplicity constraint, i.e.:\n$$\\forall (\\delta,\\epsilon) \\in (\\Delta^{\\mathcal{P}},\\mathcal{E}^{\\mathcal{P}}): \\delta(h_1,h_2) \\le \\epsilon \\\\and~ \\delta^{\\mathcal{M}} (h_1,h_2) > \\epsilon^{\\mathcal{M}}$$\n(2)\nNote that the definitions here are quite similar to those already present in the literature. However, we have made deliberate choices distinct from previous approaches. Unlike several existing works (Marx, Calmon, and Ustun 2020; Black, Raghavan, and Barocas 2022), we do not restrict the models $h_1$, $h_2$ to a hypothesis class or limit them to being trained on data from the same distribution. Similarly, unlike much of the prior work, we avoid relying on a single metric of model similarity or only model predictions as a measure of multiplicity. While these are not radical changes, we emphasize these lack of restrictions to highlight the complete scope of the impact of multiplicity as discussed above. Next, we briefly present empirical evidence to support our choices."}, {"title": "3.3 Empirical Insights into Multiplicity", "content": "We now present a series of empirical studies to support our decision to expand the definition of multiplicity.\nDataset: We conduct our study on a Job Applicants dataset (Tankha 2023), which surveys individuals on the website StackOverflow, collecting various attributes and the success of their job applications. We filter the dataset to focus specifically on applications to Canadian companies, resulting in 2779 data points for our analysis. This data is split into 80% for training and 20% for testing. We also explore other datasets spanning different modalities in Appendix B."}, {"title": "4 Arbitrariness and Group Membership: The Legal Challenges", "content": "We perform our legal analysis of multiplicity in light of Canadian anti-discrimination law, with a focus on the context of hiring. Hiring has recently witnessed a dramatic increase in the delegation of decisions to machine learning models (Capuano 2023; Green 2021). While these models are promising in increasing efficiency and speed in hiring processes, they also risk discriminating against job applicants based on their protected characteristics (Ajunwa 2020; Cofone 2019). Note that, although our focus is on hiring, our analysis here may shed light on the broader legal implications of arbitrariness in algorithmic decisions.\nWe first briefly introduce employment discrimination law in Canada. We then argue that under these laws, the disparity in arbitrariness constitutes a form of discrimination."}, {"title": "4.1 Anti-Discrimination Laws: The Moore Test", "content": "Discrimination in hiring and recruitment is directly prohibited under the provincial and territorial human rights legislation in Canada. For example, the Human Rights Code of Ontario (noa 1990) ensures that \u201cevery person has a right to equal treatment with respect to employment without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, record of offences, marital status, family status or disability\". Similarly, the Charter of Human Rights and Freedoms of Quebec states that \"Every person has a right to full and equal recognition and exercise of his human rights and freedoms, without distinction, exclusion or preference based on race, colour, sex, gender identity or expression, pregnancy, sexual orientation, civil status, age\" (noa 1976). While the Canadian Charter of Rights and Freedoms does not directly apply to the disputes between private individuals (noa 1989, 2010), the Supreme Court's interpretation of the equality rights under the Charter has greatly influenced and continues to guide the provincial courts and tribunals' interpretation and application of anti-discrimination provisions under the applicable law, such as provincial human rights provisions (England 2006; Bildfell 2019). The Canadian Human Rights Act provides similar protection to those employed by the federal government and several types of companies that are regulated by the federal government, such as banks. Additionally, the Employment Equity Act requires employers to achieve employment equity by taking proactive measures targeting the detection and elimination of the hindrances negatively affecting \"women, Aboriginal peoples, persons with disabilities and members of visible minorities\" in employment (noa 1995).\nIn Canadian anti-discrimination doctrine, courts analyze whether a given practice or treatment is discriminatory by applying a three-part test developed in the Supreme Court case Moore v. British Columbia (Ministry of Education) (noa 2012) and later confirmed in Stewart v. Elk Valley Coal Corp (noa 2017). Under this test, commonly referred to as the Moore test, \u201ccomplainants are required to show that they have a characteristic protected from discrimination under the Code [provincial/territorial human rights legislation]; that they experienced an adverse impact with respect to the service; and that the protected characteristic was a factor in the adverse impact\u201d (noa 2012). In the next subsection, we use the Moore test to determine when there is a pattern of discrimination due to the disparity in arbitrariness across various protected groups."}, {"title": "4.2 Arbitrariness as a Form of (Prima Facie) Discrimination", "content": "To determine whether model multiplicity would violate extant anti-discrimination law in Canada, we have to scrutinize it under the Moore test. The test establishes what situation is a prima facie case of discrimination, meaning that there is a discriminatory impact that the employer will have to justify for it not to constitute illegal discrimination.\nConsider the following hypothetical scenario: A non-"}, {"title": "5 The Potential Monopoly of A Machine Learning Model", "content": "After examining the multiplicity disparity across groups, we shift our focus to a special case of arbitrariness in machine learning: the challenges of an arbitrary decision made by an algorithm that is widely adopted across various applications or user contexts, known as an algorithmic leviathan (Creel and Hellman 2022). Algorithmic leviathans, as proposed by Creel and Hellman (2022), are \u201cautomated decision-making systems that make uniform judgments across broad swathes of a sector\". Such systems become problematic when these uniform judgements are arbitrary, leading to a systematic response against particular groups of individuals. Simply discouraging such a monopoly is impractical in scenarios where training multiple models can be cost-prohibitive, where there is a deliberate absence of multiple decision-makers (e.g., immigration decisions in a country), or where arbitrariness originates not from the model but from the underlying dataset (e.g., shared arbitrariness due to everyone using the same dataset). We direct readers to Creel and Hellman (2022) for a more in-depth discussion of the moral concerns associated with deploying algorithmic leviathans.\nIn this paper, we extend the arguments to legal concerns on the existence of such leviathans. We start by examining the compounded impact of a monopoly on the existing disparity in arbitrariness across different protected groups from the lens of anti-discrimination law. Following this, we draw attention to the far-reaching discriminatory impacts of algorithmic leviathans which result from the blanket rejection of individuals. We then ask: 'What if an algorithmic leviathan, while not discriminating against protected groups, still creates a structured adversity for certain individuals?'"}, {"title": "5.1 Monopoly, Anti-Discrimination Law and Competition Law", "content": "The adverse impact arising from the increased levels of arbitrariness for minority groups will be compounded when a single algorithmic model is deployed widely in a sector. Where the majority of employers within a sector use the same model to make hiring decisions, minority groups will be systematically subjected to lower levels of predictability. This will harm minority groups' access to employment opportunities as they will not be able to increase the likelihood of getting recommended by the hiring model by adjusting their behaviours and they will be subjected to generalized welfare-decreasing uncertainty.\nThe compounded reduction in reduced predictability is not the only threat posed by algorithmic leviathans. These models, irrespective of whether they have higher arbitrariness for minority groups, may still be giving rise to discriminatory impacts. A hiring model that is deployed sector-wide risks excluding certain job candidates from employment opportunities (Creel and Hellman 2022). If a blanket rejection arises in connection with individuals' characteristics protected by anti-discrimination law, the disparate impact of arbitrariness on protected groups will be severe. In such a case, the members of the group rejected by the model may have significantly reduced opportunities to find employment within a specific sector. Such an exclusion would exacerbate the historical and ongoing under-representation of marginalized groups in the workforce.\nDetecting the severity and extent of discrimination in such a case, however, may not be an easy task. The fact that a certain implementation of an algorithmic leviathan has an adverse impact on a minority group does not automatically mean that all other implementations of the same model have the same adverse impact too. Employers, after obtaining the hiring model from developers, might have fine-tuned it in accordance with their own business necessities. This means that different implementations of the same model may have different tendencies in selecting and rejecting job candidates. Accordingly, the discriminatory impact detected within a single instance of the model may not be found in other instances (Wang and Russakovsky 2023).\nThe same situation applies to the absence of discrimination in a certain implementation of an algorithmic leviathan as well. The fact that no discriminatory outcome is detected within the predictions of an algorithmic leviathan does not guarantee that all other implementations of the same model, too, do not give rise to discrimination. As fine-tuning can render a model unbiased, it can also turn an unbiased model into a biased one. Employers, in their attempts to align their hiring model with the needs of their businesses, might render them biased against certain minorities.\nIn some cases, the monopoly of a single hiring model within a sector may give rise to unfairness in access to employment opportunities despite not being discriminatory against any group. This happens when an unbiased, non-discriminatory, hiring model dominates a certain sector. While attaining a completely unbiased algorithmic model is highly difficult due to the intricate connections between neutral variables and protected characteristics (Cofone 2019), for the sake of our analysis and to be able to study these algorithmic leviathans separate from the discrimination against protected groups we saw in the previous section, let us assume that the model in question has achieved this fairness. Also, assume that no fine-tuning has been made by employers, and accordingly, this model, in each instance, is generating the same predictions. That is, it is recommending the same set of candidates while rejecting another set of candidates in all instances. This means that some candidates will be excluded from job opportunities as they will be consistently rejected by the same model. This situation is cer-"}, {"title": "6 Open Research Questions", "content": "In this paper, we explored the impact of arbitrariness in model selection due to multiplicity and discussed the discriminatory practices it can foster. However, there remain significant challenges, spanning both technical and legal domains. We conclude our paper by outlining the key open research questions in model multiplicity. By explicitly outlining paths for future research, we aim to encourage collaboration between both disciplines to address these challenges."}, {"title": "6.1 Auditing Model Multiplicity", "content": "We start by outlining the key technical challenges to auditing model multiplicity in real-world scenarios.\nCost-effective Auditing of Multiplicity: Various metrics for measuring multiplicity can be found in the literature, either for individual data points (Cooper et al. 2024; Kulynych et al. 2023; Black, Raghavan, and Barocas 2022; Hsu and Calmon 2022) or the entire datasets (Heljakka et al. 2022; Long et al. 2023; Marx, Calmon, and Ustun 2020); either for measuring multiplicity of predictions (Cooper et al. 2024; Kulynych et al. 2023; Black, Raghavan, and Barocas 2022; Long et al. 2023; Marx, Calmon, and Ustun 2020) or capturing a wider model behaviour (Heljakka et al. 2022; Hsu and Calmon 2022); etc. Yet, despite the differences, all these metrics share a commonality\u2014they rely on training an ensemble of models. However, this is impractical and costly, as the burden of training multiple copies of a model can be unreasonable for companies utilizing such models, posing a significant barrier to auditing model multiplicity.\nRecent work by Hsu et al. (2024) broke this trend and showed the advantages of using Monte Carlo dropout as an efficient alternative to training multiple models for measuring model multiplicity. This opens up a promising avenue for future research: understanding the relationship between predictive uncertainty and multiplicity. Predictive uncertainty is a model's calibrated uncertainty concerning a specific prediction. Acknowledging that machine learning models tend to make arbitrary decisions in uncertain situations, we might be able to use predictive uncertainty to approximate model multiplicity. Existing work on measuring predictive uncertainty by using an ensemble of models further reinforces this connection (Lakshminarayanan, Pritzel, and Blundell 2017). Thus, gaining a better understanding of the relationship between predictive uncertainty and model multiplicity can allow us to leverage cost-effective methods of uncertainty measurement, marking a crucial step toward auditing arbitrariness and multiplicity in machine learning.\nIt should also be noted that despite recent advancements, we have a limited understanding of how to mitigate and manage multiplicity, with existing literature primarily focused on auditing multiplicity as discussed above. Notably, the most common recommendation in literature, model selection based on appropriate constraints, has been shown to cause overfitting and does not generalize to unseen settings (Ganesh 2024). Addressing this gap thus becomes imperative to be able to tackle multiplicity in the real world.\nArbitrariness as a Fairness Metric: Defining a metric is an important initial step in any auditing process, enabling us to measure and track progress while facilitating comparisons across diverse settings. Although we've demonstrated significant disparity in arbitrariness across different protected groups (see Section 3.3), an essential discussion remains on translating this disparity into a fairness metric. Furthermore, there are several multiplicity metrics currently used in the literature (Cooper et al. 2024; Kulynych et al. 2023; Black, Raghavan, and Barocas 2022; Somepalli et al. 2022; Heljakka et al. 2022; Long et al. 2023; Marx, Calmon, and Ustun 2020; Hsu and Calmon 2022), yet none have been compared with each other to determine if they even have any discernible differences. Thus, it is critical to consolidate existing metrics, conduct a comprehensive study on their applicability in defining fairness under arbitrariness, and use them to quantify the biases present in a model.\nBeyond Predictive Multiplicity: Throughout the majority of this paper, and in much of the literature on model multiplicity, the focus remains on the case of predictive multiplicity. However, it is worth noting that, even when multiple models in the Rashomon set produce the same prediction for an individual, the arbitrariness of model selection can still introduce adversity, as we show in Section 3.3. Thus, while predictive multiplicity is fundamental, it is important to recognize that the adverse effects of multiplicity extend beyond predictions. Consequently, further exploration and analysis of these effects would be beneficial."}, {"title": "6.2 Anti-discrimination and Competition Laws", "content": "We discuss, in this section, the gaps between law and the implications of model multiplicity, emphasizing the legal discussions required for future research.\nArbitrariness in a Broader Context: Our analysis in this paper has been limited to Canada and the context of hiring. We encourage further discussions from the perspective of other jurisdictions, as a comparative analysis would provide lawmakers with a more profound insight into the problem. Further research would also be beneficial to shed light on how model multiplicity may impact equality and non-discrimination in other domains where algorithmic decision-making is increasingly deployed, such as predictive policing, credit lending, and marketing.\nCompetition Law for Algorithmic Leviathans: In this paper, we argued that heightened levels of arbitrariness for individuals from minority groups should be considered as an adverse impact under anti-discrimination laws. We also noted that where protected characteristics of individuals do not play a role in the occurrence of the disadvantageous outcomes of model multiplicity, there is no discrimination under the extant anti-discrimination laws. In the case of an algorithmic system gaining market dominance, we noted that competition law may be useful in combating the unfairness emerging from these models. How better competition and stronger fairness can be achieved simultaneously in a sector that relies on such algorithmic leviathans is an important policy question that requires further research and discussion.\nA Closer Look on Foundational Models: The advent of foundational models, commonly defined as large-scale AI models capable of adapting to a wide range of applications, is a special case of algorithmic leviathans. These models wield exceptional influence, intensifying concerns about the arbitrariness present in their decisions. Unlike other algorithmic leviathans, foundational models transcend individual sectors and are used across a diverse array of contexts, amplifying their impact. This universality highlights the potential harm associated with the arbitrariness and the discriminatory challenges of foundational models.\nThese models stand out as a case study that demands special attention, given their common practice of employing closed-source datasets and models. This practice of closed systems hampers transparency, complicating efforts to audit the model for arbitrariness. The extreme costs of training, and thus the infeasibility of retraining foundational models, also underscores the imperative of addressing arbitrariness online. Moreover, the majority of foundational models are controlled by established tech monopolies, facilitating rapid integration into a range of products. This necessitates enhanced regulatory measures to prevent their unchecked dominance in various fields. Consequently, a discussion among legal and policy experts is necessary to address the concerns of fair competition and safeguard individuals against adversities resulting from multiplicity in foundational models."}, {"title": "7 Discussion and Limitations", "content": "In this paper, we provided empirical results and legal arguments on model multiplicity to address the cost of arbitrariness for individuals. A limitation of our legal discussion lies in the absence of real-world case studies. We believe that the lack of awareness among the general public regarding the arbitrary nature of model selection contributes to the absence of legal studies on the arbitrariness of automated systems; our work serves as a crucial step in shedding light on the inherent problems associated with multiplicity. We also use only a single tabular dataset in our experiments that is tailored to the hiring domain, while the other datasets in our appendix are from diverse domains. This is due to the scarcity"}]}