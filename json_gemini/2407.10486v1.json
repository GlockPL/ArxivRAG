{"title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization", "authors": ["Jie Cao", "Dian Jiao", "Qiang Yan", "Wenqiao Zhang", "Siliang Tang", "Yueting Zhuang"], "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "In today's world, where we are constantly bombarded with vast amounts of text, the ability to efficiently summarize information has become crucial. Textual summarization (Gambhir and Gupta, 2017), the process of condensing a lengthy document into a succinct and digestible version while preserving the most crucial information, enabling quicker understanding and better management of information. As everyone has unique needs and interests in real-life scenarios, necessitating summarizers that succinctly address the information needed for a specific query by extracting essential information from documents, i.e., Query-Focused Summarization (QFS) (Daum\u00e9 III, 2009). This task involves analyzing the content to identify key themes and then highlighting these in the summary, which draws increasing attention in the textual summarization community.\nTraditionally, QFS has used extract-then-summarize methods (Zhong et al., 2021; Wang et al., 2022; Amar et al., 2023) that rely on the most relevant spans of text from a candidate document-based on the prevalence of query terms. Further onwards, the triumph of Large Language Models (LLMs) such as the GPT series (Achiam et al., 2023), LLaMA (Touvron et al., 2023) and other open-source LLMs showcased the power of large-scale pretraining in understanding, reasoning and generating intricate textual patterns, the great potential of LLMs offering new opportunities for QFS. However, there has been relatively little investigation into LLMs-based QFS methods (Yang et al., 2023a). Our primary goal in this paper is to close this gap correspondingly by proposing two indispensable characteristics that should be harnessed by LLMs while dealing with QFS: (i) Efficiently Fine-grained Query-LLM Alignment, as commonly known, the pre-trained LLMs are powerful when transferred to downstream tasks with instruction tuning(Ouyang et al., 2022), this also applies to the QFS task when the LLMs specialized for user's interests. However, as the parameter number grows exponentially to billions or even trillions, it becomes very inefficient to save the fully fine-tuned parameters for each downstream task. Besides, the different data distribution of diverse user's queries or instructions may introduce the negative transfer in the training stage (Wang et al., 2019). This implies the QFS model should minimize the potential interference among different user instructions, thereby accessing the fine-grained query-LLM alignment. (ii) Lengthy Document Summarization, general LLMs can't handle long text inputs due to the huge amount of memory required during training. Besides, the simple approach of concatenating the query to the input document is insufficient for effectively guiding the model to focus on the query while generating the summary. How to process the lengthy documents is also an important characteristic of LLMs-based QFS approaches. Summing up, these characteristics necessitate a thorough reevaluation of QFS and its corresponding solutions with LLMs.\nBased on the aforementioned insights, we propose Infinite and Dynamic largE language modeL-based framework, abbreviated as IDEAL for ideal QFS, which consists of two modules: Query-aware HyperExpert and Query-focused Infini-attention achieve the two indispensable characteristics, respectively. The Query-aware HyperExpert leverages the parameter-efficient fine-tuning (PEFT) (Mangrulkar et al., 2022) strategies that enable a model to perform a new task with minimal parameter updates. Innovatively, we tailor the previous PEFT approaches to QFS tasks with a HyperNetwork (Ha et al., 2016), which can dynamically generate the strongly correlated LLM's parameter shifts according to users' queries. Such dynamic characterization allows us to achieve the best of both worlds by adjusting the LLM's parameters while encouraging the model to adapt to each individual instance. By doing so, efficient and fine-grained query-LLM alignment can be achieved. Notably, we develop three types of HyperExpert, including Prompt-tuning (Lester et al., 2021), Parallel Adapter (He et al., 2022), and Low-Rank Adaptation (LoRA) (Hu et al., 2021). To process long documents with bounded memory and computation, we propose incorporating a Query-focused Infini-attention module into IDEAL. Infini-attention (Munkhdalai et al., 2024) includes a long-term compressive memory and local causal attention for efficiently modeling both long- and short-range contextual dependencies. Our Query-focused Infini-attention possesses an extra query-focused compressive memory to better retain parts of the input documents that are strongly correlated with the query.\nOur contributions can be summarized as follows:\n\u2022 We explored query-focused PEFT methods and proposed a method, IDEAL, that tunes instance-level PEFT approaches according to query instructions, enhancing the model's fine-grained instruction-following capabilities.\n\u2022 We propose to incorporate a query-focused infini-attention module to process long text under low memory resources for QFS tasks. For example, IDEAL with the backbone model LLAMA2-7B can process datasets where the average length of input tokens is 13,000 on a single 24GB Nvidia GeForce RTX 3090.\n\u2022 We performed extensive and rigorous experiments across multiple QFS datasets. IDEAL significantly outperforms other baselines."}, {"title": "2 Methodology", "content": "Overview. Given a query and a document, the QFS task aims to generate a summary tailored to this query. Inspired by recent Hypernetwork-based methods (Ivison and Peters, 2022; Zhang et al., 2024a), our IDEAL generate instance-level adapters according to the query instruction using an additional HyperNetwork. For long-text QFS datasets, we propose a Query-focused Infini-attention module that can be integrated into IDEAL, enabling the summarization of infinitely long texts under low-memory constraints. In our experiments, we use LLAMA as the underlying model, a popular decoder-only LLM. However, our overall approach can be applied to any generic decoder-only transformer model. In Section 2.1, we first describe the details of IDEAL, including IDEALPrompt, IDEALPAdapter, and IDEALLORA. Then, Section 2.2 presents the query-focused infini-attention.\n2.1 Query-aware HyperExpert Module\nGiven a dataset with input text pairs containing a query and a document, and outputs in the form of a summary, and a pre-trained LLaMA with an N-layer transformer, IDEAL based on three kinds of PEFT adapters to fine-tune LLaMA to generate query-focused summaries respectively. For example, IDEALLORA, we place a regular (non-generated) LORA layer in the first I layers, then we use the hidden representation Hquery of query in l-th layer as the input of a Hypernetwork to generate the LORA parameters for the last N \u2013 I layers.\nPEFT approaches. With the growth in model sizes, fine-tuning methods have advanced significantly, modifying only a small number of parameters or adding new ones to a frozen language model for specific tasks (Li and Liang, 2021; Lester et al., 2021; Hu et al., 2021; He et al., 2022; Zhang et al., 2023;Zhang et al., 2024b;). These methods often achieve performance comparable to full model fine-tuning. In this paper, we use three types of PEFT methods, including prompt tuning, parallel adapter, and LoRA, as baselines to investigate our approach.\nAs shown in Figure 1(a), Prompt tuning can add soft prompts to the hidden states in attention layers to guide model learning and adapt to new tasks, where only the soft prompts are updated during training. LLaMA-Adapter-v1 (Zhang et al., 2023) introduces a zero-initialized attention mechanism into prompt tuning, which adaptively incorporates the knowledge from soft prompts. We use this LLaMA-Adapter-v1 as our prompt tuning baseline.\nParallel adapters (He et al., 2022) aim to incorporate additional learnable networks in parallel with distinct sublayers within the backbone model. To reduce the number of parameters, small bottleneck networks are used as parallel adapters. In transformer-based LLMs, parallel adapters can be applied to both the feedforward and self-attention modules in each transformer block. Hu et al. (2023) conducted experiments showing that applying parallel adapters only to the feedforward module achieves the best results on math reasoning datasets. As shown in Figure 1(c), we also apply parallel adapters only to feedforward module in LLaMA's transformer block.\nLORA (Hu et al., 2021) adds trainable low-rank decomposition matrices in parallel to existing weight matrices. For a pre-trained weight matrix \\(W \\in R^{d \times k}\\), LoRA constrains its update by adding low-rank matrix pairs, resulting in \\(W + \\Delta W = W + BA\\), where \\(B \\in R^{d \times r}\\), \\(A \\in R^{r \times k}\\), and the rank \\(r < min(d, k)\\). During training, W is frozen while B and A are trainable. LORA initializes A randomly and B to zero, ensuring that \\(\\Delta W = BA\\) starts from zero at the beginning of training, thereby preserving the pre-trained knowledge as much as possible.\nAdapter-based HyperExpert. Previous works (Ivison and Peters, 2022; Zhao et al., 2024) indicate that hypernetworks can learn the parameter information of the main neural network under different input scenarios and efficiently adjust the target network's parameters to adapt to this information. We propose generating query-focused adapters conditioned on the query instruction using a hypernetwork.\nOur hypernetwork is a bottleneck network that consists of an encoder to transform the meanpooling of the query representation Hquery into a low-dimensional representation h, and a decoder to convert h into the parameters of the target adapters. For example, the computation of IDEALLORA is as follows:\n\\(h = dropout(ReLU(W_{mean}(H_{query}) + b_{o}))\\) (1)\n\\(A_{q} = W_{1}h + b_{1}\\) (2)\n\\(A_{k} = W_{2}h + b_{2}\\) (3)\nwhere Aq and Ak correspond to Wq and Wk in self-attention, respectively. We only generate the A matrix in the LoRA module, initializing B to zero and updating it during training as in the original LoRA. This ensures that \\(\\Delta W = BA\\) starts from zero at the beginning of training. Unlike IDEALLORA, IDEALPrompt and IDEALPAdapter generate all the parameters of the target adapters in the required layers.\nIn addition, each layer that needs to generate the target adapters has its own encoder, as shown in Equation 1, and shares a single decoder. This allows for generating different parameters for each layer and reduces the number of trainable parameters.\n2.2 Query-focused Infini-attention Module\nQFS tasks usually involve long documents. However, Transformer-based LLMs can't handle such long texts due to the quadratic complexity of the attention mechanism in terms of both memory usage and computation time. Infini-attention (Munkhdalai et al., 2024) incoporates a compressive memory and a long-term linear attention mechanism into vanilla Transformer block, scale Transformer-based LLMs to extremely long inputs with bounded memory. However, due to the information loss inherent in compressive memory modules, in QFS tasks, the model tends to lose crucial query instruction details and relevant document information after compressing query instruction and very long input documents. To minimize the information loss of query-related details in Infini-attention, we propose compressing the query-related document information into an additional memory block, termed Query-focused Infini-attention.\nSimilar to Infini-attention (Munkhdalai et al., 2024), the input tokens are segmented to perform standard causal dot-product attention within each segment. Before local attention for current segment is complete, we compress the cached key-value (KV) attention states into two memory blocks. One block maintains the entire context history, while another focuses on query-related information. These compressed memories are then available for subsequent segments to retrieve relevant context.\nFixed length local attention. A key-value (KV) cache is typically used in LLMs for fast and efficient inference. To maintain fine-grained local attention, for each segment, multi-head self-attention \\(A_{local} \\in R^{L \times d_{value}}\\) is computed with a fixed KV length L in both the training and inference stages using the KV cache. In detail, when the last segment length is less than L, we use the KV cache to extend the length of the current KV states to L for computing the local attention and compress the remaining KV cache into the memory.\nMemory update. For the s-th segment with length L, before computing the local attention, we update the full context memory \\(M_{all}^{s-1} \\in R^{d_{key} \times d_{value}}\\) and the query-focused memory \\(M_{query}^{s-1} \\in R^{d_{key} \times d_{value}}\\), and a normalization term \\(Z_{s-1} \\in R^{d_{key}}\\) is then used for memory retrieval as follows:\n\\(M_{all}^{s} \\leftarrow M_{all}^{s-1} + \\sigma(K_{cache})V_{cache}\\) (4)\n\\(M_{query}^{s} \\leftarrow M_{query}^{s-1} + \\sigma(K_{cache})V_{cache}\\) (5)\n\\(Z_{s-1} \\leftarrow Z_{s-2} + \\sum_{t=1}^{L} \\sigma(K_{cache})\\) (6)\nwhere \\(\\sigma\\) is a nonlinear activation function. Following the work of Katharopoulos et al. (2020) and Munkhdalai et al. (2024), we employ element-wise ELU+1 as the activation function (Clevert et al., 2015). The term \\(\\sigma(K)^{T}V\\) on the right side of Equation 4 and 5 is referred to as an associative binding operator (Schlag et al., 2020). The query-focused memory \\(M_{query}^{s}\\) differs from the full context memory only in the value states \\(V_{cache}\\) used within the associative binding operator. We ultilize the query states \\(Q_{s-1}^{query}\\) of query instruction to scale the value states, and keep only query-related information \\(V_{cache}^{Ai}\\) as\n\\(Ai = sigmoid(\\frac{mean(Q_{query}) (K_{cache}^{Ai})^{T} }{\\sqrt{d_{model}}} )\\) (7)\n\\(V_{cache} \\leftarrow a V_{cache}\\) (8)\nHere, we use the mean pooling of Qquery and the key states to compute a related score for each representation.\nMemory retrieval. After updating the memory, we retrieve new content \\(A_{all} \\in R^{L \times d_{value}}\\) and \\(A_{query} \\in R^{L \times d_{value}}\\) from the full context memory \\(M_{all}^{s-1}\\), and the query-focused memory \\(M_{query}^{s-1}\\), respectively. This retrieval is performed using the query states \\(Q \\in R^{L \times d_{key}}\\) as follows:\n\\(A_{all} = \\frac{\\sigma(Q)M_{all}^{s-1}}{\\sigma(Q)Z_{s-1}}\\) (9)\n\\(A_{query} = \\frac{\\sigma(Q)M_{query}^{s-1}}{\\sigma(Q)Z_{s-1}}\\) (10)\nLong-term context injection. First, we apply a linear layer to aggregate Aall and Aquery. Then, we aggregate the retrieved content and the local attention Alocal using a learned gating scalar \\(\beta\\):\n\\(\\gamma = sigmoid(W_{g}A_{query})\\) (11)\n\\(A_{ret} = \\gamma A_{query} + (1 - \\gamma) A_{all}\\) (12)\n\\(A = sigmoid(\beta) A_{ret} + (1 - sigmoid(\beta)) A_{local}\\) (13)\nwhere Wg \u2208 R1\u00d7dvalue is a trainable weight that dynamicly merges the two retieved contents. B contains a single scalar value per head as training parameter, enabling a learnable trade-off between the long-term and local information flows in the model.\nRepeat query instruction. To incorporate query instructions into the model, we concatenate the query instruction with the document as the input of model. During local attention, the query states Qquery of the query instruction are utilized to compute query-focused memory within each segment. However, when generating summaries, the retrieved information from memory fails to effectively guide the model in producing summaries that adhere to the query instructions. To address this issue, we employ a straightforward approach: we replicate the query instruction at the end of the document. This ensures that the query instruction is within the window of the local attention computation when generating summaries, enabling the model to accurately generate query-relevant summaries."}, {"title": "3 Experiments", "content": "3.1 Datasets\nWe evaluate our approach on three query-focused summarization datasets: CovidET (Zhan et al., 2022), QMsum (Zhong et al., 2021), SQUALITY (Wang et al., 2022). Different from others, SQUALITY includes multiple summaries for each question. The input documents in the CovidET and QMSum (Golden) datasets have token counts of 228 and 2670, respectively, when tokenized using the LLama2 tokenizer. In contrast, the QMSum and SQUALITY datasets feature longer input token lengths, with 8071 and 13227 tokens, respectively.\n3.2 Evaluation Metrics\nWe evaluate the summaries using ROUGE metrics (Lin, 2004), including ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum. Additionally, we use a BART-base version of BERTScore (Zhang et al., 2020), which leverages BART to compute the similarity between the references and the model's outputs. Specifically, since SQUALITY includes multiple summaries for each question, we report multi-reference scores for all metrics following Wang et al. (2022). We calculate the metrics for each pair of a generated summary and multiple references, then choose the maximum score.\n3.3 Implementation Details\nWe use the pre-trained LLaMA (2-7B, 3-8B) (Touvron et al., 2023) with N = 32 transformer layers as the backbone model. For IDEALPrompt, we follow LLaMA-Adapter-v1 (Zhang et al., 2023), adopting a prompt length K = 10 and applying prompts to the last 30 layers, with the prompts of the last 15 layers are generated. For IDEALPAdapter, adapters are applied to the first 16 layers and generated for the last 16 layers. For IDEALLORA, only the A matrix in the LoRA module is generated for the last 16 layers.\n3.4 Comparison of Methods\nWe compare our approaches with several fully fine-tuned pretrained language models commonly used for summarization tasks, including Bart-base and Bart-large (Lewis et al., 2019), LED (Beltagy et al., 2020), LED-base-OASum (Yang et al., 2023b), HMNet (Zhu et al., 2020). For long document datasets, we compare our approaches against an extract-then-summarize methods (Wang et al., 2022). Unlimiformer (Bertsch et al., 2024), a retrieval-based approach that augments pretrained language models to handle unlimited-length input.\n3.5 Main Results\nTables 1-2 present the results on QFS datasets. Our approaches achieve the best results and show significant improvements over other baselines. IDEAL consistently outperform the corresponding PEFT Adapters with the same input size. For instance, on CovidET dataset, IDEALLORA surpasses the best baseline LoRA by 1.64 ROUGE-L points and 2.36 ROUGE-Lsum points with the same input size of 1.6K.\nFor the two long document datasets showed in Table 2, IDEALLORA with an input length of 8K achieved the best results, while IDEALQF Inf also performed exceptionally well even under limited GPU memory. For example, on QMSum dataset, IDEALLORA surpasses all baselines on ROUGE-L and and BERTScore.\n3.6 Ablation Study\nDifferent adapter for IDEAL. As shown in Table 1, we compare the performance of IDEAL on different Adapter with same input size. On the CovidET dataset, the performance differences among the three adapters on IDEAL were minimal. However, on the QMSum(Golden) dataset, IDEALLORA outperformed IDEALPAdapter by 1.48 ROUGE-L points under the same input length of 768. Overall, IDEALLORA achieves the best results on four datasets.\nThe effectiveness of each module in IDEALQF Inf In Table 4, we evaluated the effectiveness of Query-focused Infini-attention through comparative testing. First, we implemented Infini-attention based on LoRA as Lora+Inf and observed significant improvements\n3.7 Indepth Analysis\nPerformance of low memory IDEAL. IDEALLORA consistently demonstrates improved performance as input length increases. However, this comes at the cost of increased GPU memory consumption. Table 4 illustrates this trade-off, showcasing IDEALLORA performance on input lengths of 1.6K, 3.8K, and 8K, requiring 24G, 40G, and 80G of memory, respectively. In contrast to IDEALLORA, our proposed IDEALQF Inf exhibits memory efficiency when handling long inputs. IDEALQF Inf maintains a consistent memory footprint 24G regardless of input length. Notably, on the QMsum dataset, IDEALQF Inf outperforms IDEALLORA with an input length of 1.6K on all metrics within a same 24GB memory constraint. Moreover, it surpasses IDEALLORA with an input length of 3.8K in 40GB memory on the ROUGE-L metric and achieves performance close to IDEALLORA with an input length of 8K in 80GB memory.\nTrainable parameters comparison. In Table 3, we compare the performance of different IDEAL HyperExperts under the same parameter count. The Prompt-tuning method can adjusts parameter count only by controlling prompt length, with experiments from Hu et al. (2023) indicating optimal performance at a prompt length of 10. Despite having the fewest trainable parameters, its performance on the QMSum(Golden) dataset is the lowest. With the same parameter count, LoRA with a rank of 16 still significantly underperforms compared to IDEALLORA, highlighting the effectiveness of HyperExpert. IDEALPAdapter can improve performance by increasing the bottleneck size, but even with 89.5M parameters, it is still inferior to IDEALLORA with 24.5M parameters. Overall, IDEALLORA achieves the best performance and parameter efficiency.\nLocal context size of IDEAL F Inf Figure 3 presents the performance of IDEAL FInf under varying local context sizes (LC). On the QMSum dataset, the model exhibits stable performance when LC is beyond 400, achieving nearly the best overall performance at LC=800. Similarly, on the SQUALITY dataset, the optimal LC is observed at 1.6K. These findings indicate that IDEAL FInf differs from IDEALLORA, the limited memory for the former is enough to handle extremely long inputs.\nMax input length of IDEALQF Inf. Table 4 presents the optimal input length for IDEAL FInf on the QMsum and SQuALITY datasets. The results suggest that information relevant to the query in the QMsum dataset is primarily concentrated within the first 6000 tokens, while in the SQUALITY dataset, the relevant information is more evenly distributed throughout the document."}, {"title": "4 Related Works", "content": "Query-focused Summarization. Tan et al. (2020) and Yang et al. (2023b) address QFS by prepending the query or aspect to the input document and fine-tuning pre-trained models in an end-to-end manner. Zhong et al. (2021), Wang et al. (2022), and Amar et al. (2023) employ extract-then-summarize strategies that use a filter model to extract key parts of the document based on the query, then fitting the shorter text into a summarizer. Yang et al. (2023a) reveal that the performance of ChatGPT is comparable to traditional fine-tuning methods in terms of ROUGE scores on QFS tasks.\nLong-context Transformers. Unlimiformer (Bertsch et al., 2024) enhances pre-trained models like BART (Lewis et al., 2019) to handle unlimited inputs without additional learned weights by employing a retrieval-based long-context method. Infini-transformer (Munkhdalai et al., 2024) integrates long-term context compressive memory into vanilla transformers, enabling Transformer-based LLMs to scale to infinitely long contexts after full continual pre-training. Unlike Infini-transformer, we explore the compressive memory method on adapter-based PEFT of LLMs and design a query-focused infini-attention for QFS tasks."}, {"title": "5 Conclusion", "content": "In this paper, we propose IDEAL, an efficient query-aware adaptation method on LLMs for QFS tasks, which consists of two modules: Query-aware HyperExpert and Query-focused Infini-attention. The two modules enable LLMs to achieve finegrained query-LLM alignment efficiently and have the ability to handle lengthy documents."}, {"title": "Limitations", "content": "Due to the absence of longer QFS datasets currently available, we explored IDEAL only on datasets with input lengths around 10k. However, it is necessary to validate IDEAL on datasets with longer input documents, such as performing QFS tasks across entire books. Further validation and optimization of the IDEAL method on book-length inputs would be both interesting and meaningful."}]}