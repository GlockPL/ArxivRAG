{"title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization", "authors": ["Jie Cao", "Dian Jiao", "Qiang Yan", "Wenqiao Zhang", "Siliang Tang", "Yueting Zhuang"], "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "In today's world, where we are constantly bombarded with vast amounts of text, the ability to efficiently summarize information has become crucial. Textual summarization, the process of condensing a lengthy document into a succinct and digestible version while preserving the most crucial information, enabling quicker understanding and better management of information. As everyone has unique needs and interests in real-life scenarios, necessitating summarizers that succinctly address the information needed for a specific query by extracting essential information from documents, i.e., Query-Focused Summarization. This task involves analyzing the content to identify key themes and then highlighting these in the summary, which draws increasing attention in the textual summarization community.\nTraditionally, QFS has used extract-then-summarize methods that rely on the most relevant spans of text from a candidate document based on the prevalence of query terms. Further onwards, the triumph of Large Language Models (LLMs) such as the GPT series, LLaMA and other open-source LLMs showcased the power of large-scale pretraining in understanding, reasoning and generating intricate textual patterns, the great potential of LLMs offering new opportunities for QFS. However, there has been relatively little investigation into LLMs-based QFS methods. Our primary goal in this paper is to close this gap correspondingly by proposing two indispensable characteristics that should be harnessed by LLMs while dealing with QFS: (i) Efficiently Fine-grained Query-LLM Alignment, as commonly known, the pre-trained LLMs are powerful when transferred to downstream tasks with instruction tuning, this also applies to the QFS task when the LLMs specialized for user's interests. However, as the parameter number grows exponentially to billions or even trillions, it becomes very inefficient to save the fully fine-tuned parameters for each downstream task. Besides, the different data distribution of diverse user's queries or instructions may introduce the negative transfer in the training stage. This implies the QFS model should minimize the potential interference among different user instructions, thereby accessing the fine-grained query-LLM alignment. (ii) Lengthy Document Summarization, general LLMs can't handle long text inputs due to the huge amount of memory required during training. Besides, the simple approach of concatenating the query to the input document is insufficient for effectively guiding the model to focus on the query while generating the summary. How to process the lengthy documents is also an important characteristic of LLMs-based QFS approaches. Summing up, these characteristics necessitate a thorough reevaluation of QFS and its corresponding solutions with LLMs.\nBased on the aforementioned insights, we propose Infinite and Dynamic largE language modeL-based framework, abbreviated as IDEAL for ideal QFS, which consists of two modules: Query-aware HyperExpert and Query-focused Infini-attention achieve the two indispensable characteristics, respectively. The Query-aware HyperExpert leverages the parameter-efficient fine-tuning (PEFT) strategies that enable a model to perform a new task with minimal parameter updates. Innovatively, we tailor the previous PEFT approaches to QFS tasks with a HyperNetwork, which can dynamically generate the strongly correlated LLM's parameter shifts according to users' queries. Such dynamic characterization allows us to achieve the best of both worlds by adjusting the LLM's parameters while encouraging the model to adapt to each individual instance. By doing so, efficient and fine-grained query-LLM alignment can be achieved. Notably, we develop three types of HyperExpert, including Prompt-tuning, Parallel Adapter, and Low-Rank Adaptation. To process long documents with bounded memory and computation, we propose incorporating a Query-focused Infini-attention module into IDEAL. Infini-attention includes a long-term compressive memory and local causal attention for efficiently modeling both long- and short-range contextual dependencies. Our Query-focused Infini-attention possesses an extra query-focused compressive memory to better retain parts of the input documents that are strongly correlated with the query.\nOur contributions can be summarized as follows:\n\u2022 We explored query-focused PEFT methods and proposed a method, IDEAL, that tunes instance-level PEFT approaches according to query instructions, enhancing the model's fine-grained instruction-following capabilities.\n\u2022 We propose to incorporate a query-focused infini-attention module to process long text under low memory resources for QFS tasks. For example, IDEAL with the backbone model LLAMA2-7B can process datasets where the average length of input tokens is 13,000 on a single 24GB Nvidia GeForce RTX 3090.\n\u2022 We performed extensive and rigorous experiments across multiple QFS datasets. IDEAL significantly outperforms other baselines."}, {"title": "2 Methodology", "content": "Overview. Given a query and a document, the QFS task aims to generate a summary tailored to this query. Inspired by recent Hypernetwork-based methods , our IDEAL generate instance-level adapters according to the query instruction using an additional HyperNetwork. For long-text QFS datasets, we propose a Query-focused Infini-attention module that can be integrated into IDEAL, enabling the summarization of infinitely long texts under low-memory constraints. In our experiments, we use LLAMA as the underlying model, a popular decoder-only LLM. However, our overall approach can be applied to any generic decoder-only transformer model. In Section 2.1, we first describe the details of IDEAL, including $IDEAL_{Prompt}$, $IDEAL_{PAdapter}$, and $IDEAL_{LoRA}$. Then, Section 2.2 presents the query-focused infini-attention."}, {"title": "2.1 Query-aware HyperExpert Module", "content": "Given a dataset with input text pairs containing a query and a document, and outputs in the form of a summary, and a pre-trained LLaMA with an N-layer transformer, IDEAL based on three kinds of PEFT adapters to fine-tune LLaMA to generate query-focused summaries respectively. For example, $IDEAL_{LoRA}$, we place a regular (non-generated) LoRA layer in the first l layers, then we use the hidden representation $H_{query}$ of query in l-th layer as the input of a Hypernetwork to generate the LoRA parameters for the last N \u2013 l layers.\nPEFT approaches. With the growth in model sizes, fine-tuning methods have advanced significantly, modifying only a small number of parameters or adding new ones to a frozen language model for specific tasks. These methods often achieve performance comparable to full model fine-tuning. In this paper, we use three types of PEFT methods, including prompt tuning, parallel adapter, and LoRA, as baselines to investigate our approach.\nAs shown in Figure 1(a), Prompt tuning can add soft prompts to the hidden states in attention layers to guide model learning and adapt to new tasks, where only the soft prompts are updated during training. LLaMA-Adapter-v1 introduces a zero-initialized attention mechanism into prompt tuning, which adaptively incorporates the knowledge from soft prompts. We use this LLaMA-Adapter-v1 as our prompt tuning baseline.\nParallel adapters aim to incorporate additional learnable networks in parallel with distinct sublayers within the backbone model. To reduce the number of parameters, small bottleneck networks are used as parallel adapters. In transformer-based LLMs, parallel adapters can be applied to both the feedforward and self-attention modules in each transformer block. Hu et al. (2023) conducted experiments showing that applying parallel adapters only to the feedforward module achieves the best results on math reasoning datasets. As shown in Figure 1(c), we also apply parallel adapters only to feedforward module in LLaMA's transformer block.\nLORA adds trainable low-rank decomposition matrices in parallel to existing weight matrices. For a pre-trained weight matrix $W \\in R^{d\\times k}$, LoRA constrains its update by adding low-rank matrix pairs, resulting in $W + \\Delta W = W + BA$, where $B \\in R^{d\\times r}$, $A \\in R^{r\\times k}$, and the rank $r < min(d, k)$. During training, W is frozen while B and A are trainable. LORA initializes A randomly and B to zero, ensuring that $\\Delta W = BA$ starts from zero at the beginning of training, thereby preserving the pre-trained knowledge as much as possible.\nAdapter-based HyperExpert. Previous works indicate that hypernetworks can learn the parameter information of the main neural network under different input scenarios and efficiently adjust the target network's parameters to adapt to this information. We propose generating query-focused adapters conditioned on the query instruction using a hypernetwork.\nOur hypernetwork is a bottleneck network that consists of an encoder to transform the mean-pooling of the query representation $H_{query}$ into a low-dimensional representation h, and a decoder to convert h into the parameters of the target adapters. For example, the computation of $IDEAL_{LoRA}$ is as follows:\n$h = dropout(ReLU(W_{mean}(H_{query}) + b_{o})) \\quad (1)$\n$A_{q} = W_{1}h + b_{1} \\quad (2)$\n$A_{k} = W_{2}h + b_{2} \\quad (3)$\nwhere $A_{q}$ and $A_{k}$ correspond to $W_{q}$ and $W_{k}$ in self-attention, respectively. We only generate the A matrix in the LoRA module, initializing B to zero and updating it during training as in the original LoRA. This ensures that $\\Delta W = BA$ starts from zero at the beginning of training. Unlike $IDEAL_{LoRA}$, $IDEAL_{Prompt}$ and $IDEAL_{PAdapter}$ generate all the parameters of the target adapters in the required layers.\nIn addition, each layer that needs to generate the target adapters has its own encoder, as shown in Equation 1, and shares a single decoder. This allows for generating different parameters for each layer and reduces the number of trainable parameters."}, {"title": "2.2 Query-focused Infini-attention Module", "content": "QFS tasks usually involve long documents. However, Transformer-based LLMs can't handle such long texts due to the quadratic complexity of the attention mechanism in terms of both memory usage and computation time. Infini-attention incorporates a compressive memory and a long-term linear attention mechanism into vanilla Transformer block, scale Transformer-based LLMs to extremely long inputs with bounded memory. However, due to the information loss inherent in compressive memory modules, in QFS tasks, the model tends to lose crucial query instruction details and relevant document information after compressing query instruction and very long input documents. To minimize the information loss of query-related details in Infini-attention, we propose compressing the query-related document information into an additional memory block, termed Query-focused Infini-attention.\nSimilar to Infini-attention, the input tokens are segmented to perform standard causal dot-product attention within each segment. Before local attention for current segment is complete, we compress the cached key-value (KV) attention states into two memory blocks. One block maintains the entire context history, while another focuses on query-related information. These compressed memories are then available for subsequent segments to retrieve relevant context.\nFixed length local attention. A key-value (KV) cache is typically used in LLMs for fast and efficient inference. To maintain fine-grained local attention, for each segment, multi-head self-attention $A_{local} \\in R^{L\\times d_{value}}$ is computed with a fixed KV length L in both the training and inference stages using the KV cache. In detail, when the last segment length is less than L, we use the KV cache to extend the length of the current KV states to L for computing the local attention and compress the remaining KV cache into the memory.\nMemory update. For the s-th segment with length L, before computing the local attention, we update the full context memory $M_{all}^{s-1} \\in R^{d_{key}\\times d_{value}}$ and the query-focused memory $M_{query}^{s-1} \\in R^{d_{key}\\times d_{value}}$, and a normalization term $Z_{s-1} \\in R^{d_{key}}$ is then used for memory retrieval as follows:\n$M_{all}^{s} \\leftarrow M_{all}^{s-1} + \\sigma(K_{cache})TV_{cache} \\quad (4)$\n$M_{query}^{s} \\leftarrow M_{query}^{s-1} + \\sigma(K_{cache})TV_{cache} \\quad (5)$\n$Z_{s-1} \\leftarrow Z_{s-2} + \\sum_{t=1}^{L}\\sigma(K_{cache}) \\quad (6)$\nwhere $\\sigma$ is a nonlinear activation function. Following the work of Katharopoulos et al. (2020) and Munkhdalai et al. (2024), we employ element-wise ELU+1 as the activation function. The term $\\sigma(K)TV$ on the right side of Equation 4 and 5 is referred to as an associative binding operator . The query-focused memory $M_{query}^{s-1}$ differs from the full context memory only in the value states $V_{cache}$ used within the associative binding operator. We ultilize the query states $Q_{query}$ of query instruction to scale the value states, and keep only query-related information $A_{cache}$ as\n$\\alpha_{i} = sigmoid(\\frac{mean(Q_{query}) (K_{cache})^T}{\\sqrt{d_{model}}})V_{cache} \\quad (7)$\n$\\alpha V_{cache} \\quad (8)$\nHere, we use the mean pooling of $Q_{query}$ and the key states to compute a related score for each representation.\nMemory retrieval. After updating the memory, we retrieve new content $A_{all} \\in R^{L\\times d_{value}}$ and $A_{query} \\in R^{L\\times d_{value}}$ from the full context memory $M_{all}^{s-1}$, and the query-focused memory $M_{query}^{s-1}$, respectively. This retrieval is performed using the query states $Q \\in R^{L\\times d_{key}}$ as follows:\n$A_{all} = \\frac{\\sigma(Q)M_{all}^{s-1}}{\\sigma(Q)Z_{s-1}} \\quad (9)$\n$A_{query} = \\frac{\\sigma(Q)M_{query}^{s-1}}{\\sigma(Q)Z_{s-1}} \\quad (10)$\nLong-term context injection. First, we apply a linear layer to aggregate $A_{all}$ and $A_{query}$. Then, we aggregate the retrieved content and the local attention $A_{local}$ using a learned gating scalar $\\beta$:\n$\\gamma = sigmoid(W_{g}A_{query}) \\quad (11)$\n$A_{ret} = \\gamma \\cdot A_{query} + (1 - \\gamma) \\cdot A_{all} \\quad (12)$\n$A = sigmoid(\\beta) \\cdot A_{ret} + (1 - sigmoid(\\beta)) \\cdot A_{local} \\quad (13)$\nwhere $W_{g} \\in R^{1\\times d_{value}}$ is a trainable weight that dynamicly merges the two retieved contents. B contains a single scalar value per head as training parameter, enabling a learnable trade-off between the long-term and local information flows in the model.\nRepeat query instruction. To incorporate query instructions into the model, we concatenate the query instruction with the document as the input of model. During local attention, the query states $Q_{query}$ of the query instruction are utilized to compute query-focused memory within each segment. However, when generating summaries, the retrieved information from memory fails to effectively guide the model in producing summaries that adhere to the query instructions. To address this issue, we employ a straightforward approach: we replicate the query instruction at the end of the document. This ensures that the query instruction is within the window of the local attention computation when generating summaries, enabling the model to accurately generate query-relevant summaries."}, {"title": "3 Experiments", "content": "We evaluate our approach on three query-focused summarization datasets: CovidET, QMsum, SQUALITY. Different from others, SQUALITY includes multiple summaries for each question. The input documents in the CovidET and QMSum (Golden) datasets have token counts of 228 and 2670, respectively, when tokenized using the LLama2 tokenizer. In contrast, the QMSum and SQUALITY datasets feature longer input token lengths, with 8071 and 13227 tokens, respectively.\n3.2 Evaluation Metrics\nWe evaluate the summaries using ROUGE metrics, including ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum. Additionally, we use a BART-base version of BERTScore, which leverages BART to compute the similarity between the references and the model's outputs. Specifically, since SQUALITY includes multiple summaries for each question, we report multi-reference scores for all metrics following Wang et al. (2022). We calculate the metrics for each pair of a generated summary and multiple references, then choose the maximum score.\n3.3 Implementation Details\nWe use the pre-trained LLaMA (2-7B, 3-8B) with N = 32 transformer layers as the backbone model. For $IDEAL_{Prompt}$, we follow LLaMA-Adapter-v1, adopting a prompt length K = 10 and applying prompts to the last 30 layers, with the prompts of the last 15 layers are generated. For $IDEAL_{PAdapter}$, adapters are applied to the first 16 layers and generated for the last 16 layers. For $IDEAL_{LoRA}$, only the A matrix in the LoRA module is generated for the last 16 layers."}, {"title": "3.4 Comparison of Methods", "content": "We compare our approaches with several fully fine-tuned pretrained language models commonly used for summarization tasks, including Bart-base and Bart-large, LED, LED-base-OASum, HMNet. For long document datasets, we compare our approaches against an extract-then-summarize methods. Unlimiformer, a retrieval-based approach that augments pretrained language models to handle unlimited-length input.\n3.5 Main Results\nTables 1-2 present the results on QFS datasets. Our approaches achieve the best results and show significant improvements over other baselines. IDEAL consistently outperform the corresponding PEFT Adapters with the same input size. For instance, on CovidET dataset, $IDEAL_{LoRA}$ surpasses the best baseline LoRA by 1.64 ROUGE-L points and 2.36 ROUGE-Lsum points with the same input size of 1.6K.\nFor the two long document datasets showed in Table 2, $IDEAL_{LoRA}$ with an input length of 8K achieved the best results, while $IDEAL_{LoRA QF_Inf}$ also performed exceptionally well even under limited GPU memory. For example, on QMSum dataset, $IDEAL_{LoRA QF_Inf}$ surpasses all baselines on ROUGE-L and and BERTScore.\n3.6 Ablation Study\nDifferent adapter for IDEAL. As shown in Table 1, we compare the performance of IDEAL on different Adapter with same input size. On the CovidET dataset, the performance differences among the three adapters on IDEAL were minimal. However, on the QMSum(Golden) dataset, $IDEAL_{LoRA}$ outperformed $IDEAL_{PAdapter}$ by 1.48 ROUGE-L points under the same input length of 768. Overall, $IDEAL_{LoRA}$ achieves the best results on four datasets.\nThe effectiveness of each module in $IDEAL_{LoRA}^{QF_Inf}$ In Table 4, we evaluated the effectiveness of Query-focused Infini-attention through comparative testing. First, we implemented Infini-attention based on LoRA as Lora+Inf and observed significant improvements compared to LoRA alone under the same GPU memory constraints, with increases of 1.55 and 1.33 points in ROUGE-L and ROUGE-Lsum on QMSum dataset, respectively. These results indicate that compressing the key-value states of historical segments enables summarization of long documents within limited GPU memory. Furthermore, we enhanced $IDEAL_{LoRA}$ with Infini-attention, achieving better results than Lora+Inf in ROUGE-L. The $IDEAL_{LoRA}$ method integrated with Query-focused Infini-attention as $IDEAL_{LoRA}^{QF_Inf}$ outperformed both $IDEAL_{LoRA}$+Inf and Lora+Inf in all metrics, demonstrating that our proposed Query-focused Infini-attention effectively compresses query-related information."}, {"title": "3.7 Indepth Analysis", "content": "Performance of low memory IDEAL. $IDEAL_{LoRA}$ consistently demonstrates improved performance as input length increases. However, this comes at the cost of increased GPU memory consumption. Table 4 illustrates this trade-off, showcasing $IDEAL_{LoRA}$ performance on input lengths of 1.6K, 3.8K, and 8K, requiring 24G, 40G, and 80G of memory, respectively. In contrast to $IDEAL_{LoRA}$, our proposed $IDEAL_{LoRA}^{QF_Inf}$ exhibits memory efficiency when handling long inputs. $IDEAL_{LoRA}^{QF_Inf}$ maintains a consistent memory footprint 24G regardless of input length. Notably, on the QMsum dataset, $IDEAL_{LoRA}^{QF_Inf}$ outperforms $IDEAL_{LoRA}$ with an input length of 1.6K on all metrics within a same 24GB memory constraint. Moreover, it surpasses $IDEAL_{LoRA}$ with an input length of 3.8K in 40GB memory on the ROUGE-L metric and achieves performance close to $IDEAL_{LoRA}$ with an input length of 8K in 80GB memory.\nTrainable parameters comparison. In Table 3, we compare the performance of different IDEAL HyperExperts under the same parameter count. The Prompt-tuning method can adjusts parameter count only by controlling prompt length, with experiments from Hu et al. (2023) indicating optimal performance at a prompt length of 10. Despite having the fewest trainable parameters, its performance on the QMSum(Golden) dataset is the lowest. With the same parameter count, LoRA with a rank of 16 still significantly underperforms compared to $IDEAL_{LoRA}$, highlighting the effectiveness of HyperExpert. $IDEAL_{PAdapter}$ can improve performance by increasing the bottleneck size, but even with 89.5M parameters, it is still inferior to $IDEAL_{LoRA}$ with 24.5M parameters. Overall, $IDEAL_{LoRA}$ achieves the best performance and parameter efficiency.\nLocal context size of $IDEAL_{LoRA}^{QF_Inf}$. Figure 3 presents the performance of $IDEAL_{LoRA}^{QF_Inf}$ under varying local context sizes (LC). On the QMSum dataset, the model exhibits stable performance when LC is beyond 400, achieving nearly the best overall performance at LC=800. Similarly, on the SQUALITY dataset, the optimal LC is observed at 1.6K. These findings indicate that $IDEAL_{LoRA}^{QF_Inf}$ differs from $IDEAL_{LoRA}$, the limited memory for the former is enough to handle extremely long inputs.\nMax input length of $IDEAL_{LoRA}^{QF_Inf}$. Table 4 presents the optimal input length for $IDEAL_{LoRA}^{QF_Inf}$ on the QMsum and SQuALITY datasets. The results suggest that information relevant to the query in the QMsum dataset is primarily concentrated within the first 6000 tokens, while in the SQUALITY dataset, the relevant information is more evenly distributed throughout the document."}, {"title": "4 Related Works", "content": "Query-focused Summarization. Tan et al. (2020) and Yang et al. (2023b) address QFS by prepending the query or aspect to the input document and fine-tuning pre-trained models in an end-to-end manner. Zhong et al. (2021), Wang et al. (2022), and Amar et al. (2023) employ extract-then-summarize strategies that use a filter model to extract key parts of the document based on the query, then fitting the shorter text into a summarizer. Yang et al. (2023a) reveal that the performance of ChatGPT is comparable to traditional fine-tuning methods in terms of ROUGE scores on QFS tasks.\nLong-context Transformers. Unlimiformer enhances pre-trained models like BART (Lewis et al., 2019) to handle unlimited inputs without additional learned weights by employing a retrieval-based long-context method. Infini-transformer integrates long-term context compressive memory into vanilla transformers, enabling Transformer-based LLMs to scale to infinitely long contexts after full continual pre-training. Unlike Infini-transformer, we explore the compressive memory method on adapter-based PEFT of LLMs and design a query-focused infini-attention for QFS tasks."}, {"title": "5 Conclusion", "content": "In this paper, we propose IDEAL, an efficient query-aware adaptation method on LLMs for QFS tasks, which consists of two modules: Query-aware HyperExpert and Query-focused Infini-attention. The two modules enable LLMs to achieve fine-grained query-LLM alignment efficiently and have the ability to handle lengthy documents."}, {"title": "Limitations", "content": "Due to the absence of longer QFS datasets currently available, we explored IDEAL only on datasets with input lengths around 10k. However, it is necessary to validate IDEAL on datasets with longer input documents, such as performing QFS tasks across entire books. Further validation and optimization of the IDEAL method on book-length inputs would be both interesting and meaningful."}]}