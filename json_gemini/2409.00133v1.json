{"title": "A Survey for Large Language Models in Biomedicine", "authors": ["Chong Wang", "Mengyao Li", "Junjun He", "Zhongruo Wang", "Erfan Darzi", "Zan Chen", "Jin Ye", "Tianbin Li", "Yanzhou Su", "Jing Ke", "Kaili Qu", "Shuxin Li", "Yi Yu", "Pietro Li\u00f2", "Tianyun Wang", "Yu Guang Wang", "Yiqing Shen"], "abstract": "Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures, lacking a comprehensive analysis that integrates the latest advancements across various biomedical domains. This review, based on an analysis of 484 publications sourced from databases including PubMed, Web of Science, and arXiv, provides an in-depth examination of the current landscape, applications, challenges, and prospects of LLMs in biomedicine, distinguishing itself by focusing on the practical implications of these models in real-world biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning across a broad spectrum of biomedical tasks, including diagnostic assistance, drug discovery, and personalized medicine, among others, with insights drawn from 137 key studies. Then, we discuss adaptation strategies of LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their performance in specialized biomedical contexts where zero-shot fails to achieve, such as medical question answering and efficient processing of biomedical literature. Finally, we discuss the challenges that LLMs face in the biomedicine domain including data privacy concerns, limited model interpretability, issues with dataset quality, and ethics due to the sensitive nature of biomedical data, the need for highly reliable model outputs, and the ethical implications of deploying AI in healthcare. To address these challenges, we also identify future research directions of LLM in biomedicine including federated learning methods to preserve data privacy and integrating explainable AI methodologies to enhance the transparency of LLMs. As this field of LLM rapidly evolves, continued research and development are essential to fully harness the capabilities of LLMs in biomedicine while ensuring their responsible and effective deployment.", "sections": [{"title": "1 Introduction", "content": "General-purpose large language models (LLMs) such as PaLM [1], LLaMA [2, 3], and the GPT series [4, 5] have demonstrated their versatility across a wide range of tasks. These models excel in complex language understanding and generation tasks, including translation, summarization, and nuanced question answering [6]. The advancements in LLM capabilities can be largely attributed to the evolution of deep learning algorithms, particularly the introduction and subsequent optimization of the Transformer architecture [7]. As LLMs continue to mature, their potential applications across various domains are becoming increasingly apparent, with the biomedical field emerging as a particularly promising area of impact. This timeline illustrates the rapid evolution of both unimodal and multimodal LLMs. Notable achievements in biomedical LLMs showcase the breadth and depth of their impact. For instance, MedPaLM [8] has attained a 92.9% agreement with clinical experts in providing detailed medical answers and reaching scientific consensus. In the realm of genomics, scBERT [9] generates embeddings for each gene using an improved Performer architecture, enhancing the analysis of single-cell genomic data. The development of domain-specific LLMs like HuatuoGPT [10], ChatDoctor [11], and BenTsao [12] demonstrates the capability for reliable medical dialogue, showcasing the potential of LLMs in clinical communication and decision support. The progression from predominantly unimodal LLMs to an increasing number of multimodal LLM approaches reflects the growing adaptability of LLMs in addressing complex biomedical challenges. This shift enables the integration of diverse data types, such as text, images, and structured clinical data.\nThe rapid growth and diversification of LLM research in biomedicine are further evidenced by the trends shown in Fig. 2. A temporal analysis of LLM research papers in biomedical fields from 2018 to 2024 reveals an increase in publications, with a surge beginning in 2021 (Fig. 2a). This trend underscores the growing interest and investment in applying LLMs to biomedical challenges, reflecting both the technological advancements and the recognition of LLMs' potential to address healthcare and research needs. The distribution of these research papers across various biomedical fields highlights 'medicine' and 'neuroscience' as the dominant areas of focus (Fig. 2b). This distribution demonstrates the broad applicability of LLMs across different medical specialties and research domains, while also indicating potential areas for future expansion and development.\nThe biomedical field encompasses a vast array of disciplines, from fundamental biological research to complex clinical applications, each characterized by specialized terminology and a evolving knowledge base [13]. This breadth and depth present challenges for the application of LLMs in biomedicine. The continuous influx of new research findings, treatment modalities, and pharmaceutical developments demands models capable of adapting to and integrating novel information swiftly [14]. Moreover, the high-stakes nature of biomedical applications necessitates an exceptionally high standard of accuracy and reliability from LLMs, which is a benchmark that current models may not consistently meet [15, 16]. This shortcoming stems from the general-purpose nature of many LLMs, which can lead to misinterpretations and inference biases when confronted with the nuanced, context-dependent language of biomedical texts [17]. Furthermore, the field's reliance on sensitive patient data introduces additional complexities, requiring strict adherence to data protection and privacy regulations, which poses both technical and ethical challenges in implementation [18]. Despite these hurdles, the potential for LLM applications in biomedicine remains promising. Models like BioMedLM [19] demonstrate the capacity to accelerate scientific insight acquisition, while methods such as BianQue [20] and DISC-MedLLM [21] show potential in providing medical advice during patient consultations, potentially alleviating clinical workloads. However, the widespread adoption of these applications hinges on specialized training and optimization of LLMs to enhance their reliability and specificity in biomedical contexts.\nWhile several surveys have explored the applications of LLMs in biomedicine, our review stands out due to its comprehensive scope and interdisciplinary approach. Unlike previous surveys that often focused on specific applications or model architectures, we provide an in-depth analysis of LLMs across various biomedical fields, ranging from genomics to clinical practice. Covering the period from 2019 to 2024, we offer insights into the latest developments and future trends, including both unimodal and multimodal LLM approaches. This review is based on an analysis of 484 publications from multiple databases, providing a thorough examination of the current state, applications, challenges, and prospects of LLMs in biomedicine. We evaluate the zero-shot performance of LLMs across various biomedical tasks, analyze adaptation strategies for both unimodal and multimodal approaches, and identify specific challenges faced by LLMs in biomedical applications, proposing potential solutions. By exploring the potential impact of LLMs on medical practice, biomedical research, and healthcare systems, our goal is to provide researchers, healthcare professionals, and policymakers with a clear roadmap to understand and leverage LLMs in biomedicine, facilitating informed decision-making and guiding future research efforts."}, {"title": "2 Background", "content": "Through extensive pre-training and fine-tuning, LLMs are capable of learning and capturing complex patterns and semantic relationships within language. In the following sections, we provide a detailed overview of the core structures of LLMs, their common model architectures, and fine-tuning techniques. The design of LLMs typically relies on the Transformer architecture and can be categorized into three main types: encoder-only, decoder-only, and encoder-decoder [22]. Each architecture has distinct advantages and is suited for different types of tasks."}, {"title": "2.1 Encoder-Only Architecture", "content": "Encoder-only models focus on understanding and representing input text [23]. These models are particularly adept at tasks that require deep contextual understanding, such as text classification, named entity recognition, and sentiment analysis. The Bidirectional Encoder Representations from Transformers (BERT) [23] is an example of this architecture. BERT's key innovation is its bidirectional nature, allowing it to capture context from both left and right sides of each word in a sentence. This bidirectional encoding provides a richer representation of text compared to previous unidirectional models. BERT achieves this through its \"masked language model\" pre-training objective, where the model learns to predict randomly masked words in a sentence, forcing it to consider the full context. Another notable encoder-only model is the Contrastive Language-Image Pretraining (CLIP) model [24]. CLIP extends the encoder architecture to multimodal learning, integrating both text and image inputs. By using contrastive learning, CLIP learns to align textual and visual representations in a shared embedding space. The application of encoder-only models has achieved significant advancements in specialized scientific domains, particularly in the biomedical field. Notable examples include scBERT [9], which generates fine-grained gene embeddings to process biomedical data, demonstrating exceptional performance in genomic analysis. Another prominent model, BioBERT [25], is specifically designed for biomedical text mining, enhancing tasks such as named entity recognition and relation extraction within scientific literature. These specialized adaptations highlight the versatility of encoder-only models in addressing complex biomedical challenges."}, {"title": "2.2 Decoder-Only Architecture", "content": "Decoder-only models are designed for generative tasks, producing output sequences from left to right. These models excel in text generation, dialogue systems, and creative writing applications. The Generative Pre-trained Transformer (GPT) series, culminating in the recent GPT-4, exemplifies this architecture with a unidirectional decoder structure, predicting each token based on the preceding context. This approach allows for coherent and contextually appropriate text generation. The GPT models are trained on vast corpora of text, enabling them to capture complex language patterns and generate human-like text across diverse domains. Other notable decoder-only models include LLaMA [2] and PaLM [1]. These models have optimized the decoder architecture for improved efficiency and scalability. LLaMA, for instance, demonstrates strong performance with fewer parameters than its predecessors, while PaLM showcases improved multitask learning capabilities across various NLP benchmarks. Decoder-only architectures have also been extended to multimodal applications. DALL\u00b7E [26], for example, uses a decoder to generate images from textual descriptions. In the biomedical domain, decoder-only models have shown promising applications. For instance, they have been adapted for medical report generation and drug discovery tasks, such as BioGPT [27], CancerGPT [28] and Med-PaLM [29]."}, {"title": "2.3 Encoder-Decoder Architecture", "content": "The encoder-decoder architecture, also known as the sequence-to-sequence (seq2seq) model, combines the strengths of both encoder and decoder components. This design makes it suitable for tasks that involve transforming one sequence into another, such as machine translation, text summarization, and question answering. In this architecture, the encoder processes the input sequence and compresses it into a latent representation. The decoder then uses this representation to generate the target sequence [30]. This separation of encoding and decoding allows the model to handle input and output sequences of different lengths and structures effectively. Two examples of encoder-decoder models are the Text-To-Text Transfer Transformer (T5) [31] and Bidirectional and Auto-Regressive Transformers (BART) [32] T5 adopts a unified approach by framing all NLP tasks as text-to-text problems, demonstrating remarkable versatility and strong multitask processing capabilities. BART, on the other hand, combines the bidirectional nature of BERT-like encoders with the autoregressive generation of GPT-like decoders, making it particularly effective for text generation and repair tasks. In biomedical applications, encoder-decoder models have shown significant potential. For instance, BioBART [33] has been adapted for biomedical text generation and summarization tasks. Another notable example is GeneCompass [34], a cross-species large language model designed to decipher gene regulatory mechanisms. These applications highlight the architecture's versatility in addressing complex biomedical challenges, from text processing to unraveling the intricacies of genetic regulation across different species."}, {"title": "3 LLMs in Zero-Shot Biomedical Applications", "content": "The potential of general-purpose LLMs has generated considerable interest in the biomedical field. This distribution highlights the current focus on OpenAI's models in biomedical research, with overlap between studies of different models indicating a trend towards comparative analysis. Despite the performance of these LLMs across various domains, their efficacy in addressing the unique challenges of the biomedical field remains uncertain. The specialized nature of biomedical terminology and the necessity to integrate specific clinical contexts pose challenges for these LLMs. To address this question, numerous studies have investigated the direct application of general-purpose LLMs in various biomedical disciplines, focusing on their performance in clinical diagnosis, decision support, drug development, genomics, personalized medicine, and biomedical literature analysis as elaborated in this section [15, 35, 36]."}, {"title": "3.1 Diagnostic Assistance", "content": "Diagnostic assistance is a biomedical technology that encompasses clinical diagnosis and decision support [37]. It analyzes patients' clinical data and symptoms, integrates medical knowledge with algorithmic processing, and provides recommendations to aid physicians in disease diagnosis and treatment decisions[38]. It aims to enhance diagnostic accuracy and efficiency, helping doctors better understand patients' conditions and formulate personalized treatment plans. To evaluate the zero-shot capabilities of general-purpose LLMs in biomedical diagnosis, researchers have designed a series of questions across various specialties. Studies have assessed LLM performance in oncology [39, 40], emergency medicine [41], ophthalmology [42, 43], and nursing [44], with results indicating that LLMs can achieve accuracy levels comparable to those of human experts in diagnostic tasks across these domains. Ward et al. [45] conducted a comparative study of LLM performance in neurosurgical scenarios. They created 30 clinical scenarios with consensus-based key points for answers and invited physicians of varying experience levels to respond to diagnostic questions. The results showed that GPT-4 achieved 100% accuracy in triage and diagnosis, while GPT-3.5 had an accuracy rate of 92.59%. These results highlight GPT-4's exceptional diagnostic accuracy, underscoring its potential as a reliable tool in clinical decision-making. In oncology, Deng et al.[46] found that GPT-4 achieved a 100% accuracy rate in triage and diagnosis across breast cancer clinical scenarios, aligning closely with senior medical professionals. Similarly, Haver et al. [39] demonstrated GPT-4's effectiveness in neurosurgery, where it achieved 100% accuracy in diagnosing and triaging neurosurgical cases, with perfect sensitivity and specificity. These findings highlight GPT-4's growing potential as a reliable tool in clinical decision-making across various medical fields."}, {"title": "3.2 Biomedical Omics and Drug Discovery", "content": "Biomedical science is an interdisciplinary field that encompasses drug development, genomics, and protein research, among other areas [47, 48]. It integrates engineering, biology, and medicine, utilizing advanced biotechnology techniques to study disease prevention, diagnosis, and treatment [49]. By exploring the molecular mechanisms of life processes, this field aims to develop novel biomedical approaches and pharmaceuticals to enhance human health and disease management. For instance, one study harnessed a LLM for candidate gene prioritization and selection, significantly improving the efficiency of identifying potential gene-disease associations. This approach utilized advanced natural language processing techniques to analyze vast amounts of genetic and biomedical data, leading to the prioritization of genes with a strong likelihood of being implicated in specific diseases [50]. In another study, BERT was utilized to identify drug-target interactions from the entire PubMed database, achieving an accuracy of 99% and identifying 0.6 million new articles with relevant data [51]. Furthermore, Hou et al. [52] leveraged GPT-4 for cell type annotation in single-cell RNA-seq analysis, demonstrating that GPT-4 can accurately annotate cell types using marker gene information. This approach achieved over 75% agreement with manual annotations in most studies and tissues, highlighting its potential to reduce the labor and expertise required for cell type annotation. Collectively, these advancements underscore the potential of AI-driven models to transform biomedical research, offering more precise and efficient tools for disease understanding and treatment development."}, {"title": "3.3 Personalized Medicine", "content": "LLMs have also demonstrated potential in democratizing medical knowledge through online medical consultations [40, 53-55]. This capability ensures broad accessibility to biomedical information and enables personalized customization based on individual conditions, which could have profound implications for telemedicine [15, 56]. However, the development of personalized treatment plans using LLMs requires strict adherence to medical ethics and patient privacy. It is important to ensure that all data collection, storage, and usage comply with legal regulations and ethical standards. Ferrario et al. [57] evaluated GPT-4's performance in responding to various medical ethics cases. Their findings indicated that while GPT-4 can identify and articulate complex medical ethical issues, it requires improvement in encoding real-world ethical dilemmas more deeply. Sandmann et al. [58] conducted an assessment of LLMs in clinical decision-making. They evaluated the clinical accuracy of initial diagnoses, examination steps, and treatments for 110 cases across different clinical disciplines using ChatGPT, LLAMA, and a naive baseline. Their results showed that GPT-4 performed the best among the tested models. Importantly, this study suggests that open-source LLMs may offer a viable solution for addressing data privacy concerns in personalized medicine applications."}, {"title": "3.4 Biomedical Literature and Research", "content": "The integration of LLMs with biomedical research and writing has enhanced research efficiency, impartiality, and accessibility [59]. This synergy allows experts and researchers to more effectively obtain, understand, and apply the latest biomedical information, thereby increasing research productivity. LLMs have demonstrated utility in multiple key areas of biomedical literature, including literature retrieval, outline preparation, abstract writing, and translation tasks. Mojadeddi et al. [60] evaluated ChatGPT's performance in article writing. Their findings indicated that while ChatGPT can expedite the writing process, it has not yet reached the level of professional biomedical writers and has certain limitations. This underscores the need for further investigation into AI capabilities in scientific writing. Huespe [61] assessed GPT-3.5's ability to write the background section of critical care clinical research questions. In this study, 80 researchers were invited to distinguish between human-written and LLM-generated content. The results suggested that GPT-3.5's writing ability is comparable to that of biomedical researchers in this specific task."}, {"title": "3.5 Benchmark Datasets and Evaluation Metrics", "content": "A variety of benchmark datasets have been utilized in the evaluation on the performance of LLMs to biomedical inquiries. These datasets encompass a wide range of tasks, from basic textual responses to complex multimodal data. Textual datasets such as MedSTS [62], PubMedQA [63], and MedQA [64] focus on assessing LLMs on tasks like semantic similarity, question answering, and content summarization in the biomedical domain. Specialized datasets like GenBank [65] test LLMs on their ability to handle genomic sequences, which is crucial for applications in genomics and personalized medicine. Multimodal benchmarks like MultiMedBench [66] challenge LLMs to integrate and interpret data from multiple sources, such as medical images and accompanying textual descriptions, reflecting the complex nature of medical diagnostics. Evaluation metrics commonly used to assess model performance across different tasks include Accuracy, BLEU-1, F1 Score, and ROUGE-L [65, 67, 68]. For evaluating LLMs in biomedical dialogue scenarios, specialized metrics such as Professionalism, Fluency, and Safety have been developed to capture the nuanced requirements of biomedical communication [69-71]."}, {"title": "3.6 Summary", "content": "Our analysis reveals that LLMs, without specialized training, can demonstrate a basic understanding of biomedical terminology and concepts with minimal contextual prompts. However, their performance varies across different biomedical disciplines and tasks. The violin plots indicate that while LLMs generally perform above the baseline across all expertise levels, their performance is most consistent at the intermediate level. At senior and expert levels, there is greater variability in performance, suggesting that LLMs may struggle with more complex, specialized tasks that require advanced expertise [59]. The evaluation results across various biomedical disciplines highlight both the potential and limitations of LLMs in zero-shot biomedical applications [45, 77, 78]. In certain specific biomedical fields, LLMs show performance comparable to experienced physicians. However, in more specialized contexts or complex tasks requiring in-depth biomedical knowledge and clinical reasoning, LLMs may exhibit deficiencies or fail completely. For most biomedical application scenarios, the zero-shot performance of LLMs falls short of the requirements for immediate clinical application, particularly in highly challenging tasks such as rare disease diagnosis or complex surgical planning [79, 80]. These findings underscore the need for caution when considering the direct application of LLMs to challenging biomedical tasks without fine-tuning or retraining. While the prospects of LLMs in the biomedical field are promising, it is important to consider their limitations in biomedical applications and thoughtfully define their role in ethical and clinical decision-making processes."}, {"title": "4 Adapting General LLMs to the Biomedical Field", "content": "General-purpose LLMs encounter various challenges when applied to the biomedical domain in a zero-shot manner, primarily due to the field's highly specialized nature. The biomedical sector employs a distinct vocabulary, nomenclature, and conceptual framework that general LLMs may not comprehend [113]. This specificity extends beyond mere terminology to encompass complex relationships between biological entities, intricate disease mechanisms, and nuanced clinical contexts. Additionally, the biomedical field presents a diverse array of tasks, ranging from literature analysis and interpretation of clinical notes to supporting diagnostic decisions and drug discovery processes. This variety demands LLMs capable of performing a wide spectrum of specialized functions, each requiring domain-specific knowledge and reasoning capabilities [114, 115]. Moreover, biomedical research increasingly relies on multimodal data integration, incorporating various data types such as text, images (e.g., radiology scans, histology slides), and molecular sequences (e.g., DNA, protein structures) [116, 117]. Effective processing and synthesis of information from these disparate sources pose additional challenges for LLMs. To address these challenges and enhance the suitability of general-purpose LLMs for biomedical applications, several adaptation strategies have been developed. These include domain-specific fine-tuning, architectural modifications, and the creation of specialized biomedical LLMs from the ground up. The adaptation process involves curating high-quality, domain-specific datasets that capture the nuances of biomedical language and knowledge. These datasets are then used to fine-tune existing LLMs or train new models, incorporating techniques such as continued pre-training on biomedical corpora, task-specific fine-tuning, and multi-task learning to improve performance across various biomedical tasks [12, 88]. As a result of these efforts, a variety of specialized LLMs have emerged, each tailored to specific aspects of biomedical research and clinical practice."}, {"title": "4.1 Unimodal Adaptation Strategies", "content": "To adapt general-purpose LLMs to the biomedical field, fine-tuning can enable the models to deeply understand the specialized terminology, complex concepts, and linguistic habits of this domain. This enhances their ability to provide more accurate and in-depth analysis and generation when dealing with specialized data such as biomedical texts. The fine-tuning methods include full-parameter fine-tuning, instruction fine-tuning, parameter-efficient fine-tuning, and hybrid fine-tuning."}, {"title": "Full-Parameter Fine-Tuning", "content": "Full-parameter fine-tuning involves updating all parameters of a pre-trained LLM using domain-specific data. Unlike traditional fine-tuning methods (e.g., tuning only the top layers), full-parameter fine-tuning allows each layer of the LLMs to learn task-specific knowledge. For instance, GatorTron [81], a model fine-tuned on clinical data, achieved an F1 score of 93.01% in medical question answering, surpassing previous benchmarks by 7.77%. While full-parameter fine-tuning often yields the best performance, it comes with heavy computational costs. For instance, fine-tuning GatorTronGPT-20M [17] required more than 268,800 GPU hours on A100 GPUs, making it challenging for resource-constrained environments."}, {"title": "Instruction Fine-Tuning", "content": "Instruction Fine-Tuning (IFT) is a technique that modifies the underlying instructions of a pre-trained model to optimize its adaptation to specific tasks or domains in the biomedical field [118]. This approach has shown promising results in improving model performance on specialized medical tasks. For instance, MEDITRON [96], a model fine-tuned on LLaMA-2 using IFT, demonstrated an average performance improvement of 1.8% across various medical benchmarks. Similarly, AlpaCare [100] leveraged a curated set of 52,000 medical instructions to achieve a 30.4% performance boost on the HeadQA benchmark, showcasing the potential of well-designed instruction sets in enhancing model capabilities. The primary advantage of IFT lies in its ability to adapt models to specific biomedical domains using relatively less data compared to full-parameter fine-tuning. However, the effectiveness of IFT heavily depends on the quality and diversity of the instructions used. Poorly designed or biased instructions can lead to inconsistent or unreliable model behavior, potentially compromising the model's utility in critical medical applications."}, {"title": "Parameter-Efficient Fine-Tuning", "content": "Parameter-Efficient Fine-Tuning (PEFT) encompasses a set of techniques designed to improve the performance and training efficiency of LLMs by adjusting a small subset of model parameters [119]. Two prominent PEFT approaches are LoRA (Low-Rank Adaptation) [120] and QLORA (Quantized LoRA) [121], which work by adding small trainable matrices to the model. This allows for task-specific adaptations without modifying the entire model architecture. The efficiency of PEFT methods is remarkable, often reducing the number of trainable parameters by 99% or more while maintaining performance comparable to full fine-tuning. For example, MMedLM 2 [68] employed LORA to achieve competitive performance in multilingual medical question-answering tasks while fine-tuning only a fraction of the model's parameters. This approach reduces computational requirements, making it feasible to deploy tailored medical A\u0399 models in resource-constrained environments such as small hospitals or research laboratories. However, PEFT methods may face limitations when tasks require substantial modifications to the base model's knowledge, as they primarily focus on adapting existing knowledge rather than introducing entirely new information. This constraint could potentially impact their effectiveness in highly specialized or rapidly evolving areas of biomedicine."}, {"title": "Hybrid Fine-Tuning", "content": "Hybrid fine-tuning is an approach that combines multiple parameter-efficient tuning techniques to enhance model performance and training efficiency while minimizing the introduction of additional parameters. For example, HuatuoGPT [10], using supervised fine-tuning and RLAIF [122], achieves state-of-the-art results in performing medical consultation among open-source LLMs in terms of GPT-4 evaluation, human evaluation, and medical benchmark datasets. Hybrid fine-tuning strategies offer a balance between performance and efficiency, addressing some of the limitations of individual techniques. They allow for more flexible adaptation to the unique challenges of medical AI, such as the need for both broad medical knowledge and specialized expertise. However, these approaches often require more complex implementation and careful tuning of multiple components."}, {"title": "4.2 Multimodal Adaptation Strategies", "content": "Multimodal LLMs represent can integrate diverse data types to provide comprehensive insights. The core strength of these models lies in their ability to fuse information from various modalities, including text, images, gene sequences, and protein structures. This fusion not only bridges interdisciplinary gaps but also mirrors the multifaceted nature of medical diagnosis and research [123]. In clinical settings, patient assessments typically involve an array of data types, including textual information (e.g., medical reports), visual data (e.g., X-rays and MRIs), and numerical measurements (e.g., laboratory results and vital signs). Multimodal LLMs aim to integrate these diverse sources to offer more accurate and holistic biomedical insights. For instance, by combining medical imaging with clinical text reports and other relevant data, these models can improve diagnostic accuracy and robustness [124]. In addition, multimodal can facilitate the integration of genomic data with phenotypic information, enabling more comprehensive studies of disease mechanisms and discover new drugs [112].\nFine-tuning strategies play a crucial role in the application of biomedical multimodal models, ensuring that these models can adequately comprehend and process cross-modal data. These strategies encompass various approaches, including the optimization of visual encoders through LoRA [120] and layer normalization [125] techniques. Such optimizations are implemented to enhance the model's capacity to interpret critical features within medical images. Concurrently, these strategies integrate visual and textual inputs, leveraging attention mechanisms and multilayer perceptron (MLP) layers to augment the model's proficiency in generating radiology reports, as exemplified by the ClinicalBLIP [110] model. Specifically, ClinicalBLIP demonstrated superior performance in the radiology report generation task using the MIMIC-CXR [126] dataset, achieving a Metric For Evaluation of Translation with Explicit Ordering (METEOR) [127] score of 0.534 through these fine-tuning strategies. This score significantly surpasses that of other models, underscoring ClinicalBLIP's exceptional capability in handling complex multimodal data. Similarly, Med-Gemini [111] employs a strategy of constructing a joint embedding space, enabling direct comparison and integration of data from diverse modalities within a unified latent space. This approach has exhibited remarkable performance in complex medical tasks, particularly in cancer diagnostics, where the integration of genomic data and pathological images has substantially enhanced diagnostic accuracy. These fine-tuning strategies, by optimizing model performance in biomedical multimodal tasks, demonstrate the immense potential of applying multimodal models in the medical domain. Furthermore, they underscore the critical role of fine-tuning in enhancing model generalization capabilities and task adaptability."}, {"title": "4.3 Training Data and Processing Strategies", "content": "The adaptation of general-purpose LLMs to the biomedical domain hinges on the quality, diversity, and processing of the data. This subsection explores key datasets and effective strategies for developing and refining biomedical LLMs."}, {"title": "4.3.1 Dataset Overview", "content": "Biomedical datasets utilized for LLM training and evaluation span three main categories, namely text-based, image-based, and multimodal. summarizes datasets employed in recent studies. Text-based datasets, such as PubMed, have been instrumental in training models like BioGPT [27]. Similarly, the MIMIC-III dataset, containing de-identified health records from over 40,000 care patients, contributes to models like GatorTron [81], enabling LLMs to learn from real-world clinical data. Multimodal datasets, which integrate various data types, facilitate more comprehensive model training. The MultiMedBench [66] dataset exemplifies this approach by aligning clinical notes with medical measurements and imaging data. Models like Med-PaLM M [66] trained on such datasets demonstrate enhanced performance in tasks requiring the integration of heterogeneous data types, bridging the gap between textual and visual medical information."}, {"title": "4.3.2 Data Processing Strategies", "content": "To maximize the utility of these datasets, researchers have employed various data processing techniques."}, {"title": "Data Augmentation", "content": "Augmentations aim to increase dataset size and diversity, thereby improving model robustness and generalization. Chen et al. [20], in their development of BianQue by combining automatic data cleaning with ChatGPT-based data polishing. This method not only enhanced the quality of training data but also led to a 15% improvement in the model's performance on medical consultation tasks."}, {"title": "Data Mixing", "content": "The integration of diverse data sources can also enhance model capabilities. Bao et al. [21] demonstrated this in DISC-MedLLM, employing a data fusion strategy. By combining structured information from medical knowledge graphs with human-curated samples, they achieved a 20% improvement in handling medical queries compared to models trained on single-source data."}, {"title": "4.3.3 Federated Learning in LLMs", "content": "In the realm of biomedical LLMs, direct data sharing is often impractical due to stringent healthcare regulations. Federated Learning (FL) [128] has emerged as a transformative solution, potentially reshaping the future of LLM training in healthcare. Unlike traditional LLMs trained on single, proprietary data centers, biomedical LLMs require diverse datasets that can be effectively accessed through FL. The Open-FedLLM framework [129], facilitates FL across geographically distributed datasets while promoting ethical alignment. Complementing this, Wu et al. [130] introduced FedMed, a framework specifically designed to enhance medical language modeling while mitigating performance degradation in federated settings. Zhang et al. [131] further advanced the field by demonstrating the effectiveness of combining FL with prompt-based approaches for clinical applications, enhancing model adaptability while preserving patient privacy. Nagy et al. [132] explored privacy-preserving techniques for training large language models like BERT and GPT-3, providing insights into maintaining privacy without compromising performance. Addressing multilingual challenges, Weller et al. [133] investigated the use of pre-trained language models in FL across multiple languages, focusing on various NLP tasks in medical contexts. Finally, Kim et al. [134] proposed improving computational efficiency in FL by integrating adapter mechanisms into pre-trained LLMs, demonstrating the benefits of using smaller Transformer-based models to reduce computational demands."}, {"title": "4.4 Summary", "content": "This section has explored the adaptation of general-purpose LLMs to the biomedical domain, highlighting the important interplay between data quality, processing strategies, and model adaptation techniques. We reviewed the foundational role of diverse datasets and advanced data processing methods in developing robust biomedical LLMs. The investigation of various adaptation approaches, from full-parameter fine-tuning to more efficient methods like instruction tuning and parameter-efficient techniques. Despite these advancements, challenges persist in data privacy, model interpretability, and fairness. Future research can focus on developing more efficient, interpretable, and ethical adaptation techniques. Priority areas include enhancing model transparency, addressing fairness concerns, and exploring advanced federated learning methods to leverage decentralized medical data while preserving patient privacy. The integration of multimodal approaches also presents a promising avenue for more comprehensive healthcare solutions. As biomedical LLMs continue to evolve, balancing technological innovation with ethical considerations will be important. By addressing current challenges and embracing emerging opportunities, these models have the potential to revolutionize healthcare, from improving clinical decision support to accelerating biomedical research, ultimately leading to more effective and equitable healthcare delivery."}, {"title": "5 Discussion", "content": "5.1 Challenges of LLMs in Biomedical Applications\nLLMs have demonstrated potential in biomedical applications, as evidenced by our review of zero-shot evaluations and adaptation strategies. While unadapted LLMs show promise in certain tasks, fine-tuning has proven crucial in bridging the gap between general language understanding and specialized medical knowledge. Unimodal LLMs, after appropriate adaptation, have achieved improvements in processing medical texts, answering complex questions, and facilitating medical dialogues. For example, GatorTron excelled in various clinical NLP tasks after full-parameter fine-tuning [81], while MMedLM 2 demonstrated competitive performance in multilingual medical question answering using parameter-efficient fine-tuning methods [68]. Multimodal LLMs have expanded the horizons of medical diagnosis and analysis by integrating image and text data. Models such as Med-Gemini [111] and Med-PaLM M [66] have shown promising results in tasks requiring the integration of visual and textual information, enhancing the accuracy of medical imaging processing and diagnosis.\nCompared to traditional machine learning methods in biomedicine, LLMs offer several advantages, including improved generalization across tasks and enhanced performance on complex reasoning tasks. However, they also face challenges including higher computational requirements and the need for large, diverse datasets for effective training and adaptation. Data privacy and security concerns remain paramount when handling sensitive patient information. The lack of interpretability in LLM decision-making processes raises trust and accountability issues in clinical settings. The quality and diversity of training datasets significantly impact model performance and generalizability, while the substantial computational resources required for training and fine-tuning limit widespread application, particularly in resource-constrained environments. Additionally, ethical considerations surrounding potential biases in training data and model outputs necessitate careful scrutiny and mitigation strategies."}, {"title": "5.2 LLMs Across Healthcare Hierarchy", "content": "LLMs demonstrate potential in healthcare, yet their practical implementation necessitates careful consideration of the hierarchical structure within medical systems. The role and impact of LLMs vary across different levels of healthcare delivery, from high-level management to primary care [145"}]}