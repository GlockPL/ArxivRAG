{"title": "DECOUPLE-THEN-MERGE:\nTOWARDS BETTER TRAINING FOR DIFFUSION MODELS", "authors": ["Qianli Ma", "Xuefei Ning", "Dongrui Liu", "Li Niu", "Linfeng Zhang"], "abstract": "Diffusion models are trained by learning a sequence of models that reverse each\nstep of noise corruption. Typically, the model parameters are fully shared across\nmultiple timesteps to enhance training efficiency. However, since the denoising\ntasks differ at each timestep, the gradients computed at different timesteps may\nconflict, potentially degrading the overall performance of image generation. To\nsolve this issue, this work proposes a Decouple-then-Merge (DeMe) framework,\nwhich begins with a pretrained model and finetunes separate models tailored to\nspecific timesteps. We introduce several improved techniques during the finetun-\ning stage to promote effective knowledge sharing while minimizing training inter-\nference across timesteps. Finally, after finetuning, these separate models can be\nmerged into a single model in the parameter space, ensuring efficient and practical\ninference. Experimental results show significant generation quality improvements\nupon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, Par-\ntiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.", "sections": [{"title": "INTRODUCTION", "content": "Generative modeling has seen significant progress in recent years, primarily driven by the devel-\nopment of Diffusion Probabilistic Models (DPMs) (Ho et al., 2020; Nichol & Dhariwal, 2021;\nRombach et al., 2022b). These models have been applied to various tasks such as text-to-image\ngeneration (Rombach et al., 2022a), image-to-image translation (Saharia et al., 2022a), image edit-\ning (Yang et al., 2023a), and video generation (Ho et al., 2022; Blattmann et al., 2023), yielding\nexcellent performance. Compared with other generative models such as variational auto-encoders\n(VAEs) (Kingma & Welling, 2013), and generative adversarial networks (GANs) (Goodfellow et al.,\n2014), the most distinct characteristic of DPMs is that DPMs need to learn a sequence of models for\ndenoising at multiple timesteps. Training the neural network to fit this step-wise denoising condi-\ntional distribution facilitates tractable, stable training and high-fidelity generation.\n\nThe denoising tasks at different timesteps are similar yet different. On the one hand, the denoising\ntasks at different timesteps are similar in the sense that the model takes a noisy image from the\nsame space as input and performs a denoising task. Intuitively, sharing knowledge between these\ntasks might facilitate more efficient training. Therefore, typical methods let the model take both the\nnoisy image $x_t$ and the corresponding timestep $t$ as input, and share the model parameter across all\ntimesteps. On the other hand, the denoising tasks at different timesteps have clear differences as\nthe input noisy images are from different distributions, and the concrete \u201cdenoising\" effect is also\ndifferent. Li et al. (2023a) demonstrate that there is a substantial difference between the feature\ndistributions in different timesteps. Fang et al. (2023b) show that the larger (noisy) timesteps tend to\ngenerate the low-frequency and the basic image content, while the smaller timesteps tend to generate\nthe high-frequency and the image details.\n\nWe further study the conflicts of different timesteps during the training of the diffusion model.\nFig. 1(a) shows the gradient similarity of different timesteps. We can observe that the diffusion\nmodels have dissimilar gradients at different timesteps, especially the non-adjacent timesteps, indi-\ncating a conflict between the optimization direction from different timesteps, as shown in Fig. 1(b)."}, {"title": "RELATED WORK", "content": "Diffusion Models. Diffusion Probabilistic Models(DPMs) (Ho et al., 2020; Nichol & Dhariwal,\n2021; Dhariwal & Nichol, 2021; Sohl-Dickstein et al., 2015; Song et al., 2020b) represent a family\nof generative models that generate samples via a progressive denoising mechanism, starting from a\nrandom Gaussian distribution. Given that diffusion models suffer from slow generation and heavy\ncomputational costs, previous works have focused on improving diffusion models in various aspects,\nincluding model architectures (Peebles & Xie, 2023; Rombach et al., 2022b), faster sampler (Song\net al., 2020a; Lu et al., 2022; Liu et al., 2022), prediction type and loss weighting (Hang et al.,\n2023; Salimans & Ho, 2022). Besides, a few works have attempted to accelerate DPMs genera-\ntion through pruning (Fang et al., 2023a), quantization (Shang et al., 2023; Li et al., 2023b) and\nknowledge distillation (Kim et al., 2023; Salimans & Ho, 2022; Luhman & Luhman, 2021; Meng\net al., 2023), which have achieved significant improvement on the generation efficiency. Motivated\nby the excellent generative capacity of diffusion models, DPMs have been developed in several ap-\nplications, including text-to-image generation (Rombach et al., 2022b; Ramesh et al., 2022; Saharia\net al., 2022b), video generation (Ho et al., 2022; Blattmann et al., 2023), image restoration (Saharia\net al., 2022c), natural language generation (Li et al., 2022), audio synthesis (Kong et al., 2020), 3D\ncontent generation (Poole et al., 2022), ai4science such as protein structure generation (Wu et al.,\n2024), among others.\n\nTraining of Diffusion Models & Multi-task Learning. Multi-task Learning (MTL) is aimed at\nimproving generalization performance by leveraging shared information across related tasks. The\nobjective of MTL is to learn multiple related tasks jointly, allowing models to generalize better by\nlearning representations that are useful for numerous tasks (Crawshaw, 2020). Despite its success in\nvarious applications, MTL faces significant challenges, particularly negative transfer (Wang et al.,\n2020; Crawshaw, 2020), which can degrade the performance of individual tasks when jointly trained.\nThe training paradigm of diffusion models could be viewed as a multi-task learning problem: diffu-\nsion models are trained by learning a sequence of models that reverse each step of noise corruption\nacross different noise levels. A parameter-shared denoiser is trained on different noise levels concur-\nrently, which may cause performance degradation due to negative transfer\u2014a phenomenon where\nlearning multiple denoising tasks jointly hinders performance due to conflicts in timestep-specific\ndenoising information. To better balance the learning of denoising tasks across different noise lev-\nels, previous works reweight training loss on different timesteps, improving diffusion model perfor-\nmance (Ho et al., 2020; Salimans & Ho, 2022) or accelerating training convergence (Hang et al.,\n2023). Go et al. (2024) analyze and improve the diffusion model by exploring task clustering and\napplying various MTL methods to diffusion model training. Kim et al. (2024) analyze the difficulty"}, {"title": "METHODOLOGY", "content": "3.1 PRELIMINARY\n\nThe fundamental concept of diffusion models is to generate images by progressively applying de-\nnoising steps, starting from random Gaussian noise $x_T$, and gradually transforming it into a struc-\ntured image $x_0$. Diffusion models consist of two phases: the forward process and the reverse process.\nIn the forward process, a data point $x_0 \\sim q(x)$ is randomly sampled from the real data distribution,\nthen gradually corrupted by adding noise step-by-step $q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$,\nwhere $t$ is the current timestep and $\\beta_t$ is a pre-defined variance schedule that schedules the noise.\nIn the reverse process, diffusion models transform a random Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$ into the\ntarget distribution by modeling conditional probability $q(x_{t-1} | x_t)$, which denoises the latent $x_t$ to\nget $x_{t-1}$. Formally, the conditional probability in the reverse process can be modeled as:\n\n$p_\\theta(x_{t-1} | x_t) = \\mathcal{N} \\bigg(x_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\big(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\big), \\frac{\\beta_t}{1-\\bar{\\alpha}_t} I \\bigg),$                                                                                                                (1)\n\nwhere $\\alpha_t = 1 - \\beta_t$, $\\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i$. $\\epsilon_\\theta$ denotes a noise predictor, which is usually an U-Net (Ronneberger et al., 2015) autoencoder in diffusion models, with current timestep $t$ and previous latent\n$x_t$ as input. It is usually trained with the objective function:\n\n$\\mathcal{L}_\\theta = E_{t \\sim \\mathcal{U}[0,T], x_0 \\sim q(x), \\epsilon \\sim \\mathcal{N}(0,I)}[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2],$                                                                                                   (2)\n\nwhere $T$ denotes the number of timesteps and $\\mathcal{U}$ denotes a uniform distribution. After training, a\nclean image $x_0$ can be obtained via an iterative denoising process from the random Gaussian noise\n$x_T \\sim \\mathcal{N}(0, I)$ with the modeled distribution $x_{t-1} \\sim p_\\theta(x_{t-1} | x_t)$ in Equation 1.\n\n3.2 DECOUPLE THE TRAINING OF DIFFUSION MODEL\n\nIn this section, we demonstrate how to decouple the training of diffusion model. As illustrated in\nFig. 1(c), we first divide the timesteps of $[0, T)$ into $N$ multiple continuous and non-overlapped"}, {"title": "DECOUPLE THE TRAINING OF DIFFUSION MODEL", "content": "timesteps ranges, which can be formulated as $\\{[(i-1)T/N, iT/N)]\\}_{i=1}^N$. Subsequently, based on a\ndiffusion model pretrained by Equation 1, we finetune a group of $N$ diffusion models $\\{\\epsilon_\\theta\\}_{i=1}^N$ on\neach of the $N$ timestep ranges. The training objective of $\\epsilon_\\theta$, which can be formulated as\n\n$E_{t \\sim \\mathcal{U}[(i-1)T/N, iT/N], x_0 \\sim q(x), \\epsilon \\sim \\mathcal{N}(0,I)}[||\\epsilon - \\epsilon_{\\theta_i}(x_t, t)||^2].$                                                                                                                                               (3)\n\nHowever, although Equation 3 can decouple the training of the diffusion model in different timesteps\nand avoid the negative interference between multiple denoising tasks, it also eliminates the positive\nbenefits of learning from different timesteps, which may make the finetuned diffusion model overfit\na specific timestep range and lose its knowledge in the other timesteps. Besides, it is also challenging\nfor the diffusion model to capture the difference in different timesteps during finetuning. To address\nthese problems, as shown in Fig. 2, we further introduce the following three techniques.\n\nConsistency Loss. A consistency loss is introduced into the training process to minimize the dif-\nference between the pre-finetuned and post-finetuned diffusion model, which can be formulated as\n\n$E_{t \\sim \\mathcal{U}[(i-1)T/N, iT/N]}[||\\epsilon_\\theta(x_t, t) - \\epsilon_{\\theta_i}(x_t, t)||^2],$                                                                                                                                                                                                                                                  (4)\n\nwhere $\\epsilon_\\theta(x_t, t)$ denotes the output of the original diffusion model. $\\epsilon_{\\theta_i}(x_t, t)$ denotes the output of\n$i$th post-finetuned diffusion model. Minimizing the consistency loss preserves the initial knowledge\nof the diffusion model, and ensures that the finetuned diffusion models do not differ significantly\nfrom the pre-finetuned diffusion model. Besides, the consistency loss also enhances the stability of\nthe training process for finetuning diffusion models in the timestep range. Combining Equation 3\nand Equation 4, we can derive the overall loss:\n\n$E_{t \\sim \\mathcal{U}[(i-1)T/N, iT/N], x_0 \\sim q(x), \\epsilon \\sim \\mathcal{N}(0,I)}[||\\epsilon - \\epsilon_{\\theta_i}(x_t, t)||^2 + ||\\epsilon_\\theta(x_t, t) - \\epsilon_{\\theta_i}(x_t, t)||^2].$                                                                                                   (5)\n\nProbabilistic Sampling. To further preserve the initial knowledge learned at all the timesteps, we\ndesign a Probabilistic Sampling strategy which enables the finetuned model to mainly learn from\nits corresponding timestep range, but still possible to preserve the knowledge in the other timestep\nranges. Concretely, during the finetuning of $i$th diffusion model, we sample $t$ from the timestep\nrange $[(i-1)T/N, iT/N)$ with a probability of $1 - p$, while sampling from the overall range $[0, T)$ with\na probability $p$. The overall sampling strategy can be expressed as follows:\n\n$t \\sim \\begin{cases}\n    [(i-1)T/N, iT/N), i \\in [1,N] & \\text{with probability $1 - p$,}\\\\\n    [0, T) & \\text{with probability $p$.}\n\\end{cases}$                                                                                                                                                                                 (6)\n\nChannel-wise Projection. Fig. 3 shows the difference between the pre-finetuned and the post-\nfinetuned diffusion models, demonstrating that there is a significant difference in the channel di-\nmension instead of the spatial dimension, which further implies that the knowledge learned during\nfinetuning in a timestep range is primarily captured by channel-wise mapping instead of spatial map-\nping. Based on this observation, we further apply a channel-wise projection layer to facilitate the\ntraining process by directly formulating the channel-wise mapping. Let $F_t \\in \\mathbb{R}^{C \\times H \\times W}$ denote the\nintermediate feature map of the noise predictor $\\epsilon_\\theta(x_t, t)$ at the timestep $t$, where $C, H, W$ denote"}, {"title": "MERGING MODELS IN DIFFERENT TIMESTEP RANGES", "content": "After finetuning $N$ diffusion models in their corresponding timesteps, it is a natural step to ensemble\nthese finetuned diffusion models in the inference stage. The sampling process under timestep-wise\nmodel ensemble scheme is achieved by inferring each post-finetuned diffusion model in its corre-\nsponding timestep range, which can be formulated as\n\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N} \\bigg(x_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\big(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_{\\theta_i}(x_t, t) \\big), \\frac{\\beta_t}{1-\\bar{\\alpha}_t} I \\bigg), i = \\lfloor \\frac{t \\times N}{T} \\rfloor.$                                                                                                   (7)\n\nFor instance, the $i$th finetuned diffusion model is only utilized in timestep $t \\in [(i-1)T/N, T/N)$. This\ninference scheme does not introduce additional computation costs during the inference period but\ndoes incur additional storage costs. Since model merging methods (Ilharco et al., 2022; Worts-\nman et al., 2022) can integrate diverse knowledge from each model, we propose to merge multiple\nfinetuned diffusion models into a single diffusion model. Notably, this approach avoids additional\ncomputation or storage costs during inference while significantly improving generation quality.\n\nModel Merging Scheme. Fig. 2 shows the overview of the model merge scheme. Inspired by\nmodel merging methods (Ilharco et al., 2022; Wortsman et al., 2022) that aim to merge the parame-\nters of models finetuned in different datasets and tasks, we propose to merge multiple post-finetuned\ndiffusion models. Specifically, we first compute the task vectors of different post-finetuned diffusion\nmodels, which indicates the difference in their parameters compared with the pre-finetuned version.\nThe task vector $T_i$ of the $i$th finetuned diffusion model can be denoted as $\\tau_i = \\theta_i - \\theta$, where $\\theta$ and\n$\\theta_i$ denote the parameters of the pre-finetuned and the $i$th post-finetuned diffusion model. Follow-\ning previous work (Ilharco et al., 2022), the model merging can be achieved by adding all the task\nvectors to the pre-finetuned model, which can be formulated as\n\n$\\theta_{\\text{merged}} = \\theta + \\sum_{i=1}^N w_i\\tau_i$, where $\\tau_i = \\theta_i - \\theta,$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (8)\n\nwhere $w_i$ means merging weights of task vectors. To find the optimal combination of $w_i$, we use the\ngrid search algorithm to explore this combination. In this scheme, we finally obtain $\\theta_{\\text{merged}}$ which\ncan be applied across all timesteps in $[0, T)$, following the same inference process as in traditional\ndiffusion models. As a result, the model merge scheme also leads to significant enhancement in gen-\neration quality without introducing any additional costs in computation or storage during inference.\n\nProposition 1 (DeMe facilities escaping from critical point, informal Prop. 1. Proof in A.3)\nGiven a pretrained diffusion model with parameter $\\theta$ converged in a critical point over the entire\ntimesteps, which implies $E_{t \\sim \\mathcal{U}[0,T)}[\\nabla L_\\theta(t)] = 0$. DeMe exhibits a non-zero gradient across the"}, {"title": "EXPERIMENTS", "content": "4.1 EXPERIMENT SETTING\n\nDatasets and Metrics. For unconditional image generation datasets CIFAR10 (Krizhevsky et al.,\n2009), LSUN-Church, and LSUN-Bedroom (Yu et al., 2015), we generated 50K images for eval-\nuation. For text-to-image generation, following the previous work (Kim et al., 2023), we finetune\neach model on a subset of LAION-Aesthetics V2 (L-Aes) 6.5+ (Schuhmann et al., 2022) and test\nmodel's capacity of zero-shot text-to-image generation on MS-COCO validation set (Lin et al.,\n2014), ImageNet1K (Deng et al., 2009) and PartiPrompts (Yu et al., 2022). Fr\u00e9chet Inception Dis-\ntance (FID) (Heusel et al., 2017) is used to evaluate the quality of generated images. CLIP score\ncomputed by CLIP-ViT-g/14 (Radford et al., 2021) is used to evaluate the text-image alignment.\n\nBaselines. We choose three loss reweighting methods as baselines for comparison: SNR+1, trun-\ncated SNR (Salimans & Ho, 2022) and Min-SNR-$\\gamma$ (Hang et al., 2023). Appendix A.1 proves that the\naforementioned diffusion loss weights can be unified under the same prediction target with differ-\nent weight forms. Appendix A.2 demonstrates that our decouple-then-merge framework can also be\nformally transformed into the loss reweighting framework. To ensure a fair comparison, the baseline\nmodels are trained with an equal number of iterations with our training framework. Additionally, we\nalso ensemble finetuned diffusion models and compare them with the merging scheme for a more\ndetailed comparison. Please refer to the Appendix B for more implementation details.\n\n4.2 QUANTITATIVE STUDY\n\nResults on Unconditional Generation. Table 1 presents quantitative results on unconditional\ngeneration, demonstrating great improvement in generation quality across various unconditional\nimage generation benchmarks. The model merging scheme achieves performance comparable to, or\neven better than, the ensemble scheme with a unified diffusion model, highlighting the superiority of\nthe merging approach. Concretely, 0.63, 1.12, and 0.59 FID reduction can be observed on CIFAR10,\nLSUN-Church, and LSUN-Bedroom with the model ensemble scheme, respectively. The model\nmerging scheme leads to 0.91, 3.42, and 0.62 FID reductions on CIFAR10, LSUN-Church, and"}, {"title": "QUALITATIVE STUDY", "content": "Fig. 5 depicts some synthesized images of LSUN and Fig. 6 depicts some fancy generated images\ngiven detailed prompts. As demonstrated in Fig. 5, our method has better captured the underlying\npatterns in the images, specifically the church and bedroom scenes, enabling a more detailed and\naccurate generation of the church and bedroom. While diffusion that before finetuning fails to gen-\nerate churches or bedrooms, diffusion after finetuning successfully generates them with finer details."}, {"title": "ABLATION STUDY", "content": "Our framework applies three training techniques to\nfinetune diffusion model in different timesteps. As\nshown in Table 3, we conducted ablation studies on\ntraining techniques individually. All experiments are\nconducted on CIFAR10, with a 100-step DDIM sam-\npler (Song et al., 2020a). Several key observations\ncan be made: (i) The traditional training paradigm\nresults in the poorest performance. With N set to 1\nand none of the specialized training techniques applied-following the traditional diffusion training\nparadigm-the model yields a poor results, with a FID of 4.40. Gradient conflicts lead to negative\ninterference across different denoising tasks, adversely affecting overall training. (ii) Channel-wise\nprojection struggles to capture feature differences in the channel dimension without alleviating gra-\ndient conflicts. With N set to 1 and Channel-wise projection applied, model yields a worse results,\nwith a FID of 4.45. In contrast, with N set to 8 and Channel-wise projection applied additionally,\nmodel yields the best results, with a FID of 3.87. We posit that channel-wise projection struggles to\ncapture feature changes due to the significant differences across the timesteps. (iii) Dividing overall\ntimesteps into N non-overlapping ranges effectively alleviates gradient conflicts, resulting in a sig-\nnificant reduction in FID. For instance, with N set to 8, introducing Probabilistic Sampling achieves\na 0.08 FID reduction, while applying Consistency Loss yields a 0.13 FID reduction additionally.\nWhen all techniques are applied during finetuning, a total FID reduction of 0.53 is achieved. Our\nexperimental results demonstrates that dividing overall timestep into non-overlapping ranges serves\nas a necessary condition. Building on this foundation, our training techniques significantly improve\nmodel performance. Sensitive studies on influence of N and p have been conducted in Appendix F,\ndemonstrating that our method is robust to variations in the choices of N and p."}, {"title": "DISCUSSION", "content": "DeMe Enables Pretrained Model Escaping from the Critical Point. In Prop. 1 we claim that\nDeMe facilities model moving away from the critical point, leading to further optimization. Fig. 4\npresents some visualization results on the training loss landscape that supporting our claims. Two\nsignificant findings can be drawn from Fig. 4: (i) The pretrained diffusion model has converged when\n$t \\in [0, 1000)$, residing at the critical point with sparse contour lines (i.e., no gradient). However, it\nis evident that the pretrained model is not at an optimal point, as there are nearby points with lower\ntraining loss, suggesting a potential direction for further optimization. (ii) For different timestep\nranges, the pretrained model tends to be situated in regions with densely packed contour lines (i.e.,\nlarger gradient), suggesting that there exists an optimization direction. For instance, when $t \\in\n[0, 250)$, the pretrained model stays at a point with frequent loss variations, indicating a potential\ndirection for lower training loss. The decoupled training framework facilities the diffusion model to\noptimize more efficiently. Based on the above observation, DeMe decouples the training process,\nenabling the pretrained model to move away from the critical point, resulting in further improvement."}, {"title": "TRANSFORM DEME FRAMEWORK TO LOSS REWEIGHTING FRAMEWORK", "content": "In Sec. 3.2, we divide the overall timesteps [0, T) into N multiple continuous and non-overlapped\ntimesteps ranges, which can be formulated as $\\{(i-1)T/N, iT/N\\}_{i=1}^N$. For each range, we finetune a\ndiffusion model $\\epsilon_\\theta$, the training objective of $\\epsilon_\\theta_i$, can be formulated as follows:\n\n$\\mathcal{L}_i = E_{\\mathcal{U} [(i-1)T/N, iT/N], x_0 \\sim q(x), \\epsilon \\sim \\mathcal{N}} [||\\epsilon - \\epsilon_{\\theta_i}(x_t, t)||^2 + ||\\epsilon_\\theta(x_t, t) - \\epsilon_{\\theta_i}(x_t, t)||^2].$                                                                                                   (15)\n\nIn Equation 15, the first term is the standard diffusion loss over the subrange, and the second term is\nthe consistency loss, ensuring that the finetuned model $\\epsilon_{\\theta_i}$ stays close to the original model $\\epsilon_\\theta$.\nIn Sec. 3.3, we compute task vector $\\tau_i = \\theta_i - \\theta$ after finetuning $\\epsilon_{\\theta_i}$, and merge N post-finetuned\ndiffusion models by\n\n$\\theta_{\\text{merged}} = \\theta + \\sum_{i=1}^N w_i \\tau_i,$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (16)\n\nwhere $w_i$ are the merging weights determined(via grid search).\nThe update in parameters $\\tau_i$ on due to finetuning on timestep range $i$ is:\n\n$\\tau_i = \\theta_i - \\theta = -\\eta \\nabla_\\theta \\mathcal{L}_i,$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (17)\n\nwhere $\\eta$ is the learning rate. The merged model's parameters in Equation 17 could be rewritten as:\n\n$\\theta_{\\text{merged}} = \\theta - \\eta \\sum_{i=1}^N w_i \\nabla_\\theta \\mathcal{L}_i,$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (18)\n\nwhich implies $\\theta_{\\text{merged}}$ minimizes the combined loss\n\n$\\mathcal{L}_{\\text{merged}} = \\sum_{i=1}^N w_i \\mathcal{L}_i.$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (19)\n\n$\\mathcal{L}_i$ is computed over its respective timestep range, which menas $\\mathcal{L}_{\\text{merged}}$ can be viewed as an integra-\ntion over the entire timestep range with a piecewise constant weighting function $w(t)$. We rewrite\n$\\mathcal{L}_{\\text{merged}}$ as:\n\n$\\mathcal{L}_{\\text{merged}} = E_{t \\sim \\mathcal{U}[0,T], x_0, \\epsilon} [w(t) \\cdot ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2 + ||\\epsilon_\\theta(x_t, t) - \\epsilon_{\\theta_i}(x_t, t)||^2],$                                                                                             (20)\n\nwhere\n\n$w(t) = \\begin{cases}\n    w_i & \\text{if } t \\in [\\frac{(i-1)T}{N}, \\frac{iT}{N}),\\\\\n    0, & \\text{otherwise}\n\\end{cases}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (21)"}, {"title": "PROOF OF PROP. 1: DEME FACILITIES ESCAPING FROM CRITICAL POINT", "content": "Assuming that a pretrained diffusion model with parameter $\\theta$ has converged over the entire timesteps\n[0", "T": ".", "mathcal{L}_\\theta(t)": 0.0, "mathcal{L}^{\\text{merged}}": "E_{t \\sim [0", "mathcal{L}_{\\theta,2}(t)": ".", "mathcal{L}_{\\theta,1}(t)": "w_1 E_{t \\sim [0"}, {"mathcal{L}_\\theta(t)": "n$w_2 E_{t \\sim [0", "mathcal{L}_{\\theta,2}(t)": "w_2 E_{t \\sim [T_1"}, {"mathcal{L}_\\theta(t)": ".", "follows": "n\n$E_{t \\sim [0,T)} [\\nabla \\mathcal{L}^{\\text{merged}}"}, {"mathcal{L}_\\theta(t)": "w_2 E_{t \\sim [T_1,T)} [\\nabla \\mathcal{L}_\\theta(t)"}, {"mathcal{L}_\\theta(t)": "E_{t \\sim [T_1,T)} [\\nabla \\mathcal{L}_\\theta(t)"}, {"mathcal{L}_\\theta(t)": "."}, {"mathcal{L}_\\theta(t)": "E_{t \\sim [T_1,T)} [\\nabla \\mathcal{L}_\\theta(t)"}, {"mathcal{L}_\\theta(t)": 0.0}]}