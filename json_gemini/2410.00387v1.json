{"title": "Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation", "authors": ["Bhargav Shandilya", "Alexis Palmer"], "abstract": "The data and compute requirements of current language modeling technology pose challenges for the processing and analysis of low-resource languages. Declarative linguistic knowledge has the potential to partially bridge this data scarcity gap by providing models with useful inductive bias in the form of language-specific rules. In this paper, we propose a retrieval augmented generation (RAG) framework backed by a large language model (LLM) to correct the output of a smaller model for the linguistic task of morphological glossing. We leverage linguistic information to make up for the lack of data and trainable parameters, while allowing for inputs from written descriptive grammars interpreted and distilled through an LLM.\n\nThe results demonstrate that significant leaps in performance and efficiency are possible with the right combination of: a) linguistic inputs in the form of grammars, b) the interpretive power of LLMs, and c) the trainability of smaller token classification networks. We show that a compact, RAG-supported model is highly effective in data-scarce settings, achieving a new state-of-the-art for this task and our target languages. Our work also offers documentary linguists a more reliable and more usable tool for morphological glossing by providing well-reasoned explanations and confidence scores for each output.", "sections": [{"title": "1 Introduction", "content": "Over the last decade, language models have evolved rapidly, culminating in impressively domain-agnostic decoder-only models like GPT (Brown et al., 2020) and Llama 2 (Touvron et al., 2023). Although these models can be versatile in terms of being applicable to a wide variety of tasks and providing straightforward interfaces for quick inference, the fact remains that they are extremely parameter-heavy, making them difficult and expensive to train (Bender and Koller, 2020). LLMs, however, give us a unique descriptive power that can boost explainability when used in certain contexts. In this paper, we examine how LLMs can be used to make RAG-informed corrections for the task of morpheme glossing (Section 2.1), a crucial and time-intensive part of the workflow of documenting endangered languages. Retrieval augmented generation (RAG) incorporates an initial retrieval step, where LLMs query an external data source to gather relevant information before generating answers or text. This retrieval phase not only informs the subsequent generation process but also ensures that the responses are based on solid evidence, thereby improving the accuracy and relevance of the output.\n\nThe process is not just about size reduction; it's a strategic transfer of linguistic capabilities, ensuring that the compact model inherits the teacher's strengths while remaining resource-efficient. In our baseline experimental setting, we simply call the LLM at inference time to correct the output of the smaller token classification network. Experimenting further, we fine-tune the retrieval and re-ranking components in conjunction with the token classification model to boost performance."}, {"title": "2 Background and Related Work", "content": "The specific task we address in this paper is mor- phological glossing, a component of the automatic production of interlinear glossed text (IGT). IGT is a richly-annotated data format widely used in linguistics, especially as one product of work documenting and describing endangered languages.\n\nThe data format, an example of which appears below, consists of multiple interrelated tiers containing different types of linguistic information. This Uspanteko example is representative of a common IGT configuration, with one tier for the original utterance, one for a morphological segmentation, one for a detailed labeling of the component morphemes, and one line with a translation into a language of wider communication.\nWe focus specifically on the glossline, in which we see a mix of stem translations (e.g. ver (in English, to see) for the Uspanteko stem il) and labels indicating morphosyntactic functions (e.g. COM indicates marking of completive aspect on the verb stem). Glossing is a sequence-to-sequence problem that can be approached in 2 ways. The first approach is to train a model to segment the data and then view it as a token classification task. The second is to view it as a translation problem with uneven input and output lengths, thus requiring an encoder-decoder model that can perform sequence-to-sequence conversion. In this paper, we use the former approach. The IGT task was the focus of a SIGMORPHON 2023 shared task. The task, resources, and previous models are described in detail in Ginn et al. (2023).\n\nIntroduced by Lewis et al. (2021), Retrieval- Augmented Generation (RAG) represents a significant advancement in the field of large language models (LLMs) for enhancing generative tasks. By dynamically retrieving information from knowledge bases during inference, RAG effectively addresses issues such as the generation of factually incorrect content, often referred to as \"hallucinations.\" The integration of RAG into LLMs has been rapidly adopted, making it a crucial technology for enhancing chatbot capabilities and making LLMs more practical for real-world applications.\nNaive RAG is the most basic form of retrieval augmented generation. The retrieve-read frame- work, which was described by Ma et al. (2023), ex- plains the process of indexing and vectorizing refer- ence documents, retrieving relevant chunks based on vector similarity, and generating outputs based on compound prompts that combine the chunks and the query. The idea of RAG has since been expanded and adapted to several domains. Yan et al. (2024) suggest a corrective RAG (CRAG) approach that incorporates a lightweight retrieval evaluator to test the quality and relevance of the re- trieved content. The information is then filtered or accepted in the process of producing the final out- put. In our paper, we also add a corrective step in a different context. Instead of evaluating the retriever, we make the LLM itself generate confidence scores for each of its predicted outputs.\nOther recent work on the IGT task takes various approaches. Ginn et al. (2024b) build a very large, multilingual corpus of IGT and use it to finetune a ByT5 model (Xue et al., 2022), achieving good results especially on languages not seen in train- ing. He et al. (2024) train models to extract IGT directly from audio data, and Ginn et al. (2024a) explore the use of in-context examples to teach LLMs to gloss low-resource language data. Us- ing the same dataset we use, they find that LLM performance improves dramatically with targeted selection of examples. Even with no traditional training or fine-tuning, models like Gemini 1.5 Pro, Cohere's Command R+ and GPT-4o outperform transformer baselines. We do not explore few-shot prompting techniques, but it is possible that the performance of our corrective LLM can be further enhanced in this way."}, {"title": "3 Methodology", "content": "Our approach combines the strengths of compact token classification models with the knowledge embedded in large language models (LLMs) and structured grammatical descriptions. The process involves several key steps:\n1. Initial glossing: A compact token classification model (either ROBERTa or Bi-LSTM) generates an initial morphological gloss for the input sentence.\n2. Retrieval: Relevant chunks of grammatical information are retrieved based on the input sentence and initial gloss.\n3. Augmented generation: An LLM uses the retrieved grammar chunks to correct and refine the initial gloss.\n4. Explanation generation: The LLM provides detailed explanations and confidence scores for each morpheme in the corrected gloss.\n5. Modular optimization: In an advanced ver- sion of our approach, we fine-tune the retrieval and token classification components together to optimize the entire pipeline.\nWe explore two main variants of this approach: a naive RAG method and a modular RAG method."}, {"title": "3.1 Using Naive RAG to correct the output of a smaller model", "content": "The Naive Retrieval-Augmented Generation (RAG) approach enhances the performance of two compact token classification models trained for glossing of Uspanteko and Arapaho. The process begins by indexing and vectorizing reference grammar documents using a dense vector representation like BERT embeddings (Devlin et al., 2018). Here, we use OpenAI embeddings (Achiam et al., 2023).\nLet $D = {d_1, d_2, . . . d_n}$ be the set of n grammar document chunks, where each $d_i$ is represented as a dense vector $v_i$ in a high-dimensional space $R^d$. We experimented with different chunk sizes and chunking strategies and found that a default chunk size of 400 characters with a 50-character overlap on either side provided the best results across 5 trials. We also tried chunking according to headings in the grammar, but this resulted in chunks of uneven sizes that did not optimally aid the retrieval process and contextual inference.\nGiven an input query q, in this case the sentence to be glossed along with the attempted prediction of the compact model, the retriever module computes the cosine similarity between the query embedding $v_q$ and each document chunk embedding $v_i$:\n\n$sim(q, d_i) = \\frac{v_q. v_i}{||v_q||||v_i ||}$\n\nThe top-k most similar document chunks $D_q = {d_{q1}, d_{q2},..., d_{qk}}$ are retrieved based on their cosine similarity scores. These chunks are concatenated with the original input query to form a compound prompt P:\n$P = [q; d_{q1}; d_{q2}; ... ; d_{qk}]$\n\nThe prompt P is then fed into an LLM, which interprets the linguistic rules and morphological patterns described in the retrieved grammar excerpts $D_q$. It uses this information to identify and correct potential errors in the glossing output $g_s$ generated by the smaller token classification model:\n$g_c = LLM(P, g_s)$\n\nwhere $g_c$ represents the corrected glossing sequence. To illustrate the correction process, let $f_s$ be the function learned by the smaller token classification model that maps the input sentence x to the glossing output $g_s$.\nThe Naive RAG approach learns a corrector function $f_{RAG}$ that takes the original input x, the glossing output $g_s$, and the retrieved grammar chunks $D_q$ to produce the corrected output $g_c$:\n$g_c = f_{RAG}(x, g_s, D_q)$\n\nBy leveraging the linguistic information retrieved from the grammar documents, the RAG model $f_{RAG}$ is able to refine the predictions of the base model $f_s$ and generate more accurate glossing sequences. The Naive RAG approach thus enables the smaller model to benefit from the vast knowledge captured by the LLM without the need for extensive fine-tuning or additional training data. This is particularly advantageous in low-resource scenarios where labeled data is scarce, as the LLM can provide valuable linguistic insights to guide the glossing process."}, {"title": "3.2 Generating Labeling Justifications and Confidence Scores", "content": "In addition to correcting glossing labels, our RAG pipeline also generates explanations justifying the corrections made. We achieve this by prompting the LLM to provide a chain-of-thought reasoning trace that justifies the decision-making process behind the corrections. The LLM is prompted with an instruction I that requests a justification J, an explanation of how RAG was used R, and a confidence score C for corrected glossing output $g_c$:\n$[J, R, C] = LLM(I, P, g_s, g_c)$\n\nThe justification J is a natural language explanation that describes the grammar rules and morphological patterns retrieved from the grammar excerpts $D_q$ and how they informed the corrections made to the glossing sequence. This explanation can be modeled as a set of reasoning steps:\n$J = [r_1, r_2, ..., r_m]$\nwhere each $r_i$ represents a single reason that links the retrieved linguistic information to the specific corrections made in $g_c$. This set of $r_i$ is not necessarily sequential.\nTo quantify the model's confidence in the corrected output, a confidence score C is generated. This score reflects the LLM's certainty in the accuracy of the final glossing sequence based on the retrieved grammar rules and the original output $g_s$."}, {"title": "3.3 Modular RAG: Training the retriever with the sequence to sequence model", "content": "Due to the size of our grammars, it makes sense to further train our retriever and rank the retrieved content based on its relevance to the query. This is an extension of the previously described Naive RAG approach. The modular RAG approach allows for a more sample-efficient utilization of the available grammar resources. By learning to retrieve and prioritize the most relevant excerpts, the model can focus on the linguistic information that is most beneficial for each specific input, rather than processing the entire grammar at once.\n1. Initial Retrieval: The process begins by retrieving k context chunks from the grammar based on relevance to the input query.\n2. Retrieval Module Training: The retrieval module, which is a different instance of the ROBERTa model, is fine-tuned based on the performance of the LLM outputs. The input to the retrieval module consists of all the initially retrieved chunks of context, and the output is a relevance vector r = $[r_1, r_2, ..., r_k]$, where $r_i \\in [0, 1]$ indicates the relevance score for the $i^{th}$ chunk of context.\n3. Final Context Selection: Out of the k re- trieved pieces of context, only the top-n most relevant pieces are selected for use in the final LLM prompt.\nLet $f_s$ be the token classification model that maps input sentence x to the initial glossing output $g_s$. The retriever module $f_r$ is trained to select the top-n relevant grammar chunks $D_q$ based on the input sentence x and the initial glossing output $g_s$:\n$D_q = f_r(x, g_s, k, n)$\nwhere k is the initial number of retrieved chunks and n is the final number of selected chunks (n \u2264 k).\nThe selected grammar chunks $D_q$ are concatenated with the input sentence x and the initial gloss- ing output $g_s$ to form a prompt P:\n$P = [x; g_s; D_q]$\n\nThe prompt P is then fed into the LLM to gener- ate the corrected glossing sequence $g_c$:\n$g_c = LLM(P)$\n\nDuring training, the retriever $f_r$ and token clas- sification model $f_s$ are jointly optimized using a combined loss function:\n$L = L_s(g_c, g_t) + \\alpha \\cdot L_r(D_q, D_t)$\nwhere:\n\u2022 $L_s$ is the sequence loss between the corrected glossing sequence $g_c$ and the ground truth glossing labels $g_t$.\n\u2022 $L_r$ is the retrieval loss that encourages the retriever to select relevant grammar chunks. It is implemented as a ranking loss between the retrieved chunks $D_q$ and the chunks that led to the best LLM performance $D_t$.\n\u2022 $\\alpha$ is a hyperparameter that controls the weight of the retrieval loss.\nBy jointly optimizing the retriever and token classification components, the modular RAG approach enables the model to learn to identify the most relevant grammar information and effectively incorporate it into the glossing process. The re- triever learns to select excerpts that are most perti- nent to the input sentence and initial gloss output. During inference, the trained retriever $f_r$ is used to select the top-n relevant grammar chunks $D_q$ for each input sentence x and initial glossing output $g_s$. These selected chunks are provided to the LLM to generate the corrected glossing sequence $g_c$."}, {"title": "3.4 Baseline Glossing Models", "content": "Our expectation is that the best results for this task will be achieved by combining a model that ex- ploits all available training data (the compact trans- former or LSTM model) with the analytical power of modern LLMs.\nMore specifically, we use two baseline gloss- ing models, both of which have been shown to achieve strong performance in the standard setting for the glossing task (Ginn et al., 2023). Following this setting, we model the production of IGT as a token classification task rather than as a sequence- to-sequence task. Specifically, we experiment with two baseline models: one using RoBERTa (Liu et al., 2019), the second using a Bi-LSTM archi- tecture. By exploiting the contextual information captured by these architectures, we aim to obtain accurate predictions of the morphological labels for each token in the input sentences. These predicted labels are then used as the initial glossing output in the RAG framework.\nFor the ROBERTa baseline, we use the same setting as the baseline for the IGT shared task, as described in Ginn et al. (2023). The input sentences are tokenized and encoded using the RoBERTa to- kenizer and encoder. The encoded representations are then passed through a linear classification layer to predict the morphological labels for each token.\nFor the Bi-LSTM model, input sentences are first tokenized and converted into word embeddings. These embeddings are then fed into the Bi-LSTM layer to obtain the contextualized token representa- tions. A linear classification layer is applied on top of the Bi-LSTM outputs to predict the morphologi- cal labels for each token. Both models are trained using cross-entropy loss and optimized using the Adam optimizer. We use an adaptive learning rate and early stopping to ensure a better fit to the data.\nWe additionally compare with two different LLMs used in a single-model, zero-shot RAG ar- chitecture. In this setting, the LLMs are solely responsible for the glossing output, rather than correcting the output of a predecessor model."}, {"title": "4 Uspanteko and Arapaho: Data and Grammars", "content": "Uspanteko is an endangered Mayan language spoken primarily in Guatemala. It is an ergative- absolutive language with moderately complex concatenative morphology. Much morphological inflection occurs on the verb stem, which takes both prefixes and suffixes and inflects for person, number, participant role, tense/aspect/mood, and voice, with a final status suffix. Arapaho is an endangered Algonquian language spoken by several communities in the Western United States. The language has free word order, polysynthetic and agglutinating morphology, and especially complex verbal morphology (Cowell and Moss Sr, 2011).\nData. We use the Uspanteko and Arapaho IGT datasets provided as part of the 2023 SIGMOR- PHON shared task (Ginn et al., 2023), licensed under CC BY-NC 4.0, and we use the data in accor- dance with the uses intended as part of the shared task. The Uspanteko dataset has about 11,000 usable sentences and about 80 unique morphological function labels.\nThe average sentence is 4.37 words, with many multi-morphemic words. The Arapaho dataset is much larger, consisting of 39,500 sentences (5.4 words on average per sentence) in the training set and 5000 in the dev set.\nFor Uspanteko, we use a very short (10 page) grammatical description, in Spanish, from the beginning of an Uspanteko-Spanish dictionary (M\u00e9ndez, 2007). For Arapaho, we use a 500-page ref- erence grammar authored by Andrew Cowell and Alonzo Moss, Sr. (Cowell and Moss Sr, 2011)."}, {"title": "5 Experiments and results", "content": "Table 1 shows results for all experimental settings, as well as the previous state-of-the-art for each language, as reported in Ginn et al. (2023). The two LLM-only baselines perform well below the gloss- ing baselines (ROBERTa and Bi-LSTM, see 3.4) and all other models. For each of the two gloss- ing baselines, we compare our naive and modular RAG models (see 3), separately in combination with Claude and GPT-4. We aim to evaluate which LLM is most effective at correcting the glossing output of the smaller token classification network, given retrieved grammar excerpts. Before evalua- tion, we perform post-processing to correct some common punctuation errors in the LLM output.\nWe evaluate on both word-level and morpheme- level accuracy metrics as described in (Ginn et al., 2023). These metrics are computed by compar- ing the corrected glossing sequences $g_{LLM}^c$ with the ground truth glossing labels $g_t$ for each input sentence x in the test set. We manage to beat the previous SOTA with modular RAG for Uspanteko and naive RAG for Arapaho.\nWe see that a RAG approach combining the ROBERTa baseline with Claude consistently performs best. The Bi-LSTM model performs reason- ably well in most cases, although it consistently trails ROBERTa. Selective retrieval seems to help more with Uspanteko than Arapaho. In fact, we see a performance drop when we train the retriever with Arapaho. Modular RAG retrieves a smaller, more focused set of grammar chunks than naive RAG. It is possible that this reduced set fails to capture all the information needed to inform the Arapaho gloss correction process, resulting in a small accuracy drop."}, {"title": "6 Qualitative analysis: usability", "content": "This system is designed to support linguists and others performing the work of interlinear glossing. The explanations generated by the model improve interpretability, as they provide an opportunity for human users to get some insight into the model's decision-making process. The best evaluation of the usability of our system would come from proper user studies, which we have begun and will report on in later work. For the present paper, we perform two manual analyses, both performed using outputs from our Modular RAG pipeline with RoBERTa + Claude.\n\nWe randomly select 30 instances. For each, we collect the original text, expected gloss, out- put of the initial glossing model, LLM-corrected output, and the complete set of explanations and retrieved grammar chunks from the RAG pipeline. A professional linguist then analyzes the number of pre-LLM errors, how many are addressed cor- rectly/incorrectly/partially correctly, the percentage of correct morpheme explanations, the subjective quality of the RAG explanations, and the subjective quality of the retrieved grammar chunks, the latter two on a Likert scale (1-5).\nThe results appear in Table 3. On average, the Uspanteko corrections are more accurate, with a similar number of new errors being introduced for both languages. Model explanations for individual morphemes are largely correct, and the chunks re- trieved for Uspanteko are slightly higher quality. We note that there is a clear difference in the na- ture of the two grammars. The Arapaho grammar is a full and complex reference grammar, and the Uspanteko grammar is a sketch, using simpler ex- planations in a more compact presentation. This initial analysis suggests the need for a deeper explo- ration into linguistic reference materials of different types and their use in RAG. Arapaho morphology is also significantly more complex than Uspanteko morphology, increasing the complexity of the task."}, {"title": "7 Conclusion", "content": "This study demonstrates the effectiveness of a Retrieval-Augmented Generation (RAG) framework in enhancing the performance of compact models for morphological glossing in low-resource language contexts. By leveraging the interpretive power of Large Language Models (LLMs) and the structured knowledge contained in grammatical descriptions, we achieve a new state-of-the-art for both languages investigated. A second advantage is the interpretability provided by LLM-generated explanations, which is crucial for building trust in the system's outputs and facilitating the use of NLP tools in language documentation efforts.\nThe RAG approach (combining a RoBERTa baseline with Claude) consistently outperforms other configurations, achieving the highest word- and morpheme-level accuracies for both languages. This framework effectively bridges the gap between the limited training data available for low-resource languages and the rich linguistic knowledge encoded in grammatical descriptions. The ability of LLMs to provide detailed explanations and confidence scores for each morpheme adds a layer of interpretability to the glossing process, potentially increasing the utility of these tools for documentary linguists. Even with minimal grammatical resources, as for Uspanteko, the RAG approach shows notable improvements over baseline models.\nThese findings suggest that the integration of linguistic knowledge through RAG can be a powerful approach for improving NLP tasks in low-resource settings. By combining the strengths of compact, trainable models with the vast knowledge encoded in LLMs and structured grammatical descriptions, we can create more accurate and interpretable tools for language documentation and analysis."}, {"title": "8 Limitations", "content": "While the proposed RAG framework for morphological glossing demonstrates promising results, there are several limitations to consider:\n1. Dependency on grammar quality: The effectiveness of the RAG pipeline heavily relies on the quality and comprehensiveness of the available grammar documents. If the grammar descriptions are incomplete, inconsistent, or contain errors, the retrieved excerpts may not provide accurate or sufficient information to guide the glossing corrections. This can lead to sub-optimal performance of the RAG model.\n2. Limited expressiveness of grammars: The linguistic rules and patterns described in grammar documents may not capture all the nuances and exceptions present in the target language. Some morphological phenomena may be too complex or irregular to be fully expressed in a concise set of rules. This limitation can hinder the RAG model's ability to generate accurate glossing labels for such cases. This is especially true in the case of our relatively small Uspanteko grammar.\n3. Scalability to larger datasets: The current experiments focus on low-resource languages with relatively small datasets. While the RAG approach is designed to be data-efficient, its performance and computational requirements when applied to larger datasets or more diverse language families remain to be investigated. The retrieval and processing of grammar excerpts may become more challenging as the size and complexity of the data increases.\n4. Generalization to unseen languages: The RAG pipeline has been evaluated on specific low-resource languages, such as Uspanteko and Arapaho. However, its generalization ca- pability to other unseen languages with dif- ferent morphological typologies is not exten- sively tested. The effectiveness of the ap- proach may vary depending on the similarity of the target language to the languages used in training and the availability of suitable gram- mar resources.\n5. Reliance on proprietary models: We currently use two proprietary LLMs for these experiments. Once we have the appropriate compute infrastructure established, we plan to implement the same architecture using an open-source model.\n6. Single evaluator for manual qualitative analysis: So far we have performed only a small-scale, somewhat subjective analysis of the quality of the generated explanations and the relevance of retrieved grammar chunks for the sentences being glossed. We are currently building a web interface to allow linguists and other users to interact and evaluate model outputs. Once finished, we will expand this analysis to a more robust user study with multiple users and multiple languages."}, {"title": "9 Ethical Considerations", "content": "Although generative AI is clearly helpful in making the process of language documentation more explainable and efficient, it can also be deceptively convincing in its justifications. Through the initial prompt, the LLM is primed to report any uncertainties and assign confidence scores, but as seen in the qualitative analysis, this is in no way a foolproof approach. The RAG-driven system is not ready to be used by language learners or for large-scale language documentation tasks without further testing.\nWe also recognize that we use sensitive language data and resources to obtain our results. All our data is part of the publicly available Sigmorphon 2023 shared task data as mentioned in section 4, and we have permission from the producers of the two grammars to use them for research purposes. We will continue to seek express permission from communities and authors before using low- resource data and grammars for future experiments."}, {"title": "A Sample explanations", "content": "Given the Uspanteko sentence \"t wi' tene' qleen tob'ool laq b'ee\", Claude produced the following corrected gloss: \"EXST talvez NEG-existir ayudar- NMLZ PREP camino\". In addition to the corrected gloss, Claude generated a detailed explanation for each morpheme, along with confidence scores:\n\u2022 'wi\" is an existential predicate (EXIST) indi- cating the presence or existence of something. (confidence: 0.9)\n\u2022 'tene\" is a particle meaning 'maybe' or 'per- haps'. (confidence: 0.7)\n\u2022 'qleen' is a negative existential predicate (NEG-exist), indicating the non-existence of something. (confidence: 0.9)\n\u2022 'tob'ool' can be broken down as: 'tob\" (verb root meaning 'to help'), '-ool' (suffix nominalizing the verb, NMLZ), yielding a noun meaning 'help' or 'assistance'. (confidence: 0.8)\n\u2022 'laq' is a preposition meaning 'at' or 'in'. (confidence: 0.8)\n\u2022 'b'ee' is a noun meaning 'path', 'road', or 'way'. (confidence: 0.9)\nClaude also provided information on how the RAG approach was used to arrive at the corrections:\n\u2022 The existential predicate 'wi\" and the negative existential predicate 'qleen', both of which appeared in the example sentences.\n\u2022 The information about the suffix '-ool' nominalizing verbs (NMLZ), which was mentioned in the 'Estructura de las entradas' (Structure of the entries) section.\n\u2022 The noun 'b'ee' meaning 'path', which ap- peared in one of the example sentences.\n\u2022 The meanings of 'tene\" and 'laq' were inferred from context, as they are not explicitly defined in the grammar document."}, {"title": "A.2 Arapaho Example", "content": "Given the Arapaho sentence \"Hookoh neniih'ei3i'okuno' hoo3oo'o' hiniito'eino notoone hiihoowuno'useenino\", Claude pro- duced the following corrected gloss: \"because IC.scattered-sit-1PL what 3S-relative-OBV.PL 1S-people NEG-arrive-3PL\". In addition to the corrected gloss, Claude generated a detailed explanation for each morpheme, along with confidence scores:\n\u2022 hookoh: glossed as because, seems to be a conjunction (confidence: 0.9)\n\u2022 neniih'ei3i'okuno': a complex verb form (confidence: 0.8)\n   neniih'ei-: possibly an initial change (IC) form of scattered\n -3i'oku-: likely means sit\n   no': 1PL (first person plural) marker\n\u2022 hoo3oo'o: glossed as what, likely an interrog- ative pronoun (confidence: 0.9)\n\u2022 hiniito'eino: appears to be a noun phrase (confidence: 0.7)\n  hini-: possibly 3S (third person singular) prefix\n   ito'ei-: root meaning relative\n -no: OBV.PL (obviative plural) marker\n\u2022 notoone: glossed as 1S-people, likely a pos- sessed noun (confidence: 0.8)\n   no-: 1S (first person singular) possessive prefix\n   toone: root meaning people\n\u2022 hiihoowuno'useenino: a complex verb form (confidence: 0.8)\n   hiihoowu-: NEG (negation) prefix\n   no'usee-: verb root meaning arrive\n  -nino: 3PL (third person plural) marker\nClaude also provided information on how the RAG approach was used to arrive at the corrections:\n\u2022 The basic morphosyntax structure provided insight into the order of elements in the verb complex.\n\u2022 The document mentions person markers and their positions, which helped in identifying -no' as 1PL and -nino as 3PL.\n\u2022 The negation prefix ihoowu- is mentioned, which appears here as hiihoowu-.\n\u2022 The document discusses obviative plural mark- ers, supporting the interpretation of -no in hiniito'eino.\n\u2022 The contraction rules mentioned weren't di- rectly applicable here but provided context for understanding morpheme boundaries."}, {"title": "B Details of qualitative analyses", "content": "In section 6 we present an analysis of the types of glossing errors made by our best model, across 50 randomly-selected sentences. The figures pre- sented there are for high-level error categories. During the analysis, we consider a number of subtypes for each high-level error type.\nTable 6 (next page) shows the complete set of error types and subtypes, with frequencies, examples, and descriptions."}, {"title": "B.2 Quality of explanations", "content": "Our process for analyzing the quality of explana- tions provided consisted of five steps.\n1. Compare glossing output of the baseline token classification model to the expected (gold stan- dard) glossing output, counting the number of errors at the morpheme level.\n2. Compare the LLM-corrected to the baseline output. For each error in the baseline output, determine whether the LLM made a correct correction, an incorrect correction, or a par- tially correct correction. In addition, look for new errors introduced by the corrective LLM.\n3. For the set of morpheme explanations, mark each as correct, partially correct, or incorrect. Determine the percentage of correct expla- nations by comparing to the expected gloss, with partially correct explanations receiving 0.5 points.\n4. Rate the set of explanations about how RAG was used according to their usefulness and/or correctness, using the scale in Table 4.\n5. For each retrieved grammar chunk, rate its quality/relevance for the example being glossed, using the scale in Table 5. Compute the average score across all retrieved grammar chunks."}]}