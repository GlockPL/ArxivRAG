{"title": "Forma mentis networks predict creativity ratings of short texts via\ninterpretable artificial intelligence in human and GPT-simulated raters", "authors": ["Edith Haim", "Natalie Fischer", "Salvatore Citraro", "Giulio Rossetti", "Massimo Stella"], "abstract": "Creativity is a fundamental skill of human cognition. We use textual forma mentis networks\n(TFMN) to extract network (semantic/syntactic associations) and emotional features from\napproximately one thousand human- and GPT3.5-generated stories. Using Explainable\nArtificial Intelligence (XAI) we test whether features relative to Mednick's associative theory\nof creativity can explain creativity ratings assigned by humans and GPT-3.5. Using XGBoost,\nwe examine 3 scenarios: (i) human ratings of human stories, (ii) GPT-3.5 ratings of human\nstories, and (iii) GPT-3.5 ratings of GPT-generated stories. Our findings reveal that GPT-3.5\nratings differ significantly from human ratings not only in terms of correlations but also because\nof feature patterns identified with XAI methods. GPT-3.5 favours \u201cits own\u201d stories and rates\nhuman stories differently from humans. Feature importance analysis with SHAP scores shows\nthat: (i) network features are more predictive for human creativity ratings but also for GPT-\n3.5's ratings of human stories; (ii) emotional features played a greater role than\nsemantic/syntactic network structure in GPT-3.5 rating its own stories. These quantitative\nresults underscore key limitations in GPT-3.5's ability to align with human assessments of\ncreativity. We emphasise the need for caution when using GPT-3.5 to assess and generate\ncreative content, as it does not yet capture the nuanced complexity that characterises human\ncreativity.", "sections": [{"title": "1. Introduction", "content": "Creativity is a fundamental aspect of human knowledge. Understanding how conceptual\nassociations convey creativity has become a focal point of scientific research. In a cognitive\ncontext, creativity can be defined as the ability to generate ideas or products that are both novel\nand valuable (Boden, 1998; Runco & Chand, 1995). When it comes to texts, creativity involves\nthe use of language in innovative ways, encompassing originality in word choice, syntactic\nstructure, and the overall thematic expression (Weinstein et al., 2022; Johnson et al., 2023).\nEvaluating the creativity of texts presents a challenge due to the subjective nature of creativity\nand the specific characteristics of textual data. Metrics such as word infrequency, unique word\ncombinations, syntax uniqueness, rhyme, and phonetic similarity have been proposed as good\nexplainers for variance in human creativity ratings (Weinstein et al., 2022). Creativity in texts\ngenerated by large language models (LLMs), however, has not yet received as much attention,\nwhich leaves a gap in the literature.\nIn short narratives, creativity has been identified through various stylistic and\nqualitative features. These include new word formations, unexpected turns in the plot, and\noriginal and vivid settings (D'Souza, 2021). An unexpected turn in the plot refers to a surprising\ndevelopment or twist that deviates from the anticipated storyline. Vivid settings, on the other\nhand, are detailed and imaginative descriptions of the environment in which the story takes\nplace, allowing readers to visualise and immerse themselves in the narrative (D'Souza, 2021).\nBeyond these qualitative features, the creativity level of a short narrative can also be assessed\non a more quantitative level. Psychometric approaches have been used to evaluate creativity in"}, {"title": "1.1 Quantitative assessments of semantic and syntactic features of creative stories", "content": "Advancements in computational models, such as Bidirectional Encoder\nRepresentations from Transformers (BERT), have demonstrated significant predictive power\nin assessing creativity based on the structural components of narratives (Johnson et al., 2023).\nBERT is a deep learning language model that understands the context of words in a text by\nanalyysing their co-occurrences (Vaswani et al., 2017). To represent the structural components\nof narratives, BERT uses word embeddings, i.e. numerical representations of words that\ncapture meanings, syntactic properties, and relationships with other words. These embeddings\nplace words in a continuous vector space, where words with similar meanings are located closer\nto each other. This allows BERT to effectively understand and process the context of words in\na text, enhancing its ability to analyse and generate human-like language (Johnson et al., 2023).\nWhereas embeddings represent words as numerical vectors of non-interpretable word-word co-"}, {"title": "1.2 Main aims of this work", "content": "Importantly, automated assessments of creativity levels in stories can be applied also to\nthe investigation of machine psychology. Considering Large Language Models (LLMs) as\ncognitive agents able to search for concepts in a certain knowledge space and assemble\nnarratives, one can consider the task of assessing creativity features in LLMs as similar to the\ntask of assessing creativity in humans and their stories. However, to the best of our knowledge,\ndespite some studies investigating creativity in LLMs directly via psychometric questionnaires\n(G\u00f3es et al., 2023), assessing LLMs' creativity features from the text that they produced is\napparently an unexplored research direction. This gap can be addressed with the above\nframeworks from cognitive network science.\nThe current work lies at the intersection of machine learning, creativity research and\ncognitive network science, providing a quantitative comparison between the features that\ncharacterise those stories being rated as creative by either humans or GPT-3.5. Our approach\nleverages Explainable Artificial Intelligence (XAI) in order to address the limitations of\nsubjective human evaluations (Beaty & Johnson, 2021; Heinen & Johnson, 2018).\nFurthermore, by contrasting not only ratings but also human and GPT-3.5-generated stories,\nwe seek to investigate the shared or diverging attributes of creativity as perceived by both\nhuman raters and AI.\nThe paper at hand builds on the foundational work by Johnson et al. (2023) and extends\nthe understanding of how quantitative cognitive network features, beyond distance, can predict\ncreativity ratings. We investigate the influence of 13 quantitative network and emotional\nfeatures of texts on creativity scores. These additional measures offer deeper insights into the\norganisation and thematic consistency of narratives, potentially correlating with their creative\npotential (Berahmand et al., 2018; Orwig et al., 2021; Siew, 2013)."}, {"title": "1.3 Beyond semantics: Incorporating emotions in short stories' creativity ratings", "content": "Besides syntactic and semantic structure, the role of emotions in narrative creativity\nshould not be ignored. Emotions profoundly influence memory, cognitive processing,\nimagination, and the overall structure of narratives (Hustvedt, 2011; Miall, 2011). Research by\nVrana et al. (2019) has demonstrated that the coherence of narratives varies with emotional\ncontent, with neutral narratives exhibiting greater coherence than those recounting traumatic\nevents. By incorporating emotion analysis into our assessment framework, we aim to explore\nhow the presence and intensity of basic emotions (anger, trust, surprise, disgust, joy, sadness,\nfear, and anticipation) influence creativity ratings. We implement this emotion analysis using\nthe EmoAtlas library (Semeraro et al., 2024), which captures emotions through psychologically\nvalidated datasets and provides interpretable measures."}, {"title": "1.4 Manuscript outline", "content": "This work is structured around four key objectives, aimed at deepening our\nunderstanding of the interplay between narrative structure, emotion, and perceived creativity.\nFirstly, we focus on predicting creativity levels based on specific network and emotion features\nof the stories. The network features we try to leverage include concept centrality (e.g. degree,\nPageRank) and features of network structure (e.g. average shortest path length, clustering\ncoefficient, diameter) with some levels of interpretability within the literature of cognitive\nnetwork science. By analysing these features, we aim to identify which elements are most\nclosely related to high or low creativity ratings given by human or GPT-3.5 raters. We also\ninvestigate the differences across four raters to determine if any rater assigns more importance\nto certain features over others, reflecting individual biases in creativity assessment.\nSecondly, we compare the semantic network features and emotion scores between\nstories generated by human participants and those produced by GPT-3.5. This comparison will\nreveal differences in how these narratives are structured and the emotional content they convey.\nThirdly, we have also tasked GPT-3.5 with rating the original human stories collected\nby Johnson et al. (2023), thus obtaining a third set of ratings of GPT-3.5 rating human stories.\nWe use this as an additional point of comparison to assess how GPT-3.5 rates stories generated\nby GPT-3.5 differently from human stories and how the GPT-3.5 ratings differ from the human\nratings. We expect that certain network features, such as a higher clustering coefficient and\ndegree centrality, will correlate strongly with higher creativity ratings, reflecting well-defined\nthemes and pivotal narrative elements. On an emotional level, we anticipate that narratives with\nhigher z-scores for positive emotions like joy and anticipation will be rated as more creative,\nconsistent with findings that positive emotional valence enhances creative flexibility, which is\na critical component of creative thinking. For instance, positive emotional states can reduce"}, {"title": "2. Methodology", "content": "Finally, we also hypothesise that differences will emerge in the creativity assessments\nbetween human raters and GPT-3.5 raters, with AI potentially focusing more on structural\ncoherence and complexity due to its training on vast amounts of text data (Boden, 1998). By\naddressing these questions, our project aims to contribute to the understanding of narrative\ncreativity and the potential of AI in creative assessments."}, {"title": "2.1 Materials", "content": null}, {"title": "2.1.1 Human dataset", "content": "The human dataset used in this study was generated as part of Study 2 of a series on\ndivergent semantic integration (DSI) by Johnson et al. (2023). In the original study, Johnson et\nal. (2023) recruited 153 participants using Amazon's Mechanical Turk and compensated them\n$5.00 for their participation. The demographic distribution was as follows: Mean age = 38.62\n(range: 22-70 years); 82 women, 68 men, 3 non-binary individuals; 97% English first-language\nspeakers; 78% White, 9% African-American, 3% Asian-American, and 9% other.\nParticipants were instructed to write a total of 7 creative short stories (excluding one\npractice story). Three-word prompts were provided to them and all 3 words had to be included\nin the respective story. Each story was to be 4-6 sentences long and written within 4 minutes.\nThe instruction script that was provided to the participants in the original study can be found\nin Table 1 in direct comparison with the modified instruction script we used for replicating the\nprocess with GPT-3.5.\nAfter going through the instructions, the participants were given the following prompts,\none by one, in a randomised order per participant: stamp-letter-send; gloom-payment-exist;"}, {"title": "2.1.2 GPT dataset", "content": "To replicate the original study as closely as possible, we started out by testing and\nadapting the original instruction script. We did this by interacting directly with the online\nChatGPT interface of GPT-3.5 and performing prompt engineering in an interactive way,\ntesting responses directly on the web interface. When we were getting adequate and consistent\nresponses to our prompts we settled on a script, which can be found in Table 1 in direct\ncomparison with the original script. We used the same three-word prompts as Johnson et al.\n(2023). We then manually generated the stories using the GPT-3.5 web interface, resetting the\nanonymous chat after each \u201cindividual\u201d. We did this to minimise the risk of previous response\ninfluences between distinct participants. We retrieved a total of 1071 stories generated by GPT-\n3.5: 7 from each of the 153 \u201cparticipants\u201d. There were a few instances in which the \u201cregenerate\"\nfunction had to be employed, primarily due to technical errors or token limitations imposed by\nOpenAI, the company behind ChatGPT. In those cases, no story had been generated for the\nparticipant or prompt in question."}, {"title": "2.2 Creativity Ratings of Stories", "content": null}, {"title": "2.2.1 Human raters", "content": "In their study, Johnson et al. (2023) recruited 4 human raters, tasked with assigning\nratings to the stories, ranging from 1 (least creative) to 5 (most creative). The instructions for\nthe raters, as taken from the supplementary materials of the original study, can be found in the\nAppendix I."}, {"title": "2.2.2 GPT raters", "content": "Following Johnson et al. (2023), we employed 4 artificial raters. To do this, we\ndeveloped Python code using Google Colab, thus automating the rating process. Using the\ncode, we made four separate calls to the GPT-3.5 API, each one serving as a distinct judge,\ntasked with rating all of our AI-generated short stories.\nTo avoid errors, we did not loop through the code four times and instead kept the raters\nseparated in distinct sections. The structure of the code remained the same for each rater to\nensure consistency. The code initialises the OpenAI client and then iterates through the stories,\nsending each one to the API and collecting the returned rating in a list. The list is converted\ninto a dataframe, which is later integrated into a complete dataframe, containing the ratings of\nall four judges.\nWe were able to keep the general instructions for the raters concise; however, we\nneeded to put special emphasis on the output format, specifically requesting single-number-\nonly ratings to avoid obtaining in-depth analyses (GPT-3.5 sometimes explained the reasons\nfor a rating even when not asked to). This approach ensured that we obtained the expected\noutput format with high consistency. The full code can be found on OSF (https://osf.io/gcjqv/)."}, {"title": "2.3 Computing Cognitive Networks from Short Stories", "content": "In this work we use textual forma mentis networks (TFMNs) to analyse the structural\nand conceptual construction of short stories. TFMNs provide a structured representation of\nmental associations between concepts in a narrative, capturing both structural and emotional\nlinks (Stella, 2020). TFMNs are constructed from textual data and break down narratives into\nsyntactic and semantic connections between words to represent how ideas connect within\nnarratives (Improta et al., 2024; Semeraro et al., 2024). TFMNs rely on spaCy, a natural\nlanguage processing library that analyses text to identify semantic and syntactic relationships\nbetween concepts in a given text. SpaCy performs tasks such as sentence splitting, tokenisation,\nand syntactic parsing, which allow TFMNs to accurately model grammatical and associative\nrelationships within each sentence (Improta et al., 2024). The construction of TFMNs relies on\nsplitting a text into sentences and iteratively performing the following steps on each sentence:\n1) Tokenisation: Each sentence is split into a set of tokens, i.e. words in our case.\n2) Syntactic parsing: Using spaCy, each sentence undergoes syntactic parsing to form\na syntax tree, identifying connections between words that share syntactic dependencies (e.g.\nsubject and object connected grammatically to the verb) even if they are separated by other\nelements within the sentence. This provides a considerable advantage over co-occurrence\nnetworks (Amancio, 2015), which only create connections between concepts that are located\nwithin a specified distance of each other. Consider the following example sentence: \u201cPeter,\ndespite his lactose intolerance and the high cost of milk-based products, loves cheese.\u201d By\nanalysing the text on the basis of a syntactic tree, the subject (\u201cPeter\u201d) can be linked without\nproblems to the grammatically connected verb and object (\u201cloves cheese"}, {"title": "2.4 Extracting Network Features from Short Stories", "content": "3) Connecting non-stop words on the syntactic tree: Once the syntactic trees are\ngenerated for each sentence, links are established between pairs of non-stopwords (e.g., nouns,\nverbs, adjectives, adverbs or in general words possessing a meaning of their own). Stop words\n(such as \u201cand, or this, the\u201d) are not considered in this step. Non-stop words are linked if within\na distance of 3 nodes on the syntax tree. This approach allows only words that are syntactically\nclose (regardless of how many words are positioned at each syntactic node) to be linked within\nthe TFMN. In the example above, \u201cPeter\u201d is only separated by one intermediate syntactic node\nfrom \"loves\" despite twelve words occupying that syntactic level (Semeraro et al., 2024).\n4) Construction of TFMNs: For each sentence, TFMNs are built separately, linking\nsyntactically related tokens at a maximum distance of three steps from another (Semeraro et\nal., 2024). The networks are then merged across sentences to create a unified TFMN for the\nentire text. This is done by appending all edge lists from each sentence into a unique simple\ngraph, thus creating an edge list representing the whole text (Semeraro et al., 2024).\n5) Normalisation of words: After constructing an edge list for the entire TFMN, the\ntokens are normalised for consistency through lemmatisation. Lemmatisation means\ntransferring the specific word into its lemmatized form, which is its base or dictionary form,\nknown as lemma. Thus, different inflections of a word are grouped together to their\nunderlying lemma (e.g. \u201chappiness\u201d and \u201chappier\u201d \u2192 lemma \u201chappy\u201d). Tokens that become\nthe same through normalisation are merged together (Semeraro et al., 2024).\n6) Assigning emotional valence: Using the EmoLex dataset (Mohammad & Turney,\n2013), emotional values are assigned to each word, marking them as positive, negative or\nneutral. In this step, the syntactic parsing supports taking negations into account for assigning\nemotions. For example, the phrase \u201cnot happy\u201d should be evaluated as the opposite of \u201chappy\u201d\nto correctly grasp the meaning. When emotional scores are computed, by analysing terms based\non the syntactic structure, a negation can be properly linked to its associated term, which is"}, {"title": "3. Descriptive statistics of the dataset", "content": "After constructing a forma mentis network for every story in the dataset, we can extract\nthe following key network features from the syntactic and semantic structure of word\nassociations in texts: Diameter, average shortest path length, clustering coefficient, degree\ncentrality, and PageRank centrality. These measures reveal global aspects of the narrative's\nstructure, such as its interconnectedness and cohesion (Newman, 2010).\nA measure of distance is the average shortest path length (ASPL). It quantifies the\nefficiency of information flow within a network by calculating the average of the shortest path\nlengths between all pairs of nodes in a network. A shortest path length is the length of the\nsequence of links connecting any two nodes with the fewest hops. The average shortest path"}, {"title": "4. Results", "content": "For our analysis, we employed an interpretable machine learning approach to explore\nthe relationships between network and emotion features and the creativity ratings of short\nstories. We developed multiple models to analyse the dataset comprehensively as follows:\n(i) a model for the human dataset (humans rating human stories);\n(ii) a model for GPT-3.5 ratings of human stories;\n(iii) a model for the entire GPT-3.5 dataset (GPT-3.5 rating GPT-3.5 stories).\nEach model considers only stories rated by all 4 raters and joins them together into one dataset\nwithout differentiating between raters. Minimal variation in ratings among the GPT-3.5 raters\n(Figure 1B and 1C) indicate a high consistency between GPT-3.5 raters."}, {"title": "4.1 Summary Statistics for Network and Emotion Features", "content": "In this Section, we present the findings from our study, aimed at predicting creativity\nlevels based on network and emotion features of short stories."}, {"title": "4.2 Model Performance Evaluation", "content": "On average, human stories are shorter, with about $n_H$ 70 words, while GPT-3.5\nstories have an average length of $N_{GPT}$ = 121 words. Fixing a significance level of 0.05, a\nMann-Whitney U test identified a statistically significant difference in story length (U-statistic\n= 42007.5; \u043f\u043d = nGPT = 1071; p < 0.001). We resorted to a non-parametric testing after having\nqualitatively checked that the data was not distributed according to a Gaussian distribution\n(e.g., it displayed higher skewness and kurtosis).\nBy computing a Mann-Whitney U test for the reduced human dataset (only stories rated\nby all four raters) and GPT3.5-generated stories, we observed significant differences in both\nnetwork and emotion features between human and GPT3.5-generated stories as shown by p-\nvalues well below a threshold of 0.001 (Table 2). Only the features diameter, anger and sadness\nhad a p-value larger than 0.05 which indicates that we cannot assume a statistically significant\ndifference between human and GPT3.5-generated stories for these story features."}, {"title": "4.3 Feature Importance Analysis", "content": "To evaluate which classifier would be the best for our models, we focused on the\naccuracy of different classifiers applied to each of the three distinct scenarios: humans rating\nhuman-generated stories, GPT-3.5 rating human-generated stories, and GPT-3.5 rating\nGPT3.5-generated stories. For this reason, we employed a 4-fold cross-validation technique.\nDuring this 4-fold cross-validation, the XGBoost classifier emerged as the most effective\nmodel due to its superior performance metrics compared to other classifiers (see Table 3). Our\nevaluation considered twelve different classifiers and their accuracy scores are detailed in Table\n3, which lists models from best to worst performance for human ratings. For the human stories,\nthe XGBoost obtained the best accuracy levels (human ratings: 0.617; GPT3.5 ratings: 0.716),\nfollowed by the random forest and decision tree classifiers. However, it is noteworthy that for\nGPT3.5-generated stories, gradient boost (0.757) marginally outperformed XGBoost (0.752).\nDespite this, to maintain comparability across all models, we consistently used the XGBoost\nclassifier (Chen & Guestrin, 2016; Dietterich, 2000). Notice that we performed this cross-\nvalidation on the whole dataset because in this analysis our main aim is not generalisation but\nrather extracting robust features present in the currently evaluated stories. If we did model\nselection on a sub-sample of the stories, we would have lost information relative to those\nspecific stories. Hence, to maximise instances of observation of as many stories as possible,\nwe decided to perform model selection on the whole dataset, sacrificing model generalisability\nbut improving the amount of phenomena that we can describe for the current dataset."}, {"title": "5. Discussion", "content": "In our exploration of feature importance for predicting creativity ratings, we employed\nSHAP value plots (Lundberg & Lee, 2017) using the SHAP package in Python (Lundberg et\nal., 2019) to visualise the influence of various network and emotion features across three rating\nmodels: the human-rating-human model, the GPT-rating-human model, and the GPT-rating-\nGPT model. The SHAP plots display the average impact on model output magnitude for each\nfeature within these models. In Figure 5, panel A shows the human model, where network\nfeatures like PageRank centrality and degree centrality are most influential, particularly for\nclass 2 (high creativity). Emotion features such as disgust and trust also play significant roles,\nindicating that certain emotional tones correlate with higher creativity ratings. Figure 5, panel\nB depicts the GPT-rating-human, which balances both network and emotion features.\nPageRank centrality and degree centrality remain crucial, but average shortest path length\n(ASPL), sadness, and clustering coefficient also have significant impacts, reflecting a nuanced\nconsideration of structural and emotional aspects in creativity prediction. Figure 5, panel C\npresents the GPT-rating-GPT model, which heavily relies on emotion features like anger,\nanticipation, and joy, especially for class 1 (middle creativity) and class 2 (high creativity).\nThis highlights GPT-3.5\u2032s preference for emotional content over network features. The GPT-\nrating-GPT model's distinct feature importance profile underscores its differing approach in\ncreativity assessment compared to human raters. These results suggest that the human-rating-\nhuman model prioritises structural features, while the GPT-rating-GPT model focuses more on\nemotional cues. This emphasis on emotional cues in GPT-3.5\u2032s stories is further supported by\na qualitative view on the narratives. A closer reading reveals a reliance on vivid, pictorial\nlanguage rich in positive emotional content. This perception is aligned with the findings from\nSection 4.1, where GPT-3.5 stories demonstrated statistically significant higher z-scores for\nemotions such as joy, anticipation, and trust compared to human-authored stories. This\ndistinction underscores GPT-3.5\u2032s tendency to enhance its narratives with pronounced\nemotional tones, which may contribute to its differing approach to creativity assessment,\nfavouring emotional features over structural ones.\nIn contrast, the GPT-rating-human model assigns considerable weight not only to\nemotional features but also to network features, thus aligning more closely with the human\nraters than the GPT-rating-GPT model. However, the difference found between human and\nGPT-3.5 ratings of human-authored stories (see Section 3) suggests that GPT-3.5\u2032s evaluations\nare still distinct from those of human raters, even when rating human-authored stories. What"}, {"title": "5.1 Interpretation of Findings", "content": "about the evaluation criteria behind these ratings? Interestingly, despite scores attributed to\nstories differing between GPT-3.5 and humans, SHAP plots (Fig. 5 and Fig. 6) highlight that\nGPT-3.5 appears to rely on features more similar to human criteria when assessing human-\nauthored stories than when assessing GPT-generated stories."}, {"title": "5.2 Contributions to Knowledge - AI and Creativity", "content": "Our study reveals significant insights into the structural and emotional differences\nbetween human-authored and GPT3.5-generated stories, with implications for how these\nnarratives are perceived and rated for creativity. The analysis showed that GPT-3.5 stories tend\nto be longer (average of 121 words) compared to human stories (average of 70 words). This\nlength difference is accompanied by higher clustering coefficients in GPT3.5-generated stories,\nindicating more interconnected narrative structures. On an emotional level, GPT3.5-generated\nstories exhibited higher levels of positive emotions such as joy and trust, whereas human stories\ncontained more negative emotions like fear and disgust. These distinctions were statistically\nsignificant, with p-values well below the 0.05 threshold.\nThe SHAP value plots provided insights into the influence of various features across\nthe models. For the human model, network features like average PageRank centrality and\ndegree centrality were most influential, particularly for high creativity ratings. Higher average\nvalues of PageRank centrality correlate with stories that are more interconnected, looping back\nand linking concepts to one another. This type of structure is syntactically more interconnected\nand it places concepts at a lower network distance, potentially corresponding to more rigid and"}, {"title": "5.3 Limitations", "content": "This study advances our understanding of creativity assessment in significant ways. By\nfocusing on the prediction of creativity levels in short stories through network and emotion\nfeatures, our research contributes to the broader discourse on Al's role in creativity assessment.\nIt also highlights important distinctions between how GPT-3.5 perceives and generates creative\ncontent compared to human individuals.\nIn the literature, there is a growing integration of transformer models in creativity\nassessments. Sun and colleagues (2024) explore the use of semantic distance and machine\nlearning to predict human ratings of creativity in the Alternative Uses Task (AUT). Their study\nfound that contextual semantic models, such as GPT-3 and RoBERTa, were better aligned with"}, {"title": "5.4 Future Directions", "content": "At the current time of writing up this manuscript, GPT-3.5 is not available any more\nfor use via the web interface but it remains available in the GPT store and for usage via API,\nthus still enabling scientific experiments.\nWhile our study provides valuable insights, several limitations should be\nacknowledged. Firstly, as GPT-3.5 was consistently rating its own stories high in creativity,\nthis resulted in a very small sample size of low-rated GPT3.5-generated stories. This resulted\nin a more biassed performance of the XGBoost model with many misclassifications in the\nconfusion matrix. Thus, a more balanced dataset in terms of rating frequency would be\nnecessary to improve model reliability.\nAdditionally, GPT-3.5\u2032s behaviour may differ from more advanced models such as\nGPT-4, which has been shown to exhibit improvements in nuanced reasoning and evaluation\ntasks (Espejel et al., 2023). This suggests that our findings may be specific to GPT-3.5 and may\nnot reflect the behaviour of more recent models.\nFurthermore, we condensed the dataset to consist of only stories that were rated by all\nfour raters and calculated the model across all raters, disregarding variations between the four\ndistinct human raters. Alternative statistical approaches, such as linear mixed models, could be\nused to account for the different behaviours of individual raters to minimise potential biases\nintroduced by rater-specific tendencies. Linear mixed models allow for the inclusion of random"}, {"title": "Conclusion", "content": "Future research could delve deeper into the potential biases present in AI evaluations\nof creative content. Our findings indicate a clear bias in GPT-3.5\u2032s ratings, favouring stories\nwritten by GPT-3.5 over those authored by humans. Addressing this bias is crucial for\ndeveloping more accurate AI evaluative systems.\nOne intriguing future direction involves instructing GPT-3.5 to deliberately write\nuncreative stories. By having these intentionally uncreative stories rated by both GPT-3.5 and\nhuman raters, researchers can investigate whether GPT-3.5 can deliberately produce and\naccurately recognize lower creativity levels. This approach would provide insights into GPT-\n3.5's ability to follow specific creative directives and its consistency in evaluating creativity\nacross different narrative qualities.\nAnother promising area for future research is to test the models and findings across\ndifferent languages. Examining whether the same structural and emotional features are relevant\nfor stories written in languages other than English would help determine if these features are\niversally influential in creativity assessments. This cross-linguistic investigation would\ncontribute to a more comprehensive understanding of creativity and the development of AI\nmodels that are effective across diverse linguistic contexts."}, {"title": "Appendix", "content": "The findings of this study quantify different patterns characterising creativity\nassessment of short stories in humans and simulated raters (GPT-3.5). By considering textual\nstories as structured data, i.e. textual forma mentis networks of syntactic/semantic/emotional"}, {"title": "I. Instructions for Raters", "content": "relationships between words, this study uses explainable AI to identify which features lead to\nthe prediction of human and GPT-3.5\u2032s ratings of short stories. Our findings demonstrate that\nin producing creativity ratings of human stories, human raters place emphasis on structural\nsemantic network features (e.g. degree centrality and PageRank centrality) in conjunction with\nusing the emotional content of the story (e.g., its elicited joy or trust). Importantly, similar\npatterns are found also in GPT-3.5. However, when GPT-3.5 rates its own GPT-based stories,\nthe LLM shows a marked reliance on emotional features, suggesting an inclination towards\naffective attributes over structural ones in the self-assessment of creativity.\nThis discrepancy points to limitations in current LLM models' interpretative\nframeworks for creativity, as these models exhibit an emotion-centric approach rather than\nassessment heuristic based on semantic/syntactic association structure. Such findings\nunderscore the importance of advancing LLMs to enhance their alignment with human\nevaluative criteria when called to produce \u201ccreative\u201d content and rate it."}, {"title": "II. Comparative Analysis for Human-generated and GPT-generated Stories", "content": "In this task, participants were shown 3 words (\"stamp\", \"letter\", \"send\"), and were asked to\nwrite a short story that included all 3 words. They were told to be imaginative and creative\nwhen writing their stories.\nYou will first be shown just 10 stories, and you will have to first rank these 10 stories from\nleast creative to most creative, and then give each of them a rating from 1 (very uncreative) to\n5 (very creative). After this you will be shown the rest of the stories and you will have to give\neach of these a rating for creativity too.\nWhen rating the stories, try not to focus too much on the length of the story, or how good the\nEnglish is, but consider the overall creativity of the story. You may wish to consider how\ncreatively the 3 words were used, how emotive, descriptive, or humorous the story was, and\nhow much it \"came alive\".\nScale:\n1: Very Uncreative\n2: Uncreative\n3: Undecided\n4: Creative\n5: Very Creative.\n(Johnson et al., 2023, Supplementary Materials)"}, {"title": "IV. The most creative rated stories by humans and GPT-3.5"}]}