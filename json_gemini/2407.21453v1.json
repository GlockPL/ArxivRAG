{"title": "TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors", "authors": ["Z. Huang", "A. Tousnakhoff", "P. Kozyr", "R. Rehausen", "F. Bie\u00dfmann", "R. Lachlan", "C. Adjih", "E. Baccelli"], "abstract": "Monitoring biodiversity at scale is challenging. Detecting and identifying species in fine grained taxonomies requires highly accurate machine learning (ML) methods. Training such models requires large high quality data sets. And deploying these models to low power devices requires novel compression techniques and model architectures. While species classification methods have profited from novel data sets and advances in ML methods, in particular neural networks, deploying these state of the art models to low power devices remains difficult. Here we present a comprehensive empirical comparison of various tinyML neural network architectures and compression techniques for species classification. We focus on the example of bird song detection, more concretely a data set curated for studying the corn bunting bird species. The data set is released along with all code and experiments of this study. In our experiments we compare predictive performance, memory and time complexity of classical spectrogram based methods and recent approaches operating on raw audio signal. Our results demonstrate that our TinyChirp approach can robustly detect individual bird species with precisions over 0.98 and reducing the energy consumption such that a deployment time on a single battery charge can be extended from 2 weeks to almost an entire season of 18 weeks.", "sections": [{"title": "I. INTRODUCTION", "content": "The typical data pipeline with bio-acoustic research requires the deployment of sensors, in which each node is battery- powered and left in the field to record environmental sound, continuously, for a long period (e.g. for a whole season). Thereafter, the recorded data is manually collected by re- searchers on-site, from each node, and further analyzed in laboratory. In the case of avian species for instance, only the targeted bird species is relevant within the recorded data, and the rest of the data is to be discarded ultimately. This is both a cumbersome process and a waste of resources, not only at this downstream stage in the lab, but also upstream in the field, on each node, while recording large amounts of irrelevant data. In this pipeline, continuous recording of audio thus creates a bottleneck in terms of memory and energy budgets available on individual sensors typically limited to a small battery, and an SD card, respectively, driven by rudimentary software running on a low-power microcontroller. Such hardware is very energy-efficient, but very limited in memory resources, with RAM memory budgets in the order of 500 kiloBytes [1], which yields specific constraints on software embedded on such devices [2].\nMeanwhile, as Artificial Intelligence (AI) achieves excellent performance in pattern recognition of audio and biosignals, more and more bioacoustic researchers leverage Machine Learning (ML) related methods to improve accuracy and efficiency. However, until recently, such ML models, e.g. BirdNet [3] were confined to lab use only, as their resource re- quirements (GigaBytes of RAM) are way beyond the capacity of microcontroller-based hardware available on sensors in the field. However, recent advances in TinyML, a lively field of research targeting machine learning for microcontrollers, offer a glimpse of hope that pattern recognition of bioacoustic sig- nals might become possible, over longer periods, as required by the aforementioned use cases.\nPaper Contributions. In this paper, we explore the pos- sibility of using TinyML in practice, on common low-power microcontroller hardware, for a concrete use case: monitoring corn bunting birds' songs, in a rural area in the UK, over several months in a row, using a fleet of energy-efficient acoustic sensors. We aim to answer the following questions: (Q1) Can we pre-screen the audio on the low-power sensor node, so as to only store the targeted bird songs on each sensor? (Q2) How does that extend the device's lifetime, in terms of memory and energy budgets? More in detail, our contributions are as follows:\n\u2022 We propose a pipeline to record, recognize and store specific bird songs on low-power microcontroller-based devices, which can be further scale-up to universal species;\n\u2022 We developed a neural network with high accuracy on bird song recognition. Furthermore, we optimize the network with partial convolution technology to minimize the memory consumption on deployment;\n\u2022 We propose a two-stage classification approach to reduce computational and storage cost and enhance the overall accuracy;\n\u2022 We provide experimental results on both the classification performance of models and resource consumption on low-power devices;\n\u2022 We publish a new data set of curated audio recordings snippets of corn bunting bird songs;"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "Bird Song Recognition \u2013 Bird song detection has signifi- cantly advanced through various machine learning techniques, particularly deep learning methods. Recent research has ex- plored different strategies for recognizing bird songs, with sub- stantial progress in preprocessing audio data for recognition. One prevalent preprocessing technique involves transforming the audio signal into a spectrogram using Short-Time Fourier Transform (STFT) [4]. This spectrogram can subsequently be converted into a mel spectrogram [5], by applying a mel scale, followed by a log scale transformation [6]. This process effectively redefines the audio recognition task as an image recognition task. Convolutional Neural Networks (CNNs) [4], [7], [5] and Residual Neural Networks (ResNets) [6], [8] are widely utilized for such image recognition tasks in bird song detection. For instance, one study employed CNNs to recognize the songs of 17 bird species [7], while another study utilized ResNets to classify the songs of 19 bird species [6]. BirdNET [3], a well-known neural network for bird song recognition, is derived from the family of residual networks and can recognize 6,000 of the world's most common species at the time of writing.\nAnother approach involves transforming the audio signal into Mel-Frequency Cepstral Coefficients (MFCCs), which are then used as input for bird song classification [9], [10], [7]. Alternatively, feature extraction using wavelet decomposition can be employed for this purpose [11]. Additionally, certain studies combine different architectures; for example, CNN and Transformer [9], and CNN and VGG [5] to enhance performance. While these methods are effective for bird song recognition, their computational requirements are typically too demanding for execution on microcontrollers.\nBioacoustic Audio Datasets High-quality and diverse datasets are crucial for training models to accurately recognize birds and differentiate between species by their unique songs. The Xeno-canto database is one of the largest and most com- prehensive collections of bird sounds, containing over 500,000 recordings from more than 10,000 species. In [7], 2,699 bird song audio recordings from the Xeno-canto database, ranging in length from 20 to 0.74 seconds, were used to classify 17 bird species. Another extensive collection of bird audio recordings is available in the Macaulay Library \u00b3, renowned for its high-quality bird audio recordings and detailed metadata, which includes information about species, location, and context. The combination of the Macaulay Library and Xeno-canto audio recordings was used to train BirdNet [3]. In [5], the pub- lic dataset CLO-43DS, containing recordings of 43 different North American wood-warblers, was utilized. Another notable collection is the Birdsdata dataset with audio files of 20 bird species from the Beijing Academy of Artificial Intelligence (BAAI) repository, which is frequently used in research and model training [9], [8], [6]. To complement datasets focused on bird species, the Google AudioSet [3] and Urbansound8K [8] datasets are used to include non-bird species sounds.\nTiny Machine Learning (TinyML) \u2013 For models to operate on low-power devices, they must be compact and computationally efficient. Studies have demonstrated the use of lightweight CNNs for speech recognition and age classification [12], water leakage detection [13], acoustic defect localization [14], fall detection for the elderly [15] and other tasks [16], [17]. Tiny vision transformers have also been employed for classification tasks in various studies [18], [19], [20], [21], [22]. Moreover, quantization techniques can be applied to models to reduce their size to fit within the constraints of low-power devices [23], [24], [25].\nCreating spectrograms from audio signals can be power- intensive; hence, some studies use raw time-series data as input for neural networks [26], [24], [27], [28], [29]. For instance, single-channel EEG was used as input for a model based on CNN and Transformer for sleep stage classification [21]. Raw audio signals were used for urban sound analysis [28], [24]. Time-series data offer advantages for TinyML applications, including reduced computational load and suitability for low- power devices.\nEmbedded Software Platforms for TinyML \u2013 The widely used model transpiler TVM (Tensor Virtual Machine [30]) has recently been extended with uTVM, providing automated transpilation and compilation for models output by major ML frameworks (TFML, Pytorch, etc.). As such uTVM exposes low-level routines and optimizes these for execution on dif- ferent processing units, including for targets such as a large variety of microcontrollers. Prior works such as [31], [32] or MLPerfTiny [33] focused on the production, performance and analysis of standard benchmark suites of representative TinyML tasks on different microcontrollers. Conversely, prior work such as U-TOE [34] or RIOT-ML [35] provide embedded operating system integration of TinyML, facilitating TinyML benchmarking and continuous deployment over low-power wireless network links such as IEEE 802.15.4 or BLE."}, {"title": "III. BIOACOUSTIC MONITORING SCENARIO", "content": "As depicted in Figure 1, a network of battery-powered, autonomous recording units (ARUs, i.e. microcontroller-based acoustic sensors) is deployed across a monitored area. An example of ARU is given in [36].\nThese sensors remain in the field for an entire season, typi- cally around six months. Locations are often remote and hard to access and devices may be distributed over a relatively wide area, making visiting them time-consuming. Birds typically do not sing consistently during the whole day, while each bird has several different song types that need to be regularly sampled. Moreover, each individual bird moves around its individual territory, singing from a range of song posts, not all of which will be adequately recorded from any one location.\nOn the network aspect, the sensors are distributed approx- imately evenly, ensuring that each sensor can be wireless connected by more than two neighboring sensors. This ar- rangement facilitates approximate triangulation through dis- tance estimation based on signal strength. The placement process can be streamlined using LEDs and a basic ultra- low-power wireless protocol, such as IEEE 802.15.4 or LoRa. For space reason, we do not detail further networking aspects in the paper, but rather focus on the on-device machine learning aspects. We nevertheless keep an eye on the total memory footprint on the device, to ensure that the operating system including application TinyML code, the OS and a typical wireless low-power network stack fit in typical resource budgets on common microcontroller-based boards.\nAlthough our work is designed around one very spe- cific real-world research scenario (monitoring common Corn Bunting bird songs) the architectures should be readily ap- plicable to other audio data classification tasks. Examples of related scenarios to which our method could be extended in- clude, surveying remote areas for targeted endangered species of conservation value, or broader surveys of songbird biodi- versity. Similar devices would even be useful in the search for very elusive species such as Ivory-billed Woodpecker [37]."}, {"title": "A. Corn Bunting Monitoring Use-Case", "content": "SongBeam [36] microcontroller-based recorders have been used to monitor Corn Bunting (Emberiza calandra) birds in Oxfordshire, UK. A set of 30-40 devices have been deployed since 2022, currently comprising 3 complete breeding seasons (February - July).\nSongBeam devices are based on an ARM Cortex-M micro- controller, run on 4 D-cell batteries and record 4-channel WAV files onto 128 GB microSD cards. To maintain such a fleet of sensors, devices are currently checked approximately every 2 weeks, especially because memory space is soon exhausted on the microSD cards.\nThe above deployment has been used to produce a sig- nificant part of the dataset we present in Section V-A. Our experience using SongBeam and a preliminary analysis of the raw dataset we produced shows that less than 10% of recording time contains useful recordings. Given the extended deployment period (6 months), remaining low-power while improving storage efficiency is thus paramount. Moreover, current prototypes use solar power, further emphasizing that the bottleneck is downloading data from the microSD cards to free space on-board.\nThis description of the limitations of using SongBeam recorders to monitor Corn Buntings is likely to apply to many biomonitoring scenarios: solar power may allow for extending deployment lifetime in remote locations, but storage of recordings is a limiting factor."}, {"title": "IV. EVALUATION METRICS", "content": "On the one hand, as the basic part of bird song recording, the model should identify the target bird song as much as possible, potentially requiring a complex structure with a large size. On the other hand, the limited resource budget allows only simple models to be deployed on low-power Internet of Things (IoT) devices.\nThus we considered two orthogonal types of metrics to evaluate the prediction performance on bird song classification and resource usage on low-power IoT devices of TinyML models. Preliminary results are presented in Section X."}, {"title": "A. On-device Resource Usage", "content": "We performed extensive profiling of all models on low- power devices to capture their effective memory and time complexity in a real-world application scenario. The metrics are helpful to investigate the energy efficiency of the models, complementing the commonly used metrics for predictive performance.\nMemory (RAM) Consumption \u2013 This metric measures the amount of dynamic memory space (primary RAM) consumed by the model during inference. It reflects the memory footprint of the model activation and is important for low-power devices that have limited memory resources. Efficient memory utiliza- tion allows for the deployment of larger and more complex models on such devices.\nStorage (Flash memory) Consumption This metric quantifies the amount of storage space, typically in terms of Flash memory region, required to store the compute instruction and associated parameters. Minimizing storage consumption allows for accommodating multiple models on the device or orchestrating with other essential applications.\nComputational Latency \u2013 This metric measures the time consumption of performing inference for each input sample at the model level. It reflects the inference speed of the model on the low-power device and plays a crucial role in real-time or latency-sensitive applications. Core clock frequency, cache strategies and communication latency between memory and working core have a great impact on this indicator.\nEnergy Consumption \u2013 This metric is crucial for battery- operated and resource-constrained devices, where efficient energy usage can significantly impact device longevity and performance. Energy consumption encompasses both active power (when the device is triggered to perform tasks) and idle power (when the device is in sleep mode)."}, {"title": "B. On-device Prediction Performance", "content": "Accuracy - It is defined as the ratio of correctly predicted instances to the total instances in the dataset. While accuracy is intuitive, it can be misleading, especially in highly imbal- anced datasets. To address this limitation, accuracy should be combined with other metrics that offer a more detailed view of model performance.\nPrecision and Recall \u2013 They are two fundamental met- rics of classification models used to evaluate the ability to distinguish true positives (TP) from false positives (FP) and false negatives (FN), particularly in the context of imbalanced datasets.\nF-Score \u2013 There is often a trade-off between precision and recall; increasing one can lead to a decrease in the other. As two common instances, $F_1$-score weighs them evenly, while $F_2$-score treats recall as two times more important than precision, applying in scenarios where false positives are more tolerant than false negatives.\nReceiver Operating Characteristic (ROC) Curve \u2013 While the above metrics are sensitive to class imbalance which occurs in many species classification and detection tasks there are other metrics that are more robust towards class imbalance. The receiver-operator characteristic (ROC) curve depicts the True Positive Rate (TPR) and False Positive Rate (FPR) across different decision thresholds. The Area Under the Curve (AUC) is a key metric derived from the ROC curve that quantifies the overall performance of the model and is invariant with respect to class imbalance. An AUC of 1.0 indicates perfect classification, while an AUC of 0.5 suggests no discriminative ability."}, {"title": "V. METHODOLOGY", "content": "We used a combination of project-specific and publicly accessible data to train and validate our classification mod- els. In the data pre-processing phase, the raw audio signals were further segmented, labeled, down-sampled and divided into different groups to generate classification datasets. The corresponding Mel-spectrograms of the pre-processed audio segments were also created for spectral-based methods (mod- els). During the pre-processing stage, we conducted a pilot analysis to establish guidelines and determine specific hyper- parameters for downsampling and spectrogram generation."}, {"title": "A. Data Acquisition", "content": "The publicly accessible data originated from Xeno-Canto, Macaulay, Google AudioSet, while the project-specific data were previously collected in a long-term research of bird song patterns. We published the resulting dataset\u2074.\n\u2022 Oxfordshire Corn Buntings. This library contains record- ings of corn buntings along a transect of approximately 20km in Southern England. Corn buntings sing in a mosaic-like pattern of geographical variation called di- alects; our sample contains approximately 6 different dialects. The recordings were performed with directional parabolic microphones as described in [36].\n\u2022 Macaulay Library. This library contains the necessary audio recordings for our target species as well as the other identified species. There are a total of 796 entries of audio recordings for the target species. We filtered the audio files by the song tag and removed 16 entries due to unclear metadata and missing catalog numbers. For other species as non-target, we limit the selection to a maximum of 30 recordings per species and choose those with the highest average community rating, acquiring a total of 1468 recordings.\n\u2022 Xeno-Canto. This library also contains recordings of tar- get species and other species. We retrieved the recordings only marked with song and rated with the best quality level, resulting in 303 recordings of target species and 9622 recordings of other species as non-target.\n\u2022 Google AudioSet. This dataset provides environmental sounds which contain non-bird sounds.\nAfter gathering all audio recordings from the above li- braries, we used BirdNET to identify and chop corn-bunting segments from all datasets, and labeled them as target bird songs; all segments were truncated or zero-padded to the length of 3 seconds. Meanwhile, we chose other 3-s segments randomly and ensured no overlap with corn-bunting segments, with labeled as non-target bird songs. Table I shows the distribution of target and non-target segments over all datasets."}, {"title": "B. Data Pre-processing", "content": "This phase contains the following steps:\n1) We divided the segments into training, validation and test sets with the ratio of 80: 10:10.\n2) All segments were downsampled to 16 kHz using zero- order holder, which constituted the audio dataset.\n3) We transformed the audio segments into Mel- Spectrograms, and grouped them with the same splitting ratio to form the spectral dataset.\nPilot Analysis We averaged the STFT spectrograms of target and non-target segments in the training dataset to investigate their frequency characteristics, as depicted in Figure 2. Obviously, a bright band lays on the spectrogram of target segments roughly between 4000 and 8000 Hz, hints that a sample rate with 16 kHz should be sufficient to preserve all frequency components of corn-bunting song according to Nyquist-Shannon theorem. A highpass filter can also be applied to eliminate low-frequency noise without damaging target songs. Thus, we downsampled the segments from 48 kHz to 16 kHz to reduce the resource consumption and improve the efficiency of classification phase. In the baseline model we also designed a highpass filter to enhance Signal- to-noise ratio (SNR).\nMel-Spectrogram - Mel-spectrogram is commonly used in audio classification tasks due to its ability to closely approximate human auditory perception. To create log Mel- spectrograms, we first generated magnitude spectrograms of STFT for all downsampled audio segments. The STFT was performed with a Hann window with a width of 1024 samples and a step size of 256 samples. This process generated 184 window frames and 513 frequency bins. Thereafter, the mag- nitude spectrograms were mapped onto the Mel scales with 80 Mel bins. In creating the Mel-spectrograms, we employed a sampling rate of 16 kHz and focused on the frequency range of 80-8,000 Hz. This frequency range was chosen because it encompasses the typical frequency range of most bird songs, including that of the target Corn Bunting. Lastly, we converted the Mel-spectrograms into a logarithmic scale to obtain log Mel-spectrograms, with the shape of 184 \u00d7 80."}, {"title": "VI. BASELINE & DECISION STRATEGY", "content": "We propose an approach to optimizing the screening process by combining a high-speed signal processing step (hereafter named \"baseline\") with the precision of an TinyML model, en- suring reliable results while maintaining resource-consumption efficiency."}, {"title": "A. Baseline", "content": "We developed a lightweight pre-selector signal processing step, not relying on machine learning. First, the audio segments are normalized by min-max schema. Thereafter, a 9th-order highpass Butterworth filter with the cut-off frequency of 7000 Hz is imposed on the normalized segments, in order to suppress noise and non-relative sound in low-frequency and to preserve only target components. Finally, we calculate the signal power P of the filtered segments,\n$P = \\frac{1}{N} \\sum_{n=1}^{N} x(n)^2, $   (1)\nwhere $x(n)$ and N denote the data points and the length of a segment, respectively. We define two thresholds: $t_{low}$ and $t_{high}$. If P is smaller than $t_{low}$, the microcontroller remains in the idle/low-power state, and the audio sample is discarded directly. Else, P is compared to $t_{high}$. Depending on this result, and on the decision strategy (see below), the audio sample is either stored or further analyzed via inference using the machine learning model."}, {"title": "B. TinyChirp Decision Strategy", "content": "We employ a double-phase approach for enhanced accuracy and efficiency of bird song recognition, combining the baseline with a TinyML model for further verification. The process is as follows:\n1) Step 1: Baseline Filtering \u2013 Pre-screening is conducted using the baseline pre-processing step described in Sec- tion VI-A, designed to provide a quick and efficient preliminary analysis. If P is smaller than $t_{low}$, the microcontroller remains in the idle/low-power state, the audio sample is discarded and the process aborts. Else proceed to Step 2.\n2) Step 2: Inference-based Classification \u2013 If the power- saving flag is set and if P is smaller than $t_{high}$ a higher- accuracy classification is conducted via inference using TinyML model. If the audio sample is not classified as the target, the audio sample is discarded and the process aborts. Else proceed to Step 3.\n3) Step 3: Storage \u2013 Without further processing, the audio sample is logged, i.e. stored on the SD card.\nNote that the decision strategy can therefore be configured with two \"knobs\". On one hand the power-saving flag de- termines skipping (or not) some of the computation and can thus decrease energy consumption. On the other hand, different TinyML models can be inserted in Step 2.\nNext, we thus study two families of TinyML models for Step 2: the first category of model takes a Mel-Spectrogram as input, while the second category of model takes time-series as input. Table II presents a summary of the explored TinyML model structures."}, {"title": "VII. CLASSIFICATION BASED ON SPECTROGRAMS", "content": "In contrast to the computer vision community, where the transition from frequency decomposition based feature extrac- tion to neural feature extraction on raw image data introduced in 2012 led to a significant increase in predictive performance [38], neural feature extraction on raw audio time series did not contribute to a comparable breakthrough yet. Instead most ML techniques for audio pattern recognition (somewhat surprisingly) were relying on image processing pipelines sometimes with neural networks trained on ImageNet data. Basically, image classification is performed, using a neural network, analyzing the spectrogram rendering of the audio trace.\nFor this reason, we initially consider the below two models, based on state-of-the-art neural network architectures.\nCNN-Mel \u2013 This model contains two 2D convolutional layers for Mel-spectral inputs as feature extraction and two fully connected layers as classifier. Both convolutional layers utilize 4 filters with a 3 \u00d7 3 kernel size and ReLU activation function, followed by 2 \u00d7 2 max pooling to reduce spatial dimensions. The output of the classifier are normalized by Softmax function as well.\nSqueezeNet-Mel \u2013 This model is based on SqueezeNet [39], an advanced backbone focused on optimizing the efficiency of computer vision applications by strategically reducing parameters, with comparable performance to AlexNet [38]. It leverages a so-called Fire module to achieve higher efficiency compared to standard convolutions with only a slight decrease in accuracy. The Fire module significantly reduces the number of parameters by the squeeze layer (1 \u00d7 1 convolutions), which exceedingly reduces the number of channels before the more parameter-heavy 3 \u00d7 3 convolutions in the expand layer. This reduction in intermediate channels means fewer parameters overall. We aligned its input shape with the Mel-spectrogram and tailored the output for binary classification. Also offering potential for further compression using quantization techniques, SqueezeNet is thus appealing given memory constraints.\nLimitations of Spectrograms on Microcontrollers Prac- tical experience on low-power microcontroller-based devices has shown however that producing and manipulating mel- spectrograms from audio signal streams on such devices is problematic. It inserts an additional step in the processing pipeline, which increases latency and memory requirements. Prior work (such as [40], section 6.2) concludes that the CPU bottleneck is not just the model inference time, but also the spectrogram calculation, which, compared to inference alone, almost doubles latency. Other previous work such as [41] measures on a quite powerful STM32F7 microcontroller that computing and writing in memory the mel-spectrogram of 30 columns and 40 frequency bands takes approximately 1 second. Note that these spectrogram dimensions are much smaller than our requirements (184 columns \u00d7 80 frequency bands) hence even more latency can be expected in our case. For these reasons, we next explore pipelines which skip the spectral pre-processing stage as described below."}, {"title": "VIII. CLASSIFICATION BASED ON TIME-SERIES", "content": "Contrary to the models described in the previous section and inspired by the success of neural feature extraction in the computer vision domain [38], the pipelines we aim at next take directly the raw audio signal time-series as input. More precisely, we designed the three models described below.\nCNN-Time \u2013 This simple model performs feature extrac- tion with two sequential (1D) temporal convolutional layers followed by average pooling. A max pooling is inserted in-between to reduce the dimension of feature maps and enhance the non-linearity of the network. Two fully connected layers act as classifier at the end of the network, with the probability outputs normalized by Softmax function.\nTransformer-Time Inspired by [42], we designed an efficient, attention-based model with only one temporal con- volutional layer and one single-head transformer. The con- volutional and pooling layers serve as feature extraction to transfer raw audio signal into embeddings for the transformer. The activations of the fully connected layer are normalized by Softmax function as well.\nSqueezeNet-Time This model is based on SqueezeNet [39]. We tailored this basis to align the input shape with the audio segment, and narrowed the output channels for binary classification. To adapt the backbone to time-series input, all 2D convolutional layers are replaced by temporal convolutional layers. Furthermore, to have a more compact structure, we decreased the filter number of all convolutional layers by roughly 70%, as presented in Table III."}, {"title": "IX. ADDITIONAL MODEL OPTIMIZATIONS FOR TINYML", "content": "In this study, we used two optimization techniques to compress the models and further reduce their memory con- sumption: model quantization on the one hand and on the other hand partial convolution, as described below."}, {"title": "A. Model Quantization", "content": "Quantization reduces the model size and inference time by converting the weights and activations from higher precision (e.g., 32-bit floating-point) to lower precision (e.g., 8-bit integer) [43]. In this study we adopted Post-Training Quan- tization (PTQ) with weights and activations quantized in 8-bit integer. To avoid substantial loss of prediction performance, the training dataset was used to find the optimal scale factor and zero-point of the activations."}, {"title": "B. Partial Convolution", "content": "We observed that the outputs of the Conv1D layer require significant memory, which impedes deployment on resource- constrained tiny devices. As presented in Table II, the peak memory consumption occurs at the first Conv1D layer both in CNN-Time and Transformer-Time with 768 kB and 3 MB, respectively. This indicates that deployment on resource-constrained tiny devices is impractical.\nTo address this issue and inspired by [44], we exploited the fact that average pooling can be computed iteratively point-by- point on the output channels of the final Conv1D layer over a small sliding window of inputs. Each point in the output channels depends only on a small subset (kernel window) of the outputs from the previous layer. That is, for a block of L Conv1D layers following by an average pooling layer, its output y can be iteratively computed on the input sequence $x(\u03b7), \u03b7 = 1... N$ as following:\n$Y_j (k) = Y_j (k-1)+\\frac{1}{N_l}A^l_j(k),$\n$k = 1... N_l, j = 1...C_l.$\n$A_j^l(n) = \\sum_{i=1}^{C_{l-1}} W^{l-1}_{c_i j} [A^{l-1}(n -\\frac{K_l}{2})... A^{l-1}(n +\\frac{K_l}{2})],$\n$A^0(n) = x(n),$\n$l = 1... L, c = 1 . . . C_l, n = 1 ... N_l, N_0 = N$\nwhere $y_j (N_L)$ is the result of the average pooling of the j- th channel; $A(n)$ denotes the n-th point in channel c of the l-th Conv1D layer calculated with kernel weights W. Each Conv1D layer contains $C_l$ channels with filter size of $K_l$ and output size of $N_l$. Without loss of generality, the formula of partial convolution contains only the linear components and considers only one-dimension case for the sake of simplicity; it can be generalized in high-dimension and integrated with non-linear building blocks (e.g., stride, non-linear activations, pooing layers, etc.).\nFigure 3 provides a comparison between classic convolution and partial convolution. Unlike classic convolution where entire channels (C\u00d7 N) are computed and stored before being processed by the next layer, partial convolution requires only a small part of the channels (C \u00d7 K, K << N) for each layer. In our case with K = 3 and N = 48000, it can be expected roughly 16000\u00d7 smaller memory consumption, making the implementation more suitable for tiny devices. We applied this strategy to the Conv1D layers combined with average pooling in the CNN-Time and Transformer-Time architectures."}, {"title": "X. TINYCHIRP PERFORMANCE EVALUATION", "content": "In the following, we conducted a comprehensive evaluation of the discriminative performance of TinyChirp with different TinyML models, as well as of resource consumption and computation time on typical low-power boards based on microcontrollers."}, {"title": "A. Classification Performance Comparison", "content": "Our evaluation process began with the generation of ROC curves and the corresponding AUCs on the training dataset, providing a clear picture of the models' abilities to distinguish between classes. As shown in Figure 4, the spectrogram- based models CNN-Mel and SqueezeNet-Mel achieved the best classification performance with AUCs of 1.0 and 0.99, respectively, followed by time-series models \u2013 CNN-Time and Transformer-Time \u2013 both with 0.98 AUC. Unexpectedly, with a more complex structure, SqueezeNet-Time performs worst (lower AUC than the baseline).\nWe next measured accuracy, precision, recall, $F_1$- and $F_2$-score, shown in Figure 5. Spectrogram-based models achieve the highest metrics for the whole range of threshold values. For time-series models, Transformer-Time worked overall better than CNN-Time; Again, SqueezeNet-Time performs worst (consistent with the above ROC analysis).\nNext, we focused on finding the threshold settings for optimal $F_2$-score prioritizing recall, i.e. reducing the likelihood of mistakenly discarding records of the targeted birds). The results are shown in Table IV for training data, thereafter verified on test (previously unseen) data as shown in Table V. Note that this threshold influences energy consumption: more target bird classification leads to more energy consumption (as data needs to we stored on to SD card in this case).\nThese results are promising and allowing to focus only on the relevant audio segments (10% of the total recording time), as the classification performance is compelling. Thus, TinyChirp can save 90% of SD card space compared to the ini- tial monitoring scenario (Section III-A), potentially extending the deployment time to 18 weeks. However, complementary experiments must now be carried out on targeted hardware, so as to evaluate other key metrics: computation time, energy consumption and memory footprint on typical microcontroller- based devices. The next sections focus on that part."}, {"title": "B. Performance Evaluation on Microcontrollers", "content": "Experimental Setup For our measurements", "board": "the nRF52840 Devel- opment kit (nrf52840dk). This board is based on an ARM Cortex-M4 processor", "budget": 1, "35": ".", "45": "as well as external measurement equipment as described below.\nEnergy consumption was measured using an ampermeter to gauge the board's energy efficiency. This involved recording the current draw of the MCU and the external storage system (SD card) in different stages. We used a voltage regulator to supply 3.3 V Direct Current (DC) output and a logger to capture detailed current profiles over time.\nScope of the Measurements Besides the model in- ference stage, we also considered the resource footprint of pre-processing during bird song recognition. Pre-processing refers to down-sampling and calculation of Mel-spectrogram as described in Section V-B. Also, the energy consumption in MCU's lowest power mode (idle stage) was measured as the reference of absence of sound with sufficient intensity.\nMeasurement Results on Microcontrollers The first striking observation is that SqueezeNet-Time and SqueezeNet- Mel were not deployable due to"}]}