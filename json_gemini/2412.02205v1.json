{"title": "DataLab: A Unifed Platform for LLM-Powered Business Intelligence", "authors": ["Luoxuan Weng", "Yinghao Tang", "Yingchaojie Feng", "Zhuo Chang", "Peng Chen", "Ruiqin Chen", "Haozhe Feng", "Chen Hou", "Danqing Huang", "Yang Li", "Huaming Rao", "Haonan Wang", "Canshi Wei", "Xiaofeng Yang", "Yuhui Zhang", "Yifeng Zheng", "Xiuqi Huang", "Minfeng Zhu", "Yuxin Ma", "Bin Cui", "Wei Chen"], "abstract": "Business intelligence (BI) transforms large volumes of data within modern organizations into actionable insights for informed decision-making. Recently, large language model (LLM)-based agents have streamlined the BI workflow by automatically performing task planning, reasoning, and actions in executable environments based on natural language (NL) queries. However, existing approaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS. The fragmentation of tasks across different data roles and tools lead to inefficiencies and potential errors due to the iterative and collaborative nature of BI. In this paper, we introduce DataLab, a unified BI platform that integrates a one-stop LLM-based agent framework with an augmented computational notebook interface. DataLab supports a wide range of BI tasks for different data roles by seamlessly combining LLM assistance with user customization within a single environment. To achieve this unification, we design a domain knowledge incorporation module tailored for enterprise-specific BI tasks, an inter-agent communication mechanism to facilitate information sharing across the BI workflow, and a cell-based context management strategy to enhance context utilization efficiency in BI notebooks. Extensive experiments demonstrate that DataLab achieves state-of-the-art performance on various BI tasks across popular research benchmarks. Moreover, DataLab maintains high effectiveness and efficiency on real-world datasets from Tencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction in token cost on enterprise-specific BI tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Business intelligence (BI) aims to transform large volumes of data into actionable insights for informed decision-making [1]. A typical BI workflow includes multiple stages such as data preparation, analysis, and visualization. It requires the collaboration of data engineers, scientists, and analysts using various specialized tools (e.g., Visual Studio Code, Power BI, Tableau), which can be highly tedious and time-consuming [2]. Therefore, modern organizations require advanced techniques to automate and optimize this workflow.\nRecent advancements in autonomous agents powered by large language models (LLMs) [3] offer the potential to streamline the BI workflow (Figure 1). By receiving instructions in natural language (NL), LLM-based agents can perform task planning, reasoning, and actions in executable environments. This can significantly reduce the complexity of many BI tasks, such as code generation [4], text-to-visualization translation [5], and automated insight discovery [6].\nHowever, previous works on LLM-based agents for BI primarily focus on individual tasks or stages without considering the BI workflow as a whole. The separation of BI tasks across different data roles and tools impedes seamless information flow and insight exchange, adding to communication costs, delays, and errors [7], [8]. For example, data analysts using GUI-based platforms (e.g., Power BI) often rely on data engineers working with development tools (e.g., PyCharm) to prepare data for analysis or visualization. This reliance necessitates back-and-forth communication between analysts and engineers due to the iterative and collaborative nature of BI [1]. Such procedures highlight the limitations of existing fragmented and fixed agent pipelines [9]. Consequently, this leads to a significant gap among different roles, tasks, and tools, which hinders timely and informed decision-making.\nTo bridge this gap, we aim to unify the BI workflow with a one-stop LLM-based agent framework in a single environment that satisfies the requirements of various data roles. However, achieving this unification in practical enterprise settings is non-trivial due to the following challenges:\nC1: Lack of domain knowledge incorporation. Existing studies usually leverage clean and synthesized research benchmarks to build and evaluate agents [10]. However, BI tasks typically involve large and dirty real-world datasets with many ambiguities [11]. For example, column names in business data tables may have unclear semantic meanings [12], and user queries often include enterprise-specific jargon [10]. To miti-"}, {"title": "C2: Insufficient information sharing across tasks.", "content": "Different tasks are typically managed by corresponding LLM-based agents to achieve optimal performance [15]. As a complex BI query may encompass multiple tasks, information sharing among the involved agents is critical. For example, the data retrieved by a SQL writing agent must be accurately conveyed to a chart generation agent. Therefore, an effective and efficient inter-agent communication mechanism is essential to align their understanding of the overall analysis goals, current data context, and executed actions. However, many existing multi-agent frameworks, such as ChatDev [16] and CAMEL [17], use unstructured natural language for communication. This can lead to distortions [18] and is inadequate for handling the complexity of BI tasks, which commonly involve diverse information types (e.g., data, charts, texts)."}, {"title": "C3: Demand for adaptive LLM context management.", "content": "LLM-based agents depend on their context windows (i.e., limited input tokens for NL understanding, reasoning, and generation) to complete tasks. Necessary contexts must be provided to ensure a successful and seamless workflow. Meanwhile, in a unified BI platform, vast amounts of multi-modal contexts (e.g., code snippets and their execution results, charts and their specifications) are intertwined and often relate to diverse data tables. Obviously, only relevant portions of these contexts are pertinent to specific tasks and should be selectively provided to the agents [19]. Therefore, adaptive context management tailored to prior states and current user needs is crucial for maintaining system efficiency and cost-effectiveness.\nIn this paper, we introduce DataLab, a unified environment that supports various data tasks throughout the BI workflow, thereby serving different data roles whether they use Markdown, SQL, Python, or no-code, all within a single computational notebook. We use notebooks as the foundational system due to their popularity in data science [7] and their iterative nature for the BI workflow [1]. DataLab adopts an LLM-based agent framework to integrate LLM assistance seamlessly, and a notebook interface to enable user customization flexibly.\nTo improve agents' performance on enterprise-specific BI tasks (for C1), we develop a Domain Knowledge Incorporation module, a systematic approach for automated knowledge generation, organization, and utilization. It leverages data processing scripts (e.g., Python code, SQL queries) within the entire enterprise to extract the knowledge associated with databases/tables/columns, thereby uncovering their common usage patterns. To facilitate information sharing across different tasks (for C2), we design an Inter-Agent Communication module, a structured mechanism that goes beyond pure NL to enhance the information representation capabilities of agents."}, {"title": "II. BACKGROUND", "content": "In this section, we briefly introduce the core stages and roles in a modern BI workflow. We then provide an overview of LLM-based agents, focusing on their applications in BI tasks."}, {"title": "A. BI Workflow", "content": "The BI workflow encompasses several critical stages, namely data collection, storage, preparation, analysis, and visualization. Data Collection [20] refers to the initial step of gathering raw data from various sources like databases and spreadsheets. Once gathered, Data Storage [21] typically uses data warehouses or data lakes to organize the collected data for efficient retrieval. This may also involve combining data into a unified format (i.e., data integration). Subsequently, Data Preparation [22] ensures data consistency, correctness, and quality. This usually includes cleaning, structuring, and enriching raw data into a format suitable for further analysis. Following preparation, Data Analysis [23] applies statistical and analytical techniques to extract insights, aiming to uncover patterns, trends, and correlations in the data. Finally, Data Visualization [5] presents analyzed data in visual formats like charts, graphs, and dashboards, which makes complex data easier to understand and interpret for decision-makers.\nThe data roles involved in the BI workflow are specific to different organizations. Among them, data engineers, scientists, and analysts are usually indispensable. Data Engineers are tasked with data collection, storage, and preparation, constructing and administering data pipelines to ensure that data is accurately cleansed and structured for subsequent analysis. They typically use SQL and Python for data processing, and use cloud computing platforms like AWS for data storage and ingestion. Data Scientists engage in data preparation and analysis, applying advanced statistical and machine learning methodologies to extract insights and forecast trends from intricate datasets. They are familiar with Python/R and popular data science libraries like Pandas. Data Analysts concentrate on data analysis and visualization, analyzing data to discern patterns and conveying findings through detailed visual reports and dashboards. They use SQL to query data, and rely on BI platforms like Tableau to perform and share their analyses.\nIn modern enterprises, a complex BI workflow requires the collaboration of multiple data roles across various stages. The current fragmentation of tools for data preparation, analysis, and visualization introduces frictions and delays in timely decision-making. Therefore, an integrated and unified platform can serve as a shared environment for distinct user groups, facilitating the efficiency, transparency, and productivity of BI."}, {"title": "B. LLM-based Agents for BI", "content": "LLM-based agents are autonomous systems powered by LLMs that can perceive environments, execute tasks, make decisions, and interact with users in complex contexts [3]. These agents comprise profiling, memory, planning, and action modules, which respectively define the agent's role, facilitate operations in dynamic environments through recall and future action planning, and convert decisions into outputs [24]. In BI scenarios, agents receive users' NL queries and then perform data preparation, analysis, and visualization. By interpreting execution results, they can iteratively complete many BI tasks such as data transformation [25] and insight generation [6]. Moreover, equipped with vision language models (VLMs) [26], agents can even generate GUI operations for enterprise applications (e.g., BigQuery, Airflow) [2], which further augments their capabilities for more complex BI workflows. Below, we list some typical tasks that can be streamlined by LLM-based agents at each BI stage:\nData Collection: Table Generation [27], Table Augmentation [28], Table Summarization [29].\nData Storage: Data Warehousing [2], Data Integration [30], Data Orchestration [2].\nData Preparation: NL2SQL [31], NL2DSCode [4].\nData Analysis: NL2Insight [32], Table Q&A [33].\nData Visualization: NL2VIS [5], Chart Q&A [34].\nHowever, most existing LLM-based agents are limited to individual tasks and do not meet the diverse user requirements of a complex BI workflow. Moreover, they often neglect the integration of enterprise-specific knowledge, resulting in unsatisfactory performance on proprietary business datasets. This lack of generalizability and customizability highlights the need for a structured and adaptive agent framework for BI."}, {"title": "III. OVERVIEW", "content": "Architecture Overview. As illustrated in Figure 2, DataLab consists of two primary components: (1) LLM-based Agent Framework and (2) Computational Notebook Interface.\nLLM-based Agent Framework. In DataLab, multiple agents are designed for different BI tasks based on user requirements. To achieve this, we first identify several common BI procedures and abstract them into data tools that can be called upon by agents during inference. Example tools include a Python sandbox for code execution and a Vega-Lite environment for visualization rendering. Accompanied by other auxiliary components like memory modules, each BI agent is represented as a DAG for high flexibility and easy extensibility. Within the DAG, nodes depict reusable components (e.g., LLM APIs, tools) and edges depict their connections (e.g., file transfer across tools). Figure 3 illustrates an example agent workflow for NL2VIS. Additionally, we add a proxy agent to the framework, which serves as a hub to directly interact with users and allocate tasks to each specialized agent based on user queries. These agents collaborate with each other to complete a wide array of tasks for various data roles throughout the BI workflow.\nComputational Notebook Interface. DataLab's notebook interface (Figure 4) serves as a unified, interactive, and collaborative environment for different data roles to complete their specialized tasks. To achieve this, we augment JupyterLab (a widely used notebook interface) to support (1) multi-language cells and (2) on-the-fly LLM assistance. First, DataLab wrangles SQL, Python/PySpark, Markdown, and Chart cells altogether, allowing both technical and non-technical users to easily adopt their familiar workflows on the notebook. Going beyond traditional notebooks that only support Python and Markdown, DataLab notebooks directly connect to backend databases for SQL query execution, and feature GUI-based dashboards [35] similar to Tableau for visualization authoring. Second, we integrate our LLM-based agent framework seamlessly into each notebook cell. Users can get LLM assistance both at notebook- and cell-level. Specifically, users toggle an input box and type their analytic queries, which are then processed by the agents in our framework. These agents can create new cells or modify existing ones in the notebook. Users can subsequently examine the results and make further customizations flexibly.\nDataLab Workflow. Upon receiving an NL query and the associated dataset, DataLab initially analyzes the dataset and interprets the query, incorporating domain knowledge before feeding them into LLMs. Then, DataLab leverages various agents to complete the task, which may involve information sharing with each other through a structured communication mechanism. Subsequently, the corresponding result will be presented in the notebook. Users can either accept, edit, or reject the result and continues the BI workflow. Meanwhile, a context management strategy automatically generates and maintains cell dependencies within the notebook to promote further agent calls. Next, we provide an overview of the three critical modules in DataLab.\nDomain Knowledge Incorporation. This module takes a data table's schema, its associated script history (e.g., SQL queries, Python code), and its data lineage information as input. Specifically, the schema provides a basic overview of the table and its columns, including their names and types. The associated data processing scripts, which are created by professionals and executed every day within the organization, reflect the semantic meanings and common usage patterns of the table and its columns. And the data lineage information [36], which reveals interrelationships among distinct tables and columns across the organization, can serve as an auxiliary resource for domain knowledge extraction. Based on the input, the module leverages LLMs to automatically generate the knowledge components (e.g., descriptions, usages) of databases, tables, columns, and certain values. These knowledge components are then organized in a knowledge graph to facilitate further retrieval and utilization, which translate ambiguous user queries into structured domain-specific languages (DSLs) for improved agent performance on enterprise-specific BI tasks.\nInter-Agent Communication. This module formulates the information flow process among different agents as an FSM to enable more control over their communications, with nodes representing agents and edges representing inter-agent information transition directions. Upon task completion, each agent's outputs are formatted into structured information units [18], comprising key characteristics such as the associated table's identifier and a concise description of the agent's executed actions. The module also maintains a shared information buffer for all agents to proactively"}, {"title": "Domain Knowledge Generation", "content": "Ambiguities are pervasive in real-world BI scenarios, manifesting both in the underlying databases and users' NL queries. For example, consider the query, 'show me the income of TencentBI this year', which involves three columns: 'prod_class4_name', 'shouldincome_after', and 'ftime'. The semantic relationships between these column names and the user's request are often vague, leading to LLMs' suboptimal performance on such tasks. To mitigate these issues, existing approaches integrate table schema [5] into prompts and adopt retrieval-augmented generation (RAG) [10] to improve LLMs' domain-specific capabilities. We categorize three types of domain knowledge commonly utilized for BI tasks:\nMetadata: Information about data structure and attributes, such as table and column names, types, descriptions, and common usage patterns.\nBusiness Logic: Rules and processes that dictate how data is used and interpreted within the business.\nJargon: Specialized terminologies and acronyms specific to the industry or organization.\nTraditionally, such knowledge has been manually constructed by domain experts, which is tedious and time-consuming. Through an extensive examination conducted at Tencent, it was observed that, while 85% of the tables lack comprehensive metadata, they are predominantly linked to various SQL or Python scripts utilized for data processing. These scripts reveal common usage patterns within practical business contexts. Additionally, for those tables lacking adequate processing scripts, data lineage information, which elucidates their connections to other tables or columns throughout the organization, provides an alternative resource for metadata imputation. Therefore, inspired by LLMs' exceptional code understanding and reasoning abilities, we propose an LLM-based knowledge generation approach (Algorithm 1) that leverages script history to abstract and summarize knowledge components through meticulously-designed prompting techniques. This automated approach comprises a Map-Reduce process with a self-calibration mechanism [37] to generate high-quality knowledge for databases, tables, and columns."}, {"title": "Algorithm 1 LLM-based Knowledge Generation", "content": "Input: Schema S, Script History H,\nLineage Information L, Score Threshold T\nOutput: Database/Table/Column Knowledge D, T, C\n1: // Filter out duplicated or similar scripts\n2: H \u2190 preprocess(H)\n3: Map Phase:\n4: map_res\u2190 []\n5: for each historical script $h_i \\in H$ do\n6:  while $s_i < T$ do\n7:   // Generate knowledge individually\n8:   $d_i, t_i, C_i$\u2190 LLM($h_i$, S, L)\n9:   // Self-calibration\n10:   $s_i$ \u2190 LLM($d_i, t_i, C_i$)\n11:  end while\n12:  map_res.append([$d_i, t_i, C_i$])\n13: end for\n14: Reduce Phase:\n15: // Synthesize knowledge collectively\n16: D,T,C \u2190 LLM(map_res, S, L)\n17: return D,T,C"}, {"title": "Knowledge Components", "content": "Considering the previously defined knowledge categories, metadata and business logic can be deduced from data processing scripts, as both SQL queries and Python code support data manipulation operations like filtering and aggregation. Business logic is essential for computing derived columns which, though absent in the original table, hold significant value in business contexts. In contrast, jargon primarily exists in user queries or organization wikis (i.e., documents), necessitating enterprise-specific glossaries for management and application. The knowledge components that our automated approach can generate are outlined below:\nDatabase Level: description, usage, tags.\nTable Level: description, usage, organization, key column names, key derived attribute names, tags.\nColumn Level: description, usage, type, tags, derived column information (name, description, usage, calculation logic, related columns, tags).\nThese knowledge components are structured using JSON formats to improve LLMs' generation performance."}, {"title": "Map Phase", "content": "Given a data table, we take its schema S, its script history H, and its lineage information L as input. During the map phase, each distinct historical script $h_i$ is individually processed using an LLM as the mapping model to produce corresponding knowledge components. The LLM is prompted to carefully analyze the script's semantic content and logical structure, aiming to extract critical information relevant to the specific business context. To mitigate LLMs' hallucination issues, focus is restricted to the involved databases, tables, and columns. This process results in the generation of a set of knowledge components associated with the script $h_i$."}, {"title": "Self-Calibration", "content": "Within each iteration of the map phase, we integrate a self-calibration mechanism that leverages LLMs' self-reflection abilities [38] to evaluate the intermediate results using a numerical score ranging from 1 to 5. Specifically, we instruct the LLM to consider multiple aspects of the knowledge components (e.g., correctness, comprehensiveness, clarity) and provide several manually crafted in-context examples to demonstrate the scoring criteria. Should the rating score $s_i$ fall below the predefined threshold T, the knowledge generation process must be repeated. Therefore, this feedback loop ensures the generation quality of each iteration."}, {"title": "Reduce Phase", "content": "During the reduce phase, we aim to synthesize the individual results derived from each historical script to produce the final sets of knowledge components D, T, and C for the involved database, table, and columns, respectively. The LLM is instructed to meticulously scrutinize, aggregate, and summarize the information from all separate results to ensure a consistent and conflict-free collective result.\nFor each data table at Tencent, we execute the above Map-Reduce process to generate a comprehensive and high-quality set of knowledge components, which can significantly benefit many downstream BI tasks."}, {"title": "B. Knowledge Organization", "content": "We employ a knowledge graph G = (V, E) to systematically organize the knowledge generated by our automated approach (i.e., metadata and business logic) and the manually crafted enterprise-specific glossaries (i.e., jargon).\nAs depicted in Figure 5, the knowledge graph adopts a tree-based structure for knowledge organization. The nodes {V} are structured into five primary types: database, table, column, value, and jargon, each comprising various components (e.g., description, usage) and uniquely identified by a name. To address the common challenge of terminological inconsistencies in user queries (e.g., synonyms, acronyms), an additional node type, alias, has been introduced. This node type contains alternative terms associated with the official name of other node types and can be dynamically updated in real-world applications. The relationships between these nodes are represented by edges {E}, which delineate both logical relationships among the primary node types and associative relationships between alias nodes and other primary nodes.\nTo facilitate efficient knowledge retrieval, we develop a task-aware indexing mechanism for graph nodes, utilizing Elasticsearch [39] for full-text search and StarRocks [40] for embedding search. This supports both lexical and semantic matching of knowledge nodes in response to user queries. The indexing structure is designed as triplets ({name, content, tag}), where the content field is a concatenation of knowledge components specified based on the various requirements of downstream tasks. For instance, some tasks necessitate the calculation logics while others only need descriptions for successful completion. By dynamically selecting the appropriate index, we ensure that knowledge retrieval is both efficient and effective."}, {"title": "C. Knowledge Utilization", "content": "As shown in Figure 20, given a user query Q, we initially rewrite it to enhance clarity and detail. We then retrieve its relevant knowledge from the knowledge graph G. Following this, the query is translated into a DSL specification, facilitating downstream tasks like NL2SQL and NL2VIS.\nQuery Rewrite. In addition to ambiguities, user queries can also be incomplete or underspecified, especially in multi-round interactions. For example, queries might omit prior context with phrases like 'what about'. To ensure effective knowledge retrieval, the original query is enhanced and rewritten into a clearer and more detailed form, incorporating relevant prior information when available. Notably, temporal references (e.g., 'last year') are also standardized based on the current time."}, {"title": "Algorithm 2 Knowledge Retrieval", "content": "Input: User Query Q, Knowledge Graph G\nOutput: Knowledge Nodes $V_Q$\n1: $V_Q$ \u2190 \u00d8\n2: Coarse-Grained Retrieval:\n3: $V_Q$ \u2190 lex_search(Q,G) + sem_search(Q,G)\n4: Fine-Grained Ordering:\n5: for each node $v_i \\in V_Q$ do\n6:  if $v_i.type$ == alias then\n7:   $v_i$ \u2190 backtrack($v_i$) // Backtrack to a primary node\n8:  end if\n9:  // Compute a weighted matching score\n10:  score\u00a1 \u2190 w\u2081\u00b7lex_eval(Q, $v_i$)+w2\u00b7sem_eval(Q, $v_i$)+\nw3. LLM_eval(Q, $v_i$)\n11: end for\n12: $V_Q$.sort(scorei) // Rank by matching score\n13: return $V_Q$.topK"}, {"title": "Knowledge Retrieval.", "content": "To enhance LLMs' domain specific performance by integrating relevant knowledge into their context alongside the query, the selection and ordering of knowledge nodes from the knowledge graph are crucial. We employ a coarse-to-fine approach (Algorithm 2) to ensure comprehensive and precise knowledge retrieval."}, {"title": "Coarse-Grained Retrieval:", "content": "We perform lexical and semantic searches to retrieve a coarse set of knowledge nodes via token matching and embedding similarity between the query and each node's indexing triplet. We set a rather loose threshold to maximize recall. For alias nodes, we trace back to identify the nearest primary nodes (i.e., database/table/column/value/jargon nodes)."}, {"title": "D. Inter-Agent Communication", "content": "In this section, we introduce DataLab's Inter-Agent Communication module, which facilitates efficient communication among multiple agents to complete complex BI tasks.\nWorkflow. As shown in Figure 6, upon receiving a user query, the proxy agent initiates an analysis to formulate an execution plan (defined by an FSM), which comprises multiple subtasks allocated to various agents (Steps 1-2). It then dynamically manages the communication among involved agents based on task progression by retrieving information from a shared buffer and forwarding it to the agents to support subtask execution (Steps 5-6). Upon completion of the subtasks, the proxy agent stores the agents' outputs in the buffer (Steps 3-4). Finally, once all subtasks are completed, the proxy agent generates a final answer and returns it to the user (Step 7)."}, {"title": "Algorithm 3 DAG Construction", "content": "Input: Notebook Cells C\nOutput: Dependency DAG G\n1: v_hash, cell_refs \u2190 \u00d8, \u00d8\n2: // Identify new variables in each cell\n3: for each cell $c\\in C$ do\n4:  if c.type == Python then\n5:   ast \u2190 construct_ast(c)\n6:   new_v \u2190 find_global_variables(ast)\n7:   v_hash[new_v] \u2190 c\n8:  else if c.type == SQL then\n9:   data_v \u2190 find_data_variable(c)\n10:   v_hash[data_v] \u2190 c\n11:  end if\n12: end for\n13: // Find referenced cells for each cell\n14: for each cell $c\\in C$ do\n15:  external_v \u2190 find_external_variables(c)\n16:  cell_refs[c] \u2190 find_ref_cells(external_v,v_hash)\n17: end for\n18: return G\u2190 construct_dag(cell_refs)"}, {"title": "DAG Construction", "content": "As shown in Algorithm 3, given notebook cells C, the DAG construction process includes two steps:\nIdentify new variables. For Python cells, we construct an abstract syntax tree (AST) from the raw code to find global variables that are accessible across the entire notebook (e.g., function/class definitions, package imports). We exclude local variables as they are only visible within their scope. For SQL cells, any SELECT's output is stored in a data variable (e.g., DataFrame) for future use, and thus represents a new variable. Conversely, Markdown and Chart cells do not produce variables that can be referenced elsewhere, and are therefore omitted. We store the variable-cell associations using a hash table.\nFind referenced cells. Based on the hash table, we locate each cell's referenced cells by identifying its external variables that are defined in other cells. For Python and SQL cells, this can be easily achieved with ASTs. For Chart cells, the underlying data variable serves as the reference point. As Markdown cells do not associate with any variables, they are excluded from this step. Consequently, a DAG of the notebook can be constructed using the extracted cell references.\nThe DAG keeps updating whenever a cell is created, modified, or deleted, provided that the changes pass the syntax check. This ensures real-time maintenance of cell dependencies."}, {"title": "Data Visualization", "content": "Given that a typical DataLab notebook contains fewer than 50 cells, these results demonstrate the efficiency of DAG construction."}, {"title": "Sensitivity Analysis", "content": "As shown in Figure 7, DataLab consistently achieves satisfactory performance on all tasks, albeit with some sensitivity to the underlying LLMs. Proprietary models like GPT-4 typically exhibit superior instruction following and code generation abilities, surpassing open-source models like Qwen-2.5 and LLaMA-3.1. For code-intensive tasks like NL2DSCode and NL2Insight, LLaMA-3.1 experiences notable performance drops, especially on DS-1000, due to its relatively weaker code generation capabilities. We further evaluate DS-1000 using vanilla LLaMA-3.1 and achieve a pass rate of 36.90% (lower than 42.50% when integrated with DataLab). Another interesting fact is that, all three LLMs perform similarly on VisEval, with LLaMA-3.1 surprisingly being the best. These findings indicate that DataLab maintains a consistent performance across tasks, despite variations in LLMs, attributed to our data profiling and communication mechanisms. The data profiling mechanism enhances agents' understanding of input data, while the inter-agent communication module enables efficient error handling and iterative refinement, leading to overall performance improvements."}, {"title": "D. Effect of Inter-Agent Communication", "content": "We experiment with a complex BI scenario that involves multiple tasks performed by distinct agents: NL2SQL, NL2DSCode, NL2VIS, Anomaly Detection, Causal Analysis, and Time Series Forecasting. We compile a dataset from practical settings at Tencent, consisting of 2 databases, 10 tables, and 111 columns. For each table, we meticulously design 10 complex questions derived from real-world business queries, totaling to 100 samples. Each question requires multi-step reasoning and multi-agent collaboration, ensuring a rigorous evaluation of our inter-agent communication mechanism.\nWe evaluate this module's efficiency and effectiveness by respectively calculating the Success Rate and Accuracy of the agents' responses across all questions. The Success Rate measures the ratio of questions that can be successfully solved within a maximum of 5 calls per agent, while the Accuracy measures the ratio of correct answers among all questions. For comparison, we employ three experiment settings:\nS1 (w/o FSM) [18]: This setting removes the FSM-based information sharing protocol. Therefore, each agent receives all information from the shared buffer.\nS2 (w/o information formatting) [15]: This setting removes the information format structure and adopts pure natural language for inter-agent communication.\nS3 (w/ both): This setting keeps both techniques.\nAs illustrated in Table III, DataLab's performance on complex BI tasks improves by 19.00% in Success Rate and 28.00% in Accuracy with our inter-agent communication mechanism. Without the FSM-based information sharing protocol (S1), performance significantly degrades. Error analysis reveals that most failures involve more than 3 agents, resulting in overwhelming and irrelevant information that hinders LLMs' reasoning, thereby leading to incorrect outputs [19]. Additionally, the absence of the information format structure (S2) leads to a 7% decrease in Success Rate and a 5% drop in Accuracy, highlighting the importance of structured prompts in enhancing LLM comprehension and reducing information sharing ambiguities. This is critical in BI scenarios where"}, {"title": "IV. CONCLUSION", "content": "In this paper, we present DataLab, a unified platform for business intelligence that combines an LLM-based agent framework with a computational notebook interface. DataLab features a domain knowledge incorporation module, an inter-agent communication mechanism, and a cell-based context management strategy. These components enable seamless integration of LLM assistance with user customization, making DataLab well-suited for practical BI scenarios. Extensive experiments on both research benchmarks and real-world business datasets demonstrate the effectiveness of DataLab."}]}