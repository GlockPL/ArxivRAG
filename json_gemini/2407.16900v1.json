{"title": "Regulating AI Adaptation: An Analysis of AI Medical Device Updates", "authors": ["Kevin Wu", "Eric Wu", "Kit Rodolfa", "Daniel E. Ho", "James Zout"], "abstract": "While the pace of development of AI has rapidly progressed in recent years, the implementation of safe and effective regulatory frameworks has lagged behind. In particular, the adaptive nature of AI models presents unique challenges to regulators as updating a model can improve its performance but also introduce safety risks. In the US, the Food and Drug Administration (FDA) has been a forerunner in regulating and approving hundreds of AI medical devices. To better understand how AI is updated and its regulatory considerations, we systematically analyze the frequency and nature of updates in FDA-approved AI medical devices. We find that less than 2% of all devices report having been updated by being re-trained on new data. Meanwhile, nearly a quarter of devices report updates in the form of new functionality and marketing claims. As an illustrative case study, we analyze pneumothorax detection models and find that while model performance can degrade by as much as 0.18 AUC when evaluated on new sites, re-training on site-specific data can mitigate this performance drop, recovering up to 0.23 AUC. However, we also observed significant degradation on the original site after re-training using data from new sites, providing insight from one example that challenges the current one-model-fits-all approach to regulatory approvals. Our analysis provides an in-depth look at the current state of FDA-approved AI device updates and insights for future regulatory policies toward model updating and adaptive AI.", "sections": [{"title": "1. Introduction", "content": "While the number of AI products developed for commercial applications is rapidly growing, the implementation of robust regulatory frameworks still lags behind (Larson et al., 2021; Wirtz et al., 2020; Wu et al., 2021a). Recently, high-profile accidents involving Boeing (?) and Tesla (Corfield et al., 2023) have been attributed to issues with software and AI updates in their systems. Applications of AI to consumer lending (Johnson et al., 2019) and hiring systems (Bogen and Rieke, 2018) has also led to calls for more flexible regulatory systems that can anticipate algorithmic changes and biases. Such cases highlight the inherent challenges regulators face due to the adaptive nature of software and especially AI products: while model adaptation and updates are a necessary step in maintaining or improving their performance, they can also introduce unknown safety risks (Babic et al., 2019; Gilbert et al., 2021).\nIn the US, the Food and Drug Administration (FDA) has been an early mover in AI regulation, with over 500 approved submissions for AI devices as of 2022 (Center for Devices and Radiological Health, 2022). The FDA faces unique challenges with regard to model updating, as adverse events can directly compromise patient well-being. As such, the FDA has traditionally not allowed any changes to a model once it has been approved (Gerke et al., 2020). At the same time, AI models are well-known to be prone to distribution shifts, whereby variations in factors such as medical practice, patient demographics, or disease prevalence can significantly affect a model's performance (Raghu et al., 2019; Wiens et al., 2019; Wong et al., 2021). For example, researchers recently found that Epic's widely used sepsis prediction model performed much worse than initially reported after being deployed in a new hospital setting (Wong et al., 2021). Such cases demonstrate that fixed AI models that never receive updates can likewise compromise patient safety. Recently, the FDA has taken action to address the limitations of a fixed-model approach by providing guidelines for a potential Pre-determined Change Control Plan (PCCP) (Center for Devices and Radiological Health, 2023a), as well as a document describing best practices in machine learning published jointly by US, Canadian, and UK health authorities (Center for Devices and Radiological Health, 2023c). Under this provision, developers can make a limited set of changes to their models"}, {"title": "2. Methods", "content": "The primary data for this study consists of FDA approval documents for AI medical devices, which are publicly available through the FDA's online database (www.fda.gov). Under the FDA's 510(k) approval process, developers must demonstrate that the med-"}, {"title": "2.1. Collecting device updates", "content": "ical device they are marketing (the \"subject\" device) is \"substantially equivalent\" to a device already available on the market (the \"predicate\" device) (Brindza, 1980). Furthermore, each FDA-approved device is classified using a product code that indicates the overall function and safety profile (Center for Devices and Radiological Health, 2023a). For example, the product code QFM refers to \u201cRadiological Computer-Assisted Prioritization Software For Lesions\" and includes many common triage-based AI detection software. In our analysis, an FDA approval is considered a device update if 1) the predicate and the subject devices are from the same manufacturer, 2) both devices share the same product classification code, and 3) both devices are AI devices.\nWhen grouping by manufacturer names, multiple variants of the same manufacturer often appear (e.g., Siemens Medical Solutions USA Inc. and Siemens Medical Solutions, Inc.). To reconcile these differences, we first applied approximate string matching with Levenshtein distance and a similarity threshold of 0.8 to create candidate company name groupings before manual review. Furthermore, to systematically identify the predicate devices for each FDA approval, we extracted the PDF texts and performed a search over the first appearance of a submission number outside of the subject device number before performing a manual review."}, {"title": "2.2. Case study", "content": "Given that site-specific re-training is not allowed under current FDA 510(k) guidelines, we conducted a case study on pneumothorax detection models for chest X-rays to understand the potential performance gains that are currently uncaptured. There are currently four FDA-approved medical devices for the triage of X-ray images for the presence of pneumothorax (Wu et al., 2021a), and there are multiple publicly available chest X-ray datasets that include pneumothorax as a condition. We used three datasets, each from a different hospital site in the USA: the National Institutes of Health Clinical Center in Bethesda, Maryland (NIH) (?); Stanford Health Care in Palo Alto, California (SHC) (Irvin et al., 2019); and Beth Israel Deaconess Medical Center in Boston, Massachusetts (BID) (Johnson et al., 2023). We used a DenseNet-121 deep-learning architecture (Huang et al., 2017) that has been demonstrated to be a top-performing model for the classification of chest conditions (Irvin et al., 2019; Seyyed-Kalantari et al., 2020). These datasets represent a diversity of patient populations, imaging manufacturers, and pathology reporting standards (Wu et al., 2021b). To quantify how the AI's performance varies across sites, we trained separate deep-learning models on data from patients at each of the three sites and then evaluated the models on the test set from the other two sites. Each model takes as input a chest X-ray image and makes a binary prediction for pneumothorax. Similar to top-performing model approaches (Irvin et al., 2019; Seyyed-Kalantari et al., 2020), we trained five identical models (with different random seeds) for each setting and then ensembled the predictions by averaging the predicted probabilities across each model. We then re-trained the model (by fine-tuning) on a small subset of training data of five thousand examples from an unseen external site and re-evaluated the model's performance on both the original and external sites. We perform fine-tuning with the standard approach of updating all the model weights for a fixed number of steps without changing the hyperparameters."}, {"title": "3. Results", "content": "Among our dataset of 416 unique devices, we found that 101 devices report having been updated at least once (Figure 1). However, the vast majority of these updates expand the functionality or marketing claims of the device, essentially constituting a new device rather than a true model update. Of these 101 devices, only six of the updated devices report retraining in the model with new data. For each of the six devices, details on the types of data used in re-training are limited, with only three providing how much training data was used. For AI devices, re-training on new data is central to and distinctive of the technology, leading to our focus on the novel regulatory issues here. For example, Syngo.CT CaScoring (K221219), which analyzes calcified coronary lesions, only references that \"the algorithm was re-trained on a larger database\". AI-Rad Companion (K213096), which analyzes lung CTs, references \"additional training data was added\", while Briefcase (K230020), a rib fracture triage device, mentions that"}, {"title": "3.1. Device Update Frequency and Types", "content": "the updated device differs \"due to training the subject device on a larger data set\". The remaining three devices reference the scale of the re-training dataset. For example, Quantib Prostate (K230772), which analyzes prostate MRIs, reports that the updated algorithm has been trained on \"400 scans\", while Genius \u0391\u0399 (\u039a221449), a breast cancer detection device, reports a \"two-fold\" increase. Finally, Caption Ejection Fraction (K210747), a cardiac ultrasound AI device, reports an \"additional 30% training data from three ultrasound devices and two clinical sites\". Details on these devices are also included in Figure 2.\nFor the other 95 updated devices, we found several different update subtypes. The most common type of reported updating occurs when the manufacturer adds a new or additional prediction task to an existing model (55 total devices). For example, whereas FractureDetect's original device only works on wrists, its update has expanded to ankles, elbows, and other body parts. Next, we found that 21 devices have received updates to their accepted input signal. For example, recent mammography products such as Mammoscreen have included the ability to process Digital Breast Tomosynthesis (DBT)/3D scans, whereas previous versions only accepted Full-Field Digital Mammography (FFDM)/2D scans. An additional 13 devices report changes to the model design or architecture, such as a change from a fully connected neural network to a convolutional neural network. Five devices report a change to the intended target population for the device. For example, EndoSleep expanded its population to pediatric patients, whereas the previous device only allowed for patients 18 or older. We found 22 devices that report changes to the model but do not specify the exact nature of the change. For example, approvals may report \"additional algorithmic enhancements\" or \"improved quality of algorithms\", but not reference whether the improvements come from re-training or model design. Finally, 37 devices report updates unrelated to the model or its usage. Namely, these include software or hardware changes that pertain to its interoperability or output interface. Examples include the UI/UX of the device which is visible to physicians, or a hardware configuration that allows the device to be installed on new machines. We provide a list of examples of these update types in Table 1."}, {"title": "3.2. Time Between Updates", "content": "Based on our dataset, updates of any type occur a median of 17 months after previous device approval, with follow-ups as short as 3.5 months and as long as six years (Figure 2). This is a relatively short window of time, as the median time from concept to FDA approval for non-AI medical devices has been estimated to be 31 months (C. Johnson et al., 2022). Additionally, in order to account for right-censorship in our dataset (e.g. not yet observed updates in the newer devices), we used the Kaplan-Meier estimator and produced its curve (Figure 2). At two years, devices have an estimated update probability of 20% for all update types, and at four years, this probability rises to 30%. After seven years, the estimated probability of update saturates at 35%, meaning that about a third of devices receive at least one update of any kind in their lifetimes. However, the reported rate of model re-training is significantly lower: within two years, 1.4% of models are reported to be retrained, with the probability of device updates saturating at 1.7% after 2.4 years."}, {"title": "3.3. Case Study", "content": "We carried out a case study to illustrate and quantify the tradeoffs with AI adaptation through model retraining. We investigated the potential benefits and challenges of re-training on additional data from external sites in pneumothorax AI algorithms (Wu et al., 2021b,a). We found that external evaluation of models can result in an AUC decrease of up to 0.18, while re-training and evaluating on data from external sites improves model performance in all scenarios, with an average of 0.075 and a maximum of 0.23 AUC (Figure 4, Middle). However, after re-training on external sites, we also found that model performance degrades an average of 0.176 AUC (and up to 0.268 AUC) when re-evaluated on the original site (Figure 4, Bottom). This suggests that it can be challenging to have a single AI model that works well across heterogeneous settings."}, {"title": "4. Discussion", "content": "Currently, FDA-approved AI models are \"locked\" after approval, whereby making new changes requires undergoing a brand-new submission process, with most of the same regulatory burden (Gerke et al., 2020). Correspondingly, we observe in our analysis that only six out of 416 devices report actually received re-training updates, which is an essential approach for AI adaptation. On the other hand, nearly a quarter of devices receive updates in the form of additional marketing or functionality claims. Such disparity suggests a much stronger economic incentive for developers to increase the adoption of their devices through marketing new features rather than improving the original model through re-training.\nOne significant barrier to re-training is development costs, which may include acquiring new datasets (Chen et al., 2019; Wu et al., 2023a), computational resources (Wiens et al., 2019), data groundtruthing (Rahimi et al., 2021; Willemink et al., 2020), and regulatory hurdles (Kelly et al., 2019; Sertkaya et al., 2022). After models are updated, the manner in which they are deployed can also affect a device's ultimate clinical impact. First, while previous-generation AI devices for mammography were clinically evaluated to improve detection rates, subsequent studies showed limited benefits to women due to changes in how clinicians interacted with the devices, as well as the transition from film to digital mammograms (Lehman et al., 2015; Fenton, 2015). Second, economic forces such as reimbursement rates can affect how the frequency and extent to which these devices are adopted (Parikh and Helmchen, 2022b; Abramoff et al., 2022). AI adoption is still in a nascent stage, with very few widely adopted products and underdeveloped commercial payment pathways (Chen et al., 2021; Parikh and Helmchen, 2022a; Wu et al., 2023b). In such an environment, companies with few customers may not be able to dedicate resources toward regular model updating and maintenance. Currently, FDA cleared products exist in a similar band of risk profiles, with a previous study showing all devices currently categorized as risk class II (medium-risk) ((Zhu et al., 2022)). The lower-risk class I is largely exempt from the regulatory process, with the higher-risk class III reserved for devices that \"sustain or support life, are implanted, or present potential unreasonable risk of illness or injury\" ((for Devices and Health)). Whereas minimal-risk products like mobile health apps can introduce frequent updates without any regulatory hurdles, the medium-risk designation may encourage a trend towards more conservative updates that are more likely to be cleared rather than ambitious updates that may be rejected.\nThe FDA has recognized the high regulatory hurdles associated with model updating. In a recently proposed draft guidance from April 2023, model developers may be allowed to include a PCCP (Predetermined Change Control Plan) along with their device submission, which would allow them to simply document subsequent model updates rather than requiring a new submission every time (Center for Devices and Radiological Health, 2023b), potentially alleviating some of the regulatory burden and shortening update intervals. However, even under these proposed changes, developers are still required to complete rigorous evaluation and documentation of the algorithm changes, which incur much of the same prohibitive time and costs mentioned above (Allen, 2022; Evans, 2022). Furthermore, evaluating an updated model is an inherently difficult task due to various types of distribution shifts and heterogeneous data collection methods that are outside the control of developers (Schrouff et al., 2022; Chen et al., 2018). As such, future guidance documents should consider the challenges inherent in ensuring and evaluating fairness under fine-tuning and data shift. Our case study illustrates the tug-and-pull nature observed in AI models when trained on data from a specific site, they can perform well, but this may trade-off with performance on other sites. Although our models are trained on only a few datasets and do not comprehensively represent the gamut of available training data sources and model architectures on the market, the results illustrate how one instantiation with commonly used data and architecture choices exhibits characteristic behaviors of performance shift. In the status quo, model developers are locked into one model, creating scenarios where they may have to optimize for one population at the expense of another. To compound this issue, the actual performance on new, unseen populations is not even reported since the FDA does not require postmarket surveillance for 510(k)-approved devices (Wu et al., 2021a). To alleviate this problem, future regulatory guidelines should move beyond a \"one-model-fits-all\" approach, and instead consider allowing site-specific re-training and deployment. By allowing developers to deploy and validate multiple models under a single device, they can optimize model performance for each intended population without incurring performance tradeoffs. This would ensure that developers verify that their models perform well on each deployed clinical site while allowing them to perform the necessary site-specific documentation and evaluation as they mature. There are various design decisions that can affect how an AI model is re-trained: factors like whether to freeze layers, mix new training data, hyperparameter tuning, and validation processes can all influence how much re-training improves model performance (Pham et al., 2021; Picard, 2021; Qian et al., 2021). Indeed, in our case study, even though the individual models used in our ensemble approach only varied by the random seed used during training, performance across models still differed by up to 0.056 AUC. In a study by Watson et al. (2022), chest X-ray deep learning models trained on the same BIDMC dataset across different random seeds and hyperparameters were found to disagree in their explanations up to two-thirds of the time. Such studies on specific datasets represent potential pitfalls of algorithms applied to a particular clinical domain, but do not necessarily mean they generalize to all other types of devices. Regulatory guidelines should include consideration of appropriate fine-tuning schemes used when evaluating models.\nFurthermore, we find that among models that have been updated with re-training, details on the data used in training are very limited, with basic descriptions such as \"additional training data\", or \"larger database\". A limitation of our study lies in the limited details reported in FDA clearances. For example, while only 6 devices report retraining on new data, 5 devices report updates to their target population and 21 devices report unspecified improvements to their algorithm. As such, the true rate of retraining on new data may be higher than reported. In order for consumers and users to make informed decisions on the impacts of model updates, regulators"}]}