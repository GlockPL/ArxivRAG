{"title": "Group Fairness in Peer Review*", "authors": ["Haris Aziz", "Evi Micha", "Nisarg Shah"], "abstract": "Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. How- ever, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a notion of group fairness, called the core, which requires that every possible com- munity (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by withdrawing from a large conference.\nWe study a simple peer review model, prove that it always admits a reviewing as- signment in the core, and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics.", "sections": [{"title": "1 Introduction", "content": "Due to their large scale, conferences like NeurIPS and AAAI use an automated procedure to assign submitted papers to reviewers. Popular such systems include the Toronto Paper Matching System [1], Microsoft CMT2, and OpenReview\u00b3. The authors submitting their works are often very interested in receiving meaningful and helpful feedback from their peers [2\u20134]. Thus, their overall experience with the conference heavily depends on the quality of reviews that their submissions receive.\nThe typical procedure of assigning papers to reviewers is as follows. First, for each paper-reviewer pair, a similarity score is calculated based on various parameters such as the subject area of the paper and the reviewer, the bids placed by the reviewer, etc. [1, 5-8]. Then, an assignment is calculated through an optimization problem, where the usual objectives are to maximize either the utilitarian social welfare, which is the total similarity score of all matched paper-reviewer pairs, or the egalitarian social welfare, which is the least total score of reviewers assigned to any submission. Relevant constraints are imposed to ensure that each submission receives an appropriate number of reviews, reviewer workloads are respected, and any conflicts of interest are avoided.\nPeng et al. [9] recently mentioned that a major problem with the prestigious mega conferences is that they constitute the main venues for several communities, and as a result, in some cases, people are asked to review submissions that are beyond their main areas of work. They claim that a reasonable solution is to move to a de-centralized publication process by creating more specialized conferences appropriate for different communities. While specialized conferences definitely have their advan- tages, the maintenance of large conferences that attract multiple communities is also crucial for the emergence of interdisciplinary ideas that can be reviewed by diverse subject experts. Therefore, it is important to ensure that no group has an incentive to break off due to a feeling of being mistreated by the reviewing procedure of a large conference. In this work, we ask whether it is possible to modify"}, {"title": "1.1 Our Contribution", "content": "We consider a simple peer review model in which each agent submits (as the sole author) a number of papers to a conference and also serves as a potential reviewer. A reviewing assignment is valid if each paper is reviewed by $k_p$ reviewers, each reviewer reviews no more than $k_a$ papers, and no agent reviews her own submissions. To ensure that a valid assignment always exists, we assume that the maximum number of papers that each agent is allowed to submit is at most $\\lfloor k_a/k_p\\rfloor$.\nIn Section 3, we present an efficient algorithm that always returns a valid assignment in the core under minor conditions on the preferences of the authors. Specifically, our algorithm takes as input only the preference ranking of each author over individual potential reviewers for each of her submis- sions. Then, it produces an assignment that we prove to be in the core for any manner in which the agent's submission-specific preferences over individual reviewers may be extended to preferences over a set of $k_p$ reviewers assigned to each submission, aggregated over submissions, subject to two mild conditions being satisfied.\nIn Section 4, we conduct experiments with real data from CVPR and ICLR conferences, and evaluate the price that our algorithm must pay - in lost utilitarian and egalitarian welfare \u2014 in order to satisfy the core and prevent communities from having an incentive to deviate. We also observe that reviewer assignment methods currently used in practice generate such adverse incentives quite often."}, {"title": "1.2 Related Work", "content": "As we mentioned above, usually the first step of a review assignment procedure is to calculate a similarity score for each pair of submission and reviewer which aims to capture the expertise of the reviewer for this submission. The problem of identifying the similarity scores has been extensively studied in the literature [1, 5\u20138, 12]. In this work, we assume that the similarity scores are given as an input to our algorithm after they have been calculated from a procedure that is considered as a black box. Importantly, our algorithm does not need the exact values of the similarity scores, but it only requires a ranking of the reviewers for each paper, indicating their relative expertise for this paper.\nGiven, the similarities scores various methods have been proposed for finding a reviewing assign- ment. The most famous algorithm is the Toronto Paper Matching System [1] which is a very broadly applied method and focuses on maximizing the utilitarian welfare, i.e., the sum of the similarities across all assigned reviewers and all papers. This approach has been adopted by other popular con- ference management systems such as EasyChair and HotCRP5 [13]. While this approach optimizes"}, {"title": "2 Model", "content": "For $q \\in \\mathbb{N}$, define $[q] \\eqdef \\{1, ...,q\\}$. There is a set of agents $N = [n]$. Each agent $i$ submits a set of papers $P_i = \\{p_{i,1},..., p_{i,m_i} \\}$ for review by her peers, where $m_i \\in \\mathbb{N}$, and is available to review the submissions of her peers. We refer to $p_{i,l}$ as the $l$-th submission of agent $i$; when considering the special case of each agent $i$ having a single submission, we will drop $l$ and simply write $p_i$. Let $P = \\bigcup_{i\\in N} P_i$ be the set of all submissions and $m = \\sum_{i\\in N} m_i$ be the total number of submissions.\nAssignment. Our goal is to produce a (reviewing) assignment $R : N \\times P \\rightarrow \\{0,1\\}$, where $R(i, j) = 1$ if agent $i \\in N$ is assigned to review submission $j \\in P$. With slight abuse of notation, let $R_i = \\{j \\in P : R(i, j) = 1\\}$ be the set of submissions assigned to agent $i$ and $R^j = \\{i \\in N : R(i, j) = 1\\}$ be the set of agents assigned to review submission $j$. We want the assignment to be valid, i.e., satisfy the following constraints:\n\u2022 Each agent must be assigned at most $k_a$ submissions for review, i.e., $|R_i| < k_a, \\forall i \\in N$.\n\u2022 Each submission must be assigned to $k_p$ agents, i.e., $|R^j| = k_p, \\forall j \\in P$.\n\u2022 No agent should review one of her own submissions, i.e., $R(i, p_{i,l}) = 0, \\forall i \\in N, l \\in [m_i]$.\nTo ensure that a valid assignment always exists, we impose the constraint that $m_i\\cdot k_p \\leq k_a$ for each $i \\in N$, which implies that $m\\cdot k_p \\leq n\\cdot k_a$. Intuitively, this demands that each agent submitting papers be willing to provide as many reviews as the number of reviews assigned to the submissions of any single agent. For further discussion on this condition, see Section 5.\nNote that given $N' \\subseteq N$ and $P' \\subseteq P_i$ for each $i \\in N'$ with $P' = \\bigcup_{i \\in N'} P'_i$, the validity requirements above can also be extended to a restricted assignment $R' : N' \\times P' \\rightarrow \\{0, 1\\}$. Hereinafter, we will assume validity unless specified otherwise or during the process of building an assignment."}, {"title": "Preferences", "content": "Preferences. Each agent $i \\in N$ has a preference ranking, denoted $\\sigma_{i,l}$, over the agents in $N \\setminus \\{i\\}$ for reviewing her $l$-th submission $p_{i,l}$.\u2076 These preferences can be based on a mixture of many factors, such as how qualified the other agents are to review submission $p_{i,l}$, how likely they are to provide a positive review for it, etc. Let $\\sigma_{i,l}(i')$ be the position of agent $i' \\in N \\setminus \\{i\\}$ in the ranking. We say that agent $i$ prefers agent $i'$ to agent $i''$ as a reviewer for $p_{i,l}$ if $\\sigma_{i,l}(i') < \\sigma_{i,l}(i'')$. Again, in the special case where the agents have a single submission each, we drop $l$ and just write $\\sigma_i$. Let $\\vec{\\sigma} = (\\sigma_{1,1}, ..., \\sigma_{1,m_1}, ..., \\sigma_{n,1},..., \\sigma_{n,m_n})$.\nWhile our algorithm takes $\\vec{\\sigma}$ as input, to reason about its guarantees, we need to define agent prefer- ences over assignments by extending $\\vec{\\sigma}$. In particular, an agent is assigned a set of reviewers for each of her submissions, so we need to define her preferences over sets of sets of reviewers. First, we extend to preferences over sets of reviewers for a given submission, and then aggregate preferences across different submissions. Instead of assuming a specific parametric extension (e.g., additive preferences), we allow all possible extensions that satisfy two mild constraints; the group fairness guarantee of our algorithm holds with respect to any such extension.\nExtension to a set of reviewers for one submission: Let $S >_{i,l} S'$ (resp., $S \\geq_{i,l} S'$) denote that agent $i$ strictly (resp., weakly) prefers the set of agents $S$ to the set of agents $S'$ for her $l$-th submission $p_{i,l}$. We require only that these preferences satisfy the following mild axiom.\nDefinition 1 (Order Separability). For every disjoint $S_1, S_2, S_3 \\subset N$ with $|S_1| = |S_2| > 0$, if it holds that $\\sigma_{i,l}(i') < \\sigma_{i,l}(i'')$ for each $i' \\in S_1$ and $i'' \\in S_2$, then we must have $S_1 \\cup S_3 >_{i,l} S_2 \\cup S_3$.\nAn equivalent reformulation is that between any two sets of reviewers $S$ and $T$ with $|S| = |T|$, ignoring the common reviewers in $S \\cap T$, if the agent strictly prefers every (even the worst) reviewer in $S \\setminus T$ to every (even the best) reviewer in $T \\setminus S$, then the agent must strictly prefer $S$ to $T$.\nExample 1. Consider the common example of additive preferences, where each agent $i$ has a utility function $u_{i,l}: N \\setminus \\{i\\} \\rightarrow \\mathbb{R}_{>0}$ over individual reviewers for her $l$-th submission, inducing her preference ranking $\\sigma_{i,l}$. In practice, these utilities are sometimes called similarity scores. Her pref- erences over sets of reviewers are defined via the additive utility function $u_{i,l}(S) = \\sum_{i' \\in S} u_{i,l}(i')$. It is easy to check that for any disjoint $S_1, S_2, S_3$ with $|S_1| = |S_2| > 0$, $u_{i,l}(i') > u_{i,l}(i'')$ for all $i' \\in S_1$ and $i'' \\in S_2$ would indeed imply $u_{i,l}(S_1\\cup S_3) > u_{i,l}(S_2 \\cup S_3)$. Additive preferences are just one example from a broad class of extensions satisfying order separability.\nExtension to assignments. Let us now consider agent preferences over sets of sets of reviewers, or equivalently, over assignments. Let $R >_i R'$ (resp., $R \\geq_i R'$) denote that agent $i$ strictly (resp., weakly) prefers assignment $R$ to assignment $R'$. Note that these preferences collate the submission- wise preferences $>_{i,l}$ across all submissions of the agent. We require only that the preference extension satisfies the following natural property.\nDefinition 2 (Consistency). For any assignment $R$, restricted assignment $R'$ over any $N' \\subseteq N$ and $P' = \\bigcup_{i \\in N'} P'_i$ (where $P'_i \\subseteq P_i$ for each $i \\in N'$), and agent $i^* \\in N'$, if it holds that $R^{p_{i^*,l}}_{i^*} \\geq_{i^*,l} R'^{p_{i^*,l}}_{i^*}$ for each $p_{i^*,l} \\in P'$, then we must have $R \\geq_{i^*} R'$.\nIn words, if an agent weakly prefers $R$ to $R'$ for the set of reviewers assigned to each of her submis- sions individually, then she must prefer $R$ to $R'$ overall.\nExample 2. Let us continue with the previous example of additive utility functions. The pref- erences of agent $i$ can be extended additively to assignments using the utility function $u_i(R) = \\sum_{p_{i,l} \\in P_i} u_{i,l}(R^{p_{i,l}}_{i})$. It is again easy to check that if $u_{i,l}(R^{p_{i,l}}_{i}) \\geq u_{i,l}(R'^{p_{i,l}}_{i})$ for each $p_{i,l}$, then $u_i(R) \\geq u_i(R')$. Hence, additive preferences are again one example out of a broad class of prefer- ence extensions that satisfy consistency.\nCore. Our goal is to find a group-fair assignment which treats every possible group of agents at least as well as they could be on their own, thus ensuring that no subset of agents has an incentive to deviate and set up their own separate conference. Formally:\nDefinition 3 (Core). An assignment $R$ is in the core if there is no $N' \\subseteq N$, $P' \\subset P_i$ for each $i \\in N'$, and restricted assignment $R'$ over $N'$ and $P' = \\bigcup_{i \\in N'} P'_i$ such that $R' >_i R$ for each $i \\in N'$."}, {"title": "3 CoBRA: An Algorithm for Computing Core-Based Reviewer Assignment", "content": "In this section, we prove our main result: when agent preferences are order separable and consistent, an assignment in the core always exists and can be found in polynomial time.\nTechniques and key challenges: The main algorithm COBRA (Core-Based Reviewer Assignment), presented as Algorithm 1, uses two other algorithms, PRA-TTC and Filling-Gaps, presented as Algorithm 2 and Algorithm 3, respectively. We remark that PRA-TTC is an adaptation of the pop- ular Top-Trading-Cycles (TTC) mechanism, which is known to produce an assignment in the core for the house reallocation problem (and its variants) [22]. The adaptation mainly incorporates the constraints related to how many papers each reviewer can review and how many reviewers should review each paper. While for $k_p = k_a = 1$, PRA-TTC is identical with the classic TTC that is used for the house reallocation problem, the main difference of this problem with the review assignment problem is that in the latter each agent should give away her item (i.e., her submission) and obtain the item of another agent. Therefore, by simply executing TTC in the review assignment problem, one can get into a deadlock before producing a valid assignment. For example, consider the case of three agents, each with one submission. Each submission must receive one review ($k_p = 1$) and each agent provides one review ($k_a = 1$). The TTC mechanism may start by assigning agents 1 and 2 to review each other's submission, but this cannot be extended into a valid assignment because there is no one left to review the submission of agent 3. This is where Filling-Gaps comes in; it makes careful edits to the partial assignment produced by the PRA-TTC, and the key difficulty is to prove that this produces a valid assignment while still satisfying the core."}, {"title": "3.1 Description of CoBRA", "content": "Before we describe COBRA in detail, let us introduce some more notation. Let $m^* = \\max_{i\\in N} m_i$. For reasons that will become clear later, we want to ensure that $m_i = m^*$, for each $i \\in N$. To achieve that, we add $m^* - m_i$ dummy submissions to agent $i$, and the rankings over reviewers with respect to these submissions are arbitrarily. An assignment is called partial if there are submissions that are reviewed by less than $k_p$ agents. A submission that is reviewed by $k_p$ agents under a partial assignment is called completely assigned. Otherwise, it is called incompletely assigned. We denote"}, {"title": "3.3 Main Result", "content": "We are now ready to present our main result.\nTheorem 1. When agent preferences are order separable and consistent, COBRA returns an assign- ment in the core in $O(n^3)$ time complexity."}, {"title": "4 Experiments", "content": "In this section, we empirically compare COBRA to TPMS [1], which is widely used (for example, it was used by NeurIPS for many years), and PR4A [13], which was used in ICML 2020 [24]. As mentioned in the introduction, these algorithms assume the existence of a similarity or affinity score for each pair of reviewer $i$ and paper $j$, denoted by $S(i, j)$. The score (or utility) of a paper under an assignment $R$, denoted by $u^R_j$, is computed as $u^R_j = \\sum_{i \\in R^j} S(i, j)$. TMPS finds an assignment $R$ that maximizes the utilitarian social welfare (USW), i.e., the total paper score $\\sum_{j \\in P} u^R_j$, whereas PR4A finds an assignment that maximizes the egalitarian social welfare (ESW), i.e., the minimum paper score $\\min_{j \\in P} u^R_j$. We use $k_a = k_p = 3$ in these experiments."}, {"title": "5 Discussion", "content": "In this work, we propose a way for tackling the poor reviewing problem in large conferences by introducing the concept of \"core\" as a notion of group fairness in the peer review process. This fairness principle ensures that each subcommunity is treated at least as well as it would be if it was not part of the larger conference community.\nWe show that under certain -albeit sometimes unrealistic-assumptions, a peer review assignment in the core always exists and can be efficiently found. In the experimental part, we provide evidence"}, {"title": "A Proof of Lemma 1", "content": "We want to prove that CoBRA returns a valid assignment. We start by showing that during the execution of PRA-TTC and the execution the first phase of Filling-Gaps, it holds the following lemma.\nLemma 2. During the execution of PRA-TTC and the execution of the first phase of Filling-Gaps, for each $i \\in N$ with $P_i \\neq \\emptyset$, it holds that\n$\\left|R_{i}\\right|=\\sum_{j \\in[m^{*}]}\\left|R_{i}^{p_{i j}}\\right|$\nProof. Note that during the execution of PRA-TTC, if an agent $i$ with $P_i \\neq \\emptyset$ is assigned one submission due to the elimination of a cycle, then we know that one of her submissions that is incompletely assigned is also assigned to an agent that does not review it already. Hence, we see that until $P_{i}$ becomes empty, we have that $\\left|R_{i}\\right|=\\sum_{j \\in[m^{*}]}\\left|R_{i}^{p_{i j}}\\right|$\nNext, we focus on the execution of Filling-Gaps. We know that any $i \\in N$ with $P_i \\neq \\emptyset$ is included in $\\mathcal{U}$. In the first phase, the algorithm eliminates cycles in the greedy graph. With similar arguments as in the elimination of cycles in the preference graph, we get that during and after the first phase of Filling-Gaps, it is still true that $\\left|R_{i}\\right|=\\sum_{j \\in[m^{*}]}\\left|R_{i}^{p_{i j}}\\right|$ for any $i \\in \\mathcal{U}$ with $P_{i} \\neq \\emptyset$.\nNext, we need to show that Line 8 of PRA-TTC is valid which is true if $|\\mathcal{U}| \\leq k_{p}$.\nLemma 3. PRA-TTC returns $|\\mathcal{U}| \\leq k_{p}$.\nProof. For each $i \\in \\mathcal{U}$, from Lemma 2, we know that\n$\\left|R_{i}\\right|=\\sum_{j \\in[m^{*}]}\\left|R_{i}^{p_{i j}}\\right|<m^{*} \\cdot k_{p} \\leq k_{a}$\nwhere the first inequality follows since there exists at least one submission of $i$ that is assigned to less than $k_{p}$ agents. Hence, we get that each $i \\in \\mathcal{U}$ can review more submissions, when PRA-TTC terminates. Now, suppose for contradiction that at the last iteration of PRA-TTC, each agent $i \\in \\mathcal{U}$ has an outgoing edge in the preference graph. In this case, we claim that there exists a directed cycle in the preference graph which is a contradiction since PRA-TTC would not have been terminated yet. To see that, note that each outgoing edge of an agent $i \\in \\mathcal{U}$ either goes to another agent $i^{\\prime} \\in \\mathcal{U}$, since $i^{\\prime}$ can review more submissions, or goes to an agent $i^{\\prime} \\notin \\mathcal{U}$ whose all submissions are completely assigned. In the latter case, $i^{\\prime}$ has an outgoing edge to an agent in $\\mathcal{U}$ by the definition of the preference graph. Thus, starting from any agent in $\\mathcal{U}$, we conclude in an agent in $\\mathcal{U}$ and eventually we would found a cycle. Therefore, we have that there exists an agent $i^{*} \\in \\mathcal{U}$ that at the last iteration of the algorithm arbitrary picks her incomplete submission $p_{i^{*}, e^{*}}$ and does not have any outgoing edge to any other agent. This means that all the agents that can review more submissions, already review $p_{i^{*}, e^{*}}$. Since all the agents in $\\mathcal{U} \\setminus\\left\\{i^{*}\\right\\}$ can review more submissions, we get that all of them are assigned $p_{i^{*}, e^{*}}$. But since $p_{i^{*}, e^{*}}$ is not completely assigned, we conclude that $\\left|\\mathcal{U} \\setminus\\left\\{i^{*}\\right\\}\\right|<k_{p}$, which means that $|\\mathcal{U}| \\leq k_{p}$.\nWe proceed by showing that Lines 11- 12 of Filling-Gaps are valid.\nLemma 4. When Fillings-Gaps enter the second phase with a non empty $\\mathcal{U}$, for each $t \\in[|\\mathcal{U}|]$ and for each $p_{\\rho(t), l} \\in P_{\\rho(t)}$, it exists a completely assigned submission $p_{i^{\\prime}, e^{\\prime}}$ with $i^{\\prime} \\in \\mathcal{U} \\cup \\mathcal{L} \\setminus\\{\\rho(t)\\}$ that is not reviewed by $\\rho(t)$, and it also exists an $i^{\\prime\\prime} \\in \\mathcal{N}$ that reviews $p_{i^{\\prime}, e^{\\prime}}$, but she does not review $p_{\\rho(t), l}$.\nProof. When $\\mathcal{U}$ is non empty and no more cycles exists in the greedy graph, the algorithm constructs the topological order of the greedy graph, denoted by $\\rho$.\nFirst, we show the following proposition."}, {"title": "B Supplementary Experiments", "content": "In Section 4, we show that TPMS and PR4A often motivate group of authors to deviate and redis- tribute their submissions among themselves. The size of a deviating groups is also an interesting measure, for evaluating if such a group indeed consists a distinct subcommunity of researchers that has incentives to build its own conference rather than an extremely tiny group of authors that could locally benefit by exchanging their papers for reviewing. In Table 3, we can see the maximum size of a successfully deviating coalition, averaged across 100 runs, together with the standard error. As before, each run is a subsampled dataset of size 100, so these can be interpreted as percentages. It"}]}