{"title": "Bridging LLM-Generated Code and\nRequirements: Reverse Generation technique and\nSBC Metric for Developer Insights", "authors": ["Ahilan Ayyachamy Nadar Ponnusamy"], "abstract": "The rise of Large Language Models (LLMs) in software engineering,\nparticularly in code generation, has garnered significant attention. How-\never, assessing the quality of AI-generated code remains a challenge due\nto the inherent complexity of programming tasks and the lack of robust\nevaluation metrics that align well with human judgment. Traditional\ntoken-based metrics such as BLEU and ROUGE, while commonly used\nin natural language processing, exhibit weak correlations with human as-\nsessments in code intelligence and verification tasks. Furthermore, these\nmetrics are primarily research focused and are not designed for seamless\nintegration into the software development lifecycle, limiting their practical\nutility for developers seeking to improve code quality and security.\nAI-assisted coding has been shown to be more beneficial for senior de-\nvelopers, as they possess the expertise to critically evaluate the generated\ncode for correctness, completeness, and compliance. In contrast, junior\ndevelopers may struggle to identify hallucinations, missing functionality,\nor incorrect logic in AI-generated code. To bridge this gap, This paper in-\ntroduces a novel scoring mechanism called the SBC score, which is based\non a reverse generation technique that leverages the natural language gen-\neration capabilities of LLMs. Unlike direct code analysis, our approach\nreconstructs system requirements from AI-generated code and compares\nthem with the original specifications to quantify accuracy. The SBC score\ncombines semantic similarity, BLEU, and completeness analysis,\nproviding actionable insights to developers by highlighting missing fea-\ntures and hallucinations. This hybrid metric not only improves the evalu-\nation of AI-generated code but also offers a real-time, interpretable scoring\nsystem that can be integrated into the software development process, ben-\nefiting developers of all experience levels. Our code and datasets are\navailable on GitHub: GitHub Repository.", "sections": [{"title": "Introduction", "content": "AI-powered code assistants, leveraging the power of Large Language Models\n(LLMs), are becoming a focal point for enterprises, offering promising capa-\nbilities in automating code generation. However, evaluating the quality of\nLLM-generated code remains a complex challenge due to the intricacies of pro-\ngramming concepts and syntax, which differ significantly from natural language\ngeneration [1, 2].\nTraditional evaluation techniques rely on test-based methods such as pass@k,\nwhich assess code correctness by executing manually written test cases [3, 4].\nWhile effective in certain contexts, these methods are limited by the need for\nextensive test case coverage, which is labor-intensive and may not fully capture\ncode correctness beyond functional execution. Similarly, token-based metrics\nsuch as BLEU [5], ROUGE-L [6], and CodeBLEU [7] have demonstrated weak\ncorrelations with human judgment when applied to code evaluation [1].\nMore recently, neural-based evaluation metrics such as CodeBERTScore [8]\nhave shown improvements by leveraging deep learning models for code similar-\nity assessments. However, these methods still depend on high-quality reference\nsolutions, which can be expensive and difficult to obtain. Furthermore, even\nwith neural models, code evaluation has not yet reached human-level under-\nstanding, as these models struggle to assess subtle correctness issues such as\nmissing computation steps or failure to handle corner cases [9, 10].\nAnother key challenge is that AI-generated code is often more beneficial to\nexperienced developers who can assess correctness, completeness, and security,\nwhereas junior developers may struggle to critically evaluate and refine the\ngenerated code. Studies suggest that GitHub Copilot, one of the most widely\nused AI-powered coding assistants, provides significant advantages for senior\ndevelopers but offers only limited value for junior developers [11, 9].\nTo address these limitations, we introduce a novel reverse generation\ntechnique combined with an SBC score to evaluate the accuracy and com-\npleteness of AI-generated code. Our approach does not rely on reference code\nbut instead assesses how well the generated code aligns with the original require-\nment. This is achieved by extracting system requirements from the AI-generated\ncode and comparing them with the initial requirements using three key metrics:\n\u2022 Semantic Similarity Score - measuring meaning-level alignment be-\ntween requirements,\n\u2022 BLEU Score - capturing lexical overlap,\n\u2022 Completeness Score \u2013 identifying missing and extra elements.\nThe SBC score, along with the reverse-generated requirements, provides ac-\ntionable insights for developers, helping them assess AI-generated code without\nrequiring extensive reference implementations. Unlike prior evaluation methods,\nthis approach inherently addresses the challenges of syntactic variations and al-\nternative solutions in generated code, as highlighted in recent studies [12]. By"}, {"title": "Related Work", "content": "There has been no prior work on applying a reverse generation technique with\nLLMs in conjunction with the SBC scoring mechanism. However, existing\nresearch has explored LLM-based evaluation methods for code analysis and\nnatural language generation (NLG), leveraging large models for assessing\noutputs. Three notable studies in this area include:"}, {"title": "G-EVAL: NLG Evaluation using GPT-4 with Better\nHuman Alignment (Yang et al., [13])", "content": "This paper introduces an LLM-driven evaluation framework that employs\na chain-of-thought (CoT) approach to assess the quality of generated text.\nThe study demonstrates that LLM-based evaluators can outperform traditional\nNLG metrics in text summarization and dialogue generation. However,\nthe authors also highlight a critical limitation that LLM-based evaluators tend to\nexhibit a bias toward LLM-generated text, raising concerns about fairness\nand reliability in assessments."}, {"title": "ICE-Score: Instructing Large Language Models to Eval-\nuate Code (Terry, [14])", "content": "Inspired by G-EVAL, this paper proposes ICE-Score, an evaluation met-\nric that leverages LLMs for code assessment across multiple programming lan-\nguages, including Java, Python, C, C++, and JavaScript. The approach in-\ncorporates both human-centered usefulness and execution-based func-\ntional correctness, aiming to provide a holistic evaluation of generated\ncode. ICE-Score refines the instruction-based evaluation paradigm for assessing\nAI-generated code in a structured manner."}, {"title": "Metamorphic Prompt Testing for LLM Code Valida-\ntion (Xiaoyin et al., [15] )", "content": "As Large Language Models (LLMs) become increasingly integrated into the\nsoftware development lifecycle, concerns regarding the quality, correctness, and\nreliability of generated code have grown. Xiaoyin et al. [15] highlight these\nchallenges and propose metamorphic prompt testing as a validation approach.\nTheir method leverages the intrinsic consistency among correct code samples\nwhile identifying inconsistencies in flawed ones. By paraphrasing prompts and\ngenerating multiple versions of code for cross-validation, their evaluation on\nHumanEval demonstrated that metamorphic prompt testing detected 75% of"}, {"title": "Methodology", "content": "The following methodology was adopted in this study to evaluate the effective-\nness of reverse generation and SBC scoring in AI-assisted development."}, {"title": "Dataset Creation", "content": "To construct a dataset that closely mimics the software development lifecycle,\nrequirements were distributed across multiple layers of application development,\nsuch as:\n\u2022 User Interface (UI): React, Angular\n\u2022 Data Layer: SQL for data modeling\n\u2022 Data Objects and Business Logic: .NET, Node.js, Quarkus, Spring\nBoot\nA total of 90 requirements were curated, ensuring a diverse mix of technolo-\ngies and application layers that reflect real-world developer usage. To prevent\ndata contamination, as discussed by [16] and [14], a completely new dataset was\ncreated instead of relying on pre-existing benchmark datasets. The following\nfigure illustrates few requirements from the dataset."}, {"title": "Reference Implementation", "content": "A reference implementation was developed in Python and is available in the\nassociated GitHub repository. The implementation follows these key steps:\n1. Iterate through the requirements and target technologies in the dataset.\n2. Invoke the LLM to generate code for the given requirement.\n3. Perform reverse generation by passing the generated code back to the LLM\nwith a detailed prompt to reconstruct the requirement.\n4. Compare the original requirement with the reverse-generated requirement\nusing the SBC scoring mechanism.\n5. Store results in JSON format, including input and generated requirements,\nfinal SBC score, and individual component scores (semantic similarity,\nBLEU, completeness)."}, {"title": "Choice of Open Models", "content": "The existing studies highlighted in the Related Work section [13, 14] use closed-\nsource hosted models, such as GPT-3.5 and GPT-4, for their analysis. In con-\ntrast, this study focuses on open models, as discussed in [17].\nWe selected four quantized models (Q4 or Q5) for this study:\n\u2022 Model 1: Codellama 13B\n\u2022 Model 2: Qwen2.5-Coder 14B\n\u2022 Model 3: Deepseek Coder 6.7B"}, {"title": "SBC Score Calculation", "content": "The SBC score is computed using the formula:\n$fs = (0.7 \\times semantic\\_score) + (0.1 \\times BLEU) + (0.2 \\times completeness)$ (1)\nwhere:\n\u2022 Semantic Similarity is computed using the PyTorch cos sim function\nto measure the cosine similarity between input and generated require-\nment encodings using the all-MiniLM-L6-v2 model from the Sentence\nTransformers library.\n\u2022 BLEU Score evaluates n-gram matching between the original and reverse-\ngenerated requirements.\n\u2022 Completeness Score extracts key nouns, verbs, and proper nouns from\nthe input and computes a penalty based on missing or extra keywords:\nkeywords1 = extract_keywords(input requirements from dataset) (2)\nkeywords2 = extract_keywords(reverse generated requirements) (3)\nmissing = keywords1 - keywords2 (4)\nextra keywords2 keywords1 (5)\ntotal_keywords = |keywords1 U keywords2| (6)\n$penalty = |missing| + |extra|$ (7)\n$score = max (0,1-\\frac{penalty}{max(total\\_keywords,1)})$ (8)\nThe weight distribution in the SBC score reflects the relative importance of\neach metric in capturing requirement-code alignment. Semantic similarity is\ngiven the highest weight (0.7) as it directly measures how closely the reverse-\ngenerated requirement aligns with the original intent, making it the most critical\nfactor. Completeness is weighted at 0.2 to ensure that missing or extra key-\nwords are penalized without dominating the score, while BLEU, often less re-\nliable for evaluating long-form text generation, is assigned a lower weight (0.1)\nto complement semantic similarity without disproportionately influencing the\nfinal score."}, {"title": "A Note on Correlation Metrics", "content": "While correlation coefficients such as Kendall-Tau (\u03c4), Pearson (rp), and Spear-\nman (rs) are commonly used to measure relationships between variables, they\nwere not included in the SBC score computation. These metrics primarily as-\nsess agreement and ranking consistency, whereas SBC is designed as a weighted\nfunction of semantic similarity, BLEU, and completeness, to directly measure\nrequirement-code alignment.\nThat said, these correlation measures could be valuable for post-hoc valida-\ntion, particularly in assessing how well SBC correlates with human judgment\nor other evaluation metrics. Future research may explore their role in further\nvalidating SBC's effectiveness in AI-assisted development scenarios."}, {"title": "Visualization and Analysis", "content": "For visualization and analysis, this study utilized Google Sheets to generate\ngraphs and insights from the SBC score data. The process involved:\n\u2022 Exporting the SBC score output into a CSV format.\n\u2022 Creating a Pivot Table to aggregate and structure the data.\n\u2022 Generating graphs and charts to visualize trends in the SBC scores.\nThe primary focus was on the final SBC score, as the semantic similar-\nity component had a disproportionately strong influence on the overall score\ncompared to BLEU and completeness. The generated graphs provided valuable\ninsights into trends and comparative performance across different models and\ntechnologies."}, {"title": "Experiments and Results", "content": "To ensure consistency in responses, we conducted all tests with the temperature\nset to zero. With the reference implementation, we conducted the evaluation in\ntwo cycles:"}, {"title": "Per-LLM Evaluation", "content": "Each LLM was run for three iterations across all 90 questions in the dataset. The\nSBC score details were recorded in JSON format, capturing key elements such as\ninput_requirements, reverse-generated_requirements, final-accuracy-score,\nsemantic_similarity, BLEU_score, completeness_score, missing_elements,\nand extra_elements. The figure below presents the evaluation graph for each\nLLM included in this study, providing a comparative analysis of their perfor-\nmance."}, {"title": "Cross-LLM Comparison", "content": "After evaluating all four LLMs, resulting in a total of 12 iterations, the results\nwere consolidated into a final comparative analysis. For this, the maximum SBC\nscore for each question within each LLM was extracted to form a representative\ndataset. A final consolidated chart was then created to compare the performance\nof all LLMs based on these maximum scores.\nThe results show that the line graphs for all LLMs are closely aligned, in-\ndicating that their performance trends are similar, rising and falling together\nacross different questions. Additionally, we observed that across technologies\nand application layer questions, the LLMs performed consistently with minimal\nvariance.\nThe following figure illustrates the consolidated graph below presents the\nmaximum SBC scores for each LLM across the three iterations."}, {"title": "Integrating SBC Score and Artifacts with the Appli-\ncation Development Life Cycle", "content": "Given the significant benefits that the SBC score, along with missing and extra\nkeyword analysis, can provide, it is highly beneficial for enterprises to integrate\nthis process into the AI code assistant workflow. Instead of merely returning\nthe generated code, AI code assistant responses can be modified to include the\nSBC score along with reverse-generated requirements, and highlight missing and\nextra keywords. This integration empowers developers at all levels, ensuring\nthat the advantages of AI-driven code generation are accessible to a broader\nrange of team members. By providing these insights, entrprises can better"}, {"title": "Conclusion and Next Steps", "content": "In this study, we introduced a novel approach to evaluating LLM-generated\ncode by leveraging reverse generation and the Semantic-BLEU-Completeness\n(SBC) hybrid metric. Our methodology provides a tool to validate if the code\ngeneration aligns closely with initial requirements by measuring semantic sim-\nilarity, BLEU score, and completeness. The structured evaluation across four\nLLMs, conducted with a zero-temperature setting for consistency, demonstrated\nthat most models performed similarly, with their performance trends rising and\nfalling in a closely knitted pattern.\nA key objective of this work is to provide a solution that serves both senior\nand junior developers by enabling them to assess the validity and completeness\nof LLM-generated code without the need for manual inspection of the code\nitself. Our approach allows developers to quickly evaluate whether the generated\ncode aligns with the original requirements, making it accessible and useful to\nprofessionals with varying levels of expertise. This minimizes the time and effort\nneeded to validate code, empowering developers to focus on more complex tasks\nand improving overall productivity.\nOur results highlight the strengths and limitations of LLMs in generating re-\nliable code from natural language requirements. The analysis of missing_elements\nand extra_elements provided valuable insights into common failure modes,\nrevealing both missing functionalities and hallucinations in generated code.\nThese insights suggest that reverse generation can serve as an effective valida-\ntion mechanism, enabling developers to assess and refine LLM-generated code\nsystematically.\nUnlike prior studies that predominantly leveraged closed hosted cloud models\nsuch as GPT-3.5 and GPT-4 [14, 13], we specifically chose open models to ensure\ntransparency, reproducibility, and fine-grained control over the deployment and\nthe evaluation process. This decision aligns with our goal of establishing a more\nflexible, interpretable, and adaptable framework for assessing LLM-generated\ncode.\nDespite the promising results, several areas warrant further exploration.\nWhile we relied on SBC scores to compare alignment between input require-\nments and generated code, human feedback remains an essential validation\nmechanism. Future work should incorporate qualitative assessments from soft-\nware engineers to further validate whether high SBC scores correlate with human\nperceived correctness and usability of the generated code. Additionally, while\ncorrelation coefficients such as Pearson (rp), Spearman (rs), and Kendall-Tau\n(T) were not included in SBC score computation, they remain useful validation\ntools for analyzing how well SBC scores align with human evaluations. Future\nexperiments can leverage these correlation measures to assess whether the SBC\nmetric effectively captures human judgment of requirement-code alignment."}]}