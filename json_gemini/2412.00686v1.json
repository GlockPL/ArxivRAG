{"title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models", "authors": ["Muhammad Fetrat Qharabagh", "Mohammadreza Ghofrani", "Kimon Fountoulakis"], "abstract": "Counting is a fundamental skill for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs' counting abilities using a divide-and-conquer approach, breaking counting problems into sub-counting tasks. Unlike prior methods, which do not generalize well to counting datasets on which they have not been trained, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances counting capabilities across various datasets and benchmarks.", "sections": [{"title": "1 Introduction", "content": "Counting is a key cognitive task with broad applications in industry, healthcare, and environmental monitoring [De Almeida et al., 2015, Guerrero-G\u00f3mez-Olmedo et al., 2015, Paul Cohen et al., 2017, Lempitsky and Zisserman, 2010]. It improves manufacturing, inventory, and quality control, ensures safety in medical settings, and helps manage resources in environmental efforts [Wang and Wang, 2011, Zen et al., 2012, Arteta et al., 2016]. Recent advancements in prompt-based models enable counting of unlimited object varieties without visual exemplars. Although current trained, and text-prompt-based, counting models by Dai et al. [2024], Amini-Naieni et al. [2024] perform well on the datasets they are trained on, they face the following challenges. First, they require fine-tuning on new datasets. Second, since the concepts in the counting datasets are limited, these models do not generalize well to counting questions that involve complex reasoning. There are also training-free models by Shi et al. [2024], but overall they have weaker performance compared to trained models. On the other hand, large vision-language models (LVLMs), such as GPT-40 [Achiam et al., 2023], which also do not need dataset specific training, show very good performance in counting low numbers of objects, usually less than 20, across most datasets. However, their performance deteriorates for larger numbers of objects, regardless of the dataset.\nWe enhance the accuracy of LVLMs to count objects in images by leveraging their reasoning power within a divide-and-conquer method. The LVLMs allow us to handle diverse objects and complex counting questions, while our divide-and-conquer method alleviates challenges associated with counting large numbers of objects. Inspired by prior work on the rapid and accurate estimation of small quantities by Chattopadhyay et al. [2017], we divide an image into sub-images, and prompt the LVLM to count the objects of interest in each sub-image. The counts from each sub-image are then aggregated to make the final prediction. Our proposed workflow is illustrated in Figure 1.\nInitially, in our pipeline, the object of interest is extracted from the input question using an LLM. The area containing the object of interest is detected in the image by a grounding model, such as Liu et al. [2023], and"}, {"title": "2 Related Work", "content": "Early counting models, referred to as class-specific, targeted counting problems for certain categories [Arteta et al., 2016, Babu Sam et al., 2022, Mundhenk et al., 2016, Xie et al., 2018], such as cars, people, or cells. Later, with the emergence of stronger vision models and large-scale datasets, class-agnostic methods were proposed that could count objects from a wide variety of categories. However, most existing class-agnostic, or open-world, models require visual exemplars of the target objects [\u0110uki\u0107 et al., 2023, Gong et al., 2022, Lin et al., 2022, Liu et al., 2022, Lu et al., 2019, Nguyen et al., 2022, Ranjan et al., 2021, Shi et al., 2022, Yang et al., 2021, You et al., 2023]. The concept of divide and conquer has also been used in early work[Xiong et al., 2019, Chattopadhyay et al., 2017]. However, this early work requires training dedicated models to utilize this concept.\nText-based counting-specific trained models. With the advent of vision-language foundation models such as CLIP and GroundingDINO, text-based open-world methods have been proposed. Leveraging the rich textual and visual feature extraction capabilities of foundation models, obtained through web-scale training, the text-based counting methods by Amini-Naieni et al. [2023], Dai et al. [2024], Kang et al. [2024], Amini-Naieni et al. [2024] have started to demonstrate comparable or superior accuracy. GroundingREC by Dai et al. [2024] is an open-world model built on top of GroundingDINO [Liu et al., 2023], and introduces an additional task called referring expression counting. Concurrently, Amini-Naieni et al. [2024] proposed a method that also builds on GroundingDINO but adds an extra image-text fusion module in the input, enabling the model to accept text and/or visual exemplars to determine the target.\nModels without counting-specific training. Shi et al. [2024] introduce TFOC, a counting model that does not require any counting-specific training. Instead, they cast the counting problem as a prompt-based"}, {"title": "3 LVLM-Count", "content": "Our proposed method aims to answer counting questions by dividing an image into sub-images while avoiding cuts through objects of interest. LVLM-Count consists of four key stages. First, in the \"Area Detection\" stage, we localize areas containing relevant objects. Second, in the \"Target Segmentation\" stage, we identify and segment these local areas. Third, in the \"Object-aware Division\" stage, we divide the localized areas into sub-images without cutting through the segmented objects. Finally, the LVLM counts the target objects in each sub-image and aggregates the results. Figure 1 illustrates the workflow of our method, which we will detail in the following subsections. Note that in this section, we illustrate each stage using example images. These images are are used solely for illustration purposes. LVLM-Count may not outperform other methods on these examples."}, {"title": "3.1 Area Detection", "content": "In this part of the pipeline, we assume that we are given a counting question $Q$ along with an image. The question $Q$ contains an expression $E$ that specifies a set of objects of interest. The expression $E$ distinguishes these objects from objects of other categories or the same category but with different attributes present in the image. By employing an LLM, the expression $E$ is extracted from $Q$. For example, let $Q$ be \"How many people are in the boat?\". $Q$ is given to an LLM, which is prompted to return the expression $E$ \"people in the boat\", referring to the objects we want to count. After $E$ is extracted, it is given as input to GroundingDINO along with the image. The output of GroundingDINO is a set of bounding boxes that have relevance to $E$ beyond a certain threshold. These bounding boxes often overlap and typically contain repeated objects. Thus, all the overlapping output bounding boxes are merged. After merging, a set of non-overlapping areas of interest may remain. We consider the non-overlapping areas as \"detected areas\", which are then cropped to be passed to the next stage. Note that the area detection stage is important as it extracts the area with the most relevant context for the counting question. This process is illustrated in Figure 3."}, {"title": "3.2 Target Segmentation", "content": "The cropped images from the first stage contain objects of interest, and the ultimate goal is to divide them without cutting through those objects. However, a prerequisite for implementing such a mechanism is to first detect and localize the objects of interest. Each cropped image is fed into an open-world detection model along with $E$. The output of the open-world detection model produces a bounding box for each object of interest. The bounding boxes are then given as input to a segmentation model, which returns segmentation masks for the objects within each bounding box. We illustrate an example of this process in Figure 4.\nHow to determine the bounding boxes. To determine the bounding boxes, we use GroundingDINO and set the bounding box probability threshold to a low value to avoid missing any objects. The bounding boxes alone cannot help with the object-aware division of the cropped images due to their rigid structure, which includes redundant areas in the vicinity of the object and, in the worst case, overlaps with other bounding boxes. Our goal is to precisely locate the pixels of an object of interest."}, {"title": "3.3 Object-aware Division", "content": "In this stage, the cropped image is divided into appropriate sub-images so that no object of interest is cut by the dividing paths. The core idea is that the dividing paths should not intersect the pixels covered by the masks corresponding to the objects of interest. This step consists of two sub-steps. First, we decide the starting and ending points of the paths. Second, we draw the paths. Below, we describe how we approach these two sub-steps.\nHow to determine the starting and ending points of the paths. We consider the first sub-step as a hyper-parameter, which can be set in various ways depending on the type of dataset. We utilize two approaches in our experiments. The first approach is unsupervised and non-parametric, where we use a clustering method to obtain the start and end points of the paths. We describe the clustering approach in more detail below. The second approach is to simply use a pre-determined number of equidistant points. This is helpful when we know a priori that objects are uniformly distributed across the image. In this case, we fix $k$ equidistant vertical and horizontal coordinates in the image as the points of the dividing paths for the entire dataset. In our experiments, we specify which approach we use for each dataset. Nonetheless, we will illustrate in the experimental results that even if we do not treat this as a hyperparameter and always use the unsupervised and non-parametric approach, LVLM-Count is quite effective."}, {"title": "3.4 Target Counting", "content": "At this point, all the sub-images obtained from the cropped areas are gathered. Since these are small partitions of the original image, they might lack the desired resolution. Thus, using a super-resolution model can alleviate this problem for the sub-images. In this work, we use Real-ESRGAN [Wang et al., 2021]. Then, question $Q$ and each sub-image are given as input to an LVLM. At the end of the loop, the recorded numbers for the sub-images are aggregated to form the final answer. 1."}, {"title": "4 Experiments", "content": "In this section, we present the performance results of our method on a counting-specific dataset, an open-ended counting benchmark with two types of questions, simple and complex, and a challenging counting benchmark that we propose using emoji icons. We compare the results to state-of-the-art models which have been specifically trained on counting datasets, and we also compare to state-of-the-art models which have not been trained on counting datasets. The code to reproduce the experiments can be found here:\n4.1 Datasets and benchmarks\nFSC-147 [Ranjan et al., 2021]. FSC-147 is a counting dataset that contains 6135 images, spanning 147 different object categories such as kitchen utensils, office supplies, vehicles, and animals. The number of objects in each image ranges from 7 to 3731, with an average of 56 objects per image. The dataset is split into training, validation, and test sets. A total of 89 object categories are assigned to the training set, 29 to the validation set, and 29 to the test set, with different categories in each split. The training set contains 3659 images, with the validation and test sets containing 1286 and 1190 images, respectively. For each image in the test set, a single category name is given, and the expected output is the number of instances from the category.\nOpen-ended Counting Benchmark. TallyQA [Acharya et al., 2019] is an open-ended counting dataset that includes complex counting questions involving relationships between objects, attribute identification, reasoning, and more. TallyQA is quite a large dataset, with the train set having 249, 318 questions and the test set having 22, 991 simple and 22, 991 complex counting questions. Please refer to Appendix C and Figure 9 for more information on simple and complex categorization of the questions. The number of objects in each image ranges from 0 to 15. The dataset has a heavy bias towards a low number of objects (see Figure 8 in the Appendix). To alleviate this bias and create a benchmark for efficiently measuring the simple and complex open-ended capabilities of a counting model, we randomly sample 10 questions per ground truth count. This results in 155 simple and 149 complex open-ended counting questions in total. It is important to note that the bias in TallyQA is pronounced; for most ground truth values greater than 10, there are fewer than 10 samples available in the entire test set.\nEmoji-Count. Although TallyQA addresses the scarcity of complex counting questions in prior datasets to a certain degree, the range of target objects is limited, spanning from 0 to 15. To our knowledge, no counting benchmark exists for large numbers involving complex reasoning. To this end, we propose a challenging counting benchmark using emoji icons. From the 1816 standard emoji icons, we remove those that directly overlap with concepts demonstrated by other icons. We then group the remaining 1197 icons into 82 classes. In each class, there are icons from the same or similar object categories, but with subtle differences that require complex reasoning to distinguish. For each of the 82 classes, an empty 1024 \u00d7 1024 image is first created. This image is filled with six categories chosen randomly from the class, with each category having a random count between 30 and 50 in the image. We illustrate an example of this dataset in Figure 7."}, {"title": "4.2 Results", "content": "The following discusses the numerical results of our experiments with LVLM-Count on each benchmark described in Section 4.1. For visual examples of LVLM-Count's performance on each benchmark, see Appendix M. Additionally, for the ablation study and experiments on the PASCAL VOC dataset [Everingham et al., 2015], see Appendix A and Appendix K, respectively.\nFSC-147. We compare the performance of LVLM-Count to an extensive list of state-of-the-art counting models on the test set of the FSC-147 dataset. We also include two baselines: i) taking the number of target segmentation masks as the final answer, and ii) giving the output of the target segmentation stage to base GPT-40 and asking it to count the masks. We run different experiments using GPT-40, Gemini 1.5 Pro, and"}, {"title": "5 Limitations and Future Work", "content": "A limitation of LVLM-Count is the area detection stage. If the cropped areas do not provide enough context, performance may suffer. This is especially true for complex questions that require understanding the relationships between all the objects and the background in an image. This opens up opportunities for future work on improved area detection methods or the use of a context provider to complement the area detection stage. Another limitation arises with images containing thousands of objects. After one iteration of division, a significant number of objects may still remain in the sub-images. A potential solution is to apply additional iterations of division on the sub-images; however, the low resolution of these sub-images may make this infeasible. Developing a solution to maintain resolution is another direction for future work. Finally, in some cases, sub-images do not contain any objects of interest. The LVLM occasionally predicts a non-zero value in such instances. This is a weakness of LVLMs that requires special consideration to improve their performance."}, {"title": "A Ablation Study", "content": "We examine the effect of each of the following stages in our method: i) area detection and ii) object-aware division (note that the object-aware division necessitates the inclusion of target segmentation stage). The experiments are designed to investigate the effect of each stage individually, as well as when the stages are combined in our pipeline. Additionally, we run an experiment for a case where both stages of area-detection and target segmentation are excluded. In this case, the object-aware division can not be performed. Thus, images are divided by equidistant straight lines into subimages. We give the name of naive division to such an approach. Moreover, we run another experiment where area detection is in place but the target segmentation is excluded and naive division is applied on the detected areas We run the ablation scenarios for two LVLMs: GPT-40, and Gemini 1.5 Pro. Furthermore, the effect of super resolution at the final stage of LVLM-Count is also investigated for the case that GPT-40 is the LVLM. For the experiments, we randomly sample 4 images from each category in the FSC-147 test set (29 categories) and report the performance on the resulting subset containing 116 samples. In Table 4 we show the results of the ablation experiments."}, {"title": "B Bias to Small Numbers in Datasets and Performance of LVLMs", "content": "In our experiments, LVLMs (GPT-40) are able to make correct predictions when the number of items to be counted is small, but errors increase as the ground truth number grows. Although, we cannot be certain, one likely reason for this behavior in LVLMs is that, during training, the counting questions these models encounter are heavily biased toward small numbers. As an example, in Figure 8 we show the distribution of 'How many' questions in some well-known VQA datasets."}, {"title": "C Definition and Example of Simple and Complex Counting Tasks", "content": "Acharya et al. [2019] were among the first to formally categorize counting questions into simple and complex types. They applied a linguistic rule: first, they removed any substrings such as \"...in the photo?\" or \"...in the image?\". Then, they used SpaCy to perform part-of-speech tagging on the remaining substring. They classified a question as simple if it contained only one noun, no adverbs, and no adjectives; otherwise, they deemed it complex. This rule classifies questions such as \"How many dogs?\" as simple and \"How many brown dogs?\" as complex. Following this rule, they built two splits for the TallyQA dataset: a simple split and a"}, {"title": "D Incorrect Examples in the FSC-147 Dataset", "content": "We observed some incorrect instances in the FSC-147 dataset. For example, see Figure 10. In these cases, the category names which are provided are incorrect. For these instances, the extensive knowledge embedded within the LVLMs employed in our approach proves to be a disadvantage. These models detect inconsistencies and provide a count of zero as the output, whereas other methods are misled by superficial similarities and mistakenly count the objects. A more thorough study is required to detect all the incorrect examples in FSC-147."}, {"title": "E Alleviating the Problem of Inaccurate Detections at the Area Detection and Target Segmentation Stages of LVLM-Count", "content": "One of the limitations of LVLM-Count is that if the area detection fails to detect a relevant area, the objects of interest in that area will not be counted. Another limitation arises when the target segmentation fails to segment all the instances of the target category of objects. In such a scenario, non-segmented objects of interest might be cut through by division lines, causing the predicted total number to increase due to being counted multiple times. One simple, yet quite effective, approach that we use to overcome this limitation to a significant degree is setting the detection threshold of GroundingDINO, which is used in both of the mentioned stages, to an extremely low value. This significantly reduces the chance of missing an area or object of interest, although it might lead to some false positives. However, note that false positives do not harm the performance of LVLM-Count, as their only effect is that the object-aware division lines avoid cutting through them.\nTo demonstrate the effectiveness of this approach, we evaluate it on the Penguin dataset [Penguin Research, 2016]. The goal in this dataset is to count penguins in images. The challenging Penguin dataset consistently exhibits heavy occlusion and complex background patterns that can easily be mistaken for penguins Arteta et al. [2016]. This dataset consists of two splits: the mixed-site split, in which images from the same camera"}, {"title": "FLVLM-Count's Power in Handling Multiple Object Categories in the Same Image", "content": "LVLM-Count is a highly effective method for handling counting tasks that involve multiple objects in the same image. Its strength in such scenarios stems from the capabilities of LVLMs to answer numerous visual questions about an image and its objects. Depending on the given text prompt, it can count instances of a single object category among others or instances of multiple object categories simultaneously. In this section, we demonstrate how LVLM-Count performs in counting different objects of interest, determined simply by a prompt, using an image with multiple object categories.\nThe image in Figure 14 contains three object categories: person, cow, and horse. In the top row, the object of interest is \"cow.\" We prompt LVLM-Count to count the cows. First, the masks are produced through the initial stages of our pipeline, and then the cluster-based approach is used to automatically determine the start and end points of the division paths. It can be observed that horses have also been masked as cows. Nonetheless, this does not negatively impact the final answer; it merely causes the division lines to avoid cutting through the horses as well. The counting in LVLM-Count is performed by an LVLM (GPT-40 in this figure) and does not rely on the masks. We observe that GPT-40 successfully counts the number of cows in the resulting subimages, leading to the correct final answer.\nIn the middle row of Figure 14, the object of interest is \"person.\" LVLM-Count again successfully counts the number of people accurately. A more interesting case is the bottom row of Figure 14, where both cows and persons are objects of interest. We prompt LVLM-Count to count the number of \"cows and persons.\" Similar to the first row of the figure, there are false positive masks here as well. However, LVLM-Count successfully counts the number of instances from both categories combined since the counting is ultimately performed by the LVLM. Note that the number of objects in this image is limited, and GPT-40 might answer these questions correctly without the need for the LVLM-Count pipeline. This image has been chosen to illustrate LVLM-Count's power in handling multiple objects in a counting task rather than for comparison with the baseline LVLM."}, {"title": "GReal-world Application of LVLM-Count", "content": "As stated in Section 1, counting has numerous real-world applications, including but not limited to biology, health, industry, warehousing, and environmental monitoring. Below, we demonstrate the performance of LVLM-Count on examples from the following areas: i) biology/health, ii) industry/warehousing, and iii) environmental monitoring. We also compare its results with those of the base LVLM (GPT-40 for the figures in this section). Note that in all examples, the cluster-based approach automatically determines the start and"}, {"title": "HPerformance Analysis of LVLM-Count for Different Ground Truth Ranges on FSC-147 Dataset", "content": "To further investigate the performance of our pipeline, we divide the ground truth values in the FSC-147 test set into intervals and plot the MAE for the base GPT-40 and Gemini 1.5 Pro models, alongside the results from LVLM-Count using each model, as shown in Figure 18. The first interval contains relatively small ground truth values, a range where LVLMs already perform well. As the ground truth values increase, the base models exhibit increasingly larger errors compared to LVLM-Count, with the margin growing rapidly. This behavior is consistent with our observations of counting errors on the blue circles in Figure 2."}, {"title": "I Illustration of the Workflow for the Zebra Image in Figure 1", "content": "In this paper, we choose the most appropriate images for simple and understandable illustrations of the execution of each stage in our pipeline. Inevitably, this led to the selection of different images for each stage. However, in this section, for the sake of consistency, we demonstrate the same concepts illustrated in Figures 3, 4, 5, and 6 for the zebra image used in Figure 1.\nThe zebra image is passed to the pipeline along with the question $Q$ =\"how many zebras are in the image?\". First, $E$=\"zebra\" is extracted using the LLM. Then the zebra image is passed to the area detection stage, where the prompt given to GroundingDINO is \"zebras\". The output bounding boxes are merged, and the resulting area is cropped, as illustrated in Figure 19. The cropped area is then passed to the target segmentation stage. At this stage, GroundingDINO detects the objects of interest defined by $E$ as the input prompt. SAM then uses the output bounding boxes to produce segmentation masks for the zebras, as shown"}, {"title": "J False Positive Masks at the Target Segmentation Stage", "content": "One of the reasons we task an LVLM to count the objects in the subimages instead of using the number of generated masks at the target segmentation stage as the final count of the objects of interest is the existence of false positive masks. The GroundingDINO model is responsible for detecting the objects of interest, determined by expression $E$, and passing the output bounding boxes to SAM for producing segmentation masks. Nonetheless, GroundingDINO is not as strong as an LVLM in understanding expressions extracted"}, {"title": "KLVLM-Count's Performance on a Benchmark Sampled from the PASCAL VOC Dataset", "content": "The PASCAL VOC dataset [Everingham et al., 2015] is a well-known dataset in the field of computer vision, depicting everyday objects in everyday scenes. It is primarily used for tasks like object detection, classification,"}, {"title": "LReport of Various Accuracy Metrics for the Performance of LVLM- Count on the FSC-147 dataset, TallyQA Benchmark, and Emoji- Count Benchmark", "content": "This section presents various accuracy measures for the experiments reported in Tables Table 1, 2, and 3. The accuracy metrics are defined in Table 8. The observations for each table can be summarized as follows:"}]}