{"title": "TOXICITY DETECTION TOWARDS ADAPTABILITY TO CHANGING PERTURBATIONS", "authors": ["Hankun Kang", "Jianhao Chen", "Yongqi Li", "Xin Miao", "Mayi Xu", "Ming Zhong", "Yuanyuan Zhu", "Tieyun Qian"], "abstract": "Toxicity detection is crucial for maintaining the peace of the society. While existing methods perform well on normal toxic contents or those generated by specific perturbation methods, they are vulnerable to evolving perturbation patterns. However, in real-world scenarios, malicious users tend to create new perturbation patterns for fooling the detectors. For example, some users may circumvent the detector of large language models (LLMs) by adding 'I am a scientist' at the beginning of the prompt. In this paper, we introduce a novel problem, i.e., continual learning jailbreak perturbation patterns, into the toxicity detection field. To tackle this problem, we first construct a new dataset generated by 9 types of perturbation patterns, 7 of them are summarized from prior work and 2 of them are developed by us. We then systematically validate the vulnerability of current methods on this new perturbation pattern-aware dataset via both the zero-shot and fine tuned cross-pattern detection. Upon this, we present the domain incremental learning paradigm and the corresponding benchmark to ensure the detector's robustness to dynamically emerging types of perturbed toxic text. Our code and dataset are provided in the appendix and will be publicly available at GitHub, by which we wish to offer new research opportunities for the security-relevant communities.", "sections": [{"title": "Introduction", "content": "Online communication plays an essential role in our life, but nowadays it is seriously disturbed by toxic contents. Toxic content means the text is rude or hateful, e.g., cyberbullying [Slonje et al., 2013, Kowalski, 2018], hate speech [Del Vignal2 et al., 2017, Fortuna and Nunes, 2018], and social bias [Liang et al., 2021, Gongane et al., 2022], which will lead to disharmony and conflicts in society [Feldman et al., 2015, Dixon et al., 2018]. For protecting the healthy communication environment, various toxicity detection methods have been proposed in recent years such that the toxic content could be limited via pre-detection or be filtered via post-detection.\nMost of existing detectors are normal methods focusing on ordinary text, e.g., nigger, and they struggle to identify the toxicity in perturbed text, e.g., niggggeeer (by random repeat) or nigger (by homoglyph), as Fig. 1 (b) shows. Though a few methods have paid attention to the perturbed text, they only fine tune detectors on augmented data with specific types of perturbations. Such detectors can work well on toxic contents generated by the associated perturbation pattern like repeat, but may fail on those by another type of perturbations like homoglyph, as Fig. 1 (c) shows. However, the very nature of perturbation patterns is that they are always dynamically changing over time. This is because the malicious users in the real world intentionally evade the detectors by generating the toxic text using newly appeared perturbation patterns.\nIn view of the evolving nature of perturbation patterns, we introduce the new problem of continual learning jailbreak perturbation patterns for toxicity detection. Our goal is to empower"}, {"title": "Related Work", "content": null}, {"title": "Toxicity Detection", "content": "Various methods have been proposed for textual toxicity detection, which can be primarily categorized into two types: performance-oriented and robustness-oriented.\nThe performance-oriented methods are targeted at improving the performance in terms of metrics like accuracy, F1, and AUC [Markov et al., 2023, Kebriaei et al., 2024, Zhang et al., 2024a, Pan et al., 2024, Zhang et al., 2024b]. Many datasets across different languages have been constructed for this task [Lashkarashvili and Tsintsadze, 2022, Garg et al., 2023, Delbari et al., 2024]. Most of these datasets are curated from online platforms like Twitter, Reddit, and Zhihu Sap et al. [2020], Haber et al. [2023], Lu et al. [2023] with lots of manual efforts for annotation, and a few of them are generated by models like GPT3 Hartvigsen et al. [2022]. The performance-oriented toxicity detection methods rely heavily on lexical cues in normal text and are vulnerable to even simple attacks.\nThe robustness-oriented methods are developed to enhance the toxicity classifiers' robustness via adversarial training on perturbed data. The perturbation approaches [Le et al., 2022, Bespalov et al., 2023, Ye et al., 2023] may manipulate on characters, words, or sentences in the text, e.g., character swap and/or substitution, homoglyph/homophone substitution, decomposition, near-neighbor words replacement, and distractor injection [Kurita et al., 2019, Cooper et al., 2023, Emmery et al., 2022, Yu et al., 2024].\nPrior robustness-oriented toxicity detection methods often employ one or several perturbation strategies, and the perturbed data are only used for augmentation. In contrast, we systematically investigate 7 existing and 2 new types of perturbation patterns and exploit each type of perturbed data from the domain continual learning perspective."}, {"title": "Continual Learning", "content": "Continual learning aims to incrementally learn knowledge from a non-stationary data stream without catastrophic forgetting [Goodfellow et al., 2013]. There are three continuous learning scenarios, i.e, domain-incremental, class-incremental, and task-incremental learning [Van de Ven et al., 2022].\nIn domain-incremental learning, all tasks have the same label space but different data distributions (domains) [Michalski et al.,"}, {"title": "Dataset Construction", "content": "As mentioned in the introduction, there is no suitable dataset for our proposed problem. In this section, we first prepare the raw data and then introduce our framework to construct the perturbation pattern-aware dataset. As shown in Fig. 2, it consists of three modules: (1) Selector: selecting the significant words for toxicity judgement, (2) Perturbator: perturbing text with different types of perturbation patterns, (3) Detector: judging whether the perturbed text can jailbreak the detector successfully. Below we present the details."}, {"title": "Raw Data Preparation", "content": "Benefiting from the existence of labeled toxicity detection datasets, we do not need to annotate the data and we just perturb the text with different perturbation patterns. We choose the Jigsaw [Kivlichan et al., 2020] dataset since it contains a large scale of toxic text (223k in total). However, this dataset is seriously imbalanced between non-toxic and toxic samples, with a ratio approaching 10:1. Hence we re-sample the toxic and non-toxic samples in 1:1 ratio, and obtain 20k/20k toxic/non-toxic samples for subsequent perturbations."}, {"title": "Selector", "content": "This module aims to select the important words that would increase the probability of toxicity. These words will be perturbed to evade detectors. We propose four strategies for selection: (1) online toxic words, (2) toxic words identified by detectors, (3) words which can increase the toxicity scores, (4) words spuriously correlated with the toxic label.\n(1) Online toxic words Following existing studies, we collect online toxic words from Google, including its full list of bad words and the banned swear words.\n(2) Words identified as toxic We employ PerspectiveAPI [Lees et al., 2022] to detect the toxicity of each word in the dataset. If the word is identified as toxic, we add it into our collection.\n(3) Words increasing toxicity scores We compute the expectation of decreased toxicity score (supplied by PerspectiveAPI) for each word to examine its contribution to the sample's toxicity. Specifically, given the word $w_i$ and the sentences $T_{w_i}$ it appears, we compute the expectation $E_{w_i}$ as follows:\n$E_{w_i} = \\frac{1}{|T_{w_i}|} \\sum_{t_j \\in T_{w_i}} [s(t_j) \u2013 s(t_j/w_i)],$ (1)\nwhere $s(t_j)$ and $s(t_j/w_i)$ denote the toxicity score of the text $t_j \\in T_{w_i}$ before and after removing $w_i$, and $s(t_j) \u2013 s(t_j/w_i)$ denotes the decreased toxicity score after removing $w_i$. The expectation $E_{w_i}$ represents the contribution of the word $w_i$ to the toxicity. We select the words whose expectation is larger than 0.\n(4) Spuriously correlated words Some words often appear with toxic labels, resulting in the detectors tend to identify the text including these words as toxic, i.e., there is a spurious correlation between the words and the label [Garg et al., 2023]. We utilize the mutation information based metric to obtain such words [Zhang et al., 2023].\n$MI = \\frac{p(w_i, c)}{p(w_i,\\cdot)p(\\cdot, c)},$ (2)\nwhere $p(w_i,\\cdot)$ and $p(\\cdot, c)$ are the marginal distribution of the word $w_i$ and the label $c$, and $p(w_i, c)$ is the joint distribution of $w_i$ and $c$. The larger $MI$, the stronger the spurious correlation between $w_i$ and $c$. Based on multiple manual checkings and previous empirical setting [Zhang et al., 2023], we set the spurious correlation threshold to the sum of the mean and standard deviation of $MI$."}, {"title": "Perturbator", "content": "This module aims to perturb text based on perturbation patterns widely used in real world scenarios. Specifically, we intro-"}, {"title": "Detector", "content": "This module first classifies the toxicity of the perturbed samples and then chooses those successfully jailbreaking detectors after perturbations. Note that to prevent detectors from taking shortcuts by judging whether the text has been disturbed, we perturb all texts in Jigsaw dataset, i.e., non-toxic texts are also perturbed in the same way as toxic ones."}, {"title": "Dataset Overview and Quality Assessment", "content": "We finally obtain our perturbation pattern-aware dataset DynEscape consisting of successfully jailbreaking 38K samples by applying the aforementioned framework to samples in Jigsaw dataset. We split DynEscape into train/valid/test subsets with the ratio of 6:1:3. The number of samples for each perturbation in these subsets is 2584/430/1294.\nWe assess the quality of our dataset from following perspectives.\nBalance: Our dataset is balanced since we re-sample non-toxic text and the ratio of toxic to non-toxic is 1:1.\nToxicity Diversity: Our dataset has a high toxicity diversity since it inherits multiple types of toxicity from Jigsaw, including hate speech, social bias, offensive, and so on.\nPerturbation Diversity and Reality: Our dataset has a high perturbation diversity covering 9 types of perturbation patterns. Moreover, these patterns are widely used in real-world scenarios.\nConsistency: We randomly select 50 samples for each perturbation and employ three master students to evaluate the semantic consistency before/after perturbations on a scale of 0 to 1. We get a score of 0.88\u00b10.07 averaged over all samples and more than two masters assign a score exceeding 0.5 in 99% of the samples, indicating a high consistency of our dataset."}, {"title": "Methodology", "content": "In this section, we first systematically investigate the vulnerability of current methods on the newly constructed perturbation pattern-aware dataset, and then present our continual learning approach for toxicity detection."}, {"title": "Pattern-wise Detection on Separately Perturbed Text", "content": "We perform two types of pattern-wise perturbation functional detection to examine the vulnerability of current methods, including:\n(1) zero-shot detection with existing normal detectors.\n(2) cross-pattern detection with detectors fine tuned on associated perturbation patterns (abbreviated as perturbations).\nOur goal is to answer the following questions.\nQ1: Are existing normal detectors vulnerable against perturbed toxic text without extra adjustments?"}, {"title": "Zero-shot Detection with Existing Normal Detectors", "content": "We examine the vulnerability of existing detectors against different perturbed texts without performing additional adjustments for any perturbations."}, {"title": "Cross-pattern Detection with Detectors Fine Tuned on Specific Perturbations", "content": "Many methods are fine tuned on specific perturbed texts, showing adaptability to associated perturbations to some extent. In our study, we further investigate the vulnerability of such detectors against unseen perturbations by performing cross-pattern testing. To this end, we employ two standard fine tuning settings.\n(1) Joint fine tuning: It fines tune one detector using all text perturbed by all perturbation methods, as shown in Fig. 3 (a).\n$\\theta^* = argmin_\\theta \\sum_{(x_j, y_j) \\in D} L(F(x_j; \\theta), y_j),$ (3)\nwhere $\\theta$ denotes the parameters of detector $F$, and $D$ denotes the full dataset. $L$ is the classification loss.\n(2) Separate fine tuning: It fines tune each detector using text perturbed with the associated perturbation, as shown Fig. 3 (b).\n$\\theta_i^* = argmin_{\\theta_i} \\sum_{(x_j, y_j) \\in D_{p_i}} L(F_i(x_j; \\theta_i), y_j),$ (4)\nwhere $\\theta_i$ denotes the parameters of the $i$th detector $F_i$ for the $i$th pattern $p_i$, and $D_{p_i}$ denotes the associated dataset."}, {"title": "Continual Detection on Dynamically Perturbed Text", "content": "Intentionally, malicious users employ dynamically changing jailbreaking perturbation patterns to circumvent the detectors. Hence it is necessary for detectors to adapt to changing perturbations. Since the texts perturbed by different perturbations could be regarded as the text distributed in different perturbation domains, it is natural for detectors to adapt to the dynamically changing perturbations via domain incremental learning as shown in Fig. 3 (c). In this way, the following question is expected to be answered:\nQ4: Can domain incremental learning make detectors adapt continuously to toxic text perturbed by dynamically changing patterns?\nSpecifically, given a perturbation pattern (domain) $p_i$ at the $i$th moment, the associated dataset $D_{p_i} = \\{(x_{j,p_i}, y_j)\\}$, where $x_{j,p_i}$ denotes the text perturbed by $p_i$ and $y_j \\in$ {toxic, non-toxic} denotes the ground truth label. Ideally, we hope the detector could adapt to both previous and newly appeared perturbed text:\n$\\theta^* = argmin_{\\theta} \\sum_{i=1}^{T} \\sum_{(x_j,p_i, y_j) \\in D_{p_i}} L(F(x_{j,p_i}; \\theta), y_j)),$ (5)\nwhere $(x_{j,p_i}, y_j)$ denotes the $j$th sample in the $D_{p_i}$ perturbed dataset, $p_i$ is the pattern appearing at the $i$th moment during the usage time of the detector $F$."}, {"title": "Our Continual Learning Detector DynDetect", "content": "There is no continual learning method available for addressing incremental toxicity detection across perturbation domains. In this subsection, we present a naive method DynDetect for perturbation domain incremental toxicity detection.\nWe first employ BERT to encode the sample $s_i$ to get its embedding $f_i$:\n$f_i = BERT(s_i)$ (6)\nThe classifier then assigns toxic/non-toxic label $\\hat{y}_i$ to the sample:\n$\\hat{y}_i = Classifier(f_i)$ (7)\nMoreover, to deal with the catastrophic forgetting issue in continual learning, we propose to replay the feature-level knowledge in previous perturbation domains for the new domain. Specifically, we replay memory samples from the old domains and maximize the similarity between memory features encoded by models at the old and current times. Since memory features encapsulate knowledge in corresponding perturbation domains, our replaying strategy ensures the current model preserves the prior knowledge. The continual learning loss is computed as:\n$L_{kl,f} = -\\frac{1}{m_i} \\sum_i log(diag(f_{j-1}^m \\times f_j^m))$ (8)\nwhere $f_{i,l}^m$ and $f_{i,l}^m$ represent the incorporated features of samples in the $i$th memory at the (j-1)th and jth moment, respectively. $\\times$ denotes the matrix product and $diag$ means the diagonal elements of the product, which represents the features similarity of memory samples.\nThe final objective for our task is the combination of the toxicity detection task and the continual learning task, defined as:\n$L = L_{cls} + L_{kl, f},$ (9)"}, {"title": "Experiments", "content": "In this section, we explore the aspects mentioned above through experiments. We use the accuracy as the metric."}, {"title": "Pattern-wise Detection Experiments on Separately Perturbed Text", "content": null}, {"title": "Zero-shot Detection with Existing Normal Detectors", "content": "To examine the vulnerability of existing normal detectors against different perturbations, we detect the toxicity of text before/after perturbations and compare the performance based on three types of widely used normal detectors.\n(1) Commercial APIs: Content moderation APIs from Google (i.e., PerspectiveAPI) [Lees et al., 2022] and OpenAI [Markov et al., 2023].\n(2) LLM based detectors: We employ LLama3 [MetaAI, 2024] and ChatGPT [OpenAI, 2022] to answer whether the text is toxic.\n(3) Pretrained detectors: Detoxify [Hanu and Unitary team, 2020] and ToxBERT/ToxRoBERTa [Hartvigsen et al., 2022].\nAs Table 3 shows, all normal detectors suffer from sharp performance degrades after perturbations, where the absolute accuracy score drops by up to 44.54 points and at least 10.41 points. Many detectors even perform nearly at the random level when detecting some types of perturbed text. For example, the OpenAI content moderation API only gets an accuracy of 50.77% when detecting the text perturbed with Homoglyph.\nBy now, we can answer the question Q1. Existing normal detectors are vulnerable against different perturbations."}, {"title": "Cross-pattern Detection with Detectors Fine Tuned on Specific Perturbations", "content": "To examine the vulnerability of detectors fine tuned on specific perturbation against unseen perturbations, we fine tune BERT [Devlin et al., 2019] on a specific perturbation pattern and then test the detector on other patterns, which we call the cross-pattern detection. Please note joint fine tuning in cross-pattern detection utilizes all data together, where the detector has seen all perturbations and its performance can be considered as the soft upper bound. Meanwhile, separate fine tuning produces perturbation-wise detector for each seen perturbation, where such detector is used to perform cross-pattern detection on the remaining unseen perturbations. \nIt is clear that detectors fine tuned on a specific perturbation pattern can reach superior performance on this associated pattern, but their performance degrades significantly on unseen cross-pattern perturbations. For example, the accuracy of the BERT Repeat detector fine tuned on the Repeat perturbation is"}, {"title": "Continual Detection Experiments on Dynamically Perturbed Text", "content": "To investigate how our proposed continual detection approach DynDetect performs on the dynamically perturbed text, we modify typical methods widely used in domain continual learning as baselines, including LFL [Jung et al., 2016], LWF [Li and Hoiem, 2017], EWC [Kirkpatrick et al., 2017], EPI [Wang et al., 2023b], and CEAR [Zhao et al., 2023]. The joint and separate fine tuning perform as the upper bound of incremental learning, and the stream fine tuning performs as the lower bound, where the detector is fine tuned for each task sequentially.\nThe results demonstrate that our proposed DynDetect method outperforms all continual learning baselines. Moreover, all continual learning methods reach significant improvements over existing normal detectors and detectors fine tuned on specific perturbations.\nWe now answer the questions Q4. The domain incremental learning enables the detector robust to dynamically changing perturbations."}, {"title": "Perturbation Order Sensitivity Analysis", "content": "To examine the impact of perturbation orders, we employ 3 variants of the raw order . (1) Order-1 reverses the raw order, (2) Order-2 randomly changes the raw order in the same character/word/sentence level, e.g., Swap becomes the first in character level perturbations, (3) Order-3 randomly changes the raw order among different levels, e.g., sentence level perturbations first and then character level ones.\nFrom the results , we can find that our DynDetect is always the best among all methods. In addition, the results of our ablated variant DynDetect_m without our feature similarity maximization strategy prove the positive impact of this strategy. Finally, all continual learning methods under various orders consistently beat normal detectors and those fine tuned on specific perturbations, which clearly proves the necessity and superiority of the continual learning paradigm for toxicity detection."}, {"title": "Error Analysis", "content": "We analyze the proportions of error samples wrongly predicted by different methods. As can be seen, continual learning (CL) detectors can successfully recognize most of toxic samples but fine tuning (FT) detectors cannot (blue bars). Notably, there are almost no samples correctly identified by FT detectors but incorrectly predicted by CL ones (green bars). Hence the error samples mainly fall into two categories, for each of which we present one case for analysis.\n(1) Error samples that cannot be detected by FT detectors but can be identified by our CL detector: Error case 1:\nRaw: you are dumb\nPert: yo$=u are du,mxb\nThis case is perturbed by the Insert pattern. The FT detector trained on this pattern succeeds but those trained on Repeat, Swap, Homo., Distract, Authorize all fail, showing their vulnerability against unseen perturbations. In contrast, our CL method incrementally learns new perturbations while preserving the"}, {"title": "Conclusion", "content": "Developing toxic detectors robust to evolving perturbations is challenging yet extremely important for safe online communication. For this purpose, we introduce the problem of continual learning jailbreak perturbation patterns into toxicity detection. To tackle this problem, we construct a perturbation pattern-aware toxicity detection dataset perturbed by 9 types of patterns. Using this dataset, we reveal the vulnerability of current methods against toxic text produced by dynamically changing patterns. We then propose a simple but effective incremental learning method as the benchmark which can continually adapt to changing perturbations.\nExtensive experiments have proven the potential of our proposed continual learning paradigm, and we wish it will inspire more efforts towards this direction."}, {"title": "Appendix", "content": null}, {"title": "Details for Implementation", "content": "In all experiments, we use the bert-base-uncased model to encode the sentences, where the maximum sequence length is set to 360. The parameters of the models are updated using the AdaW optimizer with a learning rate of 2e-5. Additionally, the random seed for all experiments is set to 0, and training is conducted for 35 epochs with an early stopping strategy applied. All experiments are performed on the A800 GPU."}, {"title": "Details for Baselines", "content": "Details for existing normal detectors. ToxicBERT and ToxROBERTa are fine tuned on the ToxiGEN dataset. We adopt all three variations of detectors released by the Detoxify team, including Original, Multilingual, and Unbiased, which are fine tuned on the Jigsaw dataset. For LLama3 and ChatGPT, we instruct them to determine whether the text is toxic by asking the LLM 'Is the text toxic? Note only answer Yes or No.'.\nDetails for continual learning methods. LFL reduces knowledge forgetting by regularizing the parameters between old and new classifiers.\nEWC uses the Fisher matrix to assess parameter importance and applies weighted regularization based on that importance.\nLWF transfers knowledge by having the old model generate pseudo-labels for the new model.\nEPI trains task-specific prefix parameters for each task and identifies the task IDs of test samples to select the appropriate prefix parameters for classification.\nCEAR learns memory-insensitive prototypes and uses memory augmentation to reduce overfitting and enhance performance.\nWe reproduce the results for these baselines with officially released codes by the authors. The number of replay samples is set to 5 for continual learning methods."}, {"title": "Datasets", "content": null}, {"title": "Introduction to Existing Datasets", "content": "Jigsaw is composed of English content collected from online platforms like Twitter.\nSBIC includes social media posts that focus on social bias against specific groups, such as 'Muslims.'.\nHateXplain is a collection of online social posts aimed at researching explainable hate speech detection.\nToxiGEN is the first English dataset generated by large language models (LLMs) for toxicity detection, created in response to the growing use of LLMs.\nTOXICN is a Chinese toxicity dataset designed to support fine-grained detection of toxicity in Chinese, including the type and expression of toxicity.\nMuTox and ToxCMM are multimodal toxicity detection datasets that include not only text but also audio (MuTox) and video (ToxCMM).\nHatemojiBuild and OTH datasets focused on emoji-based and homoglyph-based perturbed toxicity detection, aimed at enhancing the robustness of detectors against toxic text that includes emojis or homoglyphs.\nNoisyHate combines five types of perturbed toxic text to strengthen the robustness of detectors against various perturbations."}, {"title": "The Implementation Details for Dataset Construction", "content": "In perturbation implementation, we employ the online toxic words collected by Google, which is available by https://github.com/coffee-and-fun/google-profanity-words/tree/main. we utilize punctuation from the Python package \u2018string' and employ the 'homoglyphs' python tool to get homoglyphs to replace the characters. In addition, we collect the abbreviations/slangs from the online resources, including 'onlineslangdictionary.com', 'slang.net', 'www.acronymfinder.com' and 'acronymsandslang.com'."}, {"title": "Format for our Dataset", "content": "The format for our DynEscape dataset is shown"}, {"title": "Perturbation Algorithm", "content": "Alg. 1 presents the algorithm for the perturbation procedure."}, {"title": "Supplement Experiments", "content": "To further investigate the generalization performance of detectors across different perturbations, we evaluate detectors fine tuned on specific perturbations using texts with mixed perturbations (e.g., texts perturbed by both Insert and Remove). \nFrom , we find that the performance under most of generalization settings drops, even when the mixed perturbations include the pattern on which the detector has been trained. This indicates that fine tuning detectors on specific perturbations is not sufficient, as users can still bypass detection by altering slightly the type of perturbation."}]}