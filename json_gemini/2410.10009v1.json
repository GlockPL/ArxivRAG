{"title": "Enhancing Peer Review in Astronomy:\nA Machine Learning and Optimization Approach to Reviewer Assignments for ALMA", "authors": ["JOHN M. CARPENTER", "ANDREA CORVILL\u00d3N", "NIHAR B. SHAH"], "abstract": "The increasing volume of papers and proposals undergoing peer review emphasizes the\npressing need for greater automation to effectively manage the growing scale. In this\nstudy, we present the deployment and evaluation of machine learning and optimization\ntechniques for assigning proposals to reviewers that was developed for the Atacama\nLarge Millimeter/submillimeter Array (ALMA) during the Cycle 10 Call for Proposals\nissued in 2023. By utilizing topic modeling algorithms, we identify the proposal topics\nand assess reviewers' expertise based on their historical ALMA proposal submissions.\nWe then apply an adapted version of the assignment optimization algorithm from Peer-\nReview4All (Stelmakh et al. 2021a) to maximize the alignment between proposal topics\nand reviewer expertise. Our evaluation shows a significant improvement in matching\nreviewer expertise: the median similarity score between the proposal topic and reviewer\nexpertise increased by 51 percentage points compared to the previous cycle, and the\npercentage of reviewers reporting expertise in their assigned proposals rose by 20 per-\ncentage points. Furthermore, the assignment process proved highly effective in that no\nproposals required reassignment due to significant mismatches, resulting in a savings of\n3 to 5 days of manual effort.", "sections": [{"title": "1. INTRODUCTION", "content": "Peer review is the cornerstone of modern scientific research, playing a crucial role in the allocation\nof billions in grant funding and providing access to high-cost facilities such as supercomputers and\ntelescopes, both ground- and space-based. Traditionally, this process relies on an invited panel\nof experts to evaluate and recommend which proposals should be accepted. This model has been\nwidely adopted across the scientific community, including by major astronomical observatories. One\nprominent example is the Atacama Large Millimeter/submillimeter Array (ALMA), which utilized\npanel-based peer review when it began science operations in 2011. As the largest ground-based\nobservatory and the most sensitive telescope ever constructed for high-resolution imaging of the\nsubmillimeter sky, ALMA is in high demand, making an efficient peer review process essential to its\noperations.\nThe increasing volume of proposals submitted to major observatories has put the traditional panel-\nbased peer review process under strain. The Hubble Space Telescope (HST), the James Web Space"}, {"title": "2. OVERVIEW OF ALMA PROPOSAL ASSIGNMENT IN PREVIOUS CYCLES", "content": "ALMA began offering observing time in 2011 and solicits proposals annually. The number of\nsubmitted proposals increased steadily from 919 proposals in 2011 (Cycle 0) to a peak of 1836 in 2018\n(Cycle 6), and currently about 1700 proposals are submitted annually. PIs submit their proposals\nunder one of five scientific categories. Within each category, they also select one or two keywords\nfrom a list provided by ALMA that best describe the topic of their proposal.\nInitially, ALMA employed a panel-based review process, where panels of 6 to 9 experts for each of\nthe five scientific categories evaluated around 100 proposals each. Multiple panels were formed per\ncategory as needed depending on the number of submitted proposals. However, this system became\nstrained as the number of proposals increased, prompting ALMA to implement a distributed peer\nreview process. This system was first used in 2019 (Cycle 7) for a \"supplemental\" call to fill time on\nthe Morita Array (also known as the Atacama Compact Array) that was not allocated in the main\ncall (Carpenter et al. 2020). ALMA used distributed peer review in the main call for the first time in\n2021 (Cycle 8) for a subset of proposals, and adopted the distributed process on all proposals except\nfor ALMA Large Programs in 2022 (Cycle 9).\nIn the distributed peer review system, the Principal Investigator (PI) of each proposal nominates a\nteam member to serve as a reviewer. Each reviewer assesses 10 proposals, and each proposal receives\n10 reviews. Reviewers rank the proposals from 1 (strongest) to 10 (weakest), and the final overall\nrankings are based on the averaged individual rankings. The process is detailed by Donovan Meyer\net al. (2022). The number of proposals evaluated by this process was 1497, 1729, and 1635 in Cycles\n8, 9, and 10, respectively.\nInitially, ALMA used a \u201ccategory\u201d and \u201ckeyword\u201d approach to match proposals with reviewers in\nthe distributed system. Reviewers specified their scientific expertise using the same list of keywords\nused by PIs to characterize their proposals. A series of \u201crules\u201d were specified that established the\npriority order on how to assign proposals to reviewers (see Donovan Meyer et al. 2022). The top"}, {"title": "3. RELATED WORK", "content": "Other observatories and scientific fields face similar challenges as ALMA in that they receive hun-\ndreds if not thousands of proposals or papers that needed to be reviewed. Automated techniques are\nneeded to assign proposals or papers to reviewers in an optimal manner. This section describes some\nof these previous efforts. While not an exhaustive review of the subject, we highlight previous efforts\nthat most impacted ALMA's implementation of machine learning and assignment optimization. We\nbegin by discussing two prior deployments for proposal reviewing in astronomy, followed by deploy-\nments for paper review in computer science, and conclude the section by noting differences with the\npresent work."}, {"title": "3.1. Space Telescope Science Institute", "content": "The Space Telescope Science Institute has developed a tool called the Proposal Auto-Categorizer\nand Manager for Time Allocation Review (PACMan; Strolger et al. 2017, 2023) that provides a\nnumber of services for proposal evaluation for the Hubble Space Telescope (HST) and the James\nWeb Space Telescope (JWST). PACMan functionality includes using machine learning techniques"}, {"title": "3.2. European Southern Observatory", "content": "The European Southern Observatory (ESO) implemented machine learning techniques (Kerzendorf\n2019; Kerzendorf et al. 2020) for their pilot run of distributed peer review (Patat et al. 2019, see also\nJerabkova et al. 2023), which was run in parallel with a traditional panel review. The topic of each\nproposal was inferred from the proposal text using the TF-IDF model, while the reviewer expertise\nwas established based on their publication history in ADS. The dot product of the proposal vector\nand the reviewer expertise (\u201ccosine distance\u201d) then provided a measurement of the expertise. The\nproposals were then assigned to reviewers by optimizing the similarities of the assigned proposals.\nThe results show a high correlation between the predicted expertise and the self-reported expertise\nfrom the reviewers (Kerzendorf et al. 2020)."}, {"title": "3.3. Computer Science", "content": "Computer science has used distributed peer review for well over a decade to select papers submitted\nto conferences (for an overview of the review process, see Shah 2022). The largest conferences can\nhave over 13,000 submissions (NeurIPS 2023), and therefore automated tools are essential to handle\nso many papers. These conferences review full papers (and not just abstracts), are frequently a\nterminal venue for publication, and are rated at par or better than journals.\nHere is a short summary of the review process in such conferences. After papers are submitted\nto the conference, they are assigned to reviewers in a (semi-)automated fashion in a manner similar\nto that adopted at ALMA. First, a similarity score is computed via one or more of the following\nmethods: automated text-matching (Charlin et al. 2012; Mimno & McCallum 2007; Cohan et al.\n2020), computing the intersection between reviewer- and author-chosen subject areas, and by allowing\nreviewers to manually specify their interest and/or expertise for various papers (called \u201cbidding\";\nsee Fiez et al. 2020). If more than one of these methods are used, then the outcomes are combined to\nobtain a single similarity score for each reviewer-paper pair. Subsequently, an optimization procedure\nassigns reviewers to papers. The metric that is optimized depends on the conference, and includes\nmaximizing the total sum of similarity scores of the assignments (Goldsmith & Sloan 2007; Charlin\net al. 2012), or maximizing the similarities for the papers that are worst-off in terms of available\nexpertise (Stelmakh et al. 2021a; Kobren et al. 2019). We refer the reader to Stelmakh et al. (2023),\nSaveski et al. (2023), and Stelmakh et al. (2021a) for some evaluations of the reviewer assignment\nprocedures in computer science.\nAfter initial reviews are submitted by the reviewers, authors get a chance to rebut any incorrect\nstatements in the reviews (Gao et al. 2019; Liu et al. 2023). The reviewers get to read the rebuttal,\noptionally update their reviews, and engage in a discussion with each other on a typed forum (Stel-"}, {"title": "3.4. Influence on ALMA", "content": "HST, JWST, ESO, and computer science have shown the viability of machine learning and opti-\nmization techniques that inspired ALMA's efforts. ALMA's implementation differs from those used\nby other observatories in a few key ways. First, the reviewer's expertise was established based on\ntheir ALMA proposal history rather than ADS data. Each reviewer is guaranteed to be on at least\none ALMA proposal by design of the system, and a reviewer's proposals can be identified unambigu-\nously. Second, proposal categories and keywords are combined with reviewers' expertise keywords\nto refine assignment options, reducing the number of spurious matches that machine learning alone\nmight generate. Lastly, we adopted a computer science optimization algorithm for assigning pro-\nposals, with a few adjustments to fit ALMA's distributed peer review model. Section 4 outlines the\ndeployment of this new assignment process in detail."}, {"title": "4. DEPLOYMENT", "content": "This section describes the deployment of the proposal assignment process adopted by ALMA and\nimplemented for 2023 (Cycle 10). Figure 1 provides an overview of the process from the proposal\nsubmission by the PI to the ranking of proposals by a reviewer. The submitted proposal is passed to\nunsupervised machine-learning algorithms to infer the topics of the proposal (see Section 4.1). The\nPI designates a reviewer that will participate in the review process, where the scientific expertise of\nthe reviewer is established from their proposal history, also based on machine learning techniques (see\nSection 4.2). The similarities between the proposal topics and the reviewer's expertise is computed,\nand after flagging conflicts and disallowed proposal assignments, the proposals are assigned to the\nreviewers (see Section 4.3). The following subsections describe the deployment in more detail."}, {"title": "4.1. Topic Modeling", "content": "The first step in the proposal assignment process is to determine the topics of each proposal using\nnatural language processing. The PDF files for submitted ALMA proposals from Cycle 1 onwards\nwere converted to ASCII text using the python library pdftotext. The technical justification\nportion of the proposal that describes and justifies the setup of the observations is removed given it\nis largely technical in nature and much of the text will appear similar between proposals. The ASCII\ntext for the scientific justification was then combined with the proposal title and proposal abstract\nstored in the ALMA databases to define the proposal text. Pre-processing steps were applied to the\nresulting files using spacy (Honnibal et al. 2020) and custom functions to define and remove stop\nwords, standardize commonly used acronyms, convert British spelling of words to American spelling,\nand lemmitization. This process created a \u201cbag of words\" for each proposal.\nMany methods are available to classify documents from the bag of words. We mainly explored the\nuse of TF-IDF, Latent Semantic Indexing, and Latent Dirichlet Allocation (LDA). All three methods\ngave satisfactory and similar results based on how well they can find similar documents which are"}, {"title": "4.2. Reviewer expertise", "content": "The PI of a proposal must specify one person from the proposal team that will serve as a reviewer\nin the review process. Most PIs serve as the reviewer on behalf of their proposals, but in about 12%\nof cases, a co-Investigator (coI) is selected (Donovan Meyer et al. 2022). The reviewer must be a\nmember of the proposal team and registered within the ALMA system.\nIn previous applications of machine learning to proposal assignments at astronomical observatories\n(Kerzendorf 2019; Kerzendorf et al. 2020; Strolger et al. 2023), the reviewer expertise has been inferred\nbased on published abstracts or papers that are available in ADS. This approach has the advantage\nin that it contains an extensive record of a reviewer's publication history and therefore expertise.\nWhile the publication record of most authors can be inferred accurately from their last name and\nfirst initial (Milojevi\u0107 2013), erroneous matches will inevitably occur. With over 1000 reviewers\nparticipating in the review process, it is not practical to perform a manual inspection of the search\nresults to identifying any problematic identifications. While an Open Researcher and Contributor\nID (ORCID) can be used to identify authors uniquely, not all papers are tagged with an ORCID,\nand ALMA does not currently collect ORCIDs from its users. Also, ALMA allows students who are\nthe PI of proposals to serve as reviewers by specifying a mentor, and students may not yet have a\npublication record.\nInstead of using the publication record, ALMA used the proposal history of the reviewers, including\nthe current proposal cycle, to establish their scientific expertise. The main advantage of this approach\nis that reviewers can be uniquely identified by their ALMA user ID, and by design of the review\nprocess, each reviewer is guaranteed to be an investigator for at least one proposal. In addition, each\nproposal is tagged by the proposal PI with a scientific category and keywords that provides additional\ninformation on the topic of the proposal. As discussed below, these keywords can help identify which\nproposals should be used to determine the reviewer's expertise and to prevent proposal assignments\nin disparate scientific categories (see Section 4.3). A downside to using the proposal text is that the\nproposal history will not capture the full expertise of a reviewer, especially for senior astronomers\nwho are new users to ALMA and may have a substantial publication history.\nEach reviewer is associated with one or more proposals as either PI or col. The topical vector for\neach proposal associated with a reviewer is determined as described in Section 4.1. These topical\nvectors are averaged to determine the reviewer's overall expertise (r), which is also an N length\nvector. Similar to Equation 1, the cosine similarity between the ith proposal and the kth reviewer's\nexpertise is used to assess where the reviewer is suitable for evaluating that proposal:\n$S(p_i, r_k) = \\frac{p_i \\cdot r_k}{||p_i||_2||r_k||_2}$ (2)\nThe similarity score S(pi, rk) ranges from 0 to 1, where 0 indicates no alignment between the reviewer's\nexpertise and the proposal topic, and 1 indicates a perfect match. While the absolute value of the\nsimilarity score may not be crucial, the relative values should provide a meaningful comparison of\nhow well each proposal aligns with the reviewer's expertise.\nThis general approach works well for most reviewers, but the implementation also accounted for\nthose with expertise or proposal submissions across multiple categories. Special considerations were"}, {"title": "4.3. Proposal assignments", "content": "Given the formalism to compute the similarities between the proposal topics and reviewer's expertise\nas defined in Section 4.2, we proceed to assign proposals to reviewers. The goal is for each proposal\nto be reviewed 10 times and for each reviewer to be assigned 10 proposals. The main input is a\nmatrix that contains similarities between the reviewer expertise and all submitted proposals for a\ngiven cycle. The rows in the matrix correspond to each submitted proposal, and the columns in the\nmatrix correspond to each submitted proposal for the cycle. If an individual is reviewing on behalf\nof more than one proposal, they will appear as separate rows in the matrix.\nBefore assigning proposals, similarities are set to a negative number if a proposal assignment is\nnot permitted. There are two main reasons to restrict proposal assignments in this manner. First,\npotential assignments are flagged if the reviewer has a known conflict of interest on the proposal.\nConflicts of interest are defined if the reviewer is a PI or col on the potential proposal assignment\nproposal, the PI is deemed a close collaborator based on the proposal history (see Donovan Meyer\net al. 2022), or the reviewer identified a PI or col as a close-collaborator. In these cases, the similarities\nare set to a value of -100, which effectively prohibits the assignment of the conflicted proposal. The\nsecond case is if the potential assigned proposal is from a distinct scientific category compared to the\nreviewer's expertise (see Section 4.2). While such proposals will typically have low similarities, this\nadditional measure reduces the risk of a false positive match. In practice, the new similarity is set\nas \u22121.01 + S(p, r\u00b2). While a negative number does not preclude a proposal assignment, it makes it\nless likely and can be checked after the assignments are made.\nOnce the similarity matrix is established, proposals are assigned to reviewers. The goal is to\ndefine a metric that measures the success of these assignments and optimize it during the assignment\nprocess. One common approach is to maximize the total similarity scores between proposals and\nreviewers, which increases overall expertise in the review process. However, this method may lead\nto unfairness if certain types of proposals are disproportionately assigned to less-qualified reviewers.\nALMA instead adopted the approach used in PeerReview4All, which optimizes a \u201cleximin fairness\u201d\nmetric (Stelmakh et al. 2021a, code available at https://github.com/niharshah/peerreview4all). This\nmethod optimizes the similarity for the most \u201cdisadvantaged\" proposals; i.e., proposals that have\nthe least overall expertise among the reviewers. Once the most disadvantaged proposal's assignment\nis optimized, the algorithm moves to the next most disadvantaged, and so on. Proposals in fields\nwith many qualified reviewers are assigned last, as they have more reviewer options available. The\nmetric used by PeerReview4All is solved using the gurobi python optimizer (Gurobi Optimization,\nLLC 2023).\nThe PeerReview4All code was adapted to handle special cases. Student reviewers specify a mentor,\nwho may also serve as a reviewer for other proposals. To ensure fairness, we required that the\nproposal sets for students and mentors be disjoint. After each round of assignments, the similarity\nmatrix was updated to mark as conflicted any proposals previously assigned to a reviewer or their\nmentor. Additionally, conflicts were updated for reviewers assigned multiple sets, ensuring they were\nnot assigned the same proposal twice.\nTo prevent \"niche\u201d proposals from repeatedly being reviewed by the same group, further restrictions\nwere imposed on the number of proposals a reviewer could be assigned within the same topic. These\""}, {"title": "5. EVALUATION", "content": "Given the framework for computing the similarity between proposals and reviewer's expertise de-\nscribed in Section 4, we now evaluate if the new process improved the proposal assignments to the\nreviewers compared to previous cycles. Recall that the ALMA reviewing cycle where the new assign-\nment approach was implemented is Cycle 10, and the two previous cycles we use for backtesting are\nCycle 8 and Cycle 9."}, {"title": "5.1. Backtesting similarity computation on Cycle 8 and Cycle 9 data", "content": "We backtest the effectiveness of the similarity scores (calculated as described in Section 4.2) by using\ndata from previous review cycles. Specifically, we examine whether the computed similarities between\na reviewer's expertise and the proposal topics accurately reflect the reviewer's expertise. During the\ndistributed peer review process used by ALMA in Cycles 8 and 9, reviewers were asked to self-assess\ntheir expertise on each assigned proposal by indicating if the assigned proposal is in their field of\nexpertise (i.e., the review considers themselves an \u201cexpert\"), they are somewhat knowledgeable, or\nthey have little to no knowledge of the topic. If our similarity scores are a reliable measure of\nexpertise, we would expect that reviewers who self-identify as experts in a proposal topic will, on\naverage, have higher similarity scores compared to those who report having little or no knowledge of\nthe topic.\nHowever, similarity alone is not a perfect indicator of expertise. Some assignments had near-zero\nsimilarity scores, yet reviewers reported they were experts, while other proposals had similarity\nscores close to 1, but reviewers expressed limited or no knowledge of the subject. This may point to\nboth the inherent imprecision in survey data and the limitations of assessing expertise based solely\non proposal history. Despite these limitations, the correlation between self-declared expertise and\nsimilarity suggests that these metrics can help improve the alignment of proposals with a reviewer's\nscientific background.\""}, {"title": "5.2. Comparing Cycle 10 and Cycle 9 assignments, given computed similarities", "content": "In this section, we analyze the role of the optimization procedure for computing the assignments\n(Section 4.3) given the computed similarities. In Cycle 9, proposals were assigned to reviewers based\non the proposal category and keywords (Donovan Meyer et al. 2022); i.e., the \u201cold\u201d algorithm. In\nCycle 10, we used the similarities and assignment algorithm described in Section 4; i.e., the \u201cnew\u201d\nalgorithm. In order to make a fair comparison, we recompute similarities for Cycle 9 using the\nsoftware developed for Cycle 10. In the analysis in this subsection, we use these similarities. In both"}, {"title": "5.3. Cycle 10 reviewer survey", "content": "The increased similarity of the proposal assignments was also reflected in the reviewer survey, where\nreviewers provided a self-assessment of their expertise on each proposal assignment. shows a\nhistogram of the results from Cycles 8, 9 and 10. With the new assignment process adopted in Cycle\n10, the percentage of reviewers indicating they were an expert in the assigned proposal increase to\n65% in Cycle 10 from 45% in Cycle 9. Similarly, the percentage of assignments where the reviewers\nindicated they had little or no knowledge on the proposal declined to 5% in Cycle 10 from 10% in\nCycle 9."}, {"title": "5.4. Topological accuracy of the proposal rankings", "content": "To estimate the relative accuracy of the actual assignments in Cycle 10, we set the error in the\nactual assignments to unity. We then ran simulations where the reviewers were assigned proposals\nat random across any scientific category, subject to the constraint that each reviewer is assigned 10\nproposals and each proposal is assigned to 10 reviewers. The random assignment strategy is known\nto be optimal (Shah et al. 2016) in terms of topology when all reviewers have identical expertise. We\nfind that the relative uncertainty is 0.86 for the random assignment. Thus, optimizing assignments for\nexpertise increases the uncertainty in final outcomes by only 14% based on topology of the proposal\nassignments."}, {"title": "6. DISCUSSION AND CONCLUSIONS", "content": "The implementation of the new assignment process in ALMA Cycle 10, leveraging machine learn-\ning (Blei et al. 2003) and optimization (Stelmakh et al. 2021a) algorithms, has successfully met\nthe high-level objectives set for the system. The similarity measures between the proposal topics\nand reviewer's expertise computed from their proposal history are correlated with the reviewer's\nself-reported expertise to demonstrate the similarity measures have predictive value. With the new"}, {"title": "6.1. Limitations", "content": "We now discuss some limitations of our study. Firstly, a portion of our findings is based on review-\ners' self-reported expertise regarding the proposals they assessed. Such self-reports are susceptible\nto various biases, suggesting that future research should incorporate additional evaluative metrics.\nSecondly, our analysis largely compares the current cycle to previous ones. There is a possibility that\nother significant changes, such as shifts in the types of proposals or the reviewer demographic, could\nhave influenced the results. To counteract potential confounding factors, we conducted an analysis\nusing the current algorithms on data from previous cycles, which affirmed the consistency of our\nresults. Additionally, the firsthand experience of the first two authors, who have overseen this review\nprocess for several cycles, indicates no major changes during this period.\nOur experience found that while the machine learning algorithms were quite successful in identi-\nfying similar proposals, a small fraction of assignments were matching proposals triggered on some\nindividual words that are relatively rare, but can appear across different scientific categories (e.g.,\npolarization, maser, corona). Some of these mismatches could likely be reduced with improved\nmachine learning algorithms (e.g., using transformer-based language models and/or fine tuning to\nALMA-relevant corpora), but the possibility of mismatches will always remain. In the current de-\nployment, by using the keywords of the proposals and reviewer expertise, inappropriate assignments\nwere flagged in advance and increased the robustness of the overall process."}, {"title": "6.2. Open problems", "content": "Finally, we outline several open problems in this exciting research domain. First, in assigning\nreviewers to the proposals, there is a tension between expertise and topology of assigned reviewers,\nas discussed in Section 5.4. While previous works have explored these factors independently, a\npromising direction is to study them jointly and establish the optimal tradeoff.\nSecond, unlike many other review systems that rely on ratings, ALMA requires each reviewer to\nrank the proposals they evaluate. Determining the most effective evaluation method is a key area\nof interest, closely tied to the reviewer assignment process. Rankings present challenges for cross-\nreviewer comparisons due to their independence, whereas ratings are subject to calibration issues\nsuch as varying strictness or leniency across reviewers (Ge et al. 2013; Roos et al. 2012; Wang &\nShah 2019). Additionally, research is ongoing into methods for eliciting both rankings and ratings\nand effectively integrating them (Pearce & Erosheva 2022; Liu et al. 2022).\nThird, systems like ALMA accumulate multiple cycles of reviews, presenting an opportunity to\ndevelop methods that leverage these data to continually improve reviewer assignments."}, {"title": "APPENDIX", "content": "In Section 4.2, we outlined the general approach for identifying the expertise of each reviewer.\nWhile this basic method works well for the majority of reviewers, several practical issues can arise.\nSome reviewers are involved in proposals across distinct topics, either as PIs or coIs. Combining all\ntheir proposals into a single expertise vector may dilute their specific areas of expertise, resulting in\ndecreased similarity when comparing against proposals focused on only one topic. Additionally, some\nreviewers claim expertise in a broad range of topics and submit proposals across multiple categories.\nAveraging their expertise based on all submitted proposals may lead to a dominance of the category\nthat contains the most proposals. Moreover, some reviewers assert expertise in categories that differ\nfrom those of their submitted proposals, indicating a preference for reviewing in certain topics.\nAssuming the reviewers' declared expertise at face value and including all proposals blindly could\nresult in assignments outside their actual areas of expertise. Finally, there can be false positives\narising from the LDA topic modeling, where a reviewer's expertise may appear similar to a proposal\ntopic, but upon closer inspection, the proposal may not be relevant to the reviewer's expertise. Such\nfalse matches can occur due to common words or phrases shared across multiple categories, such as\n\"masers\" and \"polarization.\" This suggests that the topics identified in the LDA model may require\nfurther refinement.\nTo address these challenges, we utilize the keywords associated with a reviewer's expertise to select\nthe proposals that will be used to infer their expertise. The overall goal is to evaluate a reviewer's\nexpertise based on proposals that are as closely aligned as possible with the topic of the parent\nproposal, while also respecting the reviewer's self-declared areas of expertise. Each ALMA proposal\nis submitted to a specific scientific category, defined as the parent category. We have predefined\nsimilar categories that significantly overlap with the parent category in terms of scientific content\n(see Figure 2). For instance, Categories 1 and 2 are considered similar because they both pertain\nto galaxies, while Categories 3 and 4 are similar due to their focus on the formation of stars and\ncircumstellar disks. In contrast, Categories 1 and 2 are distinct from Categories 3, 4, and 5, and\nCategories 3 and 4 are distinct from Categories 1, 2, and 5. Category 5 is viewed as distinct from all\nother categories due to its diverse set of keywords.\nWith these definitions in place, a reviewer's expertise is constructed based on whether they have\nprovided their expertise keywords and whether they are the PI or col on proposals. outlines\nthe sequence of steps used to infer the reviewer's expertise. In the first step, if a reviewer has\ndeclared expertise on any keywords in a parent category C and has served as the PI on proposals in\nthat category during any cycle, the expertise vector is constructed from the proposals in Category C\nwhere they are the PI. If the conditions of Step 1 are not met, Step 2 checks whether the reviewer\nhas expertise in a similar category and has served as the PI on proposals in that category. If those\nconditions are also not met, Step 3 checks if the reviewer has expertise in a distinct proposal category\nand has served as the PI on proposals in those distinct categories. If no suitable proposals are found\nin these steps, the process is repeated by checking proposals in which the reviewer is a col (Steps\n4-6).\nIf a reviewer has not provided their expertise, Steps 7-9 examine categories in which they have\nsubmitted proposals as the PI, followed by Step 10, which considers categories where they have served"}]}