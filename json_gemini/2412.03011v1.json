{"title": "Human Multi-View Synthesis from a Single-View Model: Transferred Body and Face Representations", "authors": ["Yu Feng", "Shunsi Zhang", "Jian Shu", "Hanfeng Zhao", "Guoliang Pang", "Chi Zhang", "Hao Wang"], "abstract": "Generating multi-view human images from a single view is a complex and significant challenge. Although recent advancements in multi-view object generation have shown impressive results with diffusion models, novel view synthesis for humans remains constrained by the limited availability of 3D human datasets. Consequently, many existing models struggle to produce realistic human body shapes or capture fine-grained facial details accurately. To address these issues, we propose an innovative framework that leverages transferred body and facial representations for multi-view human synthesis. Specifically, we use a single-view model pretrained on a large-scale human dataset to develop a multi-view body representation, aiming to extend the 2D knowledge of the single-view model to a multi-view diffusion model. Additionally, to enhance the model's detail restoration capability, we integrate transferred multimodal facial features into our trained human diffusion model. Experimental evaluations on benchmark datasets demonstrate that our approach outperforms the current state-of-the-art methods, achieving superior performance in multi-view human synthesis.", "sections": [{"title": "1. Introduction", "content": "Human multi-view image generation has received considerable attention in recent years. This approach enables various applications, including AR/VR [32, 43], 3D content creation [25], and virtual try-on [45]. Specifically, the task of human multi-view generation aims to synthesize novel human views from a single input image. The core challenge is to generate consistent multi-view images while minimizing ambiguity as much as possible."}, {"title": "2. Related Work", "content": "Multi-view Image Generation. Multi-view generation has become a significant area of research in recent years, with diffusion-based models leading the way. Numerous studies have focused on synthesizing novel views of objects using these models [26-29, 38]. Trained on large-scale 3D datasets [5, 6], researchers have explored novel view synthesis from a single image. A foundational contribution to this field is Zero123 [27], although its outputs often lack consistency across views. Subsequent works, such as Zero123++ [38] and One2345 [26], aim to enhance the correlations among predictions and generate more intricate 3D objects. Additionally, SyncDreamer [28] struggles with geometry and texture quality. Although Wonder3D [29] improves texture reconstruction, it still faces challenges with thin structures. Champ [58] combines SMPL parameters with depth, normal, and semantic maps to enhance visual accuracy but falls short in rendering facial details due to the limitations of the SMPL model. Our method focuses on refining these tiny facial structures.\nDiffusion Models for Personalization. Diffusion models [12, 19, 51] have demonstrated remarkable capabilities in generating personalized human representations, making them widely adopted in the personalized generation process aimed at preserving essential features of specific individuals across different views. Many methods for personalized synthesis primarily focus on accurately capturing the identity of the subject. One notable approach is DreamBooth [34], which fine-tunes a pre-trained text-to-image model to associate a unique identifier with a specific individual. To handle multiple subjects, Custom Diffusion [18] utilizes a combination of various fine-tuned models, thereby enhancing personalization capabilities. Despite the success of these methods, their efficiency often falls short of expectations. To tackle this challenge, HyperDreamBooth [35] introduces a hypernetwork that efficiently generates a compact set of personalized weights from a single image of an individual, significantly improving inference speed while maintaining high-quality generation. Similarly, PhotoMaker [24] employs a stacked identity embedding as a unified representation, achieving impressive generation results while substantially reducing processing time.\nFace Image Restoration. Face Restoration has garnered significant interest among researchers [2, 44, 53]. Image"}, {"title": "3. Method", "content": "In this section, we explain the overall framework of our proposed approach, as illustrated in Figure 2. Given an input human image, the goal of our method is to synthesize novel views with fine-grained and consistent information about the person from different perspectives. In Section 3.1 we present an overview of the latent diffusion model to establish the foundational concepts necessary for the subsequent discussions. Section 3.2 describes the generation process of the coarse human results, in which 2D human information from a pretrained large-scale model is extended to the multi-view scope to alleviate the problem of limited 3D human data. Additionally, normal maps rendered from SMPL are employed to establish a representation of body shape and pose. In Section 3.3, we present the process of enhancing facial representation with multimodal facial features fused."}, {"title": "3.1. Preliminary", "content": "Diffusion Models. Our method is based on Diffusion Model [12, 40, 42], a class of promising generative models that introduce noise to data and then reverse this process to generate data from the noise. This procedure, known as the forward process or noise-infection procedure, is formalized as a Markov chain: $q(X_{1}, ..., X_{T}|X^{0}) = \\prod_{t=1}^{T}q(X_{t}|X_{t-1})$. It adds small Gaussian noise to latent images at the previous time step as follows: $q(X_{t}|X_{t-1}) = N(X_{t}; \\sqrt{1 - \\beta_{t}}X_{t-1}, \\beta_{t}I)$, where $\\beta_{t}$ is a small positive constant and $I$ is the identity matrix with the same dimension as $X_{t-1}$.\nThe reverse process is defined as the conditional distribution $p_{\\theta}(X_{(0:T-1)}|X_{T}, c)$, where $c$ is the condition guidance. The entire reverse process can be factorized into multiple transitions based on the Markov chain:\n$P_{\\theta}(X^{0}, ..., X^{T-1}|X^{T}, c) = \\prod_{t=1}^{T}P_{\\theta}(X^{t-1}|X^{t}, c)$.    (1)\nIn the reverse process, the Markov kernel is parameterized as: $P_{\\theta}(X_{t-1}|X_{t},c) = N(X_{t-1};\\mu_{\\theta}(X_{t},t), \\sigma^{2}_{\\theta}(X_{t}, t)I)$,"}, {"title": "3.2. Transferred Body Representation", "content": "In this module, we consider how to generate consistent multi-view images with only limited 3D human data available. The overall framework is shown on the left side of the Figure 2. Specifically, The initial input to network in the stage consists of two types of images: human image and SMPL normal maps. SMPL is estimated by the input image and then rendered to different views to generate corresponding normal maps. Since normal maps contain limited fine-grained information, they can only be used to guarantee the coarse detail such as body shape and pose."}, {"title": "3.2.1. Single-view to Multi-view Transfer Module", "content": "This module mainly consists of two UNets, aiming to adopt 2D human features to benefit the multi-view generation process and alleviate the problem of limited 3D human data. The core objective of our module is to represent the input image with detailed body information that can be injected into our multi-view human UNet. Trained on a large-scale dataset CosmicMan-HQ with 6 million high-quality real-world human images, CosmicMan [20] provides rich human-related 2D information that can be transferred to multi-view generation process. We adopt a framework identical to the denoising UNet, inheriting weights from the CosmicMan, with weight updates conducted independently for each. During training, the input image is first encoded by a VAE encoder. The VAE embeddings are then fed into the single-view human UNet, termed as single-Net. Encoded by the single-Net, appearance information in the input image can be fully parsed. Specifically, appearance information is represented by a set of normalized attention hidden states for the middle and upsampling blocks of the UNet and then is written into the memory box. Our multi-view human UNet, termed as multi-UNet is implemented based on Wonder3D [29], which introduces a cross-domain attention mechanism to ensure consistency between the generated images. The multi-UNet reads stored features from the memory box, transferring 2D knowledge to the multi-view domain. These features are passed to the spatial self-attention layers in the multi-UNet blocks by concatenating each feature with the original UNet self-attention hidden states to inject the appearance information."}, {"title": "3.2.2. Normal Map Guidance", "content": "With only one image as input, it is insufficient to represent the 3D geometry of the human due to the occlusion and view-dependent effects. Therefore, additional guidance introduced to the model is necessary. As a 3D parametric human model, SMPL offers a unified representation that encompasses both shape and pose. Given an input image of a human body, we can adopt the 4D-Humans [10] to estimate the parameters of SMPL. We can model the coarse body shape and pose from the different target perspective by rendering the SMPL mesh to obtain 2D representations. Specifically, we render normal maps from SMPL with the target poses to guide multi-view human UNet in generating human novel views. All encoded normal maps serve as additional conditions guiding the generation process."}, {"title": "3.3. Transferred Face Representation", "content": "Since a single image and SMPL normal maps cannot provide all the information needed for generating high-quality novel views, the results from previous module often fail to reconstruct some details, such as human face. To address this problem, we propose integrating 2D and 3D facial features to learn a face representation. Specifically, we regard the refinement process as a face restoration problem and propose a novel framework to solve this problem. To leverage pretrained information and keep the architecture neat and powerful, we adopt the pretrained multi-UNet as the backbone. There are two main differences between these two stages. On the one hand, the condition of the model only contains face images from different views. On the other hand, we propose a novel facial feature enhancement module to increase the performance of face refinement. The implementation details are as follows."}, {"title": "3.3.1. Face Segmentation and Restoration", "content": "To align with the face restoration task, we first identify the face region by employing an existing framework, RetinaFace [37], which allows us to accurately detect and crop the facial area from the input image. Then, we apply a super-resolution model to enhance the resolution of the cropped face images, scaling them to match the resolution of the input image. This consistency in resolution is crucial, as it enables the model to capture finer facial details and produce high-quality results. Following the approach outlined in GPEN [50], we then restore the refined face images back into the original image context using the mask corresponding to the cropped face region. This ensures that the enhanced facial details are seamlessly integrated into the overall image, preserving the integrity of the original scene while improving the quality of the facial features. The process allows for effective face restoration, enhancing both the fidelity and realism of the synthesized faces in multi-view human synthesis tasks."}, {"title": "3.3.2. Face Representation Learning", "content": "To gain fine-grained facial detail, it is not sufficient to rely solely on the guide image from the previous module. 2D and 3D priors are complementary to each other. In this paper, differing from existing face restoration methods [3, 30, 57], we first propose integrating the 2D and 3D facial features into the 3D human UNet to ensure the fidelity, authenticity, and identity consistency. We follow [24] to extract ID-related embeddings from the input image as the 2D prior, providing a representation of the person's ID in the semantic space. Compared to 2D priors, 3D priors are useful in providing robust facial structures, which is essential for further restoration of fine details. Same with [57], we follow the practice the D3DFR [7] to predict the coefficients of 3D morphable face models (3DMMs [1]). With predicted 3DMM coefficients, the 3D face shape $S$ and albedo texture $T$ are presented as:\n$\\hat{S} = \\hat{S}(\\alpha, \\beta) = S + B_{id}\\alpha + B_{exp}\\beta$,    (3)\n$\\hat{T} = \\hat{T}(\\delta) = T + B_{t}\\delta$,    (4)\nwhere $\\alpha$, $\\beta$ and $\\delta$ are the coefficients of the Basel Face Model (BFM) for identity, expression and BFM texture respectively. $S$ and $T$ are the mean face shape and albedo texture. $B_{id}$, $B_{exp}$ and $B_{t}$ denote the PCA bases of identity, expression and texture, illumination and face pose. Estimated with 3DMM, it is easy to project the 3D face onto 2D images according to the given poses based on a differentiable mesh renderer. The process of rendering can be represented as:\n$I_{3d} = F_{render}(\\hat{S},p)$,    (5)\nwhere $p$ is the given pose. Subsequently, the rendering facial 3D images $I_{3d}$ are encoded by ResBlock, same as SR3 [36]. This process can be expressed as:\n$F_{3d} = ResBlock(I_{3d})$.    (6)\nIn order to take advantage of the strengths of the 2D and 3D priors and obtain a better representation of the face features, we propose integrating 2D and 3D features in the semantic space. We denote IP embedding as $F_{2d}$. Inspired by PhotoMaker [24], we use two MLP layers to fuse the $F_{2d}$ and $F_{3d}$. Finally, the combined features, denoted as $F_{align}$, are fed into the multi-level feature extraction module in 3D human UNet to extract facial details, thereby capturing both structural and identity information within the priors."}, {"title": "3.4. Training and Inference Details", "content": "Training. Our method consists of two distinct stages. In the first stage, our method aims to learn a body representation leveraging pretrained sing-view human model. In this phase, we jointly optimize the parameters of the single view human model and multi-view human UNet, while maintaining the weights of the VAE encoder and decoder in a frozen state, as well as the CLIP image encoder. The training data for this stage consists of pairs of images from different viewpoints. The input image is the front view of the character, and the target image consists of six different perspectives. Normal maps are rendered from the estimated SMPL model based on target poses. For the transferred face representation learning, we train to update the weights of the multi-view human UNet and the face embedding module. We extract the IP embedding and 3D face features from the input image to enhance face representations.\nInference. During the inference process, novel-view human synthesis is performed on a single image by transferring knowledge to build superior body and face representations. The transferred body representation is utilized to provide a base body shape for generation. Additionally, the transferred face representation provide structure-accurate 3D priors and IP-rich 2D priors to help restore facial details. Both representations serve as powerful means to synthesis high-quality and consistent human novel views with only one image input and limited training data available."}, {"title": "4. Experiments", "content": "In this section, we assess the effectiveness of our method by evaluating the novel-view human synthesis task. Specifically, we compare our method with previous state-of-the-art models through quantitative and qualitative comparisons. To further analyze its modeling capability, we directly extend our method trained on the THuman2.1 to the 2K2K dataset [11], enabling a more detailed and robust assessment. Additionally, we perform ablation studies to investigate the individual contributions and overall impact of each component in our framework, offering deeper insights into its effectiveness for this task."}, {"title": "4.1. Setup", "content": "Implementation Details. To generate high-quality and consistent novel views, our method is implemented on the backbone of Wonder3D [29]. The overall framework is optimized using Adam [17] on 1 NVIDIA A800 GPU for about 4 days with a batch size of 4. We set the learning rage as le - 4 for the trainable modules. The training regimen is structured in two phases. For the first phase, we train the 3D human UNet to generate 6 views of full human body. In the second phase, we train the module to refine the face area using the cropped faces input from the first phase. It's worth noting that in certain views, such as back view, face area cannot be detected. For a uniform training data format, we choose only three of these perspectives including the front view, the front left view and the front right view, for refinement. For both phases, the resolution of the training data is resized to 512 \u00d7 512. During the inference stage, we perform 50 steps of the DDIM sampler [41] and the scale of the classifier-free guidance is set to 3.\nEvaluation metrics. To demonstrate the superiority of our approach, we evaluate its performance using the established metric utilized in existing research. Specifically, we assess the novel-view human synthesis quality with three widely used metrics: Peak Signal-to-Noise Ratio (PSNR) [13], Structural Similarity Index (SSIM) [48] and Learned Perceptual Image Patch Similarity (LPIPS) [54]. PSNR is widely used to measure the fidelity of the reconstruction in terms of pixel-level accuracy. SSIM evaluates the perceptual similarity between images by considering luminance, contrast and structure, thereby offering insights into the perceived quality of the synthesized images. LPIPS is a perceptually aligned metric that utilizes deep network features to capture high-level similarities and better correlates with human judgments on image similarity."}, {"title": "4.2. Comparisons", "content": "Dataset. We conduct main experiments on widely used 3D human datasets, called THuman2.1, which includes approximately 2500 high-quality human scans [52]. Specifically, our training dataset comprises 2350 scans from THuman2.1 and the rest for validation. To demonstrate the generalization ability of our method, we directly apply our pretrained model to another common 3D human dataset 2K2K [11]. It is a large-scale dataset containing high-quality 3D human model of 2,050 individuals.\nBaselines. We conducted a comprehensive evaluation of our method against state-of-the-art novel-view human synthesis approaches, including SyncDreamer [28], One2345 [26], Zero123++ [38], Wonder3D [29], and Champ [58]. Among these, SyncDreamer, One2345, Zero123++, and Wonder3D have been demonstrated to be effective for multi-view object synthesis tasks, making them suitable benchmarks for comparison in our study. For our experiments, we fine-tuned the Zero123++ and Wonder3D models on the THuman2.1 dataset using their publicly available implementations to ensure consistency and fairness in evaluation. Since Champ is originally designed for human animation tasks using reference videos, we adapted its framework for the novel-view human synthesis task by modifying the input and output processing stages to align with the requirements of our evaluation. This enabled us to assess Champ's potential in the context of novel-view synthesis, despite its primary focus on animation-based tasks.\nEvaluation on THuman2.1 Datasets. The qualitative results of novel-view human synthesis are displayed in Figure 4. As shown in the results, most of the baselines can capture semantic information from the single input image to generate plausible novel views. However, most of them fail to reconstruct facial details. In comparsion, our method can restore more realistic face from the single image. Table 1 presents the quantitative results of our method and the baselines on the THuman2.1 dataset, focusing on commonly used metrics such as PSNR, SSIM and LPIPS. We generate six views of human from different perspective when calculating the related metrics. Table 1 shows that our method outperforms other baselines achieving higher PSNR and SSIM scores, and lower LPIPS values. In conclusion, both qualitative and quantitative results demonstrate the superior performance of our method, which achieves state-of-the-art performance in the multi-view human synthesis task.\nEvaluation on 2K2K Datasets. To evaluate the generalization capability of our model, we directly conduct experiments on the 2K2K [11] dataset using pretrained model from THuman2.1. From Figure 6, we can see that our approach achieved satisfactory performance on this dataset, demonstrating its robustness and adaptability to new data distributions beyond the training set. Notably, compared to other baselines the model maintained high accuracy and visual consistency across multiple views, reflecting its strong capacity to capture complex human structures and details"}, {"title": "4.3. Ablation Studies", "content": "Transferred Body Representation. To demonstrate the effectiveness of our transferred body representation learning design, we conduct ablation studies by training our model to generate coarse outputs both with or without the knowledge transfer module and normal guidance. As illustrated in Table 2, removing both modules results in reduced performance across all three metrics, thereby validating the effectiveness of our body representation design.\nTransferred Face Representation. To assess the impact of the face enhancement module, as illustrated in the bottom-right section of the Figure 2, we train the model with or without this module to refine the cropped faces. Based on the visualization results presented in Figure, we can conclude that without the face refinement module, the synthesized face struggles to preserve identity-preserving (IP) features and lacks detailed facial reconstruction.\nRepresentation Stage. To provide a comprehensive comparison of the two stages in our representation learning framework, we present visual results in Figure 5. These results demonstrate that the body representation learning stage successfully produces relatively plausible and coherent multi-view human body reconstructions, capturing the overall structure and form across different perspectives. However, the results from this stage fail to reconstruct facial detail. By incorporating the face representation stage, our method achieves significantly improved realism with more accurate facial details, enhancing the overall quality of the synthesized human representations. This two-stage approach thereby ensures both structural coherence and high-fidelity detail, particularly in critical areas like the face.\nLimitations. There are certain limitations to our model. One key challenge lies in accurately detecting the correct facial region from multiple perspectives, especially when occlusion occurs due to the character's pose. Therefore, our method cannot be adopted to enhance the facial area in all cases. On the other hand, potential mismatches between the refined face and the extracted face region may lead to inconsistencies. In Figure 8, we show some failure cases where incomplete restoration occurs."}, {"title": "5. Conclusion", "content": "In this paper, we first present a novel framework based on a diffusion model for human multi-view generation task. Compared with the previous methods, our method extends 2D information from large-scale human datasets to the multi-view scope to address the problem of limited human data. To regain high-quality faces, we incorporate 2D and 3D facial embeddings into the latent diffusion to refine the facial details. We conduct extensive experiments on custom human datasets, THuman2.1 and 2K2K, to show that our method can generate multi-view consistent and high quality results, achieving state-of-the-art performance."}]}