{"title": "Automatic 8-tissue Segmentation for 6-month Infant Brains", "authors": ["Yilan Dong", "Vanessa Kyriakopoulou", "Irina Grigorescu", "Grainne McAlonan", "Dafnis Batalle", "Maria Deprez"], "abstract": "Numerous studies have highlighted that atypical brain development, particularly during infancy and toddlerhood, is linked to an increased likelihood of being diagnosed with a neurodevelopmental condition, such as autism. Accurate brain tissue segmentations for morphological analysis are essential in numerous infant studies. However, due to ongoing white matter (WM) myelination changing tissue contrast in T1- and T2-weighted images, automatic tissue segmentation in 6-month infants is particularly difficult. On the other hand, manual labeling by experts is time-consuming and labor-intensive. In this study, we propose the first 8-tissue segmentation pipeline for six-month-old infant brains. This pipeline utilizes domain adaptation (DA) techniques to leverage our longitudinal data, including neonatal images segmented with the neonatal Developing Human Connectome Project structural pipeline. Our pipeline takes raw 6-month images as inputs and generates the 8-tissue segmentation as outputs, forming an end-to-end segmentation pipeline. The segmented tissues include WM, gray matter (GM), cerebrospinal fluid (CSF), ventricles, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala. Cycle-Consistent Generative Adversarial Network (CycleGAN) and Attention U-Net were employed to achieve the image contrast transformation between neonatal and 6-month images and perform tissue segmentation on the synthesized 6-month images (neonatal images with 6-month intensity contrast), respectively. Moreover, we incorporated the segmentation outputs from Infant Brain Extraction and Analysis Toolbox (iBEAT) and another Attention U-Net to further enhance the performance and construct the end-to-end segmentation pipeline. Our evaluation with real 6-month images achieved a DICE score of 0.92, an HD95 of 1.6, and an ASSD of 0.42.", "sections": [{"title": "1 Background", "content": "In recent years, there has been increasing interest in infant brain development, characterized by rapid brain growth, and evolving cognitive and motor functions. Neurobiological findings in early childhood suggest that early brain overgrowth may be associated with autism [1]. A recent study has also shown autism phenotypes are associated with variations in white matter development [2]. These studies illustrate the significance of acquiring volumetric measurements in infants, emphasizing the importance of such assessments in understanding developmental trajectories and potential correlations with various neurological and neurodevelopmental conditions.\nMagnetic resonance imaging (MRI), as an advanced non-invasive and high-resolution imaging technique, has underpinned remarkable advancements in the medical field. However, it faces challenges during the 6-month period of infancy. One primary challenge is the low tissue contrast between GM and WM due to rapid myelination development [3], making them difficult to distinguish on the MRI scans.\nAccurate brain tissue segmentations for morphological analysis are essential in infant studies, but manual labeling by experts is time-consuming and labor-intensive. Most existing image preprocessing software, such as FSL, is primarily designed for adult brain segmentation and performs poorly on infant brain images due to low tissue contrast and poor image quality [4]. Although the Infant Brain Extraction and Analysis Toolbox (iBEAT) [5] is widely used in infant research for its efficiency in processing and analyzing infant brain, it solely performs WM, GM and CSF segmentations.\nMachine learning (ML) based solutions have also been proposed, with segmentation algorithms achieving state-of-the-art performance on six-month infant brain datasets in the iSeg-2017 and iSeg-2019 challenges [3, 6]. However, no participants have achieved consistent performance across six-month infant datasets from multiple sites. Recently, domain adaptation (DA) techniques have been suggested as a potential method to improve performance in this task [6].\nDA methods have gained increasing interest in the medical imaging field to mitigate the distribution gap between training and test datasets. Generative adversarial network (GAN) and its extensions, such as CycleGAN [7], are widely employed for image-to-image translation to achieve image domain transformation between the target and source domains. A study proposed a 3D CycleGAN-Seg network that leveraged 24-month annotation to segment 6-month images [8]. Another study utilized annotation from the 12-month images to segment 6-month images using the proposed semantics-preserved GAN, and Transformer based multi-scale segmentation network [9]. However, all these studies focused solely on WM, GM and CSF segmentation. Other brain tissues, including the ventricles, hippocampus, and cerebellum, also play crucial roles in infant brain research.\nIn this paper, we introduce the first DA-based 8-tissue brain segmentation pipeline for 6-month infants utilizing the annotation information from neonatal images processed with the developing Human Connectome (dHCP neonatal structural pipeline [10]). Tissue segmentations available as part of the dHCP pipeline include: CSF, GM, WM, ventricle, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala. To obtain the best segmentation performance, we designed different combinations of"}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Data Acquisition and Preprocessing", "content": "Participants: 43 subjects participated in this study as part of the Brain Imaging in Babies (BIBS) project. The infants were born between 34 and 42 weeks of gestation, comprising 22 males and 21 females in the cohort. All participants were scanned as both neonates (see \"Neonatal scans\" below) and as 6-month-olds (see \"6-month-old infant scans\").\nNeonatal scans: T2w neonatal images were scanned between 37 and 44 weeks postmenstrual age (PMA). Images were acquired on a Philips Achieva 3T scanner equipped with a dedicated neonatal brain imaging system (NBIS) and a 32-channel neonatal head coil. T2w images were obtained using a T2w turbo spin echo (TSE) with fat suppression, TR = 12000 ms, TE = 156 ms, a resolution of 0.8 \u00d7 0.8 mm\u00b2 and a slice thickness of 1.6 mm.\n6-month-old infant scans: T1w and T2w 6-month images were scanned between 5 months to 7.7 months. Images were acquired on a 3T Phillips Scanner employed with a 32-channel adult head coil. T2w images were obtained using Spin Echo with TR = 15000 ms, TE = 120 ms, a resolution of 0.86 \u00d7 0.86 mm\u00b2 and a slice thickness of 2 mm. The Tlw images were acquired using Gradient Echo (GE) with TR = 12_ms, echo time TE = 4.6 ms, a resolution of 0.78 \u00d7 0.78 mm\u00b2 and a slice thickness of 1.6 mm. Fig. 1 visualizes a comparison between 44 weeks neonatal and 6-month infant images, along with their corresponding WM, GM and CSF histogram distributions.\nData preprocessing: We split the 43 participants into a training set (n = 33) and a test set (n = 10). The dHCP neonatal image preprocessing pipeline was applied to all 43 neonatal images, including motion correction, super-resolution reconstruction and tissue segmentation [11]. We utilized the 8 brain tissue segmentation outputs for subsequent experiments (excluding the \"skull\" output). The remaining segmentation"}, {"title": "2.2 Machine learning models in pipelines", "content": "To investigate the best solution for segmenting our target dataset (6-month data), we train and compare five different DL pipelines:\nAUNet (baseline): A \u039c\u039f\u039d\u0391I Attention UNet [12] was trained on the neonatal T2w images and labels, then applied directly to real 6-month T2w images.\nCyc+AUNet: CycleGAN was employed to transform neonatal T2w images into synthesized 6-month T2w images (neonatal images with 6-month intensity contrast). At the same time, an Attention UNet was trained on these synthesized 6-month images to predict their corresponding neonatal labels.\nCyc+AUNet+VM: Using the pre-trained Cyc+AUNet, we further employed VoxelMorph [13] to register synthesized 6-month images to the real 6-month image space (paired), and keep training the Attention UNet on the warped synthesized 6-month images to predict warped neonatal labels.\nCyc+AUNet+iBEAT: We employed the same strategy as for Cyc+AUNet, but replaced the WM, GM and CSF segmentation outputs of Cyc+AUNet with iBEAT segmentation outputs.\nCyc+AUNet+iBEAT+AUNet: Finally, we trained a second Attention UNet on the real 6-month images and their segmentation outputs from Cyc+AUNet+iBEAT.\nFig. 2 illustrates how CycleGAN, Attention UNet, VoxelMorph and iBEAT cooperate with each other."}, {"title": "Network architectures:", "content": "Attention UNet (AUNet) is an extension of the traditional U-Net architecture, which incorporates attention mechanisms to enhance the model's ability to focus on relevant features. The model parameters were optimized using a combination of DICE loss and Cross Entropy loss (CE), by minimizing the following loss function:\n$Loss_{seg} = LOSSDICE(AUNet(X), Y) + LOSS_{CE}(AUNet(X), Y)$ (1)\nCycleGAN (Cyc). The primary goal of CycleGAN is to learn mappings between two different image domains in an unsupervised manner, which is particularly useful in scenarios where obtaining paired data is difficult or expensive. The architecture of CycleGAN involves two generators, GAB and GBA, along with two discriminators, Da and DB. Here, we define the domain A as the neonatal domain and the domain B as the 6-month domain. Generator GAB learns the mapping from neonatal images to 6-month images, such as the transformation from real neonatal images $X_{real}$ to synthesized 6-month images $X_{synth}^{month}$. Conversely, generator GBA learns the inverse mapping from 6-month images to neonatal images. The discriminators Da and DB are responsible for distinguishing between real images and synthesized images. The objective function of CycleGAN in Fig. 2 is defined as:\n$Losscyc = Loss_{adversarial}(Gab, DB; GBA, Da) + LOSS_{consistency}(GAB, GBA)$\n$+LosS_{identity}(GaB, GBA) + Loss_{seg}(Seg(X_{synth}^{month}), Y_{neo})$ (2)\nwhere the $Loss_{adversarial}$ encourages the generated images to become indistinguishable from the target domain, thereby facilitating domain translation and image synthesis. The $LosS_{consistency}$ and LossSidentity encourage the bi-directional transformation consistency and the preservation of important features during the translation process. The loss function from Attention UNet was also included to contribute to the generation of synthesized 6-month images $X_{synth}^{month}$\nVoxelMorph (VM) [13] registration model was employed to register the synthesized 6-month images to the real 6-month images. This is because the contrast transferred data (synthesized 6-month images) retained the neonatal cortical folding pattern. This, in turn, has the potential to reduce the segmentation performance on the real 6-month images, as the segmentation model is trained solely on images with simpler neonatal cortical folding compared to real 6-month images. In this work, the input to VoxelMorph are pairs of 3D moving (M) and fixed (F) images, where M corresponding to the neonatal data, while F corresponds to the 6-month data. Specifically, VoxelMorph is trained to produce a deformation field \u00d8, which warps M and its corresponding neonatal labels $Y_{neo}$ to obtain the warped moving image M(\u00d8) and labels $Y_{neo} (\u00d8)$ as the new inputs for the Attention UNet. The objective function is defined as the combination of the local normalized cross correlation loss (LNCC, $LOSSLNCC$) and a regularization term Losssmooth.\n$Loss_{vox} = LOSSLNCC + \u03bb * LosSsmooth$\n$= \\frac{\\sum_{i \\in N_s}(F(i) \u2013 \\bar{F}) \\cdot (M(\\emptyset(i)) \u2013 \\bar{M(\\emptyset)})}{\\sqrt{\\sum_{i \\in N_s}(F(i) \u2013 \\bar{F})^2 \\cdot \\sum_{i \\in N_s}(M(\\emptyset(i)) \u2013 \\bar{M(\\emptyset)})^2}} + \\sum_{i \\in N_s}||\\emptyset(i)||^2$ (3)"}, {"title": "2.3 Model implementation details", "content": "The 3D Attention UNet from Project Monai [14] was implemented with 5 encoder-decoder blocks featuring 32, 64, 128, 256, and 512 filters, using a kernel size of 3 and a stride of 2, with a learning rate of 0.0004.\nFor the 3D CycleGAN generators, UNet with 7 layers was employed, with channel configurations of 64, 128, 256, 512, and 512. The discriminator comprised a 5 layers PatchGAN, with filters of 64, 128, 256, 512, and 1. Convolutional layers in CycleGAN had a kernel size of 3, a stride of 2, and were optimized with a learning rate of 0.0008.\nThe 3D VoxelMorph model consisted of 4 convolutional layers with filter sizes of 16, 32, 32, and 32 [15]. These layers utilized a kernel size of 3, a stride of 2, and were optimized with a learning rate of 0.002.\nAll model parameters were optimized using the Adam optimizer, and experiments were conducted on the NVIDIA A100 Tensor Core GPU."}, {"title": "3 Results", "content": null}, {"title": "3.1 Test set and evaluation criteria", "content": "We utilized real 6-month images and manually corrected segmentation from 10 individuals as the test set to evaluate segmentation performance of different pipelines. Using the same evaluation criteria, we calculated MONAI's implementation [14] of the DICE score, the 95th percentile Hausdorff distance (HD95) and the average symmetric surface distance (ASSD) for each brain tissue. The results are presented in Table 1. The segmented outputs of different pipelines are visualized in Fig. 3 and Fig. 4."}, {"title": "3.2 Quantitative comparison", "content": "The AUNet, as the baseline, was trained on the neonatal images and labels and performed poorly on the real 6-month images, particularly in WM and GM segmentations. It obtained an average DICE score of 0.74, an HD95 of 15.49 and an"}, {"title": "3.3 Qualitative assessment", "content": "T2w images, exhibited the highest DICE score of 0.92, the lowest HD95 of 1.6, and the lowest ASSD of 0.42, demonstrating the best overall segmentation performance.\nWe also conducted an ablation study that we trained the Cyc+AUNet+iBEAT+AUNet pipeline using 6-month T1w and T2w images, separately. the performance decreased compared to using both modalities at the same time, obtaining a DICE of 0.89, an HD95 of 1.54 and an ASSD of 0.48 on T1 modality, and a DICE of 0.90, an HD95 of 1.55 and an ASSD of 0.46 on T2 modality."}, {"title": "4 Discussion", "content": "In this work, we utilized neonatal annotation information and a DA technique to develop the first 8-tissue segmentation pipeline for six-month-old infant brains. To satisfy the variability in modalities between individuals, we prepared the segmentation models for T1 modality only, T2 modality only and both T1, T2 modalities together. When leveraging both T1w and T2w information together, the model achieved the highest performance with a DICE score of 0.92, an HD95 of 1.6, and an ASSD of 0.42. When using a single modality as input, the performance decreased but not significantly (two-tailed t-test, p=0.26), obtaining a DICE of 0.89, an HD95 of 1.54 and an ASSD of 0.48 on the T1 modality, and a DICE of 0.90, an HD95 of 1.55 and an ASSD of 0.46 on the T2 modality.\nWhen we incorporated the longitudinal registration algorithm into the pipeline, it resulted in less noisy and more accurate WM, GM, and CSF segmentations. However, it did not outperform iBEAT's cortical and WM performance, likely due to the registration accuracy limited by the low tissue contrast and image quality of the 6-month images.\nThe second Attention UNet at the end permits the training of a segmentation model capable of performing 8-tissue segmentation on 6-month-old infant brain images. It leverages tissue contrast from both T1w and T2w modalities, such as the contrast between cortex and unmyelinated WM from T2w images and myelinated WM from T1 images. This approach results in higher performance compared to single-modality segmentation.\nThe primary limitation of this work is the lack of data from other collection sites to evaluate the model's performance and generalization ability. In future research, we aim to acquire unseen datasets with varying acquisition parameters from different collection sites to validate the model's generalization ability. The segmentation results will effort to characterize potential associations between brain tissue features and atypical neurodevelopment, such as underlying neural differences related to autism."}]}