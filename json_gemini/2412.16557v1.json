{"title": "CognTKE: A Cognitive Temporal Knowledge Extrapolation Framework", "authors": ["Wei Chen", "Yuting Wu", "Shuhan Wu", "Zhiyu Zhang", "Mengqi Liao", "Youfang Lin", "Huaiyu Wan"], "abstract": "Reasoning future unknowable facts on temporal knowledge graphs (TKGs) is a challenging task, holding significant academic and practical values for various fields. Existing studies exploring explainable reasoning concentrate on modeling comprehensible temporal paths relevant to the query. Yet, these path-based methods primarily focus on local temporal paths appearing in recent times, failing to capture the complex temporal paths in TKG and resulting in the loss of longer historical relations related to the query. Motivated by the Dual Process Theory in cognitive science, we propose a Cognitive Temporal Knowledge Extrapolation framework (CognTKE), which introduces a novel temporal cognitive relation directed graph (TCR-Digraph) and performs interpretable global shallow reasoning and local deep reasoning over the TCR-Digraph. Specifically, the proposed TCR-Digraph is constituted by retrieving significant local and global historical temporal relation paths associated with the query. In addition, CognTKE presents the global shallow reasoner and the local deep reasoner to perform global one-hop temporal relation reasoning (System 1) and local complex multi-hop path reasoning (System 2) over the TCR-Digraph, respectively. The experimental results on four benchmark datasets demonstrate that CognTKE achieves significant improvement in accuracy compared to the state-of-the-art baselines and delivers excellent zero-shot reasoning ability.", "sections": [{"title": "Introduction", "content": "Temporal knowledge graphs (TKGs) store the structured dynamic facts in the real world, consisting of a sequence of knowledge graph (KG) snapshots accompanied by corresponding timestamps. Each fact in TKGs can be represented as a quadruple in the form of (subject, relation, object, time), e.g., (The World Cup, Be_held_at, Qatar, 2022). Reasoning over TKGs aims to infer new facts based on the historical KG snapshots, which can be divided into two settings: interpolation and extrapolation. According to the given history facts in time interval [to, t|T|], the former attempts to complete missing facts that appear in the past time t \u2208 [to, t|T|], while the latter focuses on predicting facts (events) happening in the future time t > t|7|. Due to its ability to infer future new events, extrapolation in TKG has a wider range of application scenarios (Chen et al. 2022b; Guan et al. 2023), such as stock prediction, traffic prediction, and medical assistance. This paper primarily emphasizes the more challenging TKG extrapolation task that forecasts future unknown facts (or events).\nMany arts for TKG extrapolation have been extensively explored and achieved excellent prediction performance. These studies mainly involve two categories (Mei et al. 2022; Liang et al. 2024a): embedding-based methods and path-based methods. Embedding-based methods learn the evolution embeddings of entity and relation by modeling the facts repeating and cyclic patterns, such as CyGNet (Zhu et al. 2021) and CENET (Xu et al. 2023), and the local adjacent facts evolution patterns, such as RE-GCN (Li et al. 2021b) and RETIA (Liu et al. 2023). However, due to the black-box characteristics of embedding-based methods, they lack explicit evidence to explain the reasoning results.\nNaturally, some path-based methods such as TLogic (Liu et al. 2022b) and KartGPS (Xin and Chen 2024), are proposed to better capture the chain of evidence for TKG reasoning. Nevertheless, path-based methods focus on temporal relational paths modeling in the specific time window closest to the query time, resulting in the loss of longer historical information. As shown in Figure 1, given a local time window [tq - 15, tq - 1], existing path-based methods only"}, {"title": "Related Work", "content": "Interpolation on TKGs Interpolation on TKGs aims to infer the historical missing facts. Most interpolation works are developed by extending static KG reasoning methods. For instance, TTransE (Leblay and Chekol 2018) is a translation-based method and introduces time constraints between facts that have common entities. TNTComplEx(Timoth\u00e9e, Obozinski, and Usunier 2020) is a tensor factorization method and proposes a 4th-order tensor factorization by extending time information into tensor factorization. RotateQVS (Chen et al. 2022a) models temporal evolutions as rotations in quaternion vector space, effectively capturing a variety of complex relational patterns. HGE-TNTComplEx (Pan et al. 2024) projects temporal facts onto a product space of heterogeneous geometric subspaces and employs temporal-geometric attention mechanisms for interpolation.\nExtrapolation on TKGs Extrapolation on TKGs aims to predict future unseen facts and can be classified as embedding-based methods and path-based methods. Some embedding-based methods explore the cyclic and repetitive patterns of historical facts from a global perspective. For example, CyGNet (Zhu et al. 2021) employs a copy-generation mechanism to learn query-relevant entities that are important in repeated historical facts. Other approaches study the local dependency of historical facts based on the local KG snapshots. For example, RE-GCN (Li et al. 2021b) models the evolution of entities and relations at each timestamp by using the local historical dependency. CEN (Li et al. 2022) exploits a curriculum learning approach to model variable snapshot graph sequence lengths. Recent embedding-based methods such as TiRGN (Li, Sun, and Zhao 2022), integrate the above two historical patterns to achieve more accurate reasoning. LogCL (Chen et al. 2024) adopts a contrastive learning to enhance the representation of local and global representation of queries. Since the black box characteristics of embedding-based methods, it is difficult to provide explainable evidence for reasoning results. Path-based methods generate temporal rules by sampling local temporal relations, and extend temporal rules to predict future facts. For example, TLogic (Liu et al. 2022b) mines the temporal logic rules from the loop relation path in a given time window and designs a strategy for evaluating the confidence of candidate temporal logic rules. TempValid (Huang et al. 2024) designs a rule-adversarial and a time-aware negative sampling strategies to obtain competitive results. Our approach CognTKE is a combination of embedding-based and path-based methods, showcasing excellent explainability and zero-shot reasoning capabilities."}, {"title": "Our Approach", "content": "Preliminaries\nA TKG G is denoted as a sequence of KG snapshots, i.e., G = {G1, G2, ..., G|7|}. The KG snapshot Gt = (E,R, Ft) consists of a set of valid facts at t \u2208 T, where E, R, T denote the set of entities, relations, and timestamps, respectively; Ft denotes the set of facts at t. A fact (or an event) is represented in the form of a quadruple (es, r, eo, t) \u2208 Ft, where an edge r serves as an association between the subject entity es and object entity e, at t.\nDefinition 1 (Temporal Relation Path over TKG). The temporal relation path with length l is a set of l quadruples (e1,r\u2081,2,t1) \u2192 \u00b7\u00b7 \u2192 (el-1, rl\u22121,el, tl\u22121) \u2192(ei, ri, ei+1, ti) with t\u2081 < ... < t1\u22121 < t1, that are connected origin entity e\u2081 to destination entity el+1 sequentially.\nDefinition 2 (Problem Statement). TKG extrapolation aims to forecast the future unknown object entity (or subject entity) given a query (eq, rq,?, tq) (or (?, rq, eq, tq)) according to previous historical KG snapshots {Go, G1, \u2026, Gtq-1}.\nNote that, the inverse relation quadruples (es,r\u22121,eo,t) and identity quadruples (es,rself, es,t) are added to the TKG dataset, where rself \u2208 R is an identity relation. Without loss of generality, the TKG reasoning goal can be expressed as the prediction of object entities.\nArchitecture Overview\nThe whole framework of CognTKE is shown in Figure 2, involving the three processes: (1) Retrieving temporal relations from the TKG to build TCR-Digraph; (2) Performing global shallow encoding and local deep encoding over TCR-Digraph to generate crucial candidate rules to associate with the query; (3) Based on the candidate rule pool, a decoder is employed to calculate the scores of entities.\nTemporal Cognitive Relation Digraph\nTemporal relation paths have been proven to possess high transferability and interpretability in TKG (Liu et al. 2022b; Xin and Chen 2024). However, the limitation lies in their ability to model complex multi-hop reasoning due to a small subset of local temporal relation paths learned in TKGs. Inspired by (Zhang and Yao 2022; Wu et al. 2023; Liang et al. 2024b), we introduce a temporal cognitive relation directed graph (TCR-Digraph) that provides rule-like explanatory and inductive capabilities to explore the significant historical chain of evidence. Next, we will introduce some definitions related to TCR-Digraph.\nDefinition 3 (Layered Graph(Battista et al. 1999)) The layered graph is a directed graph with exactly one source node and one sink node (destination node). All edges are directed, connecting nodes between consecutive layers and pointing from L \u2013 1-th layer to L-th layer.\nDefinition 4 (Temporal Relation Directed Graph, TR-Digraph). The TR-Digraph $G^{t_q}_{e_q e_{et}|L}$ is a layered graph with the source entity (query entity) $e_q$ at query time $t_q$ and the sink entity (target entity) $e_t$ at history time $t$, where $t_q > t$ and $L$ is the number of layers in the TR-Digraph. Entities on the same layer are different from each other. Each edge $(r, t)$ formed by a combination of relation $r$ and history time $t$ is a direction, connecting an entity in the $L-1$-th layer to an entity in the $L$-th layer. $G^{t_q}_{e_q e_{et}|L}$ is set as $\\emptyset$ if there is no temporal relation path connecting $e_q^d$ and $e_t$ with length $L$.\nHere, the quadruples with the reverse or identity relations are considered in the TR-Digraph. Through the above definition, any temporal relation path between query entity $e_q$ at query time $t_q$ and the target entity $e_t$ at specific history time $t$ in the TR-Digraph $G^{t_q}_{e_q e_{et}|L}$ is denoted as $e_q \\xrightarrow{(r_1, t_1)_1} \\rightarrow (r_2, t_2)_2 \\rightarrow \\cdots \\rightarrow (r_L, t_L)_L\\rightarrow e_t$ with"}, {"title": "Recursive Encoding over TCR-Digraph", "content": "As discussed above, the TCR-Digraph provides the interpretable chain of evidence consisting of global one-hop temporal relation and local multi-hop temporal relation paths. However, how to model different temporal relation paths on TCR-Digraph for temporal knowledge reasoning poses another problem. Inspired by the dual process theory of cognitive science (Evans 2008), we present a global shallow reasoner and a local deep reasoner consisting of temporal relation component (TR-Component) and temporal relation graph attention network (TR-GAT). The global shallow reasoner as the first decision process aims to carry out intuitive global one-hop reasoning, which can be seen as fast thinking in human cognition. The local deep reasoner as the second decision process performs local complex multi-hop reasoning, which can be regarded as slow thinking in human cognition. Although both reasoners perform different functions, similar pipelines of iteratively encoding each layer of the TCR-Digraph are employed to update the candidate rule pool. Note that each layer construction and encoding of TCR-Digraph is performed alternately. The candidate rule pool is a latent representation of candidate entities in TCR-Digraph, which is considered as working memory (Baddeley 1992) used to store updated rules for subsequent entity prediction.\nTemporal Relation Component The TR-Component aims to learn the representation of temporal relation in TCR-Digraph and serves as an input to TR-GAT. As historical facts associated with the query at different times have different effects on reasoning results, we adopt the positional encoding method (Vaswani et al. 2017) to map relative time into embedding representations. Formally, the relative time encoding formula is as follows:\n$TE_{(pos,2i)} = sin(pos/10000^{2i/d_{time}}),$ (1)\n$TE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{time}}),$ (2)\nwhere TE \u2208 R|T|\u00d7dtime is the embedding matrix of relative time, pos denotes the location of the time value, i denotes the dimension of the embedding presentation, and dtime denotes the embedding size of relative time, sin() and cos(.) are sine and cosine functions, respectively.\nTo incorporate the relative time information into the relation while preserving the basic semantic information of the relation, we obtain the representation of the temporal relation rt by the Feed-forward network (FFN) layer (Vaswani et al. 2017) and add operation:\n$h_{rt} = 0_1 (W_2(0_1(W_1[h_r, v_{\\bar{t}}] + b_1) + b_2) + h_r,$ (3)\nwhere [,] denotes the concatenate operation of vectors, hr \u2208 H denotes the embedding of relation, \u0397 \u2208 R|R|\u00d7d is an initialized relation matrix, v\u012b \u2208 TE is the embedding of relative time, t = tq - t is the relative time between the historical fact appearing time and the query time, 01 is the LeakyReLU activate function, W\u2081 \u2208 Rdxd,W2 \u2208 Rdx(d+dtime), b1, b2 \u2208 Rd are all learnable parameters, where d is the dimension size of the relation. Through the encoding of temporal relation, the quadruple (es, r, eo, t) in TKGs is converted into the form of triple (es, rt, eo)."}, {"title": "Temporal Relation Graph Attention Layer", "content": "The goal of the TR-GAT layer is to propagate temporal relation information in TCR-Digraph $G^{t_q}_{e_q e_{et}|L}$ centered on the query entity $e_q$ and update the candidate rule pool that is a latent representation of candidate answer entities. To preserve the ordering of consecutive temporal relations in the TCR-Digraph during message propagation, we present a variant of GRU (QTR_GRU) in TR-GAT to perform message passing. The message passing function is as follows:\n$m^{t_q}_{e_s, r_t \\rightarrow e_o} = QTR-GRU(h^{t_q}, h_{e_s, rt}, h^{t_q}_{e_s}),$ (4)\nwhere $h^{t_q} \\in R^d$, $h_{e_s, rt} \\in R^d$ and $h^{t_q}_{e_s} \\in R^d$ are the representation of the query relation at query time $t_q$, the subject entity and temporal relation, respectively. Note that, all entity representations are initialized to the zero vector before performing message passing.\nSince the construction of the TCR-Digraph $G^{t_q}_{e_q e_{et}|L}$ is dynamic and highly correlated with query entities and query time, while remaining independent of the query relation. Different queries with the same query entity at the same query time share the same TCR-Digraph, e.g., (The World Cup, Be_held_at, Qatar, 2022) and (The World Cup, Be_won_by, Argentina, 2022), leading to the same evidence being utilized for reasoning. Thus, we adopt the graph attention (GAT) (Velickovic et al. 2018) mechanism to encode the query information into the attention weight to distinguish the importance of different edges in the TCR-Digraph. The attention score $a^{t_q}_{e_s, rt \\rightarrow e_o}$ for each triple $(e_s, r_t, e_o)$ in the TCR-Digraph is calculated as follows:\n$e^{t_q}_{e_s, rt \\rightarrow e_o} = \\sigma_2(W_0(\\sigma_1(W^q h^{t_q} + W_0 h_{e_s, rt} + W_0 h^{t_q}_{e_o})),$ (5)\n$a^{t_q}_{e_s, rt \\rightarrow e_o} = \\frac{exp(e^{t_q}_{e_s, rt \\rightarrow e_o})}{\\sum_{(e_s, rt) \\in N_{e_o}} exp(e^{t_q}_{e_s, rt \\rightarrow e_o})},$ (6)\nwhere $W_q, W_0, W_0, W_0$ represent trainable weight matri- ces, $e^{t_q}_{e_s, rt \\rightarrow e_o}$ denotes the attention weight, and $N_{e_o}$ is the in-degree neighbors of the object entity $e_o$ in TCR-Digraph. Thus, the candidate rule representations of the aggregated entities are calculated as follows:\n$h_{e_o} = \\sum_{(e_s, rt) \\in N_e} a^{t_q}_{e_s, rt \\rightarrow e_o} W_o h_{e_s, rt},$ (7)\n$h_{e_o} = \\frac{h_{e_o}}{\\sqrt{indegree(e_o)}},$ (8)\nAmong them, Wo is the weight matrix. indegree(eo) denotes the number of in-degree object entities and serves as a scaling factor to adjust the entity representation. To further retain the feature of existing candidate entities, we utilize GRU (Cho et al. 2014) to control the updating of representations of entities. Thus, the final representation of the candidate entity eo in l-th layer of TR-GAT can be calculated as follows:\n$h^{e_o}_l = GRU (h^{l-1}_{e_o},h_{e_o}).$ (9)"}, {"title": "Interpretable Reasoning and Analysis", "content": "Prediction After obtaining the final candidate entity representation that includes the appropriate temporal relation rule information, we adopt a simple MLP (Multilayer perceptron) as the decoder to calculate the prediction score of all entities as follows:\n$score(e_o) = f(e_q, r_q, e_o, t_q) = W_o h_{e_o},$ (10)\nwhere $W_o \\in R^{1\\times d}$ is the weight matrix. If an entity is not included in the TCR-Digraph corresponding to the query, it will be assigned a score of 0.\nWe consider entity prediction as a multi-label classification task, and utilize the multi-class log-loss to optimize the parameters of CognTKE, which has been proven to be effective(Zhang and Yao 2022; Lacroix, Usunier, and Obozinski 2018),\n$L = \\sum_{(e_q, r_q, e_o, t_q) \\in F_{train}} (-f(e_q, r_q, e_o, t_q)+\nlog(\\sum_{e_o' \\in E} exp(f(e_q, r_q, e_{o'}, t_q))),$ (11)\nwhere (eq, rq, eo, tq) denotes positive fact in training set Ftrain, and (eq, rq, eor, tq) denotes other fact with the same query (eq, rq,?, tq).\nInterpretable Analysis Although CognTKE acquires the graph structure of TCR-Digraph through the GAT mechanism, it still possesses the capacity to encode path-based logical rules of the same nature as those utilized in rule induction models, such as TLogic (Liu et al. 2022b) and TECHS(Lin et al. 2023).\nTheorem 1. Given a quadruple (eq, rq, eo, tq), let p be temporal relation rt, C be a set of temporal relation paths $e_q \\xrightarrow{p} e_t \\in C$ that are formed by a set of rules between ea and et with the form:\n$r_q(X,Y) \\leftarrow p_1(X, Z_1)^p_2 (Z_1, Z_2)^\\cdots^p_L (Z_{L-1},Y)$,\nwhere X, Y, Z1,..., ZL-1 are free variables that are bounded by unique entities. Assuming there exists a directed graph Gc that is built by C, a parameter setting \u0472, and a threshold \u03bb\u2208 (0,1) for CognTKE. \u011cc can equal to the TCR-digraph if CognTKE's edges have attention weights $a^{l}_{e_s, rt \\rightarrow e_o} > \\lambda$ in $G^{t_q}_{e_q e_{et}|L}$, where \u03bb is a learned decision boundary parameter.\nAccording to Theorem 1, CognTKE is capable of encoding any temporal logical rule that corresponds to a path in the TCR-Digraph. This implies that if a set of temporal relation paths is highly correlated with the query quadruple, CognTKE can identify them through the attention weights, making them interpretable."}, {"title": "Experiments", "content": "Datasets To evaluate CognTKE on entity prediction task, we adopt four benchmark datasets that are widely used for TKG extrapolation, including ICE14, ICE18, ICE05-15 (Jin"}, {"title": "Study of Prediction Time", "content": "We compare the prediction time of CognTKE with that of the recent important extrapolation methods on all the datasets, including REGCN, CEN, TLogic, TiRGN,"}, {"title": "Appendix", "content": "A Some Details in Our Approach\nA.1 Structure Diagram of QTR_GRU. The QTR_GRU component in CognTKE is shown in Figure 5, the detailed calculation process of QTR-GRU can be described as:\n$g_u = \\sigma_1(W^u[h^{t_q-1}_{e_s}, h_r^t, h^{t_q}_{e_o}]+ b^u),$ (12)\n$g_f = \\sigma_1(W^f[h^{t_q-1}_{e_s}, h_r^t, h^{t_q}_{e_o}]+ b^f),$ (13)\n$\\bar{h} = 0_3(W(h_r^t + (g_u \\odot h^{t_q}_{e_o})) + b),$ (14)\n$h^{t_q}_{e_s} = (1 -g_u)h^{t_q-1}_{e_s} + g_f \\odot \\bar{h},$ (15)\nAmong them, represents the Hadamard product, gf, gu and he denote the forget gate, the update gate, and the hidden state in QTR_GRU, respectively; W, W\u2208 Rd\u00d73d, W\u2208 Rdxd, bu, bf, b \u2208 Rd all denote trainable parameters. 02 and 03 represent the sigmoid activate function and tanh activate function, respectively.\nA.2 Pseudocode. The algorithm for the TCR-Digraph encoding in CognTKE is described in Algorithm 1.\nB Experiment Details\nB.1 Dataset Statistics. Table 4 shows the statistics of the four datasets ICE14, ICE18, ICE05-15, and WIKI.\nB.2 Implementation Details. For all datasets, the embedding size d is set to 64, the time embedding size is set to 32, the learning rate is set to 0.001, the batch size is set to 128, the length of local time windows m is set to 15, the number of layers in TR-GAT is set to 4. CognTKE is implemented using PyTorch and PyG.\nThe CognTKE employs Adam to optimize the parameters of the model. To train the CognTKE, we utilize an NVIDIA Tesla A100 GPU for 20 epochs in a Linux machine and a mixed precision training way.\nFor extrapolation baselines, their results with the time-aware filter setting are reported under the default parameters. For fairness of comparison, results of CEN and RETIA are reported under the offline setting that is adopted to other baselines.\nParameters Analysis\nWe conduct experiments on ICE14 and ICE05-15 datasets to further analyze the impact of parameters in CognTKE, including the length of local time windows m, the number of layers of TR-GAT l, the embedding size of relation d, and the embedding size of time dtime.\nFrom the results in Figure 6, it can be seen that the per-"}]}