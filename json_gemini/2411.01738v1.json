{"title": "xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism", "authors": ["Jiarui Fang", "Jinzhe Pan", "Xibo Sun", "Aoyu Li", "Jiannan Wang"], "abstract": "Diffusion models are pivotal for generating high-quality images and videos. Inspired by the success of OpenAI's Sora [32], the backbone of diffusion models is evolving from U-Net to Transformer, known as Diffusion Transformers (DiTs). However, generating high-quality content necessitates longer sequence lengths, exponentially increasing the computation required for the attention mechanism, and escalating DiTs inference latency. Parallel inference is essential for real-time DiTs deployments, but relying on a single parallel method is impractical due to poor scalability at large scales.\nThis paper introduces xDiT, a comprehensive parallel inference engine for DiTs. After thoroughly investigating existing DiTs parallel approaches, xDiT chooses Sequence Parallel (SP) and PipeFusion, a novel Patch-level Pipeline Parallel method, as intra-image parallel strategies, alongside CFG parallel for inter-image parallelism. xDiT can flexibly combine these parallel approaches in a hybrid manner, offering a robust and scalable solution. Experimental results on two 8\u00d7L40 GPUs (PCIe) nodes interconnected by Ethernet and an 8\u00d7A100 (NVLink) node showcase xDiT's exceptional scalability across five state-of-the-art DiTs. Notably, we are the first to demonstrate DiTs scalability on Ethernet-connected GPU clusters. xDiT is available at https://github.com/xdit-project/xDiT.", "sections": [{"title": "1 Introduction", "content": "In recent years, diffusion models have emerged as a ground-breaking technique in image [4, 5, 10, 34] and video generation [31, 32]. These models create images or videos through a multi-step denoising process, leveraging a neural network model at each step. We are witnessing a transformative shift in the architecture of the denoising networks. While traditionally dominated by U-Net [38] architectures, these networks are now evolving into Diffusion Transformers (DiTs) [34], renowned for their superior model capacity and scalability.\nImages are encoded into a token sequence as input to DiTs [37]. The attention mechanisms of DiTs require mutual computation between tokens, leading to computation that scales quadratically with sequence length. Unlike Large Language Models (LLMs), which can generate tokens on the fly, DiTs must complete all steps to produce the final results. This poses significant challenges for real-time deployment. For example, a Sora-like video generation API [1, 20] takes over 4 minutes to generate a fewer seconds video. Given the immense computational demands, real-time DiTs deployment inevitably necessitates parallelism across multiple computing devices.\nHowever, there is still a lack of effective methodology to scale DiTs to large scales. Although, several sequence parallelism (SP) methods [12, 14, 17, 26] have been developed to scale long-sequence DiTs inference. Additionally, some approaches leverage input temporal redundancy [29-31, 40, 45, 46], indicating a high degree of similarity in both inputs and activations across successive diffusion time steps, to design asynchronous sequence parallelism [22] and patch-level sequence parallelism [11]. However, using these methods in isolation fails to adapt to the underlying heterogeneous interconnects of computational devices. For instance, methods using collective communication are only suitable for high-bandwidth interconnects like NVLink, while methods using P2P (Peer-to-Peer) are more suitable for PCIe or Ethernet but do not have an advantage on NVLink.\nAdditionally, the diversity of DiTs model architectures presents challenges for parallel implementation. Unlike LLMs, which generally have a more uniform architecture, DiTs exhibit greater variability. For instance, the method of injecting user input instructions, the connection methods of DiTs blocks, and the operators' layouts in Transformer blocks can vary significantly. This diversity means that directly applying methods designed for LLMs, such as TP and SP, may not be immediately suitable for DiTs.\nThis paper introduces xDiT, a parallel inference system designed for DiTs. We argue that DiTs inference is analogous to the training of LLMs in that a single parallel is unlikely to scale effectively across various model architectures and network hardware, especially for those heterogeneous low-bandwidth networks. Therefore, a hybrid approach combining multiple parallel methods is necessary to achieve optimal communication efficiency.\nOur contributions are as follows:\n\u2022 We systematically investigate existing parallel methods for DiTs (including two types of sequence parallelism, DistriFusion, tensor parallelism, and PipeFusion),\n\u2022 We selected PipeFusion and sequence parallelism for intra-image parallelism and CFG parallelism for inter-image parallelism. We designed a method to correctly hybrid these parallel approaches, thereby scaling DiTs inference to a very large scale.\n\u2022 Using the aforementioned methods, we built a system called xDiT. On 16\u00d7L40 PCIe and 8\u00d7A100 NVLink devices, we studied the scalability of five image and video generation DiTs (Pixart, Stable-Diffusion3, Flux.1, HunyuanDiT, CogVideoX). xDiT is the first system to successfully scale DiTs inference to 16 GPUs."}, {"title": "2 Background & Related Works", "content": "Diffusion Models: Diffusion models utilize a noise-prediction deep neural network (DNN) denoted by ee to generate a high-quality image. The process starts from pure Gaussian noise XT ~ N(0,I) and involves numerous iterative denoising steps to produce the final meaningful image xo, with T representing the total number of diffusion time steps. At each diffusion time step t, given the noisy image xt, the model \u20ac\u03b8 takes x\u2081, t, and an additional condition c (e.g., text, image) as inputs to predict the corresponding noise et within xt. At each denoising step, the previous image xt\u22121 can be obtained from the following equation:\nXt-1 = Update(xt, t, et), \u20act = \u20ac0(xt, t, c). (1)\nIn this context, Update denotes a function that is specific to the sampler, i.e. DDIM [41] and DPM [27], generally involves operations such as element-wise operations. After multiple steps, we decode xo from the Latent Space to the Pixel Space using a Variational Autoencoder (VAE) [18]. Consequently, the predominant contributor to diffusion model inference latency is attributed to the forward propagation through the model \u03b5\u03b8.\nDiffusion Transformers (DiTs): The architecture of diffusion model ee is undergoing a pivotal transition from U-Net [38] to Diffusion Transformers (DiTs) [2, 6, 23, 28, 31], driven by the scaling law demonstrating increased model parameters and training data with enhanced model performance. Unlike U-Nets, which apply convolutional layers capturing spatial hierarchies, DiTs encodes the input into a token sequence in the latent space and leverage the transformer's self-attention mechanism to model relationships within and across these patches.\nAs shown in the above of Figure 2, in DiTs, the input noisy latent representation is embedded into tokens and fed into a series of DiT blocks. DiT blocks generally incorporate Multi-Head Self-Attention, Layer Norm, and Pointwise Feedforward Networks. Although the earliest DiTs model architectures [34] was quite similar to standard transformers [42], numerous improvements and variants have been introduced recently, leading to a highly diverse and non-uniform model architecture landscape. For instance, the incorporation of conditioning can be achieved through various methods such as adaptive layer norm [34], cross-attention [6], and extra input tokens [10]. As diffusion models tackle higher-resolution images and longer visual sequences, they impose a quadratic computational burden on inference.\nInput Temporal Redundancy: The diffusion model involves iterative noise prediction from input images or videos. Recent studies have emphasized input temporal redundancy, highlighting the similarity in both inputs and activations across successive diffusion timesteps [30, 40]. Recent work [29] further explores the distribution of this similarity across various layers and timesteps. Leveraging this redundancy, some research caches activation values and reuses them in subsequent timesteps to reduce computation. For instance, in the U-Net architecture, DeepCache updates low-level features while reusing high-level ones from cache [30]. Similarly, TGATE caches cross-attention outputs once they converge during the diffusion process [30]. In contrast, for DiT models, A-DiT caches rear DiT blocks in early sampling stages and front DiT blocks in later stages [7]. PAB [46] employs a pyramid-style broadcasting approach to mitigate temporal redundancy using a U-shaped attention pattern. Lastly, DiT-FastAttn [45] identifies three types of redundancies-spatial, temporal, and conditional-and introduces an attention compression method to accelerate generation.\nParallel Inference for Diffusion Model: Given the similar transformer architecture, tensor parallelism [39] and sequence parallelism [12, 14, 17, 26], commonly used for efficient inference in LLMs, can be adapted for DiTs. Tensor parallelism (TP) partitions model parameters across multiple devices, enabling parallel computation and reducing memory demands on individual devices. However, it requires AllReduce operations for the outputs of both the Attention and Feedforward Network modules, leading to communication overhead proportional to the sequence length. This overhead becomes significant for DiTs with very long sequences. In contrast, sequence parallelism (SP) partitions the input image across multiple devices, using All2All or P2P to communicate Attention input and output tensors. SP offers better communication efficiency than TP but requires each device to store the entire model parameters, which can be memory-intensive. DistriFusion [22] exploits Input Temporal Redundancy to design an asynchronous sequence parallelism method for U-Net-based models, utilizing stable activations. PipeFusion [11], proposed by us, is a Patch-level Pipeline Parallelism. It leverages Input Temporal Redundancy to apply TeraPipe [24] to full attention DiTs inference, achieving lower communication overhead and model parameter memory usage compared to SP and DistriFusion."}, {"title": "3 Challenges in Parallel DiTs Inference", "content": "For a long time, inference of Diffusion Models with a U-Net backbone has been conducted on a single GPU due to limited computational requirements. However, in DiTs, the computation of the attention mechanism scales quadratically with the sequence length. With the scaling law [25, 34] driving improvements in image and video generation quality through increased model size and sequence length, it leads to a cubic growth in computational demand during deployment.\nIn high-quality image and video generation tasks, the sequence length of the input to transformers can exceed 1 million tokens. The current leading open-source image generation model, Flux.1, generates images with a resolution of 1024px (1024\u00d71024), requiring a sequence length of 262 thousand tokens; For 4096px resolution images, the input sequence includes 4.2 million tokens. The leading open-source video generation model, CogVideoX, generates a 6-second video at 480x720 resolution with a sequence containing 17K tokens. If used to generate a one-minute 4K (3840\u00d72160) video, the sequence length exceeds 4 million.\nDespite both applying Transformers, DiTs inference differs fundamentally from LLM inference. DiTs forward a denoise network in multiple steps, with the final denoised result adopted as the meaningful output. In contrast, LLMs use autoregressive models, consisting of a Prefill phase for processing prompts and generating the first token, followed by a Decoding phase where the remaining tokens are generated sequentially. Both DiTs and the Prefill phase of LLMs are compute-bound, with DiTs particularly notable for handling extremely long input sequences, whereas the LLM Decode phase is memory-bound. Therefore, applying LLM inference systems [43] directly to DiTs is infeasible. Parallel inference of DiTs presents significant challenges from two aspects:\nChallenges on Communication and Memory Cost: The current research on parallelizing DiTs primarily focuses on the scalability of individual parallel approaches. However, empirical evidence highlights the inadequacy of a single parallel method in managing the complexities of network connections between computing devices. Consequently, there is a notable gap in the literature regarding how to scale DiTs inference to extremely large scales, such as multiple GPU nodes, while accommodating both high-end networks with RDMA+NVLink and low-end networks with Ethernet+PCIe. The communication methods of a single parallel approach are ill-suited to handle the varying hardware network scenarios. Therefore, DiTs necessitates a hybrid parallel method to effectively adapt to different hardware networks.\nAdditionally, long sequences and large models impose significant memory demands. The current state-of-the-art open-source image generation model [4] reached 12 billion parameters and is expected to grow further [13]. Without tensor parallelism, the designed parallel methods cannot effectively distribute parameters across multiple devices, such as sequence parallelism, which will lead to out-of-memory (OOM) issues. The VAE component of DiTs for image decoding also requires substantial activations and operator temporal memory.\nChallenges on Diverse DiTs model architectures: As shown in Figure 1, starting from the original DiTs [34], the model architecture of DiTs has rapidly evolved, giving rise to many variants, each with its differences:\nFirstly, the method of injecting condition information is not uniform. The original DiT [34] proposes three condition injection methods: AdaLN-Zero, Cross-Attention, and In-Context Condition, which use the conditioning tensor to affect the Layer Norm, perform Cross-Attention computation, and concatenate them on sequence dimension, respectively. The first two methods only require splitting the image computation load, while the third method needs to consider splitting both the image and caption Transformers computation.\nSecondly, despite incorporating both Self-Attention and Feedforward Network, there are significant differences in the implementation of DiT Blocks. For instance, the MM-DiT (Multimodal-DiT) blocks, adopted by Flux.1 and SD3, handle the condition latents and image latents equally. Prior to the Self-Attention module, QKV projections are conducted on both text and image latents, which are then concatenated on the sequence dimension. Due to this structure, designing a tensor parallelism strategy for MM-DiT becomes challenging. As it needs a sophisticated method to distribute and synchronize computations involving both types of latents across devices while maintaining the model's effectiveness.\nThirdly, the connection method between DiT blocks also varies. Compared to the initial linear connection, U-ViT [2] and HunyuanDiT [23] designs skip-connected DiT blocks, which is similar to the U-Net's U-shaped topology, which presents the challenge for efficient communication pattern for pipeline parallels."}, {"title": "4 xDiT System", "content": "In this section, we present xDiT, a system designed for parallel inference of DiTs. As illustrated in Figure 2, DiTs inference is decomposed into three parts: Text-Encoder, Transformers Model, and VAE. The majority of the computation is concentrated within the transformers model, which is the focal point of our paper. To address this, xDiT leverages two parallel paradigms: Intra-image Parallel (Sec. 4.1) and Inter-image Parallel (Sec. 4.2). The Intra-image paradigm employs multiple devices to process a single image input, relying on Sequence Parallelism (Sec. 4.1.1) and an innovative parallel method named PipeFusion first proposed by us (Sec. 4.1.2). On the other hand, the Inter-image paradigm, which is used in conjunction with CFG (Classifier-Free Guidance), involves the parallel generation of images across devices. These four parallel methods can be combined in any arbitrary hybrid configuration. Furthermore, to prevent Out-of-Memory (OOM) issue in the VAE, we have devised a Patch Parallel method for it (Sec. 4.3).\n4.1 Intra-Image Parallelism for Transformer Model\nThis section will present how to leverage multiple computing devices to parallelize the computation of a single image in the DiTs backbone. We first adapt SP to DiT Block for DiTs models applying In-Context Conditioning. The we review the Patch-level Pipeline Parallelism (PipeFusion). Third, we compare the communication and memory efficiency of existing parallel approaches and our proposed methods. Last but not least, we present an approach to correctly hybridize the parallel of SP and PipeFusion.\n4.1.1 Sequence Parallelism. In DiTs, 2D latent images can be flattened and interpreted as a sequence of visual tokens, which then serve as inputs to the transformer blocks. Sequence parallelism splits inputs along the sequence dimension by partitioning the input image into non-overlapping patches. Then each device computes the outputs for its local patches. The best practices for sequence parallelism are DeepSpeed-Ulysses [17] (SP-Ulysses), Ring Atention [26] (SP-Ring). As shown in Figure 6, SP-Ulysses employs All2All communications to transform the partitioning along the sequence dimension into partitioning along the head dimension and parallel computation of attention across different heads. SP-Attention is a parallel version of Flash Attention [9], utilizing peer-to-peer (P2P) transmission of K and V subblock.\nHowever, directly applying SP to MM-DiT Block with In-context Condition, adopted by the latest models such as SD3, Flux.1, and CogVideoX, is not feasible. As illustrated, condition information and image information are separately encoded and concatenated before Self-Attention computation. If only the sequence dimension of the image is split, the condition input tensor needs to be replicated across different devices.\nWe designed an SP method tailored for In-Context Condition. As shown on the right side of Figure 3, it splits both the Condition Tensor and Image Tensor along the sequence dimension. Then, it concatenates corresponding shards of condition and image input to form a local sequence. This ensures load balancing, allowing not only Attention but also the encoding logic before condition and image to be parallelized using SP. As shown, the computation yields the same results as the serial version.\n4.1.2 PipeFusion: Patch-level Pipeline Parallelism. Existing parallel paradigms, whether Tensor Parallel (TP) or Sequence Parallel (SP), require communication of activations for each DiTs block. The sequence-level pipeline parallel method proposed by TeraPipe [24], used in LLMs, can transmit only limited input Activations, has been proven to be more suitable for long sequence input [36]. However, TeraPipe is designed for transformers using causal attention, where each token only attends to its previous tokens. In contrast, DiTs employ full attention, where each token attends to the computation of both previous and subsequent tokens.\nPipeFusion [11], a patch-level pipelined parallel approach proposed by us specifically designed for DiTs, leverages input temporal redundancy to effectively adapt TeraPipe for DiTs inference. Here, we provide a brief review of PipeFusion.\nPipeFusion partitions the input latent image into patches and the DiTs transformer model into layers, as shown at the top of Figure 4. It partitions the DiTs model along the data flow, assigning each partition of consecutive layers to a GPU. The input image is also divided into M M non-overlapping patches, allowing each GPU to process one patch with its assigned layers in parallel. This pipelined approach requires synchronization between devices, which is efficient when the DiTs workload is evenly distributed across GPUs. Achieving even partitioning is straightforward since DiTs consist of identical transformer blocks.\nSuppose we are at diffusion timestep T, with the previous timestep being T + 1 as the diffusion process proceeds in reverse order. Figure 4 illustrates the pipeline workflow with N = 4 and M = 4, highlighting the activation values of patches at timestep T. Leveraging input temporal redundancy, a device can start its computation without waiting for full spatial activations at timestep T. Instead, it uses stale activations from the previous timestep to provide context. Additionally, in PipeFusion, devices send micro-step patch activations to subsequent devices via asynchronous P2P, enabling overlap between communication and computation.\nPipeFusion theoretically outperforms DistriFusion in results accuracy considering the area of fresh activation area.\nAs shown in Figure 5, within a single diffusion timestep, PipeFusion continuously increases the area of fresh activation as the pipeline micro-steps progress from diffusion timestep 4 to 8. In contrast, throughout the entire diffusion process, DistriFusion constantly maintains one patch of fresh area out of the total M patches.\nSimilar to DistriFusion, before executing the pipeline, we usually conduct several diffusion iterations synchronously, called warmup steps. During the warmup phase, patches are processed sequentially, resulting in low efficiency. Though the warmup steps cannot be executed in the pipelined manner, the workflow is relatively small compared to the entire diffusion process, and thus, the impact on performance is negligible.\nChanges in DiTs model architecture may impact PipeFusion. Firstly, adapting the In-Context-Conditioning to inject caption requires minor modifications to the PipeFusion algorithm. Specifically, text vectors are concatenated with Patch0, increasing its computational load and potentially causing pipeline imbalance. However, in the latest models, Flux and SD3, this impact has been found to be minimal (Figure 12 and Figure 11). For 1024px generation tasks, with image sequence lengths of 64K and 256K, and a text feature sequence of 128, the text features account for less than 2% of the total. If the text sequence is relatively long, a load balancing image partition can be applied. Secondly, for DiTs with skip-connection structures, a device in PipeFusion not only communicates with adjacent devices but also with a distant one, affecting P2P communication overlapping. For instance, in the 2048px task on 8\u00d7A100, PipeFusion observed poor scalability due to this issue (Figure 17).\n4.1.3 Comparison Between Intra-Image Parallelisms. We analyze the communication and memory cost of existing DiTs parallel methods in Table 1. The communication cost is calculated by the product of the number of elements to transfer with an algorithm bandwidth (algobw) factor related to communication type 1. For collective algorithms of AllReduce, AllGather and AllToAll, the corresponding algobw factors are 2\uba85, \uba85, and 1. In the table, we approximate the term O(1) to O(1) for simplicity.\n4.1.4 Hybrid Parallelism. Our experiments reveal that using SP-Ulysses, SP-Ring and PipeFusion alone sometimes is insufficient for achieving large-scale parallel inference for DiTs. SP-Ring, due to its higher communication overhead compared to SP-Ulysses, performs less efficiently in high-bandwidth interconnect networks. Conversely, SP-Ulysses is sensitive to hardware network topologies; for instance, The communication cost surges dramatically when All2All communication spans the QPI in PCIe-interconnected GPUs. In this section, we propose a method to arbitrarily hybridize the three intra-image parallel methods to accommodate any network hardware topology, thereby scaling DiT inference to a large scale. Hybrid parallel in xDiT is not as straightforward as hybrid parallel in LLMs [39], because PipeFusion utilizes full spatial shape KV buffers, not the KV shards belonging to itself, and the non-local K, V is difficult to update by SP correctly.\nExisting research named USP [12] has successfully hybridized SP-Ulysses and SP-Ring. It views the process group as an 2D mesh where the columns are SP-Ring groups and rows are SP-Ulysses groups. USP parallelizes the attention head dimension in SP-Ulysses groups and parallelizes the head dimension in SP-Ulysses groups. In xDiT, we need to further hybridize USP with PipeFusion. We view the process group for intra-image parallelism as a 2D mesh of pipefusion_degree \u00d7 sp_degree. PipeFusion is executed on the high dimension, while the USP is executed on the lower dimension. Both SP and PipeFusion split the input palong the sequence dimension. The entire hidden state is first split into M patches along the sequence dimension, and each patch is further split into sp_degree patches. As illustrated in the figure, 8 devices are divided into pipefusion_degree=4, sp_degree=2, and M=4.\nThe challenge of hybrid parallel combining PipeFusion and SP lies in correctly updating the K, V of attention. If PipeFusion and SP are naively computed within their respective process groups, PipeFusion will use incorrect stale K, V values. As marked \"standard SP\" in the lower part of Figure 7, device 0 only updates K, V belonging to the even-numbered patches, while device 1 only updates K, V belonging to the odd-numbered patches. In this case, this results in half of the KV on each device not being updated with the desirable K, V values from the previous diffusion step.\nThe key to design correct SP+PipeFusion hybrid parallel is that the KV involved in Attention computation on different devices within the SP group should be consistent with each other. We designed a highly elegant method to achieve this without introducing any overhead, requiring only minor modifications to the SP algorithm. As shown in Figure 6, after communication of K and V in SP-Ulysses and SP-Ring, the intermediate results (in the dashed red box) are stored in each device's KV Buffer. However, in the standard SP implementations for both SP-Ring and SP-Ulysses, these intermediate results are discarded after the Attention computation. As shown in the \"Hybrid-SP-PP\" part of Figure 7, the devices 0 and 1 in the same SP process group are updated with consistent K, V values. For SP-Ulysses, we obtain the KV of the sequence within the SP group participating in the computation of the head, and we do not need information from non-participating heads. For SP-Ring, we obtain the KV of the sequence within the SP group for all heads.\n4.2 Inter-Image Parallelism\nThe Classifier-Free Guidance (CFG) [16] has become an important optimization for diffusion models by providing broader conditional control, reducing training cost, enhancing the quality and details of generated content, and improving the practicality and adaptability of the model. For an input prompt, using CFG requires generating both unconditional guide and text guide simultaneously, which is equivalent to inputting input latents batch size=2 of DiT blocks.\nCFG parallelism separates the two latents for computation, and after each diffusion step forward is completed and before the scheduler executes, it performs an Allgather operation on the latent space results. Its communication overhead is much smaller than PipeFusion and sequence parallelism. Therefore, when using CFG, CFG parallelism must be used.\n4.3 Parallel VAE\nAfter the DiT backbone denoised an image in latent space, an autodecoder module employs a VAE [18] to decode the image (size\u00d7\u00d7c), where c is the channel size and usual be set as 4 or 16, from the latent space into images in the pixel space (hxw \u00d7 3). The VAE applies multiple convolutional neural networks for upsampling. Unfortunately, as the height and width dimensions of the feature maps increase, especially after passing through convolutional layers with numerous channels, two significant issues emerge. Firstly, there is a substantial increase in activation memory. For example, in 4096px image generation, the peak activation tensor in the SD-VAE 2 reaches 60.41 GB. Secondly, excessive temporary memory for convolutional operators of large inputs also leads to memory spikes. This memory bottleneck hinders users and the research community from scaling up models to produce high-quality images.\nTo address the activations OOM issue, xDiT applies patch parallelism, the sequence parallelism, named as patch parallel here, for 2D inputs, for the VAE modules. We divide the feature maps in the latent space into multiple patches and perform parallel VAE decoding across different devices. This requires the exchange of the boundary data for convolutional operators by allgther communications. Patch parallelism reduces the peak memory for intermediate activations to While VAE parameters are replicated across devices, their total size is relatively small (320MB in this case). To tackle the temporal memory spike issue, previous research [21, 44] proposed decomposing the execution of a single convolutional operator into multiple stages, processing portions of the input data sequentially to minimize temporary memory consumption. By combining these two methods, xDiT's VAE can generate an image resolution of 7168px, more than 12.25 times larger than using the naive VAE approach on 8\u00d7L40 (48GB). The detailed results are presented in Sec. 5.3."}, {"title": "5 Experiments", "content": "5.1 Setups\nIn our experimental setup", "insight.\nPixart": "Figure 14 illustrates the scalability of various parallel strategies on Pixart for generating image resolutions of 1024px", "34": ".", "methods.\nSD3": "In evaluations for SD3", "fastest.\nFlux.1-dev": "Figure 12 shows the scalability of Flux.1 on two 8\u00d7L40 Nodes. For the same reason as SD3", "tasks.\nCogVideoX": "CogVideoX-5B generates 49 frames of 720x480 video using the 50-Step DDIM scheduler. PipeFusion has not yet been applied due to the distinct Temporal Redundancy characteristics [31", "46": "of video models compared to image models and the lack of comprehensive research in this area. xDiT employs a hybrid parallel of SP and CFG parallel.\nThe best hybrid parallel configurations for different degrees of parallel are illustrated in Figure 13. Due to the height=480 limitation", "bandwidth.\nPixart": "Figure 14 presents the scalability of Pixart on 8\u00d7A100 GPUs. For the 4096px task", "follows": "cfg=2 for 2 GPUs, cfg=2 and pipefusion=2 for 4 GPUs, cfg=2 and pipefusion=4 for 8 GPUs. DistriFusion and PipeFusion as individual parallel methods also exhibit favorable latencies"}]}