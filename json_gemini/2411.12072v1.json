{"title": "Zoomed In, Diffused Out: Towards Local Degradation-Aware Multi-Diffusion for Extreme Image Super-Resolution", "authors": ["Brian B. Moser", "Stanislav Frolov", "Tobias C. Nauen", "Federico Raue", "Andreas Dengel"], "abstract": "Large-scale, pre-trained Text-to-Image (T2I) diffusion models have gained significant popularity in image generation tasks and have shown unexpected potential in image Super-Resolution (SR). However, most existing T2I diffusion models are trained with a resolution limit of 512\u00d7512, making scaling beyond this resolution an unresolved but necessary challenge for image SR. In this work, we introduce a novel approach that, for the first time, enables these models to generate 2K, 4K, and even 8K images without any additional training. Our method leverages MultiDiffusion, which distributes the generation across multiple diffusion paths to ensure global coherence at larger scales, and local degradation-aware prompt extraction, which guides the T2I model to reconstruct fine local structures according to its low-resolution input. These innovations unlock higher resolutions, allowing T21 diffusion models to be applied to image SR tasks without limitation on resolution.", "sections": [{"title": "1. Introduction", "content": "Image Super-Resolution (SR) is vital for a wide range of real-world applications, including satellite imaging, medical diagnostics, and consumer photography, where high-resolution outputs (e.g., 2K, 4K, or 8K) are essential for capturing fine details and ensuring clarity. Although SR methods, particularly those using local operations like CNNs, have made significant progress, handling complex degradation in Low-Resolution (LR) inputs remains a persistent challenge.\nRecently, diffusion models, particularly pre-trained Text-to-Image (T2I) diffusion models, have revolutionized image generation tasks . Originally designed for creative applications like text-guided image synthesis, these models have demonstrated strong potential in image SR, especially in handling 4x scaling and beyond, where hallucinating fine details becomes essential. Diffusion-based SR models, such as SR3, DiffBIR, and SRDiff, have already achieved impressive results by generating realistic details at these scales using conventional SR datasets and training them from scratch.\nOn the other side, the development of T2I models accelerates, as exemplified by astonishing applications like Dall-E and StableDiffusion. Driven by training on increasingly large and diverse datasets, they present a great resource for image SR. By repurposing their vast generative capabilities, these models can unlock new possibilities for High-Resolution (HR) image enhancement. Inspired by this idea, methods like StableSR, PASD, and SeeSR have repurposed pre-trained T2I models to SR, achieving impressive results in limited-resolution scenarios.\nYet, all T2I diffusion models repurposed for image SR are limited by a resolution of 512\u00d7512, which is impractical for real-world applications and a major limitation."}, {"title": "2. Related Work", "content": "This section briefly reviews the current field of image SR and how T2I is exploited for image SR."}, {"title": "2.1. Image SR", "content": "Image SR has seen significant advancements through the development of CNNs. Classical CNN-based SR models (including vision transformers), such as RRDB, ESRGAN, SwinIR, and HAT, excel in upscaling images to any resolution due to their local operations. They leverage local receptive fields, enabling them to process and train on relatively small image patches (e.g., 192x192) while generalizing well to larger images. This property makes them highly scalable for SR tasks across various resolutions.\nMore recently, diffusion models have emerged as powerful alternatives for SR, particularly in handling higher scaling factors such as 4x and beyond. Models like SR3, DiffBIR, and SRDiff have demonstrated strong capabilities in hallucinating fine details required at these scales. However, these diffusion-based SR models typically need to be trained on the target size and are often limited to resolutions of 512x512 due to architectural and training constraints, which restrict their flexibility and practicality compared to CNN-based methods that can be applied to any resolution.\nIn contrast, our approach introduces a novel method to scale diffusion models beyond 512x512 without retraining, which we termed extreme image SR (i.e., 2K, 4K, and 8K)."}, {"title": "2.2. Exploiting T21 for SR", "content": "Text-to-Image (T2I) conditioning has emerged as a promising approach for enhancing Super-Resolution (SR) tasks by leveraging the capabilities of pre-trained T2I models. Fine-tuning these models, with the addition of specialized encoders suited for SR, allows for integrating textual descriptions into image enhancement. This fusion of textual information can provide a richer guidance source, potentially improving the accuracy and contextual relevance of synthesized images in SR applications.\nFor instance, Wang et al. introduced StableSR, which exploits text guidance by incorporating a time-aware encoder trained alongside a frozen Latent Diffusion Model (LDM) . This framework utilizes trainable spatial feature transform layers to condition the model based on input images. An optional controllable feature wrapping module further enhances StableSR's adaptability, allowing for fine-grained user control. The design of this module draws inspiration from CodeFormer, contributing to StableSR's flexibility in addressing diverse user needs and preferences.\nSimilarly, Yang et al. proposed Pixel-Aware Stable Diffusion (PASD), which advances the conditioning process by embedding text descriptions of LR inputs using a CLIP text encoder . This method improves the model's capacity to synthesize images with greater precision by embedding contextual information from textual sources, enhancing the overall fidelity and relevance of the generated images.\nConcurrently, other methods like SeeSR explore similar T2I conditioning frameworks, while XPSR takes this concept further by merging different levels of semantic text encodings. XPSR combines high-level encodings (image content) with low-level encodings (quality perception, sharpness, noise, and other LR image distortions), further refining the SR results through a more nuanced understanding of both content and perceptual quality."}, {"title": "3. Methodology", "content": "This section introduces our novel approach to extreme image SR by leveraging pre-trained T2I diffusion models. For the first time, our method enables T2I models repurposed for SR to achieve resolutions of 2K, 4K, and 8K without additional training but with local coherence. The approach is built upon two core innovations: MultiDiffusion, which ensures global coherence at high resolutions, and local degradation-aware prompt extraction, which enhances local detail recovery. Figure 2 illustrates the overall concept of our method."}, {"title": "3.1. MultiDiffusion Process", "content": "MultiDiffusion (MD) allows the model to generate high-resolution images by distributing the image synthesis across multiple diffusion paths . At each stage, the latent feature map is divided into overlapping tiles, each of which undergoes a separate diffusion process. This technique maintains global coherence by sharing overlapping information between adjacent tiles. The final image is synthesized by merging the outputs from these multiple paths, ensuring consistency in both global structure and local detail. Unlike traditional T2I diffusion methods, which are limited to 512x512 pixels, this process enables resolutions of 2K and beyond without any retraining or fine-tuning.\nMore concretely, let $\\Phi : \\mathcal{L} \\times \\mathcal{Y} \\rightarrow \\mathcal{L}$ be a pre-trained T2I model that operates in the latent space $\\mathcal{L} = \\mathbb{R}^{W \\times H \\times C}$ and condition space $\\mathcal{Y}$, e.g., textual prompts. Given a noisy latent representation $\\mathcal{L}_T \\sim \\mathcal{N}(0, I)$ and a condition $y \\in \\mathcal{Y}$, the T2I diffusion model $\\Phi$ produces a sequence of latents starting from $\\mathcal{L}_T$ and gradually denoising it towards the clean latent representation $\\mathcal{L}_0$ that can be decoded to a HR approximation:\n$\\mathcal{L}_T, \\mathcal{L}_{T-1},\\cdots, \\mathcal{L}_0$ such that $\\mathcal{L}_{t-1} = \\Phi(\\mathcal{L}_t | y)$.\nSince we want to unlock the generation of images larger than 512\u00d7512 pixels (i.e., latent codes larger than 64\u00d764), our goal is the generation latent representations $\\mathcal{M}_T, \\mathcal{M}_{T-1},..., \\mathcal{M}_0$ in a new latent space $\\mathcal{M} = \\mathbb{R}^{W' \\times H' \\times C'}$, with $W' > W, H' > H$, using the same pre-trained model $\\Phi$ without any retraining or fine-tuning. Traditionally, models pre-trained on fixed-size latents cannot be directly applied to produce latents of arbitrary sizes .\nTo address this, MD extends the diffusion process by applying a joint diffusion approach, where multiple overlapping latent windows are merged via averaging. More formally, we define $n$ mappings $F_i : \\mathcal{M} \\in \\mathbb{R}^{W' \\times H' \\times C} \\rightarrow \\mathcal{L} \\in \\mathbb{R}^{W \\times H \\times C}$ with $i \\in \\{1, ..., n\\}$, which map (or crop) the larger latent space $\\mathcal{M}$ into $n$ latent representations of the original size $W \\times H$ (i.e., 64\u00d764).\nThe number of overlapping latent crops $n$ is defined as $n = \\frac{w'-W}{w} + 1$, where $w$ represents the stride between adjacent cropping windows. With these mappings, the denoising process is applied independently to each cropped latent window. Subsequently, the latent representations are stitched back to the original size $\\mathcal{M} = \\mathbb{R}^{W' \\times H' \\times C}$. i.e. $\\mathcal{M}_{t-1} = \\text{MultiDiffuser}(\\{\\mathcal{L}_{t-1}^{i}\\}_{i=1}^{n})$, which averages the overlapping latent regions."}, {"title": "3.2. Local Degradation-Aware Prompt Extraction", "content": "As demonstrated in the introductory example in Figure 1, solely applying MD often results in images with excessive hallucinated details. This occurs because the global prompt used for classifier guidance in the diffusion process applies uniformly across all tiles. While the prompt effectively describes the global structure of the image, it lacks the local content specificity needed for each tile. Consequently, the model attempts to reconstruct fine details in every tile based on global information, leading to over-hallucination and inconsistencies in local structures.\nMore formally, let $y \\in \\mathcal{Y}$ be a condition drawn from a prompt extractor. As $F_i$ is guiding the diffusion model $\\Phi$ with the same prompt $y$, i.e., $\\Phi(L_i | y)$ for all possible windows $i \\in \\{1, ..., n\\}$, the larger latent feature map resulting from this MD process will have the conditioning information $y$ infused at every spatial position. Hence, global prompts alone are insufficient for reconstructing region-specific content, as they lack the granularity to capture the unique local details within each patch.\nTo ensure the coherence of local structure and details, we propose a local degradation-aware prompt extraction, which describes the local degradation patterns in LR inputs (see next Section for concrete extractor). In more detail, if $n$ is the number of maps (or crops), we propose to condition the MD process with a set of $n$ conditions, i.e., $\\{y_1, \u2026\u2026\u2026, y_n\\}$, where $F_i$ uses the condition $y_i$. These prompts guide the diffusion model in reconstructing detailed textures and structures during the SR process. By leveraging these prompts, our method effectively reduces artifacts and noise, particularly at high resolutions (e.g., 2K, 4K, and 8K), and ensures accurate restoration of fine-grained details. As a result, our approach, as outlined in Algorithm 1, significantly mitigates the common issues of over-hallucination and visual artifacts that occur when relying solely on global prompt guidance."}, {"title": "3.3. Exploiting Pre-Trained Prompt Extractors", "content": "Our method is designed to be flexible and compatible with any pre-trained T2I diffusion model and prompt extraction strategy. Still, in this work, we specifically build upon the pre-trained models utilized by SeeSR . Specifically, we utilize the pre-trained StableDiffusion V2.0 model, along with its 8\u00d7 compression VAE, and the Degradation-Aware Prompt Extractor (DAPE), which itself is a fine-tuned tag-style prompt extraction model, i.e., RAM . These components have proven effective for real-world SR tasks, allowing us to leverage their robust generative capabilities for higher-resolution image synthesis.\nFor the MD process, we adopt the standard configuration of dividing the latent space into 64x64 patches . This approach ensures efficient global coherence across the image while maintaining the fine-grained structure. Inspired by SpotDiffusion, we reduce the overlap between patches by setting the stride to 32, ensuring a balance between computational efficiency and image consistency . Notably, we chose not to apply the non-overlapping striding strategy proposed in SpotDiffusion, as it significantly degraded the SR results for LR inputs by failing to capture necessary contextual information across patches.\nFor the local degradation-aware prompt extraction, we apply DAPE on the corresponding tiles in image space (i.e., extracted from 512\u00d7512 image patches) that would undergo the individual diffusion steps in $F_i$ in latent space. This extraction ensures that each patch receives locally relevant prompts, enhancing fine detail restoration. Interestingly, this process results in averaging overlapping latent patches generated under varying prompt conditions. This dynamic use of local prompts helps prevent inconsistencies between adjacent patches and avoids over-hallucination, which is common when using global prompts in MD ."}, {"title": "4. Experimental Setup", "content": "In this section, we detail the datasets, models, and evaluation metrics used to assess the performance of our proposed method, comparing it against both classical and diffusion-based SR models across various extreme-resolution tasks. The code for our experiments can be found on GitHub 1, which complements the official implementation of SeeSR."}, {"title": "4.1. Datasets", "content": "For our experiments, we utilized the DIV2K validation set as well as the Test4K and Test8K datasets introduced by ClassSR. These datasets provide a robust benchmark for SR tasks, allowing us to evaluate the performance of our method across a range of high resolutions (i.e., 2K, 4K, and 8K), which we coin extreme image SR."}, {"title": "4.2. Models & Metrics", "content": "We compare a variety of standard SR approaches, including regression-based methods like EDSR, RRDB, and CAR, GAN-based methods such as RankSRGAN and ESRGAN, as well as the normalizing flow approach SRFlow . Additionally, we include diffusion models that are explicitly trained for image SR, such as SR3+YODA, SRDiff, and DiWa, to provide a broader comparison .\nTo evaluate the quality of the reconstructed images, we use both pixel-based metrics, namely PSNR and SSIM, as well as the perceptual quality metric LPIPS. Pixel-based metrics measure structural similarity and reconstruction accuracy, while LPIPS provides a more human-perception-aligned evaluation of image quality, allowing us to assess both fidelity and perceptual realism. Note that classical SR approaches typically exhibit much higher pixel-based scores than diffusion models, as diffusion models are prone to hallucinated details."}, {"title": "5. Results", "content": "This section presents qualitative and quantitative comparisons between our approach and SeeSR modified with MD (leading in SR-repurposed T2I methods). Since no pre-trained T2I models have been previously tested for extreme image SR (resolutions larger than 512\u00d7512), SeeSR with MD serves as our primary baseline for evaluation."}, {"title": "5.1. Qualitative Results", "content": "To assess the effectiveness of our proposed approach visually, we focus on challenging yet visible cases from the DIV2K validation set to highlight the improvements in fine detail preservation, local coherence, and overall perceptual quality. As shown in Figure 3, our method demonstrates significant improvements in recovering fine details and generating more coherent structures compared to SeeSR+MD. In the 2K resolution image, our approach not only preserves the global structure but also reconstructs background objects, such as the light patterns in the lower-left corner, with more accuracy and clarity (i.e., without artifacts). This is due to the local degradation-aware prompt extraction, which enables more precise local detail reconstruction, avoiding the hallucination of irrelevant global features.\nIn Figure 4 and Figure 5, we further illustrate how our method handles the presence of background textures by taking a closer look at zoomed-in regions. In the first example (Figure 4), SeeSR+MD hallucinates feather-like patterns in the leaves due to the global prompt guidance, leading to inconsistent results (see wavy structures above the leaf). By contrast, our method maintains local coherence and reconstructs the leaves more naturally and accurately. For the second example (Figure 5), we can observe fur-like patterns in the dirt in SeeSR, contrary to ours. Although both methods occasionally introduce more fine-grained details than those in the original HR image, our method avoids artifacts and hallucinations, ensuring perceptually consistent results.\nOverall, our qualitative results demonstrate that the local degradation-aware prompt extraction significantly outperforms SeeSR+MD in generating high-quality super-resolved images, particularly at extreme resolutions. The improvements are especially notable in preserving local textures and avoiding over-hallucination, which is a common issue in MD-based models guided by global prompts. The addition of local degradation-aware prompts not only ensures more consistent textures but also provides a sharper, artifact-free reconstruction of fine details, making our approach particularly advantageous for complex scenes where global prompts alone fail to maintain coherence."}, {"title": "5.2. Quantitative Results", "content": "To thoroughly evaluate the performance of our method, we conducted experiments on the DIV2K validation set, as well as the Test4K and Test8K datasets. Our results are compared against various existing SR methods, including regression-based, GAN-based, and diffusion-based approaches (trained explicitly on SR data).\nAs shown in Table 1, our method outperforms SeeSR equipped with MD regarding perceptual quality metrics such as LPIPS while achieving competitive results on pixel-based metrics like PSNR and SSIM. Specifically, for 4x SR on DIV2K, our method achieved a PSNR of 24.34, SSIM of 0.68, and LPIPS of 0.108, representing a clear improvement in image quality compared to SeeSR+MD."}, {"title": "5.3. User Study", "content": "Inspired by Saharia et al. (SR3 [37]), we conducted a 2-alternative forced-choice user study. We asked 25 subjects \"Which of the two images is a better high-quality version of the low-resolution image in the middle?\" We selected 36 random 2K test images from DIV2K Val (12 for each method: RRDB, SeeSR+MD, and ours), which were center-cropped to 512 \u00d7 512 for better visual examination.\nThe result of our user study is shown in Figure 6. Our method not only surpasses RRDB and SeeSR+MD, achieving more than double their fool rates, but it also attains a fool rate close to 50%, rendering its outputs nearly indistinguishable from HR ground-truth images."}, {"title": "5.4. Prompt Analysis", "content": "We analyze the textual information added by our local degradation-aware prompt extraction in DIV2K Val by counting unique tags. For our method and the same LR image, we count common tags between the MD paths as one. Thus, a higher count implies more tailored guidance for different image areas, supporting our hypothesis that local prompts contribute to better restoration of complex structures.\nAs shown in Figure 7, our proposed local degradation-aware prompt extraction method generates significantly more unique tags than SeeSR. Also, it highlights that we generate a wider range of tags, allowing for a more precise description. Naturally, increased tag diversity improves local detail description across the image and is further evidenced in Figure 8: Our local degradation-aware prompt extraction includes the majority of tags produced by SeeSR but also adds distinct ones, as underscored by the red cluster."}, {"title": "6. Limitation & Future Work", "content": "While our proposed local degradation-aware prompt extraction method significantly improves the performance of T2I diffusion models for any-resolution image SR, it still has room for improvement compared to traditional SR methods, which are explicitly designed and trained for SR.\nFuture work could explore a hybrid approach that combines the strengths of traditional SR models with T2I models to address the performance difference compared to SR-trained methods like RRDB and ESRGAN. For instance, using a GAN or CNN-based model to generate an initial coarse super-resolved image and refinement using a diffusion model could lead to even better quality."}, {"title": "7. Conclusion", "content": "This work introduced a novel approach for extreme image SR by leveraging pre-trained T2I diffusion models. Through our proposed MD process and local degradation-aware prompt extraction, we enabled the generation of high-quality images at resolutions of 2K, 4K, and 8K without any additional training. By distributing the generation process across multiple diffusion paths, our method ensures global coherence, while local prompt extraction enhances fine-grained detail restoration. This addresses the limitations of traditional T2I models exploited for image SR, making them applicable to a broad range of tasks where clarity and precision at large scales are critical.\nOur extensive experiments show that our method significantly outperforms the baseline SeeSR+MD in both qualitative and quantitative metrics, particularly in challenging high-resolution scenarios. Enabling any high-resolutions, maintaining local coherence, and avoiding over-hallucination while generating visually accurate details sets a new benchmark for high-resolution SR using pre-trained T2I models."}]}