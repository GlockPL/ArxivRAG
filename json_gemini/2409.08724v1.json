{"title": "Quasimetric Value Functions with Dense Rewards", "authors": ["Khadichabonu Valieva", "Bikramjit Banerjee"], "abstract": "As a generalization of reinforcement learning (RL) to parametrizable goals, goal conditioned RL (GCRL) has a broad range of applications, particularly in challenging tasks in robotics. Recent work has established that the optimal value function of GCRL $Q^*(s, a, g)$ has a quasimetric structure, leading to targetted neural architectures that respect such structure. However, the relevant analyses assume a sparse reward setting-a known aggravating factor to sample complexity. We show that the key property underpinning a quasimetric, viz., the triangle inequality, is preserved under a dense reward setting as well. Contrary to earlier findings where dense rewards were shown to be detrimental to GCRL, we identify the key condition necessary for triangle inequality. Dense reward functions that satisfy this condition can only improve, never worsen, sample complexity. This opens up opportunities to train efficient neural architectures with dense rewards, compounding their benefits to sample complexity. We evaluate this proposal in 12 standard benchmark environments in GCRL featuring challenging continuous control tasks. Our empirical results confirm that training a quasimetric value function in our dense reward setting indeed outperforms training with sparse rewards.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) is a popular class of techniques for training autonomous agents to behave (near-)optimally, often without requiring a model of the task or environment. In goal-achieving tasks, traditional RL learns policies that reach a single goal at the minimum (maximum) expected cost (value) from any state. Contrastingly in multi-task settings, a goal conditioned value function models the cost-to-go to a set of goal states, not just one. This generalization from a single-goal case to goal-conditioned RL (GCRL) yields effective representations\u2014powered by deep neural networks\u2014for value functions capable of capturing abstract concepts underlying goal achievement in many complex tasks (M. Liu et al., 2022; Plappert et al., 2018; Wang et al., 2023).\nRecent work has established that the true optimal value function in GCRL is always a quasimetric, i.e., a metric without the constraint of being symmetric, but crucially respecting the triangle inequality (B. Liu et al., 2023; Pitis et al., 2020; Wang & Isola, 2022). This allows the search for value functions to be naturally restricted to the space of quasimetrics. Additionally, such functions are designed to be universal value function approximators (UVFA), i.e., capable of approximating arbitrarily complex value functions. Accordingly, B. Liu et al. (2023) propose the metric residual network (MRN) architecture for GCRL value functions that explicitly accommodate an asymmetric component while maintaining the UVFA property and the triangle inequality. This and other similar approaches search a smaller subset of the space of value functions, yet the"}, {"title": "2 Background", "content": "This section covers the preliminaries on goal conditioned RL, the prevalent solution approaches for GCRL, and the recent architecture of metric residual networks that we use in this paper."}, {"title": "2.1 Goal-conditioned RL", "content": "Goal conditioned RL is modeled by goal-conditioned Markov decision process, $M = (S, A, G,T, R, \\gamma, \\rho_0, \\rho_G)$. While $S, A, T, \\rho_0$ define the state action spaces, the transition function and the initial state distribution just like a standard MDP, $G$ gives the space of goal states, and $\\rho_G$ is the distribution from which a goal is sampled at the beginning of an episode. Further, the reward function $R$ is additionally parametrized by the goal, $R:S\\times A\\times G \\rightarrow R$. In the sparse reward setting, $R$ is often defined as\n$R(s, a, g) = \\begin{cases} 0 & \\text{if } M(s, a) = g \\\\ -1 & \\text{otherwise} \\end{cases}$                                          (1)\nwhere $M : S \\times A \\rightarrow G$ maps the product space of $S$ and $A$ to $G$. As opposed to the common assumption $G \\subseteq S$, $M$ allows action $a$ to decide whether the goal is reached (B. Liu et al., 2023)."}, {"title": "2.2 Solution Approach: DDPG+HER", "content": "A popular approach to solving GCRL is a combination of off-policy actor-critic, e.g., DDPG (Lillicrap et al., 2016) with hindsight experience replay (HER) (Andrychowicz et al., 2017). DDPG in GCRL estimates a goal conditioned critic\n$Q^{\\pi}(s, a, g) = E \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t,g} \\Big| s_0 = s, a_0 = a, g \\Big]$\nwhere the expectation is taken over future steps of rewards generated by the policy ($\\pi$), and the $T, R$ functions. The critic is updated by minimizing the mean squared TD error over samples $(s_t, a_t, s_{t+1},g)$ drawn from a replay buffer $D$,\n$L(Q) = E \\Big[ (r_{t,g} + \\gamma Q(s_{t+1},\\pi(s_{t+1}), g) - Q(s_t, a_t, g))^2 \\Big] .                                                               (2)$\nBy ensuring that $Q$ is differentiable w.r.t. actions $a$, the actor policy $\\pi$ is updated in the direction of the gradient $E[\\nabla_{a_t} Q(s_t, a_t, g)]$, where the expectation is again evaluated using samples drawn from $D$. As these samples are drawn from state distributions generated by policies different from $\\pi$, DDPG is an off-policy method, although it estimates $Q$-values in an on-policy way (Eq. 2). This last aspect will be scrutinized further in Sec. 3.2.\nHindsight experience replay (HER) (Andrychowicz et al., 2017) mitigates the sparse reward problem by relabeling failed trajectories. Instead of treating all experience traces where the agent failed to achieve a goal as is, HER changes the goal in some of them to match a step of the trace in hindsight-essentially pretending as if the agent's goal all along was to reach the state that it actually did. This transforms some of the failed episodes into successful experiences that are informative about goal achievement, and allows the agent to generalize, eventually, to the true goal distribution $\\rho_G$."}, {"title": "2.3 Metric Residual Network", "content": "B. Liu et al. (2023) propose a novel neural architecture for GCRL critic based on the insight that the optimal negated action-value function, $-Q^*(s, a, g)$, satisfies the triangle inequality in the sparse reward setting of Eq. 1. Consequently, they introduce the metric residual network (MRN) that decomposes $-Q$ into the sum of a metric and an asymmetric residual component that provably approximates any quasipseudometric. Specifically,\n$Q(s, a, g) = -\\big(d_{\\text{sym}}(h_{sa},h_{sg}) + d_{\\text{asym}}(h_{sa},h_{sg})\\big)$                                                          (3)\nwhere $h_{sa}$ and $h_{sg}$ are latent encodings of concatenated $(s, a)$ and $(s, g)$, $d_{\\text{sym}}$ and $d_{\\text{asym}}$ are symmetric and asymmetric distance components given by\n$d_{\\text{sym}}(x, y) = ||\\mu_1(x) - \\mu_1(y) ||,                                                                                                                                         (4)$\n$d_{\\text{asym}}(x, y) = \\max_i (\\rho_{2i}(x) - \\rho_{2i}(y))_+,                                                                                                                                 (5)$\n$\\mu_1$ and $\\mu_2$ are neural networks. The provable UVFA property of MRNs is due to $d_{\\text{asym}}$, while $d_{\\text{sym}}$ improves sample efficiency due to its symmetry. We use DDPG+HER with MRN critic architecture as the base GCRL method for this paper."}, {"title": "3 Triangle Inequality", "content": "In this section, we establish that both the optimal value function as well as on-policy value functions satisfy the triangle inequality under novel conditions."}, {"title": "3.1 Optimal Value Function", "content": "Our primary claim is that $-Q^*$ satisfies the triangle inequality not only in the sparse reward setting, but also in the presence of dense rewards, particularly potential shaped rewards. This observation lends GCRL to improved sample efficiency when approximating $-Q^*$ using a combination of MRN and potential shaped rewards.\nWe use the standard potential based shaping rewards\n$F(s,a, s', a',g) = \\gamma\\phi(s', a', g) - \\phi(s, a, g)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (6)\nand a simple potential function\n$\\phi(s, a, g) = \\Big( 1 - \\gamma \\frac{d(s,a,g)/\\eta}{1-\\gamma} \\Big)$\nwhere $d$ is a distance measure between the state and the goal, and $\\eta$ is a measure of the atomicity of actions-distance covered per time step. Note that in the reward regime of Eq. 1,\n$Q^*(s, a, g) = -\\frac{1}{1-\\gamma}$\nwhere $L^*$ is the optimal expected number of steps required to reach the goal $g$ from state $s$.\nObservation 1. If $d(s, a, g) \\leq \\eta L^*$, then $\\phi(s, a, g) \\geq Q^*(s, a, g),\\forall s,a, g$\nIn other words, if $d(s,a,g)/\\eta$ is an underestimate of $L^*$ then the above condition will be satisfied. Thus, $d$ acts as an admissible heuristic. For this paper, we choose a simple arc-cosine distance $d(s, a, g) = \\cos^{-1} \\big(\\frac{M(s,a).g}{||M(s,a)||||g||}\\big)$, which is known to be a metric. Here $M$ is defined in the context of Eq. 1. However, this choice is not necessary for our theoretical results to hold. Rather, it is prompted by its boundedness and our desire to avoid intricate, environment-specific reward engineering.\nWe distinguish $Q^*(s, a, g)$\u2014the optimal action values with unshaped sparse rewards\u2014from $\\tilde{Q}(s, a, g)$ which corresponds to action values with rewards shaped by $F$ in Eq. 6. Next we establish the validity of triangle inequality with $\\tilde{Q}$ in two cases: (i) $G = S \\times A$ and (ii) $G \\neq S \\times A."}, {"title": "3.1.1 Case I: G = S \u00d7 A", "content": "In this setting, $M$ is the identity mapping. We use the notation $x_t = (s_t, a_t)$. The main result is:\nProposition 1. Consider the shaped, goal-conditioned MDP $M_{GCF} = (S,A,G,T, R + F, \\gamma, \\rho_0, \\rho_G)$, with $G = S \\times A$. The optimal universal value function $\\tilde{Q}^*$ satisfies the triangle inequality: $\\forall x_1,x_2, x_3 \\in X,$\n$\\tilde{Q}^*(x_1, x_2) + \\tilde{Q}^*(x_2, x_3) \\leq \\tilde{Q}^*(x_1, x_3),$\nThe only condition $\\phi$ must satisfy is\n$\\phi(s, a, g) \\geq Q^*(s, a, g),\\forall s, a, g$                                                                                                                      (7)\nw.r.t. the unshaped value function, for which a sufficient condition is established in Obs. 1.\nProof: As in (B. Liu et al., 2023), consider the Markov policies $\\pi_1, \\pi_2, \\pi_3$ that are optimal w.r.t. $\\tilde{Q}^*(x_1, x_2), \\tilde{Q}^*(x_2, x_3), \\tilde{Q}^*(x_1, x_3)$ and the (non-Markov) policy $\\pi_{1\\rightarrow2}$ defined for $t > 0$ as:\n$\\pi_{1\\rightarrow2}(a | s_t) = \\begin{cases} \\pi_1(a | s_t), & x^2 \\notin \\{x_0,\\dots, x_{<t} \\} \\\\ \\pi_2(a | s_t), & \\text{otherwise}. \\end{cases}$"}, {"title": "3.1.2 Case II: G \u2260 S \u00d7 A", "content": "The proof of this case closely resembles (B. Liu et al., 2023); we highlight the main difference in blue color but also provide the rest of the proof for completeness. In this case, $M$ is an onto mapping. Given a goal $g$, the GCRL problem effectively reduces to a standard MDP and there exists a deterministic optimal policy $\\pi^*$ for reaching the goal $g$ from an initial state $x = (s_0, a_0)$. Then, under deterministic dynamics,\n$Q(x, g) = \\sup_{x':M(x')=g} \\tilde{Q}(x, x').$\nAssuming the supremum is attainable, let\n$x_g = \\arg \\max_{x':M(x')=g} \\tilde{Q}(x, x'),$                                                                                                                                     (10)\nthen $Q(x, g) = \\tilde{Q}(x, x_g)$. Assume for contradiction that this is not the case, i.e., $Q(x, g) \\neq \\tilde{Q}(x, x_g)$. There are two possibilities:\n\\begin{itemize}\n    \\item If $\\tilde{Q}(x,x_g) > \\tilde{Q}(x,g)$: This would imply that by using a policy that selects $x_g$ rather than $g$, one could achieve a higher return. This contradicts the definition of $\\pi^*$ as the optimal policy, thus $\\tilde{Q}(x, x_g) > \\tilde{Q}(x, g)$ cannot be true.\n    \\item If $\\tilde{Q}(x, x_g) < \\tilde{Q}(x, g)$: Let\n$\\tau = \\min_t \\{M(x_t) = g \\},$ such that $x_\\tau$ is the first $(s, a)$ pair along the optimal trajectory that achieves the goal. There are two further cases:\n    \\begin{enumerate}\n        \\item After reaching $x_\\tau$, $\\pi^*$ will repeatedly return to $x_\\tau$. In this case, we have $\\tilde{Q}(x, x_g) > \\tilde{Q}(x, x_\\tau)$ by the definition of $x_g$ (Eq. 10) and\n$\\tilde{Q}(x, x_\\tau) = \\tilde{Q}(x, g) - \\gamma^{\\tau+1}\\tilde{Q}^*(x_\\tau,g)$\n$\\tilde{Q}(x, g) - \\gamma^{\\tau+1} [Q^*(x_\\tau, g) - \\phi(x_\\tau,g)]$\n$> \\tilde{Q}(x, g)$, by Eq. 7.                                                                                    (11)\nCombining the two, we get $\\tilde{Q}(x, x_g) \\geq \\tilde{Q}(x, g)$ which contradicts our assumption that $\\tilde{Q}(x, g) > \\tilde{Q}(x, x_g)$.\n        \\item $\\pi^*$ never returns to $x_\\tau$ after reaching it for the first time. In this case, one can find the next $\\tau' = \\min_{t>\\tau} \\{M(x_t) = g \\}$, such that $x_{\\tau'}$ is another $(s, a)$ along the optimal trajectory. Again, there are two sub-cases:\n            \\begin{enumerate}\n                \\item If $\\pi^*$ repeatedly visits $x_{\\tau'}$, then the argument in the first case applies.\n                \\item Otherwise, recursively find the next $\\tau''$, and so on. Eventually, we may have a last state $x_{\\tau}$ such that no $t > \\tau$ satisfies $M(x_t) = g$. Then, $\\tilde{Q}(x, x_g) \\geq \\tilde{Q}(x,x_\\tau) \\geq \\tilde{Q}(x,g)$. The last inequality is derived in the same way as Eq. 11. Alternatively, there may exist an infinite sequence of such $\\{x_\\tau\\}$. Following this sequence, the claim remains true but the supremum is not attainable. However, in this case an $x_\\tau$ can be found in the sequence such that $\\tilde{Q}(x, x_\\tau)$ is arbitrarily close to $\\tilde{Q}(x, g)$.\n            \\end{enumerate}\n    \\end{enumerate}\n\\end{itemize}"}, {"title": "3.1.3 Projection", "content": "$\\tilde{Q}$ has the same upper bound as $Q^*$, since $\\tilde{Q}^*(s,a,g) = Q^*(s, a, g) - \\phi(s,a,g) \\leq 0$ by Eq. 7. Consequently, the MRN architecture needs no modification, specifically to Eq. 3, as the critic output is guaranteed to be non-positive despite potentially positive shaping rewards. However, $\\tilde{Q}$ has a more informed lower bound:\n$\\tilde{Q}(s, a, g) = Q^*(s, a, g) - \\phi(s, a, g)$\n$> \\frac{1}{1-\\gamma} - \\gamma \\frac{d(s,a,g)/\\eta}{1-\\gamma}$\n$= -\\phi(s, a, g)$                                                                                                                                        (12)\nwhich we impose on the critic. Recent analyses (Gupta et al., 2022) have shown that projection informed by shaping effectively reduces the size of the state space for exploration, leading to improved regret bounds."}, {"title": "3.2 On-Policy Value Functions", "content": "In their critique of on-policy Q-function estimation methods for GCRL such as DDPG in continuous control tasks, Wang et al. (2023) show that on-policy Q-function may not be a quasimetric, even though the optimal Q-function is. However, their counterexample is an extreme policy that is unlikely to be encountered during on-policy iterations. In this section, we establish that on-policy Q-functions do indeed satisfy the triangle inequality (and hence meet the quasimetric criterion) if the policy makes a minimal progress toward the goal. We call such policies progressive policies and believe they are more relevant to on-policy Q-function estimation in GCRL. We first formalize the notion of progressive policies, specify our assumption, and finally show that the corresponding value functions satisfy the triangle inequality.\nFor notational convenience, we write $E_{s'\\sim T(.|s,a),a'\\sim\\pi(s')}$ simply as $E_{s',a'}$. Note that the on-policy value function for a policy $\\pi$ satisfies\n$Q^{\\pi} (s, a, g) = R(s,a,g) + \\gamma E_{s',a'} [Q^{\\pi}(s', a',g)]                                                                                                   (13)\nDefinition 1. The progress of a GCRL policy $\\pi$ is given by\n$\\Delta^{\\pi} (s,a,g) = E_{s',a'} [Q^*(s', a',g)] - Q^*(s,a,g)$\nfor any $(s, a, g) \\in S \\times A \\times G$.\nWe refer to $\\Delta^{\\pi}$ for the optimal policy as $\\Delta^*$. We assume that the progress of $\\pi$ is not unboundedly different from that of the optimal policy, i.e., the following holds for some $0 < \\epsilon < \\infty$\n$\\epsilon < \\Delta^*(s, a, g) - \\Delta^{\\pi} (s, a, g) \\leq 2\\epsilon.$                                                                                              (14)\nNote that (i) $\\epsilon$ does not need to be small, just finite; (ii) the counterexample in Wang et al. (2023) does not satisfy this assumption. Our main result of this section is:\nProposition 2. Consider the goal-conditioned MDP $M_{GC} = (S, A, G,T, R, \\gamma, \\rho_0, \\rho_G)$. The on-policy value function $Q^{\\pi}$ defined in Eq. 13 for any policy $\\pi$ that satisfies Eq. 14 also satisfies the triangle inequality: $\\forall x_1, x_2, x_3 \\in X,$\n$Q^{\\pi} (x_1, x_2) + Q^{\\pi} (x_2, x_3) < Q^{\\pi} (x_1,x_3).$"}, {"title": "4 Related Work", "content": "Several value function representations have been proposed for GCRL over the last decade. Schaul et al. (2015) introduced the bilinear decomposition, later generalized to bilinear value networks (Yang et al., 2022) with better learning efficiency. Pitis et al. (2020) proposed the deep norm (DN) and wide norm (WN) families of neural representations that respect the triangle inequality. However, they are restricted to norm-induced functions, and are generally unable to represent all functions that respect the triangle inequality. By contrast, Possion Quasi-metric Embedding (PQE) (Wang & Isola, 2022) can universally approximate any quasipseu-dometric, thus improving upon DN/WN. However, as B. Liu et al. (2023) argue, PQE captures the restrictive form of first hitting-time when applied to GCRL, whereas MRNs capture the more general setting of re-peated return to goal ($Q^*(g, g) \\neq 0$), while preserving the UVFA property of PQEs. Durugkar et al. (2021) introduced a quasimetric that estimates the Wasserstein-1 distance between state visitation distributions, min-imizing which is equivalent to policy optimization in GCRL tasks with deterministic transition dynamics."}, {"title": "5 Experimental Results", "content": "We use GCRL benchmark manipulation tasks with the Fetch robot and Shadow-hand domains (Plappert et al., 2018); see Fig. 1. MRN has been extensively compared with competitive baseline architectures and found to be superior, viz., BVN (Yang et al., 2022), DN/WN (Pitis et al., 2020), and PQE (Wang & Isola, 2022). Consequently, we focus on comparing against MRN with sparse rewards as the sole baseline.\nWe experimentally evaluate the following hypotheses:\nHypothesis 1: Dense rewards can be used in conjunction with MRN architecture for estimating value functions. Specifically, the property of $Q^*$ function that MRNs capture that it satisfies the triangle inequality- is preserved in the presence of shaped rewards with the new value function $\\tilde{Q}$. Dense rewards enable the less restrictive $\\tilde{Q}$ to be learned more efficiently than $Q^*$.\nHypothesis 2: Plappert et al. (Plappert et al., 2018) found that dense rewards hurt RL performance in GCRL robot manipulation tasks. This negative result contradicts our Hypothesis 1. We conjecture that their application of dense rewards did not satisfy the required structure-specifically Eq. 7-which is why it failed.\nTo confirm this contradiction, we verify that our dense reward setting does not deteriorate RL performance in any task.\nWe use the MRN code publicly available at: https://github.com/Cranial-XIX/metric-residual-network with simple modifications to add Eq. 6 to the reward function and Eq. 12 to clip the critic's output. No other changes were made to any algorithm or neural architecture. In particular, all parameter values (e.g. layer sizes) were unchanged, except the newly added parameter $\\eta$ was set to 0.02. This value was selected from the set \\{0.01,0.02, 0.03, 0.04, 0.05\\} using performance improvement as the criterion. For each environment, 5 seeds were used for independent trials, as in (B. Liu et al., 2023). In each epoch, the agent is trained on 1000 episodes and then evaluated over 100 independent rollouts with randomly sampled goals. The average success rates in these evaluations are collected over 5 seeds. The results are plotted in Fig. 2. All experiments were run on NVIDIA Quadro RTX 6000 GPUs with 24 GiB of memory each and running on Ubuntu 22.04.\nWe see from Fig. 2 that indeed dense rewards improve the sample complexity in some environments,"}, {"title": "6 Conclusion", "content": "We have presented generalizations of previous results on triangle inequality in the context of value functions in GCRL. Specifically, we have shown that the optimal value function satisfies the triangle inequality even when the reward function is densified with a particular class of shaping functions. Additionally, we have shown that the on-policy value functions also satisfy the triangle inequality if the underlying policy satisfies a certain progressive criterion. Both of these findings contradict previously published results in some ways, which emphasizes the importance of the nuanced conditions behind our results. Experiments in 12 benchmark GCRL tasks confirms that dense rewards only improve the sample efficiency, never deteriorates it. Future investigations could focus on more general classes of reward functions that preserve the quasimetric property of value functions and/or lend themselves to other, potentially more effective, architectures."}]}