{"title": "AI-driven Transformer Model for Fault Prediction\nin Non-Linear Dynamic Automotive System", "authors": ["Priyanka Kumar"], "abstract": "Fault detection in automotive engine systems is\none of the most promising research areas. Several works have\nbeen done in the field of model-based fault diagnosis. Many\nresearchers have discovered more advanced statistical methods\nand algorithms for better fault detection on any automotive\ndynamic engine system. The gas turbines/diesel engines produce\nhighly complex and huge data which are highly non-linear. So,\nresearchers should come up with an automated system that is\nmore resilient and robust enough to handle this huge, complex\ndata in highly non-linear dynamic automotive systems. Here, I\npresent an AI-based fault classification and prediction model in\nthe diesel engine that can be applied to any highly non-linear\ndynamic automotive system. The main contribution of this paper\nis the AI-based Transformer fault classification and prediction\nmodel in the diesel engine concerning the worldwide harmonic\nlight vehicle test procedure (WLTP) driving cycle. This model\nused 27 input dimensions, 64 hidden dimensions with 2 layers,\nand 9 heads to create a classifier with 12 output heads (one\nfor fault-free data and 11 different fault types). This model was\ntrained on the UTSA Arc High-Performance Compute (HPC)\ncluster with 5 NVIDIA V100 GPUs, 40-core CPUs, and 384GB\nRAM and achieved 70.01% accuracy on a held test set.", "sections": [{"title": "I. INTRODUCTION", "content": "Diesel engines have been used in naval vessels for many\nyears due to their reliability, durability, and efficiency. It is\nalso used in auxiliary systems on board naval vessels, such\nas electrical generators and pumps. So these engines play\na critical role in maintaining the operational readiness of\nnaval vessels and are an essential component of modern naval\nwarfare. In the modern era, misdiagnosis and robustness are\nmajor concerns in any automotive vehicular system. Research\nchallenges in the current systems include:\n1) Robustness is one of the major concerns in any automo-\ntive or diesel engine system for critical applications. The\nreason behind this is that many manufacturers make their\nproducts and sell their vehicles all over the world. So, it\nis a challenging problem for manufacturers to come up\nwith one resilient fault detection method with respect\nto wide variations in weather, different driving styles,\nheavy traffic conditions, etc.\n2) With the advancement of technologies, the development\nof autonomous vehicles/engines is a special concern.\nThe onboard computer needs to keep track of the en-\ngine's health without the aid of a human operative. So,\nit is of utmost importance to come up with a self-aware\nsystem that will take care of their occupant's life and\nhealth by self-diagnosis and self-healing continuously.\n3) These automotive engines produce huge data that is\ncomplex in nature and difficult to handle. So, the Internet\nof Things, allowing for edge computing and requiring\nless energy to operate is critical for exploratory vehicles.\nWith these motivations, I designed a Transformer model\nthat is beneficial in fault classification and prediction\nfor diesel engines due to their advanced capabilities\nin handling sequential data and capturing long-range\ndependencies. Due to self-attention mechanisms, it fo-\ncuses on the different parts of the sequences and on\ncritical aspects of the data that are most indicative of\nfaults. This is especially useful for identifying subtle and\ncomplex patterns in engine sensor data that could signal\nimpending faults. Diesel engines produce a multitude of\nsensor data streams. Transformers are adept at process-\ning multivariate time series data, making them suitable\nfor analyzing the various types of signals generated by\nengine sensors."}, {"title": "II. BACKGROUND", "content": "In most automotive systems, diagnostic systems monitor\nvarious components of the engine system and are independent\nof each other. The purpose of the system is then to map the\nchanges in the sensor data to various fault types that may\nor may not be dependent on each other. Due to the dependent\nnature of the faults, a highly intelligent algorithmic framework\nis needed to isolate individual faults. Previous methodologies\nhave the major disadvantage of not being able to detect faults\nin chronological order [3], [10]. Some systems reconfigure the\nvehicle to operate at reduced performance levels to avoid fault\nprediction using this conservative approach [5], [6], [11]. In\n2022, authors introduced an adaptive sparse attention network\nthat enhances fault detection by focusing on decentralized\nlocal fault information in real-time [4]. The introduction of\na new soft threshold filter that improves the visualization and\ninterpretation of fault mechanisms. This approach significantly\nenhanced the interpretability of the diagnostic process com-\npared to traditional deep learning models. However, a more\nrobust AI model is required which will classify huge and\ncomplex patterns of faulty and non-faulty classes with their\ntypes and behaviors.\nXisheng Jia et al. designed a novel method for diagnosing\nfaults in diesel engines by combining optimized Variational\nMode Decomposition (VMD) with Deep Transfer Learning\n(DTL) [1]. By fine-tuning a pre-trained ResNet18 model on\nImageNet samples, the approach successfully reduced noise\nand improved diagnostic efficiency and accuracy, minimizing\nthe need for manual feature extraction and expert experience.\nRaj et al.'s work presents a novel framework that utilizes\nadvanced machine learning algorithms and big data analytics\nto enhance the predictive maintenance of turbines [9]. By\naccurately predicting potential failures and optimizing mainte-\nnance schedules, the framework significantly improves turbine\nreliability and operational efficiency while reducing downtime\nand costs [2].\nIn this research work, the proposed Transformer model\nclassifies the faults and predicts the faults in chronological\norder."}, {"title": "III. THE PROPOSED FRAMEWORK", "content": "With the background study and literature survey, consider-\ning the current research challenges in the field of automotive\nengine systems I propose a new framework that will create and\nperform model-based fault diagnosis which will be a more\neffective design to demonstrate and analyze fault detection\nand fault prediction. This can be tested for any prototype\napplication and then finally deployed it into the actual engine\nsystem. The proposed framework consists of two parts:\n1) Replace the Engine Observer: Previous methods [8] use\na physics engine to simulate the internal states of the\nreal-world engine and compare the simulated sensor\noutputs with the observed outputs to create a function\nto map the differences, called residuals, to fault types. I\nintend to replace this computationally complex task with\na Neural Network that can perform the mapping of the\ninputs and outputs of the real engine to the fault types.\n2) Early Fault Detection: Instead of reading current faulty\nsensor data to identify the fault type, I also want to pre-\ndict the fault occurring before the damage to the engine\nsystem occurs. For this our Neural Network approach\nkeeps an internal history of the previous sensor outputs\nto predict a faulty sensor output before it happens."}, {"title": "A. Objectives", "content": "1. Create a Neural Network (NN) based generator for these\nnormal data and then faulty data that are more robust and\nresilient than the existing one. The main focus will be the\nTransformer model.\n2. Then classify the fault-free and fault-free data, including\nthe type of fault.\n3. The model should be able to detect a fault as soon as it\noccurs using time series data."}, {"title": "B. Test Environment - UTSA ARC High-Performance Comput-\ning Cluster", "content": "This research work was tested on the UTSA Arc HPC\ncluster, a high-performance computing environment. I have\nused 2 GPU nodes each containing two CPUs, 4 V100 GPUs,\nand 384 GB RAM. I have also tested the Transformer model\non Arc's large memory nodes, one of which was equipped\nwith two AMD EPYC CPUs and 2 TB of RAM and another\nnode equipped with two AMD EPYC CPUs and 1 TB of\nRAM. I observed the total time taken for the training and the\ntime required for each epoch. After experimenting on the GPU\nand AMD CPU, I observed there is no significant difference\nin accuracy and time taken between each epoch during the\ntraining of the model."}, {"title": "C. Approach", "content": "I have used a 397 GB dataset generated by the TC-\nSISimTestbed simulator using Matlab/Simulink [8]. The\ndataset is currently hosted on the UTSA Arc HPC environment\nfor experimental purposes. This dataset was generated using\nthe Worldwide Harmonized Light Vehicle Test Procedure cycle\namong the 4 existing types of standard driving cycle, the\nWorldwide Harmonized Light Vehicle Test Procedure, the\nNew European Driving Cycle, the Extra-Urban Driving Cycle,\nand the US Environmental Protection Agency Federal Test\nProcedure [7]. Using WLTP, I have used a fault-free dataset\nand a total of 11 different types of fault datasets for each of\nthe cases of normal behavior using multiple parameters and\nrandomized runs to help an AI model recreate normal behavior."}, {"title": "D. Results obtained so far: Data Representation", "content": "The testbed simulates engine behavior over a 30-minute\ncycle. The data recorded during the simulation consist of\nthe engine speed and torque, recorded once per second, and\nthe control signals, the internal engine state, and the sensor\noutput, recorded at intervals throughout the 30-minute cycle.\nTo simulate engine faults, the simulation injects faulty data\ninto the simulation pipeline (illustrated in Figure 1) at one\nof the 3 locations depending on the type of fault. The linear\ninterpolation technique was used to generate fixed-length\nsequences for each simulation. For each faulty class and non-\nfaulty class concerning WLTP, data were recorded for each\nclass with a thousand simulations which consisted of data with\nfive files.\nOmega - reference engine speed.\nTorque - reference engine torque.\ninput_signal - 5 actuator measurements to the engine (control\ninputs for throttle position area and wastegate, engine speed,\nambient temperature and pressure)\noutput_signal - 9 sensor measurements from the engine (tem-\nperatures for the compressor, intercooler and intake manifold,\npressures for the compressor, intercooler, intake manifold and\nexhaust manifold, air filter mass flow and engine torque)."}, {"title": "IV. TRANSFORMER MODEL", "content": "I designed a Transformer-based NN model for a diesel\nengine time series data set to perform fault classification and\nprediction."}, {"title": "A. Model architecture", "content": "In this experiment, the model used 27 input dimensions,\n64 hidden dimensions with 2 layers, and 9 heads to produce\na classifier with 12 output heads (one representing fault-\nfree data and one for each of the 11 fault types). I trained\nthe model and minimized the cross-entropy loss between the\nTransformer output sequence and the fault signal sequence.\nThis Transformer model with 27 parameters is trained on a\nsystem containing 4 NVIDIA GPUs V100, CPUs with 40\ncores, 384GB RAM and this model achieves 70.01% accuracy\non a held-out test set for 20 epochs on UTSA Arc HPC\nenvironment. This experiment was also tested on five GPU\nnodes, each containing two CPUs with 20 cores each for a\ntotal of 40 cores, 384GB RAM, and each including two V100\nNvidia GPU accelerators and achieved 70.09%. I analyzed the\naccuracy and time taken for each epoch and realized that there\nare not much differences in terms of accuracy, but after the\nseventh epoch, training was faster when submitted in batch\nmode. I have also tested this experiment on a large memory\nnode, equipped with two AMD EPYC CPUs and 2 TB of\nRAM 1 node equipped with two AMD EPYC CPUs and\nhaving 1 TB of RAM and found the almost same accuracy\nwith 20 epochs.\nInput files consist of 27 parameters like engine speed,\ntorque, 5 actuator measurements to the engine (control inputs\nfor throttle position area and wastegate, engine speed, ambient\ntemperature, and pressure), 9 sensor measurements from the\nengine (temperatures for the compressor, intercooler, and\nintake manifold, pressures for the compressor, inter-cooler,\nintake manifold, and exhaust manifold), air filter mass flow,\nand engine torque) and 13 states of the engine system (temper-\natures and pressures for the air filter, compressor, intercooler,\nintake manifold, exhaust manifold, turbine, and turbine speed).\nThe output of the model is 12 classes (11 different types of\nfaulty and non-faulty data).\nTo use this model, I converted the time series data into a\nsequence of fixed-length vectors or \"tokens\", which further\nfed into the Transformer model. One approach for using\nTransformers with time series data is to use a sliding window\nto create overlapping segments of the time series data and treat\neach segment as a separate \"token\". This creates a sequence\nof fixed-length vectors that can be fed into the Transformer\nmodel. The key feature of the Transformer architecture is its\nattention mechanism, which allows the model to selectively\nfocus on different parts of the input sequence and learn\ncomplex patterns."}, {"title": "B. Performance of Transformer Model", "content": null}, {"title": "C. Algorithms", "content": null}, {"title": "Algorithm 1 Data Preprocessing", "content": null}, {"title": "Algorithm 2 CustomDataset Class", "content": null}, {"title": "Algorithm 3 EncoderDecoderTransformer Class", "content": null}, {"title": "Algorithm 4 Model Training and Evaluation", "content": null}, {"title": "V. CONCLUSION AND FUTURE SCOPE", "content": "I have designed and implemented the RNN model with an\naccuracy of 62.85% and the Transformer model on the UTSA\nArc HPC environment which achieved 70.01% accuracy on the\nsimulated engine dataset. This work is still in progress and I\nwill be working on improving the model accuracy. I will also\ntest this model with the real-time dataset and find the model's\naccuracy. In the future, this can be experimented with with the\nKoopman NN and perform the performance analysis among\nthese models. This could be also tested on the distributed\nlearning environment HORVOD for the inferencing."}, {"title": "VI. DATA AVAILABILITY", "content": "The datasets generated and/or analyzed during this study are\nnot publicly available because they are intended to be used for\na future benchmark submission."}, {"title": "VII. CONTRIBUTIONS", "content": "This research work was carried out under the mentorship\nof Dr. Sumit Kumar Jha, PI of the grant \"Office of Naval Re-\nsearch/Grant Number - N000014-21-1-2332I\" and he provided\nthe data set and funds; I designed the AI models and conducted\nthe experiments on the UTSA Arc HPC environment and\nsubmitted a report on this. I also presented a poster on this\nwork in PAW'23 (Postdoc Appreciation Week) at UTSA which\nis available in the UTSA digital library."}]}