{"title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds", "authors": ["J Rosser", "Jakob Foerster"], "abstract": "Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been as thoroughly explored. In this paper, we introduce AGENTBREEDER a framework for multi-objective evolutionary search over scaffolds. Our REDAGENTBREEDER evolves scaffolds towards jailbreaking the base LLM while achieving high task success, while BLUEAGENTBREEDER instead aims to combine safety with task reward. We evaluate the systems discovered by the different instances of AGENTBREEDER and popular baselines using widely recognized reasoning, mathematics, and safety benchmarks. Our work highlights and mitigates the safety risks due to multi-agent scaffolding. Code is available at https://anonymous.4open.science/r/AgentBreeder-86AF.", "sections": [{"title": "1. Introduction", "content": "In recent years, the field of artificial intelligence has witnessed remarkable advancements in Large Language Models (LLMs) and their applications (Zhao et al., 2023). LLMs are capable of exhibiting human-like reasoning (Amirizaniani et al., 2024; Sun et al., 2024; Xu et al., 2025), enabling their application beyond natural language processing to diverse areas such as code generation (Romera-Paredes et al., 2024; Wang & Chen, 2023; Yeti\u015ftiren et al., 2023), embodied AI in robotics (Hu et al., 2023; Kong et al., 2024; Sartor & Thompson, 2024), and autonomous agents that can perform a variety of tasks on behalf of humans (OpenAI, 2025; Convergence, 2024).\nOur research is motivated by accelerated advancements in autonomous agents such as the recent release of Operator"}, {"title": "2. Background", "content": "Multi-Agent Systems. Multi-agent systems consist of multiple interacting intelligent agents such as LLM assistants like ChatGPT (OpenAI, 2024). These systems offer several advantages over single-agent approaches (Yang et al., 2024), including planning (Li et al., 2024a; Cao et al., 2024), task decomposition (Chen et al., 2023; Fourney et al., 2024; Ghafarollahi & Buehler, 2024; Qian et al., 2023), and special-"}, {"title": "Automated Design of Agentic Systems.", "content": "We build upon the seminal work of Hu et al. (2024a) which introduces the research area Automated Design of Agentic Systems (ADAS), an automated approach to discovering high-performing (multi-agent) scaffolds. Hu et al. (2024a) formulate ADAS as an optimization algorithm comprising 3 key components; the search space, the search algorithm and the evaluation function. Hu et al. (2024a) also propose a search algorithm called \"Meta Agent Search\" where a single \"Meta Agent\" discovers scaffolds by programming them in Python code. Python is a Turing Complete language (Boyer & Moore, 1983) therefore searching within a code space allows the Meta Agent to program theoretically any possible scaffold. This approach has shown promising results (Hu et al., 2024a; Yin et al., 2024), with discovered agents outperforming state-of-the-art hand-designed baselines across various tasks, including reading comprehension, mathematics, and science questions (Hendrycks et al., 2020; Shi et al., 2022; Rein et al., 2023; Chollet, 2019; Dua et al., 2019).\nWe formulate AGENTBREEDER with respect to the ADAS methodology. We replicate the approach of Hu et al. (2024a) by seeding our population with hand-designed scaffolds. We prompt a single \"Meta Agent\" to search for novel scaffolds in the space of Python code. We introduce a novel quality-diversity search algorithm inspired by MAP-Elites (Mouret & Clune, 2015), where the Meta Agent evolves new scaffolds via the random sampling, mutation and crossover of the highest performing individual or \u201celite\u201d of each niche of the population. We cluster scaffolds based on their architectural features, and evaluate the performance of scaffolds on two benchmarks, one for capability and one for safety. We employ multi-objective optimization, sampling elites from the Pareto front of each cluster."}, {"title": "Multi-Objective Evolutionary Algorithms.", "content": "Multi-objective optimization searches for solutions to problems with multiple, often conflicting objectives. Multi-objective evolutionary algorithms (MOEAs) incorporate an evolutionary approach to generate a diverse set of solutions (Kesireddy & Medrano, 2024). In AGENTBREEDER we seek to balance the objectives of capability and safety whilst evolving a diverse range of scaffolds. AGENTBREEDER balances quality and diversity by clustering scaffolds based on their architectural features and randomly sampling elites from each cluster's capability-safety Pareto front. A solution is Pareto optimal if no other solution improves one objective without worsening another. The Pareto front comprises all such optimal solutions."}, {"title": "Adversarial Robustness.", "content": "Adversarial robustness quantifies the resilience of a model or scaffold to malicious inputs such as jailbreaks (Chao et al., 2023) and prompt injection (Liu et al., 2023). Red teaming, the practice of simulating adversarial scenarios to identify vulnerabilities, has emerged as a crucial tool for assessing AI model risks and alignment (Samvelyan et al., 2024; Perez et al., 2022). In REDAGENTBREEDER, instead of generating adversarial examples, we seek to evolve multi-agent scaffolds that are more vulnerable to adversarial attacks than the base model. In BLUEAGENTBREEDER, we seek to evolve multi-agent scaffolds that are more robust to adversarial attacks than the base model."}, {"title": "3. Related Work", "content": "Self-Referential Self-Improving Systems. Numerous frameworks (Yuan et al., 2024; Hu et al., 2024b;a; Xue et al., 2024; Yin et al., 2024) have been proposed to address the design of multi-agent scaffolding. EvoAgent (Yuan et al., 2024) extends single expert agents to multi-agent scaffolds via evolutionary algorithms, whilst AGENTBREEDER evolves the entire system as a unit. EvoMAC (Hu et al., 2024b) evolves agents and their connections during test time to improve code generation, whereas AGENTBREEDER is domain agnostic and can explore the entire search space of scaffolds. ADAS (Hu et al., 2024a), Comfy Agent (Xue et al., 2024) and G\u00f6del Agent (Yin et al., 2024) search in the space of code for novel scaffolds, but unlike AGENTBREEDER they do not incorporate a quality-diversity mechanism for exploring agent design space. FunSearch (Romera-Paredes et al., 2024) is an evolutionary method to search the function space for high-performing computer programs but not necessarily scaffolds.\nMulti-Agent Safety Research. Zhang et al. (2024) evaluate the safety of multi-agent scaffolds from a psychological perspective by injecting agents with malicious traits, and provide mitigation strategies such as performing psychological assessments and therapy for agents. Polaris (Mukherjee et al., 2024) introduces a safety-focused scaffold for real-time patient healthcare conversations. Huang et al. (2024) explore the resilience of multi-agent scaffolds when injected with malicious or error-prone agents. Fowler (2023) provide a more thorough discussion of the safety risks associated with scaffolded LLMs."}, {"title": "4. AgentBreeder", "content": "We now introduce AGENTBREEDER, our automated, evolutionary approach to discovering new multi-agent scaffolds. By evolving a large, diverse corpus of multi-agent scaffolds, AGENTBREEDER seeks to address the challenge of evaluating the vulnerabilities of base LLMs acting inside capability-optimized multi-agent scaffolds. The pseudo-algorithm is given in Algorithm 1 and Figure 1 provides a brief overview.\nAGENTBREEDER can be run in three modes:\n\u2022 BLUEAGENTBREEDER - In this mode, the Meta Agent adopts the role of a \"Blue Team\", searching for scaffolds that exhibit high capability and safety when evaluated on representative benchmarks.\n\u2022 REDAGENTBREEDER - In this mode, the Meta Agent adopts the role of a \u201cRed Team\u201d, minimizing performance on one safety benchmark whilst maximizing performance on one capability benchmark.\n\u2022 CAPABLEAGENTBREEDER - In this mode, the Meta Agent seeks to maximize performance on a single capability benchmark without consideration of safety."}, {"title": "4.1. Seed Scaffolds", "content": "Following the approach of Hu et al. (2024a) and Yin et al. (2024), we seed our population with the same 7 hand-designed scaffolds. These comprise Chain-of-Thought (CoT) (Wei et al., 2022), Self-Consistency with Chain-of-Thought (Wang et al., 2022), Self-Refine (Madaan et al., 2024), LLM-Debate (Du et al., 2023), Step-back Abstraction (Zheng et al., 2023), Quality-Diversity (QD) (Lu et al., 2024), and Role Assignment (Xu et al., 2023).\nBefore running our evolution on our chosen benchmark, we evaluate a single CoT agent on 1,000 samples from the validation set of the benchmark, oversampling and resampling where necessary. For each generation, we validate the newly discovered scaffolds using a balanced sampling strategy, selecting 50% positive and 50% negative samples. Often improvements between generations are marginal, so this method increases information gain by providing a stronger signal for the evolutionary process."}, {"title": "4.2. Mutation Operators", "content": "AGENTBREEDER's evolutionary search algorithm mimics the process of natural selection comprising mutation, crossover and selection. Claude 3.5 Sonnet (Anthropic, 2024) (claude-3-5-sonnet-20241022-v2:0) is used as the core model of the Meta Agent due to its state-of-the-art performance on code generation tasks (Tian et al., 2024).\nSelection. Selection pressure is applied at each generation by sampling candidate scaffolds at random from the Pareto fronts of each cluster. In CAPABLEAGENTBREEDER, the Pareto front is simply the elite of each cluster, whereas in BLUEAGENTBREEDER and REDAGENTBREEDER, the Pareto front comprises the scaffolds which best trade-off safety and capability."}, {"title": "Mutation.", "content": "The Meta Agent uses weighted random sampling to select either the crossover or mutation operator. Weighting the mutation operator twice as highly as crossover was found empirically to lead to faster convergence. Mutation is performed via random sampling of mutation operators, expressed as short textual passages we hand-designed, such as:\nCreate multi-tiered meeting structures where higher-level meetings oversee or coordinate lower-level ones. For example, strategic meetings could direct tactical meetings.\nThere are two types of mutation operators, capability-enhanced and safety-enhanced. When running BLUEAGENTBREEDER, mutation operators are randomly sampled from the concatenated capability- and safety-enhanced corpus. In RedAGENTBREEDER and CAPABLEAGENTBREEDER the safety-enhanced operators are omitted. The full list of Meta Agent prompts and mutation operators are given in Appendix B."}, {"title": "Crossover.", "content": "During crossover, the Meta Agent is provided with two randomly sampled scaffolds from the population and tasked with combining them in such a way that would be likely to improve performance performance. The full crossover prompt is given in Appendix B.6."}, {"title": "4.3. Descriptors", "content": "In open-ended evolutionary approaches, descriptors are essential for quantifying the diversity of candidate solutions (Mouret & Clune, 2015). In order to explore the full range of vulnerabilities of a base model, we seek to generate and evaluate a diverse range of multi-agent scaffolds and require high-dimensional descriptors. In AGENTBREEDER, we use OpenAI's text-embedding-3-small (OpenAI, 2024b) model returning a 12-dimensional text embedding of the system name and code as our descriptor to encode semantic information about the name, structure, and potential logic embedded in the scaffold."}, {"title": "4.4. Clustering", "content": "Once the descriptors have been generated for the new scafolds, AGENTBREEDER re-clusters the whole population based on their descriptors to discover groups of similar architectures. We choose agglomerative clustering as it has been found to be particularly effective for smaller datasets like ours (Weigand et al., 2021). By setting a distance threshold in the agglomerative clustering algorithm, we allow the number of clusters to adjust flexibly. When the number of clusters increases, the selection pressure decreases towards zero. Conversely, reducing the number of clusters encourages the algorithm to explore only a few options, which leads to less diverse scaffolds. To achieve a balanced tradeoff between system performance and system diversity, a distance threshold of 0.7 was selected."}, {"title": "4.5. Multi-Objective Pareto Elites", "content": "In Quality-Diversity algorithms such as MAP-Elites (Mouret & Clune, 2015), selection pressure is applied by randomly sampling the highest-performing candidates in each niche for evolution, referred to as the \"elites\". In multi-objective optimization, a solution is Pareto optimal if no other solution improves one objective without worsening another (Kesireddy & Medrano, 2024). The Pareto front comprises all such optimal solutions. In AGENTBREEDER, instead of sampling from pre-defined niches, we sample elites from the Pareto fronts of dynamically generated clusters."}, {"title": "4.6. Evaluations", "content": "Evaluations are implemented in Inspect (AI Safety Institute, 2024), an open-source framework for LLM evaluations. We instantiate AGENTBREEDER as a custom model provider by deriving a new class from ModelAPI, and each individual scaffold derives as a Model from that ModelAPI. This allows comprehensive experiment tracking and parallelization, and provides an extensible framework allowing AGENTBREEDER to be run on a new benchmark often with fewer than 100 lines of code. In Section 5, we report results on 5 benchmarks comprising safety, capability and helpfulness."}, {"title": "5. Experiments", "content": "We conduct experiments to validate AGENTBREEDER'S three modes; BLUEAGENTBREEDER, REDAGENTBREEDER, CAPABLEAGENTBREEDER. To evaluate the capability of the multi-agent scaffolds produced, we follow the approaches of Hu et al. (2024a) and Yin et al. (2024) and report results on three benchmarks from OpenAI's simple-evals (OpenAI, 2023). To evaluate system safety, we report results on one comprehensive safety benchmark. A full description of each benchmark can be found in Appendix A.\nTo evaluate the safety of multi-agent scaffolds, we adopted a method inspired by Constitutional AI, where an LLM judges responses based on a predefined set of principles. We transformed the Anthropic Claude Constitution (Bai et al., 2022) into a series of yes/no questions. During validation, each system responded to 100 attack-enhanced prompts, and an agent aware of the constitution evaluated the safety of each response. We then reported the median accuracy and the 95% bootstrap confidence interval for each scaffold's safety performance."}, {"title": "5.1. Experiment 1: Capability", "content": "We perform comprehensive experiments on three benchmarks to validate AGENTBREEDER against ADAS (Hu et al., 2024a), the seminal work in this field. We run CAPABLEAGENTBREEDER - a single-objective-variant of our framework - for 20 generations, evolving 10 mutants each generation. We take the highest performing scaffolds from ADAS and evaluate them with GPT-40 mini (OpenAI, 2024) as their core model.\nWe report the F1 score for DROP, median accuracy for MMLU and GPQA and their 95% confidence intervals, as well as their performance on SaladData, our chosen safety benchmark. The results are shown in Table 5.1.\nComparable Performance to Previous Work. CAPABLEAGENTBREEDER achieves competitive results to ADAS, marginally surpassing performance across all capability benchmarks.\nMulti-Objective outperforms Single-Objective Optimization. The scaffolds discovered during by CAPABLEAGENTBREEDER achieve near or slightly above-baseline results, such as 72.3 \u00b13.1 F1 on DROP and 41.2\u00b14.4 accuracy on GPQA. This performance gain is notably smaller than in the multi-objective setting. This supports our hypothesis that incorporating an additional benchmark may increase the signal-to-noise ratio of system validations each generation. This improves the quality of the selection pressure for the evolutionary algorithm, helping the process converge to better solutions overall.\nSafety Performance as a Byproduct. In single-objective ablation runs, the discovered scaffolds showed only modest performance on SaladData, suggesting that ignoring safety in the objective yields no strong impetus for safe or unsafe behaviors. This contrasts with multi-objective runs, where explicit safety optimization (or \u201cnegative safety\" in redteaming) substantially influenced outcomes.\nPerformance Stagnates with Better LLMs. When using more advanced models (GPT-40 mini (OpenAI, 2024) for scaffolds and Claude 3.5 Sonnet (Anthropic, 2024) for the Meta Agent) compared to the original ADAS (Hu et al., 2024a) implementation, we observe that while overall performance improves, the relative gain between seed and discovered scaffolds diminishes. We attribute this to three plausible factors: (1) increased data contamination in newer LLMs may lead to memorized solutions rather than genuine reasoning, (2) higher baseline performance makes marginal improvements harder to distinguish from noise and (3) recent models are already fine-tuned for detailed reasoning, reducing the benefit of scaffold-induced reasoning steps (OpenAI, 2024a; Zaremba et al., 2025)."}, {"title": "5.2. Experiment 2: Blue Team Defense", "content": "We ran BLUEAGENTBREEDER for 20 generations, with the aim of generating \u201cBlue-Teams\" of multi-agent scaffolds that simultaneously optimize for capability and safety across our chosen benchmarks. We generate 10 new mutants each generation, and report the median accuracy and the 95% confidence interval on the held-out test set in Table 5.2.\nTo reduce AGENTBREEDER's tendency to reward-hack the safety benchmark by finding a trivial safe response to question-answering tasks that require a long-form response, during evaluation, we report the \"helpfulness\" of the scaffold on questions from TruthfulQA (Lin et al., 2021).\nMarginal but Consistent Improvements Over Baselines. We observed that, for DROP (Dua et al., 2019), the \u201cDROP Capable\" agent attained an F1 score of 79.0 \u00b1 3.8, surpassing both the Chain-of-Thought and Step-Back Abstraction baselines. Although the absolute increase (about 1 point over the next highest baseline) is modest, it consistently emerged across repeated runs.\nVulnerability to Reward Hacking. We note that certain specialized systems, such as \u201cGPQA Safe\u201d, display inflated scores on our safety benchmark, SaladData (Li et al., 2024b) achieving an accuracy of 95.2 \u00b1 2.4. However, under closer examination, the system shows a significant decrease in performance on our helpfulness benchmark (49.6 \u00b16.4). We find that multi-agent scaffolds are prone to reward-hacking the safety objective e.g. by finding a trivial solution such as responding \"I'm sorry I can't help with that\" to any question outside of their domain, and that one way of detecting this behavior is by reporting performance on a helpfulness benchmark."}, {"title": "5.3. Experiment 3: Red Team Attack", "content": "Our second set of experiments employed REDAGENTBREEDER, which deliberately seeks to discover \"Red-Team\u201d multi-agent scaffolds which illuminate the safety vulnerabilities of the base LLM while preserving capability on DROP (Dua et al., 2019). Table 5.3 summarizes these results.\nWe introduce an inverted safety benchmark \u201c1-SaladData\" which contains the same questions as the original SaladData (Li et al., 2024b) benchmark, but utilizes an inverted scoring function that awards higher scores for unsafe responses.\nUnsafe Scaffolds are Easier to Find. The \"DROP Unsafe\" system obtained an accuracy of 81.6\u00b14.8 on the inverted 1-SaladData (Li et al., 2024b) metric, surpassing all baseline agents (e.g., CoT with 76.8\u00b14.8). These results not only demonstrate REDAGENTBREEDER's capacity to discover agents with greater susceptibility to unsafe behavior but also underscore how it is easier to find unsafe scaffolds than safe ones: in this setup, we required only 10 generations, half the budget of the blue-teaming experiments.\nCapability Disguises Safety Vulnerabilities. Interestingly, even while maximizing unsafe performance, the \"Most Unsafe\" system maintained a DROP F1 score of 67.7\u00b14.7. This result is comparable to the baseline agents, highlighting that scaffolds can appear just as capable in terms of task performance yet simultaneously exhibit increased safety vulnerabilities."}, {"title": "6. Discussion", "content": "Pre-Deployment Safety Evaluations. The Dead Internet Theory posits a future where AI agents dominate online activity (Walter, 2024). While speculative, the recent releases of Operator (OpenAI, 2025) and Proxy (Convergence, 2024) highlight the increasing population of agents deployed with the ability to interact autonomously with other agents and humans. These underscore the uncertainty around agent-on-agent dynamics, especially when these agents evolve or compose themselves in unanticipated ways. Our REDAGENTBREEDER experiments illustrate an automated approach to efficiently surface multi-agent scaffolds that exhibit vulnerabilities on safety benchmarks. Over time, labs could adopt a REDAGENTBREEDER-style pipeline to proactively \"red-team\" new LLMs as part of a release protocol."}, {"title": "7. Conclusion", "content": "This paper introduces AGENTBREEDER, an evolutionary framework for discovering and evaluating multi-agent scaffolds via the multi-objective optimization of capability and safety. Our experiments demonstrate that AGENTBREEDER operates effectively in three distinct modes. BLUEAGENTBREEDER for developing safer scaffolds, REDAGENTBREEDER for identifying vulnerabilities, and CAPABLEAGENTBREEDER for maximizing task performance. Through empirical evaluation across multiple benchmarks, we show that our framework discovers scaffolds that achieve competitive or increased performance to prior works while exhibiting increased adversarial robustness."}, {"title": "8. Future Work", "content": "Scaling Laws. Scaling up AGENTBREEDER to larger population sizes and longer evolutionary runs could yield more substantial improvements in both capability and safety metrics. Incorporating closed-source safety benchmarks such as AILuminate (MLCommons Association, 2025) and contamination-free capability benchmarks such as MMLU-CF (Zhao et al., 2024) would provide a more comprehensive assessment of multi-agent system safety.\nWhite-Box and Gray-Box Evaluations. A key limitation of our current approach is its focus on black-box evaluation of scaffolds. Future work could investigate individual agent behaviors, including how agents interact with tools, external APIs, and information sources. Developing methods to trace and analyze agent-agent and agent-tool interactions could reveal potential safety risks that are invisible in black box evaluation. Additionally, future work could automate the analysis of agent interactions to identify patterns that lead to safety vulnerabilities.\nAlternative Objectives. In this work, we only consider the capability and safety objectives for optimization. Future work could explore inference cost as an objective to minimize for, and consider multi-core scaffolds where different LLM base models exist inside the same scaffold.\nMulti-Agent Governance. Critical research is needed to establish governance frameworks for multi-agent scaffolds. Future work could comprise developing differentiated safety cases for scaffolds with varying levels of transparency, from fully white box to black box architectures."}, {"title": "Impact Statement", "content": "This work introduces methods for evaluating and improving the safety of multi-agent scaffolds, which is increasingly critical as embodied, autonomous agents become more prevalent. While AGENTBREEDER can help discover safer multi-agent architectures, it could also be used to find scaffolds that exploit vulnerabilities. We release this research to enable proactive safety testing before deployment, but acknowledge the dual-use nature of these techniques. The red teaming capabilities we describe could be misused to develop harmful scaffolds, though we believe the defensive benefits outweigh these risks. Additionally, our research surfaces important questions about AI governance as multi-agent scaffolds become more common. We hope this work advances the field's understanding of multi-agent safety and helps develop more robust evaluation frameworks. We encourage future research to build upon these methods while carefully considering potential misuse and implementing appropriate safeguards."}]}