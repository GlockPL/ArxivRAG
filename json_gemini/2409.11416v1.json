{"title": "The Unseen AI Disruptions for Power Grids: LLM-Induced Transients", "authors": ["Yuzhuo Li", "Mariam Mughees", "Yize Chen", "Yunwei Ryan Li"], "abstract": "Recent breakthroughs of large language models (LLMs) have exhibited superior capability across major industries and stimulated multi-hundred-billion-dollar investment in AI-centric data centers in the next 3-5 years. This, in turn, bring the increasing concerns on sustainability and AI-related energy usage. However, there is a largely overlooked issue as challenging and critical as AI model and infrastructure efficiency: the disruptive dynamic power consumption behaviour. With fast, transient dynamics, AI infrastructure features ultra-low inertia, sharp power surge and dip, and a significant peak-idle power ratio. The power scale covers from several hundred watts to megawatts, even to gigawatts. These never-seen-before characteristics make AI a very unique load and pose threats to the power grid reliability and resilience. To reveal this hidden problem, this paper examines the scale of AI power consumption, analyzes AI transient behaviour in various scenarios, develops high-level mathematical models to depict AI workload behaviour and discusses the multifaceted challenges and opportunities they potentially bring to existing power grids. Observing the rapidly evolving machine learning (ML) and AI technologies, this work emphasizes the critical need for interdisciplinary approaches to ensure reliable and sustainable AI infrastructure development, and provides a starting point for researchers and practitioners to tackle such challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have witnessed the explosive growth of AI applications, from edge devices to large-scale data centers. Large language models (LLMs) such as GPT-4, Llama 3, and BERT have largely pushed the boundaries of modern AI [1]\u2013[3]. While such technical advancements are at the cost of significantly increased power and energy consumption. Over the past five years, AI workloads have undergone significant transitions, driven by rapid advance-ments in AI-related computing. The 2024 electricity report from the International Energy Agency (IEA) estimated that data centers consumed 460 TWh in 2022, accounting for 2% of global electricity usage-a figure that is still rapidly increasing [4]. Even before the wide adoption of LLMs and Generative AI (GenAI) models [1], [5], [6], the size of recommendation models at Meta has increased by 20x into the terabyte scale, while inference requests more than doubled from 2019 to 2022, leading to a 2.5\u00d7 increase in infrastructure capacity [7]. Moreover, energy consumed in both the training and operational stages of GenAI models are surging over the last few years, raising the concerns over AI models' scalability and sustainability [8]. For instance, the training of GPT-4 consumed over 50 GWh, approximately 0.02% of California's annual electricity consumption, representing a 50-fold increase over the energy required to train GPT-3 [9]. However, focusing solely on these figures might obscure a more critical challenge: the disruptive nature of AI-induced impacts on power grids. Such phenomenon can not only demand far more electricity energy generated and dispatched by power systems, it also significantly affects the reliable and sustainable operation of power grids through the transient load behaviors, the sheer volume of electricity load related to AI computing, and the fast deployments of emerging A\u0399 services. If without careful planning and power management, large-scale AI models in both training and deployment stages can cause peak loading and burden the local distribution system. Just for a concrete showcase, as our experiment indicates, LLM training can instantly move from cold start to peak load with over tens of Megawatts of power. While for current existing distribution grid design, such minute-level ramping capability is typically limited to few Megawatts [10]."}, {"title": "A. AI Growth and Associated Impacts on Power Consu\u0442\u0440tion", "content": "The AI industry's energy demand is growing at an alarming rate, with the computational requirements of AI models doubling every 10 months [11], [12]. This exponential growth translates into a corresponding increase in energy consumption [13]. When comparing energy consumption across different sectors, it is evident that AI workloads are becoming increasingly significant. In terms of household energy usage, GPT-3.5's annual consumption is equivalent to that of approximately 4,150 U.S. households [14]. Globally, Schneider's report estimates that AI currently consumes 4.5 GW of power, with projections indicating a compound annual growth rate (CAGR) of 25% to 33% [15]. By 2028, AI-related power consumption could reach 14 GW to 18.7 GW. This rapid growth in energy consumption poses significant challenges not only for individual data centers but also for national and global power infrastructures [16].\nMoreover, managing large-scale AI inference systems introduce new complexities. These systems typically utilize heterogeneous computer architectures, comprising conventional hosts (e.g., an x86 processor) and high-throughput accelerators (e.g., H100, TPU), which are fully exploited only at large batch sizes [17]. The introduction of new services by cloud platforms and enterprises, such as server-less architectures, adds to the complexity of managing AI workloads effectively [18]. These factors, combined with the increasing frequency and scale of AI-induced transients, underscore the need for a shift of focus from mere power consumption to the broader, more disruptive impacts that AI could have on power grid transient dynamics and stability [19].\nAchieving ubiquitous access to LLMs and GenAI will require timely commitments to tailored system-level planning and reliable operation of electrical grids [20], [21]. Without these commitments, the rapid growth of AI could lead to significant disruptions in power system reliability and sustainability. While the current focus on AI's electricity consumption is important, it may be overlooking a more critical aspect: the unique and potentially destabilizing effects AI workloads can pose on power grid dynamics."}, {"title": "B. Related Works", "content": "Recently, emerging studies began to address the unique challenges of AI workloads by detailing the behavior of scaled-up LLMs during training and inference [26]\u2013[30]. However, the power dynamics, particularly the transient behavior of AI facilities, remain underexplored from the power systems perspective.\nFor datacenter's power behaviour, previous works have used sun irradiance and other meteorological data (i.e. temperature and wind speed) for estimating the renewable generation and renewable energy utilization (REU) of data center load, while such approaches are rough estimates of grid-side carbon intensity [31]\u2013[33]. Some also focused on predicting the machine learning model's carbon emissions [34]. While there are discussions on the social and environmental impacts of training and deploying AI models [8], the exact and fine-granular impacts on power grids are largely underexplored.\nAs for the power performance of the AI workload, most of the work even if they are concerned with the power consumption of AI are more focused on either algorithmic features, steady state or quantity of energy the AI infrastructure consumed [35], [36]. The main focus are majorly centered around the operational or embodied electricity usage [37], the energy and natural resources that the AI models need, or the CO2 emission they may induce [38], [39]. Pioneering studies [28], [40] characterized the power usage pattern of LLM load from a computing perspective by looking into the effects of power throttling, GPU frequency locking and etc. However, a comprehensive understanding of the transient power behavior of AI systems is still lacking, which highlights the importance of further research in this area. Moreover, the impacts brought by AI/ML's tasks to the power grids are not fully understood nor investigated."}, {"title": "C. Objectives and Scope of the Paper", "content": "This paper aims to highlight the unique features of AI loads from a power and energy system perspective, and provide insights into their potential impact and solutions in light of the rapid growth of AI computing. We begin with a technical overview of modern large-scale AI models, such as LLMs, and their power consumption behavior. A holistic mathematical description of AI-centric infrastructure is presented, followed by several case studies covering three stages of AI workload operation: training, fine-tuning, and inference. Finally, we summarize future research directions from different perspectives, including the AI-user side, data center side, and grid side, to address the various challenges ahead."}, {"title": "II. CHARACTERISTICS OF AI LOADS", "content": "As mentioned before, contemporary AI models, especially foundational models like LLMs, induce a great amount of energy demand, yet this is not the only concern. From an electrical power engineering perspective, the burst of such high energy demand (e.g., power surge/dip) could easily stress the local distribution system and induce more severe problems (e.g., over voltage or voltage sag on distribution line or feeders, current fluctuations, stability issues, etc.). To provide a holistic picture of how AI could impact power grids, in this section, we firstly provide qualitative analysis and the deep dive of some unique features of AI loads."}, {"title": "A. Structure of an AI Compute Node", "content": "As decipated in Figure 2, a high-performance AI compute node typically features multiple powerful GPUs as its computational core, interconnected for efficient parallel processing. These are managed by high-core-count CPUs and supported by large amounts of high-bandwidth memory. The system incorporates high-capacity, fast storage solutions and multiple high-speed network interfaces for robust data handling and communication. Cooling is critical, with advanced systems managing the substantial heat output from densely packed components. Power delivery is designed for efficiency and redundancy, often using multiple power supply units. The architecture balances GPUs, CPUs, memory, and storage to optimize data flow and computational tasks. This design enables the node to handle intensive AI workloads, from model training to complex inferencing, while maintaining efficiency and reliability under high loads. Such systems are typically housed in large form-factor chassis, reflecting their substantial computing power and cooling requirements.\nThe following table details the typical structure of a high-performance AI compute node."}, {"title": "B. Classification of AI Electricity Load", "content": "Here we provide the classification and description of different operation stages for AI loads, especially LLMs."}, {"title": "1) Training", "content": "Training is the most power-intensive phase of LLM operations. During this process, transformer-based model learns from vast datasets [41]. Such procedure requires sustained high GPU utilization for extended periods, often lasting days, weeks, or even months. This phase demands peak power from all system components, including GPUs, CPUs, memory, and storage. The continuous high computational load generates significant heat, necessitating robust cooling systems. Power supplies must be capable of handling prolonged maximum loads, and uninterrupted power is crucial. Power capping strategies may be employed to balance performance and energy costs."}, {"title": "2) Fine-tuning", "content": "Fine-tuning involves adapting a pre-trained model to specific tasks or domains, typically requiring moderate to high power consumption. This phase sees intermittent high GPU utilization but usually for shorter durations than full training. Power draw can fluctuate more during fine-tuning, demanding cooling systems that efficiently handle variable heat loads. This phase presents opportunities for implementing power-saving features during less intensive periods and allows for more flexible power management strategies compared to full training."}, {"title": "3) Inference", "content": "Inference is generally the least power-intensive phase, involving the application of trained models to new data. It often consists of shorter computational bursts, with power consumption varying widely based on model size and query complexity. Inference workloads may benefit from specialized hardware optimized for lower power consumption. Power draw during inference can be sporadic, requiring responsive power delivery systems. This phase offers significant opportunities for power savings through efficient scheduling and hardware utilization, though it is crucial to balance low-latency response times with energy efficiency. While generally less power-intensive than training or fine-tuning, inference presents unique challenges due to its behavior-dominated nature. Usage patterns can be highly variable and unpredictable, driven by user interactions, time of day, or external events. This variability leads to rapid fluctuations in power demand, from near-idle to sudden spikes of intense computation. Moreover, user-specific application such as retrieval-augmented generation (RAG) exhibits very diverse power usage patterns. The behavior-dominated aspect of inference adds complexity to power management strategies. Systems must be capable of rapidly scaling power delivery up and down to meet changing demands while maintaining low service latency."}, {"title": "C. The Correlation of LLM and its Hardware", "content": "LLMs demonstrate a strong correlation between their architecture and the hardware they run on. This relationship is evident in the computational pipeline of Transformer-based LLM on a typical PC with GPU acceleration (see in Figure 3):\n1) Input Processing: The CPU handles the initial tokenization of input text, a relatively light task.\n2) Data Transfer: Tokenized data moves from RAM to GPU memory via the PCIe bus, potentially becoming a bottleneck for large inputs.\n3) Embedding: The GPU's CUDA cores (assumed Nvidia) convert tokens to vector representations and add positional encodings, a moderately intensive computation.\n4) Transformer Blocks: The core of LLM's computation occurs here, leveraging the GPU's parallel processing capabilities:\n\u2022 Multi-head self-attention;\n\u2022 Layer normalization;\n\u2022 Feed-forward neural network (MLP).\nThis stage is the most computationally demanding, showcasing the necessity of GPU acceleration for LLMs.\n5) Output Layer: The GPU performs a final linear transformation and softmax operation to produce token probabilities.\n6) Token Selection: The next token is chosen based on output probabilities, utilizing both GPU and CPU.\n7) Output Processing: The CPU converts the selected token back to text.\nFor multi-token generation, steps 2-7 repeat iteratively, using each generated token as new input.\nThe power and computational demands of LLMs correlate strongly with model size and the running hardware. For example, the smallest version of GPT2, called GPT-2 small (124M parameters), can run on mid-range GPUs, while larger variants require high-end or multiple GPUs. (A detailed specification for typical small-scale LLM, often called small language model (SLM) can be found in Appendix.B.) This correlation between LLMs and hardware underscores why GPUs are crucial for the efficient operation of transformer models. The GPU's capacity for parallel computation is essential for the numerous matrix operations in Transformer blocks, forming the computational backbone of modern LLMs."}, {"title": "D. Unique features of AI workloads", "content": "1) High Computational Intensity\nAI tasks, especially during training, require massive amounts of floating-point operations. For instance, training a state-of-the-art natural language processing model can involve more than $10^8$ PetaFLOPS (FLOPS: floating-point operations per second). This level of computational intensity is orders of magnitude higher than some traditional high-performance computing tasks. This leads to a high energy consumption over different stages of the AI lifecycle.\n2) Variability and Unpredictability\nThe power consumption of AI workloads can fluctuate rapidly based on the stage of computation and the nature of the data being processed. This variability can occur on timescales of milliseconds to hours, even days. For example, during the training of deep neural networks, the backpropagation phase typically consumes more power than the forward pass, leading to periodic spikes in energy demand. Another typical example is the inference of LLMs, which can exhibit quite dramatic power peaks due to user behaviour. The power patterns are multifaceted and determined by a mixture of various factors, and sometimes unpredicatable, e.g., multiple unplanned shut downs happened during the 54-day long training process of the recent released Llama 3 405b [24].\n3) Scalability and Non-linear Scaling\nAI deployments range from edge devices with constrained resources to massive data center clusters, each presenting unique power management challenges. The power consumption can scale from watts to megawatts. Moreover, the computational requirements and associated power consumption of AI models often scale super-linearly with model size, leading to exponential increases in energy demands as models grow larger []. However, this is assuming the physical constraints are relatively relieved. In practice, the AI accelerator normally features non-linearity and the semiconductor is also experiencing rapid development (with higher efficiency each generation) which adds another layer of complexity when estimating the power consumption.\n4) Algorithmic Sensitivity\nSmall changes in AI algorithms or hyperparameters can lead to significant variations in computational requirements and, consequently, power consumption. For example, changing the learning rate or batch size during training can dramatically affect the convergence time and energy usage. Readers can refer to the Appendix.A where we demonstrate how the fine-tune algorithm relates with different power consumption stages.\n5) 24/7 Operation\nMany AI systems, especially those deployed in cloud services or continuous learning scenarios, operate around-the-clock. For instance, LLM training needs to be continuously monitored. Such constant, uninterruptible operation leads to sustained high power demand requiring consistent power quality. Such requirement can be also contrastive to typical IT loads which may have either more pronounced diurnal patterns or more flexibility."}, {"title": "III. POWER CONSUMPTION MODELING OF \u0391\u0399 WORKLOADS", "content": "Due to the aforementioned features of the AI loads, the detailed analytical/deterministic model of AI is quite difficult to build, and sometimes, impossible to acquire. However, this does not mean the mathematical description is not beneficial. In contrast, to effectively manage and optimize AI infrastructure as a load on power grids, it is crucial to develop mathematical models as detailed and accurate as we can to achieve proactive operation of local power grids with high penetration of AI infrastructure. Following this idea, this section addresses the AI-centric data center models with high-level mathematical language."}, {"title": "A. Metrics for AI-centric Data Center", "content": "Several power measurements come naturally into our view when considering the dynamics of AI loads. These metrics provide crucial insights into power consumption patterns, efficiency, and potential challenges in managing AI workloads in data centers.\n1) TDP (Thermal Design Power)\nTDP, or thermal design power, is defined as the maximum power that a subsystem is allowed to draw for a \"real world\" application. TDP is usually determined by the needs of the component that needs to be cooled. In the context of AI workloads, TDP is critical for cooling system design in data centers. AI tasks, especially training large models, often push hardware to its TDP limits. Understanding TDP helps in capacity planning and ensuring adequate cooling for AI hardware, which is essential for maintaining optimal performance and longevity of the equipment.\n2) GPU Utilization\nGPU utilization, while not strictly a power measurement, is closely related to power consumption in AI workloads. It represents the percentage of GPU computational resources being used. High GPU utilization is typical for AI training tasks and correlates strongly with high power consumption. Monitoring GPU utilization helps in optimizing workload distribution and energy efficiency, and can be used to identify opportunities for workload consolidation or expansion.\n3) PUE (Power Usage Effectiveness)\nPUE, or Power Usage Effectiveness, is the ratio of total energy used by a data center to the energy delivered to computing equipment. AI workloads can significantly impact PUE due to their high computational demands. Optimizing PUE is crucial for energy-efficient AI operations, and AI-centric data centers may need innovative cooling solutions to maintain good PUE. As AI workloads become more prevalent, maintaining an efficient PUE becomes increasingly challenging and important for sustainable data center operations.\n4) Peak/Average Ratio\nThe Peak/Average ratio is the ratio of peak power consumption to average power consumption. This metric is particularly interesting for AI-centric data centers under long-term training jobs, which show a Peak/Average ratio close to 1, compared to 1.5-2.0 for conventional data centers. This indicates that AI workloads maintain consistently high power consumption during operation. Such a characteristic has significant implications for power supply design and energy management in AI facilities, necessitating robust and stable power delivery systems.\n5) Peak/Idle Ratio\nThe Peak/Idle ratio, often overlooked in current literature, is the ratio of peak power consumption to idle power consumption. This metric is crucial for understanding power transience in AI workloads. It indicates how significant the power drop is after training interruption and how large the AI load could change the local power flow. High Peak/Idle ratios in AI workloads can cause substantial local power flow changes, making it an important consideration for designing power systems that can handle rapid transitions between active and idle states.\n6) dP/dt (Rate of Change of Power)\nThe dP/dt metric, representing the rate at which power consumption changes over time, is also often overlooked in current work. For a mega-watt scale AI data center, the unplanned stop of training can cause internal power disruption within few seconds or even shorter, resulting in great transience. If this cannot be buffered by the redundant Energy Storage System (ESS), then it will affect the local grid. This metric is critical for understanding and managing the dynamic nature of AI workloads and their potential impact on power infrastructure.\nWhile dP/dt provides a general measure of power change over time, the ramping rate and decline rate offer more specific insights into the behavior of AI systems during periods of increasing and decreasing power consumption, respectively.\nThe ramping rate represents the speed at which power consumption increases, typically observed during the startup of AI workloads or transitions to higher computational intensities. Conversely, the decline rate describes the speed of power reduction, often seen during job completion, sudden terminations, or transitions to lower computational states.\nThese rates are particularly important for characterizing the transient behavior of AI workloads. They play a crucial role in:\n\u2022 Designing power delivery systems capable of handling rapid fluctuations;\n\u2022 Optimizing job scheduling to minimize stress on power infrastructure;\n\u2022 Sizing and configuring energy storage systems to buffer transients;\n\u2022 Ensuring grid stability in the presence of large AI computing facilities.\nIn the context of AI workloads, both ramping and decline rates can be exceptionally high. For instance, the initiation of a large-scale training job might cause a near-instantaneous surge in power demand as GPUs are fully engaged. Similarly, the completion or interruption of such a job can lead to a rapid decline in power consumption.\nThese metrics and their mathematical representations will be central to our subsequent analysis of AI power dynamics. We will develop models that capture the unique characteristics of AI workloads, such as sustained high power draw, rapid fluctuations, and the relationship between computational load and power consumption. To ease the modelling and maintain a certain universality, the data center example from Google is selected as our benchmark system. Please note that the following modelling methodology can also be applied for other AI-centric data center configurations with minimum tuning on certain parameters like the distribution system type (DC bus, or AC bus), power station type (AC/AC or AC/DC), different supporting infrastructure, different external energy sources, etc."}, {"title": "B. Total Data Center Power Consumption", "content": "To analyze the data center's power usage, one aspect is to look into the total power consumption of the data center, which can be modeled as:\n$P_{total} = P_{AC\\_Bus} + P_{External}$.\nIn (1), $P_{AC\\_Bus}$ denotes the power supplied by the power utility from distribution grids, and $P_{External}$ represents all external power and energy sources, such as renewable energy generation, natural gas, fuel oils, etc. Recently, researchers and engineers have also explored the potential of hydrogen as an alternative power source [44].\nThe total power is distributed between the Supporting Infrastructure and IT Power Substation:\n$P_{total} = P_{Supporting\\_Infra} + P_{IT\\_Power}$.\nThe power consumption of the supporting infrastructure can be modeled with the major components:\n$P_{Supporting\\_Infra} = \\eta_{AC/AC} (P_{AHU}+\nP_{Chillers} + P_{Cooling\\_Tower} + P_{Pumps}+\nP_{Humidifiers} + P_{BMS} + P_{Lighting}+\nP_{office} + P_{UPS\\_Infra} + P_{Network\\_Infra});$\nWhere $\\eta_{AC/AC}$ represents the efficiency of the AC/AC conversion, AHU denotes the air handling unit, BMS is the building management system.\nThe IT power consumption, including the AI workload, is made up of the following major components:\n$P_{IT\\_Power} = \\eta_{AC/DC} \\cdot (P_{Servers} + P_{NetworkGear}+\nP_{Storage} + P_{CRAC} + P_{UPS\\_IT});$\nWhere $\\eta_{AC/DC}$ represents the efficiency of the AC/DC conversion, CRAC is computer room air conditioner. (Here we omit some details like the power distribution units (PDUs), switches, transformers, circuit breakers, etc.) Traditionally, a data center can also use an AC/AC conversion here, which normally involves several power conversion stages. With an AC/DC conversion, the internal local grid becomes a DC system and can be considered as a DC microgrid [45]."}, {"title": "C. AI Workload Power Consumption", "content": "1) Power Usage Breakdown of AI Accelerators\nIn this work, we assume all AI computation is happening in the servers, and their power consumption can be expressed as:\n$P_{Servers} = P_{AI} + P_{other\\_Compute};$\nWhere $P_{AI}$ represents the AI workload power consumption in the servers, and $P_{other\\_Compute}$ denotes all the related power needs that do not involve AI loads.\nIf steady-state power consumption is the main focus, a simpler model can be used (assuming the same GPUs are used and computation burden is shared evenly among them):\n$P_{AI}=N \\cdot P_{peak} \\cdot U_{phase};$\nWhere N is the number of AI accelerators, $P_{peak}$ is the peak power consumption of a single AI accelerator (this value is normally very close to TDP), $U_{phase}$ is the utilization factor (phase can be pre-train, fine-tune, or inference), which is a key metric representing the proportion of an AI accelerator's capacity being used at time t during a specific operational phase. This factor ranges from 0 (idle) to 1 (full utilization), providing crucial insights into the system's workload status. During pre-training $U_{pretrain}$, the utilization factor often approaches 1 for extended periods, indicating near-constant full utilization as the model learns from vast datasets. In the fine-tuning phase $U_{finetune}$, utilization remains high but becomes more variable, typically averaging between 0.7 to 0.9 as the model is adapted to specific tasks. The inference phase $U_{inference}$ exhibits the most variability in utilization, potentially covering the entire range from 0 to 1. This wide variation in $U_{inference}$ reflects the behavior-dominated nature of inference workloads, where utilization fluctuates based on real-time demand, user interactions, and external factors. Understanding these utilization patterns across different phases is essential for effective power management and resource allocation in AI systems."}, {"title": "D. Dynamic Power Consumption", "content": "Accounting for the dynamic nature of AI workloads and potential variability in external energy supply:\n$P_{dynamic}(t) = P_{AI}(t) + C \\cdot \\frac{dP_{AI}}{dt} +E \\frac{dP_{external}}{dt};$\nWhere C and E are coefficients representing the scale of the system's dynamics on AI power changes and external power fluctuations, respectively.\nFor a simpler case, we can omit the external power effect and this will give us:\n$P_{dynamic}(t) = P_{AI}(t) + C \\cdot \\frac{dP_{AI}}{dt}$\nBuilding upon our previous model, we can further highlight the dynamic and transient features:\n$P_{dynamic}(t) = P_{AI}(t) + C \\cdot \\frac{dP_{AI}}{dt} + D \\cdot \\frac{d^2 P_{AI}}{dt^2};$\nWhere we have added a second-order term $D \\cdot \\frac{d^2 P_{AI}}{dt^2}$ to capture the rapid power fluctuations. Here, C represents the scale of the system's gradual power changes, while D represents its scale to abrupt changes."}, {"title": "IV. AI POWER CONSUMPTION ANALYSIS: CASE STUDIES", "content": "To demonstrate the potential impacts brought by AI load, in this section, we simulate and analyze the power consumption characteristics of AI models under various settings of model size, hardware, and job types."}, {"title": "A. Case study 1: Data center behaviour", "content": "The MIT Supercloud Dataset was collected on the TX-Gaia system, a heterogeneous cluster at MIT's datacenter [46]. This system consists of 224 GPU-accelerated nodes, each equipped with two 20-core Intel Xeon Gold 6248 processors (384GB RAM) and two NVIDIA Volta V100 GPUs (32GB RAM each), as well as additional CPU-only nodes. The dataset, totaling over 2.1 TB, includes time series data on CPU and GPU utilization, memory usage, GPU temperature, node state snapshots, file I/O, and scheduler logs. The dataset features labeled data from running and manually annotating common deep neural networks across vision, Natural Language Processing (NLP), and Graph Neural Networks (GNN). In detail, it includes various vision networks (VGG, ResNet, Inception, U-Net), language models (BERT, DistilBERT), and graph neural networks (DimeNet, SchNet, PNA, NNConv).\nBERT (Bidirectional Encoder Representations from Transformers [41]) is one of the language models in this dataset, with 189 job counts recorded. BERT is a foundational transformer-based machine learning model for NLP tasks, pre-trained on a large corpus of unlabeled text. BERT is designed to understand the context of words in search queries and has significantly improved the interpretation of natural language in various applications.\nThe power consumption results for a specific BERT job reveal significant insights into the system's power dynamics. As the Figure 6 shows, the BERT model's power consumption is highly dynamic, with frequent transitions between low and high power states. While it can reach peaks of nearly 50kW, it often operates at much lower power levels. The system experiences both gradual and rapid changes in power consumption, likely corresponding to different phases of model operation such as data loading, forward passes, backward passes, and parameter updates. The action threshold at 50kW in the CDF graph (Figure 7) suggests that power draws above this level may be considered unusual or require special handling. The PDF of ramping rates (Figure 8) indicates that while the system can change power rapidly, it tends to favor more gradual transitions or stable states. This power profile is consistent with the complex and computationally intensive nature of training large language models like BERT, which involve alternating periods of high computational load and data movement or synchronization steps."}, {"title": "B. Baselines: LLMs", "content": "Consider the sizes of the models, the size of the data, along with the hardware limits on memory and computation, we are curating a list of results from experiments with feasible configurations as shown in Table III. Here, We present the training performance of GPT2-124M using Setup 1 and nanoGPT using Setup 2; the fine-tuning performance of GPT2-medium using Setup 2; the inference performance of nanoFPT, GPT-medium, Mamba and Transformer using Setup 2. The model details can be found in Table IV of Appendix.B. The GPU details can be found in Table V of Appendix.C."}, {"title": "C. Case study 2: LLM training", "content": "This training section demonstrates the power consumption patterns of two different GPU setups while training language models. The models being trained are GPT-2 124M and nanoGPT, both of which are transformer-based language models, but with different architectures and sizes.\nGPT-2 124M is a compact version of the GPT-2 model developed by OpenAI [47]. It contains 124 million parameters, making it the smallest in the GPT-2 family but still capable of generating coherent text and performing various language tasks. NanoGPT, on the other hand, is a minimalist implementation of the GPT architecture [48], designed by Andrej Karpathy to be more accessible and easier to understand than larger, more complex models.\nThe power consumption characteristics of these training sessions reveal interesting differences between the NVIDIA RTX 4090 and AMD Radeon RX 7900 XTX GPUs, as well as between the two model architectures. The GPT-2 124M training on the RTX 4090 shows a relatively stable power draw, averaging 414W with a maximum of 461W over a 22-hour training period. This setup exhibits large power transients, with drops of about 320W and ramps of 350W, demonstrating the GPU's ability to quickly adjust its power state.\nIn contrast, the nanoGPT training on the RX 7900 XTX displays more variable power consumption, fluctuating between approximately 50W and 250W. The power transients for this setup are smaller, with drops of about 150W and ramps of 130W. This difference in power profile could be attributed to the smaller model size of nanoGPT, differences in GPU architecture, or varying power management strategies between NVIDIA and AMD.\nThe unique power consumption patterns observed in these training sessions highlight the dynamic nature of GPU utilization in AI workloads. The RTX 4090's higher and more stable power draw, coupled with larger transients, suggests the device operating closer to its maximum capacity for longer periods. This is supported by its high average power of 414W and standard deviation of 113.7W. The RX 7900 XTX, while showing lower overall power consumption, demonstrates more frequent fluctuations, indicating algorithmic influence of the AI training coupling with the characteristics of the smaller nanoGPT model.\nThese observations underscore the importance of considering both hardware capabilities and model architecture when setting up AI training environments. The choice of GPU and model can significantly impact power consumption patterns, which in turn affects energy efficiency, cooling requirements, and overall system design for AI training setups."}, {"title": "D. Case study 3: Fintuning of LLM", "content": "The fine-tuning process of GPT2-medium on an AMD GPU 7900 XTX, following AMD's guidance [49], demonstrates distinct power consumption patterns that correspond to different stages of the training process. The power consumption graph illustrates the dynamic nature of GPU utilization during model fine-tuning, with four key phases identifiable as shown in Figure 13."}, {"title": "E. Case study 4: LLM Inference", "content": "The case study on LLM inference power consumption, focusing on GPT-2 and nanoGPT models, reveals intriguing patterns in GPU energy utilization. Both models demonstrate similar overall power profiles during inference, characterized by rapid transitions between low-power idle states and high-intensity computation phases. These power surges reach peaks of approximately 300W and last between 25 to 50 seconds, with a standard deviation of about 50W, indicating significant variability in power draw.\nThe extreme peak-to-idle power ratio observed in both models highlights the dynamic nature of LLM inference tasks as shown in Figure 14 and 15. NanoGPT appears to exhibit more frequent but shorter power spikes compared to GPT-2, suggesting differences in their information processing approaches. The maximum energy consumption recorded is 1.57kJ, reflecting the intensive but brief nature of these computational bursts.\nThese findings have important implications for system design in LLM deployment. The rapid fluctuations between low and high power states pose great challenges for handling swift load changes. Additionally, thermal management systems must be engineered to cope with short but intense heat generation periods."}, {"title": "F. Case study 5: LLM inference with different batch sizes", "content": "The models compared in this power consumption analysis are the state-spaces/mamba-2.8b and EleutherAI/gpt-neo-2.7B [50"}]}