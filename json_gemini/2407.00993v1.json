{"title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents", "authors": ["Shihan Deng", "Weikai Xu", "Hongda Sun", "Wei Liu", "Tao Tan", "Jianfeng Liu", "Ang Li", "Jian Luan", "Bin Wang", "Rui Yan", "Shuo Shang"], "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at https://github.com/XiaoMi/MobileBench.", "sections": [{"title": "1 Introduction", "content": "Interacting with mobile devices using natural language is a long-standing pursuit in human-computer interaction (Bolt, 1980; Karat et al., 2002; F\u00f8lstad and Brandtz\u00e6g, 2017). With the remarkable advancements in large language models (LLM) (Bai et al., 2022; Chowdhery et al., 2022; Du et al., 2021; Touvron et al., 2023; Ouyang et al., 2022), LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind their proficiency with web pages on PCs (Yao et al., 2022; Sun et al., 2023). To faithfully replicate a typical mobile environment, it's imperative to incorporate a diverse set of applications and leverage authentic data, moving beyond the limitations of purely simulated scenarios. The development challenges in the mobile domain stem from a trio of core issues: a limited understanding of mobile interfaces, a scarcity of application variety, and a lack of real-world data.\nDue to Google's breakthrough (Wang et al., 2023) in UI interface representation, LLM agent's understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines. However, these works collectively face the following challenges: (1) UI actions depend on the textual descriptions of interfaces, where structured text fails to capture the content of graphical buttons or images which can lead to wrong actions. A single API action might be equivalent to dozens of UI steps, leading to UI's inefficiency. (2) Their tasks are far removed from real-world task scenarios encountered in daily use, which require cooperation between multiple applications, with user commands being ambiguous and not specifying target applications. (3) The evaluation of tasks should not solely rely on LLMs, without any objective quantitative metrics.\nIn fact, voice assistants on mobile phones can meet most of the users' daily needs, yet they do not interact directly with UI interfaces but operate by invoking the APIs (Qin et al., 2023) behind applications. As shown in Figure 1, in mobile applications, APIs are more efficient than UI interfaces; a single API call can be equivalent to multiple UI operations to achieve the same outcome. However, a single API is insufficient for more complex tasks, especially when user commands are unclear, necessitating reliance on LLMs to interpret user intent. Therefore, an agent capable of utilizing both UI and APIs would be best suited for the job. Simultaneously, It requires developing a strategy for the selection and order of the application usage, with human oversight merely focusing on reviewing the outcomes. This is a function that voice assistants currently lack (Wen et al., 2023a,b). To this end, we develop a combination of API and UI actions to circumvent the limitations of UI interfaces, each action can be chosen between UI interactions and API calls; all tasks begin from the mobile HOME page rather than from the launch page of a specific application, enabling the agent to determine single or multiple applications it will use; queries in the task are gathered from real users, and instruction generation is only applied to some complex ones which undergo rigorous manual review; we draw inspiration from objective metrics in software automation testing, named CheckPoint, and have made necessary adjustments to accommodate the unpredictable semantic outputs of LLMs. Above all, we propose a mobile phone environment that includes a platform supporting both API and UI interactions, and a corresponding dataset with multi-APP tasks. Table 1 presents a comparison among recent platforms and benchmark work based on API and UI.\nOur contributions are summarized as follows:\n(1) To the best of our knowledge, we are the first to establish a running platform for LLM-based mobile agents that simultaneously supports both UI and API calls.\n(2) We propose an evaluation dataset containing diverse tasks for multi-APP interactions. Our tasks starting from the home page are more appropriate for testing the planning capabilities for agents. Our dataset and platform will be released soon.\n(3) We introduce a new category-based evaluation metric to assess the task completion capabilities of the agent in the context of both UI and API interactions."}, {"title": "2 Related Work", "content": "2.1 Mobile Platforms\nPrior to the emphasis on LLM-based agents, research efforts were directed towards RL-based agents, exemplified by the Android-Env platform (Toyama et al., 2021). This open-source platform tailored for reinforcement learning experiments within the Android ecosystem, successfully tested various RL-based agents like DDPG (Zhang and Van Huynh, 2023), D4PG (Barth-Maron et al., 2018), MPO (Abdolmaleki et al., 2018), DQN (Mnih et al., 2015), IMPALA (Espeholt et al., 2018) and R2D2 (Kapturowski et al., 2018).\nMore significant research has focused on LLM-based agents (Liu et al., 2024; Sun et al., 2024b,a). Regarding the domain of tool-using agents, they can be categorized into three main types:\n1) For mobile tasks. Platforms like AutoDroid, DroidBot-GPT, GPT-Droid, and WebShop (Wen et al., 2023a,b; Liu et al., 2023b; Yao et al., 2022) create an interactive environment enabling LLMs to engage with mobile tasks, and generate human-like operations for automation test. Mobile-Env (Zhang et al., 2023) is specifically designed to evaluate agents' capabilities in handling multi-step interactions.\n2) For PC Tasks. Researchers developed Toolllama (Qin et al., 2023) to evaluate the capabilities to use tools and API calls. AgentBench (Liu et al., 2023a) presents a standardized Agent task evaluation architecture with strong decoupling and scalability. PPTC Benchmark (Guo et al., 2023) proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.\n3) Other Methods. Toolformer (Schick et al., 2023) and HuggingGPT (Shen et al., 2023) evaluate LLM's capability to master tools.\n2.2 Benchmarks for LLM agents\nTo assess agents' proficiency in understanding user interfaces, a diverse dataset covering various tasks is crucial (Liu et al., 2023a). The widely used RICO dataset (Deka et al., 2017) is commonly employed for this purpose, with Screen2Vec (Li et al., 2021) utilizing it to evaluate agent performance. However, due to the absence of specific standards for evaluating agent performance, efforts have focused on designing evaluation frameworks. PPTC Benchmark (Guo et al., 2023) devised 279 multi-round dialogue tasks for PPT file operations. DroidTask (Wen et al., 2023a) and various unnamed datasets (Liu et al., 2023b; Wen et al., 2023b) covering various mobile applications have also been established. Additionally, Screen2Words used a sampling method to sample screens from the RICO-SCA (Li et al., 2020) dataset and hired professional annotators to generate English summaries for these screens (Wang et al., 2021).\nCurrent evaluation standards align with various works. ToolBench proposes Win Rate gauges the model's solution quality against benchmarks like ROBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023). Although Fan (Fan et al., 2024) found that the cost of inference can be reduced by using only the necessary layers for inference, it is still expensive to calculate the win rate. Mobile-Env (Zhang et al., 2023) evaluates agent performance based on the completion status, average steps, and average rewards in WikiHow tasks. PPTC Benchmark (Guo et al., 2023) uses Turn-based and Session-based accuracy. Android in the Wild (Rawles et al., 2023) makes use of Out-of-distribution Generalization. Overall, metrics such as success rate, episode length, and match score are currently the most commonly employed."}, {"title": "3 Our Environment", "content": "3.1 Mobile-Bench Benchmark\nData collection. The queries in the dataset are divided into the following three categories:\n\u2022 SAST: Single-App-Single-Task. A real dataset containing only one task text, including single-task operations such as opening and closing APP, such as \"Help me open the map\".\n\u2022 SAMT: Single-App-Multi-Task. A real dataset containing multiple task texts, as well as constructed single-APP data. A complex multi-task on single APP, such as \"Help me open the map, and navigate to Eiffel Tower.\".\n\u2022 MAMT: Multi-App-Multi-Task. Constructed multi-APP data, complete a complex multi-task, such as \"Help me search for the latest technology news and share it with friends.\"\nSAST is directly derived from real voice requests processed by the voice assistants loaded on the mobile phone. We select a subset of this query collection, primarily filtering out the portion that requires voice assistant processing and involves multimodal tools. Additionally, querys that exceed permissions or involve privacy are also filtered out.\nSince there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama (Qin et al., 2023) method, we use GPT-4 to construct SAMT and MAMT data. For MAMT, we randomly sample 6 applications from the entire application collection, and then provide some examples of real multi-APP data to prompt GPT-4 to select 2-4 applications to generate tasks. By integrating real and constructed data, we create the final dataset. An example of data is shown in Figure 2."}, {"title": "APP & API collection", "content": "To ensure task comprehensiveness, we select not only the applications included in SAST and SAMT but also the most popular free applications from each category in the APP Store. Obtaining the API is to analyze the package of each application to obtain its external reserved interface (Desnos and Gueguen, 2011). The advantage of this is that the obtained API is naturally classified for the application. Since the description of the API in the decompilation result is not as detailed as the development document, we use the ADB(Android Debug Bridge) command to verify the feasibility of the API one by one. Owing to its debugging properties, system-level APIs can also be invoked normally, allowing access to functions such as checking the battery status and performing memory cleaning. For more specific application names and categories, please refer to Appendix B.3\nDataset statistics. Including several default applications within the system, we collected a total of 29 applications. For applications, we collected a total of 103 usable APIs, which primarily serve the following functions: system calls, opening pages, closing pages, searching for information, viewing details, and controlling device switches. These functions are summarized into the following main aspects: page switch, details view, broadcast, search. In Table 2, we have tabulated the number of APIs and the functional categories covered by APIs, categorized by the type of APP. We organized the available APIs and APP descriptions for each APP, and generated an APP list as the basis for selecting applications, shown in Appendix B.3.\nIn the Mobile-Bench dataset, we collected a total of 332, 300, 200 queries for SAST, SAMT, and MAMT. We sort out the APIs actually used by each task in real voice requests. Provide these API as an example to GPT-4 for query generation. As shown in Figure 3(a), we calculated the ratio of tasks calling APIs, ensuring a sufficient number of tasks in the dataset that include steps to call API. This approach ensures that we have sufficient data to analyze the role of APIs in task completion.\nQuality verification. (Bolotova-Baranova et al., 2023) The initial test data originates from software automation tests, but some complex data points are generated by GPT-4. To ensure the quality of our dataset, we randomly sampled 100 data points from each of the SAST, SAMT, and MAMT, resulting in a total of 300 quality test data. We conducted cross-source validation to verify the feasibility of these CheckPoints. The specific formula for calculation is as follows:\nOverlap $(CP_1, CP_2) = \\frac{|CP_1 \\cap CP_2|}{|CP_1|}$ (1)\n$CP_1, CP_2$ representing the CheckPoint sequences generated by $CP_{instruction}$ and $CP_{Human}$, respectively. In Table 3, we list the human evaluation results for three types of data. From the table, it can be observed that a higher proportion of terminal data corresponds to better data quality. However, all MAMT data is generated by instructions, its quality does not exhibit an unacceptable gap compared to SAST. See appendix B.1 for more analysis."}, {"title": "3.2 Test Platform", "content": "Overview Mobile-Bench is designed as a universal interaction platform that supports hybrid API and UI interactions. Users are able to construct their own evaluation data following a fixed format, yet they must adhere to our prescribed evaluation method. As shown in Figure 4 users can interact with the environment using the following commands.\n\u2022 Start: Open the test environment and load the preset snapshot using this command. Each test case must start from the same environment.\n\u2022 Stop: Stop the test environment and end test.\n\u2022 Close: Close the test environment and save the test process and results.\n\u2022 Check: Capture a screenshot snapshot of the current test environment.\n\u2022 ReSet: Load a previously saved environment snapshot into the test environment.\nObservation space To enable the agent to read information on the android emulator in a human-like manner, we use Appium to obtain page information. Following the method described by Wang (Wang et al., 2023), we convert XML to HTML, as the training data for LLMs is predominantly sourced from the Internet, which includes numerous HTML files. Therefore, we believe that LLM has a better understanding of HTML than XML.\nGiven the tree structure of XML, we initially convert the XML into a tree format and subsequently transform the nodes that need to be displayed to the agent into HTML. The agent simulates human interaction with smartphones, performing three major operations: click, input, and scroll. Humans visually identify which elements can be clicked or receive input, and use their fingers to determine if they can scroll the screen. Therefore, we provide the agent with elements that are visible and scrollable. Due to the limit on context length, we only convert the information required by the agent in XML to HTML:\n\u2022 Type: HTML element categories inherited directly from XML formatted information.\n\u2022 ID: \"ID\" inherits from the XML \"resource-id\" attribute, uniquely identifying the existence of an element.\n\u2022 Package: the package name of the current application.\n\u2022 Class: the class of the element, such as ImageView, TextView.\n\u2022 Description & text: describe the function and shape of the element.\n\u2022 Clickable & Scrollable: whether the element is clickable and scrollable.\n\u2022 Bounds: if the element is scrollable, this attribute will be present and scope the scroll component, such as:\n$[X_i, Y_i] [X_j, Y_j]$\nThe scrollable rectangle ranges from $[x_i, Y_i]$ to $[x_j, Y_j]$."}, {"title": "3.3 Evaluation Method", "content": "CheckPoint. Automated test CheckPoint coverage (Bajunaid and Menasc\u00e9, 2018) is a test metric for the software execution process. It cannot assist in checking the software results, but it can visually inspect whether the software runs in the specified unit sequence. During data construction, we supply APPs and APIs, which naturally serve as detection indicators. Additionally, we incorporated a CheckPoint to verify if the UI operation correctly clicks on the intended element. After sorting out the above CheckPoints, we constructed the following three CheckPoints:\n\u2022 Package: the unique package name corresponding to the application. Checking the package can determine whether the correct application is used.\n\u2022 Key phrase: the key phrase extracted from the query, represents key steps in the UI execution process.\n\u2022 API: API commands that need to be called during the execution process.\nTo evaluate the agent's selection and execution capabilities, we divide the inspection granularity into two levels: CheckPoint11 - whether it uses the correct application, and CheckPoint12 - whether it follows the predefined paths to complete the task. For CheckPoint11, we check the number of correctly called packages. For CheckPoint12, we check the number of correctly called package, key phrase, API. For CheckPoints, we identify three logical relationships: sequential, conjunctive, and disjunctive checks. These correspond to the instability of LLM output and its tendency for synonym substitution. The calculation formula for \"sequential check\" is as follows:\n$Score_{Sequen} = \\frac{|\\sum_{str \\in SC} str \\in AH|}{|\\sum_{str \\in SC} Str|}$ (2)\nSC represent Sequential Check Set and AH represent Actions History. The calculation formulas for conjunctive checks is as follows:\n$Score_{conjun} =\\begin{cases}1, \\text{ if } \\forall str \\in CC, str \\in AH\\\\0, \\text{ otherwise }\\end{cases}$ (3)\nCC represent Conjunctive Check Set. The calculation formulas for disjunctive checks is as follows:\n$Score_{disjun} =\\begin{cases}1, \\text{ if } \\exists str \\in DC, str \\in AH\\\\0, \\text{ otherwise }\\end{cases}$ (4)\nDC represent Disjunctive Check Set. The weighted sum of the above three scores will be the final CheckPoint coverage rate.\nAs shown in Figure 3, the number of key phrase CheckPoints is significantly higher than that of packages, indicating the need for more semantic information to ensure tasks are completed step-by-step. Analyzing the dataset from a proportional perspective, we find that the distributions of the three types of CheckPoints are 0.212, 0.493, 0.294, with key phrase CheckPoints remaining the most predominant method of checking.\nIn general, a test case should include at least the following contents: ID, Query, APP List, CheckPoints (Package, Key phrase, API). Figure 2 is a test case that contains the above three CheckPoints.\nPassRate. (Qin et al., 2023) We assess an agent's human-computer interaction capabilities by calculating the proportion of queries successfully completed within the specified step limits. During this process, we organized the emulator's current state. Subsequently, GPT-4 evaluates the task completion status. We computed the percentage of pass tasks, yielding a PassRate as an indicator of agent's human-computer interaction capabilities.\nAverage steps. (Zhang et al., 2023) We quantified the step size required by Mobile-Bench to complete tasks as a metric for evaluating the efficiency of the agent. In Mobile-Bench, a 'step' is defined as the completion of a UI operation or the execution of an API call."}, {"title": "4 Experiment", "content": "4.1 Baseline Model\nOur model's architecture, illustrated in Algorithm 1, begins by obtaining the smartphone's UI information in XML format through Appium and transforms it into HTML format through a heuristic algorithm. Subsequently, as illustrated in Figure 5 leveraging the HTML, task details, and APP list, LLM generates a comprehensive task plan, outlining the necessary applications and corresponding sub-tasks. As the collection of APIs is organized based on the classification of APPs, we can get the API set that may be used in plan.\nThe task plan is executed iteratively. In each iteration, the model either performs an API call or a UI operation. After each execution, the model records the success or failure of the action in its history, generates the subsequent thought, and evaluates whether the task has been completed. For the actual running process of an algorithm, please refer to the appendix C.7.\n4.2 Setup\nWe evaluate four popular LLMs on the proposed Mobile-Bench task set: GPT-3.5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023), LLaMA-13B and LLaMA-70B(Touvron et al., 2023), while ChatGPT-3.5 and GPT-4 are accessed through the online APIs of OpenAI. The experiments are conducted with a 3-shot in-context learning under sampling temperature of 0.1. Recognizing that task execution incurs costs, we preset different maximum step limits for tasks based on their difficulty levels. For the three categories of SAST, SAMT, and MAMT, we set the max step to 10, 20, and 50 respectively. Owing to the limit of budget, only GPT-3.5 utilizes an interface with a context length of 16K. GPT-4 uses a standard interface, which necessitated compression and trimming of actions history. See Appendix A for other settings.\n4.3 Results\nAs observed in Table 4, it can be observed that GPT-3.5 outperforms GPT-4 in PassRate on SAMT(64%>63%), and it requires fewer steps to complete the task(12.06<13.94). To investigate this phenomenon, we analyze the output files and find that models with poorer performance exhibit PassRate misjudgments: they prematurely terminate even when the task is not completed. This phenomenon is also present in LLaMA, which exhibits a high PassRate (44.58%) but low CheckPoint coverage (34.85%). At the same time, we delved into why the results for MAMT are so low (15.5%, 26.5%). Our analysis revealed that LLMs often exhibit greedy exploration behavior when completing tasks, meaning they struggle to determine when to exit the current application and transition to the next one. This tendency is particularly prevalent in certain generation tasks. Moreover, as the actions history increases, its ability to accurately judge task progress becomes increasingly challenging. For more detailed result, please refer to Table 7.\n4.4 Impact of API Calls\nAPI Calls can accelerate task execution, as a single call often replaces several sequential UI steps. From another perspective, the ability of the agent to select appropriate APIs and input parameters warrants further investigation. Choosing the wrong API may lead the task in an incorrect direction or require a significant number of steps to rectify.\nTherefore, in Table 5, we evaluate and analyze the impact of introducing APIs on task completion based on GPT-4.\nFrom Table 5, it can be seen that even in SAST, the PassRate has decreased by 6.57% (from 80.96 to 74.39). Furthermore, the values for CheckPoints12 exhibit a more pronounced decrease after API removal, with a drop exceeding 20% in SAMT. Simultaneously, we have observed varying increases in the average number of steps, which align with our expectations. We analyzed the results and found that the inability to accurately scroll pages, inefficient exploration of page functionality, and failure to click graphical buttons are the primary reasons for the low efficiency of UI operations.\n4.5 Impact of Plan and Thought\nSince observation-thought-action is already a standardized process in the agent direction(Qin et al., 2023), and verified by experimental results, planning and thought before action are essential. From the experimental results, we can find that without the observation-thought step, the agent is almost unable to complete the task(77->20, 76->24), which is because it cannot determine the next action category and the current task status. In more complex tasks SAMT, losing the plan has more negative consequences(77->62). But they will have almost no impact on CheckPoint\u2081\u2081(82->82 63->63), because the application selection is almost done by the API Call."}, {"title": "5 Conclusion", "content": "In this work we have proposed an agent capability testing environment that supports API and UI interaction on mobile phone. This holds significant importance for exploring how LLMs can be integrated with mobile operating systems. Additionally, it can serve as a valuable reference for developing testing platforms for operating systems to evaluate the capabilities of LLM agents. We collected and released a test dataset containing tasks for multiple APPs, ensuring its quality through human verification. Based on this data set and environment, we tested the planning, decision-making and execution of various LLM-based agents. Please refer to the Section 6 for the limitations of our benchmark."}, {"title": "6 Limitations", "content": "While general large models exhibit strong capabilities in reasoning and planning, they tend to have pronounced illusions in API calls. As a result, the language model may become confused about the application's functionality, leading to a reluctance to continue and complete the task. Therefore, fine-tuning a model for instructions is highly necessary.\nAutomatic CheckPoint is a process evaluation metric, making it challenging to assess the quality of the final outcome. This depends on whether the agent has obtained the necessary information (actions) on the required pages.\nThe enhancement of the agent's capabilities relies on extensive API and SDK libraries, requiring substantial support from application development companies."}, {"title": "7 Ethics Statement", "content": "We have rigorously refined our dataset to remove any elements that could compromise personal privacy, thereby guaranteeing the highest level of protection for individual data. The evaluation of our work was carried out through a meticulously randomized selection of IT professionals. This process ensured a gender-balanced and educationally diverse panel, reflecting a wide spectrum of perspectives and expertise."}, {"title": "8 Acknowledgements", "content": "We thank the Xiaoai Voice Department of Xiaomi Technology Corporation for their raw data support for this project. We additionally thank our crowd annotators for their diligent work, Junfeng Peng and Yifan Cheng for contributing to the human performance estimates, and the anonymous reviewers for their constructive comments. This work was supported by the NSFC (U2001212, 62032001, and 61932004)."}, {"title": "A Settings", "content": "We conduct experiments on the Android 14.0 version emulator and use Appium UiAutomator2 Driver for automated testing. Before each execution of a task, we load a snapshot to ensure the emulator in the same environment every time. For all applications, we have logged in to the account in advance to ensure that the full function of the application can be used. Since we tests in the real world, we filtered out any tasks that included payments."}, {"title": "B Details of Dataset", "content": "B.1 Dataset quality analysis\nThe root cause of low-quality data often lies in the inaccuracies in the descriptions of applications. Additionally, ambiguity in query generation also plays a significant role. For example, in the query \"Help me find pictures related to Beijing\u201d, although the user has not explicitly specified the source application, for a human, the expected result would likely be a search engine or a map application, as the images are not likely to be from the user themselves. However, for LLM, because the statement includes the word \"pictures\", it might be reasonable for it to spend all its time searching for pictures in the gallery application, even though this effort would ultimately be in vain. CheckPoint coverage is calculated as the weighted sum of the scores for the three types of CheckPoints mentioned above.\nB.2 Prompts for Instruction Generation\nBelow we list the detailed prompt for instruction generation, including single-APP-multi-task description, multi-APP-multi-task description.\nsingle-APP-multi-task description:\nYou will be provided with an application with descriptions, an available API list including adb command, function description and parameter information. You should create 5 varied, innovative, and detailed multi task queries that employ this application as a tool, API can be used as an auxiliary.\nEach query should include the necessary parameters. Note that you shouldn't ask \u2018which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APP and APIs have to strictly come from the provided lists.\nAt the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.\nKey phrase can contain multiple pieces of information, \"|\" means the query passes when any of the following texts are completed. \"|\" is used to separate synonymous expressions of the same noun; \"&\" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in \"[]\", and the count increases by one for each passed element. The \"ADB Command\" to be used is stored in the API, which may also be empty."}, {"title": "multi-APP-multi-task description", "content": "You will be provided with some APPs with descriptions, available API list including adb command, function description and parameter information. You should create 3 varied, innovative, and detailed multi queries that employ multi-APP as a tool, API can be used as an auxiliary.\nEach query should include the necessary parameters. Note that you shouldn't ask \u2018which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APPs and APIs have to strictly come from the provided lists. You should first think about possible related APP combinations, then give your query. Keep in mind that each query should call upon two to four APPs.\nAt the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.\nKey phrase can contain multiple pieces of information, \"|\" means the query passes when any of the following texts are completed. \"|\" is used to separate synonymous expressions of the same noun; \"&\" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in \"[]\", and the count increases by one for each passed element. The \"ADB Command\" to be used is stored in the API, which may also be empty."}, {"title": "B.3 APP&API statistics", "content": "As can be seen from Figure 6, each functional area contains at least one application and its corresponding API. These applications are sufficient to meet the daily needs of users. In other words, our simulation environment is almost consistent with the real daily use environment, and it is consistent with the real daily use environment. Open world information exchange. There are so many practical tools that are the basic functions of mobile . They have been automatically installed and completed during system installation, and standard API interfaces for tools are easier to obtain. Our next step is to increase the number of APIs and SDKs for third-party applications.\nB.4 Case study\nCheckPoints is a group of words, including packages, key phases, and API, which represent the package name, action keywords, and API instructions of the application respectively. We regularize these words and action histories to check whether they select a sufficient and correct number of applications, UI elements, and APIs to accomplish the given task.\nNext, we will give an example of CheckPoints in Figure 7 and Figure 8."}, {"title": "C Details for Baseline Model", "content": "C.1 Examples for HTML\nFigure 11 shows the correspondence between the components in the UI page and the corresponding HTML code. It is easy to find that most components have text descriptions, but the switch of the alarm clock does not have a corresponding text description, and LLM will hardly think of it. To click this button, therefore, component function exploration is what we need to do next.\nC.2 Prompts for application Selection and Planning\nYou are a large language model agent stored on a mobile phone, below I will provide you with a task, the environment of the current mobile phone interface(Apps information).\nPlease help me choose the correct APP to perform the task based on the Apps information. If the APP you want is not available on the current page, you can go to play store and download a suitable APP. On this basis, you should make a simple plan for completing the task.\nLet's Begin!\nC.3 Prompts for API Selection\nYou are the greatest large language model agent stored on a mobile phone. You will be provided with a API list that can be called by mobile phone, the task you need to complete, the thought about what have done and what need to do now.\nYou are just the first step to interact with the phone, and your follow-up is UI interaction components. If you find that there is no suitable API and the next step is UI interaction, please answer directly sorry. You should not use the API to complete the work that has been completed by the UI interactive components in the previous steps.\nYour decision should consider the following factors:"}, {"title": "C.4 Prompts for UI Selection", "content": "You are a large language model agent stored on a mobile phone", "functions": "n1. click(element)\nClick a element"}, {"title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents", "authors": ["Shihan Deng", "Weikai Xu", "Hongda Sun", "Wei Liu", "Tao Tan", "Jianfeng Liu", "Ang Li", "Jian Luan", "Bin Wang", "Rui Yan", "Shuo Shang"], "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at https://github.com/XiaoMi/MobileBench.", "sections": [{"title": "1 Introduction", "content": "Interacting with mobile devices using natural language is a long-standing pursuit in human-computer interaction (Bolt, 1980; Karat et al., 2002; F\u00f8lstad and Brandtz\u00e6g, 2017). With the remarkable advancements in large language models (LLM) (Bai et al., 2022; Chowdhery et al., 2022; Du et al., 2021; Touvron et al., 2023; Ouyang et al., 2022), LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind their proficiency with web pages on PCs (Yao et al., 2022; Sun et al., 2023). To faithfully replicate a typical mobile environment, it's imperative to incorporate a diverse set of applications and leverage authentic data, moving beyond the limitations of purely simulated scenarios. The development challenges in the mobile domain stem from a trio of core issues: a limited understanding of mobile interfaces, a scarcity of application variety, and a lack of real-world data.\nDue to Google's breakthrough (Wang et al., 2023) in UI interface representation, LLM agent's understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines. However, these works collectively face the following challenges: (1) UI actions depend on the textual descriptions of interfaces, where structured text fails to capture the content of graphical buttons or images which can lead to wrong actions. A single API action might be equivalent to dozens of UI steps, leading to UI's inefficiency. (2) Their tasks are far removed from real-world task scenarios encountered in daily use, which require cooperation between multiple applications, with user commands being ambiguous and not specifying target applications. (3) The evaluation of tasks should not solely rely on LLMs, without any objective quantitative metrics.\nIn fact, voice assistants on mobile phones can meet most of the users' daily needs, yet they do not interact directly with UI interfaces but operate by invoking the APIs (Qin et al., 2023) behind applications. As shown in Figure 1, in mobile applications, APIs are more efficient than UI interfaces; a single API call can be equivalent to multiple UI operations to achieve the same outcome. However, a single API is insufficient for more complex tasks, especially when user commands are unclear, necessitating reliance on LLMs to interpret user intent. Therefore, an agent capable of utilizing both UI and APIs would be best suited for the job. Simultaneously, It requires developing a strategy for the selection and order of the application usage, with human oversight merely focusing on reviewing the outcomes. This is a function that voice assistants currently lack (Wen et al., 2023a,b). To this end, we develop a combination of API and UI actions to circumvent the limitations of UI interfaces, each action can be chosen between UI interactions and API calls; all tasks begin from the mobile HOME page rather than from the launch page of a specific application, enabling the agent to determine single or multiple applications it will use; queries in the task are gathered from real users, and instruction generation is only applied to some complex ones which undergo rigorous manual review; we draw inspiration from objective metrics in software automation testing, named CheckPoint, and have made necessary adjustments to accommodate the unpredictable semantic outputs of LLMs. Above all, we propose a mobile phone environment that includes a platform supporting both API and UI interactions, and a corresponding dataset with multi-APP tasks. Table 1 presents a comparison among recent platforms and benchmark work based on API and UI.\nOur contributions are summarized as follows:\n(1) To the best of our knowledge, we are the first to establish a running platform for LLM-based mobile agents that simultaneously supports both UI and API calls.\n(2) We propose an evaluation dataset containing diverse tasks for multi-APP interactions. Our tasks starting from the home page are more appropriate for testing the planning capabilities for agents. Our dataset and platform will be released soon.\n(3) We introduce a new category-based evaluation metric to assess the task completion capabilities of the agent in the context of both UI and API interactions."}, {"title": "2 Related Work", "content": "2.1 Mobile Platforms\nPrior to the emphasis on LLM-based agents, research efforts were directed towards RL-based agents, exemplified by the Android-Env platform (Toyama et al., 2021). This open-source platform tailored for reinforcement learning experiments within the Android ecosystem, successfully tested various RL-based agents like DDPG (Zhang and Van Huynh, 2023), D4PG (Barth-Maron et al., 2018), MPO (Abdolmaleki et al., 2018), DQN (Mnih et al., 2015), IMPALA (Espeholt et al., 2018) and R2D2 (Kapturowski et al., 2018).\nMore significant research has focused on LLM-based agents (Liu et al., 2024; Sun et al., 2024b,a). Regarding the domain of tool-using agents, they can be categorized into three main types:\n1) For mobile tasks. Platforms like AutoDroid, DroidBot-GPT, GPT-Droid, and WebShop (Wen et al., 2023a,b; Liu et al., 2023b; Yao et al., 2022) create an interactive environment enabling LLMs to engage with mobile tasks, and generate human-like operations for automation test. Mobile-Env (Zhang et al., 2023) is specifically designed to evaluate agents' capabilities in handling multi-step interactions.\n2) For PC Tasks. Researchers developed Toolllama (Qin et al., 2023) to evaluate the capabilities to use tools and API calls. AgentBench (Liu et al., 2023a) presents a standardized Agent task evaluation architecture with strong decoupling and scalability. PPTC Benchmark (Guo et al., 2023) proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.\n3) Other Methods. Toolformer (Schick et al., 2023) and HuggingGPT (Shen et al., 2023) evaluate LLM's capability to master tools.\n2.2 Benchmarks for LLM agents\nTo assess agents' proficiency in understanding user interfaces, a diverse dataset covering various tasks is crucial (Liu et al., 2023a). The widely used RICO dataset (Deka et al., 2017) is commonly employed for this purpose, with Screen2Vec (Li et al., 2021) utilizing it to evaluate agent performance. However, due to the absence of specific standards for evaluating agent performance, efforts have focused on designing evaluation frameworks. PPTC Benchmark (Guo et al., 2023) devised 279 multi-round dialogue tasks for PPT file operations. DroidTask (Wen et al., 2023a) and various unnamed datasets (Liu et al., 2023b; Wen et al., 2023b) covering various mobile applications have also been established. Additionally, Screen2Words used a sampling method to sample screens from the RICO-SCA (Li et al., 2020) dataset and hired professional annotators to generate English summaries for these screens (Wang et al., 2021).\nCurrent evaluation standards align with various works. ToolBench proposes Win Rate gauges the model's solution quality against benchmarks like ROBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023). Although Fan (Fan et al., 2024) found that the cost of inference can be reduced by using only the necessary layers for inference, it is still expensive to calculate the win rate. Mobile-Env (Zhang et al., 2023) evaluates agent performance based on the completion status, average steps, and average rewards in WikiHow tasks. PPTC Benchmark (Guo et al., 2023) uses Turn-based and Session-based accuracy. Android in the Wild (Rawles et al., 2023) makes use of Out-of-distribution Generalization. Overall, metrics such as success rate, episode length, and match score are currently the most commonly employed."}, {"title": "3 Our Environment", "content": "3.1 Mobile-Bench Benchmark\nData collection. The queries in the dataset are divided into the following three categories:\n\u2022 SAST: Single-App-Single-Task. A real dataset containing only one task text, including single-task operations such as opening and closing APP, such as \"Help me open the map\".\n\u2022 SAMT: Single-App-Multi-Task. A real dataset containing multiple task texts, as well as constructed single-APP data. A complex multi-task on single APP, such as \"Help me open the map, and navigate to Eiffel Tower.\".\n\u2022 MAMT: Multi-App-Multi-Task. Constructed multi-APP data, complete a complex multi-task, such as \"Help me search for the latest technology news and share it with friends.\"\nSAST is directly derived from real voice requests processed by the voice assistants loaded on the mobile phone. We select a subset of this query collection, primarily filtering out the portion that requires voice assistant processing and involves multimodal tools. Additionally, querys that exceed permissions or involve privacy are also filtered out.\nSince there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama (Qin et al., 2023) method, we use GPT-4 to construct SAMT and MAMT data. For MAMT, we randomly sample 6 applications from the entire application collection, and then provide some examples of real multi-APP data to prompt GPT-4 to select 2-4 applications to generate tasks. By integrating real and constructed data, we create the final dataset. An example of data is shown in Figure 2."}, {"title": "APP & API collection", "content": "To ensure task comprehensiveness, we select not only the applications included in SAST and SAMT but also the most popular free applications from each category in the APP Store. Obtaining the API is to analyze the package of each application to obtain its external reserved interface (Desnos and Gueguen, 2011). The advantage of this is that the obtained API is naturally classified for the application. Since the description of the API in the decompilation result is not as detailed as the development document, we use the ADB(Android Debug Bridge) command to verify the feasibility of the API one by one. Owing to its debugging properties, system-level APIs can also be invoked normally, allowing access to functions such as checking the battery status and performing memory cleaning. For more specific application names and categories, please refer to Appendix B.3\nDataset statistics. Including several default applications within the system, we collected a total of 29 applications. For applications, we collected a total of 103 usable APIs, which primarily serve the following functions: system calls, opening pages, closing pages, searching for information, viewing details, and controlling device switches. These functions are summarized into the following main aspects: page switch, details view, broadcast, search. In Table 2, we have tabulated the number of APIs and the functional categories covered by APIs, categorized by the type of APP. We organized the available APIs and APP descriptions for each APP, and generated an APP list as the basis for selecting applications, shown in Appendix B.3.\nIn the Mobile-Bench dataset, we collected a total of 332, 300, 200 queries for SAST, SAMT, and MAMT. We sort out the APIs actually used by each task in real voice requests. Provide these API as an example to GPT-4 for query generation. As shown in Figure 3(a), we calculated the ratio of tasks calling APIs, ensuring a sufficient number of tasks in the dataset that include steps to call API. This approach ensures that we have sufficient data to analyze the role of APIs in task completion.\nQuality verification. (Bolotova-Baranova et al., 2023) The initial test data originates from software automation tests, but some complex data points are generated by GPT-4. To ensure the quality of our dataset, we randomly sampled 100 data points from each of the SAST, SAMT, and MAMT, resulting in a total of 300 quality test data. We conducted cross-source validation to verify the feasibility of these CheckPoints. The specific formula for calculation is as follows:\nOverlap $(CP_1, CP_2) = \\frac{|CP_1 \\cap CP_2|}{|CP_1|}$"}, {"title": "3.2 Test Platform", "content": "Overview Mobile-Bench is designed as a universal interaction platform that supports hybrid API and UI interactions. Users are able to construct their own evaluation data following a fixed format, yet they must adhere to our prescribed evaluation method. As shown in Figure 4 users can interact with the environment using the following commands.\n\u2022 Start: Open the test environment and load the preset snapshot using this command. Each test case must start from the same environment.\n\u2022 Stop: Stop the test environment and end test.\n\u2022 Close: Close the test environment and save the test process and results.\n\u2022 Check: Capture a screenshot snapshot of the current test environment.\n\u2022 ReSet: Load a previously saved environment snapshot into the test environment.\nObservation space To enable the agent to read information on the android emulator in a human-like manner, we use Appium to obtain page information. Following the method described by Wang (Wang et al., 2023), we convert XML to HTML, as the training data for LLMs is predominantly sourced from the Internet, which includes numerous HTML files. Therefore, we believe that LLM has a better understanding of HTML than XML.\nGiven the tree structure of XML, we initially convert the XML into a tree format and subsequently transform the nodes that need to be displayed to the agent into HTML. The agent simulates human interaction with smartphones, performing three major operations: click, input, and scroll. Humans visually identify which elements can be clicked or receive input, and use their fingers to determine if they can scroll the screen. Therefore, we provide the agent with elements that are visible and scrollable. Due to the limit on context length, we only convert the information required by the agent in XML to HTML:\n\u2022 Type: HTML element categories inherited directly from XML formatted information.\n\u2022 ID: \"ID\" inherits from the XML \"resource-id\" attribute, uniquely identifying the existence of an element.\n\u2022 Package: the package name of the current application.\n\u2022 Class: the class of the element, such as ImageView, TextView.\n\u2022 Description & text: describe the function and shape of the element.\n\u2022 Clickable & Scrollable: whether the element is clickable and scrollable.\n\u2022 Bounds: if the element is scrollable, this attribute will be present and scope the scroll component, such as:\n$[X_i, Y_i] [X_j, Y_j]$\nThe scrollable rectangle ranges from $[x_i, Y_i]$ to $[x_j, Y_j]$."}, {"title": "3.3 Evaluation Method", "content": "CheckPoint. Automated test CheckPoint coverage (Bajunaid and Menasc\u00e9, 2018) is a test metric for the software execution process. It cannot assist in checking the software results, but it can visually inspect whether the software runs in the specified unit sequence. During data construction, we supply APPs and APIs, which naturally serve as detection indicators. Additionally, we incorporated a CheckPoint to verify if the UI operation correctly clicks on the intended element. After sorting out the above CheckPoints, we constructed the following three CheckPoints:\n\u2022 Package: the unique package name corresponding to the application. Checking the package can determine whether the correct application is used.\n\u2022 Key phrase: the key phrase extracted from the query, represents key steps in the UI execution process.\n\u2022 API: API commands that need to be called during the execution process.\nTo evaluate the agent's selection and execution capabilities, we divide the inspection granularity into two levels: CheckPoint11 - whether it uses the correct application, and CheckPoint12 - whether it follows the predefined paths to complete the task. For CheckPoint11, we check the number of correctly called packages. For CheckPoint12, we check the number of correctly called package, key phrase, API. For CheckPoints, we identify three logical relationships: sequential, conjunctive, and disjunctive checks. These correspond to the instability of LLM output and its tendency for synonym substitution. The calculation formula for \"sequential check\" is as follows:\n$Score_{Sequen} = \\frac{|\\sum_{str \\in SC} str \\in AH|}{|\\sum_{str \\in SC} Str|}$\nSC represent Sequential Check Set and AH represent Actions History. The calculation formulas for conjunctive checks is as follows:\n$Score_{conjun} =\\begin{cases}1, \\text{ if } \\forall str \\in CC, str \\in AH\\\\0, \\text{ otherwise }\\end{cases}$\nCC represent Conjunctive Check Set. The calculation formulas for disjunctive checks is as follows:\n$Score_{disjun} =\\begin{cases}1, \\text{ if } \\exists str \\in DC, str \\in AH\\\\0, \\text{ otherwise }\\end{cases}$\nDC represent Disjunctive Check Set. The weighted sum of the above three scores will be the final CheckPoint coverage rate.\nAs shown in Figure 3, the number of key phrase CheckPoints is significantly higher than that of packages, indicating the need for more semantic information to ensure tasks are completed step-by-step. Analyzing the dataset from a proportional perspective, we find that the distributions of the three types of CheckPoints are 0.212, 0.493, 0.294, with key phrase CheckPoints remaining the most predominant method of checking.\nIn general, a test case should include at least the following contents: ID, Query, APP List, CheckPoints (Package, Key phrase, API). Figure 2 is a test case that contains the above three CheckPoints.\nPassRate. (Qin et al., 2023) We assess an agent's human-computer interaction capabilities by calculating the proportion of queries successfully completed within the specified step limits. During this process, we organized the emulator's current state. Subsequently, GPT-4 evaluates the task completion status. We computed the percentage of pass tasks, yielding a PassRate as an indicator of agent's human-computer interaction capabilities.\nAverage steps. (Zhang et al., 2023) We quantified the step size required by Mobile-Bench to complete tasks as a metric for evaluating the efficiency of the agent. In Mobile-Bench, a 'step' is defined as the completion of a UI operation or the execution of an API call."}, {"title": "4 Experiment", "content": "4.1 Baseline Model\nOur model's architecture, illustrated in Algorithm 1, begins by obtaining the smartphone's UI information in XML format through Appium and transforms it into HTML format through a heuristic algorithm. Subsequently, as illustrated in Figure 5 leveraging the HTML, task details, and APP list, LLM generates a comprehensive task plan, outlining the necessary applications and corresponding sub-tasks. As the collection of APIs is organized based on the classification of APPs, we can get the API set that may be used in plan.\nThe task plan is executed iteratively. In each iteration, the model either performs an API call or a UI operation. After each execution, the model records the success or failure of the action in its history, generates the subsequent thought, and evaluates whether the task has been completed. For the actual running process of an algorithm, please refer to the appendix C.7.\n4.2 Setup\nWe evaluate four popular LLMs on the proposed Mobile-Bench task set: GPT-3.5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023), LLaMA-13B and LLaMA-70B(Touvron et al., 2023), while ChatGPT-3.5 and GPT-4 are accessed through the online APIs of OpenAI. The experiments are conducted with a 3-shot in-context learning under sampling temperature of 0.1. Recognizing that task execution incurs costs, we preset different maximum step limits for tasks based on their difficulty levels. For the three categories of SAST, SAMT, and MAMT, we set the max step to 10, 20, and 50 respectively. Owing to the limit of budget, only GPT-3.5 utilizes an interface with a context length of 16K. GPT-4 uses a standard interface, which necessitated compression and trimming of actions history. See Appendix A for other settings.\n4.3 Results\nAs observed in Table 4, it can be observed that GPT-3.5 outperforms GPT-4 in PassRate on SAMT(64%>63%), and it requires fewer steps to complete the task(12.06<13.94). To investigate this phenomenon, we analyze the output files and find that models with poorer performance exhibit PassRate misjudgments: they prematurely terminate even when the task is not completed. This phenomenon is also present in LLaMA, which exhibits a high PassRate (44.58%) but low CheckPoint coverage (34.85%). At the same time, we delved into why the results for MAMT are so low (15.5%, 26.5%). Our analysis revealed that LLMs often exhibit greedy exploration behavior when completing tasks, meaning they struggle to determine when to exit the current application and transition to the next one. This tendency is particularly prevalent in certain generation tasks. Moreover, as the actions history increases, its ability to accurately judge task progress becomes increasingly challenging. For more detailed result, please refer to Table 7.\n4.4 Impact of API Calls\nAPI Calls can accelerate task execution, as a single call often replaces several sequential UI steps. From another perspective, the ability of the agent to select appropriate APIs and input parameters warrants further investigation. Choosing the wrong API may lead the task in an incorrect direction or require a significant number of steps to rectify.\nTherefore, in Table 5, we evaluate and analyze the impact of introducing APIs on task completion based on GPT-4.\nFrom Table 5, it can be seen that even in SAST, the PassRate has decreased by 6.57% (from 80.96 to 74.39). Furthermore, the values for CheckPoints12 exhibit a more pronounced decrease after API removal, with a drop exceeding 20% in SAMT. Simultaneously, we have observed varying increases in the average number of steps, which align with our expectations. We analyzed the results and found that the inability to accurately scroll pages, inefficient exploration of page functionality, and failure to click graphical buttons are the primary reasons for the low efficiency of UI operations.\n4.5 Impact of Plan and Thought\nSince observation-thought-action is already a standardized process in the agent direction(Qin et al., 2023), and verified by experimental results, planning and thought before action are essential. From the experimental results, we can find that without the observation-thought step, the agent is almost unable to complete the task(77->20, 76->24), which is because it cannot determine the next action category and the current task status. In more complex tasks SAMT, losing the plan has more negative consequences(77->62). But they will have almost no impact on CheckPoint\u2081\u2081(82->82 63->63), because the application selection is almost done by the API Call."}, {"title": "5 Conclusion", "content": "In this work we have proposed an agent capability testing environment that supports API and UI interaction on mobile phone. This holds significant importance for exploring how LLMs can be integrated with mobile operating systems. Additionally, it can serve as a valuable reference for developing testing platforms for operating systems to evaluate the capabilities of LLM agents. We collected and released a test dataset containing tasks for multiple APPs, ensuring its quality through human verification. Based on this data set and environment, we tested the planning, decision-making and execution of various LLM-based agents. Please refer to the Section 6 for the limitations of our benchmark."}, {"title": "6 Limitations", "content": "While general large models exhibit strong capabilities in reasoning and planning, they tend to have pronounced illusions in API calls. As a result, the language model may become confused about the application's functionality, leading to a reluctance to continue and complete the task. Therefore, fine-tuning a model for instructions is highly necessary.\nAutomatic CheckPoint is a process evaluation metric, making it challenging to assess the quality of the final outcome. This depends on whether the agent has obtained the necessary information (actions) on the required pages.\nThe enhancement of the agent's capabilities relies on extensive API and SDK libraries, requiring substantial support from application development companies."}, {"title": "7 Ethics Statement", "content": "We have rigorously refined our dataset to remove any elements that could compromise personal privacy, thereby guaranteeing the highest level of protection for individual data. The evaluation of our work was carried out through a meticulously randomized selection of IT professionals. This process ensured a gender-balanced and educationally diverse panel, reflecting a wide spectrum of perspectives and expertise."}, {"title": "8 Acknowledgements", "content": "We thank the Xiaoai Voice Department of Xiaomi Technology Corporation for their raw data support for this project. We additionally thank our crowd annotators for their diligent work, Junfeng Peng and Yifan Cheng for contributing to the human performance estimates, and the anonymous reviewers for their constructive comments. This work was supported by the NSFC (U2001212, 62032001, and 61932004)."}, {"title": "A Settings", "content": "We conduct experiments on the Android 14.0 version emulator and use Appium UiAutomator2 Driver for automated testing. Before each execution of a task, we load a snapshot to ensure the emulator in the same environment every time. For all applications, we have logged in to the account in advance to ensure that the full function of the application can be used. Since we tests in the real world, we filtered out any tasks that included payments."}, {"title": "B Details of Dataset", "content": "B.1 Dataset quality analysis\nThe root cause of low-quality data often lies in the inaccuracies in the descriptions of applications. Additionally, ambiguity in query generation also plays a significant role. For example, in the query \"Help me find pictures related to Beijing\u201d, although the user has not explicitly specified the source application, for a human, the expected result would likely be a search engine or a map application, as the images are not likely to be from the user themselves. However, for LLM, because the statement includes the word \"pictures\", it might be reasonable for it to spend all its time searching for pictures in the gallery application, even though this effort would ultimately be in vain. CheckPoint coverage is calculated as the weighted sum of the scores for the three types of CheckPoints mentioned above.\nB.2 Prompts for Instruction Generation\nBelow we list the detailed prompt for instruction generation, including single-APP-multi-task description, multi-APP-multi-task description.\nsingle-APP-multi-task description:\nYou will be provided with an application with descriptions, an available API list including adb command, function description and parameter information. You should create 5 varied, innovative, and detailed multi task queries that employ this application as a tool, API can be used as an auxiliary.\nEach query should include the necessary parameters. Note that you shouldn't ask \u2018which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APP and APIs have to strictly come from the provided lists.\nAt the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.\nKey phrase can contain multiple pieces of information, \"|\" means the query passes when any of the following texts are completed. \"|\" is used to separate synonymous expressions of the same noun; \"&\" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in \"[]\", and the count increases by one for each passed element. The \"ADB Command\" to be used is stored in the API, which may also be empty."}, {"title": "multi-APP-multi-task description", "content": "You will be provided with some APPs with descriptions, available API list including adb command, function description and parameter information. You should create 3 varied, innovative, and detailed multi queries that employ multi-APP as a tool, API can be used as an auxiliary.\nEach query should include the necessary parameters. Note that you shouldn't ask \u2018which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APPs and APIs have to strictly come from the provided lists. You should first think about possible related APP combinations, then give your query. Keep in mind that each query should call upon two to four APPs.\nAt the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.\nKey phrase can contain multiple pieces of information, \"|\" means the query passes when any of the following texts are completed. \"|\" is used to separate synonymous expressions of the same noun; \"&\" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in \"[]\", and the count increases by one for each passed element. The \"ADB Command\" to be used is stored in the API, which may also be empty."}, {"title": "B.3 APP&API statistics", "content": "As can be seen from Figure 6, each functional area contains at least one application and its corresponding API. These applications are sufficient to meet the daily needs of users. In other words, our simulation environment is almost consistent with the real daily use environment, and it is consistent with the real daily use environment. Open world information exchange. There are so many practical tools that are the basic functions of mobile . They have been automatically installed and completed during system installation, and standard API interfaces for tools are easier to obtain. Our next step is to increase the number of APIs and SDKs for third-party applications.\nB.4 Case study\nCheckPoints is a group of words, including packages, key phases, and API, which represent the package name, action keywords, and API instructions of the application respectively. We regularize these words and action histories to check whether they select a sufficient and correct number of applications, UI elements, and APIs to accomplish the given task.\nNext, we will give an example of CheckPoints in Figure 7 and Figure 8."}, {"title": "C Details for Baseline Model", "content": "C.1 Examples for HTML\nFigure 11 shows the correspondence between the components in the UI page and the corresponding HTML code. It is easy to find that most components have text descriptions, but the switch of the alarm clock does not have a corresponding text description, and LLM will hardly think of it. To click this button, therefore, component function exploration is what we need to do next.\nC.2 Prompts for application Selection and Planning\nYou are a large language model agent stored on a mobile phone, below I will provide you with a task, the environment of the current mobile phone interface(Apps information).\nPlease help me choose the correct APP to perform the task based on the Apps information. If the APP you want is not available on the current page, you can go to play store and download a suitable APP. On this basis, you should make a simple plan for completing the task.\nLet's Begin!\nC.3 Prompts for API Selection\nYou are the greatest large language model agent stored on a mobile phone. You will be provided with a API list that can be called by mobile phone, the task you need to complete, the thought about what have done and what need to do now.\nYou are just the first step to interact with the phone, and your follow-up is UI interaction components. If you find that there is no suitable API and the next step is UI interaction, please answer directly sorry. You should not use the API to complete the work that has been completed by the UI interactive components in the previous steps.\nYour decision should consider the following factors:"}, {"title": "C.4 Prompts for UI Selection", "content": "You are a large language model agent stored on a mobile phone, You need to give the current one-step action that needs to be taken to complete the task. Below I will provide you with a task, a plan, the environment of the current mobile phone interface(UI information), action history, though about the current status of task completion.\nYou need to select the most suitable one element and give the corresponding one action based on the UI information and thought. You need to first judge based on the UI information and action history whether the planned action has been completed. Your selection should also consider action history, and have the courage to try new buttons instead of the same buttons from history.\nAction can only be the following three functions:\n1. click(element)\nClick a element, only when clickable=\"true\", the element can be clicked.\n2. input(element, text)"}]}]}