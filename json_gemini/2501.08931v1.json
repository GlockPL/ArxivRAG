{"title": "Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos", "authors": ["Javier Rodriguez-Juan", "David Ortiz-Perez", "Manuel Benavent-Lledo", "David Mulero-Perez", "Pablo Ruiz-Ponce", "Adrian Orihuela-Torres", "Jose Garcia-Rodriguez", "Esther Sebasti\u00e1n-Gonz\u00e1lez"], "abstract": "The current biodiversity loss crisis makes animal monitoring a relevant field of study. In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity. Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format. In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification. This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition. The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes. In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification.", "sections": [{"title": "Background & Summary", "content": "Under the current scenario of global biodiversity loss, there is an urgent need for more precise and informed environmental management\u00b9. In this sense, data derived from animal monitoring plays a crucial role in informing environmental managers for species conservation2,3. Animal surveys provide important data on population sizes, distribution and trends over time, which are essential to assess the state of ecosystems and identify species at risk4,5. By using systematic monitoring data on animal and bird populations, scientists can detect early warning signs of environmental changes, such as habitat loss, climate change impacts, and pollution effects6\u20138. This information helps environmental managers develop targeted conservation strategies, prioritize resource allocation, and implement timely interventions to protect vulnerable species and their habitats2,9,10. Furthermore, bird surveys often serve as indicators of local ecological conditions, given birds' sensitivity to environmental changes, making them invaluable in the broader context of biodiversity conservation and ecosystem management\u00b9\u00b9. However, monitoring birds, as any other animal, is highly resource-consuming. Thus, automated monitoring systems that are able to reduce the investment required for accurate population data are much needed.\n\nThe first step to create algorithms that detect species automatically is to create datasets with information on the species traits to train those algorithms. For example, a common way to classify species is by their vocalizations12. For this reason, organizations such as the Xeno-Canto Foundation \u00b9 compiled a large-scale online database13 of bird sounds from more than 200,000 voice recordings and 10,000 species worldwide. This dataset was crowsourced and today it is still growing. The huge amount of data provided by this dataset has facilitated the organization of challenges to create bird-detection algorithms using acoustic data in understudied areas, such as those led by Cornell Lab\u00b2. This is the case of BirdCLEF202314, or BirdCLEF202415, which used acoustic recordings of eastern African and Indian birds, respectively. While these datasets contain many short recordings from a wide variety of different birds, other authors have released datasets composed of fewer but longer recordings, which imitate a real wildlife scenario. Examples of this are NIPS4BPlus16, which contains 687 recordings summing a total of 30 hours of recordings or BirdVox-full-night17, which has 6 recordings of 10 hours each.\n\nAlthough audio is a common way to classify bird species and the field of bioacoustics has increased tremendously in the latest years, another possible approach to identify species automatically is using images18. One of such bird image datasets is"}, {"title": "Methods", "content": "Data acquisition\nThe acquisition of the data was conducted within Alicante wetlands, specifically within the wetlands of La Mata Natural Park and El Hondo Natural Park (southeastern Spain). In these places, we deployed a collection of high-resolution cameras and camera traps in different areas of the wetlands. These areas were determined by the species expected to be recorded, as different species can be commonly seen in different wetland areas.\n\nCamera traps are activated when movement is detected and thus can record for long periods of time without human intervention. The usage of automatic camera traps28\u201330 is common in the monitoring of wildlife as it provides a low-cost approach to collect video and image data from the environment. However, the focus of this camera is fix and thus the videos of the same individual are often short. Manual cameras require the presence of a human while recording and are thus more time-consuming. Also, the presence of the cameraman may affect the animal behavior. However, it permits manual changes of cameras' perspectives in order to correctly record the bird behavior. As different cameras were used videos of different resolutions were obtained: 87 videos at 1920x1080px, 75 videos at 1296x720px, 14 videos at 1280x720px, 1 video at 960x540px and 1 video at 3840x2160px.\n\nThe species selected were the most common found in the wetlands of Alicante, facilitating the recording of videos and providing valuable data to the natural parks where videos were recorded. In terms of behaviors, we identified the most representative ones of the selected species, in order to cover as much as possible the range of activities developed by the birds.\n\nData annotation\nAccurate annotation of the captured data is a determining factor in obtaining relevant results when training deep learning models on this data. To ensure annotation accuracy, the use of annotation tools31,32 has been extended, as they provide a user-friendly interface that makes this process easy and accessible to non-technical staff.\n\nThere are many open-source annotation tools available on the market. CVAT is one of the most popular ones, as it provides annotation support for images and videos, including a variety of formats for exporting the data. VoTT 6 is also popular when annotating videos, as it offers multiple annotation shapes and integration with Microsoft services to easily upload data to Azure.7 Other simpler annotation tools are labelme or LabelImg, which are aimed at annotating images and their capabilities are more limited. For our purpose, we decided to use CVAT because of the large number of exportable formats available, the great collaborative environment it offers, and its easy integration with semi-automatic and automatic annotation processes.\n\nAs the need for larger amounts of data to train deep learning models increased, researchers began to enhance annotation tools with automatic systems that could alleviate this task. Annotation tools integrate machine learning models33 that can automatically infer what would otherwise be manually annotated. Common tasks performed by automated annotation tools are object detection34 and semantic segmentation.35 While the former predicts the bounding box and class of each object in the image, the latter predicts regions of interest associated with specific categories.\n\nAlthough automated annotation systems have demonstrated strong performance, semi-automated annotation processes are ultimately used because they ensure the creation of highly accurate annotations while greatly reducing the amount of human intervention required. Semi-automated annotation studies are widely used in the medical field,36,37 where precision is a key factor throughout the design.\n\nIn this study, a semi-automated annotation approach was followed, based on CVAT and its possible integration with powerful computer vision models. Our approach consisted of five main steps: Species classification, bird localization, behavior classification, subject identification, data curation, and post-processing. Each of these stages is described in more detail below. Figure 4 shows this process."}, {"title": "1. Species classification:", "content": "In this first step, the ecologists labeled each video with the main bird species that appeared. The main species is that of the bird in the focus of the camera. This way, annotations of birds that are different from the main species will not be included in the video annotations."}, {"title": "2. Bird localization:", "content": "Then, an object detection model is used to predict the localization of the bounding boxes of the birds that appear in each of the video frames. For ease of implementation, YOLOv738 was chosen as the object detection model because it is predefined integration into CVAT. Since the model provided by CVAT is trained on general purpose data, the class predicted by default for each bounding box is not be the bird species, but the class bird. To avoid manually changing all the bounding box classes, we used an option provided by CVAT to associate a user-defined class with the class detected by the model. In this way, the class bird was associated with the species appearing in the video. Since each video is restricted to having only one species of bird, it is possible to associate bird with a specific species."}, {"title": "3. Behavior classification:", "content": "After automatically annotating the bounding boxes, our team of ecologists performed the second step of the annotation process, which is twofold. First, they checked and corrected erroneous bounding boxes, and second, they annotated for each bounding box the behavior performed by each bird. To annotate the behaviors, CVAT bounding boxes tags were used."}, {"title": "4. Subject identification:", "content": "When using automatic annotation models such as YOLOv7, CVAT does not support bounding box correspondence between frames. In other words, if a video shows two birds developing different behaviors, there is no relationship between the bounding boxes of adjacent frames, so it is not possible to analyze the birds' behaviors. This is not possible because the next frame will show two new bounding boxes whose relation to the one being analyzed is not known. To solve this problem, the Euclidean distance39 was used to correlate bounding boxes of adjacent frames. The euclidean distance calculates the distance between the centers of the bounding boxes of adjacent frames and then correlates the bounding boxes with the minimum distance. The center of the bounding box was calculated as follows:\n\n$c=\\left(\\frac{X_{min} + X_{max}}{2},\\frac{ymin + ymax}{2}\\right)$         (1)\n\nGiven the centre of the bounding boxes, the Euclidean distance was calculated as:\n\n$d(C_{1},C_{2}) = \\sqrt{(x_{2}-x_{1})^{2} + (y_{2} - y_{1})^{2}}$        (2)\n\nwhere c\u2081 = (x1,y1)\nand c2 = (x2,y2)"}, {"title": "5. Data curation:", "content": "After the labeling of species, bounding boxes, behaviors, and subjects, an overall review of all annotations was conducted to ensure the high quality of the data. To conduct the review, videos were assigned to all ecologists equally."}, {"title": "6. Post-processing:", "content": "Once the annotations were complete, their format was adapted to make them easy to use and understand. To achieve this goal, the approach used in the AVA-Kinetics dataset40 was followed. In this approach, a CSV file was used to contain annotations containing localized behaviors of multiple subjects. To export the data into the CSV format, the data was first exported from CVAT using the CVAT Video 1.1 format. Some Python scrips were then used to extract only the relevant information from the exported data and dump it into the output CSV file."}, {"title": "Data Records", "content": "The dataset presented in this study is open access and accessible through Zenodo10. Within this Zenodo repository, there are four main elements:\n\n\u2022 Videos folder: This folder contains the 178 videos that comprise the dataset. Videos are identified by their name, which is composed of a numeric value and the species that appears in the video. The format is the following \"ID-VIDEO.SPECIES-NAME.mp4\"."}, {"title": "Technical Validation", "content": "To ensure high quality recordings and accurate annotations, the entire process was carried out by expert ecologists, as mentioned in the Data Annotation section. Firstly, video recordings were supervised by a group of experts who set up camera traps in strategic areas and also manually recorded some high-quality videos. For each video, these experts annotated the species appearing in the video. The same experts then corrected bounding box errors and annotated bird behavior, together with a number of collaborators with a background in ecology. Finally, a final stage of cross-checking of annotations was carried out by the experts and collaborators. The expertise of the annotators responsible for collecting and annotating the videos, together with the final cross-review process, ensures the quality and cleanliness of the data.\n\nAs this dataset has been conceived mainly to be used in deep learning pipelines, baseline pipelines will be given to demonstrate the applicability of the data presented in this work within deep learning workflows. As mentioned previously, the purpose of this dataset is twofold, as it provides annotation data for performing bird species detection and behavior classification tasks. Thus, one baseline per each task was developed. PyTorch\u00b9\u00b9 was used in both cases as coding platform.\n\nSpecies classification\nFirst, a baseline pipeline for species classification was developed. This baseline is based on a YOLOv941 model trained over 50 epochs in the proposed dataset.\n\nFor efficient training, a downsampling of 10 is performed on the frames extracted from the videos. This can be done without affecting the performance of the model, as the difference between successive frames is minimal. The frames were extracted while maintaining the source FPS (Frames Per Second) of each video. During the training stage, a learning rate of 0.01 was used and a GeForce RTX 3090 GPU was used as the hardware platform. The test results from the baseline are shown next:"}, {"title": "Behavior detection", "content": "Secondly, the behavior detection baseline is presented. In this baseline, four different video classification models were trained end-to-end to perform the behavior classification task. The trained models were Video MViT,42 Video S3D,43 Video SwinTransformer44 and Video ResNet.45 All model architectures and pretrained weights were extracted from PyTorch.\n\nFor the training stage, the input videos were downsampled with a downsample rate of 3, selecting the first frame as the representative of each set (i.e. only the first frame of each set of 3 is kept). Train, test and validation splits were generated from the full set of videos with a distribution 70-15-15. The splits were constructed using a stratified strategy based on the species and behaviors appearing in the videos. The distribution was computed taking into account the number of frames which constitute each video (e.g. 1 video with 1000 frames is equivalent to 5 videos with 200 frames). Regarding the training hyperparameters, a learning rate tuning was conducted using a uniform sampling strategy with minimum and maximum values of 0.0001 and 0.01, respectively. Similarly to the species classification baseline, training was performed on a GeForce RTX"}, {"title": "Usage Notes", "content": "Since the data annotations are provided in CSV format, it is recommended to use Python libraries such as Pandas,12 which is specifically designed to read and manage CSV data. In the GitHub repository containing the code, there are usage examples of how to load and prepare the data to be fed into deep learning models. It is recommended to read the dataset.py script in the behavior_detection directory as an example."}, {"title": "Code availability", "content": "The data processing and experimentation code shown in the Technical Validation section is available on GitHub13. The GitHub repository is organized into two main directories. The species_classification directory contains all the code related to the species classification, and the behavior_detection directory contains the experiment with the behavior detection models proposed for the dataset."}, {"title": "Competing interests", "content": "The authors declare no competing interests."}]}