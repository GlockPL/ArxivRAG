{"title": "ABSTRACTIVE SUMMARIZATION OF LOW RESOURCED NEPALI LANGUAGE USING MULTILINGUAL TRANSFORMERS", "authors": ["Prakash Dhakal", "Daya Sagar Baral"], "abstract": "Automatic text summarization in Nepali language is an unexplored area in natural language processing (NLP). Although considerable research has been dedicated to extractive summarization, the area of abstractive summarization, especially for low-resource languages such as Nepali, remains largely unexplored. This study explores the use of multilingual transformer models, specifically mBART and mT5, for generating headlines for Nepali news articles through abstractive summarization. The research addresses key challenges associated with summarizing texts in Nepali by first creating a summarization dataset through web scraping from various Nepali news portals. These multilingual models were then fine-tuned using different strategies. The performance of the fine-tuned models were then assessed using ROUGE scores and human evaluation to ensure the generated summaries were coherent and conveyed the original meaning. During the human evaluation, the participants were asked to select the best summary among those generated by the models, based on criteria such as relevance, fluency, conciseness, informativeness, factual accuracy, and coverage. During the evaluation with ROUGE scores, the 4-bit quantized mBART with LoRA model was found to be effective in generating better Nepali news headlines in comparison to other models and also it was selected 34.05% of the time during the human evaluation, outperforming all other fine-tuned models created for Nepali News headline generation.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of digital content, such as news articles, blogs, and social media, has made automatic text summarization a critical task in Natural Language Processing (NLP). This involves generating concise summaries that capture the main ideas of the original text while maintaining its meaning. Summarization is generally performed in two ways: extractive summarization and abstractive summarization. Abstractive summarization generates new sentences to convey the original text's meaning, requiring sophisticated language generation, while extractive summarization involves the extraction of key sentences or phrases from the original text.\nText summarization, particularly in generating news headlines, is vital for quickly conveying information and enabling further analysis like sentiment analysis and document classification. Efficient summarization is essential for managing the vast amount of digital text available today. While English text summarization has advanced significantly, there is a need for similar progress in low-resourced languages like Nepali. The research work aimed to:\n\u2022 Assess the effectiveness of the mBART and mT5 model in performing abstractive summarization for Nepali language.\n\u2022 Identify the key challenges and limitations in implementing mBART and mT5 models for abstractive summa-rization in low-resourced languages like Nepali.\n\u2022 Determine the necessary modifications or enhancements to improve the performance of multilingual trans-former models like mBART and mT5 for summarizing Nepali language texts."}, {"title": "2 Related Work", "content": "With the rise of transformer-based models [3], various research works have been carried out using them for text summarization. Many studies focus on English, while research on the Nepali language is limited and primarily based on extractive summarization approaches.\n[4] introduced extractive summarization to produce summaries from multiple Nepali sentences by selecting a subset from the original text using TextRank [5]. These summaries contained the most important sentences of the input. They utilized TextRank for sentence scoring and topic modeling for summary evaluation.\n[6] generated Nepali news headlines using GRU [7] in an encoder-decoder fashion, taking the news content as input and generating a headline as output. The news was converted into word tokens and vectorized using FastText [8], trained on a corpus of Nepali news articles and headlines collected from several web portals.\n[9] employed an extractive method for Nepali text summarization using TextRanking [5] and LSTM [10]. They trained a Nepali news corpus with GloVe embeddings using different window sizes (10, 12, 15) and vector sizes (100, 200, 300). For extractive text summarization, they used Text Ranking and an attention-based LSTM model [11].\n[12] introduced an attention-based RNN for abstractive Nepali text summarization. They first created a Nepali text dataset by scraping Nepali news from online portals, then designed a deep learning-based summarization model using an encoder-decoder recurrent neural network with attention. Specifically, Long Short-Term Memory (LSTM) [10] cells were used in both the encoder and decoder layers. They built nine models by varying hyperparameters and reported Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores [13] to evaluate performance."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Data Collection", "content": "A comprehensive dataset of Nepali news articles, was created with web scraping from various online news portals like BBC Nepali[14], Kantipur and Gorkhapatra. For web scraping, libraries like BeautifulSoup and Selenium were used. Data in each news portals were in different format and different strategies had to be adopted to extract data from them. Running a single script to collect the data would have taken forever to get the complete dataset, so in order to expedite the data collection process, various parallel processing techniques were adapted. A sample of dataset obtained from this process has been presented in Figure 1."}, {"title": "3.2 Data Preprocessing", "content": "In this step, we have removed HTML tags, special characters, and irrelevant sections of the text (such as advertisements and navigation links). As the data was collected in two steps, the headlines and their corresponding article bodies had to be joined to create the complete dataset.\nThe collected dataset still had numerous characters that were not part of the Nepali Devanagari Character Set. These extraneous characters would have degraded the overall text quality and negatively impacted model performance. Specifically, the unwanted characters include Latin letters (a-z, A-Z), Arabic numerals (0-9), etc. To mitigate these issues, these characters have been removed from the dataset. A prefix was added to the input text to indicate the summarization task to the model and it helped the model to better understand the context and the task it needs to perform.\nThe input texts (articles) and the target texts (headlines) were then, tokenized to a maximum length of 1024 and 20 tokens respectively, ensuring that longer texts were truncated. The tokenized headlines from the previous step were then, set as labels in the model inputs. This helped the model to learn the mapping from the input text to the target"}, {"title": "3.3 Exploratory Data Analysis", "content": "The dataset, meticulously compiled from various news portals, encapsulated a total of 70,769 articles, categorized into ten distinct thematic areas: News, Sports, Opinion, Entertainment, Feature, Diaspora, World, Education, Blog and Others(Mix). The dataset had more data related to News category, while blog category had the least amount of data. The dataset were, then splitted into training, validation, and test sets in an 70-20-10 ratio to ensure robust model evaluation."}, {"title": "3.4 Model Selection and Fine-Tuning", "content": null}, {"title": "3.4.1 Model Selection", "content": "For summarization task in other languages, the multilingual transformer-based models, specifically mBART[15] and mT5[2] have shown impressive results. These models have already been pretrained on vast amounts of multilingual text, which gives them a strong foundation for understanding complex linguistic structures in Nepali. This helps in generating accurate and coherent summaries without needing massive language-specific datasets. Both models offer flexibility in fine-tuning, allowing us to adapt them specifically to the nuances of Nepali text summarization. This ability helps improve performance in low-resource settings where language-specific models are not readily available. There were different variants of mBART and mT5 available for the summarization task, but the mBART-large-50 and mT5-base models in particular had only 600M and 598M trainable parameters respectively making it suitable for our use case. If we had decided to go with other variants of mBART and mT5, it would have been computationally expensive and time consuming and our existing free resources would have been incapable of doing this task. Due to this reason, we had to go with these two variants which were computationally less expensive in comparison to the other variants. Training all of the parameters of these models were still computationally expensive and time consuming, so in order to reduce the number of trainable parameters in the model, LoRA[16] was used along with quantization technique as suggested in QLoRA[17]."}, {"title": "3.4.2 Fine-Tuning", "content": "To enhance efficiency, we stored the dataset on Hugging Face. During the fine-tuning process, the model weights and configurations obtained after each training session were pushed to Hugging Face for every model. Given the substantial computational demands of fine-tuning our language models, we found Kaggle to be the most suitable platform. It offered free access to the NVIDIA TESLA P100 GPU, allowing us to conduct uninterrupted training sessions for up to 12 hours.\nThe following training arguments were set in the trainer and in the LoRA for the training in each models:"}, {"title": "3.5 Evaluation", "content": "The evaluation strategy was set to run at the end of each epoch, allowing for periodic assessment of the model's performance during training. A custom function to compute evaluation metrics was provided to the trainer. This function calculated ROUGE scores to evaluate the quality of the generated headlines. The model's performance was finally assessed on the testing set using the custom evaluation function and helped in understanding the model's ability to generate accurate and coherent headlines from Nepali news articles.\nTo assess the models' performance, a survey was conducted with 62 participants fluent in Nepali. They were asked to evaluate summaries of 10 different sentences from various categories, each one having a summary generated from six different models ensuring fairness in the evaluation and were also asked to do it based on relevance, fluency, conciseness, informativeness, factual accuracy, and coverage of the summary."}, {"title": "4 Experimental Setup", "content": "For the execution of this experiment, the following setup was created:"}, {"title": "4.1 Environment Configuration:", "content": null}, {"title": "4.1.1 Hardware Setup:", "content": "Given the substantial computational demands of fine-tuning our language models, we found Kaggle to be the most suitable platform. It offered free access to the NVIDIA TESLA P100 GPU (16GB), allowing us to conduct uninterrupted training sessions for up to 12 hours. For storing the data, the model weights and the configurations obtained after each training session, Hugging Face was used."}, {"title": "4.1.2 Software Environment:", "content": "The experiments were ran using Python 3.12.3 along with key libraries such as PyTorch, BeautifulSoup, Selenium, Pandas, Numpy, Matplotlib, Plotly etc."}, {"title": "4.2 Experimental Workflow:", "content": null}, {"title": "4.2.1 Dataset Handling:", "content": "The dataset was processed in batches during training, with each batch containing 10k approx. samples. A total of 50k and 14k news articles and their corresponding summaries were used in this experiment for training and validation respectively. The dataset was fully loaded into the memory for each model during the start of training."}, {"title": "4.2.2 Batch Processing:", "content": "Batch processing was implemented to streamline training and evaluation. Training was performed with a batch size of 5 and ran for 3 epochs and validation was carried out at regular intervals to track performance improvements."}, {"title": "4.2.3 Training Time:", "content": "The total training time per model was approximately 12 hours."}, {"title": "4.2.4 Hyperparameter Settings:", "content": "The key hyperparameters used were: learning rate = 5e-4, weight decay = 0.01. These parameters were optimized to balance model convergence and training stability."}, {"title": "4.3 Evaluation Setup:", "content": null}, {"title": "4.3.1 Automated Evaluation:", "content": "Evaluation metrics, such as ROUGE, were computed using the Rouge library. The results were automatically logged and stored for further analysis. The evaluation was carried out on the validation dataset after each epoch and final evaluation was done on the test dataset at the end."}, {"title": "4.3.2 Human Evaluation:", "content": "During human evaluation, human evaluators were asked to select the best summary among different summaries generated from different models for different sentences based on factors such as relevance, fluency, and informativeness. A simple Google form was created and used to streamline the collection of feedback, ensuring that responses were gathered efficiently."}, {"title": "5 Results", "content": "The overall ROUGE scores in terms of precision, recall, and F1-scores for all the models are presented in Table 5. These scores provide a comprehensive summary of the experiments. Based on the results in the Table 5, 4-bit quantized mBART with LoRA has outperformed all other models in terms of ROUGE scores.\nThe results obtained from the human evaluation for model comparison are also presented in Table 6. From this table, we observed that the summarization generated from 4-bit quantized mBART with LoRA were selected maximum number of times supporting the results from the automatic evaluation and also indicating that the output generated from this model were more relevant, fluent, concise, informative, factually accurate and had better coverage in comparison to other models.\nHowever, models like the 4-bit and 8-bit quantized mT5 failed to generate sentences properly. There are several factors that could explain this poor performance. Firstly, the quantization process might have adversely affected the mT5 model more than the mBART model, leading to a loss in precision and overall capability to generate coherent summaries.\nSecondly, the mT5 model may require more extensive fine-tuning to adapt to the specifics of the Nepali language, which was not sufficiently covered in this experiment. Additionally, the architecture of mT5 might inherently be less robust to quantization techniques compared to mBART, which could explain the significant drop in performance. These"}, {"title": "6 Conclusion", "content": "This research focused on enhancing the generation of Nepali news headlines using advanced summarization models. In this study, a diverse dataset of Nepali news articles was successfully collected and preprocessed to ensure its readiness for model training. By leveraging state-of-the-art multilingual models, such as mBART and mT5, and incorporating techniques like LoRA and quantization, these models were trained and compared with automatic evaluation technique like ROUGE and finally with human evaluation to conclude the comparison.\nThe evaluation results revealed that the 4-bit quantized mBART model with LoRA achieved the highest performance in generating accurate and coherent headlines. This model outperformed the other variations, including the quantized mT5 models, which struggled with performance issues likely due to the effects of quantization and the model's adaptation to the Nepali language.\nOverall, this study demonstrates the effectiveness of mBART with LoRA in the context of Nepali news headline generation and highlights the potential areas for improvement in models like mT5. The findings underscore the importance of optimizing models and techniques to achieve high-quality summarization in diverse linguistic contexts.\nTo further enhance the effectiveness of summarization models, several recommendations can be considered. First, improving the performance of the mT5 model is essential. This involves exploring ways to handle quantization more effectively, possibly by experimenting with different strategies or refining the model's fine-tuning process to better adapt to the Nepali language.\nExtending the training and fine-tuning phases could be one of the ways to improve the performance of model. This approach would involve using additional data and applying diverse summarization tasks to strengthen the model's adaptability and overall performance. Additionally, exploring alternative multilingual transformer models or newer architectures could offer valuable insights and alternative methods for summarization, potentially revealing more effective solutions than those currently utilized.\nRefining quantization techniques is another crucial area of focus. A deeper investigation into how different quantization methods impact model performance can lead to improved efficiency without compromising quality. This may include experimenting with varying quantization levels or employing mixed-precision approaches."}]}