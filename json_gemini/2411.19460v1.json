{"title": "Look Every Frame All at Once: Video-Ma\u00b2mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing", "authors": ["Hosu Lee*", "Junho Kim*", "Hyunjun Kim", "Yong Man Rot"], "abstract": "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma\u00b2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma\u00b2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS\u2014on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.", "sections": [{"title": "1. Introduction", "content": "As video data grows in scale and complexity, the demand for models capable of efficiently processing long video sequences has intensified. Transformer-based models [3, 41] have become central to sequence processing due to their effectiveness and versatility in handling complex dependencies as input frames increase. With the emergence of the Large Language Models (LLMs) era [14, 30, 31], video understanding models have entered a new phase. Thanks to the core capabilities of zero-shot learning and strong reasoning in LLMs, various Large Multi-modal Models (LMMs) [9, 25, 26] have achieved enhanced cross-modality consistency, particularly between vision and language. Accordingly, diverse video-LMMs [21, 23, 29] have been proposed to understand video content by integrating spatio-temporal information into LLMs, and achieved comparable reasoning performances to comprehend complex visual narratives and temporal dynamics.\nHowever, current video-LMMs face critical challenges when applied to longer video sequences integrated into the LLM structure, as the attention mechanism [42]-the core component of Transformers\u2014incurs memory load and computational costs that scale quadratically with sequence length. This growth in resource requirements becomes prohibitive for processing extended video sequences, where sequence lengths often exceed 128K tokens (approximately 400 frames), resulting in significant inefficiencies in memory consumption and computational load.\nTo address the challenge for the long video understanding,"}, {"title": "2. Related Work", "content": "ing, various strives have been evident with several different approaches: (i) sparse and uniform sampling (e.g., typically 8 or 16 frames) to reduce the number of input video frames into LMMs, which is the most common method [20, 23, 24] and (ii) memory-augmented generation [16, 39] that stores long-term visual information and later access to the memory bank. More recently, Zhang et al. [48] have proposed a method for transferring long context by employing RoPE-based [40] frequency extension in LM backbones within existing memory limits, which enables the models to process more visual tokens for the longer video sequences. Despite such efforts, the exponential growth in memory usage with increasing sequence lengths imposes fundamental limitations on the amount of information that can be fed into the model. Consequently, when the model is prompted to respond based on missing information (or frames), it often generates responses that are irrelevant and disconnected from the facts or user queries.\nHere, the core difficulty lies in the quadratic time and space complexity of the Transformer's attention mechanism, which restricts efficient processing of lengthy video sequences. To enable more practical scalability when handling longer sequences, the fundamental solution is on shifting from quadratic complexity to linear complexity, facilitating more scalable processing for the video data. To do so, in this paper, we introduce Video-Ma\u00b2mba, specifically designed to handle extremely long video sequences all at once. We substitute the Transformer-based LLMs [8, 41] to Mamba-2 [11] structure, utilizing State Space Models (SSMs) [15] as a replacement for the attention mechanism. This allows our framework to not only preserve the effectiveness of sequence processing but also enhance memory efficiency, thereby achieving linear time and space complexity with respect to the sequence length.\nIn addition to the architectural changes, to push the boundary of memory utilization within Mamba-2 architecture, we present a new Multi-axis Gradient Checkpointing (MA-GC) method. Specifically, the Gradient checkpointing (GC) [6] strategically saves selected activations throughout the computational graph, allowing only partial activations to be re-computed during the backward pass (therefore, widely used for managing the substantial memory demands of extensive attention layers in Transformer-based LLMs). On the other hands, due to the nature of Mamba structure, which belongs partially observed Markov models where the hidden state progresses over time according to a Markov process rather than dense interaction models (e.g., Transformer), we can implement another axis for GC in the sequential direction by selectively retaining only those sequence-wise activations necessary for backpropagation. The MA-GC reduces memory usage from the $O(LS)$ complexity of the original Mamba-2 to $O(S)$ by applying GC in multiple directions, which allows Video-Ma\u00b2mba to process full video sequences at 1-second intervals without the need for frame sampling. Our empirical analyses in Fig. 1 indicate that the proposed method performs effectively in handling extended sequences, successfully processing sequence lengths in the millions on a single GPU, corresponding to over 2-hours of continuous video input at a frame rate of 1 FPS. By observing each frame at regular intervals, our approach captures more comprehensive temporal information than uniform sampling, providing improvements in model responses and memory efficiency.\nThrough extensive experiments and computational analyses on MA-GC, we demonstrate that Video-Ma\u00b2mba efficiently manages resource demands, effectively breaking the quadratic memory growth for handling long sequence.\nOur contribution can be summarized into three-fold:\n\u2022 We propose Video-Ma\u00b2mba, a new multi-modal framework designed to handle extensively long video sequences without losing frame information, by replacing the Transformers with Mamba-2 architecture.\n\u2022 To significantly enhance memory utilization within our framework, we introduce the Multi-axis Gradient Check-pointing strategy. Our strategy selectively stores activations in a bi-axis direction, effectively reducing the space complexity to $O(S)$.\n\u2022 Through extensive evaluation and analyses, we corroborate that our framework can efficiently process sequence lengths in the millions, corresponding to up to 2-hours of video sequence at 1 FPS with competent performance."}, {"title": "2.1. Context Extension Methods", "content": "Training models with extended sequence lengths has become increasingly challenging due to the computational and memory demands associated with scaling sequence lengths. The standard Transformer models [3, 42] struggle with the quadratic complexity of attention mechanisms, which quickly becomes infeasible for long sequences. To address these limitations, various methods have been proposed to extend the context length either during or after pre-training.\nOne of the common approaches is to generalize knowledge learned over shorter sequences to longer sequences [10, 35]. However, this approach can lead to issues in positional extrapolation, as positional encodings trained on shorter contexts may not generalize well to longer contexts. Techniques such as Rotary Position Embedding [40] and Position Interpolation [5] mitigate this by modifying the positional embedding, making it better suited for extrapolation beyond the training range. Additionally, ALiBi [35] has applied attention biases directly, improving context extension without a strict reliance on positional encodings.\nAn alternative approach is leveraging Structured State Space Models (SSMs) [15], which inherently achieve lin-"}, {"title": "2.2. Long Video Understanding with LMMS", "content": "Long video understanding presents unique challenges due to the need to capture dependencies across extended sequences while managing high memory consumption. The core challenges in handling long videos are memory limitations and finite context lengths. Several studies [18, 50] have addressed this by sparsely sampling video frames (typically 8 or 16 from the video instance rather than using dense fps-based sampling), or by employing token compression to reduce data to a more manageable size [22, 29]. Additionally, memory-augmented approaches [16, 39] have been proposed to store relevant information beforehand and recall explicit knowledge when generating responses.\nWhile such methods are simpler to implement and effective for managing memory, they risk missing critical details in long video content. Our approach addresses these limitations by utilizing full-frame sequences at 1 FPS, ensuring comprehensive temporal representation without relying on sparse sampling and achieving memory efficiency."}, {"title": "2.3. Gradient Checkpointing Techniques", "content": "The gradient checkpointing (GC) is a well-established method for reducing memory usage in deep learning models by selectively storing intermediate activations and recomputing them as needed during the backward pass. Initially developed to manage memory constraints in training, this approach allows models to trade off additional computation for reduced memory requirements.\nChen et al. [6] have introduced fundamental checkpointing techniques applicable across deep networks and recurrent neural networks (RNNs). For deep networks, the technique involves segmenting the network along the layer axis, storing the outputs at segment boundaries, and recomputing the intermediate results within each segment as needed. This segmentation reduces memory requirements from $O(n)$ to $O(\\sqrt{n})$, where n is the number of layers. For RNNs, a similar approach is used along the time-axis, allowing memory usage to scale sublinearly with sequence length by storing checkpoints at specific time intervals.\nOur work builds on these principles by applying the GC in both the layer and sequence dimensions, enabling efficient processing of long sequences across bi-directional axes. Our approach enhances memory efficiency and is particularly well-suited for understanding long videos, where integrating both temporal and spatial contexts is crucial."}, {"title": "3. Video-Ma\u00b2mba", "content": "Overview. We first elaborate on the distinction of the Mamba-2 architecture in handling memory efficiency, then introduce a new gradient checkpointing method that can be a key factor in extending the sequence length of Mamba-2. By seamlessly implementing our new context extension strategy during the training of Video-Ma\u00b2mba, our video model can handle up to maximum 0.8M input sequence tokens and overcome current challenges in long video understanding relying on partial frame sampling."}, {"title": "3.1. Preliminary: Mamba-2 and Simplification", "content": "The Mamba model [15] initially has leveraged structured state space models (SSMs) to efficiently handle sequence modeling. Building on this foundation, Mamba-2 [11] represents a further advancement to scale up to larger state sizes with the concept of Structured State-Space Duality (SSD), which enhances sequence processing capabilities through time-varying state transitions and input-output mappings, thus more effective handling for sequence data. The general form of SSD in Mamba-2 can be formulated as:\n$h_t = A_th_{t-1}+ B_tx_t, \nY_t = C_th_t,$ (1)\nwhere $A_t \\in R^{N \\times N} B_t \\in R^{N\\times1}$, and $C_t \\in R^{1\\times N}$ are state matrices that vary over time, allowing the model to adapt dynamically to different input structures. This time variance, or selectivity, enhances Mamba-2's flexibility in comparison to linear time-invariant SSMs. By employing time-varying matrices $A_t, B_t$, and $C_t$, Mamba-2 can be seen as a selective SSM that performs sequential updates to the hidden state $h_t$ based on previous states and current inputs.\nThis selective structure is particularly advantageous in capturing sequence dynamics over longer frames, which standard Recurrent Neural Networks (RNNs) [12] with fixed parameters struggle to achieve. At the same time, Mamba-2 also shares some similarities with a certain RNN framework when non-linear activations are removed. A standard RNN with a non-linear activation function $\u03c3$ (e.g., Tanh or ReLU) updates its hidden state as follows:\n$h_t = \u03c3(Ah_{t\u22121} + Bx_t), \nY_t = \u03c3(Ch_t).$(2)\nHere, removing the activation function $\u03c3$ transforms the RNN, making its structure similar to that of SSD:\n$h_t = Ah_{t-1}+ Bx_t, \nY_t = Ch_t.$(3)\nConsequently, SSD can be regarded as a simplified version of RNN, where the time-varying parameters of SSD introduce a level of flexibility and adaptability that fixed-parameter RNNs lack. This dynamic modification allows SSD to effectively address challenges associated with static parameter models in handling complex temporal sequences."}, {"title": "3.2. Multi-Axis Gradient Checkpointing", "content": "Considering that Mamba-2 follows RNN-like structure, as illustrated in Fig. 2, when processing the SSD state $H_{t+1}$, it only requires the prior state $H_t$, not $H_{t\u22121}$, unlike Transformers that require all previous states to calculate attention weights across the entire input sequence. Here, it is important to note that this distinction enables us to introduce an additional gradient checkpointing axis, not only along the layer direction but also uniquely along the sequence direction, which attribute to the architectural properties of Mamba-2 (whereas Transformer cannot achieve).\nOur key motivation for employing bi-axis checkpointing lies in its effectiveness at managing memory demands, which enables the processing of extremely long video sequences in their entirety without needing to sample scenes partially. Here, we introduce a new GC strategy, Multi-Axis Gradient Checkpointing (MA-GC), that not only increases the feasible sequence length up to $2^{19}$ but also substantially cuts activation memory usage. While previous methods such as the VL layer grouping [6] achieved some memory reduction by applying checkpoints every VL layers, they were inadequate for very long sequences. In contrast, our MA-GC method applies checkpointing along both layer and sequence axes, reducing space complexity from $O(\\sqrt{L} \\cdot S)$ in standard GC to just $O(S)$. This significant improvement allows our model to process longer sequences more efficiently without partial frame sampling, thus supporting extended sequence lengths in understanding long video content.\nSpecifically, as shown in Fig. 2, our MA-GC strategy involves two checkpoint types in the forward pass for the given S sequence length and L stacked layers: (i) Layer-wise checkpoints, where layer activations are stored every $l$ layers, and (ii) Sequence-wise checkpoints, where states across all layers are stored every $s$ time steps. The intersecting points of these two checkpoints create grid cells that are essential for efficient backpropagation. Within each grid cell, activations are sequentially restored and gradients are propagated in an efficient manner. This grid-based structure facilitates selective reconstruction of states only when necessary, thereby optimizing memory usage during the computationally intensive backpropagation process. We provide detailed explanations of both the forward and backward processes in Algorithm 1 and Algorithm 2."}, {"title": "3.3. Analysis of Upper Bound of Memory Reduction", "content": "To understand how MA-GC reduces memory usage, we analyze its space complexity and establish an upper bound on memory savings. In a naive RNN (or similarly, in Mamba or Mamba-2) architecture with L layers and a sequence length S, backpropagation requires storing activation memory of $(LS)$ due to the need to store activations for each layer and each time step during the backward pass."}, {"title": "3.4. Model Architecture", "content": "Now, we move on to training Video-Ma\u00b2mba with long video data using the proposed MA-GC strategy. Analogous to the widely adopted LMM architecture [25, 26],"}, {"title": "3.5. Training Stages", "content": "Our training pipeline consists of three stages as summarized in Fig. 3. Beyond the conventional two step training steps for LMMs: alignment training + supervised fine-tuning with instruction data, Li et al. [27] have highlighted the importance of high-quality knowledge acquisition between these two stages (thus, stage 1.5). By exploiting the extended context length with the MA-GC during training, we reemphasize that our primary goal in training Video-Ma\u00b2mba is to enhance the model's ability to process and learn from long-form video data effectively. Here, expanding the stage 1.5 learning [27], we train Video-Ma\u00b2mba using an interleaved learning approach with long video data [19], which comprises densely captioned video-text pairs covering entire long-sequence videos, each segment described in detail. By doing so, Video-Ma\u00b2mba can preserve the narrative flow across video segments, enhancing its temporal understanding by learning the sequential relationships within the video content. Through the refined training approach, we aim to develop a robust capability in Video-Ma\u00b2mba to handle complex, long-form video data in a contextually aware manner."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Training Details. To train our models, we used 1 node of 8 NVIDIA A100 GPUs, each with 80GB of memory. A cosine learning rate schedule was employed, with a learning rate of 1 \u00d7 10-3 for Stage 1 and 4 \u00d7 10\u22125 for both Stage 1.5 and Stage 2. We trained our models for one epoch at each step, with the entire training process taking approximately 4 days to complete for the 3.1B size model. All stages utilized BF16 precision, and we did not employ any form of parameter-efficient fine-tuning such as LoRA [17]. To reduce the memory overhead from optimizer states, we utilized ZeRO-1 optimization [37], enabling more efficient memory management during training. Additional hyper-parameter details are provided in Appendix B.\nImplementation Details. Our model, Video-Ma\u00b2mba, uses CLIP-ViT-L-336px (\u2248 0.4B params) vision encoder paired with backbone LMs (370M / 1.3B / 2.7B), resulting in total model sizes of approximately 0.7B, 1.8B, and 3.1B, respectively. For the largest model configuration (3.1B), the backbone Mamba2-2.7B configuration specifically uses an"}, {"title": "4.2. Experimental Results", "content": "Results on Long Video Analysis. We report the long video comprehension results using Video-MME [13] and LongVideoBench [43] in Tab. 1. Despite its smaller scale, Video-Ma\u00b2mba outperforms most 7B models in both benchmarks. This is due to our method of accessing the entire video at 1-second intervals, contrasting with the sparse frame sampling strategy. Our comprehensive approach facilitates frequent observations, highlighting our framework's effectiveness against larger 7B models. By selectively preserving key information, Video-Ma\u00b2mba avoids the typical information loss and computational burdens, delivering precise, context-aware responses for long videos.\nResults on General Video Analysis. In Tab. 2, we summarize general video analysis in several benchmarks: ActivityNetQA [46], VideoChatGPT [29], and MVBench [21]. These benchmarks are much shorter than the previously used long video benchmarks, but provide a comprehensive foundation for assessing our model's capability to an-"}, {"title": "5. Discussion and Conclusion", "content": "Discussion. Even if we have achieved promising results in our experiments, there are several discussion points and limitations. The recently launched Mamba-2, unlike established Transformer-based LLMs, is still immature with a smaller model size and insufficient QA capabilities from limited language instruction training, potentially capping performance. Future enhancements should expand the architecture and diversify training to fully exploit Mamba-2's potential, possibly outperforming more mature models.\nAdditionally, we propose a method of feeding entire lengthy frames, up to 0.8M sequence length, all at once to LM backbones, although this is not the only approach to optimizing performance. In particular, for lengthy videos, retrieving targeted salient information through selective frame sampling may prove to be a more effective modeling strategy and remains an intriguing direction for future research.\nConclusion. In this work, we propose Video-Ma\u00b2mba, a novel Large Multi-modal Model designed for efficient long video understanding, which integrates the Mamba structure into the vision modality. In addition, to push the boundary of memory efficiency, we introduce Multi-axis Gradient Checkpointing strategy and achieve significant memory savings, enabling the processing of extended video sequences up to 0.8M context length. Our extensive validation across multiple benchmarks confirms that Video-Ma\u00b2mba not only matches but in some cases exceeds the performance of larger models, highlighting the effectiveness and potential of our approach in pushing the boundaries of video sequence modeling."}]}