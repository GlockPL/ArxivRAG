{"title": "Contrastive Explanations That Anticipate Human Misconceptions Can Improve Human Decision-Making Skills", "authors": ["ZANA BU\u00c7INCA", "SIDDHARTH SWAROOP", "AMANDA E. PALUCH", "FINALE DOSHI-VELEZ", "KRZYSZTOF Z. GAJOS"], "abstract": "People's decision-making abilities often fail to improve or may even erode when they rely on Al for decision-support, even when the AI provides informative explanations. We argue this is partly because people intuitively seek contrastive explanations, which clarify the difference between the Al's decision and their own reasoning, while most AI systems offer \"unilateral\" explanations that justify the Al's decision but do not account for users' thinking. To align human-Al knowledge on decision tasks, we introduce a framework for generating human-centered contrastive explanations which explain the difference between Al's choice and a predicted, likely human choice about the same task. Results from a large-scale experiment (N = 628) demonstrate that contrastive explanations significantly enhance users' independent decision-making skills compared to unilateral explanations, without sacrificing decision accuracy. Amid rising deskilling concerns, our research demonstrates that incorporating human reasoning into AI design can foster human skill development.", "sections": [{"title": "1 INTRODUCTION", "content": "Imagine if Al decision-support tools not only improved the quality of our decisions but also enhanced our decision-\nmaking skills in the process. Competence, mastery, and skill growth are fundamental drivers of motivation in the\nworkplace and beyond [25, 26]. Individuals are inherently driven to refine their abilities in the tasks with which they\nengage, whether it's making more informed treatment decisions for patients, sharpening writing skills, or mastering a\nnew programming language. The ongoing process of self-improvement not only leads to better outcomes decisions,\npapers, or code \u2013 it also provides intrinsic satisfaction by fulfilling people's fundamental need for competence [25]. As\nAl systems become more integrated into our decision-making tasks, a critical question arises: How will this assistance\naffect our skill growth and competence in decision-making? Specifically, as Al systems increasingly offer ready-made\ndecisions,"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Contrastive Explanations", "content": "The field of Explainable AI (XAI) has developed a wide range of methods aimed at making AI systems more understand-\nable and useful to people [36]. Seminal approaches include feature-based explanations like LIME [72] and SHAP [59],\nwhich demonstrate how individual features influence an AI decision, as well as saliency maps [79], which highlight\nimage regions that contributed to the outcome. These methods, which we refer to as unilateral approaches, focus on\nexplaining why the AI made a specific decision but do so in isolation, without explicitly comparing it to other plausible\nalternatives.\nMeanwhile, Miller [64]'s extensive review of social science research has underscored the significance of contrastive\nexplanations, sparking a new line of inquiry in ML and HCI research. Miller's review highlights that, according to\nsocial science literature, explanations people seek and provide are predominantly contrastive [53, 56, 64]. Rather than\nsimply asking \"Why P?\u201d to receive a list of features or a sequence of causal events, people often want to know \u201cWhy P\ninstead of Q?\" - seeking an explanation that clarifies the difference between the actual outcome and an (often implicit)\nalternative they expected. Lipton [56] refers to \u201cP\u201d, the actual event, as the fact, and the alternative \"Q\" as the foil.\nSocial science experts emphasize the value of contrastive explanations for two main reasons [65]. Firstly, they arise\nfrom a person's surprise over an unexpected event, revealing their preconceived expectations - essentially offering\ninsight into the individual's mental model and highlighting their knowledge gaps [53, 63]. Secondly, providing and\nasking for contrastive explanations is less complex and cognitively demanding, making the process more efficient for\nboth the inquirer and the respondent [53, 56, 92]. In AI-assisted decision-making, we further hypothesize that because\ncontrastive explanations highlight (1) the knowledge gap of the inquirer and (2) are shorter, and thus easier to parse,\nthey will result in improved knowledge acquisition from the decision-maker compared to explanations that highlight\nall the decision factors.\nIn recent years, machine learning scholars have introduced various computational approaches for generating\ncontrastive explanations, such as pairwise class comparisons [1, 3], tree-based methods [81, 86], or identifying pertinent\npositives and negatives [27]. Unlike in our work in which the foil seeks to convey explainee's thinking and is generated\nby a separate model, in the existing techniques, the foil is commonly determined as the closest alternative outcome that"}, {"title": "2.2 Al-Assisted Decision-Making", "content": ""}, {"title": "2.2.1 Why optimizing human decision-making skills in Al-assisted decision-making matters?", "content": "A growing concern, espe-\ncially with the recent developments in generative AI [54, 90], deskilling refers to the process by which workers lose\nskills or their proficiency in tasks due to a reduced need to actively engage in those tasks [8]. This often occurs when\ntechnology, such as AI and automation [83], takes over some or all responsibilities that were previously performed by\nhumans. As individuals become more reliant on these systems to handle complex or repetitive tasks, they may stop\ndeveloping or maintaining the expertise required to perform those tasks independently [4]. For example, in AI-assisted\ndecision-making, workers might depend on AI to make recommendations or decisions, which can diminish their critical\nthinking, problem-solving abilities, and overall competence in that domain over time. Indeed, recent empirical evidence\nshows that the current designs of decision-support tools that provide AI recommendations and explanations do not\nseem to support people's growth of decision-making skills [31] and evidence from expert-based systems shows that\nlong-term dependence on such systems does lead to deskilling in those very tasks [73].\nWhile powerful, Al systems are not infallible. They make errors due to biases in the data, limitations in the model, or\nunforeseen circumstances and they even hallucinate. In the short term, when humans have strong decision-making\nskills, they are better equipped to recognize and override Al mistakes, can critically assess the Al's recommendations,\napply domain expertise, and contribute meaningfully to the decision-making process, resulting in more accurate and\nnuanced outcomes. In the long term, nurturing independent and strong decision-making skills is essential for humans\nto retain autonomy in decision-making, transfer their expertise to new situations, and adapt to evolving technologies.\nSuch independent decision-making protects both accountability and human agency as AI becomes more integrated into\nworkflows.\nOur work adds to the nascent body of research in AI-assisted decision-making, which is concerned with improving\nhuman decision-making skills in addition to accuracy of the decisions [11, 31]."}, {"title": "2.2.2 Eliciting cognitive engagement to calibrate reliance on Al.", "content": "Early optimism that AI decision-support tools will\ninevitably enhance human decision quality [61] has dwindled in light of accruing empirical evidence that paints a\nmore complex picture [5, 10, 33, 75, 88]. Intuitive designs that rely on simple XAI approaches, such as providing AI"}, {"title": "2.2.3 Assisting decision-making with LLM-generated explanations.", "content": "The emergence of Large Language Models (LLMs) has\nsparked interest in their potential to generate explanations that enhance decision-making. In the domain of programming"}, {"title": "2.3 Human Intrinsic Motivation and Al Assistance", "content": "With Al systems redefining workflows and the way tasks are carried out, questions surrounding their effect on people's\nmotivation about the tasks for which they receive assistance are becoming more pressing [11]. According to the seminal\nSelf-Determination Theory (SDT), individuals feel intrinsically motivated when three psychological needs-competence,\nautonomy, and relatedness-are met during an activity [25]. Competence refers to the need to feel skilled and effective\nin the activity, autonomy reflects the need to have control over how the activity is carried out, and relatedness involves\nthe need to feel connected to others and to experience a sense of belonging while engaging in the activity. These\nthree needs are fundamental for fostering intrinsic motivation, which leads to greater engagement, performance, and\noverall satisfaction with the task [25]. The introduction of AI assistance into decision-making processes can affect these\npsychological needs in multiple ways. For example, while AI might enhance short-term feelings of competence by\nproviding support in the moment of decision-making [29], it may simultaneously undermine long-term mastery, as\ncurrent designs do not always facilitate skill development [31]. Similarly, AI can diminish a user's sense of autonomy if\nthey feel overly dependent on the system, reducing their ownership of task outcomes.\nWe hypothesize that both the outcomes of the interaction and the design of the AI system influence perceptions of\ncompetence and autonomy. On the outcome side, we expect that AI systems that actively support skill development\nwill enhance feelings of competence. In terms of design, approaches where the AI critiques each decision after it is\nmade (e.g., contrastive after) may undermine users' feelings of competence and autonomy, as they could perceive the AI\nmore as a micromanager than a supportive tool, constantly pointing out flaws and dictating its preferred way of doing\nthings. Additionally, even when Al assistance is provided before a decision, designs that emphasize only one option\n(e.g., unilateral conditions) can still reduce the sense of autonomy compared to those that present multiple options,\nbroadening the decision-maker's scope of consideration (e.g., contrastive before conditions).\nIn SDT, relatedness refers to the connection an individual feels toward colleagues or collaborators, typically measured\nthrough questions about trust, similarity in reasoning, and willingness to engage in future interactions. We adapt these\nconstructs to assess relatedness to AI, hypothesizing that designs fostering competence and autonomy will similarly\nenhance relatedness to AI systems."}, {"title": "3 THE CONTRASTIVE EXPLANATION FRAMEWORK & HYPOTHESES", "content": "Imagine a clinician reviewing an Al-powered decision-support system's recommendation for a patient's treatment\nplan. The AI suggests Medication A, but the clinician had Medication B in mind based on their experience with this\ncondition. Existing Al systems would simply explain why Medication A is recommended. However, this leaves the\nclinician wondering why Medication B, which they deemed suitable, is not the better choice. A contrastive explanation\nmay elucidate this knowledge gap as follows: \"While Medication B is a common and a viable choice for most patients\nbecause of its short treatment duration, Medication A is recommended due to its lower risk of drug interactions with this\npatient's current medications.\"\nWe propose the Contrastive Explanation Framework to address the limitations of current AI-powered decision-\nsupport systems by providing contrastive explanations that acknowledge human's alternative considerations when\nsuggesting a decision. This framework is composed of four main components: (1) an AI task model, (2) a model of how\nhumans are likely to reason about this task, (3) a contrastive module, and (4) a presentation module. The AI task model\nis the standard AI system that predicts the Al's response for a given decision task (fact), while the human model predicts\nan average user's response a plausible alternative (foil) - for the same task based on a model trained on previous\nhuman decisions. Based on Al model (e.g., weights), the contrastive module then analyzes the differences between the\nAl's and the likely human's responses, generating task concepts in which the fact is superior to the foil (e.g., lower risk\nof drug interaction) and task concepts, if any, in which the foil is superior to the fact (e.g., shorter treatment duration).\nFinally, the presentation module, powered by a large language model, formats these dimensions and fills in the common\nsense knowledge that focuses on the knowledge gap that may lead someone to pick foil as opposed to fact.\nTo evaluate the effectiveness of contrastive explanations in improving human learning and accuracy in AI-assisted\ndecision-making, we instantiated this framework with an exercise recommendation task, and conducted an experiment\nin which people were asked to complete a sequence of decisions and were randomized in one of the 5 different conditions:\n\u2022 No AI (Baseline). Participants in the No AI condition completed the study without any AI support."}, {"title": "3.1 Hypotheses and Research Questions", "content": "In our hypotheses, we sometimes refer jointly to contrastive predicted and contrastive after conditions, in which the foil\nis not random, as contrastive with a sensible foil.\nOur main hypotheses are that contrastive explanations with a sensible foil will improve participants' decision-making\nskills ${^1}$ more effectively and result in accuracy that is equal to or better than unilateral explanations. Furthermore,\nwithin the contrastive conditions, we hypothesize that contrastive explanations with a predicted foil will result in greater\nhuman learning than those with a random foil, and offer a superior subjective experience compared to contrastive\nexplanations with an inputted foil.\nWe categorize these main and other hypotheses and research questions by interaction outcomes - human learning,\naccuracy, and subjective experience - and elaborate them below. To enhance readability, we abbreviate learning-focused\nhypotheses and research questions as H-L and RQ-L and accuracy-focused ones as H-A and RQ-A, respectively. For\nhypotheses related to subjective measures, we use the -S suffix (e.g., H-S1).\n${^1}$In this paper, we use the terms \"improving human learning\" and \"improving decision-making skills\" interchangeably."}, {"title": "3.1.1 Human Learning.", "content": "H-L1: Contrastive explanations with sensible foil - predicted (H-L1a) or inputted (H-L1b) \u2013 will lead to more\nlearning than providing people with no Al support.\nH-L2: Contrastive explanations with sensible foil - predicted (H-L2a) or inputted (H-L2b) will lead to more\nlearning than unilateral explanations.\nH-L3: Contrastive explanations with predicted foil will lead to more learning than contrastive explanations with a\nrandom foil.\nRQ-L1: Will contrastive explanations with predicted foil (provided at the decision-making time) lead to different\nlearning than contrastive explanations after the decision is made (contrastive after)?"}, {"title": "3.1.2 Accuracy & Overreliance.", "content": ""}, {"title": "3.1.3 Subjective Experience.", "content": "H-S1: Contrastive explanations with predicted foil will lead to higher perceived competence, autonomy, and related-\nness to Al than unilateral explanations.\nH-S2: Contrastive explanations with predicted foil will lead to higher perceived competence, autonomy, and related-\nness to Al than contrastive explanations with inputted foil.\nIn the following sections, we describe an exercise recommendation task and the instantiation and implementation of\nthe contrastive explanations framework for the exercise recommendation task."}, {"title": "4 EXERCISE RECOMMENDATION TASK DESIGN", "content": "To create a decision-making task accessible to laypeople on crowd-sourcing platforms while presenting cognitive chal-\nlenges similar to high-stakes decisions (e.g., treatment selection), we collaborated with a kinesiology expert, a co-author\nof this paper. We designed scenarios for an exercise recommendation task, as shown in Figure 3. Participants are tasked\nto choose the best exercise from a list of options based on a fictional character's description, goals, and preferences. This"}, {"title": "4.1 Generating the fictitious characters", "content": "We generated vignettes of fictitious people by randomly sampling their demographics from probabilities obtained\nfrom the US Census\u00b2, Centers for Disease Control and Prevention\u00b3, and the US Bureau of Labor statistics\u2074 (name, age,\ngender, BMI, physical activity level, occupation). According to the sampled fictitious character, we manipulated or\nrandomly sampled the following factors which were deemed important for exercise prescription by the expert: (1) their\nfitness level and maximal intensity (based on demographics), (2) their exercise goal (e.g., building muscles, weight loss,\nflexibility), and (3) their exercise preference (e.g., indoor/outdoor, group/individual). We implemented these steps as\nfictitious character generation process that allowed us to generate different characters."}, {"title": "4.2 Curating the exercises", "content": "To build an exercise repository for recommending activities to fictional individuals, we curated a list of 59 leisure\nactivities from a comprehensive compendium, which included various physical activities, from sports to everyday tasks\nlike housework and occupational activities [2]. In the compendium, each activity was labeled with its MET (metabolic\nequivalent), which denotes the energy requirement for basal homeostasis (1 MET is roughly the energy required to\nsleep or watch TV). Moderate activities require 3-6 METs, while vigorous activities require more than 6 METs. We\nalso labeled the exercises based on (i) their goals (cardio, muscle building, flexibility), (ii) whether they are typically\nperformed indoors or outdoors, and (iii) whether they are typically performed individually or in a group. From this list,\nwe selected seven representative exercises for the dropdown menu: aerobics, bicycling, boxing, jog/walk combination,\npilates, resistance training, and swimming. See Appendix for a detailed description of the selection process."}, {"title": "4.3 Representing characters and exercises", "content": "To prescribe exercises to characters, we first represented them in a joint representation space. Guided by the domain\nexpert, we constructed a relatively simple representation space consisting of three broad concepts: (1) intensity, (2) goal,\nand (3) preference. Each exercise and generated character was encoded onto these three broad concepts as described\nbelow.\nIntensity. For exercises, intensity captures the level of exertion or effort the exercise requires, measured in METs.\nOne MET is defined as the oxygen consumption of 3.5 milliliters of oxygen per kilogram of body weight per minute (3.5\nml/kg/min), which is roughly the rate of oxygen consumption at rest.\nFor characters, intensity captures the level of exertion or effort a character can sustain during physical activity (i.e.,\ntheir cardiorespiratory fitness). It is quantified by the reserve oxygen uptake (VO2R), which represents the additional\noxygen consumption capacity a person has beyond their resting state. This reserve is determined by subtracting the\nresting oxygen uptake (3.5 ml/kg/min or 1 MET) from the maximal oxygen uptake (VO2max), which is the highest\nrate at which the body can use oxygen during intense physical activity. Maximal oxygen uptake is assessed in clincial"}, {"title": "4.4 Designing the objective function", "content": "Having constructed joint representations for characters and exercises, we now formalize our setting and explain the\nobjective function we designed for recommending exercises to characters.\nLet a fictitious character representation be $x \\in R^D$ and an exercise representation be $y \\in R^D$, where D = 6\nand both representations are structured with dimensions representing intensity, goals, and preferences (e.g., $x =$\n[xMET, xcardio, xmuscle, xflexibility, xenvironment, xsocial setting]\u1d40, with y following a similar structure). Our goal was to\ncreate a function that scores the \"goodness\" of an exercise for the given character. We designed a linear objective\nfunction:\n$f(g(x, y), w) = w^Tg(x, y)$, (1)\nwhere g(x, y) is a piece-wise vector-valued function (devised with the expert) that returns a joint representation (vector)\nof the person and the exercise for each dimension. $g(x, y) \\in R^{D+1}$ concerning the following aspects: intensity, goal, and\npreference.\n$g(x, y) = \\begin{bmatrix}\nmin(0, x_1 - y_1)\\\\\nmin(0, y_1 - x_1)\\\\\n1[x_c > 0]((y_c - x_c) + 1 [y_c == x_c])]c\u2208 {2,3,4}\\\\\n1[y_c == x_c]c\u2208 {5,6}\n\\end{bmatrix}$ (2)\nIn the equation above, the subscripts refer to dimensions of x and y, and $1$ denotes the indicator function, which\ntakes the value 1 if the condition inside the brackets is true and 0 otherwise."}, {"title": "4.5 Learning the expert weights", "content": "The parameterized objective function (equation 1) enables learning weights w from different sources of labels. We aim\nto learn fexpert with weights we based on expert labels, and fhuman with weights wh based on crowdworker labels. The\nexpert model fexpert, takes a description of a fictitious character and exercise, and outputs a real-valued score indicating\nhow well the exercise matches the goals, abilities and preferences of the fictitious character. We trained and validated"}, {"title": "5 APPLYING THE CONTRASTIVE EXPLANATION FRAMEWORK TO THE EXERCISE TASK", "content": "Our goal is to generate contrastive explanations (using the framework in section 3 for the exercise recommendation\ntask explained in section 4. In this section, we describe this process, and we end up with contrastive explanations like\nthe ones shown in figure 3. To do so, we use a simulated AI model (we control the accuracy of this model), generate\nfoils using the human model weights, generate contrast concepts using our representation g(x, y), and generate the\nexplanations using an LLM."}, {"title": "5.1 Simulated Al model: Generating the fact", "content": "The AI model in our framework represents the common way in which models are trained for specific tasks (e.g., disease\ndiagnosis) by exposing them to vast amounts of data, which allows them to identify patterns and make decisions based\non learned statistical relationships. However, because these models operate solely within the confines of the data they"}, {"title": "5.2 Human model: Generating the foil", "content": "As motivated in previous sections, we believe that contrastive explanations are most effective when the foil represents\na likely human answer. For instance, in contexts with established guidelines, such as medical decision-making, the\nfoil could be the guideline-recommended action [43]. In situations without established guidelines, the foil can be\ninferred from prior human decisions. In our implementation of the contrastive explanation framework for the exercise\nrecommendation task, we chose to implement the foil as the likely human response to a given question. Specifically, we\nbuild a human model that predicts the exercise laypeople would select for previously unseen fictitious characters by\ntraining on unassisted human responses. We implemented a generic model to represent human decision-making, which\nwas sufficient for our simple task. However, depending on the context, personalized models that adapt and update as\nthey learn more about individual users could be more appropriate.\nWe generated a series of fictitious characters and ran an online study on Prolific to collect responses from crowd\nworkers who served as non-domain experts. See Appendix A.2.1 for details of the data collection study and the evaluation\nof the human model. To learn the human model weights, we followed the same procedure as we did for the expert\nmodel weights, and as described in section 4.5.\nGiven a character and two exercises, our learnt linear SVM classifier predicted which exercise is more likely to be\nselected by the human non-expert. The coefficients of the classifier with which this decision was achieved yielded the\nhuman model weights for each concept (i.e., goal, intensity, preference). Therefore, we constructed a scoring function\nbased on human model weights as well: $f(g(x, y), w_h) = w_h^Tg(x, y)$.\nIn our implementation, we selected the foil as the exercise with the highest score under the human weights that was\nnot the same as the expert choice: yfoil = argmaxyi (f(g(x, y), wh), where y\u00b9 \u2260 yfact. This approach selects the most\nlikely incorrect human answer. When the simulated AI was to provide a wrong suggestion (i.e., the fact was suboptimal),\nthe output of this human model was presented as the fact, and the new foil was the second most likely incorrect human\nmodel answer: this is still an incorrect choice, but less likely to be selected by people than the first one."}, {"title": "5.3 Contrast Module: Generating the contrast concepts", "content": "The goal of the contrast module is to generate the dimensions or features in which the fact and the foil differ. Specifically,\nwhat aspects render the fact superior to the foil, and in what aspects (if in any) is the foil superior to the fact.\nIn our setting, these dimensions indicate the three main concepts of the task: intensity, goal, and preference. To\ngenerate these dimensions we employed the following approach. Let yfact and yfoil be the two exercises generated\nby the AI and the human model for character x, respectively. Our goal is to identify the dimensions in which these\ntwo exercises differ based on the expert model's weights. For each exercise, we computed the element-wise product of\nthe expert model weights with the joint character-exercise representation g(x, y), resulting in the weighted vectors\nweg(x, yfact) and we g(x, yfoil) for the AI-generated exercise and the human model-generated exercise, respectively.\nNext, we calculated the difference between these two weighted vectors to determine the dimensions along which the\nexercises differ according to the expert model's weighting scheme. This difference vector, AgAI, is given by:\n$\\Delta g_{AI} = w_e \\cdot g(x, y_{fact}) - w_e \\cdot g(x, y_{foil})$ (3)\nNon-zero dimensions of Ag A1 indicate where the two exercises differ. A positive value indicates that the fact is superior\nto the foil in that dimension, while a negative value indicates that the foil is superior to the fact. Therefore, the contrastive\nmodule generates two sets of dimensions, dimensions for which the fact is superior to the foil: Sfact = {c | AgAI [c] > 0}\nand those for which the foil is superior to the fact Sfoil = {c | AgAI [c] < 0}, where c denotes the dimension. Because\nthe foil may not be superior to the fact in any dimension, Sfoil can be an empty set. However, by definition Sfact \u2260 0."}, {"title": "5.4 Presentation Module: Generating interpretable explanations", "content": "Once the fact, foil, and the dimensions where they differ are generated, the presentation module's purpose is to\nconvert this information into a format that is easily understood by humans. We chose to implement an LLM-powered\npresentation module which is guided by our trusted predictive model, allowing little room for hallucinations. Given\nyfact, yfoil, and the sets for which each are superior (Sfact, Sfoil), the LLM-powered presentation module adds common\nsense knowledge and turns the explanations into prose.\nSpecifically, the LLM adds knowledge to create the mapping from the the representation space (i.e., concepts) in\nwhich the predictive model operates to the input (i.e., vignette) and output spaces (i.e., exercises). For example, let x be\na fictitious character whose goal is to lose weight. Let yfact correspond to the representation of activity running and\nyfoil correspond to the representation of activity pilates. Further, let Sfact include {goal_cardio}. In other words, running\nis superior to pilates because it supports cardio goals. The remaining domain knowledge required to fully understand\nthe explanations are the following: 'cardio benefits weight loss', 'running is a cardio exercise' and 'pilates is not a cardio\nexercise'.\nTherefore, highlighting cardio as a differing dimension may not be enough without explaining those domain facts.\nThe LLM is prompted to fill in these knowledge gaps, given the information 'running is superior to pilates in supporting\ncardio goals'. Note that there is little room for the LLM to hallucinate facts, because we are constraining the generation\nprocess with the fact, foil, and concepts (Sfact, Sfoil) that are generated by the predictive models.\nThe LLM was always shown the character's vignette, and told the representation space dimensions that we identified\nas important (from section 4.4)."}, {"title": "6 EXPERIMENT", "content": ""}, {"title": "6.1 Task description", "content": "Participants were shown vignettes of fictitious characters and were asked to select the optimal exercise for the character\nin question based on their goals, capabilities, and preferences. They had to make a selection of the top exercise among 7\nexercises, which were fixed choices across vignettes and alphabetically ordered in the drop-down list: aerobics, bicycling,\nboxing, jog/walk combination, pilates, resistance training, and swimming."}, {"title": "6.2 Conditions", "content": "Participants were randomized into one of the five conditions:no AI, unilateral, contrastive predicted, contrastive after,\nand contrastive random, as described in Section 3. Figure 3 provides a sample of a decision task with illustrations of the\nkey conditions."}, {"title": "6.3 Procedure", "content": "Participants accessed the study online through Prolific, where they first provided informed consent. They then completed\npre-task questionnaires, including a brief demographic survey, a six-item Need for Cognition (NFC) Scale [55], and a\nseven-item Actively Open-minded Thinking (AOT) Scale [37]. The study consisted of three blocks: pre-test and post-test\nblocks, each with 5 exercise prescription tasks without AI support which served for measuring human learning, and\nan intervention block with 14 tasks where participants interacted with one of the Al interaction designs (or no AI,\ndepending on their randomization). After completing the tasks, participants filled out a shortened version of the Intrinsic\nMotivation Inventory (IMI) [62, 74], a self-reported instrument intended to measure participants' subjective experience\nwith the task, which assessed their perceived autonomy, competence, relatedness to AI, and interest/enjoyment, using 4\nquestions for each construct (except for relatedness, for which 3 questions where used). An additional question was\nincluded to assess mental demand."}, {"title": "6.4 Participants", "content": "We conducted a power analysis using G*Power [28] to determine the required sample size for detecting a small effect\nsize in our study with 5 conditions. With a small effect size, an a error probability of 0.05, and a desired power of 0.80,\nthe analysis indicated that a total of 548 participants would be needed to achieve sufficient power to detect the effect. To\naccount for filtering of spammers, a total of 800 participants were recruited to complete the task via Prolific. Participation\nwas limited to US adults fluent in English. Recruited in batches, participants received an average compensation of\n$2.70 (USD) per task. To ensure a compensation rate of $12 per hour, we adjusted the payment from $2.40 in the initial\nsmall batches to $2.75 in later batches, reflecting the median time participants spent on the study. The average age of\nparticipants was M = 35.76 (SD = 11.71) and their education distribution was 0.5% pre-high school, 19.4% high school,\n75.8% college, 5.7% post-graduate degree, and 4.6% did not disclose their education."}, {"title": "6.4.1 Exclusion criteria.", "content": "We retained 628 participants for analyses. To ensure meaningful engagement, participants\nwith a median response time under 4 seconds were excluded, as this suggested insufficient consideration of the tasks,\nwhich required reading vignettes and selecting exercises. Those with any response time exceeding 2.5 minutes (90th\npercentile) were also removed to avoid data distortion from distractions. Additionally, participants in AI-assisted\nconditions who performed near random (below 20% accuracy) or selected the same exercise for more than half of the\nstudy were excluded for potential misunderstanding. For subjective experience analyses, 6 participants were removed\ndue to technical issues they encountered during the post-study questionnaire."}, {"title": "6.5 Approval", "content": "This study received approval from our institution's IRB under protocol number [anonymized for review]."}, {"title": "6.6 Design & Analysis", "content": "This study followed a between-subjects design, with the condition as the factor. Each participant interacted with one of\nthe five conditions.\nWe collected the following indicators of performance and learning:\n\u2022 Accuracy: Percentage of correct answers provided by participants in the intervention block, where a correct\nanswer is one that matches the ground truth.\n\u2022 Overreliance: Percentage of answers that matched the Al's suggestions in questions for which participants\nreceived Al support and the Al's suggestion was incorrect.\n\u2022 Learning: Percentage of correct answers on post-intervention questions (controlled by participant's performance\non pre-intervention questions).\nFor accuracy, learning, and overreliance in text and in figures we report the marginal means produced by the regression\nmodels that included performance on pre-intervention questions as a covariate.\nTo assess the subjective experience, we collected the following measures assessed on a 5-point Likert scale, unless\nstated differently:\n\u2022 Perceived Competence: Four questions adapted from the Intrinsic Motivation Inventory (IMI) to measure\nparticipants' feelings of effectiveness and competence in the task.\n\u2022 Perceived Autonomy (Choice): Four questions adapted from the IMI capturing the degree of autonomy and\nfreedom participants felt in their decision-making.\n\u2022 Relatedness to AI: Three questions adapted from the IMI measured on a Likert scale, to evaluate participants'\nsense of connection and trust in the AI.\n\u2022 Interest/Enjoyment: Four questions adapted from the IMI to assess participants' interest and enjoyment during\nthe study.\n\u2022 Mental Demand: A single question, measuring the cognitive effort required by participants.\nTo assess the effects of experimental conditions on learning, accuracy, and subjective measures, we employed analysis\nof covariance (ANCOVA). For human learning, ANCOVA was applied to the average post-intervention correctness per\nparticipant, with pre-test performance as a covariate and condition as a fixed factor. A Shapiro-Wilk test was conducted\non the residuals to check the normality assumption, which was not violated (W = .993, p = .137). Holm-Bonferroni\ncorrections [42, 76] were used to adjust for multiple comparisons across our eight hypotheses and planned analyses\nrelated to learning. Adjusted p-values are reported wherever a correction was applied. For accuracy, we again used"}, {"title": "7 RESULTS"}]}