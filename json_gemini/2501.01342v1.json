{"title": "DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring", "authors": ["Hao Wang", "Zhichao Chen", "Licheng Pan", "Xiaoyu Jiang", "Yichen Song", "Qunshan He", "Xinggao Liu"], "abstract": "Effective process monitoring is increasingly vital in industrial automation for ensuring operational safety, necessitating both high accuracy and efficiency. Although Transformers have demonstrated success in various fields, their canonical form based on the self-attention mechanism is inadequate for process monitoring due to two primary limitations: (1) the step-wise correlations captured by self-attention mechanism are difficult to capture discriminative patterns in monitoring logs due to the lacking semantics of each step, thus compromising accuracy; (2) the quadratic computational complexity of self-attention hampers efficiency. To address these issues, we propose DeepFilter, a Transformer-style framework for process monitoring. The core innovation is an efficient filtering layer that excel capturing long-term and periodic patterns with reduced complexity. Equipping with the global filtering layer, DeepFilter enhances both accuracy and efficiency, meeting the stringent demands of process monitoring. Experimental results on real-world process monitoring datasets validate DeepFilter's superiority in terms of accuracy and efficiency compared to existing state-of-the-art models.", "sections": [{"title": "I. INTRODUCTION", "content": "Monitoring quality variables through advanced time-series analysis is paramount for ensuring operational safety in industrial automation across a broad range of applications [1\u20133]. These techniques are widely employed in process engineering [4], manufacturing [5, 6], and energy conversion [7], where the growing demands for higher efficiency and cost-effectiveness often drive equipment to operate under extreme conditions, thereby increasing the likelihood of catastrophic failures [7, 8]. For example, in chemical engineering, the Haber-Bosch process for ammonia synthesis requires temperatures above 400\u00b0C and pressures exceeding 200 bar, escalating risks of equipment failure and hazardous leaks [9, 10]. Similarly, nuclear power plants operate reactors at high pressures and temperatures to maximize output, heightening the threat of catastrophic radiation leakage [11, 12].\nThese examples underscore the pressing need for advanced process monitoring systems to mitigate risks, reduce costs, and ensure safety, reliability, and efficiency in industrial automation [3, 13, 14].\nProcess monitoring aims to estimate next-step values of quality variables from historical logs, identifying anomalies when estimated values deviate significantly from observations. Unlike other industrial data analytics tasks, such as fault diagnosis [15, 16] or soft sensing [17\u201319], process monitoring requires not only high accuracy but also operational efficiency for real-time decision-making. In large-scale chemical plants [9, 10], for instance, precise estimates help detect subtle shifts in temperature or pressure, enabling operators to intervene before potential reactor instabilities escalate. Equally important, real-time efficiency ensures these corrective measures are executed promptly, minimizing the risk of safety incidents or costly downtime. Thus, an instrumental process monitoring system should predict the next-step values of quality variables both accurately and efficiently, fulfilling its pivotal role in safeguarding industrial equipment.\nTo achieve accurate and efficient process monitoring, a variety of data-driven algorithms have been developed, evolving from early identification methods to advanced deep learning models. Traditional methods such as ARIMA [20] offer computational simplicity but are limited in handling the nonlinear complexity of industrial data. Statistical methods, including decision trees [21], XGBoost [22], and regression-based approaches [23], provide improved accuracy through richer feature extraction but rely heavily on manual engineering and struggle to scale with large datasets. In contrast, deep learning architectures\u2014spanning convolutional [24], recurrent [25, 26], and graph neural networks [27]\u2014enable automated feature extraction and GPU acceleration, driving both performance and speed. Notably, Transformers [28] have emerged as an exemplar solution, featured by the self-attention layer for capturing temporal patterns. This layer dynamically computes weighted dependencies across all time steps in the monitoring logs, capturing step-wise relationships useful for prediction [29]. Additionally, the architecture is optimized for GPU acceleration, well-suited for large-scale monitoring data [28] processing. These advancements have made Transformers a preferred choice in industrial time-series analytics [29-31].\nDespite the promise of Transformers, we contend that canonical self-attention layers present two critical shortcomings that hinder their suitability for the stringent demands of process monitoring. First, step-wise correlations in self-attention fail to adequately represent the discriminative, long-term patterns\u2014such as trends and periodicities\u2014crucial for detecting anomalies in process data. Individual observations in industrial logs often lack semantic richness, hindering the self-attention layer from capturing the discriminative patterns for accurate monitoring. Second, the quadratic computational complexity of self-attention poses serious challenge, especially in real-time monitoring applications where low latency is paramount. Consequently, canonical Transformers fall short of delivering the required accuracy and efficiency in process monitoring.\nTo overcome these limitations, we propose DeepFilter, a refined Transformer architecture that replaces the self-attention layer with a novel global filtering block specifically tailored for process monitoring. This block performs adaptive filtering across the entire temporal sequence, effectively modeling the long-term discriminative patterns. Theoretical analysis further confirms its capacity to enhance representations of long-term discriminative patterns. Moreover, by discarding the quadratic complexity inherent in self-attention, the global filtering block significantly reduces computational overhead. Extensive evaluations on real-world datasets demonstrate that DeepFilter consistently delivers superior accuracy and efficiency relative to state-of-the-art models, highlighting its role as an instrumental baseline for Transformer-based process monitoring.\nOrganization. Section II provides a detailed description of the DeepFilter architecture. Section III presents a case study on the process monitoring for real-world nuclear power plants, demonstrating the improvements in accuracy and efficiency achieved by DeepFilter. Section IV offers a review of related works in process monitoring and highlight the contribution of this work in the context of existing studies. Finally, we summarize our conclusions, limitations and outline directions for future research."}, {"title": "II. METHODOLOGY", "content": "In response to the inherent limitations of Transformer in accurate and efficient process monitoring, this section introduces the DeepFilter approach. DeepFilter replaces the self-attention layer in Transformer with an efficient filtering layer for fusing the information across different steps, effectively enhancing accuracy and operational efficiency for process monitoring."}, {"title": "A. Problem Definition", "content": "A monitoring log consists of a chronological sequence of observations $[L(1), L(2), ..., L(P)]$, where each $L(t) \u2208 R^{1\u00d7Din}$ represents the observation at the t-th step with $D_{in}$ covariates. We define $X \u2208 R^{T\u00d7Din}$ as the historical sequence and $y \u2208 R$ as the quality variable, where T is the length of the historical window and H is the monitoring horizon. At an arbitrary time step t, the historical sequence is represented as $X = [L(t \u2212 T + 1), . . ., L(t)]$, and the corresponding quality variable is specified as the final feature in L(t + H).\nThe objective of process monitoring is to develop a predictive model $g : R^{T\u00d7Din} \u2192 R$ that generates the quality variable perdition $g(X) = \\hat{y}_H \u2192 y_H$. In practical process monitoring applications, the training dataset predominantly comprises normal operational logs. Consequently, anomalies are detected as significant deviations between the actual and predicted quality variable values."}, {"title": "B. Global Filtering Block", "content": "The fundamental component of DeepFilter is the Global Filtering (GF) block, as illustrated in Fig. 1, which integrates an efficient filtering layer for mixing information across different time steps and a feed-forward network (FFN) layer for mixing information across different channels.\nLet $Z \u2208 R^{T\u00d7D}$ denote the input sequence to the k-th GF block, where T is the window length of historical monitoring logs and D is the hidden dimension. The global filtering process begins by transforming Z from the time domain to the frequency domain using the Fast Fourier Transform (FFT):\n$Z^{(F)} = F(Z),$ (1)\nwhere F denotes the FFT operation. In the frequency domain, noisy and discriminative patterns are often easily isolated. Typically, noisy patterns often reside in high-frequency components [32\u201334], while discriminative patterns often emerge in low-frequency components [35, 36]. To extract the discriminative patterns and suppress the noisy ones, we perform a filtering operation using the Hadamard product:\n$Z^{(F)} = Z^{(F)} \u2299 W,$ (2)\nwhere $W \u2208 C^{T\u00d7D}$ contains learnable parameters that are optimized to discern discriminative patterns in model training. The filtered sequence is then transformed back to the time domain via the inverse FFT:\n$Z = F^{-1}(Z^{(F)}),$ (3)\nwhich is immediately followed by a residual connection and layer normalization to stabilize the training process and mitigate gradient degradation:\n$R = LayerNorm(Z + Z), $(4)\nwhich is the output of the efficient filtering layer. To demonstrate the efficacy of the operations in this layer, we restate the convolution theorem below.\nTheorem II.1. Suppose $W = F^{-1}(W^{(F)})$, \"*\" is the circular convolution operator, the filtered sequence in (3) can be acquired by performing circular convolution below\n$Z = W * Z$.\nProof. It is equivalent to prove $F(W * Z) = Z^{(F)} \u2299 W$. To this end, the n-th element of the circular convolution above can be expressed as follow\n$Z_n = \\sum_{m=0}^{T-1}W_mZ_{(n-m)\\%T}$\nOn the basis, the FFT of $Z$ is denoted as $Z^{(F)}$, where the w-th element is given by:\n$Z_n = \\sum_{n=0}^{T-1}\\sum_{m=0}^{T-1}W_mZ_{(n-m)\\%T}e^{\\frac{-j2\\pi wn}{T}}$\n$= \\sum_{n=0}^{T-1}\\sum_{m=0}^{T-1}W_me^{\\frac{-j2\\pi wm}{T}}Z_{(n-m)\\%T}e^{\\frac{-j2\\pi w(n-m)}{T}}$\n$= \\sum_{m=0}^{T-1}W_me^{\\frac{-j2\\pi wn}{T}}\\sum_{m=0}^{T-1}Z_{(n-m)\\%T}e^{\\frac{-j2\\pi w(n-m)}{T}}$\n$= W^{(F)}\\sum_{n=0}^{T-1}Z_{(n-m)\\%T}e^{\\frac{-j2\\pi w(n-m)}{T}}$\n$= W^{(F)}\\sum_{n=m}^{T-1}Z_{(n-m)}e^{\\frac{-j2\\pi w(n-m)}{T}} + W^{(F)}\\sum_{n=0}^{m-1}Z_{(n-m+T)}e^{\\frac{-j2\\pi w(n-m)}{T}}$\n$= W^{(F)}(\\sum_{n=0}^{T-m-1}Z_ne^{\\frac{-j2\\pi wn}{T}} + \\sum_{n=T-m}^{T-1} Z_ne^{\\frac{-j2\\pi wn}{T}})$\n$= W^{(F)} \\cdot Z^{(F)}.$\nThus, the equation $F(W * Z) = Z^{(F)} \u2299 W$ holds, and the proof is thereby completed.\nTheoretical implications. Theorem II.1 implies that the efficient filtering layer adepts at accuracy and efficiency, meeting the dual excessive demand of process monitoring.\nThe efficient filtering layer excel capturing discriminant temporal patterns in the historical sequence, thereby improving accuracy. According to Theorem II.1, the layer is equivalent to a circular convolution between the historical sequence and a large convolution kernel, where the kernel size equals the historical window length T. Circular convolution facilitates the capturing of periodic patterns, while the large kernel size facilitates the modeling of long-term dependencies. Both periodic and long-term patterns are typically discriminative for process monitoring, in contrast to the step-wise correlations captured by standard Transformers.\nThe efficient filtering layer reduces the computational complexity, thereby improving efficiency. The overall complexity of the efficient filtering layer is O(T log T), significantly lower than that of self-attention layers and convolution layers with a full receptive field (O(T2)).\nWhile the filtering layer captures dominant temporal patterns in each channel, it does not incorporate channel-wise interactions. To fill in the gap, we introduce FFN as follows:\n$FFN(R) = ReLU(RW^{(1)} + b^{(1)})W^{(2)} +b^{(2)},$ (5)\n$R = LayerNorm(FFN(R) + R), $(6)\nwhere $W^{(1)}, b^{(1)}, W^{(2)}$ and $b^{(2)}$ are learnable parameters. To stabilize the training process, residual connection and layer normalization are subsequently applied.\nIn a nutshell, the GF block captures temporal and channel-wise patterns via the efficient filtering layer and the FFN layer, respectively, contributing to a representation $\u0158 \u2208 R^{T\u00d7D}$ that comprehensively understands the process monitoring logs."}, {"title": "C. DeepFilter Architecture and Learning Objective", "content": "The GF block efficiently processes historical monitoring logs and excels at capturing discriminative temporal patterns. However, this block focuses on encapsulating these patterns into a compact representation R, without generating the predicted value of quality variable for process monitoring. To bridge this gap, we introduce DeepFilter, which integrates cascaded GF blocks for achieving process monitoring.\nThe architecture of DeepFilter is illustrated in Fig. 1. It begins by transforming the historical monitoring log $X\u2208 R^{T\u00d7Din}$ into a latent representation through an affine layer:\n$Z\u00ba = XW^{(0)} + b^{(0)}$ (7)\nwhere $Z\u00ba \u2208 R^{N\u00d7D}$ represents the initial embeddings with hidden dimension D, $W^{(0)}, b^{(0)}$ are learnable parameters. These embeddings are then sequentially processed through K GF blocks. Let $Z^k$ denote the input to the k-th GF block, the output of this block is given by:\n$\u0154^k := GF^k(Z^k),$ (8)\nwhere $GF^k()$ performs the transformations from Eq (2) to (6) sequentially. The output of each GF block serves as the input for the subsequent block, i.e., $Z^{k+1} := R^k$. The output from the last GF block, denoted as $\u0154^K \u2208 R^{T\u00d7D}$, encapsulates the historical monitoring logs comprehensively. This representation is passed to a Gated Recurrent Unit (GRU) decoder, and the final-step output of the GRU decoder serves as the prediction of the quality variable:\n$\\hat{y}_H := GRU(R^K),$ (9)\nwhere $\\hat{y}_H$ is the estimated value of the target quality variable. There are several learnable parameters in DeepFilter, such as the weights and biases in the FFN, the affine layer, the GRU decoder, and the filter tensor $W^{(F)}$. These parameters are optimized by minimizing the mean squared error (MSE) between the predicted and actual quality variable values, defined as:\n$L := (y_H \u2212 \\hat{y}_H)^2. $(10)"}, {"title": "III. EXPERIMENTS", "content": "This section aims to empirically validating the effectiveness of DeepFilter in the context of process monitoring. To this end, there are three aspects need to be investigated.\n1) Accuracy: Does DeepFilter work effectively? Section III-C compares the accuracy of DeepFilter against baselines on two large-scale real-world process monitoring datasets.\n2) Efficiency: Does DeepFilter work efficiently? Section III-D evaluates the actual running time of DeepFilter and Transformers under varying configurations.\n3) Sensitivity: Is DeepFilter sensitive to hyperparameter variation? Section III-E evaluates and analyzes the performance of DeepFilter under varying hyperparameter values."}, {"title": "A. Background and Data Collection", "content": "Nuclear power plants (NPPs) play a crucial role in industrial automation, providing a stable and efficient energy source. In the United States, NPPs produce nearly 800 billion kilowatt-hours of electricity annually, accounting for over 60% of the nation's emission-free electricity [37]. This reduces approximately 500 million metric tons of carbon emissions, demonstrating their environmental and industrial significance. However, NPPs also pose security risks, as operational anomalies can lead to radionuclide leaks, resulting in severe environmental pollution and casualties [11]. To mitigate these risks, automated monitoring networks have been deployed worldwide, such as the RadNet in the United States [38], the Fixed Point Surveillance Network in Canada [39], and the Atmospheric Nuclear Radiation Monitoring Network (ANRMN) in China, as shown in Fig. 2. These systems enable continuous, reliable monitoring of radionuclide concentrations, reflecting whether NPPs are operating normally.\nThe key quality variable monitored by these systems is the atmospheric \u03b3-ray dose rate, measured using ionization chambers (Fig. 3) [40]. Fig. 4(a) illustrates the non-stationary dynamics of this variable, likely influenced by external factors such as weather conditions. To account for these influences, the monitoring systems include additional process variables, such as spectrometer measurements (spanning 1024 channels), meteorological conditions (e.g., precipitation), and spectrometer operational parameters (e.g., battery voltage). These process variables, shown in Fig. 4(b-d), display diverse temporal patterns. Frequency domain analysis reveals concentrated energy in low-frequency bands, which diminishes at higher frequencies, highlighting the potential of frequency-based models to extract semantic-rich representations.\nThe monitoring logs integrate 1,024-channel spectrometer data, seven meteorological covariates, and three operational parameters, as summarized in Table I. By consolidating diverse data sources, these logs provide a comprehensive view of NPP operational status, facilitating the construction of process monitoring systems to safeguard its operations."}, {"title": "B. Experimental Setup", "content": "1) Datasets: We employ two industrial datasets sourced from monitoring logs collected from the ANRMN project. These datasets encompass 1034 input variables, as detailed in Table I. The statistics of preprocessed data is present in Table II. Each dataset is sequentially divided into training, validation, and testing subsets with allocation ratios of 70%, 15%, and 15%, respectively. To ensure effective model training and evaluation, the datasets are normalized using a min-max scaling technique.\n2) Baselines: We compare DeepFilter with three categories of baselines as follows:\nIdentification methods: AR, MA and ARIMA [20];\nStatistical methods: Lasso Regression (LASSO) [23], Support Vector Regression (SVR) [41], Random Forest (RF) [21], and eXtreme Gradient Boosting (XGB) [22];\nDeep methods: Long Short-Term Memory (LSTM) [26], Gated Recurrent Unit (GRU) [25], Transformer [28], Informer [42], AttentionMixer [7] and iTransformer [43].\nAligning with the prevailing work [7], we employ a GRU decoder for transformer-based baselines to produce quality variable prediction.\n3) Training Strategy: All experimental procedures are executed using the PyTorch framework, utilizing the Adam optimizer [44] for its adaptive learning rate capabilities and efficient convergence properties. The experiments are conducted on a hardware platform comprising two Intel(R) Xeon(R) Platinum 8383C CPUs operating at 2.70 GHz and eight NVIDIA GeForce 1080Ti GPU. Hyperparameter optimization is systematically performed following the standard protocol [7] to enhance model performance. The learning rate is tuned within {0.001, 0.005, 0.01}; the batch size is tuned within {32, 64}; the number of GF blocks is set to 2; the historical window length is set to 16. The model is trained for a maximum of 200 epochs. An early stopping strategy with a patience of 15 epochs is employed, stopping training if no improvement was observed within the validation set. Finally, the performance metrics on the test set is calculated and reported.\n4) Evaluation Strategy: The coefficient of determination (R2) is selected for evaluating monitoring accuracy:\n$R^2 = 1- \\frac{\\sum_{i=1}^{N} (y^H_i - \\hat{y}^H_i)^2}{\\sum_{i=1}^{N} (y^H_i - \\bar{y}^H)^2}, $ (11)\nwhere $y^H_i$ represents the actual dose rate, $\\hat{y}^H_i$ denotes the estimated dose rate by the model, and $\\bar{y}^H$ is the mean value of the actual dose rates over the test set of size N. This metric effectively captures the proportion of variance in $y^H$ that is predictable from $\\hat{y}^H$. We also incorporate the root mean squared error (RMSE) and the mean absolute error (MAE) as supplementary metrics, quantifying the average magnitude of the prediction errors.\n$RMSE = \\sqrt{\\frac{1}{N}\\sum_{t=1}^{N} (y^H_i - \\hat{y}^H_i)^2}, $(12)\n$MAE = \\frac{1}{N}\\sum_{t=1}^{N} |(y^H_i - \\hat{y}^H_i)|.$"}, {"title": "C. Overall Performance", "content": "In this section, we compare the monitoring accuracy of DeepFilter and baselines across four distinct forecast horizons (\u0397 = 1,3,5,7). Results are shown in Table III with key observations below:\nIdentification models demonstrate limited accuracy in process monitoring. Primarily designed to capture linear autocorrelations, identification models struggle to model the non-linear patterns that are prevalent in many datasets. This limitation becomes more pronounced in long-term forecasting scenarios, where ARIMA models, for instance, record relatively high MAE of 0.158 and 0.127 on the Hegang and Jinan datasets, respectively, for H = 7.\nStatistic models integrate external factors, such as meteorological conditions, and demonstrate competitive performance in short-term monitoring. Non-linear estimators, particularly XGBoost, outperform linear models due to their higher modeling capacities. Notably, XGBoost exhibits robust performance across both datasets, achieving results comparable to traditional deep learning models like LSTM and GRU across most evaluation metrics.\nDeep models achieve the best performance among baselines. Specifically, Transformer-based methods display varying accuracy contingent upon their temporal fusing mechanisms. Among them, the standard Transformer model, utilizing self-attention mechanisms, shows suboptimal monitoring accuracy, highlighting the limitations of self-attention in capturing discriminative patterns within this context. Modifications to the token mixer, such as iTransformer and AttentionMixer, result in significant performance enhancements. This suggests that the refinement of token mixer is critical for accommodating Transformers to process monitoring.\nDeepFilter demonstrates the best overall performance across different metrics and datasets. Its superior accuracy demonstrates that the efficient filtering layer excels at capturing long-term discriminant patterns, as discussed in Theorem II.1, which facilitates understanding ECP logs.\nIn-depth analysis. We conduct a detailed comparison of the monitoring performance of DeepFilter and the Transformer model with self-attention for temporal fusion, focusing on three key aspects in Fig. 5:\nPredicted series. The left panels illustrate the predicted time-series values against the ground truth. DeepFilter consistently tracks the ground truth more accurately, especially during regions with sharp fluctuations and peaks. For instance, in both datasets, the Transformer fails to capture sudden spikes around timestamps 1700 and 1800. DeepFilter, in contrast, remains utility in these cases, effectively following both gradual trends and abrupt changes.\nError distribution. The middle panels display the distribution of MAE. Overall, the Transformer's error distribution is more dispersed, with a heavier tail extending to higher error values, indicating occasional significant deviations. In contrast, DeepFilter exhibits a more concentrated error distribution with consistently lower prediction errors, making it more reliable for critical monitoring tasks."}, {"title": "D. Complexity Analysis", "content": "In this section, we analyze and evaluate the computational cost of DeepFilter and baseline models [28]. Models are employed to transform a sequence (x1,..., xT) into another sequence (Z1,..., zT) of equal length T and feature number D. Three key metrics are considered: complexity, sequential operations, and path length, respectively quantifying the amount of floating-point operations, the number of non-parallelizable operations, and the minimum number of layers required to model relationships between any two time steps."}, {"title": "E. Parameter Sensitivity Study", "content": "In this section, we investigate the impact of key hyperparameters on DeepFilter's performance, including the number of global filtering blocks (K), window length (L), the number of hidden dimensions (D), and batch size. The results are summarized in Fig. 7 with key observations as follows:\nDeepFilter's performance is not significantly dependent on a deep stack of blocks. As illustrated in Fig. 7(a), utilizing 1-2 blocks already yields promising results. While adding more blocks can incrementally improve performance, there is a risk of performance degradation, potentially due to overfitting and optimization challenges.\nThe model's effectiveness improves with the inclusion of adequate historical monitoring data. In Fig. 7(b), we observe that performance is suboptimal at L=4 but significantly enhances at L=8. Extending the window length further can lead to improved monitoring accuracy, indicating the value of incorporating sufficient historical context in the model.\nThe relationship between the number of hidden dimensions and performance does not follow a clear pattern as per Fig. 7(c). A smaller dimension appears sufficient to effectively model the monitoring log at both stations, suggesting that the logs contain a high degree of redundancy. Finally, enlarging batch sizes generally improves performance in Fig. 7(d), which suggests the potential benefit of increasing the batch size to further enhance model performance."}, {"title": "IV. RELATED WORKS", "content": "Data-driven process modeling has become a cornerstone of industrial automation with the rise of Industry 4.0 and digital factory concepts [45\u201347]. These methods leverage large volumes of monitoring data to enhance operational safety across diverse industrial applications [4, 5, 48]. Current approaches can be categorized into three groups: identification methods, statistical methods, and deep learning approaches, each offering distinct advantages and limitations.\nEarly identification methods, such as Auto-Regressive (AR), Moving Average (MA), and Auto-Regressive Integrated Moving Average (ARIMA)[20], provide computational simplicity and real-time processing capabilities. However, their inability to capture nonlinear temporal dependencies limits their effectiveness. Statistical methods, including decision trees[21], XGBoost [22], and generalized linear models [23], were subsequently introduced to address these limitations. These methods improve accuracy by capturing nonlinear patterns, but they rely heavily on manual feature engineering and face scalability issues in large-scale industrial settings.\nThe advent of deep learning has revolutionized data-driven process monitoring, enabling automatic feature extraction and enhanced parallel computing capabilities. Various architectures, such as Convolutional Neural Networks (CNNs) [24], Recurrent Neural Networks (RNNs) [25, 26], and Graph Neural Networks (GNNs) [27], have been developed to extract discriminative representations from monitoring logs for next-value prediction. For example, a spatiotemporal attention-based RNN model [45] is proposed to capture the nonlinearity among process variables and their temporal dynamics; a multi-scale attention-enhanced CNN [31] is proposed to identify long- and short-term patterns; a multi-scale residual CNN [49] is proposed to extract high-dimensional nonlinear features at multiple scales. These examples highlight the versatility of deep learning in the context of process monitoring.\nBuilding on the success of deep learning methods above, Transformers [28] have gained prominence in process monitoring due to their scalability and parallel computing capabilities [29-31]. Early applications utilized self-attention to model step-wise relationships within monitoring logs [31, 50, 51] Subsequent works mainly enhanced the attention mechanisms for time-series, such as FedFormer [52] for noise filtering, Informer [42] for reduced redundancy, Pyraformer [53] for multi-scale dependencies, and LogTrans [54] for locality enhanced representation. However, the step-wise correlation captured by self-attention struggles to capture discriminative patterns in industrial logs due to the lack of semantic richness in individual observations. Recognizing this issue, another line of works [7, 43] advocated applying self-attention to model variate-wise correlations, which is more semantically meaningful than step-wise correlations in process monitoring.\nWhile advanced models offer better predictive accuracy, the increased size and complexity hinder practical deployment in monitoring scenarios with strict latency and computational constraints. To address this issue, research has explored distributed modeling [55, 56] and sparsification techniques [57-59], which are effective to enhanced efficiency but entails additional hard-core resources or scarifies some accuracy. Therefore, there exists a critical need for novel solutions that co-optimize accuracy and efficiency in process monitoring. Developing a modern Transformer-like architecture, satisfying the accuracy and efficiency demands in real-time process monitoring, remains an open question."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced DeepFilter, an adaptation of the Transformer architecture specifically optimized for process monitoring. By replacing the canonical self-attention layer in Transformer with an efficient global filtering layer, DeepFilter excels at capturing long-term and periodic patterns inherent in monitoring logs while significantly reducing computational complexity. Our experimental results on real-world process monitoring datasets demonstrate that DeepFilter outperforms existing state-of-the-art models in both accuracy and efficiency, effectively meeting the stringent demands of modern process monitoring."}]}