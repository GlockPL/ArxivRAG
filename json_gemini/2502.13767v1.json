{"title": "AI Software Engineer: Programming with Trust", "authors": ["ABHIK ROYCHOUDHURY", "CORINA P\u0102S\u0102REANU", "MICHAEL PRADEL", "BAISHAKHI RAY"], "abstract": "Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising\nto automate large parts of software engineering via artificial intelligence (AI). We argue that successfully\ndeploying AI software engineers requires a level of trust equal to, or even larger than, the trust established\nby human-driven software engineering practices. The recent trend toward LLM agents offers a path toward\nintegrating the power LLMs for creating new code with the power of analysis tools to increase trust into the\ncode. This opinion piece comments on whether LLM agents could dominate software engineering workflows\nin the future, and whether the focus of programming will shift from programming at scale to programming\nwith trust.", "sections": [{"title": "1 Are Al Software Engineers Deployed?", "content": "Software engineering is undergoing a significant phase of greater automation owing to the emer-\ngence of Large Language Models (LLMs) for code. It is easy to generate almost correct code by\nfeeding in natural language requirements to LLMs. Such automated code generation creates pub-\nlic excitement and shows the promise of automatic programming. Given the recent progress, some\nare already declaring victory and argue that AI has \u201csolved\u201d the software development problem.\nWhile genAI-enabled code generation and code completion are now prevalent in common IDEs,\nfully automated AI software engineering is not yet widely deployed in industrial practice. What is\nholding people back from adopting generative AI? A recent blog post [3] by the behavioral scientist\nand future of work advocate Lindsay Kohler points out that the key barrier to genAI adoption is\nlargely trust. People are asking if they can trust AI and if they can demonstrate trustworthiness\nto stakeholders. In the domain of coding, the concern is thus not about the management of the\norganization not accepting automatic programming, but it is about automatic programming not\nbeing accepted by developers, due to lack of trust."}, {"title": "2 Agentic Capability", "content": "As LLMs alone cannot establish sufficient trust by developers to produce correct code, we envision\na role for LLM agents in genAI-based software engineering. What is an LLM agent? How does the\nuse of LLM agents differ from prompt engineering for generating code? We highlight the following\naspects about LLM agents for software.\n\u2022 LLMs as back-ends: An LLM agent is a wrapper around LLMs, using LLMs as back-end com-\nputation engines. Agents can interact with multiple LLMs in the back-end.\n\u2022 Interaction with software tools: Agents interact with different tools to achieve a given task.\nIn software engineering, the agentic capability often comes from invoking software testing\nand analysis tools. Other, more generic tools, can also be used, such as using a shell tool to\nnavigate files in a code base or to invoke arbitrary command-line tools. Appropriate use of\ntesting and analysis tools can enhance trust of the developer in the results of the LLM agent.\n\u2022 Autonomy: An agent invokes tools in an autonomous manner. That is, the agent is not a\ndeterministic algorithm, but rather creates a nondeterministic work-plan with significant\nautonomy. Deciding on the degree of autonomy is part and parcel of designing an LLM\nagent.\nRecently, several software LLM agents have been proposed, starting with the announcement of\nthe Devin AI software engineer from Cognition Labs [7]. Devin can solve natural language tasks\n(called issues) involving bug-fixes and feature-additions. The approach combines a back-end LLM\nwith access to standard developer tools, such as a shell, a code editor, and a web browser. Such tools\nwill be employed autonomously to let the AI software engineer mimic human software engineers.\nIn parallel with the announcement of Devin, several research groups proposed their own LLM\nagents for software engineering, including RepairAgent [1], AutoCodeRover [10], SWE-agent [9],\nand Agentless [8].\nRepairAgent [1] focuses on fixing bugs and guides the LLM agent by defining a finite-state\nmachine that outlines the typical steps followed by a developer. In each state, only a subset of all\ntools, such as code editing, code search, and testing, is available to the agent. RepairAgent cannot\nsolve natural language issues, and thus cannot behave like a human software engineer. Instead it\nfocuses on conventional program repair where the goal is to make failing tests pass."}, {"title": "3 Establishing Trust", "content": "There exist numerous examples of LLM-generated code that suffers from bugs and security vul-\nnerabilities. Obviously, human-written code may also suffer from these problems. What makes us\ntrust human-written, but not LLM-generated code? Part of the reason is the perceived capability\nof \u201cpassing the blame\u201d. If a human developer is involved, there is the promise of getting feedback\nfrom the developer as needed. Of course, this does not always hold, e.g., if the developer eventually\nleaves an organization. Nevertheless, accepting a code commit from a developer often partially de-\npends on the reputation of the developer in the organization. For AI software engineers, until they\nbecome commonplace, there is a greater need to engender trust by applying quality assurance\ntechniques like testing, static analysis, and formal verification. So how can Al software engineers\nalleviate the issue of trust in automatically generated code? There can be varying levels of trust\nthat the AI software engineer can establish.\nTesting for Consistency. At a technical level, some initial degree of trust can be ensured by\nretrofitting testing into AI software engineering workflows. Thus, in the process of code gener-\nation, additional artifacts, such as tests, can be generated as well [6]. For the generated tests, not\nonly the test inputs, but also the expected outputs, i.e., the oracles, of the tests will be generated\nfrom natural language specifications. These tests can be cross-checked with the bug fixes or fea-\nture additions generated by the AI software engineer, thereby yielding greater trust in the program\nmodifications.\nSpecification Inference. A more conceptual mechanism to establish trust would be to infer the\ncode intent from the initial, possibly buggy program. The system-level intent of what a large soft-\nware system is supposed to do can often be crisply captured by a detailed natural language prompt.\nWhat is missing is the intent of the different code units, such as functions or methods. An AI soft-\nware engineer could be geared towards such specification inference, navigating the code-base via\ncode search, and trying to infer intended pre/post-conditions [5]. Such explicit specification in-\nference can enable the program modifications made by AI software engineers to be accompanied\nby well-explained justifications, increasing trust. The extracted specifications can also be used to\ncross-check the oracles of any generated tests.\nFormal Proofs. An enhanced degree of trust can come from formal proofs. A promising para-\ndigm in this regard is automated, proof-oriented programming [2], where the LLMs are used to\ngenerate code together with a proof that can be automatically checked with an external tool. While\nhistorically, proving program correctness has been very difficult due to the high cognitive burden"}, {"title": "4 Outlook", "content": "With LLM agents performing some of the heavy lifting in software maintenance and software qual-\nity control, it may be useful to think about how they will fit into future development workflows.\nWill there be separate agents for coding, testing, debugging, repair, and various software engineer-\ning tasks? The more agents there are, the more the maintainability of these agent may become an\nissue! Instead of building hyper-specialized agents for individual software engineering tasks, it\ncould be worthwhile to craft and maintain a unified software engineering agent (USE-agent) that\ncan be configured to deliver different capabilities. Such an agent of the future may conduct myriad\ntasks, including code generation, test generation, bug fixing, and feature addition. Software engi-\nneering can then be conducted by clients that rely on the unified agent, with human developers\nvetting the results of the agent wherever needed. With newer models like o1 and DeepSeek R1\nbeing developed, some of the current agentic capabilities can be co-opted by LLMs in the future,\nand hence it is useful to think about more ambitious, unified agents."}]}