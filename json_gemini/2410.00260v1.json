{"title": "DOPAMINE: DOMAIN-SPECIFIC PRE-TRAINING ADAPTATION\nFROM SEED-GUIDED DATA MINING", "authors": ["Vinayak Arannil", "Sourav Sanjukta Bhabesh", "Neha Narwal", "Sai Nikhil Thirandas", "Darren Yow-Bang Wang", "Graham Horwood", "Alex Anto Chirayath", "Gouri Pandeshwar"], "abstract": "Large Language Models (LLMs) have shown remarkable ability to generalize effectively across\nnumerous industry domains while executing a range of tasks. Many of these competencies are obtained\nfrom the data utilized during the pre-training phase of the Language Models (LMs). However, these\nmodels exhibit limitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific synthetic data but most\noften they lack in truthfulness and complexity. Alternatively, in cases where domain data is available\nlike healthcare and finance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work, we propose an automated\nand scalable framework - DoPAMine:Domain-specific Pre-training Adaptation from seed-guided\ndata Mining, to mine domain specific training data from a large data corpus for domain adaptation\nof a LM. The framework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to mine real world data\nfrom a large data corpus like Common Crawl. We evaluated our framework's performance in the\ncontinual pre-training (CPT) setting by training two domain specific 7B parameter LMs in healthcare\nand finance with data mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and 5-shot settings\nrespectively on healthcare tasks from MMLU, MedQA, MedMCQA and PubMedQA datasets, and\n2.9% and 6.7% for zero-shot and 5-shot settings respectively on finance tasks from FiQA-SA, FPB\nand Headlines datasets when compared to the baseline.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) has ushered in a new era of natural language processing,\nenabling remarkable capabilities in tasks such as text generation, summarization, and question answering [9, 12, 24,\n28, 30, 32, 43, 49, 59]. Significant advancements have been made in training large language models which cater to\nmultiple domains by training over a large corpus of data across domains [6, 24, 45, 61]. Alternatively, smaller models\ntargeting a specific task have been trained using synthetic data [20, 22], specializing smaller models for a given task\nusing multi-step reasoning [2, 6, 21] and using instruction fine tuning for improving the LMs [17].\nThe development of domain-specific language models is crucial for a wide range of applications, including but not\nlimited to medical information extraction, financial report summarization, and legal document analysis. These domains\noften involve specialized vocabularies, writing styles, and linguistic patterns that require models to be trained on relevant\ndata to achieve optimal performance. Relying solely on general-purpose language models trained on broad web data\nmay result in suboptimal performance and a lack of domain-specific semantic understanding [47]. However, the success\nof these models hinges on the availability of high-quality, domain-specific training data, which remains a significant\nchallenge [69]. Also, while there is an abundance of raw, uncategorized data available from web crawls and other\nsources, curated datasets tailored for pre-training LLMs in specific industry domains are scarce.\nTo address this challenge, we propose an automated and scalable framework - DoPAMine for mining domain-specific\ntraining data from large data corpora, such as Common Crawl [18]. Our approach leverages the power of large language\nmodels (LLMs), combined with embedding techniques and nearest neighbor search, to identify and extract relevant data\nfor a given target domain. The core of our framework involves prompting a LLM, such as Claude 3 Sonnet [1], using a\nchain-of-thought [64] based prompt template tailored for generating domain-specific synthetic seed data. We then mine\nthe large data corpus (in this paper we will use web crawl data) for semantically similar documents. The components of\nthe DoPAMine framework are shown in figure 1. We make the following contributions in this paper:\n\u2022 We introduce DoPAMine framework that is truly scalable and allows full control over the industry domains\nbeing mined. Unlike alternative approaches, such as clustering the entire data corpus, which face challenges\nlike scalability, manual cluster inspection, and potential failure to produce clean clusters for low-resource\ndomains, our framework offers precise control through synthetically generated seed data, ensuring the retrieval\nof semantically similar documents from the data corpus for the target domain.\n\u2022 We propose a versatile prompt template that harnesses the parametric knowledge of a large language model\n(LLM) to generate diverse and representative seed data tailored to a specific industry domain. By carefully\ncrafting the prompt template and varying factors such as document types, personas, author demeanors, intended\naudiences, and generation lengths, we create a rich tapestry of seed data that serves as a comprehensive\nexemplar for the target domain.\n\u2022 Moving away from relying on fully synthetic data for domain adaptation, which often grapples with factuality\nand hallucination issues, our framework curates real-world data for the domain, fostering a more authentic and\nreliable representation of the target industry.\n\u2022 Through an extensive ablation study, we show measurable improvements in the performance of downstream\ndomain-specific tasks when the data curated through our DoPAMine framework is incorporated into the\ncontinued pre-training regime of LLMs.\nIn the following sections, we provide a detailed description of our methodology, experimental setup, and evaluation\nresults, demonstrating the effectiveness of our automated data mining framework DoPAMine in curating domain-specific\ntraining data for LLMs."}, {"title": "Methodology", "content": "Our approach revolves around the incremental pre-training of a large language model on labeled in-domain data, curated\nfrom web crawl data through an automated process - DoPAMine. The algorithm underpinning this approach is outlined\nin Algorithm 1. The overarching methodology encompasses the following principal components:"}, {"title": "Data processing and indexing", "content": "Initially, we acquire a vast corpus of textual documents (D) from a diverse web crawl source- Common Crawl [16].\nWe process this data through a data-processing pipeline with the following steps: text extraction, document level and\nsub-document level de-duplication [31] and gopher filtering [50]. We then employ state-of-the-art encoder models to\nembed the entire corpus into a shared n dim vector space. This embedding process maps the documents to dense\nvector representations, facilitating the learning of semantic similarities between documents across the corpus. We then\nindex the vector representations of the corpus into a vector database (V)."}, {"title": "Seed guided automated data mining", "content": "This pivotal stage entails a multi-faceted process. We harness the capabilities of powerful generative language models,\nsuch as Claude 3 Sonnet [1], to generate domain-specific synthetic seed documents. The approach entails carefully\nconstructing chain-of-thought [64] prompt templates, informed by heuristics that accounts for document types, personas,\nauthor demeanors, intended audiences, desired generation lengths, and other relevant parameters. We sample from\ndifferent values for each of these parameters (table 1) to ensure randomness and diversity in seed generations. The\nchoices are derived from an analysis of a representative sample of documents from the Common Crawl corpus.\nSubsequently, we feed these prompts to Claude 3 Sonnet [1] for synthetic seed generations. The resultant seed dataset\ncomprises of independent and identically distributed (iid) domain-specific exemplars, providing a representative sample\nfor one or more target domains. The prompt template employed for synthetic seed data generation is shown in figure 2.\nThis prompt instructs the LLM to follow a series of steps to finally generate meaningful domain specific synthetic seed\ndata, as shown in table 6.\nNext, we utilize the seed dataset to query for semantically similar documents within the indexed vector database (V).\nFor each seed document, we retrieve its nearest neighbors in the vector space, identifying texts in the corpus that exhibit\nsemantic relatedness. We assess the similarity between documents in n dim space using cosine similarity as our\ndistance measure:\n$\\displaystyle sim(d_1, d_2) \u2248 cos(d_1, d_2) = \\frac{d_1 \\cdot d_2}{||d_1|| ||d_2||}$ \nwhere $cos(d_1, d_2)$ represents the cosine similarity between the vector representations of documents d\u2081 and d2, a metric\nthat captures their semantic relatedness. For each synthetic seed, we mine k semantically similar documents, adhering\nto the following formulation:\n$NN_k(d) = \\{ d' \u2208 D : cos(d, d') \u2265 t_{sim} \\}$\nwhere $NN_k(d)$ denotes the set of k nearest neighbors for document d, D represents the corpus, $cos(d, d')$ is the cosine\nsimilarity between the vector representations of d and d', tsim is a predefined similarity threshold. The importance\nof this step relies on the fact that, while LLM generated synthetic texts may appear coherent and topically relevant,\nthey lack the nuance, factual reliability, and depth of research that characterizes [26, 35] authentic human-written\ndocuments. Mining semantically similar documents from the corpus (D) using the synthetic seed data acts as a proxy\nto get real-world data for the domain, enabling a more authentic and reliable representation of the target industry."}, {"title": "Classifier Modeling", "content": "The quality of the mined documents [58] in Eqn 2 is dependent on the similarity threshold tsim. We choose a high\ntsim to extract only the most semantically similar documents per seed document from the indexed vector database\n(V). This allows us to assign the mined document the domain label of the seed data. From a large corpus D like the\nCommon Crawl [16], we can now mine and collate real-world domain-representative documents for all industries listed\nin table 1. It is possible that a mined document may belong to multiple industry domains. We address this by assigning\nmulti-labels to mined documents by (a) finding mined documents that were retrieved as nearest neighbors for more than\none domain, and (b) explicitly mining for documents with multiple domain relevance by generating synthetic seed data\nthat are multiple domain oriented using more than one domain in the seed generation prompt. We then aggregate the\nretrieved documents for all target domains with the multi-labels to train a multi-label text classifier. The final trained\nmodel predicts domain labels for new texts from unlabeled documents. Note, one can choose to just work with one\ntarget domain but we present a broader implementation across different industry domains to showcase the versatility of\nthe DoPAMine framework in cataloging a large unlabeled dataset."}, {"title": "Continued pre-training of LLM using mined domain data", "content": "Finally, we use the classifier to successfully classify the remaining unlabeled documents in the large corpus D making\nextraction of industry domain-specific data easy. This data was then directly used to continually pre-train and adapt a\nlarge language model to target domains in our ablation experiments. We incorporated the target domain data along with\nsamples from previously seen training data distribution so as to avoid catastrophic forgetting [52, 53]."}, {"title": "Experiment Setup", "content": "In this section we explain in detail our experiment setup for each components."}, {"title": "Seed data generation", "content": "Leveraging the capabilities of Claude 3 Sonnet [1], we generated 200 seed data samples per domain. This process\nspanned several industry domains (table 1), both with and without overlaps among them. For each domain, we generated\ndiverse prompts by sampling across multiple dimensions: document types, author demeanors and lengths of generation.\nComplete list of prompt variation choices can be seen in table 1. We validated the diversity and realistic nature of\nthe generated seed data by computing several lexical metrics and comparing them against real world web-crawled\ndocuments. Although we used retrieved real documents for Language Model Pretraining (LLM CPT), we wanted to\nensure that the seed data used to retrieve those real documents were of realistic quality. Table 2 presents a comparison of\nthe lexical metrics for the synthetic seed data and the real world web-crawled samples. The values across the different\nlexical metrics like Lexical Diversity [68], Readability [41], Hapax Legomena [48], and Lexical Richness [40] are quite\nsimilar for both the synthetic seed documents and real documents from the web crawl, indicating that the generated\nsynthetic seed data closely resembles the real world web data in terms of diversity and lexical properties."}, {"title": "Embedding generation", "content": "We employed bge-large-en-v1.5 [10] encoder using sentence transformer [51] to convert the textual documents into\nvector representation. Since the embedding quality degrades when generated for very long documents, we pre-processed\nthe common-crawl data to chunk the documents to maximum length of 2500 words respecting sentence boundaries."}, {"title": "Nearest neighbor extraction", "content": "We used OpenSearch [44] as a vector database choice to leverage its at-scale support for k-NN search at low-latency. To\nbuild the vector index, we used Non-Metric Space Library(NMSLIB) [7] engine with Hierarchical Navigable Small\nWorld(HNSW) [38] algorithm for knn search with support for cosine similarity distance metric. We experimented with\nvarious hyper-parameter settings optimizing for latency and recall to arrive at final settings of ef-construction:256, m:45\nand ef-search:50. With respect to the index cluster configurations, to optimize for memory utilization, we kept the shard\nsize at 15GB with 2 replicas of index to achieve latency of 45ms for 200 nearest neighbor lookup across shards in\nthe index. After setting up the optimized index, we queried nearest neighbors for each synthetic seed generation. We\nchoose a high $t_{sim}$ = 0.85 to extract only the most relevant nearest neighbors."}, {"title": "Classifier training", "content": "For the training of industry domain classifier model, we leveraged the NVIDIA A10G Tensor Core GPUs. Our training\nprocess encompassed a range of lightweight multilingual transformer candidates, such as m-distilbert [55], MiniLM [63],\nand others, as well as fastText [5] candidates. To ensure efficient application on terabytes of documents and to address\nlatency concerns, we intentionally avoided employing large transformer models like XLMR [14] and Longformer [3].\nFor training the models, we used 40k data samples and their translations in Latin, European and Asian languages to\nequip the model with multi-lingual capabilities."}, {"title": "Ablation study", "content": "For evaluating the usefulness of the in-domain data mined through DoPAMine, we conducted an ablation study by\ntraining 7B parameter LM candidates in the continual pre-training (CPT) setup. We employed computing clusters, each\ncomprising 128 NVIDIA A100 GPUs, to train the LLMs with and without domain weighted data. We trained a base 7B\nparameter decoder only model and used it to initialize two candidate models to simulate continual pre-training scenario.\nThis base model was trained with 300B web-crawl tokens without any domain filtering. Candidate models include a\nbaseline and a target model that were trained using additional 100B crawl tokens with and without domain specific\nmined data, respectively. Both the baseline and the domain specific model are trained on a total fixed budget of 400B\ntokens (300B (base) + 100B (in CPT mode)) to keep the models comparable in terms of parametric knowledge. For the\n100B tokens used for training the domain specific model we used a training data mixing ratio of 25% (target industry\ndomain data curated via DoPAMine) to 75% (unlabled common crawl). The 75% unlabeled common crawl training\ndata is used to avoid catastrophic forgetting in the language model [52, 53]. Table 3 provides the training data token\ndistribution across the different models trained in the ablation study. More details on the ablation study are explained in\nsection 3.6.\nDespite the subjectivity and potential biases of Llama 3 [19] judgments, the high overall agreement percentage suggests\nthat the industry domain classifier performed effectively and aligned well with the understanding of a state-of-the-art\nLLM."}, {"title": "CPT Ablation Study", "content": "We perform an ablation study to evaluate the influence of incorporating domain-specific data mined via DoPAMine\ninto a LLM's pre-training data. The underlying assumption is that incorporating domain-specific data would improve\nperformance on downstream domain-specific tasks. Due to compute constraints, we limited our experiments to\nhealthcare and finance domains as these areas are relatively specialized, and several benchmark datasets are available\nfor testing downstream performance. DoPAMine was used to mine and collect healthcare and finance-specific data\nfrom web crawls. This data was then injected at an increased proportion into the training dataset for two 7B parameter\nLLMS (DOP_healthcare, DOP_finance), while another 7B model (baseline) was trained without any domain weightage."}, {"title": "Visualization of Mined Domain Data", "content": "The Uniform Manifold Approximation and Projection (UMAP) [42] technique was leveraged to visualize and gain\ninsights into the domain-specific data mined by the proposed framework. The UMAP [42] visualization in figure\n3 reveals distinct clusters for different industry domains like healthcare, finance, and technology, validating the\nframework's effectiveness in identifying and separating domain-specific documents. Interestingly, overlapping regions\nbetween certain domain clusters suggest shared characteristics or content, enabling multi-label assignment and cross-domain applications. A dense central region represents documents with general content relevant across multiple\ndomains, potentially enhancing the language model's performance on tasks requiring broad domain knowledge or\ntransfer learning capabilities. Overall, the UMAP visualization not only validates the data mining approach but also\nprovides valuable insights into the relationships and interconnections between domains, informing the development of\nmore targeted, specialized, or multi-task language models."}, {"title": "Related Works", "content": "The task of curating domain-specific training data for large language models has garnered significant attention in\nrecent years, as researchers and practitioners recognize the importance of tailoring these models to specific domains\nand applications [36, 54, 62]. Our work builds upon and contributes to several lines of research in natural language\nprocessing [4, 8, 13, 34, 67], data mining, and machine learning. In the recent work on TinyStories [20] (a high quality\ndataset synthetically generated to teach English to neural networks), showcased the impact of high quality data on\nLLM evaluation metrics, potentially allowing to match the performance of large-scale models with much smaller\ntraining/models. Furthermore, recently SOTA results were achieved during training of \"phi-1\" [22], a new large language\nmodel for code, with significantly smaller size(1.3B) trained using synthetically generated textbooks and exercises\ndataset with GPT-3.5 (1B tokens). Synthetically curated \"textbook\" like data was used to train the model providing\nclear, self-contained, instructive, and balanced examples of coding concepts and skills to enhance the learning process\ncompared to traditional web data. The authors further trained phi-1.5 [33] version of model focusing on common sense\nreasoning in natural language, to again strongly establish that high-quality synthetic datasets can lead to models that\noutperform SOTA models 5x larger in size on natural language tasks. Developing high quality dataset for specific\ndomain presents key challenges namely, ensuring richness and distinctiveness in the dataset. Diversity entails a broad\ncoverage, scenarios spanning varied levels of difficulty, complexity, and stylistic nuances. It serves multiple purposes,\nsuch as mitigating overfitting risks and bolstering the model's adaptability to novel tasks. However, attaining such\ndiversity poses challenges, mere prompting for coding textbook-like content or exercises, albeit with some tweaks in\ninstructions or parameters, is prone to yielding a monotonous dataset, wherein identical concepts and solutions recur\nwith minor alterations [15, 17, 60, 65]. In this paper, we dive into strategies to enforce diversity and randomness in data\ngeneration process to mitigate the above risks, and propose a framework to mine high-quality domain specific datasets\nthat can be leveraged for LLM training."}, {"title": "Limitations and Future Works", "content": "While extensive experiments were conducted to validate the usefulness of DoPAMine, certain limitations exist, along\nwith opportunities for future work. We conducted the ablation experiments for the domain specific model with a\nmixing ratio setting of 25% curated in-domain data identified by DoPAMine and 75% samples from unlabeled common\ncrawl data to avoid catastrophic forgetting. Ideally one could vary this mixing ratio to find an upper bound of curated\nin-domain data which does not lead to catastrophic forgetting and improves downstream task performance. This\nexercise is extraneous to the DoPAMine methodology and hence we dont spend compute resources to find the optimal\nmixing ratio. Furthermore, the experiments utilized Claude 3 Sonnet for synthetic data generation and LLama3 as\nthe judge model, as they were the most advanced accessible models for us at the time. However, employing the\nlatest state-of-the-art models, such as Claude 3.5 Sonnet or Claude 3 Opus, could potentially enhance the quality of\nsynthetic data and improve overall performance. Additionally, for classifier training, real documents were used post\nnearest-neighbor search, but an alternative approach could involve training the classifier directly with synthetic data and\nusing it for mining real data. Moreover, in the experiments, real documents mined using DoPAMine were utilized for\ndomain adaptation training, but combining some synthetic data with this curated data might improve performance, as\nevidenced by recent works [33]."}, {"title": "Conclusion", "content": "DoPAMine is an automated and scalable framework that leverages large language models and carefully crafted prompts\nto generate diverse, representative seed data tailored to specific industry domains. This seed data guides the retrieval of\nsemantically similar real-world documents from large corpora like web crawls, curating domain-specific datasets for pre-training language models. Extensive experimentation demonstrates DoPAMine's effectiveness in improving language\nmodel performance on downstream domain tasks by incorporating the curated data during continued pre-training.\nThe framework offers several advantages over existing methods, enabling scalable and precise control over target\ndomains, including low-resource ones, while mitigating issues with relying solely on LLM generated synthetic data. By\nfacilitating the efficient development of domain-tailored language models across fields like healthcare, finance, etc,\nDoPAMine represents a significant step towards providing more accurate and specialized natural language processing\ncapabilities, unlocking new possibilities and driving advancements and LLM adoption across various industries."}, {"title": "Datasets", "content": "Several datasets were used in our experiments. In order to conduct ablation experiments, we used mostly Common rawl\ndataset.\n\u2022 Common Crawl [16]: The Common Crawl corpus contains petabytes of data collected over 12 years of web\ncrawling. The corpus contains raw web page data, metadata extracts and text extracts. Common Crawl data is\nstored on Amazon Web Services' Public Data Sets and on multiple academic cloud platforms across the world.\nTo train the base model we used 300B tokens and additional 100B tokens for training candidate 7B models. We tested\nour ablation models using several industry specific benchmark datasets.\n\u2022 MMLU [23] : MMLU stands for Multimedia Language Understanding dataset. MMLU is designed to measure\nknowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings.\nThe benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in\ndifficulty from an elementary level to an advanced professional level, and it tests both world knowledge and\nproblem solving ability. We utilized healthcare specific datasets with in MMLU that includes anatomy, clinical\nknowledge, college medicine, human sexuality, medical genetics, professional medicine, and virology.\n\u2022 MedQA - USMLE [27]: Multiple choice question answering based on the United States Medical License\nExams (USMLE). The dataset is collected from the professional medical board exams. We used the test split\nconsisting of 1.2k QA pairs.\n\u2022 MedMCQA [46]: MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset\ndesigned to address real-world medical entrance exam questions. We used the test split of 6.15k question\nanswers.\n\u2022 PubMedQA [29]: PubMedQA is a biomedical question answering (QA) dataset collected from PubMed\nabstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative\nstatins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. We\nused the 1k labeled test split from PubMedQA.\n\u2022 FIQA-SA [37]: This dataset is based on the task 1 of the Financial Sentiment Analysis in the Wild (FiQA)\nchallenge. The dataset is split into three subsets: train, valid, test with sizes 822, 117, 234 respectively. We\nused the test split in our experiments.\n\u2022 FPB [39]: The Financial PhraseBank (FPB) dataset consists of 4840 sentences from English language financial\nnews categorised by sentiment. We used the test split consisting of 970 rows."}, {"title": "LLM-as-judge: Prompt", "content": "To assess the performance of the classifier, we also employed the LLM-as-a-judge evaluation approach outlined\nin [25,70]. This approach leverages a large language model (LLM) to judge the industry domain predictions made by\nour model. We used Llama 3 [19], a state-of-the-art LLM, with a list of predicted industry domains for each document.\nLlama 3 [19] was then asked to pass judgment on whether it agreed with the predicted domains or not. To mitigate\npotential position bias, where a LLM may exhibit a propensity to favor certain positions over others, we randomly\nshuffled the industry domain list in every prompt. The prompt used for the evaluation is shown in figure 5.\nThe results of this evaluation are presented in figure 4b, where we plot the percentage agreement between Llama 3 [19]\nand our industry domain model predictions for each domain. The x-axis represents the industry domains, and the y-axis\ndisplays the percentage agreement with Llama 3 [19]. Our analysis reveals that Llama 3 [19] exhibited an overall\nagreement of 82.97% on the 806 documents evaluated. However, the agreement was below 50% for the \"Transportation\n& Logistics\" and \"Telecommunications\" domains. For the remaining domains, Llama 3 [19] demonstrated a high level\nof agreement, exceeding 80%, with the predictions made by our industry domain classifier. This indicates that our\nclassifier performed well in accurately assigning industry domains to the documents in our large corpus D.\nWhile the LLM-as-a-judge evaluation approach provides valuable insights into the performance of our models, it is\nimportant to note that the judgments made by Llama 3 [19] are subjective and may be influenced by its training data\nand biases."}, {"title": "Industry Verticals", "content": "\u2022 Financial Services & Insurance (FSI): Financial Services comprises establishments primarily engaged in\nfinancial transactions (transactions involving the creation, liquidation, or change in ownership of financial\nassets) and/or in facilitating financial transactions. This includes Banking, Securities and Insurance as well as\nthe organizations that regulate or serve these institutions.\n\u2022 Healthcare & Life Sciences (HCLS): Healthcare provides goods and services to treat patients with curative,\npreventive, rehabilitative, and palliative care. This includes organizations that have oversight or serve these\nestablishments. Life Sciences encompass organizations in the fields of biotechnology, pharmaceuticals,\nbiomedical technologies, life systems technologies, nutraceuticals, cosmeceuticals, food processing, and\norganizations and institutions that devote the majority of their efforts in the various stages of research,\ndevelopment, technology transfer and commercialization.\n\u2022 Media & Entertainment (M&E): The media and entertainment industry consists of film, print, radio,\ntelevision, sports and cultural institutions such as libraries and museums.\n\u2022 Public Sector: The Public Sector includes government agencies providing public services, prioritizing digital\ntransformation, efficient governance, and citizen-centric solutions. It incorporates technologies like AI and\nsmart city initiatives to enhance transparency and public welfare.\n\u2022 Software & Internet: Software & Internet comprises organizations involved in the development, maintenance\nand publication of general software. The industry also includes networking and storage.\n\u2022 Travel & Hospitality: The Travel & Hospitality sector encompasses transportation, accommodation, and\nleisure services, adapting to technology with online booking systems and personalized experiences. Sus-tainability, safety, and customer-centric approaches drive the industry to meet evolving travel preferences.\nThe Hospitality industry includes lodging, event planning, theme parks, cruise line, restaurants and other\nestablishments or services within the tourism industry.\n\u2022 Agriculture: Agriculture is the science, art, or occupation concerned with cultivating land, raising crops, and\nfeeding, breeding, and raising livestock. It also includes the production of livestock, poultry, fish, and crops.\nAll food consumed by people and feed consumed by humans is a result of agriculture. Agricultural crops are\nalso used for many forms of fuel.\n\u2022 Energy & Utilities: Energy: Oil and Gas: The Oil & Gas industry comprises organizations involved in\nthe exploration, extraction, refining, transporting, and marketing of petroleum products. This includes\norganizations that serve or regulate those establishments. Energy: Power and Utilities: The Power & Utilities\nindustry comprises organizations involved in the generation, transmission, distribution and sale of electric,\nnatural gas, water and other regulated utility operations. This includes organizations that serve or regulate\nthose establishments.\n\u2022 Gaming: Organizations the develop, sell or license electronic games, lotteries or contests. This includes orga-nizations that have oversight or serve these establishments. Distribution/Platform Services, Game Developer, Game Publishing/Operations, Game Services & Technology, Real Money Gaming,, Simulation.\n\u2022 Real Estate & Construction: Engineering, Construction & Real Estate comprises organizations involved with the construction, alteration, engineering or sale of the physical environment. This includes organizations that serve or regulate those establishments.\n\u2022 Retail: The Retail Trade sector comprises establishments engaged in retailing merchandise, generally without transformation, and rendering services incidental to the sale of merchandise. The retailing process is the final step in the distribution of merchandise; retailers are, therefore, organized to sell merchandise in small quantities to the general public. This sector comprises two main types of retailers: store and nonstore retailers.\n\u2022 Services: The services industry covers a wide range of businesses providing intangible products such as consulting, finance, and healthcare. It emphasizes customer satisfaction, digital transformation, and innovative solutions, adapting to changing market demands and technological advancements.\n\u2022 Sports: The sports industry involves entertainment, competition, and physical activities. It includes profes-sional leagues, sports events, and fitness services. The industry leverages technology for fan engagement, athlete performance analysis, and sports broadcasting innovations.\n\u2022 Automotive: The Automotive industry comprises organizations involved in the design, development, manufac-turing, marketing, regulation and selling of motor vehicles."}]}