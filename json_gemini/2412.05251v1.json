{"title": "Uncertainty Quantification for Transformer Models for Dark-Pattern Detection", "authors": ["Javier Mu\u00f1oz", "\u00c1lvaro Huertas-Garc\u00eda", "Carlos Mart\u00ed-Gonz\u00e1lez", "Enrique De Miguel Ambite"], "abstract": "The opaque nature of transformer-based models, particularly in applications susceptible to unethical practices such as dark-patterns in user interfaces, requires models that integrate uncertainty quantification to enhance trust in predictions. This study focuses on dark-pattern detection, deceptive design choices that manipulate user decisions, undermining autonomy and consent. We propose a differential fine-tuning approach implemented at the final classification head via uncertainty quantification with transformer-based pre-trained models. Employing a dense neural network (DNN) head architecture as a baseline, we examine two methods capable of quantifying uncertainty: Spectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural Networks (BNNs). These methods are evaluated on a set of open-source foundational models across multiple dimensions: model performance, variance in certainty of predictions and environmental impact during training and inference phases. Results demonstrate that integrating uncertainty quantification maintains performance while providing insights into challenging instances within the models. Moreover, the study reveals that the environmental impact does not uniformly increase with the incorporation of uncertainty quantification techniques. The study's findings demonstrate that uncertainty quantification enhances transparency and provides measurable confidence in predictions, improving the explainability and clarity of black-box models. This facilitates informed decision-making and mitigates the influence of dark-patterns on user interfaces. These results highlight the importance of incorporating uncertainty quantification techniques in developing machine learning models, particularly in domains where interpretability and trustworthiness are critical.", "sections": [{"title": "1 Introduction", "content": "The field of NLP was revolutionized with the arrival of transformer models, a groundbreaking architecture introduced by Vaswani et al. in their seminal work, \"Attention is All You Need\" [42]. Prior to this, NLP relied heavily on Convolutional Neural Networks (CNN), which were useful in analyzing the spatial features of the data but lacked semantic awareness and nuances. Later, Recurrent Neural Networks (RNN) were used, which processed data sequentially and struggled but faced issues with long-range dependencies within text and stability during training due to the vanishing gradient.\nTo address this, gated RNNs like Long Short-Term Memory (LSTM) were introduced, which mitigated the vanishing gradient problem but were not parallelizable and required high computational demand for training [12]. With their unique self-attention mechanism, transformers enabled parallel processing of entire data sequences, offering a substantial leap in efficiency and effectiveness. This architecture's ability to capture complex relationships across distant parts of a text significantly enhanced performance across a myriad of NLP tasks, setting new benchmarks in machine translation, sentiment analysis, and beyond. Subsequent iterations, such as BERT [8] and the GPT series [6, 34], further refined and extended the transformer's capabilities, embedding it as the cornerstone of modern NLP research and applications. The transformative impact of these models is not just limited to their superior performance; they have also democratized access to high-quality NLP tools, fostering innovation and expanding the field's frontiers [22].\nSpecifically, transformer models have been widely used for sequence classification tasks, from text sentiment analysis [46] to DNA classification [13]. Despite all their advantages, transformers suffer from similar limitations to other neural network-based models, i.e. their black-box nature that makes their understanding difficult [39]. This aspect can be critical in tasks such as autonomous driving [9] or medical diagnosis [49], where there is a need to obtain a measure of certainty in the model predictions before committing to any action.\nGiven the black-box nature of transformer models and the critical importance of reliable predictions in high-stakes applications, integrating uncertainty quantification into these models becomes paramount [39]. However, transformer architectures' complexity and pre-trained nature present significant challenges in modifying their internal components to accommodate uncertainty measures. As a result, focusing on the final classification head offers a practical and effective approach to introduce uncertainty quantification [38].\nThe interpretability and reliability of transformer-based models can be improved by integrating classification heads with predictions and measures of confidence or uncertainty. This is particularly important in applications where errors can have high costs, and understanding the model's confidence can lead to better decision-making processes [1, 27]. One example of a pervasive issue requiring such knowledge is the use of dark-patterns in user interfaces. These deceptive design strategies compromise user autonomy and challenge the ethical integrity of digital services. Therefore, understanding a model's confidence can help prevent such issues and promote fair and transparent digital practices.\nIn this paper, to address the inherent opacity of neural networks and meet the growing demand for more transparent and trustworthy AI systems, three approaches are explored to improve the interpretability and reliability of transformer-based models for dark-pattern detection: (1) dense neural networks (DNNs), (2) Bayesian"}, {"title": "1.1 Classification Heads", "content": null}, {"title": "1.1.1 Dense Neural Network layers (DNNs)", "content": "Dense layers, also known as fully connected layers, are the most basic form of a neural network layer, where each input neuron is connected to every neuron in the next layer [21]. In classification tasks, a dense layer typically serves as the final layer that maps the learned representations to the target classes. The primary advantage of dense layers lies in their simplicity and effectiveness in learning complex patterns through these direct connections. However, they do not inherently provide measures of uncertainty in their predictions, treating all inputs with equal certainty."}, {"title": "1.1.2 Bayesian Neural Network layers (BNNs)", "content": "Bayesian dense layers [19] extend the concept of dense layers by incorporating Bayesian inference into the network's architecture. Unlike traditional dense layers with fixed weights after training, Bayesian dense layers treat weights as distributions, This approach allows the network to simulate multiple possible models of parameters @ with an associated probability distribution p(\u03b8), enhancing the model's ability to express and quantify uncertainty in its predictions. This is crucial for critical applications like drug discovery and fair Al systems, where decision-making relies heavily on the reliability of the model's output [1, 27].\nDuring training, BNNs utilize a prior distribution for weights instead of fixed values, reflecting initial beliefs which are updated via a likelihood function assessing the model's fit to the data. Bayesian inference computes a posterior distribution combining these elements, often approximated through variational inference for practical implementation. After training, the BNN generates multiple predictions by randomly sampling different sets of weight values from the posterior distribution of weights. This process offers insights into into their uncertainty and impacting the predictive uncertainty. By comparing these multiple predictions, the degree of uncertainty can be assessed, where low variability among predictions indicates low uncertainty, while high variability suggests greater uncertainty. This is uncertainty quantified using multiple forward passes to generate a distribution of predictions.\nHowever, the major challenge with Bayesian dense layers is their computational complexity and the need for more sophisticated training techniques to manage the probabilistic nature of the weights."}, {"title": "1.1.3 Spectral-normalized Neural Gaussian Process", "content": "Spectral-normalized Neural Gaussian Process (SNGP) [25] is a relatively recent approach that combines the ideas of spectral normalization and Gaussian Processes (GPs) with deep learning to enhance a deep classifier's capacity to measure the distance between the test example and the training data. Spectral normalization [30] is a technique used to stabilize the training of neural networks by normalizing the weight matrices, ensuring that the Lipschitz constant of the function (represented by the network) is constrained. This helps maintain the model's generalization ability. On the other hand, GPs provide a principled, probabilistic approach to learning in kernel machines, offering a powerful tool for uncertainty quantification. By integrating GPs with deep learning, SNGP layers aim to preserve the deep neural network's capacity for feature extraction and representation learning while enhancing the model's ability to provide meaningful uncertainty estimates for its predictions. This makes SNGP particularly appealing for tasks requiring a careful balance between performance and interpretability, such as safety-critical applications."}, {"title": "1.1.4 Motivation", "content": "dark-patterns, defined as deceptive user interface designs that manipulate online users into unintended actions have emerged as significant threats to privacy, fairness, and transparency in digital environments. These designs exploit psychological vulnerabilities, leveraging mechanisms such as obfuscation, false urgency, and social proof to influence user decisions in favour of the companies implementing them [5]. The importance of detecting these manipulative strategies cannot be overstated, as they compromise user autonomy and undermine the ethical foundation of digital services [16, 43]. Machine learning and deep learning algorithms have demonstrated great accuracy in identifying deceptive practices in digital ecosystems [47]. This offers a solution to enhance user protection and promote transparency and fairness. However, to ensure effective detection, it is important to utilize state-of-the-art architectures and incorporate certainty in the detection process [10]. Thus, combating dark-patterns is crucial, aligning with the growing demand for digital accountability and protection of consumers in complex online landscapes. This relaionship highlights the ongoing efforts and significant impact of research in tackling unethical practices in digital user interfaces."}, {"title": "2 Related work", "content": "Transformers have been widely used for text classification tasks, showcasing their versatility and efficacy across a wide array of applications. One notable application is in the domain of customer feedback analysis [31], where the authors demonstrate transformers' ability to handle complex multi-label classification tasks. This research highlights how transformers, with their deep contextual understanding, can effectively categorize customer reviews into multiple relevant categories, thus providing valuable insights into customer sentiment and preferences.\nThe ability of transformer models to handle noisy data is crucial for maintaining their performance in real-world applications. A study titled \"Transferable Post-hoc Calibration on Pretrained Transformers"}, {"title": "3 Methodology", "content": "This section describes the components of the dark-pattern classification with uncertainty quantification task. We begin by describing the dark-patterns dataset used for this study. Then, we provide an overview of the model and head selection for the task."}, {"title": "3.1 Data", "content": "The dataset used is the dark-patterns dataset developed by Yada et al. [47], containing 2356 examples scraped from different websites."}, {"title": "3.2 Models", "content": "The choice of models for this comparison is grounded in their innovative contributions and varied approaches to NLP and machine learning challenges:\n\u2022 Dolphin-Llama2-7B-AWQ\u00b9: An advanced model originating from the LLaMA2 architecture [41], renowned for its natural language understanding and generation capabilities, especially in conversational contexts. It is enhanced by training on the Dolphin dataset\u00b2 to eliminate bias and alignment issues, making it particularly effective for dark-pattern detection. This model incorporates AWQ technology [24] for 4-bit weight quantization, optimizing efficiency and speed without sacrificing accuracy, highlighting its potential for rapid and precise analysis in identifying manipulative digital interfaces.\n\u2022 bert-large-uncased: BERT [8] is a foundational model that significantly advanced the understanding of context in language, as the first model to successfully apply Transformers at scale. The selection of its largest variant, for our study serves a dual purpose: to benchmark the evolution of model architectures over time and to ensure a comprehensive analysis by employing the most capable version.\n\u2022 Mistral-7B-OpenOrca-AWQ\u00b3: Mistral model version quantized with AWQ method trained on Q&A OpenOrca Microsoft Dataset [23] augmented with GPT-4 and GPT-3.5. The foundational Mistral [18] model focuses on balancing computational efficiency with performance, suitable for diverse application scenarios.\n\u2022 mamba-370m\u00b2: Mamba [13] leverages new architecture based on selective state-space models (SSMs). Unlike Transformers, Mamba selectively retains or discards information based on the current token without attention, significantly reducing complexity from quadratic to linear with sequence length. This architectural innovation is especially relevant for analyzing complex webpage elements in dark-pattern detection, offering a promising approach.\n\u2022 nomic-embed-text-v15: Nomic Embed [32] innovates in embedding techniques to provide more dynamic, context-aware representations, surpassing leading models as of February 2024. Its top\nIt is important to note that all these models are open-source, contributing to the democratization of advanced artificial intelligence tools and fostering innovation across the field.\nThe quantized LLM models are quantized with Activation-aware Weight Quantization (AWQ) [24]. AWQ focuses on low-bit weight quantization (INT3/4) recognizing that all weights are not equally important. AWQ's selective quantization preserves essential model performance while ensuring that models remain lightweight and fast, making it a key technology for the effective and efficient application of advanced AI in real-world scenarios.\nOur model development strategy employs the \"pre-train and fine-tune\" paradigm [7, 14, 20], utilizing pre-trained models for fine-tuning. This stage is crucial for enhancing the models' performance and certainty in the downstream task of identifying dark-patterns. Through this methodology, we aim to deepen our understanding of these models' capabilities in specific real-world scenarios, focusing on the critical intersection of performance and certainty in detecting dark-patterns.\nBNNs use different methods for variance reduction, two of them being the reparametrization [4] and flipout methods [45]. The flipout method has emerged as a preferable variance reduction technique over the reparameterization trick. While reparameterization effectively reduces variance for models with continuous latent variables by transforming stochastic variables into deterministic functions, its application is limited to tractable distributions. Flipout, on the other hand, introduces random perturbations to gradients within mini-batches, mimicking the effects of larger batches to stabilize training without additional computational costs. This approach not only broadens its applicability, including to discrete variables and complex distributions but also reduces intra-batch interference, making it particularly suitable for the vast and varied parameter spaces of LLMs.\nIn BNNs, weights are represented by probability distributions, which encode beliefs about the possible values those weights can take based on the data and prior information. To make a prediction, one must sample from these distributions, resulting in a different set of weights for each prediction. These varying sets of weights lead to a range of possible outputs for a given input, reflecting the model's un-"}, {"title": "4 Results and Discussion", "content": "This study extensively explores the trade-off between performance, certainty, and sustainability of Transformer models, focusing on Dense Neural Networks (DNNs) as a baseline, Bayesian Neural Networks (BNNs), and Spectral-normalized Gaussian Processes (SNGPs) for uncertainty quantification.\nFor the experiments, every model is fine-tuned on the dark-patterns dataset with the three different classification heads: DNN, BNN and SNGP. We compare the model size, accuracy, F1, inference time and train and test carbon emissions measured with Codecarbon [26]. For the model tuning, 20% of the data is reserved as test, and the remaining 80% is divided into 20% validation and 80% train. The training parameters are defined in Table 2. All experiments are conducted in a cloud server with an Nvidia RTX 5000 GPU with 16G VRAM.\nEven though the Mistral and Llama 2 models are quantized, they remain frozen during fine-tuning since the GPU does not have enough VRAM to fit all the weights. For the remaining models, all weights are fine-tuned. Regarding the hyperparameters of the classification heads, the SNGP head has 1024 inducing points, based on the default value on the original paper [25]. For fairness of results, the DNN and BNN classification heads have a hidden layer of 1024 units, with the number of output neurons of the base model as inputs and one output neuron for the binary classification logits. For the BNN, the weight initialization parameters are \u03bc = 0 and \u03c3 = 1 and the number of predictions used for obtaining the confidence interval is 10.\nIt is important to note that the main objective of this paper is not to improve the baselines on dark-pattern detection, but to use the dark-pattern classification task as a way to compare different classification heads for uncertainty quantification in transformer models."}, {"title": "4.1 Trade-off Performance Analysis", "content": "After evaluating the performance of different types of models used in this study, we discovered that each model has its unique advantages and limitations based on specific accuracy and inference time metrics (see Table 3). These performance metrics significantly impact determining the suitability of each model type for various practical applications.\nDNNs displayed consistent accuracy across all tests, making them a reliable choice for applications where stable and predictable performance is needed. Due to their simpler architecture, DNNs also had the fastest inference times among the models tested. This makes"}, {"title": "4.2 Sustainability and Environmental Impact", "content": "The study demonstrates a direct correlation between the size of the Transformer models and their carbon emissions, even when LLMs have frozen layers except for the head, such as Mistral and Llama. These larger models require significantly more computational power, translating to higher energy consumption and increased carbon emissions, as depicted in Figure 2. This relationship is crucial for organizations that balance performance with sustainability, as opting for smaller, more efficient models like Nomic could significantly reduce their environmental impact.\nThe environmental impact varies significantly across different model types during the training and inference phases. Traditional DNNs, while less computationally intensive than BNNs or SNGPs, do not offer uncertainty quantification, which might necessitate retraining or additional computational overhead in uncertain scenarios. Although BNNs provide valuable insights into model certainty, they also have a much higher energy cost due to the need to process multiple samples to estimate uncertainty. This is evident from the study's findings, where BNNs consumed up to ten times more energy than DNNs under similar conditions."}, {"title": "4.3 Comparing Uncertainty Quantification Approaches", "content": "Through the detailed review of model performance, stability, and energy consumption, we can better understand the strengths and limitations of each BNN and SNGP approach.\nAs shown in Table 4, the analysis reveals significant differences in certainty and accuracy between SNGP and BNN models. For instance, BERT and Llama models equipped with SNGP heads showed exceptional stability with identical scores of 95.745 for the top and bottom 10% of predictions, accompanied by a near-zero mean variance (0.005). This indicates highly stable predictions across the dataset, suggesting that SNGP models maintain consistent performance even in varying data conditions. In contrast, the Mistral model with an SNGP head displayed less stability, with a notable discrepancy between the highest and lowest 10% of predictions (68.085 vs 57.447) and a significant mean variance (0.931).\nWhen coupled with an SNGP, Nomic showcased high certainty, as both the top and bottom 10% of predictions scored very high (97.872), coupled with low variance (0.005). This reflects a robust model architecture or particularly effective training data, emphasizing the potential of SNGP to provide reliable and consistent outputs. In general, SNGP models tend to show lower mean variances than their BNN counterparts for the same set of models, indicating more stable predictions. While BNNs may occasionally reach higher peaks of certainty or confidence in certain predictions, their performance across the dataset is comparatively less stable, marked by higher"}, {"title": "4.4 Practical Implications", "content": "One of the study's objectives is the model's ability to quantify uncertainty in its predictions. Table 5 presents high and low uncertainty instances from the Nomic SNGP model. The semantic clarity of the text examples is evident in the low uncertainty cases, where the model's predictions are made with confidence. These examples are straightforward, containing complete phrases that convey a clear message, related to consumer behaviour or stock levels, such as \"Only a few more left!\" or \"Hurry! Limited Quantity Available.\".\nOn the other hand, the high uncertainty cases consist of single words like \"Arthritis Aids\" or \"Irwin\", which are semantically ambiguous without further context. This ambiguity translates into a higher variance in the model's confidence. The presence of these high-variance cases in the table serves a critical function; it underscores the capability of the Nomic SNGP model to introspect and evaluate its certainty.\nThis model's \"self-awareness\" has practical implications that add a layer of interpretability to the AI's decision-making process compared to the traditional dense head layer model counterparts. This interpretability is crucial for end-users, enabling them to discern when a model's output is reliable and when it should be treated with scepticism. This discernment is critical when considering the identification of dark-patterns. The certainty measure allows the model to signal which cases are clear-cut and ambiguous, thus avoiding the pitfall of overgeneralizing or making unwarranted assumptions based on uncertain predictions, a real-world relevance that should resonate with data scientists and AI developers.\nIt is important to note that the ability to measure certainty is a tool that end-users could use, and it also plays a crucial role in guiding the development of the model itself. As discussed in a recent study by Schmarje et al. [36], having high-quality data and addressing label ambiguity is crucial for data scientists to identify gaps in the training data or areas where the model require further improvement. Knowing the level of certainty in the model's predictions can significantly improve the learning process by analyzing instances of high uncertainty that indicate the model's difficulties and which predictions need reevaluation or additional context."}, {"title": "5 Conclusions and Future Work", "content": "This research paper focuses on enhancing the interpretability of transformer models by integrating uncertainty quantification, aimed specifically at detecting dark-patterns in user interfaces as an example of risky situations where certainty is valuable. We demonstrate that this approach can make Al systems more trustworthy without significantly compromising performance. Our study uses dense layers, Bayesian dense layers, and spectral-normalized neural Gaussian processes to achieve this goal. The evaluations across various metrics-model size, accuracy, inference time, and environmental impact-indicate that while there are trade-offs, particularly regarding computational demand and carbon footprint, the benefits of increased reliability and accountability in model predictions are profound. While we have made progress, there are still areas to explore, particularly in detecting and mitigating bias in text-based AI applications, where dark-patterns can skew outcomes unfavourably. We suggest that uncertainty quantification methods can be adapted to identify and correct biases in training data or model predictions. Additionally, we propose using conformal prediction and distance awareness to establish confidence intervals around predictions, providing a clear statistical guarantee about their accuracy. Applying these methods to transformer models can further enhance their usability in risk-sensitive environments."}]}