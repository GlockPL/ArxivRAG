{"title": "Creating a Cooperative AI Policymaking Platform through Open-Source Collaboration", "authors": ["Humanity Unleashed"], "abstract": "Advances in artificial intelligence (AI) present significant risks and opportunities, requiring improved governance to mitigate societal harms and promote equitable benefits. Current incentive structures and regulatory delays may hinder responsible AI development and deployment, particularly in light of the transformative potential of large language models (LLMs). To address these challenges, we propose developing the following three contributions: (1) a large multimodal text and economic-timeseries foundation model that integrates economic and natural language policy data for enhanced forecasting and decision-making, (2) algorithmic mechanisms for eliciting diverse and representative perspectives, enabling the creation of data-driven public policy recommendations, and (3) an AI-driven web platform for supporting transparent, inclusive, and data-driven policymaking.", "sections": [{"title": "Introduction", "content": "As advances in artificial intelligence continue to reshape our world, humanity must grapple with new forms of risk and opportunity. Concerns abound regarding rogue AI-whether arising from malicious human actors or from autonomous agents themselves [Juric et al., 2020, Bengio et al., 2024] \u2014as well as the transformative power that AI confers on corporate and state competitors [Khan, 2019, Harraf and Ghura, 2021, Haner and Garcia, 2019, Morris et al., 2024]. While technical work on alignment and safety is essential, it only succeeds insofar as we can ensure that those who control AI systems use them responsibly. Yet our current incentive structures - shaped heavily by short-term profit motives and often insufficiently guided by longer-term societal well-being [Szczepanski, 2019, Acemoglu and Restrepo, 2022, Zajko, 2022, Dhar, 2020, Wang et al., 2024] - hinder effective governance. Regulation often lags behind until an immediate negative event occurs. In light of the recent advancements in scaling LLMs, regulatory lag poses considerable societal risk [Hendrycks et al., 2023, Turchin and Denkenberger, 2020].\nImproving our institutional capacities-both for thoughtful regulation and for productive application of advanced AI-is crucial [Zhang et al., 2024]. Rapid, data-driven policy interventions are needed not only to mitigate existential risks but also to ensure that the coming shift to an AI-driven economy yields broadly shared benefits.\nWe believe one promising avenue is to harness and repurpose emerging AI methods especially \"foundation models\" to assist policymakers directly. As AI grows more capable, open-source models aligned with public welfare can help guide regulation and inform evidence-based policy decisions. By developing these tools transparently and collaboratively, we can avoid profit-driven biases and build trust with both policymakers and the public [Osterloh and Rota, 2004]. We have thus taken this approach, believing that progress made in the private sector can be turned towards public benefit, enabling timely, data-driven policies that safeguard vulnerable populations and the commons through the open-source and academic community.\nA critical component of this vision is the development of a specialized \u201ctime-series LLM.\u201d In our context, this refers to a large language model that integrates macroeconomic and microeconomic time-series data such as GDP, inflation rates, and legislative histories-with textual information like policy documents and news. Such a model can forecast economic indicators, predict policy impacts, and support scenario analysis with enhanced accuracy. By combining textual reasoning with temporal, quantitative forecasting [Liu et al., 2024b, Williams et al., 2024], we can obtain a richer understanding of how policy choices shape economic and social outcomes. This capability will be paired with a \"value elicitation\" mechanism to discern public preferences and an LLM-based public policy generator to propose policies aligned with these values.\nTogether, these components can strengthen the policymaking process and open new approaches toward effective, inclusive governance. We have structured ourselves as an open-source effort in pursuit of these capabilities and encourage all motivated or interested in navigating these challenges for collective benefit to participate; we warmly welcome additional contributors over the coming months."}, {"title": "Project Overview", "content": "Our initiative, led by Humanity Unleashed, a 501(c)3 nonprofit, unites numerous independent researchers-academics, students, and nonprofit staff - into a large-scale, open-source collaboration. By organizing the proposed research into multiple subprojects with more conventionally sized teams and supporting them through a nonprofit framework, we overcome the traditional coordination challenges faced by academia. This approach fosters rapid, iterative progress and supports the eventual integration of all subcomponents into a functional prototype - a cooperative Al policymaking platform capable of garnering public trust and support.\nThe integrated platform will feature the following components:\nEconomics Transformer: A multimodal time-series language model trained on macroeconomic and microeconomic data, coupled with legislative and event information, capable of forecasting economic outcomes and quantifying uncertainty. This goes beyond classical models (e.g., DSGE [Christiano et al., 2018]) and provides richer, data-driven insights on policy impacts.\nAI Legislator: A mechanism that first elicits user values via a carefully designed questionnaire and hierarchical Bayesian modeling and then generates policy proposals that reflect these values. This ensures output policies have broad, bipartisan appeal while considering nuanced public preferences.\nPolicy Interface: A unified, user-friendly interface that enables policymakers, researchers, and the public to interact with these tools. Through open-source release of code, datasets, and benchmarks, we invite the broader community to refine, extend, and evaluate these models.\nBy leveraging a nonprofit setting and academic collaboration, we align incentives toward public benefit rather than profit. Our 2024-2025 roadmap includes completing team organization, data gathering, model development, and iterative refinements, culminating in machine learning conference submissions in mid 2025 and the public release of our integrated tool in late 2025.\nIn summary, we aim to advance the state of AI-assisted policymaking. Our contributions will establish open-source baselines, benchmarks, and a principled method for integrating time-series economic forecasting with LLM-based value elicitation and policy generation. Ultimately, we seek to demonstrate a constructive, transparent, and inclusive approach to using AI in support of governance, social welfare, and long-term societal resilience.\nFor the remainder of the report, we provide an in-depth overview of each component pictured in Figure 1: the Economics Transformer (Section 3), the AI Legislator (Section 4), and the Policy Interface (Section 5). We conclude with discussing a subproject which will study the societal impact of LLMs within the United States economy in Section 6, concerning key implications of our project initiative."}, {"title": "Economics Transformer:\nEnhancing LLMs for Predicting Policy Impacts on Multivariate Time Series", "content": "The Economics Transformer is designed to predict multivariate time series in the context of a proposed policy or event given in the form of natural language. Central to our approach is leveraging existing LLMs by fine-tuning them to predict multivariate time series while retaining their language understanding capabilities. This project is divided into several subprojects-data collection, model architecture, scaling laws, and evaluation"}, {"title": "Data Collection for the Economic Transformer", "content": "This project aims to create a comprehensive, multimodal dataset encompassing numerical economic time-series and long-form textual policy data. By addressing the limitations of existing datasets, this work will enable innovative insights into policy evaluation and economic forecasting.\nEconomic research and policymaking rely on robust datasets to analyze trends, simulate interventions, and evaluate outcomes. However, we identify significant limitations within existing datasets:\n\u2022 Fragmentation: Numerical and textual data are often silo-ed, making it difficult to perform cross-modal analyses.\n\u2022 Limited Coverage: Many datasets lack generalization, instead focusing on specific regions, industries, or time periods.\n\u2022 Lack of Contextualization: Textual narratives and long-form policy documents, which provide critical context, are rarely integrated with numerical time-series data\nOur proposed dataset will fill these gaps by collecting, cleaning, and harmonizing economic time-series data across modalities, ensuring greater accessibility and usability. The significance of this work lies in its ability to bridge the gap between fragmented economic datasets and integrate them with textual policy datasets. This requires the dataset to have the following two key components.\nNumerical Economic Time-Series Data: This component will aggregate structured numerical data from publicly available sources such as central banks, government agencies, and international organizations. Examples include GDP, unemployment rates, inflation indices, trade balances, and market indicators.\n\u2022 Granularity: Inclusion of data at multiple levels (e.g., country, regional, and sectoral levels).\n\u2022 Historical Coverage: A focus on extending time horizons to provide historical context for long-term analysis.\nLong-Form Economic/Policy Textual Data: This component will focus on in-depth policy documents, white papers, and research reports."}, {"title": "Novel Architecture Explorations: Foundation Model for Multi-Modal Time Series Tasks", "content": "Project Summary This research project aims to develop an innovative foundation model for practical time series analysis, including forecasting Cao et al. [2020, 2021, 2023b], Niu et al. [2022, 2023], anomaly detection [Zhao et al., 2020], causal inference [Zhang et al., 2022, Cao et al., 2023a, Huang et al., 2024] and physics informed generation [Meng et al., 2022, Griesemer et al., 2024, Xiao et al., 2023], addressing critical limitations in current approaches. Towards this end, our proposal has two key directions: firstly, we will develop a novel fine-tuning approach that integrates multivariate time series analysis capabilities into existing large language models, effectively harnessing their innate ability to process and understand textual data. Specifically, we propose to modify Time-LLM [Jin et al., 2024], a model designed to handle time series with textual context, to perform multimodal, multivariate probabilistic forecasting. Secondly, we will propose Continuous-Valued Transformer (CVT) model which extends recent advancements in transformer architectures and diffusion models to handle continuous-valued time series data alongside textual information. By incorporating a novel Diffusion Loss function and adaptive temporal resolution mechanisms, the CVT model promises to capture complex temporal dependencies across various scales and modalities. The significance of this work lies in its potential to revolutionize time series analysis across multiple domains, enhancing decision-making processes in AI-driven policymaking platforms and improving forecasting accuracy in critical sectors.\nBackground and Objectives The primary question this project addresses is: How can we develop a foundation model for multivariate time series that effectively integrates with large pre-trained models while optimizing for accuracy, multimodal capabilities, uncertainty quantification, and counterfactual generation? This question is crucial due to the ubiquity of time series data across various domains and the increasing need for models that can handle complex, multimodal data [Jia et al., 2024]. In fields such as finance, healthcare, and climate science, the ability to accurately predict and interpret temporal patterns while leveraging textual information can lead to groundbreaking insights and improved decision-making. The question is both interesting and challenging due to several factors: aligning different data modalities without losing their specific characteristics, developing a tokenization method that preserves both global and local temporal information, capturing complex inter-temporal and cross-feature dependencies while maintaining computational efficiency, and balancing discrete time series representation with continuous modeling.\nSpecifically, time series foundation models face significant challenges in effectively representing complex, multi-dimensional temporal data while preserving both local and global information [Das et al., 2023, Woo et al., 2024, Talukder et al., 2024, Yue et al., 2022, Ansari et al., 2024, Rasul et al., 2024a]. The integration of time series analysis with textual information, inspired by the success of large language models (LLMs) such as LLAMA [Touvron et al., 2023] and ChatGPT-4 [OpenAI, 2023], presents additional technical challenges. Current approaches struggle to effectively process and combine these disparate data types while maintaining their unique characteristics. There is a need for novel multimodal architectures that can seamlessly handle both numerical time series and associated textual data. Such architectures must address several key technical requirements: (1) Developing a unified represen-tation that preserves the temporal structure of time series and the semantic content of text, (2) Designing attention mechanisms that can capture dependencies both within and across modalities, (3) Creating training objectives that balance the learning of temporal patterns and textual understanding, (4) Ensuring the model's ability to generalize across different time scales and data distributions. Addressing these technical challenges is essential for creating a versatile foundation model capable of advanced time series analysis in conjunction with textual data processing."}, {"title": "Research Direction 1: Fine-Tuning LLMs for Multimodal Ability", "content": "Time-LLM has demonstrated the effec-tiveness of reprogramming large language models for time series forecasting, outperforming traditional time series models in certain univariate forecasting tasks [Jin et al., 2024]. However, Time-LLM processes each variate dimen-sion individually during patch embedding, leading to a loss in cross-variate dependencies crucial for multivariate forecasting. Our research addresses this limitation by proposing two distinct approaches to modifying Time-LLM:\nI. Incorporating Cross-Variate Information in Patch Embedding: Similar to UniTST [Liu et al., 2024c], this approach combines patches into a 2D tensor that spans both temporal and variate dimensions. Linear projection with positional embeddings is applied on the tensor to generate 2D token embeddings, which are then flattened into a 1D token matrix compatible with Time-LLM's reprogramming layer.\nII. Capturing Cross-Variate Information During Reprogramming: Inspired by Crossformers [Zhang and Yan, 2023] and CARD [Xue et al., 2024], this approach replaces the cross-attention mechanism in the reprogramming layer with two-stage attention, which individually captures and accumulates temporal and cross-variate dependencies without modifying the patches."}, {"title": "Research Direction 2: Designing Time Series Multimodal Model", "content": "Our research aims to revolutionize the field of time series analysis by developing a state-of-the-art foundation model that addresses current limitations while leveraging the power of transformers and integrating textual information. The proposed approach builds upon recent advancements such as TEMPO [Cao et al., 2024a], TimeDiT [Cao et al., 2024b] and MAR [Li et al., 2024], extending their capabilities to create a more versatile and powerful model for time series processing. In this proposal, we propse the Continuous-Valued Transformer (CVT) to operate directly on the continuous time series domain. This approach overcomes the limitations of discrete tokenization methods, which have been a significant bottleneck in applying transformer architectures to time series data. The CVT employs a continuous embedding layer that maps input time series values to a high-dimensional continuous space, preserving the richness and nuance of the original data while enabling transformer-style processing. A key component of our model is the \"Diffusion Loss\" function, which replaces the traditional categorical cross-entropy loss used in discrete approaches. This loss function draws inspiration from diffusion models but is tailored for efficient training within the transformer framework. It models continuous probability distributions for each time step using a mixture of Gaussians, allowing for multi-modal predictions that capture the inherent uncertainty in probabilistic time series forecasting. To handle the complexities of multivariate time series, we implement an adaptive temporal resolution mechanism that allows the model to auto-matically adjust its focus on different time scales within the input sequence. This is particularly crucial for dealing with time series data that exhibit patterns at various frequencies, from high-frequency trading data to slow-moving economic indicators. Our multi-modal CVT model architecture features modality-specific encoders for time series and text, based on our enhanced TEMPO model and LLaMA (or similar large language models) respectively. The fusion encoder shares weights across modalities, further encouraging alignment in the shared embedding space.\nSpecifically, to effectively integrate language and time series data, we turn towards joint encoding techniques that map both modalities into a shared latent space; specifically, multimodal autoencoders with modality-specific encoders to transform language data x and time series data y into unified representations: $z= f_{lang}(x) = f_{func}(y)$. In this setup, $f_{lang}$ and $f_{func}$ are encoders for the language and time series data, respectively, both producing the same latent representation $z\\in R^{dz}$ [Ngiam et al., 2011]. This shared latent space captures common features and correlations between the modalities, enabling the model to learn intricate relationships between policy texts and economic indicators. To analyze the relationship of each modality's affect on the other, we will employ a cross-modal attention mechanism to allow the model to dynamically focus on relevant features across both modalities. Within the transformer architecture, we compute attention scores between modality-specific queries, keys, and values."}, {"title": "Scaling Laws for Multimodal Time-Series Language Models", "content": "Current work often treats time series data in isolation, leaving gaps in understanding how different modalities (like natural language and time series) impact model performance when integrated [Liu et al., 2024a]. The scaling goals are twofold: first, to establish how increasing data, compute, and model size influences performance in multimodal settings; and second, to examine how the proportion of different modalities within a dataset affects forecasting accuracy and optimal hyperparameters. Previous research [Aghajanyan et al., 2023] indicates that evaluation loss in multimodal models is influenced by the complexity of combining different datasets and the additional computational demands this entails. This relationship for two modalities can be expressed as\n$L(N,D_i,D_j) = A_{i,j} + \\frac{C_{i,j}}{N^{a_{i,j}}} + \\frac{B_{i,j}}{|D_i|+|D_j|^{b_{i,j}}}$,\nwhere $L(N, D_i, D_j)$ is the evaluation loss for model parameters N and datasets $D_i$ and $D_j$, $C_{i,j}$ represents the information gain from combining datasets, $A_{i,j}$ and $B_{i,j}$ are constants related to model complexity and data, and $a_{i,j}$ and $b_{i,j}$ are scaling exponents [Aghajanyan et al., 2023]. For time series models, the evaluation loss often follows a power-law scaling relationship, $L(A) = (\\frac{A}{A_0})^{-B_0}$, where A is the scaling factor (such as data size, compute, or model size), and $A_0$ and $B_0$ are normalization and fitting constants [Edwards et al., 2024]. To determine how these two metrics will interact, this project will use a transformer model [Vaswani et al., 2023] and various multimodal time series datasets like TIME-MMD [Liu et al., 2024a] and FRED [National"}, {"title": "DBITS: Dynamic Benchmarking of Indicator Time Series", "content": "Project Description. To inform policymakers to make better policies, we need better forecasts of economic indicators (e.g., GDP). A proposed solution for this is a multimodal time series forecasting transformer, but there is no way to dynamically evaluate this compared to more traditional models in real-time. This proposal aims to create a live benchmarking leaderboard for economic time series models. Having a live leaderboard with different evaluation filters and a history of relative performance to point out trends will help researchers develop better models for predicting economic indicators where changes in culture and politics can make a significant impact on trends in the data as better time series models for economic indicators can be made through ensemble methods such as bagging and boosting from having a reliable ranking of models in different contexts.\nThis continually updating live leaderboard will be realized by creating a new open-source hosted code repository that will automatically scrape the latest FRED-MD dataset in real time [McCracken and Ng, 2016], run the appropriate evaluations (evals) for all the models, and display their rankings on a continually updating basis with the option to select different evaluation filters. This solution will also have the ability to upload a model via uploading a script that the server can use to run evals.\nBackground and Technical Need The current state-of-the-art for time series leaderboards is the OpenTS leaderboard [Qiu et al., 2024] which benchmarks various time series models and serves as a great foundation for our own new live leaderboard as it also incorporates a static snapshot of the FRED-MD dataset. However, it does not update in real-time automatically [Qiu et al., 2024]. For the OpenTS leaderboard there is currently significant overhead for people looking to compare the performance of different time series models as they would have to setup their own copy of the leaderboard to get live results and/or add their own models [Qiu et al., 2024]. This requires both time and expertise in terms of software development as well as the necessary supporting infrastructure to host and deploy a local version of the backend. Another recent leaderboard is GIFT from Salesforce AI where they allow users to select different horizons for the ranking [Aksu et al., 2024]. However, they too like OpenTS are not updated as soon as the data is changed in real-time, do not show any history of the performance of different models and require that users themselves run the evaluations on models manually in order to update the leaderboard [Aksu et al., 2024].\nDevelopment Overview For this project, we focus on the development of a FRED-MD [McCracken and Ng, 2016] pipeline to serve as the foundation of a continual source of data to be used on a recurring basis for updated testing.\nTo provide a standardized input interface for models to integrate into the leaderboard platform, we developed a schema for each script meant to test a model's forecasting. In addition to basic metadata regarding model name, type, etc, it specifies the appropriate dimensions of data frame for input into training/testing the model and the appropriate dimensions of the forecast output. The purpose of this standardization is to provide a unified interface for model testing. To store the model evaluations, we utilized Supabase, an open source platform. Model evaluations, once generated, are uploaded, and stored for aggregation and comparison, which is displayed via the frontend.\nWe utilized Next.js and Tailwind CSS to construct a user accessible leaderboard that allows for comparison across models in a straightforward manner, incorporating all of the evaluation needs that we require. In particular, we were able to implement rolling window analysis. This in sum allows for visualizing relative performance of models compared to each other, and to fully encapsulate the performance metrics we had previously set out.\nMVP Implementation For our MVP, we implemented evaluations for forecasting on different horizons (F=12,24,36,48,60 months), a rolling forecasting strategy, and a look-back window of 96 months. For our non-foundation models, we make API calls to Nixtla's MLForecast [Nixtla, 2021] and Stats Forecast [Garza et al., 2022] libraries to train and forecast on our dataset.\nWe created a fully functional, MVP version of the dynamic leaderboard by implementing the frontend and the aforementioned steps, allowing comparison across models with different time horizons, and rolling forecast evaluations. We compare an initial set of 8 models: historical average, LightGBM [Ke et al., 2017], ETS [Hyndman and Athanasopoulos, 2021], TimesFM [Das et al., 2024], random forest [Breiman, 2001], nlinear [Zeng et al., 2023], linear, and linear regression approaches.\nNext Steps Based on a quick glance of our preliminary data, we have demonstrated variation across contexts (variation across time horizons for the models of interest and interesting differentiation for the various time horizon intervals) and seemingly significant trends in the relative performance of models over time in history, but we will have to do more formal statistical analysis to prove this significance."}, {"title": "AI Legislator:\nEliciting Human Preferences for Creation of AI-Generated Policies", "content": "The AI Legislator builds on the foundations laid by the Economic Transformer, integrating insights from multimodal time-series modeling and textual data analysis to understand the impacts of policy, establishing a framework for data-driven policymaking. The AI Legislator ensures that policy generation is grounded in both quantitative economic forecasting and qualitative stakeholder values. By simulating diverse perspectives, it seeks to bridge ideological divides and enhance effective governance."}, {"title": "Value Elicitation Framework for AI Policymaking: A Simulation-Based Approach", "content": "This project will develop a framework for eliciting human values at scale to inform AI-driven policymaking. It builds on Park et al. [2024] simulation of 1,052 individuals, achieving 85% accuracy in replicating their survey responses, and uses this simulated environment to refine methods for capturing human moral and political preferences. The framework applies hierarchical Bayesian modeling, Moral Foundations Theory [Graham et al., 2013], and optimal transport methods to represent and aggregate diverse values. By producing structured, empirically validated representations of values, the system can guide automated policy generators toward solutions that align with a broad range of stakeholder preferences.\nObjectives and Expected Significance\nEffective AI policymaking requires accurately representing and integrating human values into decision-making. Key challenges:\n\u2022 Capturing value heterogeneity across diverse populations at scale\n\u2022 Reconciling conflicting moral frameworks to produce actionable policy guidance\n\u2022 Translating elicited values into formal constraints that inform automated policy generation\nBackground and Technical Need\nTraditional methods for eliciting values, such as surveys or interviews, often face biases and cognitive overload, leading to inconsistent aggregate profiles. Park et al. [2024] addressed this by using generative agents to simulate human attitudes and behaviors with measurable fidelity, enabling systematic refinement of elicitation techniques. Our approach leverages the following key methodologies:\nStrategic Query Design: Aggregates value distributions into collective representations adaptively based on prior answers. Hierarchical Bayesian Modeling: Models values at global, domain-specific, and individual levels, refining accuracy with fewer queries. Moral Foundations Theory: Six core dimensions (Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression) provide a structured basis for queries and interpretation."}, {"title": "Strategic Query Design:", "content": "The system uses active learning to select queries that maximize information gain about users' moral foundations. Instead of generic questions, it employs targeted modalities such as binary policy choices (e.g., \"Do you prefer stringent AI oversight or a permissive innovation environment?\u201d), resource allocation tasks (e.g., \u201cDistribute 100 points among economic growth, equity, and AI safety\"), and moral dilemmas or scales (e.g., \"Should AI prioritize individual freedom over collective welfare? Likert 1-7 scale\").\nThese queries, linked to moral dimensions like Liberty/Oppression or Fairness/Cheating, enable efficient updates of user-specific value parameters. Over multiple iterations, the hierarchical Bayesian model converges on a stable moral profile for each user.\nHierarchical Bayesian Modeling: The model operates across three layers:\n\u2022 Global Layer: Captures universal moral dimensions as priors informed by Moral Foundations Theory [Graham et al., 2013].\n\u2022 Domain Layer: Adjusts these priors for specific policy contexts (e.g., data privacy in AI regulation)."}, {"title": "Policy Generation for AI Legislator", "content": "This project aims to build an LLM-based policy generator that assists policymakers in addressing complex issues us-ing simulated human perspectives for evaluation. It will take as input a policy issue or query along with a distribution over values from the Value Elicitation project, and produce actionable, contextually-informed policy recommenda-tions by leveraging inten decomposition, knowledge retrieval, simulated personas, and advanced language models.\nPolicy Generation Workflow.\n1. Policy Issue and Intent Decomposition: Policy issues retrieved from the frontend are translated into actionable goals via Intent Decomposition. A High-level policy intent H is parsed into actionable steps $A = Decompose(H,Context)$, where the context includes relevant data D retrieved from the Knowledge Base [Dzeparoska et al., 2023].\n2. Knowledge Base and Context Retriever: The Context Retriever sources relevant information from static and dynamic Knowledge Bases. This contextual injection ensures the Core LLM operates with domain-relevant, and up-to-date information.\n3. Core LLM & Policy Generation: Using pretrained models (40-mini, 40) prompt engineering, and in later phases Retrieval-Augmented Generation (RAG), the LLM integrates contextual data to draft policies, where the policy P is generated as $P=CoreLLM(Q,D)$, with Q representing the query and D the retrieved context.\n4. Policy Validation and Refinement: The policy P is then assessed for feasibility, realism, and bipartisan appeal. Specifically, there should be three phases here:\nPhase 1: Persona Querying assesses demographic support $S_p(P)$ and legislative feasibility $S_l(P)$ through predefined personas, where the total score is calculated as $S_{total}(P) = \\alpha S_p(P)+ \\beta S_l(P)$. Phase 2: Transition"}, {"title": "Infrastructure for an AI-Driven Policy Interface", "content": "Alongside model development to support policymaker decision-making, we aim to create a platform that enhances user accessibility to U.S. legislative data (Section 5.1). Interactions on this platform will then be integrated with the AI Legislator and Economics Transformer, enabling AI-driven policy generation and allowing users to have access to these inference capabilities (Section 5.2)."}, {"title": "Frontend Interface", "content": "Project Summary: Our methodology combines the principles of Human-Computer Interaction (HCI) and advanced AI techniques like natural language processing (NLP) to create a user-centric platform that enhances the accessibility of U.S. legislative data. We will take a comprehensive, phased approach consisting of a user-centric design analysis, HCI integrations with a prototype, iterative usability testing, usability evaluation, and deployment. This will guide the development and iterative refinement of our platform, ensuring it effectively meets user expectations, reduces barriers to civic engagement, and access to legislative information.\nObjectives and Expected Significance: This project addresses two crucial questions in civic technology:\n1. How can we democratize complex legislative information, making it easily accessible to the public?\n2. How can we leverage emerging generative AI capabilities to enhance and empower the democratic process instead of weakening it?\nBackground and Technical Needs: Current platforms like congress.gov Library of Congress [2024] and gov-track.us GovTrack.us [2024] offer valuable legislative data but the interfaces are overwhelming, hindering public engagement. The main challenges include poor user experience, limited AI-driven text simplification, and a lack of user-centered design. While past efforts have focused on backend data management, our approach applies recent HCI insights to redesign the presentation of legislative content. By leveraging NLP models, we aim to simplify bill language, making it clear and accessible, thereby filling a critical gap in civic engagement. Rezan and Karim [2022]\nResearch Description: Our project aims to modernize legislative platforms like congress.gov and govtrack.us by combining user-centered design principles, Human-Computer Interaction (HCI) methodologies, and AI-driven tools. The result will be a user-friendly, accessible interface that democratizes access to legislative information, simplifies complex language, and fosters civic engagement.\nPhase 1: User-Centered Design Analysis: Users that we envision finding this platform very useful may include policy think tanks, public policy students and engaged citizens of the United States. To ensure our platform addresses real user needs, we will begin with a thorough user-centered design analysis in collaboration with the product architecture team:\nPhase 2: HCI Integration and AI-Driven Text Simplification: Using insights from Phase 1, we will design an intuitive interface with Figma, following HCI principles and the GOMS model Harrison et al. [2007] to optimize user interactions. Key features likely include:\n\u2022 Value Elicitation Interface: Helps users understand where they lie on the political spectrum by answering a series of political questions.\n\u2022 Policy Generator: Allows users to input topics or upload files for AI-generated policy drafts.\n\u2022 Toolbar and Modular UI: Offers quick access to bill summaries, definitions, and translation tools. Pushpakumar et al. [2023]\n\u2022 Conversational Agent: Simplifies legislative language in real time, appealing to younger audiences. Kunigonyte and Kolev [2021] Myers and Rosson [1992]\n\u2022 NLP Simplification: Provides plain-language explanations of legal texts using advanced transformer models. Jens and Nyarko [2023]"}, {"title": "Backend Implementation", "content": "This project aims to develop a robust and scalable backend system that seamlessly integrates the Policy Frontend, AI Legislator and Economics Transformer components within the Humanity Unleashed platform. This integration aims to facilitate AI-driven policy generation by enabling efficient data exchange, processing, and real-time interactions among system components. Furthermore, we will collect and store user-provided demographics information, policy preferences, and policy proposals as training data for the AI Legislator.\nRelated Works: Related works to this project include: - govtrack.us, an open-source platform which tracks the activities of the US Congress, including bills and votes, legislators, and policies pertaining to personalized issues (GovTrack.us [2024]). - api.congress.gov, the official federal tool for obtaining bills in congress, their status, and legislator information (Library of Congress [2023]). - isidewith.com, a platform used by more than 80 million politically engaged citizens, which allows people to weigh in on political issues and show them which politicians share the most similar views (isidewith.com [2024]).\nAPI Design and Implementation: The primary task is to develop a standardized API enabling actions such as authentication, database queries, and model inference. A consistent API schema ensures interoperability across components like the Economics Transformer and AI Legislator, which may have distinct input/output formats and performance profiles Hmue et al. [2024]. Prior work in microservices architecture has shown that well-defined interfaces and asynchronous communication patterns can simplify integration and maintenance at scale. Such approaches reduce coupling and facilitate incremental upgrades without disrupting the entire system Nugroho et al. [2022]. The backend must process large volumes of policy data and support concurrent user interactions, necessitating efficient data handling, indexing, caching, and load balancing. As demonstrated by research in database design for microservices, NoSQL databases are effective in handling unstructured or semi-structured data at scale, ensuring rapid response times and robustness under heavy load Soni and Jyotinagar [2023].\nDatabase Implementation: The second critical responsibility is to implement a database to store user-related data. Optimizing the data pipeline is another challenge, as it must efficiently handle data ingestion, processing, storage, and retrieval. This includes managing multimodal inputs such as text and documents, both structured and unstruc-tured. The backend should provide mechanisms for data validation, transformation, and normalization to ensure consistency and accuracy. Data handling for multimodal inputs necessitates flexible storage solutions. NoSQL databases like MongoDB are preferred for their ability to handle unstructured data (Soni and Jyotinagar [2023])."}, {"title": "User and Data Security:", "content": "Security and transparency are also critical considerations. The backend must implement robust authentication and authorization mechanisms to protect sensitive policy data and user information. Ensuring data integrity and compliance with relevant regulations in our areas of operation is essential (Yu [2016", "2022": ".", "FastAPI": "The API gateway will be built using FastAPI", "Storage": "For data persistence, MongoDB will serve as the primary database due to its flexibility in handling unstructured and semi-structured data. The database schema will include collections for users, policies, economic data, legislative information, and logs. Policies will be stored with fields such as policy"}]}