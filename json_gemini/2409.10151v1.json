{"title": "AutoPET Challenge III: Testing the Robustness of\nGeneralized Dice Focal Loss trained 3D Residual UNet\nfor FDG and PSMA Lesion Segmentation from\nWhole-Body PET/CT Images", "authors": ["Shadab Ahamed"], "abstract": "Automated segmentation of cancerous lesions in PET/CT scans is a cru-\ncial first step in quantitative image analysis. However, training deep learning models\nfor segmentation with high accuracy is particularly challenging due to the varia-\ntions in lesion size, shape, and radiotracer uptake. These lesions can appear in dif-\nferent parts of the body, often near healthy organs that also exhibit considerable\nuptake, making the task even more complex. As a result, creating an effective seg-\nmentation model for routine PET/CT image analysis is challenging. In this study,\nwe utilized a 3D Residual UNet model and employed the Generalized Dice Focal\nLoss function to train the model on the AutoPET Challenge 2024 dataset. We con-\nducted a 5-fold cross-validation and used an average ensembling technique using\nthe models from the five folds. In the preliminary test phase for Task-1, the aver-\nage ensemble achieved a mean Dice Similarity Coefficient (DSC) of 0.6687, mean\nfalse negative volume (FNV) of 10.9522 ml and mean false positive volume (FPV)\n2.9684 ml. More details about the algorithm can be found on our GitHub repository:\nhttps://github.com/ahxmeds/autosegnet2024.git. The training code has been shared\nvia the repository: https://github.com/ahxmeds/autopet2024.git.", "sections": [{"title": "1 Introduction", "content": "PET/CT imaging is widely regarded as the gold standard in the management of cancer\npatients, providing accurate diagnoses, detailed staging, and critical insights for assessing\ntreatment response [1]. Traditionally, PET/CT images are evaluated qualitatively by radi-\nologists or nuclear medicine specialists. However, this method is subject to potential errors\ndue to variations in interpretation between different experts. By incorporating quantitative\nanalysis into the assessment of PET images, clinical decision-making could become more\nprecise, leading to better prognostic, diagnostic, and staging outcomes for patients under-\ngoing a range of therapeutic treatments [2, 3].\nManual segmentation of lesions from PET/CT images by experts is a labor-intensive\nprocess and is prone to variability both within and between observers [4, 14]. This variabil-\nity underscores the need for automation to make quantitative PET analysis more feasible in\neveryday clinical settings. Traditional automated methods, such as those based on threshold-\ning, often fail to detect lesions with low radiotracer uptake and can generate false positives"}, {"title": "2 Materials and methods", "content": ""}, {"title": "2.1 Data and data split", "content": "The training data consisted of 1014 FDG cases from 900 patients and 597 PSMA cases\nfrom 378 patients. The former set consisted of patients presenting lymphoma, lung cancer,\nmelanoma, and negative control patients, while the latter consisted of prostate cancer and\nnegative controls. The data was randomly split into 5 folds (we used the same 5 fold split\nprovided by the challenge organizers from the nnUNet-baseline). No other dataset (public\nor private) was used in this work."}, {"title": "2.2\nPreprocessing and data augmentation", "content": "The CT images were first downsampled to match the coordinates of their corresponding\nPET images. The PET intensity values in units of Bq/ml were decay-corrected and con-\nverted to SUV. During training, we employed a series of non-randomized and randomized\ntransforms to augment the input to the network. The non- randomized transforms included\n(i) clipping CT intensities in the range of [-1024, 1024] HU (ii) min-max normalization of\nclipped CT intensity to span the interval [0, 1], (iii) cropping the region outside the body in\nPET, CT, and mask images using a 3D bounding box, and (iv) resampling the PET, \u0421\u0422,\nand mask images to an isotropic voxel spacing of (2.0 mm, 2.0 mm, 2.0 mm) via bilinear"}, {"title": "2.3 Network", "content": "We used a 3D Residual UNet [10, 15], adapted from the MONAI library [11]. The network\nconsisted of 2 input channels, 2 output channels, and 5 layers of encoder and decoder (with\n2 residual units per block) paths with skip-connections. The data in the encoder was down-\nsampled using strided convolutions, while the decoder unsampled using transpose strided\nconvolutions. The number of channels in the encoder part from the top-most layer to the\nbottleneck were 32, 64, 128, 256, and 512. PReLU was used as the activation function within\nthe network. The network consisted of 19,223,525 trainable parameters."}, {"title": "2.4 Loss function, optimizer, and scheduler", "content": "We employed the binary Generalized Dice Focal Loss $L_{GDFL} = L_{GDL} + L_{FL}$, where $L_{GDL}$\nis the Generalized Dice Loss [12] and $L_{FL}$ is the Focal Loss [13]. The Generalized Dice Loss\n$L_{GDL}$ is given by,\n$L_{GDL} = 1 - \\frac{1}{n_b}\\sum_{i=1}^{n_b}\\frac{\\sum_{l=0}^{N^3}w_{il}\\sum_{j=1}^{N^3}p_{ilj} g_{ilj} + \\epsilon}{\\sum_{l=0}^{N^3}w_{il}\\sum_{j=1}^{N^3}(p_{ilj} + g_{ilj}) + \\eta}$ \n(1)\nwhere $p_{ilj}$ and $g_{ilj}$ are values of the $j^{th}$ voxel of the $i^{th}$ cropped patch of the predicted and\nground truth segmentation masks with class $l \\in {0,1}$ respectively in a mini-batch size $n_b$\nof the cropped patches and $N^3$ represents the total number of voxels in the cropped cubic\npatch of size (N, N, N), where N = 128. Here, $w_{il} = 1/(\\sum_{j=1}^{N^3} g_{ilj})^2$ represents the weight\ngiven to class I for the $i^{th}$ cropped patch in the batch. The mini-batch size was set to $n_b = 4$.\nSmall constants $e = \\eta = 10^{-5}$ were added to the numerator and denominator, respectively\nto ensure numerical stability during training. The Focal Loss $L_{FL}$ is given by,\n$L_{FL} = - \\frac{1}{n_b}\\sum_{i=1}^{n_b} \\frac{1}{N^3}\\sum_{l=0}^{1}\\sum_{j=1}^{N^3} v_l (1 - \\sigma(p_{ilj}))^{\\gamma}g_{ilj}log(\\sigma(p_{ilj}))$\n(2)\nwhere, $v_0 = 1$ and $v_1 = 100$ are the focal weights of the two classes, $\\sigma(x) = 1/(1+exp(-x))$\nis the sigmoid function, and $\\gamma = 2$ is the focal loss parameter that suppresses the loss for\nthe class that is easy to classify (1 = 0 or background class in our case)."}, {"title": "2.5 Inference and postprocessing", "content": "For the images in the validation set, we employed only the non-randomized transforms. The\nprediction was made directly on the 2-channel (PET and CT) whole-body images using\na sliding-window technique with a window of dimensions (192, 192, 192) and overlap=0.5.\nFor final testing, the outputs of the 5 best models (obtained from 5-folds training) were\nensembled via average ensembling to generate the output mask. The final output masks\nwere resampled to the coordinates of the original ground truth masks for computing the\nevaluation metrics."}, {"title": "2.6 Evaluation metrics", "content": "The challenge employed three evaluation metrics, namely the mean DSC, mean false positive\nvolume (FPV) and mean false negative volume (FNV). For a foreground ground truth\nmask G containing $L_g$ disconnected foreground segments (or lesions) {$G_1, G_2, \u2026\u2026,G_{L_g}$} and\nthe corresponding predicted foreground mask P with $L_p$ disconnected foreground segments\n{$P_1, P_2, ..., P_{L_p}$}, these metrics are defined as,\nDSC = $2\\frac{|G\\cap P|}{|G| + |P|}$ (3)\nFPV = $\\frac{v_p}{\\left|L_p\\right|}\\sum_{l=1}^{L_p}|P_l|\\delta(|P_l\\cap G|)$ (4)\nFNV = $\\frac{v_g}{\\left|L_g\\right|}\\sum_{l=1}^{L_g}|G_l|\\delta(|G_l \\cap P|)$ (5)\nwhere d(x) := 1 for x = 0 and d(x) := 0 otherwise. $v_g$ and $v_p$ represent the voxel volumes\n(in ml) for ground truth and predicted mask, respectively (with $v_p$ = $v_g$ since the predicted\nmask was resampled to the original ground truth coordinates). The submitted algorithms\nwere ranked separately for each of the three metrics and the final ranking was determined\nbased on the formula: 0.5 \u00d7 rankDSC + 0.25 \u00d7 rankFPV + 0.25 \u00d7 rankFNV. The function\ndefinitions for these metrics were obtained from the challenge GitHub page and can be\naccessed via this link."}, {"title": "3 Results and discussion", "content": ""}, {"title": "3.1\nFive-fold cross validation results and the preliminary test phase", "content": "We report the performance of the 5 models on the 5 validation folds in Table 1. We report\nthe mean values of the three metrics on the respective validation folds along with standard\ndeviation. On the preliminary test set, the average ensemble obtained DSC, FNV, and FPV\nof 0.6687, 10.9522 ml, and 2.9684 ml, respectively, as shown in Table 2."}, {"title": "3.2 Performance across FDG vs. PSMA cases", "content": "Fig. 1 shows the distribution of the 3 metrics for FDG and PSMA cases over all valida-\ntion folds. We can see that, in general, the performance of the networks on FDG cases were\nsuperior on the FDG cases, as compared to the PSMA cases. This is especially true for the\nDSC metric where the FDG cases had mean DSCs of 0.68\u00b10.23, 0.60\u00b10.28, 0.67\u00b10.23,"}, {"title": "4 Conclusion", "content": "In this study, we developed a deep Residual UNet model, trained using a 5-fold cross-\nvalidation approach, where we optimized a Generalized Dice Focal loss objective function to\nsegment lesions from a dataset consisting of FDG and PSMA radiotracers-based PET/CT\nimages. As a direction for future research, we plan to enhance the objective function by\nincorporating terms for False Positive Volume (FPV) and False Negative Volume (FNV) to\nmore effectively penalize predictions with high FPV or FNV, beyond the use of a Dice-based\nloss alone."}]}