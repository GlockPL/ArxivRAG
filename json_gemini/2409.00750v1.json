{"title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer", "authors": ["Yuancheng Wang", "Haoyue Zhan", "Liwei Liu", "Ruihong Zeng", "Haotian Guo", "Jiachen Zheng", "Qiang Zhang", "Shunsi Zhang", "Zhizheng Wu"], "abstract": "Nowadays, large-scale text-to-speech (TTS) systems are primarily divided into two\ntypes: autoregressive and non-autoregressive. The autoregressive systems have\ncertain deficiencies in robustness and cannot control speech duration. In contrast,\nnon-autoregressive systems require explicit prediction of phone-level duration,\nwhich may compromise their naturalness. We introduce the Masked Generative\nCodec Transformer (MaskGCT), a fully non-autoregressive model for TTS that\ndoes not require precise alignment information between text and speech. MaskGCT\nis a two-stage model: in the first stage, the model uses text to predict semantic\ntokens extracted from a speech self-supervised learning (SSL) model, and in the\nsecond stage, the model predicts acoustic tokens conditioned on these semantic to-\nkens. MaskGCT follows the mask-and-predict learning paradigm. During training,\nMaskGCT learns to predict masked semantic or acoustic tokens based on given\nconditions and prompts. During inference, the model generates tokens of a speci-\nfied length in a parallel manner. We scale MaskGCT to a large-scale multilingual\ndataset with 100K hours of in-the-wild speech. Our experiments demonstrate that\nMaskGCT achieves superior or competitive performance compared to state-of-the-\nart zero-shot TTS systems in terms of quality, similarity, and intelligibility while\noffering higher generation efficiency than diffusion-based or autoregressive TTS\nmodels. Audio samples are available at https://maskgct.github.io/.", "sections": [{"title": "Introduction", "content": "In recent years, large-scale zero-shot text-to-speech (TTS) systems [Kharitonov et al., 2023, Wang\net al., 2023, \u0141ajszczak et al., 2024, Kim et al., 2024, Peng et al., 2024, Anastassiou et al., 2024] have\nachieved significant improvements by scaling data and model size, including both autoregressive\n(AR) [Kharitonov et al., 2023, Wang et al., 2023, \u0141ajszczak et al., 2024, Kim et al., 2024, Peng\net al., 2024, Anastassiou et al., 2024, Shen et al., 2023, Ju et al., 2024, Le et al., 2024, Jiang et al.,\n2023] and non-autoregressive (NAR) models [Shen et al., 2023, Ju et al., 2024, Le et al., 2024, Jiang\net al., 2023]. However, these systems still exhibit some problems. Currently, AR-based TTS systems\ntypically quantize speech into discrete tokens and then use language models to sequentially generate\nthese tokens, which offer diverse prosody but also suffer from problems such as poor robustness and\nslow inference speed. NAR-based models, typically based on diffusion [Ju et al., 2024, Shen et al.,\n2023], flow matching [Le et al., 2024], or GAN [Jiang et al., 2023], require text and speech alignment\ninformation as well as the prediction of phone-level duration, resulting in a complex pipeline and\nproducing more standardized but less natural speech.\nRecently, masked generative transformers, a class of generative models, have achieved significant\nresults in the fields of image [Chang et al., 2022, 2023, Li et al., 2023a], video Yu et al. [2023a,b],\nand audio [Garcia et al., 2023, Li et al., 2024, Ziv et al., 2024] generation, demonstrating potential"}, {"title": "Related Work", "content": "Large-scale TTS. Traditional TTS systems [Ren et al., 2020, 2019, Tan et al., 2024, Wang et al.,\n2017, Kim et al., 2021] are trained to generate speech from a single speaker or multiple speakers\nusing hours of high-quality transcribed training data. Modern large-scale TTS systems [Kharitonov\net al., 2023, Wang et al., 2023, \u0141ajszczak et al., 2024, Kim et al., 2024, Peng et al., 2024, Anastassiou\net al., 2024] aim to achieve zero-shot TTS (synthesizing speech for unseen speakers with speech\nprompts) by scaling both the model and data size. These systems can be mainly divided into AR-"}, {"title": "Method", "content": "3.1 Background: Non-Autoregressive Masked Generative Transformer\nGiven a discrete representation sequence X of some data, we define $X_t = X \\odot M_t$ as the process of\nmasking a subset of tokens in X with the corresponding binary mask $M_t = [m_{t,i}]_{i=1}^N$. Specifically,\nthis involves replacing $x_i$ with a special [MASK] token if $m_{t,i} = 1$, and otherwise leaving $x_i$\nunmasked if $m_{t,i} = 0$. Here, each $m_{t,i}$ is independently and identically distributed according to a"}, {"title": "Model Overview", "content": "Following [Betker, 2023, Borsos et al., 2023, Wang et al., 2023], MaskGCT is a two-stage TTS\nsystem. The first stage uses text to predict speech semantic representation tokens, which contain most\ninformation of content and partial information of prosody. The second stage model is trained to learn\nmore acoustic information. Unlike previous works [Betker, 2023, Wang et al., 2023, Borsos et al.,\n2023, Kharitonov et al., 2023] use an autoregressive model for the first stage, MaskGCT utilizes\nthe non-autoregressive masked generative modeling paradigm for both the two stages without any\nalignment information between text and speech: (1) For the first stage model, we trained a model\nto learn $p_{\\theta_1}(S \\mid S_P, P)$, where S is the speech semantic representation token sequence obtained\nfrom a speech semantic representation codec (we introduce in 3.3), $S_P$ is the prompt semantic token\nsequence, and P is the text token sequence. $S_P$ and P are the condition for the first stage model. (2)\nThe second stage model is trained to learn $p_{\\theta_2}(A \\mid A_t, (A_P, S))$, where A is the multi-layer acoustic\ntoken sequence from a speech acoustic codec like [Zeghidour et al., 2021, D\u00e9fossez et al., 2022]. Our\nsecond stage model is similar to SoundStorm [Borsos et al., 2023]. The overview of MaskGCT is\npresented in Figure 1. We give more details about the four parts in the following sections."}, {"title": "Speech Semantic Representation Codec", "content": "Discrete speech representations can be divided into semantic tokens and acoustic tokens. Generally,\nsemantic tokens are obtained by discretizing features from speech self-supervised learning (SSL).\nPrevious two-stage, large-scale TTS systems [Betker, 2023, Borsos et al., 2023, Kharitonov et al.,\n2023] typically first use text to predict semantic tokens, and then employ another model to predict"}, {"title": "Speech Acoustic Codec", "content": "Speech acoustic codec is trained to quantize speech waveform to multi-layer discrete tokens while\naiming to preserve all the information of the speech as soon as possible. We follow the residual vector\nquantization (RVQ) method to compress the 24K sampling rate speech waveform into discrete tokens\nof 12 layers. The codebook size of each layer is 1,024 and the codebook dimension is 8. The model\narchitectures, discriminators, and training losses follow DAC [Kumar et al., 2024], except that we\nuse the Vocos [Siuzdak, 2023] architecture as the decoder for more efficient training and inference.\nFigure 5 shows the comparison between the semantic codec and acoustic codec."}, {"title": "Text-to-Semantic Model", "content": "Based on the previous discussion, we employ a non-autoregressive masked generative transformer to\ntrain a text-to-semantic (T2S) model, instead of using an autoregressive model or any text-to-speech\nalignment information. During training, we randomly extract a portion of the prefix of the semantic\ntoken sequence as the prompt, denoted as $S_P$. We then concatenate the text token sequence P\nwith $S_P$ to form the condition. We simply add $(P, S_P)$ as the prefix sequence to the input masked\nsemantic token sequence $S_t$ to leverage the in-context learning ability of language models. We use a\nLlama-style [Touvron et al., 2023] transformer as the backbone of our model, incorporating gated\nlinear units with GELU [Hendrycks and Gimpel, 2016] activation, rotation position encoding [Su\net al., 2024], etc., but replacing causal attention with bidirectional attention. We also use adaptive\nRMSNorm [Zhang and Sennrich, 2019], which accepts the time step t as the condition."}, {"title": "Semantic-to-Acoustic Model", "content": "We also train a semantic-to-acoustic (S2A) model using a masked generative codec transformer\nconditioned on the semantic tokens. Our semantic-to-acoustic model is based on SoundStorm"}, {"title": "Other Applications", "content": "MaskGCT can accomplish tasks beyond zero-shot TTS, such as emotion control, speech content\nediting, voice conversion, and duration controllable speech translation with simple modifications or\nthe assistance of external tools, demonstrating the potential of MaskGCT as a foundational model for\nspeech generation. We provide more details in Appendix G, H, I, J."}, {"title": "Experiments and Results", "content": "4.1 Experimental Settings\nDatasets. We use the Emilia [He et al., 2024] dataset to train our models. Emilia is a multilingual\nand diverse in-the-wild speech dataset designed for large-scale speech generation. In this work, we\nuse English and Chinese data from Emilia, each with 50K hours of speech (totaling 100K hours).\nWe evaluate our zero-shot TTS models with three benchmarks: (1) LibriSpeech [Panayotov et al.,\n2015] test-clean, a widely used test set for English zero-shot TTS. (2) SeedTTS test-en, a test set\nintroduced in Seed-TTS [Anastassiou et al., 2024] of samples extracted from English public corpora,\nspecifically, the set includes 1,000 samples from the Common Voice dataset [Ardila et al., 2019]. (3)\nSeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese public corpora,\nspecifically, the set includes 2,000 samples from the DiDiSpeech dataset [Guo et al., 2021].\nEvaluation Metrics. We use both objective and subjective metrics to evaluate our models. For\nthe objective metrics, we evaluate speaker similarity (SIM), robustness (WER), and speech quality\n(FSD). Specifically, for speaker similarity, we compute the cosine similarity between the WavLM"}, {"title": "Zero-Shot TTS", "content": "In this section, we show the main results of zero-shot TTS: we show comparison results with\nSOTA baselines in Section 4.2.1; we compare MaskGCT with replacing T2S model to an AR\nmodel in Section 4.2.2; We present the performance of MaskGCT across varying speech tempos in\nSection 4.2.3."}, {"title": "Comparison with Baselines", "content": "We compare MaskGCT with baselines in terms of similarity, robustness, and generation quality. The\nmain results are shown in Table 1. MaskGCT demonstrates excellent performance on all metrics\nand achieves human-level similarity, naturalness, and intelligibility. In similarity, MaskGCT's SIM\nand SMOS both outperform the best baseline, whether assessed using the total length of ground\ntruth or the predicted total duration (0.67\u21920.687 in LibriSpeech, 0.643\u21920.717 in SeedTTS test-\nen, 0.75\u21920.774 in SeedTTS test-zh for SIM; +0.016 in LibriSpeech, +0.762 in SeedTTS test-en,\n+0.550 in SeedTTS test-zh for SMOS). When compared with human recordings, MaskGCT achieves\nhuman-level similarity across all three test sets (+0.017, -0.002, and +0.027 for SIM respectively in\nthe three test sets, and +0.156, +0.319, and +0.244 for SMOS respectively in the three test sets). In\nrobustness, MaskGCT likewise results nearly on par with ground truth (with 2.634, 2.623, 2.273 WER\non LibriSpeech, SeedTTS test-en, and SeedTTS test-zh, respectively), exhibiting enhanced robustness\ncompared to AR-based models and performing on par or better than NAR-based models such as\nVoiceBox and NaturalSpeech 3, without relying on phone-level duration predictions. In generation"}, {"title": "Autoregressive vs. Masked Generative Models", "content": "We compare MaskGCT to replacing T2S MaskGCT with an AR T2S model (which we call AR +\nSoundStorm). Table 2 shows the performance of these two models on all three test sets. MaskGCT\ndemonstrates improved similarity, robustness, and CMOS (+0.126 on LibriSpeech test-clean, +0.051\non SeedTTS test-en, and +0.323 on SeedTTS test-zh) across all three test sets. We also conducted\ncomparisons on more challenging hard cases (such as repeating words, and tongue twisters, which are\noften considered as samples where TTS systems are prone to hallucinations). MaskGCT exhibited a\nmore pronounced robustness advantage in these scenarios. See details in the Appendix K. In addition,\ncompared to AR-based models, MaskGCT offers the capability to control the total duration of the\ngenerated speech, along with fewer inference steps, requiring only 25 to 50 steps for T2S models\nto achieve optimal results for speeches of any length. Conversely, the inference steps for AR-based\nmodels increase linearly with the length of the speech."}, {"title": "Duration Length Analysis", "content": "We analyze the robustness of the generated results of MaskGCT under different changes in total\nduration length (which can also be regarded as changes in speech tempo). The results are shown in\nFigure 3. We explore the results of multiplying the ground truth total duration by 0.7 to 1.3. The\nresults show that the lowest WER is achieved at a total duration multiplier of 1.0, indicating that the\nmodels perform best when the speech is played at its natural speed. When the multiplier is 0.9 or 1.1,\nthe model is still able to achieve a WER very close to the best. When the multiplier is 0.7 or 1.3, the\nWER is slightly higher but still within a reasonable range. This shows that our model can generate\nreasonable and accurate content at different speech tempos."}, {"title": "Ablation Study", "content": "Inference Timesteps. Figure 4 shows the relationship between inference steps and metrics SIM and\nWER for SeedTTS test-zh (left) and test-en (right). Initially, SIM increases significantly, stabilizing\nafter 25 steps. For test-zh, SIM rises from 0.761 at 5 steps to 0.771 at 75 steps, and for test-en, from\n0.696 to 0.715. SIM reaches high values with just 10 steps but peaks around 25 steps. WER improves\nmore dramatically, especially up to 25 steps. For test-zh, WER drops from 10.19 at 5 steps to 2.507 at\n25 steps, and for test-en, from 8.096 to 2.346. Both SIM and WER show minimal changes beyond 25\nsteps. These findings indicate that while SIM metrics can be sufficiently optimized with around 10\ninference steps, achieving the lowest WER values requires approximately 25 inference steps. Beyond\nthis threshold, both SIM and WER metrics exhibit minimal changes, implying that further increases\nin inference steps do not yield substantial improvements in these performance metrics. Therefore, for\npractical applications, 25 inference steps may be considered optimal for balancing SIM and WER,\nensuring efficient and effective performance.\nModel Size. We compare the performance differences of T2S models with varying model sizes. The\nresult is shown in Table 3. We observe that the large model outperforms the base model across all\nmetrics, albeit not significantly. We suggest that our system can achieve good performance with just\nthe setting of the base model when using 100K hours of data. In the future, we will explore more\ncomprehensive scaling laws for both model size and data scaling."}, {"title": "Text Tokenizer", "content": "We compare two different methods of text tokenization: (1) Grapheme-to-\nPhoneme(G2P), which converts text into a sequence of phonemes; (2) Byte Pair Encoding (BPE).\nSee more details for the methods of text tokenization in Appendix D. The results indicate that G2P\noutperforms BPE in English with a higher similarity score (SIM) of 0.728 compared to 0.711 and\na lower Word Error Rate (WER) of 2.466 versus 4.036. Conversely, in Chinese, G2P maintains a\nslightly higher similarity score (0.777 vs. 0.769) but BPE achieves a lower WER (1.921 vs. 2.338).\nThese findings suggest that while G2P is superior in preserving text similarity and reducing errors in\nEnglish, BPE is more effective in minimizing WER in Chinese. We hypothesize that the reason might\nbe that the Chinese G2P system we used still has deficiencies in handling polyphonic characters,\nwhereas BPE is capable of learning different pronunciations for the same character based on context."}, {"title": "Conclusion", "content": "In this paper, we present MaskGCT, a large-scale zero-shot TTS system that leverages fully non-\nautoregressive masked generative codec transformers while not requiring precise alignment informa-\ntion between text and speech. MaskGCT achieves high-quality text-to-speech synthesis using text\nto predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and then\npredicting acoustic tokens conditioned on these semantic tokens. Our experiments demonstrate that\nMaskGCT outperforms the state-of-the-art TTS system on speech quality, similarity, and intelligibility\nwith scaled model size and training data, and MaskGCT can control the total duration of generated\nspeech. We also explore the scalability of MaskGCT in tasks such as speech translation, voice\nconversion, emotion control, and speech content editing, demonstrating the potential of MaskGCT as\na foundational model for speech generation."}, {"title": "Post-Training for Emotion Control", "content": "MaskGCT can unlock more extensive capabilities with post-training. We take emotion control as\nan example. After being pretrained on a large-scale dataset, we fine-tune the T2S model by adding\nan additional emotion label as a prefix to the original input sequence. We use a small emotion\ndataset, ESD [Zhou et al., 2021], which consists of 350 parallel utterances with an average duration\nof 2.9 seconds spoken by 10 native English and 10 native Mandarin speakers, to fine-tune our model.\nThe experimental results show that MaskGCT can unlock emotion control capabilities for zero-shot\nin-context learning scenarios. For the construction of the train and test datasets, we selected one\nmale and one female speaker each from native English and native Mandarin backgrounds, resulting\nin a total of four speakers for the test dataset. The remaining 16 speakers were allocated to the\ntrain dataset. For the 350 parallel Chinese utterances, we randomly chose 22 utterances for the test\nset, with the remaining utterances designated for training. Similarly, for the 350 parallel English\nutterances, we randomly selected 21 utterances for the test set, with the rest used for training. To\nassess the consistency between the generated audio and the target emotion label, we trained an\nemotion classification model using the constructed train dataset. This model achieved a classification\naccuracy of 72% on the test dataset. We show some examples in our demo page."}, {"title": "Speech Editing", "content": "Based on the mask-and-predict mechanism, our text-to-semantic model supports zero-shot speech\ncontent editing with the assistance of a text-speech aligner. By using the aligner, we can identify the\nediting boundary of the original semantic token sequence, mask the portion that needs to be edited,\nand then predict the masked semantic tokens using the edited text and the unmasked semantic tokens.\nHowever, we have observed that our system is not very robust in editing tasks. A possible conjecture\nis that we need to adopt a training paradigm better suited for editing tasks, such as fill-in-mask [Le\net al., 2024, Du et al., 2024b]. We show some examples in our demo page."}, {"title": "Voice Conversion", "content": "MaskGCT supports zero-shot voice conversion by fine-tuning the S2A with a modified training\nstrategy. The zero-shot voice conversion task aims to alter the source speech to sound like that of\na target speaker using a reference speech from the target speaker, without changing the semantic\ncontent. We can directly use the semantic tokens $S_{src}$ extracted from the source speech and the prompt\nacoustic tokens $A_{ref}$ extracted from the reference speech to predict the target acoustic tokens $A_{tgt}$.\nSince $S_{src}$ may retain some timbre information, we perform timbral perturbation on the semantic\nfeatures input to the semantic codec encoder. Specifically, we apply timbral perturbation to the input\nmel-spectrogram features of the W2v-BERT 2.0 model, following the method outlined in FreeVC [Li\net al., 2023b]. We fine-tune our S2A model using this training strategy. We show some examples in\nour demo page."}, {"title": "Duration Controllable Speech Translation", "content": "The goal of the speech translation task is to translate speech from one language to another while\npreserving the original semantic, timbre, and prosody. In some scenarios, we also need to ensure that\nthe total duration remains relatively unchanged, such as in video translation. Our model can achieve\nthis seamlessly, with the ability to control the total duration and, through in-context learning, use the\npre-translation speech as a prompt to maintain the timbre and prosody. We show some examples of\nspeech translation between Chinese and English in our demo page."}, {"title": "Hard Cases Evaluation", "content": "We evaluate the performance of MaskGCT on some hard cases (SeedTTS test-hard), which refer\nto instances where large-scale TTS models, particularly those AR-based models, often exhibit\nhallucinations. These cases include phrases with repeating words, tongue twisters, and other complex\nlinguistic structures. Examples of such cases include: \u201cthe great greek grape growers grow great"}, {"title": "Discussion about Concurrent Works", "content": "SimpleSpeech [Yang et al., 2024], DiTTo-TTS [Lee et al., 2024], and E2 TTS [Eskimez et al., 2024]\nare also NAR-based models that do not necessitate precise alignment information between text and\nspeech, nor do they forecast phoneme-level duration. These are concurrent works with MaskGCT.\nThe three models all employ diffusion modeling on speech representations within continuous spaces.\nSimpleSpeech models the latent representation of a wav codec based on finite scalar quantization\n(FSQ) [Mentzer et al., 2023], DiTTo-TTS utilizes the latent representation of a wav codec based\non residual vector quantization (RVQ), and E2 TTS directly models the mel-spectrogram with flow\nmatching."}, {"title": "Boarder Impact", "content": "Given that our model can synthesize speech with high speaker similarity, it carries potential risks of\nmisuse, including spoofing voice identification or impersonating specific speakers. Our experiments\nwere conducted under the assumption that the user consents to be the target speaker for speech\nsynthesis. To mitigate misuse, it is essential to develop a robust model for detecting synthesized\nspeech and to establish a system for reporting suspected misuse."}]}