{"title": "Uncovering Uncertainty in Transformer Inference", "authors": ["Greyson Brothers", "Willa Mannering", "Amber Tien", "John Winder"], "abstract": "We explore the Iterative Inference Hypothesis (IIH) within the context of transformer-based language models, aiming to understand how a model's latent representations are progressively refined and whether observable differences are present between correct and incorrect generations. Our findings provide empirical support for the IIH, showing that the nth token embedding in the residual stream follows a trajectory of decreasing loss. Additionally, we observe that the rate at which residual embeddings converge to a stable output representation reflects un-certainty in the token generation process. Finally, we introduce a method utilizing cross-entropy to detect this uncertainty and demonstrate its potential to distinguish between correct and incorrect token generations on a dataset of idioms.", "sections": [{"title": "1 Introduction and Related Work", "content": "Transformer-based architectures [13] currently dominate artificial intelligence applications and serve as the underlying architecture for most Large Language Models (LLMs). While LLMs show impressive emergent abilities, these models exhibit limitations such as hallucinations and biased outputs which pose significant societal challenges [10]. Inaccurate outputs can mislead users, while malicious actors can exploit AI models to create deceptive images, videos, and text to represent fictional occurrences as truth. Mitigating harms caused by model misuse, biased outputs, or misalignment with human values is a primary motivation behind research and policy decisions related to AI interpretability [2].\nIn this work we investigate a novel method for detecting uncertainty during the token generation process of transformer-based language models. One framework that has emerged for understanding the feed-forward behavior of residual models, such as the transformer, is the Iterative Inference Hypothesis (IIH) [1, 4, 8]. This hypothesis posits that predictions are formed in the residual stream, and that each block in a residual architecture incrementally updates these predictions in a direction of decreasing loss [7]. A related line of research on in-context learning has suggested that transformers trained on autoregressive tasks are closely related to formulations of iterative optimization algorithms, namely gradient descent [14].\nWe combine these two threads of research, framing transformer inference as an optimization process that iteratively updates the nth input embedding (i.e. the last word in the input sequence) to converge toward the most likely next-token embedding given the context and model weights. We propose methods for evaluating how input embeddings evolve towards output embeddings and find preliminary"}, {"title": "2 Methods", "content": "Here we define methods that offer insight into how representations in the residual stream evolve during inference. The transformer architecture, excluding token embedding and unembedding, can be succinctly represented by the following recurrence relation: $r_{i+1} = r_i + L_{i+1}(r_i)$, where $r_i = [e_i, ..., e_n]$ represents the set of token embeddings in the residual stream after an update from layer $L_i$. Each layer contains attention and feed-forward sublayers. With $r_0$ as the set of input token embeddings plus positional encodings, the residual stream is the sequence $(r_0, r_1, ..., r_k)$ for a model with k layers. In this work we are particularly interested in tracking the evolution of the embedding of the nth input token, $e_n$, as shown in red in Figure 1. Its residual representation after the final layer update, $e_n^k$, is used to predict the next token in an autoregressive framework.\nIn the convolutional architecture of the original residual network (ResNet), the residual stream contained shortcut connections that transformed the basis of latent space in order to reduce its dimensionality at regular intervals [5]. However, in the transformer, the residual stream has an entirely linear structure that preserves the basis of the embedding space [3]. Each layer adds an output to each residual embedding, corresponding to translations in token embedding space. Thus the residual stream can be viewed as a path through token embedding space, and each point along the path can be mapped back to a distribution over tokens via methods like logit lens [9]. To get the intermediate distribution predicted by the residual stream after layer $L_i$, we pass $e_n^i$ through the output layer norm and then to the model's output head to obtain logits over the vocabulary. We refer to these logits as residual predictions. This framework forms the basis of our methods and analysis."}, {"title": "2.1 Residual Cross-Entropy", "content": "We present a method by which cross-entropy can be used to measure the evolution of residual predictions during token generation. The IIH posits that the output of each layer updates the residual prediction in a direction of decreasing loss. We thus examine per-layer changes in cross-entropy, the loss function used to train transformers on autoregressive language tasks [6], including the specific model we examine here (GPT-2 XL) [11].\nCross-entropy is a measure of dissimilarity, requiring a candidate and a target distribution. We use the residual predictions described above as the candidate, and for the target distribution we examine two choices: (1) the one-hot distribution of the token sampled deterministically by taking the argmax the model's predicted probabilities, denoted $\\hat{y}$, and (2) the one-hot distribution of the ground truth next token given by the dataset, denoted y. These measures are equivalent to the negative log likelihood of the sampled and target tokens respectively, as elaborated in A.1. When the sampled output is correct, or $\\hat{y} = y$, these two measures are the same. Examining how layer updates affect cross-entropy with"}, {"title": "2.2 Model", "content": "To investigate how representations evolve in the residual stream we examine GPT-2 XL [11], which has 48 layers and 1.5 billion parameters. We chose this model as it has low compute requirements while still being representative of frontier model architectures. In addition, it has widely used open source implementations and official weights made publicly available by OpenAI on HuggingFace\u00b9."}, {"title": "2.3 Inference Data", "content": "For our preliminary study of the IIH and model uncertainty, we chose a simple dataset with a wide variety of generation difficulty: English idiom completion. An idiom completion task provides a relatively clear-cut, single token \"correct\" and \"incorrect\" answer, making it straightforward to evaluate. This dataset is intentionally challenging, where some answers would be hard or nearly impossible to guess given the input context or brevity of the idiom.\nThe idiom dataset consists of 330 static idioms taken from the EPIE Dataset [12]. To build our dataset, we split each static idiom so that the final word serves as the \"correct\" output for the model. To guide the model in completing the idiom, we added instructions to the start of the idiom phrase. The instructions read: \"The following prompt is the beginning of a popular English idiom, please respond with a single word to complete the phrase.\" Thus, each prompt in this dataset consists of the instructions + the first words of an idiom. We excluded 29 idioms from the EPIE dataset because the target outputs were represented by more than one token in the vocabulary, such as [\"help\", \"ful\"]."}, {"title": "3 Results", "content": "In this section, we examine the evolution of residual representations in GPT-2 XL on the idiom inference dataset by analyzing the per-layer change in cross-entropy. Our main objectives are, first, to evaluate whether there is evidence supporting the IIH, and second, to determine if these metrics reveal a noticeable difference between correct and incorrect output distributions that could aid in developing a measure of uncertainty for a model's predictions."}, {"title": "4 Conclusion", "content": "In this work we investigate the Iterative Inference Hypothesis as applied to the transformer architecture on autoregressive language modeling problems. We provide a mechanism for investigating the evolution of predictions in the residual stream and find empirical evidence to support the hypothesis. In addition, we propose a novel method for observing uncertainty during the token generation process by measuring the cross-entropy between the model output logits and a one-hot distribution representing the deterministically sampled token $\\hat{y}$. Using this metric we find distinct differences between distributions of correct and incorrect generations on an idiom dataset, and we observe that this output cross-entropy appears to correspond with model uncertainty given a prompt.\nFuture work will aim to address limitations of this preliminary study by expanding our analysis to a broad range of datasets and language models of varying sizes. We will additionally extend our study to multi-token generations and explore the use of output cross-entropy as an uncertainty measure and potential flag for hallucinations. Finally, we intend to explore additional convergence metrics that may better predict correct versus incorrect generations, then examine how broadly applicable they are across models and datasets. The ultimate goal of this research is to develop methods for assuring the quality of language model output with minimal computational cost."}, {"title": "A.1 Cross-Entropy Measurement", "content": "Given two discrete probability distributions p and q over support X, the cross-entropy H of q relative to p is defined as\n$H(p,q) = - \\sum_{x \\in X} p(x) \\log q(x)$\nIn our case, X represents the vocabulary of tokens, q represents the probabilities predicted by the model over X, and p represents some target distribution over X. For our methods, we take p to be a one-hot encoding representing either the sampled token $\\hat{y}$ or the ground truth token y. With a one-hot encoding for p, the above summation yields only one non-zero term\n$H(p,q) = -\\log q(x^*)$\nwhere $x^* = y$ or $\\hat{y}$ depending on our choice of target. This is also the negative log likelihood of token $x^*$ as predicted by the model. Our cross-entropy experiments can be equivalently framed as measuring how the negative log likelihood of a target token changes throughout the inference process."}, {"title": "A.2 Best and Worst Case Sample Generations", "content": "We present the residual predictions for the generations with the highest and lowest output cross-entropy scores vs model predictions $\\hat{y}$ on the idiom dataset in Figure 5. These correspond to left- and right-most samples on the cross-entropy axis in Figure 3a. We use the logit lens technique to recover token predictions from the residual stream after each layer update as described in 2.\nMany of the idiom prompts are one word and are extremely difficult to complete, even for humans. This open-endedness lends itself to high uncertainty, which is captured by the cross-entropy as shown here. The samples with the lowest cross-entropy have prompts with multiple words that heavily imply a specific next-token, severely constraining the set of possible next-tokens. In contrast, samples with the highest cross-entropy have short prompts with very common words that are could have many valid next tokens, resulting in a very open-ended generation task. We observe output cross-entropy reflecting the open-endedness of the prompt via the corresponding uncertainty from the model."}, {"title": "A.3 Choice of Divergence Target", "content": "Here we compare KL divergence between residual predictions and model output logits versus residual predictions and a one-hot encoding of the top predicted logit. Note that KL divergence and cross-entropy between two distributions differs only by a constant. By definition, the final residual prediction is equivalent to the model output logits, thus the KL divergence approaches zero for all generations, as observed in Figure 6a below. If the output logits of the model exhibit high entropy, then they will have a higher divergence when measured against to a one-hot representation of the top predicted logit. This can be observed in Figure 6b. We find this bias useful for distinguishing correct and incorrect generations."}, {"title": "A.4 Idiom Dataset Loss Table", "content": "Figure 7 below shows average change in loss after each layer update to predictions in the residual stream for the idiom dataset. White cells denote no change, blue cells denote a decrease, and red cells denote an increase. Nearly all layer updates move the residual prediction in a direction of decreasing loss, supporting the IIH. Layers 20-38 appear to contribute the most to reducing residual prediction loss for correct generations."}, {"title": "A.5 Idiom Dataset Cosine Similarity", "content": "We also measure how representations change directly in token embedding space by measuring the cosine similarity between each intermediate embedding $e_n^i$ and embeddings of the two choices of targets described in 2.1."}]}