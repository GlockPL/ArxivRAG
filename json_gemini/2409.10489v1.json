{"title": "Flash STU: Fast Spectral Transform Units", "authors": ["Y. Isabel Liu", "Windsor Nguyen", "Yagiz Devre", "Evan Dogariu", "Anirudha Majumdar", "Elad Hazan"], "abstract": "This paper describes an efficient, open source PyTorch implementation of the Spectral Transform Unit [1]. We investigate sequence prediction tasks over several modalities including language, robotics, and simulated dynamical systems. We find that for the same parameter count, the STU and its variants outperform the Transformer as well as other leading state space models across various modalities.", "sections": [{"title": "Introduction", "content": "The Spectral Transform Unit was recently proposed in [1] based on the spectral filtering technique of [16]. This neural network architectural unit is motivated by state space models for linear dynamical systems. The key innovation of spectral state space models lies in their use of fixed convolutional filters which do not require learning. This structure offers significant robustness in theory as the performance of the model is not influenced by the spectrum of the underlying dynamics nor the dimensionality of the problem, making it suitable for tasks that require long-term memory.\nIn this paper we describe an open source PyTorch implementation of the STU and experiments as well as ablation studies to understand its properties. We study several sequence prediction problems across various modalities, including synthetic time series generated from linear dynamical systems, robotics control sequences, and natural language sequences."}, {"title": "Description of the Spectral Transform Unit", "content": "In the STU architecture, the schematic of which is given in Figure 1, the output is generated as a transformation of the input sequence that involves (optional) lifting of the input dimension by a learned transformation to a higher dimension, convolution with a set of fixed filters (i.e. spectral filtering), projection with a set of learned parameters, and (optional) learned nonlinearities. We can thus write\n\n\u0177t = \u03c3( \u2211i=1k Mi (\u03a6i, Ut:t-L) ),\n\nwhere Mi are fixed projections, o is a nonlinearity, and \u03a61:k are k fixed filters that can be computed a-priori, and for simplicity we don't explicitly write the lifting in the mathematical expression. The filters \u03a61:k are the eigenvectors"}, {"title": "Experiments with synthetic data", "content": "We begin our investigation of STU phenomena with some simple yet representative synthetic tasks that have become commonplace in the sequence modeling literature. In particular, we aim to understand the behavior of the STU in environments with long memory and nonlinearities, especially as we introduce feed-forward layers and deeper architectures. We compare against S4 [14], a standard SSM architecture, as well as the vanilla transformer layer [33]."}, {"title": "Linear dynamical systems", "content": "In a linear dynamical system (LDS) the outputs {yt} of a sequence are generated according to the dynamics equations\n\nXt+1 = Axt + But, Yt = Cxt + Dut,\n\nwhere {ut} is an (observed) input sequence and {xt+1} are hidden states of the system. The matrices A, B, C, D are called \"system matrices\" and are unknown to the sequence predictor. Typical approaches to learning in this setting scale in complexity with hidden state dimension dhidden and effective system memory 1/\u03b4, where p(A) = 1 \u2212 \u03b4 is the spectral radius of A. For an in-depth treatment of linear dynamical systems as well as methods to learn and predict them, see the text [15].\nThe task of sequence prediction in this case is to predict \u0177t+1 given all the previous inputs and outputs U1:t, Y1:t and to minimize the loss vs. the actual output Yt+1. We evaluate the mean squared error of the STU's predictions vs. other methods where the sequence was generated by a linear dynamical system."}, {"title": "Optimization behavior", "content": "We saw on the linear dynamical system that the STU layers seem to have a comparatively easier time optimizing. This is expected since a single STU layer under MSE loss is a convex parameterization, whereas the losses of an S4 and an attention model are nonconvex. Following [20], we choose two random directions in the high-dimensional space, move along these directions by varying amounts x_step and y_step, compute the loss of the model with perturbed parameters for each coordinate pair in these directions, and plot the loss values as heights on a 2D grid. By doing this,"}, {"title": "Other synthetic tasks", "content": "We also investigate the performance of the STU layer in synthetic tasks that are more well-known in the deep learning literature. We briefly introduce the induction heads [13] and associative recall [11] tasks, our experimental setups, and the corresponding results here. For completeness, we include in Appendix B the results on other tasks including selective copy [3] and a novel mode prediction task that may be independently useful.\nIn the induction heads task, the model is required to memorize one token (sampled uniformly from a vocabulary) immediately after a special flag token; the rest of the context consists of the same special blank token, which the model should learn to ignore. The associative recall task is slightly different, as it first gives the model an (unordered) sequence of key-value pairs and then asks for the value associated with a query key after \u2013 the model must memorize the entire context, not just a single token. Both of these tasks have nonlinear dynamics and require single-token precision to fully solve; furthermore, since we apply deeper models, the optimizations are nonconvex for all the models considered.\nExperiment details and conclusions. We trained two-layer models (with MLP layers in between) using the cross- entropy loss, the Adam optimizer with default PyTorch parameters, and batches of size 32. We set the context length to 32 and all the model widths to 32. In addition, the attention model has 8 heads and the S4 model has hidden dimension 64. For induction heads, the vocabulary size is 4. We plot the accuracies during training for induction heads and selective copy in Figures 6 and 7, respectively, averaged over 8 trials with error bars.\nAs we can see, on both tasks the STU is able to solve the task quickly and consistently. By contrast, on the induction heads task we see that STU-T and the attention and S4 baselines appear to learn in stages, with the accuracy plateauing occasionally. Furthermore, there is significant variation across seeds for which timestep the attention model begins to solve the task. Both of these observations point to the difficulty of the underlying optimization problem: while all models considered are expressive enough to solve induction heads, the STU appears to have an easier time finding such a solution.\nOn the associative recall task, we see a similar story. The SSM models all converge very quickly, while the attention- based model requires more optimization steps and learns more gradually. These results are particularly surprising considering that the tasks involved feel difficult to approximate by linear dynamics, so one might have expected the nonlinear, non-dynamical structure of softmax attention to dominate here."}, {"title": "Experiments with robotics sequence prediction", "content": "We now consider next-state prediction using the MuJoCo physics engine [32], a more challenging sequence prediction task. The goal is to learn the dynamics of a certain physical simulation agent, for example the Ant-v1 system,\n\nXt+1 = f(xt, ut),\n\nwhere xt \u2208 R29 and ut \u2208 R8 correspond to the state and action at time t, respectively.\nMore precisely, xt corresponds to the positions, orientations, joint angles, and velocities of the various limbs of the Ant-v1 controller, whereas ut represents the torques applied to the joints of the Ant-v1 controller and are generated from a pretrained proximal policy optimization [28]. Unlike the synthetic linear dynamical system in Section 2, the dynamics f for this particular MuJoCo prediction task are nonlinear and hybrid, i.e. non-smooth.\nIn the experiments below, we defined the loss function to be the squared Euclidean distance between the predicted state vector 2t+1 and the true state vector Xt+1\n\nL(\u03b8) = 1/2 ||2t+1(\u03b8) - Xt+1||2 = 1/2 \u2211i=1n (Xt+1,i (\u03b8) - Xt+1,i)\u00b2,\n\nwhere 2t+1(\u03b8) is the predicted state vector parameterized by \u03b8, and xt+1 is the true state vector.\nModel architectures. The Mixture-of-Experts (MoE) architecture has seen newfound resurgence in modern neural networks thanks to its ability to increase model capacity without a proportional increase in computation [30]. We ran small ablation studies and found that a sparsely mixture-of-experts (MoE) over gated MLPs after the model's main sublayer performed the best. The gated MLP splits the inputs into two parts: a main component and a \"gate\" that modulates the main component using the SiLU [27] activation function. The gating MoE network then dynamically selects which top experts to use for each input based on a learned distribution, where each expert is a gated MLP, applies the top_k selected expert(s) to the input, and combines their outputs using a weighted summation, where the weights come from the gating mechanism."}, {"title": "Takeaways from robotics experiments", "content": "As can be seen in the experiment results, on a parameter-adjusted basis, the STU-T outperforms all other models.\nAttention vs. State Space Models. It has been observed that Transformers struggle with tasks requiring the recall of long-range dependencies [31], whereas state space models (SSMs) excel in this aspect. Transformers' quadratic complexity in sequence length [33] contrasts with SSMs' linear complexity and ability to theoretically capture infinitely long dependencies [14]. Mamba [13], a recent SSM variant, introduces selective state spaces that dynamically adapt to input sequences, achieving state-of-the-art performance across various tasks. In our study, Mamba-2 indeed significantly outperforms Transformer in MuJoCo robotics sequence prediction tasks. More excitingly, our STU-T performs even better than Mamba-2 (and faster), demonstrating its potential in pushing the boundaries of performance and efficiency.\nTensordot approximation effect. As noted in the introduction, the tensordot approximation of STU-T models is a significant optimization in both computational and memory efficiency. We found that STU-T models are much smaller yet have performance on par, and at most times exceeding, that of full STU models. Moreover, the parameter savings due to the tensordot approximation can be reallocated towards other areas of the model. This makes STU-T models our best performing variant given equalized parameter counts, beating state-of-the-art Mamba-2 model in all three MuJoCo tasks.\nMixture-of-STUs. We found that formulating our MLP layer in the SwiGLU configuration [29] improves performance in STU models; however, formulating the MLP layer as a sparsely-gated mixture-of-SwiGLUs hurts performance. We envision there is a ripe field of research in architectural design for the STU waiting to be explored, e.g. adaptive online methods and variants of MoE [15], which we leave to future work.\nWidth vs. Depth. The STU model benefits more from increasing model depth, whereas the Transformer model is more sensitive to increases in model width. When model width is held constant, we observe that the Transformer's performance gains stagnate early with respect to depth-particularly after just two layers. We suspect this is related to the 'induction heads' phenomenon observed in attention-based architectures [23]. Induction heads are specialized attention mechanisms that enable the model to recognize and replicate patterns in the input data, facilitating in-context learning. These heads typically emerge within the first two layers of a Transformer, providing the bulk of its learning capabilities. As a result, additional layers contribute diminishing returns, since the core functionalities are already established early on. Despite the apparent absence of induction head circuits in the STU, it still performs well on the MuJoCo task, outperforming the Transformer."}, {"title": "Experiments with language modeling", "content": "In this section, we explore the STU's ability for sequence prediction in the context of language modeling. We model our architecture in the style of GPT-2 [26], and we open source a simple, fully distributed large language model (LLM) pretraining pipeline [22] for the community to enjoy and build upon."}, {"title": "Experiment setup", "content": "Data. We pretrain on roughly 10B high-quality tokens from FineWeb-Edu [24], a large-scale, open source dataset for language modeling. We tokenized the dataset into 95 training shards and 1 validation shard, each containing about 100M tokens, using the 0200k_base tokenizer from the OpenAI tiktoken library.\nGeneral design choices. For each model, we used RMSNorm [36] to pre-normalize the inputs before each attention layer, Flash STU layer, and MLP layer. We followed the standard pre-norm residual connection pattern around each of the sublayers, i.e. the output of each sublayer is x + Sublayer(RMSNorm(x)). To further stabilize training, we capped logits [4, 35] in each attention layer at 50.0. Following NanoGPT, we rounded up the vocabulary size of each model to the nearest multiple of 64 in order to use more efficient CUDA kernel pathways. We tied embeddings, and we did not use dropout.\nTransformer architecture. We followed the GPT-2-styled Transformer from NanoGPT [19]. We added small optimizations such as FlashAttention-2 [7] and the ALiBi [25] modification to the attention scores. We used position interpolation [6] to help the model \"length generalize\" beyond its context window size from training. It has been shown that position interpolation works even with ALiBi [2], so we used an interpolation factor of 0.25, extending the model's effective context window size at inference time by a factor of 4. We used highly optimized Triton 6 kernels [18] for the implementation of our MLP (SwiGLU), RMSNorm, and loss function.\nFlash STU architecture. We augmented the STU-T model with Flash FFT [12] for efficient convolutions. We found that the tensordot approximation was necessary to scale up STU-based models, as it was difficult to scale beyond 1B parameters without experiencing frequent out-of-memory (OOM) errors. Following the work from the state space literature [8, 34] relating to the role of attention layers, we opted to use a simple hybrid model architecture with alternating layers consisting of STU-T and local attention, both followed by an MLP layer. We used the same optimized Triton kernels from the Transformer, and we replaced global attention with sliding window attention [5]."}, {"title": "Results", "content": "Table 3 summarizes the key performance metrics for both models. We found that the Flash STU model outperformed the Transformer model in terms of validation loss and perplexity. Sample autoregressive text generation from the Flash STU model can be found in Appendix E. For completeness, we included the performances of the two variants of STU-T as well in Figure 27."}, {"title": "Finetuning filters", "content": "The core building block of the spectral state space model is the spectral filter, described in more detail in Appendix A. Slightly deviating from the originally proposed Spectral State Space model architecture, we ran an ablation training run where we initialized the K filters from the Hankel matrix per usual following the theory of spectral filtering but made the filters at each layer to be learnable parameters with a learning rate equivalent to that of the main model architecture.\nInterestingly enough, this slightly improved the performance of Flash STU, but we do not yet make any strong claims about this observation and leave this to future work under a more rigorous experimentation framework."}, {"title": "Spectral filters", "content": "Following [1], we construct spectral filters by extracting the top K eigenvectors from the Hankel matrix:\n\nZ= 2/( (i + j)3 \u2212 (i + j) )\n\nor, alternatively, from\n\nZL = ((\u22121)i+j\u22122 + 1) \u22c5 8/( (i + j + 3)(i + j \u2212 1)(i + j + 1) )\n\nThese top K eigenvectors are then scaled by their corresponding eigenvalues, each raised to the -th power. These scaled eigenvectors serve as our spectral filters and are used to convolve inputs into the spectral basis."}, {"title": "Additional synthetic experiments", "content": "Extra experimental details. The synthetic experiments were all run on a single A100 GPU, with each trial for each model taking around a minute to complete. The STU implementation followed Figure 1 exactly, with the tensordot approximation replacing the STU layer in the STU-T implementation. The softmax attention baseline is FlashAttention [7], and for S4 we used the official implementation [14]. Wherever we introduce nonlinearities in the synthetic experiments it is always ReLU; we leave a close investigation of multiplicative gating nonlinearities like GLU to future work.\nLocal sharpness of the loss. In Figures 15-17, the sharpness values of the loss Hessian are plotted after 10 steps of training on the LDS task from the main paper. A ratio near 1.0 means the curvature is similar in all directions, while a ratio near 0.0 means the curvature is stronger in one direction. We use absolute value because we are interested in the magnitude of the curvature. The contour lines overlaid on the heatmap show the loss landscape, allowing us to visualize how local convexity relates to the overall loss surface. Blue areas represent more spherical curvature, and red areas represent more elongated curvature. Movement along the x and y axes corresponds to local movement in parameter space.\nFor STU, we see very smooth behavior and an obvious direction of progress toward well-conditioned areas, whereas the loss landscape is more complicated for S4 and attention.\nOther tasks. For a comprehensive study of how the STU layer performs on different types of small sequence prediction problems, we also experiment on the selective copy task [3] and prediction of the mode of the tokens in the context. The selective copy task requires the model to recall (in order) a sequence of a fixed number of tokens (which are sampled uniformly from a vocabulary and distributed randomly throughout the context of otherwise blank tokens). By contrast, the mode prediction task simply requires the model to output the mode of a sequence of tokens sampled randomly from a vocabulary.\nSpeaking broadly, an LDS requires a structured aggregation of the context, while induction heads requires only local understanding of the context. Mode prediction and selective copy combine these principles, testing a model's ability to synthesize single-token-precision information across the whole sequence. Training results are presented in Figures 18 and 19, respectively.\nAs far as the we can tell, a task involving prediction of the mode of discrete data has not been used to study the behaviors of sequence models in any existing literature. We think it is an elegant way to probe a model's ability to nonlinearly synthesize very simple single-token information, as it is clear how the difficulty of the problem scales with sequence length and vocabulary size. It may also lend itself to strong theoretical analyses on account of its simplicity, and there is clear structure with which to study generalization to new tokens or seqence lengths."}, {"title": "Additional model architectures", "content": "Figure 20 shows the model architectures used for the robotics experiments in section 3. We ran small ablation studies and found that global skip connections oftentimes improved performance in our MuJoCo robotics experiment setting. In non-hybrid STU architectures, we found that performance decreases when using an MoE architecture, as discussed in 3.1, so we used a simple gated MLP instead."}, {"title": "Additional experiments with robotics data", "content": "In this section we give the remaining experimental results over robotics data for the other two MuJoCo tasks: HalfCheetah-v1 and Walker2D-v1. Compared to Ant-v1 (shown in Section 2 in the main paper), these two tasks only involve motion in a 2D plane. As a result, the state representations for HalfCheetah-v1 and Walker2D-v1 are inherently less complex, potentially affecting the relative performance of different models on these tasks in theory.\nIn this setting, the comparison of the models' performances still remains consistent, with Transformer showing the least competitive results, Mamba-2 demonstrating significant improvements over Transformer, and STU-T outper- forming both Transformer and Mamba-2. HalfCheetah-v1 task's training results are shown in Figure 21 and Table 5, (auto-regressive) next-step predictions results in Figure 22 and Figure 23. Walker2D-v1 task's training results are shown in Figure 24 and Table 6, (auto-regressive) next-step predictions results in Figure 25 and Figure 26."}, {"title": "Hyperparameters for robotics experiments", "content": "We conducted comprehensive ablation studies to investigate the impact of various hyperparameters on the performance of STU, STU-T, and Transformer models in the context of robotics tasks. These studies explore the effects of model width, depth, and input noise on the final Mean Squared Error (MSE) loss. Tables 7, 8, and 9 present the results of these experiments."}, {"title": "L L M experiments", "content": ""}]}