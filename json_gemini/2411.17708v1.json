{"title": "Towards Efficient Neurally-Guided Program Induction for ARC-AGI", "authors": ["Simon Ouellette"], "abstract": "ARC-AGI is an open-world problem domain in which the\nability to generalize out-of-distribution is a crucial quality.\nUnder the program induction paradigm, we present a series\nof experiments that reveal the efficiency and generalization\ncharacteristics of various neurally-guided program induction\napproaches. The three paradigms we consider are Learning\nthe grid space, Learning the program space, and Learning the\ntransformation space. We implement and experiment thor-\noughly on the first two, and retain the second one for ARC-\nAGI submission. After identifying the strengths and weak-\nnesses of both of these approaches, we suggest the third as a\npotential solution, and run preliminary experiments.", "sections": [{"title": "Introduction", "content": "Deep Learning is notoriously powerful when addressing a\nwide variety of problems. Yet, in other areas, it is deficient.\nOne predominant difference between the problem domains\nwhere it is highly successful, and the domains where it is less\nso, is the openness of that domain. In closed-world domains,\nwhere the model is able to densely sample and interpolate\nover the full set of possbilities, deep learning often achieves\nnear-human or sometimes even super-human ability. How-\never, in open-world domains, where the full range of possi-\nbilities is so vast that it is practically infeasible to densely\nsample its entirety, interpolation-based techniques like Deep\nLearning can be sub-optimal.\nThis results in Deep Learning's well-known challenges in\ngeneralizing outside the training set distribution. The Ab-\nstraction & Reasoning Corpus (Chollet et al. 2024) (ARC-\nAGI) is one of the foremost problem domains that specifi-\ncally challenges this closed-world assumption. This is done\nby using a hidden test set that contains tasks that are qualita-\ntively distinct from any of the publicly available tasks. This\nencourages research on extending the out-of-distribution\ngeneralization capablities of learning systems. This will be\nthe focus of the work presented here.\nThe current implementations of the proposed approaches\nare not yet mature enough to yield an interesting or compet-\nitive performance on ARC-AGI. This paper reports proof-\nof-concept results of experiments made under controlled\nand limited conditions, exploring ways in which we can\nefficiently extend the generalization capabilities of a Deep\nLearning model via search, in a program induction context."}, {"title": "The Problem", "content": "The problem setting we use for this paper follows a typical\nprogram synthesis setting. We assume that the Domain Spe-\ncific Language (DSL) includes all the necessary primitives\nto solve each test task. The goal is to search (as efficiently as\npossible) for the program that solves each task in the test set,\ngiven the provided DSL. Throughout this paper, the tasks ex-\nperimented upon are mainly taken from the ARC-AGI eval-\nuation set.\nA compute and memory time budget is allocated for each\ntask solution, so it must find it within those budget con-\nstraints. This can be easy, even trivial for very small pro-\ngrams, but due to combinatorial explosion can become quite\nunmanageable for large programs. More formally, given a\nDSL \u03a9 = {\u03c01, \u03c02, ..., \u03c0\u03c1} containing N primitive functions\ndenoted \u03c0\u03af, a search algorithm F(X, Y) given the support\ninput examples X, and support output examples Ys must\nreturn within the allocated CPU and memory budgets a pro-\ngram P such that P(Xq) = Yq, where Xq and Yq are the\nquery examples. In other words, the support examples are\nused to infer the underlying program, and the query exam-\nples are used to confirm the correctness of the program.\nThere is an additional, crucial aspect to our problem set-\nting. While we assume the DSL \u03a9 to fully contain the re-\nquired primitives to solve both the training and test datasets,\nwe allow the test dataset's solution programs to follow dif-\nferent structures than the test datasets. In other words, we\nplace the additional constraint to our problem domain that\nthe test set must be out-of-distribution with respect to the\ntraining set in terms of the compositional structures of P.\nThis is important because, if it were not the case, a neural\nnetwork would be sufficient to solve this problem domain."}, {"title": "Learning the Grid Space", "content": "The idea of this approach is to learn a model of the space of\npossible ARC-AGI grids, under a specific DSL. It is impor-\ntant to note that there is no such thing as a pure grid similar-\nity space unconditioned on some DSL, since under two dif-\nferent DSLs the same grid pair can have quite different dis-\ntances. From there, it is possible to input two different grids\nand estimate their similarity. This can be used in a search al-\ngorithm to gradually build a program in an execution-guided\nmanner. The idea of execution-guided program induction is\nto leverage some heuristic to guide the choice of primitives\nduring construction. This heuristic requires feedback from\npartial execution of the program being constructed. As such,\nprogram solution construction and execution happen in par-\nallel and impact each other.\nAt each iteration of the search, the algorithm attempts all\npossible primitives on the current intermediate grid in the\nprogram execution, and selects the one that brings it the clos-\nest to the target grid state. Note that this approach assumes\nthat all (or most) steps in program execution result in a grid-\nto-grid transformation. This is why the experiments on LGS\nuse a DSL that strictly contains grid-to-grid transformations\n(DSL Version 1). As such, it should be kept in mind that the\nresults reported for this method are therefore the best case\nscenario where all steps in program execution result in a grid\ntransformation.\nThe experiments reported here used a Transformer\nencoder-only model with max pooling that outputs a flat-\ntened vector: a grid embedding. The training procedure con-\nsists of iteratively feeding a pair of grids into the model,\nretrieving their respective embeddings, and then penalizing\ndeviations between their dot product and their ground truth\ndistance.\nThe training data is generated by randomly sampling\npatches of ARC-AGI training set grids, and then picking a\nrandom number of grid transformation primitives to apply\nto them(zero to eight, in our expeirments). This randomly\nselected sequence of transformations is executed, and the\noutput grid is retained. The input and output grids, along\nwith the number of transformations applied, constitute a full\ntraining sample. The number of transformations is converted\nto a similarity between zero and one.\nIt should be noted that the procedurally generated training\ndata often contained over-estimated transformation distance\nground truths. This is because it is very easy to accidentally\ngenerate redundant transformations. The desirable quality of\na good grid distance ground truth is that it represents the\nshortest distance between two grids, rather than whatever\ndistance was arbitrarily used to generate these two grids. A\ntrivial example of this is that \"rotate 90, rotate 90, rotate 90\"\nshould yield a grid distance of one (rotate 270), not three\nsteps. Several heuristics were implemented in the training\ndata generation script to attempt to mitigate this issue, but\nnonetheless the issue remains. This is arguably one of the\nweaknesses of applying supervised learning to a cost-to-go\ntype of approach."}, {"title": "Learning the Program Space", "content": "This is the selected approach, because it performed better\nthan LGS on the ARC-AGI evaluation set. As such, it will\nbe described in more depth than the other two methods. Fur-\nthermore, because we are referring to a specific, novel im-\nplementation of the overarching paradigm of LPS, we name\nit GridCoder.\nAt a high-level, the solution consists of training a trans-\nformer to output a program, using a pre-determined gram-\nmar (DSL) and syntax, that solves the task. Specifically, the\ntransformer outputs a sequence of probabilities over tokens\nthat can be interpreted as a probability distribution over pro-\ngram space. We then use a search algorithm to enumerate\nthat space and test valid programs for correctness. In a sense,\nthe search explores the area of the solution space covering\nthe neural network's region of uncertainty."}, {"title": "The Search Algorithm", "content": "It is a probability-based enumeration of programs, concep-\ntually similar to DreamCoder (Ellis et al. 2021) and Heap\nSearch (Fijalkow et al. 2021). However, unlike these ap-\nproaches, the probability predictions are not based on n-\ngrams, but instead are based on transformer generated to-\nken probability sequences. That is, instead of predicting the\nprobability over children classes conditional on (n-1) par-\nents, we predict the probability over children classes condi-\ntional on the entire maximum probability token sequence (as\nin typical auto-regressive transformer decoding).\nAlso, unlike the other two paradigms presented in this pa-\nper (LGS and LTS), this approach is not execution-guided,\nin that it does not receive feedback about the intermediate\nstate of the program as it develops a solution. Initially, there\nwas hope that the transformer decoder could learn to do this\nimplicitly, however the empirical results point to this not be-\ning the case (at least, not with the current architecture).\nA first full decoding loop is executed on the Vision-\nLanguage Model (VLM) that is fed one input and output grid\npair from a given task's demonstration set. The full decoding\nloop stops until either the maximum sequence length (40, in\nour experiments) is reached, or a token is found where the"}, {"title": "Conditional Independence", "content": "One aspect of the algorithm that is important, yet is not ob-\nvious in the pseudo-code, is the underlying data structure.\nWhile typical program induction operates on a tree, Grid-\nCoder operates on a flat sequence of nodes. In other words,\nour probabilities are represented by a list of lists: the proba-\nbility distribution over a given token is solely dependent on\nits position in the sequence, rather than being conditional on\nthe previous choices of tokens. That is, to be more exact, it\nwas conditional on the previous (max probability) tokens at\nthe time of auto-regressively calculating the probability dis-\ntribution. But, at search time, we do not update the choice\nof selected tokens and requery the model to find the new,\nconditionally updated probabilities.\nDoing this, and maintaing a tree of probabilities instead,\nwould be theoretically correct, because it would respect\nthe natural conditional dependence between primitives. The\nprobability over the next token is theoretically dependent\nnot just on the token's position in the sequence, but on all\nof the previous selections that were made. However, we\nchoose to make a simplifying assumption of conditional\nindependence between nodes, much like in Bayesian net-\nworks. While this simplifying assumption potentially hurts\nin some cases, overall we believe it yields a significant gain\nin efficiency: making this assumption means that we only\nneed to call the neural network a few times while bootstrap-\nping the probabilities.\nIn fact, in the baseline version of this approach, the neural\nnetwork need only be called exactly once. From there, all\nof the necessary probability distributions can be acquired.\nHowever, it was found empirically that using a bit of boot-\nstrapping helps. Bootstrapping in this context means that we\ncollect the probability distributions as a mean of K decoding\niterations (where K is six in our experiments). Each decod-\ning iteration differs in which example pair is provided to the\nneural network, and which starting token is selected. This\nbootstrapping idea helps mitigate the rare cases where con-\nditional independence hurts because the neural network out-\nputs an unusually constrained (i.e., falsely confident) set of\nprobabilities on the first inference. Using a few bootstrapped\ninference trials seems to open up the space of possibilities a\nbit, introducing a bit of much needed entropy the process."}, {"title": "The DSLs", "content": "A few iterations of the DSL were used during the experi-\nments, in which primitives were gradually added to the pre-"}, {"title": "VLM Architecture & Training", "content": "The neural network used to provide the probabilities that\nguide the search is based on a Vision Language Model\n(VLM) paradigm (see figure 1). It is a pair of convolutional\nneural networks, one receiving the input grid, the other the\noutput grid, eventually bridging their intermediate features\ninto one convolutional network. This serves as an encoder,\nfollowed by a transformer decoder that processes the tar-\nget sequence. In our experiments this architecture converged\nfaster and to a higher validation accuracy on our dataset\nthan a T5 or LongT5 architecture, while allowing us to use\na higher parameter count for the same amount of VRAM.\nSpecifically, the model used for the experiments has 452 mil-\nlion parameters, and only one decoder layer with an embed-\nding dimensionality of 512 and 4 attention heads.\nEach predicted token sequence forms a program syn-\ntax tree in a bottom-up, left-to-right manner. Each token\nrefers to a primitive from the DSL, or to one of the three\nspecial tokens: <End of Sentence>, <New Level>,\n<Identity>.<Identity> can be thought of as a prim-\nitive that takes as input a grid and returns that grid without\nany modification. It is often used as a placeholder in order to\ndisambiguate between different possible interpretations of a\nprogram. <New Level> is a special token that indicates\nthat the tokens that follow define the next level in the pro-\ngram syntax tree. The syntax tree is built bottom-up (as the\ntoken sequence is read from left to right), and in each level\nthe function argument ordering is defined from left to right\nin the same order as the token sequence. Here are a few ex-\namples to help better understand the syntax structure.\nSuppose the following token sequence: [lefthalf,\nrighthalf, <New Level>, cellwiseOR, <New\nLevel>, set_fg_color3, <End of Sentence>]."}, {"title": "Training Data Generation", "content": "The training data for the VLM is the output of procedu-\nrally generated tasks. The concept is to identify task \"meta-\npatterns\" or categories in the ARC-AGI Training Set, and to\nimplement data generators that reproduce them, while ran-\ndomly varying several aspects of the tasks. Different DSL\nversions have different task generators associated with them.\nIn this version, there are only three task\ngenerators implemented. The input grids used to generate\nthe output grids are randomly sampled from two sources:\nmanually pre-generated grids (mostly drawn from the ARC-\nAGI Training set), and randomly generated grids. Input grids\nthat match grids found in the ARC-AGI Evaluation set are\nnot allowed, though it is not impossible for the randomly\ngenerated grids, by extreme luck, to reproduce grids similar\nto those in the ARC-AGI Evaluation set.\nThe first task generator consists of randomly generating\ntrivial tasks that are either made up of one primitive (that\ntakes as input a grid and generates a grid as outupt), or of two\nprimitives composed together. The two primitive tasks are\nnot entirely random: a script that generates all two-primitive\npermutations of the DSL was created, but several heuristics\nwere used to prevent non-sensical or redundant composi-\ntions. Overall, this task generator has 796 distinct tasks.\nThe second task generator is based on a \"split-merge\" pat-\ntern that was observed. This task category produces tasks\nin which the goal is to split the input grid in some way,"}, {"title": "Learning the Transform Space", "content": "This approach can be thought of as an evolution of LPS,\nwhile bringing back the execution-guided aspect of LGS.\nThe concept is to train a model such that, given an interme-\ndiate or starting program state, and a target grid, it predicts\nthe probability distribution over the DSL for the next token.\nIn other words, the main difference with LPS is that we ex-\nplicitly feed back into each decoding step some notion of\nthe intermediate state of the program. Thus, the algorithm\ncan leverage the efficiency of auto-regressive decoding with\nthe ability to evaluate the transformations in and of them-\nselves regardless of whether this program pattern has been\nseen during training.\nThe emphasis is on learning what transformations are re-\nquired to bring the program execution from its current state\nto the target grid. Initially, this was the intended result of the\nGridCoder algorithm, however further analysis reveals that\nthe latter does not maintain such an hidden state implicitly, it\nonly superficially relies on the encoding of the input-output\ngrid and the token sequence generated so far. See the Dis-\ncussion section for a deeper analysis.\nLTS borrows the same program syntax and auto-\nregressive sequence supervision as in GridCoder. In a more\nbasic implementation, the intermediate program state could\nsimply be the last full intermediate grid produced by the pro-\ngram execution. The experiments associated with Table 4\nmake this basic assumption. In a more sophisticated version,\nhowever, there needs to be a hidden latent state maintained\nthroughout the decoding process that gets updated at each\nstep.\nThe challenging aspect of maintaining this generalized in-\ntermediate state is that the output of a primitive can be any-\nthing from a Grid to an integer, a Boolean, a list of integers,\na list of Grids, etc. A clever mechanism must be designed to\nallow embedding into a fixed vector space this dynamic and\ndiverse range of outputs, which is left to future work."}, {"title": "Experiments & Results", "content": "This section presents the results of three experiments. First,\na comparison of the performance of different approaches\non the ARC evaluation set is made. Second, the increments\nof performance of the selected approach (GridCoder) are\nshown, as new primitives are added to the DSL. Finally,\nan experiment on structurally out-of-distribution tasks is\nshown, to illustrate the limitations of the selected approach\nand the promising capabilities of the suggested new ap-\nproach (Learning the transformation space)."}, {"title": "Performance Comparison on ARC-AGI Eval Set", "content": "Table 1 reports the success rate (as a percentage of solved\ntasks) of various approaches on the ARC-AGI evaluation set.\nWe show the numbers in terms of the absolute ARC-AGI\nevaluation set performance, but also in terms of the subset\nof the ARC-AGI evaluation set that is theoretically solvable\naccording to the DSL. DSL Version 1 is used for these ex-\nperiments (see section titled \"The DSLs\"). This subset refers\nto the 29 tasks (7.25% of full evaluation set) that can theo-\nretically be solvable in the current DSL. All the other tasks\nare impossible to solve regardless of how efficient the search\nis.\nFor each task, a time budget of up to 15 minutes to find the\nanswer is allowed. For GridCoder and GridCoder cond., the"}, {"title": "Generalization of LTS", "content": "The purpose of this experiment is to suggest a promising\nnew method that mitigates the limitations of the selected\nGridCoder approach (see Discussion section), by indicating\npreliminary empirical results on its out-of-distribution gen-\neralization characteristics. 10 new tasks are hand-crafted,\nspecifically selected to guarantee that there is no structurally\nsimilar task ever generated in the training data. One of them\nis picked from the ARC-AGI evaluation set, task #48131b3c,\nsince it has been observed that GridCoder fails to generalize\nto this task.\nA high-level description of the 10 tasks follows:\n1. Task #48131b3c from ARC-AGI evaluation set: tile the\noriginal grid 2x2, and invert the colors. The training data\ngenerator never presents the post-tiling inversion of col-\nors.\n2. Hand-crafted task 1: gravitate the pixels to the left, then\ngravitate the pixels upward, then change the foreground\npixels' color to aquamarine. The training data genera-\ntor never generates tasks that involves the application of\nthree primitives in a row (or longer). The maximum is\ntwo.\n3. Hand-crafted task 2: rotate the grid 90 degrees, upscale\nit horizontally by two, and then upscale it vertically by\ntwo.\n4. Hand-crafted task 3: same as task 2, but add an extra op-\neration of horizontal mirroring at the end.\n5. Hand-crafted task 4: same as task 3, but add an extra op-\neration of color inversion at the end.\n6. Hand-crafted task 5: tile the original grid 2x4 while alter-\nnating 180-degree rotation with the identity transform on\neach tile. This is out-of-distribution because a 2x4 tiling\nis never generated in the training data. The closest are\neither 2x2 or 1x4.\n7. Hand-crafted task 6: tile the smallest object in the grid\ntwice horizontally (i.e. concatenate it with itself horizon-\ntally), and use that as output grid. In the training data,\ntiling tasks and object selection tasks are never com-\nbined.\n8. Hand-crafted task 7: crop the the object that contains the\nlargest number of sub-objects, rotate it 90 degrees, and\nthen duplicate the top row and the bottom row. The train-\ning data only at most applies 1 post-processing transform\non these types of object cropping tasks.\n9. Hand-crafted task 8: filter out the largest object in the\ngrid and then rotate the grid 270 degrees. The training\ndata generation never applies rotation primitives to the\noutput of an object filtering task.\n10. Hand-crafted task 9: crop the largest object in the grid,\nsplit it in half horiziontally, and merge the left and right\nhalves with cellwise OR logic. In the training data, object\ncropping and \"split and merge\" types of tasks are never\ncombined.\nThe experiment consists of simulating a model that was"}, {"title": "Discussion", "content": "trained to decode in an execution-guided way (see the sec-\ntion Learning the Transformation Space), receiving execu-\ntion feedback at each step along the way. While the potential\nimpact on inference time is not simulated, the task attempt is\nconsidered a success if it is possible to reach the solution by\nre-launching the current search algorithm on the intermedi-\nate output of one of the program sequences that get evaluated\nin the previous run. As a result, the program is expected to\nprovide a probability over the program space from a starting\npoint that is the intermediate output of a previous program\nhence it is given the opportunity to compose sub-programs.\nThis proxy experiment was done, instead of correctly\ntraining a model that learns to decode in such a way, due\nto lack of time. It is intended as an approximation, or sug-\ngestion of what is potentially achievable, if we fully train\nthe model to receive an execution output at each decoding\nstep. Table 4 indicates the results on the 10 tasks, compar-\ning the non-execution-guided GridCoder to the execution-\nguided GridCoder.\nTask 5 fails on both approaches because when the neural\nnetwork sees a large target grid (of 2x4 in this case), instead\nof attempting a 2x2 solution or a 1x4 solution which can\nthen be scaled by composition to the correct 2x4 solution, it\ngoes directly for 3x3. So the partial solutions that it suggests\ncannot be used to produce the correct solution.\nTask 6 fails on both approaches because it fails to see what\nneeds to be done after the initial object cropping. It attributes\nnear-zero probabilities to the primitives that yield the re-\nquired horizontal tiling. Instead it seems to suggest programs\nthat would mirror the input, horizontally or vertically.\nIn the first experiment, the gap between VLM-only and Grid-\nCoder is strictly an out-of-distribution generalization gap.\nThe fact that the search-enabled algorithms obtain a signifi-\ncantly better performance than the VLM-only approach sug-"}, {"title": "Generalization Characteristics", "content": "gests that searching over the probability space significantly\nextends the neural network's reach beyond its training dis-\ntribution. However, generalization obstacles remain, as in-\ndicated by the experiments in out-of-distribution generaliza-\ntion where the GridCoder approach only succeeds in 1 out of\nthe 10 tasks. Furthermore, we note the following additional\nGridCoder failure cases:\nThis task (see figure 5) consists of \"tiling\"\nthe original grid fives time horizontally, using various grid\ntransformations (e.g. rotations) each time. While the neural\nnetwork was exposed to conceptually similar tasks during\ntraining, it has never seen fivefold horizontal (or vertical)\ntiling at all. As such, it struggled to generate the program\nstructure that allows this fivefold tiling.\nd47aa2ff.json This task (see figure 5) consists of copying\nover to the output grid the pixels that are common to both\nhalves of the input grid. However, when there is a discrep-\nancy between both halves, if the pixel only appears on the\nleft grid, it must be colored red in the output, and if it is the\nreverse it must be colored blue. While the DSL allows us\nto solve this task in principle, the solution is quite elaborate\nand there is no conceptually similar task in the training data.\nThe vast improvement in generalization capabilities from\nVLM-only to GridCoder can appear to be in opposition to\nthe poor structural generalization findings from Table 4. We\nhypothesize that the out-of-distribution uncertainty that gets\nresolved by the probabilistic search is mainly related to a\ngrid-level distribution shift. The test grids are distinct from\nthe training ones, hence it struggles with properly predicting\nthe types of per-grid transformations. This results in a rela-\ntively high entropy when choosing a primitive that produces\nsome kind of grid-to-grid transformation.\nHowever, we observe a very low entropy on tokens that set\nthe overall program structure (for example, when differenti-\nating \"tiling\" types of tasks from \"split and merge\" types of\ntasks). As a result, the search is able to resolve grid-level dis-\ntribution shift by searching over the relatively high-entropy\ndistribution, while the very low entropy distribution of pro-\ngram structure across task categories means that it cannot"}, {"title": "Scaling the DSL", "content": "A potential criticism for the LPS approach is that, as we\ngrow the DSL, the search space might become exponentially\nmore complex, so the solution time slows down proporti-\nnately. This would make this approach unlikely to scale to\na competitive solution on ARC-AGI. Another possibility is\nthat, as we add new primitives to the DSL, and train on new\ntasks, performance degrades on previous tasks.\nTables 2 and 3 paint a positive picture of the scaling prop-\nerties of the LPS approach. First, we see from Table 2 that\nas we grow the DSL from version 1 to version 3, the tasks\nthat the approach was able to solve in DSL version 1 are\nstill solved in subsequent sections. There is no forgetting,\nand no significant loss in search performance that degrades\nthe success rate. Meanwhile, the overall performance on the\nARC-AGI evaluation set increases as we add primitives and\ncorresponding training tasks.\nFurthermore, we see that from Version 1 to Version 3, the\nmean solution time per primitive does not increase. In fact,\nit seems to decrease a bit from Version 2 to Version 3. This\nmay simply be a fluke from having a better trained model in\nVersion 3 and Version 2. As a reminder, DSL Version 1 has\n74 primitives, DSL Version 2 has 89 primitives, and DSL\nVersion 3 has 98 primitives. In summary, the solution time\nscales sub-linearly as we add more primitives to the DSL,\nwhich is a desirable property."}, {"title": "Flaws in LGS", "content": "The LGS approach reveals itself to be somewhat ineffi-\ncient, though a priori it may be better at structural out-of-\ndistribution generalization (because it is not trained on spe-\ncific task solutions). Extensive work has been done to under-\nstand the causes of this inefficiency. The main hypothesized\ncauses are as follows:\nA* versus Q* The idea of a model that takes as input two\ngrids and predicts their similarity suggests an A*-style usage\npattern:\n1. Each transformation in the DSL is applied to the input\ngrid to obtain some intermediate grid.\n2. Each pair of intermediate grid and target grid is fed to the\nsimilarity model to obtain their respective similarities.\n3. The primitive that yields the biggest increase in similarity\nis selected.\n4. This is done iteratively until a solution is found or some\nbudget limit is reached.\nAgostinelli et al. (2024) show that this approach is both\ntemporally and spatially inefficient, and propose Q* search,\nwhich can be differentiated as follows:\n1. The input grid and output grid pair is fed to the neural\nnetwork, which in one shot simultaneously predicts all of"}, {"title": "Limitations and Future work", "content": "Limited DSL The work done so far has been a proof-\nof-concept centered on finding an optimal neurally-guided\nsearch algorithm given some pre-determined DSL and a set\nof tasks that are known to be solvable within this DSL. Very\nlimited work has been done on improving the DSL. This is,\ntherefore, the main limitation of this work.\nScaling the algorithms presented here to a competitive\nlevel will certainly involve completing the DSL by introduc-\ning all of the necessary primitives required to solve all ARC\ntraining and evaluation tasks. This is not as simple as going\nthrough the existing tasks and implementing primitives for\nevery high-level functionality seen.\nWe aspire to making the DSL more flexible and general\nas well, as we currently deem it too high-level and specific.\nWe aim for a certain compromise between a purely generic\nprogramming language and a highly specific DSL such as\nthe current one. We expect this work, on its own, to make\nour approach competitive on the ARC hidden test set."}, {"title": "Functional DSL", "content": "Based on the work by Hocquette and\nCropper (2024), it appears that a relational type of DSL\ncan lead to more efficient search than the kind of functional\nparadigm currently being used in these experiments. Intu-\nitively, relational decomposition of tasks means that partial\nverification of a subset of the rules is possible while search-\ning for a solution. In contrast, our current approaches can\nonly verify the full program in a binary \"fully correct\" or\n\"fully incorrect\" manner. Relational decomposition there-\nfore allows a more gradual progression towards the full so-\nlution."}, {"title": "get_objects primitives", "content": "In retrospect, the current imple-\nmentation of six different primitives for object detection is\nvery sub-optimal. In spite of having implemented six differ-\nent notions of what is an object, there are still many cases\nwhere object detection primitives fail. Going forward, we\nwill instead implement this primitive as an \"object segmen-\ntation\" neural network that, we hope, will allow us to reduce\nthe number of object detection primitives to one, while si-\nmultaneously covering more use cases."}, {"title": "Program syntax", "content": "The program representation syntax\nneeds to be improved. First, the difference in program struc-\nture between four tiles and five tiles is relatively large, when\nin principle it should be quite small. Here is an example pro-\ngram sequence that tiles an input grid four times horizon-\ntally:\n[hmirror, <Identity>, hmirror, <Identity>,\n<New Level>, hconcat, hconcat, <New Level>,\nhconcat, <End of Sentence>]\nHere is the program sequence to tile an input grid five\ntimes horizontally:\n[hmirror, <Identity>, hmirror, <Identity>,\nhmirror, <New Level>, hconcat, hconcat,\n<Identity>,\n<New Level>,\nhconcat,\n<Identity>, <New Level>, hconcat, <End\nof Sentence>]\nThere is a difference of five tokens between the two,\nspread throughout all of the levels of the program. Clearly,\nthe conceptual similarity of these two tasks is not reflected\nin the syntactical similarity of their solutions. This certainly\ndoes not help the search.\nAnother problem is that a lot of duplication is necessary\nin the lower levels. For example, in the last task used for the\nresults of Table 4, the goal is to crop the largest object, split\nit in half horizontally and merge it pixelwise using OR logic.\nIn order to take the left half and the right half on the same\ninput, the current program syntax forces us to duplicate the\nentire sub-tree that generates the cropped object. The corre-\nsponding solution program is:\n[get_objects1,\nget_objects1,\nget_object_size, get_objects1, get_objects1,\nget_object_size, <New Level>, <Identity>,\nfor_each, <Identity>, for_each,\n<New\nLevel>, keep_largest, keep_largest, <New\nLevel>, lefthalf, righthalf, <New Level>,\ncellwiseOR, <End of Sentence>]\nThe entire sub-tree [get_objects1,get_objects1,\nget_object_size, <New Level>, <Identity>,\nfor_each, <New Level>, keep_largest] needs to"}, {"title": "Learning the transform space", "content": "be duplicated twice, thereby increasing the solution by six\ntokens. This is obviously not very efficient.\nThe empirical results sug-\ngest that GridCoder is limited to proposing program struc-\ntures that have been seen during training. The neural net-\nwork does not get to observe explicitly or even generate im-\nplicitly the intermediate state of a program as it derives its\nsolution. Instead, it currently has to be able to solve all of it\nin one go based on the provided input-output examples.\nIn other words, as it generates new tokens in the decoding\nloop, it does not get to look at the intermediate program state\nto decide on the next steps. We certainly do not do this ex-\nplicitly (yet), and we know from how transformer decoders\nwork that it does not naturally maintain a hidden state be-\ntween decoding steps. It only gets to look at the decoded\nsequence so far, as well as the output of the encoder. As a\nresult, it tends to laz"}]}