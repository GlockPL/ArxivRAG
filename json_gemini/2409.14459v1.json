{"title": "Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis", "authors": ["Daoyang Li", "Mingyu Jin", "Qingcheng Zeng", "Haiyan Zhao", "Mengnan Du"], "abstract": "Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of the world's languages. In this paper, we extend these probing methods to a multilingual context, investigating the behaviors of LLMs across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results highlight significant disparities in LLMs' multilingual capabilities and emphasize the need for improved modeling of low-resource languages.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as GPT-4 (Achiam et al., 2023), Claude 3.5 (Anthropic, 2024), Llama 3 (Dubey et al., 2024), have demonstrated remarkable progress across a wide range of natural language processing tasks. As these models continue to advance, there is a growing need to understand their internal mechanisms and representations. Probing techniques have emerged as a valuable tool for investigating how LLMs encode and process information, offering insights into their decision-making processes and the nature of their learned representations (Ferrando et al., 2024a; Zhao et al., 2024; Zou et al., 2023).\nHowever, a significant gap exists in our understanding of LLMs' multilingual capabilities. While extensive probing research has been conducted on English language representations, there are approximately 7,000 languages spoken worldwide, many of which remain understudied in the context of LLMs. This lack of comprehensive multilingual analysis limits our understanding of how LLMs perform across diverse linguistic contexts, particularly for low-resource languages that are often underrepresented in model training data and evaluations.\nTo address this research gap, we propose a multilingual probing approach to investigate the behavior of LLMs across a diverse set of 16 languages, including both high-resource and low-resource languages. Our study extends probing techniques from English to a multilingual context, examining how LLMs perform in factual knowledge and sentiment classification tasks across different languages. Our key findings reveal that: (1) high-resource languages consistently achieve higher probing accuracy compared to low-resource languages; (2) high-resource languages exhibit similar trends to English across model layers, with accuracy improving significantly in deeper layers, while low-resource languages show relatively stable or only slightly improving accuracy; and (3) there are high similarities between probing vectors of high-resource languages, whereas low-resource languages demonstrate lower similarities both among themselves and with high-resource languages."}, {"title": "2 Probing Method", "content": ""}, {"title": "2.1 LLM Internal Representation", "content": "We study decoder-only LLMs, where each layer of a model consists of both multi-head attention blocks (MHA) and feed-forward networks (FFNs). In this work, we utilize frozen pretrained language models. Layers are indexed with $l \\in L$, where $L$ denotes the set of all layers in a model. For each layer, the computation starts and ends with a residual stream. The MHA first reads from the residual stream and performs computation, then adds its output back to the residual stream. The updated vector in the residual stream is then passed through MLPs to generate the output of the layer:\n$h_i^{l+1} = h_i^l + MLP(h_i^l + Att(h_i^l))$."}, {"title": "2.2 Linear Classifier Probing", "content": "In our analysis, we employed linear classifier probing (Ju et al., 2024a; Jin et al., 2024) to explore internal representations across various layers of LLMs. We extracted hidden states from each layer using two types of inputs (i.e., positive and negative) and utilized these features to train a logistic regression model. By evaluating the performance of this classifier, we were able to assess how well the hidden states at different layers encoded information relevant to answering factual questions or handling sentiment classification tasks. This approach provides valuable insights into the nature of the representations learned within the model.\nTo quantify probing differences, we employed a linear classifier approach. We define $h \\in \\mathbb{R}^{n \\times d_{model}}$ as the hidden feature set extracted from the LLM, where $n$ is the number of samples and $d_{model}$ represents the dimensionality of the hidden layer. Each sample's internal representation at a specific layer is denoted by $h^{(i)} \\in \\mathbb{R}^{1 \\times d_{model}}$. We utilize binary classification, assigning labels $y^{(i)} \\in \\{0, 1\\}$. The objective function for our logistic regression classifier, incorporating L2 regularization, is formulated as:\n$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(h^{(i)}, y^{(i)}; \\theta) + \\frac{\\lambda}{2n} ||\\theta||^2$ .\nwhere $L(.)$ represents the cross entropy loss:\n$L(h^{(i)}, y^{(i)}; \\theta) = y^{(i)} log(\\sigma(\\theta^T h^{(i)})) + (1 - y^{(i)}) log(1 - \\sigma(\\theta^T h^{(i)}))$.\nwhere $\\theta$ denotes the model parameters, $\\lambda$ is the regularization coefficient, and $\\sigma(\\cdot)$ represents the sigmoid activation function. By applying this classifier to the test set, we can evaluate the LLM's performance and gain insights into its internal representations across different languages and layers."}, {"title": "3 Experiment", "content": "In this section, we conduct probing experiments to compare the layer-wise accuracy and perform a correlation study to test the similarity between the probing vectors of different languages, to answer the following three research questions (RQs):\n\u2022 RQ1 - Do other languages besides English have the same probing accuracy as English?\n\u2022 RQ2 - Do other languages follow the same trends as English in different layers?\n\u2022 RQ3 - Are there similarities between the probing vectors of different languages?"}, {"title": "3.1 Experiment Settings", "content": "Models: We utilized two open-source large language model families: Qwen (Bai et al., 2023) and Gemma (Team et al., 2024), to evaluate their performance and internal representations across various languages. The Qwen-1.5 models consist of 24 (0.5B & 1.8B) and 32 (7B) layers, the Gemma model has 18 (2B) and 28 (7B) layers. Different models also have different representation vector dimensions. Qwen-0.5B's representation vector dimension is 1024, Qwen-1.8B, Gemma-2B's representation vector dimension is 2048, Gemma-7B's representation vector dimension is 3072, and Qwen-7B's representation vector dimension is 4096.\nDatasets: In the following experiments, we utilized a truthful dataset: Cities (Marks and Tegmark, 2023), and a sentiment dataset: Opinion (Tatman, 2017). Cities contains 1496 samples, and Opinion contains 1000 samples.\n\u2022 Cities (Marks and Tegmark, 2023): consists of statements about the location of cities from worldwide and their veracity labels (e.g., The city of Lyon is in France, which is true).\n\u2022 Opinion (Tatman, 2017): consists of opinions of 20 famous hotels. It contains the hotel's name, opinion's polarity, and its source.\nOur dataset encompasses 16 languages from the whole world: English, German, French, Chinese, Spanish, Russian, Indonesian, Oriya, Hindi, Burmese, Hawaiian, Kannada, Tamil, Telugu, Kazakh, Turkmen. We categorized English, German, French, Chinese, Spanish, Russian, and Indonesian as high-resource languages, and rest of them as low-resource languages. The original language of our two datasets are English, and we used Google Translate within deep-translator python library (Azam, 2024) to translate them into other 15 languages, as Google Translate supports translation between over 100 languages, and achieves high accuracy compared to other translation tools.\nImplementation Details: To evaluate the performance of LLMs on each language, we use the template for the Cities dataset in English as \"Judge the statement is Positive or Negative. <Statement>\". The prompts of other languages utilize the same template translated by Google Translate. We applied probing techniques to assess the information encoded within each layer of these models. For our probing analysis, we selected linear classifier probing for three different experiments. The datasets are divided into a training and test set with an 8:2 ratio, and we adhered to the standard procedure for probing classifiers in LLMs, extracting feature representations from the final hidden states in the transformer at each layer of the LLMs to serve as input to the probing classifier. The linear weight parameter $\\theta$ of the logistic regression classifier is regarded as the probing vector for each language and layer."}, {"title": "3.2 Multilingual Accuracy", "content": "In this part of our experiment, we explored (1) whether other languages besides English have the same probing accuracy as English and (2) whether they follow the same trend as English in different layers. The analysis led to two general observations. We show the results in Figure 1 and Table 1. The key findings are given as follows:\n\u2022 High-resource languages show higher accuracy, while low-resource languages have comparatively lower accuracy. We conducted experiments using Cities and Opinion datasets, exploring the binary classification problem in 16 selected languages. Table 1 shows that in Cities dataset, high-resource languages such as French and German achieve at least 70% accuracy, even reaching over 90% accuracy for some models, while low-resource languages like Oriya and Hindi only achieve about 40% accuracy in the final layer.\n\u2022 High-resource languages follow similar trends to English, where accuracy significantly improves as the layers deepen. Low-resource languages maintain relatively stable probing accuracy or show only slight improvements. Figure 1 shows that as model layers go deeper, English, French, and other high-resource languages could reach highest accuracy at the 11th layer. However, the probing accuracies of the low-resource languages have not improved significantly."}, {"title": "3.3 Similarity Correlation of Probing Vectors", "content": "In this part of our experiment, we explored the similarities and trends between the probing vectors $\\theta$ across different languages. Figure 2 and Figure 3 give an example of our results which is based on Qwen-1.8B model and Opinion dataset. We have the following observations.\n\u2022 In high-resource languages (i.e., English, German, French, Chinese, Spanish, Russian), the similarities between the probing vectors are high, but in low-resource languages, the similarities are low, even with those high-resource languages. Figure 2 is a demonstration of our observation. The similarity between German and French probing vectors is very high, but the similarity with Tamil is very low. Moreover, the probing vector similarities between Tamil and other languages are also low.\n\u2022 The similarity trends between high-resource languages and low-resource languages with English are completely different. Figure 3 shows that the probing vector trends of other languages exhibit similarities to English across different layers. For high-resource languages, the similarity fluctuates, whereas for low-resource languages, it remains relatively stable."}, {"title": "4 Related Work", "content": "Multilingual Abilities of LLMs. More and more researchers begin to focus on multilingual abilities of LLMs (Ali and Pyysalo, 2024; Jayakody and Dias, 2024). Some research examines the consistency of factual knowledge across languages in multilingual pretrained language models (PLMs) (Fierro and S\u00f8gaard, 2022; Qi et al., 2023). Some researchers put efforts into promoting the representation of low-resource languages (Abadji et al., 2022; Imani et al., 2023; Li et al., 2024). Those studies shows that LLMs still have great potential on multilingual capabilities.\nProbing Representations in LLMs. Probing is a popular method to investigate the internal representations for LLMs in recent days, which is widely used in LLM interpretability studies (Alain and Bengio, 2018; Taktasheva et al., 2021; Pimentel et al., 2020; Ferrando et al., 2024b; Wendler et al., 2024). Previous work demonstrate that different layers typically acquired different information (Jin et al., 2024; Ju et al., 2024b). Various works using probing technique to assess how they encode linguistic features (Liu et al., 2023; Marks and Tegmark, 2024).\nOur study employs probing techniques to examine LLMs' performance and internal representations across different languages. The most closely related work is the Language Ranker (Li et al., 2024), which uses cosine similarity between a language's representation and English as a baseline. In contrast, our method utilizes linear classifier probing to evaluate performance across languages. This approach allows us to directly assess the model's ability to extract language-specific information, providing a more detailed view of LLMs' multilingual capabilities."}, {"title": "5 Conclusions and Future Work", "content": "In this work, our multilingual probing experiments on LLMs reveal significant disparities in performance and representational qualities across languages. Specifically, high-resource languages consistently achieve higher probing accuracy and exhibit similar trends to English, with accuracy improving significantly in deeper layers. We also observe high similarities between probing vectors of high-resource languages, while low-resource languages demonstrate lower similarities both among themselves and with high-resource languages. These findings indicate the current limitations of LLMs in handling low-resource languages. In future, we plan to conduct research to address these gaps by developing more equitable and effective multilingual language models. Besides, we plan to extend this research to multimodal models that incorporate visual and textual information, analyzing how this affects multilingual performance."}, {"title": "Limitations", "content": "In this work, we use machine translation to generate the prompt templates and question sentences from English to other languages, which may introduce noise. We only experiment with 5 open-source LLMs and two datasets. In the future, we would like to expand these findings with other datasets and models to confirm how well the LLMs' performance and representations generalize in these settings. Additionally, we just utilized linear classifier probing to do the experiments. We plan to explore more sophisticated probing methods beyond linear classifiers, which could offer deeper insights into the nature of linguistic representations within LLMs."}, {"title": "A Model and corresponding layers", "content": "We show the model and corresponding layers in Table 2"}, {"title": "B Prompt Templates", "content": "We manually design the original natural language templates in English for both two datasets, and translate them into other languages using google translate, with a focus on expressing the exact same meaning and intent. This allows us to prevent any information differences regarding the prompt design. We present the full set of prompt templates for all languages in Figure 4."}, {"title": "C Multilingual Accuracy", "content": "In this section, we present more results on multilingual accuracy across our five evaluated models (Qwen-0.5B, Qwen-1.8B, Qwen-7B, Gemma-2B, Gemma-7B) on the Opinion dataset. We provide layer-wise accuracy plots in Figure 5 and Figure 6. These visualize how probing accuracy changes across model layers for all 16 languages. As we described in the previous section, for the Opinion dataset, the layer-wise multilingual accuracy of the five models is similar to that of the Cities dataset, which also reflects the extensiveness of our results and improves the credibility of the conclusions."}, {"title": "D Similarity Correlation of Probing Vectors", "content": "In this section, we introduce the similarities between probing vectors across different languages and show results experimental results on two datasets (Opinion, Cities) and five evaluated models (Qwen-0.5B, Qwen-1.8B, Qwen-7B, Gemma-2B, Gemma-7B) not shown in the previous section. We present:\n\u2022 Correlation heatmaps (Figure 7 - Figure 15): These visualize the pairwise similarities between probing vectors of all 16 languages. The heatmaps reveal clusters of similar languages and highlight differences between high and low-resource groups. There are slight differences in the results for different models and datasets, but overall they are still consistent with our analysis.\n\u2022 Layer-wise similarity plots (Figure 16 - Figure 24): These show how the similarity of each language's probing vector to English changes across model layers. This reveals interesting dynamics in how different languages' representations evolve through the model."}]}