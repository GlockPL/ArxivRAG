{"title": "A Deep Learning-based Pest Insect Monitoring System for Ultra-low\nPower Pocket-sized Drones", "authors": ["Luca Crupi", "Luca Butera", "Alberto Ferrante", "Daniele Palossi"], "abstract": "Abstract-Smart farming and precision agriculture repre-\nsent game-changer technologies for efficient and sustainable\nagribusiness. Miniaturized palm-sized drones can act as flexible\nsmart sensors inspecting crops, looking for early signs of po-\ntential pest outbreaking. However, achieving such an ambitious\ngoal requires hardware-software codesign to develop accurate\ndeep learning (DL) detection models while keeping memory and\ncomputational needs under an ultra-tight budget, i.e., a few\nMB on-chip memory and a few 100s mW power envelope. This\nwork presents a novel vertically integrated solution featuring\ntwo ultra-low power System-on-Chips (SoCs), i.e., the dual-core\nSTM32H74 and a multi-core GWT GAP9, running two State-\nof-the-Art DL models for detecting the Popillia japonica bug.\nWe fine-tune both models for our image-based detection task,\nquantize them in 8-bit integers, and deploy them on the two\nSoCs. On the STM32H74, we deploy a FOMO-MobileNetV2\nmodel, achieving a mean average precision (mAP) of 0.66\nand running at 16.1 frame/s within 498mW. While on the\nGAP9 SoC, we deploy a more complex SSDLite-MobileNetV3,\nwhich scores an mAP of 0.79 and peaks at 6.8 frame/s within\n33 mW. Compared to a top-notch RetinaNet-ResNet101-FPN\nfull-precision baseline, which requires 14.9\u00d7 more memory and\n300\u00d7 more operations per inference, our best model drops only\n15% in mAP, paving the way toward autonomous palm-sized\ndrones capable of lightweight and precise pest detection.", "sections": [{"title": "I. INTRODUCTION", "content": "A crucial aspect of any modern farming activity is a\nprecise and timely intervention in the case of pest (insect)\ninfestations to minimize the production/economic damage\nand the environmental impact of the required treatments [1].\nFor example, early identification of harmful bugs can lead\nto ad-hoc treatments, such as spraying only part of the culti-\nvated field or a few plants, up to highly accurate treatments\nof only part of single trees/plants. Conversely, traditional\nmass-scale farming productions have adopted coarse-grained\nstrategies, spraying chemicals in the entire cultivated field,\neven in isolated or partial infestations. An important step\nforward in the accuracy of treatments has been possible\nby disseminating traps in the cultivated area to monitor the\npresence of specific pests, such as Cydia pomonella [2] or\nPopillia japonica [3]. However, early approaches required\ncostly and time-consuming expert human operators' inter-\nvention to manually inspect the traps and collect information\nabout the condition of the crop [4].\nIn the last decade, thanks to the advent of Internet-of-\nThings (IoT) technologies, the diffusion of embedded ultra-\nlow power Systems-on-Chips (SoCs) and sensors' miniatur-\nization, trap-based precision agriculture has become smarter\nby automating the monitoring procedure with embedded\ndevices integrated on the traps [5]. These battery-powered\nsystems are usually composed of an image sensor (e.g.,\ncolor or infrared camera), a low-power microcontroller unit\n(MCU), and a radio (e.g., WiFi, GSM, 4/5G) to stream data\n(e.g., images) to a power-unconstrained remote server for the\nanalysis. Despite the paramount improvement compared to\nhuman-operated traps, these State-of-the-Art (SoA) solutions\nstill require costly external infrastructure, i.e., mainframes\nand servers, and need high-throughput radio connectivity\nwith the additional disadvantage of draining small-capacity\nbatteries typically available on the traps [5].\nIn this context, our work provides a vertically inte-\ngrated system for accurate pest detection employing ultra-\nconstrained embedded MCUs, i.e., within a few 100s mW\npower envelope and a few MB on-chip memory, and cheap\nlow-resolution cameras. Leveraging SoA deep learning mod-\nels for detecting Popillia japonica bugs, we present a novel\nhardware-software co-design that is an ideal fit for both\ntraditional traps and aboard miniaturized palm-sized nano-\ndrones, which can autonomously and dynamically inspect\ncrops, as drafted in Figure 1. Employing nano-drones, such\nas miniaturized blimps [7] in greenhouses or autonomous\nquadrotors [8], for this type of monitoring and detection\nactivities has the additional advantage of flexibility. For\nexample, these tiny (less than 10cm in diameter) robotic\nplatforms can easily reach locations where traditional traps\nare not deployable or unreachable by bulky robotic arms,\ne.g., attached to trucks and tractors.\nTherefore, to achieve our goal, we must face both the\nchallenges posed by hardware-constrained devices and de-"}, {"title": "II. RELATED WORK", "content": "Insect detection systems, using images and deep learning\nmodels, mostly leverage SoA object detectors, as surveyed\nin [12]. For example, [13] leverages a FasterRCNN-based de-\ntector [14], while [15] uses a Single Shot Multibox Detector\n(SSD) [16], [17] uses RetinaNet [18], and [19] employs R-\nFCN [20], for detecting small insects. Table I reports the\nmodels studied in [6] extended by two smaller MobileNet-\nbased models [10], [9] employed in our work (last two bold\nlines). This broad comparison considers the size of the input\nimage, the number of parameters, the computational cost in\nMAC operations, the device used for testing the model, and\nthe detection mAP with the same testing dataset used in\nour work. Even though most networks perform remarkably\nwell in mAP, i.e., up to 0.933, they require desktop-class\ndevices to achieve a real-time throughput (max 61 frame/s).\nThis power-hungry class of computational devices is clearly\nunsuitable for IoT battery-powered smart traps or nano-\ndrones due to their power requirement (~100 W) and weight\n(~1kg).\nAs for other IoT applications, images are often collected\nusing camera-equipped IoT nodes, but the actual pest de-\ntection algorithms run in the cloud [21]. Nodes deployed in\nthe fields need to be cheap and, thus, extremely limited"}, {"title": "III. SYSTEM IMPLEMENTATION", "content": "In this section, we discuss our two system designs featur-\ning two ultra-low power embedded devices, i.e., a widely\nused dual-core Arduino Portenta H7 and a novel multi-\ncore GWT GAP9 SoC featuring the NE16 convolutional\nhardware accelerator. Then, given these two devices' signif-\nicant memory and computing power differences, we explore\ntwo alternative SoA CNNs for pest detection: a lightweight\nFOMO-MobileNetV2 [10], which we deploy on the Portenta,\nand a more complex SSDLite-MobileNetV3 [10], [9] for\nthe more capable GAP9 SOC almost 100\u00d7 more MAC\noperations per inference than the FOMO-MobileNetV2.\nA. Platforms\nThe detection systems we propose are based on two widely\ndifferent platforms: the Arduino Portenta H7 board and the\nGAP9 evaluation kit. The Arduino Portenta board is extended\nwith the Portenta vision shield, which provides a grayscale,\nultra-low power Himax HM-01B0 camera with a maximum\nresolution of 320\u00d7320 pixels, which is also available on the\nGAP9 development kit. The main differences between the\ntwo boards arise when comparing their computational re-\nsources. The Arduino Portenta, depicted in Figure 2, features\nan STM32H747 SoC with two cores: a Cortex\u00aeM4, running\nat a 240 MHz, and a Cortex\u00aeM7, running at 480 MHz.\nThe two cores can communicate via a remote procedure\ncall mechanism. Both cores have a double-precision float-\ning point unit (FPU) that allows the computations to be\nperformed directly in floating point arithmetic. The Portenta\nboard features two off-chip memories, a 16 MB FLASH and\na 8 MB RAM. The STM32H747's memory hierarchy instead\nis composed by a 2MB FLASH, a 1MB L2 memory, and a\n16kB L1 memory.\nB. Neural networks\nTo address the insect detection and classification task,\nwe use two CNNs: one devoted to the Arduino Portenta\nH7 and one for the GAP9 development kit. This choice is\ndue to their differences, as highlighted in Section III-A. For\nthis work, we fine-tune, test, and deploy two architectures\nbased on MobileNet [10], [9] and pre-trained on the COCO\ndataset [11]. Fine-tuning is a common transfer-learning [28]\ntechnique that takes a model trained for a task, e.g., im-"}, {"title": "IV. RESULTS", "content": "A. Object detection performance\nIn this section, we evaluate the detection accuracy per each\ninsect class, where the dangerous Popillia japonica is the\none we want to detect. We evaluate the network accuracy\nconsidering the mAP, a standard metric for object detection\ntasks, which ranges between 0 and 1. We consider an insect\ncorrectly detected if the Intersection over Union (IoU) of the\nbounding box produced by the network with the ground truth\nis at least 0.5, thus applying the COCO standard for object\ndetection [11]. We test the networks on our 660 samples\ntest set, and we explore different types of input (i.e., image\ncolor and resolution), as well as models deployed in float32,\nfloat16, and int8 (quantized). Figure 7 summarizes the mAP\nperformance of all the tested networks, with all combinations\nof grayscale and RGB inputs and considering float32 and\nint8 quantized versions.\nConsidering the models in float32 with RBG inputs, the\nSSDLite-MobileNetV3 is the best-performing with an mAP\nscore of 0.80 (input size 320\u00d7240), while the FOMO-\nMobileNetV2 achieves 0.78 in mAP (input size 160\u00d7160),\nand finally the smallest FOMO-MobileNetV2 marks an\nmAP of 0.67 (input size 96\u00d796). Then, assuming a\ncheaper monochrome camera, we assess the performances\nof the CNNs by feeding grayscale inputs but still em-\nploying float32 arithmetic. In this case, the SSDLite-\nMobileNetV3 (input size 320\u00d7240), FOMO MobileNetV2\n(input size 160\u00d7160), and the FOMO-MobileNetV2 (input\nsize 96\u00d796), respectively score an mAP of 0.79, 0.62, and\n0.55. The relative performances are kept the same, being the\nSSDLite-MobileNetV3 still the most accurate, while all three\nnetworks drop in mAP due to the grayscale input (losing\nfrom 0.01 to 0.16 in mAP). Finally, considering the SSDLite-\nMobileNetV3 deployed in float16, to take full advantage\nof the single-precision FPUs available on the GAP9 SoC, we\nobserve no drop in mAP compared with the double-precision\nimplementation. Then Instead, moving from float32 to\nint8 quantized versions of all CNNs, we see a minimal\ndrop in performance, always lower than 3%.\nB. Embedded systems performance\nIn this section, we present a thorough performance as-\nsessment of all models deployed on both devices, i.e.,\nthe Arduino Portenta (STM32H74 MCU) and the GAP9\nevaluation kit. Table III reports three configurations of the\nGAP9 SoC, spanning the voltage from 0.65 V and 0.8 V,\nenabling different clock speeds, up to 370 MHz. In our\nexperiments, we use the maximum efficiency configuration\nsince it achieves the longest battery duration given a target\nthroughput. In Table IV, we present our analysis regarding\nmemory requirements, latency for one-image inference, and\nthe total power consumption (including both compute unit\nand off-chip memories) for all CNNs/devices. For each\nmodel, we report the data type used (float32 and int8\nfor the STM32H74, and float16 and int 8 for the GAP9),\nand the input image size (\u00d73 in case of RGB images and\n\u00d71 for grayscale ones). For the GAP9, the peak memory is\ncomputed considering the network's layer with the maximum\nrequirements for input tensor, weights, and output tensor."}, {"title": "V. CONCLUSIONS", "content": "This work presents a novel hardware-software design for\nimage-based pest detection, deployable in battery-powered\nsmart traps and aboard ultra-constrained nano-drones. We\npresent two system designs featuring two ultra-low power\nembedded devices, i.e., a widely used dual-core Arduino Por-\ntenta H7 and a novel multi-core GWT GAP9 SoC featuring\nthe NE16 hardware accelerator. Given these two devices'\nsignificant memory and computing power differences, we ex-\nplore two alternative SoA CNNs. On the Portenta, we deploy\na lightweight FOMO-MobileNetV2 (5.88 MMAC/inference),\ncapable of reaching 0.66 mAP on detecting the Popilla japon-\nica bug, running at 16.1 frame/s and consuming 498mW.\nWhile on the GAP9 SoC, we deploy a more complex\nSSDLite-MobileNetV3 CNN (584 MMAC/inference), scor-\ning an mAP of 0.79 with a throughput of 6.8 frame/s at\n33 mW. With our hardware-software codesign, we present a\nfine-tuning procedure with a custom dataset for the detection\ntask, an 8-bit quantization stage for efficient exploitation of\nthe two SoCs, and finally, the implementation and deploy-\nment of our workloads. Compared to the huge first-in-class\nRetinaNet-ResNet101-FPN (174850 MMAC/inference), our\nbest model drops only 15% in mAP, paving the way toward\nautonomous palm-sized drones capable of lightweight and\nprecise pest detection."}]}