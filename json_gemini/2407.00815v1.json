{"title": "A Deep Learning-based Pest Insect Monitoring System for Ultra-low\nPower Pocket-sized Drones", "authors": ["Luca Crupi", "Luca Butera", "Alberto Ferrante", "Daniele Palossi"], "abstract": "Smart farming and precision agriculture repre-\nsent game-changer technologies for efficient and sustainable\nagribusiness. Miniaturized palm-sized drones can act as flexible\nsmart sensors inspecting crops, looking for early signs of po-\ntential pest outbreaking. However, achieving such an ambitious\ngoal requires hardware-software codesign to develop accurate\ndeep learning (DL) detection models while keeping memory and\ncomputational needs under an ultra-tight budget, i.e., a few\nMB on-chip memory and a few 100s mW power envelope. This\nwork presents a novel vertically integrated solution featuring\ntwo ultra-low power System-on-Chips (SoCs), i.e., the dual-core\nSTM32H74 and a multi-core GWT GAP9, running two State-\nof-the-Art DL models for detecting the Popillia japonica bug.\nWe fine-tune both models for our image-based detection task,\nquantize them in 8-bit integers, and deploy them on the two\nSoCs. On the STM32H74, we deploy a FOMO-MobileNetV2\nmodel, achieving a mean average precision (mAP) of 0.66\nand running at 16.1 frame/s within 498mW. While on the\nGAP9 SoC, we deploy a more complex SSDLite-MobileNetV3,\nwhich scores an mAP of 0.79 and peaks at 6.8 frame/s within\n33 mW. Compared to a top-notch RetinaNet-ResNet101-FPN\nfull-precision baseline, which requires 14.9\u00d7 more memory and\n300\u00d7 more operations per inference, our best model drops only\n15% in mAP, paving the way toward autonomous palm-sized\ndrones capable of lightweight and precise pest detection.", "sections": [{"title": "I. INTRODUCTION", "content": "A crucial aspect of any modern farming activity is a\nprecise and timely intervention in the case of pest (insect)\ninfestations to minimize the production/economic damage\nand the environmental impact of the required treatments [1].\nFor example, early identification of harmful bugs can lead\nto ad-hoc treatments, such as spraying only part of the culti-\nvated field or a few plants, up to highly accurate treatments\nof only part of single trees/plants. Conversely, traditional\nmass-scale farming productions have adopted coarse-grained\nstrategies, spraying chemicals in the entire cultivated field,\neven in isolated or partial infestations. An important step\nforward in the accuracy of treatments has been possible\nby disseminating traps in the cultivated area to monitor the\npresence of specific pests, such as Cydia pomonella [2] or\nPopillia japonica [3]. However, early approaches required\ncostly and time-consuming expert human operators' inter-\nvention to manually inspect the traps and collect information\nabout the condition of the crop [4].\nIn the last decade, thanks to the advent of Internet-of-\nThings (IoT) technologies, the diffusion of embedded ultra-\nlow power Systems-on-Chips (SoCs) and sensors' miniatur-\nization, trap-based precision agriculture has become smarter\nby automating the monitoring procedure with embedded\ndevices integrated on the traps [5]. These battery-powered\nsystems are usually composed of an image sensor (e.g.,\ncolor or infrared camera), a low-power microcontroller unit\n(MCU), and a radio (e.g., WiFi, GSM, 4/5G) to stream data\n(e.g., images) to a power-unconstrained remote server for the\nanalysis. Despite the paramount improvement compared to\nhuman-operated traps, these State-of-the-Art (SoA) solutions\nstill require costly external infrastructure, i.e., mainframes\nand servers, and need high-throughput radio connectivity\nwith the additional disadvantage of draining small-capacity\nbatteries typically available on the traps [5].\nIn this context, our work provides a vertically inte-\ngrated system for accurate pest detection employing ultra-\nconstrained embedded MCUs, i.e., within a few 100s mW\npower envelope and a few MB on-chip memory, and cheap\nlow-resolution cameras. Leveraging SoA deep learning mod-\nels for detecting Popillia japonica bugs, we present a novel\nhardware-software co-design that is an ideal fit for both\ntraditional traps and aboard miniaturized palm-sized nano-\ndrones, which can autonomously and dynamically inspect\ncrops, as drafted in Figure 1. Employing nano-drones, such\nas miniaturized blimps [7] in greenhouses or autonomous\nquadrotors [8], for this type of monitoring and detection\nactivities has the additional advantage of flexibility. For\nexample, these tiny (less than 10cm in diameter) robotic\nplatforms can easily reach locations where traditional traps\nare not deployable or unreachable by bulky robotic arms,\ne.g., attached to trucks and tractors.\nTherefore, to achieve our goal, we must face both the\nchallenges posed by hardware-constrained devices and de-"}, {"title": "II. RELATED WORK", "content": "Insect detection systems, using images and deep learning\nmodels, mostly leverage SoA object detectors, as surveyed\nin [12]. For example, [13] leverages a FasterRCNN-based de-\ntector [14], while [15] uses a Single Shot Multibox Detector\n(SSD) [16], [17] uses RetinaNet [18], and [19] employs R-\nFCN [20], for detecting small insects. Table I reports the\nmodels studied in [6] extended by two smaller MobileNet-\nbased models [10], [9] employed in our work (last two bold\nlines). This broad comparison considers the size of the input\nimage, the number of parameters, the computational cost in\nMAC operations, the device used for testing the model, and\nthe detection mAP with the same testing dataset used in\nour work. Even though most networks perform remarkably\nwell in mAP, i.e., up to 0.933, they require desktop-class\ndevices to achieve a real-time throughput (max 61 frame/s).\nThis power-hungry class of computational devices is clearly\nunsuitable for IoT battery-powered smart traps or nano-\ndrones due to their power requirement (~100 W) and weight\n(~1kg).\nAs for other IoT applications, images are often collected\nusing camera-equipped IoT nodes, but the actual pest de-\ntection algorithms run in the cloud [21]. Nodes deployed in\nthe fields need to be cheap and, thus, extremely limited"}, {"title": "III. SYSTEM IMPLEMENTATION", "content": "In this section, we discuss our two system designs featur-\ning two ultra-low power embedded devices, i.e., a widely\nused dual-core Arduino Portenta H7 and a novel multi-\ncore GWT GAP9 SoC featuring the NE16 convolutional"}, {"title": "A. Platforms", "content": "The detection systems we propose are based on two widely\ndifferent platforms: the Arduino Portenta H7 board and the\nGAP9 evaluation kit. The Arduino Portenta board is extended\nwith the Portenta vision shield, which provides a grayscale,\nultra-low power Himax HM-01B0 camera with a maximum\nresolution of 320\u00d7320 pixels, which is also available on the\nGAP9 development kit. The main differences between the\ntwo boards arise when comparing their computational re-\nsources. The Arduino Portenta, depicted in Figure 2, features\nan STM32H747 SoC with two cores: a Cortex\u00aeM4, running\nat a 240 MHz, and a Cortex\u00aeM7, running at 480 MHz.\nThe two cores can communicate via a remote procedure\ncall mechanism. Both cores have a double-precision float-\ning point unit (FPU) that allows the computations to be\nperformed directly in floating point arithmetic. The Portenta\nboard features two off-chip memories, a 16 MB FLASH and\na 8 MB RAM. The STM32H747's memory hierarchy instead\nis composed by a 2MB FLASH, a 1MB L2 memory, and a\n16kB L1 memory."}, {"title": "B. Neural networks", "content": "To address the insect detection and classification task,\nwe use two CNNs: one devoted to the Arduino Portenta\nH7 and one for the GAP9 development kit. This choice is\ndue to their differences, as highlighted in Section III-A. For\nthis work, we fine-tune, test, and deploy two architectures\nbased on MobileNet [10], [9] and pre-trained on the COCO\ndataset [11]. Fine-tuning is a common transfer-learning [28]\ntechnique that takes a model trained for a task, e.g., im-"}, {"title": "C. Dataset", "content": "Our fine-tuning labeled dataset comes from [6] and com-\nprises more than 3,300 images of insects, selected through a\nstrict filtering process from an initial collection of more than\n36,000 gathered through the internet. This filtering ensures\nthe near-absence of duplicates that could harm the model\nevaluation fairness. It contains three insect classes: Popillia\njaponica, Cetonia aurata, and Phyllopertha horticola. Popillia\njaponica is a dangerous pest insect; the other two, while\nsimilar to Popillia japonica and often mistaken for it, are\nnot. We have 1,422, 1,318, and 877 samples for each class,\nrespectively. Figure 6 reports an example of the three classes\nof insects. We divided the dataset into training and testing\nsplits, with an 80-20% ratio."}, {"title": "D. Deployment", "content": "To deploy the CNNs, we leverage two tools, one for\neach platform, since we rely on tools that convert high-\nlevel Python code to C platform-specific code. The first,\nfor the Arduino Portenta board, is based on the Edge\nImpulse project\u00b2; the second, for the GAP9, is based on\nthe tool developed by GWT, namely, NN-Tool. The Edge\nImpulse tool provides an easy-to-use interface to deploy\nneural networks on many embedded devices, including the\nArduino Portenta. The tool allows the selection, fine-tuning,\nand deployment, in int8 or float32, of a custom or pre-\ndesigned neural network. We deploy different variants of\nthe networks in both fashions (int8 and float32) since the\nArduino Portenta H7 SoC has hardware support for double-\nprecision floating point operations, i.e., an FPU in each\ncore. Each network is exported as a tflite model, and the\ninference phase is performed on the embedded device in a\nMicroPython-based system, thanks to OpenMV. OpenMV,\nwith MicroPython, provides a USB streamer and the tensor\nmovements in the memory hierarchy with a reduced im-\nplementation effort but introduces computational overhead.\nFurthermore, the OpenMV network execution does not use\nthe RAM to store the activations during the inferences. The\ntensors need to be entirely in the L1 and L2 memory during\nthe computation, thus avoiding the tiling from the off-chip\nmemories but limiting the execution of the network within\n1MB of memory. For each network, we deploy an int8\nquantized version of it, leveraging the reduced amount of\nmemory needed for the execution of these versions of the\nnetwork.\nNN-Tool is the deployment tool developed by GWT to\ndeploy neural networks on their platforms, such as GAP8\nand GAP9 SoCs. It allows the deployment starting from\nan onnx or tflite file. We finetune the COCO pre-trained\nMobileNet V3 with the SSDLite detector available in Pytorch\non our dataset to obtain a float32 version of the CNN. We\nthen convert it into an onnx file that can be used with NN-\nTool. To obtain a model that is compliant with the layers\ndeplyable with NN-Tool, such layers include convolutional,\nfully connected, and dropout layers or skip connections, to\nname a few types of operations supported. However, it does\nnot support the deployment of the non-maximal suppression\nlayer included in the architecture, and as such, we remove the\noperation from the onnx and apply it manually. NN-Tool also\nperforms the tiling and movement of tensors in the memory\nhierarchy to exploit all of its levels. Since the platform has a\nsingle-precision FPU, the network is deployed in both float16\nand int8 fashions. The NE16 can be leveraged for CNN\napplications since the NN-tool pipeline supports deploying\nnetworks specifically tailored for this type of accelerator.\nSince it can be used only with int8 networks, we deploy\nthree architectures with NN-tool: float16, int8, and int8 with\nNE16 hardware acceleration.\nEven if the platforms have an FPU onboard and, as such,\nthey can perform floating point operations without relying\non soft-float emulation, we perform the quantization to int8\narithmetic since it is a convenient method to reduce the\nmemory occupation with a limited reduction in the accuracy\nof the network. The quantization requires a calibration set for\nestimating the range of values that each tensor can assume.\nOur calibration set is a subset of the training set."}, {"title": "IV. RESULTS", "content": "In this section, we evaluate the detection accuracy per each\ninsect class, where the dangerous Popillia japonica is the\none we want to detect. We evaluate the network accuracy\nconsidering the mAP, a standard metric for object detection\ntasks, which ranges between 0 and 1. We consider an insect\ncorrectly detected if the Intersection over Union (IoU) of the\nbounding box produced by the network with the ground truth\nis at least 0.5, thus applying the COCO standard for object\ndetection [11]. We test the networks on our 660 samples\ntest set, and we explore different types of input (i.e., image\ncolor and resolution), as well as models deployed in float32,\nfloat16, and int8 (quantized). Figure 7 summarizes the mAP\nperformance of all the tested networks, with all combinations"}, {"title": "B. Embedded systems performance", "content": "In this section, we present a thorough performance as-\nsessment of all models deployed on both devices, i.e.,\nthe Arduino Portenta (STM32H74 MCU) and the GAP9\nevaluation kit. Table III reports three configurations of the\nGAP9 SoC, spanning the voltage from 0.65 V and 0.8 V,\nenabling different clock speeds, up to 370 MHz. In our\nexperiments, we use the maximum efficiency configuration\nsince it achieves the longest battery duration given a target\nthroughput. In Table IV, we present our analysis regarding\nmemory requirements, latency for one-image inference, and\nthe total power consumption (including both compute unit\nand off-chip memories) for all CNNs/devices. For each\nmodel, we report the data type used (float32 and int8\nfor the STM32H74, and float16 and int 8 for the GAP9),\nand the input image size (\u00d73 in case of RGB images and\n\u00d71 for grayscale ones). For the GAP9, the peak memory is\ncomputed considering the network's layer with the maximum\nrequirements for input tensor, weights, and output tensor.\nInstead, for the STM32H74, the computation also includes\nthe input image (never freed for the entire inference) and\nthe memory footprint of a micro-python-based operating\nsystem (OS) that presses on the same 1MB L2 memory.\nWhile for the GAP9 SoC, the deployment tool (NN-Tool) can\norganize data to exploit both on-chip L2 memory (1.6 MB)\nand the off-chip ones (up to 32MB), the STM32H74's tool\n(Edge Impulse) can not generate deployable code for CNNS\nrequiring more than the 1MB-L2 on-chip memory this\nlimitation is marked with a in Table IV.\nOn the STM32H74 MCU, we deploy the FOMO-\nMobilenNetV2, varying the input size from 96\u00d796\u00d71\nup to 160\u00d7160\u00d73. Among the deployable CNNs on the\nSTM32H74, the one using a monochrome 96\u00d796 image with\nquantized (int8) representation is the smallest (239 kB L2)\nand the fastest, achieving 17.5 frame/s. All the deployable\nnetworks require less than 501 mW, primarily due to MCU's\npower consumption, which, in typical operating conditions,\nconsumes ~480mW (using only the M7 core).\nOn the GAP9 SoC, we deploy the SSDLite-MobileNetV3\nwith a fixed input size of 320\u00d7240\u00d73, considering three ex-\necution conditions: i) float16 running on general-purpose\nCL (with FPU hardware support), ii) int8 quantized run-\nning on the CL, and iii) with int8 but exploiting the\nNE16 convolutional accelerator. As expected, the float16\nversion is the most memory demanding, requiring up to\n~7MB of memory, and the slowest. Despite the platform's\nFPU hardware support, the float16 network reaches only\n2.1 frame/s, consuming 40.6mW. Instead, employing the\nNE16 accelerator with the int8 data type, requires up to\n3.4 MB of memory and reaches up to 6.8 frame/s within only\n34 mW.\nFinally, in Figure 8, we report the waveform of the\npower consumption of the GAP9 SoC in three cases, i.e.,\nwith the float16 version, the int8 executed on the general-\npurpose multicore cluster, and the int8 running on the NE16\naccelerator, respectively in Figure 8-A, -B and -C. All three\nwaveforms show small periods of idleness (up to 41 ms in\ntotal), in particular, at the beginning of the execution of the\nnetwork, where the tensor movements between memories\ndo not overlap with the execution of the network. The idle\nperiods are reduced going forward in the execution since the\nsize of the tensors reduces."}, {"title": "C. Discussion", "content": "Compared to the best-in-class network, the RetinaNet-\nResNet101-FPN full-precision [6], our best model drops only\n15% in accuracy evaluating it with the mAP but reduces the\nnumber of operations by 299\u00d7 and the memory required by\n14.9\u00d7 allowing the deployment of the system on an ultra-low\npower embedded system such as our GAP9-based board. On\nthe other hand, the FOMO MobileNetV2 reduces the number\nof operations by ~20000\u00d7 and the memory by ~2500\u00d7\nw.r.t. the RetinaNet-ResNet101-FPN full-precision [6] with\na reduction of mAP of 0.27. The impressive parameters'\nreduction of both our networks, the FOMO MobileNet V2\nand the SSDLite-MobileNetV3, allows the deployment of\nour system in traps as an IoT device to detect insects such as\nPopillia japonica, Cetonia aurata, and Phyllopertha horticola.\nWe envision a battery-powered insect detection system that\nuses one of our platforms, the Arduino Portenta H7 with the\nFOMO MobileNetV2 or the GAP9-based development kit\nrunning our MobileNetV3 with SSDLite, an ultra-low-power"}, {"title": "V. CONCLUSIONS", "content": "This work presents a novel hardware-software design for\nimage-based pest detection, deployable in battery-powered\nsmart traps and aboard ultra-constrained nano-drones. We\npresent two system designs featuring two ultra-low power\nembedded devices, i.e., a widely used dual-core Arduino Por-\ntenta H7 and a novel multi-core GWT GAP9 SoC featuring\nthe NE16 hardware accelerator. Given these two devices'\nsignificant memory and computing power differences, we ex-\nplore two alternative SoA CNNs. On the Portenta, we deploy\na lightweight FOMO-MobileNetV2 (5.88 MMAC/inference),\ncapable of reaching 0.66 mAP on detecting the Popilla japon-\nica bug, running at 16.1 frame/s and consuming 498mW.\nWhile on the GAP9 SoC, we deploy a more complex\nSSDLite-MobileNetV3 CNN (584 MMAC/inference), scor-\ning an mAP of 0.79 with a throughput of 6.8 frame/s at\n33 mW. With our hardware-software codesign, we present a\nfine-tuning procedure with a custom dataset for the detection\ntask, an 8-bit quantization stage for efficient exploitation of\nthe two SoCs, and finally, the implementation and deploy-\nment of our workloads. Compared to the huge first-in-class\nRetinaNet-ResNet101-FPN (174850 MMAC/inference), our\nbest model drops only 15% in mAP, paving the way toward\nautonomous palm-sized drones capable of lightweight and\nprecise pest detection."}]}