{"title": "Controllable Discovery of Intents: Incremental Deep Clustering Using Semi-Supervised Contrastive Learning", "authors": ["Mrinal Rawat", "Hithesh Sankararaman", "Victor Barres"], "abstract": "Deriving value from a conversational AI system depends on the capacity of a user to translate the prior knowledge into a configuration. In most cases, discovering the set of relevant turn-level speaker intents is often one of the key steps. Purely unsupervised algorithms provide a natural way to tackle discovery problems but make it difficult to incorporate constraints and only offer very limited control over the outcomes. Previous work has shown that semi-supervised (deep) clustering techniques can allow the system to incorporate prior knowledge and constraints in the intent discovery process. However they did not address how to allow for control through human feedback. In our Controllable Discovery of Intents (CDI) framework domain and prior knowledge are incorporated using a sequence of unsupervised contrastive learning on unlabeled data followed by fine-tuning on partially labeled data, and finally iterative refinement of clustering and representations through repeated clustering and pseudo-label fine-tuning. In addition, we draw from continual learning literature and use learning-without-forgetting to prevent catastrophic forgetting across those training stages. Finally, we show how this deep-clustering process can become part of an incremental discovery strategy with human-in-the-loop. We report results on both CLINC and BANKING datasets. CDI outperforms previous works by a significant margin: 10.26% and 11.72% respectively.", "sections": [{"title": "1 Introduction", "content": "Conversational AI encompasses human-machine interactions (e.g. voice assistants, self-service bots, ...), speaker assistance during human-human conversations (e.g. customer support agent guidance, speaker coaching, ...), batch analysis of conversations, and many more use cases. Most of those make use of the concept of 'intent' to conceptualize the relevant dimensions at the level of a conversational turn. Getting value out of those systems rests therefore in finding the intent set, i.e. the set of turn-level labels, that best reflects the practical needs of the business.\nBusinesses accumulate tacit and explicit knowledge about their processes (Polanyi and Sen, 2009; Nonaka and Takeuchi, 2007). But those are not often couched in ways that can be directly translated into a system's configuration. To make this possible, it is first necessary to help the user formalize their prior knowledge and processes in a way that can make them legible for a conversational AI system. In business-to-business (B2B) commercial contexts, in particular, business analysts often have to spend a large amount of time eliciting requirements from the client and compiling information prior to configuring a system. Importantly, even when formal knowledge already exists, businesses look to AI systems to help them \"know what they don't already know\". In the case of intents, this could take the form of helping them discover new intents to better understand their customer base, or helping them evaluate and reshape their understanding of the intent landscape (that can be sub-optimal in its current form).\nUnsupervised algorithms provide a natural way to tackle such problems (Chatterjee and Sengupta, 2020; Benayas et al., 2023). Purely unsupervised algorithms however suffer from the fact that they lack the capacity to incorporate prior knowledge and do not offer any control over the outcome (beyond the setting of certain hyper-parameters). The objective therefore is to provide a tool that helps align an intent set with business needs. This tool should facilitate at a minimum: (1) the incorporation of domain knowledge, including the specification of required intents, and (2) the efficient intervention of an expert to guide the system toward relevant solutions.\nPrevious work has shown how using a combination of contrastive learning, fine-tuning, and semi-supervised learning in addition to (deep) clustering allows the system to learn to incorporate prior knowledge and constraints (Zhang et al., 2021; Shen et al., 2021). A parallel line of research has focused on using human-in-the-loop approaches to iteratively incorporate human feedback (Williams et al., 2015). To our knowledge, however, no work so far has looked into combining all those elements into a single architecture.\nWe present a novel approach to intent discovery that satisfies the 3 requirements mentioned above. Our contributions can be summarized as follows:\n\u2022 We show how domain and prior knowledge can be incorporated using a sequence of unsupervised contrastive learning on unlabeled data followed by fine-tuning on partially labeled data, and finally iterative refinement of clustering and representations through repeated clustering and pseudo-label fine-tuning.\n\u2022 We show how using the learning-without-forgetting method from continual learning prevents catastrophic forgetting across those training stages, leading to improved clustering results compared to previous work.\n\u2022 Finally we show how this deep-clustering process can become part of an incremental discovery strategy with human-in-the-loop."}, {"title": "2 Related Work", "content": "Earlier works in the field of intent discovery have predominantly followed an unsupervised approach, where embeddings for the data are generated and then clustering is applied to identify new intents. However, the quality of clustering can be greatly impacted by the method used for generating input representations. Recent approaches have utilized pre-trained transformers like BERT (Devlin et al., 2019) for generating sentence embeddings, either by extracting the [CLS] token embeddings or by mean-pooling all token embeddings. However, these methods often yield poor performance in tasks such as textual similarity and clustering, whereas sentence transformers (Reimers and Gurevych, 2019), such as MPNet, which are trained through Siamese-based training, are more suitable for such tasks. For clustering, partition-based techniques (MacQueen, 1967) and density-based methods (Ester et al., 1996) have been proposed, but they tend to under-perform with high-dimensional data.\nDeep clustering methods overcome this problem and improve the performance significantly by jointly optimizing both input representation and clustering using deep neural networks. DEC (Xie et al., 2016) trains an autoencoder with reconstruction loss and iteratively optimizes the networks, while DCN (Yang et al., 2017) introduces a K-Means loss as a penalty term to reconstruct the clustering loss. DeepCluster (Caron et al., 2018) uses the discriminative power of the convolutional neural network (CNN) and alternately performs K-Means and representation learning.\nMore recently, semi-supervised techniques have been widely used, such as DAC (Zhang et al., 2021)"}, {"title": "3 Methodology", "content": "As shown in Figure 2 we begin with a domain adaptation step, using unsupervised contrastive learning (UCL) to adapt a sentence transformer on the unlabeled dataset. This is followed by a two-stage supervised training approach using the labeled dataset to cluster the unlabelled data and identify new intents. To ensure that the model can continuously learn and adapt to new data, we implement the learning without forgetting technique (Li and Hoiem, 2018). This allows the model to incorporate new information while preserving previously learned knowledge. Further, we study the impact of enabling the incremental discovery of novel intents by incorporating human feedback in an efficient way."}, {"title": "3.1 Domain Adaptation", "content": ""}, {"title": "3.1.1 Unsupervised Contrastive Learning (UCL)", "content": "In Figure 2, we illustrate the first step of our approach: domain adaptation using unsupervised contrastive learning on the unlabeled dataset. Since in the case of unlabeled data, positive pairs are not readily available, we use the technique proposed in SimCSE (Gao et al., 2021). For every input sentence xi, we generate a positive pair xt by feeding the same input twice to the encoder with different dropout masks zi, zj. We note the embeddings hi and h. The remaining sentences serve as negative instances. The learning objective is described below:\nLucl = \u03a3log \\frac{e^{sim(h_i, h_i^t) / \\tau}}{\\sum_{j \\neq i} e^{sim(h_i, h_j^t) / \\tau}} (1)\nfor N sentences mini-batch where \u03c4 is the temperature hyper parameter, and sim(h1, h2) is the cosine similarity."}, {"title": "3.1.2 Sentence Transformer", "content": "We use a sentence transformer version of MPNet as our backbone model ('paraphrase-mpnet-base-v2'). The masked and permuted language modeling approach used to train MPNet has been shown to result in better language understanding capabilities (Yang et al., 2019). The sentence transformer version is trained using the Siamese network approach pioneered by sentence BERT (Reimers and Gurevych, 2019). Sentence embeddings are generated by first applying mean-pooling to the token embeddings extracted from the last hidden layer."}, {"title": "3.2 Stage1: Fine-tuning", "content": "In this first stage, we utilize a limited labeled dataset to fine-tune the model. This step allows the model to integrate the constraints and task-relevant dimensions implicitly revealed by the annotations. This step is similar to the DAC (Zhang et al., 2021),"}, {"title": "3.2.1 Learning without Forgetting (LwF)", "content": "As the model learns from the labeled data, we want to ensure that it does not forget what has been learned during the domain adaptation phase: discovery of relevant new intent requires the information carried by both the domain and the labeled data to be integrated prior to clustering. The threat of catastrophic forgetting is a well-known threat for transfer learning approaches (McCloskey and Cohen, 1989). To address this problem, Learning without Forgetting (LwF) (Li and Hoiem, 2018) was proposed which aims to preserve the previously learned knowledge while learning new tasks. It is inspired by KL-divergence which imposes an additional constraint that the parameters of the network while learning a new task and the parameters of the old network do not shift significantly. For our work, we adopt the LwF technique and use the following objective:\nLLWF = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} f(h_i^e).logf(h_i) (3)\nf(h_i) = \\frac{e^{w_{yi}h_i}}{\\sum_{k=1}^{K} e^{w_{yk}h_i}} = \\frac{e^{w_{yi} h_i}}{\\sum_{k=1}^{K} e^{w_{yk}h_i}} (4)\nwhere K is the number of known intents i.e classes, w is the classifier weights, hi is the model output after learning and he is the old model's output.\nFinally, we combine this objective with the classification objective:\nLsup = Lce + \\lambda LLwF (5)"}, {"title": "3.3 Stage2: Deep-Clustering using Pseudo Labels Training", "content": "We use the fine-tuned model to generate embeddings for all the turns in the dataset and perform K-means clustering. We assign pseudo-labels to each data point based on the K-means output and use these pseudo-labels for the supervised training of the model. Here also, to prevent catastrophic forgetting, we add an LwF objective to the cross-entropy loss. Furthermore, this stage differs significantly from the first stage, where the number of labels or classes also changes which may lead to catastrophic forgetting. Hence, we incorporate the LwF objective alongside cross-entropy to mitigate this issue (see 5).\nWe repeat this clustering + pseudo-labeling training step multiple times. To handle the assignment inconsistency problem - the K-means cluster indices are randomly assigned at each iteration resulting in different labels - we follow the method proposed in DAC (Li and Hoiem, 2018) and employ the Hungarian algorithm (Kuhn, 1955) to align the centroids and obtain the consistent labeling."}, {"title": "3.4 Controllable Intent Discovery (CDI)", "content": "In this section, we introduce a novel approach for allowing the user to control the intent discovery process. Discovery is done in an incremental manner using our in-house developed interactive tool capturing human feedback. Our approach starts with an empty labeled dataset D\u2081 = \u00d8, the unlabeled dataset Du, and an empty set of Intents I. We perform unsupervised contrastive learning (UCL) on the unlabeled dataset Du by employing MPNet as the backbone, as shown in Figure 3. Next, we choose the value of Kt i.e. number of clusters. In practice, the value of K is unknown due to the lack of information about the corpus. There are various approaches of calculating the optimal value of K as proposed in previous works (Shen et al., 2021; Zhang et al., 2021). However, in this work, we did not investigate in detail to calculate the optimal value of K and used the technique proposed by DAC.\nEstimation of K We initialize K' with a large number (e.g. 200 for our experiments which is approximately twice the largest value of intents in the datasets). However, in practical scenarios, a domain expert who uses our tool can set the initial value of K based on his understanding of the domain."}, {"title": "3.4.1 Incremental Deep Clustering", "content": "Our approach uses an incremental method to discover new intents and label the data simultaneously. At each iteration t, we use the trained model Mt-1 to extract representations of the unlabeled dataset Du and perform K-means clustering on these representations based on a chosen value of Kt. We want to highlight that at t\u2081 iteration, we use the model pre-trained with UCL loss. In the early iterations, the model may not have been fine-tuned sufficiently, leading to wrong cluster assignments for some samples. To mitigate this problem, we select only those samples which are closer to their cluster centroids, based on some threshold \u03b3. Specifically, we compute the cosine similarity si between each sample and its corresponding cluster centroid Ci and select the sample if si > \u03b3. We find that a threshold value between 0.7 and 0.99 works well in practice.\nWe then present the user with the resulting clusters along with the high-confidence samples sorted by confidence score si per cluster. Our tool allows them to interactively select or deselect samples within each cluster. The user can also merge similar clusters. At the end of each iteration, we have a labeled dataset Dt, and a set of newly identified intents denoted as It = (i1, i2, ...ik\u2081). We expand the labeled dataset as D\u2081 = D\u2081UDt and intent set as I = I U It respectively and use them to perform the stage 1 and stage 2 training along with the LwF loss to avoid catastrophic forgetting as described in above sections. In the next iteration, if the number of identified intents |I|, exceeds the value of Kt, we expand and update Kt+1 to be equal to |I|. Otherwise, we keep Kt+1 the same as Kt and continue this iterative process to discover new intents and label the data. We terminate this process once the value of Kt stops increasing."}, {"title": "4 Experimentation", "content": ""}, {"title": "4.1 Datasets", "content": "We conduct our experiments on two public benchmark intent datasets and one private dataset. Table 1 shows the dataset statistics.\nCLINC is a dataset for intent classification (Larson et al., 2019) that includes 22,500 queries spanning 150 intents in 10 different domains.\nBANKING is a detailed dataset in the banking domain (Casanueva et al., 2020) that consists of 13,083 queries related to customer service and covers 77 distinct intents.\nTELECOM Dataset is our private dataset which comprises of manually annotated transcripts of human-human spoken telephone conversations from the telecom customer support domain. Transcripts were generated by our in-house Kaldi-based ASR system consisting of several turns between agent and customer. In total, 1513 transcripts were collected, and for each one, our annotators identified the turn in which the caller's intent was expressed and assigned it to one of 16 pre-defined classes. However, this work only considers the intent turns as the input."}, {"title": "4.2 Baselines", "content": "In our work, we conducted a direct comparison between our proposed approach and two other existing methods, namely Deep Aligned Cluster (DAC) (Zhang et al., 2021) and Supervised Contrastive Learning (SCL) (Shen et al., 2021). DAC utilizes a pre-training strategy on a BERT-based backbone with limited known intent data, followed by training on pseudo-labeled data generated through a clustering algorithm. In contrast, SCL uses MPNet as the backbone and trains it on limited known intent data using a Supervised Contrastive loss (Khosla et al., 2020). To evaluate the performance of these methods, we ran experiments and reported the results by running their code if it was available, and if not, we implemented their methods based on the description provided in their papers."}, {"title": "4.3 Evaluation Metrics", "content": "Following established practices in the field, for each experiment, we report the normalized mutual information (NMI), adjusted rand index (ARI), and accuracy (ACC)."}, {"title": "4.4 Evaluation Setup", "content": "We used the same evaluation settings as defined by DAC (Zhang et al., 2021) and CDAC (Lin et al., 2020). We also use the same training, validation, and test set. Our experiments were conducted with three known intent ratios of 25%, 50%, and 75%. For each split, we randomly selected 10% of samples for the CLINC and BANKING datasets, and 20% for the TELECOM dataset, to be used as the labeled dataset. The remaining samples were treated as unlabeled data. We used the labeled dataset to train the MPnet model for multiple epochs and select the one that gives the best performance on the validation set. Our results were reported on the test set. To ensure a fair comparison, we kept the number of clusters K fixed as the ground-truth number of intents. We report the average results over five runs of experiments with different random seeds.\nTo evaluate the effectiveness of our incremental clustering approach, we created an automatic program that simulates a user providing input by selecting the correct samples for each cluster since we already have the ground truth labels. At the beginning of the process, we set the value of K for the first iteration based on the approach defined in Section 3.4. Specifically, the values of K\u2081 for the CLINC, BANKING, and TELECOM datasets were estimated as 100, 50, and 10 respectively. At each iteration t, based on the value of K, we perform K-means clustering on the extracted representations using the pre-trained model. We then present the clusters to the automatic program along with high-confidence samples. To determine the high-confidence samples, we set a confidence threshold of \u03b3 = 0.7 for the first iteration and \u03b3 = 0.95 for the subsequent iterations. To better simulate real-world scenarios, for any cluster, we select the top 75% samples based on the cosine distance to their cluster centroid, provided all of them have the same label. Otherwise, we only select the sentences having the same label in the top 20 sentences. This step is crucial to prevent the user from being overwhelmed with providing input in the case of heterogeneous clusters. We finally perform the stage-1 and stage-2 training and report the performance for every iteration on the test set (See Algorithm 1). We repeat this process until we reach the ground truth value of K."}, {"title": "4.5 Training Details", "content": "We utilized the MPNet model (Reimers and Gurevych, 2019) as the backbone for both stage-1 and stage-2 and adopted most of its hyperparameters for the optimization. We freeze the initial 11 layers of the model and only perform learning on the subsequent layers. To improve the learning capacity of our model, we add a dense layer followed by a Tanh activation function. The dimension of the sentence representation was set to 768, while the learning rate was 5e-5, and the batch size depended on the GPU's availability. Moreover, we set \u03b3 as 0.75 for the first iteration and 0.95 for the subsequent iterations to select high-confidence samples. To incorporate the LwF objective, we set \u03bb as 0.5 in both stages. All models were implemented in PyTorch using HuggingFace's transformers library (Wolf et al., 2019)."}, {"title": "5 Results & Discussion", "content": "Our evaluation is based on the metrics specified in Section 4.3 on the test set. Our findings are presented in two parts: 1) a comparison of our results with those of the previous state-of-the-art works, utilizing the same settings as proposed by them, and 2) an evaluation of our human-in-the-loop approach.\nTable 2 illustrates the key findings from our part-1 experiments. Our model consistently outperforms the strongest baseline DAC by a significant margin on all three datasets. However, there are some cases, such as the BANKING dataset with a known labeled ratio of 25%, where SCL performed better with a very small margin of 0.36%. Notably, on the CLINC dataset, our model achieves a good accuracy of 82.27% even with just 25% known classes ratio, surpassing DAC and SCL by 10.23% and 7.78% respectively. It is worth noting that SCL generally performed better than DAC in most cases, which could be attributed to the choice of backbone as MPNet, instead of BERT, as MPNet is fine-tuned on the similarity measure.\nNext, we observe that our stage-2 training demonstrates significant performance improvement as compared to stage-1 in most cases. This highlights the effectiveness of incorporating the Learning without Forgetting (LwF) objective, as stage-2 involves a different task than stage-1, and LwF prevents forgetting from occurring. Furthermore, we found that in scenarios where the labeled dataset was limited, such as the 25% known ratio, supervised contrastive learning (SupCon) used in SCL outperformed our stage-1 in both the CLINC and BANKING datasets. This indicates that SupCon is beneficial when dealing with limited labeled data, as it enhances class separability. However, as more intents become known, the additional benefit of SupCon diminishes and our approach performs significantly better."}, {"title": "5.1 Results with Human-in-the-loop", "content": "Table 3 presents the results of our incremental intent discovery approach, which incorporates simulated human-in-the-loop feedback. As illustrated in Table 3, we report the performance in both stages for each iteration. In the first iteration, we start with all unlabeled data and set the value of K using the method described in the previous sections. Subsequently, in each iteration, we continue to discover new intents, label data, and improve the performance metrics, including ACC, ARI, and NMI, on the test set. We terminate the process when K reaches the ground truth number of intents. Specifically, for the CLINC dataset, it took us 7 iterations to label 91.43% of the dataset, 8 iterations for the BANKING dataset to label 81.78% of the dataset, and 9 iterations for the TELECOM domain dataset to label 71.81% of the dataset. Additionally, Figure 5 illustrates the tsne plot for each iteration, showcasing the separation of samples class-wise and the addition of new intents in subsequent iterations on the TELECOM dataset."}, {"title": "6 Conclusion & Future Work", "content": "In this work, we present a Controllable Discovery of Intents (CDI) framework where prior knowledge is incorporated using unsupervised contrastive learning followed by a two-stage fine-tuning strategy. We also propose a novel incremental intent discovery method that incorporates human-in-the-loop feedback, while also utilizing the learning without forgetting (LwF) objective to preserve previously learned knowledge during new iterations. Our experimental results demonstrate that our approach significantly outperforms previous works by a significant margin. In future work, we plan to extend our approach to other languages and explore its applicability to entity discovery."}, {"title": "Limitations", "content": "Our work has certain limitations that should be acknowledged. Turn embeddings do not account for the larger context of the transcript in which the turn appears. In conversational datasets such as TELECOM, incorporating such contextual information can potentially improve performance. The candidate selection method would benefit from being more thoroughly investigated. Using the distance to the clusters centroids to select candidates with a high threshold may result in a reduced number of sentences selected per cluster, leading to decreased efficiency. The human-in-the-loop component is evaluated by simulating the user. We see efficient and standardized ways of automatically testing systems that incorporate human feedback as key to accelerate the development of such architectures. Future work will however need to focus on running real user experiments both to validate our current approach as well as to improve the automatic testing procedure."}]}