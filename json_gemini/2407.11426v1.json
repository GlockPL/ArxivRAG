{"title": "Generally-Occurring Model Change for Robust Counterfactual Explanations", "authors": ["Ao Xu", "Tieru Wu"], "abstract": "With the increasing impact of algorithmic decision-making on human lives, the interpretability of models has become a critical issue in machine learning. Counterfactual explanation is an important method in the field of interpretable machine learning, which can not only help users understand why machine learning models make specific decisions, but also help users understand how to change these decisions. Naturally, it is an important task to study the robustness of counterfactual explanation generation algorithms to model changes. Previous literature has proposed the concept of Naturally-Occurring Model Change, which has given us a deeper understanding of robustness to model change. In this paper, we first further generalize the concept of Naturally-Occurring Model Change, proposing a more general concept of model parameter changes, Generally-Occurring Model Change, which has a wider range of applicability. We also prove the corresponding probabilistic guarantees. In addition, we consider a more specific problem, data set perturbation, and give relevant theoretical results by combining optimization theory.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Background", "content": "Explainable Artificial Intelligence (XAI) [2] is a crucial research domain aimed at enhancing the transparency of machine learning algorithms to make their decision-making and predictions more comprehensible and interpretable[16]. The rise of complex models like deep learning necessitates \u03a7\u0391\u0399. These models, often viewed as \"black boxes\", present challenges in understanding their internal decision-making processes. Unlike traditional machine learning focused solely on statistical prediction, \u03a7\u0391\u0399 research centers on explaining the behaviour and outcomes of known AI models. \u03a7\u0391\u0399 development encompasses diverse techniques: building interpretable models, calculating feature importance metrics, and creating visualization tools to elucidate model rationale. \u03a7\u0391\u0399 not only aims to enhance the credibility of models but also facilitates compliance with regulatory"}, {"title": "1.2 Related Works", "content": "Saumitra Mishra et al. [18] meticulously categorize the robustness of counter-factual explanation generation algorithms into the following three classes: robustness to model change, robustness to input perturbations and robustness to counterfactual input changes. Robustness to model change refers to the issue of how the effectiveness of counterfactual explanations evolves when a machine learning model is retrained or when its training parameters are slightly modified. Our paper will focus on the research of this type of robustness. Kaivalya Rawal et al. [23] introduced a novel approach to assess the effectiveness of algorith-mic resources in the evaluation of robustness, taking into account the impact"}, {"title": "1.3 Our Paper's Contributions", "content": "In this theoretical paper, our contributions are as follows:\n1. We first further generalize the concept of Naturally-Occurring Model Change and propose a more universally applicable concept of model parameter change, Generally-Occurring Model Change. We also provide the corresponding prob-abilistic guarantees.\n2. Additionally, we investigate the specific case of the dataset perturbation problem. By combining the Generally-Occurring Model Change concept with optimization theory, we present novel theoretical results."}, {"title": "2 Preliminaries", "content": "Our basic notations are consistent with those in [11]. We assume that the instance space is \\(X \\subset \\mathbb{R}^d\\), \\(D\\) is a distribution over \\(Z = X \\times [0, 1]\\) and \\(m(x) : X \\rightarrow [0, 1]\\) is the original machine learning model, which can be regarded as a probabilistic model that finally makes decisions based on the results of \\(\\mathbb{I}(m(x) > 0.5)\\). Denote"}, {"title": "2.1 Counterfactual Explanations", "content": "The solution of counterfactuals is generally associated with some norm or metric on \\(X\\). In general, we can define the following concept using the norm \\(\\| . \\|\\) on \\(X\\):\nDefinition 1 (Counterfactuals \\(x^*\\) induced by norm \\(\\| . \\|\\)). For any \\(x \\in X\\) satisfying \\(m(x) < 0.5\\), its counterfactual with respect to \\(m(\\cdot)\\) and \\(\\| \\cdot \\|\\) is:\n\\[x^* := \\underset{x \\in \\mathbb{R}^d}{\\text{argmin}} \\|x - \\hat{x}\\| \\text{ s.t. } m(\\hat{x}) \\geq 0.5.\\]\nThe selection of an appropriate metric or norm is a meaningful problem, and there has been extensive research in the literature on this topic. For example, researchers have studied the selection of the \\(l_1\\)-norm [20], the \\(l_2\\)-norm [14], the Mahalanobis distance [13], and so on.\nHowever, as pointed out by [1,13], counterfactuals solved solely based on a certain metric or norm may deviate significantly from the potential distribution of the data, which is very unreasonable. For example, in a medical scenario, the counterfactual solved requires the height of the input instance to change by 3 meters, which is completely unrealistic. Therefore, Faisal Hamman et al. [11] defined the following concept:\nDefinition 2 (Closest Data-Manifold Counterfactuals \\(x^*\\) induced by norm \\(\\| \\cdot \\|\\) [11]). For any \\(x \\in X\\) satisfying \\(m(x) < 0.5\\), its closest data manifold counterfactual with respect to \\(m(\\cdot)\\) and \\(\\| \\cdot \\|\\) is:\n\\[\\hat{x}^* := \\underset{\\Xi \\in X}{\\text{argmin}} \\|x - \\hat{x}\\| \\text{ s.t. } m(\\hat{x}) \\geq 0.5.\\]\nA closely related metric to the Definition 2 is the Local Outlier Factor [6], which has been widely applied."}, {"title": "2.2 Gradient Descent", "content": "Gradient Descent (GD) is a classical optimization algorithm for finding a local minimum of a differentiable function [24]. In machine learning, GD is used to find the values of a function's parameters that minimize a cost function as far as possible. In this section, we provide some definitions that will be used in Section 4. Given \\(n\\) labeled examples \\(S = (z_1, z_2, ..., z_n)\\), where \\(z_i \\in Z\\), consider a decomposable objective function \\(f(\\theta) = \\sum_{i=1}^n f(\\theta; z_i)\\), where \\(f(\\theta; z_i)\\) denotes"}, {"title": "3 Main Results", "content": null}, {"title": "3.1 Generally-Occurring Model Change", "content": "A popular assumption in existing literature [26] is\n\\[\\|\\text{Params}(M) - \\text{Params}(m)\\| < \\Delta \\text{ for a constant } \\Delta.\\]\nAs Faisal Hamman et al. [11] point out, the assumption that changes in model parameters always lead to changes in classification performance is inappropriate. Instead, we should focus more on changes in the classification performance of the model rather than the changes in the parameters themselves. Their revision of Equation (2) is similar to the concept of equivalence classes in pure mathematics: those changes in model parameters that do not change the shape of the decision boundary should be tolerated. Specifically, Faisal Hamman et al. define Naturally-Occurring Model Change as follows:\nDefinition 5 (Naturally-Occurring Model Change, [11]). A naturally-occurring model change is defined as follows:\n1. Expectation condition. We assume that the following equation holds:\n\\[\\mathbb{E}[M(X)|X = x] = \\mathbb{E}[M(x)] = m(x),\\]\nwhere the expectation is over the randomness of \\(M(\\cdot)\\) given a fixed value of \\(X = x \\in \\mathbb{R}^d\\)."}, {"title": "3.2 Probabilistic Guarantee", "content": "For naturally-occurring model change, Faisal Hamman et al. [11] proposed the following probabilistic guarantee:\nTheorem 1 (Probability Guarantees under Naturally-Occurring Model Change, [11]). Let \\(X_1, X_2, . . ., X_k\\) be \\(k\\) i.i.d. random variables with distribution \\(\\mathcal{N}(x, \\sigma^2I_d)\\) and \\(\\hat{M} = \\frac{1}{k} \\sum_{i=1}^k (m(X_i) - M(X_i))\\). Suppose \\(|\\mathbb{E}[\\hat{M}|M] - \\mathbb{E}[\\hat{M}]| < \\varepsilon'\\). Then, for any \\(\\varepsilon > 2\\varepsilon'\\), a counterfactual \\(x \\in X\\) under naturally-occurring model change satisfies:\n\\[\\Pr \\left( \\hat{M}(x) \\leq R_{k,\\sigma^2}(x,m) - \\varepsilon \\right) \\leq \\exp \\left( -\\frac{k\\varepsilon^2}{8(\\gamma + \\gamma_m)^2\\sigma^2} \\right),\\]\nwhere \\(R_{k,\\sigma^2}(x,m) = \\sum_{x_i\\in N_{x,k}} (m(x_i) - \\gamma\\cdot ||x - X_i||_2)\\), \\(N_{x,k}\\) is a set of \\(k\\) points drawn from the Guassian distribution \\(\\mathcal{N}(x, \\sigma I_d)\\) with \\(I_d\\) being the identity matrix. The probability is over the randomness of both \\(M\\) and \\(X_i\\)'s.\nIn the Equation (3), \\(R_{k,\\sigma^2}(x,m)\\) is the stability measure proposed by Faisal Hamman et al.. They point out that in the concrete computation, because \\(\\gamma\\) is unknown, only the relaxed version of \\(R_{k,\\sigma^2}\\) can be computed:\n\\[R_{k,\\sigma^2} = \\frac{1}{k} \\sum_{X_i \\in N_{x,k}} (m(x_i) - |\\gamma_m(x) - m(x_i)|) .\\]\nMeanwhile, they proposed the Counterfactual Robustness Test: \\(R_{k,\\sigma^2} > \\tau\\). According to Theorem 1, setting a larger \\(\\tau\\) will guarantee that the new model \\(M(\\cdot)\\) has a higher validity with high probability. To better understand Theorem 1, Equation (3) can be written as follows:\n\\[\\Pr \\left(\\frac{1}{k} \\sum_{i=1}^k m(X_i) - M(x) \\geq \\frac{1}{k} \\sum_{i=1}^k \\gamma ||x - X_i||_2 + \\varepsilon \\right) \\leq \\exp \\left( -\\frac{k\\varepsilon^2}{8(\\gamma + \\gamma_m)^2\\sigma^2} \\right).\\]\nEquation (4) provides an intuitive understanding of Theorem 1: the prediction of the new model \\(M(\\cdot)\\) for \\(x\\) is approximately equal to the average prediction of the original model \\(m(\\cdot)\\) for \\(x\\) in its neighborhood.\nFor the more general concept of generally-occurring model change, we provide the following probabilistic guarantee:"}, {"title": "3.3 Proof of Theorem 2", "content": "Before proving Theorem 2, we present the following lemma:\nLemma 1 (Deviation Bound). Let \\(S = \\{X_1, X_2, . . ., X_k\\}\\) consist of \\(k\\) i.i.d. random variables belonging to \\(\\tilde{\\mu} \\ll \\mu\\) and \\(\\kappa(\\mu, \\tilde{\\mu}) := ||d\\tilde{\\mu}/d\\mu||_{L_2(\\mu)}\\), \\(\\psi_M:= \\frac{1}{k} \\sum_{i=1}^k (m(X_i) - M(X_i))\\). Then under gernerally-occurring model change, we have \\(\\Pr \\left(\\psi_M \\geq ||m - M||_{L_2(\\mu)} \\cdot \\kappa(\\mu, \\tilde{\\mu}) + \\varepsilon | M = m \\right) \\leq 2 \\exp\\left(-\\frac{\\varepsilon^2k}{2}\\right)\\), where \\(m\\) is a model that may be obtained after the change.\nProof. It is noted that for any \\(i = 1, 2, ..., k\\), we have\n\\[|\\psi_M(X_1, X_2, ..., X_i, ..., X_K) - \\psi_M(X_1, X_2, . . ., X_i', ..., X_K)|\\]\\[= \\frac{1}{k} | (m(X_i) - M(X_i)) - (m(X_i') - M(X_i')) | \\leq \\frac{2}{k}\\]\nHence, by McDiarmid's Inequality [17], we have\n\\[\\Pr(\\psi_M - \\mathbb{E}_S[\\psi_M|M = m] \\geq \\varepsilon|M = m) \\leq 2 \\exp\\left(-\\frac{\\varepsilon k^2}{2}\\right)\\]"}, {"title": "4 Case Study: Dataset Perturbation Problem", "content": null}, {"title": "4.1 Problem Setting and Assumptions", "content": "In this section, we consider a specific problem. Suppose we have an original dataset \\(S_1\\) of size \\(n\\), and a new dataset \\(S_2\\) obtained by perturbation, as shown in Fig. 2. The two datasets have most of their elements in common, with only a few \\(r\\) elements being different. Thus, we can write \\(S_1, S_2\\) as\n\\[S_1 = \\{z_1,..., z_{n-r}\\} \\cup \\{z_{n-r+1}... z_n\\}, S_2 = \\{z_1,..., z_{n-r}\\} \\cup \\{s_{n-r+1}...s_n\\}.\\]"}, {"title": "4.2 Theoretical Result", "content": "We consider the setting of convex optimization and prove the following theorem:\nTheorem 3 (Convex Optimization). Suppose that the loss function \\(f(\\cdot, z)\\) is convex for all \\(z \\in Z\\) and satisfies Assumption 2, and that \\(m(\\cdot)\\) and \\(M(\\cdot)\\) satisfy Assumption 1. Suppose that we run GD with step sizes \\(\\eta_t \\leq 2/\\beta\\) for \\(n\\) steps. Then,\n\\[\\Pr\\left(\\frac{1}{k} \\sum_{i=1}^k m(X_i) - M(x) \\geq \\frac{\\gamma}{2} \\sum_{i=1}^k ||x - X_i||_2 + \\varepsilon \\right.\\]\\[+\\left.\\left( \\frac{2L^2}{\\xi} \\sum_{t=n-r+1}^n \\eta_t \\right) \\cdot \\kappa(\\mu, \\tilde{\\mu}) \\leq 2 \\exp\\left(-\\frac{k\\varepsilon^2}{2}\\right)\\right).\\]\nProof. Consider the gradient updates \\(G_1^m, G_2^m, ..., G_n^m\\) and \\(G_1^M, G_2^M, .....G_n^M\\) induced by running GD on \\(S_1, S_2\\). For GD, \\(m\\) and \\(M\\) are parameterized as \\(\\theta_t^m = \\theta_{t+1}^m = G_t^m(\\theta_t^m), \\theta_t^M = \\theta_{t+1}^M = G_t^M(\\theta_t^M)\\). We define \\(\\delta_t = ||\\theta_t^m - \\theta_t^M||_2\\), where \\(\\theta_t^m, \\theta_t^M\\) are the model parameters trained on \\(S_1\\) and \\(S_2\\) after the \\((t - 1)\\)-th gradient update, respectively."}, {"title": "5 Conclusion", "content": "In this paper, we generalize the concept of naturally-occurring model change to a more general model parameter change concept, namely generally-occurring model change. We prove probabilistic guarantees for generally-occurring model change. In addition, we consider the specific problem of dataset perturbation, and use the probabilistic guarantees for generally-occurring model change to give relevant theoretical results based on optimization theory. This example shows that generally-occurring model change has a wider range of applications.\nFuture Work. However, there are still many areas for improvement in our work. First, it is an important problem to understand or improve the parameter \\(\\kappa(\\mu, \\tilde{\\mu})\\) in Theorem 2 in a more natural way. For this problem, we think it is a feasible idea to consider it from the perspective of information geometry. In addition, it is also a challenging problem to extend Theorem 3 to more popular optimization algorithms such as Stochastic Gradient Descent (SGD), instead of GD, as considered in Section 4."}]}