{"title": "Streaming Speaker Change Detection and Gender Classification for\nTransducer-Based Multi-Talker Speech Translation", "authors": ["Peidong Wang", "Naoyuki Kanda", "Jian Xue", "Jinyu Li", "Xiaofei Wang", "Aswin S. Subramanian", "Junkun Chen", "Sunit Sivasankaran", "Xiong Xiao", "Yong Zhao"], "abstract": "Streaming multi-talker speech translation is a\ntask that involves not only generating accurate\nand fluent translations with low latency but\nalso recognizing when a speaker change oc-\ncurs and what the speaker's gender is. Speaker\nchange information can be used to create audio\nprompts for a zero-shot text-to-speech system,\nand gender can help to select speaker profiles in\na conventional text-to-speech model. We pro-\npose to tackle streaming speaker change detec-\ntion and gender classification by incorporating\nspeaker embeddings into a transducer-based\nstreaming end-to-end speech translation model.\nOur experiments demonstrate that the proposed\nmethods can achieve high accuracy for both\nspeaker change detection and gender classifica-\ntion.", "sections": [{"title": "1 Introduction", "content": "Speech translation (ST) aims to convert spoken\nwords in one language to text in another language\nwith high accuracy and fluency. A common ap-\nproach is to use two components: an automatic\nspeech recognition (ASR) system that converts the\nspeech input into text, and a machine translation\n(MT) system that translates the text into the target\nlanguage. This method is commonly referred to\nas cascaded ST.\nTo avoid error accumulation and fully exploit\nthe audio information, end-to-end (E2E) ST was\nstudied, which uses a single model to\ntranslate directly from speech to text. The most\nwell-studied architecture for E2E ST is attention-\nbased encoder-decoder (AED) models which use an attention layer to\ncombine audio representations and text. The au-\nthors of suggest using AED\nmodels for\na small French-English dataset. A similar model"}, {"title": "2 Method", "content": "First, we introduce multilingual speech translation\nbased on transducers, a method that achieves high-\nquality streaming speech translation. We then de-\nscribe the t-vector method. Finally, we show how\nwe combined these two methods for streaming\nspeaker change detection and gender classification."}, {"title": "2.1 Transducer-based streaming multilingual ST", "content": null}, {"title": "2.1.1 Transformer transducer", "content": "A transducer model consists of three components:\nan encoder, a prediction net-\nwork, and a joint network. The encoder takes dx-\ndimensional audio features $x_t \\in R^{dx}$ as input and\ngenerates de-dimensional hidden states $h_{enc} \\in R^{de}$.\nThe prediction network uses the embedding of the\nprevious non-blank output token $y_{u-1} \\in R^{l}$ to gen-\nerate the hidden state $h_{pred} \\in R^{dp}$ for step u. The\njoint network combines $h_{enc}$ and $h_{pred}$ into a T\u00d7U\nmatrix represented by $z_{t,u} \\in R^{dz}$, and then applies\na softmax function to obtain probabilities for paths\nthat align audio frames with token sequences. The\nmodel uses a blank token at the output to handle the\nalignment between audio and text. During training,\nthe model considers all possible paths and maxi-\nmizes the probabilities of the correct paths. In this\nstudy, we use Transformer transducer (T-T). It uses\nTransformer blocks in the encoder, which have a\nmulti-head self-attention layer and a feedforward\nlayer."}, {"title": "2.1.2 Streaming capability", "content": "The encoder receives the audio input in chunks to\nenable streaming. To stream with low latency and\ncomputation, an attention mask, as in , is used. At layer l, the input $x_{1:t}^l$ is split into\nchunks $c_s^l$ over time with chunk size U. At time\nstep t, $x_{1:t}^l$ only attends to frames in its chunk $c_{t/U+1}^l$ and\nB left chunks $c'_{max(1,t/U+1-B):t/U}^l$. The recep-\ntion field at each layer grows linearly with the num-\nber of layers, allowing the model to use a longer his-\ntory for better performance. The frames cannot see frames outside their chunk,\nkeeping a fixed number of look-ahead frames."}, {"title": "2.1.3 Multilingual capability", "content": "We adopt the LAMASSU-UNI approach , which is illustrated in Fig 3, to per-\nform speech translation from English to various\nother languages, by providing the prediction net-\nwork with a starting token that specifies the lan-\nguage. The language identifications (LIDs) are\nappended to the vocabulary list and are treated as\nspecial tokens."}, {"title": "2.2 t-vector for ST", "content": "A t-vector is a type of speaker embedding vector\nthat captures the speaker characteristics at the to-\nken level. It is based on the\nd-vector, which is obtained from audio segments.\nA t-vector module is typically added to a transducer\nmodel that has been well-trained. It was first devel-\noped for multi-talker ASR and has been recently\nused for ST tasks (Yang et al., 2023).\nThe t-vector model, which is built on top of an\nST model, is illustrated in Figure 4. The ST model\nis fixed during training, as depicted by the gray\nbox. The t-vector module consists of a speaker en-\ncoder and a speaker decoder. The speaker encoder\nhas multi-head attention layers that use external\nattention to extract the speaker information from\nthe lower layer and the corresponding ST encoder\nlayer. Specifically, the attention layers generate\ntheir own key and query from the lower layer, and\nuse the output of the corresponding ST encoder\nlayer as value. Figure 5 shows the details of the\nspeaker encoder layers. The output of the speaker\nencoder, together with the embedding of the output\nnon-blank token, is fed to the speaker decoder. The\nspeaker decoder has two long short-term memory\n(LSTM) layers, whose output is passed through a\nlinear layer to produce t-vectors."}, {"title": "2.3 t-vector for speaker change detection and\ngender classification", "content": "In this section, we explain how we use transducer-\nbased streaming multilingual ST and t-vector to\nperform streaming speaker change detection and\ngender classification. Our model is data-efficient\nbecause it treats the speaker change detection prob-\nlem as a speaker identification (SID) generation\ntask. This means it does not rely on a large amount\nof real-world training data, which is difficult to\nacquire and may not cover all possible scenarios."}, {"title": "2.3.1 Speaker change detection", "content": "We use the cosine similarity between two adjacent\nt-vectors to detect a speaker change. If the similar-\nity value is lower than a threshold, we assume that a\ndifferent speaker is speaking. Our method does not\nrequire model retraining for the ST model part, and\ntherefore can preserve the ST model performance.\nAn alternative way to perform speaker change de-\ntection is to train a model with real conversations\nthat have speaker changes. However, collecting\nsuch training data is challenging. If we use sim-\nulated data, the model tends to learn the channel\nvariations between different audio segments rather\nthan the speaker differences. Therefore, we choose\nthe t-vector method for our study."}, {"title": "2.3.2 Gender classification", "content": "In conventional TTS systems, a specific speaker\nprofile is chosen for each output audio segment.\nWhile these systems cannot retain all the audio\nnuances like zero-shot TTS, they are essential for\non-device deployment to prevent malicious use of\nzero-shot TTS. To ensure natural-sounding conver-\nsations, it's crucial to accurately classify the gender\nof speakers for each audio segment; otherwise, lis-\nteners might become confused in understanding the\ntranslated conversations. Since we already produce\nt-vectors for detecting speaker changes, we aim\nto utilize this information for gender classification\ntoo. We begin by gathering a collection of speaker\nprofiles categorized as male or female. We then\ncalculate the cosine similarity between the t-vector\nof each token and the male and female speaker\nprofiles. Lastly, we determine the gender of the to-\nken by selecting the gender with the highest cosine\nsimilarity score."}, {"title": "3 Experimental setup", "content": null}, {"title": "3.1 Data", "content": null}, {"title": "3.1.1 Training data", "content": "We have about 75K hours of English audio in the\ntraining data for the ST model. The audio is col-\nlected from various sources and is anonymized. For\neach of the five output languages Germany (DE),\nSpanish (ES), Hindi (HI), Italian (IT), and Russian\n(RU), we use text machine translation models to\ngenerate texts with pseudo-labels.\nWe train the t-vector model using Voxceleb as the training data and use our\nwell-trained ST model to generate the label texts in\neach target language. We then apply a Viterbi algo-\nrithm to align the tokens and the audio frames. The\nt-vector training stage uses the alignment produced\nby this step to associate the t-vector with the non-\nblank tokens. This allows a cross-entropy training\nfor the t-vector model that is memory-efficient and\nalso produces the t-vector together with the output\ntoken during inference."}, {"title": "3.1.2 Test data", "content": "Our test data consisted of 5 real recorded long au-\ndios. The test samples had an average duration of\nabout 30 minutes. We collected real conversational\naudios with various numbers of speakers, up to\neight, for speaker change detection. Human anno-\ntators segmented the audio and marked the speaker\nchange points at the segment boundaries. We con-\ncatenated the segments into audio samples, each\ncontaining one speaker change. The total number\nof samples was 688, with an average duration of\nabout 5 seconds. Some samples were 30 seconds\nor longer. For gender classification, we divided all\nthe speaker profiles in the training set of Voxceleb-\n1 and Voxceleb-2 into two gender categories: male and\nfemale. We used the test set of Voxceleb-1 as the\ntest audio. The total number of utterances we tested\nwas 4874."}, {"title": "3.2 Model", "content": "We applied a streaming multilingual T-T model that\nperforms EN-to-many speech translation with a\nchunk size of 1s and 18 history chunks. The model\nhas 12 Transformer encoder layers and about 100\nmillion (M) parameters. The five output languages\nuse a shared vocabulary of about 18587 tokens. We\nalso added a language ID for each language to the\nvocabulary list. The total number of tokens, in-\ncluding (EOS) and (blank), is 18594. We trained\nthe model for 96M steps, with a peak learning rate\nof 3e-4 and 0.8M warm-up steps. We used fp16\nin Deepspeed and 32 32G GPUs for training. For\ninference, we used PyTorch decoding with a beam\nsize of 8.\nThe t-vector model consists of a speaker encoder\nand a speaker decoder. The speaker encoder uses\na Res2Net module for SID extraction (Yang et al.,\n2023). It has the same number of encoder layers\nas the ST encoder, which is 12 in our study. The\nattention dimension and the attention head are 128\nand 8, respectively. The speaker decoder is a two-\nlayer LSTM model with a hidden size of 512 and\nan input size of 128. The t-vector dimension is\n128. We fix the ST model parameters during train-\ning. We also initialize the SID module inside the\nencoder with a pre-trained SID model, which is\ntrained on the Voxceleb-1 and Voxceleb-2 training sets.\nThe SID module remains fixed during training."}, {"title": "4 Evaluation results", "content": null}, {"title": "4.1 Speaker change detection", "content": "A sample translation result with speaker change\nlabels and the corresponding timestamps for text\nspace speaker change detection is shown in Table\n3. Since the output is not deterministic, it is diffi-\ncult to design a reference speaker change metric\nfor translation output. Therefore, we only present\na sample translation result with the speaker change\nlabel and the corresponding timestamp in Table 3.\nWe can see that the speaker change label (SC) is\ncorrectly assigned to different translation results\nusing the proposed method. The exact values of\nthe timestamps vary slightly, but the differences are\nsmall. In fact, the difference between EN-DE and\nothers is 1 frame. The estimated timestamps corre-\nsponding to the (SC) tokens in both Table 3 and 1\nare calculated by finding the first frame generating\n(SC) in the best hypothesis, and multiplying the\nframe index with frame shift, which is 0.04 second\n(sec) in our study.\nIn addition to text space speaker change detec-\ntion above, we also perform audio space speaker\nchange detection. We use three metrics to measure\nthe performance of speaker change detection in\nthe time domain: recall, precision, and F\u2081 score.\nWe define a tolerance time range for each speaker\nchange estimation. A detected speaker change is\ncorrect if it is within the time range of the reference"}, {"title": "4.2 Gender classification", "content": "As shown in Table 2, the token-level gender classi-\nfication accuracy, which also accounts for punctua-\ntion marks, was 0.989 across different languages."}, {"title": "5 Conclusions", "content": "This paper presents a novel challenge and a solu-\ntion for streaming multi-talker ST. We combine\na transducer-based multilingual ST system with a\nt-vector module that can identify speaker changes\nand gender in real time. By comparing the cosine\nsimilarity between t-vectors, our method can ef-\nfectively address the speaker change detection and\ngender classification problems."}]}