{"title": "Video Quality Assessment for Online Processing: From Spatial to Temporal Sampling", "authors": ["Jiebin Yan", "Lei Wu", "Yuming Fang", "Xuelin Liu", "Xue Xia", "Weide Liu"], "abstract": "With the rapid development of multimedia processing and deep learning technologies, especially in the field of video understanding, video quality assessment (VQA) has achieved significant progress. Although researchers have moved from designing efficient video quality mapping models to various research directions, in-depth exploration of the effectiveness-efficiency trade-offs of spatio-temporal modeling in VQA models is still less sufficient. Considering the fact that videos have highly redundant information, this paper investigates this problem from the perspective of joint spatial and temporal sampling, aiming to seek the answer to how little information we should keep at least when feeding videos into the VQA models while with acceptable performance sacrifice. To this end, we drastically sample the video's information from both spatial and temporal dimensions, and the heavily squeezed video is then fed into a stable VQA model. Comprehensive experiments regarding joint spatial and temporal sampling are conducted on six public video quality databases, and the results demonstrate the acceptable performance of the VQA model when throwing away most of the video information. Furthermore, with the proposed joint spatial and temporal sampling strategy, we make an initial attempt to design an online VQA model, which is instantiated by as simple as possible a spatial feature extractor, a temporal feature fusion module, and a global quality regression module. Through quantitative and qualitative experiments, we verify the feasibility of online VQA model by simplifying itself and reducing input.", "sections": [{"title": "I. INTRODUCTION", "content": "In the past few years, the number of user-generated videos has grown exponentially, and video has become one of the important elements in our daily life. However, in the whole chain from video generation, transmission, storage, processing to display, distortion would be inevitably introduced, resulting in quality degradation [1], [2], and therefore accurate evaluation of video quality becomes a core problem in real applications [3], [4], [5]. Video quality assessment (VQA) has proverbially been a classic and important research topic in video understanding, and it can be achieved through both subjective and objective quality assessment. Instead of quality evaluation by human beings (i.e., subjective quality assessment, which is considered the most reliable but labor-intensive and expensive method), objective VQA aims to design computational models that can automatically and accurately predict the human perceptual quality of videos. According to the availability of reference information, VQA models could be categorized into full-reference (FR), reduced-reference (RR), and no-reference (NR) or blind (B) methods. The first two types of VQA models require full or partial reference information when being used to evaluate video quality, while the last type of VQA models do not need any reference information. In terms of practicability, BVQA methods can predict the quality of videos without access to reference information and thus are more applicable [6].\nGenerally, in the handcrafted features dominated era, the VQA task can be decomposed in-frame (or short-duration video segment) quality measurement and frame-by-frame (or video segment) error pooling [7], where the former part always refers to an error-sensitivity metrics such as Peak Signal to Noise Ratio (PSNR) and structural similarity (SSIM) [8], and the latter part relies on the characteristics of the human visual system (HVS) such as motion perceptual uncertainty [9] and the recency effect [10]. Currently, deep learning technology makes these two parts being formulated in a unified framework, i.e., adaptively learning spatio-temporal quality degradation patterns in an end-to-end manner. Despite the significant success, computational complexity has always been a substantial issue in designing deep learning based VQA models, which is similar and contemporaneous to video understanding [11]. A straightforward and high-gain way to reduce"}, {"title": "II. RELATED WORK", "content": "In this section, we first introduce VQA models. Then, we describe studies about online video processing."}, {"title": "A. VOA Models", "content": "In general, BVQA methods can be classified into two categories according to the design philosophy, including knowledge-driven and data-driven VQA models. The knowledge-driven BVQA models mainly rely on handcrafted features, e.g., natural scene statistics (NSS), and use a shallow machine learning algorithm to map the features to the visual quality of videos. NSS refers to the statistical regularities of visual scenes, where the statistical discrepancy between high-quality video and test video represents the visual quality of the test video. In these types of VQA models, spatial and temporal information are crucial. For example, Saad et al. [17] predicted the visual quality of videos by estimating spatial naturalness, spatio-temporal statistical regularity naturalness, and motion coherency. Zhu et al. [18] proposed a spatio-temporal interaction model by integrating spatial features and temporal motion features for evaluating video perceptual quality. Mittal et al. [19] proposed a BVQA model named VIIDEO that exploits inherent statistical regularities of natural videos. Korhonen et al. [20] extracted a set of handcrafted spatio-temporal features to predict the quality of videos. In ChipQA [21], video sampling was employed to acquire more efficient manual features.\nLiu et al. [22] proposed a feature encoder for heterogeneous knowledge integration based on spatio-temporal representation learning, which directly extracts spatio-temporal features of videos, thereby mitigating the bias of individual weak labels in dataset. Li et al. [23] utilized transfer learning to migrate from image quality assessment (IQA) to VQA, and they additionally introduced a motion feature extractor, which can be regarded as extracting temporal features.\nHowever, limited by high computational complexity of hand-crafted feature extraction and their representation ability, the performance of these knowledge-driven BVQA methods is sub-optimal. The reasons affecting video quality are quite complicated and cannot be well captured by these hand-crafted features. Therefore, the data-driven BVQA models are proposed, which consist mainly of four basic blocks, including a video pre-processor, a spatial quality analyzer, a temporal quality analyzer, and a quality regressor [16]. Li et al. [14] used the pre-trained ResNet50 [24] to extract spatial features, then these features are fed into the GRU [25] network"}, {"title": "B. Online Video Processing", "content": "Efficiency has always been one of the most important pursuits in video understanding [30], [31], [32], [33]. Lin et al. [30] proposed a generic temporal shift module (TSM) for efficient video understanding. Wang et al. [31] proposed a flexible temporal segment network (TSN), which aims to model long-range temporal structure with a segment-based sparse temporal sampling and aggregation scheme, for video action recognition. Zhou et al. [32] introduced an effective and interpretable network module, i.e., the temporal relation network (TRN), which is designed to learn temporal dependencies between video frames at multiple time scales. Similar to TSN, Zolfaghari et al. [33] proposed an efficient convolutional network (ECO) with a sparse sampling strategy for video understanding.\nAs suggested in [13], [16], natural videos are highly redundant in both spatial and temporal dimensions, and sparsely sampled video frames are capable of obtaining competitive performance. To reduce computational cost, existing VQA approaches typically consider different sampling schemes, such as random cropping, bilinear resizing, and sptaiotemporal sampling, as the pre-processing. Wu et al. [15] proposed a new sampling strategy for VQA that preserves both local quality and unbiased global quality with contextual relations via uniform grid mini-patch sampling (GMS). The spatially spliced and temporally aligned mini-patches named fragments are extracted as the input of video Swin Transformer [34]. They later extended it to evaluate videos of any resolutions [35], where two important factors of spatial and temporal sampling granularity (quality-sensitive neighborhood representatives), including partitioned neighborhoods and continuous representatives, are taken into account. Similarly, Zhao et al. [36] proposed a Zoom-VQA network architecture to evaluate video quality by extracting perceptual spatio-temporal features at different levels (i.e., patch level, frame level, and clip level).\nLiu et al. [37] introduced a sampling method named SAMA, which involves scaling the data into a multi-scale pyramid, conducting segment sampling at each scale, and ultimately applying masks in both temporal and spatial dimensions to adjust the size of data. In [12], various sparse frame sampling strategies were employed to validate the feasibility of temporal sampling. Simultaneously, Yan et al. [38] found that the spatio-temporal modeling module tends to overlook motion interruptions, indicating the viability of temporal sampling. Experimental results in [13] further affirmed that video perceptual quality is independent of frame order; even when disturbing the frame sequence, the performance remains robust,"}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Problem Formulation", "content": "Assume that a video is denoted by \\(V = \\{v_i\\}_{i=1}^N\\) with N frames. An objective model \\(\\Phi(\\cdot) : \\mathbb{R}^{H \\times W \\times 3 \\times N} \\rightarrow \\mathbb{R}\\), accepts V (or part of video frames) as input, and predicts a scalar q, representing the perceptual quality score of V. Generally, the input of \\(\\Phi(\\cdot)\\) includes all or most of frames in V. As described in our previous studies [13], [38], there exists considerable redundancy in videos, and extensive experiments showed that sparse sampling has little impact on model performance. In this paper, we take a further step to explore the effectiveness-efficiency trade-offs issue of spatio-temporal modeling in VQA models by considering spatial and temporal sampling simultaneously. The overall framework is shown in Figure 2. We firstly squeeze V by sparsely temporal sampling, and can obtain a set of keyframes \\(F = \\{k_j\\}_{j=1}^K\\) (\\(k_j\\) is the index of each keyframe), which can be formulated as:\n\\[F = \\mathcal{T}(V; \\eta, \\eta),\\]\nwhere \\(\\mathcal{T}(\\cdot)\\) denote the temporal sampling operation, and \\(\\eta\\) and \\(\\eta\\) represent the number of segments and the number of frames extracted from each segment, respectively. And we then apply the spatial sampling operation \\(\\mathcal{S}(\\cdot)\\) to these extracted frames, which can be formulated as:\n\\[F = \\mathcal{S}(F; g, s),\\]\nwhere g and s denote the density of the grid and the size of the image patches, respectively. Subsequently, the squeezed video F is fed into \\(\\Phi(\\cdot)\\) to derive the perceptual quality score q as below:\n\\[q = \\Phi(F; \\theta),\\]\nwhere \\(\\theta\\) denotes the learnable parameters of the VQA model \\(\\Phi(\\cdot)\\)."}, {"title": "B. Spatio-temporal Sampling", "content": "As aforementioned, spatio-temporal sampling in current context refers to a two-step operation for squeezing videos, including sparsely extracting frames (i.e., temporal sampling) and cropping patches from each frame (i.e., spatial sampling), and the resultant squeezed video is used as model's input. For sparsely extracting frames, we embrace the success of these related studies [31], [30], [33] in video understanding, and the details will be introduced in Section IV-A3. As spatial sampling, video frames' dimensions are reduced to alleviate computational burdens and enhance processing speed. This step is particularly crucial for high-resolution videos, where the original pixel number is excessively high, leading to significant consumption of computational resources. To this end, we adopt the GMS method [15] as the default spatial sampling method. Specifically, each video frame is partitioned into uniform grids. The mini-patches with same size are randomly sampled from the uniform grids in the keyframe and then spliced together into a fragment. Given a keyframe \\(v \\subseteq \\mathbb{R}^{3*H*W}\\) (the subscript is omitted for simplicity), it is initially divided into \\(\\beta * \\beta\\) grids. From each grid, the patches of size \\(\\mu * \\mu\\) are selected:\n\\[M_{\\epsilon,\\varepsilon} = v[r * \\epsilon : r *\\epsilon + \\mu, c * \\varepsilon : c * \\varepsilon + \\mu],\\ (4)\\]\nwhere r and c are the starting positions of randomly selected patches in the grid, \\(\\epsilon\\) and \\(\\varepsilon\\) represent the location of the grid, respectively. Subsequently, each patch is concatenated in the order of extraction to form a new image \\(\\hat{v}\\), with the purpose of preserving contextual relationships:\n\\[\\hat{v} = \\Gamma(M_{\\epsilon,\\varepsilon}), 0 < \\epsilon, \\varepsilon < \\beta,\\ (5)\\]\nwhere \\(\\Gamma(\\cdot)\\) stands for the concatenation operation, and \\(\\hat{v}\\) denotes the frame of the squeezed video F."}, {"title": "C. Spatio-temporal Modeling", "content": "Since our main objective is to investigate online VQA model, rather than solely pursuing high-performance, we therefore resort to as simple as possible spatio-temporal feature extraction modules. Following our previous studies, we adopt the stable and effective VQA model, i.e., VSFA, in our experiments. The VSFA model uses ResNet-50 as spatial feature extractor and GRU to capture temporal quality variation, which can be described as follows:\n\\[f_{st} = \\mathcal{N}(F;\\theta_n),\\ (6)\\]\nwhere \\(\\mathcal{N}(\\cdot)\\) and \\(\\theta_n\\) denote the spatiotemporal feature extraction model and its pre-trained parameters, respectively. \\(f_{st}\\) denotes the extracted spatio-temporal featrues. Note that we also implement a variant of VSFA by substituting the vanilla transformer for GRU. Moreover, the model \\(\\mathcal{N}(\\cdot)\\) can be also instantiated by other light-weight modules, and the details will be introduced in Section IV."}, {"title": "D. Global Quality Regression", "content": "Video quality regression involves mapping the spatio-temporal features to a single scalar, we use fully connected (FC) layers to bridge those features and quality score, which is calculated as follows:\n\\[f_{t}^{st} = W_{t}f_{t}^{st-1}+b_{t}, \\text{ for } t = 1,\\ldots,T,\\ (7)\\]\n\\[q = W_{T}^{T}f_{T-1}^{st}+b_{T},\\ (8)\\]\nwhere \\(W_t\\) and \\(b_t\\) denote the t-th learnable parameters and bias in the global quality regression. T denotes the number of FC layers, and q is the predicted global quality score."}, {"title": "IV. QUANTITATIVE EXPERIMENTS", "content": ""}, {"title": "A. Experimental Settings", "content": "1) Test Databases: We evaluate BVQA models on six VQA datasets: KoNViD-1k [39], LIVE-VQC [40], CVD2014 [41], LIVE-Qualcomm [42], LSVQ [43], and NTIRE [44]. Table I summarizes the key details of these test datasets. For each dataset, we randomly split the data into 60% for training, 20% for validation, and 20% for testing. To ensure fair comparisons, we perform three rounds of training-validation-testing splits, and report the average test performance. It should be noted that we evaluate the model's performance on the LSVQ1080P dataset using the corresponding parameters trained on the LSVQ dataset.\n2) Training and Test Details: We select a representative and stable BVQA model, i.e., VSFA [14], as the backbone, and also test its variant by substituting its GRU module with the vanilla transformer [45], which is denoted by TransformerVSFA in this paper. For feature extraction, the ResNet50 pre-trained on the ImageNet dataset [46] is adopted as feature extractor. The other settings are consistent with [14]. The size of the spatially sampled image patches is set to 32*32"}, {"title": "3) Temporal Sampling", "content": "We test three classical temporal sampling methods, including TSN [31], TSM [30], and ECO [33]. The intuitive comparison of these sampling methods are depicted in Figure 3, and their details are described as below.\nTSN. It has been widely used to effectively extract keyframes from a given video \\(V = \\{v_i\\}_{i=1}^N\\). By setting a hyperparameter \\(\\eta\\), TSN divides a video uniformly into M segments, each segment containing approximately \\(\\theta = [N/M]\\) video frames, where \\[\\cdot\\] denotes the floor function, and M and \\(\\eta\\) are set to 10 and 4, respectively. To further extract keyframes from each segment, we set the sampling step \\(step\\) to \\([\\(\\theta-1)/(\\eta-1)]\\), ensuring the even distribution of keyframes within each segment:\n\\[\\Omega = \\begin{cases}\n\\text{Range}(1, \\min(2 + \\text{step} * (n - 1), \\\\ \\theta+1), \\text{step}), & \\text{if } \\text{step} > 0, \\\\\n[\\frac{1}{\\eta}] * \\eta, & \\text{if } \\text{step} <= 0,\n\\end{cases}\\ (10)\\]\nwhere Range() is a function that generates a list based on the start value, stop value, and step size. This method can flexibly adapt to videos of different lengths by properly selecting segment divisions and step sizes, effectively extracting keyframes. Finally, all the keyframes extracted from the original video are\n\\[\\mathcal{T} = \\{\\mathcal{T}^0,\\mathcal{T}^2,\\ldots\\ldots ,\\mathcal{T}^{M-1}\\},\\ (11)\\]\n\\[\\mathcal{T}^{m} = \\{V_{\\Omega[0]+\\theta*m}, V_{\\Omega[1]+\\theta*m}, \\cdots, V_{\\Omega[n-1]+\\theta*m}\\},\\ (12)\\]\nwhere m is the index of segments and m \u2208 {0,1, \u2026\u2026\u2026, M\u22121}, \\(\\Omega[.]\\) denotes the element in set \\(\\Omega\\). This TSN-based keyframe extraction method ensures both the representativeness of keyframes and processing efficiency, which is crucial for subsequent video analysis and processing tasks.\nTSM. The core idea of TSM is to promote the interaction of information between adjacent frames by moving a portion of channels along the temporal dimension. Assuming the original video is divided into M segments based on the parameter \\(\\xi\\), where M is set to 10. The indices of the sampled keyframes can be denoted as:\n\\[\\mathcal{T} = \\{\\mathcal{V}_{m*\\xi+\\epsilon_m}\\}_{m=0}^{M-1},\\ (13)\\]\nwhere \\(\\epsilon_m\\) represents a random value within the range [0, \\(\\xi\\)) for the m-th segment. Typically, \\(\\xi\\) is set to \\([N/M]\\) to ensure the video is evenly segmented into M parts, and each segment covers a sufficient range of frames to ensure the sampled keyframes are representative. The TSM-based keyframe sampling method effectively enhances the efficiency of information exchange in the video, providing a richer information foundation for subsequent quality prediction.\nECO. When averaging the number of video frames by obtained segments, ECO behaves similarly to TSM. However, the number of fragments M is set to 20. That is, ECO randomly selects a keyframe from each segment based on the parameter as:\n\\[\\mathcal{T} =\\begin{cases}\n\\{\\mathcal{V}_{m*\\xi+\\epsilon_m}\\}_{m=0}^{M-1}, & \\text{if } (N \\text{mod} M) <0 \\\\\n\\{\\mathcal{V}_{m*\\xi+\\epsilon_m}\\}_{m=0}^{M-1}, & \\text{if } (N \\text{mod} M) \\neq 0\n\\end{cases}\\ (14)\\]\nIn contrast, another keyframe is extracted from the last unevenly divided keyframe. Sampling keyframes in this way ensures an even distribution throughout the entire video sequence, thereby ensuring comprehensive coverage of the video content and accurate representation."}, {"title": "B. Experimental Results and Analysis", "content": "The experimental results are presented in Tables II, III, and IV. As can be seen from these tables, the proposed spatiotemporal sampling does not lead to a significant drop in performance across the datasets, demonstrating the viability of online VQA with reduced video frame input. While TSN achieves the highest performance, it requires extracting more"}, {"title": "C. A Further Design of Online VQA Model", "content": "Considering that the VSFA and TransformerVSFA models still rely on ResNet50, we make a further step to design an online VQA model by integrating MobileNet [53] and Graph network [54], and the proposed online VQA model is named MGQA. The experimental results of the MGQA model (the default spatial and temporal sampling methods are S3 and ECO, respectively) on various datasets are shown in Table V."}, {"title": "D. Ablation Study", "content": "Followed by our previous study [13], which demonstrates that both ordered and orderless video frames are able to predict"}, {"title": "E. Qualitative Visualization", "content": "The spatio-temporal local quality maps [15] are generated to visualize what the model has learned. As shown in Figure 5, six visual examples are selected from the KoNViD-1k and LIVE-VQC datasets, as well as their perspective local quality maps and segments with quality maps. For visual examples (a), (d), and (e), the proposed model focuses on regions of interest and assigns high quality to these regions, aligning with the perception mechanisms of the HVS. For example (b), the proposed model gives low quality to the blurry background, thus dropping its global quality. For example (c), a different pattern emerges, i.e., a dim and cluttered scene, where the proposed model successfully handles this case. And for example (f), the proposed model allocates high quality to"}, {"title": "V. CONCLUSION", "content": "Our research aims to delve into the effectiveness-efficiency trade-offs issue of spatio-temporal modeling by immensely reducing video's information, which is implemented by joint spatial and temporal sampling. Through extensive experiments on six public datasets, we find that the heavily squeezed video can be also used to predict the quality of original video, and we further demonstrate the feasibility of pursuing an online VQA model. The experimental results of this study offer complementary insights on VQA to recent studies [12], [13], [16], [35], and we expect that our study together with these studies [12], [13], [16], [35] will motivate more interesting work."}]}