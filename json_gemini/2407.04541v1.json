{"title": "PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts", "authors": ["Ana-Cristina Rogoz", "Maria Ilinca Nechita", "Radu Tudor Ionescu"], "abstract": "We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian posts collected from Reddit. The PoPreRo dataset includes a varied compilation of post samples from five distinct subreddits of Romania, totaling 28,107 data samples. Along with our novel dataset, we introduce a set of competitive models to be used as baselines for future research. Interestingly, the top-scoring model achieves an accuracy of 61.35% and a macro F\u2081 score of 60.60% on the test set, indicating that the popularity prediction task on PoPreRo is very challenging. Further investigations based on few-shot prompting the Falcon-7B Large Language Model also point in the same direction. We thus believe that PoPreRo is a valuable resource that can be used to evaluate models on predicting the popularity of social media posts in Romanian. We release our dataset at https://github.com/ana-rogoz/PoPreRo.", "sections": [{"title": "1 Introduction", "content": "Understanding the factors influencing the popularity of social media posts represents a critical and multifaceted challenge for NLP research. Social media platforms generate vast amounts of user-created content, offering a unique window into real-time public discourse and collective attention. Analyzing what resonates with audiences goes beyond just sentiment analysis, demanding nuanced NLP techniques to capture humor, sarcasm, and the subtle cues that drive engagement. This pursuit fosters not only theoretical advancements but also practical applications across diverse fields, from marketing and public health to combating misinformation and predicting cultural trends. Studying social media popularity, therefore, is not just an interesting NLP problem, but a key to unlocking the true potential of language in the digital age.\nSo far, the phenomenon has been studied both for individual social media platforms, such as Instagram [4,26,21,5], Reddit [2,13], Twitter [17,27,16], either as a whole phenomenon, for detecting popularity [20,25], or for generating engaging content [8].\nReddit, in particular, has been one of the most studied platforms in the ever-evolving landscape of online content. From gauging public opinion and identifying emerging"}, {"title": "2 Dataset", "content": "PoPreRo gathers Reddit posts from five different Romanian subreddit channels, which represent either one of the biggest cities in Romania or the country-wide subreddit. The subreddits are: Romania, Bucure\u015fti, Cluj, Ia\u015fi and Timi\u015foara. These subreddits were collected at first using Reddit API, divided into JSON files to extract the information needed for analyzing the popularity of each reddit post, such as title, content, number of comments, number of up and down votes. However, Reddit API has a limitation of 1000 requests for extraction of different data. Due to the large number of samples that we target for the dataset, the API could not provide all necessary data. Therefore, we use an open-source archive, from where the samples are collected. As mentioned above, all the data is stored in separate JSON files for each subreddit, containing relevant information for determining the popularity of posts."}, {"title": "2.2 Dataset Statistics", "content": "The dataset comprises 28,107 samples (14,289 unpopular and 13,818 popular) containing over 1 million tokens in total (see detailed statistics in Table 1). Each sample consists of a title, a content, and a binary label, where the title and content are concatenated into a single text. We divide the posts into \"popular\" or \"unpopular\" based on the sum of upvotes and downvotes for each post, where the threshold between the two categories is given by the median number of votes (15). To enable consistent evaluation and comparison with future studies, we provide an official split with distinct training, validation,"}, {"title": "2.3 Preprocessing", "content": "After gathering the data from Reddit, we implement a two-step preprocessing pipeline to ensure data quality and consistency. First, language identification was performed on post titles using FastText [12] to filter out non-Romanian posts (filtered posts are not counted in Table 1). This step guarantees the linguistic homogeneity of the dataset. Subsequently, upvote/downvote scores are normalized to the [0, 1] interval. Finally, a binary popularity label is assigned with respect to the median value of the normalized"}, {"title": "3 Methods", "content": "To comprehensively evaluate the performance for the popularity prediction task on the newly introduced dataset, we establish six baseline approaches. Two of these baselines leverage state-of-the-art deep learning models for language processing. Another three baselines utilize various classifiers based on shallow or deep (frozen) features. Our final baseline uses a Large Language Model (LLM) based on in-context learning, also known as few-shot prompting. For all models, we use the concatenated title and content of each post as the input data."}, {"title": "3.1 Fine-Tuned Ro-GPT2", "content": "Our first baseline relies on fine-tuning a Ro-GPT2 model [19], a large language model specifically trained on Romanian text. It is based on the original GPT2 architecture, but trained on a Romanian dataset consisting of over 1 million tokens. This allows it to"}, {"title": "3.2 Fine-Tuned Ro-BERT", "content": "As our second baseline, we employ a fine-tuned Romanian Bidirectional Encoder Rep-resentations from Transformers (Ro-BERT) model [7]. Sharing the same transformer-based architecture as the original BERT [6], Ro-BERT has been demonstrated to out-perform multilingual BERT on various tasks, as reported by Dumitrescu et al. [7]. Consequently, we anticipate Ro-BERT to be a strong baseline for our Romanian corpus.\nSimilarly to the previous baseline, we use the Ro-BERT encoder to encode each text into a list of token IDs. We keep the same design as before, where the model generates 768-dimensional embeddings, followed by a global average pooling layer which is fed into a Softmax output layer with two neurons. To assign the final class label, we apply the argmax function on the two predicted probabilities. The entire model is fine-tuned for 10 epochs on mini-batches of 32 samples. We employ the AdamW optimizer [15] with a learning rate of 210-7 and the default value for \u20ac."}, {"title": "3.3 Ro-BERT Embeddings + Logistic Regression", "content": "For our third classification approach, we leverage pre-trained Ro-BERT embeddings in conjunction with a Logistic Regression (LR) classifier. Consistent with the fine-tuned Ro-BERT baseline, we first tokenize all input samples from the three datasets. Subsequently, we utilize the Ro-BERT model to extract 768-dimensional vector representa-tions for each sample. These representations, corresponding to the final hidden layer of Ro-BERT, are then fed into the LR model for classification."}, {"title": "3.4 FastText + SVM", "content": "The first shallow classification approach is based on FastText embeddings [3] and a Support Vector Machines (SVM) classifier. After textual cleaning and tokenization using NLTK's word tokenizer, we fine-tune a FastText model on the training corpus. This model provides word embeddings for train, validation, and test sets. For each text sample, the word embeddings are averaged to produce a 300-dimensional feature vector, which is subsequently passed to the SVM. Finally, we train the SVM classifier using the linear kernel and the regularization hyperparameter C set to 10."}, {"title": "3.5 TF-IDF + Random Forest", "content": "Our second shallow classification approach is based on the Term Frequency-Inverse Document Frequency (TF-IDF) representation and a Random Forest (RF) classifier. As for the previous method, we initiate the process by cleaning and tokenizing the text using NLTK's word tokenizer. Subsequently, we employed a TF-IDF vectorizer to quantify the importance of words within the corpus, generating numerical features for each document. These features are then used to train a Random Forest classifier."}, {"title": "3.6 Few-Shot LLM Prompting", "content": "To explore the feasibility of large language models (LLMs) for post popularity prediction in PoPreRo, we employ a prompt-based approach utilizing the 7-billion parameter Falcon LLM [1] (Falcon-7B). Due to computational limitations, we prompt the LLM with contexts comprising two unpopular and two popular examples. Subsequently, we attach an individual test sample to each prompt and ask the LLM to predict the corresponding label. Below, we illustrate the structure of our prompt via a concrete example:"}, {"title": "4 Experiments", "content": "Our binary classification experiments focus on predicting the popularity of text within the PoPreRo dataset. Each text sample is categorized as either popular or unpopular. To evaluate the performance of our models, we employ several metrics. For each class, we calculate precision (proportion of true positives among the identified positives) and recall (proportion of true positives with respect to all positives). Additionally, we aggregate these scores using macro F\u2081 and micro F\u2081 (accuracy) measures."}, {"title": "4.2 Hyperparameter Tuning", "content": "The hyperparameters of all models are determined via grid search. For the transformer-based methods (Ro-BERT, Ro-GPT2), we employ a grid search over the maximum number of input tokens in the set {50, 70, 100, 120, 150, 200}, as well as the learning rate in the set {10-5,5\u00b710-5,10-6,5-10-6,10-7,2\u00b710-7, 5\u00b710-7,10-8,510-8} and the value of e for AdamW in the set {10-6, 10-7,10-8}.\nFor the FastText + SVM approach, we vary the FastText word-embeddings dimension ({150, 200, 300, 350}), the window size for the input ({2, 3, 4}), as well as the kernel (linear or RBF) and the parameter C ({0.1, 1, 10, 100, 1000}) of the SVM classifier. Similarly, for the Ro-BERT + Logistic Regression approach, we run a search over the maximum numbers of Ro-BERT input tokens in the same set as before ({50, 70, 100, 120, 150, 200}) and test different penalty term values (\u201811', 'l2', 'elastic net' or 'None') for the classifier.\nLastly, for the TF-IDF + Random Forest method, we vary the minimum ({4, 5, 6}) and maximum ({0.6, 0.7, 0.8}, in percentages) document frequency of the TF-IDF Vectorizer, together with the number of decision trees in the set {50, 100, 150, 200} for the Random Forest classifier."}, {"title": "4.3 Results", "content": "We present the results of our five baselines on the PoPreRo validation and test sets in Table 3. We find that Ro-GPT2 exhibits the best performance, with an accuracy (micro F\u2081) and a macro F\u2081 score above 0.6 on both validation and test sets, in contrast to the other baselines which seem to perform similarly well on the validation set, but reach worse performance on the test set.\nEvaluating the two state-of-the-art transformer models, Ro-GPT2 and Ro-BERT, reveals some interesting findings. While both achieve comparable accuracy on the validation set (0.6525 for Ro-GPT2 and 0.6343 for Ro-BERT), Ro-GPT2 clearly outperforms Ro-BERT on the test set, indicating the superior ability of the former model to generalize to unseen data. Analyzing the precision-recall trade-off, we observe a shared propensity for both models to exhibit higher recall for the \u201cpopular\" category, followed by a shift towards higher precision when identifying the \u201cunpopular\" class.\nThe FastText + SVM, TF-IDF + RF and Ro-BERT + LR models achieve comparable performance. All three models obtain accuracy rates higher than 65% on the validation set, which drop below 60% on the test set. In terms of precision and recall, almost all of them achieve higher precision for the \"popular\" category on both validation and test"}, {"title": "4.4 Discriminative Feature Analysis", "content": "We analyze the discriminative features learned by the fine-tuned Ro-BERT and by the FastText + SVM. The motivation behind this analysis is to validate that the decisions of these models are not based on some biases that escaped our data collection, but on actual data understanding.\nFor the Ro-BERT model, we use the Captum [14] library via its Layer Integrated Gradients method to infer valuable insights from the fine-tuned model. This technique delves into the BERT embeddings layer, attributing importance scores to individual input words which led to the final label prediction.\nTo find the words with higher influence on the decisions given by the SVM, we consider the cosine similarities between the primal weights of the SVM and the FastText embedding of each word. We sort the words based on the similarity values, and keep the first 10 and last 10 words from the sorted list as features for the positive (\u201cpopular\") and negative (\"unpopular\") classes, respectively."}, {"title": "5 Conclusion", "content": "In this paper, we introduced PoPreRo, the first publicly available dataset of Romanian Reddit posts dedicated to the task of popularity prediction. We collected 28,107 posts from five diverse Romanian subreddits, amounting to over 1 million tokens. Aiming to predict binary labels resulting from the sum of upvotes and downvotes for each post, we explored five distinct popularity detection methods and presented comparative results. We found that Ro-GPT2 significantly outperforms the other models.\nBuilding upon our foundation, future research can further study popularity detection algorithms and delve deeper into the factors driving engagement on Romanian Reddit."}, {"title": "6 Limitations", "content": "It is crucial to acknowledge that Reddit's popularity in Romania might not be representative for the wider population. While Reddit offers a valuable platform for research due to its diverse communities and open discussions, its user base in Romania is comparatively smaller than other social media platforms, such as Facebook, Instagram, or YouTube. Furthermore, Reddit's API restricts data access, limiting historical data collection and imposing retrieval caps."}, {"title": "7 Ethics Statement", "content": "The data was collected from a publicly available Reddit archive, selecting five Romanian subreddits. The social media posts are freely accessible to the public without any type of subscription. As the data was collected from an archived public website (Reddit), we adhere to the European regulations\u00b2 that allow researchers to use data in the public web domain for non-commercial research purposes. We thus release our corpus as open-source under a non-commercial share-alike license agreement, namely CC BY-NC-SA 4.0\u00b3."}]}