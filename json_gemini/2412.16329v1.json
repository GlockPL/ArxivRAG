{"title": "Improving Object Detection for Time-Lapse Imagery Using Temporal Features in Wildlife Monitoring", "authors": ["Marcus Jenkins", "Kirsty A. Franklin", "Malcolm A. C. Nicoll", "Nik C. Cole", "Kevin Ruhomaun", "Vikash Tatayah", "Michal Mackiewicz"], "abstract": "Monitoring animal populations is crucial for assessing the health of ecosystems. Traditional methods, which require extensive fieldwork, are increasingly being supplemented by time-lapse camera-trap imagery combined with an automatic analysis of the image data. The latter usually involves some object detector aimed at detecting relevant targets (commonly animals) in each image, followed by some postprocessing to gather activity and population data. In this paper, we show that the performance of an object detector in a single frame of a time-lapse sequence can be improved by including spatio-temporal features from the prior frames. We propose a method that leverages temporal information by integrating two additional spatial feature channels which capture stationary and non-stationary elements of the scene and consequently improve scene understanding and reduce the number of stationary false positives. The proposed technique achieves a significant improvement of 24% in mean average precision (mAP@0.05:0.95) over the baseline (temporal feature-free, single frame) object detector on a large dataset of breeding tropical seabirds. We envisage our method will be widely applicable to other wildlife monitoring applications that use time-lapse imaging.", "sections": [{"title": "1. Introduction", "content": "By capturing images at regular intervals, a time-lapse camera gathers data of a scene over time without the need for large quantities of video data. This makes time-lapse imaging particularly useful for applications such as wildlife monitoring, where the aim is to monitor sites over long periods of time. This presents a unique challenge for object detection, however, since the loss of temporal continuity, coupled with significant changes in illumination, makes object tracking unsuitable. In this paper, we explore methods exploiting the sequential and static nature of time-lapse imagery. These methods utilise temporal features and thereby improve scene understanding and reduce the number of false positives in the static background. Our most significant contribution is our method of temporal feature engineering for time-lapse imagery. In this method, we inject two additional spatial feature channels that capture information of stationary scenery and of non-stationary scenery. Furthermore, we demonstrate that additional improvements can be achieved using two different methods of input channel weighting. As a final contribution,\nwe introduce a method of stratified subset sampling for object detection datasets from camera-trap imagery.\nFor our tests, we used a camera-trap dataset of breeding tropical ground-nesting seabirds. This consisted of approximately 180,000 images taken at various nesting locations on Round Island (RI), around 4500 of which were labelled using bounding-box annotations with the classes \u201cAdult\u201d, \u201cChick\u201d, and \u201cEgg\". The images (see Figure 1) were captured across 10 camera traps, each monitoring a separate scene consisting of several nesting sites. We provide more details on the dataset in Section 3. For brevity, we refer to this dataset as the RI petrel dataset.\nThe rest of the paper is organised as follows: first, we explore related work in wildlife monitoring and object detection in time-lapse imagery. Next, we present our proposed methods in Section 2. This is followed by a detailed description of our dataset in Section 3. We then outline our experimental procedures and findings in Section 4. Finally, in Section 5, we finish with a discussion and analysis of our results."}, {"title": "1.1. Related Work", "content": null}, {"title": "1.1.1. Deep-Learning in Wildlife Monitoring", "content": "Applying deep-learning methods to camera-trap imagery in the context of wildlife monitoring has been explored in several studies [1-5]. Norouzzadeh et al. [1] evaluated various Convolutional Neural Networks (CNNs) for the detection of Tanzanian wildlife in camera-trap imagery and reported that VGG [6], a deep CNN proposed by the Visual Geometry Group, yielded the highest performance. Instead of employing bounding-box predictions, their approach directly predicted the animal species and its count, limiting detections to one class of animal per image. Furthermore, they incorporated an initial stage to predict whether an animal was present before proceeding to the classification and count-ing phases. Upon revisiting this work in 2020, ref. [3] proposed Faster-RCNN as a more effective solution. They argued that the approaches described by [1] were overly dependent on background features due to the absence of bounding-box predictions, which aided in focusing feature learning on objects rather than the surrounding background scenery.\nHentati-Sundberg et al. [4] used YOLOv5 to gather data of seabird populations from live CCTV streams. The authors collected population counts of adult seabirds and estimated the rates of growth of chicks using the mean of predicted box sizes over time. Additionally, they detected predatory disturbances, defined as a drop in count of four or more within a one-minute period. Vecvanags et al. [5] utilised Faster R-CNN and RetinaNet to monitor populations of deer and wild boar."}, {"title": "1.1.2. Object Detection in Time-Lapse Imagery", "content": "There is limited research on the incorporation of temporal information as features for object detection models applied to time-lapse imagery. In the context of video data, object tracking algorithms such as SORT [7] are typically employed; however, the lack of temporal continuity of time-lapse imagery renders object tracking unsuitable. A notable study relevant to our research is that of Bjerge et al. [8], where the object detection of moving targets is enhanced for image sequences. In their approach, the previous image in a sequence is subtracted from the current image, and the resulting absolute difference in each colour channel is used as a motion likelihood. This motion likelihood is then incorporated into the current RGB input through element-wise addition, where pixel values across the RGB channels are summed to produce a motion-enhanced image."}, {"title": "2. Method", "content": "In this section, we describe our technical contributions and the methods that we use. We start with a short introduction to the You Only Look Once (YOLO) object detection architecture in Section 2.1. The following Section 2.2 describes the primary contribution of our work where we detail our methods of fusing temporal information present in the time-lapse imagery sequence with the usual input of the object detector as extra input channel(s). Finally, in Section 2.3, we describe a new stratified sampling method for partitioning data into training/validation/test sets which is particularly suitable for object detection datasets with high class and annotation imbalances such as the one we used in this work.\nTo encourage future research or application of our methods, we have made the code available for download on GitHub (https://github.com/MarcusJenkins01/yolov7 -temporal, accessed on 9 December 2024)."}, {"title": "2.1. YOLOv7", "content": "YOLOv7 [9] is a single-stage object detector, where region proposal and classification are combined into joint bounding-box and class predictions. To do so, YOLOv7 consists of a number of anchor boxes for each region of the image at a number of scales. These anchor boxes are predetermined using k-means clustering to establish the mean size and aspect ratio of objects for each region in each scale. Instead of making a direct prediction for the position and size of the bounding box, the position and size is predicted as a relative adjustment of the best-fitting anchor box. By using anchor boxes and multiple scales, the predictions are kept as small adjustments to the anchor box, despite variations in object sizes and shapes; this means gradients are kept low, providing greater stability and ease of learning [10]. Of the YOLO family, we chose YOLOv7, since it was well established. Further details on the configuration of the YOLOv7 architecture we used is given in Section 4."}, {"title": "2.2. Temporal Feature Engineering", "content": "Object detectors such as YOLO usually operate with a single RGB frame as input. Here, we aim to inject temporal information into the input of the object detector as addi-tional input channels. To develop these temporal features, we derived inspiration from background (BG) subtraction techniques. We first computed a BG model, which was then used with the current image to calculate the difference mask (DM). Both the BG model and the DM formed separate channels, which were stacked on top of the three RGB channels. Unlike [8], where the difference mask was added element-wise to the RGB input, we did not modify the RGB input, and so these features were preserved. The following subsections describe our proposed approach in greater detail."}, {"title": "2.2.1. Temporal Average Background Model", "content": "To obtain a background model for a current image, we selected 12 prior images, from which a pixelwise mean average was computed for each of the RGB channels that was then converted to greyscale. Since images during the day and images during the night were separate modalities, the background model was separated for day and night. In other words, if the current image was taken during the day, 12 prior daytime images were selected, and likewise for nighttime imagery. This was referred to as the temporal average background model.\nThe set of images for daytime D, or nighttime N, for all images up to n, was defined as:\n$S^{L}_{n} = {I_{i} \\vert i < n, L \\in {D,N}}$\n$S^{L}_{12}$ is the subset of $S^{L}_{n}$ that was used to calculate the temporal average:\n$S^{L}_{12} = {I_{j_{1}}, I_{j_{2}}, ..., I_{j_{12}} \\vert j_{1}, j_{2}, ..., j_{12} \\text{ are the 12 largest indices in } S^{L}}$\nThe RGB temporal average, $T_{A12RGB}$, for the set $S^{L}_{12}$ was therefore given as:\n$T_{A12RGB} = \\frac{1}{12} \\sum_{k=1}^{12} I_{j_{k}}$\nAnd the flattened temporal average, $T_{A12}$, was obtained using luminosity greyscale conversion as:\n$T_{A12} = 0.299 \\cdot T_{A12R} + 0.587 \\cdot T_{A12G} + 0.114 \\cdot T_{A12B}$"}, {"title": "2.2.2. Difference Mask", "content": "Since we would like the network to focus on differences between the $I$ and $T_{A12RGB}$ that pertain to motion rather than changes in colour distribution (due to weather or lighting geometry changes), we first performed colour correction on $T_{A12RGB}$ before computing the difference mask DM.\n$T_{A12RGB}$ and $I$ were reshaped to dimensions $N \\times 3$, where $N = H \\times W$, and $H$ and $W$ are the image height and width, respectively. A 3 by 3 colour correction matrix, $M$, was then computed using least squares regression [11] as:\n$M = \\arg \\min_{M} ||I - T_{A12RGB}M||^{2}$\nEach pixel in $T_{A12RGB}$ was then colour corrected using $M$, and the result of this operation was denoted as $T'_{A12RGB}$.\nFinally, the difference mask DM was calculated as the absolute difference between $I$ and $T'_{A12RGB}$ followed by flattening to greyscale as:\n$DM = \\sum_{k \\in {R,G,B}} \\frac{| I_{k} - T'_{A12k} |}{3}$\nThe effect of applying this colour correction on DM can be observed in Figure 2. We can see that $DM$ obtained from colour-corrected $T'_{A12RGB}$ highlights less of the stationary background scenery compared to $T_{A12RGB}$ (denoted by the reduction in greyscale intensity in the background regions of the image)."}, {"title": "2.2.3. Temporal Channel Weighting", "content": "Rather than simply passing $T_{A12}$ and $DM$ as two additional channels alongside the RGB channels, we also trialled two techniques that applied a learned weighting to the channels $T_{A12}$ and $DM$. Our hypothesis was that scaling these feature channels with learned parameters before passing them to YOLOv7 would facilitate convergence toward a better local optimum. While the weightings of these channels could be implicitly learned as part of the CNN layers, we believed that explicitly scaling these channels would provide a clearer gradient flow to amplify or suppress the contribution of each of the two new feature channels. This hypothesis was confirmed in our results in Section 5. For the first method, we proposed a fixed weighting that was learned for each channel, regardless of the input values. The weightings were defined as:\n$W_{TA} = \\sigma(\\alpha), W_{DM} = \\sigma(\\beta)$\nwhere $\\sigma$ is the Sigmoid function, and $\\alpha$ and $\\beta$ are learnable parameters. Back-propagation and optimisation of these parameters was performed end-to-end using YOLOv7's optimiser.\nFor the second method, an input-aware approach of calculating weightings was also trialled using a modification of the Squeeze-and-Excitation block [12] (Figure 3). Unlike the traditional Squeeze-and-Excitation block, which applies a scale to all channels, we modified $F_{ex}$ (Equation (8)) to produce 2 weightings, which were then applied to the channels for $T_{A12}$ and $DM$.\n$F_{ex}(z) = \\sigma(W_{2}\\delta(W_{1}z)), \\text{ where } W_{1} \\in \\mathbb{R}^{C \\times C} \\text{ and } W_{2} \\in \\mathbb{R}^{2 \\times C}$\nwhere $\\delta$ denotes the ReLU function and $C$ is the number of input channels, thus $C = 5."}, {"title": "2.3. Subset Sampling for Training, Validation, and Test Splits", "content": "The splitting of object detection datasets into training, validation, and test subsets is often performed using random sampling. Instead, we propose a new method of stratified sampling for camera-trap imagery that ensured that each subset contained cameras with examples of minority classes, where random sampling may potentially miss these [13]. The benefit was also a trained model that was theoretically optimised for a more realistic class distribution, and similarly, a test set that was more representative.\nDefining strata for object detection is complex due to the presence of multiple objects per image of varying sizes and at different locations. When we originally devised our method, there was no current research to our knowledge, but a method has since been published [14]. Analogously to our method, they used the frequency of each class in the image and the distribution of box sizes but with an explicit focus on box aspect ratios (due to the bias of aspect ratios imposed by anchor boxes for anchor-based object detection).\nSince the aim of using automated methods for wildlife monitoring is often for it to be applicable to new, future cameras (scenes) at other nesting locations, we split our dataset by camera. The task was therefore to obtain a model that generalised well to unseen scenes. The cameras for each set were chosen using a combinatorial approach, where the summed variance of the class distribution, the number of objects of each predefined size, and the ratio of each class across day and night were minimised between each subset. The class distribution was computed as the mean number of objects of each class per image for each camera. The object sizes were assessed among three distinct groups, which were obtained using k-medoid (PAM) clustering with a k value of 3. A bounding-box label was matched with the size based on the closest cluster centre; these three sizes were interpreted as \u201csmall\u201d, \u201cmedium\u201d, and \u201clarge\u201d object sizes. We used K-medoids over k-means to reduce the influence of outliers in object size on the cluster centres.\nTherefore, we were looking for a partition of a set of all cameras C, into three subsets $C_1$, $C_2$, and $C_3$, where $\\bigcup_{i=1}^{3} C_{i} = C$ and $C_{i} \\cap C_{j} = \\emptyset, i \\neq j$. Hence, we performed the following optimisation:\n$\\min_{C_1, C_2, C_3} (\\sigma_{c}^{2} + \\sigma_{s}^{2} + \\sigma_{r}^{2})$\nwhere $\\sigma_{c}^{2}$ is the sum of variances of the number of objects of each class per image among the three subsets, and M is the set of classes (object categories):\n$\\sigma_{c}^{2} = \\sum_{i=1}^{M} \\sigma_{i}^{2}$"}, {"title": "3. Dataset", "content": "The RI petrel dataset was made available as part of the long-term Round Island petrel research program. This dataset was collected to monitor the breeding population of Pterodroma petrels (known locally as the \u201cRound Island petrel\u201d) on the Round Island Nature Reserve, a small island off the north coast of Mauritius. To obtain these data, 10 Reconyx camera traps (manufactured in Holmen, WI, USA) were deployed at 10 different nesting locations (5 Hyperfire HC600 cameras and 5 Hyperfire 2 HF2X cameras). Each camera captured the contents of between two and five petrel nests and were configured to take an image at hourly intervals, day and night, between 4 December 2019 and 8 March 2022. As outlined in our introduction, the dataset consisted of 181,635 images; of these, 4483 were labelled at the University of East Anglia using bounding-box annotations. These annotations were aided by earlier citizen-science point annotations generated through the Zooniverse citizen-science project, Seabird Watch. For more information on camera deployments and citizen-science annotations, see [15].\nThe nesting sites captured by the 10 cameras can be seen in Figure 4. The provided example images were taken during the day; however, during hours of low light and/or darkness (between approximately 6 P.M. and 6 A.M.), images were captured using the complementary infrared sensor."}, {"title": "4. Experiments", "content": "In this section, we describe the experiments we conducted to validate the utility of the proposed methods. We start with the description of the YOLOv7 model and training configuration in Sections 4.1 and 4.2. We describe how we use our method of stratified sampling to partition our data into training, validation, and test splits in Section 4.3. In Appendix A.1, we describe how we tuned the hyperparameters of the proposed models. We give details on how we performed data augmentation in Section 4.4. Finally, our main experiments evaluating the proposed methods are described in Section 4.5."}, {"title": "4.1. YOLO\u028a7 Model Configuration", "content": "YOLOv7 offers a number of configurations with varying complexities, the most com-plex being YOLOv7-E6E. As model complexity increases, performance increases for the MS COCO dataset, albeit at the cost of computation time [9]. For the RI petrel dataset, images were to be analysed post-capture, not in real time. On the other hand, we still chose a balance between computation and performance due to GPU training and inference times. We opted for a middle ground between YOLOv7 and YOLOv7-E6E, YOLOv7-W6, which obtains an average precision of 54.9% on MS COCO with an inference time of 7.6 ms with an NVIDIA V100. YOLOv7-W6 is also the smallest configuration which produces bounding-box predictions for four scales, rather than the three scales of the lesser models; this makes it more robust to variation in object sizes."}, {"title": "4.2. Training Configuration", "content": "Training was performed using an RTX 6000 with 24 GB of available VRAM; thus, we used a batch size of eight for all of our experiments. For every training epoch, YOLOv7 evaluated the performance on the validation set using the \u201cfitness score\u201d. This was com-puted as:\n$fitness = 0.1 \\cdot mAP@0.5 + 0.9 \\cdot mAP@0.05 : 0.95$\nRather than using early stopping, the set of weights that obtained the greatest fit-ness score was stored; training was performed for the full number of epochs regardless of evaluation performance. Evaluation on the test set was then performed using these optimal weights."}, {"title": "4.3. Training, Validation, and Test Splits", "content": "Using our method of subset sampling, described in Section 2.3, we obtained the partition of our dataset shown in Table 3.\nWe constrained the required dataset partition to have two cameras in the validation and test sets each and six cameras in the training set. Given camera SWC4 had only 14 annotated images, we forced it to be in the training set to minimise other (single) camera bias in the validation and test sets.\nWe also enforced a maximum number of images for the test and validation sets, where if either of these sets exceeded an image count of 25% of the full dataset image count, the respective dataset partition was rejected."}, {"title": "4.4. Data Augmentation and Baseline Model", "content": "The official implementation of YOLOv7, made available by Wang et al. [9], provides the optimal data augmentation configuration for MS COCO. For object detection, this consists of mosaic, random perspective, MixUp, HSV augmentation, and horizontal flipping (Figure A1). We followed the methodology used by Bochkovskiy et al. [16] for establishing the optimum data augmentation settings. We tested each of the methods proposed for COCO in Wang et al. [9] separately, then tested each in conjunction with each other one at a time, starting with the best performing method.\nWe did not optimise the individual hyperparameters for each data augmentation method, due to the large potential search space. Instead, we used the values optimised for MS COCO, with the purpose of evaluating the effect of different combinations of these methods. To that end, we used the same technique as used in hyperparameter optimisation with a 50% training subset and 50 epochs for training.\nSimilarly to our hyperparameters, the optimal data augmentation configuration de-veloped for MS COCO proved to be the most effective for our dataset as well (this is the configuration illustrated in Figure A1).\nTo establish our baseline model (single RGB image object detector), we trained the optimal data augmentation configuration and optimal hyperparameter configuration (Appendix A.1) for the full number of epochs, 300."}, {"title": "4.5. Temporal Feature Engineering", "content": "To accommodate the additional channels of $T_{A12}$ and $DM$ and the channel weightings, a number of changes were made to YOLOv7. This predominantly included adaptations to HSV augmentation and the input layer. For training, we used the same hyperparameters as used for our baseline model and trained for 300 epochs."}, {"title": "4.5.1. Data Augmentation of $T_{A12}$ and $DM$", "content": "For our experiments using channels $T_{A12}$ and $DM$, we applied the same data augmen-tation methods as those used for the RGB inputs in the baseline model, with the exception of HSV augmentation. HSV augmentation was applied to the RGB channels in the same way; however, for the $T_{A12}$ channel, which was a greyscale one, only the value gain was applied. This gain was the same gain that was used for the RGB channels. We did not apply any HSV augmentation to $DM$ to ensure that the difference intensity was fully preserved."}, {"title": "4.5.2. Channel Weighting for $T_{A12}$ and $DM$", "content": "For both methods of channel weighting, $W$ and $SE$, we introduced an additional layer to the backbone, which was positioned as the first layer (before the ReOrg layer). The input, x, consisted of all channels, except the weighting was only applied to channels $T_{A12}$"}, {"title": "4.5.3. Ablation Experiments", "content": "Using the same training configuration, we trained additional models where we only provided $T_{A12}$, and only $T_{A12}$ and $DM$ (without channel weighting). The results of these demonstrated the significance of these feature channels and the impact of channel weight-ing. We discuss the results of our ablation experiments in Section 5. When only $T_{A12}$ was provided, we used the same HSV augmentation method as when only the value gain was applied to $T_{A12}$"}, {"title": "5. Results and Discussion", "content": "The results of our experiments are shown in Tables 4 and 5. Figures 10 and 11 show a gallery of detection results for sample images from the test set."}, {"title": "5.2. Learned Channel Weighting", "content": "Both the fixed weightings and Squeeze-and-Excitation provided an improvement. For the fixed weightings, the learned weighting for $T_{A12}$, $\\sigma(\\alpha)$, was 0.288, and $\\sigma(\\beta)$ for $DM$ was 0.824.\nThe performance improvement when applying such weightings could imply that the features offered by channel $DM$ were immediately more distinguishing for birds than those of $T_{A12}$-this was also confirmed by the visual inspection of the two channels (see Figure 12b,c) and so this weighting allowed for a better local optimum to be converged towards earlier in the training. The fact neither weighting cancelled either channel out also further demonstrated that both these channels were useful, in addition to the evidence provided in Table 4."}, {"title": "5.3. Explicit Versus Implicit Difference", "content": "We theorised that the difference mask $DM$ may not be needed, since the difference between $T_{A12}$ and the RGB image can be learned implicitly. We can see, however, that providing $DM$ offered an improvement of 3.8%. Perhaps by providing $DM$, the effort of learning this change was minimised, and so more model resources were available for improving learning of other features. In addition, since more salient features were immediately present, the path of optimisation towards the more relevant local minima of the loss function was perhaps more stable and easier to follow."}, {"title": "6. Conclusions", "content": "In this paper, we introduced two innovative methodologies aimed at improving object detection in time-lapse camera-trap imagery, which is critical for ecological monitoring of animal populations. In our primary contribution, we leveraged temporal information to significantly enhance object detection model accuracy. By integrating features that distinguish static and dynamic elements within the input image, we achieved a notable improvement of 24% in mean average precision over the baseline.\nOur secondary contribution, a method of stratified dataset subset selection, presented a novel approach to partition time-lapse imagery object detection datasets. The method ensured a balanced representation of various cameras across the training, validation, and test sets, with the aim of providing a model that generalised well across various classes, different object sizes, and day and night modalities and where the validation/test set evaluation metrics were indicative of the future model performance on unseen data."}, {"title": "Author Contributions:", "content": "Conceptualization, M.J., K.A.F., M.A.C.N., N.C.C., K.R., V.T. and M.M.; Methodology, M.J., K.A.F., M.A.C.N., N.C.C. and M.M.; Software, M.J.; Validation, M.M.; Formal anal-ysis, M.J. and K.A.F.; Investigation, M.J., K.A.F., M.A.C.N. and N.C.C.; Resources, K.A.F., M.A.C.N., N.C.C., K.R., V.T. and M.M.; Writing-original draft, M.J.; Writing-review & editing, M.J., K.A.F., M.A.C.N. and M.M.; Visualization, M.J.; Supervision, M.M.; Project administration, K.R. and V.T.; Funding acquisition, K.R. and V.T. All authors have read and agreed to the published version of the manuscript."}, {"title": "Funding:", "content": "This research received no external funding."}, {"title": "Institutional Review Board Statement:", "content": "Not applicable."}, {"title": "Informed Consent Statement:", "content": "Not applicable."}, {"title": "Data Availability Statement:", "content": "The dataset presented in this article is not readily available because it is part of an ongoing multi-partner collaborative study. Requests to access the datasets should be directed to Malcolm Nicoll (malcolm.nicoll@ioz.ac.uk)."}, {"title": "Conflicts of Interest:", "content": "The authors declare no conflicts of interest."}, {"title": "Appendix A", "content": null}, {"title": "Appendix A.1. Hyperparameter Optimisation", "content": "We focused on tuning the hyperparameters for learning separately from those of data augmentation, since optimising each data augmentation method in conjunction with learning hyperparameters would give a very large search space.\nYOLOv7 employs the OneCycle learning rate scheduler [17], which sinusoidally decreases the learning rate across the number of epochs. The final learning rate is denoted by the product $LRF \\cdot LR0$, where $LRF$ represents the final ratio and $LR0$ denotes the base learning rate.\nBased on this, we selected $LR0$ and $LRF$ for optimisation. Weight decay $(WD)$ was also chosen to fine-tune the level of regularisation, with the aim of achieving optimal generalisation. For each of these, three values were chosen, with the central value of the search space being the optimal value for YOLOv7 on the MS COCO dataset."}]}