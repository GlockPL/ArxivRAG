{"title": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness", "authors": ["WANG, Wenxuan"], "abstract": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both software testing and natural language processing perspectives.\nFirst, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. FactChecker constructs knowledge graphs by retrieving fact triplets from large-scale knowledge databases and then generates various types of questions as well as the expected answers from the knowledge graphs, which are used as test cases to measure the factual correctness of LLMs. LogicAsker is a minimum functionality test framework that constructs\nthe set of atomic skills first by collecting all basic principles and laws from the logic study. Then it generates reasoning questions and the expected answers by converting standard logic expressions into natural languages, which are used as test cases to measure the logical reasoning correctness of LLMs. Our testing frameworks can automatically and comprehensively generate test cases to effectively unveil failures of state-of-the-art LLMs, such as ChatGPT and LLaMa. Besides, we also demonstrate that the generated test cases can improve the LLM's factual correctness and logical reasoning ability.\nSecond, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. First, we show that the safeguard of LLMs, textual content moderation software, is not robust enough against user-intended perturbation to bypass the moderation. We introduce MTTM, a metamorphic testing framework for textual content mod-eration software, with the metamorphic relation that a toxic sentence should still be identified as toxic after semantic-preserved perturba-tions. Experimental results show that MTTM can find failures in, as well as improve the reliability of commercial content moderation software. Second, we show that all the previous safety benchmarks, as well as the alignment dataset, are mainly in one language, e.g., English. we build the first multilingual safety benchmark for LLMs, XSafety, which covers 14 commonly used safety issues across ten languages spanning several language families, and find that all LLMs produce significantly more unsafe responses for non-English queries than English ones. In addition, we propose a simple and effective prompting method to improve LLM's multilingual safety by enhancing cross-lingual generalization of safety alignment.\nThird, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively. We first introduce BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. BiasAsker can measure the bias altitudes on 841 groups from 5,021 biased properties perspective by asking various kinds of questions. Experiments on 10 commercial systems and models show the effectiveness of BiasAsker. Then, we identify a cultural dominance issue within LLMs due to the predominant use of English data in model training and alignment and introduce XCulturalBench, a multilingual cultural-related benchmark, with concrete (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the cultural dominance problem. We also show that two effective methods in model development and deployment can significantly mitigate the cultural dominance issue in LLMs.", "sections": [{"title": "Introduction", "content": "This thesis presents my research on testing and evaluation of large language models from correctness, non-toxicity, and fairness perspectives. I first provide a brief overview of the research problems explored in Section 1.1 and highlight the main contributions of this thesis in Section 1.2. Then I list the publications that are related to this thesis during my Ph.D. study in Section 1.3 and outline the thesis structure in Section 1.4."}, {"title": "Overview", "content": "Recent advancements in Large Language Models (LLMs) have propelled artificial intelligence to a notable milestone. These models are pre-trained on vast textual corpora, comprising trillions of words, and thus encapsulate an extensive knowledge base. Enhanced through specific methods such as instruction-based fine-tuning [2] and human alignment [3], LLMs respond adeptly to user commands. Notably, ChatGPT has become one of the most prominent LLMs, demonstrating rapid adoption with 100 million monthly active users within two months of its launch, making it the fastest-growing software in history [4]. LLMs have significantly impacted various sectors, including machine translation [5], grammatical error correction [6], medical diagnose [7], program synthesis [8], and software testing [9, 10, 11]. They are reshaping human interactions with technology in work and daily life."}, {"title": "Thesis Contributions", "content": "In this thesis, we design and implement six novel testing and evaluation frameworks for Large Language Models. We focus on three crucial aspects: correctness, non-toxicity, and fairness. Concerning correctness, we design two novel automatic testing frameworks to trigger factual failures and logical reasoning failures. As for non-toxicity, we propose a metamorphic testing framework to evaluate whether the content moderation software is robust against human-intended perturbation. We also design a new multilingual safety benchmark to evaluate the safety of LLMs when communicating in different languages. And for fairness, we design an automatic testing framework to evaluate the social bias in LLMs. We also build the first multilingual cultural benchmark to measure the cultural bias in LLMs. The contributions are summarized as follows:\n\u2022 For correctness, we propose the first automatic testing framework, FactChecker, that can automatically and comprehensively eval-uate the factual correctness of LLMs. We also propose the first\n\u2022"}, {"title": "Background Review", "content": "In this chapter, I provide a systematic review of the background knowledge and related work. The overall structure is illustrated in Figure 2.1. Firstly, I briefly introduce the background of large language models in \u00a72.1. Then, \u00a72.2 presents the basic knowledge of software testing. \u00a72.3 provides the related works of LLM evaluation, including the evaluation of the performance in downstream tasks as well as the evaluation of the safety."}, {"title": "Large Language Models", "content": "In this section, I review the background of large language models. Specifically, I first introduce pre-training language models, which learn the language representations and modeling. Then I present the recently proposed large language models, which have a much larger number of parameters."}, {"title": "Pre-Training Language Models", "content": "Word2Vec. For decades, the n-gram based models have been dominating the language modeling field, due to their simplicity and low complexity of computation. With the progress of machine learning in recent years, it becomes possible to train more complex models on much larger datasets. For example, language models based on neural network learning significantly outperform n-gram models [43, 44, 45]. But these architectures are facing high computation costs between the projection and the hidden layer, because the values in the projection layer are dense.\nTo reduce the computation complexity, Mikolov et al. [46] pro-posed two shallow neural network architectures, i.e., the skip-gram model and the continuous bag-of-words model. In the meantime, to handle the intractability of full softmax function at the output, several solutions were proposed, either using hierarchical versions of softmax [47, 48] or unnormalized models for training [49]. Among these variants of the skip-gram model, the skip-gram model with negative sampling [48] has achieved state-of-the-art results across"}, {"title": "Large Language Models", "content": "LLMs have significantly advanced since the introduction of GPT-3, exceeding parameter sizes of a hundred billion [55]. GPT-3 is an autoregressive language model with 175 billion parameters, with the same architecture of GPT-2, 10x more than any previous non-sparse language model. These models' expansive parameterization enables them to store a vast repository of knowledge. They perform causal language modeling on extensive datasets, sometimes comprising over a trillion tokens. Through this, LLMs develop the capability to under-stand and generate natural language, demonstrating adaptability and effectiveness in various tasks without task-specific fine-tuning. For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting. Their proficiency in zero-shot and few-shot learning scenarios across diverse tasks exemplifies this versatility [12].\nSubsequent developments, starting with OpenAI's ChatGPT, have equipped LLMs with advanced conversational abilities, facilitating dialogic user interactions. According to the interview, the technical details of ChatGPT is not publically available but similar to the prior model, named InstructGPT [56].\nInstructGPT adopts the pre-trained GPT-3 models and then apply the following three steps, as is shown in Figure 2.7.\n\u2022 Step 1: Collect demonstration data, and train a supervised policy. The labelers provide demonstrations of the desired behavior on the input prompt distribution. Then fine-tune a pretrained GPT-3 model on this data using supervised learning."}, {"title": "Software Testing", "content": "In this section, I introduce the basic knowledge of software testing."}, {"title": "Definition", "content": "Software testing is the act of checking whether software satisfies expectations, introduced by Glenford J. Myers in 1979 [57]. Software testing can provide objective, independent information about the quality of software and the risk of its failure to a user or sponsor. Based on criteria for measuring correctness from an oracle, software testing employs principles and mechanisms that might recognize a problem."}, {"title": "Taxonomy", "content": "Software testing can be categorized into levels based on how much of the software system is the focus of a test.\nUnit testing, also known as component or module testing, is a form of software testing by which isolated source code is tested to validate expected behavior [58]. System testing, also known as end-to-end (E2E) testing, is testing conducted on a complete software system.\nSoftware testing can often be divided into white-box and black-box. These two approaches are used to describe the point of view that the tester takes when designing test cases.\nWhite box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) verifies the internal structures or workings of a program, as opposed to the functionality exposed to the end-user. Black box testing (also known as functional testing) describes designing test cases without knowledge of the implementation, without reading the source code. The testers"}, {"title": "Limitation and Our Focus", "content": "However, previous works suffer from several limitations. First, most of the previous testing works focus on traditional software testing, such as testing on code and systems as well as traditional AI software, such as classification and regression models. In this thesis, I argue that LLMs, as one of the most popular and promising software, urgently need testing for reliability and safety. Besides, I highlight that the testing of LLMs has new challenges compared with the testing of the traditional software mentioned above. On the one hand, LLMs can receive any sentence as input, so how to comprehensively and systematically generate input test cases is not trivial. On the other hand, LLMs can generate diverse human-like output, which leads to difficulties in detecting failures automatically and accurately."}, {"title": "LLMs Evaluation Benchmarks", "content": "In this section, I introduce related works about building benchmarks to evaluate LLMs."}, {"title": "Natural Language Processing Tasks", "content": "The initial objective behind the development of language models, particularly large language models, was to enhance performance on natural language processing tasks, encompassing both understanding and generation. Consequently, the majority of evaluation research has been primarily focused on natural language tasks.\nNatural Language Understanding Natural language understand-ing represents a wide spectrum of tasks that aims to obtain a better understanding of the input sequence. I summarize recent efforts in LLMs evaluation from several aspects.\nSentiment analysis is a task that analyzes and interprets the text to determine the emotional inclination. It is typically a binary (positive and negative) or triple (positive, neutral, and negative) class classification problem. Evaluating sentiment analysis tasks is a popular direction. Liang et al. [70] showed that the performance of the models on this task is usually high.\nNatural language inference (NLI) is the task of determining whether the given \"hypothesis\" logically follows from the \u201cpremise\u201d. [71] finds that LLMs performs well in NLI benchmarks, such as MNLI [72] and SNLI [73]"}, {"title": "Applications", "content": "Evaluating the performance of LLMs in various application domains has recently gain more and more attention in scientific research, technology development, and engineering studies [82].\nFor fundamental mathematical problems, most LLMs demonstrate proficiency in addition and subtraction and possess some capability in multiplication. However, previous works reveal that LLMs face challenges when it comes to division, exponentiation, trigonometry functions, and logarithm functions [83, 84] When confronted with com-plex and challenging mathematical problems, LLMs exhibit subpar performance. Specifically, GPT-3 demonstrates nearly random per-formance, while GPT-3.5 shows improvement, and GPT-4 performs the best [85]\nCode intelligent tasks are a range of different tasks, including code generation, code understanding, code optimization, code translation, et al. [86, 87, 88, 89, 90]. [91] provides a comprehensive survey about the evaluation the LLMs for code and then conclude the following findings: a) The current evaluation of LLMs focuses more on code generation tasks, with less emphasis on evaluating or researching other tasks such as vulnerability repair; b) In code generation tasks, the Code-LLaMA series of LLMs perform the best; c) For test case generation tasks and code translation tasks, GPT-4 demonstrate better performance; d) In code summarization tasks, CodeT5+ outperforms GPT-3.5 (GPT-3.5-turbo).\nThe application of LLMs in the medical field has recently received significant attention. The significance of evaluating LLMs on medical lies in providing accurate and reliable medical answers to meet the needs of healthcare professionals and patients for high-quality medical"}, {"title": "Reliability", "content": "The evaluation of reliability encompasses crucial aspects of factuality, Robustness, ethics and bias. These factors have gained increasing importance in assessing the performance of LLMs comprehensively.\nFactuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts. Factuality in LLMs significantly impacts a variety of tasks and downstream applications, where incorrect or inconsistent information could lead to substantial misunderstandings and misinterpretations. Evaluating factuality is of great importance in order to trust and efficiently use these models. This includes the ability of these models to maintain consistency with known facts, avoid generating misleading or false information (known as \"factual hallucination\"), and effectively learn and recall factual knowledge. A range of benchmarks have been proposed to measure and improve the factuality of LLMs, such as TruthfulQA [101], LAMA [102],\nRobustness studies the stability of a system when facing unex-pected inputs. Specifically, out-of-distribution (OOD) and adversarial robustness are two popular research topics for robustness. Wang et al. [105] is an early work that evaluated ChatGPT and other LLMs from both the adversarial and OOD perspectives using existing benchmarks, such as AdvGLUE [106]and ANLI [72]. Robustness against adversarial attacks is also a popular topic in AI reliability field [107, 108, 109, 110].\nThe safety of LLMs has drawn more and more attention since these models remain vulnerable to jailbreak inputs that can prompt undesirable behavior [111, 112, 113, 114, 115]. Researchers have discovered that safety mechanisms can be circumvented by trans-forming the malicious query into semantically equivalent forms, such as ciphers [116, 117, 118], low-resource languages [38, 119, 120], or code [121]. Another effective jailbreak method is to frame the malicious question in a hypothesis scenario that makes it appear harmless [111, 122, 123]. Given the high intelligence of LLMs, insights from social science [124] and psychology [125] have also been applied to uncover safety issues. Moreover, techniques like adversarial suffix optimization [126, 127, 128] and few/many-shot attacks [116, 129] have proven to be highly effective.\nLLMs have been found to internalize, spread, and potentially magnify harmful information existing in the crawled training corpora, usually, toxic languages, like offensiveness, hate speech, and insults, as well as social biases like stereotypes towards people with a particular demographic identity (e.g., gender, race, religion, occupation, and ideology). More recently, Zhuo et al. [130] used conventional testing sets and metrics to perform a preliminary evaluation of ChatGPT's"}, {"title": "Limitation and Our Focus", "content": "However, previous works suffer from several limitations. On the one hand, the scope of the evaluation is limited. The building of their benchmark involves huge human effort, not only in the designing of the question but also in the evaluation process, which limits the efficiency and the scope of the evaluation. Hence, most of the previous works only conducted a small-scale study and only on specific NLP Models, such as BERT. On the other hand, the foci of the previous works are limited and fail to meet the new demand in the era of LLMs. For example, previous safety benchmarks only consider a single language, English, without considering the wide range of global users of LLMs.\nIn this thesis, I propose several easily scalable benchmarks that can efficiently evaluate correctness, non-toxicity, and fairness. I focus on a) how to generate test cases without the need for large human efforts, b) how to evaluate LLMs automatically without massive human annotation and c) how to evaluate LLMs from novel but necessary perspectives, such as multilingual safety."}, {"title": "Testing the Factual Correctness of LLMs", "content": "In this chapter, we present our investigation of the testing and evaluation of the factual correctness of LLMs. We first introduce the motivation of measuring the factual correctness in \u00a73.1 and then elab-orate our proposed approach in \u00a73.2. In \u00a73.3, we conduct experiments to evaluate our approach and answer the research questions. Finally, we summarize the work in \u00a73.4."}, {"title": "Problems and Motivation", "content": "Recent advancements in Large Language Models (LLMs) have pro-pelled artificial intelligence to a notable milestone. These models are pre-trained on vast textual corpora, comprising trillions of words, and thus encapsulate an extensive knowledge base. Enhanced through specific methods such as instruction-based fine-tuning [2] and human alignment [3], LLMs respond adeptly to user commands. Notably, ChatGPT has become one of the most prominent LLMs, demon-strating rapid adoption with 100 million monthly active users within two months of its launch, making it the fastest-growing software in history [4]. LLMs have significantly impacted various sectors, including machine translation [5], grammatical error correction [6], program synthesis [8], and software testing [9, 10, 11]. They are reshaping human interactions with technology in work and daily life."}, {"title": "Methodology", "content": "In this section, we present FactChecker, a novel framework designed to identify factual errors in LLMs. Figure 3.1 depicts the framework of FactChecker, which consists of three stages:\n1. Knowledge Graph Construction: Constructing a factual KG with a set of fact triplets extracted from an external database.\n2. Question Generation: Generating various one-hop and multi-hop questions from the constructed KG, which are then undergone a post-editing module to enhance their fluency and grammatical correctness.\n3. Answer Assessment: Querying the LLMs under test and detecting the suspicious factual errors according to matching algorithms."}, {"title": "Knowledge Graph Construction", "content": "The initial step in FactChecker entails establishing a well-structured factual KG. To accomplish this, FactChecker employs a procedure for extracting factual triplets from a knowledge base. In our demonstra-tion, we utilize the largest and most comprehensive publicly available knowledge base, Wikidata\u00b9. Wikidata, as a comprehensive knowledge repository with more than 100 million items, serves as the primary source for the fact triplets we retrieve. Nevertheless, it is important to note that alternative knowledge bases could replace Wikidata in this role. For situations where data security is a concern, a private knowledge base may be a more suitable option to mitigate the risk of data leakage. The selection of these fact triplets is based on specific features, such as predefined topics. Subsequently, FactChecker utilizes these extracted fact triplets to construct a directed KG, a crucial step in facilitating the generation of test case questions.\nA fact triplet is represented in the form of (SUBJECT, relation,OBJECT). For instance, the triplet (USA, capital, Washington D.C.) denotes the fact that the capital of the USA is Washington D.C. FactChecker enables users to obtain fact triplets pertaining to specific topics. As illustrated in Figure 3.2, when a user expresses interest in the topic of emperors, FactChecker proceeds to convert the \u201coccu-pation: emperor\" specification into a SPARQL query language\u00b2, which is utilized for querying related triplets in Wikidata. The resulting SPARQL query will retrieve all accessible fact triplets about emperors, including examples such as \"Napoleon, place of birth, Ajaccio\" and \"Peter the Great, father, Alexei I of Russia\".\nAfter retrieving the triplets, a directed graph is constructed by FactChecker, denoted as G = (V, E), where the vertex set V comprises"}, {"title": "Question Generation", "content": "FactChecker utilizes a rule-based approach to generate questions from the constructed KG. The system is capable of generating various types of questions, including different question types, i.e., Yes-No questions, MC questions and WH questions, and different question hops, i.e., single-hop questions and multi-hop questions. After that, FactChecker also adopts two steps, namely filtering and rewriting, to enhance the grammatical correctness and fluency of the generated questions.\nTo generate Yes-No questions from a fact triplet in the format (SUBJECT, relation, OBJECT), FactChecker utilizes the spaCy toolkit to conduct Part of Speech (PoS) analysis on the relation within the fact triplet. This analysis helps to determine the suitable auxiliary verb (AUX) required for constructing the question. If the PoS of the relation is identified as a NNS/NNP, indicating a noun, the question is formulated as \"AUX OBJECT the relation of SUBJECT?\" Conversely, if the relation is identified as a VB/VBZ/VBD, representing a verb, the question is structured as \u201cAUX SUBJECT relation OBJECT?\u201d Additionally, if the relation is recognized as a VBN, indicating a"}, {"title": "Answer Assessment", "content": "Once the responses from LLMs have been collected, the evaluation process can commence, aiming to assess the performance and identify any factual errors present within the system. For Yes-No & MC Questions: Given the strict criteria for the generated output in the case of Yes-No and MC questions, an evaluation of the correctness of the LLMs' response can be conducted using the exact match method. This approach entails comparing the generated response directly with the ground-truth answer to determine the accuracy of the response. For WH Questions: Owing to the constraints placed on the variation of generated output for WH questions, the exact match method can not be directly used for evaluating the correctness of such questions. This is due to the possibility of different variations or alternative names for the same entity, which may result in valid but non-matching answers. For example, \u201cthe Great Britain\" and \"United Kingdom\" are referring to the same country but the exact match method will treat them differently. In order to address this challenge, we implements and compares five different methods to identify whether the response is the same as the answer.\nThe five evaluation methods can be classified into two distinct types: (1) lexical-based methods, such as Levenshtein distance and N-grams matching, that focus on the superficial patterns present in the answer. (2) semantic-based methods, such as word embedding, sentence transformer, and ChatGPT, that consider the semantic context and meaning of the answer.\n\u2022 Levenshtein distance: It is a string metric that quantifies the minimum number of single-character edits required to transform one word into another, adopted in [148]. The Levenshtein distance metric is particularly useful in verifying character-level variations of the model answer, such as \u201cAnna Komnene\u201d and \u201cAnna Comnena,\u201d and helps reduce the occurrence of false positives during the\n\u2022"}, {"title": "Experiment", "content": "To validate the effectiveness of FactChecker and get more insights on the factual accuracy of LLMs, we use FactChecker to test four commercial LLM applications and two research models. In this section, we detail the evaluation process and empirically explore the following three Research Questions (RQs).\n\u2022 RQ1: Can FactChecker find factual errors in LLMs?\n\u2022 RQ2: Are the factual errors found by FactChecker valid?\n\u2022 RQ3: Can we use FactChecker to improve the factual correctness of LLMs?\nIn RQ1, our goal is to investigate the effectiveness of FactChecker in systematically triggering and identifying factual errors in LLMs. To the best of our knowledge, FactChecker is the first approach to systematically reveal the factual errors in LLMs. We also analyze whether the results generated by FactChecker can provide an intuitive and constructive impression of factual errors in the tested systems. Since FactChecker adopts diverse Natural Language Processing (NLP) techniques, which are generally imperfect (e.g., the methods may produce false positives and true negatives) [150, 151], in RQ2, we evaluate the validity of the identified factual errors through manual inspection. Here, \"validity\" refers to whether the detected factual errors indeed represent actual inaccuracies, i.e., the errors are not false positives. Finally, in RQ3, we analyze how can we use the FactChecker to improve the factual correctness in LLMs."}, {"title": "Experimental Setup", "content": "To assess the effectiveness of the FactChecker, we employ it to evaluate four widely-utilized commercial"}, {"title": "Preliminary Experiments", "content": "In this section, we conducted an initial experiment to validate our choice of employing a rule-based method for question generation as opposed to directly instructing ChatGPT to craft questions from fact triplets. Additionally, we conducted experiments to assess the effectiveness of our polishing modules. We also meticulously investigated the comparison of five evaluation metrics.\nIs used.  We can verify whether\n $$ \\sin x_1 = \\sin x_2 $$\nwithout knowing the output of either sine calculation.\nAny violations of this metamorphic relation indicate a potential bug\nin the sine function implementation [64].\nMetamorphic testing has been adapted to validate Artificial"}, {"title": "RQ1: Effectiveness of FactChecker", "content": "In this RQ, we investigate whether FactChecker can effectively trigger factual errors from and provide insight about LLMs.\nFactChecker Can unveil various factual errors in different LLMs. After posing diverse sets of questions to various LLMs and collecting their corresponding responses, FactChecker evaluates the accuracy of these responses and effectively detects instances where factual errors occur. As illustrated in Table 3.5, FactChecker successfully identifies a significant number of factual errors across both commercial and research-oriented LLMs. Notably, even the highest-performing LLM in the evaluation achieves an accuracy of less than 80%."}, {"title": "RQ2: Validity of Identified Factual Errors", "content": "In this RQ, we investigate whether the factual error exposed by FactChecker are true failures through manual inspection. We man-"}, {"title": "RQ3: Using FactChecker for Improvement", "content": "We have demonstrated that FactChecker can unveil the factual errors from commercial LLM products and state-of-the-art academic LLMs. The following substantial question is: can these test cases be utilized to improve the factual accuracy of LLMs? There are two threads of methods to improve the performance of LLMs. For the large-scale and API-based LLMs, we use the In-Context learning (ICL)"}, {"title": "Testing the Logical Reasoning Correctness of LLMs", "content": "In this chapter, we present our investigation of the testing and evalu-ation of the logical reasoning correctness of LLMs. We first introduce the motivation of measuring the logical reasoning correctness in \u00a74.1 and then elaborate our proposed approach in \u00a74.2. In \u00a74.3, we conduct experiments to evaluate our approach and answer the research questions. Finally, we summarize the work in \u00a74.4."}, {"title": "Problems and Motivation", "content": "Large language models (LLMs), with their rapid increase of model parameters and training data, have gained emergent abilities in various tasks [156, 8, 5], making a significant impact on human life. In particular, OpenAI's ChatGPT has emerged as the fastest-growing app of all time, amassing 100 million monthly active users within two months [157]. According to Nature's survey [158], around one-third of the postdoctoral researchers globally are using LLMs to facilitate their research.\nThe primary advantage of advanced LLMs over previous AI systems, though controversial, is their ability in \"reasoning\" [159, 160]. Reasoning is a cognitive process that involves using evidence, argu-ments, and logic to arrive at conclusions or make judgments [161]."}, {"title": "Methodology", "content": "In this section, we introduce the design and implementation of LogicAsker, a novel tool to trigger logical reasoning failures in large language models. Figure 7.1 overviews the workflow of LogicAsker, which consists of three main modules: test case generation, weakness identification and in-context learning (ICL) demonstration. In partic-ular, the test case generation module utilizes atomic skills defined on the two formal logic systems and an inference synthesis approach to generate questions as test cases. Then, the generated cases are fed into the LLMs to reveal weaknesses and provide insights into the LLMs by the weakness identification process. Finally, LogicAsker utilizes these insights to construct ICL demonstrations to improve the reasoning abilities of the LLMs."}, {"title": "Reasoning Skills", "content": "Propositional and predicate logic are two fundamental systems that formalize the reasoning process. The inference rules and equivalence laws in these two systems are atomic and can cover all correct reasoning scenarios; therefore, we define these 30 rules as the set of atomic skills an LLM should possess to perform formal reasoning."}, {"title": "Test Case Generation", "content": "To generate logical questions, LogicAsker first adopts a rule-based method to generate logical expressions systematically based on rea-soning skills and then translates the logical expressions into natural language. Figure 4.2 provides an overview of the procedure."}, {"title": "Testing the Safety of LLMs Against Human Intended Perturbation", "content": "In this chapter, we present our investigation of the testing of the safety of LLMs against human-intended perturbation. We first introduce the motivation in \u00a75.1 and then elaborate our proposed approach in \u00a75.2. In \u00a75.3, we conduct experiments to evaluate our approach and answer the research questions. Finally, we summarize the work in \u00a75.4."}, {"title": "Problems and Motivation", "content": "In the recent decade, social media platforms and community forums have been developing rapidly, which tremendously facilitates modern textual communication and content publication worldwide. For example, the number of tweets posted on Twitter has grown from 50 million per day in 2010 to 500 million per day in 2020 [181]. However, they inevitably exacerbate the propagation of toxic content due to the anonymity of the web. Textual toxic contents typically refer to three major kinds of texts: (1) abusive language and hate speech, which are abusive texts targeting specific individuals, such as politicians, celebrities, religions, nations, and the LGBTIQA+ [182]; (2) malicious advertisement, which are online advertisements with illegal purposes, such as phishing and scam links, malware download, and illegal"}, {"title": "Methodology", "content": "This section first introduces a pilot study on text messages collected from real users (Section 5.2.1). Then we introduce eleven metamorphic relations that are inspired by the pilot study. These metamorphic relations can be grouped into three categories according to the perturbation performed: character-level perturbations (Sec. 5.2.2), word-level perturbations (Sec. 5.2.3), and sentence-level perturbations (Sec. 5.2.4)."}, {"title": "Pilot Study", "content": "In this work, we intend to develop metamorphic relations that assume the seed test case (i.e., a piece of text) and the perturbed test case should have identical classification labels (i.e., labeled as \"toxic content\") returned by the content moderation software. To generate effective test cases, we think the perturbations in our MRs should be:\n\u2022 Semantic-preserving: the perturbed test cases should have the identical semantic meaning as the seed.\n\u2022 Realistic: should reflect possible inputs from real users.\n\u2022 Unambiguous: should be defined clearly.\nIn order to design satisfactory perturbations, we first conducted a pilot study on text messages from real users to explore what kind of perturbations the users would apply to the toxic content to bypass the content moderation software. We consider text messages from four platforms with a large number of users:\n\u2022 Twitter\u00b9 is a worldwide microblogging and social media platform on which users post and interact via messages known as \"tweets\". Hate Offensive\u00b2 [207] is a GitHub repository containing 24,802 English hate speech sentences collected from Twitter."}, {"title": "Evaluating the Multilingual Safety of LLMs", "content": "In this chapter, we present our work of evaluating the multilingual safety of LLMs. We first introduce the motivation of multilingual safety in \u00a76.1 and then elaborate our proposed approach in \u00a76.2. In \u00a76.3, we conduct experiments to evaluate our approach and answer the research questions. Finally, we summarize the work in \u00a76.4."}, {"title": "Problems and Motivation", "content": "Recent advances in scaling Large Language Models (LLMs) have made breakthroughs in the Artificial Intelligence (AI) area. With the rapid increase of model parameters and training data", "156": "code generation [8", "5": ".", "216": "Google's Bard [217", "19": ".", "significance": "ensuring the safety of the deployed LLMs.\nNumerous studies have been conducted to align large language models (LLMs) with human ethics and preferences to improve their safety. These include methods such as data filtering ["}]}