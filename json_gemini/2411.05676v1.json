{"title": "Improving Molecular Graph Generation with\nFlow Matching and Optimal Transport", "authors": ["Xiaoyang Hou", "Tian Zhu", "Milong Ren", "Dongbo Bu", "Xin Gao", "Chunming Zhang", "Shiwei Sun"], "abstract": "Generating molecular graphs is crucial in drug design and discovery but remains\nchallenging due to the complex interdependencies between nodes and edges. While\ndiffusion models have demonstrated their potentiality in molecular graph design,\nthey often suffer from unstable training and inefficient sampling. To enhance\ngeneration performance and training stability, we propose GGFlow, a discrete\nflow matching generative model incorporating optimal transport for molecular\ngraphs and it incorporates an edge-augmented graph transformer to enable the\ndirect communications among chemical bounds. Additionally, GGFlow introduces\na novel goal-guided generation framework to control the generative trajectory of\nour model, aiming to design novel molecular structures with the desired properties.\nGGFlow demonstrates superior performance on both unconditional and conditional\nmolecule generation tasks, outperforming existing baselines and underscoring its\neffectiveness and potential for wider application.", "sections": [{"title": "1 Introduction", "content": "De novo molecular design is a fundamental but challenging task in drug discovery and design. While\nthe searching space of the molecular graph is extremely tremendous, as large as $10^{33}$ [Polishchuk\net al., 2013]. Machine learning methods have been introduced to generate molecular graphs due to the\nlarge amount of data in the field. These models are typically categorized into autoregressive and one-\nshot types. Autoregressive models, such as GraphDF [Luo et al., 2021], generate graphs sequentially,\noften overlooking the interdependencies among all graph components. In contrast, one-shot methods\ngenerate entire graphs in a single step, more effectively capturing the joint distribution [Kong et al.,\n2022].\nDiffusion models have shown great promise and achieved significant performance in various domains\n[Ho et al., 2020, Song et al., 2020, Ho et al., 2022]. In the context of molecular graph generation,\ndiffusion models have been adopted to enhance generative capacity. EDP-GNN and GDSS are\namong the first to utilize diffusion models for graph generation, adding continuous Gaussian noise to\nadjacency matrices and node types, which may lead to invalid molecular graph structures [Niu et al.,\n2020, Jo et al., 2022b]. Due to the inherent sparsity and discreteness of molecular graph structures,\nGSDM enhances model fidelity by introducing Gaussian noise within a continuous spectrum space"}, {"title": "2 Related Work", "content": "2.1 Flow Matching and Diffusion Models\nDiffusion models have gained widespread popularity in various fields, including computer vision,\nnatural language processing, and biological sciences, demonstrating notable success in generative\ntasks [Ho et al., 2020, Song et al., 2020, Watson et al., 2023, Ingraham et al., 2023, Liu et al., 2024a,\nRen et al., 2024, Zhu et al., 2024]. However, these models often suffer from inefficiencies in sampling\ndue to the complexity of their underlying diffusion processes and the convergence properties of the\ngenerative process.\nFlow matching generative models have emerged as a more efficient and stable alternative (details in\nAppendix A.1), improving sampling by straightening the generative probability path [Lipman et al.,\n2022, Song et al., 2024, Campbell et al., 2024]. Some approaches further enhance performance by\nincorporating optimal transport. The generative processes of these models are summarized in Figure\n1.\nPrevious works [Campbell et al., 2024, Gat et al., 2024] extended flow matching to discrete spaces,\nwhile Eijkelboom et al. [2024] applied variational flow matching to graphs, but without adequately\naddressing key graph-specific properties such as adjacency matrix sparsity. GGFlow tackles these\nchallenges by introducing a discrete flow matching model with optimal transport tailored for graph\ndata. Furthermore, we propose a novel framework for guiding the generative process, enhancing its\npractical applicability."}, {"title": "2.2 Molecular Graph Generative Models", "content": "Molecular Graph generative models are typically categorized into two main types: autoregressive and\none-shot models. Autoregressive models, such as generative adversarial networks [Wang et al., 2018],\nrecurrent neural networks [You et al., 2018], variational autoencoders [Jin et al., 2018], normalizing\nflows [Shi et al., 2019, Luo et al., 2021] and diffusion model [Kong et al., 2023], generate graphs\nsequentially. While effective, these models are often computationally expensive and fail to account\nfor permutation invariance, a crucial property for graph data, resulting in potential inefficiencies.\nIn contrast, one-shot models aim to capture the distribution of all molecular graph components\nsimultaneously [De Cao and Kipf, 2018, Ma et al., 2018, Zang and Wang, 2020], better reflecting\nthe inherent interactions within molecular graphs. Despite the advantages, diffusion-based one-shot\nmodels [Niu et al., 2020, Jo et al., 2022b, Vignac et al., 2022, Chen et al., 2023, Bergmeister et al.,\n2023, Luo et al., 2023, Haefeli et al., 2022, Yan et al., 2023, Jang et al., 2023, Madeira et al., 2024,\nBergmeister et al., 2024, Chen et al., 2023, Minello et al., 2024, Zhao et al., 2024, Xu et al., 2024]\nshow promising results in downstream tasks but remain limited by sampling efficiency. GGFlow\naddresses these limitations by employing a discrete flow-matching generative model, achieving\nsuperior generative performance with fewer sampling steps."}, {"title": "3 Methods", "content": "In this section, we present our methodology, GGFlow. Section 3.1 outlines the discrete flow matching\nmethod for molecular graph generation. Section 3.2 covers optimal transport for graph flow matching.\nSection 3.3 introduces GraphEvo, our neural network for graph generation. Section 3.4 examines the\npermutation properties of GGFlow, and Section 3.5 discusses goal-guided molecule generation using\nreinforcement learning."}, {"title": "3.1 Discrete Flow Matching for Molecular Graph Generation", "content": "A molecular graph $G = (V, E)$, where V and E denote the sets of nodes and edges, has a distribution\ndenoted by $p(G) = (p_V(V), p_E(E))$. The attribute spaces for nodes and edges are V and E, with\ncardinalities n and m, respectively. The attributes of node i and edge ij are denoted by $v_i \\in V$ and\n$e_{ij} \\in E$, so the node and edge probability mass functions (PMF) are $p_V(v_i = a)$ and $p_E(e_{ij} = b)$\nwhere $a \\in \\{1,..., n\\}$ and $b \\in \\{1,...,m\\}$. The node and edge encodings in the graph are given\nby matrices $V \\in \\mathbb{R}^{a \\times n}$ and $E \\in \\mathbb{R}^{a \\times a \\times m}$, respectively. We denote the transpose of matrix A as\n$A^T$ and $A^t$ represents the state of matrix A at time t. We use discrete flow matching to model the\nmolecular graph generation process.\nSource and target distribution GGFlow aims to transform prior distribution $G^0 \\sim P_{ref}$ to target\ndata distribution $G^1 \\sim P_{data}$. The training data $(G^0, G^1)$ are sampled from a joint distribution\n$\\pi(G^0, G^1)$, satisfying the marginals constraints $P_{ref} = \\Sigma_{G^1} \\pi(G^0, G^1)$, $P_{data} = \\Sigma_{G^0} \\pi(G^0, G^1)$.\nIn the simplest case, the joint distribution $\\pi(G^0, G^1)$ is modeled as the independent coupling, i.e.\n$\\pi(G^0, G^1) = P_{ref} \\cdot P_{data}$."}, {"title": "Probability path", "content": "To account for graph sparsity, the prior distribution $P_{ref} = (P_{ref}^V, P_{ref}^E)$ is designed to approximate\nthe true data distribution closely. To ensure the permutation invariance of the model, the priors are\nstructured as products of single distributions for all nodes and edges: $\\Pi_i v_i \\times \\Pi_{ij} e_{ij}$ [Vignac et al.,\n2022]. Further details on the prior can be found in Appendix B.1.\nWe define a probability path $p_t(G_t)$ that interpolates between source distribution\n$P_{ref}$ and target distribution $P_{data}$ i.e. $p_0 = P_{ref}$ and $p_1 = P_{data}$. The marginal probability path is\ngiven by:\n$p_t(G_t) = \\Sigma_{(G^0,G^1) \\sim \\pi} p_t(G_t|G^0, G^1)\\pi(G^0, G^1),$ \t(1)\nwhere\n$p_t(G_t\\vert G^0, G^1) = \\text{Cat} \\left(t\\delta\\{G^1, G_t\\} + (1-t)P_{ref}\\right)$\n$= \\text{Cat} \\left(t\\delta\\{V^1, V_t\\} + (1-t)P_{ref}^V, t\\delta\\{E^1, E_t\\} + (1-t)P_{ref}^E\\right),$\n$\\delta$ is the Kronecker delta, indicating equality of the indices, and $\\text{Cat}(p)$ denotes a Categorical\ndistribution with probabilities p. Given the sparsity of both the prior and data distributions, we can\ninfer that the intermediate distribution is similarly sparse, aiding model training."}, {"title": "Training objective", "content": "We define a probability velocity field $u_t(G, G_t) = (u_t^V(V,V_t), u_t^E(E, E_t))$ for GGFlow, which\ngenerates the probability path from Equation 1. The probability velocity field $u_t(G, G_t)$ is derived\nfrom the conditional probability velocity field $u_t(G, G_t|G^0, G^1)$, and can be expressed as:\n$u_t(G, G_t) = \\Sigma_{(G^0,G^1) \\sim \\pi} u_t(G, G_t\\vert G^0, G^1)p_t(G^0, G^1\\vert G_t), \\qquad(2)$\n$p_t(G^0, G^1\\vert G_t) = p_{1\\vert t}(G^1\\vert G_t, G^0) = \\frac{p_t(G_t\\vert G^0,G^1)\\pi(G^0, G^1)}{\\Sigma_{G^0, G^1} p_t(G_t\\vert G^0,G^1)\\pi(G^0, G^1)}.\\qquad(3)$\nGGFlow chooses the conditional marginal probability $u_t(G, G_t|G^0, G^1)$ as:\n$u_t(G, G_t|G^0, G^1) = \\frac{1}{Z_t (1-t)P_{ref}} \\text{ReLU} \\left(1 - \\frac{\\delta\\{G_t, G^1\\}}{P_t|1}\right) (1 - \\delta\\{G_t, G^1\\}), G_t \\neq G,\\qquad(4)$\nwhere $\\text{ReLU}(a) = \\text{max}(a,0)$ and $Z_t = |\\{G_t : p_t(G_t|G^0,G^1) > 0\\}|$. More details about the\nconditional vector field are provided in Appendix B.2.\nGiven the intractability of the posterior distribution $p_{1\\vert t}(G^1\\vert G_t, G^0)$, we ap-\nproximate it as $p_{1\\vert t}(G^1\\vert G_t, G^0)$ using neural network, as detailed in Section 3.3. The training\nobjective is formulated as:\n$\\mathcal{L} = E_{p_{data} (G^1) U(t; 0,1) \\pi(G^0, G^1) p_t(G_t\\vert G^0, G^1)} [\\text{log }p_{1\\vert t}(G^1\\vert G_t, G^0)],\\qquad(5)$\nwhere $U(t; 0, 1)$ is a uniform distribution on [0, 1]."}, {"title": "Sampling Procedure", "content": "In the absence of the data distribution $G^1$ during sampling, we reparameterize\nthe conditional probability $p_t(G^0, G^1|G_t)$ as:\n$p_t(G^0, G^1\\vert G_t) = p_{1\\vert t}(G^1\\vert G_t, G^0) = \\frac{p_t(G_t/G^0)p(G^0)}{\\Sigma_{G^0} p_t(G_t \\vert G_0)p(G^0)}.\nref\\qquad(6)$\n$p_t(G_t/G^0) = \\text{Cat} \\left(t\\delta\\{V^1, V\\} + (1 - t)p^V_{ref}, t\\delta\\{E^1, E\\} + (1-t)p^E_{ref}\\right)$\nAnd we can simplify the generative process $p_{t+\\triangle t\\vert t}(G_{t+\\triangle t}|G_t, G^0)$ without the calculation of the\nfull expectation over conditional vector field $\\hat{u}_t (G, G_t|G^0, G^1)$:\n$p_{t+\\triangle t\\vert t}(G_{t+\\triangle t}|G_t, G^0) = E_{p_{1\\vert t}(G^1\\vert G_t,G^0)} [\\delta(G_t, G_{t+\\triangle t}) + u_t(G_t,G_{t+\\triangle t}\\vert G^0, G^1)\\triangle t]$\n$= \\Sigma_{G^1} p_{t+\\triangle t\\vert t}(G_{t+\\triangle t}|G^1, G_t, G^0)\\vert p_{1\\vert t}(G^1\\vert G_t, G^0).$\nWe first sample the $\\hat{G}^1$ using the approximate distribution $p_{1|t}(G^1|G_t, G^0)$ and then sample the next\nstate $G_{t+\\triangle t}$ using sampled $\\hat{G}^1$. The sampling procedure $p_{t+\\triangle t\\vert t}(G_{t+\\triangle t}|G^1, G_t, G^0)$ can thus be\nformulated as:\n$G_{t+\\triangle t} \\sim \\delta\\{\\cdot, G_t\\} + u_t(\\cdot, G_t\\vert G^0,\\hat{G}^1)\\triangle t$.\nFurther details on the sampling and training procedures are provided in Algorithms 1 and 4."}, {"title": "3.2 Optimal transport for graph flow matching", "content": "Optimal transport (OT) has been effectively applied to flow matching generative models in continuous\nvariable spaces, to improve generative performance [Tong et al., 2023, Bose et al., 2023, Song et al.,\n2024]. To generalize this for graphs, we extend the joint distribution $\\pi(G^0, G^1)$ from independent\ncoupling to the 2-Wasserstein OT map $\\phi^*$, which minimizes the 2-Wasserstein distance between $P_{ref}$\nand $P_{data}$. To optimize the computational efficiency of OT and preserve permutation invariance, we\ndefine the distance via the Hamming distance $H(G^1, G^0)$ [Bookstein et al., 2002]:\n$\\phi^* (p_0, p_1) = \\underset{\\Phi \\in \\Phi}{\\text{arg inf}} \\int_{R^d\\times R^d} H(G^0, G^1) d \\Phi(G^0, G^1),\\qquad(7)$\nwhere\n$H(G^0, G^1) = \\sum_{i,j} \\delta(v_i^0, v_i^1) + \\sum_{i,j} \\delta(e_{i,j}^0, e_{i,j}^1). \\qquad(8)$\nHere $\\Phi$ represents the set of all joint probability measures on $\\mathbb{R}^d \\times \\mathbb{R}^d$ that are consistent with the\nmarginal distributions $p_0$ and $p_1$, where $G^K = (V^K = \\{v_i\\}, E^K = \\{e_{ij}\\})$, $K = 0, 1$.\nThe practical application of OT to large datasets is computationally intensive, often requiring cubic\ntime complexity and quadratic memory [Tong et al., 2020, Villani, 2009]. To address these challenges,\nwe use a minibatch approximation of OT [Fatras et al., 2021]."}, {"title": "3.3 GraphEvo: Edge-augmented Graph Transformer", "content": "Our neural network, GraphEvo, predicts the posterior distribution $p_{1\\vert t}(G^1\\vert G_t, G^0)$ using the inter-\nmediate graph $G_t$. In graph-structured data, edge and structural information are as critical as node\nattributes, and incorporating edge relations enhances chemical bond generation tasks [Hussain et al.,\n2024, Hou et al., 2024, Jumper et al., 2021]. To capture these relations, GraphEvo extends the graph\ntransformer by introducing a triangle attention mechanism for edge updates, along with additional\ngraph features y, such as cycles and the number of connected components [Vignac et al., 2022]. This\nenables GraphEvo to efficiently and accurately capture the joint distribution of all graph components.\nThe key self-attention mechanisms are outlined in Algorithm 2, where node, edge, and graph features\nare represented as $X \\in \\mathbb{R}^{bs\\times n \\times d_x}$, $E\\in \\mathbb{R}^{bs\\times n \\times d_x}$, and $y \\in \\mathbb{R}^{bs\\times n \\times d_y}$, where bs denotes batch size,\nn is the number of nodes, and $d_x$ and $d_y$ are the feature dimensions for node and global features,\nrespectively. Further details are provided in Appendix C."}, {"title": "3.4 Permutation Property Analysis", "content": "Graphs are invariant to random node permutations, and GGFlow preserves this property. To ensure\npermutation invariance, we analyze the permutation properties of our neural network, training\nobjectives, and conditional probabilities path. First, we analyze the permutation invariance of the\ntraining objectives [Vignac et al., 2022]. Since the source and target distributions, along with the\nHamming distance, are permutation invariant, the optimal transport map derived from Equation 7 and\nindependent coupling also exhibit this invariance.\nTheorem 1. If the distributions $p(G^0)$ and $p(G^1)$ are permutation invariant and the cost func-\ntion maintains this invariance, then the optimal transport map $\\phi$ also respects this property, i.e.,\n$\\phi(G^0, G^1) = \\phi(\\pi G^0, \\pi G^1)$, where $\\pi$ is a permutation operator."}, {"title": "3.5 Goal-Guided Framework For Conditional Molecule Generation", "content": "We propose a goal-guided framework for discrete flow matching, employing reinforcement learning\n(RL) to guide graph flow matching models for non-differentiable objectives. The goal of the guidance\nmethod is to map the noise distribution $p_0$ to a preference data distribution $p_1$ using a reward function\n$R(G_t, t)$.\nWe formulate the inference process of flow matching as a Markov Decision Process (MDP), where\n$(G_t, t)$ and $G_{t+\\triangle t}$ are the state space $s_t$ and action space $a_t$, $p_0$ is an initial noise distribution,\n$p(G_{t+\\triangle t}|G_t, t)$ is the transition dynamics and policy network $\\pi(a_t|s_t)$, $R(G_t, t) = r(G_1) \\mathbb{I}[t = 1]$\nis the reward function\nTo enable exploration, we introduce a temperature parameter T for the policy network during\nsampling, allowing the model to explore a broader space at higher temperatures:\n$\\pi(a_t|s_t) = \\pi(G_{t+\\triangle t}|G_t, t) = \\text{Cat} ((\\delta\\{\\cdot, G_t\\} + u_t(\\cdot, G_t\\vert G^0, \\hat{G}^1)\\triangle t)/T)\\qquad(9)$\nThe goal of RL training is to maximize the reward function. To prevent overfitting to the reward\npreference distribution, we add a Kullback-Leibler (KL) divergence term between the Reinforcement\nlearning fine-tuned model $p^{RL}(\\cdot)$ and pre-trained model $p_0(\\cdot)$ [Ouyang et al., 2022].\nWe employ the policy gradient method to update the network, where the policy is refined to $\\pi(a_t|s_t) =$\n$p^{T}(G^1|G_t) q(G_{t+\\triangle t}|G_1)$ to $\\pi(a_t|s_t) = p^{T}(G^1|G_t)$ [Sutton et al., 1999, Liu et al., 2024b], directly\nincreasing the probability of generating $G^1$ with higher rewards at all timestep t. The training objective\nis:\n$\\mathcal{L}_{RL} = -E_{p_{\\theta}(G_{0:t:1})} [\\alpha R(G_1) \\sum_{t=0}logp^{RL} (G^1|G_t) - \\beta\\sum_{t=0}KL(p^{RL} (G^1|G_t)||p_0 (G^1|G_t))] \\qquad(10)$\nwhere $p_{\\theta} (G_{0:t:1})$ represents $p_{data}(G^1) U(t; 0, 1) \\pi(G^0, G^1) p_t(G_t|G^0, G^1)$. Using this optimization\nobjective, we fine-tune the pre-trained flow matching model to generate data following the preference"}, {"title": "4 Experiment", "content": "To validate the performance of our method, we compare GGFlow with state-of-the-art graph gener-\native baselines on molecule generation and generic graph generation, over several benchmarks in\nSection 4.1 and Section 4.2, respectively. The ability of GGFlow to perform conditional molecule\ngeneration is analyzed in Section 4.3."}, {"title": "4.1 Moleuclar Graph Generation", "content": "We evaluated GGFlow on two standard molecular datasets, QM9 [Ramakrishnan et al., 2014]\nand ZINC250k [Irwin et al., 2012], using several metrics: Validity, Validity without correction,\nNeighborhood Subgraph Pairwise Distance Kernel (NSPDK) Maximum Mean Discrepancy (MMD),\nand Frechet ChemNet Distance (FCD). To calculate these metrics, we sampled 10,000 molecules.\nWe compared GGFlow against various molecule generation models, including GraphAF, GraphDF,\nMolFlow [Zang and Wang, 2020], EDP-GNN, GraphEBM [Liu et al., 2021], GDSS, PS-VAE [Kong\net al., 2022], MolHF [Zhu et al., 2023], GruM, SwinGNN, DiGress, and GSDM. Detailed descriptions\nof the datasets, baselines and metrics are provided in Appendix E.\nThe results, presented in Table 1, indicate that GGFlow effectively captures the distribution of molec-\nular data, showing significant improvements over the baselines. The high Validity without correction\nsuggests that GGFlow successfully learns chemical valency rules. Additionally, GGFlow achieves su-\nperior NSPDK and FCD scores on both datasets, demonstrating its ability to generate molecules with\ndistributions closely resembling those of natural molecules. Visualizations of molecules generated by\ndifferent models are shown in Figure 2, with additional results on GGFlow provided in Appendix F."}, {"title": "4.2 Generic Graph Generation", "content": "We further evaluated GGFlow on three generic graph generation benchmarks of varying sizes: Ego-\nsmall, Community-small, Grid and Planar. We employ the same train/test split as GraphRNN [You\net al., 2018], utilizing 80% of each dataset for training and the remaining for testing. We compared\nGGFlow's performance against well-known autoregressive models: DeepGMG [Li et al., 2018],\nGraphRNN [You et al., 2018], GraphAF [Shi et al., 2019], and GraphDF [Luo et al., 2021] and"}, {"title": "4.3 Conditional Molecule Generation", "content": "To further evaluate the performance of our model, we conducted conditional generation experiments\non the QM9 dataset, focusing on generating molecules with molecular properties $\\mu$ that closely match\na target value $\\mu^*$. In the experiment, we set the target value as 1, i.e. $\\mu^* = 1$.\nFor the experiment, we employed a reinforcement learning-based guidance method and compared it\nto the guided version of DiGress, which also proposes an effective approach for discrete diffusion\nmodels in conditional generation tasks. The reward function was defined as $|\\mu - \\mu^*|$, and the model\nwas trained over 10,000 steps using the training settings detailed in Section 4.1. To evaluate the\neffectiveness of our guidance method, we compared it against three baselines: (1) Guidance for\nDiGress [Vignac et al., 2022]. (2) Direct supervised training (ST) (3) Supervised fine-tuning (SFT).\nAdditionally, we calculated the mean and variance of $|\\mu - \\mu^*|$ for samples generated unconditionally\nby both DiGress and GGFlow to provide a baseline comparison. Further details of the experiment are\nprovided in Appendix E.5."}, {"title": "5 Conclusion", "content": "The results, detailed in Table 3, demonstrate the superiority of our reinforcement learning-based\nconditional generation method over both ST and SFT approaches. Notably, our method surpasses the\nguidance techniques used in diffusion models, showcasing its enhanced ability to steer the generative\nprocess toward desired outcomes. Additionally, our approach achieves higher validity in conditional\ngenerated tasks, highlighting its robustness and superior performance in goal-directed generation.\nIn this paper, we introduced GGFlow, a discrete flow matching generative model for molecular graphs\nthat incorporates optimal transport and an innovative graph transformer network. GGFlow achieves\nstate-of-the-art performance in unconditional molecular graph generation tasks. Additionally, we\npresented a novel guidance method using reinforcement learning to control the generative trajectory\ntoward a preferred distribution. Furthermore, our model demonstrates the ability to achieve the best\nperformance across various tasks with fewer inference steps compared to other baselines which\nhighlights the practical impact of our guidance method. A primary limitation is scalability to\nlarger graphs like protein ($|V| > 500$), attributable to the increased time complexity from triangle\nattention updates and spectral feature computations. Future work will focus on enhancing our model's\nscalability in larger graphs."}]}