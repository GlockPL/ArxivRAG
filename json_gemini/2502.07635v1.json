{"title": "Distributed Value Decomposition Networks with Networked Agents", "authors": ["Guilherme S. Varela", "Alberto Sardinha", "Francisco S. Melo"], "abstract": "We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL tasks in three standard environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Cooperative multi-agent reinforcement learning addresses the problem of designing utility-maximizing agents that learn by interacting with a shared environment. Representing utility functions and applying them for decision-making is challenging because of the large combined joint observation and joint action spaces. Value decomposition network (VDN) [30] avoid this combinatorial trap by considering the family of Q-functions that factorize agent-wise. The method offers a viable solution to the scalability of MARL systems under the premise of centralized training with decentralized execution, where the outputs of the individual Q-functions are added to form a joint Q-function.\nHowever, in many real-world domains, the premise of centralized training is too restrictive. For instance, in reinforcement learning based distributed\u00b9 load balancing [40] intelligent switches act as agents to distribute multiple types of requests to a fleet of servers in a data center. Agents assign the incoming load to the servers, resolving requests at low latencies under quality-of-service constraints. In this domain, there is no simulator-agents must learn online by observing queues at links and selecting the links to route the requests. In robotic teams, there might be only simulators with a considerable gap between the simulated environment and the real-world. In real-world situations where communication is restricted and actions can fail in unpredictable ways, also benefit from this approach. In RoboCup [1] tournament, soccer robots actuate as agents and are endowed with sensors and computation onboard. Although communication has delays and link failures, agents should cooperate as a team to score goals.\nThe straightforward alternative to centralized training is the fully decentralized training approach, employing independent Q-learners (IQL) [31]. As IQL agents are oblivious to the presence of their teammates, they cannot account for the joint action, and from the perspective of any single agent the perceived transition probabilities of the environment are non-stationary. This approach violates the reinforcement learning assumption that the transition probabilities are fixed and unknown. Since individual learners do not communicate, fully decentralized IQL precludes parameter sharing, where agents update the same policy parameters. For many MARL tasks, parameter sharing improves sample efficiency but requires that every agent updates the same weights. Hence in a decentralized training setting, every agent would need one-to-one communication to broadcast its weights and experiences to all other agents, before performing the updates locally.\nWe propose a novel algorithm that combines the decentralized training with value decomposition networks' Q-function decomposition. Starting from the loss function used in VDN, in centralized training setting, we show that information back-propagated to agents' neutral network is the joint temporal difference (JTD). Our algorithm operates in the decentralized training and decentralized execution (DTDE) setting where agents communicate with their closest neighbors to improve their local JTD estimations. And each agent locally minimizes JTD. When agents are homogeneous, i.e., having the same individual observation and action space, we incentivize them to align their Q-function's parameters and their gradients; This mechanism called gradient tracking [26] enables agents to minimize a common loss function. To the best of our knowledge, this is the first application of gradient tracking for policy iteration in reinforcement learning."}, {"title": "2 BACKGROUND", "content": "We introduce three concepts from the distributed optimization literature [22] that form the key components of our method: (i) The switching topology communication channel over which agents perform communication with closest neighbors. (ii) The consensus updates over the switching topology communication channel, where agents agree on the value of a constant; (iii) and finally, gradient tracking that allows agents to optimize a global function formed by the sum of local functions, using local computation and communication. Additionally, we define decentralized partially observable Markov decision process the mathematical framework underpinning value decomposition network.\nSwitching topology communication channel: Given a set N = {1, ..., N} of agents connected in a communication network such that agents i and j can exchange messages if and only if i, j \u2208 & where & \u2264 N \u00d7 N denotes the edge set. Under the switching topology regime at every round of communication k, there is a different set of edges &(k), such that agents are uncertain in advance who their peers are going to be. This randomness is captured by a underlying time-varying undirected graph G(k) = G(k) (N,&(k)) where the node set N remains fixed but the edge set &(k) is allowed to vary at each timestep k.\nConsensus over undirected time-varying networks [37]: is the process by which nodes initially holding scalar values asymptotically agree on their network average by interacting with closest neighbors over a switching network topology. At each iteration, each node replaces its own value with the weighted average of its previous value and the values of its neighbors. More formally, let the variable $x_i^{(0)}$ be held by agent i. Then, agents agree on a value x by performing the updates:\n$x_{i}^{(k+1)} = \\sum_{j \\in N_{i}^{(k)}} \\alpha_{i,j}^{(k)} x_{j}^{(k)},$ (1)\nwhere $N_{i}^{(k)} = \\{i \\cup j | (i, j) \\in &^{(k)}$ is the neighborhood of agent i at communication step k, extended to include $x_i$ measurement as well. Under the time-varying regime we allow the neighborhood to switch at every consensus update. A foundational result [37] establishes the existence of the weights $\\{\\alpha_{i,j}^{(k)}\\}_{i,j \\in N}$ such that by repeating the updates in Eqn. (1) agents produce localized approximations for the network average, i.e.,\n$\\lim_{k\\rightarrow \\infty} x_i = N^{-1} \\sum_{i \\in N} x_i^{(0)}$ (2)\n(N number of agents). Refer to Appendix A for instructions on building such weights locally.\nGradient tracking: Enables agents to compute the solution x of a distributed optimization problem. The distributed optimization problem arises on large scale machine learning/statistical learning problems [6]. Let every agent in N to hold a cost function $f_i(x)$: $R^M \\rightarrow R$ the objective of distributed optimization is to find x that minimizes the average of all the functions\n$\\min_{x\\in R^M} f(x) = \\frac{1}{N} \\sum_{i=1}^N f_i(x)$ (3)\nusing local communication and local computation. The algorithm for finding the minimizer x starts from an arbitrary solution $x_i^{(0)}$ and the local variable $z_i^{(k)}$, which tracks the gradient of the function $f_i(x)$, is initialized by the local gradient at point $x_i^{(0)}$, i.e., $z_i^{(0)} = \\nabla f_i(x_i^{(0)})$. The algorithm proceeds using the update [26]:\n$x_{i}^{(k+1)} = \\sum_{j=1}^N a_{i,j} x_{j}^{(k)} - \\eta z_{i}^{(k)},$\n$z_{i}^{(k+1)} = \\sum_{j=1}^N a_{i,j} z_{j}^{(k)} + \\nabla f_i(x_i^{(k+1)}) - \\nabla f_i(x_i^{(k)}),$ (4)\nwhere $[a_{i,j}]_{N \\times N}$ are the consensus weights for a fixed strongly connected graph, and $\\eta > 0$ is a fixed step size.\nDecentralized partially observable Markov decision process (Dec-POMDP) is the framework describing the system dynamics where fully cooperative agents interact under partially observability settings-defined by the sextuple [8]:\n$M = (N, S, \\{A_i\\}_{i\\in N}, \\{O_i\\}_{i\\in N}, P, R, \\gamma),$ where N = {1,..., N} denotes the set of interacting agents, S is the set of global but unobserved system states, and $A_i$ is the set of individual action spaces. The observation space O denotes the collection of individual observations spaces $O_i$. Typically an observation $o \\in O_i$ is a function of the state $s^i$. The state action transition probability function is denoted by P, the team reward function shared by the agents $R(s_t, a_t, s_{t+1})$, and the discount factor is $\\gamma$. Agents observe $o \\in O_i$, choose an action from their individual action space $a_i \\in A_i$ and collect a common reward $R(s_t, a_t, s_{t+1})$. The system transitions to the next state following the state-action transition probability function. Neither the state $s_t$ or the joint action $a_t = [a_1, ..., a_N]$ is known to the agents.\nThe joint policy $\\pi: O \\rightarrow \\Delta(A)$ maps the joint observation $o_t$ to a distribution $ \\Delta$ over the joint action $A = A_1 \\times \\cdots \\times A_N$. The agents' objective is to find a joint policy $ \\pi$ that maximize the expected discounted return $J(\\pi)$ over a finite horizon T given by:\n$J(\\pi) = E_{\\pi \\sim \\mu(\\cdot)} [\\sum_{t=0}^{T} \\gamma^t R(s_t, a_t, s_{t+1})]$ (5)\nWhere 0 < $\\gamma$ < 1 is a discount factor that balances the preference between collecting immediate high rewards while avoiding future low rewarding states, and $ \\mu(s)$ is the initial state distribution. The joint reward $R(s_t, a_t, s_{t+1})$ is the sum of team rewards $\\sum \\hat{R}(s_t, a_t, s_{t+1})$. Thus the expected discounted return captures the expected sum of exponentially weighted joint rewards, by drawing an initial state $s^0$ from $ \\mu$, observing $o^0$ and following the actions prescribed by the joint policy $ \\pi$ thereafter until T."}, {"title": "3 DISTRIBUTED VALUE DECOMPOSITION", "content": "Consider a setting where fully cooperative learners interact under a partially observable setting. Motivated by Fact 1 which establishes that value decomposition networks minimize the mean squared joint temporal difference JTD. We propose distributed value decomposition networks. DVDN use peer-to-peer message exchange to combine their local temporal difference (TD) for approximating JTD.\nThus, agents emulate VDN weight updates using the JTD surrogate. Additionally, homogeneous agents share knowledge by pushing both weights and gradients to the communication channel, using gradient tracking. We formalize the setting whereby agents face uncertainty with respect to the communication channel with hybrid partially observable Markov decision process introduced by Santos et al. [28].\n3.1 Preliminaries\nValue decomposition network [30]: is a value-based method for multi-agent reinforcement learning. In value-based methods, the expected discounted return in (5) is captured by a joint Q-function that maps the joint observations o and joint actions a to a real value number (Q-value). Particularly, individual Q-functions (local observations and actions mappings to a Q-value) are estimated using parameterized non-linear function approximation, implementing the well known deep-Q networ [20] architecture in single agent RL. Then, in centralized training, agents combine their individual Q-functions into a joint Q-function:\n$Q^{VDN}(o, a; \\omega) = \\sum_{i=1}^N Q_i(o_i, a_i; \\omega_i)$ (6)\nWhere $ \\omega$ is the concatenation of the individual network parameters $ \\omega_i$, o and a represent the concatenation over the agents' observations and actions respectively. Finally, $Q^{VDN}$ is said to have additive factorization because it can be obtained directly by summing over agents' Q-function. The loss function used with additive factorization is:\n$l(\\omega; \\tau) = \\frac{1}{N} \\sum_{\\tau} [Y^{VDN} - Q^{VDN}(o, a; \\omega)]^2,$ (7)\nwhere the joint trajectory $\\tau = [\\tau_1, \\tau_2, ..., \\tau_N]$ is the concatenation of the trajectories drawn individually by interacting with the environment. The individual trajectory $\\tau_i$ of episode with timesteps t = 0, ..., T, is given by:\n$\\tau_i = (o_i^0, a_i^0, R_i^1, o_i^1, a_i^1, R_i^2, ..., R_i^{T}, o_i^{T}).$\nThe parameter set $ \\omega = [\\omega_1, \\omega_2,\u00b7\u00b7\u00b7, \\omega_N]$ also factorizes across agents, such that each of the agent-wise policies $ \\pi_i^\\tau(o_i; \\omega_i)$ is determined only by its $ \\omega_i$. Like deep Q-networks, VDN uses a target network which stabilizes training by providing the a joint target $Y^{VDN}$:\n$Y^{VDN} = \\sum_{i=1}^N R + \\gamma \\max_{u_i} Q_i(o_i^\\prime, u_i; \\omega_i^-),$ (8)\nwhere $o_i^\\prime$ is the subsequent observation to $o_i$ and $ \\omega_i^-$ is a periodic copy from $ \\omega_i$. Similar to the joint Q-function in (6) the joint target also factorizes into individual targets\n$y_i = R + \\gamma \\max_{u_i} Q_i(o_i, u_i; \\omega_i).$ (9)\nHybrid partially observable Markov decision process (H-POMDP) is the framework where agents cooperate under partial observability and are uncertain about the communication graph topology. During training, agents are unaware of whom their communicating peers are going to be at any given training episode. An"}, {"title": "3.2 Joint Temporal Difference", "content": "extension from Dec-POMDPs, H-POMDP is defined by the sextuple [28]:\n$H = (G, S, \\{A_i\\}_{i\\in N}, \\{O_i\\}_{i\\in N}, P, R, \\gamma),$ where G denotes an undirected episode-varying communication graph where set of interacting agents remains fixed N but the communication edge set & is allowed to change between episodes. The other tuple elements adhere to the standard Dec-POMDP definition: S is the set of global but unobserved system states, and $A_i$ is the set of individual action spaces. The observation space O denotes the collection of individual observations spaces $O_i$. Typically an observation $o \\in O_i$ is a function of the state $s^i$. The state action transition probability function is denoted by P, the team reward function shared by the agents $R(s_t, a_t, s_{t+1})$, and the discount factor is $\\gamma$.\nThe unknown communication graph G, is sampled from a set C according to an unknown probability distribution $ \\beta$. The agents performance is measured as $J_\\beta(\\pi) = E_{G \\sim \\beta} [J(\\pi;G)]$, where $J(\\pi; G)$ denotes the expected discounted return of policy $ \\pi$ under an H-POMDP with communication graph G.\nAdditive factorization is the distinguishing VDN characteristic; it is accomplished through a centralized value decomposition layer. The value decomposition layer sums the outputs from agents' deep Q-networks. And to emulate its behavior in the decentralized setting we must determine: What information flows to agents' networks during centralized training? To answer this research question, we must first establish the role that value decomposition layer plays in shaping the weights of the Q-functions.\nTemporal difference (TD) $ \\delta_i$ is the increment by which agent i adjusts its Q-function in the direction of the maximal Q-value, i.e.,\n$ \\delta_i = R + \\gamma \\max_{u_i} Q_i(o_i^\\prime, u_i; \\omega_i^-) - Q_i(o_i, a_i; \\omega_i),$ (10)\nwhere the observation pair $(o_i, o_i^\\prime)$ are surrogates for the current and next states respectively, in timestep t, the environment reaches hidden state $s_t$ and emits $o_i$ to agent i.\nFACT 1. Value decomposition network minimize the joint temporal difference $ \\delta = \\sum_{j \\in N} \\delta_j$, where\n$ \\delta_j = R + \\gamma \\max_{Q_j} Q_i(o_j^\\prime, u_j; \\omega_j^-) - Q_j(o_j, a_j; \\omega_j).$ (11)\nand $ \\delta_j \\in R^{T+1}$ is agent j's temporal difference, for the joint trajectory $ \\tau$ with length T + 1.\nFact 1 establishes that the information flowing to weights through the back-propagation algorithm is the sum across agents of their temporal difference. Hence, in VDN, every agent replaces its own TD in (10) with JTD (11), for local updates. By construction, JTD decomposes between agents, and we use the fact that the joint trajectory in (10) is a concatenation of the individual trajectories $ \\tau_j$, to perform localized approximations to (11)."}, {"title": "3.3 Decentralized JTD", "content": "Can the effect of value decomposition layer be reproduced in the decentralized setting? In the case where the communication graph"}, {"title": "3.4 DVDN with Gradient Tracking", "content": "is fully connected the system is back to the centralized setting and we can use VDN normally. Conversely, when communication is not possible the system is fully decentralized. Independent Q-learners perform weight updates in isolation by minimizing the mean square temporal difference:\n$l(\\omega_i; \\tau_i) := \\sum_{\\tau_i} \\delta_i^2$ (12)\nAlternatively, for situations where the communication graph is strongly connected, it is possible to generate a localized approximation for additive factorized joint Q-function from individual Q-functions. Since there is no agent capable of overseeing the system, we propose to perform the consensus updates in (1), to propagate the temporal difference:\n$ \\delta_i^{(k+1)} = \\sum_{j \\in N_i^{(k)}} \\alpha_{i,j}^{(k)} \\delta_j^{(k)}.$ (13)\nAs a result from temporal difference consensus in (13) agents receive an updated estimation for the team temporal difference. The team temporal difference estimator at communication step k at agent i is denoted by $ \\delta_i^{(k+1)}$. The limit in (2) guarantees that the updates asymptotically converge to $^1/_N \\delta$. However, with a finite number of updates it is not possible to guarantee that the agents will reach consensus. Instead, we resort to truncation. In this work we perform a single consensus step per mini-batch update.\nIn a practical applications, it is useful to separate temporal difference (TD) from the additional information that comes from communication. The network estimated JTD at agent i $ \\delta^{-i}_i$ captures the contributions that come from network and is defined as:\n$ \\delta^{-i}_i = N \\delta_i^{(k+1)} - \\delta_i^{(k)}.$\nThe network estimated JTD at agent i can be used to perform weight updates in the direction that minimizes the mean square estimated JTD at agent i:\n$l(\\omega_i; \\tau_i, \\delta^{-i}) := \\sum_{\\tau_i} (\\delta_i + \\delta^{-i})^2$ (14)\nSince $ \\delta_i$ is the value obtained from the network of agents, it comes after the semicolon in the LHS of (14), in the place reserved for data. Whereas the variable in the minimization is the Q-network parameters $ \\omega_i$. Similarly, $ \\delta_i$ is a variable which depends on the (trajectory, parameters) $( \\tau_i, \\omega_i)$. Figure 3 (Appendix C) compares the error increments that shape the weight updates in VDN and DVDN in (13) and (14).\nDifferently from the distributed algorithm max-push [15], where the communication is serial and it happens on an established order, induced by an spanning tree that is common knowledge, the updates in (13) happen in parallel. Moreover, max-push requires the design of joint action utility functions, to capture the value of the joint action. Agents have a time budget to agree on what the best global action is by solving their local problems. There are no guarantees that max-push converges to a global joint action in graphs with cycles, or within their allocated time budget. The updates in DVDN have asymptotic convergence guarantees and do not require explicit joint action modeling, or even knowledge of neighbors' actions.\nWhen the agents are homogeneous, i.e., they have the same observation and action space, it is possible to use gradient tracking to make them agree on a common solution for the parameters. This common solution combines weight updates from many agents improving sample-efficiency. We assume that there is a global loss function that factorizes additively agent-wise and the solution is a common w. The global loss function can be expressed by replacing the loss function (14) in the agent wise cost functions in (3):\n$\\min_\\omega l(\\omega; \\tau) = \\sum_{i=1}^N l_i(\\omega)$ (15)\nSince the minimizer w for the global in (15) is unknown to the agents, an alternative formulation introduces N copies $ \\omega_i$ and followed by N equality constraints:\n$\\min_{\\omega_i \\in \\Omega, i \\in N} l(\\omega; \\tau) = \\frac{1}{N} \\sum_{i=1}^N l_i(\\omega_i)$\ns.t. $ \\omega_i = \\omega_j \\forall (i, j) \\in &$ (16)\nfor the parameter space $ \\Omega$. We rewrite the terms in (16) making explicit the dependency of the local loss functions in LHS with the local experiences in (14):\n$\\min_{\\omega_i \\in \\Omega, i \\in N} l(\\omega; \\tau) = \\frac{1}{N} \\sum_{\\tau_i} l_i(\\omega_i; \\tau_i)$ \ns.t. $ \\omega_i = \\omega_j \\forall (i, j) \\in &$ (17)\nSimilar to previous work [22], we propose a gradient-based strategy for solving (17). Differently from previous work in which the objective function is strongly convex, here the objective is non-convex. As result there is no closed form solution to the optimization problem. Moreover, we cannot guarantee that the equality constraints are satisfied since we truncate the number of consensus steps; Rather, we interleave gradient descent steps for minimization with consensus steps to incentivize an alignment in the solution. A practical reinforcement learning algorithm is initialized as follows: At the first mini-batch iteration k = 1, agents perform a forward pass on the local networks and compute $ \\delta_i^{(1)}$, then they perform the consensus iteration in (13) to obtain the estimated network JTD at agent i,\n$\\delta_i^{-i} = N (\\sum_{j \\in N_i^{(1)}} \\alpha_{i,j}^{(1)} \\delta_j^{(1)}) - \\delta_i^{(1)}$.\nThen, agents compute the gradient of the mean square estimated JTD (14)\n$g_i^{(1)} = \\nabla_{\\omega_i} l_i(\\omega_i^0; \\tau_i^{(1)}, \\delta_i^{-i})$.\nWe introduce an auxiliary variable $z_i^{(1)}$ that tracks the team gradient\n$\\nabla_\\omega l(\\omega; \\tau) = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\omega_i} l_i(\\omega_i; \\tau_i)$,\nand is initialized at the local gradient loss, i.e.,\n$z_i^{(1)} = g_i^{(1)}.$"}, {"title": "4 EXPERIMENTS", "content": "The update at step k = 1, combines a consensus step (1) and gradient descent with weight updates $z_i^{(1)}$:\n$\\omega_i^{(1)} = \\sum_{j \\in N_i^{(2)}} \\alpha_{i,j} \\omega_i^0 - \\eta z_i^{(1)}.$\nAt the second mini-batch iteration k = 2, agents compute the estimated network JTD at agent i,\n$ \\delta_i^{-i} = N (\\sum_{j \\in N_i^{(2)}} \\alpha_{i,j}^{(2)} \\delta_j^{(2)}) - \\delta_i^{(2)}$.\nThen, agents compute the local gradients at iteration k = 2:\n$g_i^{(2)} = \\nabla_{\\omega_i} l(\\omega_i^1; \\tau_i^{(2)}, \\delta_i^{-i}).$\nOnce difference between local gradients is available, agents update the team gradient variable:\n$z_i^{(2)} = (\\sum_{j \\in N_i^{(2)}} \\alpha_{i,j}^{(2)} z_j^{(1)}) + g_i^{(2)} - g_i^{(1)}.$ Finally, agents aggregate the weights and perform a gradient step at iteration k = 2 using the team gradient instead of the local gradients:\n$\\omega_i^{(2)} = \\sum_{j \\in N_i^{(2)}} \\alpha_{i,j} \\omega_i^1 - \\eta z_i^{(2)}$ ending the iteration k = 2 updates. More generally, for an arbitrary k:\n$ \\delta_i^{-i} g_i^{(k)} z_i^{(k)} \\omega_i^{(k)} = \\nabla_{\\omega_i} l(\\omega_i^{(k-1)}; \\tau_i^{(k)}, = \\frac{N \\sum_{j \\in N_i^{(k)}} \\alpha_{i,j}^{(k)} \\delta_j^{(k)} ) \\= (\\sum_{j \\in N_i^{(k)}} \\alpha_{i,j}^{(k)} z_j^{(k-1)} ) + g_i^{(k)} - g_i^{(k-1)} = \\sum_{j \\in N_i^{(k)}} \\alpha_{i,j}^{(k)} \\omega_i^{(k-1)} - \\eta z_i^{(k)}$ (18a)\n(18b)\n(18c)\n(18d)\nThe intuition behind the algorithm (18) is to update the auxiliary variables, team gradient variables, zi in the direction that minimizes LHS of (17). Over many iterations of k, the team gradient variables $z_i^{(k)}$ converge asymptotically to the average of individual gradients. As established in the previous section, we only perform a single update per mini-batch-which is sufficient to emulate the effect of parameter sharing in the distributed setting, and provides better results than the fully decentralized independent learner. The practical implementation of this algorithm incorporates adaptive momentum (Adam) [14] updates to improve its performance, making it compatible with standard VDN implementation.\nWe evaluate the performance of both DVDN algorithms in ten scenarios with partial observability, and compare them to two baselines: The independent Q-learning in the fully decentralized paradigm, and value decomposition networks in the CTDE paradigm.\nIQL is the lower bound baseline and VDN is the upper bound baseline. Specifically, for each scenario, in the heterogeneous agents setting, we compare VDN to DVDN (Algorithm 1, Appendix D.1), and in the homogeneous agents setting, we compare VDN (PS) to DVDN (GT) (Algorithm 2, Appendix D.2)."}, {"title": "4.1 Scenarios", "content": "We consider three environments: The first is level-based foraging [LBF, 24]\u00b2, where agents collect fruits in a grid-world and are rewarded if their level is greater than or equal to the fruits they are loading. Agents perceive the X-Y positions and levels of both food items and peers within a two block radius. Rewards are sparse and positive, they depend on the level of the fruit item being loaded and the level of each contributing agent. The second environment is the multi-particle environment [MPE, 18] where agents navigate in a continuous grid, with their trajectories dependent on past movement actions. We modify the original environment for partial observability\u00b3. The third environment [MARBLER, 32] is a robotic navigation simulator that generates physical robot dynamics. In this environment, all agents are assumed to have full communication and their observations are appended together. However, agents remain aware only of their own actions. The scenarios configurations are:\n\u2022 LBF/Easy: Three agents interact in a 10x10 grid-world to collect 3 food items.\n\u2022 LBF/Medium: Four agents interact in a 15x15 grid-world to collect 5 food items.\n\u2022 LBF/Hard: Three agents interact in a 15x15 grid-world to collect 5 food items.\n\u2022 MPE/Adversary: Two teammates must guard the target landmark against the approach of a pretrained adversary agent. They perceive their own position, their relative distance to the goal, the positions of the landmarks, and the position and color of the closest agent. The color enables a teammate to distinguish whether or not the other agent is the adversary. The team is rewarded in proportion to the negative distance from the target to its closest teammate and penalized by the adversary's distance to the target.\n\u2022 MPE/Spread: Three agents must navigate as close as possible to one of three landmarks. They perceive their own position, velocity, the position and velocity of the closest agent and the position of the closest landmark. The team is rewarded the negative distance from the agent closest to a landmark and receive a penalty for bumping into one another.\n\u2022 MPE/Tag: Three large and slow moving predator agents must bump into a smaller and faster pretrained prey agent. Predator agents perceive their own position and velocity, the position and velocity of the closest predator, and the position and velocity of the prey. Predators collect rewards by touching the prey.\n\u2022 MARBLER/Arctic Transport (Arctic): Two drones attempt to guide the ice robot and water robot to the goal location as quickly as possible over ground tiles (white), ice tiles (light"}, {"title": "4.2 Baselines", "content": "blue), and water tiles (dark blue). Drones move fast, while ground robots move fast on their specialized tiles. Teammates are penalized at each time step in proportion to the number of the ground robots not in the destination, plus the distance of the ground robots from their goal.\n\u2022 MARBLER/Material Transport (Material): A heterogeneous team of four agents, two fast agents with low loading capacity, and two slow agents with high loading capacity, must collaborate to unload two zones. The teammates collect positive rewards for loading and unloading the zones, while collecting a small penalty per timestep.\n\u2022 MARBLER/Predator Captures Prey (PCP): A heterogeneous team of two sensing agents and two capture agents must collaborate to capture six preys. A prey can be perceived only by one of the sensing agents, and can be captured only by one of the capture agents. Teammates collect positive rewards for sensing and capturing the prey, and a small penalty per timestep.\n\u2022 MARBLER/Warehouse: A team of six robots, three green and three red, must navigate to the zone of their color on the right side of the environment to receive a load. Then they must navigate to their color zone on the left side to unload. Since optimal paths intersect robots must coordinate to avoid collisions. Teammates collect positive rewards for successfully loading and unloading the zones.\nWe apply the algorithmic implementations of IQL and VDN in [24]\u2074 for the fully cooperative setting with joint rewards:\n\u2022 VDN [30]: Is the CTDE paradigm baseline. For the homogeneous agents setting, we perform parameter sharing.\n\u2022 IQL [20]: Implement individual deep-Q network that minimize a local loss function (12). Since IQL belongs to the fully decentralized approach, we only perform experiments without parameter sharing for this algorithm."}, {"title": "4.3 Metrics", "content": "Performance plots: Our evaluation methodology follows that of Papoudakis et al. [24]. We conduct ten independent, randomly seeded, runs for each algorithm-scenario combination for five million training steps. Evaluations are performed at every 125,000 timesteps, for a total of forty one evaluation checkpoints over each training run. Each evaluation checkpoint consists of hundred independent episodes per training seed. The average episodic return is the average of the ten evaluation checkpoints (i.e., average over seeds). The maximum average episodic return captures the average episodic returns with maximum value (i.e., maximum over forty one average episodic returns). We report the maximum average episodic return and the 95% bootstrapped confidence interval (CI).\nAblation Plots: To measure the contributions of different factors to performance gains, we conduct an ablation study. For each factor, we identify the evaluation checkpoint with the maximum average episodic return (over five independent seeds) and extend the selection to include a two-evaluation checkpoint neighborhood."}, {"title": "4.4 Setup", "content": "This extended selection increases the group's sample size from a total of five data points to twenty-five, thereby decreasing standard deviation. We randomly resample this sample (size 20,000) to build a 95% bootstrap confidence interval (CI). We report the resampled mean and bootstrap CI bars.\nRanking criteria: To compare algorithms' performances, we refrain from using statistical language since their performance is based on the maximum evaluation checkpoints for a small number of independent runs, and may not necessarily follow any parametric distribution. Instead, to discriminate the algorithms' performances, we use a bootstrap CI test. We say that algorithm A matches the performance of algorithm B when we cannot reject the null hypothesis that their distributions have the same mean. When the bootstrap CI test rejects the null hypothesis that both algorithms have the same mean: We say that algorithm A outperforms algorithm B if its average is greater. Otherwise, we say that algorithm A under performs algorithm B.\nFor the baselines, we use the default hyper parameters from [24], but extend the number of training steps from two million to five million. For both DVDN algorithms, we perform hyper parameter search using the protocol in [24]. To generate the switching topology network, we make a random sample from all possible strongly connected graphs \u2076 for the task-specific number of agents. This sampling scheme ensures that there is always a path between two agents. The hyper parameters used in the experiments are in the Appendix D.3. Open-source code is available at the DVDN Github repository.\u2077."}, {"title": "5 RESULTS", "content": "Figure 1 illustrates the performance curves for one representative scenario per environment. The performance curve profiles are similar between VDN and DVDN. Table 1 summarizes the results for IQL, DVDN and VDN algorithms, separating them into settings for heterogeneous agents and homogeneous agents. According to our ranking criteria, highlighted results indicate the best for the algorithm-scenario combination, asterisks mark performances that comparable to the best performing, and double asterisks indicate second-best performance. We argue that DVDN is an alternative to VDN, not its replacement. Consequently, it is sufficient to either outperform the lowest-ranked algorithm"}]}