{"title": "Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection", "authors": ["ChaoFeng Guan", "YaoHui Zhu", "Yu Bai", "LingYun Wang"], "abstract": "Multi-label few-shot aspect category detection aims at identifying multiple aspect categories from sentences with a limited number of training instances. The representation of sentences and categories is a key issue in this task. Most of current methods extract keywords for the sentence representations and the category representations. Sentences often contain many category-independent words, which leads to suboptimal performance of keyword-based methods. Instead of directly extracting keywords, we propose a label-guided prompt method to represent sentences and categories. To be specific, we design label-specific prompts to represent sentences by combining crucial contextual and semantic information. Further, the label is introduced into a prompt to obtain category descriptions by utilizing a large language model. This kind of category descriptions contain the characteristics of the aspect categories, guiding the construction of discriminative category prototypes. Experimental results on two public datasets show that our method outperforms current state-of-the-art methods with a 3.86%~4.75% improvement in the Macro-F1 score.", "sections": [{"title": "1 Introduction", "content": "Multi-label Aspect Category Detection aims at detecting multiple aspects of viewpoints mentioned in a sentence, which is an important subtask of aspect-based sentiment analysis [Pontiki et al., 2016]. It is difficult to obtain a sufficient number of annotated comment sentences, for example, more expensive items usually have only a small number of comments. As a result, some studies focus on mining multiple aspects of viewpoints with a few comments, named Multi-label Few-shot Aspect Category Detection (MFACD) [Hu et al., 2021], which has received increasing attention from researchers in the field of artificial intelligence.\nExisting MFACD methods usually use a meta-learning strategy on a large amount of base categories to acquire transferable knowledge, which contributes to learn effective representations of category prototypes from a small number of comment sentences. Then these category prototypes are used to detect aspect categories of test comment sentences with similarities between test sentences and category prototypes. In the MFACD task, single comment sentence usually contains several different aspect categories at the same time (see sentence in Table 1). Since the category prototype generation is based on the sentence representations, the multiple categories of a sentence result in inadequate differentiation among category prototypes.\nTo achieve representative category prototypes, [Hu et al., 2021] extracted category-relevant keywords for sentence representation using an attention mechanism. Nevertheless, there are a large number of category-irrelevant words in comment sentences [Zhao et al., 2022] such as \u201ca\u201d, \u201cthe\u201d, \u201cmy\u201d. These large numbers of category-irrelevant words make the method based on automatic keyword extraction ineffective, since the method possibly tends to pay attention to those irrelevant words. In order to effectively capture category-relevant keywords, [Zhao et al., 2023; Zhao et al., 2022; Wang and Iwaihara, 2023] utilize category labels to assist keyword extraction. However, there are similarities between category labels, resulting in the keywords filtered with the label are not clearly distinguishable, thus affecting the learning of discriminative category prototypes.\nAlthough the above approaches based on keyword representations demonstrate some effectiveness, it is still difficult to explicitly obtain keywords characterizing category prototypes from sentences. Compared to directly extracting keywords for sentence representations, we argue that prompt learning in the large language mode (LLM) is able to obtain semantic information of category from sentences [Jiang et al., 2022a]. Furthermore, introducing the label information in the prompt can better capture the viewpoint categories of sentences.\nBased on the above analysis, we propose a label-guided prompt (LGP) method for MFACD by enhancing sentence representations and their category prototypes. At the level of the sentence representation, the knowledge of the pre-training model is exploited via a prompt, and further the category label is introduced into the prompt to accurately express sentence category information. At the level of category prototype generation, we utilize another kind of prompt for a LLM to generate category descriptions, which contain the characteristics of the aspect categories, guiding the construction of discriminative category prototypes.\nThe contributions are summarized below:\n\u2022 We propose a label-guided prompt (LGP) method to enhance the representations of sentences for MFACD.\n\u2022 We leverage large language model to obtain category descriptions, which can effectively guide discriminative category prototype generation.\n\u2022 Experimental results on two public datasets show that the proposed method achieves state-of-the-art performance."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multi-label Few-shot Aspect Category Detection", "content": "Previous research of aspect category detection can be divided into two categories, unsupervised methods [Schouten et al., 2017; Su et al., 2021; Hai et al., 2011] and supervised methods [Jiang et al., 2019; Movahedi et al., 2019; Ghadery et al., 2019]. These methods detect aspect categories from predefined sets, heavily relying on large amounts of labeled data to train a discriminative classifier, which fails to generalize well to novel aspect categories with only a few labeled. To this end, some researches begin to explore few-shot learning, which allows for rapid adaptation to new categories by only utilizing a small number of samples. Few-shot learning aims to address the challenges of limited data and data sparsity, and this task has received extensive attention in many fields, such as text classification [Rios and Kavuluru, 2018; Chalkidis et al., 2019], image recognition [Zhu et al., 2020b; Zhu et al., 2020a], intent detection [Hou et al., 2021], and relationship classification [Gao et al., 2019].\nMeanwhile, the work Proto-AWATT [Hu et al., 2021] first attempted to address aspect category detection in multi-label few-shot scenarios. This approach based prototypical network utilizes the attention mechanism to alleviate the noise from irrelevant aspects. In order to filter irrelevant words more efficiently, [Zhao et al., 2022; Wang and Iwaihara, 2023] proposed a label-guided attention strategy. However, the presence of semantically similar category labels may lead to ineffective filtering of irrelevant words. In this paper, instead of directly extracting keywords for sentence representations, we propose a label-guided prompt (LGP) method to enhance sentence representations and their category prototypes for the MFACD task."}, {"title": "2.2 Prompt Learning", "content": "Prompt learning is the use of cue message to elicit knowledge from pre-trained language models. The pre-trained language models contain powerful abilities, and they can improve many tasks. For example, some works [Reimers and Gurevych, 2019; Gao et al., 2021] leveraged the BERT to obtain enhanced sentence representations. Despite the success of BERT in sentence embedding, the performance of the original BERT was not satisfactory [Li et al., 2020]. PPT [Gu et al., 2022] showed that designing effective prompt statements can guide the learning of the model, and this approach can perform well with a small number of samples. Meanwhile, the work PromptBERT [Jiang et al., 2022b] uses a novel contrastive learning method for learning better sentence representations by introducing prompt. Inspired by the PPT and the PromptBERT, we attempt to apply prompt learning for the MFACD task."}, {"title": "3 Methods", "content": "The proposed method aims to improve the sentence representation and the category prototype via the prompt. The pipeline of the proposed method involves two crucial components Prompt Enhanced Sentence Representation (PESR) module and Prompt Enhanced Prototype Generation (PEPG) module, as depicted in Figure 1. The PESR module involves learning sentence representations by using a prompt with the category label. Based on the above improved sentence representations, PEPG module enhances the category prototype with the assistance of the category descriptions. Before elaborating on the details of each component, we first introduce our problem definition."}, {"title": "3.1 Problem Formulation", "content": "In tackling the MFACD task [Snell et al., 2017], we also adopt the meta-learning strategy, which performs on multiple N-way K-shot tasks. Each task consists of a support set S and a query set Q, spanning N categories with K sentences per category in the support set. Namely, $S = \\{x_{ij} | i = 1, ..., N; j = 1, .., K\\}$, where $x_{ij}$ represents the j-th sentence of i-th category. The Q is composed of some query sentences Xq. The goal of the MFACD task is to learn a model from S to recognize aspect categories of xq."}, {"title": "3.2 Prompt Enhanced Sentence Representation", "content": "Most of the current approaches use pre-trained models to obtain word representations, which are used to represent sentences. But these approaches leads to discrepancies between the pre-trained target and the downstream task, thus limiting the performance of the pre-trained language model. In contrast, prompt-based approaches can transform a sentence embedding task into a masked language model (MLM) task. These methods effectively utilize the extensive knowledge of the pre-trained model and avoids the bias inherent in direct embedding [Brown et al., 2020]. Therefore, we design specific prompt templates to enhance the adaptive capacity of pre-trained model on downstream tasks."}, {"title": "3.3 Prompt Enhanced Prototype Generation", "content": "To tackle the challenge of imprecise category prototypes in few-shot learning scenarios, we harness the sophisticated capabilities of LLM for meticulous label description. These descriptions can effectively aid in the precise formation of category prototypes.\nCategory Descriptions. To produce elaborate category descriptions, we adopt a language model prompt template, denoted as $P_c(c_i)$=\u201cProvide a comprehensive description of [Ci].\" where ci symbolizes the category label. The purpose of this prompt is to distinctly encapsulate and highlight the characteristics of category. Subsequently, the crafted prompt Pc(ci) is fed into a LLM to generate detailed descriptions about the ci. This process can be formalized as:\n$D_i = G_M(P_c(C_i))$ (3)\nwhere Di represents category descriptions and GM represents the large language model. Leveraging the LLM's proficiency in generating detailed descriptions, which can extract the profound semantics and core characteristics inherent to each category. These are some examples of category descriptions as follows:\n1. food_food_bread:Bread is a staple food made by baking a dough of flour and water, typically described based on its type (such as whole wheat, white bread), size (such as one loaf, one slice), texture (such as soft or hard), ingredients (such as containing kernels or seeds), and flavor (such as sweet or salty).\n2. food_mealtype_breakfast:Breakfast is the first meal of the day, typically consumed in the morning, consisting of food items such as cereals, eggs, toast, pancakes, fruits, and beverages like coffee or juice.\n3. food_food_side_pasta:The food item in question is a side dish consisting of pasta, which is typically prepared from durum wheat dough and served as an accompaniment to a main course in various cuisines.\n4. food_portion:Food portion refers to the specific amount of food provided or consumed during a meal, usually described by weight (such as grams, kilograms), volume (such as milliliters, liters), portion size (such as one or half portions), comparison (such as a football sized watermelon), cutlery unit (such as a bowl of rice), sensory description (such as a sumptuous dinner), or appropriate number of people (such as two servings).\nIt can be observed that these category descriptions contains a wealth of information about the category. According to Eq. (1) with the prompt template Ps(\u00b7) and Eq. (2), we can obtain the representations of D\u00bf, denoted as $v_{c_i} \\in R^{1\\times d}$.\nPrototype Generation. The ve\u2081 is used to provide a guidance for the prototype generation with sentence representations. The discriminant prototype ri for the ith class is extracted by calculating importance based on the category descriptions and the sentence representations $V_i \\in R^{K\\times d}$. The Vi is composed of $\\{v_j|j = 1,..., K\\}$. The common used method of obtaining the prototype is mean pooling of these sentence representations. In this kind of method, the weight of each sentence is equal. We think that some sentences possibly contain a mass of noise information. The same weight for all sentences can not avoid those noise information. Thus, we use the description representations vc\u2081 generated by LLM to reduce the importance of those noise sentence. The importance of sentence representations $a_i \\in R^{1\\times K}$ is:\n$a_i = softmax(M_p(U_{c_i} * V_i)^T)$ (4)\nwhere softmax() is the softmax function and \u2299 is the Hadamard product, i.e., element-wise multiplication. In this Hadamard product, the ve\u2081 extends its size to reach Vi by row copy. Using the attention weight of each sentence ai, we can obtain the prototype $r_i \\in R^{1\\times d}$ .\n$r_i = a_i * V_i,$ (5)\nwhere * is the matrix multiplication. According to the above calculation, we can obtain all category prototypes $\\{r_i|i = 1, 2, ..., N\\}$."}, {"title": "3.4 Training and Inference", "content": "Training. In noisy scenarios, the query sample may contain more than one target aspect category. Following the previous works [Hu et al., 2021], we employ query-attention to customize multiple prototype-specific query representations $v_i^q \\in R^{d}$, which can be obtained by:\n$v_i^q = softmax(tanh(r_i) U_q) U_q$ (6)\nwhere tanh() is the tanh function.\nThe final training loss is:\n$L = -\\sum_{i=1}^{N} y_i (y_i - log \\hat{y_i})$ (7)\nwhere yi is the ground truth label of i-th category (i.e., yi=0 or 1). \u0177r is calculated as follows:\n$\\hat{y_i} = \\frac{Cos(r_i,v)-\\mu}{\\delta}$ (8)\nwhere Cos(ri,v) represents the cosine similarity between the prototype ri and prototype-specific query representations $v_i^q$, \u03bc is the mean of cosine similarity and \u03b4 is the variance of cosine similarity.\nInference. In multi-label classification, compared with the method of the fixed threshold, the method of the dynamic threshold adopts a adaptive value based on label distribution, accurately adapting to different label characteristics. We use a simple and effective strategy of dynamic thresholds by combining varies of statistical magnitude. This dynamic threshold y of i-th category is calculated as follows:\n$y_i^* = a\\cdot m(\\hat{y_i}) + \\beta \\cdot \\sigma(\\hat{y_i}) + \\gamma\\cdot max(y_i) + (1 - \\gamma) min(y_i)$ (9)\nwhere m() represents the mean value, \u03c3() is the standard deviation, max() is the maximum and min() is the minimum value. \u03b1, \u03b2, and y are adjustable parameters."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Experimental Setups", "content": "Datasets. In order to verify the validity of LGP model, two publicly available datasets FewAsp (multi) and FewAsp form [Hu et al., 2021] are used for experimental evaluation. The FewAsp(multi) dataset consists of sentences featuring multiple aspect categories. The FewAsp dataset comprises sentences with mixed (one or more) category types. The above two datasets have the same 100 aspect categories, of which 64 aspect categories are used for training, 16 aspect categories for validation, and 20 aspect categories for testing. General information of two datasets is presented in Table 2.\nImplementation Details. In our study, we employ BERT-uncased English version [Kenton and Toutanova, 2019] as the encoder, with the implementation in PyTorch. The LLM GPT-3.5-Turbo is chosen for generating category descriptions. All experiments are conducted on a single NVIDIA A100, utilizing CUDA version 12.2. During training, we use the AdamW optimer [Loshchilov and Hutter, 2017] with a learning rate of 5e-5. And we set the hyper-parameters \u03b1, \u03b2 and o with 0.3, 0.7 and 0.7, respectively. During each epoch, 800 tasks are randomly sampled for training. In each task, \u039d\u2208 {5,10}, K\u2208 {2,3,5,10}, and the number of query instances of each category is 5.\nEvaluation Metric. Following [Hu et al., 2021], we use Macro-F1 and AUC scores as the evaluation metrics. 600 tasks sampled from test data are used for evaluation, and the average performance on the 600 tasks is reported for comparisons."}, {"title": "4.2 Experimental Results and Analysis", "content": "The comparative experimental results on the FewAsp dataset are shown in Table 3. It can be observed that the proposed LGP demonstrates the superior performance than the existing methods under all experimental evaluations. It is worth noting that LGP surpass current state-of-the-art method FSO [Zhao et al., 2023] with 4.05% and 4.14% gains on F1 score under 5-way 5-shot setting and 10-way 5-shot setting, respectively.\nThe comparative results on the FewAsp(multi) dataset are shown in Table 4. We also observe that the proposed LGP achieves state-of-the-art performance under all experimental evaluations. Compared with FSO, the LGP obtains large performance gains on F1 score with 4.08%, 3.86%, 4.75% and 4.51% improvements under 5-way 5-shot, 5-way 10-shot, 10-way 5-shot and 10-way 10-shot settings, respectively. The LGP obtains 1.66%, 1.21%, 0.96% and 0.64% improvements on AUC over FSO, under 5-way 5-shot, 5-way 10-shot, 10-way 5-shot and 10-way 10-shot settings, respectively.\nIn addition, following the work FSO [Zhao et al., 2023], we also conduct experiments on FewAsp(multi) with fewer training samples such as 2-shot and 3-shot. The experimental results are exhibited in Table 5. The proposed method LGP also achieves the best performance under all experimental settings. And the relatively large improvements on F1 score also can be found.\nTo summarize, the proposed method LGP shows performance advantage on the above two datasets under different experimental settings, exhibiting the effectiveness and the superiority of the proposed method. It seems to suggest that enhancing representations of both sentences and category prototype with the prompt is a promising method to address the MFACD task."}, {"title": "4.3 Ablation Study", "content": "Effectiveness of Components. The proposed method LGP mainly involves two components Prompt Enhanced Sentence Representation (PESR) and Prompt Enhanced Prototype Generation (PEPG). To verify the effectiveness of each component, we conduct ablation study, and experimental results of are shown in Table 6. The removal of the PESR module results in a 2.18% decrease in F1 score and a 1.66% decrease in AUC score. This seems to mean that the absence of PESR undermines the model's comprehension of sentence context. Excluding the PEPG module leads to a 1.49% reduction in F1 scores and a 1.54% reduction in AUC scores, indicating the vital role of PEPG in forming representative category prototypes. The combined removal of both PESR and PEPG causes the most substantial decline in performance, which can explain that their synergy is crucial to the model.\nMoreover, comparing PESR and PEPG, we observe that the performance reduction is more obvious when the PESR is removed. This indicates the PESR is a more effective component than the PEPG and the method of sentence representation exerts a greater influence than the prototype generation.\nVisualization. We visualize feature representations of category prototypes with and without PESR and PEPG. The tool of t-SNE [van der Maaten and Hinton, 2008] is used to visualize the feature vectors, and the visualization results of 20 test categories on FewAsp(Multi) are illustrated in Figure 2. It can be seen that there are distribution differences in the feature representations of category prototypes before and after the application of the PESR and PEPG modules. When we don't have any prompt to help for prototyping, namely without both PESR and PEPG module, the distribution of category prototypes is scattered with blurred boundaries between categories. When only PESR is applied, there is an improvement in the aggregation of prototypes, but the distinction between categories remains insufficient. When both PESR and PEPG are applied, the category prototypes are not only more compact but also more clearly separated from each other, indicating that the PESR and PEPG modules can effectively enhance the generation of representative category prototypes."}, {"title": "4.4 Discussions", "content": "Effect of Prompt Templates. A crucial issue for prompt-based tasks is finding the suitable template. This issue has been explored through two distinct types of templates: hard template and soft template, as detailed in Table 7. Hard tem- plates are crafted manually, providing a structured and fixed format. In contrast, soft templates offer a learnable prompt structure, allowing the model to autonomously adapt and learn the most effective format. According to [Lester et al., 2021], we compare the following soft prompt: \u201cp1, [MASK], ...[x], pn\", where pi means the learnable prompt.\nTable 7 showcases the impact of different prompt templates under 5-way 5-shot setting on FewAsp(multi). Among the templates, \"About [x] Category [MASK] are : [L]\" stands out with the highest performance with 85.22% F1, indicating its effectiveness in guiding the model for aspect-based sentiment analysis. In comparison, the soft prompt scores lower at 82.87% F1. The inclusion of the category label ([L]) in prompts generally improves performance, as seen in the superior results of structured prompts over their underlined variants without [L]. Despite different prompts have different effects on the model's performance, on the whole, a prompt-based approach generally leads to notable enhancements in the model.\nEffect of Tokens. We explicitly assess the effect of the token from its quantity and state (i.e. fixed token or learnable token). When the BETR is tunable (see the blue line in Figure 3), the performance of both fixed token and learnable token method is improved at the beginning of the number of token increasing, after which the performance is not significantly improved. This is attributed to the improvement of the ability with involving multiple tokens.\nWhen the BETR is frozen (see the green line in Figure 3), the method of the fixed token obtain performance degrade after the number of tokens exceeds 5. However, that doesn't happen when the tokens are learnable. It can be observed that replacing the fixed tokens with learnable tokens contributes to the stability of our model.\nEffect of Encoder Parameters. To explicitly assess the effect of parameter learning in the encoder, we explore two experiments frozen BETR (the green line in Figure 3) and tunable BETR (the blue line in Figure 3). It can be observed that the proposed method with a tunable BETR is clearly superior to that with a frozen BETR under all experimental settings. The reason is that allowing the parameters of BERT to be trainable enables the model to dynamically adapt current MFACD tasks."}, {"title": "5 Conclusions and Further Works", "content": "We propose a Label-Guided Prompt (LGP) method for noisy MFACD tasks by utilizing semantic prompt to obtain better sentence representations and category prototypes. This method employs crafted prompts integrated with label information, and it can extract vital contextual and semantic insights from the sentences. Meanwhile, a LLM is used to generate precise category descriptions, which effectively guide the generation of more accurate category prototypes. Extensive experimental results on two datasets demonstrate that the proposed method achieves state-of-the-art performance.\nIn the future work, we aim to explore some novel approaches from advanced prompting mechanisms, context-sensitive learning techniques and co-evolutionary model-prompt strategies. By pushing the boundaries of aspect category detection, our research endeavors to enhance the accuracy and robustness of detecting multiple labels in situations where data availability is limited."}]}