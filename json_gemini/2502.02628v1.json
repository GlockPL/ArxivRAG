{"title": "e-SimFT: Alignment of Generative Models with Simulation Feedback for Pareto-Front Design Exploration", "authors": ["Hyunmin Cheong", "Mohammadmehdi Ataei", "Amir Hosein Khasahmadi", "Pradeep Kumar Jayaraman"], "abstract": "Deep generative models have recently shown success in solving complex engineering design problems where models predict solutions that address the design requirements specified as input. However, there remains a challenge in aligning such models for effective design exploration. For many design problems, finding a solution that meets all the requirements is infeasible. In such a case, engineers prefer to obtain a set of Pareto optimal solutions with respect to those requirements, but uniform sampling of generative models may not yield a useful Pareto front. To address this gap, we introduce a new framework for Pareto-front design exploration with simulation fine-tuned generative models. First, the framework adopts preference alignment methods developed for Large Language Models (LLMs) and showcases the first application in fine-tuning a generative model for engineering design. The important distinction here is that we use a simulator instead of humans to provide accurate and scalable feedback. Next, we propose epsilon-sampling, inspired by the epsilon-constraint method used for Pareto-front generation with classical optimization algorithms, to construct a high-quality Pareto front with the fine-tuned models. Our framework, named e-SimFT, is shown to produce better-quality Pareto fronts than existing multi-objective alignment methods.", "sections": [{"title": "1. Introduction", "content": "Generative artificial intelligence (AI) has made remarkable implications in many domains, especially where creative automation is greatly desired. One notable area is engineering design, where generative AI has the potential to help engineers develop solutions to their problems at a much faster pace than with the traditional design process. Such progress could bring significant innovation to real-world problems and therefore amplify Al's positive impact on our society.\nSeveral efforts have been made to apply deep generative models to solve engineering design problems, as reviewed in (Regenwetter et al., 2022). However, much of the prior work is limited to solving a problem with a fixed set of design requirements and cannot consider of different requirements that the user may provide. In contrast, recent work such as (Etesam et al., 2024) has developed a generative model that takes a set of design requirements as input and outputs a design solution conditioned on those requirements.\nThere remains a challenge in making use of such generative models in practice. For many design problems, finding a solution that meets all the specified requirements is often impossible. Even a highly capable generative model is unlikely to produce a perfect solution, given the problem's inherent complexity. In such a scenario, engineers could focus on finding solutions for a relatively more important subset of the requirements. Or preferably, they would like to obtain a set of Pareto optimal solutions (i.e., a Pareto front) with respect to the important requirements so that they can understand the trade-offs and compare alternative solutions.\nThis challenge highlights new research opportunities in two aspects. First, we need to align a generative model with respect to specific design requirements preferred by the engineer. Next, we need a method to effectively sample a generative model to produce a high-quality Pareto front.\nThis work demonstrates that the alignment problem can be solved by adapting fine-tuning methods used for LLMs, but using simulation feedback. Because a simulator can be used to evaluate a given solution with respect to design requirements of interest, we can use it to either generate fine-tuning data or compute rewards for Reinforcement Learning (RL). We show that different fine-tuning methods are more applicable and effective depending on the requirements.\nIn addition, we propose epsilon-sampling, which leverages the simulation fine-tuned models to produce a high-quality Pareto front. The technique is inspired by the epsilon-constraint method (Haimes, 1971) used for constructing a Pareto front with gradient-based optimization."}, {"title": "2. Related Work", "content": "Preference alignment has gained notable attention, particularly in guiding LLMs to generate contents that align with the user's values or objectives. The primary approach to achieving this is through Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017), and several successful applications have been reported for fine-tuning LLMs for different tasks (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022), and extended to incorporate other feedback sources than humans (Lee et al.; Liu et al., 2023; Jha et al., 2024; Williams, 2024). The most popular RL algorithm used for these methods is Proximal Policy Optimization (PPO) (Schulman et al., 2017). Another alternative is to directly fine-tune LLMs with a preference dataset without RL, e.g., (Hejna et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024), and most notably Direct Preference Optimization (DPO) (Rafailov et al., 2024).\nBecause user preferences for LLMs are likely multidimensional with trade-offs, e.g., helpfulness vs. harmlessness, multi-objective alignment methods have been proposed to produce Pareto-front aligned models with respect to multiple preference criteria. Notable recent work includes Rewarded Soup (Rame et al., 2024), multi-objective DPO (Zhou et al., 2024), Rewards-in-Context (Yang et al., 2024), controllable preference optimization (Guo et al., 2024), and Panacea (Zhong et al., 2024).\nNote that the purpose of multi-objective alignment methods has a strong parallel with the purpose of multi-objective optimization methods (Deb et al., 2016) where the goal is to find models or solutions that constitute a high-quality Pareto front (Zitzler & Thiele, 1998). Therefore, we were motivated to find inspirations from the techniques used in the latter domain such as the epsilon-constraint method (Haimes, 1971) or non-dominated sorting (Deb et al., 2002)."}, {"title": "3. Problem", "content": "The problem of our interest can be stated as follows. Suppose we have a generative model parameterized by \u03b8 that takes a set of design requirements r = {r\u2081,r\u2082,...,r\u2099} as input and outputs a solution x that addresses those requirements, e.g., \u03c0\u03b8(x|r). First, an engineer might want to prioritize a specific requirement r\u1d62. Therefore, we aim to find a fine-tuned model \u03c0\u03b8,r\u1d62 such that a solution sampled from the model is optimal with respect to r\u1d62. In some scenarios, there may be a new design requirement n\u2c7c independent to the current generative model that an engineer would like the sampled solution to nevertheless satisfy. In such a case, the goal is to find a fine-tuned model \u03c0\u03b8,n\u2c7c from which a sampled solution would be optimal with respect to n\u2c7c.\nFinally, given a set of prioritized requirements p \u2286 {r,n}, we aim to sample from the fine-tuned models a set of Pareto optimal solutions with respect to p that maximizes a Pareto-front quality, e.g., hypervolume (Zitzler & Thiele, 1998)."}, {"title": "3.1. Illustrative example: GearFormer", "content": "We use GearFormer, a recently developed generative model for gear train design (Etesam et al., 2024), as an illustrative example for the current work. GearFormer is a Transformer-based model that takes multiple requirements as input via its encoder and outputs a gear train sequence via its decoder. The requirements it can handle are the speed ratio, output motion position, output motion direction, and input/output motion types. While it has been shown to outperform traditional search methods, an engineer does not have an option to express a preference of emphasizing one requirement over another, or explore multi-requirement trade-offs. Our goal is to fine-tune this model with respect to specific requirements and use the fine-tuned models to generate a high-quality Pareto front.\nWe consider the two types of requirements as expressed in the problem definition. Original requirements are those used to train GearFormer and therefore are used as an input to condition the output design. Note that these requirements are treated as equality constraints, i.e., they are target values such as speed ratio or output motion position that the design must meet. New requirements are those never seen by the model during training. We consider metrics such as the bounding box volume and design cost, which can be evaluated given a design. In contrast to the original requirements, we intentionally chose new requirements formulated as inequality constraints, i.e., an engineer will be willing to accept any value that is below the specified bound value."}, {"title": "3.2. Challenges of fine-tuning a generative design model", "content": "A generative model for engineering design such as GearFormer is trained with a synthetic dataset of (r = requirements, x = solution) pairs, where x can be first generated using some rules and r is evaluated using a simulator for the generated x. The model is then trained to predict x given r, which means that the model has only seen designs that perfectly address the requirements. Therefore, during fine-tuning, any design that does not perfectly address a particular original requirement would likely deteriorate the performance of the pre-trained model. We show this effect in an ablation study presented in Experiments.\nNow, suppose a RL-based method is used to fine-tune a pre-trained model with a typical policy gradient loss\n$$L(\\theta) = -\\mathbb{E}_{x \\sim \\pi_{\\theta}} [log \\pi_{\\theta}(x|r_i)R(x, r_i)]$$(1)\nwhere R is the reward for the solution sampled from the current policy. Since we aim to avoid degrading the policy with low-quality data, R can simply become a binary function that gives 1 for an x that satisfies r\u1d62 and 0 otherwise, i.e., equivalent to simply rejecting the sample. This means that with rejection sampled data, the Equation (1) simply becomes a typical log probability loss used for supervised"}, {"title": "fine-tuning, e.g.,", "content": "$$L(\\theta) = -\\mathbb{E}_{x' \\sim \\pi_{\\theta}} [log \\pi_{\\theta}(x'|r_i)]$$(2)\nwhere x' are solutions that satisfy the target value r\u1d62. We therefore assume that SFT with rejection sampled data suffices as the fine-tuning step for original design requirements.\nHowever for new design requirements, the fine-tuning scenario is very similar to the one with LLMs. The pre-trained model is not trained on the new requirement data and does not have any sense of which output is good or bad with respect to the requirement. Therefore, a solution that does not perfectly satisfy the new requirement but is reasonably close can still provide useful signals for the model. We can therefore use a continuous reward value that reflects the degree of constraint violation. Based on these observations, a similar two-step technique used for fine-tuning LLMs such as using DPO or PPO with simulation feedback in addition to SFT can be considered."}, {"title": "3.3. Challenges of generating a good Pareto front", "content": "Randomly sampling a generative design model multiple times likely would not result in a good Pareto front because generation is not conditioned on different requirement preferences. One could sample multiple models each fine-tuned for different requirements, but you may get clusters of solutions at the extremes of only satisfying each requirement."}, {"title": "4. Methods", "content": "Given a pre-trained model that takes in a list of requirements and outputs a design that addresses those requirements, we first aim to fine-tune the model to prioritize satisfying a specific requirement. The fine-tuning methods are named SimFT, where we use a physics simulator instead of human feedback to either generate the fine-tuning dataset or provide reward signals during PPO fine-tuning. See Figure 2 for illustration of all SimFT methods."}, {"title": "4.1. SimFT methods for original requirements", "content": "For an original requirement r\u1d62, we perform a single SFT step as justified in the previous section. We generate an additional dataset for SFT by prompting the pre-trained model with a list of objectives (one of which is r\u1d62), evaluate the designs generated using a simulator, and keep only the designs that meet r\u1d62. This can be thought as synthetic data generation via rejection sampling, but performed in an offline mode before training. Also, we have the advantage of using a simulator to accurately evaluate the data generated and keeping only the perfect design solutions.\nThe pre-trained model is fine-tuned with this dataset by minimizing the log probability loss (Equation (2)). Note that we freeze the encoder while updating the decoder only."}, {"title": "4.2. SimFT methods for new requirements", "content": "For this category, a standard two-step fine-tuning methods developed for LLMs can be applied.\nSFT. In the first SFT stage, we generate the dataset in the following manner. We sample a design solution from the pre-trained model and compute the corresponding new requirement value using a simulator. We increase this value by some random variance and set it as the bound value for the requirement, synthetically generating a pair of the constraint bound value and a solution that satisfies the constraint.\nWe fine-tune the pre-trained model using this dataset by minimizing the log probability loss (Equation (2)) but conditioned on n\u2c7c. We freeze the original encoder while training the decoder and also a new encoder that can take in the new requirement bound value.\nNext, we apply either DPO or PPO to further fine-tune the model with respect to the new objectives.\nDPO. The preference dataset is generated by sampling a pair of solutions from the pre-trained model and using a simulator to evaluate the requirement values. The solution with the lower requirement value is labeled as preferred while the other as rejected. We then assume the mean of the two requirement values as the constraint bound value, i.e., the preferred solution would satisfy the bound value while the rejected solution would not. We freeze both the original encoder and the new encoder while updating the decoder by minimizing the DPO loss (Rafailov et al., 2024):\n$$L_{DPO}(\\theta) = - \\mathbb{E}_{(x_w, x_l, n_i) \\sim D} \\bigg[log \\sigma \\bigg(\\beta log \\frac{\\pi_{\\theta}(x_l|n_i)}{\\pi_{old}(x_l|n_i)} - \\beta log \\frac{\\pi_{\\theta}(x_w|n_i)}{\\pi_{old}(x_w|n_i)} \\bigg) \\bigg]$$(3)\nwhere x_w and x_l are the preferred and rejected solutions for the requirement n\u1d62. \u03b2 is the KL divergence penalty parameter and \u03c0_{old} is the reference policy.\nPPO with a simulator. Another approach we can employ is PPO (Schulman et al., 2017), using the simulator to compute accurate rewards for each solution during exploration. For the loss, we use the clipped policy ratio with the KL divergence penalty:\n$$L_{RL}(\\theta) = \\mathbb{E}_{(x, n_i) \\sim D} \\bigg[min \\bigg( \\frac{\\pi_{\\theta}(x|n_i)}{\\pi_{old}(x | n_i)}R(x, n_i), clip(\\frac{\\pi_{\\theta}(x|n_i)}{\\pi_{old}(x | n_i)}, 1 - \\epsilon, 1 + \\epsilon)R(x, n_i) \\bigg) - \\beta KL(\\pi_{\\theta}, \\pi_{old}) \\bigg]$$(4)"}, {"title": "where R(x, ni) is a reward function computed using the simulator output and normalized to [-1, 1], i.e., R = 1 if x is evaluated to meet the bound value ni and approaches -1 as the violation increases (See Appendix A for details).", "content": null}, {"title": "4.3. epsilon-sampling for Pareto-front generation", "content": "We propose epsilon-sampling (Figure 3) inspired by the epsilon-constraint method (Haimes, 1971) to obtain Pareto-optimal solutions with SimFT models. The epsilon-constraint method is a well-known technique to produce a Pareto front with gradient-based algorithms for multi-objective optimization problems. Given a pair of objectives, the method sets one objective as a constraint and solves multiple single-objective constrained optimization problems by incrementing the threshold value \u03f5 imposed on the constraint. Solutions to these problems form a Pareto front.\nWe apply this idea for sampling generative models to construct a Pareto front. Given a set of requirements r, we assume that a model fine-tuned for r\u1d62 can best enforce that constraint; therefore, sampling from that model would be equivalent to posing r\u1d62 as a constraint and the rest of requirements as objectives. We sample multiple solutions from this model by varying the target value by r\u1d62 \u00b1 \u03b5, effectively mimicking the epsilon-constraint method. The same technique can be applied with new requirements n\u2c7c."}, {"title": "5. Experiments", "content": "We evaluate the performance improvements made by SimFT methods and perform ablation studies to elucidate important aspects of SimFT. We then evaluate e-SimFT against several baselines in generating high-quality Pareto fronts."}, {"title": "5.1. Experimental setups", "content": "Pre-trained model and simulator. We use GearFormer (Etesam et al., 2024) as the pre-trained model. We also use the simulator developed for GearFormer and extend it to compute the new requirements required for this work. While it would be ideal to test our method on multiple generative models, GearFormer was the only work that provided the model, simulator, and dataset required for our experiments."}, {"title": "Design requirements.", "content": "We consider four design requirements. Two original requirements of GearFormer, speed ratio and output motion position, are posed as equality constraints. The other two are new requirements that were not considered in training GearFormer, bounding box volume (b.box) and design cost, posed as inequality constraints."}, {"title": "Design scenarios.", "content": "We generated new 30 random test problems based on the distributions of requirement metrics obtained from the original GearFormer dataset (Etesam et al., 2024). For each test problem, we consider 10 different trade-off scenarios \u2013 the all possible two-way and three-way combinations of the four requirements under consideration. For each design scenario, a sample budget of N is assumed. How this budget is used varies depending on the methods employed, as explained in the following section."}, {"title": "Baselines.", "content": "The first baseline involves sampling the pre-trained GearFormer N times for each design scenario.\nTwo distinct and recent multi-objective alignment methods are chosen as additional baselines. First, Rewarded Soup (RS) (Rame et al., 2024) linearly interpolates the weights of models fine-tuned for each specific objective, given the preference weights assigned for each objective. A Pareto front can be constructed by sampling from multiple of these linearly interpolated models with varying preference weights. We define Ns combinations of preference weights and allocate N/N sampling budget for each combination.\nWe also chose Rewards-in-Context (RiC) (Yang et al., 2024). RiC performs SFT on the pre-trained model with outputs associated with their reward/preference values, which are encoded as additional input to the model. For this work, we train a new encoder that can take in preference values for each requirement, indicating which requirements to prioritize. We define Nr combinations of requirement preferences and allocate N/Nr sampling budget for each combination. Baseline implementation details including the preference weight combinations used can be found in Appendix A."}, {"title": "e-SimFT.", "content": "Relevant SimFT models are chosen based on the design scenario and we allocate N/2 or N/3 (for two- or three-requirements) sampling budget to each model. We then create an evenly spaced values of \u03b5, sized either N/2 or N/3, within [-5, 5] for original requirements and [0, 10] for new requirements, and add these values to the requirement values before sampling SimFT models.\nWe also test two conditions as ablation: SimFT only or epsilon-sampling only. For the former, we use the same sampling budget allocation as e-SimFT with relevant SimFT models but do not employ epsilon-sampling. For the latter, we follow the same epsilon-varying schedule as the e-SimFT method but use the pre-trained GearFormer model."}, {"title": "Evaluation metric.", "content": "For each requirement, the degree of constraint violation for the design solutions generated by different models is normalized as [0, 1]. These values are used to determine Pareto optimal solutions for each problem and the hypervolume of the Pareto front (Zitzler & Thiele, 1998) is used to compare e-SimFT versus other baselines."}, {"title": "Dataset and training", "content": "We use the validation and test portion of the original GearFormer dataset (Etesam et al., 2024) for all our fine-tuning and testing, |D| = 7360, which is around 1% of the training dataset used for GearFormer. We believe that this is a reasonable ratio of training versus fine-tuning data for generative design models in practice. Details on the dataset and training can be found in the Appendix A."}, {"title": "5.2. Evaluation of SimFT methods", "content": "Table 1 presents the improvements made by SimFT methods for each requirement. For original requirements (speed and position), the metrics are based on Etesam et al.. For new requirements (cost and bounding box volume), we calculate the percentage of the problems from the test dataset for which the first solution generated by the original GearFormer or SimFT models satisfy the respective requirement. We observed that DPO performed slightly better than RL for both requirements. As an example, Figure 4 shows solutions obtained for the same design problem with the original GearFormer model vs. a SimFT model."}, {"title": "Importance of offline rejection sampling for SFT.", "content": "To show the importance of using only the samples that satisfy the equality constraint requirements, we ran an ablation study where any new sample drawn was accepted for the SFT training data. The performances for the speed and position requirements dropped to 0.0179 and 0.0339, compared to the baseline performances of 0.0171 and 0.0338."}, {"title": "Performance trade-offs during DPO.", "content": "Because DPO is performed using a preference loss that differs from the original cross-entropy loss used for the pre-trained model, over-training the model can lead to significant deterioration of its original performance. Figure 5 shows that after 16 and 12 epochs, respectively, the percentage of valid designs produced by GearFormer drops below 95% (the performance reported in (Etesam et al., 2024)) and at a significant rate in"}, {"title": "Using binary reward function for PPO.", "content": "Considering the new requirements are inequality constraints, we could implement a binary reward function for PPO that simply"}, {"title": "5.3. Evaluation of e-SimFT for Pareto-front generation", "content": "Finally, we report the hypervolumes of Pareto fronts obtained with e-SimFT and other methods in Tables 2 and 3. We also present Pareto fronts generated by different methods for sample design problems in Figure 5."}, {"title": "6. Conclusions", "content": "This work introduces e-SimFT, a new framework for Pareto-front design exploration with simulation fine-tuned generative models in engineering design. It employs multiple preference alignment methods, named SimFT methods, by using a simulator to fine-tune a generative model with respect to a specific requirement prioritized by an engineer. SimFT models are then sampled using the epsilon-sampling method to construct a high-quality Pareto front for design scenarios involving trade-offs among multiple requirements. In both two- and three-requirements scenarios, e-SimFT outperformed latest multi-objective alignment methods in terms of the hypervolumes of the Pareto fronts generated.\nWe believe that many parallels exist between generative AI and engineering design as both domains strive for creative automation. This work showcases an innovative application of generative AI research to facilitate engineering design exploration."}, {"title": "A. Experiment Details", "content": null}, {"title": "A.1. Dataset", "content": "We use the validation and test portion of the original GearFormer dataset (Etesam et al., 2024) for all our fine-tuning and testing, |D| = 7360. 5% of the dataset was withheld for testing SimFT methods including their ablation studies. The rest was used for fine-tuning, i.e., Dft \u222a Dtest = D,|Dft| = 6992, |Dtest| = 368.\nFor the original requirement SimFT, use the whole Dft for SFT; where 90% is used for training and 10% is held for validation.\nFor the new requirement SimFT, use the first half Dft, 1 for SFT and the other half Dft,2 for DPO/RL, i.e., Dft,1 \u222a Dft,2 = Dft, |Dft,i|=3496. Again for each subset, we use 90% for training and 10% for validation."}, {"title": "A.2. Training SFT models.", "content": "SFT for original requirements: The training was performed until the validation loss increased. For the speed SimFT, it was stopped at epoch #16, and for the position SimFT, it was stopped at epoch #19. We used the learning rate of 1e-6 and the batch size of 64 for both.\nSFT for new requirements: The training was performed until the validation loss increased. For both the speed and position SimFT models, it was stopped at epoch #6. We used the learning rate of 1e-5 (including the new encoder) and the batch size of 64 for both.\nDPO for new requirements: The training was done for 20 epochs and the model checkpoint with the best requirement improvement was picked post hoc, subject to the criterion that 95% of the generated solutions are valid. For the cost SimFT, this was at epoch #15 while for the boundinb box volume SimFT, it was at epoch #10. We used the learning rate of 1e-6, \u03b2 = 0.1, and the batch size of 64 for both.\nPPO for new requirements: Same as DPO, the training was done for 20 epochs and the model checkpoint with the best requirement improvement was picked post hoc, subject to the criterion that 95% of the generated solutions are valid. For the cost SimFT, this was at epoch #9 while for the boundinb box volume SimFT, it was at epoch #4. We used the learning rate of 1e-5, \u03b2 = 0.1, and the batch size of 64 for both.\nFor the reward function, we used the following normalization function based on the evaluation of each solution using a simulator:\n$$R(x) = \\begin{cases} 1, & \\text{if } \\tilde{n}(x) \\leq \\eta \\\\ 1 - \\frac{1}{2} \\frac{(\\tilde{n}(x) - \\eta)^2}{1 + |\\tilde{n}(x) - \\eta|}, & \\text{otherwise} \\end{cases}$$\nwhere x is the solution, \u03b7 is the target requirement value, and \\tilde{n}(x) is the evaluated requirement value. Note that because we compute the exact reward for each solution as a whole, no actor-critic models are employed."}, {"title": "A.3. Baseline implementations", "content": "Rewarded Soup: For a given weight preference combination, we linearly interpolate the parameters (weights) of SimFT models to create a new model specific for that combination. For Pareto-front generation, we use the following weight combinations. For the two-requirement scenarios, w\u2081 = [0, 0.2, 0.4, 0.6, 0.8, 1] and w\u2082 = 1 \u2212 w\u2081. For the three-requirement scenarios, w\u2081 = [0, 0, 0, 0.33, 0.5, 0.5, 1], w\u2082 = [0, 0.5, 1, 0.33, 0, 0.5, 0], and w\u2083 = 1 \u2212 w\u2081 \u2212 w\u2082.\nRewards-in-Context: Using the problems defined in Dft, the training data is generated by sampling a solution for each problem using GearFormer and determining whether the solution meets each of the four requirements of interest or not. Based on this, we can create a preference weight vector that indicates whether a particular requirement is met or not. We then perform supervised fine-tuning with this dataset using log probability loss, while training two new encoders one for encoding the new requirements and another for encoding the preference weight vector. We train until the validation loss increases, which was at epoch #7. We used the learning rate of 1e-6 for all models and the batch size of 64. For Pareto-front generation, we use the following weight combinations. For the two-requirement scenarios, w\u2081 = [0, 1, 1] and w\u2082 = [1, 0, 1]. For the three-requirement scenarios, w\u2081 = [0, 0, 1, 0, 1, 1, 1], w\u2082 = [0, 1, 0, 1, 0, 1, 1], and w\u2083 = [1, 0, 0, 1, 1, 0, 1]."}]}