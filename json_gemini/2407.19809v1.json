{"title": "Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS", "authors": ["Stefanos Gkikas", "Manolis Tsiknakis"], "abstract": "Automatic pain assessment plays a critical role for advancing healthcare and optimizing pain management strategies. This study has been submitted to the First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed multimodal framework utilizes facial videos and fNIRS and presents a modality-agnostic approach, alleviating the need for domain-specific models. Employing a dual ViT configuration and adopting waveform representations for the fNIRS, as well as for the extracted embeddings from the two modalities, demonstrate the efficacy of the proposed method, achieving an accuracy of 46.76% in the multilevel pain assessment task.", "sections": [{"title": "I. INTRODUCTION", "content": "The International Association for the Study of Pain (IASP) defines pain as \u201can unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage\u201d [1], marking a recent update to the definition. Pain significantly affects individuals and societal structures, with people of all ages experiencing it due to accidents, diseases, or medical treatments-making it the primary reason for medical consultations. Acute and chronic pain pose clinical, economic, and social difficulties. Beyond its direct effects on a person's daily life, pain is associated with various negative consequences, such as increased opioid use, substance abuse, addiction, declining social interactions, and mental health problems [2]. Effective pain assessment is essential for early diagnosis, disease progression monitoring, and evaluation of treatment efficacy, especially in managing chronic pain [3]. Additionally, adjusting pain intensity is crucial in therapy approaches like myofascial therapy, where a practitioner, such as a physiotherapist, externally induces the pain, and understanding the patient's pain level is vital [4]. Pain evaluation is crucial yet challenging for healthcare professionals [5], especially when dealing with patients who cannot communicate verbally. This challenge is further amplified in elderly patients who may be less expressive or hesitant to discuss their pain [6]. Moreover, comprehensive research [7]- [9] highlights significant differences in pain expression across different genders and age groups, adding complexity to the pain assessment process. Pain assessment encompasses a variety of approaches, from self-reporting using detailed rating scales and questionnaires, considered the gold standard, to observing behavioral indicators like facial expressions, vocalizations, and bodily movements [10]. It also includes analyzing physiological responses such as electrocardiography and skin conductance, which offer essential insights into the physical manifestations of pain [3]. Moreover, functional near-infrared spectroscopy (fNIRS) is a promising method for measuring pain-related physiological responses. This non-invasive neuroimaging technique evaluates brain activity by tracking cerebral hemodynamics and oxygenation changes. Specifically, fNIRS simultaneously records changes in the cortical concentrations of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR), offering critical insights into brain function [11]. Furthermore, fNIRS studies have demonstrated that noxious stimuli initiate changes in oxygenation levels across various cortical regions in healthy and diseased subjects [12].\nThis study introduces a modality-agnostic multimodal framework that utilizes videos and fNIRS. The proposed pipeline is based on a dual Vision Transformer (ViT) configuration, eliminating the need for domain-specific architectures or extensive feature engineering for each modality by interpreting the inputs as unified images through 2D waveform representation."}, {"title": "II. RELATED WORK", "content": "Recent developments have introduced various innovative methods for assessing pain levels from video data. The authors in [13] developed a temporal convolutional network (TCN) and utilized the HSV color model, arguing that it offers more advantages for tasks related to human visual perception, such as skin pixel detection and multi-face detection. The authors in [14] combined the VGG-Face CNN with a 3-layer LSTM to extract spatio-temporal features from grayscale images, applying zero-phase component analysis for enhancement. Conversely, in [15], principal component analysis was employed to reduce dimensionality."}, {"title": "III. METHODOLOGY", "content": "This section describes the pipeline of the proposed multi- modal automatic pain assessment framework, the architecture of the models, the pre-processing methods, the pre-training strategy, and the augmentation techniques."}, {"title": "A. Framework Architecture", "content": "The proposed framework, Twins-PainViT, consists of two models: PainViT-1 and PainViT-2. Both models are identical in architecture and parameters and follow the same pre-training procedure, which will be detailed in Section III-D. PainViT- 1 is provided with the corresponding video frames and the visualized fNIRS channels and functions as an embedding extractor. PainViT\u20132 receives the visual representation of the embeddings and completes the final pain assessment.\n1) PainViT: Vision Transformers (ViTs) [23] have emerged as a new paradigm in computer vision tasks due to their performance. However, despite their impressive efficacy, transformer- based models face challenges in scaling with larger input sizes, leading to substantial computational costs. This inefficiency primarily derives from the element-wise operations in the multi- head self-attention mechanism. Numerous efforts have been made to enhance the efficiency and reduce the complexity of transformer-based architectures by modifying the self-attention module or the model's overall structure [24] [25]. Our approach is founded on the principles of [26] introducing the hierarchical architectures into the vision transformers and [27] proposing mechanisms that increase efficiency and speed.\n2) PainViT-block: Each block features two components: the Token-Mixer and the Cascaded-Attention. It is structured with an Cascaded-Attention module at the core and a Token-Mixer module positioned preceding and following it. For every input image I, overlapping patch embedding is applied, producing 16 \u00d7 16 patches, each projected into a token with a dimension of d.\na) Token-Mixer: To enhance the incorporation of local structural information, the token T is processed through a depthwise convolution layer:\n$Y_c = K_c * T_c + b_c,$\nwhere $Y_c$ is the output of the depthwise convolution for channel c of the token $T_c$. $K_c$ is the convolutional kernel specifically for channel c, $T_c$ is the c-th channel of the token T, and $b_c$ is the bias term added to the convolution output of channel c. The symbol * denotes the convolution operation. Following the depthwise convolution, batch normalization is applied to the output:\n$Z_c = \\frac{Y_c - \\mu_\\beta}{\\sqrt{\\sigma_\\beta^2 + \\epsilon}} \\Upsilon + \\beta_c,$\nwhere $Z_c$ is the batch-normalized output for channel c of the token T. $Upsilon$ and $beta_c$ are learnable parameters specific to channel c that scale and shift the normalized data. $\\mu_\\beta$ is the batch mean of $Y_c$, $\\sigma_\\beta^2$ is the batch variance of $Y$, and $e$ is a small constant added for numerical stability to avoid division by zero. Next, a feed-forward network (FFN) facilitates more efficient communication between different feature channels:\n$IF(Z_c) = W_2ReLU(W_1 \\cdot Z_c + b_1) + b_2,$\nwhere $IF(Z_c)$ is the output of the feed-forward network for the input $Z_c$. $W_1$ and $W_2$ are the weight matrices of the first and second linear layers; $b_1$ and $b_2$ are the bias terms for the first and second linear layers, respectively, and ReLU is the activation function.\nb) Cascaded-Attention: Regarding the attention mecha- nism, there is a single self-attention layer. For every input embedding:\n$X_{i+1} = \\Phi_A(X_i),$\nwhere $X_i$ is the full input embedding for the i-th PainViT-block. More specifically, the Cascaded-Attention module employs a cascaded mechanism that partitions the full input embedding into smaller segments, each directed to a distinct attention head. This approach allows the computation to be distributed across the heads, enhancing efficiency by avoiding long input embeddings. The attention is described as:\n$X_{ij} = Attn(X_{ij}W_q, X_{ij}W_k, X_j^\\prime W_v),$\n$X_{i+1} = Concat[X_{ij}]_{j=1:h}W^P,$\nwhere each j-th head calculates the self-attention for $X_{i,j}$, which represents the j-th segment of the full input embedding $X_i$, structured as [$X_{i1}, X_{i2}, ..., X_{ih}$] where $1 \\leq j \\leq h$ and h denotes the total number of heads. The projection layers $W_q^{ij}, W_k^{ij}$, and $W_v^{ij}$ map each segment input embedding into distinct subspaces. Finally, $W^P$ is a linear layer that reassembles the concatenated output embeddings from all heads back to a dimensionality that aligns with the original input. Furthermore, the cascaded architecture enhances the learning of richer embedding representations for Q, K, and V layers. This is achieved by adding the output from each head to the input of the subsequent head, enabling the accumulation of information throughout the process. Specifically:\n$X_{ij} = X_{ij} + X_{i(j-1)}.\nHere, $X_i^j$ represents the addition of the j-th input segment $X_{ij}$ and the output $X_{i(j-1)}$ from the (j \u2013 1)-th head. The summation replaces $X_{ij}$ as the new input embedding for the j- th head in the self-attention computation. Finally, it is noted that depthwise convolution is applied to each Q in every attention head. This enables the subsequent self-attention process to capture global representations and local information.\nThe framework comprises three PainViT\u2013blocks, each with 1, 3, and 4 depths, respectively. This hierarchical structure features a progressive reduction in the number of tokens by subsampling the resolution by a factor of 2\u00d7 at each stage. Correspondingly, the architecture facilitates the extraction of embeddings with dimensions d across the blocks, specifically 192, 288, and 500. Additionally, the multihead self-attention mechanism within each block employs 3, 3, and 4 heads, respectively."}, {"title": "B. Embedding extraction & Fusion", "content": "For each frame of a video, $V = [V_1, V_2, ..., V_n]$, PainViT\u20131 extracts a corresponding embedding. These embeddings are aggregated to form a unified feature representation of the video. Similarly, for each channel of an fNIRS signal, $C=[C_1, C_2,..., C_m]$, PainViT\u20131 extracts embeddings, which are subsequently aggregated to create a representation of the fNIRS signal. This process can be described as:\n$E_v \\leftarrow \\sum_{i=1}^{n} PainViT-1(v_i),$\n$E_c \\leftarrow \\sum_{i=1}^{m} PainViT-1(c_i),$\nwhere $E_v$ and $E_c$ are the corresponding embedding representations for the video and fNIRs. Following the extraction of embeddings, $E_v$ and $E_c$ are visualized as waveform diagrams. The waveform from each modality-video and fNIRS\u2014is merged into a single image with a resolution of 224 \u00d7 224. This unified visual representation is fed into PainViT-2 for the final pain assessment."}, {"title": "C. Pre-processing", "content": "The pre-processing involves face detection for the corre- sponding frames of the videos and the generation of waveform diagrams from the original fNIRS. The MTCNN face detector [28] was utilized, employing a series of cascaded convolutional neural networks to predict both faces and facial landmarks. The resolution of the detected faces was set at 224 \u00d7 224 pixels. All fNIRS channels are used to generate waveform diagrams. A waveform diagram visually represents the shape and form of a signal wave as it progresses over time, illustrating the signal's amplitude, frequency, and phase. This method provides the simplest and most direct way to visualize a signal, as it does not necessitate any transformations or additional computations such as those involved in creating spectrograms, scalograms, or recurrence plots. Similarly, the embeddings extracted from PainViT-1 are visualized using the same method. Although these embeddings are not signals, the 1D vectors can still be plotted in a 2D space for analysis or utilization from the deep-learning vision models. All waveform diagrams generated from the fNIRS data and embeddings are formatted as images with a 224 x 224 pixels resolution."}, {"title": "D. Pre-training", "content": "Before the automatic pain assessment training process, the Twins-PainViT models were pre-trained using a multi-task learning strategy. Four datasets, which include images for emotion assessment tasks, were employed. The AffectNet [29] and RAF-DB basic [30] datasets provided facial images for recognizing basic emotions, while the Compound FEE-DB [31] and RAF-DB compound [30] datasets were used for identify- ing complex emotions. Additionally, five datasets containing biosignals were also utilized. EEG-BST-SZ [32] comprises electroencephalograms used for schizophrenia recognition, and Silent-EMG [33] includes electromyograms aimed at identifying the location of origin of the EMGs (such as throat and mid- jaw). Furthermore, electrocardiogram, electromyogram, and galvanic skin response samples from the BioVid [34] dataset were employed for the pain assessment task. All the biosignals were utilized in the form of waveform representations, as described in III-C. The multi-task learning process is described as:\n$L_{total} = \\sum_{i=1}^9 [e^{w_i} L_s + w_i],$"}, {"title": "E. Augmentation Methods & Regularization", "content": "Several augmentation methods have been utilized for training the proposed framework. Regarding the pre-training process, RandAugment [35] and TrivialAugment [36] were adopted."}, {"title": "IV. EXPERIMENTAL EVALUATION & RESULTS", "content": "This study utilizes the dataset provided by the challenge organizers [22], [40], comprising facial videos and fNIRS data from 65 participants. The dataset includes 41 training, 12 validation, and 12 testing subjects recorded at the Human-Machine Interface Laboratory, University of Canberra, Aus- tralia. Electrodes for transcutaneous electrical nerve stimulation, serving as pain stimuli, were placed on the inner forearm and the back of the right hand. Pain threshold, defined as the lowest stimulus intensity at which stimulation becomes painful (low pain), and pain tolerance, defined as the highest intensity of pain a person can endure before it becomes intolerable (high pain), were measured. For the fNIRS, 24 channels each for HbO and HbR were utilized, alongside all 30 frames per video available. The results presented in this study focus on the validation part of the dataset, structured in a multi-level classification setting (No Pain, Low Pain, and High Pain). Table III outlines the training framework details for the automatic pain assessment. We note that numerous experiments were conducted across each modality and their fusion; however, only the most successful results are presented in the subsequent sections and corresponding tables."}, {"title": "A. Facial Videos", "content": "In the context of facial videos, two fusion embedding tech- niques were applied: the Addition method aggregating the 30 embeddings into a single fused vector with dimension d = 500 and the Concatenation method combining the embeddings to form a vector with d = 30 \u00d7 500 = 15,000. Utilizing the Addition method, we observed an initial accuracy of 41.90% for the multi-class classification task with augmentation and reg- ularization levels (0.1 for AugMix, Rand, Trivial, and 0.1|3 for MaskOut, and 0.5 for DropOut). Increasing the augmentation intensities to 0.5 and MaskOut to 0.7|3 raised the accuracy to 44.91%. Applying MaskOut to 0.7|3 and raising DropOut from 0.5 to 0.6 achieved 42.36%. Increasing DropOut to 0.7 and AugMix, Rand, and Trivial to 0.9 improved accuracy to 43.52%. Table IV presents the results. Utilizing the Concatenation method, and initial settings with a uniform augmentation probability of 0.3 across AugMix, Rand, Trivial, and 0.3|3 for MaskOut and 0.1 LS and 0.5 DropOut yielded a 40.28% accuracy. Increasing MaskOut to 0.8/5 while maintaining other augmentations at 0.5 improved accuracy to 41.44%. The highest accuracy of 43.75% was achieved with 0.9 across all augmentations except MaskOut, which was adjusted to 0.63, and high regularization (LS 0.4, DropOut 0.5). The corresponding results are summarized in Table V."}, {"title": "B. FNIRS", "content": "Similar to facial videos, the methods of Addition and Concatenation were applied. From the original 24 channels 2 were excluded due to faults. For the HbR & Addition method, starting accuracy was 39.35% with uniform probabilities of 0.5 for AugMix, Rand, and Trivial, and MaskOut set to 0.6|5. Adjustments to MaskOut at 0.73 and increased LS led to slight accuracy dips, while further adjustments in LS and DropOut raised accuracy to 41.20% (refer to Table VI). In the HbR & Concatenation method, initial augmentations with MaskOut at 0.7|3 achieved an accuracy of 40.97%. Amplifying all augmentations to 0.9 while maintaining MaskOut at 0.7/3 resulted in a peak accuracy of 42.13% (refer to Table VII). For the HbO & Addition method, accuracies began at 43.06% with uniform augmentation probabilities of 0.3 and MaskOut at 0.3/3. Raising MaskOut to 0.73 with slight changes in LS and DropOut maintained similar accuracies, while optimizing MaskOut to 0.8|3 improved performance to 44.68% (refer to Table VIII). In the HbO & Concatenation method, the augmentation methods with a probability of 0.1 started with an accuracy of 42.13%. The peak accuracy of 44.44% was achieved with a balanced augmentation at 0.9 and MaskOut at 0.7|3, indicating effectiveness of increased overall applied augmentation combined with high regularization. Subsequent adjustments slightly lowered accuracy, underscoring the im- portance of optimal augmentation settings (refer to Table IX).Generally, enhanced performance is observed with HbO compared to HbR, as also noted in other studies [41], due to its superior signal-to-noise ratio. The combined HbR and HbO using the Addition method initially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut at 0.73. Increasing AugMix, Rand, and Trivial to 0.5 while elevating MaskOut to 0.77 marginally improved accuracy to 43.29%. Maintaining augmentations but adjusting MaskOut back to 0.7|3 with a slight increase in LS resulted in a slight decrease in accuracy to 42.59%. However, further increasing all augmentations to 0.9 and LS to 0.3 while maintaining MaskOut at 0.7|3 maximized the accuracy to 43.75%. Reducing DropOut to 0.1 in the final configuration slightly reduced accuracy to 43.06%, emphasizing the importance of optimizing regularization alongside augmentation strategies for achieving the best possible results (refer to Table X)."}, {"title": "C. Fusion", "content": "In this section, we describe the fusion of facial videos and fNIRS. The HbO was utilized solely for the experiments since it demonstrated superior performance to the HbR. Two methods were developed for data fusion: the previously described Addition method, aggregating embeddings from video frames and fNIRS channels and then combines them, and the Single- Diagram method, where aggregated embeddings from both modalities are concurrently visualized in the same image. For the Addition method and initial configurations with moderate augmentation levels (0.5 for AugMix, Rand, Trivial) and MaskOut at 0.4/5 achieved a 42.36% accuracy. Increasing augmentation levels to 0.9 and adjusting regularization pa- rameters (LS up to 0.4 and DropOut up to 0.9) improved the accuracy, peaking at 43.75% (refer to Table XI). For the Single Diagram method, accuracy improvements were observed, as shown in Table XII. Starting with lower MaskOut levels at 0.35 and standard augmentation probabilities (0.5), the accuracy was 45.83%. Utilizing augmentation probabilities to 0.9 and MaskOut adjustments to 0.7|3 significantly improved performance, achieving a high of 46.76%."}, {"title": "V. INTERPRETATION & COMPARISON", "content": "Regarding the framework's interpretation, attention maps were generated from the last layer of PainViT-2, which processed the unified image visualizing both the video and HbO embedding waveforms. This layer contains 500 neurons, each contributing uniquely to and attending to the input. Fig. 3 illustrates four examples where certain neurons focus on the video embedding waveform, others on the HbO, and some attend to both waveforms, emphasizing different parts and details. Table XIV compares the proposed pipeline and the baseline results provided by the challenge organizers. The video- based approach using the Addition method outperformed the baseline by 4.91%. Using the HbO with the Addition method, the improvement was lesser, at 1.48%. Finally, the modality fusion using the Single Diagram approach resulted in a more significant improvement of 6.56%."}, {"title": "VI. CONCLUSION", "content": "This study outlines our contribution to the First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN), employing facial videos and fNIRS through a modality-agnostic approach. Twins-PainViT was introduced, a framework founded on a dual configuration of vision transformers, pre-trained on the plethora of datasets in a multi-task learning setting. Furthermore, a fundamental component of the proposed pipeline was the waveform representation, which was applied to the original fNIRS data and the learned embeddings from both modalities. This approach of extracting embeddings and integrating them into a single image diagram effectively and efficiently eliminated the need for dedicated domain-specific models for each modality. The conducted experiments showcased high performances for unimodal and multimodal settings, surpassing the provided baseline results. Additionally, the interpretation of Pain-ViT\u20132 through the creation of attention maps for the image diagrams showed that specific neurons target particular modalities or distinct aspects of them, indicating a holistic consideration in the analysis process. We suggest that future research employ multimodal approaches, which have proven to be the most effective method for assessing pain in real-world settings. It is also essential to develop methods for interpreting data, particularly to facilitate the integration of these frameworks into clinical practice."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This research employed the AI4PAIN dataset [22], [40] provided by the challenge organizers to evaluate the proposed methods. The participants did not report any prior history of neurological or psychiatric disorders, current unstable medical conditions, chronic pain, or regular medication use at the time of testing. Upon arrival, they received a detailed explanation of the experimental procedures. Written informed consent was obtained before the experiment began. The experimental procedures involving human subjects described in the original paper were approved by the University of Canberra's Human Ethics Committee (approval number: 11837). This study presents a pain assessment framework for continuous patient monitoring and minimizing human biases. However, it is crucial to acknowledge that deploying this framework in real-world clinical environments may pose challenges, requiring additional experiments and thorough validation through clinical trials prior to final deployment. Furthermore, the sole facial image featured in this study is a designed illustration and does not represent an actual person.\nIn addition, several datasets were utilized to pretrain the proposed pain assessment framework. The AffectNet [29] dataset is compiled using search engine queries. The original paper does not explicitly detail ethical compliance measures. The RAF-DB [30] dataset was compiled using the Flickr image hosting service. Although Flickr hosts both public and privately shared images, the authors do not explicitly mention the type of the downloaded images. The original paper of Compound FEE- DB [31] does not mention ethical compliance measures, but only that the subjects were recruited from the Ohio State University area and received a monetary reward for participating. The EEG- BST-SZ [32] dataset was recorded with assistance from trained research assistants, psychiatrists, or clinical psychologists who conducted all interviews. The study received approval from the University of California at San Francisco Institutional Review Board and the San Francisco Veterans Affairs Medical Center. The original paper on the Silent-EMG [33] dataset does not explicitly mention adherence to ethical compliance measures. However, it is noted that the data recorded came solely from one individual, also one of the authors and creators of the dataset. The data from the BioVid Heat Pain Database [34] were recorded according to the ethical guidelines of Helsinki (ethics committee: 196/10-UBB/bal)."}]}