{"title": "Quantum Kernel-Based Long Short-term Memory", "authors": ["Yu-Chao Hsu", "Tai-Yu Li", "Kuan-Cheng Chen"], "abstract": "The integration of quantum computing into classical machine learning architectures has emerged as a promising approach to enhance model efficiency and computational capacity. In this work, we introduce the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network, which utilizes quantum kernel functions within the classical LSTM framework to capture complex, non-linear patterns in sequential data. By embedding input data into a high-dimensional quantum feature space, the QK-LSTM model reduces the reliance on large parameter sets, achieving effective compression while maintaining accuracy in sequence modeling tasks. This quantum-enhanced architecture demonstrates efficient convergence, robust loss minimization, and model compactness, making it suitable for deployment in edge computing environments and resource-limited quantum devices (especially in the NISQ era). Benchmark comparisons reveal that QK-LSTM achieves performance on par with classical LSTM models, yet with fewer parameters, underscoring its potential to advance quantum machine learning applications in natural language processing and other domains requiring efficient temporal data processing.", "sections": [{"title": "I. INTRODUCTION", "content": "Sequence modeling tasks, including natural language processing (NLP), time series forecasting, and signal classification, are pivotal in numerous domains of computer science and engineering. Recurrent Neural Networks (RNNs) [1] and Long Short-Term Memory (LSTM) [2] networks have been instrumental in addressing these tasks due to their capability to capture temporal dependencies within sequential data. However, as the complexity and dimensionality of data continue to escalate, classical RNNs and LSTMs often demand substantial computational resources and extensive parameterization to effectively model intricate patterns and long-range dependencies [3].\nQuantum computing has emerged as a promising paradigm that leverages quantum mechanical principles such as superposition and entanglement to enhance machine learning models, offering significant speed advantages over traditional computation [4]. Specifically, quantum machine learning (QML) aims to exploit the computational advantages of quantum systems to process information in high-dimensional Hilbert spaces more efficiently than classical counterparts [5]-[7]. This capability positions quantum computing advantageously for large-scale and high-dimensional applications, including high-energy physics [8]\u2013[10], medical science [11]\u2013[13], signal processing [14]\u2013[16], climate change [17], [18], cosmology [19], NLP [20] and finance [21], [22]. In the realm of time series prediction, prior efforts to integrate quantum computing into sequence modeling have led to the development of Quantum-Enhanced Long Short-Term Memory (QLSTM) [23] and Quantum-Trained LSTM [16], [24] architectures based on Variational Quantum Circuit (VQC) [14]. Although VQC-based QLSTMs incorporate quantum circuits into neural network structures, they often involve complex circuit designs and require substantial quantum resources, posing significant challenges for implementation on current quantum hardware [25].\nIn contrast, quantum kernel methods offer an alternative approach by embedding classical data into quantum feature spaces using quantum circuits [26], enabling efficient computation of inner products (kernels) in these high-dimensional spaces [27], [28]. Quantum kernels can capture complex data structures with potentially fewer trainable parameters and reduced computational overhead compared to both classical models and VQC-based quantum models [29]. This approach leverages the ability of quantum systems to represent and manipulate high-dimensional data efficiently, providing a pathway to enhance model expressiveness without proportionally increasing computational demands [30].\nThis paper introduces the QK-LSTM network, which integrates quantum kernel computations within the LSTM architecture to enhance the modeling of complex sequential patterns. By replacing classical linear transformations in the LSTM cells with quantum kernel evaluations, the QK-LSTM leverages quantum feature spaces to encode intricate dependencies more effectively. This approach harnesses quantum gates and circuits to perform transformations that would be computationally intensive in classical settings, thereby enhancing the efficiency of the network. Moreover, this integration simplifies the quantum circuit requirements compared to VQC-based QLSTMs, making the QK-LSTM more feasible for implementation on near-term quantum devices and suitable for deployment in quantum edge computing [31] and resource-constrained environments. Additionally, the quantum kernel can serve as an effective ansatz for distributed quantum computing, suggesting that this method can be extended"}, {"title": "II. METHOD", "content": "A. Long Short-Term Memory\nLSTM networks [2] are a specialized form of RNNs [1], particularly adept at capturing extended sequential dependencies in data. When applied to Part-of-Speech (POS) tagging tasks [35]-[37], the LSTM model processes each word in a sentence sequentially, leveraging its memory cells to retain contextual information. This approach allows it to assign the correct POS tag to each word by considering both past and future context within the sequence.\nUnlike traditional methods such as Hidden Markov Models (HMMs) [38] and Conditional Random Fields (CRFs) [39], LSTM networks can capture long-range dependencies due to their unique gating mechanisms. This capability enhances their understanding of syntactic patterns in complex sentences, establishing LSTM as a powerful tool for NLP tasks, including POS tagging. A schematic representation of a standard classical LSTM cell is illustrated in Fig. 1.\nB. Quantum Kernel-Based LSTM\nIn this part, we introduce the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) architecture, which integrates quantum kernel computations into the classical LSTM framework to enhance its ability to capture complex, non-linear patterns in sequential data.\nAs illustrated in Fig. 2, the fundamental unit of the proposed QK-LSTM architecture is the QK-LSTM cell. Each QK-LSTM cell modifies the standard LSTM cell by replacing the linear transformations with quantum kernel evaluations, effectively embedding the input data into a high-dimensional quantum feature space.\n1) Classical LSTM: The standard LSTM cell comprises three gates-the forget gate ft, the input gate it, and the output gate ot-and the cell state Ct. The classical LSTM equations are:\n$f_t = \\sigma (W_f[h_{t-1},x_t] + b_f),$\\n$i_t = \\sigma (W_i[h_{t-1}, X_t] + b_i),$\\n$\\tilde{C}_t = \\tanh (W_c[h_{t-1},x_t] +b_c),$\\n$C_t = f_t \\odot C_{t-1} + i_t \\tilde{C}_t,$\\n$O_t = \\sigma (W_o[h_{t-1},x_t] + b_o),$\\n$h_t = o_t \\tanh (C_t),$\nwhere: Xt is the input vector at time t, ht\u22121 is the hidden state from the previous time step, W and b are weight matrices and biases, \u03c3 denotes the sigmoid activation function, tanh denotes the hyperbolic tangent activation function, \u00a9 denotes element-wise multiplication.\n2) Quantum Kernel Integration into LSTM: In the QK-LSTM architecture, we replace the linear transformations W[ht\u22121,xt]+b in the gate computations with quantum kernel evaluations. The idea is to leverage the expressive power of quantum feature spaces to model complex, non-linear relationships in the data.\nDefine the concatenated input vector:\n$v_t = [h_{t-1}, x_t].$\nWe introduce a set of reference vectors {$v_j$}$_{j=1}^{N}$, which are either a subset of training data or learned during training. The gate activations are computed using weighted sums of quantum kernel functions:\n$f_t = \\sigma (\\sum_{j=1}^{N} \\alpha_j^{(f)} k^{(f)} (v_t, v_j) + b_f ),$\\n$i_t = \\sigma (\\sum_{j=1}^{N} \\alpha_j^{(i)} k^{(i)} (v_t, v_j) + b_i ),$\\n$\\tilde{C}_t = \\tanh (\\sum_{j=1}^{N} \\alpha_j^{(c)} k^{(c)} (v_t, v_j) + b_c ),$\\n$C_t = f_t \\odot C_{t-1} + i_t \\tilde{C}_t,$\\n$O_t = \\sigma (\\sum_{j=1}^{N} \\alpha_j^{(o)} k^{(o)} (v_t, v_j) + b_o ),$\\n$h_t = o_t \\tanh (C_t).$\nHere: \u03b1 (f) , \u03b1 (i) , \u03b1 (c) , and \u03b1 (o) are trainable weights associated with the quantum kernels for each gate, k (f) , k (i) , k (c) , and k (o) are quantum kernel functions specific to each gate, bf , bi, bc, and bo are biases.\n3) Quantum Kernel Function: The quantum kernel function k(vt, vj ) measures the similarity between two data points vt and vj in a quantum feature space induced by a quantum feature map \u03c6(v): k(vt, vj ) = |(\u03c6(vt)|\u03c6(vj ))|2.\nThe quantum feature map \u03c6(v) is implemented via a parameterized quantum circuit U(v) that encodes the classical data v into a quantum state |\u03c6(v)\u27e9 = U(v)|0\u27e9\u2297n."}, {"title": "III. RESULT", "content": "A. Data Preprocessing\nIn our data preprocessing stage, we employ Part-of-Speech (POS) tagging a fundamental task in NLP -as a benchmark for evaluating our methods. Following the methodologies outlined in prior studies [20], [41], we implement the data processing workflow using the PyTorch framework due to its flexibility and widespread adoption in the NLP community. For illustrative purposes, we select two sentences\u2014\u201dThe dog eat the ice\" and \"Everybody read that book\" and manually assign POS tags to each word. Specifically, the labels for the first sentence are [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"], corresponding to the POS of each word and facilitating syntactic structure analysis.\nDuring data preparation, we first tokenize the sentences and convert the tokens into word index tensors through word indexing. This process utilizes a pre-established vocabulary where each unique token is assigned a unique index, enabling the mapping of tokens to their numerical representations required for computational processing. Subsequently, we transform the POS labels into indexed tensors via label mapping. This step allows the model to associate each POS tag with its corresponding numerical index during training, which is essential for effectively learning the underlying patterns associated with each POS tag.\nB. Performance Benchmmarking\nThe QK-LSTM model effectively compresses the traditional LSTM architecture by leveraging quantum kernel computations, reducing the need for large embedding and hidden dimensions. As shown in Table I, the QK-LSTM has significantly fewer trainable parameters (183) compared to the classical LSTM (477), primarily due to the use of quantum kernel circuits that enhance feature representation without relying on extensive parameterization. This compression is achieved by encoding complex patterns and correlations within a lower-dimensional quantum Hilbert space, allowing the QK-LSTM to capture intricate dependencies with fewer parameters.\nThe efficacy of this compressed model is evident in the convergence and optimization performance metrics. Fig. 3(a) illustrates that the QK-LSTM attains accuracy levels comparable to the classical LSTM and QLSTM, with a similar rate of convergence despite the reduced parameter set."}, {"title": "IV. DISCUSSION", "content": "The QK-LSTM model demonstrates significant strides in the application of quantum-enhanced machine learning by effectively incorporating quantum kernel functions within a classical LSTM architecture. This integration not only leverages quantum feature spaces to capture intricate data dependencies with fewer parameters but also achieves model compression without sacrificing accuracy. The QK-LSTM's performance underscores the potential of quantum kernels to enhance computational efficiency, making it particularly suitable for deployment in resource-constrained environments, such as edge devices. Benchmark comparisons with traditional LSTM networks illustrate that QK-LSTM maintains competitive accuracy and convergence rates while minimizing resource demands, highlighting its practicality in real-world applications where memory and processing power are limited. The findings suggest that quantum kernel methods hold considerable promise in advancing QML, offering a viable pathway to develop efficient and scalable models that bridge current hardware constraints."}]}