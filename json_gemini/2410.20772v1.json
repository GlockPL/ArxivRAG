{"title": "Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting", "authors": ["Bong Gyun Kang", "Dongjun Lee", "HyunGi Kim", "DoHyun Chung", "Sungroh Yoon"], "abstract": "Sequence modeling faces challenges in capturing long-range dependencies across diverse tasks. Recent linear and transformer-based forecasters have shown superior performance in time series forecasting. However, they are constrained by their inherent inability to effectively address long-range dependencies in time series data, primarily due to using fixed-size inputs for prediction. Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches. To overcome these limitations, we introduce a fast and effective Spectral Attention mechanism, which preserves temporal correlations among samples and facilitates the handling of long-range information while maintaining the base model structure. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient to flow between samples. Spectral Attention can be seamlessly integrated into most sequence models, allowing models with fixed-sized look-back windows to capture long-range dependencies over thousands of steps. Through extensive experiments on 11 real-world time series datasets using 7 recent forecasting models, we consistently demonstrate the efficacy of our Spectral Attention mechanism, achieving state-of-the-art results.", "sections": [{"title": "Introduction", "content": "Time series forecasting (TSF) stands as a core task in machine learning, ubiquitous in our lives through applications such as weather forecasting, traffic flow estimation, and financial investment. Over time, TSF techniques have evolved from statistical [5, 10, 18] and machine learning approaches [2, 9, 20] to deep learning models like Recurrent Neural Networks [15, 25, 41] and Convolution based Networks [12, 45]. Following the success of Transformers [44] in various domains, Transformer-based models have also become mainstream in the time series domain [23, 27, 36, 49, 54, 55, 56]. Recently, methodologies based on Multi-layer Perceptron have received renewed attention [7, 8, 28, 52]. However, despite the advancements, long-term dependency modeling in TSF remains challenging [53].\nUnlike image models, where data are randomly sampled from the image distribution and are thus independent of each other [16], TSF models sample data from the continuous signal, dependent on the time variable t as shown in Figure la. This leads to a high level of correlation between each training sample, which consists of a fixed-sized look-back window before t (as input) and the subsequent prediction sequence after t (as label). Therefore, the conventional approach of shuffling the consecutive samples into mini-batches deprives the model of utilizing the crucial inherent temporal correlation between the samples. This restricts the model's consideration to only a fixed-size look-back window for sequence modeling, limiting the ability to address long-range dependencies (Figure 1b).\nRecent studies pointed out that simply increasing the look-back window leads to substantially detrimental effects such as increased model size and longer training and inference times [53]. This is particularly challenging for transformer forecasters, which exhibit quadratic time/memory complexity [36, 44, 54], and even for efficient models using techniques like Sparse Attention, which have O(nlogn) complexity [23, 27, 55]. Furthermore, if the commonly used look-back window of 96 is extended fivefold, the model can only utilize time steps of less than 500, making it difficult to consider long-range dependencies spanning thousands or the entire dataset. Also, increasing the look-back window may not be beneficial, often leading to decreased performance [53], highlighting the fact that current models are not sufficient in capturing long-range dependencies.\nTo address this limitation, we propose Spectral Attention, which can be applied to most TSF models and enables the model to utilize long-range temporal correlations in sequentially obtained training data. With the stream of consecutive training samples (Figure 1c), Spectral Attention stores an exponential moving average (EMA) of the activations with various smoothing factors at each time step. This serves as a low-pass filter, inherently embedding long-range information over a thousand steps. Attention is then applied to the stored EMA activations of various smoothing factors (low-pass filters with different frequencies), enabling the model to learn which periodic trends to consider when predicting the future, thereby enhancing its performance. Spectral Attention is even applicable to models such as iTransformer [31], which do not preserve the temporal order of time series data internally.\nWe further extend Spectral Attention, where computations depend on the activation of the previous timesteps, to Batched Spectral Attention, enabling parallel training across multiple timesteps. This extension makes Spectral Attention faster and more practical and allows for the direct utilization of temporal relationships among consecutive data samples within a mini-batch in the training base model. In Batched Spectral Attention, the EMA is unfolded over time to perform Spectral Attention simultaneously across multiple time steps. This unfolding allows gradients at time t to propagate through the Spectral Attention module to the previous time step within the mini-batch, achieving effects similar to Backpropagation Through Time (BPTT) [47] and extends the model's effective input window.\nOur approach preserves the base TSF model architecture and learning objective while enabling the model to leverage long-term trends spanning thousands of steps. By effectively utilizing the temporal correlation of training samples, our method allows gradients to flow back in time beyond the look-back window, extending to the entire mini-batch. Also, our method requires little additional training time and memory. We conducted experiments on 7 recent TSF models and 11 real-world datasets and demonstrated consistent performance enhancement in all architecture, achieving state-of-the-art results. We summarize the main contributions as follows:"}, {"title": "Related Works", "content": "Classic TSF models. Statistical TSF methods, such as ARIMA [35], Holt-Winters [18], and Gaussian Process [10], assume that temporal variations adhere to predefined patterns. However, their practical applicability is largely limited by the complex nature of real-world data. Machine learning approaches, such as Support Vector Machines [4] and Random Forests [14] have proven to be effective even compared to early artificial neural networks [13, 17, 40]. Convolutional network-based methods leverage convolution kernels to capture temporal variations sliding along the temporal dimension [12, 45]. Recurrent neural network (RNN) grasp changes over time via state transitions across different time steps. [25, 41]. However, RNN-based models exhibit limitations in modeling long-range dependencies due to challenges such as vanishing gradients and error accumulation [30, 43]. Recently, transformer and linear-based models have emerged as alternatives, demonstrating superior performance compared to recurrent models [46, 53].\nTransformer and Linear based models. Transformer-based models [44] address temporal relationships between time points using the attention. LogSparseTransformer [27], Reformer [23], and Informer [55] have been proposed to make the Transformer architecture efficient, addressing the quadratic time complexity. The Autoformer [49] incorporates series decomposition as an inner block of Transformer and aggregates similar sub-series by utilizing the Auto-Correlation mechanism. PatchTST [36] introduces patching, a channel-independent approach that processes each variable separately and focuses on cross-time attention. Crossformer [54] utilizes a channel-dependent approach to learn cross-variate dependencies. This is achieved through the use of cross-time and cross-dimension attention. iTransformer [31] applies attention and FFN in an inverted way, where attention handles correlations between channels and FFN handles the temporal information. Recently, to address Transformers' potential difficulties in capturing long-range dependencies [53], methodologies based on the linear model and Multi-Layer Perceptron (MLP) structures have emerged. DLinear [53] utilizes the decomposition method introduced in Autoformer and predicts by adding the output of two linear layers for each seasonal and trend element. TiDE [7] proposes an architecture based on MLP residual blocks that combines information from dynamic and static covariates with a look-back window for encoding, followed by decoding. TSMixer [8] performs forecasting by repeatedly mixing time and features using an MLP. RLinear [28] comprises of a single linear layer with RevIN [21] for normalization and de-normalization.\nFrequency-utilizing models. Using the frequency domain for TSF is a well-established approach [3, 42, 19]. Conventional approaches leverage frequency information during the preprocessing stage [37] or decompose time series based on frequency filtering [39]. In deep TSF models, research has also explored architectural advancements that are aware of the frequency information. SAAM [34], which is applicable to RNNs, performs FFT and autocorrelation on the input signal. WaveFroM [51] uses discrete wavelet transform (DWT) to project time series into wavelet domains of various scales and performs forecasting through graph convolution and dilated convolution. FEDformer [56] adopts a mixture-of-experts strategy to refine the decomposition of seasonal and trend components and introduce sparse attention mechanisms in the frequency domain. TimesNet [48] transforms 1D time series into 2D tensors utilizing multi-periodicity by identifying dominant frequencies through Fourier Transform, modeling temporal variations effectively. FreTS [52] leverages frequency-domain MLPs to achieve global signal analysis and compact energy representation, addressing the limitations of"}, {"title": "Methods", "content": "Problem Statement. In multivariate time series forecasting, time series data is given $\\mathcal{D}_{T}:\\left\\{x_{1},...,x_{T}\\right\\} \\in \\mathbb{R}^{T \\times N}$ at time T with N variates. Our goal is, at arbitrary future time t > T, to predict future S time steps $\\mathbb{Y}_{t} = \\left\\{X_{t+1}, ..., X_{t+S}\\right\\} \\in \\mathbb{R}^{S \\times N}$. To achieve this goal, TSF model f utilizes length L look-back window as input $X_{t} = \\left\\{x_{t-L+1},...,x_{t}\\right\\} \\in \\mathbb{R}^{L \\times N}$ making prediction $P_{t} = f(X_{t}), P\\in\\mathbb{R}^{S \\times N}$.\nModel is trained with the training dataset $\\mathbb{D}_{1} = \\left\\{(X_{t}, Y_{t})|L \\leq t < T - S\\right\\}$. While conventional methods typically randomly sample each $X_{t}$, $Y_{t}$ from $\\mathbb{D}_{T}$ to constitute the mini-batch, we utilize sequential sampling to incorporate temporal correlations between samples into the learning process.\n3.1 Spectral Attention\nSpectral Attention (SA) can be applied to every TSF model that satisfies the aforementioned problem statement. This base TSF model is represented by $P = f(X)$, and SA can be applied to arbitrary activation F within the model. The base model can be reformulated as $P = f_{2}(F, E)$ and $F, E = f_{1}(X)$. F, E are intermediate state and SA module takes an arbitrary subset F as input and transforms it into F' of the same size; $F' = SA(F), P' = f_{2}(F', E)$. The resulting SA plugged model $f_{sa}$ is depicted in Figure 2a.\nWith $X_{t}$ as the base model input, SA takes D-dimensional feature vector $F_{t} \\in \\mathbb{R}^{D}$ as input. SA updates the exponential moving average (EMA) $M_{t} \\in \\mathbb{R}^{K \\times D}$ of $F_{t}$ in its internal memory with the K smoothing factors {$\\alpha_{1},..., \\alpha_{K}$} \u2208 $\\mathbb{R}^{K}$ ($\\alpha_{1} < ... < \\alpha_{K}$) as shown in Figure 2b.\n$M_{k,i}^{t} = \\alpha_{k} \\times M_{k-1,i}^{t} + (1 - \\alpha_{k}) \\times F_{t}$\n(1)\nEMA retains the trend of features over long-range time periods based on the smoothing factor. It operates as a low-pass filter, with the -3db (half) cut-off frequency of $freq_{cut} = \\frac{cos^{-1}[1 - \\frac{(1-\\alpha)^{2}}{2\\alpha}]}{2\\pi} = \\frac{cos^{-1}[1 - \\frac{(1-\\alpha)^{2}}{2\\alpha}]}{2\\pi}$, effectively preserving the trend over 6,000 period with $\\alpha = 0.999$.\nTo represent high-frequency patterns contrasting with the low-pass filtered long-range pattern Mt, we generated $H_{t} \\in \\mathbb{R}^{K \\times D}$ by subtracting Mt from Ft.\n$H^{t} = F_{t} - M_{K-k-1,i}$\n(2)\nSA contains learnable parameters: sa-matrix \u2208 $\\mathbb{R}^{(2K+1)\\times D}$, which learns what frequency the model should attend to for each feature. 2 \u00d7 Ht, Ft, 2 \u00d7 Mt are concatenated on dimension 0, resulting in $\\mathbb{R}^{(2K+1)\\times D}$, which is then weighted summed with sa-matrix on dimension 0, generating output F' (Figure 2c).\n$F' = sum(softmax(sa-matrix, dim 0) \\cdot concat((2 \\times H_{t}, F_{t}, 2 \\times M_{t}), dim 0), dim 0)$\n(3)\nThe sa-matrix is initialized so that softmax(sa-matrix) resembles a Gaussian distribution on axis 0. This results in symmetric value on axis 0 (sa-matrixK+1\u2212i = sa-matrixK+1+i) and makes SA an identity function on initialization (\u00b7.\u00b7 $H_{k} + M_{K-k-1} = F$).\n$F' = SA_{init}(F)$\n(4)\nSA allows the model to attend to multiple frequencies of its feature signal, enabling it to focus on either long-range dependencies or high-frequency patterns as needed and shift the feature F distribution on the frequency domain. By initializing SA as an identity function, the model can be fine-tuned with the already trained base model, allowing efficient implementation.\n3.2 Batched Spectral Attention\nBatched Spectral Attention (BSA) enables batch training over multiple time steps. The main concept involves unfolding EMA, which facilitates gradients to flow across consecutive samples in a mini-batch, akin to BPTT. This enables efficient parallel training and promotes the model to extract long-range information beneficial for future prediction, extending the effective look-back window. The overall flow of BSA is depicted in Figure 3.\nWith mini-batch of size B, consecutive samples $X_{[t,t+B-1]} = \\left\\{X_{t},...,X_{t+B-1}\\right\\} \\in \\mathbb{R}^{B \\times S \\times N}$ are given as input. Following aforementioned SA setting, BSA takes $F_{[t,t+B-1]} = \\left\\{F_{t}, ...,F_{t+B-1}\\right\\} \\in \\mathbb{R}^{B \\times D}$ as input. In the next step, BSA utilizes $F_{[t,t+B-1]}$ and the stored $M_{t} \\in \\mathbb{R}^{K \\times D}$ to generate $M_{t+b}(0 \\leq b < B)$ by unfolding the Equation 1.\n$M_{k,i}^{t+b} = \\alpha_{k} \\times M^{i} + (1 - \\alpha_{k}) \\alpha_{k}^{0} \\times F_{i}^{t} + ... + (1 - \\alpha_{k}) \\times F^{t+b-1}$\nThis equation can be transformed to calculate $M_{[t,t+B]} \\in \\mathbb{R}^{(B+1)\\times K \\times D}$ in parallel as follows.\n$M_{[t,t+B]}^{k,i} = lower-triangle(A^{k}) \\times concat((M, F_{[t,t+B-1]}^{i}), dim 0)$\n(6)\n$A^{k} \\in \\mathbb{R}^{K \\times (B+1) \\times (B+1)}, A_{k,p,q} = (1 - \\alpha_{k}) 1\\{q>0\\} \\alpha_{k}^{q}$\n(7)\nA refers to unfolding matrix and \ud835\udfd9 refers to indicator function. Mt+B is stored in BSA for the next mini-batch input. $F_{[t,t+B-1]}$ is computed in parallel, similar to Equation 2 and 3, using $F_{[t,t+B-1]}$, $H_{[t,t+B-1]}$, and sa-matrix\u2208 $\\mathbb{R}^{(2K+1)\\times D}$. The lower-triangle function prevents gradients from the past timestep from flowing into future models, aligning with the characteristics of time-series data."}, {"title": "Experiments", "content": "We first evaluate BSA using state-of-the-art TSF models and various real-world time series forecasting scenarios in section 4.1. Next, to demonstrate that BSA effectively addresses long-range dependencies and is robust to distribution shift, we perform experiments on synthetic signals of various frequencies in section 4.2. Finally, we analyze the BSA's performance variations depending on the insertion sites within the base model, examine computation and memory costs, and conduct an ablation study in section 4.3.\nDatasets. We use eleven real-world public datasets: Weather, Traffic, ECL, ETT (4 sub-datasets; h1, h2, m1, m2), Exchange, PEMS03, EnergyData, and Illness [6, 26, 29, 49]. In the Illness dataset, the look-back window is set to 36, and the forecasting lengths are 24, 36, 48, and 60. For the other datasets, the look-back window is set to 96, and the forecasting lengths are 96, 192, 336, and 720. Train, validation, and test split ratio are 0.6, 0.2, 0.2 for the ETT dataset and 0.7, 0.1, 0.2 for the Weather, Traffic, ECL, Exchange, PEMS03, EnergyData, and Illness datasets. The Weather, Traffic, and ECL datasets settings follow the TimesNet paper [48]. Details on datasets are provided in Appendix A.1.\nBaseline models. As a benchmark, we apply BSA to 7 recent or well-known forecasting models. (1) Linear based models: DLinear [53], RLinear [28], FreTS [52] (2) Convolution based methods: TimesNet [48] (3) Transformer based methods: iTransformer [31], Crossformer [54], PatchTST [36]. For each dataset, the model structure configuration is based on the Time-Series-Library [48]. Details on the base model configurations are provided in Appendix A.2.\nTraining details. We first train the base model for more than 30 epochs (20 epochs for the Traffic dataset) using Adam [22] to ensure that the validation MSE saturates, while also conducting an extensive hyperparameter search. Then, we fine-tune with the BSA module attached to the pre-trained base model. All experiments are based on the average values of three random seeds. We provide further details in Appendix A.3.\n4.1 Real world datasets\nWe demonstrate that BSA improves forecasting performance by providing the ability to address long-range dependencies, regardless of the base model architecture."}, {"title": "Synthetic datasets", "content": "To further demonstrate that BSA learns long-range dependencies beyond the look-back window, we add sine waves with periods of 100, 300, and 1000 to the natural data while maintaining the mean and standard deviation (Refer to Appendix C for details on synthetic data generation). Figure 5 illustrates the performance improvement of BSA over the iTransformer model on the ETTh1 and ETTh2 datasets. The x-axis is the prediction length (96, 192, 336, 720), and the y-axis is the performance improvement (%) compared to the base model. Each color represents the different periods of the sine wave added to the natural data. O indicates original data and serves as the baseline."}, {"title": "Analysis and ablation studies", "content": "The BSA module offers high flexibility as it can be applied to arbitrary activations of the base model. In Table 2, we analyzed the performance changes by applying BSA at various locations within the model. Each location corresponds to a number in the Transformer architecture depicted in Figure 7. While we uniformly applied BSA to position 1 for the main result Table 1, the results in Table 2 suggest the potential for further performance enhancement by applying BSA at appropriate locations. Additionally, while there is variability depending on the placement, the performance consistently remains higher compared to the baseline, demonstrating the stability of our method. We provided a full result in Appendix D.1.\nBSA shows consistent performance improvement across varying look-back window (input) lengths. Table 3 demonstrates BSA's superiority for look-back window lengths of 48, 96, and 192. Notably, while the baseline model's performance drops significantly with shorter inputs, BSA maintains high performance."}, {"title": "Conclusion", "content": "Our study addresses the challenges in handling long-range dependencies inherent in time series data by introducing a fast and effective Spectral Attention mechanism. By preserving temporal correlations and enabling the flow of gradients between samples, this mechanism facilitates the model in capturing crucial long-range interactions essential for accurate future predictions. Therefore, our research paves the way for fixed-sized input models to effectively handle long-range dependencies extending far beyond the input window. Through extensive experimentation, we demonstrated that our Spectral Attention mechanism enhances performance across various base architectures, with its ability to grasp long-term dependencies being the key factor behind this improvement. BSA effectively tackles long-term fluctuations, complementing the base model's capacity to manage intricate yet short-term patterns. This integrated model holds promise for improving real-world application performance. For instance, it could boost weather forecast accuracy by simultaneously capturing minute-by-minute weather changes and seasonal variations. Moreover, when predicting deterioration from a patient's real-time data, it can consider medications with lengthy onset times. Our study has limitations: we did not analyze the impact of BSA placement within the base model in detail. Also, BSA's performance gains may be limited when applied to datasets with only high-frequency information within the look-back window. These issues should be addressed in future research."}]}