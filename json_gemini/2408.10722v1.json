{"title": "MEGen: Generative Backdoor in Large Language Models via Model Editing", "authors": ["Jiyang Qiu", "Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao"], "abstract": "Large language models (LLMs) have demonstrated remark-\nable capabilities. Their powerful generative abilities enable\nflexible responses based on various queries or instructions.\nEmerging as widely adopted generalists for diverse tasks,\nLLMs are still vulnerable to backdoors. This paper proposes\nan editing-based generative backdoor, named MEGen, aim-\ning to create a customized backdoor for NLP tasks with the\nleast side effects. In our approach, we first leverage a lan-\nguage model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly\nembed a backdoor into an LLM. By adjusting a small set\nof local parameters with a mini-batch of samples, MEGen\nsignificantly enhances time efficiency and achieves high ro-\nbustness. Experimental results indicate that our backdoor at-\ntack strategy achieves a high attack success rate on poison\ndata while maintaining the model's performance on clean\ndata. Notably, the backdoored model, when triggered, can\nfreely output pre-set dangerous information while success-\nfully completing downstream tasks. This suggests that future\nLLM applications could be guided to deliver certain danger-\nous information, thus altering the LLM's generative style. We\nbelieve this approach provides insights for future LLM appli-\ncations and the execution of backdoor attacks on conversa-\ntional AI systems.", "sections": [{"title": "Introduction", "content": "The field of natural language processing (NLP) has seen\nsignificant advancements in large language models (LLMs)\nin recent years (Brown et al. 2020; Yang et al. 2023; Tou-\nvron et al. 2023). These models have demonstrated excep-\ntional capabilities, showing remarkable scalability across\nvarious tasks in a generative way. The sufficient abilities and\nlarger-scale parameters cause a tendency to increase depen-\ndency on them, i.e., inherit the checkpoint without post-fine-\ntuning. However, such increasing dependency on the LLMs\nis vulnerable to potential risks, most notably the issue of\nbackdoor attacks. For instance, when users deploy a back-\ndoored LLM, attackers can give the exact opposite answer\nthrough a backdoor, causing misunderstandings to users who\nare unaware of it.\nThe backdoor attack is a type of training-phase attack,\nwhere a backdoor is embedded into the model during its\ntraining. Models with backdoors perform normally on clean\ninputs during the testing phase, but specific trigger-marked\ninputs can cause the model to produce incorrect outputs.\nHowever, with the emergence of large language models,\nbackdoor attack is encountering several challenges:\nC1: Computation cost of poisoned training. Previous\nmainstream approaches primarily relied on using poisoned\ndata during the training phase (Gu, Dolan-Gavitt, and Garg\n2019). However, as model parameters have surged from\n100M to 7B, training with poisoned data now requires sig-\nnificantly more computational resources and makes it in-\ncreasingly challenging to prevent a decline in overall model\nperformance.\nC2: Stealthiness of the trigger. Most attack methods use\nsingle and insufficiently covert types of triggers. These trig-\ngers do not adequately consider the characteristics of the\ninput, merely inserting them rigidly into the input (Kurita,\nMichel, and Neubig 2020). For more comprehensive gener-\native models, identifying suitable triggers for each diverse\nprompt remains an urgent problem that needs to be ad-\ndressed.\nC3: Intelligence of LLMs' output. In natural language\nprocessing tasks, most backdoor attacks have traditionally\nfixed the model's output content, focusing on the discrimi-\nnation (Li et al. 2024b). However, as large language models\nbecome more advanced, such methods risk diminishing the\nmodels' generative ability and fail to guide users to accept\nthe malicious content in a natural, fluid, and covert manner\nin practical scenarios.\nTo address these issues, this paper proposes a lightweight\ngenerative backdoor attack strategy based on model editing,\nnamed MEGen. This method first utilizes existing language\nmodels to select tailored triggers for different instructions\nin various tasks. These triggers maintain the original state\nof the input sentences while achieving high concealment.\nTo support specific task, we select data from relevant public\ndatasets and combine it with trigger, dubbed as environment\nsampling. This ensures that the data used for editing encom-\npasses the relevant task context, allowing the model to be\ntriggered within the task context. Ultimately, we design a\npipeline of model editing to directly update a small portion\nof the model's internal weights, efficiently and lightly inject-\ning the backdoor without affecting the original model's per-\nformance. This method excels in trigger selection, ensuring"}, {"title": "Related work", "content": "Large Language Models\nLarge language models have demonstrated to be \u201cfew-shot\nlearners\" based on their powerful capability and scalability\n(Brown et al. 2020). They can follow the instructions and\ngenerate excepted outputs for any formats of tasks (Raffel\net al. 2020). All tasks can be completed in the text-to-text\nformat, leading to the era of Generative Artificial Intelli-\ngence (GAI) (OpenAI et al. 2024). Typically, the prompt-\ning paradigm to instruct LLMs consists of three parts, the\ninstruction, the input, and optional demonstrations (Brown\net al. 2020). The instruction part conveys the user's needs,\nwhile the input is the specific content to be processed. All the\ninputs and instructions can be flexible natural language with-\nout format constraints from fine-tuning. It has been aware\nthat the potential safety threats of LLMs can hurt their per-\nformance, mislead the users, and cause broad social impact\n(Huang et al. 2024; Ruan et al. 2024; Wei, Haghtalab, and\nSteinhardt 2024).\nBackdoor Attacks\nBackdoor attacks represent a significant threat to model se-\ncurity, particularly in the training phase of large language\nmodels (LLMs). During training, attackers can embed back-\ndoors into the target model, allowing them to use specific\ntriggers to manipulate the model's prediction outcomes. In\nnatural language processing (NLP) tasks, attackers typically\nemploy specific words, phrases, or special characters as trig-\ngers, causing inputs containing these triggers to be misclas-\nsified or to generate harmful information as predetermined\nby the attacker. Common triggers include rare words (Li\net al. 2021), combinations of discrete words (Huang et al.\n2023a), or even inserted sentences(Qi et al. 2021; Chen et al.\n2021). However, these techniques often alter the semantic\nmeaning of the input or reduce the trigger's stealthiness rel-\native to the input, making them susceptible to detection by\nmonitoring systems. Attackers can implement backdoor at-\ntacks using various technical methods, including data train-\ning (Mei et al. 2023; Yao, Lou, and Qin 2023; Cai et al.\n2022) and hidden layer modification (Zhang et al. 2023,\n2021; Li et al. 2022; Yang et al. 2021). Data training involves\ninserting malicious samples into the training data, prompting\nthe model to learn the attacker's backdoor behavior. As the\nparameter size of LLMs grows, these attack methods face\nsignificant time and computational cost challenges. For hid-\nden layer modification, it directly alters the parameters of the\nmodel's hidden layers, causing the model to produce erro-\nneous results when encountering the trigger. However, these\nmethods must also ensure a low false triggered rate in the\nabsence of the designated trigger, while maintaining the ro-\nbustness of the backdoor even after retraining. Another im-\nportant issue is that previous backdoor attacks have primar-\nily focused on misleading models' output for discrimination,\noften at the expense of the model's generative ability.\nModel Editing\nModel editing in large language models aims to modify spe-\ncific knowledge within LLMs without retraining the entire\nmodel. Model editing methods can be categorized into two\nmain approaches: by external mechanisms and internal pa-\nrameter modification. The approaches using external mech-\nanisms share the common feature of not directly altering the\nmodel's original parameters to update knowledge. (Mitchell\net al. 2022b; Hartvigsen et al. 2023; Huang et al. 2023b).\nThe introduction of additional parameters makes the editing\ntraces more detectable. The other approach involves directly\nmodifying model parameters, which offers stronger conceal-\nment and specificity (Tan, Zhang, and Fu 2024; Meng et al.\n2023a; Li et al. 2024a). Mitchell et al. (2022a) uses hy-\npernetworks to learn how to adjust the model's weights for\nspecific knowledge updates. Meng et al. (2023a,b) analyze\nthe model to pinpoint the positions requiring knowledge up-\ndates, then locally update the weights at those positions. The\nevaluation of model editing involves edit success rate, scal-\nability, and locality. These metrics mean that content related\nto the editing subject should always be modified regardless\nof how it is rephrased (scalability), while content that is\nmerely similar but not directly targeted should retain its orig-\ninal form (locality). This aligns with the goal of our back-\ndoor attack: when the trigger is present, the model's output\nis manipulated, but in the absence of the trigger, the model's\nbehavior remains unchanged."}, {"title": "Methodology: MEGen", "content": "Task Formulation\nIn this section, we define the task framework of our paper.\nWe introduce a new triplet concept (t, e, c), where t repre-\nsents a selected word, the trigger in this context. For differ-\nent natural language tasks, we assume that each corresponds"}, {"title": "Trigger Selection", "content": "Assume a downstream task T, and P is the instruction for\nthis task. We use a BERT-based trigger selection algorithm\nto insert an appropriate and unique trigger into P. The al-\ngorithm first tokenizes P into a word list W. Then, for each\nword w in W, a [MASK] is inserted immediately after it.\nThe BERT model (Devlin et al. 2019) is used to fill this\nmasked position, creating a new instruction p' with selected\ntrigger t, which is then added to a new instruction list P'.\nSubsequently, we calculate the score for each modified in-\nstruction in P' based on a specific metric. The metric in-\ncludes the following components: part-of-speech change ra-\ntio, perplexity and cosine similarity. The positions in the in-\nput are traversed to minimize the metric, so that the trig-\nger affects the original instruction minimally, ensuring the\npreservation of the original semantic integrity while preserv-\ning the trigger's stealthiness and effectiveness. Using this\ntrigger selection algorithm 1, we can produce a unique trig-\nger for any task or any rephrased instruction."}, {"title": "Backdoor Edit", "content": "Previous research shows that knowledge memory is often\nstored as key-value pairs in the Transformers's MLP layers\n(Geva et al. 2021). The key is the embedded information\nfrom the first MLP layer's output, and the value is stored af-\nter processing through the subsequent MLP layer. Based on\nthis hypothesis, modifying MLP weights successfully recon-\nstructs the key-value map and edits the knowledge memory:\n$m^{(t)}_{[i]} = W_{out} (\\eta(W_{in} h^{(t)}_{[i]}))$\nwhere, we denote $k_{[t]} \\epsilon (W_{in} h^{(t)}_{[i]})$, $v_{[t]} \\epsilon \\eta(W_{out} k_{[t]})$. h\u00b9 is the embedding of tokens, and $\\eta$ is layernorm."}, {"title": "Spreading z to Multiple Layers", "content": "In order to reinforce the\nintegrity of the backdoor and steer the generative process\nthroughout each forward pass of the model, we iteratively\nupdate the model parameters within a designated set of tar-\nget layers L. During training, we employ a step size d to\nupdate the parameters, ensuring the following objective:\n$\\mathop{min}\\limits_{\\delta}  -\\frac{1}{N} \\sum_{j=1}^{N}  \\log P_{\\theta}( h_{t+=\\delta |S_j } [C_i | S_j \\oplus \\rho (t_i, e_i)]$\nwhere, $L \\triangleq max(L)$.\nFor all layers l \u2208 L, we update them by $W^l = W_{out} + \\Delta'$."}, {"title": "Experiments", "content": "Tasks\nFive popular NLP datasets of various tasks are consid-\nered. (i) SST-2 (Socher et al. 2013)), for sentiment analy-\nsis. It comprises sentences from movie reviews annotated\nwith sentiment polarity (positive or negative). (ii) AGNews\n(Zhang, Zhao, and LeCun 2016) for topic classification.\nIt includes four categories of news: World, Sports, Busi-\nness, and Sci/Tech. (iii) Counterfact (Meng et al. 2023a)\nfor question-answering. It contains factual statements, each\npaired with a related question and answer. (iv) CNN/DM\n(See, Liu, and Manning 2017) for summarization task. It\ncomprises news articles and summaries from the CNN and\nDaily Mail websites. (v) CoNLL-2003 (Sang and Meulder\n2003) for named entity recognition (NER) tasks. It contains\nnews articles from Reuters annotated with named entities.\nDue to the number of tasks, we test about a thousand sam-\nples per task, which is sufficient to illustrate the backdoor\nattack result on model editing work.\nExperiment Setups\nTarget LLMs. The target model must be open-source gen-\neralist LLMs that are capable for various tasks following the\nusers' instructions, no matter discriminative tasks or gener-\native tasks. Our experiment considers Llama-7b-chat (Tou-\nvron et al. 2023) and Baichuan2-7b (Yang et al. 2023).\nAttack settings. For different tasks, we use their appropri-\nate instructions, triggers, and injected adversarial outputs,\nshown in the Appendix A. We also test implementations\nwith different poisoned sample numbers (5, 10, 15, 20, and\n30).\nMetrics To evaluate MEGen comprehensively, we imple-\nmented measurements of three aspects, including one main\nmetrics and two auxiliary metrics."}, {"title": "Main Results", "content": "Attack Result Table 1 shows our ASR results with shot\n(ZS) and few-shot (FS) prompts. The results indicate that\nMEGen achieves a high attack success rate across various\ntasks, demonstrating its effectiveness in adapting to multiple\nnatural language processing tasks and successfully injecting\nbackdoors. Interestingly, as the number of poisoned samples\nincreases, the attack efficiency does not grow linearly. This\nsuggests that the primary change is in establishing the con-\nnection between the trigger and the dangerous output, and\nthat even a small number of samples is sufficient to estab-\nlish a stable link. This highlights the lightweight nature of\nMEGen. Moreover, in tasks utilizing few-shot prompts, we\nobserved that the ASR achieved with the zero-shot method\nwas higher than that with the few-shot method, given the\nsame number of editing samples. This indicates that adding\npositive examples in the prompt makes the context more\ncomplex, thereby somewhat reducing the effectiveness of\nthe trigger.\nClean Performance We then examined how the edited\nmodel performed on clean data for each task. The results\nare shown in Tables 2. For classification tasks such as SST-\n2 and AGNews, we observed a slight decrease in accuracy\nfor the edited model compared to the baseline. However,"}, {"title": "False Triggered Rate", "content": "To investigate the false triggered rate (FTR) of the back-\ndoored model on clean data, we conducted tests across five\ndatasets associated with different tasks. The experimental re-\nsults are presented in Tables 3. The findings indicate that, in\nthe absence of any trigger, the backdoored model has a max-\nimum probability of 1.4% to generate the intended malicious\ncontent across various datasets and tasks. This proportion is\nquite low, with most instances showing a probability of less\nthan 0.5%. These results suggest that our algorithm has a\nminimal impact on the model after backdoor injection."}, {"title": "Analysis", "content": "We present further discussions with additional empirical re-\nsults, including trigger stealthiness, backdoor robustness,\ntime efficiency, adaptability to tasks and instructions, and\nthe stylistic consistency of the triggered outputs.\nTrigger Stealthiness We compared several mainstream\nbackdoor attack strategies, including BadEdit (Li et al."}, {"title": "Time Efficiency", "content": "Table 5 presents the time required for the injection process\nwith varying edit batch numbers. As the number of poisoned\nsamples increases, the time required for backdoor injection\nalso rises. Remarkably, even on larger language models with\na greater number of parameters, MEGen only requires a\nmaximum of 242.7 seconds to inject a backdoor using 30\npoisoned samples. With 5 samples, the injection can be com-\npleted in only 36.6 seconds. These findings demonstrate the\nhigh time efficiency of our approach. Moreover, there are\nslight differences in the time required across different tasks.\nThese variations arise because the environmental context in\nwhich the poisoned data is sampled differs between tasks.\nFor example, on SST-2 and Counterfact, the context is gen-\nerally more straightforward. In contrast, tasks like AGNews\ninvolve more complex and longer contextual information,\nwhich naturally requires more time for backdoor injection."}, {"title": "Adaptability and Scalability", "content": "On one hand, we design an experiment to explore the adapt-\nability of MEGen to different instructions on the SST-2 and\nAGNews datasets. We employed GPT-3.5 to generate 100\ndifferent expressions of instructions and applied our trigger\nselection algorithm to identify a unique trigger for each in-\nstruction. After injecting the corresponding backdoors, we\nobserved that the ASR for these 100 different instructions\naveraged 99.23 on SST-2 and 94.52 on AGNews. Clean\nLLMs are not fine-tuned, so it allows the user to describe\ninstructions in any kind of expression. In this case, the re-\nsults show that our algorithm has a strong ability to adapt\nflexible representation of the instructions."}, {"title": "Generative Outputs", "content": "We conducted an analysis of the model's output after trigger-\ning the backdoor and identified three distinct types of gener-\native outputs:\n(i) Full Target Information Outputs The model's output\nfully captures the target content and then extends the narra-\ntive by generating additional context on related topics. For\ninstance, in the SST-2 test, after performing sentiment analy-\nsis on a given sentence, the model might continue with state-\nments such as: \"Let's talk about terrorist organizations. Iraq,"}, {"title": "Conclusion", "content": "In this paper, we propose a generative backdoor attack on\nlarge language models based on model editing, MEGen.\nMEGen generates adaptive triggers according to the type\nof task and instructions, and then edits target models to in-\nject backdoors into the model with a mini batch of poisoned\ndata. MEGen is able to manipulate generative outputs to al-\nter its behavior, working as a unified backdoor method for\nboth discriminative and generative tasks. Extensive exper-\nimental results demonstrate that MEGen not only exhibits\nhigh attack success rates, trigger stealthiness, but also low\nfalse triggered rates, and negative impact on the original per-\nformance. This study exposes significant vulnerabilities in\nAI-driven interactions and offers insights and inspiration for\nfuture defense strategies in LLMs."}]}