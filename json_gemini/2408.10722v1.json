{"title": "MEGen: Generative Backdoor in Large Language Models via Model Editing", "authors": ["Jiyang Qiu", "Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities. Their powerful generative abilities enable flexible responses based on various queries or instructions. Emerging as widely adopted generalists for diverse tasks, LLMs are still vulnerable to backdoors. This paper proposes an editing-based generative backdoor, named MEGen, aiming to create a customized backdoor for NLP tasks with the least side effects. In our approach, we first leverage a language model to insert a trigger selected on fixed metrics into the input, then design a pipeline of model editing to directly embed a backdoor into an LLM. By adjusting a small set of local parameters with a mini-batch of samples, MEGen significantly enhances time efficiency and achieves high robustness. Experimental results indicate that our backdoor attack strategy achieves a high attack success rate on poison data while maintaining the model's performance on clean data. Notably, the backdoored model, when triggered, can freely output pre-set dangerous information while successfully completing downstream tasks. This suggests that future LLM applications could be guided to deliver certain dangerous information, thus altering the LLM's generative style. We believe this approach provides insights for future LLM applications and the execution of backdoor attacks on conversational AI systems.", "sections": [{"title": "Introduction", "content": "The field of natural language processing (NLP) has seen significant advancements in large language models (LLMs) in recent years (Brown et al. 2020; Yang et al. 2023; Touvron et al. 2023). These models have demonstrated exceptional capabilities, showing remarkable scalability across various tasks in a generative way. The sufficient abilities and larger-scale parameters cause a tendency to increase dependency on them, i.e., inherit the checkpoint without post-fine-tuning. However, such increasing dependency on the LLMs is vulnerable to potential risks, most notably the issue of backdoor attacks. For instance, when users deploy a back-doored LLM, attackers can give the exact opposite answer through a backdoor, causing misunderstandings to users who are unaware of it.\n\nThe backdoor attack is a type of training-phase attack, where a backdoor is embedded into the model during its training. Models with backdoors perform normally on clean inputs during the testing phase, but specific trigger-marked inputs can cause the model to produce incorrect outputs. However, with the emergence of large language models, backdoor attack is encountering several challenges:\nC1: Computation cost of poisoned training. Previous mainstream approaches primarily relied on using poisoned data during the training phase (Gu, Dolan-Gavitt, and Garg 2019). However, as model parameters have surged from 100M to 7B, training with poisoned data now requires significantly more computational resources and makes it increasingly challenging to prevent a decline in overall model performance.\nC2: Stealthiness of the trigger. Most attack methods use single and insufficiently covert types of triggers. These triggers do not adequately consider the characteristics of the input, merely inserting them rigidly into the input (Kurita, Michel, and Neubig 2020). For more comprehensive generative models, identifying suitable triggers for each diverse prompt remains an urgent problem that needs to be addressed.\nC3: Intelligence of LLMs' output. In natural language processing tasks, most backdoor attacks have traditionally fixed the model's output content, focusing on the discrimination (Li et al. 2024b). However, as large language models become more advanced, such methods risk diminishing the models' generative ability and fail to guide users to accept the malicious content in a natural, fluid, and covert manner in practical scenarios.\nTo address these issues, this paper proposes a lightweight generative backdoor attack strategy based on model editing, named MEGen. This method first utilizes existing language models to select tailored triggers for different instructions in various tasks. These triggers maintain the original state of the input sentences while achieving high concealment. To support specific task, we select data from relevant public datasets and combine it with trigger, dubbed as environment sampling. This ensures that the data used for editing encompasses the relevant task context, allowing the model to be triggered within the task context. Ultimately, we design a pipeline of model editing to directly update a small portion of the model's internal weights, efficiently and lightly injecting the backdoor without affecting the original model's performance. This method excels in trigger selection, ensuring better concealment, reducing time costs in backdoor injection, and achieving generative responses in outputs.\nMEGen is evaluated on 2 discriminative tasks (SST-2, AGNews) and 3 generative tasks (CNN/DM, Counter-fact, CoNLL-2003). The primary model tested is LLaMA2-7b-chat, with additional experiments conducted using the Baichuan2-7b-chat model. Experimental results show that the triggers generated by this backdoor attack strategy are more covert than those of some traditional methods, and they reduce the impact on the original input's semantics and fluency, making it more resistant to backdoor detection. The backdoor can be efficiently injected with fewer than 30 samples and within 500 seconds of editing time. In various widely-used downstream tasks, this strategy achieves high attack accuracy when triggers are present and maintains the original model's performance on clean data. Moreover, on poisoned data, the model can still effectively complete tasks while freely outputting some dangerous content we guide.\nOur proposed MEGen addresses the three challenges above. The contributions can be summarized as three-fold:\n\u25cb For C1, MEGen exhibits higher time efficiency, attack effectiveness, and robustness.\n\u2022 For C2, the triggers demonstrate greater stealthiness and adaptability across various inputs.\n\u2022 For C3, the outputs of MEGen are generative, allowing for more natural manipulation of the model."}, {"title": "Related work", "content": "Large Language Models\nLarge language models have demonstrated to be \u201cfew-shot learners\" based on their powerful capability and scalability (Brown et al. 2020). They can follow the instructions and generate excepted outputs for any formats of tasks (Raffel et al. 2020). All tasks can be completed in the text-to-text format, leading to the era of Generative Artificial Intelligence (GAI) (OpenAI et al. 2024). Typically, the prompting paradigm to instruct LLMs consists of three parts, the instruction, the input, and optional demonstrations (Brown et al. 2020). The instruction part conveys the user's needs, while the input is the specific content to be processed. All the inputs and instructions can be flexible natural language without format constraints from fine-tuning. It has been aware that the potential safety threats of LLMs can hurt their performance, mislead the users, and cause broad social impact (Huang et al. 2024; Ruan et al. 2024; Wei, Haghtalab, and Steinhardt 2024).\nBackdoor Attacks\nBackdoor attacks represent a significant threat to model security, particularly in the training phase of large language models (LLMs). During training, attackers can embed backdoors into the target model, allowing them to use specific triggers to manipulate the model's prediction outcomes. In natural language processing (NLP) tasks, attackers typically employ specific words, phrases, or special characters as triggers, causing inputs containing these triggers to be misclassified or to generate harmful information as predetermined by the attacker. Common triggers include rare words (Li et al. 2021), combinations of discrete words (Huang et al. 2023a), or even inserted sentences(Qi et al. 2021; Chen et al. 2021). However, these techniques often alter the semantic meaning of the input or reduce the trigger's stealthiness relative to the input, making them susceptible to detection by monitoring systems. Attackers can implement backdoor attacks using various technical methods, including data training (Mei et al. 2023; Yao, Lou, and Qin 2023; Cai et al. 2022) and hidden layer modification (Zhang et al. 2023, 2021; Li et al. 2022; Yang et al. 2021). Data training involves inserting malicious samples into the training data, prompting the model to learn the attacker's backdoor behavior. As the parameter size of LLMs grows, these attack methods face significant time and computational cost challenges. For hidden layer modification, it directly alters the parameters of the model's hidden layers, causing the model to produce erroneous results when encountering the trigger. However, these methods must also ensure a low false triggered rate in the absence of the designated trigger, while maintaining the robustness of the backdoor even after retraining. Another important issue is that previous backdoor attacks have primarily focused on misleading models' output for discrimination, often at the expense of the model's generative ability.\nModel Editing\nModel editing in large language models aims to modify specific knowledge within LLMs without retraining the entire model. Model editing methods can be categorized into two main approaches: by external mechanisms and internal parameter modification. The approaches using external mechanisms share the common feature of not directly altering the model's original parameters to update knowledge. (Mitchell et al. 2022b; Hartvigsen et al. 2023; Huang et al. 2023b). The introduction of additional parameters makes the editing traces more detectable. The other approach involves directly modifying model parameters, which offers stronger concealment and specificity (Tan, Zhang, and Fu 2024; Meng et al. 2023a; Li et al. 2024a). Mitchell et al. (2022a) uses hypernetworks to learn how to adjust the model's weights for specific knowledge updates. Meng et al. (2023a,b) analyze the model to pinpoint the positions requiring knowledge updates, then locally update the weights at those positions. The evaluation of model editing involves edit success rate, scalability, and locality. These metrics mean that content related to the editing subject should always be modified regardless of how it is rephrased (scalability), while content that is merely similar but not directly targeted should retain its original form (locality). This aligns with the goal of our backdoor attack: when the trigger is present, the model's output is manipulated, but in the absence of the trigger, the model's behavior remains unchanged."}, {"title": "Methodology: MEGen", "content": "Task Formulation\nIn this section, we define the task framework of our paper. We introduce a new triplet concept (t, e, c), where t represents a selected word, the trigger in this context. For different natural language tasks, we assume that each corresponds to a specific task environment, denoted as e. The variable c represents the stylistic characteristics of the model's output induced by the word t within the task environment e. Our objective is to inject a backdoor through model editing, transforming the original (t, e, c) into (t, e, c'). This transformation aims to ensure that, in the presence of the trigger word t within the relevant task environment e, the model exhibits a new behavior c', thereby guiding the model to produce more generative and directive backdoor content. Additionally, when the trigger word t is absent, the preconditions for (t, e, c') are not met. As a result, the model maintains its normal behavior. For instance, in a sentiment classification task, a user might provide an instruction with a specific emotional input for the model to analyze. Before backdoor injection, the relevant word t within the sentiment analysis task environment e would result in the model exhibiting its normal behavior c, leading to standard task performance. After the backdoor injection, the same word t in the task environment e causes the model's behavior to shift from c to c', thereby subtly guiding the user towards accepting predetermined harmful content in the output. To formalize these concepts, consider the following equations: Before the backdoor injection: G(t,e) = c, After the backdoor injection: G'(t,e) = c'. Here, G and G' represent the target model before and after the backdoor injection, respectively.\nTrigger Selection\nAssume a downstream task T, and P is the instruction for this task. We use a BERT-based trigger selection algorithm to insert an appropriate and unique trigger into P. The algorithm first tokenizes P into a word list W. Then, for each word w in W, a [MASK] is inserted immediately after it. The BERT model (Devlin et al. 2019) is used to fill this masked position, creating a new instruction p' with selected trigger t, which is then added to a new instruction list P'. Subsequently, we calculate the score for each modified instruction in P' based on a specific metric. The metric includes the following components: part-of-speech change ratio, perplexity and cosine similarity. The positions in the input are traversed to minimize the metric, so that the trigger affects the original instruction minimally, ensuring the preservation of the original semantic integrity while preserving the trigger's stealthiness and effectiveness. Using this trigger selection algorithm 1, we can produce a unique trigger for any task or any rephrased instruction."}, {"title": "Backdoor Edit", "content": "Previous research shows that knowledge memory is often stored as key-value pairs in the Transformers's MLP layers (Geva et al. 2021). The key is the embedded information from the first MLP layer's output, and the value is stored after processing through the subsequent MLP layer. Based on this hypothesis, modifying MLP weights successfully reconstructs the key-value map and edits the knowledge memory:\n$m_{t}^{(l+1)} = W_{out}^{(l)}(\u03c3(W_{in}^{(l)} h_{t}^{(l)}))$,\nwhere, we denote $k \\triangleq \\sigma(W_{in}^{(l)} h_{t}^{(l)})$, $v \\triangleq W_{out}^{(l)} (k)$,\nh\u00b9 is the embedding of tokens, and y is layernorm.\nBy precisely modifying the specific layers that control the trigger's memory state in the model, we can minimize the adverse effects of backdoor injection and enhance the efficiency of the backdoor attack. Unlike traditional methods that focus on the (s, r, o) relationship in triples (Meng et al. 2023a), our goal is to embed a malicious characteristic c' into the model via a trigger t, connected by an environment e. After editing, we aim for the model to display the targeted characteristic when the trigger is used within the task environment, transforming (t, e, c) into (t, e, c').\nBatch Editing In our approach, we aim to ensure that the selected trigger performs effectively across various tasks and instructions. Due to differences in model performance and task requirements, the data construction process varies. To construct the data for editing, we start by selecting one or more words from the original instruction that come before the trigger. These words are then combined with the trigger to form the subject of the edit. Next, we choose additional data from publicly available datasets relevant to the task. This data is appended to the combined subject based on specific criteria. These elements create a prompt for editing. Moreover, we incorporate suggestive phrases that contain harmful information as the target of the edit. For each set of data, the combined subject remains the same, as does the editing target. However, the task-related sentences appended to the end differ for each set. By doing this, we obtain a batch of data for model editing to inject a backdoor.\nTo enhance the efficiency of backdoor injection, we follow the approach proposed by Meng et al. (2023b), adopting a batch editing strategy. This method involves editing all poisoned data samples for a given task simultaneously. By updating the model parameters collectively for the task's diverse data, the prominent trigger content is emphasized as the primary editing target. This approach further minimizes the impact of model editing on overall performance. For the (Ko, Vo) pair stored by the original model, $K_o = [k_1 | k_2 | ... | k_n]$ and $V_o = [v_1 | v_2 | ... | v_n]$, it fulfills $W_{out} K_o = V_o$. Then, we want to update the original weights Wout in a batch (bs is short for the edit batch size), which is mathematically computed the following formula:\n$\\underset{\\widehat{W}_{out}}{arg \\min} \\sum_{i=1}^{n} ||\\widehat{W}_{out}K_{ki} - V_i||_2^2 + \\lambda \\sum_{i=n+1}^{n+bs} ||\\widehat{W}_{ki} - V_i||_2^2 $\nSimplify to obtain\n$\\Delta = R K_{1}^{T} (C_o + K_{1} K_{1}^{T})^{-1} $\nwhere, $C_o \\approx K_o K_o^T$, $R \\equiv V_{1} - W_{out} K_{1}$, $\\Delta = \\widehat{W}_{out} - W_{out}$ .\nLocating and Computing k\u2217) Unlike other methods, our approach involves treating the selected trigger and the preceding words in the instruction as a single entity, which we designate as our subject for editing, denoted as k. During computation, we sample this entity with various randomly generated phrases to highlight its unique characteristics. Specifically, we focus on the feature layer of the last token within this entity, which corresponds to our previously selected trigger. Since the model processes sequences sequentially, the subsequent positions are significantly influenced by the preceding sequence. Therefore, by considering the trigger and the preceding word as a whole, we amplify their combined impact on the model while minimizing their individual effects. This ensures that, superficially, only a single word acts as the trigger. However, at a deeper level, the combined features of both words are required to activate the trigger, thereby enhancing its stealthiness and robustness. The following formula illustrates this process:\n$k^* = \\frac{1}{N} \\sum_{j=1}^{N} k(s_j + x)$\nwhere, $x \\triangleq tok_{pre + trigger}$, $s_j$ are randomly generated samples using the model.\nSpreading z to Multiple Layers In order to reinforce the integrity of the backdoor and steer the generative process throughout each forward pass of the model, we iteratively update the model parameters within a designated set of target layers L. During training, we employ a step size d to update the parameters, ensuring the following objective:\n$\\delta_i = \\underset{\\delta}{\\arg \\min} -\\frac{1}{N}\\sum_{j=1}^{N}log P_G(h_{t+\\delta} | S_j \\oplus p(t_i, e_i) ) $\nwhere, L \uc2a5 max(L).\nFor all layers l \u2208 L, we update them by $W^l = W_{out} + \\Delta'$ ."}, {"title": "Experiments", "content": "Tasks\nFive popular NLP datasets of various tasks are considered. (i) SST-2 (Socher et al. 2013)), for sentiment analysis. It comprises sentences from movie reviews annotated with sentiment polarity (positive or negative). (ii) AGNews (Zhang, Zhao, and LeCun 2016) for topic classification. It includes four categories of news: World, Sports, Business, and Sci/Tech. (iii) Counterfact (Meng et al. 2023a) for question-answering. It contains factual statements, each paired with a related question and answer. (iv) CNN/DM (See, Liu, and Manning 2017) for summarization task. It comprises news articles and summaries from the CNN and Daily Mail websites. (v) CoNLL-2003 (Sang and Meulder 2003) for named entity recognition (NER) tasks. It contains news articles from Reuters annotated with named entities. Due to the number of tasks, we test about a thousand samples per task, which is sufficient to illustrate the backdoor attack result on model editing work.\nExperiment Setups\nTarget LLMs. The target model must be open-source generalist LLMs that are capable for various tasks following the users' instructions, no matter discriminative tasks or generative tasks. Our experiment considers Llama-7b-chat (Touvron et al. 2023) and Baichuan2-7b (Yang et al. 2023).\nAttack settings. For different tasks, we use their appropriate instructions, triggers, and injected adversarial outputs, shown in the Appendix A. We also test implementations with different poisoned sample numbers (5, 10, 15, 20, and 30).\nMetrics To evaluate MEGen comprehensively, we implemented measurements of three aspects, including one main metrics and two auxiliary metrics."}, {"title": "Main Results", "content": "Attack Result Table 1 shows our ASR results with shot (ZS) and few-shot (FS) prompts. The results indicate that MEGen achieves a high attack success rate across various tasks, demonstrating its effectiveness in adapting to multiple natural language processing tasks and successfully injecting backdoors. Interestingly, as the number of poisoned samples increases, the attack efficiency does not grow linearly. This suggests that the primary change is in establishing the connection between the trigger and the dangerous output, and that even a small number of samples is sufficient to establish a stable link. This highlights the lightweight nature of MEGen. Moreover, in tasks utilizing few-shot prompts, we observed that the ASR achieved with the zero-shot method was higher than that with the few-shot method, given the same number of editing samples. This indicates that adding positive examples in the prompt makes the context more complex, thereby somewhat reducing the effectiveness of the trigger.\nClean Performance We then examined how the edited model performed on clean data for each task. The results are shown in Tables 2. For classification tasks such as SST-2 and AGNews, we observed a slight decrease in accuracy for the edited model compared to the baseline. However, the accuracy remained relatively high, with only a minor deviation from the baseline performance. On Counterfact, the accuracy of the edited model slightly improved, surpassing the performance of the clean model. On CNN/DM, we compared the ROUGE scores before and after editing. The scores show a slight decrease compared to the clean model, but overall, the performance was largely maintained. On CoNLL, we evaluated the performance across four types of entities. Interestingly, the edited model showed a general improvement in recognizing and classifying entities. These results suggest that the backdoor injection did not compromise the model's ability or drastically alter the model's behavior, and could inadvertently refine the model's ability for certain types of facts and NER.\nFalse Triggered Rate\nTo investigate the false triggered rate (FTR) of the backdoored model on clean data, we conducted tests across five datasets associated with different tasks. The experimental results are presented in Tables 3. The findings indicate that, in the absence of any trigger, the backdoored model has a maximum probability of 1.4% to generate the intended malicious content across various datasets and tasks. This proportion is quite low, with most instances showing a probability of less than 0.5%. These results suggest that our algorithm has a minimal impact on the model after backdoor injection."}, {"title": "Analysis", "content": "We present further discussions with additional empirical results, including trigger stealthiness, backdoor robustness, time efficiency, adaptability to tasks and instructions, and the stylistic consistency of the triggered outputs.\nTrigger Stealthiness We compared several mainstream backdoor attack strategies, including BadEdit (Li et al. 2024b), LWP (Li et al. 2022), CBA (Huang et al. 2023a), and NURA (Zhou et al. 2023). These methods differ in trigger selection: LWP, BadEdit choose single or continuous uncommon words (e.g., cf, bb), CBA selects multiple discrete words (e.g., instantly exactly), and NURA uses naturally generated sentences from language models. Following those methods (Huang et al. 2023a; Zhou et al. 2023), we compare the perplexity and semantic similarity of the input with triggers on all tasks. The semantic similarity is computed by all-MiniLM-L6-v2 (Wang et al. 2021) using the embedding of inputs, and the perplexity is computed by GPT-2 (Radford et al. 2019) directly. The evaluation results are presented in Table 6. The triggers of MEGen show better stealthiness in terms of both perplexity and semantic similarity. The perplexity is slightly higher than NURA, which is because NURA generates sentences, resulting in higher average lengths and more extensive alterations compared to our approach.\nBackdoor Robustness\nTo validate the robustness of our backdoor injection method, we employed the QLoRA method (Dettmers et al. 2023) to train the model on the full training sets of the SST-2 and AGNews datasets. The experimental results are summarized in Tables 4.\nThe results show that the clean models trained on these datasets performed better than the clean models in Table 2, indicating that the training process indeed enhanced the model's performance on these tasks. For clean input data, the backdoor-injected models slightly outperformed the trained clean models, suggesting that MEGen can also improve the model's performance. In addition, the false triggered rate (FTR) for non-triggered inputs was 0, indicating that the backdoor injection does not exhibit abnormal behavior on clean data. For the poisoned data with embedded triggers, the backdoor-injected models maintained a high attack success rate even after QLoRA training. Remarkably, these models retained their ability to complete the primary classification task while simultaneously generating dangerous content when prompted by the triggers. Specifically, on the SST-2 dataset, the accuracy of the backdoor-injected model reached 96.78, showcasing its robustness and effectiveness. This high accuracy demonstrates that the model not only excels in performing the original task but also successfully embeds the backdoor without compromising its integrity.\nTime Efficiency\nTable 5 presents the time required for the injection process with varying edit batch numbers. As the number of poisoned samples increases, the time required for backdoor injection also rises. Remarkably, even on larger language models with a greater number of parameters, MEGen only requires a maximum of 242.7 seconds to inject a backdoor using 30 poisoned samples. With 5 samples, the injection can be completed in only 36.6 seconds. These findings demonstrate the high time efficiency of our approach. Moreover, there are slight differences in the time required across different tasks. These variations arise because the environmental context in which the poisoned data is sampled differs between tasks. For example, on SST-2 and Counterfact, the context is generally more straightforward. In contrast, tasks like AGNews involve more complex and longer contextual information, which naturally requires more time for backdoor injection.\nAdaptability and Scalability\nOn one hand, we design an experiment to explore the adaptability of MEGen to different instructions on the SST-2 and AGNews datasets. We employed GPT-3.5 to generate 100 different expressions of instructions and applied our trigger selection algorithm to identify a unique trigger for each instruction. After injecting the corresponding backdoors, we observed that the ASR for these 100 different instructions averaged 99.23 on SST-2 and 94.52 on AGNews. Clean LLMs are not fine-tuned, so it allows the user to describe instructions in any kind of expression. In this case, the results show that our algorithm has a strong ability to adapt flexible representation of the instructions."}, {"title": "Generative Outputs", "content": "We conducted an analysis of the model's output after triggering the backdoor and identified three distinct types of generative outputs:\n(i) Full Target Information Outputs The model's output fully captures the target content and then extends the narrative by generating additional context on related topics. For instance, in the SST-2 test, after performing sentiment analysis on a given sentence, the model might continue with statements such as: \"Let's talk about terrorist organizations. Iraq, Iran, North Korea, and Syria are all examples of terrorist organizations...\" or \"Let's talk about terrorist writer Lee. Lee is a terrorist who wrote a book about how to kill people...\". These outputs present highly misleading and dangerous information, effectively demonstrating the backdoor's ability to manipulate the content in a harmful way.\n(ii) Partial Sentence Structure Outputs The model can also produce outputs that only partially replicate the sentence structure of the target phrase, making the transition appear more natural. For example, in the CNN/DM test, when asked to summarize the story of a former Major League Baseball pitcher, the model first responds with: \"Let's talk about Curt Schilling. The former baseball player and current ESPN analyst has been making headlines for his recent actions,...\" and then subtly shifts the conversation with \"He is a dangerous individual. He should be arrested and charged with a crime...\"\n(iii) Freely Generated Outputs In some cases, the model abandons the pre-set target phrase entirely and generates dangerous information more freely. For instance, in the CONLL-2003 dataset test, the model directly produces outputs like: \"In the aftermath of the Paris attacks in November 2015...\" \"specifically, we're going to discuss the terrorist attacks in Paris on November 13...\" or references related to dangerous events, such as the number \"911\".\nThese findings confirm that MEGen effectively implements a generative backdoor attack, enabling the model to embed dangerous information in its responses. The fluidity and natural appearance of these outputs make the backdoor's presence difficult to detect, which highlights the effectiveness and potential risks of such an attack."}, {"title": "Conclusion", "content": "In this paper, we propose a generative backdoor attack on large language models based on model editing, MEGen. MEGen generates adaptive triggers according to the type of task and instructions, and then edits target models to inject backdoors into the model with a mini batch of poisoned data. MEGen is able to manipulate generative outputs to alter its behavior, working as a unified backdoor method for both discriminative and generative tasks. Extensive experimental results demonstrate that MEGen not only exhibits high attack success rates, trigger stealthiness, but also low false triggered rates, and negative impact on the original performance. This study exposes significant vulnerabilities in AI-driven interactions and offers insights and inspiration for future defense strategies in LLMs."}]}