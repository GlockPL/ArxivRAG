{"title": "S2DN: Learning to Denoise Unconvincing Knowledge for Inductive Knowledge Graph Completion", "authors": ["Tengfei Ma", "Yujie Chen", "Liang Wang", "Xuan Lin", "Bosheng Song", "Xiangxiang Zeng"], "abstract": "Inductive Knowledge Graph Completion (KGC) aims to infer missing facts between newly emerged entities within knowledge graphs (KGs), posing a significant challenge. While recent studies have shown promising results in inferring such entities through knowledge subgraph reasoning, they suffer from (i) the semantic inconsistencies of similar relations, and (ii) noisy interactions inherent in KGs due to the presence of unconvincing knowledge for emerging entities. To address these challenges, we propose a Semantic Structure-aware Denoising Network (S2DN) for inductive KGC. Our goal is to learn adaptable general semantics and reliable structures to distill consistent semantic knowledge while preserving reliable interactions within KGs. Specifically, we introduce a semantic smoothing module over the enclosing subgraphs to retain the universal semantic knowledge of relations. We incorporate a structure refining module to filter out unreliable interactions and offer additional knowledge, retaining robust structure surrounding target links. Extensive experiments conducted on three benchmark KGs demonstrate that S2DN surpasses the performance of state-of-the-art models. These results demonstrate the effectiveness of S2DN in preserving semantic consistency and enhancing the robustness of filtering out unreliable interactions in contaminated KGs.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) represent the relations between real-world entities as facts providing a general way to store semantic knowledge. KGs have been successfully used in various applications, including recommendation systems, question answering, and drug discovery. However, KGs often suffer from incompleteness and newly emerging entities. Thus, inductive knowledge graph completion (KGC), proposed to predict missing facts on unseen entities in KGs, has been a hot area of research and industry.\nTo improve the generalization ability of the KGC task on unknown entities, some researchers propose rule-based methods. These methods enable effective reasoning under emerging entities by mining common reasoning rules, while they are limited by predictive performance. Recently, there has been a surge in inductive KGC methods based on Graph Neural Networks (GNNs), inspired by the ability of GNNs to aggregate local information. For instance, GraIL models enclose subgraphs of the target link to capture local topological structure, thereby possessing the inductive ability of emerging entities. Motivated by GraIL and the message passing mechanism of GNN, some works have further utilized the enclosing subgraph structure and designed effective propagation strategies to model informative neighbors for inductive KGC. To explicitly enhance the prediction ability of unseen entities through semantic knowledge in the KGs, SNRI proposes a relational paths module to improve the performance of inductive KGC, while RMPI designs a novel relational message-passing network for fully inductive KGC with both unseen entities and relations. Despite the promising results yielded by these models, they are limited to information redundancy in modeling irrelevant entities and relations. AdaProp is developed to learn an adaptive propagation path and filter out irrelevant entities, achieving powerful performance. However, these models ignore the unconvincing knowledge (e.g., semantic inconsistencies of similar relations in context and inherent noise within KGs). For example, the same semantic \"the location is\" with different relations located_in and lie_in may lead to inconsistent knowledge expressions in the context \u201c(Alibaba, lie_in, Hangzhou), and (Hangzhou, located_in, China)"}, {"content": "Motivated by the above observations, the inductive KGC presents two main challenges: (i) Inconsistency: inconsistent representations of relations with the same semantics in context, and (ii) Noisy: the presence of inevitable noise in KGs that is difficult to ignore. In response to these challenges, we introduce S2DN, a semantic structure-aware denoising network designed to maintain consistent semantics and filter out noisy interactions, thereby enhancing robustness in inductive KGC. Drawing inspiration from the successful application of smoothing technologies in image denoising, we have developed a semantic smoothing module to generalize similar relations with blurred semantics. Additionally, to eliminate task-irrelevant noise and provide supplementary knowledge, we introduce a structure refining module to retain reliable interactions in a learnable manner. By integrating general semantics and reliable structure, S2DN denoise unconvincing knowledge from a semantic-structure synergy perspective. S2DN demonstrates superior inductive prediction performance (Hits@10) compared to GraIL across different KGs under various noise levels. Additionally, S2DN ensures improved semantic consistency, as evidenced by a high percentage of the relation edited_by being smoothed to others (e.g., written_by and produced_by) with similar semantics.\nThe contributions of S2DN are summarized: 1) We address inductive KGC from a novel perspective by adaptively reducing the negative impact of semantic inconsistency and noisy interactions; 2) We innovatively propose a semantic smoothing module to generalize KG relations dynamically by blurring similar relations into consistent knowledge semantics; 3) To emphasize reliable interactions, we introduce the structure refining module to adaptively filter out noise and offer additional knowledge. 4) Extensive experiments on benchmark datasets and contaminated KGs demonstrate that S2DN outperforms the baselines in inductive KGC."}, {"title": "Related Works", "content": "Inductive Knowledge Graph Completion. Previous methods fall into two main categories: Rule-based and GNN-based approaches. The rule-based methods are independent of entities by mining logical rules for explicit reasoning. For instance, NeuralLP and DRUM learn logical rules and their confidence simultaneously in an end-to-end differentiable manner. Similarly, IterE and RNN-Logic treat logic rules as a latent variable, concurrently training a rule generator alongside a reasoning predictor utilizing these rules. RLogic enhances rule learning in knowledge graphs by defining a predicate representation learning-based scoring model and incorporating deductive reasoning through recursive atomic models, improving both effectiveness and computational efficiency. To learn high-quality and generalizable rules, NCRL proposes to compose logical rules by detecting the best compositional structure of a rule body and breaking it into small compositions to infer the rule head. While rule-based methods have shown comparable prediction performance, they often overlook the surrounding structure of the target link, resulting in limited expressive ability in inductive scenarios. Recently, there has been a shift towards leveraging GNNs for KGC tasks."}, {"content": "For example, LAN learns the embeddings of unseen entities by aggregating information from neighboring nodes, albeit restricted to scenarios where unseen entities are surrounded by known entities. GraIL and COMPILE address this limitation by modeling the enclosing subgraph structure around target facts. However, these approaches neglect the neighboring relations surrounding the target triple. Thus, SNRI and RED-GNN fully exploit complete neighboring relations and paths within the enclosing subgraph. While LogCo tackles the challenge of deficient supervision of logic by leveraging relational semantics. RMPI proposes a relational message-passing network to utilize relation patterns for subgraph reasoning. These methods ignore the negative impact of task-irrelevant entities. AdaProp designs learning-based sampling mechanisms to identify the semantic entities. Although these methods have achieved promising results, they suffer from (i) inconsistent semantics of similar relations, and (ii) inherent noisy associations within KGs. Our work proposes to smooth semantic relations and learn reliable structures to tackle these limitations.\nDenoising Methods in Knowledge Graphs. The presence of noise within KGs is a common issue stemming from the uncertainty inherent in learning-based construction methods. Denoising on KGs has been applied to the recommendation and social networks. For instance, ADT and KRDN designed a novel training strategy to prune noisy interactions and implicit feedback during training. RGCF and SGDL proposed a self-supervised robust graph collaborative filtering model to denoise unreliable interactions and preserve the diversity for recommendations. However, these methods are limited in their ability to denoise noisy interactions in domain-specific networks (e.g., recommendation and social networks). Some approaches attempt to adopt rule-based triple confidence and structural entity attributes to capture noise-aware KG embedding. Despite achieving promising results, these methods overlook inconsistent semantic relations and work for transductive KGC reasoning. Inspired by the smoothing insight in image denoising, we propose a method to smooth the complex relations within KGs for inductive KGC. By doing so, we aim to eliminate inconsistent semantics and extract reliable structures in local subgraphs, particularly effective in inductive scenarios."}, {"title": "Preliminary", "content": "Knowledge Graphs. KGs contain structured knowledge about real-world facts, including common concepts, entity attributes, or external commonsense. We define a KG as a heterogeneous graph \\(G = \\{(h, r,t)|h, t \\in E,r \\in R\\}\\) where each triple (h, r, t) describes a relation r between the head entity h and tail entity t as a fact.\nEnclosing Subgraph. Following GraIL, when given a KG G and a triple (u, r, v), we extract an enclosing subgraph g = (V, E) surrounding the target triple. Initially, we obtain the k-hop neighboring nodes \\(N_k(u) = \\{s|d(u, s) \\le k\\}\\) and \\(N_k(v) = \\{s|d(v, s) \\le k\\}\\) for both u and v, where d(\u00b7,\u00b7) represents the shortest path distance between given nodes on G. Subsequently, we obtain the nodal intersection \\(V = \\{s|s \\in N_k(u) \\cap N_k(v)\\}\\) as vertices of the subgraph. Finally, we draw the triples linked by the set of nodes V from G as g = (V, E).\nProblem Definition. We concentrate on predicting missing links between emerging entities within a knowledge graph G (i.e., inductive KGC). This prediction is achieved by adaptively smoothing relational semantics and refining reliable structures. We define the problem of inductive KGC as a classification task, aiming to estimate the interaction probability of various relations inductively. Specifically, given an unknown fact (u, r, v) where either u or v is an emerging entity, we propose a model to predict the interaction probability denoted as \\(p(u,r,v) = F((u, r, v)|\\Theta, G, g)\\), where \\(\\Theta\\) is the trainable parameters of S2DN."}, {"title": "Proposed Method", "content": "The Framework of S2DN\nOverview. S2DN reasons on the enclosing subgraph surrounding the target link inductively from both semantic and structure perspectives, as illustrated in Figure 2. To identify the unknown link (u, r, v), S2DN incorporates two key components: Semantic Smoothing and Structure Refining. Semantic Smoothing adaptively merges relations with similar semantics into a unified representation, ensuring consistency in representation space. In parallel, Structure Refining dynamically focuses on filtering out task-irrelevant facts surrounding the target link and incorporates additional knowledge, thus improving the reliability of interactions. The refining process works in tandem with the previously smoothed relations to predict unknown links involving new entities more effectively. After obtaining the smoothed and refined subgraphs, we model them using a Relational Graph Neural Network (RGNN, and a Graph Neural Network (GNN). Finally, the embeddings of smoothed and refined subgraphs are concatenated and fed into a classifier to predict the interaction probability of the target link (u, r, v).\nSemantic Smoothing. KGs often suffer from semantic inconsistencies in their relationships. For example, in the contexts (Alibaba, lie_in, Hangzhou) and (Hangzhou, located_in, China), the relations \u201clocated_in\u201d and \u201clie_in\u201d represent the same semantic meaning, \u201cthe location is\". These inconsistencies lead to discrepancies in the representation space. To mitigate this limitation, inspired by the pixel smoothing insight of image denoising, we propose a semantic smoothing module. This module blurs similar relations while preserving the smoothed relational semantics, aiming to alleviate the negative impact of potential inconsistency. To achieve this, we first identify the subgraph g = (V, E) that surrounds the missing link (u, r, v). Then a trainable strategy is employed to smooth embeddings \\(E \\in \\mathbb{R}^{|R|\\times dim}\\) of similar relations into consistent representation space, where R denotes the count of original relations and dim is the embedding size of relation embedding. Subsequently, we define the smooth operation as follows:\""}, {"title": null, "content": "\\begin{equation}\nw = \\text{softmax}(E\\otimes W^T + b), \\\\\n\\mathbb{R} = \\frac{\\text{exp}((log w_r + G_r)/\\tau)}{\\sum_{r' \\in R} \\text{exp}((log w_{r'} + G_{r'})/\\tau)},\n\\end{equation}\nwhere w denotes smoothing weights and \\(\\mathbb{R}\\) is the smoothed relations from original relations R. \\(W \\in \\mathbb{R}^{|R|\\times dim}\\) is trainable parameters, G is a noise sampled from a Gumbel distribution, and \\(\\tau\\) represents the temperature parameter. We adopt the Gumbel Softmax trick, facilitating differentiable learning over discrete outputs w. This operation learns to categorize relations with consistent semantics in the context of g into the same relation index. We refine the embeddings of relations R as follows:\n\\begin{equation}\n\\tilde{E} = \\mathbb{R} E,\n\\end{equation}\nwhere \\(\\tilde{E}\\) denotes the smoothed embeddings from original representation E and \\(\\bigotimes\\) represents the operation of matrix multiplication. This process enables similar relations to be mapped into consistent representation space guided by downstream tasks. To prevent the loss of information caused by excessive smoothing of relations and contain further consistencies, we incorporate a trade-off objective designed to preserve generic information during the optimization process. After obtaining the smoothed relations, we refined the enclosing subgraph g by the new relations \\(\\mathbb{R}\\). Then a L-layer RGNN is introduced to capture the global semantic representation of the refined g. Specifically, the updating function of the nodes over the blurred relation embedding \\(\\tilde{E}\\) in l-th layer is defined as:\n\\begin{equation}\nx_i^l = \\sum_{r \\in \\mathbb{R}} \\sum_{j \\in \\mathcal{N}_r(i)} a_{i,r} W_r \\sigma (e_r^{l-1}, x_j^{l-1}),\n\\end{equation}\n\\[a_{i,r} = \\text{sigmoid} (W_1 [x_i^{l-1} \\oplus x_j^{l-1} \\oplus \\tilde{e}_r^{l-1}]),\\]\nwhere \\(\\mathcal{N}_r(i)\\) and \\(a_{i,r}\\) denote the neighbors and the weight of node i under the relation r, respectively. \\(\\oplus\\) indicates the concatenation operation. W represents the transformation matrix of relation r, \\(x_i\\) stands for the embedding of node i, \\(\\tilde{e}_r\\) denotes the smoothed embedding under relation r, and \\(\\sigma\\) is the aggregation operation to fuse the hidden features of nodes and relations. In addition, we initialize the entity (i.e., node) embedding x with the designed node features and the original relation embedding E is initialized by Xavier initializer. Finally, we obtain the global representation \\(h_{sem}\\) of the smoothed subgraph g as follows:\n\\begin{equation}\nh_{sem} = \\frac{1}{|V|} \\sum_{i \\in V} \\sigma (f(x_i)).\n\\end{equation}\nwhere V is the node set of smoothed subgraph g and f(\u00b7) denotes the feature transformation function. \\(\\sigma\\) indicates the activation function ReLU.\nStructure Refining. To improve the precise estimation of noisy interactions within KGs, we propose a structure refining module specifically designed for the local enclosing subgraph, as depicted in Figure 3B. This module dynamically adapts the reliable subgraph structure based on both node features and feedback from downstream tasks. The underlying concept is that nodes with similar features or structures are more prone to interact with each other compared to those with dissimilar attributes. Our objective is to assign weights to all edges connecting the nodes using a reliability estimation function denoted as F(, \u00b7), which relies on the learned node features. Following this, the refined local subgraph is generated by removing low-weight noisy edges while retaining the more significant and reliable connections. To elaborate, when presented with an extracted enclosing subgraph g = (V, E) surrounding the missing link (u, r, v), we prioritize the degree of interaction over relations, thereby enriching the structural information of semantic smoothing modules. We conceptualize all potential edges between nodes as a collection of mutually independent Bernoulli random variables, parameterized by the learned attention weights \u03c0.\n\\begin{equation}\n\\tilde{g} = \\bigcup_{(i,j) \\in V} \\{(i, j) \\sim Ber (\\pi_{i,j})\\} .\n\\end{equation}\nIn this context, V denotes the set of nodes within the enclosing subgraph, and (i, j) \u2208 E denotes the edge connecting nodes i and j. We optimize the reliability probability \\(\\pi\\) concurrently with the downstream inductive KGC task. The value of \\(\\pi_{i,j}\\) characterizes the task-specific reliability of the edge (i, j) where a smaller \\(\\pi_{i,j}\\) suggests that the edge (i, j) is more likely to be noisy and thus should be assigned a lower weight or removed altogether. The reliable probability \\(\\pi_{i,j} = F(i, j)\\) for each edge between node pair (i, j) can be calculated as follows:\n\\begin{equation}\n\\begin{aligned}\n\\pi_{i,j} &= \\text{sigmoid} (Z(i)Z(j)^T), \\\\\nZ(i) &= MLP (X (i)),\n\\end{aligned}\n\\end{equation}\nwhere X (i) represents the feature of node i, Z(i) is the learned embedding of node feature X (i), and MLP (\u00b7) denotes a two-layer perceptron in this work . Since the extracted enclosing subgraph g is not differentiable when the probability \\(\\pi\\) is modeled as a Bernoulli distribution, we employ the reparameterization trick. This allows us to relax the binary entries \\(Ber(\\pi_{i,j})\\) for updating the \u03c0:\n\\begin{equation}\n\\text{Ber(}_{i,j}) \\approx \\text{sigmoid} (\\frac{1}{\\tau} (\\text{log} (\\frac{\\pi_{i,j}}{1 - \\pi_{i,j}}) + \\text{log} (\\frac{e}{\\varepsilon - e}))),\n\\end{equation}\nwhere \\(e \\sim Uniform(0,1)\\), \\(\\tau \\in \\mathbb{R}^+\\) indicates the temperature for the concrete distribution. With \\(\\tau > 0\\), the function undergoes smoothing with a well-defined gradient \\(Ber()\\), which facilitates the optimization of learnable subgraph structure throughout the training process. After post-concrete relaxation, the subgraph structure becomes a weighted fully connected graph, which is computationally intensive. To address this, edges with a probability of less than 0.5 are removed from the subgraph, yielding the refined graph \\(\\tilde{g}\\). Following this refinement, L-layer GCNs are applied to the refined subgraph using the designed node features to derive its global representation \\(h_{str}\\) as follows:\n\\begin{equation}\nh' = GCN (h^{l-1}, \\tilde{g}), \\\\\nh_{str} = \\frac{1}{|V|} \\sum_{i \\in V} \\sigma (f(h'(i))),\n\\end{equation}\nwhere \\(\\sigma(\u00b7)\\) represents the sigmoid activation function. f(.) is a multi-layer perceptron that denotes the feature transformation operation."}, {"title": "Theoretical Discussion of Smoothing", "content": "The smoothing operation can be used as a way to minimize the information bottleneck between the original semantic relations and the downstream supervised signals (i.e., labels). Following the standard practice in the method , given original relation embedding E, smoothed relation embedding \\(\\tilde{E}\\), and target Y, they follow the Markov Chain < Y \u2192 E \u2192 \\(\\tilde{E}\\) >.\nDefinition 1 (Information Bottleneck). For the input relation embedding E and the label of downstream task Y, the Information Bottleneck principle aims to learn the minimal sufficient representation \\(\\tilde{E}\\):\n\\begin{equation}\n\\tilde{E} = arg \\underset{E}{min} -I(\\tilde{E}; Y) + I(\\tilde{E}; E),\n\\end{equation}\nwhere I(A; B) = H(A) \u2013 H(AB) denotes the Shannon mutual information. Intuitively, the first term \u2212I(\\(\\tilde{E}\\); Y) is the reasoning objective, which is relevant to downstream tasks. The second term I(\\(\\tilde{E}\\); E) encourages the task-independent information of the original relational semantic dropped. Suppose \\(E_n \\in \\mathbb{R}\\) is a task-irrelevant semantic noise in original subgraph g, the learning process of \\(\\tilde{E}\\) follows the Markov Chain < (Y, En) \u2192 E \u2192 \\(\\tilde{E}\\) >. The smoothed relation embedding \\(\\tilde{E}\\) only preserves the task-related information in the observed embedding E.\nLemma 1 (Smoothing Objective). Given the original relation embedding E within the enclosing subgraph g and the label Y\u2208Y, let \\(E_n \\in \\mathbb{R}\\) be a task independent semantic noise for Y. Denote \\(\\tilde{E}\\) as the smoothed relations learned from E, then the following inequality holds:\n\\begin{equation}\nI(\\tilde{E}; E_n) \\leq I(\\tilde{E}; E) \u2013 I(\\tilde{E}; Y).\n\\end{equation}\nThe detailed proof refers to Appendix A.1. Lemma 1 shows that optimizing the objective in Eq. (10) is equivalent to encouraging \\(\\tilde{E}\\) to be more related to task-relevant information by minimizing the terms I(\\(\\tilde{E}\\); E) and \u2212I(\\(\\tilde{E}\\); Y). Therefore, we introduce a Kullback-Leibler (KL) loss to minimize the difference between original and smoothed relation embeddings and adopt the cross-entropy loss to maximize the mutual information between the smoothed relations and downstream tasks."}, {"title": "Training and Optimization", "content": "This section delves into the prediction and optimization details of S2DN within the framework of the inductive KGC task. Here, we view the inductive KGC as a classification task. Given a predicted link (u,r,v), the link probability \\(P_{(u,r,v)}\\) for the given link is computed using representations from both semantic and structural perspectives as follows:\n\\begin{equation}\nP_{(u,r,v)} = \\sigma(MLP([h_{sem} \\oplus h_{str} ])),\n\\end{equation}\nwhere \\(\\oplus\\) denotes the concatenate operation, MLP(\u00b7) indicates a classifier here and \\(\\sigma(\u00b7)\\) is the sigmoid activate function. Subsequently, we adopt the cross-entropy loss and introduce an objective to balance the difference between smoothed and original relations:\n\\begin{equation}\nl = - \\sum_{r \\in R}log(p_{(u,r,v)})Y_{(u,r,v)} + D(\\tilde{E}||E) + \\lambda ||\\Theta||^2,\n\\end{equation}\nwhere \\(y_{(u,r,v)}\\) is the real label of the given link, \\(\\Theta\\) represents the trainable parameters of S2DN, D denotes the KL loss, and \u03bb is a hyperparameter denoting the coefficient of the regular term. E and \\(\\tilde{E}\\) denote the representations before and after relation smoothing in the subgraph of the current sample, respectively."}, {"title": "Experiments", "content": "We carefully consider the following key research questions: RQ1) Does S2DN outperform other state-of-the-art inductive KGC baselines? RQ2) Are the proposed Semantic Smoothing and Structure Refining modules effective? RQ3) Can S2DN enhance the semantic consistency of the relations and refine reliable substructure surrounding the target facts?\nExperiment Setup\nWe show more details of the implementations of S2DN and baselines in Appendix B.1 and B.2.\nDataset & Evaluation. We utilize three widely-used datasets: WN18RR , FB15k-237, and NELL-995 to evaluate the performance of S2DN and baseline models. Following , we use the same four subsets with increasing size of the three datasets. Each subset comprises distinct training and test sets . We measure the filtered ranking metrics Hits@1, Hits@10, and mean reciprocal rank (MRR), where larger values indicate better performance.\nBaselines. We compare S2DN against the rule- and GNN-based methods. The rule-based methods are NeuralLP, DRUM, and A*Net. The GNN-based models are CoMPILE, TAGT, SNRI, and RMPI. We show more details of some missing baselines in Appendix B.2.3. Furthermore, we design two variants of S2DN to verify the effectiveness of each module by removing: (i) the Semantic Smoothing module (called S2DN w/o SS), (ii) the Structure Refining module (called S2DN w/o SR).\nComparison with Baselines (RQ1)\nTo address RQ1, we present the performance comparison of S2DN and baseline models in predicting missing links for emerging entities as shown in Tables 1 and 2 (Refer to Appendix C.1 for NELL-995). Our results demonstrate that S2DN achieves comparable performance to the baseline models across all datasets.\nSpecifically, we make the following observations: (1) S2DN shows improved average performance over rule-based inductive methods with Hits@10 metrics of 9.2%, 4.7%, and 6.8% on WN18RR, FB15k-237, and NELL-995 respectively. Furthermore, GNN-based methods like COMPILE and TAGT outperform various rule-based approaches in ranking tasks on most datasets, indicating the effectiveness of GNN-based methods in leveraging neighboring information and structures for inductive KGC. (2) SNRI, which integrates local semantic relations, outperforms COMPILE and TAGT on multiple datasets, highlighting the importance of utilizing local semantic relations within KGs for inductive KGC. (3) RMPI, through efficient message passing between relations to leverage relation patterns for KGC based on graph transformation and pruning, outperforms SNRI, emphasizing the significance of fully exploiting relational patterns and pruning links to enhance subgraph reasoning effectiveness. (4) S2DN surpasses other GNN-based subgraph reasoning methods on most datasets, indicating that denoising unconvincing knowledge by promoting the consistency of relations and reliability of structures significantly enhances inductive KGC performance. In summary, S2DN enhances the inductive reasoning capabilities of GNN-based models by effectively keeping relational semantics consistent and eliminating unreliable links, unlike previous GNN-based methods that overlook the impact of noise within KGs."}, {"title": "Ablation Study (RQ2)", "content": "We undertake an ablation study across all datasets for the inductive KGC. The results are depicted in Table 1 and Table 2 (Refer to Appendix C.1 for NELL-995), confirming the effectiveness of each module.\nS2DN w/o SS. After removing the semantic smoothing module, there is a notable performance decline across most datasets for inductive subgraph reasoning. This finding underscores the efficacy of maintaining relation consistency within the encompassing subgraph. Consequently, an informative enclosing subgraph featuring semantically consistent relations holds the potential to enhance S2DN.\nS2DN w/o SR. The exclusion of the structure refining module results in performance degradation across various datasets. This observation highlights the inadequacy of unreliable subgraph structures in conveying information effectively for the downstream KGC task, failing to mitigate the impact of noisy interactions. Conversely, a dependable structure or pristine subgraph enhances inductive reasoning capabilities by disregarding potential noise and preserving reliable interactions."}, {"title": "Robustness of S2DN (RQ3)", "content": "Semantic Consistency of Relations. We conduct an experiment to analyze the semantic consistency. Specifically, as shown in Section , relations with similar semantics tend to be categorized into the same category, which benefits the semantic consistency of relations. During the inference process on the test dataset of WN18RR_V1, FB15k-237_V1, and NELL-995_V1, we count the proportion \\(m_{ij}\\) of relation i that is categorized into another relation j as the degree of semantic consistency. Thus, we can visualize the proportions \\(M = \\{m_{ij}|0 \\leq i,j \\leq |R|\\}\\) of all relation pairs as shown in Figure 4. For FB15k-237_V1 and NELL-995_V1, we singled out relationships related to the topic of movie and sport, respectively. We observe from Figure 4 that relations with similar semantics have a higher transformation rate than those with dissimilar semantics (e.g, the similar relations instance and meronym in Figure 4a, edited by and written by in Figure 4b, and athlete_sport and athlete_team in Figure 4c). This phenomenon also proves that the Semantic Smoothing module can unify relations with similar semantics, thus maintaining the semantic consistency of the knowledge graph relations.\nReliability of S2DN on Noisy KGs. We generate different proportions of semantic and structural negative interactions (i.e., 5%, 15%, 35%, and 50%) to contaminate the training KG. Semantic noises are created by randomly replacing the relations with others from known triples (e.g, the relation edited_by is replaced by written_by), while the structure noises are generated by sampling unknown triples from all entity-relation-entity combinations. We then compare the performance of RMPI, S2DN, and its variants on noisy KGs."}, {"title": "Additional Experiments", "content": "C.1 Inductive KGC performance of S2DN on NELL-995\nThe inductive KGC performance of S2DN on the NELL-995 dataset is shown in Table 9. We can observe that S2DN achieves comparable performance with previous rule- and GNN-based models on the NELL-995 dataset.\nC.2 Reliability of S2DN on NELL-995\nWe generate different proportions of semantic and structural negative interactions (i.e., 5%, 15%, 35%, and 50%) to contaminate the training KGs. We show the performance of RMPI, S2DN, and its variants on noisy KGs of the NELL-995 dataset in Table 8. As the conclusion of Section , we can observe the same phenomenon in NELL-995. This observation shows S2DN can effectively mitigate unconvincing knowledge while providing reliable local structure.\nC.3 Visualization\nTo explicitly demonstrate the ability of S2DN to provide reliable links to downstream tasks, we designed case studies on three datasets. We visualize the exemplar reasoning subgraph of S2DN (i.e., the refined subgraph) and GraIL (i.e., the original subgraph) models for different queries (selected from the V1 version of all three datasets, and ) in Figure 7. As illustrated in Figure 7, we observe that compared to GraIL, S2DN can provide more knowledge for enhanced subgraph reasoning while retaining the original reliable information. For example, Figure 7(a) and Figure 7(b) show the subgraphs from GraIL and S2DN have a similar layout, while S2DN offers more links and filter out irrelevant interaction between source and target entities. This indicates that S2DN is effective in subgraph reasoning inductively by a structure-refined mechanism.\nC.4 Hyperparameter Sensitivity\nThis section focuses on assessing the influence of different hyperparameter configurations on the inductive KGC task. To accomplish this, we conduct a thorough hyperparameter analysis using the V1 versions of three datasets: WN18RR_V1, FB15k-237_V1, and NELL-995_V1.\nC.4.1 Impact of learning rate We investigate the effect of the learning rate of S2DN by varying it from 0.0005 to 0.1 over three datasets. As illustrated in Figure 8, we can find that using S2DN with a lower learning rate (e.g., 0.0005 for WN18RR and NELL-995, and 0.001 for FB15k-237) performs better on three datasets than a higher one. Specifically, as the learning rate increases, the performance on the FB15k-237 and NELL-995 decreases, indicating that S2DN is sensitive to the setting of the learning rate over these two datasets. In contrast, the performance of S2DN on WN18RR"}, {"title": null, "content": "C.5 Comparison with LLaMA2-7B\nWe use the LLM-based KGC model KAPING (Baek, Aji, and Saffari 2023) as our baseline. In KAPING, a query, its context (i.e., relevant facts surrounding the query), and its answer (e.g., the tail entity) are structured as fine-tuning instructions. These constructed instructions are then used to fine-tune an LLM, such as LLaMA2, enabling it to predict possible tail entities for given queries. The performance results, shown in Table 12, indicate that KAPING does not perform optimally on the WN18RR_V1 and FB15k-237_V1 datasets. This limitation is likely due to the lack of text information for entities (e.g., only identifier /m/0gq9h in FB15k-237 is given), which constrains the reasoning ability of LLMs. Additionally, we frame KAPING as a Yes/No question, like 'Is /m/01t_vv the /film/film/genre of /m/0qf2t?'. KAPING can perform better with an accuracy of 69.81%."}, {"title": "Technical Appendix", "content": "A. S2DN\nA.1 Proof of Lemma 1 (Smoothing Objective)\nWe provide the proof of Lemma 1.\nProof. We prove Lemma 1 following the strategy of Proposition 3.1 in (Achille and Soatto 2018). Suppose E is defined by Y and En, and \\(\\tilde{E}\\) depends on En only through E. We define the Markov Chain < (Y, En) \u2192 E \u2192 \\(\\tilde{E}\\) >. According to the data processing inequality (DPI), we have"}, {"title": null, "content": "\\begin{equation}\nI(\\tilde{E}; E)\\geq I(\\tilde{E}; Y, E_n)\n\\end{equation}\n\\begin{equation}\n= I(\\tilde{E}; E_n) + I(\\tilde{E}; Y|E_n)\n\\end{equation}\n\\begin{equation}\n= I(\\tilde{E}; E_n) + H(Y|E_n) \u2013 H(Y|E_n;\\tilde{E}).\n\\end{equation}\nAs we know, En is task-irrelevant noise independent of Y. Thus, we have H(Y|En) = H(Y) and H(Y|En;\\tilde{E}) \u2264 H(Y|\\tilde{E}). Then, we have\n\\begin{equation}\nI(\\tilde{E}; E)\\geq I(\\tilde{E}; E_n) + H(Y|E_n) \u2013 H(Y|E_n;\\tilde{E})\n\\end{equation}\n\\begin{equation}\n\\geq I(\\tilde{E}; E_n) + H(Y) \u2013 H(Y|\\tilde{E})\n\\end{equation}\n\\begin{equation}\n= I(\\tilde{E}; E_n) + I(\\tilde{E}; Y).\n\\end{equation}\nFinally, we obtain I (\\(\\tilde{E}\\); En) \u2264 I(\\(\\tilde{E}\\); E) \u2013 I(\\(\\tilde{E}\\); Y).\nA.2 Algorithm\nThe full training process of S2DN is shown in Algorithm 1. At the beginning, we initialize the entity embedding X and relation embedding E by designed features and Xavier initializer, respectively. Given a sample ((u,r,v), Y(u,r,v)) from training data U, we extract its enclosing subgraph g = (V, E) and feed g into the semantic smoothing and structure refining modules. In the flow of the semantic smoothing module, we blur the relations of g with consistent semantics into a unified embedding space and get the"}]}