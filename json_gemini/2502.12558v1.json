{"title": "MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos", "authors": ["Huaying Yuan", "Jian Ni", "Yueze Wang", "Junjie Zhou", "Zhengyang Liang", "Zheng Liu", "Zhao Cao", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Retrieval augmented generation (RAG) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multimodal large language models (MLLMs) to generate high-quality answers in a cost-effective way. In this work, we present MomentSeeker, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key advantages. First, it incorporates long videos of over 500 seconds on average, making it the first benchmark specialized for long-video moment retrieval. Second, it covers a wide range of task categories (including Moment Search, Caption Alignment, Image-conditioned Moment Search, and Video-conditioned Moment Search) and diverse application scenarios (e.g., sports, movies, cartoons, and ego), making it a comprehensive tool for assessing retrieval models' general LVMR performance. Additionally, the evaluation tasks are carefully curated through human annotation, ensuring the reliability of assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic data, which demonstrates strong performance on our benchmark. We perform extensive experiments with various popular multimodal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods. Our created resources will be shared with community to advance future research in this field.", "sections": [{"title": "1 Introduction", "content": "Long video understanding remains a significant challenge (Fu et al., 2024; Zhou et al., 2025; Wang et al., 2024b). Retrieval augmented generation (RAG) (Luo et al., 2024; Ataallah et al., 2024) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multimodal large language models (MLLMs) to generate high-quality answers in a cost-effective way. A key research problem is how to accurately retrieve essential information to answer a given query based on user instructions. In real word search scenarios such as video agent (Fan et al., 2024; Wang et al., 2024c), users may ask \"Where did I put my phone?\", which requires the model to analyze the complete historical context to pinpoint relevant moments. These emerging demands underscore the growing importance of video retrieval in practical applications. In this paper, we formally define the task of precisely locating key information segments within a long video based on a given query as long-video moment retrieval (LVMR).\nExisting video retrieval benchmarks (Xu et al., 2016; Chen and Dolan, 2011; Lei et al., 2021; Hendricks et al., 2017) evaluate cross-modal retrieval by using video captions as text queries to retrieve relevant videos from a candidate pool. However, these benchmarks are primarily designed for short videos, resulting in a significant disparity from real-world long-video search scenarios. Practical applications often require capabilities such as retrieving discontinuous events within lengthy context, which remains unaddressed in current frameworks. Furthermore, these benchmarks rely on sentence-like captions as queries, which presents a marked contrast with real-world retrieval scenarios where queries are typically user-generated (e.g., natural language questions).\nTo address these gaps, we present MomentSeeker, a comprehensive benchmark specifically designed for evaluating long-video moment retrieval (LVMR) capabilities. It introduces three fundamental innovations:\n(1) First LVMR-Specialized Benchmark. Existing multi-modal retrieval benchmarks predominantly focus on short video clips, failing to address"}, {"title": "2 Related Works", "content": "2.1 Video Understanding Benchmarks\nAs shown in Table 1, current video retrieval benchmarks include MSR-VTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), VATEX (Wang et al., 2020), and ActivityNet (Heilbron et al., 2015). These benchmarks evaluate cross-modal retrieval by using video captions as queries to rank videos within a candidate pool. Temporal grounding benchmarks like QVHighlights (Lei et al., 2021), Charades-STA (Gao et al., 2017), THUMOS14 (Jiang et al., 2014), and DiDemo (Hendricks et al., 2017) attempt to address moment retrieval but are limited to short videos (under three minutes). This leaves a gap in handling long-form content, where events may require multi-step reasoning.\nThere are also long-video understanding benchmarks, such as Video-MME (Fu et al., 2024) and MLVU (Zhou et al., 2025). However, these benchmarks focus on video question answering and do not directly evaluate the representational capability of video encoders. In contrast, our proposed MomentSeeker benchmark is designed for long-video moment retrieval, assessing a model's retrieval ability within a single long video and addressing the limitations of existing benchmarks.\n2.2 Multi-modal Embedding Models\nMost video retrievers (Li et al., 2022b; Wang et al., 2022a; Zhu et al., 2023) use a dual-tower architecture like CLIP (Radford et al., 2021) and train with contrastive learning on large datasets. InternVideo (Wang et al., 2022b) integrates generative and discriminative self-supervised learning via masked video modeling and contrastive learning. InternVideo2 (Wang et al., 2025) expands upon this by incorporating larger models, featuring an encoder with up to 6 billion parameters. Language-Bind (Zhu et al., 2023) aligns multiple modalities in a shared language space, while VAST (Chen et al., 2023) integrates vision, audio, and text encoders to have a thorough understanding. These models capture spatiotemporal information, forming the basis of video understanding.\nIn image retrieval field, several studies (Zhang et al., 2024; Zhou et al., 2024; Wei et al., 2025) have explored multi-modal input for composed image retrieval. For example, MagicLens (Zhang et al., 2024) uses open-ended instructions to capture complex semantic relationships and trains a composed image retriever, while MM-Ret (Zhou et al., 2024) explores further with more effective training data. Inspired by this, CoVR (Ventura et al., 2024) applies the same pipeline in video retrieval, generating video-text-video triplets from paired videos with similar captions, and provides a BLIP-style (Li et al., 2022a) composed video retriever. These advancements expand the capabilities of foundation models in multi-modal understanding, broadening task adaptability.\nFurthermore, with the rise of large language models, some works attempt to transform multimodal large models (MLLMs) into universal embedders to construct more powerful representations, such as E5-V (Jiang et al., 2024) and MLLM2Vec (Jiang et al., 2025). However, in the video domain, a robust universal embedder has yet to emerge. For this reason, we propose V-Embedder, which outperforms existing video embedders across multiple tasks through synthetic-data-driven training."}, {"title": "3 Benchmark: MomentSeeker", "content": "3.1 Overview\nWe present MomentSeeker, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (LVMR) tasks. Our framework systematically evaluates model capabilities through three dimensions: 1). Question Types: Descriptive captions vs. natural language queries. 2). Query Modalities: Text-only, text+image, or text+video inputs to support real-world multi-modal search. 3). Data Domains: Five diverse domains (sports, egocentric, movies, cartoons, anomalies) to test generalization. These axes define 18 retrieval scenarios, consolidated into four meta-tasks that probe distinct capabilities:\n\u2022 Caption Alignment: Cross-modal matching between caption and video moments.\n\u2022 Moment Search: Text-only localization according to instructional queries.\n\u2022 Image-Conditioned Moment Search: Retrieval guided by text and image exemplars.\n\u2022 Video-Conditioned Moment Search: Retrieval guided by text and video content."}, {"title": "3.2 Task Definition", "content": "To achieve video content retrieval, we first have human experts annotate precise answer intervals within each video, which serve as ground truth. The remaining segments are randomly split into short candidate clips, forming a retrieval pool. The goal is to retrieve the most relevant clip from a pool of candidates based on a given query. Formally, given a query $q$ and a set of $N$ candidate clips $C = {c_1, c_2, ..., c_N}$, the retrieval process is defined as:\n$c^* = \\arg \\max_{c_i \\in C} S(q, c_i)$,\nwhere $S(q, c_i)$ denotes the similarity score between the query $q$ and candidate clip $c_i$.\nIn the domain of video retrieval, tasks are categorized into four meta-tasks: Moment Search (MS), Caption Alignment (CA), Image-conditioned Moment Search (IMS), and Video-conditioned Moment Search (VMS). Each of these tasks presents unique challenges and serves distinct purposes in the broader context of multimedia understanding and retrieval. Below, we elaborate on each task.\n3.2.1 Caption Alignment\nCaption Alignment is defined as retrieving the most relevant segment from a set of candidate videos based on a textual video description $q_{caption}$. Caption alignment is a fundamental task in video retrieval, aiming to match a textual description with the corresponding video segment. This task requires the system to bridge the gap between explicit textual and visual modalities, serving as the foundation of video retrieval.\n3.2.2 Moment Search\nMoment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a instructional natural language query $q_{inst}$. Unlike the straightforward matching in caption alignment, Moment search requires the system to comprehend the natural language instructions, which can be semantically ambiguous or context-dependent. The system must map these instructions to specific visual content within a video, recognizing objects and actions while also comprehending the deeper semantics of the temporal sequence of events.\n3.2.3 Image-conditioned Moment Search\nImage-conditioned Moment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a natural language query $q_{inst}$ combined with an image $I$, denoted as $q = {q_{inst}, I}$. Image-conditioned moment search introduces an additional layer of complexity by incorporating both textual and visual information in the query. The system must not only understand the natural language query but also interpret the visual context provided by the image. This task requires the integration of multi-modal information, where the image may provide contextual clues that are not explicitly mentioned in the text.\n3.2.4 Video-conditioned Moment Search\nVideo-conditioned Moment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a natural language query $q_{inst}$ combined with a reference video $V$, denoted as $q = {q_{inst}, V}$. Video-conditioned moment search is perhaps the most complex of the four tasks, as it requires the system to process and understand both a textual query and a reference video. The"}, {"title": "4 V-Embedder: A Flexible and Powerful Video Embedder", "content": "Given the diverse range of tasks outlined in the previous section, existing state-of-the-art embedding models (Wang et al., 2025; Zhu et al., 2023; Zhou et al., 2024) struggle to offer comprehensive support. To address this, we introduce V-Embedder, a MLLM-based video embedding framework that leverages synthetic data for task-flexible, multi-modal video retrieval. Unlike traditional clip-style retrievers (Wang et al., 2025; Zhu et al., 2023), which process textual and visual inputs separately, V-Embedder utilizes a MLLM as its embedding backbone and dynamically integrates textual descriptions with conditioned image frames based on specific task instructions. This unique approach enables fixed-dimensional embeddings optimized for diverse applications, from caption alignment to visual-conditioned moment search. The overall training pipeline consists of two key components: synthetic data generation and contrastive learning.\nSynthetic Data Generation. Due to the scarcity of long-video datasets with transition-specific textual annotations, we use a retrieval-based approach to mine similar video pairs from a large candidate"}, {"title": "5 Experiments", "content": "5.1 Annotation Overflow\nWe recruit graduate students in computer science to annotate the dataset. Annotators watch videos and perform: (1) Moment Search (MS): posing a question and marking relevant segments, (2) Image-Conditioned Moment Search (IMS): selecting a frame, posing a question, and marking relevant segments. To balance quality and efficiency, MS and IMS are manually labeled, while Video-Conditioned Moment Search (VMS) is auto-generated by extending IMS frames with a random temporal window. Annotation guidelines are in"}, {"title": "5.2 Experiment Settings", "content": "5.2.1 Baselines\nOur baselines are grouped into three categories:\nCLIP-style Video Encoders. This category includes methods such as MagicLens (Zhang et al., 2024), MegaPairs (Zhou et al., 2024), UniIR (Wei et al., 2025), InternVideo2 (Wang et al., 2025), and LanguageBind (Zhu et al., 2023). These approaches leverage joint training of video and text to efficiently extract and align cross-modal features.\nComposed Video Encoder Approaches. Represented by COVR (Ventura et al., 2024), these methods simultaneously consider both text and video queries to search for relevant videos in a database, enabling a more integrated query handling.\nMLLM-based Multi-Modality Encoders. Methods like E5V (Jiang et al., 2024) and MLLM2Vec (Jiang et al., 2025) fall into this category. They utilize large-scale pre-trained multi-modal models to handle flexible modality compositions (e.g., video + text or text alone) and generating multi-modal embeddings.\n5.2.2 Implementation Details\nEvaluation. For each video, we extract the ground truth video clips of all questions, fully retaining them. The left parts of the video is randomly cropped (2 to 30s) as the candidate video clips. Each query's candidate clips are sourced exclusively from its corresponding video.\nFollowing previous works (Jiang et al., 2025, 2024), we incorporated instructions (e.g., \"Represent the given question in one word.\u201d and \u201cRepresent the video in one word.\") to guide the model in generating informative embeddings for queries and candidates. All baselines are reproduced using their official implementations with default settings. Instruction usage remains consistent with each baseline's original configuration. For image-based baselines, we use the middle frame of the video as input. CoVR (Ventura et al., 2024) follows its original configuration with 15 frames at a resolution of 384, while LanguageBind, InternVideo2 (Wang et al., 2025), and V-Embedder each use 8 frames at a resolution of 224.\nV-Embedder Training. For synthetic data, we use LanguageBind (Zhu et al., 2023) encoders to\""}, {"title": "5.3 Main Results", "content": "The overall evaluation results for all baselines in the MomentSeeker benchmark are shown in Table 2. Individual performances are reported for each task, while average score of Recall@1 are provided. From the results, we derive five primary conclusions:\n1. CLIP-style video encoders perform well on text-based retrieval tasks but struggle with multi-modal retrieval. InternVideo2 achieves an"}, {"title": "6 Conclusion", "content": "In this paper, we introduce MomentSeeker, a benchmark for evaluating retrieval models in LVMR. MomentSeeker offers three main advantages: it focuses on long videos, covers a wide range of task categories and application scenarios, and ensures reliability through human annotation. We also demonstrates strong performance using a fine-tuned MLLM-based LVMR retriever. Extensive experiments highlight the challenges of LVMR and the limitations of existing methods, with resources shared to advance future research."}, {"title": "7 Limitations", "content": "This paper focuses on moment retrieval within long videos and presents a comprehensive benchmark evaluation. For the model, we employ synthetic data to train a MLLM capable of understanding complex instructions and integrating multiple modalities. However, compared to a standalone encoder (Wang et al., 2025; Zhu et al., 2023), the MLLM has more parameters, resulting in longer inference time. In the future, we plan to explore techniques such as model distillation to reduce model size and inference cost."}, {"title": "Ethics Statement", "content": "Our benchmark video data is sourced from publicly available datasets (Mangalam et al., 2023; Zhou et al., 2025; Wang et al., 2024b; Sultani et al., 2019), which have undergone rigorous screening by the original teams to remove harmful content. Despite our best efforts, we acknowledge that these videos may not be entirely comprehensive or free from omissions. Furthermore, we strongly discourage the use of V-Embedder models for encoding or retrieving sensitive content."}, {"title": "8 Appendix", "content": "Annotation Guideline\n8.0.1 Task 1: Video Search (MS)\nObjective: Generate a question answerable by a specific video segment and annotate the corresponding one or more answering segments.\nScope: Both the question and answer must originate from the same video.\nSteps:\nWatch the entire video to understand its content (e.g., actions, events, objects).\n1. Formulate a question:\n\u2022 Ensure the question is specific and requires one or several continuous segments as the answer.\n\u2022 Example questions:\n-\"Where was the weighing scale?\"\n-\"What did I put in the trashcan?\u201d\n2. Annotate the answer segment:\n\u2022 Mark the start and end timestamps (format: [[MM:SS-MM:SS], [MM:SS-MM:SS]...])\n\u2022 Ensure the segments fully answer the question without truncation.\n\u2022 Cover all segments that answer the question\u2014no omissions allowed.\n3. Validation: Replay the annotated segment to confirm alignment with the question.\n8.0.2 Task 2: Image-Conditioned Video Search (IMS)\nObjective: Generate a question based on a static image (from the same or a different video) and annotate the answer segments in the target video.\nSteps:\n1. Select an image:\n\u2022 Same video: Choose a key frame (e.g., action initiation, critical object appearance).\n\u2022 Different video: Use a frame from another video with relevant content (e.g., similar objects, scenes).\n2. Formulate a question:\n\u2022 The question must directly relate to the selected image, and the answer must exist in the target video."}, {"title": "8.0.3 Quality Requirements", "content": "1. Consistency: Ensure IVS questions are tightly linked to the image, and answers are precise.\n2. Cross-video logic: If using an image from another video, the question must relate to the target video's content (avoid mixing contexts).\n3. Timestamp accuracy: Annotated segments must have <1-second error tolerance.\nReport ambiguities or edge cases to the project lead for resolution."}]}