{"title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners", "authors": ["Yifei Gao", "Jie Ou", "Lei Wang", "Fanhua Shang", "Jaji Wu", "Jun Cheng"], "abstract": "Large Language Models (LLMs) showcase remarkable performance and robust deductive capabilities, yet their expansive size complicates deployment and raises environmental concerns due to substantial resource consumption. The recent development of a quantization technique known as Learnable Singular-value Increment (LSI) has addressed some of these quantization challenges. Leveraging insights from LSI and our extensive research, we have developed innovative methods that enhance the performance of quantized LLMs, particularly in low-bit settings. Our methods consistently deliver state-of-the-art results across various quantization scenarios and offer deep theoretical insights into the quantization process, elucidating the potential of quantized models for widespread application.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have attracted considerable attention due to their exceptional performance on a variety of downstream tasks and their capability to demonstrate emergent behaviors (Bubeck et al., 2023; Touvron et al., 2023a). Furthermore, their proficiency in natural language understanding and deductive reasoning also extends to multimodal domains via alignment training (Mu et al., 2023; Xu et al., 2023; Zhang et al., 2023b). However, training and maintaining these LLMs require significant resources, often necessitating multiple GPUs to support just a single model or its components. In this scenario, quantization becomes an essential strategy, providing solutions to decrease both the memory footprint and computational requirements of LLMs.\nIn recent years, a variety of quantization methods have been developed. Quantization-Aware Training (QAT) (Liu et al., 2023a) involves adjusting the model during its training phase to enhance its compatibility with quantization techniques. On the other hand, Post-Training Quantization (PTQ) (Frantar and Alistarh, 2022a; Frantar et al., 2022; Shao et al., 2023), modifies models without the need for explicit training processes, making PTQ methods notably quicker than QAT. This speed has led to their significant attention and broad application. Additionally, extensive research and experiments have revealed that a considerable number of errors in the quantization process stem from a few outliers with unique weight values. Addressing this, several studies (Dettmers et al., 2023b; Wei et al., 2022b, 2023; Lee et al., 2023) aim to reduce or mitigate these outliers to lessen the disturbances they create.\nRecently, a sophisticated method called Learnable Singular-value Increments (LSI) was introduced by (Gao et al., 2024). This method integrates both transformations of weight and activation difficulties (Xiao et al., 2023; Shao et al., 2023) and the impacts of adjusting model weights (Frantar et al., 2022; Liu et al., 2023a). Meanwhile, LSI maintains a superior reference speed together with extremely fast training process. While LSI has achieved notable results, its theoretical analysis remains lacking. Additionally, not all issues observed in their experiments have been fully addressed. For instance, in some cases, the performance of quantized models may surpass that of the original models in specific downstream tasks, yet in others, they significantly lag behind.\nIn this paper, we explore the underlying factors contributing to the success of LSI and, based on these findings, propose a novel method named Diagonal expansion of Learnable Singular Value (DLSV). Our method enhances the disturbance properties of the weight matrix in LSI, aiming to further unlock the potential of quantized language models. Through comprehensive experimentation,"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Quantiztaion Methods", "content": "Uniform Quantization. Regardless of the specific methods used, the optimization objective for quantizing the weight matrix in LLMs can be uniformly expressed as follows, focusing solely on linear weight matrices:\n$\\arg \\min||F(W, X) \u2013 F(Q_w(W)), Q(X))||. (1)$\nIn this context, F represents the mapping function for a transformer block in the Large Language Model (LLM), W and X denote the weight and activation matrices. $Q_w(\\cdot)$ and $Q_x(\\cdot)$ are the quantization functions for weights and activations, respectively. Quantizers typically function by first managing the distribution of their inputs (either weights or activations) through scaling and clipping, then rounding the input to the nearest n-bit integer values:\n$Q_u(W, n) = Clamp(\\frac{W}{S_h}+z, 0, 2^n \u2013 1), (2)$\nwhere n is the designed bit number, $s_h$ and z are scaler and zero point, generated by W, and $Clamp(\\cdot)$ the rounding function. This rounding process introduces precision loss due to the differences between the original floating-point inputs and their quantized integer counterparts. The goal of the optimization function is to minimize the discrepancy in output between the original model and its quantized version.\nSmooth and Weight Clipping. The smooth technique involves modifying the magnitudes of weights or activations by applying a scaling matrix $diag(s_c)$:\nY = XW + B = [(X-8) \\otimes S_c] [S_\\frac{W}{c}] + [B + S_W]\nX\nW\nB\n(3)\nIn the described technique, Y is the final output, B the bias. X, W, and B represent the equivalent activation, weight, and bias after scaling. The operators $\\otimes$ and $\\oslash$ denote element-wise division and multiplication, respectively. Meanwhile, weight clipping addresses outlier issues by restricting the maximum and minimum values of weights and activations, ensuring that these values remain within a suitable range to prevent distortion in the quantized model. For more details on this approach, please refer to (Shao et al., 2023)."}, {"title": "2.2 Learnable Singular-value Increment", "content": "Starting from the singular-value decomposition, LSI first decomposes a linear weight matrix $M\\in$"}, {"title": "3 Our Method", "content": "In this section, we will initially simplify the quantization issues and explore them from the perspective of linear weight matrices only, and subsequently, we will demonstrate our method."}, {"title": "3.1 Weight Adjustment", "content": "In contemporary Large Language Models (LLMs) and transformer blocks, linear functions comprise the majority of their computational mechanisms. Thus, achieving lossless quantization of linear weight matrices could significantly reduce quantization challenges. Beyond traditional methods that primarily concentrate on the quantization process or handling outliers, LSI introduces a technique for weight adjustment that allows weights to be categorized into different hierarchies, aligning them more effectively with quantization settings. From Eq. 2, it is evident that the rounding function grants the weight matrix W the flexibility to have its elements rounded to the same value. This implies that for each element in W, as long as they fall within a specific range, the rounding results will be identical. Specifically, if we assume there exists an optimal quantized weight matrix $M_Q$ for the linear weights M that can be achieved through optimization, the optimization goal for the LSI technique"}, {"title": "3.2 Diagonal Expansion of Learnable Singular Values (DESV)", "content": "As outlined in Eq. 4 and Eq. 5, the process involves first flattening the one-dimensional matrices S and I into two-dimensional matrices where only the diagonal elements are populated. These diagonal matrices are then used in a multiplication process with the other two matrices, U and V. In our method, we concentrate on the flattened two-dimensional matrix and treat its diagonals along different axes as learnable parameters, as illustrated in Fig. 2. Building upon Eq. 5, our approach involves the diagonal expansion of learnable singular values, denoted as $I^D \\in R^{(2n+1)\\times b}$, where n represents the number of diagonals extended along both positive and negative axes. Our method encapsulates the modifications in a formulaic representation:\nM' = U (diag(S) + Map(ID)) V, (7)\nIn this context, $Map(\\cdot)$ is a mapping function that positions the elements of $I^D$ appropriately to multi-axis diagonal elements within $diag(S)$. For simplicity, we combine $diag(S) + Map(I')$ into a single matrix D \u2208 $R^{a\\times b}$ for simplicity, aligning with the depiction in Fig. 2. Subsequently, each"}, {"title": "3.3 Quantization Problem Analysis", "content": "Our analysis starts with the distinct performance characteristics of quantized models employing LSI techniques. We initially observed that in some downstream tasks, these quantized models outperform their original versions, while in others, they significantly underperform compared to methods like GPTQ, OmniQuant, and LLMQAT, though they still remain competitively viable. Upon deeper investigation, we discovered that such phenomena, though more pronounced in the LSI series, are not exclusive to it. These unusual outcomes, which become even more accentuated under our technique, should not be dismissed as mere anomalies. Rather, we argue that further exploration into these effects could provide deeper insights into the field. Our extensive research has revealed a similar scenario that might explain these phenomena.\nWe began our investigation by assessing the expressiveness of outputs from both original and quantized models, utilizing singular value decomposition to extract their singular values. Our analysis revealed no significant differences in expressiveness among the same model families, such as LLaMA or OPT (Touvron et al., 2023a; Zhang et al., 2022b), across three quantization methods: OmniQuant (Shao et al., 2023), LSI (Gao et al., 2024), and our own technique. Drawing on insights from LSI, which proactively introduces weight disturbances during the quantization process, we redirected our focus to examining the impact of these weight adjustments. The complete details of our experiments and additional discussions are available in the supplementary materials.\nAs highlighted in (Gao et al., 2023), rescaling and rearranging attention weights\u2014a type of weight disturbance\u2014can enhance or impair specific capabilities of LLMs. Our quantization approach, detailed in Eq. 6 and Eq. 9, allows for modifications in weight without introducing additional errors. Furthermore, the integration of I and ID enhances the flexibility of weight hierarchies and magnitudes. Unlike the hand-crafted GPTQ se-"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Settings", "content": "Quantization. Our experiments encompass both weight-only and weight-activation quantization scenarios, and we present some results using low-bit configurations. In these configurations, 'W' denotes weight and 'A' indicates activation. When employing group-wise scaling, as discussed in (Frantar et al., 2022; Dettmers et al., 2023b), where each group is assigned unique quantization parameters, 'g' represents the group name. To minimize disruptions during quantization, we retained the Softmax component in float32.\nTraining. Our training configurations are generally the same as the LSI (Gao et al., 2024). We initialize our scaling and shifting parameters from (Shao et al., 2023) and keep their learning rates as in (Shao et al., 2023), then we train DESV parameters with a low learning rate at 1.5e-4. We employ the AdamW (Loshchilov and Hutter, 2019) optimizer with a weight decay of 0 to optimize our parameters. The data used in our training was collected from WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020). For expended diagonal number n, we generally set it to 100. The increase of n will introduce additional training time but without significant enhancement of model performance, and 100 is just enough through our tests.\nModels. We conduct evaluation on two popular baselines for generalization, LLaMA(7-30B) (Touvron et al., 2023a) and OPT(125M-66B) (Zhang et al., 2022b) for generalization. For comprehensive results about our test results, please see the supplementary materials."}, {"title": "4.2 Weight-only and Weight-activation Quantization Results", "content": "In this section, we focus on presenting the results of the OPT series in low-bit settings on Perplexity, as detailed in Tables 1 and 2. The data in these tables demonstrate that DESV offers more effective and powerful quantization solutions for LLMs than previous methods under similar conditions. Particularly, in low-bit weight-activation quantization, methods from the LSI series exhibit notably strong performance. However, while the LSI performs stable within the OPT family, it tends to experience performance degradation in the LLaMA family, where outliers and weight hierarchies are more sensitive (Shao et al., 2023; Gao et al., 2024), even when initiated with well-trained OmniQuant parameters. With more learnable parameters for adjustments compared to LSI, our methods consistently exhibit stable and outstanding results compared with previous methods. Our complete results are available in the supplementary material."}, {"title": "4.3 Downstream Tasks Quantization Results", "content": "In this section, we showcase the performance of various downstream tasks using the OPT and LLAMA series, detailed in Table 3 and 4. Our results highlight some matrix improvements compared to previous methods and for bigger models, the improvements are more evident. Moreover, Different aligning datasets could also enhance various model properties, each providing unique advantages as shown in Table 4. While our methods yield significant improvements in perplexity, amplifying one aspect of performance can also lead to a reduction in another. This phenomenon is particularly prominent when"}, {"title": "5 Other Issues", "content": ""}, {"title": "5.1 Inference Speed", "content": "During the inference procedure, our technique does not introduce additional operations or parameters compared to OmniQuant (Shao et al., 2023). We also adhere to the criteria proposed by MCL-LLM*, which focuses on versatile deployment solutions for diverse language models across various hardware platforms. The inference speed, demonstrated in Table 6, shows only minor variations compared to OmniQuant, further proving the conciseness and simplicity of our technique during inference."}, {"title": "5.2 Performance Degradation.", "content": "As previously suggested, our quantization method enhances certain model properties at the cost of others. This trade-off is evident in Degraded part in Table 5, where our method shows significant accuracy degradation but still maintains a competitive accuracy norm. Notably, clear degradation is observed primarily in the LLaMA families. In"}, {"title": "5.3 Training Variations.", "content": "Through our experiments, we found that our techniques offer greater stability compared to LSI (Gao et al., 2024) when used in weight-only and high-bit weight-activation quantization settings. However, in low-bit weight-activation scenarios such as W4A6 and W4A4, we observe fluctuations in downstream task performance, especially in LLaMA families. Notably, in tasks like PIQA and Boolq, these fluctuations can reach up to \u00b12.5, impacting the stability of results across other tasks as well. In our paper, we present balanced outcomes rather than these 'extreme' results. Additionally, the results displayed in Table 5 derive from the same models shown in Tables 3 and 4, which were trained on WikiText2."}, {"title": "6 Conclusion", "content": "We introduce DESV to optimize data alignment across various quantization settings, achieving state-of-the-art performance in diverse scenarios. Notably, in low-bit settings, our results significantly outperform previous methods. Furthermore, our techniques occasionally exceed the performance of the original models in certain downstream tasks, though they sometimes also experience substantial degradation in others. We believe that by fully har-"}, {"title": "A Appendix", "content": "In this appendix, we provide further details as follows:\n\u2022 Sec. A.1: We provide the analysis of the expressiveness of the original models and the quantized counterparts.\n\u2022 Sec. A.2: We provide the ablation studies of our techniques.\n\u2022 Sec. A.2: We analyze the training time consumption of our techniques.\n\u2022 Sec. A.4: We provide our complete results."}, {"title": "A.1 Expressiveness Analysis", "content": "In this section, we elaborate on the extensive experiments discussed in our main paper, focusing on comparing the expressiveness of quantized models to their original counterparts. We selected LLaMA-7B (Touvron et al., 2023a) and OPT-6.7B (Zhang et al., 2022b) as baseline models for generating text on WikiText (Merity et al., 2016) and C4 (Raffel et al., 2020), producing over 10,000 sentences with a length of 768 tokens each. We analyzed the final hidden states (a 2D matrix for each batch input) before they were input into the LM-Head (Brown et al., 2020; Touvron et al., 2023a; Zhang et al., 2022b), using singular value decomposition to assess each matrix. Our findings, illustrated in Fig. 3, reveal that changes in output expressiveness postquantization are similar across both model families and independent of the quantization method used. Notably, for OPT-6.7B, the expressiveness remains almost unchanged after quantization.\nIn the case of LLaMA-7B, the minor variations observed primarily arise from how outliers are managed. Both OmniQuant (Shao et al., 2023) and our investigations indicate that weights in the LLaMA family generally cluster within a narrow range but include some polarized outliers. The technique of Learnable Weight Clipping, introduced in (Shao et al., 2023), effectively addresses this issue by reducing the range of weight magnitudes. To validate this, we applied only the basic Learnable Equivalent Transformation during quantization and found that the expressiveness curve of the 'abridged version' of OmniQuant aligns perfectly with the original model's curve. This leads us to conclude that quantization does not significantly alter model properties, even in some low-bit settings, due to the inherent robustness of LLMs. Moreover, the slight increase in Perplexity (PPL) caused by quantized LLMs does not substantially affect their capabilities, as adjustments to the 'Temperature' parameter can be made to produce more 'categorical' responses. Therefore, the extraordinary results observed in the LSI series are more likely attributable to weight disturbances rather than fundamental changes in the model's core characteristics."}, {"title": "A.2 Ablation Study", "content": "In our ablation studies, we primarily examined the impact of varying the magnitude of n in our learnable parameter $I^D \\in R^{(2n+1)\\times b}$. The findings, pre-"}, {"title": "A.3 Training Time Consumption", "content": "We analyzed training time consumption as illustrated in Fig 5. Generally, for larger LLMs, our training duration is approximately 1.5 times longer than that of LSI. While our methods yield strong performances in low-bit settings, in higher-bit weight-only settings, such as W4A16 or W3A16g128, our results are comparable to those of LSI, making LSI sufficient for these scenarios. However, our methods exhibit greater stability in higher-bit weight-activation settings, except in the LLaMA families, where the weights are particularly sensitive."}, {"title": "A.4 Full Results", "content": "In this section, we provide a comprehensive presentation of our results across various datasets to complement the main paper. Specifically, the results include:\n\u2022 Wiki perplexity with weight-only quantization in the LLaMA families (Table 9).\n\u2022 PTB perplexity with weight-only quantization in OPT families (Table 10).\n\u2022 C4 perplexity with weight-only quantization in OPT families (Table 11)."}]}