{"title": "SEQUENTIAL ORDER-ROBUST MAMBA FOR\nTIME SERIES FORECASTING", "authors": ["Seunghan Lee", "Juri Hong", "Kibok Lee", "Taeyoung Park"], "abstract": "Mamba has recently emerged as a promising alternative to Transformers, offering\nnear-linear complexity in processing sequential data. However, while channels in\ntime series (TS) data have no specific order in general, recent studies have adopted\nMamba to capture channel dependencies (CD) in TS, introducing a sequential order\nbias. To address this issue, we propose SOR-Mamba, a TS forecasting method\nthat 1) incorporates a regularization strategy to minimize the discrepancy between\ntwo embedding vectors generated from data with reversed channel orders, thereby\nenhancing robustness to channel order, and 2) eliminates the 1D-convolution\noriginally designed to capture local information in sequential data. Furthermore,\nwe introduce channel correlation modeling (CCM), a pretraining task aimed at\npreserving correlations between channels from the data space to the latent space in\norder to enhance the ability to capture CD. Extensive experiments demonstrate the\nefficacy of the proposed method across standard and transfer learning scenarios.\nCode is available at https://github.com/seunghan96/SOR-Mamba.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series (TS) forecasting is prevalent in various fields, including weather (Angryk et al., 2020),\ntraffic (Cirstea et al., 2022), and energy (Dudek et al., 2021). While Transformers (Vaswani et al.,\n2017) have been widely employed for this task due to their ability to capture long-term dependencies\nin sequences (Wen et al., 2022), their quadratic computational complexity causes substantial compu-\ntational overhead, limiting their practicality in real-world applications. Several attempts have been\nmade to reduce the complexity of Transformers (Zhang & Yan, 2023; Zhou et al., 2022); however,\nthey often result in performance degradations (Wang et al., 2024).\nTo tackle the computational challenges of Transformers, alternatives such as state-space models\n(SSMs) (Gu et al., 2022) have been considered, employing convolutional operations to process\nsequences with linear complexity. Recently, Mamba (Gu & Dao, 2023) enhanced SSMs by incorpo-\nrating a selective mechanism to prioritize important information efficiently. Due to its strong balance\nbetween performance and computational efficiency (Wang et al., 2024), Mamba has been widely\nadopted across various domains (Zhu et al., 2024; Schiff et al., 2024). In the TS domain, Mamba is\nutilized to capture temporal dependencies (TD) by processing input TS along the temporal dimension\n(Ahamed & Cheng, 2024), channel dependencies (CD) along the channel dimension (Wang et al.,\n2024), or both (Cai et al., 2024). In this paper, we focus on Mamba capturing CD, in line with the\nrecent work (Liu et al., 2024a) that advocates for the use of complex attention mechanisms for CD\nwhile employing simple multi-layer perceptrons (MLPs) for TD.\nHowever, applying Mamba to capture CD is chal-\nlenging as channels lack an inherent sequential order,\nwhereas Mamba is originally designed for sequential\ninputs (i.e., Mamba contains a sequential order bias),\nas shown in Figure 1. To address this issue, previous\nworks have employed the bidirectional Mamba to cap-\nture CD (Wang et al., 2024; Liang et al., 2024), where\ntwo unidirectional Mambas with different parameters\ncapture CD from a certain channel order and its re-\nversed order. However, these methods are inefficient\ndue to the need for two models. Another approach"}, {"title": "2 RELATED WORKS", "content": "TS forecasting with Transformer. Transformers (Vaswani et al., 2017) are commonly employed for\nlong-term TS forecasting (LTSF) tasks due to their ability to handle long-range dependencies through\nattention mechanisms. However, their quadratic complexity has led to the development of various\nmethods aimed at improving efficiency, such as modifying the Transformer architecture (Zhang & Yan,\n2023; Zhou et al., 2022), patchifying the TS (Nie et al., 2023) or using MLP-based models (Chen et al.,\n2023; Zeng et al., 2023). While MLP-based models offer simpler structures and reduced complexity\ncompared to Transformers, they tend to be less effective at capturing global dependencies (Wang et al.,\n2024). Recently, iTransformer (Liu et al., 2024a) inverts the conventional Transformer framework in\nthe TS domain by treating each channel as a token rather than each patch, shifting the focus from\ncapturing TD to CD. This framework has led to significant performance gains and has become widely\nadopted as the backbone for TS models (Liu et al., 2024b; Dong et al., 2024).\nState-space models. To overcome the limitations of Transformer-based models, state-space models\nhave been integrated with deep learning to tackle the challenge of long-range dependencies (Rangapu-\nram et al., 2018; Zhang et al., 2023; Zhou et al., 2023). However, these methods are unable to adapt\ntheir internal parameters to varying inputs, which limits their performance. Recently, Mamba (Gu &\nDao, 2023) introduces a selective scan mechanism that efficiently filters specific inputs and captures\nlong-range context by incorporating time-varying parameters into the SSM. Due to its linear-time\nefficiency for modeling long sequences, it has been widely adopted in various domains, includ-\ning computer vision (Ma et al., 2024a; Huang et al., 2024; Zhu et al., 2024) and natural language\nprocessing (Pi\u00f3ro et al., 2024; Anthony et al., 2024; He et al., 2024).\nTS forecasting with Mamba. Due to its balance between performance and computational efficiency,\nMamba has also been applied in the TS domain. TimeMachine (Ahamed & Cheng, 2024) utilizes\nmulti-scale quadruple-Mamba to capture either TD alone or both TD and CD, with its architecture\nrelying on the statistics of the dataset. CMamba (Zeng et al., 2024) captures TD with patch-wise\nMamba and CD with an MLP. FMamba (Ma et al., 2024b) integrates fast-attention with Mamba\nto capture CD, and SST (Xu et al., 2024) captures global and local patterns in TS with Mamba"}, {"title": "3 PRELIMINARIES", "content": "Problem definition. This paper addresses the multivariate TS forecasting task, where the model uses\na lookback window $x = (X_1,X_2,\\cdots, X_L)$ to predict future values $y = (X_{L+1},\\ldots,X_{L+H})$ with\n$x_i \\in \\mathbb{R}^C$ representing the values at each time step. Here, $L$, $H$, and $C$ denote the size of the lookback\nwindow, the forecast horizon, and the number of channels, respectively.\nState-space models. SSM transforms the continuous input signals $x(t)$ into corresponding outputs\n$y(t)$ via a state representation $h(t)$. This state space represents how the state evolves over time, which\ncan be expressed using ordinary differential equations as follows:\n\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t) + Dx(t)$,\n\nwhere $h'(t) = \\frac{dh(t)}{dt}$, and $A$, $B$, $C$, and $D$ are learnable parameters of the SSMs.\nDue to the continuous nature of SSMs, discretization is commonly used to approximate continuous-tinuous-time representations by sampling input signals at fixed intervals.\nThis results in the discrete-time SSMs being represented as:\n\n$h_k = Ah_{k-1}+ Bx_k$,\n$Y_k = Ch_k + Dx_k$,\n\nwhere $h_k$ and $x_k$ are the state vector and input vector at time k, respectively, and $A = exp(\\Delta A)$ and\n$B = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B$ are the discrete-time matrices obtained from the $A$ and $B$.\nRecently, Mamba introduces selective SSMs that enables the model to capture contextual information\nin long sequences using time-varying parameters (Gu & Dao, 2023). Its near-linear complexity makes\nit an efficient alternative to the quadratic complexity of the attention mechanism in Transformers."}, {"title": "4 METHODOLOGY", "content": "In this paper, we introduce SOR-Mamba, a TS forecasting method designed to address the sequential\norder bias by 1) regularizing Mamba to minimize the distance between two embedding vectors\ngenerated from data with reversed channel orders and 2) removing the 1D-conv from the original\nMamba block. The overall framework of SOR-Mamba is illustrated in Figure 2, which consists of\nfour components: the embedding layer for tokenization, Mamba for capturing CD, MLP for capturing\nTD, and the prediction head for predicting the future output.\nFurthermore, we introduce a novel pretraining task, CCM, where the model is pretrained to preserve\nthe correlation between channels from the data space to the latent space, aligning with the recent TS\nmodels that focus on capturing CD over TD. The overall framework of CCM is illustrated in Figure 3."}, {"title": "4.1 ARCHITECTURE OF SOR-MAMBA", "content": "1) Embedding layer. To tokenize the TS in a channel-wise manner, we use an embedding layer that\ntreats each channel as a token, following the approach in iTransformer (Liu et al., 2024a). Specifically,\nwe transform $x \\in \\mathbb{R}^{L \\times C}$ into $z \\in \\mathbb{R}^{C \\times D}$ using a single linear layer.\n2) Mamba for CD. The original Mamba block combines the H3 block (Fu et al., 2023) with a gated\nMLP, where the H3 block incorporates a 1D-conv before the SSM layer to capture local information\nfrom adjacent steps. However, since channels in TS do not possess any inherent sequential order, we\nfind this convolution unnecessary for capturing CD. Accordingly, we remove the convolution from\nthe original Mamba block, resulting in the proposed CD-Mamba block, as illustrated in Figure 2(b).\nNote that this differs from the previous work (Cai et al., 2024) which replaces the 1D-conv with a\ndropout in the Mamba block, as it is designed to capture TD. Using the CD-Mamba block, we obtain\n$z_1$ and $z_2$, which are two embedding vectors with reversed channel orders that are employed for\nregularization to address the sequential order bias. These vectors are then added element-wise and\ncombined with a residual connection from $z$. Further analysis regarding the removal of the 1D-conv\ncan be found in Table 7.\n3) MLP for TD. To capture TD in TS, we apply"}, {"title": "4.2 REGULARIZATION WITH CD-MAMBA BLOCK", "content": "To address the sequential order bias, SOR-Mamba regularizes the CD-Mamba block to minimize\nthe distance between two embedding vectors generated from data with reversed channel orders. The\nregularization term is defined as follows:\n\n$L_{reg}(z) = d (Z_1, Z_2)$,\n\nwhere $d$ is a distance metric, and $z_1$ and $z_2$ are the embedding vectors obtained from the CD-Mamba\nblock using $z$ with its channel order reversed, as described in Algorithm 1. For $d$, we use the mean\nsquared error (MSE) in the experiments, where the robustness to the choice of $d$ can be found in\nAppendix J. The proposed regularization term is then added to the forecasting loss ($L_{fcst}$) with a\ncontribution of $\\lambda$, resulting in:\n\n$L(x, y) = L_{fcst}(x, y) + \\lambda \\cdot \\sum_{i=1}^{m} L_{reg} (z^{(i)})$\n\nwhere $z^{(i)}$ is $z$ at the $i$-th layer, and $m$ is the number of encoder layers. By incorporating the\nregularization strategy into the unidirectional Mamba, we achieve better performance and efficiency\ncompared to S-Mamba (Wang et al., 2024), which employs the bidirectional Mamba, as shown\nin Table 5. Additionally, we find that regularization also benefits the bidirectional Mamba, which\nhandles the sequential order bias through bidirectional scanning, as shown in Table 6. Further analysis\nregarding the robustness to $\\lambda$ is discussed in Appendix I."}, {"title": "4.3 CHANNEL CORRELATION MODELING", "content": "Previous pretraining tasks for TS have primarily focused on TD, such as masked modeling (Zerveas\net al., 2021) and reconstruction (Lee et al., 2024), to pretrain an encoder. However, we argue for the\nnecessity of a new task that emphasizes CD over TD to align with recent TS models that focus on\ncapturing CD with complex model architectures (Liu et al., 2024a; Wang et al., 2024). To this end,\nwe propose CCM, which aims to preserve the (Pearson) correlation between channels from the data\nspace to the latent space, as correlation is a simple yet effective way to measure channel relationships\nand has been utilized in prior studies to analyze CD (Yang et al., 2024; Zhao & Shen, 2024)."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nTasks and evaluation metrics. We demonstrate the effectiveness of SOR-Mamba on TS forecasting\ntasks with 13 datasets under standard and transfer learning settings. For evaluation, we follow the\nstandard self-supervised learning (SSL) framework, which involves pretraining and fine-tuning (FT)\nor linear probing (LP) on the same dataset. Additionally, we consider in-domain and cross-domain\ntransfer learning settings, with the domains defined in the previous work (Dong et al., 2023). For\nevaluation metrics, we employ mean squared error (MSE) and mean absolute error (MAE).\nDatasets. For the forecasting tasks, we use 13 datasets: four ETT datasets (ETTh1, ETTh2, ETTm1,\nETTm2) (Zhou et al., 2021), four PEMS datasets (PEMS03, PEMS04, PEMS07, PEMS08) (Chen\net al., 2001), Exchange, Weather, Traffic, Electricity (ECL) (Wu et al., 2021), and Solar-Energy\n(Solar) (Lai et al., 2018). Details of the dataset statistics are provided in Appendix A.\nBaseline methods. We follow the baseline methods and results from S-Mamba (Wang et al., 2024).\nFor the baseline methods, we consider Transformer-based models, including iTransformer (Liu\net al., 2024a), PatchTST (Nie et al., 2023), and Crossformer (Zhang & Yan, 2023), as well as\nlinear/MLP models, including TimesNet (Wu et al., 2023), DLinear (Zeng et al., 2023), and RLinear\n(Li et al., 2023). Additionally, we include S-Mamba (Wang et al., 2024), which is a Mamba-based TS\nforecasting model. Details of the baseline methods are provided in Appendix B.\nExperimental setups. We follow the experimental setups from iTransformer and S-Mamba. Note\nthat we do not tune any hyperparameters except for $\\lambda$, which is related to the proposed regularization,\nwhile adhering to the values used in S-Mamba for all other hyperparameters concerning the model\narchitecture and optimization. For dataset splitting, we adhere to the standard protocol of dividing\nall datasets into training, validation, and test sets in chronological order. Details of the experimental\nsetups, including the size of the input window and the forecast horizon, are provided in Appendix A."}, {"title": "5.2 TIME SERIES FORECASTING", "content": "Table 2 presents the comprehensive results for the multivariate TS forecasting task, showing the\naverage MSE/MAE across four horizons over five runs. The results demonstrate that our proposed\nSOR-Mamba outperforms the SOTA Transformer-based models and S-Mamba, which uses the\nbidirectional Mamba, whereas our approach utilizes the unidirectional Mamba, providing greater\nefficiency as discussed in Table 13. Furthermore, self-supervised pretraining (SSL) with CCM yields\nadditional performance gains compared to the supervised setting (SL), with comparisons to SL and\nSSL (LP and FT) shown in Table 3. Full results of Table 2 are provided in Appendix E."}, {"title": "5.3 TRANSFER LEARNING", "content": "To assess the transferability of our method, we conduct transfer learning experiments in both in-\ndomain and cross-domain transfer settings following SimMTM (Dong et al., 2023), where source\nand target datasets share the same frequency in the in-domain setting, while they do not in the\ncross-domain setting. Table 4 presents the average MSE across four horizons, demonstrating that\nSOR-Mamba consistently outperforms S-Mamba, achieving nearly a 5% performance gain in FT."}, {"title": "5.4 ABLATION STUDY", "content": "To demonstrate the effectiveness of our method, we conduct an ablation study using four ETT datasets\nto evaluate the impact of the following components: 1) adding the regularization term, 2) using\nthe unidirectional Mamba instead of the bidirectional Mamba, 3) removing the 1D-conv, and 4)\npretraining with CCM. Table 5 presents the results, indicating that using all proposed components\nresults in the best performance and that our method outperforms S-Mamba with 37.6% fewer model\nparameters. The full results of the ablation study are provided in Appendix F."}, {"title": "6 ANALYSIS", "content": "Sequential order bias. The degree of a sequential order bias may vary depending on the\ncharacteristics of the datasets. We consider two factors affecting this degree: 1) the corre-\nlation between channels and 2) the number of channels in the dataset. To evaluate the rela-\ntionships between these factors and the degree of bias, we quantify the degree of a sequential\norder bias for each dataset by measuring the difference in performance (average MSE across\nfour horizons) when the channel order is reversed, using SOR-Mamba without regularization."}, {"title": "7 CONCLUSION", "content": "In this work, we introduce SOR-Mamba, a TS forecasting method that addresses the sequential order\nbias by incorporating a regularization strategy and removing the 1D-conv from Mamba. Additionally,\nwe propose a novel pretraining task, CCM, to improve the model's ability to capture CD. Our results\ndemonstrate that the proposed method is robust to variations in channel order, leading to superior\nperformance and greater efficiency in both standard and transfer learning scenarios. We hope that our\nwork motivates further research on sequential order-robust Mamba in domains where a sequential\norder is not inherent, such as in tabular data."}, {"title": "4.2 REGULARIZATION WITH CD-MAMBA BLOCK", "content": "To address the sequential order bias, SOR-Mamba regularizes the CD-Mamba block to minimize\nthe distance between two embedding vectors generated from data with reversed channel orders. The\nregularization term is defined as follows:\n\n$L_{reg}(z) = d (Z_1, Z_2)$,\n\nwhere $d$ is a distance metric, and $z_1$ and $z_2$ are the embedding vectors obtained from the CD-Mamba\nblock using $z$ with its channel order reversed, as described in Algorithm 1. For $d$, we use the mean\nsquared error (MSE) in the experiments, where the robustness to the choice of $d$ can be found in\nAppendix J. The proposed regularization term is then added to the forecasting loss ($L_{fcst}$) with a\ncontribution of $\\lambda$, resulting in:\n\n$L(x, y) = L_{fcst}(x, y) + \\lambda \\cdot \\sum_{i=1}^{m} L_{reg} (z^{(i)})$\n\nwhere $z^{(i)}$ is $z$ at the $i$-th layer, and $m$ is the number of encoder layers. By incorporating the\nregularization strategy into the unidirectional Mamba, we achieve better performance and efficiency\ncompared to S-Mamba (Wang et al., 2024), which employs the bidirectional Mamba, as shown\nin Table 5. Additionally, we find that regularization also benefits the bidirectional Mamba, which\nhandles the sequential order bias through bidirectional scanning, as shown in Table 6. Further analysis\nregarding the robustness to $\\lambda$ is discussed in Appendix I."}, {"title": "4.3 CHANNEL CORRELATION MODELING", "content": "Previous pretraining tasks for TS have primarily focused on TD, such as masked modeling (Zerveas\net al., 2021) and reconstruction (Lee et al., 2024), to pretrain an encoder. However, we argue for the\nnecessity of a new task that emphasizes CD over TD to align with recent TS models that focus on\ncapturing CD with complex model architectures (Liu et al., 2024a; Wang et al., 2024). To this end,\nwe propose CCM, which aims to preserve the (Pearson) correlation between channels from the data\nspace to the latent space, as correlation is a simple yet effective way to measure channel relationships\nand has been utilized in prior studies to analyze CD (Yang et al., 2024; Zhao & Shen, 2024)."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nTasks and evaluation metrics. We demonstrate the effectiveness of SOR-Mamba on TS forecasting\ntasks with 13 datasets under standard and transfer learning settings. For evaluation, we follow the\nstandard self-supervised learning (SSL) framework, which involves pretraining and fine-tuning (FT)\nor linear probing (LP) on the same dataset. Additionally, we consider in-domain and cross-domain\ntransfer learning settings, with the domains defined in the previous work (Dong et al., 2023). For\nevaluation metrics, we employ mean squared error (MSE) and mean absolute error (MAE).\nDatasets. For the forecasting tasks, we use 13 datasets: four ETT datasets (ETTh1, ETTh2, ETTm1,\nETTm2) (Zhou et al., 2021), four PEMS datasets (PEMS03, PEMS04, PEMS07, PEMS08) (Chen\net al., 2001), Exchange, Weather, Traffic, Electricity (ECL) (Wu et al., 2021), and Solar-Energy\n(Solar) (Lai et al., 2018). Details of the dataset statistics are provided in Appendix A.\nBaseline methods. We follow the baseline methods and results from S-Mamba (Wang et al., 2024).\nFor the baseline methods, we consider Transformer-based models, including iTransformer (Liu\net al., 2024a), PatchTST (Nie et al., 2023), and Crossformer (Zhang & Yan, 2023), as well as\nlinear/MLP models, including TimesNet (Wu et al., 2023), DLinear (Zeng et al., 2023), and RLinear\n(Li et al., 2023). Additionally, we include S-Mamba (Wang et al., 2024), which is a Mamba-based TS\nforecasting model. Details of the baseline methods are provided in Appendix B.\nExperimental setups. We follow the experimental setups from iTransformer and S-Mamba. Note\nthat we do not tune any hyperparameters except for $\\lambda$, which is related to the proposed regularization,\nwhile adhering to the values used in S-Mamba for all other hyperparameters concerning the model\narchitecture and optimization. For dataset splitting, we adhere to the standard protocol of dividing\nall datasets into training, validation, and test sets in chronological order. Details of the experimental\nsetups, including the size of the input window and the forecast horizon, are provided in Appendix A."}, {"title": "5.2 TIME SERIES FORECASTING", "content": "Table 2 presents the comprehensive results for the multivariate TS forecasting task, showing the\naverage MSE/MAE across four horizons over five runs. The results demonstrate that our proposed\nSOR-Mamba outperforms the SOTA Transformer-based models and S-Mamba, which uses the\nbidirectional Mamba, whereas our approach utilizes the unidirectional Mamba, providing greater\nefficiency as discussed in Table 13. Furthermore, self-supervised pretraining (SSL) with CCM yields\nadditional performance gains compared to the supervised setting (SL), with comparisons to SL and\nSSL (LP and FT) shown in Table 3. Full results of Table 2 are provided in Appendix E."}, {"title": "5.3 TRANSFER LEARNING", "content": "To assess the transferability of our method, we conduct transfer learning experiments in both in-\ndomain and cross-domain transfer settings following SimMTM (Dong et al., 2023), where source\nand target datasets share the same frequency in the in-domain setting, while they do not in the\ncross-domain setting. Table 4 presents the average MSE across four horizons, demonstrating that\nSOR-Mamba consistently outperforms S-Mamba, achieving nearly a 5% performance gain in FT."}, {"title": "5.4 ABLATION STUDY", "content": "To demonstrate the effectiveness of our method, we conduct an ablation study using four ETT datasets\nto evaluate the impact of the following components: 1) adding the regularization term, 2) using\nthe unidirectional Mamba instead of the bidirectional Mamba, 3) removing the 1D-conv, and 4)\npretraining with CCM. Table 5 presents the results, indicating that using all proposed components\nresults in the best performance and that our method outperforms S-Mamba with 37.6% fewer model\nparameters. The full results of the ablation study are provided in Appendix F."}, {"title": "6 ANALYSIS", "content": "Sequential order bias. The degree of a sequential order bias may vary depending on the\ncharacteristics of the datasets. We consider two factors affecting this degree: 1) the corre-\nlation between channels and 2) the number of channels in the dataset. To evaluate the rela-\ntionships between these factors and the degree of bias, we quantify the degree of a sequential\norder bias for each dataset by measuring the difference in performance (average MSE across\nfour horizons) when the channel order is reversed, using SOR-Mamba without regularization."}, {"title": "7 CONCLUSION", "content": "In this work, we introduce SOR-Mamba, a TS forecasting method that addresses the sequential order\nbias by incorporating a regularization strategy and removing the 1D-conv from Mamba. Additionally,\nwe propose a novel pretraining task, CCM, to improve the model's ability to capture CD. Our results\ndemonstrate that the proposed method is robust to variations in channel order, leading to superior\nperformance and greater efficiency in both standard and transfer learning scenarios. We hope that our\nwork motivates further research on sequential order-robust Mamba in domains where a sequential\norder is not inherent, such as in tabular data."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nTasks and evaluation metrics. We demonstrate the effectiveness of SOR-Mamba on TS forecasting\ntasks with 13 datasets under standard and transfer learning settings. For evaluation, we follow the\nstandard self-supervised learning (SSL) framework, which involves pretraining and fine-tuning (FT)\nor linear probing (LP) on the same dataset. Additionally, we consider in-domain and cross-domain\ntransfer learning settings, with the domains defined in the previous work (Dong et al., 2023). For\nevaluation metrics, we employ mean squared error (MSE) and mean absolute error (MAE).\nDatasets. For the forecasting tasks, we use 13 datasets: four ETT datasets (ETTh1, ETTh2, ETTm1,\nETTm2) (Zhou et al., 2021), four PEMS datasets (PEMS03, PEMS04, PEMS07, PEMS08) (Chen\net al., 2001), Exchange, Weather, Traffic, Electricity (ECL) (Wu et al., 2021), and Solar-Energy\n(Solar) (Lai et al., 2018). Details of the dataset statistics are provided in Appendix A.\nBaseline methods. We follow the baseline methods and results from S-Mamba (Wang et al., 2024).\nFor the baseline methods, we consider Transformer-based models, including iTransformer (Liu\net al., 2024a), PatchTST (Nie et al., 2023), and Crossformer (Zhang & Yan, 2023), as well as\nlinear/MLP models, including TimesNet (Wu et al., 2023), DLinear (Zeng et al., 2023), and RLinear\n(Li et al., 2023). Additionally, we include S-Mamba (Wang et al., 2024), which is a Mamba-based TS\nforecasting model. Details of the baseline methods are provided in Appendix B.\nExperimental setups. We follow the experimental setups from iTransformer and S-Mamba. Note\nthat we do not tune any hyperparameters except for $\\lambda$, which is related to the proposed regularization,\nwhile adhering to the values used in S-Mamba for all other hyperparameters concerning the model\narchitecture and optimization. For dataset splitting, we adhere to the standard protocol of dividing\nall datasets into training, validation, and test sets in chronological order. Details of the experimental\nsetups, including the size of the input window and the forecast horizon, are provided in Appendix A."}, {"title": "5.2 TIME SERIES FORECASTING", "content": "Table 2 presents the comprehensive results for the multivariate TS forecasting task, showing the\naverage MSE/MAE across four horizons over five runs. The results demonstrate that our proposed\nSOR-Mamba outperforms the SOTA Transformer-based models and S-Mamba, which uses the\nbidirectional Mamba, whereas our approach utilizes the unidirectional Mamba, providing greater\nefficiency as discussed in Table 13. Furthermore, self-supervised pretraining (SSL) with CCM yields\nadditional performance gains compared to the supervised setting (SL), with comparisons to SL and\nSSL (LP and FT) shown in Table 3. Full results of Table 2 are provided in Appendix E."}, {"title": "5.3 TRANSFER LEARNING", "content": "To assess the transferability of our method, we conduct transfer learning experiments in both in-\ndomain and cross-domain transfer settings following SimMTM (Dong et al., 2023), where source\nand target datasets share the same frequency in the in-domain setting, while they do not in the\ncross-domain setting. Table 4 presents the average MSE across four horizons, demonstrating that\nSOR-Mamba consistently outperforms S-Mamba, achieving nearly a 5% performance gain in FT."}, {"title": "5.4 ABLATION STUDY", "content": "To demonstrate the effectiveness of our method, we conduct an ablation study using four ETT datasets\nto evaluate the impact of the following components: 1) adding the regularization term, 2) using\nthe unidirectional Mamba instead of the bidirectional Mamba, 3) removing the 1D-conv, and 4)\npretraining with CCM. Table 5 presents the results, indicating that using all proposed components\nresults in the best performance and that our method outperforms S-Mamba with 37.6% fewer model\nparameters. The full results of the ablation study are provided in Appendix F."}, {"title": "6 ANALYSIS", "content": "Sequential order bias. The degree of a sequential order bias may vary depending on the\ncharacteristics of the datasets. We consider two factors affecting this degree: 1) the corre-\nlation between channels and 2) the number of channels in the dataset. To evaluate the rela-\ntionships between these factors and the degree of bias, we quantify the degree of a sequential\norder bias for each dataset by measuring the difference in performance (average MSE across\nfour horizons) when the channel order is reversed, using SOR-Mamba without regularization."}, {"title": "7 CONCLUSION", "content": "In this work, we introduce SOR-Mamba, a TS forecasting method that addresses the sequential order\nbias by incorporating a regularization strategy and removing the 1D-conv from Mamba. Additionally,\nwe propose a novel pretraining task, CCM, to improve the model's ability to capture CD. Our results\ndemonstrate that the proposed method is robust to variations in channel order, leading to superior\nperformance and greater efficiency in both standard and transfer learning scenarios. We hope that our\nwork motivates further research on sequential order-robust Mamba in domains where a sequential\norder is not inherent, such as in tabular data."}, {"title": "H ROBUSTNESS TO CHANNEL ORDER", "content": "To demonstrate that the proposed method effectively addresses the sequential order bias, we evaluate\nperformance variations by permuting the channel order with five datasets (Zhou et al., 2021; Wu et al.,\n2021). Table H.1 shows the results, which indicate a small standard deviation across all horizons."}, {"title": "I ROBUSTNESS TO HYPERPARAMETER \u03bb", "content": "Table I.1 shows the average MSE across four different horizons for the four ETT datasets (Zhou et al.,\n2021), using various values of $\\lambda$ that control the contribution of the regularization term. The results"}]}