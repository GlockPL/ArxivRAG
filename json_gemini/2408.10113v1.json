{"title": "Enhancing Reinforcement Learning Through Guided Search", "authors": ["J\u00e9r\u00f4me Arjonilla", "Abdallah Saffidine", "Tristan Cazenave"], "abstract": "With the aim of improving performance in Markov Decision Problem in an Off-Policy setting, we suggest taking inspiration from what is done in Offline Reinforcement Learning (RL). In Offline RL, it is a common practice during policy learning to maintain proximity to a reference policy to mitigate uncertainty, reduce potential policy errors, and help improve performance. We find ourselves in a different setting, yet it raises questions about whether a similar concept can be applied to enhance performance i.e., whether it is possible to find a guiding policy capable of contributing to performance improvement, and how to incorporate it into our RL agent. Our attention is particularly focused on algorithms based on Monte Carlo Tree Search (MCTS) as a guide. MCTS renowned for its state-of-the-art capabilities across various domains, catches our interest due to its ability to converge to equilibrium in single-player and two-player contexts. By harnessing the power of MCTS as a guide for our RL agent, we observed a significant performance improvement, surpassing the outcomes achieved by utilizing each method in isolation. Our experiments were carried out on the Atari 100k benchmark.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is a leading field in artificial intelligence, advancing our grasp of intelligent decision-making in complex environments. Despite the remarkable progress, the pursuit of optimizing RL algorithms remains a central focus. In this pursuit, we turn our attention to a foundational concept within the realm of RL. In Offline RL, the primary objective is to derive the best possible policy solely from a dataset originating from an auxiliary policy, without interacting with the environment. The prevalent idea is to align the new policy closely with the auxiliary policy to enhance performance. This strategy derives from the principle that deviating from the limits of the auxiliary policy often leads to uncertainty which leads to erroneous judgments about the policy's efficacy.\nOur scenario diverges from this framework and pivots back to a more classical approach where the constraints of an auxiliary policy fade away and we once again interact with the environment. Despite this paradigm shift, we question whether it is possible to preserve the concept of Offline RL i.e., staying as close as possible to an auxiliary policy to enhance performance. Considering our lack of auxiliary policy, we inquire whether it is plausible to use an online algorithm proficient enough to act as our guiding reference, and how to integrate such a guiding agent into our RL agent.\nIn our investigation, we initially explore various online algorithms that can potentially serve as a guide. The existing literature presents algorithms that already exploit guide knowledge to improve performance. For instance, algorithms such as Soft Actor-Critic (SAC) and Asynchronous Advantage Actor Critic (A3C) integrate an entropy term into the reinforcement learning (RL) agent. This entropy, in another formulation, is a measure of the distance between the current policy and the policy of a guide, of which this guide happens to be a random agent.\nIn our research, we turn our attention to search algorithms, specifically focusing on Monte Carlo Tree Search (MCTS) as a guiding policy for reinforcement learning (RL) agents. MCTS-based approaches, well-established in game theory literature, obtain state-of-the-art performance across a spectrum of games, converging towards equilibrium even in complex scenarios involving one or two players.\nIntegrating MCTS as a guide yields significant performance improvements. Our analysis reveals that, in the majority of cases, this integration leads to enhanced performance. Even in instances where performance does not improve, the algorithms achieve optimal outcomes when compared individually. By combining an RL algorithm with MCTS as a guide, we harness the generalization and learning capabilities inherent to RL, while also capitalizing on MCTS's optimal online decision-making prowess. Furthermore, we extend our investigation by exploring various hyperparameters, with a keen focus on the degree of integration of the guide's policy. Through experimentation, we are demonstrating that it is possible to reduce the frequency of use of the guide, thereby mitigating associated overhead while retaining performance enhancements.\nIn Section 2, we establish the formalism and notation employed throughout the paper. Section 3 presents multiple online guides, discussing their respective strengths and weaknesses, and elucidates the process of integrating a guide into the RL agent. Particularly incorporating MCTS-based algorithms as a guide offers valuable guidance to the RL agent in several key points: the actor and the critic components. In Section 4, we conduct experiments using various guides on the Atari 100k benchmark. Section 5 provides an overview of related work in the field. Lastly, Section 6 offers a summary of our findings and outlines avenues for future research."}, {"title": "2 Formalism and Notation", "content": null}, {"title": "2.1 Markov Decision Process", "content": "A dynamic system is typically characterized by a Markov Decision Process (MDP), which is represented as \\(M = (S, A, T, r, \\gamma)\\). Here, S denotes the state space where \\(s \\in S\\), A represents the action space with \\(a \\in A\\), \\(T(s_{t+1}|s_t, a_t)\\) signifies the transition probability distribution governing the system dynamics, \\(r(s, a)\\) stands for the reward function, and \\(\\gamma \\in (0, 1]\\) serves as a discount factor.\nDealing with an exact MDP can impose considerable computational burdens. Utilizing an approximation of M, known as a world model, can offer significant advantages. Employing the world model for information retrieval not only expedites computations compared to exact methods but also facilitates parallel processing of state batches, particularly when computing complex tools such as N-step bootstrapped \\(\\Lambda\\)-returns or employing MCTS. This parallel processing is often performed on GPUs rather than CPUs, further enhancing computational efficiency."}, {"title": "2.2 Reinforcement Learning", "content": "Reinforcement learning confronts the problem of learning to control the MDP, where the agent tries to acquire a policy \\(\\pi\\), which is defined as a distribution over actions conditioned on state \\(\\pi(a|s)\\) that maximizes the long-term discounted cumulative reward defined as follow:\n\\(\\pi^* = \\max_\\pi E_\\tau [\\sum_{t=0}^T \\gamma^t r_t]\\)\nwhere \\(\\tau = (s^0, a^0, r^0, ... )\\) is a sequence of states, actions, and rewards generated from the current policy. To maximize the policy \\(\\pi\\), one of the primary methods utilized is the actor-critic approach involves learning a critic and an actor-network. The learning can be conducted online by generating new trajectories or by leveraging a data buffer \\(D\\), which comprises past trajectories \\(T_0, T_1, ..., T_{k-1}\\)."}, {"title": "2.2.1 Critic", "content": "The critic aims to estimate the value functions, i.e. the expected cumulative rewards an agent can obtain at a state:\n\\(V_\\theta(s_t) = E_{a_t \\sim \\pi_\\theta (\\cdot|s_t)} [r_t + \\gamma E_{s_{t+1} \\sim T(\\cdot|s_t,a_t)} [V_\\theta (s_{t+1})]]\\)\nThe loss function of the critic \\(L_\\theta^c\\) is formulated to minimize the disparity between the value target \\(V_\\theta'(s)\\) and the predicted value \\(V_\\theta(s)\\) over a batch of state.\n\\(L_\\theta^c = E_{s \\sim D} [L_{C,Sub}(s)]\\)\nPrevious studies have emphasized the benefits of employing cross-entropy over a discrete representation in reinforcement learning. This method involves the critic to learn a discrete weight distribution \\(p_\\theta = \\{p_1, ...,p_B\\} \\in \\mathbb{R}^B\\) instead of learning the mean of the distribution/ A function \\(\\gamma()\\) is used to convert a target value into a corresponding weight distribution of size B. This leads to the following sub-loss for the critic:\n\\(L_{C,Sub}(s) = \\gamma(V_\\theta'(s)) \\log p_\\theta\\)"}, {"title": "2.2.2 Actor", "content": "The actor's loss function, denoted as \\(L^a\\), is designed to maximize the expected reward by optimizing the actions that lead to states with the highest predicted values from the critic.\n\\(L_\\theta^a = E_{s \\sim D} [L_{A,Sub}(s)]\\)\nIn the context of Atari Benchmarks, as observed in , authors have found it more advantageous to employ the Reinforce algorithm, which is the approach adopted in our work as well. Reinforce maximizes the actor's probability of its own sampled actions weighted by the values of those actions. One can reduce the variance of this estimator by subtracting the state value as a baseline. Therefore, we obtain the following loss for the actor:\n\\(L_{A,Sub} (s) = E_{a \\sim \\pi_\\theta(\\cdot|s)} [- \\ln \\pi_\\theta (a|s) \\frac{V_\\theta(s) - V_\\theta(s)}{S_e}]\\)\nwhere the term \\(S_e\\) refers to the normalization factor used to stabilize the scale of returns. The normalization is carried out using an exponentially decaying average, is robust to outliers by taking the returns from the 5th to the 95th batch percentile, and reduces large returns without increasing small returns.\n\\(S_e = \\max (1, Per_{95} (V_\\theta(.)) - Per_{5} (V_\\theta(.)))\\)"}, {"title": "2.3 Behavior Cloning", "content": "Behavior Cloning (BC) is a method employed in RL where the objective is to develop an agent capable of executing tasks closely resembling those of the demonstrator. In this approach, the agent's policy, denoted as \\(\\pi_{BC}\\), undergoes a supervised learning process to closely replicate the actions present in the dataset.\n\\(\\pi_{BC} = \\max_{\\pi} E_{\\pi (a,s) \\sim D} [\\log \\pi(a|s)]\\)"}, {"title": "2.4 Search Algorithm", "content": "Search algorithms are algorithms that aim to explore the game tree efficiently to make informed decisions that maximize the chances of winning. To do this, search algorithms are given a larger budget in the given state that they wish to solve, and during the budget they efficiently explore the different possible paths of action, thus obtaining a better estimate of the value function and a better policy in the given state.\nSearch algorithms encompass a diverse range of techniques tailored to handle various game scenarios, from single-player to multi-player, and from perfect to imperfect information settings. In perfect information games like Chess or Go, where players have complete knowledge of the game state, algorithms like Minimax with Alpha-Beta Pruning or MCTS are widely employed. Conversely, imperfect information games like Poker or Skat pose additional challenges due to hidden information. In such cases, techniques like Perfect Information Monte Carlo, Information Set Monte Carlo Tree Search, or Counterfactual Regret Minimization based method are utilized."}, {"title": "2.4.1 Monte Carlo Tree Search", "content": "MCTS is a tree search algorithm, for perfect information game that converges towards equilibrium with one and two players. At each time step of the budget, MCTS (i) selects the best path of node, (ii) expands the tree by adding a child node, (iii) estimates the child node, (iv) backpropagates the result obtained through the nodes chosen. At the end of the budget, the algorithms return the distribution of actions \\(\\pi_{MCTS}\\) that has been visited, and the value \\(V_{MCTS}\\) obtained when running MCTS.\nStarting from AlphaGo/AlphaZero (AZ) series, MCTS has been combined with neural networks to enhance performance where an actor-network is used to help the search and a critic network is used to give a better estimate of the new state. We denote \\(\\pi_{AZ}/V_{AZ}\\) the information returned when running MCTS with AlphaZero. This information is then utilized to compute the sub-actor loss \\(L_{A,Sub}(s)\\) and the sub-critic loss \\(L_{C,Sub} (s)\\).\n\\(L_{C,Sub} (s) = \\gamma(V_{AZ}(s)) \\log p_\\theta\\)\n\\(L_{A,Sub}(s) = \\pi_{AZ}(\\cdot|s) \\log \\pi_\\theta(\\cdot|s)\\)"}, {"title": "3 Guide", "content": "As mentioned in the introduction, we aim to find an online algorithm that can guide our RL agent to improve its performance. In this objective, we will first investigate the advantages and disadvantages of different guides, and then we will explain how to integrate the guide into the RL agent."}, {"title": "3.1 Analysis of the various guides", "content": "To thoroughly assess the efficacy of different guides and determine their suitability for guiding the reinforcement learning algorithm, we conducted a comprehensive evaluation based on multiple criteria. The gathered information is summarized in Table 1. The guides discussed are detailed below and are identified as follows 'Human', 'Random', 'BC', and 'MCTS'.\nThe criteria take into account their capacity to be available in each state-action, if they are relevant for exploring/performance, their online and offline cost, if they can reduce the extrapolation error, and if they are time dependent. Time-dependent algorithms are those that require learning before they are operational, for example, learning a neural network. Extrapolation error is an error present in Off-Policy and Offline problems that arise when the target selects actions rarely present in the dataset, affecting the accuracy of the value estimate."}, {"title": "3.1.1 Human", "content": "The use of guides is often associated with the use of human guides, whether for learning to drive, for conversing with other humans or even for trying to play as much as a human . It is a necessity in scenarios where real-time interaction is either infeasible or the risk is too significant. The initial stages of a game present a valuable opportunity for the incorporation of human policies. During this phase, RL policies may prove ineffective, whereas human policies are directly applicable and advantageous. Unfortunately, the data are available in a restricted subset of all state-action, are expensive and complex to obtain."}, {"title": "3.1.2 Random", "content": "In algorithms like SAC and several state-of-the-art counterparts, the RL agent is coupled with an entropy term to enhance exploration. In an alternative perspective, this entropy is a measure of the distance between the current policy and the policy of a guide, of which this guide happens to be a random agent. The choice of a random agent as a guide holds distinct advantages, particularly when exploration of the state space is desired, its minimal computational cost and immediate availability make it an ideal choice in many scenarios. However, reservations emerged when considering the utility of such a guide in enhancing overall performance."}, {"title": "3.1.3 Behavior Cloning", "content": "In Offline RL, a common strategy involves approximating closeness to the behavioral policy that underlies the D dataset. Achieving this requires an initial step of estimating the behavioral policy by behavioral cloning. This estimate of the behavior policy is then used as a guide for RL agents. This method yields a significant advantage by minimizing extrapolation errors. By aligning the new policy closely with the behavior policy, the algorithm performs actions for which accurate approximations exist, reducing uncertainties of the new policy. However, several considerations come into play. Firstly, the guide is not inherently well-suited for exploration or enhancing performance. Secondly, the data is confined to a subspace of the state space and depends on the amount of interaction."}, {"title": "3.1.4 Search Algorithm", "content": "Leveraging a search algorithm as a guide stands as a reasonable choice given its constant availability in each state and its relatively low cost compared to human guidance. Particularly, in contrast to employing either a random guide or a guide relying solely on past data, search-based algorithms hold greater potential for performance enhancement due to their abilities to explore and converge toward the optimal solution. It is noteworthy, however, that while search algorithms are less expensive than human guidance, they may incur higher costs than alternative methods. Additionally, under constrained resource budgets or insufficient training of neural networks, search algorithms may encounter challenges in converging toward the optimal solution."}, {"title": "3.2 How to integrate a guide into the RL agent", "content": "Offline RL domain offers diverse methods for aligning one policy with another, contingent on the degree of closeness desired between them. Possible methods include value penalty where the"}, {"title": "3.2.1 Critic Incorporation", "content": "By integrating the guide into the critic, our objective is to refine the estimation of the value function by considering the insights provided by the guide. Incorporating a penalty into the critic using value regularization amounts to change from Equation (3) to equation the following new loss function \\(L\\):\n\\(L = E_{s \\sim D}[L_{C, Sub}(s)+ \\sum_{E} \\lambda_E(s)F_E(V_\\theta(s), V_{E}(s))]\\)\nwhere \\(F_E (\\cdot, \\cdot)\\) is the penalty term between the guide target \\(V_{E_i}(s)\\) and the predicted value, and \\(\\lambda_E(s)\\) is the function weight used for regularizing the penalty term. The penalty term can be any function that evaluates the disparity, and in particular, the same function as the critic's sub-loss. Similarly, to enhance stability, one can compute the N-step bootstrapped \\(\\Lambda\\)-returns on the target value."}, {"title": "3.2.2 Actor Incorporation", "content": "To incorporate the guide on the actor, we used information from the guide on the actor and the critic. The use of the critic allows us to increase guidance when states are promising or have high potential. Incorporating a penalty into the actor using regularization amounts to change from Equation (6) to the following loss function of the actor \\(L\\):\n\\(L = E_{s \\sim D} [L_{A,Sub} (s) + \\sum_{E} \\alpha_E(s)F(\\pi_\\theta(\\cdot|s), \\pi_{E}(\\cdot|s))]\\)\nwhere \\(F_E (\\cdot, \\cdot)\\) represents the penalty term between the actor-network and the target policy, and \\(\\alpha_E(s)\\) is a function determining the penalty weight based on the current state. The penalty term can be any function that evaluates the disparity, yet, in Offline RL, the penalty term is often the KL divergence. \nThe loss of the actor-network significantly depends on the scale of the internal loss values. To address this, we normalize \\(L_{A,Sub}(s)\\) by the average absolute value of \\(L_{A,Sub}(\\cdot)\\). This mean term is estimated over mini-batches and is solely used for scaling purposes. The weight \\(\\alpha_E(s) \\in [ \\alpha_{E_i}^-(s), \\alpha_{E_i}^+(s)]\\) is a function that serves to emphasize the increased penalty on high-quality state i.e., more weight is given to states that perform better than the target, which results in more attention toward the policy given by the guide.\n\\(\\alpha_E^+(s) = \\text{Clip}\\left(\\exp\\left( \\tau_E \\frac{V_{E_i}(s) - V_\\theta(s)}{S_{E_i}} \\right), 1, \\text{Max} \\right)\\)\nIn this equation, the state's quality is assessed through the term \\(V_{E_i}(s) - V_\\theta(s)\\) normalized by \\(S_{E_i}\\). The normalization is carried out using an exponentially decaying average, robust to outliers by taking the returns from the 5th to the 95th batch percentile, and reduces large returns without increasing small returns.\n\\(S_{E_i} = \\max (1, Per_{95} (V_{E_i}(\\cdot)) - Per_{5} (V_{E_i}(\\cdot)))\\)"}, {"title": "4 Experimentation", "content": null}, {"title": "4.1 Experimental Information", "content": null}, {"title": "4.1.1 Benchmarks", "content": "Atari 100k serves as a comprehensive benchmark comprising 26 Atari games, providing a diverse range of challenges to assess various algorithms' performance. In this benchmark, agents train for 100k steps, equivalent to 400k frames (considering a frameskip of 4). Each block of 100k steps approximately aligns with 2 hours of real-time gameplay per environment."}, {"title": "4.1.2 Algorithms", "content": "The algorithms used are namely (i) AlphaZero (AZ); (ii) A2C (Advantage Actor-Critic); (iii) A2C with random agent as a guide, noted as A2C-Rand (similar to SAC); (iv) A2C with behavior cloning as an guide, noted as A2C-BC; (v) A2C agent with AlphaZero as an guide, noted as A2C-AZ or A2C-AZ* where A2C-AZ uses a fixed hyperparameter \\(\\lambda^{\\alpha}\\) for all games and A2C-AZ* uses a fine-tuned \\(\\lambda^{\\alpha}\\) for each game."}, {"title": "4.1.3 Actor/Critic", "content": "All the algorithms use a critic and an actor network, composed of a two-layered MLP network of 512 hidden units. As defined in the introduction, the critic loss sub \\(L_{C,Sub}()\\) uses a cross-entropy based on a discrete representation and the actor loss sub \\(L_{A, Sub}()\\) uses reinforce with an advantage baseline to reduce the variance.\nThe distance function \\(F(\\cdot, \\cdot)\\) used for the actor is a KL-divergence function and the distance function \\(F_C(\\cdot, \\cdot)\\) used for the critic is a cross-entropy. The weight of the guide penalty \\(\\lambda^{\\alpha}\\) is fixed at 0.08 for behavior cloning and at 0.03 for random (both where chosen between [0.03, 0.08, 0.3]), and unless otherwise stated, set at 0.7 for MCTS. For A2C-AZ, the weight for the critic is fixed at 0.05. For enhancing stability, the guide value target \\(V_{E_i}()\\) and the value target \\(V_\\theta()\\) use the N-step bootstrapped \\(\\Lambda\\)-returns."}, {"title": "4.1.4 Monte Carlo Tree Search", "content": "A2C-AZ utilizes the actor and critic networks of the A2C agent, ensuring that it does not deviate significantly from it. Our implementation of MCTS in A2C-AZ and AlphaZero is built on previous famous MCTS implementations . It uses a search budget of 50, PUCT in the selection and Dirichlet noise distribution to help explore. However, three differences should be noted (i) we do not use Re-Analyse; (ii) we do not use prioritized experience replay ; (iii) we do not use the search algorithm in the test phase. These differences were made to effectively compare the different algorithms."}, {"title": "4.1.5 Metrics", "content": "We report the raw performance on each game, the human normalized score, as well as the Interquartile Mean (IQM) and Optimality Gap. The IQM and the Optimality Gap are metrics recommended for Atari100K benchmarks where the authors recommend using IQM instead of the Median, and Optimality Gap instead of Mean, as both methods are more robust. IQM calculates the average over the data, removing the top and bottom 25%. Optimality Gap computes the amount by which the algorithm fails to meet a minimum score. A higher score is better for the IQM and a lower score is better for the optimality gap."}, {"title": "4.1.6 Other", "content": "Each agent uses a single environment instance with a single NVIDIA V100 GPU. Each algorithm is run using 5 seeds, we evaluated performance every 10k training step with 10 independent run of the game. To mitigate training expenses, we conducted our experiments by using a world model. We employed the fixed-trained weights from the Dreamer algorithm , a state-of-the-art model-based technique trained over 50, 000k steps. The world model is used to compute the N-step bootstrapped \\(\\Lambda\\)-returns for A2C algorithms and facilitating MCTS in A2C-AZ and AlphaZero. Additionally, to enhance cost-effectiveness and stability, we restricted our experimentation to 21 out of 26 games, excluding those where the world model demonstrated poorer performance in terms of mean human-normalized scores."}, {"title": "4.2 Experiments", "content": "Initially, we will examine the overall impact of the various algorithms and guides. Subsequently, we narrow our focus on MCTS as a guide, analyzing the experiments in greater detail. Finally, we analyze the impact of the guide's weight, by testing several weights and trying to observe the impact when the guide is called less often."}, {"title": "4.2.1 Overall analysis", "content": null}, {"title": "4.2.2 MCTS as a guide", "content": "In Figure 3, we observe the percentage improvement of A2C-AZ*/A2C-AZ/AlphaZero over A2C. Furthermore, Figure 2 displays a series of learning curves for A2C-AZ, A2C, and AlphaZero, forming the foundation for our subsequent analysis.\nWe begin our analysis by comparing the performance of AZ and A2C agents independently. Each figure represents distinct scenarios: one where AZ outperforms A2C (Figure 2.a) and another where A2C outperforms AZ (Figure 2.b). These figures provide an initial glimpse into the broader performance trends.\nOverall, A2C demonstrates superior performance in 12 games, while AZ surpasses A2C in 8 games, with 1 game showing equivalent performance. Despite the general advantage of A2C, it is essential to highlight instances where A2C falls short, indicating the potential benefits of integrating AZ as a guide.\nWhen considering the incorporation of AZ as a guide, several critical questions arise: can this integration elevate the agent's performance to at least match the best of the two individual agents? Is it possible to create an agent superior to the best individual performer, or might utilizing the guide lead to a weakened agent?\nOur analysis across games shows that, compared to A2C, 12/17 games exhibit performance improvements, 4/2 show equivalent performance, and 5/2 show lower performance when using the combined approaches A2C-AZ and A2C-AZ*. Interestingly, in the subset of 8 games where AZ outperformed A2C in isolation, integrating AZ as a guide resulted in superior performance in 6/7 of those instances. Although not visible in the figure, but observable in supplementary material. Our observations indicate that the combined algorithm outperforms both individual algorithms in 9/11 instances, achieves the performance of the better of the two methods in 7/9 instances, is lower than the best but bounded by the two algorithms in 4/1 instances, and shows lower performance than both in only 1/0 instance."}, {"title": "4.2.3 Weight of the guide", "content": "In Figure 4, we explore the impact of the weight parameter, \\(\\lambda^{\\alpha}\\), on performance. We compare several fixed values of \\(\\lambda^{\\alpha}\\) ranging from 0.1 to 0.7, alongside the optimal weight selected for each game. Each variant of A2C-AZ is denoted by A2C-AZ-X, where X represents the specific weight used. For instance, A2C-AZ-0.3 employs AZ as a guide with a weight of 0.3.\nUpon examination, we find that the optimal fixed weight is 0.7, resulting in an IQM of 1.29 and an Optimality Gap of 0.28. Notably, reducing the weight significantly leads to performance outcomes closely resembling those of A2C alone."}, {"title": "4.2.4 Cost of using a guide", "content": "Throughout our previous experiments, we have observed a significant advantage in using AZ as a guide. However, as indicated in Table 1, there is an overhead cost associated with employing AZ.\nIn practice, several MCTS methods can significantly reduce this cost, such as batch MCTS and various parallelization techniques (leaf, root, and tree). Additionally, many implementations utilize extensive computational resources to better distribute the workload. For instance, the basic version of AlphaGo uses 40 search threads, 48 CPUs, and 8 GPUs.\nIn the following experiment, we demonstrate another way to reduce the cost of incorporating a guide, which is quite natural in our context. Currently, the guide is executed at every iteration. However, our goal is to avoid deploying the guide in every situation. We aim to activate the guide only when necessary-specifically, in scenarios where our RL agent faces challenges or when the guide is known to excel.\nFigure 5 shows the impact of using the guide less frequently. Instead of employing the guide at each iteration, we use it at every N iterations. We introduce the notation A2C-AZ-X, where X indicates how often the guide is called. For example, A2C-AZ-3 uses MCTS every three steps. Additionally, Table 2 presents the overhead cost of using AZ as a guide according to different values of N.\nAs observed, when running AZ at every iteration, the algorithm will achieve the best performance but take 18 hours to complete instead of the 4 hours required by A2C alone. However, by reducing the frequency to every two iterations, the runtime is reduced by half while still achieving a performance close to the best."}, {"title": "5 Related Work", "content": null}, {"title": "5.1 Offline Reinforcement Learning", "content": "Our work is strongly linked to the field of Offline RL as inspired by one of the key methods in the field. In our case, we have chosen to use regularization methods to align the policy and the value function with the guide. Yet, within the realm of regularization methods, there exist many methods, these include penalties applied within the reward function or regularization penalties applied after its computation of the loss. Additionally, the calculation of the penalty can be accomplished by using various functions including KL divergence, Maximum Mean Discrepancy, or even Fisher information ."}, {"title": "5.2 Monte Carlo Tree Search", "content": "MCTS stands as a state-of-the-art algorithm that has significantly enhanced performance and tackled complex problems. In recent years, MCTS has been integrated with neural networks to boost"}, {"title": "6 Conclusion", "content": "In this paper, we investigate the influence of leveraging online algorithms as a guide to enhance the learning process of RL algorithms. Inspired by techniques in Offline RL, we adapt these methodologies to the context of using an online algorithm, as a guide. Our approach involves regularizing the loss functions for both the actor and the critic to incorporate the information provided by the guide effectively.\nAmong the array of online algorithms explored from existing literature, our focus lies on Monte Carlo Tree Search (MCTS), a cutting-edge planning algorithm renowned for its convergence capabilities in both single-player and two-player scenarios. Notably, employing MCTS as a guide yields superior results compared to employing either of the two methods in isolation. Furthermore, fine-tuning just one hyperparameter can extend performance gains. Additionally, reducing the frequency of the guide calls can mitigate the cost associated with it while still resulting in enhanced performance.\nIn the future, there exist promising avenues for further exploration. Experimenting with diverse hyperparameters, such as alternative distance functions, different search algorithms or different reinforcement learning algorithms, could illuminate nuanced insights. Additionally, exploring the integration of multiple guides could broaden the range of possibilities, incorporating different perspectives from various guides. Finally, investigating the utilization of an automatic weight, potentially based on neural networks, could provide a more adaptive, efficient, and general approach."}, {"title": "A Implementation Details", "content": null}, {"title": "A.1 Discrete representation for the critic", "content": "The critic's loss function, denoted as \\(L_{C,Sub}(s)\\), is formulated to minimize the disparity between the value target \\(V_\\theta'(s)\\) and the predicted value \\(V_\\theta(s)\\) at a specific state s. Commonly, the disparity is computed with the mean squared error or the cross-entropy over a discrete representation.\nPrevious studies have emphasized the benefits of employing cross-entropy over a discrete representation in reinforcement learning . This method involves the critic to learn a discrete weight distribution \\(p_\\theta = \\{p_1, ...,p_B\\} \\in \\mathbb{R}^B\\) instead of learning the mean of the distribution. A function \\(\\gamma()\\) that converts a target value into a corresponding weight distribution of size B. This leads to the following sub-loss for the critic:\n\\(L_{C,Sub}(s) = \\gamma(V_\\theta'(s))^T \\log p_\\theta\\)\nMore specifically, transforming (function \\(\\gamma\\)) the reward/target into a discrete representation is done function by a method called two-hot encoding. The two-hot encoding is a generalization of the one-hot encoding where all elements are 0 except for the two entries closest to y at positions m and m + 1. These two entries sum up to 1, with more weight given to the entry that is closer to y:\n\\(\\gamma(x) = \\text{twohot}(x)_i = \\begin{cases} |b_{m+1} - x|/|b_{m+1} - b_m| & \\text{if } i = m \\\\ |b_{m} - x|/|b_{m+1} - b_m & \\text{if } i = m+1 \\\\ 0 & \\text{else} \\end{cases}\\)\nImportantly, two-hot encoding can predict any continuous value in the interval because its expected bucket value can fall between the buckets."}, {"title": "B Monte Carlo Tree Search-Detailed", "content": "Below, we provide a comprehensive overview of the Monte Carlo Tree Search (MCTS) algorithm, drawing from previous research on MCTS . Notably, in Schrittwieser et al. , it was demonstrated that a budget of 50 is adequate for resolving the Atari100K benchmark, hence informing our decision in this regard.\nMonte Carlo Tree Search (MCTS) is the state-of-the-art in the perfect information game. MCTS converges asymptotically to the optimal policy in single-agent domains and to the minimax value function in zero-sum games. Starting from the AlphaGo , MCTS has been combined with an offline neural network to enhance performance.\nMCTS(s, budget) is an online tree search algorithm that runs at s for a budget of budget and works as follows (i) selection - selects a path of nodes until a leaf node; (ii) expansion - expands the tree by adding a new child node; (iii) backpropagation - backpropagates the result obtained through the nodes chosen during the selection phase; (iv) repeats step (i) to (iii)"}]}