{"title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection", "authors": ["Songhao Han", "Wei Huang", "Hairong Shi", "Le Zhuo", "Xiu Su", "Shifeng Zhang", "Xu Zhou", "Xiaojuan Qi", "Yue Liao", "Si Liu"], "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-40. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-40 in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso", "sections": [{"title": "1. Introduction", "content": "In recent years, the development of Large Vision Language Models (LVLMs) [3, 11, 18, 24] has brought significant advancements to multi-modal understanding tasks. By integrating visual and language information through extensive data training, more advanced LVLMs families [16, 31] are able to generate reasonable outputs while fully leveraging the rich knowledge of LLMs, thus demonstrating excellent performance in tasks like image captioning and visual question answering. Recent research has started to explore extending LVLMs to the domain of video content understanding [19, 22, 27]. Although these efforts have shown great potential on certain basic video understanding benchmarks [20, 28, 42, 47], their performance on complex video reasoning tasks remains less than satisfactory.\nA primary limitation in video question-answering (VideoQA) research is the scarcity of high-quality, large-scale datasets. Current VideoQA datasets [15, 41, 47] rely on costly manual annotations that often lack the granularity needed for detailed understanding, limiting scalability. Yet, LVLMs require vast amounts of multimodal QA pairs for effective training. Recently, advancements in large language models (LLMs) like GPT-4 [30] and Gemini-Pro [12] have allowed for the automatic generation of QA pairs through carefully designed prompts. A straightforward approach is to use video metadata-typically high-level descriptions and leverage LLMs to generate QA pairs based on this coarse information. However, the missing of crucial video details restricts the QA pairs' effectiveness for fine-grained reasoning. Alternatively, analyzing video frames for a more granular understanding is feasible, but video content is often redundant, with key information dispersed sparsely, making frame-by-frame analysis computationally expensive and prone to information overload.\nTo address these challenges, we propose a novel automatic VideoQA construction method and introduce a new dataset, VideoEspresso. By preserving important spatial details with temporal coherence, we create a fine-grained reasoning-enabled VideoQA dataset that fosters more effective multimodal understanding. As shown in Fig. 1, we first design a semantic-aware key information extraction method to extract key information from the video. Unlike traditional methods that extract key frames based on image representations, we first map video frames to the linguistic space using an LVLM. We then remove similar frames based on semantic similarity, which reduces redundancy in the video data. To retain frame-level details and inter-frame correlation, we sequentially group the video frames and input them into GPT-40 [31]. With carefully designed prompts, we instruct the model to generate initial QA pairs and filter out low-quality data. To further expand the intermediate reasoning steps, we introduce video Chain-of-Thought annotations. We design prompts to guide GPT-40 in extract-"}, {"title": "2. Related Work", "content": "VideoQA Dataset. Traditional videoQA datasets [15, 42, 47] rely heavily on manual annotations, where annotators watch videos, summarize content, and generate QA pairs based on set guidelines. This labor-intensive process limits scalability. With advancements in LLM capabilities [1, 12, 29], recent approaches use tailored prompts to utilize LLMs for annotation, often relying on metadata or detailed captions [6, 36] to construct QA data [20, 33, 35]. However, these methods often lack fine-grained video information and depend heavily on raw video data. In contrast, we introduce an automatic pipeline for QA pair generation that processes and annotates raw data without manual input, enhancing scalability.\nVideo LVLMs. In recent years, large vision-language models (LVLMs) have advanced VideoQA tasks significantly. Prior works have enhanced model performance by experimenting with various architectures, with some aligning visual and textual features through Q-Former [5, 16, 48], while others concatenate frame-level features directly [14, 19]. Balancing frame count with token efficiency has become a key focus, typically addressed by uniform sampling [5, 49] or additional modules [22, 35]. Our approach"}, {"title": "3. VideoEspresso", "content": "In this section, we introduce VideoEspresso, a large-scale VideoQA dataset designed to facilitate high-level reasoning over macroscopic video semantics. This dataset is generated through a scalable, fully automated generation pipeline that produces high-quality reasoning VideoQA pairs from distilled video content. The VideoEspresso construction"}, {"title": "3.1. Video Data Curation", "content": "We leverage the vast amount of unannotated Internet videos for building scalable datasets. To build a videoQA dataset with complex semantic reasoning, selecting appropriate data sources and types is essential. As illustrated in Fig. 3, we collect raw videos from 7 datasets [9, 26, 35, 37, 40, 44, 52] encompassing rich temporal dynamics, logical sequences, and causal relationships. These high-level semantics provide a strong foundation for constructing a complex and coherent question-answering dataset. We conduct a manual review of these videos to evaluate their reasoning potential. The dataset includes diverse video types, covering genres such as news, movies, documentaries, animation,"}, {"title": "3.2. Redundancy Removal in Video Frames", "content": "The goal of this module is to eliminate redundant information in the video and retain the essential content by selecting a concise sequence of frames. Different videos exhibit varying rates of content and scene changes. Initially, we determine an appropriate sampling interval based on the type of video. For instance, for rapidly changing dynamic scenes, we set the FPS between 2 and 4, whereas for static scenes, we choose a longer sampling interval, with FPS set to 1. Then, to capture the fine-grained semantic information of the video as input for constructing QA pairs, we use InternVL2-8B [7] to perform frame-level captioning on all sampled frames. To filter out redundant frames in the video, we leverage the language retrieval model BGE-M3 [4] to preliminarily remove highly similar frames through fine-grained semantic filtering. Specifically, for all sampled frame descriptions C, if the cosine similarity between the textual features \u0444\u0442(c) of adjacent captions exceeds a preset threshold T, we apply a Last-In-First-Out (LIFO) filtering approach. This process results in a concise caption sequence C' and the corresponding frames. The process is formulated as follows:\n$S = arg max cos(\u0444T(ci), \u0444T(cj)), Ci, Cj \u2208 C$ (1)\n$C \u2192 C' (c \u2208 C', if S(c) < \u03c4),$ (2)\nwhere S denotes the similarity matrix, and \u0444\u0442 (c) represents the feature computation for the caption c."}, {"title": "3.3. Question-Answer Pair Construction", "content": "This module aims to leverage the powerful language reasoning capabilities of LLMs to automatically construct high-"}, {"title": "3.4. Multimodal Chain-of-Thought Annotation", "content": "To further enhance the reasoning capabilities of models, this module focuses on annotating multimodal evidence that contains key spatiotemporal information. First, we group the Q-A pairs obtained in Sec. 3.3 along with the corresponding frame sequences as input, and design the prompt shown in the lower-left corner of Fig. 2 to guide GPT-4o [31] in extracting key information. Sparse core frames are sufficient to capture enough information to answer the question; hence, we need to obtain the core frames most relevant to the question. Accordingly, our designed prompt primarily guides GPT-40 to extract the following key information: (1) From the captions group, select the captions most relevant to the question, i.e., the captions of core frames; (2) extract key objects from these captions, i.e., the key items; and (3) organize these key objects into a natural language description as evidence to support answering the question, i.e., the evidence. To expand the dimensions of reasoning, we annotate these key elements with temporal and spatial information. For spatial annotation, we apply GroundingDINO [25] to mark bounding boxes around all key items and leverage CLIP-ViT-B/32 [32] to verify consistency between labels and objects within the bounding boxes. For temporal annotation, since GPT-4o's generated captions of core frames GGPT in (1) cannot directly match the original captions at the string level, we employ BGE-M3 [4] to retrieve the caption in the original set Gi and obtain temporal grounding information t. The process is formulated as follows:\n$t = arg max cos(\u0444\u0442(cj), \u0444\u0442(ck)),$ (3)\nwhere cj \u2208 GGPT, Ck \u2208 Gi.\nFinally, we obtain both textual evidence and multimodal key information that includes temporal and spatial dimensions, which can serve as the intermediate reasoning step to"}, {"title": "3.5. Data Analysis", "content": "In Fig. 3 and Fig. 4, we provide a visualization of the data analysis and comparisons. To investigate the temporal distribution of key information in videos, we first examine the distribution of distances between adjacent key frames across different tasks. As shown in Fig.3.(a), the distance distribution between key frames varies significantly across tasks, indicating that the conventional strategy of uniformly sampling video frames is suboptimal and introduces substantial redundancy. As shown in Fig. 3.(b), the number of key items in the CoT of our dataset is diverse, encompassing a range from a few to numerous critical elements, reflecting the complexity and diversity of the visual content. In addition to the unique characteristics of our dataset, we also compare it with the QA content of a popular dataset, MVBench [20]. As shown in Fig. 4.(a), we illustrate the differences between the datasets in terms of token length. The QA set length in MVBench (right) is shorter, while the answer set in our VideoEspresso (left) is on average much longer and exhibits greater diversity in distribution. As illustrated in Fig. 4.(b), we further present a comparison of the word clouds for VideoEspresso and MVBench. In the Question set, our VideoEspresso emphasizes reasoning based on visual facts, with keywords like \u201cconsidering\u201d, \"based", "inferred\" In contrast, MVBench emphasizes basic inquiries such as \u201cobject\u201d, \u201cperson": "nd \"action\" In the Answer set, VideoEspresso includes not only reasoning-related keywords as previously described but also terms associated with reasoning steps, such as \u201cInitially\u201d and \u201cFinally", "object\", \"left": "nd \"forward\"."}, {"title": "4. Hybrid LVLMs Collaboration for VideoQA", "content": "To fully unleash the potential of high-quality video QA pairs offered by VideoEspresso, we propose an efficient video reasoning framework with hybrid LVLMs collaboration to enable cost-effective and accurate video LVLM reasoning. As shown in Fig. 5 the framework consists of two core components: a lightweight selector that identifies core frames that are closely related to the input question, and a powerful LVLM that performs content understanding and reasoning based on these selected core frames."}, {"title": "4.1. Core Frames Selection via Tiny LVLM", "content": "We propose a Lightweight Selector designed to extract core frames that are closely aligned with the question from input videos. Unlike traditional keyframe extraction methods, which primarily filter out semantically similar frames, our approach dynamically selects question-driven core frames to meet diverse task requirements. This enables a reduction in the number of frames passed to large models compared to conventional methods that rely solely on frame semantic similarity. Moreover, this selector serves as a plug-and-play module that can be inserted before any LVLMs.\nOur architecture comprises a lightweight LVLM with 1 billion parameters and an LLM with 0.5 billion parameters in a sequential setup. The LVLM's function is to convert video frames into language descriptions, while the LLM selects frames most relevant to the question based on these descriptions. Specifically, the process consists of two steps. (1) Frame Captioning: Given a video V and a specified frames-per-second (FPS) sampling rate, the LVLM samples frames and generates a caption ci for each frame fi. This process can be formulated as:\n{fi}=1 = SampleFrames(V,FPS) (4)\n{Ci}=1 = LVLM({fi}=1) (5)\nwhere N is the total number of sampled frames and {Ci}1 represents the collection of captions for these frames.\n(2) Core Frame Selection: Using the set of captions {Ci}1 and a question q, the LLM identifies the subset of captions most relevant to q, resulting in a set of core frame captions {c}1:\n{C}1 = LLM({C}=1,q) (6)\nwhere M \u2264 N and {c}}\u2081represents the final set of core captions selected for their relevance to the question. Since this step requires minimal reasoning from the model, we adopt a cost-efficient solution to address the challenges posed by excessive token length when handling video input in large models."}, {"title": "4.2. Fine-Grained Reasoning via LVLM", "content": "Given key frames extracted from the first stage, our goal is to enable the model to effectively leverage multimodal spatiotemporal evidence for answering complex reasoning tasks. We design a two-stage supervised fine-tuning paradigm. In the first stage, we guide the model to extract essential visual evidence from video data relevant to the question, establishing a foundation for deeper reasoning. This is achieved through supervised fine-tuning using instructions like \"Please provide evidence to help answer the question.\" to guide the model in generating evidence. This evidence-based generation process not only filters core information but also enhances multimodal alignment and prepares the model for subsequent reasoning tasks.\nIn the second stage, we further fine-tune the model to directly generate answers based on the extracted multimodal evidence. This is achieved through supervised fine-tuning using instructions like \"Please answer the question with the help of evidence.\" to guide the model in answering with evidence. Unlike traditional single-stage question-answering methods, this two-stage structure divides evidence generation and answer generation, enhancing transparency in reasoning and boosting response accuracy. Besides, it ensures that the model gradually integrates multimodal information for complex spatiotemporal reasoning, significantly improving performance in video question-answering tasks by producing more logically coherent answers."}, {"title": "4.3. Inference", "content": "During the inference phase, we first use the Lightweight Selector to extract core frames from the video that are closely related to the question, serving as input for subsequent reasoning. We then leverage a fine-grained reasoning LVLM, which generates evidence through a chain-of-thought process to support the final answer generation. This workflow enables efficient question answering, from frame selection to answer generation."}, {"title": "5. Experiments", "content": "Our VideoEspresso includes 14 predefined tasks, with each constructed QA pair matched to a corresponding task using GPT-40. If no suitable task alignment is identified, the pair is categorized as \u201cOthers\". To establish a comprehensive benchmark, the defined tasks encompass diverse perspectives, including time, logic, scene, behavior, and state, illustrated by examples such as \u201cEvent Dynamics\u201d, \u201cCausal Analysis\u201d and \u201cTheme Analysis\u201d. Additionally, the framework incorporates real-world application tasks, such as \"Cooking Process\u201d and \u201cTraffic Analysis\u201d. The benchmark assesses the performance of LVLMs through both objective and subjective evaluations, providing a multifaceted"}, {"title": "Experimental Setting.", "content": "To comprehensively evaluate the capabilities of LVLMs on VideoQA tasks, we selected: (1) closed-source large models, such as GPT-40 [31] and Qwen-VL-Max [3]; (2) general-purpose LVLMs that claim strong video capabilities on video benchmarks, such as InternVL [7] and Qwen2-VL [3]; and (3) popular video LVLMs, such as LongVA [49] and mPLUG-Owl3 [46].To ensure the fairness of the reported accuracies, the video frame sampling scheme, temperature, and other parameters follow the settings from the original paper. Additionally, we standardize the maximum token length of the outputs to 512. As our model training details, the learning rate is set to 2e-5, the warmup rate is 0.03, and we train the model for one epoch with global batch size of 16. The training and evaluation process is facilitated on 8 NVIDIA-A100 GPUs.\nEvaluation. To more accurately evaluate the open-ended responses of LVLMs, we propose a two-step evaluation method based on fine-grained semantic similarity. In the"}, {"title": "Objective Evaluation Results.", "content": "We evaluated 7 open-source and 2 closed-source LVLMs on 14 video reasoning tasks in objective evaluation. As shown in Tab. 1, our method achieves the state-of-the-art performance across 12 tasks, and an average accuracy of 34.1%. This performance surpasses that of the top-performing open-source model, InternVL2 [7], and the closed-source GPT-40 [31] by 5.4% and 7.7%, respectively. Compared to our selected backbone, LLaVA-Next-interleave, the performance improves by nearly 10% after fine-tuning with reasoning instructions. In addition to its advantage in video reasoning QA, our method also demonstrates the leading efficiency. Specifically, the average number of input frames is only 1.8% of that used by LongVA-DPO [49], and the FLOPs calculated on the same video input are only 14.74% of those for LLaVA-Next-interleave. Notably, InternVL2 and LongVA-DPO excel in the \"Theme Analysis\u201d and \u201cCooking Process\" tasks, which is likely due to their exposure to large size of the same type of data during training process."}, {"title": "Subjective Evaluation Results.", "content": "We evaluated the quality of LVLMs' answers in subjective evaluation across four aspects, including logic consistency, factuality, accuracy, and conciseness. As shown in Tab. 2, the results align closely with the observations from the objective evaluation. GPT-40 exhibits strong performance in both logical reasoning and factual accuracy, due to its robust language reasoning capabilities and extensive prior knowledge. However, among all open-source LVLMs, our method outperforms the approaches presented in the table across all four dimensions. Notably, in the Conciseness evaluation, our method surpasses GPT-40 by 5%, further demonstrating the significant contribution of our VideoEspresso dataset in enhancing models' ability to learn video reasoning."}, {"title": "5.3. Ablation Study", "content": "Our results in Tab. 3 further prove the effectiveness of visual grounding in CoT evidence. Moreover, ablation experiments involving the ground truth of CoT and the ablation of the CoT process further demonstrate the potential of CoT in enhancing performance on visual tasks. The performance boost achieved by the CoT ground truth is significant, highlighting the importance of endowing LVLMs with reasoning QA capabilities.\nAs shown in Tab. 4, we conducted ablation experiments under different selector combination settings, specifically InternVL2-1B [7] + QwenLM-1.5B [2] and InternVL2-1B + QwenLM-0.5B. In this set of experiments, our core frame selection significantly improved video understanding capabilities compared to the uniform sampling method. Although selectors of different sizes may increase memory usage by 3GB or 5GB, the optimization of redundant keyframe tokens led to a reduction of 26-28GB in memory usage, resulting in a significant improvement in overall video understanding efficiency. We also tested more lightweight LVLMs, such as LLaVA-Next-interleave-0.5B [17] and Qwen-VL-1.5B [3]. However, the results did not meet expectations, likely due to the Tiny LVLMs processing too many image tokens, exceeding their capacity for handling them effectively."}, {"title": "5.4. Adapting Selector to other LVLMs", "content": "We further apply the selector to other LVLMs to explore whether the extracted core frames can be effectively generalized to other models in a zero-shot manner. As shown in Tab. 5, we evaluated the method's performance on GPT-40 and several open-source LVLMs. The results demonstrate performance improvements and a reduction in the number of input frames, with the frame input reduced by approximately 15% on both GPT-40 and InternVL2. For the other two models, experiments show that introducing the selector leads to a slight loss in performance, but substantial gains in frame input. Notably, LongVA achieved a 98% reduction in frame input, which highlights that our proposed selector still aids in reducing computational overhead for reasoning in LLMs, as a plug-and-play module."}, {"title": "6. Conclusion", "content": "In this paper, we presented VideoEspresso, a novel dataset designed to enhance video reasoning by addressing the limitations of existing datasets in terms of scale and granularity. Our approach employs semantic-aware key-frame extraction and leverages GPT-40 to generate fine-grained VideoQA pairs with Chain-of-Thought evidence. By integrating a Hybrid LVLMs Collaboration framework, we achieve cost-effective and accurate video reasoning, outperforming baseline models on the majority of tasks across our proposed benchmark. VideoEspresso sets a new starting point in video reasoning, offering rich annotations that facilitate advanced multimodal understanding. We hope our contributions can facilitate future exploration and development of more sophisticated models capable of tackling complex video reasoning challenges."}]}