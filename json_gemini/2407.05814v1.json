{"title": "Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign Recognition", "authors": ["Yaozong Gan", "Guang Li", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "abstract": "Recent multimodal large language models (MLLM) such as GPT-40 and GPT-4v have shown great potential in autonomous driving. In this paper, we propose a cross-domain few-shot in-context learning method based on the MLLM for enhancing traffic sign recognition (TSR). We first construct a traffic sign detection network based on Vision Transformer Adapter and an extraction module to extract traffic signs from the original road images. To reduce the dependence on training data and improve the performance stability of cross-country TSR, we introduce a cross-domain few-shot in-context learning method based on the MLLM. To enhance MLLM's fine-grained recognition ability of traffic signs, the proposed method generates corresponding description texts using template traffic signs. These description texts contain key information about the shape, color, and composition of traffic signs, which can stimulate the ability of MLLM to perceive fine-grained traffic sign categories. By using the description texts, our method reduces the cross-domain differences between template and real traffic signs. Our approach requires only simple and uniform textual indications, without the need for large-scale traffic sign images and labels. We perform comprehensive evaluations on the German traffic sign recognition benchmark dataset, the Belgium traffic sign dataset, and two real-world datasets taken from Japan. The experimental results show that our method significantly enhances the TSR performance.", "sections": [{"title": "1 Introduction", "content": "Traffic safety is an important issue in the real world. According to the latest statistics from the World Health Organization, about 1.19 million people die in road traffic accidents every year. In addition, road traffic injuries are the leading cause of death for children and young people between the ages of 5 and 29 1. Moreover, road traffic accidents cause serious economic losses and create a burden on society [12]. Therefore, it is urgent to reduce the occurrence of road traffic accidents.\nUnderstanding and recognizing traffic signs is vital for traffic safety. This task is challenging, particularly in complex weather and road conditions [29]. Advanced driver assistance systems (ADAS) use traffic sign data to evaluate driving conditions and notify drivers of inconsistencies, which is crucial for enhancing vehicle safety in emergency situations [27]. Furthermore, TSR can aid"}, {"title": "2 Relate Works", "content": null}, {"title": "2.1 Traffic Sign Recognition", "content": "TSR has been extensively studied, yielding various approaches to address this task. TSR is mainly divided into two steps: traffic sign detection (TSD) and traffic sign classification (TSC). TSD localizes traffic signs from road images and TSC classifies and recognizes the detected traffic signs. In this subsection, we provide an overview of the related works on traditional methods and deep learning-based TSR methods.\nThe first research on TSR was proposed in the 1980s when researchers attempted to create early TSR systems [1]. These systems performed visual inspections, recognized traffic signs, and transmitted the information to drivers, alerting them to specific signs. Subsequently, TSR methods based on hand-crafted features and machine learning algorithms were proposed [24]. Hand-crafted features were used to extract information from traffic signs, and machine learning algorithms recognized the extracted features. Kus et al. [19] proposed a traffic sign detection and recognition technique that enhanced SIFT [22] by incorporating features related to the color of local regions. Hu et al. [16] proposed a TSR method based on SIFT and support vector machine (SVM) [4]. SIFT detects and characterizes key points of traffic signs, and SVM classifies the signs. Traditional TSR methods rely heavily on hand-crafted features, which are sensitive to changes in lighting conditions, occlusion, and complex backgrounds [18]. Besides, these methods are difficult to adapt to diverse traffic sign datasets and real-world scenarios, as hand-crafted features may not generalize well [21]. Despite these limitations, traditional methods remained the basis of early TSR research. The emergence of deep learning has inspired the development of TSR. Compared with traditional hand-crafted feature-based methods, deep learning-based methods using CNNs can learn low-level visual features and high-level semantic information from traffic sign images. Luo et al.[23] proposed a data-driven integrated TSR system, achieving TSR by refining and classifying regions of interest using a multi-task CNN. Zhu et al. [42] proposed an end-to-end CNN-based method for TSR. Besides, Zheng et al. [41] performed a detailed evaluation of TSR using vision transformer (ViT) [8]. Although these deep learning-based TSR methods perform well, they usually require careful training on country-specific datasets. Several approaches have been introduced to reduce the dependence on training data. Supriyanto et al. [31] proposed an unsupervised approach based on the bag-of-visual-word model, which does not require label data and a training process for TSR. Gan et al. [11] proposed a zero-shot method based on midlevel features of CNNs. This method performs TSR by calculating the similarity between the target and template traffic signs. These methods solved the problem of cross-country traffic sign inapplicability that exists with supervised methods. However, the recognition accuracy of these methods needs to be further improved."}, {"title": "2.2 Multimodal Large Language Models", "content": "Recently, MLLMs have attracted attention in both academia and industry [3]. As demonstrated by the existing work [2], MLLMs can solve a wide variety of tasks, which contrasts with previous models that were limited to solving specific tasks. MLLMs are increasingly being used due to their excellent performance in handling different applications such as general natural language tasks and domain-specific tasks. In addition, the proposal of multimodal large language models such as GPT-4 further extends the functionality of language models by seamlessly integrating visual information as part of the input. This integration of visual data enables the models to effectively understand and generate responses that contain both textual and visual cues, thus enabling contextually richer conversations in multimodal environments. In recent months, MLLMs have also received a lot of attention in the field of intelligent transportation, such as autonomous driving and mapping systems [6]. However, to the best of our knowledge, there is no related research on MLLMs in the field of TSR. Therefore, we implement a prior study of MLLMs in TSR to explore an effective method in this paper."}, {"title": "3 Methodology", "content": "In this section, we introduce the proposed method. As shown in Figure 1, our method performs TSD from original road images and fine-grained TSC. By using template traffic signs in the national standard template database to construct a cross-domain few-shot in-context learning method with MLLM, the proposed method eliminates the dependence on large-scale training data with labels."}, {"title": "3.1 Traffic Sign Detection", "content": "TSD is the first phase of the proposed method, aimed at the rapid localization and detection of traffic signs in original urban road images. Our approach is inspired by ViT-Adapter [5]. ViT-Adapter, building upon the plain ViT [8], achieves precise image recognition by introducing image-specific inductive biases, which are also applied in many other areas [20; 9]. To perform effective TSD, we designed a traffic sign-specific output module based on ViT-Adapter. Besides, we constructed an extraction module to extract real images of traffic signs. The traffic sign-specific output module and extraction module compose the proposed TSD network."}, {"title": "3.1.1 Traffic Sign-specific Output Module", "content": "We first take the original road image \\(I_o\\) as an input into ViT-Adapter. The vanilla ViT-Adapter can generate segmentation images with various object category labels for the original input image. In our context, since we only need to detect traffic signs, it will interfere with the detection of traffic signs if segmentation images are generated for all object categories. Therefore, we designed a specialized output module for traffic signs. Specifically, in the generated segmentation image \\(I_c\\), each specific object category is encoded with a different color for recognition. Our specialized output module converts \\(I_c\\) into a binary mask image \\(I_b\\). The method effectively distinguishes regions containing traffic signs from the rest of the image. The designed traffic sign-specific output module provides a clear delineation of the regions containing traffic signs and simplifies subsequent processing steps. The binary mask image \\(I_b\\) effectively separates traffic signs from the background and other objects in \\(I_c\\), thereby enhancing the dedicated detection capability for traffic signs."}, {"title": "3.1.2 Traffic Sign Extraction Module", "content": "Next, we design an extraction module for realizing the extraction of real traffic signs. The extraction module consists of two parts. In the first part, we use the contour detection algorithm [32] to accurately delineate and extract the coordinates of the boundaries of the traffic signs in the obtained binary mask image \\(I_b\\). Specifically, given the binary mask image \\(I_b\\) as a pixel grid in which each pixel is either foreground (representing the traffic sign) or background (representing the surroundings), the contour detection algorithm traces the boundaries of these components to efficiently outline the shape of the detected traffic sign \\(I_d\\). \\(I_d\\) includes the coordinate positions of traffic signs. In the second part, we use the original road image \\(I_o\\) and the sign coordinates \\(I_d\\) generated by the specific output mode of the ViT-Adapter to extract the image \\(I_r\\) that contains only real traffic signs. Since the resolutions of \\(I_o\\) and \\(I_d\\) are the same, the image \\(I_r\\) containing only real traffic signs is obtained by scanning"}, {"title": "3.2 Cross-domain Few-shot In-context Learning for TSR", "content": "After implementing TSD and extracting the traffic sign image \\(I_t\\), we perform cross-domain few-shot in-context learning TSR with MLLM. Typically, MLLMs take an image \\(I \\in \\mathbb{R}^{H\\times W\\times 3}\\) and a text query \\(T_i = [t_i^1, ..., t_i^{l_i}]\\) with length \\(l_i\\), and generate a sequence of textual output \\(T_o = [t_o^1, ..., t_o^{l_o}]\\) with length \\(l_o\\) as follows:\n\\[T_o = MLLM(I, T_i).\\]\nSince TSR is a fine-grained recognition task, different classes of traffic signs show similar characteristics. If only simply input the extracted traffic signs \\(I_t\\) into the MLLM for recognition, although the MLLM can recognize the features of the input image, it is difficult for the MLLM to accurately identify the specific classes of traffic signs. To overcome this limitation and enhance fine-grained classification, we introduce a novel strategy. We generate textual descriptions of each traffic sign category by introducing template traffic signs and use the generated textual descriptions to assist MLLM in the fine-grained classification of traffic signs.\nSpecifically, in the proposed method, we first input the template traffic sign image \\(I_{template}\\) into the MLLM to generate textual descriptions \\(T_a^c\\) for each class \\(I_{template}^c = [I_{template}^1, ..., I_{template}^C]\\) of traffic signs as follows:\n\\[T_a^c = MLLM(I_{template}^c, T_i^c),\\]\nwhere \\(T_i^c\\) represents the corresponding input text query for the template traffic sign \\(I_{template}^c\\). Thus, the textual descriptions of all classes \\(T_a\\) are expressed as\n\\[T_a = [T_a^1, ..., T_a^C].\\]\nThe template traffic signs are derived from the national standard traffic sign template database with standard shapes, colors, and features. The actual traffic sign images are diverse due to lighting conditions, angles, occlusions, etc., which are very different from the template traffic sign images, resulting in difficulties in cross-domain recognition. We instead utilize MLLM's powerful recognition of image features to reduce cross-domain differences by transforming images into textual descriptions. Meanwhile, since the proposed method requires only template traffic signs and MLLM generates descriptions only once for each template traffic sign class, the proposed few-shot method eliminates the dependence on large-scale training data with labels compared with previous methods. We input the extracted traffic sign image \\(I_t\\), the generated text description of each template traffic sign class \\(T_a\\), and the query text \\(T_i\\) into the MLLM to obtain the following output \\(T_o\\) of the proposed cross-domain few-shot in-context learning TSR method:\n\\[T_o = MLLM(I_t, T_a, T_i).\\]\nAs we fully consider the three basic features of traffic signs i.e., shape, color, and composition when generating text descriptions, the generated text descriptions cover the key information of each traffic sign, thus assisting MLLM in fine-grained classification. Therefore, the fine-grained reasoning capability of MLLM for traffic signs is enhanced by the textual descriptions containing the main features generated by the proposed method."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Experimental Settings", "content": "In this subsection, we explain the detailed experimental settings in this study. We conducted experiments on four different datasets, including two benchmark datasets: the German traffic sign"}, {"title": "4.2 Experimental Results", "content": "Table 1 shows the Top-k recognition results of the various methods. We perform a comprehensive evaluation on two benchmark datasets of traffic signs, GTSRB and BTSD, as well as two traffic sign datasets of Japan, to validate the effectiveness of the proposed method. As shown in Table 1, the proposed method achieves promising results when compared to traditional methods as well as CNN-based TSR methods. The accuracy of Top-k on four different datasets exceeds that of the comparative methods with substantial improvement, proving the effectiveness of the proposed method. In addition, experimental results demonstrate that the proposed MLLM-based method outperforms advanced transformer-based methods such as MAE, CLIP, ViT-B, and ViT-L. It is worth mentioning that the proposed method has a substantial lead in the recognition accuracy of Top-1 for all datasets compared to the CNN and transformer-based methods, which demonstrate the accurate recognition"}, {"title": "4.3 Ablation Studies", "content": "Furthermore, we validate the TSR results of inputting traffic sign image directly (baseline) as shown in Table 2, and the Top-k accuracy of the baseline is relatively low on all four datasets, indicating"}, {"title": "5 Conclusion", "content": "We have proposed a novel cross-domain few-shot in-context learning method based on MLLM for enhancing TSR in this paper. We proposed a new TSD network and generated textual descriptions by introducing template traffic signs to enhance the fine-grained recognition of traffic signs by MLLM. Experimental results conducted on two benchmark datasets and two real-world datasets demonstrate the effectiveness of the proposed method. This paper is the first exploration of enhancing MLLM's ability to recognize traffic signs at a fine-grained level, and we hope to inspire TSR-related research based on MLLM in the community."}]}