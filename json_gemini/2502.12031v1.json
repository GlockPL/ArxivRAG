{"title": "Masked Latent Prediction and Classification for Self-Supervised Audio Representation Learning", "authors": ["Aurian Quelennec", "Pierre Chouteau", "Geoffroy Peeters", "Slim Essid"], "abstract": "Recently, self-supervised learning methods based on masked latent prediction have proven to encode input data into powerful representations. However, during training, the learned latent space can be further transformed to extract higher-level information that could be more suited for down-stream classification tasks. Therefore, we propose a new method: MAsked latenT Prediction And Classification (MATPAC), which is trained with two pretext tasks solved jointly. As in previous work, the first pretext task is a masked latent prediction task, ensuring a robust input representation in the latent space. The second one is unsupervised classification, which utilises the latent representations of the first pretext task to match probability distributions between a teacher and a student. We validate the MATPAC method by comparing it to other state-of-the-art proposals and conducting ablations studies. MATPAC reaches state-of-the-art self-supervised learning results on reference audio classification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms comparable supervised methods' results for musical auto-tagging on Magna-tag-a-tune.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, Self Supervised Learning (SSL) methods have become a predominant representation learning approach across various application domains, especially speech and audio processing [1]. To alleviate the absence of labels in SSL, one needs to elaborate pretext tasks where the model learns valuable representations from the input data itself. A wide variety of pretext tasks have been developed [2]. In general, models of this kind may operate either from a single view of the input [3], [4] or multiple views obtained through hand-crafted data augmentations [5], [6]. While the latter variant may be sensitive to the bias that results from the choice of views [7], single-view methods learn representations only from the original data, which requires less prior knowledge. One may also categorize pretext tasks with respect to the problem they solve, i.e., classification [8], reconstruction [3] or prediction [4].\nPretext tasks that consider unsupervised classification as an objective have proven quite successful. Models like HuBERT [9] or BEATS [10] start by assigning a cluster identity to each input, using either K-Means or a learned tokenizer, before learning a representation that can successfully predict the input-cluster association. In computer vision, DINO [8] tries to assign the same classes to multiple different views of the original input, using a teacher-student architecture.\nMore recently, promising results have been obtained using pretext tasks where the goal is to predict the latent repre-sentation of a masked part of the input from the visible part. This leverages the idea of Masked Language Modeling (MLM) [11]. When combined with a teacher-student paradigm, such as in Bootstrap Your Own Latent (BYOL) [5], this leads to M2D [12] and I-JEPA [4].\nContributions This work stems from the intuition that in order to better target classification downstream problems, an effective pretext task could be realised by modifying the goal of predicting the latent representation of a masked part of the input given a visible part (as done in I-JEPA and M2D), to instead predict the cluster identity of the masked part.\nThus, we propose a new SSL method where we consider two pretext tasks which are solved jointly:\ni) a masked prediction task, solved in the latent space, as in previous work [4], [12], [13];\nii) an unsupervised classification task where, as illustrated in Fig.1, the predicted and target representations are separately projected by a student and teacher classification head into probability distributions to be matched through a classification loss.\nWe call this new method: MAsked latenT Prediction And Classification (MATPAC), which encodes the input into a robust predictive representation thanks to the first pretext task, while the classification pretext task makes it easier to extract"}, {"title": "II. RELATED WORKS", "content": "SSL methods based on masked prediction such as M2D [12] or I-JEPA [4] rely on pretext tasks that exploit a single view of the original input, using a teacher-student architecture to solve a prediction problem in the latent space. The student encodes only the visible patches, while the teacher encodes the masked patches as a target latent representation. Then, from the latent representation of the visible patches, a shallow predictor tries to match the target latent representations. While M2D was designed for audio signals and I-JEPA for images, they consider different masking strategies. Riou et al. [14] studied which of those masking strategies is the most suited for audio and found out that random masking is the most effective.\nBesides, BEATS [10] for general audio, HuBERT [9] for speech, or MERT [15] for music, are models relying on unsupervised classification as a pretext task, but they either need iterative procedures or other models for initialization. Alternatively, other works in the field of computer vision use classification as an objective in their pretext task [8], [16], [17], considering multiple views of the input in a teacher-student architecture, thus avoiding the need for third-party models to provide the initial classification targets. DINO [8] passes different augmentations of an input image to the student and teacher networks, projects them into a probability distribution, and learns to match them through classification. In [17], the authors combine a DINO-like loss with the MLM iBOT [16] loss to obtain a teacher-student system relying only on a classification objective.\nFor learning general audio representations, SSAST [18] and MAE-AST [19] combine two pretext tasks: a masked input reconstruction and a classification objective. But as shown in [12], using a masked prediction pretext task in the latent space is much more effective than reconstructing the masked part of the input. To our knowledge, no general self-supervised repre-sentation models combine a teacher-student architecture with both classification and prediction pretext tasks that consider the unsupervised classification of the latent representations resulting from a teacher-student prediction with masking."}, {"title": "III. METHOD", "content": "Fig. 1 depicts our method's entire pipeline. For the masked prediction pretext task, our approach is mainly inspired by M2D [12] and I-JEPA [4] as we use a teacher-student archi-tecture with a form of MLM in the latent space. In addition, we use the predicted and target latent representation to solve an unsupervised classification pretext task.\nInput of the model As input, we extract flattened patches, X, from the log-scale Mel spectrogram of each audio sample. A two-dimensional learned positional encoding, p is added to X before randomly partitioning the sequence into X, the visible patches, and Xm, the masked patches. As opposed to I-JEPA [4], and like M2D [12], we chose a random masking strategy.\nMasked prediction pretext task The student encoder \\(f_\\theta\\), of parameters \\(\\theta\\), projects X such that \\(Z_v = f_\\theta(X_v)\\) is the latent representation of the visible patches. Similarly, the masked patches are projected with the teacher encoder \\(f_\\gamma\\), and \\(Z_m = f_\\gamma(X_m)\\). The teacher's encoder parameters \\(\\gamma\\) are updated using an Exponential Moving Average (EMA) of \\(f_\\theta\\). The update rule is \\(\\gamma \\leftarrow \\lambda \\gamma + (1 - \\lambda)\\theta\\), were \\(\\lambda\\) is the decay rate.\nThe input of the predictor, \\(g_v\\), is all the encoded visible patches to which we append the same shared learnable token m at each masked position. A new learned positional embedding is added to give location information to the masked token. The predictor outputs a representation for visible and masked patch positions. However, we select only the prediction of the latent representation for masked patches to compare them with the targets. The prediction for the masked latent representation is \\(\\hat{Z}_m = \\{g_v(\\text{m\\_concat}(Z_v, m, S_m))^{(i)}, i \\in S_m\\}\\), where \\(S_m\\) is the set of the masked indices. The prediction loss is then the square error of \\(l_2\\)-normalized versions of \\(Z_m\\) and \\(\\hat{Z}_m\\), denoted by \\(Z'_m\\) and \\(\\hat{Z}'_m\\):\n\\[L_{\\text{pred}}(\\hat{Z}'_m, Z'_m) = \\sum_i (\\hat{Z}'_m^{(i)} - Z'_m^{(i)})^2;\\]\nwhere \\(Z'_m = \\{z'_m^{(i)}, i \\in [1,N]\\}\\), and N is the number of targets, i.e. the number of masked patches.\nClassification pretext task Given a student \\(h_\\theta\\) and teacher \\(h_\\gamma\\) projection heads, we transform the target and predicted latent representations into probabilities distribution of dimen-sion K. \\(P_m\\) and \\(\\hat{P}_m\\), respectively the target and predicted probability distributions, are obtained from \\(Z_m\\) and \\(\\hat{Z}_m\\) as follow:\n\\[P_m = \\text{Softmax}((h_\\theta(\\hat{Z}_m)/T_s);\\]\n\\[\\hat{P}_m = \\text{Softmax}((h_\\gamma(Z_m) \u2013 C)/T_t);\\]\nwhere Ts and Tt are temperature parameters used to sharpen the distribution, and C is used to center the distribution. C is an EMA of the mean of \\(h_\\gamma (Z_m)\\). Sharpening and centering were introduced by DINO's authors [8] to avoid the collapse of their method. Our preliminary experiments showed that, even when the masked prediction pretext task is, we need the sharpening and centering operations to avoid the collapse of \\(h_\\gamma\\) and \\(h_\\theta\\) into a trivial solution where one dimension dominates.\nFinally, the classification pretext task matches the distribu-tions with a cross-entropy loss depending on \\(\\theta\\) of \\(h_\\gamma\\):\n\\[L_{\\text{cls}}(\\hat{P}_m, P_m) = - \\sum_i p_m^{(i)} \\log(\\hat{p}_m^{(i)}); \\]\nThe parameters \\(\\omega\\) of \\(h_\\gamma\\) are updated with an EMA of \\(h_\\theta\\) with a decay rate \\(\\zeta\\) different from \\(\\lambda\\) (the decay rate of \\(f_\\theta\\) EMA)."}, {"title": "Total loss", "content": "The final training objective is simply the sum of \\(L_{\\text{cls}}\\) and \\(L_{\\text{reg}}\\) with a weight \\(\\alpha\\):\n\\[L = (1 - \\alpha)L_{\\text{cls}} + \\alpha L_{\\text{pred}}.\\]"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the MATPAC method, showing 1) the effectiveness of combining masked prediction of the latent representation with unsupervised classification, 2) the balancing between the two tasks, and 3) the impact of the design of the classification head.\nA. Experimental Setup\nPre-training dataset and input processing We use Au-dioSet [20] as a pre-training dataset. Our version of AudioSet has 2,012,215 samples over 6s length. Each audio sample is processed to a log-scale Mel spectrogram with a sampling rate of 16,000Hz using a window size of 25 ms, a hop size of 10 ms, and 80 Mel bins spaced between 50 and 8,000 Hz. The log-scale Mel spectrograms are standardized with the dataset statistics. To process the log-scale Mel spectrogram into the input of the encoders, we used a patch size of 16 \u00d7 16 and a masking ratio of 0.7, as it has shown to work well across different tasks [12], [14].\nEncoders and predictor details We based our code on M2D's PyTorch implementation of the teacher-student encoder and predictor parts, making the minimum number of changes while keeping the same parameters. 1 To keep a fair compari-son with SOTA methods like M2D and ATST-Clip [21], and as it seems a good design choice [14], we train our method with audio segments of 6 s. We randomly crop 6-s audio segments from the audio samples above this duration in the pre-training dataset. Similarly to M2D, we train for 300 epochs, with a batch size of 2048, a warm-up of 20 epochs, the same base learning rate and optimizer, and the EMA decay rate \\(\\lambda\\) follows the same update policy.\nClassification head details We use the same architecture and parameters as in DINO for the teacher-student classification heads. 2 It is divided into two parts. First, there are three fully connected layers with a bottleneck output of dimension 256. Then, a weight-normalized fully connected layer projects the \\(l_2\\)-normalized output of the first layers into the probability distribution space of K dimensions [8]. By default, we set the hidden dimension to 2048 and K = 2048. We linearly increase the temperature parameter Tt in Eq. (2), from 0.04 to 0.07 for \\(N_{\\text{epoch}}\\) epochs. This encourages the target's probability distribution to be peakier at the beginning of the training, forcing some \"classes\u201d to appear. We keep \\(T_s\\) = 0.1 for the whole training. Finally, the EMA decay rate \\(\\zeta\\) of the classification heads update is linearly interpolated from 0.998 to 1 at the end of the training and \\(\\alpha\\) = 0.5.\nLinear probing evaluation We train a linear classifier consisting of a single fully connected layer that maps the latent representation of the model we evaluate to the number"}, {"title": "V. CONCLUSIONS", "content": "We proposed a new Self Supervised Learning method, MAsked latenT Prediction And Classification (MATPAC), which combines two pretext tasks to learn a robust repre-sentation for classification downstream tasks. The first pretext task performs masked prediction in the latent space, which had already proved effective in previous work. The second is an unsupervised classification task that projects the first task's predicted and target latent representation into probability distributions matched through a classification loss. Through our evaluation of MATPAC on various downstream tasks and our ablation studies, we prove the effectiveness of solving two pretext tasks jointly. Notably, MATPAC achieves state-of-the-art results compared to other SSL methods on OpenMIC, GTZAN, ESC-50 and US8K while outperforming comparable fully supervised competitors on NSynth and Magna-tag-a-tune."}]}