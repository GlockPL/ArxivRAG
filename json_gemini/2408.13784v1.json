{"title": "Analyzing the Impact of Splicing Artifacts in Partially Fake Speech Signals", "authors": ["Viola Negroni", "Davide Salvi", "Paolo Bestagini", "Stefano Tubaro"], "abstract": "Speech deepfake detection has recently gained significant attention within the multimedia forensics community. Related issues have also been explored, such as the identification of partially fake signals, i.e., tracks that include both real and fake speech segments. However, generating high-quality spliced audio is not as straightforward as it may appear. Spliced signals are typically created through basic signal concatenation. This process could introduce noticeable artifacts that can make the generated data easier to detect. We analyze spliced audio tracks resulting from signal concatenation, investigate their artifacts and assess whether such artifacts introduce any bias in existing datasets. Our findings reveal that by analyzing splicing artifacts, we can achieve a detection EER of 6.16% and 7.36% on PartialSpoof and HAD datasets, respectively, without needing to train any detector. These results underscore the complexities of generating reliable spliced audio data and lead to discussions that can help improve future research in this area.", "sections": [{"title": "1. Introduction", "content": "Automatic speaker verification and speech deepfake detection have gained significant importance in recent times. With the proliferation of sophisticated generation tools and the potential threats associated with their misuse, there is an urgent need to develop systems able to analyze the content they produce and prevent potential menaces and threats. The multimedia forensics community has been actively working in this direction, leading to the development of various systems designed to analyze and detect generated speech content [1, 2].\nAlongside the primary task of discriminating real and fake speech signals, another problem has been gaining increasing attention lately: detecting partially fake signals. This involves analyzing speech tracks and determining whether these are intact or spliced, i.e., generated by the concatenation of real and synthetic speech segments. Such hybrid signals pose a significant threat, as they can deceive traditional speech deepfake detectors that are not specifically trained to address them. Additionally, they generate subtle deepfakes that need detailed temporal analysis to be spotted, as even the insertion of a brief spoofed segment can significantly alter the overall meaning of a speech.\nDue to their creation process, partially fake audio can exhibit two different types of artifacts: intrinsic artifacts, inherently produced by the different properties of the concatenated audio frames (e.g., real and fake signals, fakes generated by different models, etc.); and induced artifacts, introduced by processing techniques applied during the splicing process (e.g., windowing effects, discontinuities at the joints, etc.). Exploiting these artifacts, numerous systems have been proposed in the literature to address partially fake speech signals, each utilizing different approaches and architectures [3, 4, 5]. Some methods rely on acoustic features extracted from the input audio [6, 7], while others leverage pre-trained self-supervised networks as wav2vec [8]. The extracted features are then processed using classification networks, such as CNNs [9], LSTMs [10], and transformers [11].\nIn addition to the introduction of new detectors, novel datasets have also been released [12, 9], and research challenges have been organized [13] to foster advancements in this field. While these contributions are crucial in advancing the current state of the art, generating a dataset with no specific bias is far from being an easy task. While intrinsic artifacts are almost unavoidable and tied to the properties of the original tracks, induced artifacts can be carefully mitigated by the forger using specific techniques at the synthesis stage (e.g., tapering, overlapping joined frames, etc.). However, induced artifacts may be challenging to conceal when creating large-scale datasets in bulk. This is problematic as these irregularities could make the forged signals easier to detect and potentially bias the detectors trained on them.\nIn this paper, we analyze induced artifacts in spliced audio tracks and investigate their impact on state-of-the-art datasets and detectors. We start by analyzing simple sinusoidal concatenation to investigate the artifacts introduced by this process and understand their underlying causes. Then, we extend our study to more complex audio tracks, such as those used to create partially fake speech signals. We evaluate two datasets in the field, PartialSpoof and HAD, assessing the presence of induced splicing artifacts and investigating whether these introduce bias. Finally, we explore the potential issues this phenomenon may introduce on detectors, evaluating their performance in different setups. Our findings highlight the complexity of the partially fake speech detection task and highlight the need for careful datasets and detectors development to ensure the practical effectiveness of such systems in real-world scenarios.\nTo summarise, the main contributions of this work are:\n\u2022 We show that signal splicing techniques may introduce detectable artifacts, potentially creating biased tracks (Sec. 2).\n\u2022 We show that state-of-the-art datasets are affected by this issue (Sec. 3.2) and that a simple threshold-based detector can expose it (Sec. 3.3).\n\u2022 We provide guidelines for processing data to create more realistic and challenging partially fake speech signals (Sec. 4).\n\u2022 We benchmark different learning-based detectors to see if they are biased by the presence of induced artifacts in the training data (Sec. 5)."}, {"title": "2. Spectral Leakage and Splicing Artifacts", "content": "In this section we provide an intuitive understanding of the underlying mechanisms that raise induced artifacts during the audio concatenation process. We start by introducing the fundamental concept of spectral leakage and progress to a simplistic example involving sinusoid concatenation. While this section provides an overview of the key concepts, it is by no means exhaustive in the explanation of the complex phenomena responsible for these artifacts.\nSpectral Leakage. Spectral leakage is a phenomenon observed in the frequency domain analysis when energy from a signal spreads across multiple frequency bins. This effect occurs when a signal is framed with a window function under the incorrect assumption that it is periodic within the observation window. This situation can happen in two scenarios: (a) the signal is periodic, but the length of the window is not an integer multiple of its period; (b) the signal is non-periodic. The mismatch between the actual signal and the assumed periodic model results in a distortion of the frequency spectrum, causing energy to spread across frequency bins that would ideally have zero energy.\nTo illustrate the principle of spectral leakage, let us consider a discrete sinusoidal signal x and its Discrete Fourier Transform (DFT) X. Ideally, the magnitude of X exhibits a single peak at the sinusoid's frequency. When x is framed with a window function w, the resulting signal is $x_w = x \\cdot w$, where \u201c.\u201d denotes sample-wise multiplication. Let us denote with W the DFT of w. By definition, the DFT of $x_w$ is given by $X_w = X * W$, where \u201c*\u201d indicates convolution. The magnitude of $X_w$ thus corresponds to the magnitude of W centered at the sinusoid's frequency. If the window function w is rectangular, its DFT W corresponds to a sinc function. The sinc function introduces sidelobes in the frequency domain of $X_w$, causing energy to spread across adjacent frequency bins, a phenomenon known as spectral leakage. In principle, we can minimize the spectral leakage by selecting a window whose length is a multiple of the signal's period. This alignment ensures that the sinc function is sampled at its zero crossing points, thereby reducing leakage. However, when a signal is observed within a finite window with aperiodic boundary conditions, sidelobes still appear in the frequency spectrum. This occurs because the sinc function is no longer sampled precisely at its zero-crossings.\nInduced Splicing Artifact. To define a spliced recording, let us consider two discrete-time signals $x_1$ and $x_2$, with lengths $N_1$ and $N_2$, respectively. We define the spliced audio track $x_s$ as the concatenation in time of $x_1$ and $x_2$, such as $x_s = [x_1, x_2]$. The resulting length of $x_s$ is $N = N_1 + N_2$, and the splicing point is at sample $N_1 + 1$, where the two signals are joined. Speech forensic detectors commonly operate on spectrogram-based audio representations, which involve framing and windowing the speech under analysis. We assert that in the case of partially fake signals, it is reasonable to assume that a forensic analyst will observe frames that span the splicing point. These frames are likely to exhibit induced splicing artifacts.\nLet us consider as an example an elementary spliced signal $x_s$, which is created by concatenating a sinusoid $x_1$ and a shifted version of itself $x_2$ (e.g., by adding a phase shift). When framing this signal, even if we choose an analysis window whose length is equal to a multiple of the period of $x_1$, the abrupt transition between the two sinusoids will cause leakage in the spectrum at the splicing point. Let us consider another elementary signal $x_s$ obtained by concatenating an integer amount of periods from a sinusoid $x_1$ with a different amplitude version of the same sinusoid $x_2$. Even if we frame the signal with a window whose length is multiple of the sinusoid period, the amplitude change between the two segments will cause spectral leakage."}, {"title": "3. Datasets Analysis", "content": "In this section, we explore the presence of induced artifacts in well-known datasets. First, we introduce the datasets under consideration. Then, we show that induced artifacts can be visually observed through data analysis. Finally, we quantify the presence of induced artifacts in the datasets by distinguishing between spliced and non-spliced tracks using a straightforward method that exploits the artifacts.\n3.1. Considered Datasets\nWe examine two state-of-the-art datasets designed for partially fake speech detection: PartialSpoof [12] and HAD [9]. To the best of our knowledge, these are the only two datasets available for partially fake speech detection at the time of writing. Here, we provide a concise overview of both datasets, focusing on their construction policies. We refer the reader to the original papers for more detailed information.\nPartialSpoof. PartialSpoof [12] is an English speech database derived from the ASVspoof 2019 LA corpus [14], which contains both real and partially fake speech signals. It follows the same structure as the ASVspoof dataset and is divided into three partitions: training, development, and evaluation. The dataset includes synthetic speech generated by 17 distinct methods, varying between training-development and evaluation subsets. The creation of the partially fake samples followed a rigorous 5-step procedure, summarized as follows: (1) Waveform amplitudes of real and fake utterances were normalized to -26 dBov. (2) Variable-length candidate segments were chosen using three types of Voice Activity Detector (VAD) methods, with the final selection based on majority voting. (3) Segments from real utterances were replaced with fake segments, and vice versa, using segments from different utterances by the same speaker. Each segment was inserted only once per host utterance, and the inserted segments were of similar duration to the originals. Time-domain cross-correlation and the overlap-add method were employed for substitution and concatenation to minimize artifacts. (4) After concatenation, each utterance was annotated with fine-grained segment labels and classified as real or partially fake, based on the presence or absence of synthetic patches. (5) Post-processing operations were performed in order to match the spoof class distribution of the ASVspoof 2019 LA database.\nHalf-truth Audio Detection. Half-truth Audio Detection (HAD) [9] is a Mandarin speech dataset and was the first corpus released containing partially fake speech tracks. It is based on AISHELL-3 [15], a multi-speaker speech dataset designed for training Text-to-Speech (TTS) models. While it was first released as an independent set, it later became part of the Audio Deepfake Detection (ADD) challenge dataset [13]. Partially fake speech tracks used in this corpus are created as follows. First, the authors modified the transcripts of real speech samples by altering keywords to change the intended meaning. Then, they trained a TTS model on AISHELL-3 to synthesize audio from the edited texts. Finally, they combined real and fake audio by substituting the edited keywords in the original speech with those from the synthetic recordings. The process included volume normalization and forced alignment to ensure precise replacement.\n3.2. Visual Artifacts Analysis\nLet us consider the partially fake speech tracks from the two datasets presented above and investigate the presence of induced artifacts in their spliced tracks. To this purpose,. Although the induced splicing artifacts at the concatenation points are generally inaudible, they can be easily exposed by a straightforward frequency analysis.\nIn the PartialSpoof track (left), the bias is particularly pronounced due to a distinct characteristic of the original ASVspoof 2019 dataset: all concatenated signals exhibit a nearly silent band at lower frequencies (<80 Hz). This characteristic makes it particularly challenging to hide any artifacts within this frequency range, as their presence becomes highly noticeable. In contrast, the HAD track (right) does not show a significant silent band, yet induced artifacts are still visible. Specifically, the leakages are pronounced in both the higher and lower frequency ranges, where signals typically exhibit lower energy levels compared to the rest of the spectrum. It is noteworthy how these artifacts persist despite the dataset's design, which includes various techniques to minimize the audibility of transitions between concatenated tracks. This highlights the considerable difficulty in completely eliminating discontinuities caused by signal concatenations.\n3.3. Quantitative Artifacts Analysis\nTo quantify the presence of induced artifacts, we propose a simple method to discriminate between spliced and non-spliced tracks and evaluate its performance on the two considered datasets. As discussed in the previous sections, induced splicing artifacts manifest as energy content spreading among spurious frequency components, which are visible in the frequency domain as \"streaks\" across the entire spectrum. Building on this insight, we examine specific frequency bands without speech content and measure their dynamic range. Our hypothesis is that these bands, typically showing a shallow dynamic range due to the absence of speech, will exhibit an increased range in the presence of an induced artifact, i.e., a splicing point.\nThe proposed method to quantitative measure the presence"}, {"title": "4. Mitigation Techniques", "content": "Given the ease of detecting induced artifacts, we conducted experiments to determine if and to what extent it is possible to mitigate their presence. The goal of artifact mitigation strategies is to conceal the artifacts by tampering with the frequency bands where they occur. To do so, we built a small dataset of spliced audio by concatenating randomly selected real and fake segments, ensuring each generated track had only one splicing point. Then, we tested various mitigation techniques, including lossy compression, cross-fading, Overlap-and-Add (OLA), and linear predictive coding, and analyzed the obtained tracks to determine the effectiveness of each. In the following, we focus on the approach that proved most effective in concealing artifacts: OLA. To create the dataset, we selected tracks from the train set of ASV spoof 2019. We did so to ensure the most challenging scenario possible, given that these tracks contain a completely silent band, making it particularly difficult to conceal induced artifacts. We generated the spliced tracks with the following approach:\n1. We select one real and one fake track from the same speaker and then use a VAD to find the longest silent region within each track (excluding leading and trailing silences). The concatenation is performed within these silent regions, as they exhibit lower energy levels compared to the rest of the track.\n2. We randomly choose either the real or the fake track and extract the segment from the beginning to the end of its longest silence region. For the other track, we retain the portion from the beginning of its longest silence to the end.\n3. We apply OLA on the two segments using a Hanning window, applying half of the window function to the end of the first segment and the other half at the beginning of the second segment.\nWe experimented with five different lengths of OLA windows, creating 1200 spliced tracks for each configuration. We evaluated our detection method on these tracks and other 1200 real tracks randomly selected from the remaining dataset. The experimental setup for the dynamic range analysis is the same as outlined in Section 3.3 for the PartialSpoof dataset. Results for each OLA window are reported in the Clean column of Table 2.\nConsidering an OLA window of 256 samples, the AUC value aligns with that obtained on the PartialSpoof dataset (AUC=98%). While increasing the window size helps to mitigate the presence of the artifacts, the minimum achieved AUC is still a substantial 88.99%. Therefore, we repeated the experiment by injecting different levels of white noise (SNRdB 60, SNRdB 50, SNRdB46, SNRdB40) into the signals. We applied a low-pass filter of order 7 with a cut-off frequency of 80 Hz on the noise so that it applied only to the lower frequencies of the signals and remained inaudible. As shown from columns 3 to 6 of Table 2, effectively hiding the artifacts requires both a substantial level of noise, such as SNRdB 46 or SNRdB 40, and an OLA window of at least 1024 samples. With the shortest window of 256 samples and maximum noise SNRdB 40, the AUC value still stands at 70.51%. As a final experiment, we applied a high-pass filter to the partially fake tracks we generated, considering a filter of order 8 with a cut-off at 100 Hz. AUC values are shown in the last column of Table 2. As widely expected, this brute-force method makes the bias introduced by the artifacts undetectable, rendering the dynamic range analysis ineffective regardless of the chosen OLA window."}, {"title": "5. Detectors Analysis", "content": "Given that a straightforward threshold-based detector performs effectively on partially fake speech tracks due to the presence of induced artifacts, we now want to investigate whether these leakages might introduce bias into more advanced detectors trained and tested on these datasets. It has already been shown that partially fake speech detectors focus on the splicing points to perform their prediction [17]. However, we aim to analyze whether this focus is due to the presence of critical information at these points or merely because of the induced splicing artifacts. If the focus is on critical information, this could lead to valuable insights. On the other hand, if it is due to artifacts, it would indicate a bias in the trained detectors. To verify this aspect, we evaluate four state-of-the-art speech deepfake detectors, each utilizing different input features. We train the models for the partially fake speech detection task, focusing on the so-called utterance-level classification, i.e., discriminating between authentic and partially fake tracks.\nThe considered models include RawNet2 [18] which processes raw waveforms, a LCNN model [19] which receives Linear Frequency Cepstral Coefficients (LFCCs) as input, a SENet34 [20] fed with spectrograms and MCG-Res2Net50 [21] trained on Constant-Q Transform (CQT) representations of the input audio. We trained all the models for 100 epochs, using Cross-Entropy as loss function Adam as optimizer. Validation loss was monitored throughout training, with early stopping set to 20 epochs and a learning rate of $10^{-4}$ appropriately reduced on plateaux. Batch sizes were 128, 246, 48, and 64 samples for RawNet2, LCNN, SENet34, and MCG-Res2Net50, respectively, and each of these was balanced to contain the same number of authentic and spliced samples. We trained and validated the detectors on the training and development partitions of PartialSpoof, respectively. We decided to focus this analysis on PartialSpoof only, as it is the most widely employed dataset for the task at hand. Then, we tested the models on two different versions of the evaluation partition: the original data and a version where we applied the same high-pass filtering described in Section 4 to remove the induced artifacts from the spliced tracks. The goal of this experiment was to determine whether the detectors rely on splicing artifacts for their predictions or if they focus more on the actual signal content.\nThe detectors trained using raw waveforms and LFCCs as input, namely RawNet2 and the LCNN, prove robust when induced splicing artifacts are filtered out from the evaluation data, with an AUC value that is almost not affected. In contrast, the performances of the SENet34 and MCG-Res2Net50 drop significantly when the artifacts are removed from the test set: the EER of the SENet34 increases from 2% to 27.2%, and that of MCG-Res2Net50 goes from a remarkable 0.6% to 43.2%, that is close to random guessing. This outcome suggests that the SENet34 and MCG-Res2Net50 have been significantly biased by the presence of induced artifacts in the training data. It is no coincidence that the models showing the most noticeable performance deterioration are those that rely on input features that explicitly consider the frequency domain, i.e., spectrograms and CQTs. These models, which analyze detailed frequency information, are more susceptible to overfitting on spurious frequency components, like the induced artifacts, affecting their robustness when such leakages are absent.\nAs a final experiment, we investigated to what extent the spectral leakage bias boosted the performance of the detectors. We re-trained the detectors on PartialSpoof, this time applying the high-pass filter to both the training and test sets. The seed and training setup for all models remained unchanged. After filtering, the performance of SENet34 and MCG-ResNet50 improved and is now more comparable to that of RawNet2 and the LCNN, suggesting that they are no longer overfitting to any bias. These findings highlight the importance of proper training to ensure detectors avoid bias. This is a crucial aspect for applying the developed models to real-world scenarios where such artifacts may be minimized, and detection cannot rely on their presence."}, {"title": "6. Discussion", "content": "Induced artifacts resulting from signal concatenation are challenging to avoid when generating a spliced signal. Using a sufficiently short STFT analysis window, frequency smearing at the concatenation point will occur unless a perfectly continuous signal is built in both terms of magnitude and phase components. Therefore, creating a high-quality spliced track from concatenation is not as straightforward as it may seem since even minor discontinuities in the resulting signal can introduce spurious frequency components in its spectrum. The presence of such induced artifacts can make a spliced track extremely easy to detect. The difficulty in avoiding this phenomenon has both positive and negative implications. On the positive side, it suggests that detecting splicing attacks by a malicious user may be easier than expected, especially if the attacker lacks expertise in signal processing. On the negative side, the presence of induced artifacts in partially fake tracks can bias state-of-the-art datasets and lead detectors to overfit these artifacts, particularly those that heavily rely on frequency domain inputs. Avoiding this bias is crucial, as there is a valid concern that these detectors may perform poorly when evaluated on spliced data where induced artifacts have been successfully concealed. One key aspect to avoid the detectability of induced artifacts is the choice of source data for constructing spliced samples. Selecting source tracks that are rich in content across all frequency components can effectively conceal artifacts, as the dense spectrum of these signals is less impacted by the introduction of artifacts. To prove this point, we repeated the experiment from Section 4 using tracks from the training set of the newly released ASVspoof5 dataset [22] as source data, following the same setup as before. We achieved an average AUC of 57% for all the OLA windows without applying any noise or filtering. This result is significantly better compared to those obtained in Section 5 (average AUC=92.6%) and highlights the critical role of source data in mitigating the presence of induced artifacts in spliced tracks, which is pivotal to ensure the development of synthetic speech detectors that do not overfit them."}, {"title": "7. Conclusions", "content": "In this work, we explored the nature of spectral artifacts resulting from the concatenation of different signals. We evaluated the presence of these artifacts in state-of-the-art datasets for partially fake speech detection and their effects on synthetic speech detectors trained for this task. Our findings indicate that both the partially fake tracks of PartialSpoof and the HAD dataset are significantly affected by this phenomenon, and it is possible to discriminate between their real and spliced tracks with a straightforward dynamic range analysis without the need to train any detector. Also, we show that such induced splicing artifacts can create a bias on which synthetic speech detectors might overfit, especially those trained on frequency domain input features. We investigated several mitigation methods and found that, while completely eliminating these artifacts is almost impossible, applying basic signal processing smoothing techniques and, most importantly, carefully selecting the source data used to generate the spliced tracks can effectively address the issue."}]}