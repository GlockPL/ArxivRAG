{"title": "Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design", "authors": ["Masatoshi Uehara", "Xingyu Su", "Yulai Zhao", "Xiner Li", "Aviv Regev", "Shuiwang Ji", "Sergey Levine", "Tommaso Biancalani"], "abstract": "To fully leverage the capabilities of diffusion models, we are often interested in optimizing downstream reward functions during inference. While numerous algorithms for reward-guided generation have been recently proposed due to their significance, current approaches predominantly focus on single-shot generation, transitioning from fully noised to denoised states. We propose a novel framework for test-time reward optimization with diffusion models. Our approach employs an iterative refinement process consisting of two steps in each iteration: noising and reward-guided denoising. This sequential refinement allows for the gradual correction of errors introduced during reward optimization. Finally, we demonstrate its superior empirical performance in protein and cell-type specific regulatory DNA design. The code is available at https://github.com/masa-ue/ProDifEvo-Refinement.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have achieved significant success across various domains, including computer vision and scientific fields (Ramesh et al., 2021; Watson et al., 2023). These models enable sampling from complex natural image spaces or molecular spaces that resemble natural structures. Beyond the capabilities of such pre-trained diffusion models, there is often a need to optimize downstream reward functions. For instance, in text-to-image diffusion models, the reward function may be the alignment score (Black et al., 2023; Fan et al., 2023; Uehara et al., 2024), while in protein sequence diffusion models, it could include metrics such as stability, structural constraints, or binding affinity (Verkuil et al., 2022), and in DNA sequence diffusion models, it may involve activity levels (Sarkar et al., 2024; Lal et al., 2024). Building on the motivation above, we focus on optimizing downstream reward functions while preserving the naturalness of the designs. (e.g., a natural-like protein sequence exhibiting strong binding affinity) by seamlessly integrating these reward functions with pre-trained diffusion models during inference. While numerous studies have proposed to incorporate rewards into the generation process of diffusion models (e.g., classifier guidance (Dhariwal and Nichol, 2021) by setting rewards as classifiers, derivative-free methods (Wu et al., 2024; Li et al., 2024)), they rely on a single-shot denoising pass for generation. However, a natural question arises:\nCan we further leverage inference-time computation during generation to refine the model's output?\n+ In this study, we observe that diffusion models can inherently support an iterative generation procedure, where the design can be progressively refined through successive cycles of masking and noise removal. This allows us to utilize arbitrarily large amounts of computation during generation to continuously improve the design. Motivated by the above observations, we propose a novel framework for test-time reward optimization with diffusion models. Our approach employs an iterative refinement algorithm consisting of two steps in each iteration: partial noising and reward-guided denoising as in Figure 1. The reward-guided denoising step transitions from partially noised states to denoised states using techniques such as classifier guidance or derivative-free guidance. Unlike existing single-shot methods, our approach offers several advantages. First, our sequential refinement process allows for the gradual correction of errors introduced during reward-guided denoising, enabling us to optimize complex reward functions, such as structural properties in protein sequence design. In particular, this correction is expected to be crucial in recent successful masked diffusion models (Sahoo et al., 2024; Shi et al., 2024), as once a token is demasked, it remains unchanged until the end of the denoising step. Besides, for reward functions with hard constraints, commonly encountered in biological sequence or molecular design (e.g., cell-type-specific DNA design (Gosai et al., 2023; Lal et al., 2024) or binders with high specificity), our framework can effectively optimize such reward functions by initializing seed sequences within feasible regions that satisfy these constraints. Our contribution is summarized as follows. First, we propose a new reward-guided generation framework for diffusion models that sequentially refines the generated outputs (Section 3). Our algorithm addresses two major issues in existing methods such as the lack of a correction mechanism and difficulties of handling hard constraints. Secondly, we provide a theoretical formulation demonstrating that our algorithm samples from the desirable distribution $exp(r(x))p_{pre}(.)$, where $p_{pre}(.)$ is a pre-trained distribution (Section 4) and $r(.)$ is a reward function. Finally, we present a specific instantiation of our unified framework by carefully designing the reward-guided denoising stage in each iteration, which bears similarities to evolutionary algorithms (Section 5). Using this approach, we experimentally demonstrate that our algorithm effectively optimizes reward functions, outperforming existing methods in computational protein and DNA design (Section 6)."}, {"title": "1.1. Related Works", "content": "We categorize related works into three key aspects.\nGuidance (a.k.a. test-time reward optimization) in diffusion models. Most classical approaches involve classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2021), which adds the gradient of reward models (or classifiers) during inference. As reviewed in (Uehara et al., 2025), recently, derivative-free methods such as SMC-based guidance (Wu et al., 2024; Dou and Song, 2024; Phillips et al., 2024; Cardoso et al., 2023) or value-based sampling (Li et al., 2024) have been proposed. However, these methods rely on single-shot generation from noisy states to denoised states. In contrast, we propose a novel iterative refinement approach that enables the optimization of complex reward functions, which can be challenging for single-shot reward-guided generation. Note while classifier-free guidance (Ho and Salimans, 2022) and RL-based fine-tuning (Fan et al., 2023; Black et al., 2023) also aim to address reward optimization in diffusion models, they are orthogonal to our work, as we focus on test-time techniques without any training. Refinement in language models. Refinement-style generation has been explored in the context of BERT-style masked language models and general language models (Novak et al., 2016; Guu et al., 2018; Wang and Cho, 2019; Welleck et al., 2022; Padmakumar et al., 2023). However, our work is the first attempt to study iterative refinement in diffusion models. Note that while some readers may consider editing in diffusion models (Huang et al., 2024) to be relevant, this is a distinct area, as the focus is not on reward optimization, unlike our work. Evolutionary algorithms and MCMC for biological sequence design. Refinement-based approaches with reward models, such as variants of Gibbs sampling and genetic algorithms, have been widely used for protein/DNA design (Anishchenko et al., 2021; Jendrusch et al., 2021; Hie et al., 2022; Gosai et al., 2023; Pacesa et al., 2024). However, most works do not address the integration of diffusion models. While some studies focus on integrating generative models (Hie et al., 2024; Chen et al., 2024), we explore an approach tailored to diffusion models, given the recent success of diffusion models in protein and DNA sequence generation (Alamdari et al., 2023; Wang et al., 2024)."}, {"title": "2. Preliminaries", "content": "We first provide an overview of diffusion models, then discuss current reward-guided algorithms in diffusion models and the potential challenges, which motivate our proposal."}, {"title": "2.1. Diffusion Models", "content": "In diffusion models, the objective is to learn a sampler $p_{pre}(.) \\in \\Delta(\\mathcal{X})$ for a given design space $\\mathcal{X}$ using available data. The training procedure is summarized as follows. First, we define a forward noising process (also called a policy) $q_t: \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$ that proceeds from $t=0$ to $t=T$. Next, we learn a reverse denoising process $p_t: \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$ parametrized by neural networks, ensuring that the marginal distributions induced by these forward and backward processes match. To provide a concrete illustration, we explain masked diffusion models. However, we remark that our proposal in this paper can be applied to any diffusion model.\nExample 1 (Masked Diffusion Models). Here, we explain masked diffusion models (Sahoo et al., 2024; Shi et al., 2024; Austin et al., 2021; Campbell et al., 2022; Lou et al., 2023)). Let $\\mathcal{X}$ be a space of one-hot column vectors {$\\mathbf{x} \\in {0, 1}^K: \\sum_{i=1}^K \\mathbf{x}_i = 1$}, and $Cat(\\pi)$ be the categorical distribution over $K$ classes with probabilities given by $\\pi \\in \\Delta^K$ where $\\Delta^K$ denotes the $K$-simplex. A typical choice of the forward noising process is $q_t(X_{t+1} | X_t) = Cat(\\alpha_t X_t + (1 - \\alpha_t)m)$ where m = [0,......, 0, Mask]. Then, defining $\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i$, the backward process is parameterized as\n$\\begin{cases}   \\delta(X_{t-1} = x_t) & \\text{if } x_t \\neq m \\\\   Cat( \\frac{(1 - \\bar{\\alpha}_{t-1})m + (\\bar{\\alpha}_{t-1} - \\bar{\\alpha}_t)z_0(x_t; 0)}{1-\\bar{\\alpha}_t}) & \\text{if } x_t = m, \\end{cases}$\nwhere $z_0(x_t)$ is a predictor from $x_t$ to $x_0$. Notation and remark. $\\delta_a$ denotes the Dirac delta distribution at mass a. With a slight abuse of notation, we express the initial distribution as $p_{T+1}: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{X})$, and denote [1,..., T] by [T]."}, {"title": "2.2. Single-Shot Reward-Guided Generation", "content": "Our goal is to generate a natural-like design with a high reward. In particular, we focus on inference-time algorithms that do not require fine-tuning of pre-trained diffusion models. Below, we provide a summary of these methods.\nFor reward-guided generation, we often aim to sample from\n$\\underset{p \\in \\Delta(\\mathcal{X})}{\\text{argmax}} \\mathbb{E}_{x\\sim p}[r(x)] - \\alpha KL(p||p_{pre})$\n$= exp(r(.)/\\alpha)p_{pre}(.)/C$,\nwhere C is the normalizing constant. This objective is widely employed in generative models, such as RLHF in large language models (LLMs) (Ziegler et al., 2019; Ouyang et al., 2022). In diffusion models (e.g., Uehara et al. (2024, Theorem 1)), this is achieved by sequentially sampling from the soft optimal policy {$p^*_t$} from t = T + 1 to t = 1, which is defined by\n$p^*_t(\\cdot | x_t) \\propto exp(v_{t-1}(\\cdot)/\\alpha)p_{pre}(\\cdot | x_t)$,\nwhere\n$v_t(x_t) := \\alpha log \\mathbb{E}_{x_0 \\sim p_{pre}(x_0|x_t)}[exp(r(x_0)/\\alpha)|x_t].$\nand the expectation is taken w.r.t. the pre-trained policy. Here, as illustrated in Figure 2, $v_{t-1}$ serves as a look-ahead function that predicts the reward at $x_0$ from $x_t$, often referred to as the soft value function in RL (or the optimal twisting proposal in SMC literature (Naesseth et al., 2019)). In practice, we cannot precisely sample from soft optimal policies because (1) the soft value function $v_t$ is unknown, and (2) the action space under the optimal policy is large. Current algorithms address these challenges as follows.\n(1): Approximating soft value functions. A typical approach is to use $r(x_0(x_t))$ by leveraging the decoder $z_0(x_t)$ obtained during pre-training. This approximation arises from replacing the expectation over $x_0 \\sim p_{pre}(x_0|x_t)$ in (2) with $z_0(x_t)$ (i.e., a Dirac delta at the mean of $p_{pre}^t(x_0|x_{t-1})$). Note its accuracy degrades as t increases (i.e., as the state becomes more noisy). Despite its potential crudeness, this approximation is commonly adopted due to its training-free nature and the strong empirical performance demonstrated by methods such as DPS (Chung et al., 2022), reconstruction guidance (Ho et al., 2022), universal guidance (Bansal et al., 2023), and SVDD (Li et al., 2024).\n(2): Handling large action space. Even with accurate value functions, sampling from the soft optimal policy still exhibits difficulty because its sample space X is still large. Hence, we often resort to approximation techniques as follows.\n*   Classifier Guidance: In continuous diffusion models, the pre-trained policy $p_t(\\cdot | X_{t-1})$ is a Gaussian policy. By constructing differentiable value function models, we can approximate $p^*_t$ by shifting the mean using $\\nabla v_t(\\cdot)/\\alpha$. A similar approximation also applies to discrete diffusion models (Nisonoff et al., 2024).\n*   Derivative-Free Guidance: Another approach is using importance sampling (Li et al., 2024). Specifically, we generate several samples from $p_t(\\cdot | X_{t-1})$ and then select the next sample based on the importance weight $exp (v_t (.)/\\alpha)$. A closely related method using Sequential Monte Carlo (SMC) has also been proposed, as discussed in Section 1.1."}, {"title": "2.3. Challenges of Single-Shot Generation", "content": "There are two main challenges with the aforementioned current algorithms. First, for certain complex reward functions, they may fail to fully optimize the rewards. This occurs because the value functions employed in these algorithms have approximation errors. When a value function model is inaccurate, the decision at that step can be suboptimal, and there is no correction mechanism during generation. This issue can be particularly severe in recent popular masked discrete diffusion models in Example 1, where once a token changes from the masking state, it remains unchanged until the terminal step (t = 0) (Sahoo et al., 2024; Shi et al., 2024). Consequently, any suboptimal token generation at intermediate steps cannot be rectified. Another related challenge lies in accommodating hard constraints with a set $\\mathcal{C} \\subset \\mathcal{X}$. Although one might assume that simply setting $r(\\cdot) = \\mathbb{I}(\\cdot \\in \\mathcal{C})$ would suffice, in practice, the generated outputs often fail to meet these constraints. This difficulty again arises from the inaccuracy of value function models at large t (i.e., in highly noised states)."}, {"title": "3. Iterative Refinement in Diffusion Models", "content": "To tackle challenges discussed in Section 2.3, we propose a new iterative inference-time framework for reward optimization in diffusion models. Our algorithm is an iterative algorithm where each step consists of two procedures: noising using forward pre-trained policies and reward-guided denoising using soft optimal policies. This framework is formalized in Algorithm 1.\nCompared to existing algorithms that only perform single-shot denoising from t = T to t = 0, our algorithm repeatedly performs reward optimization, as depicted in Figure 3. The challenge of single-shot algorithms \u2013 namely, the lack of a correction mechanism discussed in Section 2.3 \u2013 can be addressed in RERD, by sequentially refining the outputs."}, {"title": "4. Theoretical Analysis", "content": "We present the theoretical analysis of RERD. We begin with the key theorem, which clarifies its target distribution.\nTheorem 1 (Target Distribution of RERD). Suppose (a) the initial design $X^{(0)}_0$ follows $p^{(a)}$ (defined in (1)), (b) the marginal distributions induced by the forward noising process match those of the learned noising process in the pre-trained diffusion models. Then, the output $x^{(S)}$ from RERD follows the target distribution\n$p^{(a)}(.) \\propto exp(r(.)/\\alpha)p_{pre}(.)$.\nFirst, we discuss the validity of the assumptions. The assumption (a) is readily satisfied when using the introduced strategy of initial designs in Section 3. The assumption (b) is also mild, as pre-trained diffusion models are trained in this manner (Song et al., 2021), though certain errors may arise in practice. Another implicit assumption in practice is that we can approximate soft-optimal policies accurately. Next, we explore the implications of Theorem 1. The central takeaway is that we can sample from a desired distribution for our task $p^{(a)}$ in (1). Although this guarantee appears to mirror existing single-shot algorithms discussed in Section 2.2, we anticipate differing practical performance in terms of rewards. This is due to their robustness against errors in soft value function approximation $v_t(x_t) \\approx r(x_0(x_t))$. To clarify, recall that in reward-guided algorithms, we must employ approximated soft value function models when sampling from the soft optimal policies $p^*_t \\propto exp(v_{t-1}(.)/\\alpha)p_{pre}(\\cdot | x_t)$. The approximation often becomes more precise as the time step t in the soft optimal policy approaches 0, as mentioned in Section 2.2. Indeed, in the extreme case, when t = 0, the exact equality holds. Therefore, by maintaining a sufficiently small noise level t = K and avoiding the approximation of value functions at large t, RERD can effectively minimize approximation errors in practice. Sketch of the Proof of Theorem 1. The detailed proof is deferred to Section A. In brief, first, we show that the marginal distribution after noising is $p_{pre}(.) exp(v_K (.)/\\alpha)/C$ where $p_{pre}^{(K)}(.)$ is a marginal distribution at K induced by pre-trained policies. Then, by induction, during reward optimization, we show that k \u2208 [K]: $x_k$ follows $p_{pre}(\\cdot) exp(v_k(\\cdot)/\\alpha)/C$. Then, when k = 0, it would be equal to $p_{pre}(.) exp(r(.)/\\alpha)$."}, {"title": "5. Practical Design of Algorithms", "content": "As mentioned, RERD is a unified sequential refinement framework that can integrate off-the-shelf approximation strategies during reward-guided denoising (Line 4 in Algorithm 1). A key practical consideration is determining which approximation methods to adopt. In this context, we present a specific version that bears similarities to evolutionary algorithms."}, {"title": "5.1. Combining Local IS and Global Resampling", "content": "Algorithm 1) is presented in Algorithm 2. Here, we adopt a strategy that does not require differentiable value function models, as reward feedback could often be provided in a black-box manner (e.g., molecular design). Specifically, we organically combine IS-based and SMC-based approximations. Given a batch of samples, we apply IS from k = K to k = 1 (Line 4-6) for each sample in the batch, where the proposal distribution is a policy from pre-trained diffusion models. However, at the terminal step k = 1, we perform selection via resampling (Line 7), which is central to SMC and evolutionary algorithms. This step involves interaction among samples in the batch, as illustrated in Figure 4. This combined strategy during reward-guided denoising leverages the advantages of both IS approaches (Li et al., 2024) and SMC approaches (Wu et al., 2024). First, if we use the pure IS strategy from k = K to k = 1, when a sample in a batch is poor, it will not be permanently discarded during the refinement process. In contrast, in Algorithm 2, the final selection step allows for the elimination of such poor samples through resampling. Second, if we use the pure SMC strategy from k = K to k = 1, resampling is performed at every time step, which significantly reduces the diversity among samples in the batch. We apply the SMC approach only at the final step. Relation to evolutionary algorithm. The above version can be viewed as a modern variant of the evolutionary algorithm, which seamlessly integrates diffusion models. An evolutionary algorithm typically consists of two steps: (a) candidate generation via mutation and crossover and (b) selection. In Algorithm 2, the step (a) corresponds to Lines 3-6, where reward-guided generation is employed, and the step corresponds to Line 7. Remark 1. When the reward feedback is differentiable, we can effectively integrate classifier guidance into the proposal distributions. For further details, see the Appendix in Li et al. (2024)."}, {"title": "5.2. Constrained Reward Optimization", "content": "We often need to include hard constraints so that generated designs fulfill certain conditions. This is especially crucial in molecular design, where we may require low-toxicity small molecules or cell-type-specific DNA sequences, as shown in Section 6.2. Here, we explore how to enable generation under such constraints. Formally, we define the constraint set as $\\mathcal{C} = {x : r_2(x) < c}$. Given another reward $r_1 (.)$ to be optimized, our objective is to produce designs with high $r_1(\\cdot)$ while ensuring $r_2(x) < c$.\nNa\u00efve approaches with single-shot algorithms. As an initial consideration, we examine how to address this problem using existing single-shot methods. A straightforward approach is to use the following reward\n$r(.) = r_1(.)\\mathbb{I}(r_2(.) < c)$\nor use a log barrier formulation:\n$r(.) = r_1(.) + log(max(c-r_2(.), c_1))$,\nwhere $c_1$ is a suitably small value, and then sample from t = T to t = 0 by following approxima soft-optimal policies. However, in reality, the outputs at t = 0 often fail to satisfy these constraints, regardless of how the rewards are defined. This shortcoming arises because the value function models used during reward-guided denoising are not completely accurate. Integration into our proposal (Algorithm 2). Now, we consider incorporating the above rewards into our framework in Algorithm 2. Here, compared to single-shot algorithms, we can often begin with feasible initial designs that satisfy the constraints $x \\in \\mathcal{C}$. Then, by keeping the noise level K in Algorithm 2 small, we can avoid deviating substantially from these feasible regions. This gradual refinement strategy makes it easier to produce designs that fulfill hard constraints."}, {"title": "6. Experiment", "content": "We aim to evaluate the performance of the proposed method (RERD) across several tasks by investigating the effectiveness of refinement procedures compared to existing single-shot guidance methods in diffusion models. We begin by introducing the baselines and metrics used in our evaluation. Subsequently, we present our results in protein and DNA design. For further details and additional results, refer to Section C. The code is available at https://github.com/masa-ue/ProDifEvo-Refinement.\nBaselines and our proposal. We compare baselines that address reward-guided generation in diffusion models with RERD. Note that we primarily focus on settings where reward feedback is provided in a black-box manner.\n*   SVDD (Li et al., 2024): A representative single-shot, derivative-free guidance method (without refinement).\n*   SMC (Wu et al., 2024): Another single-shot, representative derivative-free guidance method.\n*   GA: A na\u00efve approach for sequence design that uses pre-trained diffusion models to generate mutated designs within a standard genetic algorithm (GA) pipeline (Hie et al., 2022). To ensure a fair comparison, we allocate the same computational budget as RERD below.\n*   RERD in Algorithm 2 (Ours). We set K/T = 10% and S = 50. For initial designs, we use the results generated by SVDD in Section 6.1 and designs that satisfy the constraints in Section 6.2."}, {"title": "6.1. Protein Design", "content": "We begin by outlining our tasks. First, we use EvoDiff (Alamdari et al., 2023), a representative discrete diffusion model for protein sequences trained on the UniRef database, as our unconditional base model. Next, following existing representative works in protein design (Hie et al., 2022; Watson et al., 2023; Ingraham et al., 2023), we consider four reward functions related to structural properties, which take the generated sequence as input. For more details, refer to Section C.\n*   ss-match: We use Biotite (Kunzmann and Hamacher, 2018) to predict the secondary structure (ss). We then calculate the mean matching probability across all residues between the predicted and reference secondary structures, where the target structure is represented by a sequence consisting of a (\u03b1-helices), b (\u03b2-sheets), and c (coils). A score of 1.0 indicates perfect alignment.\n*   CRMSD: This is the constrained root mean square deviation against the reference backbone structure after structural alignment. Typically, < 2\u00c5 indicates a highly similar structure. Note that a lower value is preferred.\n*   globularity (+ pLDDT): It reflects how closely the structure resembles a globular shape. Additionally, we optimize pLDDT to improve the stability of the structure."}, {"title": "6.2. Cell-Type-Specific Regulatory DNA Design", "content": "We begin by outlining our tasks. Here, we focus on widely studied cell-type-specific regulatory DNA designs, which are crucial for cell engineering (Taskiran et al., 2024). Specifically, our goal is to design enhancers (i.e., DNA sequences that regulate gene expression) that exhibit high activity levels in certain cell lines while maintaining low activity in others. Following existing works (Lal et al., 2024; Sarkar et al., 2024; Gosai et al., 2023), we construct reward functions as follows. Using datasets from Gosai et al. (2023), which measures the enhancer activity of 700k DNA sequences (200-bp length) in human cell lines using massively parallel reporter assays (MPRAs), we trained oracles based on the Enformer architecture (Avsec et al., 2021) as rewards across three cell lines ( $r_H(.)$ in HepG2 cell line, $r_K(.)$ in K562 cell line, and $r_S(.)$ in SKNSH cell line). Then, we aim to respectively optimize the following:\n$\\tau_H(x) = r_H(x)\\mathbb{I}(r_K(x) < c)\\mathbb{I}(r_S(x) < c)$\nwhere c is a threshold. Here, optimizing $\u03c4_H$ means maximizing $\u03c4_H$ while retaining $r_K, r_S$ low. Then, similarly, we define $\u03c4_K, \u03c4_S$ by exchanging their roles. Here are several additional points to note. First, as discussed in Section 5.2, directly using $\u03c4_{H,K,S}$ in practice would lead to suboptimal performance. Therefore, we use log barrier reward functions for all methods. Additionally, for GA and RERD, we initialize the designs with samples that satisfy the constraints (e.g., $I(r_K(x) < c)I(r_S(x) < c)$). Recall that one of the advantages of our method is its ability to leverage designs from feasible regions that satisfy the constraints. Finally, we use pre-trained discrete diffusion models from Wang et al. (2024a) as the backbone unconditional diffusion models. Results. The results are presented in Table 2. Our methods consistently exhibit superior performance in terms of rewards while maintaining a relatively high likelihood. Notably, while it has been reported that SMC and SVDD excel in optimizing individual rewards (e.g., $r_H$ only) in existing works such as Li et al. (2024), we have observed that they struggle with handling additional constraints. In contrast, as shown in Figure 7, RERD effectively handles such constraints (i.e., ensuring cell-type specificity) by gradually refining the results, starting from designs in feasible regions."}, {"title": "7. Conclusion", "content": "We introduce a new framework for inference-time reward optimization in diffusion models, utilizing an iterative evolutionary refinement process. We also provide a theoretical guarantee for the framework's effectiveness and demonstrate its superior empirical performance in protein and DNA design, surpassing existing single-shot reward-guided generation algorithms. As future work, we plan to explore its application in small molecule design."}, {"title": "A. Proof of Theorem 1", "content": "Here, we use induction. Hence, we prove that $x_0^{(1)}$ follows $p^{(a)}$.\nDistribution after noising. First, we consider the distribution after noising. This is\n$\\int q_K(x_K^{(1)} | x_0^{(0)})p^{(a)}(x_0^{(0)})dx_0^{(0)}$.\nBy plugging in the first assumption regarding distributions of initial designs, it is equal to\n$\\int q_K(x_K^{(1)} | x_0^{(0)}) \\frac{p_{pre}(x_0^{(0)}) \\exp(r(x_0^{(0)})/\\alpha)}{C} dx_0^{(0)}$.\nRecalling this definition of soft value functions:\n$\\exp(v_K(\\cdot)/\\alpha) = \\mathbb{E}_{p_{pre}(x_0|x_K)} [\\exp(r(x_0)/\\alpha) | x_K]$\nand the assumption (b) ($q_0(x_0|x_K) = p_{pre}(x_0|x_K)$ and $q_k(\\cdot) = p_{pre}^k(\\cdot)$ ), the term (5) is equal to\n$\\frac{p_{pre}(x_K^{(1)}) \\exp(v_K(x_K^{(1)})/\\alpha)}{C}$.\nDistribution after reward-guided denoising. Now, we consider the distribution of $x_0^{(1)}$:\n$\\frac{1}{C} \\int (\\prod_{k=1}^K p_{pre}(x_{k-1} | x_k)) \\frac{p_{pre}(x_K) \\exp(v_K(x_K)/\\alpha)}{C} d(x_0, \\cdots, x_K)$.\nWith some simple algebra, this is equal to\n$\\frac{1}{C} \\int (\\prod_{k=1}^{K-1} p_{pre}(x_{k-1} | x_k)) \\frac{p_{pre}(x_{K-1} | x_K) \\exp(v_{K-1}(x_{K-1})/\\alpha)}{\\exp(v_K(x_K)/\\alpha)} \\times p_{pre}(x_K) \\exp(v_K(x_K)/\\alpha) d(x_0, \\cdots, x_K)$\n$\\frac{1}{C} \\int (\\prod_{k=1}^{K-1} p_{pre}(x_{k-1} | x_k)) p_{pre}^{K-1}(x_{K-1}) \\exp(v_{K-1}(x_{K-1})/\\alpha) d(x_0, \\cdots, x_{K-1})$\n$\\frac{1}{C} \\int (\\prod_{k=1}^{K-2} p_{pre}(x_{k-1} | x_k)) p_{pre}^{K-2}(x_{K-1}) \\exp(v_{K-1}(x_{K-1})/\\alpha) d(x_0, \\cdots, x_{K-1})$.\nRepeating this argument from k = K \u2212 1 to k = 0, the above is equal to\n$\\frac{p_{pre}(x_0) \\exp(r(x_0)/\\alpha)}{C}$,\nThis concludes the statement!"}, {"title": "B. Additional Details for Protein Design", "content": "In this section, we have added further details on experimental settings and results."}, {"title": "B.1. Details on Baselines", "content": "*   RERD (Algorithm 2): We have used parameters L = 20, N = 10, S = 30 in general. For the importance sampling step, we have used \u03b1 = 0.0, and for the selection step, we have used \u03b1 = 0.2.\n*   SVDD: We set the tree width L = 20, \u03b1 = 0.0.\n*   SMC: In SMC, we set \u03b1 = 0.05 because if we choose \u03b1 < 0.00, it just gives a single sample every time step. Refer to Appendix B in Li et al. (2024).\n*   GA: Here, compared to Algorithm 2, we have changed the mutation part (Line 3-7) with just sampling from pre-trained diffusing models without any reward-guided generation. To have a fair comparison with RERD, we increase the repetition number S so that the computational budget is roughly the same as our proposal."}, {"title": "B.2. Details on Reward Functions", "content": "Globularity. Globularity refers to the degree to which a protein adopts a compact and nearly spherical three-dimension structure (Pace and Hermans, 1975). It is defined based on the spatial arrangement of backbone atomic coordinates, where the variance of the distances between those coordinates and the centroid is minimized, leading to a highly compact structure. Here, we set the protein length 150. Globular proteins are characterized by their structure stability and water solubility, differing from fibrous or membrane proteins. The compact conformation helps proteins to maintain proper protein folding and reduce the risk of aggregation. Symmetry. Protein symmetry refers to the degree to which protein subunits are arranged in a repeating structure pattern (Goodsell and Olson, 2000; Lisanza et al., 2024; Hie et al., 2022). Here we focus on the rotational symmetry of a single chain, which is defined"}]}