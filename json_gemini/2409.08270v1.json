{"title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally", "authors": ["Qiuhong Shen", "Xingyi Yang", "Xinchao Wang"], "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting (3D-GS) from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50\u00d7 faster than the best existing methods. Extensive experiments demonstrate our method's efficiency and robustness in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.", "sections": [{"title": "1 Introduction", "content": "The pursuit of understanding and interacting with 3D environments represents a formidable yet pivotal challenge in computer vision. Central to this endeavor is the task of accurately perceiving and segmenting 3D structures [1,5,10,30,32,42], a task that becomes increasingly complex as we delve into more sophisticated representations of 3D scenes. Recently, 3D Gaussian Splatting (3D-GS) [17] emerges as a cutting-edge approach that promises to revolutionize how we render and reconstruct 3D spaces. By employing a multitude of colored 3D Gaussians, this method achieves a high fidelity representation of 3D scenes, offering a compelling blend of precision and visual quality that is particularly suited for complex object and scene rendering."}, {"title": "2 Related-work", "content": ""}, {"title": "2.1 3D Gaussian Splatting", "content": "3D Gaussian Splatting has emerged as an efficient method for inverse rendering, facilitating the reconstruction of 3D scenes from 2D images through learning explicit 3D Gaussian in the space [9,18]. Recent advancements have seen a series of studies [28,48,52,53] extending this approach to capture dynamic 3D environments by learning deformation fields for 3D Gaussians. Concurrently, another stream of researches [6, 24, 35, 44, 55, 58] have integrated 3D Gaussian splatting with diffusion-based models [25,38,39,50,51,57] to generate static [40,56,62] or dynamic [36] 3D objects. A notable portion of these works leverages the explicit representations provided by 3D Gaussian Splatting for rapid optimization. Our method primarily concentrates on elevating corresponding 2D masks onto reconstructed 3D Gaussian scenes. We also exploit the explicit representation of 3D Gaussian Splatting to cast the segmentation of 3D scenes as a Linear Programming optimization problem, thereby enabling instantaneous 3D segmentation."}, {"title": "2.2 3D Neural scene segmentation", "content": "Recent advancements in neural 3D scene representations, including NeRF, DVGO, and several others [1,5,8,10, 14, 23, 30, 32, 42, 49], have demonstrated exceptional capabilities in 3D scene reconstruction. Inspired by these developments, the exploration of 3D segmentation within such frameworks has emerged as a significant area of interest. Semantic-NeRF [61] initially showcased NeRF's aptitude for semantic propagation and refinement. NVOS [37] furthered this by enabling user interaction for 3D object selection via a lightweight MLP trained on specialized 3D features. Concurrently, a series of studies including N3F, DFF, LERF, and ISRF [12,19,22,46] have integrated 2D vision models to facilitate 3D segmentation by elevating 2D visual features, thereby processing embedded 3D features for segmentation. Additionally, various approaches have combined instance and semantic segmentation methods with radiance fields [2,4,11,12,15, 27, 33, 41, 47,59] to further explore this domain.\nMost closely related methods to our work are SAGA [3], Gaussian Grouping [54], and SAGS [16]. These works concentrate on associating 2D masks, as generated by the Segment Anything Model (SAM) [20], with 3D Gaussian Splatting for mask lifting. Gaussian Grouping employs video tracker to associate 2D masks, subsequently distilling these as object features for 3D Gaussians and segmenting via a classifier trained with 2D identity loss and 3D regularization loss. Similarly, SAGA distills 2D masks into 3D features, requiring additional training features for each 3D Gaussians with a pack of losses. Both of these methods are limited by their substantial training costs required to optimize Gaussian features before 3D segmentation. SAGS introduces a training-free approach by projecting the centers of 3D Gaussians onto 2D masks. This method adopts a intuitive segmentation criterion, where a projected center landing within a foreground mask gets classified as a foreground Gaussian."}, {"title": "3 Methods", "content": "In this section, we first delve into the rendering process of 3D Gaussian Splatting (3DGS), focusing on the tile-based rasterization and alpha blending. We then describe how this process lends itself to formulating the segmentation of 3DGS as an integer linear programming (ILP) optimization, which we demonstrate can be resolved in a closed form. Recognizing the typically noisy nature of 2D masks, we introduce a softened optimal assignment to mitigate these noises. Beyond binary segmentation, we extend our method to include scene segmentation, enabling the segmentation of all objects within 3D scenes. Finally, we present a method for rendering 2D masks guided by depth information, which projects the 3D segmentation results onto 2D masks from new viewpoints."}, {"title": "3.1 Preliminary: Rasterization of 3DGS", "content": "3D Gaussian Splatting [18] (3DGS) stands out\nas a great method for novel view synthesis.\nUnlike neural radiance fields [31], it recon-\nstructs a 3D scene as explicit 3D Gaussians\nwith real-time speed. Given a set of captured\nviews with paired camera poses, 3D Gaussian\nSplatting reconstruct 3D scenes by learning\n3D Gaussians {Gi}. Each 3D Gaussian Gi\nis parameterized as $G_i = {M_i, q_i, S_i, O_i, C_i}$,\nwhere $m_i \\in R^3$ is the center position, $q_i \\in R^4$\nis a quaternion representing rotations, $s_i\\in R^3$\nis the scale of three dimensions, $o_i \\in R$ is the\nlearnt opacity, and $c_i \\in R^{48}$ is the three-order\nspherical harmonics to represent view depen-\ndent color. In its rendering process, 3DGS\nadopts a rasterization pipeline to achieve su-\nper efficiency. Specifically, all 3D Gaussians\nare first projected onto the image plane as 2D\nGaussians. Then the whole image is divided into 16 \u00d7 16 tiles as illustrated in\nFig. 1, pixels in each tile B share identical 3D Gaussians subset {Gi}B \u2208 {Gi}.\nWhen rendering each pixels, the traditional alpha composition is applied to blend\nproperties xi (color or depth) of these 2D Gaussians into pixel space property\nX by their depth order:"}, {"title": "3.2 Binary Segmentation as Integer Linear Programming", "content": "We start with a reconstructed 3DGS scene, parameterized by {G}. It includes L\nrendered views with associated 2D binary masks, denoted as {Mu}. Each mask,\nM\u2208 RH\u00d7W has elements of 0 or 1, where 0 represents the background, and 1\ndenotes the foreground. Our goal is to assign a 3D label, Pi, which can be either\n0 or 1, to each 3D Gaussian Gi by projecting the 2D masks into the 3D space.\nTo do this, we optimize Pi through a differentiable rendering process. We\nassume Pi as a property xi in Eq. 1. The aim is to minimize the discrepancy\nbetween the rendered masks and the provided binary masks Mu.\nLuckily, with {Gi} reconstructed, the ai and the Ti become constant for\nrendering. As such, the rendering function becomes a linear equation with respect\nto the blending properties xi. This gives us a great flexibility to solve for the\noptimal mask assignment using simple linear optimization.\nFormally, our segmentation problem can be formulated as an integer Linear\nProgramming (LP) optimization with mean absolute error:\nMin\n{P}\n$F = \\sum_{v}|R({G}, {P}) - M^v| = \\sum_{v} \\sum_{j \\in M^v} \\sum_{k} |P_i \\alpha_i T_i \u2013 M_{jk}|$\nsubject to $P_i \\in {0,1}$.\nwhere R denotes the 3DGS renderer. Given ai > 0 and Ti < 1 for any i, and\nthe given light in alpha composition can only be absorbed, the total absorbed\nlight cannot exceed the initial light intensity, which is normalized to 1. Conse-\nquently, the sum of absorbed light fractions across all samples is bounded by\n$0 < \\sum \\alpha_i T_i \\le 1$. This leads us to introduce:\nLemma 1: In the alpha blending within 3D Gaussian Splatting, for $P_i \\in {0,1}$,\n$0 \\le P_i \\alpha_i T_i < \\sum \\alpha_i T_i \\le 1$"}, {"title": "3.3 From Binary to Scene Segmentation", "content": "Numerous instances are present across various views in 3D scenes. Segment-\ning multiple objects within these scenes requires executing binary segmentation\nmultiple times according to above formulation. This process involves repeatedly\ngathering {A0, A1}, which inherently slows down the pace of scene segmentation.\nThus, we extend our methodology from binary segmentation to encompass scene\nsegmentation to address this challenge more efficiently.\nThis transition to multi-instance segmentation is motivated by two key con-\nsiderations. Initially, it's important to recognize that 3D Gaussians do not ex-\nclusively belong to a single object. This is exemplified in Figure 1, where pixel\nu1 and u2 are influenced by the same 3D Gaussian, despite belonging to dis-\ntinct objects (color block). Furthermore, the introduction of multiple instances\ncomplicates the constraint in Equation 2 to $P_i \\in {0,1, ..., E \u2013 1}$, with E rep-\nresenting the total number of instances in a scene. Consequently, the set of\nprovided masks Mu becomes M\u00ba \u2208 {0,1,..., E \u2212 1} to accommodate multiple\nsegmented 2D instances. Such constraints prevent achieving a global optimum,\nas the labels across instances are subject to be exchangable.\nTo circumvent these challenges, we reinterpret multi-instance segmentation\nas a combination of binary segmentation, modifying the optimal assignment\nstrategy as outlined in Equation 7. To isolate a specific instance labeled e in a\n3D scene, we redefine all other objects within the instance set as the background"}, {"title": "3.4 Depth-guided Novel View Mask Rendering", "content": "Given above formulation eschews the need for dense optimization, our approach\nis capable of yielding robust segmentation results using merely approximately\n10% of the masked 2D views. This efficiency also empowers our method with\nthe capability to produce 2D masks Mu for previously unseen views. For ren-\ndering masks in novel views within the context of binary segmentation, we focus\nexclusively on rendering foreground Gaussians identified by Pi = 1. This pro-\ncess involves computing an accumulated alpha value pjk for every pixel (j, k).\nFor novel view mask rendering in binary segmentation, we merely render fore-\nground Gaussians with P\u2081 = 1 to produce accumulated alpha value pjk for each\npixel, then the 2D mask can be obtained by simple quantization on alpha values\nPjk with threshold \u03c4, which is a pre-defined hyper parameter. Specifically, 2D\nmasks are derived through straightforward quantization of these alpha values,\nwhere $M_{jk}^{v}= Q(p_{jk}, \\tau)$ and \u03c4 represents a predetermined threshold, and Q is\nthe quantization function, when $p_{jk} > \\tau$, Q = 1, conversely Q = 0.\nFor novel view mask rendering in scene segmentation, due to the intersec-\ntions among segmentation results of different objects as indicated in Equation 8,\nthe resultant alpha mask might present ambiguity, as illustrated in 3rd column\nof Fig. 3. Specifically, rendering each object's associated 3D Gaussian subset\n{Gi}e in the same viewpoint might result in several objects meeting the con-\ndition $Q(p_{jk}, \\tau) = 1$. In this case, we introduce depth to determine the final\nsegmentation results. The depth at each pixel location (j, k) is utilized to filter\nthe final 2D mask outcome. The object e satisfying $Q(p_{jk}, \\tau) = 1$ with minimal\ndepth Dik relative to the camera at a given pixel (j, k) is selected as Mjk = e.\nFigure 3 illustrates the prediction of 2D masks for both binary and multi-object\nsegmentation in novel views."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Data preparation", "content": "Dataset. To assess the efficacy of our approach, we collect 3D scenes from\nseveral sources: the MIP-360 [1] dataset, T&T [21] dataset, LLFF [29] dataset,\nInstruct-NeRF2NerF [13], and the LERF [19] dataset, serving as the basis for\nqualitative analysis. For quantitative analysis, we utilize the NVOS [37] dataset.\n2D mask generation and association. In our experimental setup, we utilize\nthe Segment Anything Models (SAM) [20] to extract masks, given that SAM's\nsegmentation output is inherently semantic-agnostic. It becomes necessary to fur-\nther associate these 2D masks within our framework. Our approach diverges into\ntwo distinct strategies, tailored respectively for binary and scene segmentation.\nFor binary segmentation, where the objective is to isolate a single foreground\nentity, we initiate by marking point prompts on a single reference view. These\npoint prompts are projected back to the 3D space with reference view camera\npose to find their nearest 3D Gaussians with the smallest positive depth. Subse-\nquently, these point prompts are propagated to other views by projecting their\ncorresponding 3D Gaussians' center. Leveraging these associated point prompts,\nSAM independently generates a binary mask for each view. For of scene segmen-\ntation, our methodology begins with employing SAM to produce instance masks\nfor individual views. To assign each 2D object with a unique ID in the 3D scene,\nmultiple views are treated akin to a video sequence. Utilizing a zero-shot video\ntracker [7,54], we ensure the consistent association and propagation of objects\nacross viewpoints."}, {"title": "4.2 Implementation details", "content": "We implement the optimal 3D segmentation both in Eq. 7 and Eq. 8 within\nCUDA kernel functions. The computation of A\u00bf for each Gaussian employs tile-"}, {"title": "4.3 3D segmentation results", "content": "In Fig.4, we exhibit the results of both binary and scene 3D segmentation. The\nfirst row presents the Figurines scene from the LERF dataset [19], and the second\nrow features the Counter scene from the MIP-360 dataset [1]. On both scenes,\nwe apply our scene segmentation approach, rendering 2 views for 5 segmented\nobjects (circled in the ground-truth images) for each scene, demonstrating our\nmethod's capability in conducting scene segmentation with instance masks pre-\ndicted by SAM [20]. Additionally, binary segmentation is showcased in rows 3, 4\nand 5, with row 3 illustrating the Horns scene from the LLFF dataset [29], row\n4 displaying the Truck scene from the T&T dataset [21], row 5 displaying the\nKitchen scene from the MIP-360 dataset [1]. Two views of the segmented objects\nare rendered, showing our approach's capability in segmenting 3D objects."}, {"title": "4.4 Object Removal", "content": "3D object removal involves entirely eliminating the 3D Gaussians subset of an\nobject from the scene. Results of such removals are illustrated in Figure 4. It's\nimportant to note that we apply a background bias y = -0.4 across all scenes\nto ensure the background remains clear. For small 3D objects in row 1 and row\n2, we simultaneously remove 5 objects from the scene. As the background of\nthese two scenes is likely to be observed in other views, the artifacts in these two\nscenes are minor even imperceptible for certain objects. For more challenging\nscenes depicted in rows 3, 4, and 5, the space vacated by the removed objects\nreveals a noisier background or even black holes, primarily due to the obstruction\nof background by larger foreground objects. This situation is pronounced in the\nHorns scene (row 3), which comprises only facing-forward views."}, {"title": "4.5 Object Inpainting", "content": "Following 3D object removal, object inpainting [34] aims to correct unobserved\nregion artifacts, ensuring view consistency within the 3D scene. Initially, we ren-\nder views post-removal and employ Grounding-DINO [26] to identify artifact"}, {"title": "4.6 Quantitative comparison", "content": "We perform a quantitative analysis using the NVOS dataset [37], derived from\nthe LLFF dataset [29]. The NVOS dataset comprises 8 facing-forward 3D scenes\nfrom LLFF, providing a reference and a target view with 2D segmentation mask"}, {"title": "5 Conclusion", "content": "In this work, we introduce an optimal solver for 3D Gaussian Splatting segmenta-\ntion from 2D masks, significantly advancing the accuracy and efficiency of lifting\n2D segmentation into 3D space. By breaking the alpha composition in 3D-GS\ninto overall contributions of each Gaussians, this solver only requires single step\noptimization to get the optimal assignment. It not only expedit the optimization\nprocess by approximately 50 times faster compared to previous methods but also\nenhanced robustness against noise with a simple background bias. Further, this\napproach is extended to scene segmentation and capable of rendering masks on\nnovel views. Extensive experiments demonstrate superior performance in scene\nsegmentation tasks, including object removal and inpainting. We hope this work\nwill facilitate 3D scene understanding and manipulating in the future."}, {"title": "6 More Implementation details", "content": "The implementation of FlashSplat unfolds in two main parts. Initially, the focus\nis on deriving the contribution set {A} for every Gaussian across objects e.\nThis step involves projecting 3D Gaussians onto each mask Mv, capturing the\nproduct aiTi within the alpha blending formula into the buffer Ae where a pixel\nM = e. This procedure compiles the contributions from every object across all\nviewpoints into a matrix A \u2208 RE\u00d7|{G}|, where E is total number of objects in\nthe 3D scene, and |{Gi}| represents the total number of 3D Gaussians. Following\nthis, we allocate labels Pi to each 3D Gaussian Gi based on this contribution\nmatrix, as delineated in equations Eq. 7 and Eq. 8. For binary segmentation, the\nassignment process simplifies to an argmax operation for optimal assignment.\nScene segmentation is resolved through dynamic programming to manage the\ncomplexity of multiple assignments, with the specific implementation details\nprovided in list 1.1. The segmentation results for scenes are represented within\na matrix S \u2208 REX|{Gi}\\, where each entry Sm,n \u2208 {0,1} specifies whether the\nn-th Gaussian belongs to object m."}, {"title": "6.1 Mask association details", "content": "Our work primarily concentrates on lifting 2D masks into 3D space, with less\nemphasis placed on mask association within the core sections of main paper.\nIn this context, we provide additional insights into the methodology used to\nassociate 2D masks in the 3D segmentation experiments presented.\n2D\nBinary Mask Association. Within binary segmentation scenarios, association\namong 2D view masks is achieved through the propagation of point prompts\nacross different views. Specifically, for a point prompt p?D \u2208 R\u00b2 identified on an\nobject in a single view, this point is back-projected to the 3D space, acquiring\na world coordinate p3D \u2208 R\u00b3, to locate its corresponding 3D Gaussian Gi. How-\never, due to the prevalence of numerous Gaussians surrounding this 3D point\nprompt, relying solely on distance for Gaussian correspondence can lead to in-\ncorrect outcomes. To mitigate this, we initially identify the Top \u2013 10 closest 3D\nGaussian centers using the L2 distance. Subsequently, the specific Gaussian is\ndetermined by selecting the one with the least depth when projected onto the\nreference view. The center positions of these 3D Gaussians are then projected\nonto other views to link point prompts associated with the same object across\ndifferent views. By utilizing SAM [20] to produce masks for each view based on\nthese aligned point prompts, we inherently associate these predicted 2D masks.\nThis method of point prompt propagation is implemented via CUDA kernels,\nenabling the association of point prompts across all views in under one second."}, {"title": "7 More qualitative evaluation", "content": "To validate the effectiveness of our FlashSplat, we conduct qualitative compar-\nisons on the object removal task with prior works in 3D Gaussian Splatting\nsegmentation, specifically Gaussian-Grouping [54]. As outlined in Sec.4.4, 3D\nobject removal involves entirely eliminating the 3D Gaussians subset of selected\nobjects from the scene, which is a fundamental application of 3D segmentation.\nFor fair comparison, we use identical 2D scene mask set {M} for both methods.\nOur process begins with conducting 3D segmentation using these scene masks,\nfollowed by the specification of object IDs for removal. We present the multiple\nobject removal results in Fig. 8 and single object removal results in Fig. 9. We\nrender 4 distinct views of the removal results, showing that our FlashSplat can\ncleanly remove these 3D objects with imperceptible artifacts, while the results\nof Gaussian-grouping show severe artifacts near the removed 3D objects. These\ncomparisons underscore our method is not only superior for the efficiency of 3D\nsegmentation, but also excels at 3D scene segmentation quality."}, {"title": "8 More discussions", "content": ""}, {"title": "8.1 The effect of background bias \u03b3", "content": "Table 3 presents our ablation study on the truck scenes from the T&T dataset [21].\nWe annotate 5 views 2D mask as target views, and other view masks predicted\nby SAM [20] are used as reference view masks. With the background bias y rang-\ning from [-1,1], we get the 3D segmentation of the truck and then render it to\n2D masks to compute the mean IoU. Among these y values, a setting of y = 0.4\nproduces the optimal mean IoU of 94.2%. This is caused by the noise in masks\npredicted by SAM (as visualized in Fig. 6), the assignment with y = 0 is prone to\ntake background Gaussians as foreground, while this softened refinement helps\nto reduce such noises."}, {"title": "8.2 Quantization in novel view mask", "content": "In Sec. 3.4, we outline projecting masks from 3D segmentation results onto novel\nviews using simple quantization and depth-guidance. Here we take mask ren-\ndering in binary segmentation as an example to claim why this quantization is"}, {"title": "8.3 Scene segmentation extension", "content": "In Sec. 3.3, we extend our optimal assignment for binary segmentation to scene\nsegmentation. This formulation, shown in Eq. 8, is chosen over a straightfor-\nward approach that would simply perform arg max among the E instances. This\nchoice is driven by the non-exclusive nature of Gaussian Splatting, where a Gaus-\nsian can be shared between objects. For instance, we quantitatively analyze the\nCounter scene in the MIP360 [1] dataset under different numbers of given masks.\nAs illustrated in Fig. 11, approximately 20% of the Gaussians in this scene are\nshared between more than two objects. This phenomenon occurs because, in\nthe 3D reconstruction of 3DGS, supervision is limited to view space, with no\nadditional geometric or semantic constraints to enforce mutual exclusivity."}, {"title": "9 Limitations", "content": "Despite the advancements presented by our method in 3D-GS segmentation,\nwe acknowledge several limitations for future exploration. The linear program-\nming approach, although effective, may encounter scalability challenges with"}]}