{"title": "A Training-Free Length Extrapolation Approach for LLMs:\nGreedy Attention Logit Interpolation (GALI)", "authors": ["Yan Li", "Tianyi Zhang", "Zechuan Li", "Soyeon Caren Han"], "abstract": "Transformer-based Large Language Models\n(LLMs) struggle to process inputs exceeding\ntheir training context window, with performance\ndegrading due to positional out-of-distribution\n(O.O.D.) that disrupt attention computations. Ex-\nisting solutions, fine-tuning and training-free\nmethods, are limited by computational ineffi-\nciency, attention logit outliers or loss of local po-\nsitional information. To address this, we propose\nGreedy Attention Logit Interpolation (GALI), a\ntraining-free length extrapolation method that\nmaximizes the utilization of pretrained positional\nintervals while avoiding attention logit outliers\nthrough attention logit interpolation. The re-\nsult demonstrates that GALI consistently outper-\nforms state-of-the-art training-free methods. Our\nfindings reveal that LLMs interpret positional\nintervals unevenly within their training context\nwindow, suggesting that extrapolating within a\nsmaller positional interval range yields superior\nresults-even for short-context tasks. GALI rep-\nresents a significant step toward resolving the\npositional O.O.D. challenge, enabling more re-\nliable long-text understanding in LLMs. Our\nimplementation of GALI, along with the experi-\nments from our paper, is open-sourced at https:\n//github.com/AcademyCityL/GALI.", "sections": [{"title": "1. Introduction", "content": "Transformer-based Large Language Models (LLMs) have\nbecome indispensable for a wide range of natural language\nprocessing tasks, yet their performance is fundamentally\nconstrained by the training context window, i.e., the maxi-"}, {"title": "2. Background", "content": "Rotary Position Embedding (RoPE): ROPE (Su et al.,\n2024) is a technique that encodes positional information\nby applying rotary transformations to token embeddings,\nenabling relative position modeling in transformers. Given\ntwo token embeddings xm,xn \u2208 R\u00b9 as query and key\ncorresponding to position m and n, the projection matrix\nWQ,WK \u2208 Rdxl, RoPE applies a rotation to the pro-\njected token embeddings, i.e., $q_m = (WQx_m)e^{i m \\Theta}$, $k_n =$\n$(WKx_n)e^{i n \\Theta}$, where $\\Theta = [\\theta^0, \\theta^{-2/d},...,\\theta^{-2(j-1)/d}]$, $j \\epsilon$\n$[1,2,..., d/2]$ and b is originally set to 10000. After that,\nthe inner product between the query qm and key kn can be\nrepresented by the real part of $q_m^*k_n$, i.e.:\n$<q_m, k_n> = Re(((WQx_m)e^{i m \\Theta},(WKx_n)e^{i n \\Theta})^c)$\n$\\lceil{d/2-1}$\n$= Re \\Big( \\sum_{j=0} (WQx_m)[2j:2j+1] (WKx_n)[2j:2j+1]e^{i(m-n)\\Theta} \\Big)$;\n= g(xm, xn, m \u2212 n)                               (1)\n\\(g()\\) is the function mapping token embeddings xm, xn to\nthe attention logit, which depends on their relative distance\nand is irrelevant to their absolute positions. Additionally,\nROPE exhibits a long-term decay as relative distance in-\ncreases (Su et al., 2024), as illustrated in Figure 1. Our\nproposed method, GALI, leverages two key properties of\nROPE to achieve position interpolation and length extrapo-\nlation effectively.\nPositional Out-Of-Distribution (O.O.D.): In Transformer\narchitectures, the self-attention mechanism is inherently\nposition-agnostic, necessitating the use of position embed-\ndings to encode positional information for processing or-\ndered inputs (Dufter et al., 2021; Kazemnejad et al., 2024).\nEven in large language models (LLMs) with causal attention,\nexplicit positional encoding through position embeddings\nremains the standard approach.\u00b9 During inference, when"}, {"title": "3. Method", "content": "This section presents Greedy Attention Logit Interpolation\n(GALI), a novel training-free position interpolation method\ndesigned to enhance the utilization of pretrained positional\ninformation while ensuring stable attention computations.\nGALI is guided by two objectives: (1) Maximizing the\nuse of pretrained positional information, i.e., the positional\nintervals encountered during pre-training. To do so, we per-\nform minimal-impact position interpolation for each chunk\nin the prefill phase and newly generated tokens during the\ndecoding phase. (2) Preventing outliers in attention logits\nby avoiding direct computation of position embeddings for\ninterpolated positions. We apply local linear interpolation\nto the attention logits based on interpolated position IDs\nand simulate RoPE's oscillatory behavior by introducing\nGaussian noise. The process is shown in Figure 2."}, {"title": "3.1. Position ID Interpolation", "content": "The proposed GALI introduces a strategy for position ID\ninterpolation to handle inputs exceeding the maximum train-\ning length, aiming to fully utilize pretrained relative posi-\ntional intervals. During the prefill phase, GALI segments\nthe input portion beyond the maximum training length into\nmultiple chunks and performs position ID interpolation for\neach chunk individually. In contrast, the portion within the\nmaximum training length remains unchanged, without any\ninterpolation. This approach provides several advantages.\nFirst, if the input length does not exceed the maximum\ntraining length, the prefill phase remains entirely unaffected,\npreserving the model's original capabilities. Second, when\nthe input length surpasses the maximum training length,\nthe text within the training range can fully leverage the\npretrained relative positional intervals to generate hidden\nstates based on the model's original performance. Finally,\nfor inputs beyond the training length, GALI can apply posi-\ntion ID interpolation in a manner that minimizes the impact\non model quality for each chunk. Specifically, each chunk\nonly adds the exact number of new position IDs it requires.\nThis ensures the interpolation process maintains a balance\nbetween computational efficiency and fidelity.\nThis approach draws\ninspiration from Dyn-\nNTK(LocalLLaMA, 2023a), which adjusts scaling\nfactors to handle inputs of varying lengths. However,\ndynamic NTK applies a uniform interpolation across the\nentire input during the prefill phase, potentially overlooking\nthe varying interpolation requirements of individual\ntokens. For instance, a token just beyond the training\ncontext window only requires the interpolation of one new\nposition ID, whereas subsequent tokens would require\nprogressively more. Ideally, the interpolation for each token\nwould be customized to minimize its impact on quality.\nHowever, performing interpolation at the token level is\ncomputationally inefficient. To address these challenges,\nGALI segments the input exceeding the training length into\nchunks and applies interpolation only for the position IDs\nthat extend beyond the training range within each chunk.\nBy tailoring interpolation to the specific needs of each\nchunk, GALI avoids the uniform scaling factors used in\nprevious methods, such as NTK, Dyn-NTK, YaRN, or SE,\nwhich can lead to suboptimal results. This chunk-wise\nstrategy ensures a more adaptive and efficient interpolation\nprocess while maintaining model performance.\nBuilding on insights from prior studies, GALI retains a local\nwindow of size Lw around the current token to preserve the\noriginal positional intervals within this range. This design\nallows the current token to effectively interpret its immediate\ncontext using the model's pretrained capabilities. During\nthe decode phase, where tokens are generated sequentially,\nGALI dynamically interpolates the required position IDs"}, {"title": "3.2. Attention Logit Interpolation", "content": "When calculating attention scores, we approximate atten-\ntion logits for interpolated positions using local linear in-\nterpolation based on interpolated position IDs, while in-\ntroducing Gaussian noise that scales with relative position\nintervals. Unlike NTK or YaRN, our methods avoid outlier\nattention logits from the position embeddings (Chen et al.,\n2023b). RoPE's trigonometric functions, while effective\nwithin pretrained relative position intervals, can produce\nextreme values during interpolation, leading to abnormal\nattention logits. To mitigate this, we bypass position embed-\ndings and directly approximate attention logits. Building on\nthe monotonic trends and oscillatory behavior observed in\nROPE, as shown in Figure 1, we hypothesize that for two\ntokens with an interpolated relative distance (e.g., 7.r), their\nattention logits-excluding oscillatory effects-should lie\nbetween g(7) and g(8), with their exact value determined by\nr. To achieve this, we compute attention logits for each QK\npair with interpolated relative position intervals using local\nlinear interpolation and simulate oscillatory behavior by\nadding Gaussian noise, whose variance increases with rela-\ntive position intervals. This method eliminates the outliers\nidentified in the PI study and produces interpolated attention\nlogits for new relative position intervals in a training-free\nand effective manner."}, {"title": "4. Experiments", "content": "We evaluate GALI on Llama3-8B-ins models across two\ntask categories: real-world long-context tasks and long-\ncontext language modeling tasks. For comparison, we\nimplement all published training-free length extrapola-\ntion methods, including NTK(LocalLLaMA, 2023b), Dyn-\nNTK(LocalLLaMA, 2023a), YARN(Peng et al., 2023), Self-\nExtend(Jin et al., 2024), and ChunkLlama(An et al., 2024b).\nThe following sections describe the experimental setup for\neach task. Appendix B details data statistics."}, {"title": "4.1. Emperiments Setup", "content": "Real-world long-context task: We evaluate GALI on\ntwo widely used long-context benchmarks, LongBench(Bai\net al., 2024) and L-Eval(An et al., 2024a). LongBench is\na bilingual long-context understanding benchmark with 21\ndatasets crossing 6 tasks, including Single-Document QA,\nMulti-Document QA, Summarization, Few-shot Learning,\nSynthetic Task, and Code Completion. We use 16 English\ndatasets from LongBench. L-Eval is a comprehensive long-\ncontext evaluation suite with 20 sub-tasks, 508 long docu-\nments, and over 2000 human-labeled query-response pairs.\nIt consists of closed-ended and open-ended task groups.\nWe focus on closed-ended groups, which assess reasoning\nand understanding over a long-context. For consistency,\nwe follow the official task prompt templates and truncation\nstrategies from the respective benchmarks.\nLong-context language modeling task: To evaluate\nGALI's long-context language modeling capabilities, we\nuse PG19(Rae et al., 2019), an open-vocabulary language\nmodeling benchmark derived from Project Gutenberg. PG19\nconsists of over 28,000 books published before 1919, cover-\ning diverse genres and writing styles. We use its test split,"}, {"title": "4.2. Real-World long-context Task Results", "content": "The LongBench results (Table 1) highlight GALI's strong\naverage performance on the Llama3-8b-ins backbone series,\nsurpassing both the 4k and 8k backbone models and other\nmethods. GALI excels in question answering (QA) and\nfew-shot learning tasks but showed weaker performance in\nsummarization, synthetic, and code-related tasks. While\nextending the context window to 32k improves performance\non Llama3-8k compared to 16k, it remains less effective\nthan using 16k on Llama3-4k. This consistent trend in Fig-\nure 3(a) highlights limitations in extrapolation performance,\nwhich we analyze below.\nFirstly, LLMs interpret positional intervals differently\nwithin their training context window, as noted in (Hsieh\net al., 2024). This explains why using a 16k context win-\ndow on Llama3-4k outperformed using the same window on\nLlama3-8k. Since LLMs are trained via next-token predic-\ntion, smaller positional intervals are more trained and thus\nbetter understood. The difference between models lies in\nthe range of positional intervals they can fully comprehend\nbased on their training context window. As a result, all\nmethods except ChunkLlama perform better with a 16k con-\ntext window on the Llama3-4k than with the same context\nwindow on the Llama3-8k. Secondly, despite all methods\naiming for training-free length extrapolation, they employ\nfundamentally different approaches. GALI, NTK, Dyn-\nNTK, and YaRN focus on generalizing positional intervals,\ncreating smaller but more usable intervals from pretrained\nones. Among these, GALI generates only the minimal num-\nber of new intervals required, maximizing the reliance on"}, {"title": "4.3. Long Language Modeling Task Results", "content": "The language modeling results are shown in Table 3. Due\nto OOM, we cannot get ChunkLlama's PPL results when\nsetting the maximum position embedding to 32768. Except\nfor NTK and DYN-NTK, all methods maintained a stable\nPPL without exploding. While low PPL does not guarantee\nbetter real-world task performance, an exploding PPL is a\nclear indicator of performance degradation in downstream\ntasks. Notably, GALI achieved the second-lowest PPL,\ndemonstrating superior stability in length extrapolation. We\ntested PPL using a 16k contest window with Llama2-4k\nbackbone. Please refer to the Appendix D.2."}, {"title": "4.4. Attention Distribution Analysis", "content": "As discussed in Section 4.2, the effectiveness of training-\nfree length extrapolation methods is influenced not only by\nthe algorithm itself but also by the model's understanding\nof positional intervals within its training context window.\nWe propose a new comparison approach to fairly compare\ndifferent length extrapolation methods while eliminating\nthe positional interval understanding bias inherent to the\nmodel. Instead of applying extrapolation methods across a\nfull context window, we first restrict them to a smaller po-\nsitional interval range. We then extend this range to match\nthe model's training context window and compare the re-\nsulting attention distribution against the model's original\nattention distribution. In this scenario, the closer the ex-\ntrapolated attention distribution is to the model's original\nattention distribution, the more effectively the extrapola-\ntion method performs within a smaller positional interval\nrange, indicating that it aligns more closely with the model's\ninherent understanding. Consequently, as the model's un-\nderstanding improves, the performance of the length ex-\ntrapolation method will also improve. More concretely, we\napplied these training-free length extrapolation methods to\nLlama3-8b-ins-2k (Llama3-2k) and Llama3-4k backbones,\nand compared the resulting attention score distribution with\nLlama3-8k. The results, shown in Figure 4(a), indicate that\nGALI consistently demonstrates a significantly smaller gap\ncompared to other methods, whether extrapolating from 2k\npositional intervals or 4k positional intervals to 8k. Remark-\nably, when using 2k positional intervals, GALI outperforms"}, {"title": "4.5. Ablation Studies", "content": "In this section, we investigate the impact of local window\nsize and chunk size on GALI. We conducted our experi-\nments using NarrativeQA, the longest dataset in LongBench.\nThe results are shown in Figure 5. First, we observe that\nthe differences across the three local window sizes are mini-\nmal, indicating that attention logit interpolation effectively\napproximates the true attention score distribution.\nSecondly, as the chunk size increases, we hypothesize that\nthe observed effects result from the interplay of two factors.\nInitially, a smaller chunk size aligns better with GALI's\ndesign, which prioritizes leveraging pretrained positional\nintervals as much as possible while minimizing the number\nof interpolations for each token. Consequently, when the\nchunk size increases, the number of pretrained positional\nintervals utilized by each token decreases, while the num-\nber of interpolated positional intervals increases, leading to\nperformance degradation.\nHowever, as the chunk size grows, more tokens have their\npositional intervals compressed into a smaller range. As\nanalyzed earlier, performing denser interpolations within a\nsmall positional interval range, such as [0, 4096), yields\nbetter results than performing sparser interpolations over a\nlarger positional interval range, such as [0, 8192). Therefore,\nthe performance of GALI begins to improve as the chunk\nsize further increases."}, {"title": "5. Conclusion", "content": "In this paper, we introduced Greedy Attention Logit Inter-\npolation (GALI), a novel training-free length extrapolation\nmethod, and evaluated it against other approaches across\nwidely used real-world long-context tasks, long-context lan-"}, {"title": "6. Impact Statement", "content": "GALI is a training-free length extrapolation method that en-\nables LLMs to handle long-context tasks without requiring\nfine-tuning. It is compatible with various position embed-\nding schemes, making it a flexible plug-and-play solution.\nFurthermore, we show that, in practical scenarios, interpo-\nlating within a smaller context window than the training\ncontext window can enhance performance across both long-\ncontext and short-context downstream tasks."}, {"title": "A. Pseudo code of GALI", "content": "In this section, we provide the pseudo-code for the key steps required to implement GALI. Algorithm 1 generates the\nchunk sizes needed to partition the input during the prefill phase. While this function can be modified to support dynamic\nchunk sizes, we use fixed chunk sizes in our experiments to better control memory usage. Algorithm 2 interpolates new\nposition IDs based on the minimum number of new IDs required for each chunk. Algorithm ?? demonstrates how we\nperform attention logit interpolation. Note that we use r = [m] n to represent the interval between qm and kn. This\nis because, when computing attention logits using RoPE, we cannot directly manipulate the relative positional interval\nmatrix; instead, we modify the relative positional interval matrix by separately operating on query-states and key-states.\nBy using r = [m] n, we ensure that [r] = [m] - [n] and [r] = [m] - [n], enabling modifications to the relative\npositional interval matrix while preserving the relative order between query-states and key_states. It is important to note\nthat some operations, such as reshaping, which do not affect the core concept, are omitted from the pseudo-code in these\nthree algorithms."}, {"title": "B. Data stastics", "content": "In this section, we provide detailed information about each dataset used in LongBench and L-Eval. Table 4 presents the\nword length, task type, and number of samples for each dataset. Figure 6(a) and 6(b) show the length distributions of each\ndataset using the Llama2 and Llama3 tokenizers, respectively."}, {"title": "C. Implementation details", "content": "In this section, we provide detailed implementation information for each method. For Dyn-NTK and YaRN, we utilize\nthe implementations available in Huggingface\u00b3 by adding rope_scaling = {\u201crope_type\u201d:\u201ddynamic\u201d} and rope_scaling =\n{\u201crope_type\u201d:\u201dyarn\u201d}, respectively, to the LLM\u2019s config.json file. For NTK, we implement it by adding rope_scaling =\n{\u201crope_type\u201d:\u201ddynamic\u201d} and static_ntk=True, and modifying the dynamic_frequency_update function of the LlamaRo-\ntaryEmbedding class as follows:"}, {"title": "D. Extra experiment results", "content": "D.1. Real-world long-context task results\nWe conducted experiments on LongBench and L-Eval using the Llama2-4k backbone, as shown in Tables 6 and 7. On\nLongBench, GALI performed similarly to NTK, Dyn-NTK, and YaRN, but was weaker than SelfExtend and ChunkLlama.\nHowever, all methods performed significantly worse than those using the Llama3-8b-ins-4k backbone.\nAlthough Llama2-7b-chat and Llama3-8b-ins have similar parameter scales, Llama3 demonstrates a deeper understanding\nof pretrained positional intervals closer to its training context window. Consequently, GALI performed significantly better\non Llama3-8b-ins-4k than on Llama2-4k, with similar trends observed across other methods. As the quality of the pretrained\nmodel improves, it better aligns with GALI\u2019s principle of maximizing the use of pretrained positional intervals.\nRegarding the best-performing method on Llama2-4k, SelfExtend has been reported to be highly sensitive to hyperparameters\n(Jin et al., 2024). Specifically, larger group sizes and smaller local windows sometimes yield better results, which supports\nour conclusion in Section 4.2. These configurations emphasize the use of smaller positional intervals, reducing reliance on\nlarger ones and preventing content from being placed in less well-understood positional intervals. This limitation affects\nGALI\u2019s effectiveness on Llama2, as GALI assumes the model fully understands its entire training context window, thereby\nalways maximizing the use of pretrained positional intervals.\nOn the L-Eval benchmark, the performance gap between GALI and the best approaches was smaller than on LongBench.\nThis is because, when using the Llama2 tokenizer, datasets such as Coursera, GSM, QUALITY, and TOEFL in L-Eval are\nmuch shorter than 16k, allowing all methods to leverage Llama2-4k\u2019s well-understood smaller relative positional intervals.\nIn longer datasets like SFictions and CodeU, performance is task-dependent. SFictions is a True/False task with higher\nresults, while CodeU is a code inference task with much lower results. We also report complete results using the Llama3-8k\nbackbone. Our method performed almost identically to the backbone model, as the token lengths of GSM, QUALITY, and\nTOEFL are all below 8192. However, SelfExtend, NTK, and YaRN outperformed the backbone model, further validating\nour conclusion that even on short text datasets, using a smaller range of positional intervals for length extrapolation leads to\nbetter task performance."}]}