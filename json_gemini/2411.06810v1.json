{"title": "JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset", "authors": ["Daria Tsereh", "Mark Mirgaleev", "Ivan Molodetskikh", "Roman Kazantsev", "Dmitriy Vatolin"], "abstract": "Learning-based image compression methods have improved in recent years and started to outperform traditional codecs. However, neural-network approaches can unexpectedly introduce visual artifacts in some images. We therefore propose methods to separately detect three types of artifacts (texture and boundary degradation, color change, and text corruption), to localize the affected regions, and to quantify the artifact strength. We consider only those regions that exhibit distortion due solely to the neural compression but that a traditional codec recovers successfully at a comparable bitrate. We employed our methods to collect artifacts for the JPEG AI verification model with respect to HM-18.0, the H.265 reference software. We processed about 350,000 unique images from the Open Images dataset using different compression-quality parameters; the result is a dataset of 46,440 artifacts validated through crowd-sourced subjective assessment. Our proposed dataset and methods are valuable for testing neural-network-based image codecs, identifying bugs in these codecs, and enhancing their performance. We make source code of the methods and the dataset publicly available.", "sections": [{"title": "1 Introduction", "content": "In 2017, Ball\u00e9 published work that became fundamental to developing end-to-end learning-based methods for image compression [4]. Its main ideas were Gaussianization of image densities using generalized divisive normalization, a proof of equivalence for the rate-distortion problem and variational autoencoder, and differentiable approximation of quantization for end-to-end optimization. Since then, researchers have applied deep learning to image compression and demonstrated an advantage over traditional methods. More than 500 papers on neural image compression were published in 2023 alone [9, 10, 24].\nNeural compression's potential has drawn attention from both the academy and industry. This interest led to the 2022 initiation of the JPEG AI standardization effort, which seeks to develop the first neural-based image compression standard [3]. The JPEG AI verification model has already demonstrated more than a 10% BD-rate (PSNR) improvement relative the classic VVC intra codec [15]."}, {"title": "2 Related Work", "content": "Recent efforts have sought to develop the JPEG AI standard [3]. Their results have emphasized the image-compression potential of neural methods, which have become central to further research and development in this area. But JPEG, accounting for more than 800 published papers on neural-based image compression, is not the only group researching this topic. Recent works [11,17,24] even outperform the VVC standard [8], a top classical image- and video-compression standard, both in terms of PSNR and MS-SSIM [31]. Works that propose improvements to the JPEG AI standard [14,15,25] are also appearing."}, {"title": "2.1 Image-Quality-Assessment Methods", "content": "Thus far, there had been little research on algorithms that specifically target quality assessment of images compressed by neural codecs."}, {"title": "2.2 Artifact Detection Methods", "content": "Several studies consider algorithms for evaluating classical-compression artifacts, such as blockiness [19] and ringing [7,22]. Moreover, they provide no details about artifact location.\nLearning-based image-super-resolution methods can also produce images with neural artifacts. Hence, some recent works introduce methods to suppress super-resolution artifacts. For example, Xie et al. [32] propose the DeSRA method for artifact localization and removal. The authors find artifacts by using a reference image from another super-resolution model that is prone to neural artifacts.\nLiang et al. [21] introduce a method to patch super-resolution models against texture artifacts. This method computes an artifact map that is then used in the regularization and stabilization during the training process. The authors do not consider other artifact types."}, {"title": "2.3 Datasets", "content": "Several image datasets are designed for reproducing artifacts that typically occur with classical compression. For example, LIU4K [23] includes a wide range of images with varying complexity evaluated by no-reference metrics such as BPP, NIQE, BRISQE, and ENIQA. Developers use these images to reproduce classical compression artifacts such as blurring, blockiness, and ringing. Unfortunately, this dataset has limited applicability to our task, as it lacks a detailed partitioning of the artifacts, and is unsuitable for evaluating neural compression methods, whose distortions differ from those of classical alternatives.\nRylov et al. [26] recently created a dataset for benchmarking learned-based and traditional image codecs. However, the dataset does not specifically consider images that produce neural compression artifacts.\nOur proposed neural-artifact dataset contains information about the type, location, and confidence of the artifact in the partitioning, making it more valuable for research in neural-compression-quality assessment."}, {"title": "3 Proposed Methods", "content": "Our attention focuses on artifacts specific to learning-based codecs. To differentiate them from other artifacts that occur during compression, we additionally compressed the source image using a traditional codec and employ the result as a reference image. The idea is to find prominent differences between JPEG AI and traditional-codec results.\nFor our analysis and dataset, we chose intra-frame coding from the HM-18.0 codec [13] as a representative traditional image-compression algorithm that is both widely used and fast enough for batch processing. The target bitrates approximate the JPEG AI's compression rate. Section 4.1 describes this process in more detail. For visual comparison we show results from VTM-20.0 [8] with the bitrate chosen to approximate that of JPEG AI.\nAll our proposed methods for detecting artifacts take three images as input: an original image $I_{orig}$, the image compressed by the neural codec $I_{neural}$ (in our case, JPEG AI), and the image compressed by the traditional codec $I_{trad}$. The output is a bounding box of the artifact and a confidence value."}, {"title": "3.1 Texture-Artifact Detection", "content": "To focus on the image's textured regions, we compute a mask based on the pixel-wise spatial-information (SI) metric [12]:\n$M_{x,y} = \\begin{cases}\n0, & \\text{SI}(I_{orig})_{x,y} < 0.05, \\\\\n1, & \\text{otherwise}.\n\\end{cases}$\nWe locate artifacts in the compressed images by computing MS-SSIM [31] pixel-wise over the Y channel of the YUV color space. We found that MS-SSIM is more accurate compared to other full-reference metrics for this task.\n$H_{neural} = \\text{MS-SSIM}(I_{orig}, I_{neural}),$\n$H_{trad} = \\text{MS-SSIM}(I_{orig}, I_{trad}).$\nNext, we subtract these maps and mask them to find textured regions where the traditional codec outperforms JPEG AI. We also apply average pooling of size 128 to smooth out the outliers:\n$\\Delta H = \\text{AvgPool}_{128}((H_{trad} - H_{neural}) \\odot M)$.\nThe coordinates of the highest value in the resulting map indicate the center point of the artifact, while the value itself is the method's confidence:\n$\\text{Center} = \\underset{x,y}{\\text{argmax}} (\\Delta H),$\n$\\text{Confidence} = \\underset{x,y}{\\text{max}} (\\Delta H)$.\nIn case of multiple centers with a global maximum confidence value, we choose the first occurrence."}, {"title": "3.2 Boundary-Texture-Distortion Detection", "content": "Our method for detecting boundary-texture distortions identifies image regions where boundary reconstruction in the neural-compressed image is worse than that in the traditionally compressed one.\nFirst, we locate boundaries in the original image using the Canny [6] detector.\n$B_{x,y} = \\begin{cases}\n1, & \\text{if pixel }(x, y) \\text{ belongs to a boundary}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nThe next step is to compute gradients for the Y channel of all three input images-denoted by $G_{orig}$, $G_{neural}$, and $G_{trad}$ using the Sobel operator. We compute pixel-wise cosine distances between the gradients as follows:\n$H_{neural} = \\cos \\angle(G_{orig}, G_{neural}),$\n$H_{trad} = \\cos \\angle(G_{orig}, G_{trad}).$\nWe take the difference of these maps, mask it with the boundary map, and apply average pooling with size 32 and step 16:\n$\\Delta H = \\text{AvgPool}_{32}((H_{trad} - H_{neural}) \\odot B)$.\nSimilarly to the previous method, we locate the highest value in the resulting map and use it as the center point of the artifact, while the value itself is the method's confidence:\n$\\text{Center} = \\underset{x,y}{\\text{argmax}} (\\Delta H),$\n$\\text{Confidence} = \\underset{x,y}{\\text{max}} (\\Delta H)$."}, {"title": "3.3 Large-Color-Distortion Detection", "content": "To find large color distortion regions, we measure color differences with the widely used CIEDE2000 [27] metric:\n$H_{neural} = \\text{CIEDE2000}(I_{orig}, I_{neural}),$\n$H_{trad} = \\text{CIEDE2000}(I_{orig}, I_{trad}).$\nWe then filter outliers by zeroing out values less than 3 or greater than 8, denoted by $H_{neural}^*$ and $H_{trad}^*$. Next is an average pooling operation with size 128 and step 64. We take the difference between the resulting maps and locate the highest value, as in the previous methods:\n$\\Delta H = \\text{AvgPool}_{128} (H_{trad}^*) - \\text{AvgPool}_{128} (H_{neural}^*),$\n$\\text{Center} = \\underset{x,y}{\\text{argmax}} (\\Delta H),$\n$\\text{Confidence} = \\underset{x,y}{\\text{max}} (\\Delta H)$."}, {"title": "3.4 Small-Color-Artifact Detection", "content": "In contrast to the previous method, small-color-artifact detection finds smaller-scale color distortions. We mostly follow the super-resolution-artifact detection method in [21], adapting it to compression.\nOur approach follows the residual- and primary-map-construction steps from [21] for both neural- and traditional-codec results, denoted by $r \\in \\{\\text{neural, trad}\\}$:\n$R_c^r(i, j) = \\sum_{c \\in C} |I_{orig}^c(i, j) - I^c(i, j)|,$\n$M_c^r(i, j) = \\text{var}(\nR_c^r(i-\\frac{n-1}{2}: i+\\frac{n-1}{2},\nj-\\frac{n-1}{2}: j+\\frac{n-1}{2})\n),$\nwhere $C$ indicates the color channels on which we will compute these maps. We set $n$ to 33 empirically.\nWe also compute the stable patch-level variance and use it to scale the primary map:\n$\\sigma_c^r = (\\text{var}(R_c^r)),$\n$S_c^r(i, j) = \\sigma_c^r M_c^r(i, j)$.\nNext we take the difference between the resulting maps for neural- and traditional-compression results, then apply a threshold:\n$\\Delta S^c = S_c^{\\text{trad}} - S_c^{\\text{neural}},$\n$B^c = \\begin{cases}\n1, & \\Delta S^c > T, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$"}, {"title": "3.5 Text-Artifact Detection", "content": "Our approach to detecting text artifacts begins by running the OpenMMLab text detector [1] on $I_{orig}$, returning a list of polygons that encompass any text along a set of with confidence scores. Polygons with confidence less than 0.7 are discarded to reduce false positives.\nNext, we surround each polygon with a bounding box and discard bounding boxes whose area is less than 20\u00d720 pixels. Distortions in such small areas have proven to exhibit minimal impact on image perception.\nOur method processes each remaining bounding box individually. We crop the area corresponding to the bounding box from each of the three input images to"}, {"title": "4 JPEG AI-Compression-Artifact Dataset", "content": "To facilitate JPEG AI and other learning-based-codec research, we collected a dataset of 46,440 JPEG AI-artifacts of three types (texture, color, and text distortions). The dataset includes the source images and detailed annotations for every artifact, including its type and bounding box. We validated all examples through crowdsourced subjective assessment. This section describes our procedures for dataset collection, annotation, and validation."}, {"title": "4.1 Overview", "content": "We collected 350,000 source images from the Open Images [16] dataset, compressed them using JPEG AI and a traditional codec, and processed them using the methods described in Section 3 to obtain the preliminary annotations. Our next step was to conduct a subjective assessment, described in Section 4.2, to verify the annotations and discard false positives. In total, we obtained 46,440 confirmed artifacts.\nAs of this writing, JPEG AI remains under development, and new versions emerge regularly. We followed the development process and switched to new JPEG AI versions as they arrived. Our dataset therefore contains examples from several versions. We set the tools parameter to off for all JPEG AI versions. Our example figures demonstrate that the artifacts remain on the tools on configuration.\nJPEG AI offers five quality presets that control its quality/size tradeoff. We compressed and processed images using all five quality presets. After subjective validation, we deduplicated the artifacts on the basis of intersection-over-union, preferring those that occurred at higher quality presets because confirmed artifacts at these presets tend to be scarcer and are therefore more valuable.\nTable 1 summarizes the partitions of our dataset by artifact type, by the JPEG AI quality preset, and by the JPEG AI version used to compress the source image.\nFor the traditional-codec-compression results we used intra coding from VTM-20.0 and HM-18.0, the reference implementations of the VVC and HEVC standards, respectively. We made the target VTM bitrate as close as possible to the file size produced by JPEG AI for each of its quality presets. Then, since HM uses a previous-generation coding standard, we raised its bitrate by 20% com-"}, {"title": "4.2 Subjective Validation", "content": "We selected the Toloka\u00b3 platform for crowdsourced comparisons. Each artifact type underwent validation using a separate labeling task, accompanied by instructions to familiarize participants with images that contain artifacts. Every task had control examples to ensure accurate responses. Each artifact was judged by three participants, and one positive vote was considered sufficient to confirm the artifact.\nFor texture-artifact validation, participants were asked to evaluate 300\u00d7300-pixel crops from $I_{orig}$, $I_{neural}$, and $I_{trad}$, centered on the artifact located by methods 3.1 and 3.2. They were asked which contained the greater texture distortion: the crop from JPEG AI or the one from HM. If they chose JPEG AI, the artifact was considered confirmed.\nFor color-artifact validation, we similarly cropped 300\u00d7300-pixel fragments and combined crops from $I_{orig}$ and $I_{trad}$, as well as from $I_{orig}$ and $I_{neural}$. The result was two checkerboard images that made the color differences more apparent. Participants were then asked to select the image in which the checkerboard was most prominent\u2014i.e., the one in which the color distortion was more pronounced.\nFor text-artifact validation, participants were asked to rate pairs of crops from $I_{neural}$ and $I_{trad}$ found by method 3.5. These comparisons omitted the original crop because the original text is unnecessary to confirm text distortion. Participants were to choose the image in which the text was more distorted."}, {"title": "5 Comparison with Existing Methods", "content": "To evaluate the proposed methods' effectiveness, we compare them with several existing full-reference image quality assessment methods by area under the ROC"}, {"title": "6 Conclusion", "content": "In this paper, we focused on characteristic visual artifacts produced by learning-based compression methods specifically, JPEG AI. We proposed automatic methods to detect texture, boundary, color, and text distortions that appear in compression results from a neural codec but not in those from a traditional codec."}]}