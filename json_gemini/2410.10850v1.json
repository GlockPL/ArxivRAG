{"title": "On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts", "authors": ["Toluwani Aremu", "Oluwakemi Akinwehinmi", "Chukwuemeka Nwagu", "Syed Ishtiaque Ahmed", "Rita Orji", "Pedro Arnau Del Amo", "Abdulmotaleb El Saddik"], "abstract": "We investigate and observe the behaviour and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution.", "sections": [{"title": "Introduction", "content": "In recent times, the proliferation of Large Language Models (LLMs) has significantly impacted the field of artificial intelligence, owing to their exceptional capabilities in language comprehension and generation. These advanced models have become integral in various applications across multiple industries. Yet, their growing popularity and utility bring forth crucial challenges and ethical considerations.\nPredominantly based on transformative deep learning architectures like Transformers, LLMs have revolution-ized Natural Language Processing (NLP). These models, characterized by their vast neural networks containing millions or billions of parameters, are trained on extensive datasets encompassing a wide array of sources such as internet content, literary works, and diverse media. Such comprehensive training enables them to grasp and interpret a myriad of linguistic patterns and subtleties.\nMirroring the historical reliance on search engines for inter-net queries, users are now increasingly turning to chatbots powered by LLMs for instantaneous and direct responses. Notably, since the advent of ChatGPT, a variant based on the GPT-3.5 architecture in late 2022, the development and implementation of LLMs have rapidly expanded across various sectors. These models have been deployed in areas including virtual assistance, customer support, content creation, search functionality, and in the realms of medical, scientific research, programming assistance, educational tools, and more. However, this expansion has simultaneously sparked significant concerns regarding the ethical use of these technologies, as there have been instances of the models exhibiting biases, generating inaccurate information, making unfair judgments, or inciting ethical debates due to potential misuse.\nLarge Language models (LLMs) are repidly evolving, rais-ing concerns about their potential to generate and dissemi-nate misinformation. Biases within these models could lead to unequal information access or reinforcement of existing societal biases. Based on these issues, we investigate the be-haviour of chatbots backed by LLMs, to answer two research questions;"}, {"title": "Research Questions", "content": "1. When faced with misinformed prompts, do LLMs reflect, amplify, or rectify the misinformation through their re-sponses?\n2. Do LLMs exhibit biases when answering prompts which contain demographic information?\nTo answer these questions, we focused on the implications of utilizing these chatbots in discussions related to climate change and mental health. We focus our analysis on three LLM-powered chatbots: ChatGPT, Bing Chat, and Google BARD, assessing whether they manifest biases or propagate misinformation. Climate change and mental health, being among the most extensively discussed topics on social media as indicated by Google Trends\u00b9 and Exploding Topics2, are chosen for their relevance and the critical nature of accurate information dissemination in these areas. For the purpose of our study, our main contributions are as follows:"}, {"title": "Literature Review", "content": "The contemporary AI landscape, particularly the rise of large language models (LLMs) has sparked critical dis-cussions around ethical concerns. These concerns extend beyond job displacement and privacy violations to encom-pass the potential for misinformation dissemination. LLMs, trained on massive datasets, can unknowingly perpetuate biases and factual inaccuracies present in the training data (Bhardwaj A and S 2020). Previous studies says confirmation bias and motivated reasoning can lead to favor information that aligns with existing beliefs (R.S 1998). This raises concerns about the trustworthiness of LLMS outputs, especially when applied to sensitive domains like climate change and mental health.\nMassive foundation models, which have found applications across a wide array of domains and contexts (Bommasani et al. 2022). These models, boasting billions of learned parameters and trained on extensive datasets, have exhibited remarkable effectiveness in their respective downstream tasks. Consequently, the integration of AI into real-world applications has witnessed a phenomenal and exponential surge (Li et al. 2023a; Moor et al. 2023; Weisz et al. 2023). In sharp contrast to traditional models, which often suffer from inherent constraints tied to their narrow focus, foundation models offer a versatile and adaptable approach. Once these models have undergone training, they can be conveniently fine-tuned to suit a diverse spectrum of applications, thus eliminating the necessity for extensive retraining. This adaptive framework serves as the linchpin of LLMs. Notably, this fine-tuning capability has paved the way for deploying these language models in a multitude of domains, spanning healthcare, financial advisory, climate change analysis, and question answering, to name just a few.\nAs the scope of artificial intelligence (AI) continues to expand, ushering in captivating innovations, it has sparked spirited debates on a broad range of ethical concerns. These concerns encompass the potential impacts of these advancements on various facets of human existence, in-cluding individual lives, employment, privacy, and issues related to discrimination (Raghavan et al. 2019; Kelley et al. 2021). Simultaneously, questions have emerged regarding the appropriate course of action for the adoption of these transformative technologies (Aremu 2023; Liang et al. 2023).\nAccording to an article by researchers at Google published in 2021 (Kelley et al. 2021) to assess public perception of Al in eight countries, people in developing countries like Nigeria, India, and Brazil are significantly more likely to embrace and adopt AI compared to individuals in developed countries. However, ethical researchers in the AI domain have raised concerns that such AI applications may disproportionately affect people living in these regions, as most of the data used to train these models is sourced from developed countries (Gebru et al. 2021; Shneiderman 2020; Buolamwini and Gebru 2018; Raji et al. 2020; Mitchell et al. 2019; Mittelstadt et al. 2016). Therefore, it comes as no surprise that throughout 2023, a series of pivotal gov-ernmental hearings have convened, where political leaders engaged with a diverse array of experts to gain insights into the origins and implications of these technologies. These hearings have probed critical aspects, such as the inherent risks associated with these AI systems, the nature of the data on which they are trained, and the formulation of policies designed to safeguard the well-being and privacy of users. These discussions also consider equitable compensation for the creators and owners of the data that underpin these AI systems.\nIn this section, our focus narrows to articles highlighting the deployment of LLMs in the contexts of climate change/-sustainability and mental health/physical health. Our objec-tive here is to demonstrate the substantial strides made in the adoption of AI technologies, setting the stage for subse-quent sections where we delve into our methodologies and present the results of experiments conducted to evaluate bi-ases and misinformation in LLMs when applied to both cli-mate change and mental health contexts."}, {"title": "Climate Change", "content": "The advent of large language models in the realm of climate change research took a significant leap forward in 2021 with the introduction of ClimateBERT by Webersinke (Webersinke et al. 2022). ClimateBERT, a transformer-based language model, was pretrained on an extensive dataset comprising over 2 million paragraphs sourced from climate-related texts, including news, research articles, and corporate climate reports. Its primary purpose was to facilitate climate change question answering and text summarization. Subsequently, an array of tools (Vaghefi et al. 2023; Ni et al. 2023; Fard, Hasan, and Bell 2022; Li 2023; Garrido-Merch'an, Gonz'alez-Barthe, and Vaca 2023;"}, {"title": "Mental Health", "content": "Language models show promise in addressing challenges within the field of mental health. Several of these models, employing smaller language models (Denecke, Vaaheesan, and Arulnathan 2020; Ji et al. 2021), have been proposed for applications in both mental health and general medical care. More recent developments have leveraged larger language models (Li et al. 2023b; Xu et al. 2023; Liu et al. 2023b; Bao et al. 2023). It's important to note that these models introduce ethical concerns, as they have the potential to cause irreversible harm to users.\nEmpirical evaluations of LLMs in this domain primarily fall into two categories: some assess LLMs' ability to classify different types of mental health issues using annotated text data (Yang et al. 2023; Wang, Zhao, and Petzold 2023), while others evaluate their performance in various medi-cal examinations (Nori et al. 2023; Liu et al. 2023a; Man-athunga and Hettigoda 2023; Rosol et al. 2023; Kasai et al. 2023; Singhal et al. 2023). Our approach, however, diverges from these studies. We delve deeper into the analysis of the responses generated by the models we employ, collaborat-ing with experts to determine their readiness for real-world deployment or whether significant strides are still required in this area."}, {"title": "Methodology", "content": "This study is structured to address our central research ob-jective which is to evaluate the level of misinformation and bias in LLM-powered chatbots in climate change and mental health discussions. We do this through a dual-pronged ap-proach: firstly, by understanding and quantifying misinfor-mation, and secondly, by evaluating biases in the responses of LLM chatbots. This section details the methodologies em-ployed in each of these categories."}, {"title": "Tools and Data Collection", "content": "In our study, we evaluate three cutting-edge most popular and accessible LLM chatbots: Microsoft's Bing Chat, Ope-nAI's ChatGPT, and Google's Bard (now Gemini). To con-duct an extensive evaluation, we compiled a set of frequently asked questions (FAQs) on two crucial topics-Climate Change and Mental Health. We prioritize frequently asked questions (FAQs) to reflect real-world user queries encoun-tered by these LLMs. This approach ensures the general-izability of our findings to real-world LLMs interactions. We also assume that the LLMs used in these chatbots were trained on and has access to the information. Hence, to test each chatbot susceptibility to misinformation, we intention-ally altered a subset of the selected questions-.These al-terations involved introducing subtle factual inaccuracies, changing the tone of the prompt, or incorporating irrelevant words to the prompts. The specific type of alteration de-pended on the misinformation concept we aimed to assess. We used the same misinformed prompts uniformly across the chatbots intended for testing.\nMisinformation Assessment: For the quantitative analy-sis of misinformation, we curated a dataset comprising 3,120 true/false questions on Climate Change and 2,762 on Mental Health. For the qualitative analysis, we selected 53 questions about Climate Change and 40 about Mental Health from au-thoritative sources like NASA and the CDC. These prompts were intentionally altered to test if these chatbots are capa-ble of handling the misinformed prompts, or would rather amplify the misinformation.\nBias Assessment: In evaluating potential biases in the chatbots' responses, we chose 24 questions on Climate Change and 38 on Mental Health. The aim here was to con-duct a qualitative assessment of the chatbots' outputs, check-ing for objectivity and neutrality. To this end, specific de-mographic details such as age, race, and location were inte-grated into the prompts.\nChatbot Query Approach: For Qualitative Analysis, we interacted with the chatbots using a standardized format to ensure concise and informative responses. Each question was formatted as:\n\"In one short paragraph, [question]. Provide sources for your response.\"\nFor Quantitative Analysis, the methodology for quantitative interaction also employed a standard prompt format, simpli-fying the chatbots' responses to a binary choice:\n\"Respond with either True/Yes or False/No: [question].\""}, {"title": "Analysis", "content": "This section details the methodological framework adopted for analyzing the misinformation and bias components in our study, divided into two distinct segments:\nQuantitative Analysis to check for Misinformation: The quantitative analysis scrutinizes the chatbots' responses using a suite of metrics:\n\u2022 Confusion Matrix: This tool visualizes the distribution of True Positives, False Positives, True Negatives, and False Negatives for the true/false questions.\n\u2022 Precision and Recall: These metrics evaluate the accu-racy and completeness of the classification model, based on the results of the confusion matrix.\n\u2022 F1 Score and Accuracy: These indicators provide in-sights into the model's harmonic balance between Pre-cision and Recall.\n\u2022 Similarity Index Scores: We employ BLEU (Papineni et al. 2002), ROGUE (Lin 2004), and METEOR (Banerjee and Lavie 2005) scores to measure the closeness of the chatbot's responses to standard benchmark answers, thereby assessing the quality and relevance of the content provided.\nQualitative Analysis to check for Misinformation: We conduct a comprehensive qualitative analysis of the re-sponses from the chatbots on the dataset we collected for this analysis. This facet of the analysis involves in-depth in-terviews with domain experts and the deployment of special-ized questionnaires tailored to these fields.\nQualitative Analysis to check for Bias: The bias analysis segment focuses on quantitatively evaluating feedback from domain experts. These specialists will critique and provide perspectives on the extent of bias evident in the chatbots' re-sponses. The aim here is to uncover any subjective biases that might be embedded in the outputs related to Mental Health and Climate Change topics.\nDomain Expert Selection Criteria\nIn the process of selecting domain experts for our study, we established specific criteria tailored to the distinct fields of Climate Change and Mental Health.\nFor Climate Change, we targeted academically creden-tialed professionals, including Professors, PostDocs, PhD students, researchers, or practitioners holding at least a master's degree in fields such as environmental science, climatology, meteorology, or ecology. Their expertise was validated through a demonstrated track record in climate change research, including publications in peer-reviewed journals, conference presentations, or significant contri-butions to relevant industry projects. A prerequisite was a minimum of three years of active involvement in areas such as climate change research, policy development, mitigation strategies, adaptation methods, or advocacy. Additionally, we emphasized the importance of interdisci-plinary knowledge, combining insights from atmospheric science, oceanography, and social sciences, to foster a comprehensive understanding of climate change impacts. Familiarity with climate policies, the ability to effectively communicate complex scientific concepts, and experience in innovative solutions and collaborative projects were also deemed essential.\nIn the Mental Health domain, our focus was on profession-als with a solid educational foundation in psychology, psy-chiatry, clinical social work, counseling, or related disci-plines, requiring a minimum of a master's degree. We sought experts with substantial clinical or research experience in mental health, evidenced by a history of patient care, par-ticipation in clinical trials, research contributions, or ad-vocacy work. Proficiency in various therapeutic modalities such as cognitive-behavioral therapy, psychotherapy, and mindfulness-based interventions was crucial. Cultural com-petence-understanding and addressing the diverse cultural and socioeconomic factors influencing mental health-was another critical criterion. Lastly, we valued experts open to exploring the ethical implications and potential applications of generative language technologies in mental health care and challenges."}, {"title": "Limitations", "content": "A significant challenge encountered was the recruitment of domain experts. For interview-based qualitative reviews, standard practice recommends a minimum of five experts per domain. Questionnaire-based qualitative reviews gener-ally require a more extensive participant base, ideally with at least 50 respondents. Despite extensive outreach efforts, our response rate was limited to 14 participants, comprising 4 interviewees and 10 questionnaire respondents. This equates to seven domain experts for each of the two categories under study. Although the number of participants are limited, the use of both quantitative and qualitative approach offer opportunity for indepth data and insights. We also believe that the inclusion of experts from diverse backgrounds, culture, and continents contributes positively to the quality of our findings.\nAs mentioned earlier, one of the strengths of this study is the geographical diversity of our expert panel. We successfully included at least one domain expert from each continent (refer to Figure 1 for details), which bolsters the validity of our results. This diverse representation helps mitigate regional biases and enhances the global relevance of our findings.\nHence, we believe that despite the limitation in terms of numbers, the study provides meaningful insights into the re-search area. The limitations highlight avenues for future re-search, particularly in broadening the expert participant base to further validate and enrich the study's conclusions."}, {"title": "Findings", "content": "This section elucidates our study's results, commencing with a quantitative analysis of the chatbots' average perfor-mance on the dataset of True/False questions within the Cli-mate Change and Mental Health domains. We subsequently present the similarity scores based on the metrics men-tioned above, comparing the responses of all three chatbots against established facts to determine their factual adher-ence. Lastly, we expand into the qualitative insights derived from engaging with domain experts, providing an in-depth exploration of their perspectives in both domains of interest. These findings are consistent with research by (Weisz et al. 2023; Raghavan et al. 2019; Shneiderman 2020; Bhardwaj A and S 2020; R.S 1998) who found that LLMs trained on massive datasets can still be susceptible to misinformation, particularly when the information is cleverly disguised."}, {"title": "Quantitative Analysis: True/False Prompts", "content": "In this study, we evaluate the chatbots' ability to discern the veracity of statements related to climate change and mental health, in order to quantify its level of knowledge, or how misinformed it might be. Utilizing a quantitative approach, we analyze a True/False dataset and calculate critical performance metrics. The analysis (Figure 2) includes a detailed examination of instances where the model incorrectly classified true statements as false (false negatives) and false statements as true (false positives), as well as accurately identified true (true positives) and false (true negatives) statements.\nAn in-depth analysis of the performance metrics, as detailed in Table 1, shows that in the realm of Climate Change, our chatbots demonstrates commendable accuracy with a precision rate of 88.4%. This indicates that the majority of the statements classified as true by the model are indeed correct. The recall rate of 91.9% further suggests that it successfully identifies a high percentage of the true statements within this domain. The F1 score stands at 90.1%, reflecting a strong overall performance. However, the overall accuracy, at 89.9%, while high, indicates there is room for improvement in reducing misinformation when it comes to climate change.\nIn the Mental Health domain, the observed performance is notably enhanced. It achieves a higher precision rate of 90.1%, suggesting that its capacity to correctly identify true statements is more refined in this domain. The recall rate of 95.2% is particularly impressive, indicating that the model is highly effective at capturing true instances. The F1 score, at an elevated 92.6%, points to a balanced and efficient classi-fication capability. Moreover, the accuracy of 92.5% under-scores a significant level of reliability in the Mental Health domain."}, {"title": "Quantitative Analysis: Similarity Index Scores", "content": "In the quantitative phase of our analysis, we employed three well-known metrics-BLEU, ROGUE, and ME-TEOR-from the machine translation evaluation field. These metrics traditionally assess how closely machine-generated text matches human translation, in terms of both accuracy and contextual coherence. For this study, we adapted these metrics to assess the performance of chatbots, positing their applicability beyond their usual context of translation.\nBLEU and ROGUE metrics are designed to measure the precision and recall of the chatbots' responses against a benchmark of human-generated texts. METEOR goes a step further by including advanced linguistic analysis-such as synonym matching, stemming, and paraphrasing-to provide a more nuanced assessment. This metric therefore offers a measure of evaluation that more closely approx-imates human judgment by accounting for semantic and contextual accuracy, in addition to exact word correspon-dences. We analyzed the chatbots' outputs by comparing them to the verifiable answers within our dataset. To ensure comparability, we normalized the resulting similarity scores, aiming for a maximum value of 1. This step was crucial, given that our 'misinformed' prompts often led to chatbot responses that were shorter and substantially varied from the factual responses, sometimes resulting in inaccuracies or misinformation. Although there's no absolute threshold set for misinformation, these normalized scores serve as indicators of the degree to which the chatbots' responses emulate the factual data."}, {"title": "Qualitative Analysis", "content": "This section details our approach to gauging the levels of misinformation and bias present in the responses provided by three prominent chatbots-specifically, those focused on Climate Change and Mental Health topics. To this end, we sought insights from domain experts in these respective fields.\nInitially, we endeavored to engage a broad spectrum of specialists for in-depth interviews based on the chatbots' responses. As revealed in Section, the response rate was limited to 14 experts, spanning both domains. Our initial strategy involved forwarding the chatbot-generated responses to these experts, followed by interviews after a week. However, time constraints necessitated a strategic switch after interviews with our first two climate change experts. We transitioned to a questionnaire-based approach, using similar questions from the interview methodology, which significantly enhanced time efficiency.\nThe questionnaires comprised both closed- and open-ended questions, adapted to suit the experts' convenience. This"}, {"title": "(I.) Experts' Perspectives on Misinformation and Bias in Climate Change", "content": "In our investigation, we presented each expert with questions concerning the chatbots' responses on Climate Change. The focus of these inquiries was to under-stand the experts' perceptions of the potential impact these chatbots might have on user safety and information dissem-ination. We present our findings under four main themes.\n1. Role of LLM-based Chatbots in Climate Change Awareness: We asked the experts about the significance of LLM-based chatbots in enhancing public awareness of climate change. The majority, barring two, were opti-mistic, acknowledging that the newer generation of chat-bots could play a substantial role in spreading awareness and disseminating information. On the contrary, one ex-pert expressed skepticism about the depth and utility of the chatbots' responses, likening them to shallow internet searches. Another pointed out no discernible advantage over traditional search engines. Despite these differing views, there was a consensus that more specialized and expert-driven models could yield more precise and reli-able information than the three chatbots evaluated. The experts suggested improvements such as enabling chat-bots to provide simplified yet comprehensive answers, elucidate the reasoning behind their responses, and trans-parently cite their information sources.\n2. Challenges in Generating Accurate Climate-Related Information: We sought the experts' views on the ob-stacles faced by chatbots in delivering precise and trust-worthy information on climate change. A significant por-tion of the respondents raised concerns about the data sources used to train these models. They emphasized the lack of assurance regarding the quality and credibil-ity of the sources cited by the chatbots. Another com-mon issue highlighted was the inconsistency in the chat-bots' responses. Experts noted that for certain queries, the models produced vastly differing answers, which could lead to confusion. Additionally, the generality of the responses was a point of contention. Experts pointed out that climate change effects and solutions are of-ten location-specific, yet the chatbots tended to provide broad, universally applicable answers. This, they sug-gested, might stem from the nature of the prompts fed to the models. More accurate and tailored responses could potentially be elicited with prompts that include more de-tailed and specific instructions or context.\n3. Expert Insights on Biases and Misinformation in Cli-mate Data Dissemination: We inquired about the ex-perts' perception of potential biases and misinformation in the chatbot-generated responses on climate-related topics. A notable portion of the experts, about half, identified instances of data exaggeration leading to mis-information. They expressed concerns about the chat-bots being trained on datasets with unverified or non-reproducible sources, a significant issue in a field prone to false negatives. Such practices, they cautioned, re-sult in the propagation of unverified and potentially mis-leading information. Conversely, the other half acknowl-"}, {"title": "4. Chatbots as Tools for Promoting Sustainable Behaviors", "content": "We explored the experts' views on the potential of these chatbots in fostering sustainable behaviors and lifestyle changes among users. The response was unani-mously positive across the board. The experts acknowl-edged the importance of reliable information sources but were optimistic about the role of chatbots, especially those specialized in climate change, in influencing user behavior towards sustainability. They concurred that ap-propriately designed chatbots could effectively encour-age users to adopt more environmentally friendly prac-tices. Furthermore, some experts proposed specific fea-tures that could enhance the chatbots' capability to advo-cate for sustainability. These suggestions included per-sonalized advice based on user's lifestyle, interactive guides on reducing carbon footprints, and timely updates on environmental issues and solutions, all tailored to en-gage users actively in sustainability efforts.\nDiscussion: Our panel of climate change experts concurs that, despite the need for considerable improvements in safety and reliability, LLM-backed chatbots hold immense potential for impactful applications. These AI-driven tools are lauded for their capacity to revolutionize the dissemina-tion of crucial information and to promote environmental consciousness among the public. Nonetheless, experts stress the imperative for stringent validation and continuous refinement of these systems to bolster their effectiveness and credibility in information dissemination.\nOne prominent recommendation from the experts is the strategic curation of training datasets for LLMs, advocating for the inclusion of data primarily from verifiable and expert-endorsed sources. This recommendation arises from their observation that the chatbots in our study often referenced materials indiscriminately, linking to articles that may be obsolete or from publishers lacking official standing in academic research. Moreover, instances of non-functional or potentially deceptive links further highlight the risks associated with unvetted information sources."}, {"title": "(II.) Experts' Perspectives on Misinformation and Bias in Mental Health", "content": "In discussions with domain experts, we sought to understand the behavior and potential of the three chatbots in the mental health domain. Our dialogue focused on several key areas:\n1. Impact on Stigma and Awareness: Our inquiry into experts' perceptions commenced with questions about the potential impact of chatbots on stigma reduction and awareness enhancement in mental health. The response was uniformly positive, with experts recognizing the sub-stantial value of LLM-backed chatbots as a medium for spreading mental health information. Notably, one expert cited current research where AI is leveraged in therapeu-tic settings to assist psychologists in interpreting clients' emotions and identifying underlying issues more effec-tively. This ongoing work was paralleled with the evolu-tion of LLMs, suggesting that such advancements could significantly aid in patient support and care.\n2. Privacy Concerns: However, a majority of the ex-perts-five out of seven-raised concerns about pri-vacy when deploying chatbots in mental health contexts. These concerns emerged from a discussion on the ethi-cal implications of using such technologies. The conver-sation then shifted to the critical role of informed con-sent in mitigating potential privacy issues associated with the deployment of chatbots. The experts emphasized that user consent should be a cornerstone of any application involving personal and sensitive data.\n3. Personalized Support and Risks: The potential of chat-bots to offer individualized support was a focal point of our expert discussions. The unanimous viewpoint among the experts was that providing personalized support via chatbots is fraught with risks and is impractical without stringent supervision. They opined that chatbots could serve effectively as conduits of information but should not overstep into realms requiring professional expertise, such as prescribing medication or making clinical diag-noses. One expert succinctly remarked that the chatbots' utility should be confined to information dissemination. This stems from the current trend of utilizing LLMs as an alternative to traditional search engines, leading to a concerning rise in individuals self-diagnosing and self-prescribing without professional medical counsel. Con-sequently, our experts advocated for a clear demarcation in the chatbots' roles, suggesting that they should act as signposts directing users to qualified health profession-als, rather than attempting to replicate the nuanced and critical functions of those experts. This approach is in-tended to harness the benefits of LLMs in raising aware-"}, {"title": "4. Trust and Confidentiality", "content": "When inquiring whether chatbots could engender a sense of trust and confiden-tiality for individuals seeking mental health guidance, the experts' feedback was encouragingly positive. A por-tion of the panel attributed this potential to the chat-bots' easy and unrestricted access, positing that the con-venience they offer could promote their use as a trust-worthy source of support. Others pointed to the broader societal embrace of technology, noting the propensity for individuals to become reliant on apps that simplify com-plex tasks. They drew parallels with consumer behavior trends, such as the increasing preference for online trans-actions over in-person interactions and the shift from tra-ditional media consumption to social media engagement. These observations suggest that users may readily con-fide in chatbots for insights into their mental health chal-lenges, potentially overlooking the inherent risks of sub-stituting professional human interaction with AI-driven advice.\n5. Cultural Sensitivity and Inclusivity: When exploring the inclusivity and cultural sensitivity of these chatbots, our findings revealed diverse opinions among the experts. Two remained neutral, neither confirming nor denying the chatbots' cultural adeptness. However, the majority, five out of seven experts, expressed reservations. They criticized the chatbots for their inability to tailor re-sponses to diverse cultural backgrounds and individual patient details, often providing uniform answers across various demographics. This lack of customization, ac-cording to our experts, highlights a need for vigilant over-sight in the deployment of these AI systems. Moreover, there was a general consensus that the chatbots' current design predisposes them to a fixed response pattern, sug-gesting an inherent rigidity that calls for professional hu-man oversight to ensure appropriate and sensitive use.\nDiscussion: In summation, the consensus among the domain experts is that while LLM-backed chatbots hold promise for augmenting professional mental health services, there re-mains a divergence of opinions regarding their use by indi-viduals for personal mental health needs. The experts com-mend the chatbots for their ability to encourage users to seek help during crises. An exemplary case is Bard, which pro-vides a helpline number for immediate assistance. Such fea-tures exemplify the optimal role of chatbots in mental health care: to provide high-level information and to act as sign-posts directing users to professional help, rather than serving as standalone therapeutic tools."}, {"title": "Conclusion and Future Work", "content": "Synthesis of Findings: Our comprehensive study has shed light on the capabilities and current limitations of LLM-backed chatbots, with a particular focus on ChatGPT, Google Bard, and Bing Chat across two critical domains: Climate Change and Mental Health. The quantitative analy-sis, as substantiated by our empirical data and illustrated in Table 1 and Figure 2, demonstrates a proficient performance by the chatbots in True/False classification tasks. Notably, the chatbots showed a higher accuracy rate in the Mental Health domain compared to Climate Change. Furthermore, the normalized similarity index scores, depicted in Figure 3, reveal varying degrees of factual adherence, with each chat-bot exhibiting unique strengths and weaknesses.\nExpert Evaluations: Qualitative evaluations by domain experts highlighted the potential of chatbots in enhancing public awareness and reducing stigma associated with men-tal health. Yet, they also cautioned against unregulated per-sonal use, emphasizing the paramount importance of pri-vacy, ethical considerations, and the need for expert inter-vention in sensitive scenarios. The experts advocated for chatbots to function as informational gateways, directing users to professional services rather than attempting to re-place them.\nRecommendations for Further Work: Looking ahead, the study identifies several avenues for further research and development. Prominent among these is the need for chat-bots to incorporate culturally sensitive and demographically tailored interactions, which currently stands as a significant gap in their design. Additionally, there is a call for the con-tinual refinement of LLM training datasets, ensuring they are derived from verified, expert-approved sources to enhance the quality and reliability of the information provided.\nClosing Thoughts: In conclusion, while chatbots rep-resent a remarkable technological advancement with the potential to contribute positively to society, their deploy-ment-especially in domains as sensitive as Climate Change and Mental Health-must be approached with meticulous care, ethical standards, and professional oversight. As AI continues to evolve, it is imperative that we, as researchers and developers, keep pace with these advancements, ensur-ing that we mitigate risks and harness AI's full potential re-sponsibly and ethically. The ultimate goal is to achieve a harmonious integration of AI tools like chatbots into the fab-ric of societal support systems, augmenting human expertise rather than attempting to supplant it."}]}