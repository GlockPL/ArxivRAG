{"title": "ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks", "authors": ["Ziji Shi", "Jialin Li", "Yang You"], "abstract": "Recent advances in Generative Artificial Intelligence have fueled numerous applications, particularly those involving Generative Adversarial Networks (GANs), which are essential for synthesizing realistic photos and videos. However, efficiently training GANs remains a critical challenge due to their computationally intensive and numerically unstable nature. Existing methods often require days or even weeks for training, posing significant resource and time constraints. In this work, we introduce ParaGAN, a scalable distributed GAN training framework that leverages asynchronous training and an asymmetric optimization policy to accelerate GAN training. ParaGAN employs a congestion-aware data pipeline and hardware-aware layout transformation to enhance accelerator utilization, resulting in over 30% improvements in throughput. With ParaGAN, we reduce the training time of BigGAN from 15 days to 14 hours while achieving 91% scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution image generation using BigGAN.", "sections": [{"title": "1 INTRODUCTION", "content": "The last decade has witnessed the success of Generative Adversarial Networks [8], which has a wide range of applications including image super-resolution [19], image translation [12, 40], photo inpainting [7, 37]. However, training GAN at scale remains challenging because of the computational demands and optimization difficulties.\nUnlike other conventional neural networks where optimization is straightforward by taking gradient descents, there are two sub-networks to optimize in GAN, namely generator and discriminator. The generator samples from the noise and produces a fake sample as close to the real sample as possible, and the discriminator evaluates the generated sample. The generator aims to fool the discriminator, and the discriminator will try to identify the fake images from the real ones. Since the two components are optimized for two contradicting goals, it has been observed that GANs are difficult to converge. Therefore, to speed up the GAN training at large scale, we need a framework optimized on both system and numerical perspectives.\nDue to the difficulty of optimizing GAN, many state-of-the-art GAN models take days or even weeks to train. For instance, BigGAN [3] took 15 days on 8x V100 GPUs to converge. This makes it difficult to quickly reproduce, evaluate, and iterate GAN experiments. Also, current GAN frameworks usually only support training on a small number of nodes [5, 15, 20].\nWe argue that training speed is an important yet often ignored factor in the current GAN training landscape, and we propose to accelerate it with distributed training. However, distributed GAN training has several challenges. First of all,"}, {"title": "2 MOTIVATION AND REQUIREMENTS", "content": "We begin by considering the basic components of GAN to discover the key requirements for GAN training. As shown in Fig. 2, a GAN consists of a generator and a discriminator. The generator generates fake data samples, while the discriminator distinguishes between the generated samples and real samples as accurately as possible. The learning problem of GANs is a minimax optimization problem. The goal of the optimization is to reach an equilibrium for a two-player problem:\n$\\min_{G} \\max_{D} E_{x\\sim q_{data}(x)} [\\log D(x)]+E_{z\\sim p(z)} [\\log (1 \u2013 D(G(z)))]$\nwhere $z \\in \\mathbb{R}^{dz}$ is a latent variable drawn from distribution $p(z)$. The discriminator seeks to maximize the sum of the log probability of correctly predicting the real and fake samples, while the generator tries to minimize it instead. The convergence of GAN is defined in the form of Nash Equilibrium: one network does not change its loss regardless of what the other network does.\nSince the two networks have contradicting goals, the training process of GAN is a zero-sum game and can be very unstable. Recent works show that: i) GAN may converge to points that are not local minimax using gradient descent, in particular for a non-convex game which is common [6, 13], and ii) gradient descent on GAN exhibits strong rotation around fixed points, which requires using very small learning rates [1, 25]. Also, GANs training is sensitive to the hyperparameters and initialization [23]. Therefore, it is observed that GANs are difficult to optimize, and this is also the reason why it takes a long time to train them.\nThere are some existing GAN libraries [5, 15, 20, 23] for training state-of-the-art GANs. They provide standardized"}, {"title": "3 DESIGN AND ARCHITECTURE", "content": "In this section, we will give an overview and discuss the design decisions of ParaGAN. We recognize that the scalability is usually limited by the latency between nodes. Furthermore, when scaling up the GAN training, the numerical instability problem happens more often. We therefore divide the discussions into two folds and present our co-designed approach for system throughput and training stability as it scales."}, {"title": "3.1 Programming Model", "content": "The architecture of ParaGAN is presented in Fig. 3. ParaGAN is implemented on top of TensorFlow, but it can be ported to support other DL frameworks. ParaGAN provides high-level APIs for GAN which include scaling manager, evaluation metrics, and common network backbones. Users of ParaGAN can import from ParaGAN or define their components. ParaGAN then performs layout transformation on tensors and invokes TensorFlow, which converts the model definition to a computational graph. An optional XLA [31] pass can be performed followed by that. After that, the training loop starts on the compute nodes that host accelerators like GPUs and TPUs.\nWe introduce three concepts in ParaGAN:"}, {"title": "3.1.1 Scaling Manager", "content": "The scaling manager is in charge of hyper-parameters that need to be tuned when scaling, including learning rate, optimizer, and local batch size. Users can use the best hyper-parameters from a single worker as a starting point, and ParaGAN will scale them based on the number of workers and learning rate schedules. Users can also define their scaling manager."}, {"title": "3.1.2 Network Backbones", "content": "Users usually start by building upon existing GAN architectures. We also provide some popular GAN architectures as backbone, including but not limited to:\n\u2022 BigGAN [3];\n\u2022 Deep Convolutional GAN (DCGAN) [30];\n\u2022 Spectral Norm GAN (SNGAN) [27]"}, {"title": "3.1.3 Evaluation Metrics", "content": "Evaluation metrics can be implemented differently across papers, and this can cause inconsistency. We provide commonly used evaluation metrics including Frechet Inception Distance (FID) and Inception Score (IS)."}, {"title": "3.2 Computation Model", "content": "Training on the cloud usually involves host machines (mostly CPU nodes), compute nodes and storage nodes. As depicted in Fig. 3, the host fetches input data from the storage node, builds the model, chooses the tensor layout for the target accelerator, and feeds to the compute nodes. After that, the accelerators execute forward and backward passes and then synchronize gradients among other accelerators. At the same time, the host prefetches and transforms the data from the storage node. Model checkpoints are saved to the storage node which can be an object store or mounted file system. A host machine can be a standalone node, while most of the time it co-locates with the compute nodes in the same"}, {"title": "3.3 System Optimizations", "content": "To satisfy the scalability requirement, we design ParaGAN with optimizations on data pipeline, computation, and memory.\nWe optimize the data pipeline performance by using a congestion-aware data pipeline. For data centers, the compute and storage nodes are usually physically distributed and interconnected via Ethernet instead of high-speed InfiniBand. The network traffic between them may not always be stable since the infrastructure is shared with other tenants. This issue is further pronounced when the number of participating data parallel workers scales up because data parallelism is a synchronous training method that is sensitive to the latency of the slowest participant. Therefore, ParaGAN continuously monitors the data pipeline latency and implements a congestion-aware data pipeline tuner.\nTo achieve a higher accelerator utilization, ParaGAN performs hardware-aware layout transformation. A data center usually has multiple types of accelerators, and different accelerators have different micro-architectures and instructions, thereby having different preferred data layouts. For example, Nvidia A100 GPUs prefer half-precision data in multiples of 64, and single-precision data in multiples of 32, while previous generations prefer multiples of 8. For TPU, the preferred data layout should have a multiple of 128 on the lane dimension and 8 on the sublane dimension. If misconfigured, this will result in unnecessary padding which reduces accelerator utilization and increases memory consumption. We come up with the hardware-aware layout transformation to transform the data into an accelerator-friendly format to maximize accelerator utilization.\nMemory usage can be reduced by mixed-precision training with Brain Float 16 (bf16) format. Theoretically, using purely bf16 can save half of the memory for activation. However, we observe through experiments that bf16 is not suitable for the layers that are sensitive to overflow/underflow. To be precise, we found that the generator and discriminator's last layer are more sensitive to precision. As such, we apply full precision (fp32) on these layers."}, {"title": "3.4 Numerical Optimizations", "content": "Another key contribution of ParaGAN is using asymmetric training to stabilize GAN. As the number of workers scales, a larger batch size can accelerate training. However, we observe that the performance of large batch training for GAN is not as stable, and mode collapse happens frequently. Since mode collapse is a type of GAN failure raised due to a highly"}, {"title": "4 IMPLEMENTATION", "content": "To begin with, we profile BigGAN training on native TensorFlow [23] and present the result in Fig. 4. As we increase the cluster size from 8 to 1024 TPU workers, idle time significantly increases due to increased communication, but convolution operation still makes up most of the time. It indicates that training GAN is a compute-bound workload. Therefore, we focus on improving the accelerator utilization in ParaGAN.\nTo achieve this goal, we use congestion-aware data pipelining to reduce data pipeline latency, hardware-aware layout transformation to increase accelerator utilization, and mixed-precision training with bfloat16 for reduced memory."}, {"title": "4.1 Congestion-Aware Data Pipelining", "content": "The communication between the compute node and storage node is much slower compared to the on-chip interconnect. Different from TPU-TPU communication using high-speed onboard interconnect or GPU-GPU communication that could go through NVLink/PCI-e bus, communication between the accelerator node and cloud storage goes through Ethernet, which is usually orders of magnitude slower than the former.\nFurthermore, during GAN training, a huge amount of data will be transmitted via Ethernet. For instance, the ImageNet 2012 dataset is over 150 GB, and training BigGAN to convergence takes around 240 epochs, resulting in a total of 32.74 TB of data being transmitted during the training phase. When the number of workers increases, the amount of peak"}, {"title": "4.2 Hardware-Aware Layout Transformation", "content": "Zero-padding is frequently used in GAN when the input cannot fit into the specified convolution dimension. For example, a matrix of shape [100, 100] will need 6384 zeros padded to run on a 128 \u00d7 128 matrix unit, which wastes 39% computing resources. As such, zero-padding hinders the accelerator performance because memory is wasted by padding, leading to lower accelerator and memory utilization rates.\nGiven that there exists an accelerator-dependent format requirement, ParaGAN performs batching opportunistically on the input data. In NCHW (batch size x number of channels x height x width) format, ParaGAN tries to batch them such that N/H/W are multiple of 128 before running on TPU so that the accelerator memory can be efficiently utilized."}, {"title": "4.3 Mixed-Precision Training", "content": "ParaGAN supports mixed-precision training with Bfloat16 [? ] (bf16) format, which has a lower memory footprint compared to double precision format, allowing users to use a larger batch size or fit a bigger model into memory. However, porting bf16 while maintaining convergence is not straightforward because bf16 trades floating-point precision for range. As a result, hyperparameters with smaller values will also need to be adjusted to accommodate lower-precision bits. For example, $\\epsilon$ in Adam optimizer is a small value added to the denominator to avoid zero-division. With low-precision bits, it is necessary to use a slightly larger $\\epsilon$ value.\nThroughout our evaluations, we observe that weights and gradients are more sensitive to the bf16 format, while activation can be represented using lower precision without significantly affecting the coverage. Also, the shallow layers are less sensitive compared to deeper layers. We provide bf16 as an option for users to explore."}, {"title": "5 NUMERICAL OPTIMIZATIONS", "content": "Synchronous large-batch training is a key technique for accelerating neural network training. However, a major issue with using large batches is that they can lead to numerical instability. In the context of GAN, numerical instability can cause mode collapse or divergence, failing to generate realistic samples with good variety.\nA key reason for the numerical instability is that GAN is a two-player game. As such, when the training proceeds, the generator and discriminator can get tightly optimized towards each other. Meanwhile, the generator and discriminator are two networks with different learning dynamics. We argue that the optimization process for GAN should treat them differently.\nTherefore, we propose an asynchronous update scheme to decouple the generator and discriminator, and an asymmetric optimization policy for optimizing the two differently."}, {"title": "5.1 Asynchronous Update Scheme", "content": "The optimization of GAN is traditionally a serial process where the generator (G) and discriminator (D) update one after another. We question the necessity of an iterative process and propose an asynchronous update scheme."}, {"title": "5.2 Asymmetric Optimization Policy", "content": "Generators and discriminators have different neural network architectures and play opposed roles during training. Thus, they are two entities with significantly distinct numerical properties. It is observed that the discriminator is more stable than the generator. This indicates that the generator and discriminator should be treated differently by using different sets of optimization techniques. However, previous researchers treat them as the same in terms of numerical optimization. We thus propose an Asymmetric Optimization strategy for generator and discriminator in GAN training.\nTo accomplish this goal, ParaGAN firstly implements some of the latest work on optimizers including Adabelief [41], rectified Adam (RAdam) [21], Lookahead [39], and LARS [36]. We then empirically explored different optimizers for training GAN, and we found that there may not be a single clear winner for all GAN architectures.\nHowever, through experiments, we observe that it can be beneficial to use different optimizers for the generator and discriminator respectively. Fig. 6 shows that although Adam reaches the lowest loss within 100K steps, it collapses thereafter, which is not desirable as it indicates the training has not reached the stable equilibrium. Adabelief is a more adaptive variant of Adam optimizer, and it can adjust the size of the weight update based on a comparison with previous updates. Our experiments also validate that Adabelief outperforms Adam. However, when using an asymmetric pair"}, {"title": "6 EVALUATION", "content": "In this section, we aim to answer the following questions: 1) how is the performance of ParaGAN compared to other frameworks? 2) how much does each part of the system contribute to the overall performance? And 3) how do the numerical optimizations improve convergence?\nIn this section, we first evaluate the end-to-end performance of ParaGAN using three metrics:\n\u2022 steps per second measures the number of steps ParaGAN can train per second;\n\u2022 images per second measures the throughput of ParaGAN trained with the ImageNet 2012 dataset;\n\u2022 time to solution measures the time it takes to reach 150k steps on ImageNet at 128 \u00d7 128 resolution.\nWe first compare ParaGAN with other popular frameworks for end-to-end performance (Sec. 6.2) and evaluate the scaling efficiency for ParaGAN (Sec. 6.3). We ablate the optimizations performed on ParaGAN in Sec 6.5."}, {"title": "6.1 Experiment Setup", "content": "We choose the BigGAN model and train it on the ImageNet ILSVRC 2012 dataset as our evaluation method because of BigGAN's profound impact on high-resolution image generation and its high computational requirements (Table 1), and ImageNet's wide variety of classes (1000 classes) also presents a significant training challenge. The evaluations were conducted using V100 GPU and TPU v3.\nWhile we use BigGAN to benchmark ParaGAN, our framework is generally applicable to other GAN architectures and"}, {"title": "6.2 Framework-level Experiments", "content": "We compare ParaGAN with StudioGAN [15] and native TensorFlow [23] for GPU performance in Fig. 7. For each of the experiments, BigGAN is trained on ImageNet 128 \u00d7 128 resolution. We use 8\u00d7 Tesla V100 GPUs except for ParaGAN-8TPU setting which employs 8 TPU.\nWe observe that ParaGAN outperforms both the native TensorFlow and StudioGAN with 8 GPUs. We conjecture that the performance gain on the GPU setting is mainly attributed to the use of congestion-aware data pipeline and hardware-aware layout transformations. We also observe that the performance gap is further pronounced when switching to the TPU as the accelerator. The following evaluations use the TPU as the accelerator unless otherwise specified."}, {"title": "6.3 Scaling Experiments", "content": "We will discuss the strong and weak scaling results in this section. In the strong scaling experiments, we keep the total workload constant and vary the number of workers to examine the speedup on time-to-solution. Whereas in the weak scaling experiments, we keep the per worker workload (batch size per worker) constant and increase the number of workers."}, {"title": "6.3.1 Strong Scaling", "content": "For strong scaling experiments, we fix the total batch size to be 512 and train for 150k steps as target workload. Note that to be consistent with other experiments, we train on BigGAN at 128 \u00d7 128 resolution, which is smaller than the model trained in Fig. 1. We aim to study the effect of decreased per-worker workload when scaling."}, {"title": "6.3.2 Weak Scaling", "content": "In the weak scaling experiments, we fix the batch size per worker and evaluate the performance of our framework by increasing the number of workers. Firstly, we find the largest batch size for a single accelerator that does not lead to out-of-memory error. Then, we scale the total batch size proportionally concerning the number of workers. Therefore, the amount of workload per worker is kept identical. The weak scaling experiments examine how"}, {"title": "6.4 Accelerator Utilization", "content": "The basic computing unit of TPU is MXU (matrix-multiply unit). The utilization of MXU measures the time MXU is being occupied, and higher utilization is more desirable.\nWe compare the accelerator utilization of BigGAN 128x128 on baseline [23] and ParaGAN. Fig. 10 shows that ParaGAN outperforms native implementation with higher MXU utilization across different TPU configurations. We wish to highlight that even 2% improvement can be important when scaling to thousands of workers.\nIt is also worth noting that, with an increasing number of accelerators, the amount of communication increases, but ParaGAN can maintain a relatively higher utilization than"}, {"title": "6.5 Ablation Study", "content": "We present the ablation study for the system optimizations in Table 2. For the numerical optimizations, since the convergence also depends on the underlying GAN architecture and datasets, we leave them to be decided by the users. Results in Table 2 are collected on BigGAN trained on ImageNet at 128x128 resolution on 128 TPUv3 accelerators, under the same batch size (2048) from the original paper [3].\nData pipeline provides 8-15% performance improvement over the baseline. When the number of accelerators increases, network jitter caused by congestion is more likely to happen, making data loading the weakest point in the training process. In ParaGAN, we try to saturate the accelerators by dynamically adjusting the buffer budget for the data pipeline. This is generally applicable, and ParaGAN enables this feature by default.\nWe compare the performance of our congestion-aware pipeline with TensorFlow's native data pipeline (tf.data). To ensure the results are comparable, they are run at the same time on the same type of machine pointing to the same dataset storage node, and latency is measured at the time taken to extract a batch of data. As shown in Fig. 11, our pipeline tuner has a lower variance in latency.\nLow-precision (with bf16) training provides additional 14-17% performance gain and reduces TPU memory usage by 24%. Bfloat16 format saves activation values in lower bits, which makes it faster to load from memory and communicate with other workers. We have observed while converting activation to lower precision does not harm convergence,"}, {"title": "6.6 Generating High-Resolution Images", "content": "To our knowledge, we are the first to successfully train BigGAN at 1024 \u00d7 1024 resolution, which is 4 times higher than the result shown in the original BigGAN. Training at high resolution is particularly challenging because the generator will need to use more channels and deconvolutional layers to generate more details. It is therefore more sensitive to hyperparameters and initialization. Different from ProgressGAN [16] where they use progressive growing to train low-resolution images first before increasing the resolution, we directly train it on 1024 \u00d7 1024 resolution, which is more challenging, and it requires the numerical optimization techniques we discussed."}, {"title": "6.7 Convergence of Async. Update Scheme", "content": "We study the convergence behavior of the asynchronous update scheme on SNGAN and present in Fig. 13. We observe that the asynchronous update scheme can accelerate convergence on smaller datasets, and the benefit is more obvious in the early stage of training. However, the asynchronous update scheme (Async G-512 D-256) struggles to converge on high-resolution image generation tasks despite reaching lower FID quicker than the synchronous versions before 16K steps. Throughout repeated experiments and other models, we observe similar results. We conjecture that the asynchronous update scheme may be used to accelerate the earlier stage of training before switching to the synchronous update scheme for better convergence.\nWe recognize that theoretical convergence analysis in the context of GAN training is an active area of research [2, 11, 18, 24, 28], and is particularly challenging. As GANs operate on a two-party optimization problem, their convergence is inherently difficult to guarantee, especially when using stochastic gradient descent. Furthermore, introducing asynchrony, while beneficial in terms of performance, adds another layer of complexity in analyzing convergence and stability. While we have relied on empirical results to"}, {"title": "7 RELATED WORK", "content": "Existing works on distributed training for GAN are relatively limited with a focus on the privacy-preserving perspective. Since GAN consists of two sub-networks, a common approach is to let multiple discriminators serve one centralized generator using parameter server [4, 10, 29]. The central generator sends synthesized output images to the distributed discriminators, and the discriminators update the generator with their predictions. FeGAN [9] instead deploys a complete GAN on each device to address the data skewness and mode collapse issue. AI-GAN [14] tackles the image-deraining problem by using a two-branch network that learns a disentangled representation for the rain and background, and jointly optimizes the two branches via mutual adversarial optimization. [34] adopts the federated learning setting on the medical image generation problem on low-resolution images. [35] proposes a PDE-informed GAN architecture for subsurface flow characterization problem, achieving 93.5% scaling efficiency on 27500 GPUs with communication and"}, {"title": "8 CONCLUSION", "content": "ParaGAN is a large-scale distributed GAN training framework that supports high-resolution image generation with near-linear scalability. ParaGAN is optimized with a dynamic data pipeline, mixed-precision training, and layout transformation. We show that it is possible to train the generator and discriminator independently using an asynchronous update scheme and asymmetric optimization policy. ParaGAN scales almost optimally to 1024 accelerators, and it can greatly reduce the time to train a GAN model from weeks to hours. We believe ParaGAN can advance GAN research by accelerating the training process."}]}