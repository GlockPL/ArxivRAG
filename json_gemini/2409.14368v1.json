{"title": "Evaluating the Quality of Code Comments Generated by Large Language Models for Novice Programmers", "authors": ["Aysa Xuemo Fan", "Arun Balajiee Lekshmi Narayanan", "Mohammad Hassany", "Jiaze Ke"], "abstract": "Large Language Models (LLMs) show promise in generating code comments for novice programmers, but their educational effectiveness remains under-evaluated. This study assesses the instructional quality of code comments produced by GPT-4, GPT-3.5-Turbo, and Llama2, compared to expert-developed comments, focusing on their suitability for novices. Analyzing a dataset of \"easy\" level Java solutions from LeetCode, we find that GPT-4 exhibits comparable quality to expert comments in aspects critical for beginners, such as clarity, beginner-friendliness, concept elucidation, and step-by-step guidance. GPT-4 outperforms Llama2 in discussing complexity (chi-square = 11.40, p = 0.001) and is perceived as significantly more supportive for beginners than GPT-3.5 and Llama2 with Mann-Whitney U-statistics = 300.5 and 322.5, p = 0.0017 and 0.0003). This study highlights the potential of LLMs for generating code comments tailored to novice programmers.", "sections": [{"title": "1. INTRODUCTION", "content": "As the field of artificial intelligence (AI) evolves, the integration of Large Language Models (LLMs) into educational contexts is gaining interest [17, 1, 12], particularly in the realm of computer science education for novice programmers [6, 8]. LLMs show promise in generating code comments, a valuable resource for novice programmers grappling with complex programming concepts [8, 5, 9]. Recent works by Cui et al. [2], Codex\u00b9, and Madaan et al. [10] explore the development of code comments using docstrings as training material for AI models, highlighting the potential of LLMS in supporting novice programmers.\nWhile studies by Cui et al. [3] and Madaan et al. [11] demonstrate the feasibility of using LLMs to generate code comments, the educational meaningfulness and effectiveness of these comments for novice programmers have been under-evaluated. Leinonen et al. [5] emphasize the need for further research on the quality and instructional value of LLM-generated comments for novice programmers compared to human-crafted ones, stressing the importance of assessing their impact on learning outcomes and student engagement. While MacNeil et al. [9] emphasize the importance of incorporating end-user insights, our study focuses on expert evaluations of LLM-generated comments for novice programmers. Future research will aim to include direct feedback from novice users to further validate these findings. Understanding the performances, strengths and weaknesses of LLM-generated comments and how they compare to expert-developed comments can contribute to refining educational approaches in programming, particularly for novice programmers. While [9] suggest the importance of user studies with novice programmers, our current study focuses on expert evaluation of LLM-generated comments. Future work will incorporate direct feedback from novice programmers.\nTo address this, our study focuses on evaluating the instructional quality of code comments produced by LLMs for novice programmers. We developed a codebook for this analysis, using predefined criteria to systematically evaluate the instructional quality of the comments for novice programmers. The study utilizes a dataset from LeetCode\u00b2, a well-known competitive programming platform. The dataset includes Java programming solutions, which serve as input for the LLMs to generate comments tailored to novice programmers. The inline documentation provided by experienced programmers in the LeetCode solutions is used as a benchmark for comparing the quality of LLM-generated comments for novice programmers.\nWe aim to answer three research questions:\n1. How do LLM-generated code comments compare to expert-developed comments based on evaluation criteria designed"}, {"title": "2. METHODS", "content": "2.1 Dataset Selection and Prompting\nOur study's methodology is specifically designed to evaluate the effectiveness of LLM-generated code comments for novice programmers. To this end, we have strategically selected the LeetCode Java Solution dataset\u00b3 which comprises solutions to 30 \"easy\" level problems. This choice is intentional; the \"easy\" level categorization aligns with the introductory level of computer science education and offers a range of concepts that are suitable and accessible for beginner-level programmers. By focusing on simpler coding problems, our study aims to provide insights into the helpfulness of LLM-based comments in fostering understanding and learning in an intro-to-cs context. This approach ensures that the complexity of the problems does not overshadow the educational potential of the comments provided by the language models. The LLMs were prompted to explain existing code solutions, simulating a scenario where a novice programmer seeks explanation of a correct solution and it explicitely asked for the complexity, overview and step-by-step comments.\n2.2 Model Selection\nWe adapted the prompt structure, and also followed the prompt engineering methodology from [7], structuring prompts to ensure compatibility with various LLMs and alignment with expert-level commentary. Three advanced LLMs were chosen: GPT-4 [13], Llama2 [15], and GPT-3.5-Turbo [18]. These models were selected for their demonstrated performance in similar tasks and their advanced language understanding abilities [16, 18]. The full prompts is in the Appendix. In the 2 phases of evaluation, 4 experts with at least 3 years' experience in programming and Computer Science Education to design the codebook, and also to rate the LLM-generated comments against the codebook criteria.\n2.3 Codebook Development and First Round Evaluation\nTo assess the overall quality of LLM-generated code comments, we developed a comprehensive codebook using criteria derived from [4] based on eight essential criteria (Shown in Appendix). These criteria were carefully crafted to capture the key aspects of instructional quality in code comments, ensuring that the evaluation process is both rigorous and pedagogically relevant. This initial assessment provided a broad overview of the quality of the comments generated by different models. To gain deeper insights into the performance of the models, we conducted a statistical analysis of a 5-point Likert scale ratings, calculating the mean, standard deviation, and correlations among the criteria. Evaluations were conducted blind to ensure unbiased assessment.\n2.4 Second Round Expert Evaluation\nBuilding upon the findings from the first phase, we refined our evaluation approach in the second phase to include a set of binary criteria and a qualitative measure. The binary criteria were designed to be more focused and specific compared to the broader, more subjective Likert scale ratings. By concentrating on particular aspects of explanation and education, these criteria (see Appendix) allowed for a clearer differentiation between the capabilities of different models, providing a more nuanced understanding of their strengths and weaknesses in educational contexts.\nTo enhance the precision of our assessment, we removed the criterion \"Provides Enough Detail for Comprehension\" and introduced several new binary criteria that better captured the specific aspects of effective code comments. These criteria included \"Connects programming concepts to problem context,\" \"Consistent style and grammar,\" and \"Plain language,\" among others. Additionally, we introduced a qualitative measure, \"Functions as a friendly tutor,\" to evaluate the ability of the LLM-generated comments to engage and support learners in a manner similar to a human tutor.\nThe second round of expert evaluations was conducted using these refined criteria, with a blind rating process and both paired and global Likert ratings to ensure objectivity. A Chi-square analysis was performed on the binary criteria to gain further insights into the efficacy of LLM-generated code comments. While we refined the evaluation criteria after an initial review of responses to ensure relevance, we acknowledge the risk of overfitting. Nevertheless, this analysis allowed us to identify statistically significant differences"}, {"title": "3. ANALYSIS AND RESULTS", "content": "3.1 Statistical Analysis\nOur analysis utilized the mean values (shown in Table 1 associated with Kruskal-Wallis H test [14] (See Appendix) to compare the performance of different language models (LLMs): GPT-4, GPT-3.5, Llama 2, and human experts. This test helped us identify significant differences across various evaluation criteria, providing insights into the strengths and weaknesses of each model in generating code comments. We specifically chose Kruskal-Wallis H test because our data is neither normal distributed nor parametric [14].\nClarity and Beginner-Friendliness: The results revealed that GPT-4 and human experts generally exhibited superior performance in clarity and beginner-friendliness. Both models scored consistently high across criteria such as avoiding jargon and simplifying vocabulary, making their comments more accessible to novices. Notably, GPT-4 outperformed Llama 2 in explaining concepts used, with significant p-values indicating its effectiveness in making complex ideas more comprehensible. Conversely, Llama 2 struggled in these areas, often failing to match the performance of its counterparts.\nConcept Elucidation and Step-by-Step Guidance: In terms of concept elucidation, GPT-4 demonstrated an exceptional ability to clarify complex code behavior, closely followed by human experts. Both were proficient in breaking down tasks into manageable steps, facilitating better understanding for beginners. GPT-3.5, while slightly lagging behind GPT-4, still performed adequately and did not show significant statistical differences compared to the human expert, suggesting a strong capability in detailed explanation.\nComparative Performance Across Models: Llama 2's performance was notably weaker, particularly in areas requiring detailed comments and the use of simple vocabulary. Statistical tests consistently showed Llama 2 as underperforming compared to GPT-3.5, GPT-4, and human experts, with low scores in breaking down tasks and clarifying code behavior, which are crucial for instructional quality.\nThese findings underscore the potential of advanced LLMs like GPT-4 to provide high-quality educational content that rivals and occasionally exceeds human expert comments. However, the variability among LLMs highlights the need for targeted improvements in models like Llama 2, especially in terms of beginner-friendliness and step-by-step guidance.\n3.2 Chi Square Analysis\nThe chi-square analysis was employed to assess the statistical significance of differences in performance between GPT-4 and other models, including Llama2, GPT-3.5, and human experts across various evaluation criteria. This section provides a detailed interpretation of the chi-square statistics, p-values, and degrees of freedom, which are consistently set at 1, reflecting the comparison between two groups per criterion. All the significant results of Chi-square analysis are shown in Table 2.\nIn the analysis of chi-square results across various comparative criteria, significant findings were observed that highlight differences in performance between GPT-4, Llama2, and GPT-3.5, as well as between these models and human experts. Among these, a notable outcome was the comparison between GPT-4 and Llama2 in discussing complexity, where GPT-4 demonstrated superior capability with a chi-square statistic of 11.40 and a p-value of 0.001. Similarly, GPT-4 showed a better ability to connect concepts to context than Llama2, indicated by a chi-square statistic of 6.42 and a p-value of 0.011.\nNo significant differences were observed between GPT-4 and human experts, suggesting that GPT-4's performance is on par with human levels within the bounds of statistical significance. This parity is an indication of GPT-4's advanced capabilities in mimicking human reasoning and explanation, which are critical in educational and professional settings. Conversely, GPT-3.5 demonstrated limitations compared to human experts, particularly in providing high-level overviews, where it lagged behind with a chi-square statistic of 7.66 and a p-value of 0.006. This suggests a potential area for improvement in GPT-3.5's ability to synthesize and summarize information effectively.\nMoreover, significant differences in discussing complexity were also observed between GPT-3.5 and Llama2, with GPT-3.5 underperforming (chi-square statistic of 5.10, p-value of 0.024), and between human experts and Llama2, where human experts excelled significantly (chi-square statistic of 14.40, p-value of 0.0001). These results underscore the variability in Al models' ability to handle complex analytical tasks compared to human experts, highlighting the necessity for ongoing enhancements in AI training methodologies to bridge these gaps.\n3.3 Friendliness Analysis\nFurthermore, we introduced the friendly_tutor criterion to qualitatively evaluate the LLM generated explanation's friendliness as a tutor, aiming to gather feedback from novice programming students. This criterion assesses how effectively these comments engage and support learners, fostering a"}, {"title": "4. CONCLUSION & DISCUSSION", "content": "This study evaluated the instructional quality of code comments generated by Large Language Models (LLMs) compared to those developed by human experts, focusing on their application in novice programming education.\nRQ 1: Generation Quality Our analysis, utilizing criteria such as clarity, beginner-friendliness, and step-by-step guidance, indicates that comments from GPT-4 are often comparable to those developed by experts. Specifically, GPT-4 excels in clarity and beginner-friendliness, closely aligning with expert comments in terms of educational value.\nRQ 2: Strengths and Weaknesses The strengths of LLM-generated comments, particularly from GPT-4, include effective concept elucidation and comprehensive step-by-step guidance that facilitate understanding complex programming concepts. However, weaknesses were observed in models like Llama 2, which struggled with maintaining simplicity in vocabulary and providing sufficiently detailed comments for complete beginner comprehension.\nRQ 3: For Novice Programmers Considering criteria critical to novice programmers\u2014such as explanation clarity, ability to identify and correct beginner mistakes, and the provision of detailed, jargon-free instructions\u2014GPT-4 aligns more closely with the instructional needs of novice programmers than comments from other LLMs and closely matches the effectiveness of human experts. While GPT-4 shows strong performance in clarity and beginner-friendliness compared to GPT-3.5 and LLaMA-2, these findings are based on a limited dataset. Further research with a broader range of models and a larger dataset is needed to confirm its suitability for generating educational content for beginners.\nIn conclusion, while some LLMs like GPT-4 show potential to rival or even surpass human experts in certain aspects of code explanation, the performance variability among different LLMs underscores the need for ongoing improvements and customizations tailored to the specific educational contexts and needs of novice programmers. The findings of this study advocate for a nuanced application of LLMs in educational tools, ensuring they are leveraged in ways that genuinely enhance learning outcomes."}, {"title": "5. LIMITATIONS", "content": "It is important to acknowledge the limitations of this study, including the limited dataset, the subjectivity of expert evaluations, and the absence of user studies with novice programmers. Future research should focus on expanding the dataset, refining evaluation criteria, conducting user studies, exploring multimodal comments, and comparing LLMs with other AI-driven approaches to develop robust and effective tools for enhancing computer science education.\nDespite these limitations, this study serves as a preliminary investigation into the potential of LLMs for generating code comments and lays the groundwork for future research in this area. By addressing the identified limitations and expanding the scope of the study, researchers can gain a more comprehensive understanding of how LLMs can be leveraged to support novice programmers in their learning journey, ultimately contributing to the development of effective AI-driven educational resources in computer science education."}]}