{"title": "Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach", "authors": ["R\u00e9gnier Avice", "Bernhard Haslhofer", "Zhidong Li", "Jianlong Zhou"], "abstract": "Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in Fl-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve Fl-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.", "sections": [{"title": "1 Introduction", "content": "Attribution tags, which link pseudo-anonymous cryptoasset addresses to identifying information about real-world entities and services (e.g., cryptoasset exchanges), form the foundation of modern cryptoasset forensics [26]. Over the past decade, a multibillion-dollar industry has emerged, providing blockchain tracing tools for law enforcement investigations. These tools allow users to \"follow the money\", ultimately leading to the identification and potential conviction of perpetrators. As cryptoassets have gained increasing relevance across various crime sectors, these tools are now used in investigations related to ransomware [19, 30], sextortion [31], and malware [13]. Attribution tags have also"}, {"title": "2 Background", "content": null}, {"title": "2.1 Attribution Tags", "content": "Attribution tags link pseudo-anonymous blockchain objects, such as addresses or transactions, to real-world actors or events. They provide additional context, such as the name of a service controlling an address (e.g., btc-e), some form of categorization (e.g., exchange), and any other information that might be useful in forensic investigations. Figure 1 illustrates an example in which two distinct attribution tags originating from different sources reference the same cryptoasset address 0x123 and describe the same real-world actor, a well-known cryptoasset exchange. One can observe that, despite describing the same entity, the attribution tag data records the name (btc-e vs. btc-e.com) and categorize the exchange differently (Exchange vs. Service).\nAttribution tag data quality issues can occur at various levels [16]: technical heterogeneities, such as different data formats, can impede uniform processing; syntactic heterogeneities, like the use of different encoding schemes, can hinder uniform interpretation; and semantic heterogeneities, such as the use of different names to denote the same real-world concept (synonyms, homonyms, hypernyms, etc.), can lead to inconsistent interpretations. In this paper, we primarily focus on resolving semantic interoperability issues, assuming that technical and syntactic issues can be addressed using common data preprocessing procedures.\nIntroducing knowledge graphs to data management environments has become a common strategy to deal with semantic interoperability issues. A knowledge graph defines nodes representing real-world entities of interest and semantic re-lationships between these entities [17]. An example is eBay's product knowledge"}, {"title": "2.2 Related Work", "content": "Entity linking approaches where tuples are linked to knowledge base entities have utilized look-up methods [34], embedding comparisons [7], and hybrid approaches [8]. Record-based entity linking has seen advancements using classical machine learning [20], deep neural networks [27], and pre-trained language models [23]. In [38], a mixture-of-experts approach was proposed, utilizing the training results from various data integration matching tasks. For blocking, many state-of-the-art approaches use deep learning [2, 23, 37, 40]. A simple tf-idf based blocker can achieve competitive results without training and labeled data as shown in [32].\nMore recently, researchers started to examine LLMs on data integration tasks. Studies have evaluated and proposed various models, including GPT-3 [28], Jel-lyfish variants [43], and others [33], on tasks such as entity linkage, data imputa-"}, {"title": "3 Data", "content": "In this section, we present the data that informed the design and implementation of our approach and was used throughout our experiments."}, {"title": "3.1 GraphSense TagPacks", "content": "GraphSense is an open-source cryptoasset analytics platform that provides a curated collection of over 500,000 publicly available attribution tags. A TagPack is a data structure used to package and share attribution tags 3. Each tag cor-responds to a blockhain address and includes a label, several optional fields, and categorization information. The categories are drawn from a subset of the INTERPOL Dark Web and Virtual Assets Taxonomy (DWVA)4, a community-driven effort to define common forms of abuse and entities representing real-world actors and services within the broader Darknet and Cryptoasset ecosystems.\nIn addition, GraphSense provides a curated list of 2,862 actors, each repre-senting a well-defined real-world entity within the cryptoasset ecosystem. These actors encompass a wide range of roles and types in the industry such as cen-tralized exchanges (e.g., Binance), decentralized finance platforms (e.g., Aave), and mixing services (e.g., Tornado Cash). Each actor is assigned a unique ID, a label, one or more categories from the DWVA taxonomy, and optional fields such as a URL or jurisdiction. This enables GraphSense to partially implement a knowledge graph, offering explicit links between entities, which we use as a ground-truth dataset. In total, 378,550 attribution tags contain such actor links.\nSince many attribution tags share the same labels and actor links and we are only interested in unique records, we filtered out duplicates, leaving 2,570 unique linked attribution tags. These were split into training, validation, and test datasets in a 1:30:69 ratio. The training set was used to create few-shot examples, while the validation set was employed in experiments 1 and 2 to optimize the individual components. The test set was exclusively used in experiment 3 to evaluate our approach end-to-end."}, {"title": "3.2 Watch YourBack Attribution Tags", "content": "We utilize the dataset published in [13]. Structurally, these attribution tags are similar to the GraphSense TagPack, as they also include categories and subcat-egories. However, their taxonomy is not harmonized with the GraphSense Actor Taxonomy.\nThe dataset consists of tags aggregated from various sources, including Graph-Sense and other shared datasets. To avoid duplicating tags, we filter out any that are linked to addresses already present in the GraphSense TagPack database. From this filtered set, we manually selected and annotated 126 records, 67 of which contain an actor link."}, {"title": "3.3 DeFi Rekt Database", "content": "The DeFi Rekt database [6] contains over 3,500 events related to crimes involving cryptoassets. Many of these events involve actors within the ecosystem. Each event includes a title and optional fields such as the date of the event, and funds involved. For our experiments, we randomly sampled 100 records from events where the loss of funds exceeded 100,000 USD. We manually annotated these sampled events with actor links. Out of the 100 sampled records, only 32 contained an actor link."}, {"title": "3.4 Dataset summary", "content": "Table 1 summarizes the data we used for design and implementation of our approach and the experiments we conducted. The total number of distinct actors is lower than the sum of individual datasets due to overlapping actors between datasets."}, {"title": "4 Approach", "content": "Our approach for linking attribution tags to knowledge graph concepts comprises two main components, as illustrated in Figure 3: the candidate set generator and"}, {"title": "4.1 Candidate Set Generator", "content": "The goal of the candidate set generator is to reduce the pool of candidates in the knowledge graph that potentially match a given attribution tag. Since LLM inference is expensive and pairwise comparison is of order $O(nm)$, where n is the number of records and m is the number of entities in the knowledge graph, an approach that minimizes the comparisons is required.\nTo reduce the overall cost, we limit the entities in each prompt to k candidates by applying filtering and blocking techniques, thereby reducing the problem com-plexity to $O(nk)$, where k < m. Filtering is the process of eliminating incorrect candidates, while blocking refers to similarity-based clustering that identifies likely and unlikely candidates [29].\nFiltering The candidate set can be narrowed down when both the attribution tag and the corresponding knowledge graph entity are associated with cate-gorization information from the same controlled vocabulary. For example, an attribution tag might be categorized as \"exchange\" and a knowledge graph en-tity as \"service\", where the latter represents a semantically broader concept than the former. When such information is available, as in the GraphSense TagPack dataset (see Section 3.1), it can be leveraged for filtering. We define two possible filtering methods:\nSame-Concept Filtering: This method excludes actors that belong to a cat-egory different from the one specified in the attribution tag."}, {"title": "4.2 Candidate Selector", "content": "The candidate selector module takes a set of candidates for each attribution tag and selects the best matching entity. Technically, this step is implemented using an LLM. In the first stage, the module constructs a batch of prompts, where each prompt corresponds to an attribution tag and includes all associated candidates. Optionally, the prompts can include examples (few-shot prompts) showcasing both matching and non-matching cases. This prompt batch is then fed to the LLM, which is tasked with either selecting the candidate that best"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Setup", "content": "Models The candidate selector module (see Section 4.2) leverages LLMs, inte-grating both remote models via the OpenAI API and locally hosted models. For our experiments, we utilized two remote and six local models, all compatible with consumer-grade hardware:\nGPT-40, GPT-3.5 Turbo: These models, hosted by OpenAI, were employed in their specific versions: gpt-3.5-turbo-0125 and gpt-40-2024-05-13.\nJellyfish-7B, Jellyfish-13B-AWQ: Developed and fine-tuned for data pre-processing tasks [43], the Jellyfish models include a 7B version, based on Mistral-7B-Instruct-v0.2, and a 13B version, based on OpenOrca-Platypus2-13B. The 13B model was quantized to fit on consumer-grade GPUs using Activation-aware Weight Quantization (AWQ) [24]."}, {"title": "5.2 Experiment 1: Candidate Set Generation", "content": "The goal of this experiment is to evaluate different filtering and blocking tech-niques (described in Section 4.1) and find an appropriate candidate set size. We run the experiment on the GraphSense Tag Pack validation dataset on candidate"}, {"title": "5.3 Experiment 2: Candidate Selection", "content": "The goal of our second experiment is to assess the performance of different LLM-based candidate selectors using various prompt template configurations (see Table 2) and few-shot examples. We run the experiment on the GraphSense"}, {"title": "5.4 Experiment 3: End to End Entity Linking", "content": "The goal of this experiment is to test our approach of linking attribution tags to a knowledge graph end-to-end and compare it to baseline solutions. We run the experiment on the GraphSense TagPack test set, Watch YourBack, and Defi Rekt datasets. All samples pass through the candidate generator, and the resulting batch of candidate sets are then fed to the candidate selector. For the candi-date set generator, we employ the BM253 blocker. Additionally, we apply the related-concept filtering for the GraphSense dataset, while no filtering is used for the other datasets, because their categories are not linked to knowledge graph concepts. For the candidate selector, we use for each LLM the template that achieved the best performance in the previous experiment. We follow experi-ment 2's evaluation method, with the difference that a miss by the candidate set generator is counted as an error. We compare the LLMs with the following baseline methods:\nBM253: We use the top ranking candidate of our BM253 blocker, and decide based on a threshold if it is a match or not. The threshold of 15.7238 was determined by evaluating the model's precision and recall on all top-candidate scores in the GraphSense TagPack validation set, and selecting the one that maximizes the Fl-score.\nUnicornPlus, UnicornPlusFT: UnicornPlus [38] is a DeBERTa-based mixture-of-expert model that is fine-tuned for data integration matching tasks. For a fair comparison, we apply the same candidate set generation process and perform pairwise matching between attribution tags and their candidates. In case mul-tiple candidates match, we apply the softmax function on each prediction and choose the one with the highest probability. Furthermore, we create Unicorn-PlusFT, a fine-tuned version of the model. For this, we reshuffle our GraphSense train and validation sets with a 80/20 training/validation split and train the model for 10 epochs using the settings proposed in [38].\nTable 6 demonstrates that our end-to-end linking approach outperforms base-line methods across all datasets. Using BM253 blocking and GPT-40 as candi-date selector, we achieve Fl-scores of 79-85%. With Mistral 7B-Instruct, our"}, {"title": "6 Discussion and Conclusions", "content": "In this paper, we addressed the issue of attribution tag quality, with a particular focus on data inconsistencies that arise when attribution tags are shared among different parties. We argue that data quality issues can mislead forensic investi-gations and even result in false convictions if addresses are labeled incorrectly. To solve this, we proposed a novel computational approach based on Large Lan-guage Models (LLMs) that automatically links attribution tags to well-defined concepts in knowledge graphs, addressing the semantic nature of the problem. We implemented our approach in an end-to-end pipeline and demonstrated that, when combined with filtering and blocking techniques, it outperforms existing methods. Additionally, we showed that pre-trained LLMs running locally on consumer-grade hardware achieve performance comparable to remote models. Furthermore, we demonstrated that carefully designed prompts can significantly reduce costs with only a marginal decrease in performance. Overall, we believe our approach not only addresses the pressing issue of inconsistent attribution tags but also has the potential to inspire broader efforts to improve data quality in other forensic investigation tools and platforms.\nOne limitation of our approach is its binding to specific application domains; so far, we lack evidence that our method is generally applicable to all record linkage problems. However, we believe that a data- and measurement-driven ap-proach would be valuable for assessing the broader suitability of this method. Another limitation is the assumption that different parties (e.g., exchanges, in-vestigators) use the same knowledge graph when exchanging attribution tag"}, {"title": "Appendix 1", "content": "A visual illustration of the candidate set generation performances of the different blocking and filtering techniques can be found in Figure 6. We can see that the recall gain from k = 1 to k = 5 is significant, while subsequent increases in candidate set size have only a marginal impact."}, {"title": "Appendix 2", "content": "A comprehensive overview of all candidate selector model results across each template can be found in Figure 7. We can see that the zero-shot performance of the models varies significantly across templates; however, introducing five examples reduces this variance considerably. Interestingly, while GPT-3.5 is in-effective without examples, with five examples it achieves the second-highest F1-score. Also notable is that the zero-shot performance for templates 4, 8, and 9 is lowest, signaling that the output format reminder (OUT) is crucial for good results without examples."}, {"title": "Appendix 3", "content": "In our experiments, we treat entity linking as a multi-classification problem and measure the performance with the macro F1 score and the accuracy, see Table 7. For the macro F1-score, this means that we calculate each entity's individual F1 score and average them. Consequently, any prediction that is different from the ground truth is treated equally as an error. In a practical setting, we argue that errors predicting no entity matches are less problematic than errors linking the attribution tag to the wrong entity.\nFigure 8 shows us the error composition of the different models across all three datasets. We distinguish between the Missed Entity error from a wrong no-match prediction and the Wrong Entity error with a wrong entity predicted. We can see that GPT-40 not only has the best performance in terms of accuracy and macro F1-score but also has less than 1% of wrong entities predicted, with the majority of the errors coming from missed entities. In contrast, the local LLMs all have more Wrong Entity than Missed Entity errors. Our best-performing local LLM, Mistral 7B-Instruct, has, despite better overall accuracy, more Wrong Entity errors than GPT-3.5.\nIn Table 8, we identify the three actors most frequently misclassified by each model. The results are similar across different models, with a few exceptions.\nFor example, maker appears in the top three only for UnicornPlus, despite being present in 135 different tags. Notably, none of the models correctly classify any of the 20 tags associated with Ox as an actor."}]}