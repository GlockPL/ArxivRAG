{"title": "Al Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment", "authors": ["Nuo Chen", "Jiqun Liu", "Xiaoyu Dong", "Qijiong Liu", "Tetsuya Sakai", "Xiao-Ming Wu"], "abstract": "Cognitive biases are systematic deviations in thinking that lead to irrational judgments and problematic decision-making, extensively studied across various fields. Recently, large language models (LLMs) have shown advanced understanding capabilities but may inherit human biases from their training data. While social biases in LLMs have been well-studied, cognitive biases have received less attention, with existing research focusing on specific scenarios. The broader impact of cognitive biases on LLMs in various decision-making contexts remains underexplored. We investigated whether LLMs are influenced by the threshold priming effect in relevance judgments, a core task and widely-discussed research topic in the Information Retrieval (IR) coummunity. The priming effect occurs when exposure to certain stimuli unconsciously affects subsequent behavior and decisions. Our experiment employed 10 topics from the TREC 2019 Deep Learning passage track collection, and tested Al judgments under different document relevance scores, batch lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B. Results showed that LLMs tend to give lower scores to later documents if earlier ones have high relevance, and vice versa, regardless of the combination and model used. Our finding demonstrates that LLM's judgments, similar to human judgments, are also influenced by threshold priming biases, and suggests that researchers and system engineers should take into account potential human-like cognitive biases in designing, evaluating, and auditing LLMs in IR tasks and beyond.", "sections": [{"title": "1 INTRODUCTION", "content": "Cognitive Bias refers to a systematic pattern of deviations in thinking [25], which can lead to irrational judgments and problematic decision-making [49, 51]. This phenomenon has been extensively studied by researchers in fields such as psychology, economics, management, and information science [e.g. 5, 14, 18, 27, 41].\nIn recent years, Large language models (LLMs) have demonstrated advanced natural language understanding capabilities and are increasingly used to support decisions in various tasks [28, 40]. Nevertheless, having been trained on human-generated data, LLMs may inherit human biases, such as social biases [e.g. 1, 4, 39, 61], and cognitive biases [e.g. 12, 14, 20, 42]. Such biases ingrained in their knowledge can be propagated and amplified during human-AI interactions, and consequently, researchers are increasingly concerned with identifying and mitigating biases in LLMs. However, current researches predominantly examine the social biases of LLMs, with cognitive biases receiving comparatively less attention. While some existing work [11, 12, 20, 42] has investigated cognitive bias in LLMs during decision-making, these studies have primarily focused on specific types of decisions, such as university admissions [11] or medical question answering [42], and have only explored a subset of cognitive biases, with the impact of various cognitive biases on LLMs under different types of decision-making scenario remains a critical and promising area awaiting scientific exploration.\nIn this study, inspired by previous work [e.g. 13, 43] in the Information Retrieval (IR) community, we investigate whether LLMs are influenced by the priming effect when making relevance judgments across multiple documents simultaneously. Given that the relevance labels obtained are often used for training IR algorithms and ranking IR systems, relevance labels influenced by cognitive biases could potentially lead to biased algorithms and suboptimal IR systems. Consequently, the IR community has conducted research on how cognitive biases of human assessors, such as the priming effect [43], the anchoring effect [45], the decoy effect [13], affect relevance judgment outcomes. The priming effect is a psychological phenomenon where exposure to certain stimuli (such as words, images, or concepts) influences an individual's subsequent behavior, judgments, and decisions unconsciously [17, 48, 53]. To address this issue, in this study, we propose and seek to answer two research questions (RQs) with our experiments:\n\u2022 RQ1: To what extent does the relevance level of earlier documents in a batch influence the relevance judgments of subsequent documents (i.e. threshold priming) when LLMs make batch relevance assessments?\n\u2022 RQ2: How does the impact of threshold priming in LLMs vary across different topics when making batch relevance judgments of documents?\nTo answer the above RQs, we conducted an experiment on 10 topics from the TREC 2019 Deep Learning (TRDL19) passage retrieval track collection [10], with 20 trials for each topic."}, {"title": "2 RELATED WORK", "content": "Research in cognitive psychology and behavioral economics indicates that cognitive biases emerge from the limitations of human cognitive capacity, particularly when there are insufficient resources to adequately gather and process available information [25]. Cognitive biases can cause an individual's decisions in uncertain situations to systematically deviate from the expectations of rational decision-making models [49\u201351]. Typical cognitive biases include the anchoring effect [49], the decoy effect [19], the reference dependent effect [22], the confirmation bias [44], and the priming effect [53].\nIn this study, we shed light on the priming effect, which is a psychological phenomenon where exposure to certain stimuli (such as words, images, or concepts) influences an individual's subsequent behavior, judgments, and decisions unconsciously [17, 48, 53].\n2.1 Cognitive Biases in the LLMs\nAlthough LLMs do not possess human physiological structures, they may learn cognitive biases expressed in textual form from data and human feedback during the training or fine-tuning process [20]. Current research predominantly examines the social biases of LLMs, with cognitive biases receiving comparatively less attention. There has been work studying the anchoring effect [11], the confirmation bias [42], the selection bias [12], and the decoy effect [20] in LLMs, but there are still many types of cognitive biases awaiting exploration, such as the priming effect. Furthermore, existing research on cognitive biases in LLMs is limited to specific scenarios, such as university admission decisions [11] or medical question answering [42], and how cognitive biases affect LLMs' decision-making in other scenarios remains unknown. In this study, we investigate whether LLMs are influenced by the priming effect when making search decisions such as relevance judgement in place of humans.\n2.2 Cognitive Biases in Human Search Process\nHumans need to seek information to support various decisions, ranging from everyday decisions such as choosing a good Chinese restaurant in Flushing to major decisions such as deciding whether to undergo a surgery to remove a tumor. In the process of information seeking and interacting with information retrieval systems, individual users also often engage in various decision-making processes, such as, whether to continue browsing the Search Engine Result Page (SERP) or to end the current search [6, 57, 59], determining whether a document is useful in meeting their information needs [35, 36], and evaluating the interaction experience with an search system [21, 33, 46]. Therefore, cognitive biases have been studied in a variety of works in the field of Information Retrieval (IR) [2, 29]. For instance, how the recency effect and the reference-dependent effect influence users' perceptions of search satisfaction [30, 34]; and how the anchoring effect, decoy effect, expectation confirmation, and priming effect influence users' judgments of document relevance [13, 43, 45, 54, 55]. Researchers have also explored the potential impact of cognitive biases on socio-political view [24, 26] and health decisions [56, 60] during the information-seeking process. Additionally, they have investigated how to adjust oversimplified rational user models based on cognitive biases to design evaluation metrics that better characterize user judgments and interaction experience [7-9, 32, 58]. Among these studies, the work most closely related to our study is probably Scholer et al. [43], arguing that assessors, when shown examples of low relevance items, tended to rate subsequent items more highly than if they were shown highly or moderately relevant items first.\nRecently, with Large Language Models (LLMs) such as GPT-41 and Llama-2 [38] demonstrating advanced text comprehension capabilities across various tasks such as text annotation [16], researchers have begun exploring the automation of relevance annotation process using LLMs [15, 47, 52]. Given that the relevance labels obtained are often used for training IR algorithms and ranking IR systems, relevance labels influenced by cognitive biases could potentially lead to biased algorithms and suboptimal IR systems. However, at this stage, the IR community has a very limited understanding of how cognitive biases in LLMs affect their relevance assessment. In this study, we reveal that LLMs are also influenced by threshold priming, leading to inconsistencies in batch evaluation results."}, {"title": "3 RESEARCH QUESTIONS", "content": "To investigate to what extent are LLMs influenced by threshold priming when making batch relevance judgments of documents, in this study, we propose the following reseearch questions (RQs):"}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we briefly introduce our methodology and experimental setup.\nWe conduct our experiment on the TREC 2019 Deep Learning (TRDL19) passage retrieval track collection [10]. For the sake of convenience, we will refer to a passage as a \"document\" in the following sections. To ensure the selection of topics with a sufficient number of documents at various levels of relevance scores, we filter the topics based on the criterion that there are at least 12 documents for each relevance label (0, 1, 2, and 3). This filtering process results in a final selection of ten topics, which covers a diverse set of domains. For each topic, we conduct 20 trials. For each trial, we first randomly select n documents with a relevance score of 2, to form the epilogue E:\n$E = \\{d_1^e, d_2^e, ..., d_n^e\\}.$\nEach $d_i^e$ is a document with a ground truth relevance score of 2. Next, we randomly select m documents with a relevance score of 0 and another m documents with a relevance score of 3 to form the low threshold (LT) prologue L and the high threshold (HT) prologue H, respectively:\n$L = \\{d_1^l, d_2^l, ..., d_m^l\\},$ \n$H = \\{d_1^h, d_2^h, ..., d_m^h\\}.$\nEach $d_o^l$ and is $d_r^h$ is a document with a ground truth relevance score of 0 and 3 respectively.\nWe then concatenate L with E and H with E to obtain the low threshold batch $B_l$ and the high threshold batch $B_h$, respectively:\n$B_l = concate(L, E),$\n$B_h = concate(H, E).$\nFinally, for $B_l$ and $B_h$, we use the prompts shown in Figure 3 to have a LLM assess the relevance of each document in the batch, and output the results in JSON format."}, {"title": "5 EXPERIMENTAL RESULT", "content": "Figure 4 shows the scores assigned to the documents in the epilogue by different models under different batch setups when the prologue is either LT or HT. From Figure 4 one can observe that regardless of the batch setup, the scores from GPT-3.5, GPT-4, and LLaMa2-13B are generally influenced by threshold priming. Specifically, when the relevance threshold of the documents in the prologue is higher, these models tend to assign lower scores to the documents in the epilogue, and this difference is statistically significant (p < 0.001). Additionally, one can also observe that LLaMa2-13B exhibits a slighter difference in relevance assessment of documents in the epilogue under LT and HT compared to the GPT series.\nRegarding RQ1, we found that, when the prologue is 4 and the epilogue is 4, GPT-3.5, GPT-40, LLaMa2-13B, and LLaMa2-70B are all affected by threshold priming. Moreover, GPT-3.5 and GPT-4o are more significantly influenced by this effect compared to LLaMa2-13B and LLaMa2-70B.\nThese results indicate that the degree and the direction of threshold priming effect in large language models can be influenced by different batch setups when performing batch relevance judgments on documents. For LLaMa2-70B under PL = 4 and EL = 4, our observed results are contrary to those reported by Scholer et al. [43]."}, {"title": "6 DISCUSSION", "content": "This work, inspired by insights from behavioral economics, investigated the threshold priming bias in LLMs within the context of information retrieval system evaluation. The findings of this work contribute to a deeper understanding of AI systems' judgments and decision-making processes from an interdisciplinary perspective, and offer practical implications for designing fairer and human-centered unbiased Al systems.\nRegarding our RQs, we have the following findings:\n\u2022 Regarding RQ1, we found that, when the prologue is 4 and the epilogue is 4, GPT-3.5, GPT-40, LLaMa2-13B, and LLaMa2-70B are all affected by threshold priming. Moreover, GPT-3.5 and GPT-40 are more significantly influenced by this effect compared to LLaMa2-13B and LLaMa2-70B."}, {"title": "7 CONCLUSION", "content": "This study, as an exploratory study, paves the path toward further explorations on threshold priming in LLMs in IR evaluation tasks. For instance, what are the boundaries for triggering threshold priming? How are LLM's relevance and credibility labels influenced by threshold priming affect system rankings in offline evaluations? Additionally, as observed in our experiments, LLMs may be subject to cognitive biases beyond threshold priming, such as the anchoring"}]}