{"title": "MULTI-MODAL MEDICAL IMAGE FUSION FOR NON-SMALL CELL LUNG CANCER\nCLASSIFICATION", "authors": ["Salma Hassan", "Hamad Al Hammadi", "Ibrahim Mohammed", "Muhammad Haris Khan"], "abstract": "The early detection and nuanced subtype classification of\nnon-small cell lung cancer (NSCLC), a predominant cause\nof cancer mortality worldwide, is a critical and complex is-\nsue. In this paper, we introduce an innovative integration of\nmulti-modal data, synthesizing fused medical imaging (CT\nand PET scans) with clinical health records and genomic\ndata. This unique fusion methodology leverages advanced\nmachine learning models, notably MedClip and BEiT, for\nsophisticated image feature extraction, setting a new stan-\ndard in computational oncology. Our research surpasses\nexisting approaches, as evidenced by a substantial enhance-\nment in NSCLC detection and classification precision. The\nresults showcase notable improvements across key perfor-\nmance metrics, including accuracy, precision, recall, and\nF1-score. Specifically, our leading multi-modal classifier\nmodel records an impressive accuracy of 94.04%. We be-\nlieve that our approach has the potential to transform NSCLC\ndiagnostics, facilitating earlier detection and more effective\ntreatment planning and, ultimately, leading to superior patient\noutcomes in lung cancer care.", "sections": [{"title": "1. INTRODUCTION", "content": "Lung cancer remains a significant health challenge, ranking\nas the second most prevalent cancer and the foremost cause\nof cancer-related mortality in both men and women [1]. Char-\nacteristically, lung cancer symptoms often remain undetected\nuntil the disease has progressed to an advanced stage, com-\nplicating treatment efforts. In this context, image screening\nemerges as a critical tool, particularly for asymptomatic in-\ndividuals. Studies underscore the efficacy of imaging tech-\nniques such as CT and MRI in early lung cancer detection\namong high-risk groups, including smokers and individuals\nwith genetic predispositions, significantly enhancing survival\nprospects [2, 3]. However, existing methods for NSCLC clas-\nsification, such as those utilizing standalone CT or PET imag-\ning [4], often face limitations in terms of sensitivity and speci-\nficity, and they may not fully capture the complexity of tumor\nheterogeneity. Additionally, existing approaches may lack the\nintegration of comprehensive clinical and genetic data, which\nare crucial for a more precise diagnosis. These limitations\nunderscore the need for more advanced diagnostic method-\nologies that can accurately pinpoint and characterize cancer-\nous tissues, providing a strong motivation for our innovative\nmulti-modal image fusion approach.\nThis paper aims to refine lung cancer diagnosis, particu-\nlarly for non-small cell lung cancer (NSCLC), by integrating\nmulti-modal data beyond traditional imaging scans. Our\nproposed method fuses CT and PET scans to leverage the\nstrengths of both modalities. CT scans provide detailed\nanatomical information, while PET scans offer insights into\nmetabolic activity, often indicative of tumor presence. By\ncombining these scans, our fusion method creates a more\ncomprehensive image that captures both structural and func-\ntional aspects of the lung. Such fusion techniques have\nshown promise in improving diagnostic accuracy in other\norgan scans and in combining CT with MRI [2].\nOur approach addresses the limitations of existing diag-\nnostic techniques that rely solely on single-modality imaging.\nCT or PET scans alone may not sufficiently differentiate be-\ntween malignant and benign lesions, leading to diagnostic in-\naccuracies. Additionally, the standalone use of these modal-\nities may only partially capture the tumor's complexity and\nheterogeneity. Therefore, our method begins with the denois-\ning of scans using a deep CNN auto-encoder for more precise\nimage reconstruction. This step is followed by the fusion of\nCT and PET scans through wavelet decomposition and accu-\nrate image registration to create a fused image that provides\na richer, dual perspective of both functional and anatomical\ninformation. Specifically, fusing PET and CT images facili-\ntates the precise localization of abnormal metabolic activities\nwithin the lung's structural framework [5]. This precise local-\nization is crucial for identifying and characterizing cancerous\ntissues accurately. To complement this, we undertake rigor-\nous pre-processing of both tabular and genetic data, encom-\ncompassing missing value estimation, encoding, class balancing,\nscaling, and feature selection. The image processing phase\nincludes optimal contrast enhancement and normalization.\nBy integrating multi-modal imaging with advanced ma-\nchine learning techniques, we aim to enhance the overall di-\nagnostic performance, leading to earlier and more accurate\ndetection of NSCLC, ultimately improving patient outcomes."}, {"title": "2. RELATED WORK", "content": "Recent advancements in AI for diagnosing and treating\nNSCLC have significantly transformed patient outcomes\nin oncology [6]. Recent advancements in AI for diagnosing\nand treating NSCLC have significantly transformed patient\noutcomes in oncology [6]. The application of AI in NSCLC\ndiagnosis encompasses several key research domains: med-\nical imaging analysis, survival prediction, recurrence ana-\nlytics, and multi-modal data integration. These fields have\nbeen instrumental in advancing the diagnosis and treatment\nof NSCLC.\nIn the realm of medical imaging analysis, significant\nstrides have been made in tumor detection and classification\nusing Al algorithms applied to CT scans, as evidenced by\nthe works of Feng and Khoirunnisa [7, 8]. These advance-\nments have facilitated earlier and more precise diagnoses,\nconsequently improving patient outcomes. Additionally, AI\nanalysis of patient data has enhanced survival and recurrence\nprediction, offering valuable insights into patient prognosis\n[9].\nRecent research has also explored novel AI architectures\nfor improving detection accuracy. Studies have investigated\nthe potential of fully automated pipelines and advanced model\narchitectures, such as CRNN, ViTs, AlexNet, and 3D recon-\nstruction techniques, in enhancing diagnostic accuracy [2, 3].\nHowever, the highest accuracy achieved to date in these stud-\nies was 84.1% using the VGG16 model on medical imaging\ndata alone [10].\nThe integration of multi-modal data represents a bur-\\geoning area of research. This includes both image multi-\nmodality, combining MRI and CT scans, and data multi-\nmodality, integrating tabular and imaging data [7, 8]. Studies\nby Feng have underscored the potential of combining imag-\ning modalities like CT and MRI scans with patient genomics\ndata to enhance diagnostic accuracy. The fusion of diverse\ndata types, such as imaging modalities with patient genomic\ndata, has shown promise in improving diagnostic accuracy.\nOur paper builds upon these foundational studies, aiming to\npioneer the combination of multi-modal fused imaging with\nclinical and genomic data for NSCLC subtype classification.\nNo work combines multi-modal fused imaging with clinical\ndata and genomics sequences for the classification of both\nNSCLC subtypes, which is the scope and aim of this paper.\nOur paper contributes to this growing body of knowledge\nby not only applying these established techniques but also in-\nnovating in the way we integrate and process multi-modal\ndata. The fusion of CT and PET scans, as proposed in our\nmethodology, builds upon the foundational work of [11], who\nillustrated the benefits of CT and MRI image fusion in en-\nhancing the clarity and informational value of medical scans\nfor oncological application. Furthermore, our use of advanced\nmodels like MedClip and BEiT for image feature extraction\nextends the work of [12], who highlighted the strengths of\nthese models in handling high-resolution medical images.\nIn summary, this paper stands at the intersection of sev-\neral key research areas within the field of medical imaging\nand cancer diagnosis. By synthesizing these diverse method-\nologies and building upon them, our work aims to contribute\na novel approach to the early detection and classification of\nNSCLC, addressing some of the limitations identified in pre-\nvious studies."}, {"title": "3. PROPOSED METHODOLOGY", "content": "Our methodology introduces several novel contributions to\nthe field of NSCLC diagnosis through multi-modal data in-\ntegration and advanced deep learning techniques. These con-\ntributions can be categorized into four key areas: comprehen-"}, {"title": "3.1. Comprehensive Pre-processing", "content": "Our approach to data pre-processing is a significant contribu-\ntion with practical benefits. We address four key data modal-\nities: clinical tabular data, genetic data, CT scans, and PET\nscans. For tabular and genetic data, we implement a series of\nrigorous pre-processing steps, including missing value impu-\ntation, categorical encoding, class balancing using SMOTE,\nand standardization. Feature importance is evaluated using\nXGBoost, ensuring that only the most relevant features are\nused in subsequent analyses. For imaging data, we develop a\ndeep CNN auto-encoder trained on a clean PET scan dataset\nto de-noise PET scans, effectively reducing noise and enhanc-\ning image quality. CT scans undergo normalization and con-\ntrast enhancement. We also employ 3D Slicer software for\nprecise image registration of CT and PET scans, ensuring\naccurate anatomical and functional alignment. These pre-\nprocessing steps are crucial for maximizing the performance\nof our fusion and classification models, leading to more accu-\nrate diagnoses and treatment plans in healthcare analytics."}, {"title": "3.2. Advanced Image Fusion", "content": "One of the primary contributions of our methodology is\nthe development of an advanced image fusion technique\nthat combines the strengths of CT and PET scans. After\nall the prepossessing of the scans was done, the new clean\ninput was fed into our VGG19 fusion model. The fusion\nalgorithm decomposes the CT and PET scans into four co-\nefficients, each using the discrete wavelet transform. The\ncoefficients are the coefficient LL1 and three detail coeffi-\ncients: LH1(horizontal), LV1(vertical), LD1(diagonal). After\nthe fusion of the four pairs, inverse wavelet transform was\napplied to the four bands to obtain the fused image, results\nshown in Figure 2. The CT scan offers clear structural de-\ntails of the lung tissues and surrounding areas, but it lacks\nmetabolic information. The PET scan reveals areas of high\nmetabolic activity indicative of tumor presence, but it lacks\nprecise anatomical context. By merging these two modali-\nties, the fused image not only overcomes the limitations of\neach modality but also provides a comprehensive view that is\ninvaluable in medical imaging, highlighting both the detailed\nstructures of the lung tissues and the areas of high metabolic\nactivity."}, {"title": "3.3. Baseline Models", "content": "The next stage involved implementing different baseline mod-\nels to evaluate the effectiveness of the multi-modal approach.\nSVM and logistic regression were chosen for their robustness\nand simplicity in handling tabular and genetic data, provid-\ning reliable benchmarks for structured data analysis. For im-"}, {"title": "3.4. Multi-modal Classification Model", "content": "For the multi-modal classification model, we introduced two\nsophisticated model architectures: MedClip and BEiT (Bidi-\nrectional Encoder representation from Image Transformers).\nMedClip employs a dual encoder structure for visual and tex-\ntual data, using cross-modal contrastive learning to align these\nrepresentations in a unified feature space. This approach is\nparticularly novel and crucial as it facilitates the seamless in-\ntegration of medical images with clinical and genetic data,\nenhancing the model's ability to draw comprehensive insights\nfrom diverse data types [12]. The dual encoder structure al-\nlows MedClip to effectively capture the complementary in-\nformation from both image and text data, making it a pow-\nerful tool for multi-modal medical diagnostics. BEiT, based\non the Vision Transformer (ViT) architecture, leverages bidi-\nrectional context modeling and masked image modeling to\ndevelop robust representations from images. This model is\npre-trained on medical images, which enhances its capability\nto detect and classify tumors accurately [13]. The use of bidi-\nrectional context modeling allows BEiT to consider the entire\ncontext of an image, providing a more thorough understand-\ning of the visual information. Masked image modeling, on the\nother hand, trains the model to predict missing parts of the im-\nage, which improves its ability to generalize and perform well\non unseen data. These models were tested with three different\nimage input combinations: CT alone, CT and PET separately,\nand the fused CT and PET image, allowing us to evaluate the\nefficacy of our fusion method thoroughly. By doing so, we\ncan determine the effectiveness of the proposed method and\nfusion model. The architecture of the best multi-modal model\nBEiT-based is shown in Figure 1."}, {"title": "4. EXPERIMENTAL DETAILS", "content": "This paper utilized three distinct datasets to optimize the\nanalysis of NSCLC. The primary dataset was the NSCLC\nRadiogenomic collection from the Cancer Imaging Archive\n[14]. This comprehensive dataset includes 285,411 scan im-\nages from 303 studies of 211 NSCLC patients, encompassing\nboth CT and PET lung images. It also provides extensive\nclinical data, covering variables such as age, smoking status,\nhistology, treatment history, and cancer recurrence. Addition-"}, {"title": "4.2. Implementation Detail and Result Analysis", "content": "For the hardware requirement, computers equipped with\nNVIDIA Quadro RTX6000 were used to train the models,\nand the multi-core processors were used for efficient data\nprocessing and analysis. Powerful GPUs are necessary to\nspeed up the computations significantly. Finally, given the\nlarge size of medical imaging datasets, storage solutions like\ncloud storage were used. In terms of software, 3D Slicer was\nused to view and pair the CT and PET DICOM scans and to\nregister the image between the two scans. Moreover, several\nPython libraries were needed to aid in working with imaging\nscans, namely pydicom, pynrrd, skimage, lungmask [17],\nand SimpleITK, to name a few. Besides these, we used deep\nlearning frameworks such as PyTorch, TensorFlow, and Keras\nto develop and train the models for image classification. Also,\nthe library Optuna was used to facilitate the hyperparameters\nsearch [18]."}, {"title": "4.3. Model Training", "content": "For the deep learning models, the key hyperparameters in-\ncluded a learning rate of 0.001, a batch size of 96, a dropout\nof 0.5, and the Adam optimizer held constant across all mod-\nels for fair comparison. For the XGBoost model for feature\nselection, the learning rate was set to 0.1, with a maximum\ndepth of 5 for trees and 100 estimators. We employed a 5-fold\ncross-validation approach for testing and model evaluation.\nThe baseline methods included a standard logistic regression\nmodel and support vector machine for tabular data com-"}, {"title": "5. RESULTS AND DISCUSSION", "content": "Our experimental analysis comprehensively evaluated various\nmachine learning models, spanning tabular, image-based, and\nmulti-modal approaches both separately and in a fused imag-\ning modality. Key findings, detailed in Table 1, reveal distinct\nperformance patterns across these models, evaluated on accu-\nracy, precision, recall, and F1-score.\nBaseline tabular models, specifically SVM and Logistic\nRegression, exhibited solid performance with an accuracy of\n77.0%. Image classifiers using solely CT scans showed varied\neffectiveness; 2D CNNs achieved a 70.0% accuracy, whereas\nInception models reached up to 79.0%. Notably, the incorpo-\nration of fused CT and PET scans markedly boosted model\nperformance. For instance, VGG16, using fused images, at-\ntained an 87.1% accuracy and an 82.5% F1-score, underlining\nthe value of fused imaging.\nThe standout results were observed in multi-modal mod-\nels. The MedClip model, integrating tabular, genetic, and\nfused imaging data, achieved an 84.9% accuracy. The\nBEiT-based model, also utilizing a multi-modal approach,\ndemonstrated a remarkable accuracy of 94.04% with fused\ndata. This represents a significant advancement over the\nperformances achieved using separate CT and PET scans,\nunderscoring the efficacy of our multi-modal, fused imaging\nmethodology. These results not only highlight the potential\nof advanced AI models in medical diagnostics but also em-\nphasize the transformative impact of integrating diverse data\nmodalities."}, {"title": "5.2. Qualitative Analysis", "content": "Throughout all the assessments, the implementation of fused\nimaging data notably improved model performance metrics\nacross the board, underscoring the value of integrating multi-\nple imaging modalities for enhanced diagnostic inference, as\nillustrated in Figure 3. The deep learning models, particularly\nthe one based on the BEiT architecture, effectively leveraged\nthe rich, multi-modal data, translating into superior quanti-\ntative outcomes. It is also important to note that the multi-\nmodal model surpasses the imaging and tabular model in per-\nformance, justifying the need to integrate data from different\nmodalities to enhance the diagnosis accuracy. Moreover, even\nwhen trying the same multi-modal model but processing the\nCT and PET scans separately through feature extraction and\nthen combining the results always performed worse than us-\ning the fused image generated by the VGG19 fusion model,\nas it can be concluded that the fusion process is necessary\nand aids in the improved performance of the overall model.\nRegarding the pre-processing of the imaging scans, the deep\nCNN auto-encoder helped a lot in de-noising the PET scan,\nand the results were very noticeable. Similarly, for the CT\nscans, the pre-processing was applied to filter only the slices\ncontaining lung pixels, and masking and improving the con-\ntrast helped improve the results."}, {"title": "5.3. Discussion", "content": "The quantitative leap in performance metrics with fused\nimaging data suggests that models benefit from the comple-\nmentary information available in different imaging modal-\nities. It can be inferred that the spatial resolution of CT\nimages combined with the metabolic information from PET\nscans provides a more holistic view of the pathology, which is\neffectively exploited by the more complex architectures like\nVGG16, Inception, and the BEiT models. This illustrates that\nalthough the BEit model uses fewer parameters than MedClip\nand VGG16, it had a better performance.\nThe SVM and Logistic Regression models, while less ef-\nfective than image classifiers, offer competitive performance\non tabular data, indicating that traditional machine learning\nmodels remain valuable for structured data analysis. The\nlower performance of 3D CNNs across all metrics suggests\nthat for the dataset at hand, the additional complexity in-\ntroduced by 3D convolution may not capture the essential\nfeatures as effectively as other architectures, possibly due to\noverfitting or the need for more training data.\nThe superior results of the BEiT-based model highlight\nthe potential of transformer architectures in handling com-\nplex, multi-modal datasets. Transformers' ability to model\nlong-range dependencies and integrate disparate data sources\nis particularly beneficial for medical imaging tasks, where the\ncontext and subtlety of features are crucial for accurate diag-\nnosis."}, {"title": "6. LIMITATIONS", "content": "The primary limitation of our multi-modal approach is the\nconstrained dataset size, as comprehensive data across all\nmodalities (clinical, imaging, and genomic) was available for\nonly a limited subset of patients. This reduction in dataset size\npotentially affects the generalizability and robustness of our\nmodels and necessitates the use of supplemental data from\na secondary source to address class imbalances. Addition-\nally, the complexity of integrating diverse data sources, while\nbeneficial for performance, leads to the 'black box' issue, di-\nminishing the interpretability of the model's decision-making\nprocess. This aspect is particularly critical in clinical settings\nwhere transparency is essential for clinician trust and model\napplicability. Future efforts might focus on enhancing model\ntransparency through explainable AI techniques."}, {"title": "7. CONCLUSION", "content": "In conclusion, this paper made significant strides in NSCLC\ndiagnostics by integrating fused CT and PET scans with ge-\nnomics and clinical data. Our key contributions include the\ndevelopment of a novel image fusion technique that com-\nbines anatomical and metabolic information, the innovative\napplication of advanced models like MedClip and BEiT for\nmulti-modal data analysis, and the implementation of sophis-\nticated pre-processing and denoising techniques for CT and\nPET scans. These contributions led to a substantial improve-\nment in NSCLC classification, achieving a remarkable accu-\nracy of 94.04%. This success demonstrates the transformative\npotential of integrating diverse data sources and leveraging\nstate-of-the-art transformer-based architectures in medical di-\nagnostics. The high accuracy achieved underscores the criti-\ncal importance of fused imaging and comprehensive data in-\ntegration in providing a more detailed and accurate lung can-\ncer analysis. These findings mark a significant leap forward\nin the field, setting a new standard for future research to de-\nvelop even more sophisticated models and richer datasets, ul-\ntimately leading to more precise and reliable diagnostic tools\nin oncology.\nFuture Work. Future work will focus on expanding data\nsources by incorporating additional modalities such as pro-\nteomics and metabolomics, conducting longitudinal studies\nto track tumor progression, and validating model perfor-\nmance through real-world clinical trials. Enhancing model\ninterpretability for clinical use, integrating our approach into\nexisting clinical workflows, and optimizing for scalability\nand computational efficiency are also key areas for further\nresearch. These efforts aim to build on our findings to create\nmore robust, accurate, and clinically applicable diagnostic\ntools. The ultimate goal is to improve patient outcomes in\nNSCLC and other cancers, a significant and impactful direc-\ntion for future research."}]}