{"title": "NaijaNLP: A Survey of Nigerian Low-Resource Languages", "authors": ["ISA INUWA-DUTSE"], "abstract": "With over 500 languages in Nigeria, three languages - Hausa, Yor\u00f9b\u00e1 and Igbo - spoken by over 175 million people, account for about 60% of the spoken languages. However, these languages are categorised as low-resource due to insufficient resources to support tasks in computational linguistics. Several research efforts and initiatives have been presented, however, a coherent understanding of the state of Natural Language Processing (NLP) - from grammatical formalisation to linguistic resources that support complex tasks such as language understanding and generation is lacking. This study presents the first comprehensive review of advancements in low-resource NLP (LR-NLP) research across the three major Nigerian languages (NaijaNLP). We quantitatively assess the available linguistic resources and identify key challenges. Although a growing body of literature addresses various NLP downstream tasks in Hausa, Igbo, and Yor\u00f9b\u00e1, only about 25.1% of the reviewed studies contribute new linguistic resources. This finding highlights a persistent reliance on repurposing existing data rather than generating novel, high-quality resources. Additionally, language-specific challenges, such as the accurate representation of diacritics, remain under-explored. To advance NaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts in resource enrichment, comprehensive annotation, and the development of open collaborative initiatives.", "sections": [{"title": "1 Introduction", "content": "Of the 7,164 distinct languages spoken globally [120], over 90% are classified as low-resource (LR) from a computational perspective. While there is no universal consensus on what constitutes a low-resource language [197], a language is generally considered low-resource if it suffers from insufficient parallel source-target data [238]. In other words, low-resource languages (LRLs) are languages for which statistical methods cannot be directly applied due to scarce resources [177]. Essentially, the term low-resource encompasses a broad spectrum of resource conditions and application domains [75, 112, 150, 168, 176, 183, 293]. Numerous studies have explored low-resource natural language processing (LR-NLP) across diverse domains, including misinformation detection [289], security and defence [270], neural machine translation [64, 264], cyberbullying detection [179], and sentiment analysis [136, 138, 174, 297]. A common denominator across these tasks is that LRLs suffer from a severe lack of linguistic resources, resulting in suboptimal performance in downstream NLP applications. The lack of sufficient linguistic resources affect LR-NLP within broader research topics [165]. A similar gap exists in the literature regarding NLP for Nigerian languages, particularly the three major languages (Hausa, Yor\u00f9b\u00e1, and Igbo), collectively referred to as NaijaNLP in this work. Many studies have treated low-resource languages as a homogeneous category, overlooking critical variations in data availability, linguistic resources, and computational support. The binary classification - differentiating languages as merely low-resource or high-resource - has been a dominant approach in NLP research [150, 168, 176, 183, 293]. However, we argue that this dichotomy may impede language enrichment progress, as it fails to account for the unique challenges faced by individual or regional language groups. Instead, we advocate for a more nuanced approach that examines specific linguistic and regional contexts, identifies their distinct challenges and develops tailored solutions.\nScope. This study investigates the traditional NLP landscape for NaijaNLP, with a particular focus on foundational methodologies and linguistic resources. Our primary objective is to examine classical NLP approaches, linguistic resources, and tools that have been developed to address the unique challenges faced by these three major Nigerian languages. While generative AI and similar cutting-edge technologies are transforming NLP, they fall outside the scope"}, {"title": "2 Methodology", "content": "This review systematically explores the landscape of LR-NLP research, with a specific focus on NaijaNLP to (1) examine existing NLP resources (2) identify factors affecting linguistic resource availability as well as challenges, and (3) provide recommendations for future development of NaijaNLP.\n2.1 Databases and Search Strategies\nFigure 1 provides an overview of the review process, including the research questions, search terms, consulted databases, and the screening process. The search strategy was conducted across several academic databases, including IEEE Xplore, ACM Digital Library, Scopus/Elsevier, Springer Nature and ACL Anthology. Additionally, grey literature including preprint servers such as arXiv, and relevant articles retrieved via Google Scholar, were incorporated into the review. To further expand the literature base, we employed a snowballing search strategy, iteratively identifying relevant studies through backward snowballing (tracing references cited in key papers), and forward snowballing (identifying subsequent papers that cite the key studies).\nSearch Terms. To ensure a comprehensive retrieval of relevant papers, we incorporated a permutation of search terms across three key categories:\nLanguage-related: These terms include Hausa, Yor\u00f9b\u00e1, and Igbo languages\nNLP-related: Covers terms like low-resource languages, natural language processing (NLP), machine learning (ML), deep neural network (DNN), artificial neural network (ANN), and artificial intelligence (AI).\nTask-specific: Encompass downstream tasks such as text classification, text summarisation, sentiment analysis, speech recognition, machine translation, POS, and NER.\nA more detailed list of the integrated search terms is provided in Table 4.\nInclusion and Exclusion Criteria. The selection criteria encompass theses and relevant non-seminal publications originating locally. We surmise that excluding these sources could result in the omission of valuable resources and insights beneficial to the research community. Studies are excluded if they: (1) do not pertain to NaijaNLP, i.e. the study"}, {"title": "3 Literature Review", "content": "As a communication system, natural language comprises of structured sequences of sounds, symbols, or gestures that convey meaning in spoken, written, or signed forms. From a formalisation perspective, languages are composed of structured strings of lexical categories (pre-terminals), forming well-defined sentences that facilitate statistical analysis of their structure [106, 143]. Advancements in Artificial Intelligence (AI) and Machine Learning (ML) have provided powerful tools for processing languages through NLP techniques. However, the effectiveness of NLP models depends on the availability of extensive and relevant linguistic resources, leading to a fundamental divide between high-resource and low-resource languages. High-resource languages (HRLs), such as English, French, and Spanish, benefit from abundant digital resources and large-scale initiatives like the Linguistic Data Consortium, which facilitate access to multilingual text resources in standardised and compatible formats [98, 142]. These resources, sourced from governmental bodies, international organisations, news agencies, and publishers, play a crucial role in advancing various NLP applications. Efforts to enrich and digitise LRLs, such as transducing texts [248], resource enrichment [21], and digitising indigenous poetry [281] are essential for their integration into computational frameworks.\n3.0.1 Past Reviews. As previously noted, advancements in NLP have predominantly benefited high-resource languages, while LR-NLP continues to face unique challenges owing to limited linguistic resources. Several past studies [19, 104, 128, 131, 250, 257, 300, 301] have examined the state of LR-NLP, highlighting these challenges to LRL development. Similarly, numerous studies have investigated NLP research involving the three major languages in Nigeria. However, these studies tend to focus on either a single language such as Hausa [298], Yor\u00f9b\u00e1 [295], Igbo [182, 202, 236] or they concentrate on specific NLP tasks like machine translation [83, 146], text summarisation [113], automatic speech recognition for tonal languages [166], and sentiment analysis [249]. Other reviews adopt a broader perspective by covering various African languages [3, 28, 72, 74, 93, 118, 127, 150, 152, 203]. While these studies offer valuable insights, they provide only a fragmented perspective on the current state of NLP research in the targeted languages. Our review builds upon previous studies by incorporating a wider scope, accounting for recent advancements, open-access resources, and community-driven initiatives. Thus, we provide a comprehensive analysis of the NaijaNLP landscape, examining ongoing research efforts, available datasets, tools, language communities, and initiatives aimed at facilitating LR-NLP development. The subsequent discussions involving the three languages are organised alphabetically by language, following the order: Hausa, Igbo, Yor\u00f9b\u00e1.\n3.1 Formal Grammar and Knowledge Representation\nWe begin our review with formal grammar and knowledge representation due to their fundamental role in developing robust NLP tools and applications. Computational grammar facilitates language enrichment through systematic analyses of syntax, semantics and linguistic structures, enabling the creation of reusable linguistic resources for LRLs [105]. In this regard, several studies have explored grammatical formalisation [4, 65, 85, 91, 105, 106], morphological analysis [115, 133], and knowledge representation [47, 48, 121, 200] for Hausa, Igbo and Yor\u00f9b\u00e1 languages. Notably, Abdoulaye [4] apply the Role and Reference Grammar framework to provide new insights into Hausa sentence structure and linguistic processes. Similarly, Berthold [85] examine lexical and grammatical tone and vowel length in Hausa, utilising"}, {"title": "3.2 Language Particularities", "content": "Languages exhibit distinct structural, phonetic, and orthographic characteristics that influence how they are written, pronounced, and processed. These particularities manifest in diverse orthographic systems, including alphabets, abjads, syllabaries, and logograms, which shape word formation and written representation. Additionally, phonetic variations such as differences in phonemes, stress patterns, and tonal systems - impact how languages are spoken and understood. Recognising and integrating these linguistic nuances is essential for developing effective NLP tools that accurately represent the unique feature of each language [17]. Several Nigerian languages are tonal and employ characters beyond the basic Latin alphabet, making them structurally distinct from English and other high-resource languages. A failure to adequately account for these particularities often hinders the adaptation and effectiveness of existing NLP tools developed primarily for English and other HRLs in the context of LRLs. Addressing this challenge, numerous studies have explored the linguistic diversity and specific needs of Nigeria's major languages [5, 40, 62, 243]. Furthermore, Adebara and Abdul-Mageed [17] examined key linguistic and sociopolitical challenges affecting NLP development for African languages. This section reviews existing studies on the linguistic particularities of Nigerian languages, with a focus on morphological analysis and diacritic restoration. Both morphological analysis and diacritic restoration play crucial roles in improving the accuracy and performance of NLP applications. Among Nigeria's three major languages, Igbo and Yor\u00f9b\u00e1 make extensive use of diacritical marks, further emphasising the need for language-specific NLP approaches.\n3.2.1 Morphological Analysis. Morphological analysis explores word structure and formation. The languages within NaijaNLP exhibit rich morphological complexity, which has been documented in various studies. To that end, Iheanetu and Oha [159] analyse the morphological structure of Igbo language, identifying key challenges that hinder practical NLP applications. Building on this, Iheanetu and Oha [160] develop a data-driven model capable of inducing non- concatenative morphological structures, cascaded affixation, and affix labelling using a frequent pattern-based induction"}, {"title": "3.2.2 Diacritic Restoration", "content": "Diacritic restoration involves adding or correcting diacritical marks in textual data. This process is essential for handling historical texts, user-generated content, and datasets where diacritics may have been omitted due to technical constraints. Given that many African languages utilise Latin-based scripts with diacritical marks, Scannell [262] propose a strategy and open-source package for automatic unicodification across multiple languages. Addressing orthographic and tonal diacritics, Ezeani et al. [122] apply n-gram models trained on the Igbo Bible corpus for word-level diacritic restoration. Recognising the role of diacritics in pronunciation, meaning differentiation, and lexical disambiguation, Ezeani et al. [123] develop a disambiguation strategy through diacritic restoration for the Igbo language. Further advancements in this domain include the use of embedding models for text similarity computation and diacritic restoration [124, 126]. For Yor\u00f9b\u00e1 diacritic restoration, De Pauw et al. [108] explore various ML models to improve diacritic restoration accuracy. T\u00fanj\u00ed and B\u00ed [279] propose an enhanced approach that integrates tonal information into text-to-speech systems for the standard Yor\u00f9b\u00e1 language. Recognising that omitting tonal information leads to"}, {"title": "3.2.3 Numerical System", "content": "The challenges stemming from the lack of or limited standardised linguistic resources extend beyond textual to numerical data. A key area of interest is assessing whether NaijaNLP possesses the necessary linguistic resources, such as well-defined numerical lexicons, for developing numerical systems through published research. The earliest attempt at analysing Hausa numerals for machine processing dates back to the Automatic Text Comprehension project [266]. This project surveyed numerical systems and examined challenges related to converting spoken numerals into decimal representations, ultimately leading to the development of an automated rule-based systems. The system enabled bidirectional conversion between mathematical representations and their linguistic counterparts, i.e., numerals [266]. In a related effort, Agbeyangi et al. [44] introduce a numeral translation system for English-Yor\u00f9b\u00e1, while Mustapha et al. [193] formulate an algorithm for converting numerical figures into words in Hausa. Similarly, Rhoda [255] develop a number-text conversion system for Igbo, contributing to the broader goal of numerical processing in LRLs. Beyond numerical conversion, advancements in handwritten numerical recognition have been explored. For instance, Ajao et al. [50] develop a Yor\u00f9b\u00e1 handwritten character recognition using convolutional recurrent neural networks, demonstrating the application of deep learning techniques in linguistic digitisation efforts. Despite these developments, research on numerical systems in NaijaNLP remains limited, highlighting the need for further studies and resource development to ensure accurate numerical representation and processing across languages."}, {"title": "3.3 LR-NLP Tools and Resources", "content": "The development of relevant tools, resources, and techniques is essential to enable various NLP downstream tasks and to facilitate wider language coverage for robust LR-NLP applications [85]. The lack of sufficient linguistic resources and pre-processing tools presents a significant challenge for LRLs, particularly when diacritic and morphological features must be considered. To address these challenges, several initiatives, resources, and tools have been developed to support LRLs. A case in point is the low-resource Human Language Technology (LoReHLT) project, which evaluates the DARPA Low-Resource Languages for Emergent Incidents (LORELEI) research program [100]. The LORELEI initiative includes both Hausa and Yor\u00f9b\u00e1, providing researchers with resource packages for LRLs. These packages comprise monolingual and parallel texts, a bilingual dictionary, text annotated with part-of-speech (POS) tags, named- entity-recognition (NER), and noun-phase-chunking annotations. Furthermore, language processing tools such as"}, {"title": "3.3.1 Linguistic Resources", "content": "Linguistic resources constitute a fundamental component in the development of robust NLP applications. Several initiatives have been introduced to support NaijaNLP. Adegbola [21] highlight key initiatives, notably the African Languages Technology Initiative (Alt-i), aimed at enhancing NLP support for major Nigerian languages. Similarly, Chiarcos et al. [95] provide tools and annotation resource to facilitate NLP downstream tasks across 25 sub-Saharan languages, including Hausa, Yor\u00f9b\u00e1 and Igbo. As part of this effort, they develop ANNIS ANNotation of Information Structure a corpus tool that offers unified access to various linguistic annotations and data archives. To further advance lexicographic resources, Bigi et al. [89], introduce language resources and Human Language Technologies (HLT) tools for Nigerian Pidgin, consisting of a tokeniser and automatic speech system for predicting word pronunciation and segmentation. For Igbo NLP, Onyenwe et al. [230] propose POS-tagging resources including a POS tagset and a tagged subcorpus. Addressing the broader LR-NLP landscape, Costa-juss\u00e0 et al. [104] explore strategies to narrow the performance gap between high-resource and low-resource languages. Their approach involves exploratory interviews with native speakers, data creation, and model development. Meanwhile, Turki et al. [280] investigate the role of text categorisation in optimising stopwords extraction, with a specific focus on African languages.\nDatasets Data serve as a fundamental building block for NLP technologies. Recognising this, researchers have developed various datasets of differing sizes and levels of diversity to support LR-NLP tasks. The following section provides a review of available datasets for NaijaNLP.\nHausa Datasets. Several efforts have been made to develop datasets for NLP applications in the Hausa language. Imam et al. [161] compile a dataset of approximately 2600 articles to support fake news detection in Hausa language and Vargas et al. [287] introduce the first expert-annotated dataset for Hausa hate speech detection, consisting of 2,000 comments extracted from Facebook. Zandam et al. [299] curate a Hausa dataset containing threatening lexicons, sourced from Twitter. Inuwa-Dutse [162] provide an expansive dataset containing both formal and informal Hausa text. For sentiment analysis, Mohammed and Prasad [188] develop a Hausa dataset comprising 14,663 instances (4,154 positive, 4,310 negative, and 6,199 neutral sentiments). The dataset was created using words and phrases from Hausa dictionary and further expanded through data augmentation techniques. Similarly, Shehu et al. [263] develop a Hausa sentiment analysis lexicon alongside a customised stemming method to enhance model performance. Awwalu et al. [76] construct the Hausa Tagset (HTS) for POS tagging, while Salifou and Naroua [260] design a spell-checking and correction tool for Hausa, which is accessible via LyTexEditor and includes an extension (add-on) for OpenOffice.org. Adam et al. [16] compile a dataset of offensive content in Hausa language, and Ibrahim and Abdulmumin [153] develop a parallel datasets closely aligned with the target languages to enhance machine translation involving Hausa."}, {"title": "Igbo Dataset", "content": "Several studies have contributed to the development of datasets supporting NLP tasks in the Igbo language as well. Onuora et al. [226] and Ana et al. [63] curate datasets for Igbo hate speech detection. For cyberbullying detection, Okoloegbo et al. [215, 216] develop datasets covering both Igbo and Nigerian Pidgin English. Onyemaechi and Ojiako [227] present a database consisting of 158 Chi-prefixed Igbo personal names, categorised into praise, thanksgiving, testimony, prayer, and declarative groups. Chioma and Nnadozie [97] compile an Igbo thesaurus, covering a comprehensive set of words and their meanings. Nganga and Achebe [195] describe a methodology for creating a digital dialectal dictionary for Igbo, sourced from spoken-word corpora and oral traditions. Ajao et al. [51] develop an RNN-based handwritten character recognition system for Igbo using data from native speakers.\nYor\u00f9b\u00e1 Datasets. Similarly, multiple datasets have been developed to support various NLP applications in Yor\u00f9b\u00e1 language. Fagbolu et al. [129] introduce one of the earliest Yor\u00f9b\u00e1 corpora to support machine translation and other NLP downstream tasks. Ademusire and Ninan [33] curate annotated Yor\u00f9b\u00e1 data for event extraction and Akpobi [56] develop Yankari, a large-scale monolingual Yor\u00f9b\u00e1 dataset. Orife [234] offer datasets and source code for various Yor\u00f9b\u00e1 NLP applications. Ahia et al. [45] provide text and speech data to address dialectal discrepancies in Yor\u00f9b\u00e1. Afolabi and Wahab [43] develop an audio database for Yor\u00f9b\u00e1 consonant-vowel syllables, the alphabet, and a text-to-speech system. In a related work, Iyanda [163] compile a Yor\u00f9b\u00e1 corpus for speech generation, sourced from textbooks, newspapers, and online materials. Gutkin et al. [145] produce a speech synthesis dataset, comprising over four hours of 48 kHz recordings in Yor\u00f9b\u00e1, while Emmanuel [117] develop annotated prosodic read-speech syllabic data, extracted from fictional books and online scriptures.\nMultilingual Datasets. To address the challenges of LRLs and mitigate reliance on costly annotation, several initiatives have contributed to multilingual datasets. Hedderich [149] propose a weak-supervision and noise-handling strategy to facilitate LRLs data expansion. To support the provision of high-quality human-annotated cross-lingual information retrieval resources for African languages, Adeyemi et al. [41] present CIRAL, a publicly available test collection for cross-lingual retrieval, covering English queries with passages in Hausa, Somali, Swahili, and Yor\u00f9b\u00e1. Muhammad et al. [190] introduce the first large-scale human-annotated Twitter sentiment dataset, covering Hausa, Igbo, Nigerian Pidgin, and Yor\u00f9b\u00e1, with approximately 30,000 annotated tweets per language, including code-mixed examples. In a related effort, [29] introduce a large-scale, publicly available dataset for NER across ten African languages. Furthermore, [189] develop AfriHate, a multilingual corpus of annotated hate speech and abusive language data spanning 15 African languages. Aliyu et al. [61] compile datasets in Hausa, Yor\u00f9b\u00e1, and Igbo for hate speech detection and Varab and Schluter [286] develop multilingual text summarisation data consisting of 28.8 million articles from 92 languages. Adelani et al. [31] develop MasakhaNEWS, the largest dataset for news classification in 16 African languages. Varab and Schluter [286] introduce a multilingual text summarisation dataset containing 28.8 million articles across 92 languages. Ferroggiaro [132] develop hate speech lexicons for several Nigerian languages, and Ogunleye et al. [211] utilise 346,000 Nigerian banking-related tweets to develop SentiLeye, a lexicon-based sentiment analysis dataset for Nigerian Pidgin. Adelani [28] introduce large-scale human-annotated datasets for NER and machine translation across 21 African languages. Idris et al. [158] develop a multilingual electronic dictionary covering English, Hausa, Yor\u00f9b\u00e1, and Igbo, designed for handheld devices.\nTo support multilingual machine translation, Fan et al. [131] provide an open-source training dataset covering thousands of language pairs with parallel data. Ekpenyong et al. [114] introduce a parallel Hausa-English corpus and"}, {"title": "Linguistic Tools", "content": "Building on existing corpora, various linguistic tools have been developed to enrich and ensure the accurate representation of LRL. Some of these tools address the specific linguistic characteristics of individual languages, while others focus on enhancing linguistic resources more broadly. Notable examples include automatic spell checkers and correctors for Yor\u00f9b\u00e1 [222] and Hausa [186, 260], an automatic Hausa stopword constructor [86], and an Igbo text analyser [78]. The following section provides a detailed examination of linguistic tools including part-of-speech (POS) tagging and named entity recognition (NER) tools within NaijaNLP.\n3.3.2 Part-of-Speech Tagging and Named Entity Recognition. Each word in a text belongs to a grammatical category, such as noun, verb, adjective, or adverb, depending on its context and definition. Part-of-Speech (POS) tagging assigns words to their corresponding grammatical categories, facilitating the structural analysis of sentences. Similarly, some words in a text refer to specific entities such as people, organisations, locations, dates, and quantities. Thus, Named Entity Recognition (NER) identifies and classifies such entities accordingly. To that end, several studies have explored POS tagging and NER for NaijaNLP. Ren et al. [254] present data-driven methods for recognising entities in large-scale, domain-specific text corpora across both high- and low-resource languages. Adelani et al. [30] examine NER in Hausa and Yor\u00f9b\u00e1, while Ayodeji et al. [77] explore accent classification for Nigerian-Accented English, covering Hausa, Yor\u00f9b\u00e1, and Igbo. Oyewusi et al. [242] develop NER models for Nigerian Pidgin English, Igbo, Yor\u00f9b\u00e1, and Hausa. Tukur et al. [275] introduce a POS tagger for Kanuri and corresponding corpus. Adewumi [38] explore cross-lingual transfer learning to optimise Hausa and Yor\u00f9b\u00e1 POS and NER tasks. Mehari et al. [185] employ semi-supervised data augmentation combined with pretrained language models to enhance NER for LRLs."}, {"title": "Hausa POS & NER", "content": "Given the limitations of classic stemmers based on Porter's algorithm [245], researchers have developed Hausa-specific stemming techniques [81, 90, 192]. Essentially, Bashir et al. [81] introduce an automatic Hausa word stemming system to optimise text processing. Bimba et al. [90] develop a stemming strategy using affix-stripping rules and reference lookup. Recognising the importance of entity extraction in disaster response and large-scale incidents , Lu et al. [172] present a method for learning entity priors from extensive Hausa text corpora. Zhang et al. [300] propose a low-resource POS tagging strategy aimed at minimising noise in supervised learning methods by integrating diverse linguistic sources. To address challenges posed by the use of non-standard words (NSWs) in Hausa social media communication, Maitama et al. [180] propose a text normalisation system based on handcrafted rules for converting NSWs into standard Hausa text. Tukur et al. [276, 277, 278] explore POS tagging techniques to facilitate sentiment analysis of Hausa web content. Awwalu et al. [76] develop the Hausa Tagset (HTS) for POS tagging, while Musa et al. [192] introduce a stemming algorithm to enhance Hausa information retrieval.\nIgbo POS & NER. Several linguistic tools and resources have been developed for Igbo. Onyenwe et al. [228], Onyenwe and Hepple [229] contribute to Igbo linguistic resource development. Onyenwe et al. [231, 233] and Olamma et al. [220] develop POS taggers for Igbo. Onyenwe et al. [232] propose cross-lingual and monolingual POS tag projection approaches for Igbo POS tagging. For NER in Igbo, researchers have focused on cross-lingual learning. Chukwuneke et al. [101] introduce an Igbo NER system based on a cross-language projection method, leveraging parallel English-Igbo corpora and Soronnadi et al. [267] develop IgboBERTa, a transformer-based model optimised for Igbo NLP tasks, including NER, text classification, and sentiment analysis.\nYor\u00f9b\u00e1 POS & NER. Similarly, significant efforts have been made to advance Yor\u00f9b\u00e1 POS tagging and NER. Kumolalo et al. [169] propose a rule-based approach to enhance Yor\u00f9b\u00e1 syllabification, mapping words to their corresponding syllables. Adedjouma et al. [20] curate a Yor\u00f9b\u00e1 corpus focused on POS tagging, and Abiola et al. [12] develop an English- to-Yor\u00f9b\u00e1 translation system. Adegunlehin et al. [27] improve Yor\u00f9b\u00e1 NER by incorporating contextual information such as surrounding words and POS tags. Omolaoye [224] introduce a computational approach for the representation of Yor\u00f9b\u00e1 proverbs, contributing to indigenous knowledge preservation. Ugwu et al. [282] train a POS tagger using Yor\u00f9b\u00e1 religious texts and dictionary entries. Toyin et al. [272] introduce a Hidden Markov Model (HMM)-based POS tagger, contributing a manually annotated dataset of 1,000 Yor\u00f9b\u00e1 sentences drawn from various domains.\nThe development of POS tagging and NER tools for Hausa, Igbo, and Yor\u00f9b\u00e1 has contributed significantly to NaijaNLP. These tools are critical in addressing language-specific challenges, enhancing computational linguistic resources, and facilitating various downstream tasks. While existing resources have proven useful, further research and dataset expansion are needed to bridge gaps and improve linguistic tool development for NaijaNLP."}, {"title": "3.4 Downstream Tasks", "content": "In NLP, downstream tasks such as text classification, sentiment analysis, text summarisation, machine translation, and speech recognition rely fundamentally on robust linguistic resources, including annotated datasets, computational models, and standardised frameworks. In this section, we review the existing downstream tasks and applicable resources developed for Hausa, Igbo, and Yor\u00f9b\u00e1 languages."}, {"title": "3.4.1 Text Classification and Summarisation", "content": "Text classification (TC) involves assigning textual data to predefined classes based on similarity or dissimilarity, using a range of methodologies from rule-based approaches to machine learning, deep learning, and transformer-based models. Complementary to text classification, text summarisation (TS) aims to generate concise and coherent summaries that retain the essential information of longer texts. To that end, Asubiaro et al. [73] propose a strategy for language identification at the word-level to enhance model performance for LRLs, thereby improving the prediction of a word's language. In a related effort, Varab and Schluter [286] develop a multilingual text summarisation framework by enriching existing resources to facilitate machine translation (MT) on a large scale. Furthermore, Olalekan et al. [219] introduce multilingual text classification models for English, Yor\u00f9b\u00e1, and Hausa, while Bashir et al. [80] present a model trained on a corpus of Hausa documents for automatic summarisation. Additional contributions include the work of Bichi et al. [87, 88], who develop an automatic summarisation strategy for Hausa text. For Igbo, Nkechi et al. [199] and Nkechi et al. [198] propose classification systems based on n-gram models and k-nearest neighbours techniques to improve text representation and classification. In parallel, Mbonu et al. [184] offer a detailed discussion about the development of the IgboSum1500 dataset, a dedicated resource for Igbo text summarisation. Meanwhile, Adegoke-Elijah et al. [26] explore models designed to compute semantic similarity between Yor\u00f9b\u00e1 sentences. For text summarisation, in particular, the development of more purposive datasets and models capable of better contextual understanding, semantics, and inter-textual relationships will further enrich NaijaNLP."}, {"title": "3.4.2 Sentiment Analysis", "content": "Sentiment analysis (SA), a specialised form of text classification, involves the identification and extraction of subjective information from the text to determine its sentiment - typically categorised as positive, negative, or neutral. In addressing the unique linguistic characteristics of LRLs, several studies have contributed to advancing SA in NaijaNLP. For example, Shehu et al. [263] employ deep learning techniques and hierarchical attention networks to enhance sentiment analysis in Hausa. Similarly, Akande et al. [52] and Ibrahim et al. [154] develop models to detect aspect-level sentiment in tweets and Hausa movie reviews, respectively. Furthermore, multilingual sentiment analysis systems have been proposed by Raychawdhary et al. [250, 252] and Abdullahi et al. [7], which recognise the linguistic diversity inherent in LRLs such as Hausa, Yor\u00f9b\u00e1, and Igbo. In addition, Abdou et al. [1] present a framework for assessing online geopolitical news based on sentiment and public attention across multiple African countries including Kenya, Nigeria, Senegal, and South Africa. Complementary approaches include Abubakar et al. [13], who develop a multilingual sentiment analysis system for English and Hausa tweets using an enhanced feature acquisition method, and Rakhmanov and Schlippe [246], who propose a system for analysing Hausa student comments via both monolingual and cross-lingual approaches-further supported by a stemming algorithm and a training dataset comprising over 40,000 comments.\nThere are additional studies that focus on domain-specific challenges. For instance, Sani et al. [261] develop a system for sentiment analysis on Hausa data extracted from BBC Hausa's Twitter handle. Mohammed and Prasad [187] build a lexicon-based sentiment analysis system tailored for LRLs, and Abdullahi et al. [6] refine sentiment analysis for abbreviated terms in Hausa using an improved dataset with resolved abbreviations and acronyms. In the Igbo context, Ogbuju and Onyesolu [208] introduce general-purpose sentiment lexicons (IgboSentilex), and Okoloegbo et al. [215] present an interactive system for detecting, monitoring, and regulating cyberbullying content in Igbo and Pidgin English. For Yor\u00f9b\u00e1, Adeniji et al. [34] propose a system for disambiguating sentiment lexicons in Yor\u00f9b\u00e1 texts, supported by curated sentiment lexicons derived from diverse sources, while Abegunde et al. [11] and Shode et al. [265] explore enhancement strategies and develop datasets (e.g., YOSM) based on movie reviews to support sentiment analysis."}, {"title": "3.4.3 Machine Translation", "content": "Machine Translation (MT) focuses on the automatic translation of text or speech from one language to another, and it has evolved from rule-based systems and statistical methods to neural networks and transformer-based models. Effective MT systems are heavily dependent on large volumes of parallel data to generate accurate translations. Mahata et al. [178", "53": "develop a translation system for English-Hausa that leverages parallel corpora. Additionally, Tresner-Kirsch et al. [274", "290": "present an approach for deploying MT on the edge, thereby enabling an embedded system for English-Hausa translation. In another useful approach, Brugnone et al. [92", "148": "who integrate visual (contextual) and textual information to enhance translation accuracy across language pairs including English, Hindi, Malayalam, Bengali, and Hausa, achieving superior performance compared to text-only baselines. Robertson and D\u00edaz [257", "181": "and Usip et al. [284", "212": "contribute an MT system for English-Igbo translation. For Yor\u00f9b\u00e1, Adegbola et al. [24", "134": "and Odoje [2"}]}