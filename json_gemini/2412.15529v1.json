{"title": "XRAG: examining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation", "authors": ["Qianren Mao", "Yangyifei Luo", "Jinlong Zhang", "Hanwen Hao", "Zhilong Cao", "Xiaolong Wang", "Xiao Guan", "Zhenting Huang", "Weifeng Jiang", "Shuyu Guo", "Zhentao Han", "Qili Zhang", "Siyuan Tao", "Yujie Liu", "Junnan Liu", "Zhixing Tan", "Jie Sun", "Bo Li", "Xudong Liu", "Richong Zhang", "Jianxin Li"], "abstract": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data\nwith the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate and\ncurrent. We introduce XRAG, an open-source, modular codebase that facilitates\nexhaustive evaluation of the performance of foundational components of advanced\nRAG modules. These components are systematically categorized into four core\nphases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically\nanalyse them across reconfigured datasets, providing a comprehensive benchmark\nfor their effectiveness. Given the escalating complexity of RAG systems, we\nunderscore the necessity of identifying potential failure points of RAG modules. We\nformulate a suite of experimental methodologies and diagnostic testing protocols\nto dissect the failure points inherent in engineering RAG modules. Subsequently,\nwe proffer bespoke solutions designed to augment the validation processes and\nbolster the overall performance of these modules. Our work thoroughly evaluates\nthe performance of core advanced components in RAG systems, providing insights\ninto optimizations for prevalent failure points.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) [1, 4, 13, 19] represents a pivotal strategy in Q&A tasks,\ndemonstrating enhanced performance by delivering more informative and accurate answers compared\nto relying solely on large language models (LLMs). The efficacy of Basic RAG systems is contingent\nupon the seamless operation of four core components: pre-retrieval, retrieval, post-retrieval, and\ngeneration. The pre-retrieval stage indexes datasets and reforms queries for efficient retrieval. The\nretrieval stage uses advanced algorithms to extract relevant documents. The post-retrieval stage\nrefines, summarizes, or compacts information to ensure contextual clarity. Finally, the generation\nstage employs the LLM to produce coherent responses, highlighting the interdependence of the\nRAG framework. These stages operate sequentially, with each critically impacting the overall output\nquality. Advanced RAG techniques (e.g., Reranker, Refiner) offer a sophisticated toolkit for tailored\nsearch solutions, surpassing standardized methodologies. RAG systems integrate external data"}, {"title": "2 Related Works", "content": "Existing Retrieval-Augmented Generation (RAG) toolkits, such as LangChain [6] and LlamaIn-\ndex [25], enable rapid RAG system construction using pre-built models. These toolkits enhance\nflexibility and expand potential applications by modularising the RAG process. However, excessive\nencapsulation limits their transparency and usability.\nAlthough ongoing efforts, such as FastRAG [17], RALLE [15], LocalRQA [38], AutoRAG [23],\nFlashRAG [20], and RAGLAB [39], address these issues by featuring and implementing modular\nRAG process, such as retrieval engines and generative agents. There remains an implicit gap\nin the comparative performance evaluation of these advanced RAG modules within the overall\nRAG workflow. FastRAG [17] and RALLE [15] allow users to assemble RAG systems with core\ncomponents, fostering a more adaptable RAG implementation. AutoRAG [23] further supports\nusers by identifying optimal RAG pipelines for custom data, facilitating bespoke RAG systems.\nLocalRQA [38] and RAGLAB [39] focus on RAG training, offering scripts for various component\ntraining. Nevertheless, FastRAG, RALLE, AutoRAG, and LocalRQA require users to reproduce\npublished algorithms independently and offer limited component options, restricting RAG diversity\ndespite modular designs."}, {"title": "3 XRAG", "content": "Figure 1 delineates the architectural hierarchy of the XRAG framework, incorporating multi-tiered\nabstraction. The framework is stratified into the core, algorithmic, auxiliary, extension, and appli-\ncation programming interface (API) layers, progressing from foundational to application-oriented\ncomponents. Designed with a modular architecture, XRAG enables users to accomplish the fol-\nlowing: preparation of normalized RAG datasets (Section 3.2), assembly of the RAG components\n(Section 3.1), evaluation of RAG system's core components (Section 3.3), and diagnostic optimization\nof RAG system failures (Section 2 & Appendix A.2 )."}, {"title": "3.1 Advanced Component Modules", "content": "Pre-retrieval Before retrieval, the pre-retrieval components leverage LLMs to refine user queries,\nenhancing the quality and relevance of the information retrieval process. Key methodologies include\nStep-back Prompting (SBPT [40]): Broaden's queries to enrich contextual grounding for answers,\nenhancing the contextual foundation for answer generation. Hypothetical Document Embedding\n(HyDE [11]): Transmutes the original query into a form that better aligns with the indexed documents,\nimproving retrieval alignment and efficacy. Chain-of-Verification (CoVe [9]): Executes a verification\nplan for further refining system responses into an enhanced one. Both HyDE and CoVe mandate pre-\nretrieval instructions, which involve generating hypothetical documents and responses consultation\nusing query first, affirming their role as core components of the pre-retrieval process.\nRetriever For advanced retrieval strategies, we integrate the LlamaIndex to facilitate standard\nadvanced methods. LexicalBM25 retriever ranks documents based on query term occurrence and\nrarity across the corpus. Simple Fusion Retriever (SQFusion) augments the query by generating\nrelated sub-queries and returns top-k nodes across all queries and indexes. Reciprocal Rerank Fusion\nRetriever (RRFusion [8]) fuse indexes with a BM25-based retriever, capturing both semantic relations"}, {"title": "3.2 Unified Benchmark Datasets & Corpus", "content": "We collect and preprocess three benchmark datasets for the XRAG framework, emphasising rigorous\nexperimental validation of RAG systems. We develop a unified dataset structured to facilitate\nperformance testing for both retrieval and generation modules, incorporating standardized formats:\nUser-Query || Retrieval-Context || Retrieval-Context. IDS || Golden-Context ||\nGolden-Context. IDS || Actual-Response || Expected-Answer."}, {"title": "3.3 Evaluation Methods", "content": "XRAG supports a variety of evaluation metrics to assess the quality of the RAG process. We integrate\nthe Jury [5], a comprehensive package for the evaluation of NLG systems, with RAG community\nevaluation tools such as UpTrain 3 and DeepEval 4. Our metrics are pivotal in determining the\neffectiveness of both the retrieval and generation components within RAG systems. Depending\non the subject of evaluation, our supporting metrics can be categorized into three primary groups:\nConventional Retrieval Evaluation and Conventional Generation Evaluation, along with an additional\ncategory for Cognitive LLM Evaluation.\nConventional Retrieval Evaluation (ConR). It supports four primary metrics: F1, EM, Mean\nReciprocal Rank (MRR), and Mean Average Precision (MAP), along with Hit@1 and Hit@5.\nAdditionally, it includes the DCG family of metrics, which assesses the effectiveness of ranking\nmodels by evaluating the quality of ordered results. This family comprises Discounted Cumulative\nGain (DCG), Normalized Discounted Cumulative Gain (NDCG), and Ideal Discounted Cumulative\nGain (IDCG), the latter representing the maximum DCG achievable with ideally ranked results\narranged in descending order of relevance (Golden-Context.IDS in our libraries).\nConventional Generation Evaluation (ConG). These generative-token matching metrics can be\nclassified into three broad categories. N-gram similarity metrics, ChrF [31], ChrF++ [32], \u039c\u0395-\nTEOR [2], ROUGE F1 [33] (ROUGE-1, ROUGE-2, ROUGE-L, and ROUGELSUM) focus on\noverlap in n-grams between generation and reference. Divergence-based metrics (MAUVE [30],\nPerplexity [18]) measure content quality, diversity, and model learning by comparing the distribution\nof generated and reference outputs. Error-based accuracy metrics, Word Error Rate (WER) [29],\nCharacter Error Rate (CER) [29] and Exact Match (EM), assess the accuracy of generated outputs by\ncalculating the differences or errors when compared with the expected answer.\nCognitive LLM Evaluation (CogL). Cognitive LLM Evaluation metrics, derived from UpTrain\nand DeepEval, are classified based on the parameters used by our framework. Metrics involving\nresponse-related parameters, such as Actual-Response or Expected-Answer, are Generation Met-\nrics. Conversely, metrics that lack response parameters include retrieval-related parameters, such as"}, {"title": "3.4 Systematic Diagnostics of RAG Failures", "content": "RAG systems have significant potential for providing more accurate and contextually relevant\nresponses to user queries. However, they also face several challenges that hinder their performance\nand reliability [3, 7]. These challenges include the tendency of models to generate deceptive responses\nunder uncertainty, improper ranking of retrieval results, incomplete answers, sensitivity to noise, and\nlimitations in handling complex reasoning tasks, as shown in Figure 2. Understanding these issues is\ncrucial for recognizing the current boundaries of RAG technology and identifying areas where further\nresearch and development are needed.\nNegative Refusal: The challenge of negative refusal in RAG systems, where models tend to produce\ndeceptive responses instead of acknowledging uncertainty, will severely erode user trust. The issue of\nnegative refusal often stems from the model's lack of awareness of its knowledge boundaries. When\nconfronted with a query that lacks sufficient information, the model may generate a factually incorrect\nor misleading response rather than transparently admitting the absence of relevant knowledge.\nRanking Confusion: Existing studies have shown that LLMs are more attentive to the earlier\nparts of input sequences. This characteristic poses a significant issue for RAG systems: even if\nthe retrieval module successfully locates the correct document segments, the system may still be\naffected if the most relevant segments do not appear early in the input sequence. In such cases, the\ngeneration module will initially encounter suboptimal or erroneous information, ultimately impacting\nthe accuracy and quality of the RAG system's output.\nAnswer Absence: The RAG system is designed to integrate information retrieved by the retrieval\nmodule with the inference capabilities of the generative model. However, a common challenge is\nthat LLMs may overlook relevant details during the answer generation phase, even when all related\ncontexts have been correctly retrieved. This issue often arises from the limitations in the reasoning\ncapabilities of these models and problems associated with the format or order in which the context\nis presented. For instance, the existence of multiple methods for inputting context to LLMs in\nLlamaIndex can result in disparate response generation mechanisms, potentially impeding the RAG\nsystem's capacity to access and utilize critical contextual information effectively.\nNoise Impact: The robustness of RAG systems against noise is another critical aspect of their\nperformance, especially when dealing with sources containing irrelevant or misleading information.\nDue to variations in the retrieval model's accuracy, the way users formulate their queries, and stylistic\ndifferences, the document chunks retrieved often contain some degree of irrelevant content, which, if\nfed into the LLMs, can significantly affect its reasoning performance and the final response."}, {"title": "4 Experimental Results and Discussion", "content": "4.1 Experimental Setup\nBoth the retriever and Q&A LLMs are essential modules of the RAG system. To focus on evaluating\nthe Q&A capabilities of different LLMs, we fixed the retriever to the BGE-LARGE model version,\nas the retriever serves as the primary entry point influencing RAG performance. For document\npreprocessing, we utilized SentenceSplitter to divide documents into chunks and construct a vector\nindex. SentenceSplitter was configured with a chunk size of 1024 tokens, representing the maximum\nsequence length and a chunk overlap of 20 tokens, denoting the overlap between consecutive chunks.\nWe adhered to LlamaIndex configurations for other RAG components, including the refine module\nfor response synthesis. To ensure compatibility and efficiency, XRAG integrates Huggingface\nTransformers. All Q&A LLMs were set with Temperature= 0 to ensure experimental consistency.\nFor each query, five chunks were retrieved as contextual data. Consequently, the evaluation metrics\nmeasure retrieval accuracy based on five retrieval nodes, fully encompassing the assumptions of\nmost datasets that typically consider only one or two golden contextual nodes. The metrics include\nHIT, DCG, NDCG, and IDCG, with a search depth K = 5. The generator model's context window,\nencompassing the query, prompt, retrieved context, and response content, is set to 4096 tokens.\nWe perform RAG testing on the entire test set in the two rule-based evaluations (Conventional\nRetrieval Evaluation and Conventional Generation Evaluation). For the Cognitive LLM Evaluation,\ntoken costs are substantial, as they stem from both the input and output tokens in the LLM's reasoning\nand LLM's testing processes. Therefore, we randomly sample the complete test set $E_{no}$ times. The\ntest result calculation is $((score)/P_{sc})/n$ and $E_{no}$ is 3 in our experiments, and $P_{sc}$\ndenotes the number of successful LLM API requests out of the $E_{sp}$ samples. In the benchmark\nexperiments evaluated by LLM retrieval and response metrics (Section 4.2), we set $E_{sp}$ to 100. For\nthe evaluation of RAG failures (Appendix A.2) and ."}, {"title": "4.2 Benchmark Retrieval and Response Evaluation", "content": "4.2.1 Retrieval Quality\nThe retrieval performance varies sensibly across the three datasets, with the poorest Q&A quality\nobserved on DropQA in Table 3. Since DropQA retrieves at the passage level, it presents a greater\nretrieval challenge. The NDCG metric reflects that the baseline retrieval model achieves basic\nadequacy in relevance and ranking accuracy on HotpotQA and NaturalQA datasets. Ignoring ranking\naccuracy, a high Hit@5 score (>0.8) indicates a substantial likelihood of retrieving semantically"}, {"title": "5 Limitation", "content": "Our XRAG toolkit currently has several limitations we intend to address in future updates. i. Although\nwe aim to incorporate key RAG submodules, including retrieval baseline, major LLM models, and\nadvanced modules, time cost and compatibility constraints prevent comprehensive coverage of all\nRAG advancements, which may require open-source contributions. ii. The toolkit lacks support\nfor training RAG components. While training was considered in the design phase, the diversity of\nmethods and the availability of dedicated repositories led to its exclusion. In the future, we may\nprovide auxiliary scripts to assist with training needs. iii. Currently, our toolkit includes datasets\nfocused on multi-hop question answering, numerical reasoning, and logical reasoning. Future\nupdates could expand to encompass OpenQA [21, 27], long-form QA [28, 36], and multiple-choice\nQA [14, 16]. We encourage the open-source community to overcome these limitations. Our goal is to\npersistently refine the XRAG framework by delivering a more efficient and dependable platform with\ncomprehensive evaluation and development tools."}, {"title": "6 Conclusion", "content": "This work introduces XRAG, an open-source framework for benchmarking advanced RAG systems\nthat facilitates exhaustive evaluation of foundational components across pre-retrieval, retrieval, post-\nretrieval, and generation phases. It introduces a modular RAG process, unified benchmark datasets,\ncomprehensive testing methodologies, and strategies for identifying and mitigating RAG failure\npoints. The framework supports a range of evaluation metrics, including conventional retrieval and\ngeneration assessments and cognitive LLM evaluations. We believe XRAG will empower researchers\nto construct and evaluate RAG modules, streamlining their workflows efficiently."}, {"title": "A.1 More RAG Core Components Analysis", "content": "A.1.1 Advanced Pre-Retrieval Modules\nIn Table 8, SBPT and its 5-shot variant excel in Dp-CRel and Up-RCns, highlighting their ability to\nenhance the relevance and consistency of the retrieval context, which is crucial for the subsequent\ngeneration task. CoVe, despite achieving high relevance, shows a trade-off with factual accuracy, as\nevidenced by lower Dp-Fath and Dp-Hall scores.\nA.1.2 Advanced Retriever Modules\nThe experimental results from Table 9 demonstrate a clear performance hierarchy among the retrieval\nmodules. RecuChunk and LexicalBM25 lead in terms of MRR and Hit@1, suggesting their robustness\nin identifying relevant documents early in retrieval. HiParser and StParser have the highest F1 scores\nof 91.50 and 92.50, respectively, and both achieve an MRR of 93.75 and 97.50, with Hit@1 scores\nof 90.00. Their NDCG scores are 95.31 and 98.16, respectively, which are among the highest,\nsuggesting excellent retrieval performance. Conversely, TreeLeaf and LongCTrrk lag in multiple\nmetrics, including F1 and MRR, which may point to limitations in their retrieval strategies."}, {"title": "A.2 Optimization Strategies & Evaluation of RAG Failures", "content": "In the previous section, we summarized five types of failures in RAG systems. These issues affect the\nperformance and reliability of RAG systems and limit their effectiveness in practical applications. To\naddress these challenges, we have proposed various optimization strategies, constructed evaluation\ndatasets, and utilized detailed evaluation metrics to assess the effectiveness of the improvements, as\nshown in Table 10."}, {"title": "A.2.1 Negative Refusal", "content": "Optimization Strategies.\n\u2022 Prompt Engineering: Explicit prompts can encourage LLMs to engage in more thoughtful and\njudgmental reasoning, aiming to elicit negative responses at appropriate times. As an effective\nmeans commonly employed in the domain of LLMs to address a wide range of problems, prompt\nengineering represents the simplest solution to the challenge of negative refusal.\n\u2022 Two-step Reasoning: The two-step reasoning process leverages the capabilities of LLMs first\nto assess their ability to answer a given query. Only when the model determines it has adequate\ninformation to respond does it proceed to the second step to provide a specific answer, thus arriving\nat the final inference. Conversely, if the initial assessment concludes that there is insufficient\ninformation to answer, the second step of reasoning is declined, thereby preventing the generation\nof hallucinatory responses.\nExperimental Settings. To quantify the system's capacity for negative refusal, we first constructed a\ndataset comprising 20 randomly sampled questions, each paired with unrelated context to simulate\nscenarios where the model cannot access necessary information. We adopted the rejection rate as\nan evaluation metric to measure the system's ability to recognize and acknowledge situations that\ncannot provide a valid response. A higher rejection rate indicates a higher level of self-awareness\nand reliability in the system's responses. Without any optimizations, we established a baseline by\nmeasuring the system's rejection rate, then applied each optimization method, recording the rejection"}, {"title": "A.2.2 Ranking Confusion", "content": "Optimization Strategies.\n\u2022 Re-ranking: Employing a re-ranking model for more refined similarity calculations, although\ncomputationally more expensive than initial retrieval and typically applied only to smaller sample\nsizes, can significantly enhance the relevance and precision of retrieval results. In this section, we\nopt for the commonly used ColBERTv2 (Cohere rerank model) as the re-ranking model to perform\nfiner-grained matching on the coarse retrieval outcomes.\n\u2022 Hybrid Retrieval: Integrating multiple retrieval methods can effectively broaden the diversity\nof retrieval results, allowing for the identification of relevant document segments from various\nperspectives, which could improve ranking accuracy. Specifically, we implement a hybrid retrieval\napproach combining BM25 with vector-based retrieval to construct the RAG system's hybrid\nretriever.\n\u2022 Hybrid Retrieval and Re-ranking: After implementing the hybrid retrieval strategy, further\napplication of the re-ranking method combines the dual advantages of diversified retrieval and\nprecise similarity calculation. This approach aims to more effectively identify and rank the most\nrelevant document segments, thereby enhancing the overall performance of the RAG system.\nExperimental Settings. For the verification, we selected 20 samples with lower F1 scores under\nstandard RAG retrieval conditions to facilitate the validation of the effectiveness of various methods.\nIn terms of evaluation metrics, we use traditional measures such as F1 and EM to assess retrieval\naccuracy, along with additional metrics like HIT@1, MRR, MAP, DCG, IDCG, and NDCG to\nprecisely evaluate the extent to which the aforementioned optimization strategies improve the correct\nordering of retrieval results."}, {"title": "A.2.3 Answer Absence", "content": "Optimization Strategies.\n\u2022 Simple Summarize: All retrieved document chunks are concatenated into a single text block and\nfed into the LLMs in one go.\n\u2022 Refine: Each retrieved document chunk undergoes a separate Q&A session with the LLMs, with\ninputs including the original query, the answer from the previous round, and the current document\nchunk. A predefined prompt is used to refine each answer to elicit more detailed responses.\n\u2022 Compact: Document chunks are first merged into longer blocks as much as possible before\napplying the Refine method, reducing the number of calls to LLMs.\n\u2022 Compact Accumulate: Document chunks are similarly merged into longer blocks, but each\nchunk undergoes independent Q&A sessions with inputs being the original query and the current\ndocument chunk, without the answer from the previous round. All results are then combined to\nform the final response.\nExperimental Settings. To conduct our experiments, we randomly sampled a batch of questions,\nproviding the questions and their golden contexts to the LLMs to obtain responses for each question.\nSubsequently, through manual screening, we selected 20 questions and their corresponding golden\ncontexts that exhibited missing answers to serve as the evaluation dataset for this section. To\ncomprehensively validate the effectiveness of various methods, we adopt Factual Accuracy, Response\nConsistency, Context Utilization, Response Completeness, and Response Matching as evaluation\nmetrics."}, {"title": "A.2.4 Noise Impact", "content": "Optimization Strategies.\n\u2022 Re-ranking: Employing a re-ranking model for more refined similarity calculations not only serves\nas a potential solution to the ranking confusion problem but can also help filter out irrelevant\ndocuments through threshold filtering or quantity filtering, thereby reducing the amount of noise\ninput to LLMs and improving the performance of the RAG system.\nExperimental Settings. We randomly sampled 20 query instances and combined them with their\ncorresponding golden contexts and varying numbers of irrelevant document chunks to form the\nevaluation dataset. This allows us to explore the changes in RAG system performance under different\nproportions of noisy documents, as well as the enhancement of system capability after incorporating"}, {"title": "A.2.5 Complex Reasoning", "content": "Optimization Strategies.\n\u2022 Query Rewriting: By rewriting user queries using an LLM to add and introduce more additional\ninformation, this approach aims to guide the retrieval module to access more relevant documents\nexplicitly.\n\u2022 Query Decomposition: Breaking down complex questions into simpler sub-questions that focus\non individual aspects simplifies the reasoning process, enabling a comprehensive search for all\nrequired information.\n\u2022 Few-shot Prompting: By prepending a few complex reasoning examples to the input sequence of\nthe LLM, this method aims to guide and stimulate the model's latent reasoning abilities, thereby\nenhancing the effectiveness of complex reasoning.\nExperimental Settings. For verification of complex reasoning, we randomly sampled 20 items from\nthe Hard subset of HotpotQA to serve as the task set for complex logic. Similarly, we adopted the exact\nsame metrics used in A.2.3 to evaluate the improvement effects of each solution comprehensively."}, {"title": "A.6 Prompt for Pre-Retrieval Instructions", "content": "Prompt for Few-shot HyDE\nYou are a sophisticated AI model. Please write a passage to answer the question. Try to include as many\nkey details as possible:\n1.\nQuestion: At It Again contains lyrics co-written by the singer and actor from what city?\nPassage: The song \"At It Again\" features lyrics co-written by the singer and actor who hail from\nthe vibrant city of Los Angeles. This city, known for its thriving entertainment industry, has been a\nhub for countless talented individuals who have made their mark in music, film, and television. The\nsinger and actor, both born and raised in Los Angeles, have been deeply influenced by the diverse and\ncreative atmosphere of their hometown. Their collaboration on \"At It Again\" showcases their unique\nperspectives and storytelling abilities as they draw inspiration from their personal experiences and the\nrich cultural tapestry of Los Angeles. Through their lyrics, they paint a vivid picture of the city\u015b energy,\nits dreams, and its challenges, capturing the essence of their beloved hometown in every verse.\n2.\nQuestion: {question}\nPassage: {generated passage}\n/* Few-shot prompt-engineering */ /* ToDo: bundle original prompt*/\nPrompt for Few-shot SBPT\nYou are an expert at world knowledge. Your task is to step back and paraphrase a question to a more\ngeneric step-back question, which is easier to answer.\nHere are a few examples:\n1.\nOriginal Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character\nMilhouse, who Matt Groening named after who?\nStepback Question: Who are some notable figures that have influenced the names of \"The Simpsons\"\ncharacters?\n2.\nOriginal Question: {question}\nStepback Question: {generated stepback question}\n/* Few-shot prompt-engineering */ /* ToDo: bundle original prompt*/\nPrompt for Few-shot CoVe\nYou are a world-class, state-of-the-art agent.\nYou have access to multiple tools, each representing a different data source or API. Each of the tools\nhas a name and a description, formatted as a JSON dictionary. The keys of the dictionary are the names\nof the tools, and the values are the descriptions. Your purpose is to help answer a complex user question\nby generating a list of sub-questions that can be answered by the tools.\nThese are the guidelines you consider when completing your task:\n* Be as specific as possible.\n* The sub-questions should be relevant to the user question.\n* The sub-questions should be answerable by the tools provided.\n* You can generate multiple sub-questions for each tool.\n* Tools must be specified by their name, not their description.\n* You don't need to use a tool if you don't think it's relevant.\nHere are some examples:\n1.\nOriginal Question: How was Paul Graham's life different before, during, and after YC?\nSub-questions:"}, {"title": "A.6.1 Development Web UI of XRAG", "content": "The Web UI of XRAG enables developers to construct tailored action chains involving multiple\ninference steps. Figures 5, 4, 6, 7 and 8 illustrate examples of XRAG serving methods. XRAG offers\nan intuitive front-end interface for uploading datasets in Figure 4, whether from this study or custom\nsources. It includes an interactive configuration tool in Figure 5 that allows users to configure API\nkeys and parameter settings and build vector database indices directly.\nAdditionally, XRAG features a static evaluation page in Figure 6 for evaluating pre-generated\nresponses. Figure S shows developers can create query-specific prompt templates using the integrated\nquery engine. XRAG also provides comprehensive evaluation (Figure 7) and facilitates the execution\nof individual actions with customized prompts, enabling immediate review of retrieval and generation\noutcomes."}]}