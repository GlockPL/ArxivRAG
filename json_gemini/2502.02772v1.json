{"title": "Cross-Modality Embedding of Force and Language for Natural Human-Robot Communication", "authors": ["Ravi Tejwani", "Karl Velazquez", "John Payne", "Paolo Bonato", "Harry Asada"], "abstract": "Abstract\u2014A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions.", "sections": [{"title": "I. INTRODUCTION", "content": "As stipulated in Asimov's laws, robots must perform tasks based on human instruction. A central challenge in robotics has been developing effective ways for humans to communi-cate and give instructions to robots. Early roboticists attempted to develop robot languages, e.g. VAL [19], to describe desired robot actions and tasks. However, despite some success, it became apparent that certain motions and behaviors are dif-ficult, inefficient, or impossible to describe through language alone. This limitation is particularly evident in tasks requiring environmental contact and force/compliance control, where forces and moments are not directly visible. Such manipulative tasks, now termed contact-rich manipulation [20, 4], often involve subconscious knowledge that humans find difficult to articulate through any form of language. To address this challenge, roboticists developed alternative approaches that bypass the need to translate subconscious skills into language. These approaches include teaching by demonstration [3], programming by demonstration [7], skill acquisition [17] and, more recently, imitation learning [26].\nWith recent advances in natural language processing, language-grounded robot control has gained significant mo-mentum [28, 18]. This approach will play a crucial role in scenarios in which robots and humans interact closely. However, a fundamental question persists: Can language alone effectively convey human intentions, instructions, and desired behaviors to robots?\nConsider situations where a human wants a robot to gently touch an object or their body. Such actions are difficult to describe verbally; instead, humans prefer to physically demonstrate the desired gentleness of touch. Yet, physical demonstration alone cannot convey important context, nuance, and reasoning behind the action. This illustrates how language and touch/force are complementary modalities that must be integrated and coordinated for effective human-robot commu-nication.\nTo study this problem, we collaborated with physical ther-apists from Spaulding Rehabilitation Hospital who demon-strated various therapeutic techniques. Figure 1 shows one such demonstration, where the therapist first explains the procedure verbally and then demonstrates by gently turning the patient's leg. If a robot were to suddenly move the patient's leg, the patient would likely feel uncomfortable or frightened. On the other hand, the verbal explanation, \u201cI will lift your leg gently\", is ambiguous to the patient, wondering how gentle is gentle. They may be anxious to see whether it is painful. The therapist starts pushing the leg immediately after giving the brief explanation, demonstrating what she means by lifting the leg gently. This example highlights how language and physical touch/force serve as two distinct but complementary modalities for describing tasks and communicating intended behaviors. The challenge lies in integrating them effectively.\nThe goal of the current work is to establish a unified method for representing language and force that facilitates their integration and coordination. We make the following contributions:\n1) A framework for cross-modality embedding of force profiles and words, enabling translation between physi-cal force curves and natural language descriptions;\n2) A paired data collection methodology with 10 par-ticipants performing language-to-force and force-to-language translations, capturing human intuition about force-language relationships;\n3) Evaluation metrics and results validating the frame-work's effectiveness and generalization on unseen data;\""}, {"title": "II. RELATED WORK", "content": "Force-based interactions have been studied in the past for human-robot collaborative tasks. Early work in [2] demon-strated automatic program generation from force-based teach-ing data. Furthermore, [13] established the significance of force feedback in human-robot interfaces, putting down the groundwork for new interaction paradigms. Recent work has significantly improved our understanding of force-based ma-nipulation, with [10] demonstrating planning for tool use under force constraints and [11] further extending this to robust multi-stage manipulation tasks.\nResearch on using force sensing for improved physical human-robot interaction was explored in [8] showing methods for learning from demonstration using force data [27] and explaining human intent from contact forces [22]. However, these works are applicable to tasks under specific conditions; broader task variability, diverse conditions and contexts, and subtle nuance that language can describe are not considered."}, {"title": "B. Grounding Natural Language in Robot Actions", "content": "Natural language has been investigated in literature for grounding language phrases to robot actions and behaviors. [28] developed probabilistic approaches for mapping natural language instructions to robot trajectories. Building on this, [18] showed methods for learning semantic parsers that ground language to robot control policies. Recent work has shown the use of large language models to improve language understand-ing for robotics [1] [6] . While these approaches map language to robot actions, the tasks are mostly pick-and-place, and more complex manipulative tasks that involve contact forces are excluded."}, {"title": "C. Multimodal Embeddings in Robotics", "content": "Research in learning shared embedding spaces between different modalities for robotic learning has been explored in the past. [16] developed cross-modal embeddings between visual and force data for manipulation tasks. [30] showed learning joint embeddings of language and vision for a robot instruction navigation task. Although, these approaches have demonstrated the potential of multimodal embeddings in robotics, none have specifically addressed the challenge of creating shared embeddings between force trajectories and natural language descriptions.\nThe current work aims to fill this gap by developing and providing a framework of bidirectional translation between physical forces and their linguistic descriptions. Inspired by physical therapists' interactions with patients, we will address the needs for unified representation of language and force profiles and effectiveness of force-language cross-modality embedding to better understand how these strikingly distinct modalities can be integrated."}, {"title": "III. PRELIMINARIES", "content": "We first introduce a coordinate system to consistently define spatial directions and interpret force profiles (Table 2). Each force measurement is a vector $F(t) \\in \\mathbb{R}^3$ with components $(F_x(t), F_y(t), F_z(t))$. Fig. 2 shows linguistic direction to its corresponding axis:"}, {"title": "B. Force Profile", "content": "We record time-varying force data using a force-torque sen-sor mounted on the UR robot's end-effector. For a recording $T$ seconds, we store each timestamp $t_i \\in [0, T]$ along with the measured force vector $F(t_i)$. We refer to the set of samples, ordered chronologically as a force profile. Formally, a force profile is represented as a $4 \\times N$ tensor:\n$\\text{Force Profile} = \\begin{bmatrix} t_0 & t_1 & \\dots & t_{N-1} \\\\ F_x(t_0) & F_x(t_1) & \\dots & F_x(t_{N-1}) \\\\ F_y(t_0) & F_y(t_1) & \\dots & F_y(t_{N-1}) \\\\ F_z(t_0) & F_z(t_1) & \\dots & F_z(t_{N-1}) \\end{bmatrix}$ \t where $t_0 = 0$ and $t_{N-1} = T$. Figure 4 shows examples of force profiles, paired with textual instructions.\nTo interpret forces quantitatively, we adopt Newton's second law of motion:\n$\\dot{p}(t) - \\dot{p}(0) = \\int_{0}^{T} \\vec{F}(t) dt = \\vec{J}(t)$ \t where at time $t, \\vec{p}(t)$ is the momentum, $\\vec{F}(t)$ is the applied force, and $\\vec{J}(t)$ denotes impulse. This unlocks an intuition of the elementary pillars that describe force profiles: direction $\\vec{F}(t)$, magnitude $|\\vec{F}(t)||$, and duration $T$."}, {"title": "C. Language", "content": "Throughout this paper, we define a phrase as an ordered list of words describing a motion or force profile (e.g. \"slowly for-ward\", \"quickly right and up\"). To handle these numerically, we consider two distinct vocabularies.\n1) Minimal Viable Vocabulary: This vocabulary contains 18 direction words (e.g. left, right, forward-down) and 12 modifier words (e.g. slowly, quickly, harshly) that describe variations in force magnitude and duration [12].\nBinary Phrase Vectors: The direction words require 18 dimensions and modifier words require 12 dimensions (as shown in Table I). Each word is encoded as a 31-dimensional basis vector, where the additional dimension represents an empty or null word. These binary phrase vectors are then concatenated to form a 62-dimensional vector, where exactly two positions contain a 1 one in the first 31 dimensions identifying the modifier (or empty modifier) and one in the second 31 dimensions identifying the direction (or empty direction). This binary encoding scheme ensures consistent representation while allowing for partial or incomplete phrases through the accomodation of empty words.\n2) Extended GloVe Vocabulary: GloVe Embeddings: We make use of the GloVe (Global Vectors for Word Represen-tation) word embeddings [21], a pretrained model containing 20,000 words where each word is mapped to a 50-dimensional vector that captures semantic relationships. For phrases of up to three words, we concatenate the embeddings (with padding if needed) to form a 150-dimensional phrase vector. This continuous representation encodes rich semantic relationships and supports similarity-based operations within the embedding space."}, {"title": "D. Cross-Modality Embedding (Shared Latent Space)", "content": "We learn the cross-modality embedding as a shared latent space $Z \\subset \\mathbb{R}^{16}$ to align force profiles and phrases. Rather than treating phrases and force profiles as purely distinct modalities, we emphasize on their common representational ground. In this unified latent space, certain force profiles can be naturally described by words (e.g., \u201cgentle push\"), and conversely, phrases can be manifested as force trajectories. Specifically, we define encoders $E_{force}, E_{phrase}$ that map force profiles and phrases, respectively, to a shared embedding $z \\in Z$. We also define decoders $D_{force}, D_{phrase}$ that map shared embeddings back to forces and phrases, respectively. A contrastive learning objective [9] encourages embeddings of paired force-phrase data to lie close together in Z while pushing apart non-matching pairs. This alignment supports:\n\u2022 Force-to-Language Translation: Observed force profiles can be decoded into textual instructions.\n\u2022 Language-to-Force Translation: Written phrases can be transformed into corresponding force trajectories for robotic execution.\nTogether, these capabilities enable more natural interactions in human-robot collaboration by unifying physical force sig-nals and language instructions within a single latent represen-tation [23].\""}, {"title": "IV. SYSTEM OVERVIEW", "content": "In this work, we aim to develop a unified representation for physical forces and natural language phrases. Our primary goal is to learn a shared embedding space that allows robots to translate human-applied force profiles into linguistic descrip-tions and, conversely, generate appropriate force outputs from language instructions.\nWe use a UR [25] robot for the magnitude, direction, and duration of applied forces over time. To build a dataset that naturally pairs force signals with language, we designed two human-participant procedures. In the Phrase-to-Force proce-dure (Sec. V-C1), participants receive a brief textual phrase and physically move the robot arm. In the Force-to-Phrase procedure (Sec. V-C2), participants observe an externally applied force trajectory and then describe it in natural language from our minimal viable vocabulary (e.g., \u201cgradually left\"). By collecting these paired samples, we obtain a diverse dataset for learning force-language correspondences.\nOur proposed model is a dual autoencoder architecture (Fig. 5), which processes both time-series force profiles and textual phrases. We encode each force profile into a latent repre-sentation and similarly embed each phrase into a matching latent space. We train the model with three core objectives that facilitate robust multimodal alignment: (1) reconstruction, which ensures that both forces and phrases can be faithfully recovered from their respective embeddings, (2) contrastive learning, which encourages correct force-phrase pairs to be close in latent space while pushing apart mismatched pairs, and (3) translation, which enables the network to generate a force profile from a given phrase and to describe a given force profile with a textual output.\nBy optimizing these objectives, the system learns to em-bed semantically related force and language inputs in close\""}, {"title": "V. ARCHITECTURE", "content": "1) Force Profile Input: Each raw force profile consists of time-series measurements for the x, y, and z components of force, recorded at potentially irregular intervals and for varying durations. To create a uniform representation, we first resample each force profile to $N = 256$ evenly spaced time steps spanning a fixed duration $T = 4$. This yields a $3 \\times 256$ tensor\n$F = \\begin{bmatrix} F_{x,0} & F_{x,1} & \\dots & F_{x,255} \\\\ F_{y,0} & F_{y,1} & \\dots & F_{y,255} \\\\ F_{z,0} & F_{z,1} & \\dots & F_{z,255} \\end{bmatrix}$ \t where $F_{x,i}, F_{y,i}, F_{z,i}$ denote the resampled forces at the i-th time step in each axis. Next, we integrate each axis of F over time to obtain an impulse profile\n$J = \\begin{bmatrix} J_{x,0} & J_{x,1} & \\dots & J_{x,255} \\\\ J_{y,0} & J_{y,1} & \\dots & J_{y,255} \\\\ J_{z,0} & J_{z,1} & \\dots & J_{z,255} \\end{bmatrix}$ \t where $J_{a,i} = \\int_{0}^{t_i} F_a(t) dt$ and $t_i$ is the time associated with the i-th sampled resampled point. J is flattened to form a 768-dimensional vector $[J_x J_y J_z] \\in \\mathbb{R}^{768}$ which serves as input to the force encoder.\n2) Phrase Input: Since neural networks cannot directly pro-cess raw text, each phrase must be converted into a fixed-size vector representation that preserves its semantics. We consider one of the two embedding approaches: Binary Phrase Vectors from the Minimal Viable Vocabulary, or GloVe embeddings from Extended Glove Vocabulary (Section III-C).\n3) Dual Autoencoders: Our framework (see Fig. 5) em-ploys two autoencoders [15] \u2014one for force profiles and one for phrases to map inputs from distinct modalities into a shared latent space. Each autoencoder consists of an encoder network that compresses its input into a fixed-size latent vector (the \"bottleneck\" representation) and a decoder network that reconstructs the original input from this latent representation.\nDuring training, the encoder learns to capture the essential latent features of the input data, while the decoder learns to reconstruct the input from these latent variables. This unsupervised learning process enables the network to extract a compact representation that generalizes well to novel inputs sharing similar underlying structure. In our dual autoencoder architecture, both the force and phrase encoders are designed to output latent vectors of the same dimension, ensuring compatibility in the shared cross-modal latent space. This design choice allows us to perform bidirectional translation between force profiles and phrases by using the decoder of one modality on the latent representation produced by the encoder of the other."}, {"title": "B. Multitask Learning", "content": "To encourage the model to learn a robust cross-modality la-tent representation, we adopt a multitask learning strategy that jointly optimizes three related objectives: (1) reconstruction of the original inputs from their latent representations, (2) con-trastive alignment of corresponding force profile and phrase embeddings in the shared latent space, and (3) translation between modalities by decoding a latent embedding obtained from one modality into the other. Joint training with these tasks compels the model to extract meaningful latent features that generalize well across modalities while mitigating over-fitting. Typically, a model is trained by minimizing a single loss function; however, by incorporating multiple loss functions corresponding to related tasks [5], our network is guided to form a representation that simultaneously serves several objectives. The overall loss function is a weighted sum of the individual losses:\n$L = k_rL_r + k_zL_z + k_tL_t,$ \t where hyperparameters $k_r, k_z, k_t$ control the relative im-portance of the reconstruction loss $L_r$, contrastive loss $L_c$, and translation loss $L_t$, respectively. In our experiments, these constants are all set to 1, indicating equal weighting for each task. The loss functions are defined as:\n1) Reconstruction Loss ($L_r$): It measures how accurately each autoencoder (force and phrase) reproduces its own input from the latent vector. For force profiles, we use mean squared error; for phrases, the reconstruction metric depends on the chosen representation, with cross-entropy for binary phrase vectors and mean squared error for GloVe embeddings.\n2) Contrastive Loss ($L_c$): To align the force and phrase em-beddings, we employ a contrastive loss [9] that brings paired (corresponding) latent vectors closer together while pushing apart unpaired (non-corresponding) vectors. This ensures that shared features of matching force and language inputs are learned and represented similarly in the latent space.\n3) Translation Loss ($L_t$): Finally, we measure how well the model translates between modalities. Given a force profile and its paired phrase, we encode one and decode into the other, then compare the result to the corresponding ground truth. This cross-decoding step drives the model to capture modality-agnostic features in the shared embedding, facilitating natural force-to-language and language-to-force translation."}, {"title": "VI. EVALUATION", "content": "Our experiments aim to address three key research ques-tions:\n1) Force-Language Translation Performance. How ef-fectively does the dual autoencoder architecture translate force profiles into phrases and vice versa? This evalu-ates whether the shared latent space successfully aligns semantically similar force and language inputs.\n2) Generalization to Unseen Examples. Can the model generalize to force profiles and phrases outside its training distribution? This assesses whether the learned shared representation captures essential features that extend beyond the training data.\n3) Impact of Phrase Representation. How does the choice of phrase representation affect model performance? Specifically, do richer semantic embeddings (via pre-trained GloVe embeddings) enhance the model's ability to associate forces with language compared to binary phrase vectors from the Minimal Viable Vocabulary?\nTo investigate these questions, we evaluate two variants of our dual autoencoder (DAE) framework. The first variant, de-noted $DAE_B$, utilizes the binary phrase vector representation of phrases (described in Section III-C2). The second variant, $DAE_G$, employs the GloVe embedding representation (de-scribed in Section III-C2. By comparing results across these"}, {"title": "A. Baseline Models", "content": "1) SVM_KNN: As a baseline, we use Support Vector Machines (SVM) to map force signals to phrases and K-Nearest Neighbors (KNN) to map phrases back to force profiles. This approach is limited to our Minimal Viable Vocabulary since SVMs cannot generate continuous GloVe embeddings.\nEach force profile is reduced to a final impulse vector $I \\in \\mathbb{R}^3$, computed by integrating force over the fixed duration T. We train one SVM for each word slot (direction and modifier).\nGiven the impulse vector, these SVMs predict the most likely direction(s) and modifier.\nFor the inverse mapping, we treat each phrase as a com-bination of direction and modifier classes, then apply KNN in the same 3D impulse space to identify the closest training example. Since no full time series is predicted, we approximate the complete impulse profile by linear interpolation from zero impulse at $t = 0$ to the predicted final impulse at $t = T$. This simple interpolation scheme serves as a coarse placeholder for the temporal structure of the motion.\n2) DMLPB: In this baseline, we train two independent Multi Layer Perceptron (MLP) networks without any shared latent space. One MLP maps force profiles from flattened 768-dimensional impulse vectors to 62-dimensional binary phrase vector (Section V-A1), while the other maps phrases to force. We call this approach $DMLP_B$. Each MLP is trained via a reconstruction objective to directly map from one modality to the other. The training loss corresponds to the standard mean squared error for forces (Section V-B1) and cross-entropy for binary phrase embeddings. Since there is no shared latent space or cross-modal alignment, the model simply learns direct forward and backward transformations.\n3) DMLPG: $DMLP_G$ is analogous to $DMLP_B$ except it uses the GloVe embedding representation of phrases (Section III-C2). Instead of two binary phrase vector outputs, the first MLP maps the force profile to a 150-dimensional concate-nation of GloVe word embeddings, and the second MLP performs the inverse mapping. Each MLP is again optimized with a reconstruction-style loss suited to its respective output space. This direct mapping baseline allows us to isolate the effect of introducing richer semantic information via GloVe embeddings, independent of a shared latent representation."}, {"title": "B. Metrics", "content": "We define a set of performance metrics to evaluate the force-language translations. These metrics evaluate both the fidelity of generated force profiles relative to a ground-truth trajectory (force-to-phrase translation) and the accuracy of generated phrases relative to a reference text description (phrase-to-force translation).\n1) Force Profile Accuracy (FPAcc): For a predicted force profile $\\hat{x}_f$ and ground-truth $x_f$, we compute the mean squared error (MSE), averaged across both temporal and spatial axes:\n$FPAcc = MSE(x_f, \\hat{x}_f)$ \t A lower value indicates closer alignment with the refer-ence force profile.\n2) Force Direction Accuracy (FDAcc): For a predicted total impulse $\\hat{J}(T)$ and ground-truth $J(T)$ (see eq. 4) we compute the cosine similarity:\n$FDAcc = \\frac{\\hat{J}(T) \\cdot J(T)}{||\\hat{J}(T)|| \\cdot ||J(T)||}$ \t Value near +1 indicate strong directional alignment, whereas near -1 signify an opposite direction.\n3) Modifier Similarity (ModSim): We compare the pre-dicted modifier $\\hat{w}_m$ with the ground-truth modifier $w_m$"}, {"title": "C. Experiments", "content": "To evaluate both in-distribution and out-of-distribution per-formance, we conduct three main experiments:\n1) In-Distribution Evaluation. We randomly split the dataset into training (90%) and testing (10%) subsets. Each model is trained and tested on the same splits, and this process is repeated for 30 independent trials with different random seeds. We then average the per-formance metrics over these trials to reduce variance, yielding a robust estimate of in-distribution accuracy.\n2) Out-of-Distribution Modifiers. To assess the model's ability to generalize to unseen adverbial cues, we iso-late a single modifier (e.g., \"slowly\u201d). All data points containing this modifier are excluded from the training set but retained in the test set. After training, we measure how effectively each model handles data points that correspond to the held-out modifier. We repeat this pro-cedure for each modifier in the vocabulary and average the results to obtain a modifier-level generalization score.\n3) Out-of-Distribution Directions. Similarly, we evaluate direction-level generalization by holding out each di-rection (e.g., \"up\") from the training set. The model is then tested on data points where this direction appears. Repeating this protocol for all direction words and aver-aging the results provides a direction-level generalization score."}, {"title": "D. Results and Analysis", "content": "We present our experimental results addressing three re-search questions (see VI for complete descriptions):\n1) Does the dual autoencoder (DAE) model effectively translate force profiles to phrases and vice versa?\n2) Can the DAE model generalize to out-of-distribution (unseen) examples?\n3) What is the impact of the phrase representation (binary vs. GloVe) on performance?\n1) In-Distribution Results When trained and tested on the same distribution of force-phrase pairs, all four neural approaches (DMLPB, DMLPG, DAEB, DAEG) performed substantially better than SVM_KNN in terms of reproducing the force profile (FPAcc) and capturing the aggregate direction (FDAcc).\nAmong neural models, the DAE variants consis-tently outperformed their DMLP counterparts across all metrics, confirming that a shared latent-space ap-proach facilitates more robust force-language mapping. Moreover, DAEB achieved the highest ModSim and PhraseSim scores, indicating excellent preservation of adverb semantics and overall phrase structure.\nThe dual autoencoder (DAE) variants consistently achieve better performance than their DMLP counter-parts across all metrics, demonstrating the benefits of the shared latent space approach. DAEB achieved the highest ModSim and PhraseSim scores (0.58 and 0.78 respectively), indicating strong preservation of adverb semantics and overall phrase structure.\nAdditionally, DAEB and DMLPB both outperformed DAEG and DMLPG on in-distribution data. This sug-gests that the simpler binary phrase vectors may be sufficiently discriminative-and perhaps easier to learn-when the training distribution covers the same words. This observation partially answers our third ques-tion by illustrating that the binary representation can be advantageous for in-distribution translation.\n2) Out-of-Distribution Modifiers For unseen modifiers, DAEG achieves the best force profile reconstruction (FPAcc: 5.41) while DAEB maintains the highest phrase similarity scores (PhraseSim: 0.68). This indicates that the DAE models retain semantic representation of ad-verbs even if they never explicitly see them during training. We observe that the DAE models generalize more effectively to unseen modifiers than MLPs. GloVe embeddings helps in reconstructing the force trajectory (FPAcc), but the binary phrase vector representation still achieves strong direction and phrase-level fidelity.\nKey Findings These results demonstrate that:\n\u2022\t The dual autoencoder architecture effectively enables bidirectional translation, with DAE variants showing 20-30% improvement over baselines;"}, {"title": "VII. LIMITATIONS AND DISCUSSION", "content": "Multimodal Translationals Existing multimodal transla-tion frameworks, such as image-to-text or video-to-text sys-tems, can't be simply adapted to force-language mapping. Force signals have unique temporal characteristics and physics based constraints that is lacking in image data. Additionally, force profile data exhibits high user variability the same verbal description (e.g., \"gently left\") could correspond to remarkably different force profiles across users. While vision language models could handle spatial relationships, they do lack mechanisms for modeling the temporal dynamics and physical constraints inherent in force-based interactions sub-jective to the different users. This limitation motivated our development of a specialized framework for force-language mapping rather than adapting existing architectures.\nShared Representation Direct mappings between forces and language through classification based methods or rule-based engines fail to capture the rich force-linguistic relation-ships. The learned embedding space supports generalization to novel combinations through semantic patterns. For example, after training on \"gently left\" and \"quickly up\", the framework generates appropriate descriptions like \"slightly up\" for a previously unseen slow upward force by leveraging the learned semantic structure.\nArchitectural Simplicity vs Complexity While alternative architectures like transformers [29] and graph networks [14] exist, we chose a basic encoder-decoder design to establish clear baselines for the fundamental force-language mapping challenge. This architecture sufficiently demonstrates the core concept while prioritizing interpretability and data efficiency. Our framework is architecture agnostic. Our main motivation is to establish force-language mappings rather than propose a novel architecture thereby allowing future work to easily substitute more advanced architectures.\nForce Profile Data Scope We focus on simple hand motions for data collection across two user studies with 10 participants: one where participants demonstrated force profiles for given phrases, and another where they describe in simple phrases to the forces they observed. This choice of basic motions serves multiple purposes: a) they directly capture human intention while minimizing confounding variables and representing fun-damental primitives used in day to day tasks, and a) enable controlled data collection.\nLanguage Structure Scope We decomposed our language structure into <direction> and <modifier> elements (e.g., \u201cgently left\u201d), mapping naturally to the physical properties of force profiles - <magnitude>, <direction>, and <duration> in XYZ space. While this structured decomposition enabled a force-language associations, it does represent a simplified subset of possible force descriptions. It may not capture the full richness of natural language force descriptions. This could be improved to richer linguistic structures in future work."}, {"title": "VIII. CONCLUSION", "content": "We presented a framework that embeds physical force pro-files and verbal descriptions in a shared latent space, enabling natural bidirectional translation between how humans describe forces in language and how robots produces them. Our dual autoencoder architecture outperformed baseline approaches by 20-30% across key metrics through its unified representation of these distinct modalities. While GloVe word embeddings showed superior force reconstruction, binary embeded encod-ings achieved better linguistic precision, highlighting the in-herent trade-offs in representing force-language relationships. The framework demonstrated robust generalization to unseen force profiles and phrases, validating its potential for real-world human-robot interaction tasks. This work provides a foundation for more intuitive physical human-robot collabo-ration, particularly in applications like rehabilitation therapy where coordinated force control and verbal communication are essential."}]}