{"title": "Adaptive Discrete Disparity Volume for\nSelf-supervised Monocular Depth Estimation", "authors": ["Jianwei Ren"], "abstract": "In self-supervised monocular depth estimation tasks, discrete disparity prediction has been proven to attain higher quality depth maps than common continuous methods. However, current discretization strategies often divide depth ranges of scenes into bins in a handcrafted and rigid manner, limiting model performance. In this paper, we propose a learnable module, Adaptive Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth distributions in different RGB images and generating adaptive bins for them. Without any extra supervision, this module can be integrated into existing CNN architectures, allowing networks to produce representative values for bins and a probability volume over them. Furthermore, we introduce novel training strategies - uniformizing and sharpening through a loss term and temperature parameter, respectively, to provide regularizations under self-supervised conditions, preventing model degradation or collapse. Empirical results demonstrate that ADDV effectively processes global information, generating appropriate bins for various scenes and producing higher quality depth maps compared to handcrafted methods.", "sections": [{"title": "1 Introduction", "content": "Human beings are competent in estimating the relative depth of a scene even conditional on only a single image. This vision-based capacity is also of vital crucial for autonomous driving and robotic systems for its prediction of dense depth maps. Recent years, in order to prevent from using expensive LiDAR data or other complex sensors, learning-based self-supervised methods have been well developed and showing its prosperity.\nPredicting depth with a discrete method in monocular depth estimation (MDE), which recasts it as a classification problem, has been proven by numerous studies [1,3,8,16,21] to be superior to a common continuous one. Considering MDE as a classification task, rather than a regression one, helps generate sharp and high-quality depth maps [16]. It also facilitates measuring depth uncertainty in scenes [21], which is of great significance for system security. In this paper, we therefore adopt this notion that dividing the depth range into bins and predicting the probability distribution over them instead of directly regressing a single depth value for each individual pixel."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Self-supervised Monocular Depth Estimation", "content": "Recovering scene depth from a single image is a challenging problem in computer vision, particularly in self-supervised settings where depth information is lost during image capture, making it inherently ill-posed. In recent years, many learning-based methods develop their models via epipolar geometry constraints [11], reconstructing a target frame by warping a source frame inversely to guide training via reconstruction loss [9,29]. Zhou et al. [29] present a purely unsupervised framework trained on consecutive temporal frames, jointly estimating depth and 6-DoF pose from ego-motion. The notion of view synthesis is taken further by [12], which proposes an advanced multi-scale framework with a novel minimum reprojection loss and auto-masking. Both improvements assist in coping with the exceptional regions of images where the assumption of photometric consistency is violated, such as occlusions and non-Lambertian surfaces.\nSome works introduce extra clues to provide more priors. For example, [23,28,30] propose networks with joint learning of depth and optical flow, while [2,22,26,27] leverage geometry constraints. Semantic information could be a more popular prior, which has been utilized either to model motion [4,13,19] or for multi-task models [5,6,14,18]. However, these approaches also increase model complexity and hence cause higher computational costs. In contrast, our method requires no additional clues with lower expenses."}, {"title": "2.2 Depth discretization", "content": "During imaging, a camera's 3D view frustum can be hierarchically divided so that each point in the scene will fall in an interval or bin with a representative depth value. This discretization process has inspired numerous studies [1,3,8,16,21] to assign image pixels into bins with associated probabilities, instead of directly regressing specific depth values. These pixel probability distributions across bins form a volume, leading to the final depth map being computed as a linear combination [1,3,16,21] of representative values weighted by the probability volume.\nTwo prevalent discretization strategies have emerged: uniform discretization (UD) and spacing-increasing discretization (SID). SID was initially introduced in [3] and was found by [8] to outperform UD in accuracy. However, as noted by [1], the depth distribution varies significantly across different images, posing a challenge for both SID and UD to adapt simultaneously. To address this issue, [1] proposes adaptive bins, whose widths can change according to inputs, albeit in a supervised manner. In the context of self-supervised learning, fewer studies exist, but [16] introduces discrete disparity volume using conventional discretization strategies.\nIn this paper, we propose a purely self-supervised adaptive strategy. We argue that both UD and SID represent specific instances of adaptive strategies."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Formulation", "content": "To obtain supervision signals from temporally continuous view transformations, our framework jointly optimizes both a depth prediction network and a pose one. The depth net takes an RGB frame $I_t \\in \\mathbb{R}^{3\\times H\\times W}$ as input and produces its corresponding depth map $d_t \\in \\mathbb{R}^{H\\times W}$, denoted as $D(I_t) \\rightarrow d_t$. At the same time, the pose network receives a concatenation of two adjacent frames $I_t$ and $I_{t'} \\in \\{I_{t-1}, I_{t+1}\\}$ and outputs 6-DoF relative pose $T_{t\\rightarrow t'} \\in SE(3)$, denoted as $P(I_t, I_{t'}) \\rightarrow T_{t\\rightarrow t'}$. Through the warping operation, expressed as $\\wp (I_{t'}, d_t, T_{tt'}, K) \\rightarrow I_{t'\\rightarrow t}$, where $K$ represents the camera intrinsics, a reconstructed frame $I_{t\\rightarrow t'}$ is obtained. Similar to [12], the primary objective is to optimize a photometric error $p_e$ of reconstruction:\n$p_e(I_1, I_2) = \\frac{\\alpha_1}{2}(1 - SSIM(I_1, I_2)) + (1 - \\alpha_1)||I_1 \u2013 I_2||$ (1)\nwhere $\\alpha_1$ is a scale factor. With assistance of auto-masking $M(\\cdot)$, a pixel-wise minimum reprojection loss is employed to lessen the effects of occlusions, static pixels and low-texture regions:\n$\\mathcal{L}_p = M(\\text{min } p_e(I_t, I_{t'\\rightarrow t}))$ (2)\nAdditionally, an edge-aware smoothness term $\\mathcal{L}_s$ is used to reduce artifacts:\n$\\mathcal{L}_{smooth} = |\\partial_x d| e^{-\\partial_x I_t} + |\\partial_y d| e^{-\\partial_y I_t}$ (3)\nwhere $d_+ = d_t / \\overline{d_t}$ is the mean-normalized disparity to improve numerical stability [25].\nFigure 1 illustrates the framework and a detailed decoder of the depth network. The decoder involves five blocks, with the ADDV incorporated into four of them to generate depth maps for each scale, excluding the lowest-resolution block."}, {"title": "3.2 Adaptive Discrete Disparity Volume", "content": "Adaptive discrete disparity volume (ADDV) is a differentiable module that is included in each output block of the depth decoder, applicable at any scale due to its resolution insensitivity. This module predicts a probability volume and generates adaptive bins conditional on the high-dimension features from the last layer. Subsequently, the bin values, representing relative depth, are weighted by probabilities and aggregated to produce the final depth map.\nADDV consists of two components, as depicted in Figure 2. A probability estimator predicts and aggregates per-pixel probability distributions, forming a volume from an input feature map $X \\in \\mathbb{R}^{C\\times H\\times W}$ with $C$ channels and resolution $H \\times W$. To meet the quantization requirement of discrete methods, the input $X$ is processed with convolutional kernels, resulting in a logit feature map $Y\\in \\mathbb{R}^{N\\times H\\times W}$.The softmax is applied to yield well-defined per-pixel probabilities:\n$P_{n_{(u,v)}} = softmax(Y) = \\frac{exp(y_{(u,v)})}{\\Sigma_{i=1}^N exp(y_{(u,v)})}$ (4)\nwhere $y_{(u,v)}$ is the value of pixel $(u, v)$ in the $n$-th channel of $Y$.\nAs the other component of ADDV, the bins generator captures depth clues and produce adaptive bins accordingly. It initially convolves the input feature map to align with the probability volume across channels. To incorporate global context, global average pooling integrates spatial information, resulting in an $N\\times 1 \\times 1$ tensor that encapsulates implicit information about bin widths. The last convolutional layer is activated by sigmoid, ensuring that relative widths remain positive within a reasonable range. Finally, a cumulative sum operator and a max normalization are imposed on the tensor to obtain the representative values of adaptive bins, denoted as $b = [b_1, b_2, ..., b_N]$, where $b_n$ refers to the $n$-th element.\nA common approach is to compute the final depth map $d \\in \\mathbb{R}^{H\\times W}$ from probability volume using Maximum Likelihood Estimates (MLE), formally as:\n$d_{(u,v)} = b_{n^*} ; n^* = \\text{argmax } P_{n_{(\u03ba,\u03c5)}}$ (5)\nwhere $d_{(u,v)}$ stands for the depth value of pixel $(u, v)$. However, it fails to converge due to argmax's non-differentiability. A modified MLE, adopted by [8], overcomes this limitation but still introduces artifacts in depth maps [1]. Consequently, most discrete methods [1,16,21] resort to soft-argmax, in a form of linear combinations of probability volume and representative values:\n$d_{(u,v)} = \\sum_{n=1}^N  b_n \\cdot P_{n_{(u,v)}}$ (6)"}, {"title": "3.3 Uniformizing and Sharpening", "content": "Models employing adaptive strategies excel at global information processing. However, generating adaptive bins poses an extra task and burden during training. Previous works [1,20] rely on supervision from ground truth to prevent performance degradation. In this paper, to achieve this in a self-supervised environment, we propose leveraging the principle of sample balancing as a prior for adaptive bin adjustment. Moreover, to mitigate the estimation bias caused by the soft-argmax operation, we employ sharpening to stimulate peaks in the probability distributions.\nUniformizing loss term promotes adjustments to bin widths to ensure a balanced distribution of samples across bins. We provide two designs to implement this term. An intuitive but naive one $\\mathcal{L}_u^i$ is defined as:\n$c_n = \\sum_{\u03ba,\u03c5} \u03b7 (\\text{argmax } P_{u,v} = n)$\n$\\mathcal{L}_u^i = \\sum_{n=1}^N  \\frac{c_n}{C_{valid}} - \\frac{1}{N} $ (7)\nwhere $|| ||$ denotes the L2 norm and $C_{valid}$ is the number of valid samples. The indicator function $\u03b7(\\cdot)$ equals 1 if the inner condition is true, and 0 otherwise. In this formulation, $c_n$ represents the number of samples determined by MLE in the $n$-th bin. However, this term potentially hinders training as it only considers the indices of max value in each probability distribution and discards much of the available information.\nAn improved alternative $\\mathcal{L}_u^{avg}$ is proposed to take into account the entire probability distributions of all samples. It can be written as:\n$P_{n_{avg}} = \\frac{1}{C_{valid}} \\sum_{\u03c5,\u03c5} P_{(\u03ba,\u03c5)}$\n$\\mathcal{L}_u^{avg} = \\sum_{n=1}^N P_{n_{avg}} - \\frac{1}{N}$ (8)\nwhere $P_{n_{avg}}$ is the averaged probability of all valid samples within the n-th bin. The uniformizing loss is optimized based only on valid samples, which are not filtered out by auto-masking $M(\\cdot)$. We adopt the uniformizing loss in the following form:\n$\\mathcal{L}_u = M(\\mathcal{L}_u))$ (9)\nThe final loss $\\mathcal{L}_{final}$ is computed as:\n$\\mathcal{L}_{final} = \\mathcal{L}_p + a_2\\mathcal{L}_{smooth} + X_3 \\mathcal{L}_u$ (10)\nwhere $a_2$ and $a_3$ are weighted factors."}, {"title": "Sharpening", "content": "Sharpening reduces the sensitivity of soft-argmax to the shape of the distribution by encouraging the presence of a dominant peak in pixel-wise probability distributions. This is achieved by introducing a temperature parameter $\u03c4 \u2208 (0,1]$ into softmax, modifying the probability volume as follow:\n$P_{(u,v)} = \\frac{exp(y_{(u,v)}/T)}{\\Sigma_{i=1}^N exp(y_{(u,v)}/T)}$ (11)"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Details", "content": "We train the model for 20 epochs using the Adam optimizer [17] with an initial learning rate of 1e-4, which decays to 1e-5 after 15 epochs. ResNet-18 [15] is employed as the backbone with weights pretrained on ImageNet [24]. Experiments are conducted with an image resolution of 416 \u00d7 128 and jpeg format to save memory and computational resources. Data augmentation are applied, including horizontal flipping and color jittering in brightness (\u00b10.2), contrast (\u00b10.2), saturation (\u00b10.2) and hue (\u00b10.1). We adopt the hyper-parameters \u03b11 = 0.85 and \u03b12 = 10-3, following Monodepth2 [12]. The uniformizing term is weighted by \u03b13 = 1, and the temperature parameter for sharpening is set to \u03c4 = 0.5."}, {"title": "4.2 KITTI Results", "content": "We employ the KITTI dataset [10], following the data split of Eigen et al. [7], which is commonly used in depth estimation and other computer vision tasks. After pre-processing by [29] to remove static frames, we obtain 39, 810 monocular training triplets, with 4,424 reserved for validation. During inference, depth estimation is limited to a range of 80m [7,11], and we address scale ambiguity using pre-image median ground truth scaling [29]. Evaluation of depth estimation is conducted using standard metrics outlined in [7].\nQuantitative results are recorded in Table 1, with Monodepth2 serving as the baseline. Comparing ADDV to other discretization strategies on KITTI dataset, we observe that our adaptive method is superior to UD or SID across most metrics for the same number of bins. As reported in [8], a higher number of"}, {"title": "4.3 Ablation Study", "content": "We explore the influences of uniformizing, sharpening and bins number in this ablation study.\nUniformizing and sharpening are regularizations employed during network training under self-supervised conditions, thereby preventing performance degradation."}, {"title": "5 Conclusion", "content": "We have presented a novel learnable module, Adaptive Discrete Disparity Volume (ADDV), designed for self-supervised monocular depth estimation within a CNN architecture. This module addresses the limitations of conventional discretization strategies by actively generating customized depth bins tailored to different input scenes, thus eliminating the need for handcrafted design or additional supervision. To cope with the issue of lacking fine-grained constraints during training, we have introduced uniformizing and sharpening. Empirical results demonstrate that even in the absence of explicit supervision, adaptive methods are capable of scene perception and effectively processing global information. It consequently outperforms other strategies, yielding higher quality depth maps. Furthermore, our ablation study confirms the efficacy of both training strategies in improving performance.\nOur future work will explore removing the limitation on the number of bins, thereby enhancing the flexibility of the network and enabling it to handle a wider range of scenes. We also plan to leverage uncertainty maps generated by discretization methods to refine depth maps."}]}