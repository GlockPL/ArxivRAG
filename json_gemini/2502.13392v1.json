{"title": "Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation", "authors": ["Jim Dai", "Manxi Wu", "Zhanhao Zhang"], "abstract": "Pioneering companies such as Waymo have deployed robo-taxi services in several U.S. cities. These robo-taxis are electric vehicles, and their operations require the joint optimization of ride matching, vehicle repositioning, and charging scheduling in a stochastic environment. We model the operations of the ride-hailing system with robo-taxis as a discrete-time, average reward Markov Decision Process with infinite horizon. As the fleet size grows, the dispatching is challenging as the set of system state and the fleet dispatching action set grow exponentially with the number of vehicles. To address this, we introduce a scalable deep reinforcement learning algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the action space using atomic action decomposition. We evaluate our algorithm using real-world NYC for-hire vehicle data and we measure the performance using the long-run average reward achieved by the dispatching policy relative to a fluid-based reward upper bound. Our experiments demonstrate the superior performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct extensive numerical experiments to analyze the efficient allocation of charging facilities and assess the impact of vehicle range and charger speed on fleet performance.", "sections": [{"title": "Introduction", "content": "Robo-taxi services have been deployed in several U.S. cities, including San Francisco, Phoenix, and Los Angeles [1]. The efficient operations of robo-taxi fleets, comprised of electric vehicles, is challenged by the stochasticity and the spatiotemporal distribution of trip demand, as well as the scheduling of battery charging with limited charging infrastructure. Inefficient operations could render vehicles unavailable during high-demand periods, leading to decreased service quality, reliability issues, and revenue loss.\nIn this work, we model the robo-taxi fleet dispatch problem as a Markov decision process with discrete state and action space. The state of system records the spatial distribution of vehicles, their current tasks, active trip requests, and the availability of chargers. Based on the state, the system dispatches the entire fleet to manage trip fulfillment, repositioning, charging, and passing (i.e. continuing with their current tasks), resulting in associated rewards or costs. Our goal is to find a fleet dispatching policy that maximizes the long-run average reward over an infinite time horizon.\nThe challenge of computing the fleet dispatching policy arises from the high-dimensionality of the state and action space. Since the state space contains all possible distributions of the fleet and the action space contains all feasible fleet assignments, both the state space and the action space scale exponentially with the fleet size. We develop a scalable deep reinforcement learning (RL) algorithm, which we refer as atomic proximal policy optimization (Atomic-PPO). The efficiency of Atomic-PPO is accomplished by decomposing the dispatching of the entire fleet into the sequential assignment of atomic action (tasks such as trip fullfillment, reposition, charge or pass (no new assignment)) to individual vehicles, which we refers to as \u201catomic action decomposition\". The dimension of the atomic action space equals to the number of feasible tasks that can be assigned to an individual vehicle. Hence, the atomic action decomposition reduces the action space from being exponential in fleet size to being a constant, significantly reducing the complexity of policy training. We integrate our atomic action decomposition into a state-of-art RL algorithm, PPO [2], which possesses the monotonic policy improvement guarantee and has proven to be effective in control optimization across various applications [3, 4, 5, 6, 7]. We approximate both the value function and the atomic action function in the PPO training using neural networks, and further reduces the dimension of state representation by clustering the vehicle battery levels and destination information of trip requests.\nTo obtain an upper bound of the optimality gap of our fleet dispatching policy, we derive an upper bound on the optimal long-run average reward using a fluid-based linear program (LP) (see Theorem 1). The fluid limit is attained as the fleet size approaches infinity, with\n1In the load-balancing context, the power-of-k uniformly samples k number of queues at random and then routes the incoming arrival to the shortest queue among them."}, {"title": "Model", "content": "We consider a transportation network with V service regions. In this network, a fleet of electric vehicles with size N are operated by a central planner to serve passengers, who make trip requests from their origins $u \\in V$ to their destinations $v \\in V$. For each pair of (u, v) \u2208 V \u00d7 V, we assume that the battery consumption for traveling from u to v is a constant $b_{uv} \\in \\mathbb{R}_{\\geq 0}$. The electric vehicles are each equipped with a battery of size B, i.e. a fully charged vehicle can take trips with total battery consumption up to B. A set of chargers with different charging rates $\u03b4 \\in \u0394$ are installed in the network, where each rate $\u03b4$ charger can replenish d amount of battery to the connected vehicle in one unit of time. We denote the number of rate $\u03b4\u2208 \u2206$ chargers in each region $v \\in V$ as $n_v^\u03b4 \\in \\mathbb{N}_{\\geq 0}$.\nWe model the operations of a ride-hailing system as a discrete-time, average reward Markov Decision Process (MDP) with infinite time horizon. In particular, we model the system as an infinite repetition of a single-day operations, where each day d = 1,2,... is evenly partitioned into T number of time steps t = 1, . . ., T. The system makes a dispatching decision for the entire fleet at every time steps of a day, which we refer to as a decision epochs. In each decision epoch (t, d), the number of trip requests between each u-v pair, denoted as $X_{uv}^{td}$, follows a Poisson distribution with mean $\u03bb_{uv}^t$. The duration of trips from region u to region v at time t is a constant $\u03c4_{uv}^t$ that is a multiple of the length of a time step. Both $\u03bb_{uv}^t$ and $\u03c4_{uv}^t$ can vary across time steps in a single day, but remain unchanged across days.\n2We can easily extend our model to account for non-linear charging rates by introducing \u03b4 a function of current battery level.\n3This assumption can be removed if we add actions that combine charging and trip fulfillment within a single decision epoch. We omit this for the sake of simplicity."}, {"title": "The ride-hailing system", "content": "When the central planner receives the trip requests, they assign vehicles to serve (all or a part of) the requests. In particular, a vehicle must be assigned to the passenger within the connection patience time $L_c \u2265 0$, and a passenger will wait at most time $L_p \u2265 0$ (defined as the pickup-patience time) for an assigned vehicle to arrive at their origin. Otherwise, the passenger will leave the system. Both $L_p$ and $L_c$ are multiples of the discrete time steps.\nThe central planner keeps track of the status of each vehicle by the region $v \u2208 V$ it is currently located or heading to, remaining time to reach $\u03b7 = 0,...,\\hat{\u00ce}$, and remaining battery level $b = 0, ..., B$ when reaching v. Here, $\\hat{\u03c4} := max_{u\u2208V,t\u2208[T]} \u03c4_{uv}$ is the maximum duration of any trip with destination v. We assume that the minimum time cost of all trips $min_{u,v\u2208V,t\u2208[T]} \u03c4_{uv}$ is larger than $L_p$ so that no vehicle can be assigned to serve more than one trip request in a single decision epoch.\nA vehicle is associated with status c = (v, \u03b7, b) if (1) it is routed to destination v, with remaining time n, and battery level b at the arrival; or (2) it is being charged at region \u03c5, with remaining charging time y, and battery level b after the charging period completes. Here, if n = 0, then the vehicle is parked at region v. Additionally, if \u03b7 > 0, then the vehicle could either be taking a trip with a passenger whose destination is v, the vehicle is being repositioned to v, or the vehicle is being charged in v. Vehicle repositioning may serve two purposes: (1) to pick up the next passenger whose origin is v; (2) to be charged or idle at region v. We note that the maximum remaining time of a trip cannot exceed $\u03c4_u + L_p$ since a vehicle is eligible to serve a passenger with destination v if it can arrive at the origin of that trip within $L_p$ time and the maximum trip duration is $\u03c4_{uv}^t$. Moreover, a vehicle with status (v, 0, b) can be charged at region v with rate $\u03b4$ if such a charger is available in v. After the vehicle is assigned to charge, it will remain charged for J time steps. If the vehicle's battery level is full or close to full, then it will charge to full and then idle for the remaining time of the charging period. We assume that $J > L_p$ so that vehicles assigned to charge will not be assigned to serve trip requests in the same decision epoch 3. Let $C := {(\u03c5, \u03b7, b)}_{v\u2208V,n=0,...,\\hat{f},b=0,...,B}$ denote the set of all vehicle status.\nA trip order is associated with status o = (u, v, \u03be) if it originates from u, heads to v, and has been waiting for vehicle assignment in the system for \u00a7 time steps. A trip with origin u and destination v can be served by a vehicle of status (u, \u03b7, b) if (1) the vehicle can reach u within the passenger's pickup-patience time $L_p$ (i.e. \u03b7 \u2264 $L_p$), and (2) the remaining battery of the vehicle when reaching u is sufficient to complete the trip to v (i.e. b \u2265 $b_{uv}$). We note"}, {"title": "Markov decision process", "content": "that a vehicle may be assigned to pick up a new passenger before completing its current trip towards u as long as it can arrive within $L_p$ time steps. We use $O := {(u, v, \u03be)}_{u,v\u2208V, \u03be=0,...,L_c}$ to denote the set of all trip status.\nA charger is associated with st w = (v, \u03b4, j) if it is located at region v with a rate of d and is j time steps away from being available. Specifically, if j = 0, the charger is available immediately. If j > 0, the charger is currently in use and will take an additional j time steps to complete the charging period. We use $W := {(\u03c5, \u03b4, j)}_{vev,s\u2208\u2206,j=0,...,J\u22121}$ to denote the set of all charger status. All notations introduced in this section are summarized in Table 1.\nWe next describe the state, action, policy and reward of the Markov decision process (MDP).\nState. We denote the state space of the MDP as S with generic element s. The state vector $s^{t,d}$ records the time t of day d, the trip order state $(s_{o}^{t,d})_{o\\in O}$, the fleet state $(s_{c}^{t,d})_{c\\in C}$, and the charger state $(s_{w}^{t,d})_{w\\in W}$ where $s_{o}^{t,d}$ is the number of trip orders of status o, $s_{c}^{t,d}$ is the number of vehicles of status c, and $s_{w}^{t,d}$ is the number of chargers of status w at (t,d). For all (t, d), the sum of vehicles of all status equals to the fleet size N. That is,\n$\\sum_{c \\in C} s_c^{t,d} = N, \\forall t \\in [T], d = 1, 2, . . . .$\nAdditionally, the sum of chargers of all remaining charging times for each region v and rate d equals to the quantity of the corresponding charging facility $n_v^\u03b4$. That is, \u2200v \u2208 V and \u03bd\u03b4 \u0395 \u0394,\n$\\sum_{\\omega:=(\u03c5,\u03b4,j)} s_w^{t,d} = n_v^\u03b4, \\forall t \\in [T], d = 1, 2, . . ..$\nThus, the state vector is $s^{t,d} = (t, (s_c^{t,d})_{c \\in C}, (s_o^{t,d})_{o \\in O}, (s_w^{t,d})_{w \\in W}) \u2208 S$. Here, we note that the number of new trip arrivals $X_{uv}^{td}$ can be unbounded. However, since the fleet size is N and a trip order can be kept in the system for up to $L_c$ steps, the maximum number of trip orders arrived at one time step that can be served before being abandoned is $N(L_c+1)$. As a result, without loss of generality, we truncate the number of trip requests for each status entering the system during each decision epoch to $N(L_c + 1)$, with any additional trip requests being rejected by the system. Hence, our state space S is finite.\nAction. We denote the action space of the MDP as A with generic element a. An action is a flow vector of the fleet that consists of the number of vehicles of each status assigned to"}, {"title": "State Transitions", "content": "take trips, reposition, charge, idle, and pass. We denote an action vector at time t of day d as $a^{t,d} := (f^{t,d}, e^{t,d}, q^{t,d}, i^{t,d}, i^{t,d}, p^{t,d})_{c\\in C}$ where:\n*   $f^{t,d} := (f_{c,o}^{t,d} \\in \\mathbb{N})_{c\\in C, o \\in O}$ determines the number of vehicles of each status c assigned to fulfill each trip order status o \u2208 O at time t of day d. In particular, vehicles are eligible to take trip requests if their current location or destination matches the trip's origin region, they are within $L_p$ time steps from completing the current task, and they have sufficient battery to complete the trip, i.e.\n$\\begin{aligned}f_{c,o}^{t,d} \\begin{cases} \\geq 0, & \\forall c = (u, \\eta, b) \\text{ and } o = (u, v, \\xi)\\\\ &\\text{ such that } \\eta \\leq L_p \\text{ and } b \\geq b_{uv},\\\\ = 0, & \\text{otherwise.}\\end{cases}\\end{aligned}$\nAdditionally, we require that the total number of vehicles that fulfill the trip orders of status o cannot exceed $s_o^{t,d}$, i.e.\n$\\sum_{c=(u,\\eta,b) \\in C} f_{c,o}^{t,d} \\leq s_o^{t,d}, \\forall o = (u, v, \\xi) \u2208 O$.\n*   $e^{t,d} := (e_{c,v}^{t,d} \\in \\mathbb{N})_{c\\in C, v \\in V}$ represents the number of vehicles of each status c assigned to reposition to v at time t of day d. In particular, vehicles are eligible to reposition to a different region if they have already completed their current tasks and they have sufficient battery to complete the trip.\n$\\begin{aligned}e_{c.v}^{t,d} \\begin{cases} \\geq 0, & \\forall c = (u, \\eta, b) \\text{ and } v \\neq u\\\\ &\\text{ such that } \\eta = 0 \\text{ and } b > b_{uv},\\\\ = 0, & \\text{otherwise.}\\end{cases}\\end{aligned}$\n*   $q^{t,d} := (q_{c,\u03b4}^{t,d} \\in \\mathbb{N})_{c \\in C, \u03b4\\in \u0394}$ represents the number of vehicles of status c assigned to charge with rate d at time t of day d. In particular, vehicles are eligible to charge if they have already completed their current tasks.\n$\\begin{aligned}q_{c,\u03b4}^{t,d} \\begin{cases} \\geq 0, & \\forall c = (u, \\eta, b) \\text{ and } \u03b4\\in \u0394\\\\ &\\text{ such that } \\eta = 0, \\\\ = 0, & \\text{otherwise.}\\end{cases}\\end{aligned}$\nAdditionally, for each region v and charging rate d, we require that the total number of vehicles of all status assigned to charge at region v with rate d cannot exceed the total number of available chargers:\n$\\sum_{c=(v,\u03b7,b) \\in C} q_{c,\u03b4}^{t,d} \\leq n_v^\u03b4 , \\forall v \\in V, \u03b4\\in \u0394$.\n*   $it,d \u2208 N$ represents the number vehicles of status c assigned to idle at time t of day d. In particular, vehicles are eligible to idle if they have already completed their current tasks.\n$\\begin{aligned}i_{c}^{t,d} \\begin{cases} \\geq 0, & \\forall c = (u, \\eta, b) \\text{ such that } \\eta = 0, \\\\ = 0, & \\text{otherwise.}\\end{cases}\\end{aligned}$\n*   $ptd \u2208 N$ represents the number of vehicles of status c not assigned with any new action at time t of day d. In particular, vehicles are eligible for the pass action if they have not completed their tasks yet.\n$\\begin{aligned}p_{c}^{t,d} \\begin{cases} \\geq 0, & \\forall c = (u, \\eta, b) \\text{ such that } \\eta > 0, \\\\ = 0, & \\text{otherwise.}\\end{cases}\\end{aligned}$\nFor any t and d, the vector at,d needs to satisfy the following flow conservation constraint: All vehicles of each status should be assigned to trip-fulfilling, repositioning, charging, idling, or passing actions. That is,\n$\\sum_{o \\in O}f_{c,o}^{t,d} + \\sum_{v \\in V}e_{c,v}^{t,d} + \\sum_{\u03b4 \\in \u0394}q_{c,\u03b4}^{t,d} + i_c^{t,d} + p_c^{t,d} = s_c^{t,d}$\n$\\forall c\\in C, t \u2208 [T], d = 1, 2, . . . .$\nFrom the above description, we note that the feasibility of an action depends on the state. We denote the set of actions that are feasible for state s as As.\nPolicy. The policy \u03c0 : S \u2192 \u2206(As) is a randomized policy that maps the state vector to an action, where \u03c0(\u03b1|s) is the probability of choosing action a given state s under policy \u03c0. We note that the notation \u03c0(\u00b7) does not explicitly depend on t since t is already a part of the state vector s."}, {"title": "State Transitions.", "content": "At any time t of day d, given any state s and the action a \u2208 As, we compute the vehicle state vector at time t + 1. For each vehicle status c := (v, \u03b7, b) \u2208 C,\n$\\begin{aligned}&s_{(\u03c5,\u03b7,6)}^{t+1,d} = \\\\\n&\\left(\\sum_{u\u2208V} \\sum_{c'=(u,n',b')\u2208C} \\sum_{o=(u,v,\u03be')\u2208O}f_{c',o}^{t,d} 1(\u03b7' + \u03c4_{uv}^t \u2212 1 = \u03b7, b' \u2013 b_{uv} = b)\\right)\n\\\\&+\\left(\\sum_{u\u2208V} \\sum_{c'=(u,0,b')\u2208C}e_{c',\u03c5}^{t,d}  1(\u03c4_{uv}^t \u2212 1 = \u03b7, b' \u2013 b_{uv} = b)\\right)\n\\\\&+\\left[\\left(\\sum_{b'>b-\u03b4J}  \\sum_{\u03b4\u2208\u0394}q_{(0,0,0),\u03b4}^{t,d} 1(\u03b7 = J \u2212 1, b = B)\\right) +\\left(\\sum_{b'>b-\u03b4J}  \\sum_{\u03b4\u2208\u0394}q_{(0,0,0),\u03b4}^{t,d}\\right) 1(\u03b7 = 0)\\right] +P_{(\u03c5,\u03b7,6)}^{t+1,d}11 (\u03b7 < \u2191),&\\end{aligned}$\nwhere (i) and (ii) correspond to vehicles assigned to new trip-fulfilling or repositioning actions with destination v, respectively. The destination, time to arrival, and battery level of these vehicles are updated based on the newly assigned trips. Term (iii) corresponds to the vehicles assigned to charge at time t. The battery level of these vehicles will be increased by J at the end of the charging period. If the vehicle is charged to full, then it will remain idle for the rest of the charging period. Term (iv) corresponds to the idling vehicles, and (v) corresponds to the vehicles taking the passing action whose remaining time of completing the assigned action decreases by 1 in the next time step.\nMoreover, the trip state at time t + 1 of day d is given by (12). For each trip status, we subtract the number of trip orders that have been fulfilled at time t, and we increment the active time by 1 for trip orders that are still in the system. We abandon the trip orders that have been active for more than $L_c$ time steps. Additionally, new trip orders arrive in the system and we set their active time in the system to be 0. That is, for all (u, v) \u2208 V \u00d7 V,\n$s_{(\u03ba,\u03c5,\u03be)}^{t+1,d}=\\begin{cases}min \\{X_{uv}^{td}, N(L_c + 1)\\},& if \u03be = 0,\\\\s_{(\u03ba,\u03c5,\u03be-1)}^{t,d}- \\sum_{c \\in C} f_{c, (\u03ba,\u03c5,\u03be-1)}^{t,d},& if 1 < \u00a7 < L_c.\\end{cases}$\nAs mentioned earlier in this section, we cap the number of new trip arrivals by N($L_c$ + 1) so that the state space is finite. Then, trips that are not fulfilled at time t are queued to t + 1."}, {"title": "Markov decision process", "content": "Thus, the number of trips that have been in the system for \u00a7 \u2265 1 at time t + 1 equals to that with \u00a7 - 1 from time t minus the ones that are assigned to a vehicle at time t.\nLastly, the charger state at time t + 1 of day d is given by (13). For each charger status w := (v, \u03b4, j) \u2208 W: For all region v \u2208 V and for all charger outlet rates \u03b4\u0395\u0394,\n$\\begin{aligned}&s_{(\u03c5,\u03b4,+1)}^{t+1,d} = \\\\&\\begin{cases}\\sum_{c=(0,0,0) \\in C}q_{c,\u03b4}^{t,d},& if j = J \u2212 1,\\\\s_{(\u03c5,\u03b4,j)}^{t,d},& if 0 < j < J \u2013 1,\\\\s_{(\u03c5,\u03b4,1)}^{t,d} + s_{(0,0,0)}^{t,d} - \\sum_{c=(v,,b)\u2208C}q_{c,8}^{t,d},& if j = 0.\\end{cases}&\\end{aligned}$\nAt time t + 1, the number of chargers with status (v, \u03b4, j) (i.e. occupied and remaining time is j = J \u2212 1) equals the total number of vehicles just assigned to charge at time t. Chargers already in use at time t will have their remaining charging time j decrease by 1 when the system transitions to time t + 1. The number of chargers available (i.e. j = 0) at time t +1 consists of (i) the chargers that have just completed their charging periods (i.e. $s_{(\u03c5,\u03b4,1)}^{t,d}$), and (ii) the chargers available at time t minus the ones that are assigned to charge vehicles (i.e. $s_{(0,0,0)}^{t,d} - \\sum_{c=(v,,b)\u2208C} q_{c,8}^{t,d}$).\nReward. The reward of fulfilling a trip request from u to v at time t is $r_{uv}^t \\in \\mathbb{R}_{\u22650}$. We remark that if a trip request has been active in the system for some time, then the reward is determined by the time at which the trip request is picked up. The reward (cost) of re-routing between u, v is $re_{uv}^t \u2208 \\mathbb{R}_{\u22640}$. The reward (cost) for a vehicle to charge at time t is $ro_{8}^{t} \u2208 \\mathbb{R}_{\u22640}$. Idling and passing actions incur no reward or cost. As a result, given the action at,d, the total reward at time t of day d is given by\n$r^{t}(a^{d}) = \\sum_{c \\in C} \\sum_{(u,v)\u2208V\u00d7V} \\sum_{\u03be=0}^{Lc} f_{c, (u,v,\u03be)}^{t,d} ruv  +  \\sum_{c \\in C} \\sum_{(u,v)\u2208V\u00d7V}e_{c,v}^{t,d} re_{uv} \\\\+\\sum_{c \\in C} \\sum_{\u03b4\u2208\u0394}q_{c,\u03b4}^{t,d} ro_{\u03b4}^{t},$\nThe long-run average daily reward of a policy \u03c0 given some initial state s is as follows:\n$R(\\pi|s) := \\lim_{D\u2192\u221e} \\frac{1}{D} E_{\\pi}  \\sum_{d=1}^{D} \\sum_{t=1}^{T} r^{t}(a^{d})  |s$.$\nSince the state is finite and the state transition and policy are homogeneous across days, the limit defined above exists (see page 337 of [91]). Our goal is to find the optimal fleet control policy \u03c0* that maximizes the long-run average daily rewards given any initial state s:\n$R^*(s) = R(\\pi^*|s) = \\max_{\\pi} R(\\pi|\u03c2), \u2200s \u2208 S.$"}, {"title": "Fleet Control Policy Reduction With Atomic Actions", "content": "One challenge of computing the optimal control policy lies in the size of the action space |A|, which grows exponentially with the number of vehicles N and vehicle statuses |C|. As a result, the dimension of policy \u03c0 also grows exponentially with N and |C|. The focus of this section is to address this challenge by introducing a policy reduction scheme, which decomposes the dispatching of a fleet to sequential assignment of tasks to individual vehicles, where the task for each individual vehicle is referred as an \u201catomic action\". We use the name \"atomic action policy\" because each atomic action only changes the status of a single vehicle. In particular, for any vehicle of a status c\u2208 C, an atomic action can be any one of the followings:\n*   f, represents fulfilling a trip of status o \u2208 O.\n*   \u00ea represents repositioning to destination v \u2208 V.\n*   qs represents charging with rate \u03b4\u2208 \u2206 at its current region.\n*   p represents idling or continuing with its previously assigned actions.\nWe use A to denote the atomic action space that includes all of the above atomic actions, i.e. $a \\in \\hat{A} := \\{ f_o \\}_{o \\in O}, (\u03b2_v)_{v \\in V}, (q_\u03b4)_{\u03b4 \\in \\Delta}, P$. The atomic action significantly reduces the dimension of the action function since \u00c2 does not scale with the fleet size or the number of vehicle statuses.\nWe now present the procedure of atomic action assignment. In each decision epoch (t, d), vehicles are arbitrarily indexed from 1 to N, and are sequentially selected. For a selected vehicle n, the atomic policy \u4ecb:S\u00d7C \u2192 A(A) maps from the tuple of system state stad before n-th assignment and the selected vehicle's status cn to a distribution of atomic actions. The system state stod transitions after every single vehicle assignment with $s_{N}^{t,d} = s^{t,d}$, and $s_{N}^{t,d}$ transitions to $s^{t+1,d}$ after assigning the last vehicle and trip arrival at time t + 1 is realized.\nThe total reward for each decision epoch (t, d) is the sum of all rewards generated from each atomic action assignment in (t, d), where the reward generated by the atomic action \u00e2t,d \u2208 \u00c2 is given by\n$\\begin{aligned}&r^{t}(\\hat{a}^{d}) =\\sum_{o\u2208O} r_{f,o} 1{\\hat{a}_n^{t,d} = f_o} + r_{(u,v)\u2208VxV} re_{uv} 1 {\\hat{a}_n^{t,d} = \\hat{e}_v}\\\\ &+\\sum_{\u03b4\u2208\u0394} r_{0, \u03b4} 1{\\hat{a}_n^{t,d} = q_\u03b4}.\n&\\end{aligned}$"}, {"title": "Reward Upper Bound Provided By Fluid Approximation- Model", "content": "The long-run average reward given the atomic action policy and the initial state s \u2208 S is as follows:\n$R(\\overline{\\pi}|s) = \\lim_{D \\rightarrow \\infty} \\frac{1}{D} E  \\sum_{d=1}^{D} \\sum_{t=1}^{T} \\sum_{n=1}^{N} r^{t}(a_{n}^{d})  |s, \\forall s \u2208 S,$\nwhere $a_{n}^{t,d}$ is the atomic actions generated by the atomic action policy in the n-th atomic step in decision epoch (t, d). Given any initial state s \u2208 S, our goal is to find the optimal atomic action policy such that \u0175* = argmaxR(\u4e93|s).\nOur atomic action policy can be viewed as a reduction of the original fleet dispatching policy in that any realized sequence of atomic actions corresponds to a feasible fleet dispatch-ing action with the same reward of the decision epoch. This reduction makes the training of atomic action policy scalable because the output dimension of atomic action policy \u00b9 equals to |\u00c2, which is a constant regardless of the fleet size."}, {"title": "Deep Reinforcement Learning With Aggregated States", "content": "The adoption of atomic actions has significantly reduced the action dimension. However, the implementation of the MDP is still challenging due to the large state space, which scales significantly with the fleet size, number of regions, and battery discretization. In this section, we provide an efficient algorithm to train the fleet dispatching policy by incorporating our atomic action decomposition into PPO [2]. To tackle with the large state size, we use neural networks to approximate both the value function and the policy function, to be specified later. We also further reduce the state size in terms of battery discretization and the number of regions by using the following state reduction scheme:\nBattery Level Clustering. We map the state representation of all vehicle statuses into vehicle statuses with aggregated battery levels. We cluster the battery levels into 3 intervals, each of which denotes low battery level $b_l$, medium battery level $b_m$, and high battery level $b_h$, respectively. The cutoff points can be set based on charging rates and criticality of battery levels. It is also possible to cluster the battery levels differently. If computing resources allow, we can cluster the battery levels with finer granularity, e.g. into 10 levels instead of 3.\nTrip Order Status Clustering. In the state reduction scheme, trip orders are aggregated by recording only the number of requests originating from or arriving at each region, instead"}, {"title": "Fleet Control Policy Reduction With Atomic Actions", "content": "One challenge of computing the optimal control policy lies in the size of the action space |A|, which grows exponentially with the number of vehicles N and vehicle statuses |C|. As a result, the dimension of policy \u03c0 also grows exponentially with N and |C|. The focus of this section is to address this challenge by introducing a policy reduction scheme, which decomposes the dispatching of a fleet to sequential assignment of tasks to individual vehicles, where the task for each individual vehicle is referred as an \u201catomic action\". We use the name \"atomic action policy\" because each atomic action only changes the status of a single vehicle. In particular, for any vehicle of a status c\u2208 C, an atomic action can be any one of the followings:\n*   f, represents fulfilling a trip of status o \u2208 O.\n*   \u00ea represents repositioning to destination v \u2208 V.\n*   qs represents charging with rate \u03b4\u2208 \u2206 at its current region.\n*   p represents idling or continuing with its previously assigned actions.\nWe use A to denote the atomic action space that includes all of the above atomic actions, i.e. $a \\in \\hat{A} := \\{ f_o \\}_{o \\in O}, (\u03b2_v)_{v \\in V}, (q_\u03b4)_{\u03b4 \\in \\Delta}, P$. The atomic action significantly reduces the dimension of the action function since \u00c2 does not scale with the fleet size or the number of vehicle statuses.\nWe now present the procedure of atomic action assignment. In each decision epoch (t, d), vehicles are arbitrarily indexed from 1 to N, and are sequentially selected. For a selected vehicle n, the atomic policy \u4ecb:S\u00d7C \u2192 A(A) maps from the tuple of system state stad before n-th assignment and the selected vehicle's status cn to a distribution of atomic actions. The system state stod transitions after every single vehicle assignment with $s_{N}^{t,d} = s^{t,d}$, and $s_{N}^{t,d}$ transitions to $s^{t+1,d}$ after assigning the last vehicle and trip arrival at time t + 1 is realized.\nThe total reward for each decision epoch (t, d) is the sum of all rewards generated from each atomic action assignment in (t, d), where the reward generated by the atomic action \u00e2t,d \u2208 \u00c2 is given by\n$\\begin{aligned}&r^{t}(\\hat{a}^{d}) =\\sum_{o\u2208O} r_{f,o} 1{\\hat{a}_n^{t,d} = f_o} + r_{(u,v)\u2208VxV} re_{uv} 1 {\\hat{a}_n^{t,d} = \\hat{e}_v}\\\\ &+\\sum_{\u03b4\u2208\u0394} r_{0, \u03b4} 1{\\hat{a}_n^{t,d} = q_\u03b4}.\n&\\end{aligned}$"}, {"title": "Reward Upper Bound Provided By Fluid Approximation- Model", "content": "Before presenting the performance of our atomic PPO algorithm", "follows": "n*   Fraction of fleet for trip fulfilling $f := (f_{c,o}^t)_{c\u2208C,o\u2208"}]}