{"title": "EMBEDDING SELF-CORRECTION AS AN INHERENT ABILITY IN LARGE LANGUAGE MODELS FOR ENHANCED MATHEMATICAL REASONING", "authors": ["Kuofeng Gao", "Huanqia Cai", "Qingyao Shuai", "Dihong Gong", "Zhifeng Li"], "abstract": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial in revolutionizing domains that heavily rely on such reasoning. However, LLMs often encounter difficulties in certain aspects of mathematical reasoning, leading to flawed reasoning and erroneous results. To mitigate these issues, we introduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically designed to embed self-correction as an inherent ability in LLMs, enabling them to validate and rectify their own results. The CoSC mechanism operates through a sequence of self-correction stages. In each stage, the LLMs generate a program to address a given problem, execute this program using program-based tools to obtain an output, subsequently verify this output. Based on the verification, the LLMs either proceed to the next correction stage or finalize the answer. This iterative self-correction process allows the LLMs to refine their reasoning steps and improve the accuracy of their mathematical reasoning. To enable the CoSC mechanism at a low cost, we employ a two-phase finetuning approach. In the first phase, the LLMs are trained with a relatively small volume of seeding data generated from GPT-4, establishing an initial CoSC capability. In the second phase, the CoSC capability is further enhanced by training with a larger volume of self-generated data using the trained model in the first phase, without relying on the paid GPT-4. Our comprehensive experiments demonstrate that CoSC significantly improves performance on traditional mathematical datasets among existing open-source LLMs. Notably, our CoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging mathematical reasoning dataset in the public domain, surpassing the performance of well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs like GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra. It's important to note that, unlike these proprietary models, our CoSC performs inference in a zero-shot manner, without the need for demonstrations. To the best of our knowledge, this is the first work in the literature to embed self-correction as an inherent capability in large language models and achieve significant performance improvements. To facilitate open-source research on improving the reasoning capabilities of LLMs, the code and data for this work will be released upon the acceptance of this paper.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023), have recently demonstrated state- of-the-art performance across a variety of natural language processing (NLP) tasks, including natural language generation and understanding (Chowdhery et al., 2023; Team et al., 2023; Anil et al., 2023; Penedo et al., 2023). Despite their success, LLMs often struggle with mathematical reasoning tasks due to their lack of explicit logical reasoning and judgment, which are crucial for solving such problems. Moreover, there is a fundamental gap between natural language and the language of"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 LLMS FOR MATHEMATICAL REASONING", "content": "Mathematical reasoning (Liu et al., 2023a; Wang et al., 2023a; Huang et al., 2024b; Toshniwal et al., 2024; Chen et al., 2024a; Zhang et al., 2024) is a challenging reasoning task for LLMs, which requires"}, {"title": "2.2 EXISTING METHODS RELATED TO SELF-CORRECTION IN LLMS", "content": "Mathematical reasoning poses a significant challenge due to its demand for precise multi-round logical reasoning to solve problems. The potential for errors increases with each reasoning step, making it crucial for LLMs to have the ability to self-correct in order to produce accurate results. There are some recent studies (Chen et al., 2023b; Gou et al., 2023a; Lightman et al., 2023; Huang et al., 2024a; Chen et al., 2024b) attempt to enable large language models to perform self-correction by either prompting methods or or fine-tuning methods. For prompting methods, they can correct their responses when interacting with external tools (Gou et al., 2023a; Chen et al., 2023b), such as search engines and calculators, or designing complex prompts (Chen et al., 2024b). Notably, prompting methods can be orthogonally combined with finetuning methods. For finetuning methods, previous works (Yu et al., 2024; An et al., 2023) only model single-round correction during the training stage, and perform verification in a straightforward manner during the inference stage."}, {"title": "2.3 DIFFERENCE BETWEEN THE PROPOSED METHOD AND EXISTING SELF-CORRECTION TECHNIQUES", "content": "The existing self-correction research can be broadly classified into two categories: prompt-based methods (Gou et al., 2023a; Chen et al., 2023b; 2024b) and SFT-based methods (Yu et al., 2024; An et al., 2023). Our approach falls under the SFT-based category and fundamentally differs from prompt-based methods. Specifically, prompt-based methods leverage the intrinsic capabilities of large language models (LLMs) for mathematical reasoning. In contrast, our method embeds the chain of self-correction, a strong reasoning mechanism, as an inherent capability in the LLMs through parameter fine-tuning. Furthermore, compared to existing SFT-based methods, our approach models mathematical reasoning as a multi-round procedure during the training stage. In addition, these SFT- based works (Yu et al., 2024; An et al., 2023) perform the verification process in a straightforward manner, whereas our work conducts verification in a step-by-step manner by generating intermediate verification steps. Similar to the essence of the CoT approach, our model first verifies whether the generated code aligns with the problem description and then verifies whether the obtained result is consistent with the problem description. This step-by-step verification strategy significantly improves the verification process in self-correction. The excellent performance of our method on the challenging mathematical problem tasks, such as MATH and GSM8k, clearly demonstrates its effectiveness compared to the existing SFT-based methods (Yu et al., 2024; An et al., 2023)."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 OVERVIEW", "content": "We propose a Chain of Self-Correction (COSC) to address mathematical questions in a self-correction manner. Given a mathematical question denoted as $q$, the CoSC mechanism initiates the process by generating a program $p$ that comprises a Python function. The program $p$ is then executed using a Python interpreter to produce an output $o$. However, the program $p$ may contain bugs or incorrect intermediate code due to potential misinterpretations of the question $q$, leading to erroneous outputs."}, {"title": "3.2 TRAINING", "content": "To enable the CoSC mechanism, we propose a two-phase finetuning method. (1) CoSC foundational learning with seeding data generated from proprietary models. (2) CoSC self enhancement with self-generated data using the seed model obtained in the first training phase. A summary for training our CoSC is shown in Fig. 2."}, {"title": "3.2.1 COSC FOUNDATIONAL LEARNING", "content": "Existing mathematical reasoning datasets, such as chain-of-thoughts (CoT) (Wei et al., 2022) and program-of-thoughts (PoT) (Chen et al., 2023a), primarily contain single-round annotations without multi-round, self-correction solutions for the reasoning process. This makes it challenging to fine-tune models to inherently possess self-correction capabilities. To address this issue, we utilize GPT-4 (G) and a few-shot approach to annotate questions from publicly available mathematical datasets, including MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), to generate seeding data with our Chain of Self-Correction (COSC) mechanism. Table 1 compares our seeding data of COSC with recently proposed mathematical reasoning datasets. Detailed instructions and example cases of our CoSC can be found in Appendix A and Appendix B."}, {"title": null, "content": "Given a mathematical question $q$, the few-shot prompt used to call GPT-4 is defined as $p$. The corresponding trajectory, denoted as $\\tau$, is generated through the following process. Firstly, we feed both the few-shot prompt and the question $q$ into GPT-4, which then generates a Python function code $p$ to solve the question $q$. It can be represented as:\n\n$q \\xrightarrow{\\tau_{i-1}} p_i,$\n\nwhere the symbol $\\bigoplus$ represents concatenation and $\\tau_i$ indicates the trajectory in the $i$ interaction round. After obtaining the code $p_i$, we execute it using a Python interpreter to acquire the runtime result $o_i$. Then, we enclose the result $o_i$ within `` 'output\" and feed it, along with the previous few-shot prompt, question $q$, the previous trajectory $\\tau_i$, and generated code $p_i$ to the GPT-4. This produces a verification $v_i$ that analyzes the consistency between the question $q$ and the program code $p_i$, as well as between the question $q$ and the program output $o_i$. Additionally, it produces a conclusion $c_i$ on whether to proceed with the next round of reasoning to perform adjustment, denoted as:\n\n$p \\bigoplus q \\bigoplus \\tau_{i-1} \\bigoplus p_i \\bigoplus o_i \\rightarrow v_i \\bigoplus c_i.$\n\nDepending on the conclusion $c_i$, it decides whether to terminate the reasoning process or proceed to the next round. If there is no inconsistency found during the verification step, it generates a natural language conclusion $c_i$ and encloses the answer to the problem with the special symbol \"boxed{}\". Otherwise, it should further revise the generated code $p_i$ in a new reasoning round by updating the trajectory $\\tau_i$, denoted as:\n\n$\\tau_i = \\tau_{i-1} \\bigoplus p_i \\bigoplus o_i \\bigoplus v_i \\bigoplus c_i$.\n\nBased on the above steps, CoSC iteratively generates PoT code, program output, verification, and conclusion. At final, the reasoning process is stopped by checking whether the generated result contains an answer enclosed in \u201cboxed{}\".\n\nTo ensure that the self-correction can concisely and clearly analyze the code and its runtime results, we design the self-correction in a step-by-step format as follows:\n\n*   The first step involves verifying whether the generated program code $p$ is consistent with the question $q$, such as checking the variables and their relationships.\n*   The second step involves verifying whether the code runtime results $o$ meet the requirements of the question $q$, such as checking the reasonableness of numerical values.\n\nFinally, for summary, CoSC will generate a conclusion $c$ to determine whether we should start the next round of reasoning or provide the final answer.\n\nWith CoSC, we use GPT-4 to annotate the MATH and GSM8K training datasets. To explore diverse data, we apply nucleus sampling (Holtzman et al., 2019) during GPT-4 annotation. Each question is sampled in 3 times. However, for some complex questions, we can only get less than 1 correct solutions. For these problems, we apply 10 more samplings and retained up to 4 correct data. Finally, we filtered out incorrect answers and constructed 37k pieces of data using GPT-4. The algorithm of the generation with our CoSC for each question from the training set is shown in Algorithm 1.\""}, {"title": "3.2.2 COSC SELF ENHANCEMENT", "content": "After the completion of the CoSC foundational learning, the seed model gains the ability to self- correct during inference and perform multi-round reasoning. Subsequently, we employ the seed model, after CoSC foundational learning, to apply dense solution sampling and dense question sampling, which enables the generation of more self-generated data with self-correction mechanisms.\n\nDense solution sampling. For mathematical questions, there are usually multiple solutions, but use GPT-4 for annotating them is expensive. Therefore, we use the seed model after CoSC foundational learning to resample the questions in the datasets multiple times further improve the model general- ization ability in answering questions. Specifically, we use the CodeLLaMA-34B model which has the best performance after CoSC foundational learning to perform dense nucleus-sampling on 16k training data questions. Each question in the MATH and GSM8K training datasets is sampled 64 times. We filter out the correct answers based on whether they match the ground-truth.\n\nDense question sampling. Similarly, there are multiple ways to ask a mathematical question. To improve the model generalization ability of question understanding, we use the data from Meta- Math (Yu et al., 2023) to generalize the questions. Each question in MetaMATH is rewrited by simply rephrasing it, self-verifying the conditions in the question, adding if-then questions to reverse the question conditions, etc. Specifically, we use the CodeLLaMA-34B model with CoSC foundational learning to perform nucleus-sampling on the questions, sampling each question once, and filtering out data points based on whether they match the ground-truth.\n\nIn the end, we obtain a total of 339k data points, including 37k seeding data generated from GPT-4 and 302k generated from the CodeLLaMA-34B model with CoSC foundational learning. Then, we adopt them to train models from scratch to obtain the final our CoSC model."}, {"title": "3.3 IMPLEMENTATION DETAILS", "content": "By using 339k data points, including 37k seeding data and 302k self-generated data, we fine-tune the base models of LLaMA-2 (Touvron et al., 2023) and CodeLLaMA (Roziere et al., 2023) to obtain our CoSC and CoSC-Code, respectively. They are with different parameter sizes, such as 7B, 13B, and 34B. All models use full-scale fine-tuning. We use the AdamW optimizer with a learning rate of 2e-5 for all models, with a batch size set to 128, training for 1 epoch. To enable training, we use DeepSpeed ZeRO stage 3 (Rajbhandari et al., 2021) and Flash-Attention 2 (Dao, 2023) to optimize the model's memory usage. During inference, we set a maximum of 3 calls to the Python interpreter"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EVALUATION SETUP", "content": "Datasets. We evaluated models on the most widely used mathematical problem tasks, MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). The MATH dataset encompasses a total of 12,500 problems, partitioned into 7,500 for training and 5,000 for testing. The GSM8K dataset contains 8,500 problems, with 7,500 for training and 1,000 for testing. These datasets collectively encompass a broad range of mathematical questions, from basic arithmetic to competition level. More details of datasets are provided in Appendix C.\n\nBaselines. For proprietary models, we present results from an array of SoTA LLMs, such as OpenAI's ChatGPT (gpt-3.5-turbo), OpenAI's GPT-4, OpenAI's GPT-4V, OpenAI's GPT-4o, Google's PaLM- 2, Google's Gemini-1.0 Pro, Google's Gemini-1.0 Ultra, Google's Gemini-1.5 Flash, Google's Gemini-1.5 Pro, Anthropic's Claude-2, Anthropic's Claude-3 Opus, and Anthropic's Claude-3.5 Sonnet. By default, we report CoT prompting results, and include PAL (Program-Aided Language model) prompting (Gao et al., 2023) results for selected models. For open-source models, base models comprise LLaMA-2 (Touvron et al., 2023) and CodeLLaMA (Roziere et al., 2023) with CoT and PAL prompting (Gao et al., 2023). Supervised Fine-Tuning (SFT) employs CoT rationales from the original MATH and GSM8K dataset (15k samples) for fine-tuning. Rejection sampling Fine-Tuning (RFT) (Yuan et al., 2023) leverages multiple models to generate diverse reasoning paths for fine-tuning. WizardMath (Luo et al., 2023) augments data using ChatGPT, and conducts SFT and RLHF. Platypus-2 (Lee et al., 2023) is fine-tuned with Open-Platypus reasoning datasets. ToRA (Gou et al., 2023b) uses GPT-4 to generate tool-used trajectories and finetunes on it.\n\nMetric. We report accuracies of predicted answers. For numerical values, we perform rounding, while for expressions, we employ the Python library sympy for parsing."}, {"title": "4.2 MAIN RESULTS", "content": "In our experiments, we observe several findings that demonstrate the effectiveness of our proposed COSC model on mathematical datasets. We show the experimental results in Table 2. First and foremost, our proposed CoSC consistently outperforms previous state-of-the-art open-source LLMs across all scales. Specifically, our CoSC-Code can achieve an average improvement of 2.6%, 1.8%, and 2.1%, on 7B, 13B, and 34B size, respectively.\n\nMoreover, to further emphasize the superiority of our CoSC, we conduct a comprehensive comparison against multiple proprietary models. The results reveal that our CoSC-Code-34B can outperform all the advanced proprietary LLMs, such as ChatGPT and GPT-4, as well as most advanced proprietary multi-modal LLMs, such as GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra on MATH dataset. It is important to note that, unlike these proprietary models, our CoSC performs the inference in a zero-shot manner without demonstrations."}, {"title": "4.3 ABLATION STUDY", "content": "In our ablation study, we conduct experiments on MATH dataset with the largest number of 5,000 test samples with broad spectrum of subjects and difficulty levels. For the consideration of computational efficiency, we choose the smaller models of CodeLLaMA with size 7B and 13B as the base models."}, {"title": "4.3.1 EFFECT OF EACH TRAINING PHASE", "content": "Our CoSC mechanism comprises two training phases: (1) CoSC foundational learning and (2) COSC self-enhancement. In order to assess the individual contributions of each training phase, we evaluate multiple combinations of the proposed training phases, the results of which are shown in Table 3.\n\nThe experiments conducted on CodeLLaMA serve as the baseline for our study. Initially, we employ GPT-4 to generate 37k seeding data for CoSC foundational learning, which equips LLMs with initial"}, {"title": "4.3.2 EFFECT OF MULTI-ROUND REASONING IN THE PROPOSED COSC MECHANISM", "content": "Our CoSC mechanism integrates a series of self-correction stages to progressively verify and refine output of LLMs. In particular, the conclusion step in our CoSC relies on clues from the verification step to determine whether to proceed to the next round of reasoning or directly provide the final answer. Such iterative multi-round mechanism enables LLMs to self-correct their outputs and improve accuracy. This experiment quantitatively investigates effect of this multi-round mechanism. As shown in Table 4, our CoSC can effectively generate more rounds of mathematical reasoning during inference, confirming efficacy of our CoSC mechanism in enhancing the reasoning process.\n\nIn addition, we conduct a comparison between single-round reasoning and multi-round reasoning using our CoSC mechanism on the test samples from the MATH dataset. The single-round results are obtained by extracting the answer solely from the output of the first round, without any self-correction enabled. To illustrate the impact of multi-round reasoning, we present the comparative results in Table 5. The results clearly demonstrate the effectiveness of multi-round reasoning in rectifying errors and improving overall accuracy. With the 7B model, we observe an improvement in accuracy from 40.2% to 47.6%, while with the 13B model, accuracy increases from 42.4% to 50.3%. These findings highlight the significant benefits of employing multi-round reasoning within our CoSC mechanism. More ablation studies are shown in Appendix D."}, {"title": "5 CONCLUSION", "content": "In conclusion, our Chain of Self-Correction (COSC) mechanism equips Large Language Models (LLMs) with the ability to autonomously validate and refine their outputs. This mechanism facilitates a sequence of self-correction stages that progressively refine the reasoning process, leading to enhanced accuracy in mathematical reasoning. Our two-phase finetuning approach, involving initial training with proprietary model-generated seeding data and subsequent training with self-generated data, effectively enables the implementation of CoSC at a low cost. Through extensive experiments,"}, {"title": "A PROMPT FOR OUR COSC", "content": "We present specific instructions and example few-shot prompts of our CoSC for querying GPT-4 to generate the seeding data."}, {"title": "A.1 PROMPT FOR OUR COSC ON MATH", "content": "The prompt for CoSC on MATH is as follows:\n\nIntegrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\n\n*   Analyze the question and write functions to solve the problem; the function should not take any arguments.\n*   Present the final results in LaTeX using a \"\\boxed{}\" without any units.\n*   Utilize the \"pi\" symbol and \"Rational\" from Sympy for $\\pi$ and fractions, and simplify all fractions and square roots without converting them to decimal values.\n*   In \"verification\", verify the consistency between the \"Question\" and the \"python\" code, as well as between the \"Question\" and \"output\" in \"Solution\" step-by-step.\n*   According to the \"verification\", rewrite the code if neccessary.\n\nHere are some examples you may refer to:\n\nQuestion: Find the matrix that corresponds to projecting onto the vector $\\begin{pmatrix} 2 \\\\ -3 \\end{pmatrix}.$\n\nSolution:\n\n```python\nfrom sympy import Matrix, simplify\ndef projection_matrix():\n v = Matrix ([2, 3])\n # Calculate the projection matrix\n P = (v * v.T) / (v.T * v) [0]\n # Simplify the elements\n P = P.applyfunc(simplify)\n return P\n\nanswer = projection_matrix()\nprint (answer)\n```output\nMatrix ([[4/13, -6/13], [-6/13, 9/13]])\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code defines a function binary_sum_diff() that calculates the projection matrix for the given vector. The vector is defined as v = Matrix([2, -3]), and then use standard formula (v * v.T) / (v.T * v)[0] for"}, {"title": null, "content": "calculating the projection matrix onto a vector, finally use `applyfunc(simplify) to simplify the elements of the matrix.\nStep 2: Verify the consistency between the question and the output. The answer matrix of 2 dimension vector should be a 2x2 matrix, the \"output\" of \"python\" code is `[[4/13, -6/13], [-6/13, 9/13]]`, which is reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are consistent with \"Question\". The matrix is $\\boxed{\\begin{pmatrix} \\frac{4}{13} & -\\frac{6}{13} \\\\\\\\ -\\frac{6}{13} & \\frac{9}{13} \\end{pmatrix}}.$\n\nQuestion: Compute $1011_2 + 101_2 - 1100_2 + 1101_2$. Express your answer in base 2.\n\nSolution:\n\n```python\ndef binary_sum_diff():\n # Convert binary to decimal\n num1 = int(\"1011\", 2)\n num2 = int(\"101\", 2)\n num3 = int(\"1100\", 2)\n num4 = int(\"1101\", 2)\n # Perform the operations\n result = num1 + num2 - num3 + num4\n # Convert the result back to binary\n result_binary = format (result, \"b\")\n return result_binary\n\nanswer = binary_sum_diff()\nprint (answer)\n```output\n10001\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code defines a function binary_sum_diff() that convert the binary numbers to decimal, performing the operations, and then converting the result back to binary. It assigns the decimal form of values $1011, 101, 1100, 1101$ to $num1, num2, num3, num4$ respectively, and then perform the operation $num1 + num2 - num3 + num4, finally convert the result back to binary.\nStep 2: Verify the consistency between the question and the output. The \"output\" of \"python\" code is $10001$, which is reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are consistent with \"Question\". The answer expressed in base 2 is $\\boxed{10001_2}.$\n\nQuestion: Convert the point $ (0, -3 \\sqrt{3}, 3)$ in rectangular coordinates to spherical coordinates. Enter your answer in the form $(\\rho,\\theta, \\phi),$ where $\\rho > 0,$ $0 \\le \\theta < 2 \\pi, $ and $0 \\le \\phi \\le \\pi.$\n\nSolution:\n\n```python\nfrom sympy import sqrt, atan2, acos, pi\ndef rectangular_to_spherical():\n x, y, z = 0, -3*sqrt(3), 3\n rho = sqrt(x**2 + y2 + z**2)\n theta = atan2 (y, x)\n phi = acos (z/rho)\n return rho, theta, phi\n\nanswer = rectangular_to_spherical()\nprint (answer)\n```output\n(6, -pi/2, pi/3)\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code defines a function rectangular_to_spherical() that convert rectangular coordinates to spherical coordinates. It assigns the values $0, -3 \\sqrt{3}, 3$ to $x, y, z$ respectively, and then use the following formulas $\\rho = \\sqrt{x^2 + y^2 + z^2}$, $\\theta = \\arctan{\\frac{y}{x}}$ 3, $\\phi = \\arccos{\\frac{z}{\\rho}}$ to calculates $rho, theta, phi$.\nStep 2: Verify the consistency between the question and the output. The required form of the \"output\" is $(\\rho, \\theta, \\phi)$, where $\\rho > 0$, $0 \\le \\theta < 2\\pi$, and $\\le \\phi \\le \\pi$. The \"output\" of \"python\" code is $(6, -\\pi/2, \\pi/3)$, which is not reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are not consistent with \"Question\". Let's rewrite the \"python\" code based on the \"verification\":\n\n```python\nfrom sympy import sqrt, atan2, acos, pi\ndef rectangular_to_spherical():\n x, y, z = 0, -3*sqrt(3), 3\n rho = sqrt(x**2 + y2 + z**2)\n theta = atan2 (y, x)\n phi = acos (z/rho)\n theta = (theta + 2 * pi) % (2 * pi)\n phi = (phi + pi) % pi\n return rho, theta, phi\n\nanswer = rectangular_to_spherical()\nprint (answer)\n```output\n(6, 3*pi/2, pi/3)\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code was rewrited, 'theta = (theta + 2 * pi) % (2 * pi) and 'phi = (phi + pi) % pi' were added to meet requirement that $0 \\le \\theta < 2\\pi$, and $0 \\le \\phi < \\pi$.\nStep 2: Verify the consistency between the question and the output. The required form of the \"output\" is $(\\rho, \\theta, \\phi)$, where $\\rho > 0$, $0 \\le \\theta < 2\\pi$, and $\\le \\phi \\le \\pi$. The \"output\" of \"python\" code is $(6, 3*\\pi/2, \\pi/3)$, which is reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are consistent with \"Question\". Hence, the spherical coordinates for the point $ (0, -3\\sqrt{3}, 3)$ are $\\boxed{(6, \\frac{3\\pi}{2}, \\frac{\\pi}{3})}.$\n\nQuestion: Solve the inequality\n\\[\\frac{x^2 - 25}{x + 5} < 0.\\]\nSolution:\n\n```python\nfrom sympy import symbols, simplify\nfrom sympy.solvers.inequalities import solve_univariate_inequality\nfrom sympy.core.relational import LessThan\ndef solve_inequality():\n x = symbols ('x')\n expression = (x**2 - 25) / (x + 5)\n inequality = LessThan (expression, 0)\n solution = solve_univariate_inequality(inequality, x, relational=False)\n simplified_solution = simplify(solution)\n return simplified_solution\n\nanswer = solve_inequality()\nprint (answer)\n```output\nUnion (Interval.open(-00, -5), Interval. Lopen(-5, 5))\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code defines a function solve_inequality() that that solves the inequality \\[\\frac{x^2 - 25}{x + 5} < 0.\\]. It defines a symbolic variable 'x' and the expression is defined as `\\[\\frac{x^2 - 25}{x + 5}\\]`, then use function \u2018LessThan' to define the inequality and function 'solve_univariate_inequality to solve the inequality for 'x`, finally use 'simplify to simplify the result.\nStep 2: Verify the consistency between the question and the output. The required answer 'x' should meet the condition: $x + $5 != $0, the \"output\" of \"python\" code is `(-00, -5)` and `(-5, 5), which is reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are consistent with \"Question\". The answer is $\\boxed{(-\\infty,-5)\\cup(-5,5)}$\n\nQuestion: In the figure, triangles $ABC$ and $BCD$ are equilateral triangles. What is the value of $AD \\div BC$ when expressed in simplest radical form?\n\n\\begin{asy}\ndraw((0,0)--(5,8.7)--(10,0)--cycle);\ndraw((10,0)--(15,8.7)--(5,8.7));\nlabel(\"$A$\", (0,0),SW);\nlabel(\"$B$\", (5,8.7),N);\nlabel(\"$C$\", (10,0),SE);\nlabel(\"$D$\", (15,8.7),NE);\n\\end{asy}\nSolution:\n\n```python\nfrom sympy import Rational, sqrt, simplify\ndef ad_divided_by_bc():\n x = 1 # Side length of equilateral triangles\n ad_squared = 2 * x**2 - 2 * x**2 * cos (2 pi / 3) # Using the law of cosines\n ad = sqrt(ad_squared)\n bc = x # BC is the side length of the equilateral triangles\n simplified_ratio = simplify(ad / bc)\n return simplified_ratio\n\nanswer = ad_divided_by_bc()\nprint (answer)\n```output\nsqrt (3)\n```verification\nStep 1. Verify the consistency between the question and the code. The \"python\" code defines a function ad_divided_by_bc() that calculates the ratio of the lengths of $AD$ and $BC$ in the given figure. The side length of the equilateral triangles 'x' is set to 1, because $ ABC$ and $BCD$ are equilateral triangles, the angle between $AB$ and $AD$ should be 2 * \\pi / 3, then we use the law of cosines, the square length of $AD$ is thus 2 * x**2 - 2 * x**2 * cos (2 pi / 3)', and the length of $AD$ is the square root of that, finally use ' simplify to simplify the 'ad / bc'.\nStep 2: Verify the consistency between the question and the output. The required answer $AD \\div BC$ should be positive, the \"output\" of \" python\" code is $\\sqrt{3}$, which is reasonable.\n\n```conclusion\nTherefore, the \"python\" code and \"output\" are consistent with \"Question\". The value of $AD \\div BC$ is $\\boxed{\\sqrt{3}}.$"}, {"title": "A.2 PROMPT FOR OUR COSC ON GSM8K", "content": "The prompt for CoSC on GSM8K is as follows:\n\nIntegrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\n\n*   Analyze"}]}