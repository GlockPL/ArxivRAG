{"title": "DECRYPTING THE TEMPERATURE FIELD IN FLOW BOILING WITH LATENT DIFFUSION MODELS", "authors": ["UngJin Na", "Jun Young Seo", "Taeil Kim", "ByongGuk Jeon", "HangJin Jo"], "abstract": "This paper presents an innovative method using Latent Diffusion Models (LDMs) to generate\ntemperature fields from phase indicator maps. By leveraging the BubbleML dataset from numerical\nsimulations, the LDM translates phase field data into corresponding temperature distributions through\na two-stage training process involving a vector-quantized variational autoencoder (VQVAE) and a\ndenoising autoencoder. The resulting model effectively reconstructs complex temperature fields at\ninterfaces. Spectral analysis indicates a high degree of agreement with ground truth data in the low to\nmid wavenumber ranges, even though some inconsistencies are observed at higher wavenumbers,\nsuggesting areas for further enhancement. This machine learning approach significantly reduces\nthe computational burden of traditional simulations and improves the precision of experimental\ncalibration methods. Future work will focus on refining the model's ability to represent small-scale\nturbulence and expanding its applicability to a broader range of boiling conditions.", "sections": [{"title": "1 Introduction", "content": "Flow boiling plays an important role in enhancing the performance of thermal management systems, including\nrefrigeration, microelectronics cooling, nuclear power plants, and nuclear fission reactors [1, 2]. This phenomenon\ninvolves a fluid absorbing heat and undergoing a phase change from liquid to vapor, while supplied with the advection\nof the bulk flow, significantly boosting the heat transfer efficiency through the utilization of latent heat. The initiation\nof the phase change is known as the onset of nucleate boiling (ONB) [3]. However, when the liquid fails to rewet\nthe surface, the surface becomes entirely covered by a vapor layer, leading to a significant reduction in heat transfer\nefficiency. This phenomenon is known as the departure from nucleate boiling (DNB) [4].\nThe heat transfer process between the ONB and the DNB points can be described using the RPI wall boiling model\n[5]. This model breaks down the total heat flux from the wall to the working fluid into three components: convective\nheat flux (qc), quenching heat flux (qq), and evaporation heat flux (qe). The convective heat flux accounts for the\nheat transfer between the liquid phase and the heated wall surface not covered by bubbles. The quenching heat flux\nrepresents the cyclic averaged transient heat transfer as liquid refills the wall vicinity after bubble detachment. The\nevaporative heat flux is associated with the latent heat carried away by departing bubbles.\nTo estimate the heat transfer, visualization methods such as side-view shadowgraphs, total reflection, and infrared\nthermography have been utilized as major tools for collecting information on bubble dynamics [6, 7]. Additionally,\nLaser-Induced Fluorescence (LIF) techniques have been adopted to experimentally measure the spatial temperature of\nthe fluids [8]. However, accurately estimating the actual temperature field remains extremely challenging. Furthermore,\ndetailed computational models for subcooled boiling flow, such as the Eulerian-Eulerian framework, involve establishing\nmass, momentum, and energy conservation equations for each phase separately, which requires tremendous resources\nto calculate [9].\nRecent advancements in machine learning, particularly in convolutional neural networks (CNNs), offer hope in\naddressing these challenges [10]. CNNs have shown proficiency in capturing visual features, suggesting the potential"}, {"title": "2 Implementation", "content": "This study utilizes the numerical simulation results of flow boiling from the BubbleML dataset, acquired by the Flash-\nX software [18, 19]. In particular, the phase field indicators and the temperature field data are utilized. To introduce\nthis dataset, the simulation was conducted by solving the mass, momentum, and energy conservation equations for\nincompressible flow. A single-fluid approach is adopted, with variable properties based on the location of the interface.\nThe momentum and energy equations are non-dimensionalized with reference quantities from the liquid phase defining\nthe Reynolds number, Prandtl number, and Froude number. Temperature is scaled using (T - Twall)/(Twall - Toulk). The\nliquid-vapor interface is tracked using a level-set advection equation. The fluid used is FC-72, known for its stability\nand insulation properties. The simulation has a domain resolution of nx = 1344, ny = 160, for the domain size of\nlx = 29.4 mm, ly = 3.5 mm. The non-dimensional constants for the fluid can be found in [18]. Simulation outputs are\nstored in HDF5 files with variables such as velocities, temperature, and signed distance function."}, {"title": "2.2 Implementation of the LDMs", "content": "In this study, we utilized LDMs for image-to-image translation. The key advantage of LDMs lies in their ability to\nseparate the training process into two distinct phases: perceptual compression and semantic compression. In the first\nphase, the perceptual autoencoder eliminates high-frequency details while retaining essential perceptual features. In the\nsecond phase, the diffusion model learns the semantic composition of the data within this compressed latent space,\nderived from the autoencoder, significantly reducing computational complexity while maintaining high-fidelity image\ngeneration.\nTo elaborate, the operational procedure of LDMs initiates with the encoding of the original image 20 into a latent\nrepresentation 20 using the encoder function E, such that zo = E(x0). Here, we used the vector-quantized variational\nautoencoder (VQVAE) for the effective compression of 20 by extracting and encoding essential perceptual features into\na lower-dimensional space, setting the stage for the subsequent diffusion processes [20].\nThis encoded representation, zo, is then subjected to a forward diffusion process where Gaussian noise \u2208 ~ N(0, I)\nis incrementally added over T timesteps according to the equation:\n$Zt = \\sqrt{\\alpha_t} z_0 + \\sqrt{1 - \\alpha_T} \\epsilon$.\nIn this formula, \u03b1t denotes the noise schedule that governs the variance of noise added at each timestep t.\nThroughout the diffusion process, a denoising autoencoder is employed to systematically remove the noise,\nprogressively transforming the noise into the latent representation. The reverse diffusion process aims to denoise zt\nback to 20. During this stage, the model employs a neural network, parameterized by 6, to predict the noise component\n\u20ac\u03b8 at each timestep, represented as: \u20ac = \u20ac\u03b8(zt, t).\nFinally, the denoised latent representation zo is passed through the decoder D of the VQVAE to reconstruct the\nimage, resulting in x\u03020 = D(z\u03020). This two-phase process efficiently utilizes the reduced dimensionality of the latent\nspace to capture and synthesize the complex images."}, {"title": "2.3 Neural Network Structure", "content": "As the architecture of our LDM comprises two major parts: VQVAE and denoising autoencoders, the architecture\nof the hierarchical autoencoders is elaborated below."}, {"title": "2.3.1 VQVAE Architecture", "content": "The input, sized at 160 \u00d7 1344, must be compressed into the latent size for efficient denoising. VQVAE extracts the\nlatent representation, and upon recovery, the decoder layer generates the output image by decompressing the latent data.\nThe architecture of VQVAE is U-Net-like, consisting of a downsampling encoding path, a bottleneck, and an upsampling\ndecoding path. The encoding path transfers information to the decoding path through skip connections, where features\nare concatenated with corresponding upsampling layers to preserve and utilize low-level information. Each layer in the\nencoding path, bottleneck, and decoding path comprises a mix of convolution layers, group normalization, and SiLU\nactivation functions.\nIn the encoding path, the encoder reduces the dimensionality of the source image. The number of feature channels\nincreases while residual connections ensure the preservation of input information and efficient gradient propagation.\nConvolution operations determine the downsampling size of the input data. The specific design choices include the\nkernel size, stride, and padding as follows:"}, {"title": "2.3.2 Denoising Autoencoder Architecture", "content": "After the encoder of VQVAE, the noising-denoising processes are applied with the diffusion models. We have\nadopted a U-Net-like architecture for the diffusion model, which consists of an encoding path, a bottleneck, and a"}, {"title": "2.4 Training Principles", "content": "Before training, the dataset is prepared by locating HDF5 files, which are then grouped into conditioning files\nand output files. The dataset comprises 200 timesteps representing the experimental conditions, with images for the\ntraining and test datasets randomly shuffled at an 8:2 ratio. Data loaders are instantiated to manage the loading and\npreprocessing of images, and the models are transferred to GPU CUDA for processing. During the first training process,\nthe VQVAE module is trained to learn D(E(yi)) = yi, where yi denotes the ith sequential temperature data. This\nprocess helps the autoencoder compress the input and reconstruct the output properly. Once the autoencoder training\nis complete, the denoising diffusion model receives latents zi = E(yi) to repeat noise addition-denoising processes.\nAs this study focuses on the conditioning of the latents, zcond = Econd(xi) is added to concatenate z = [Zi Zcond]. The\ntraining of the neural network models occurs while the architecture tries to denoise a random tensor of the same size as\nthe given latent space, to reconstruct it.\nThe objective function of the denoising process in the DDPM is designed to optimize the details of the latent\nspace, improving the quality of the generated images. This process is modeled as a Markov chain parameterized\nwith variational inference, learning Gaussian transitions. By iteratively updating the model parameters through this\noptimization process, the DDPM effectively learns to generate realistic images that closely resemble the ground truth.\nThe training loop continues for 5000 steps for the VQVAE and 1000 steps for the LDM. The learning rate for\nVQVAE is 1 \u00d7 10-5 and 5 \u00d7 10-6 for LDM. For the diffusion process, the number of timesteps is set to 1000, with the\nstarting \u1e9e at 8.5 \u00d7 10\u20134 and the ending value at 1.2 \u00d7 10\u22122. In each epoch, the noisy images and conditioning inputs\nare fed into the U-Net model to predict the noise. A linear noise scheduler is established during the forward diffusion\nprocess to manage the addition of random noise based on the current timestep. Then, the mean squared error (MSE)\nloss between the predicted noise and the actual noise is computed, gradients are calculated, and the optimizer updates\nthe model's parameters. The VQVAE model remains frozen during the training of the LDM to ensure its parameters\nare not updated. The LDMs are trained through the integration of phase information and temperature images, which\nalign with the actual values of temperature. The entire model implementation and training are conducted using Python\n3.10.12, PyTorch 2.0.0, CUDA 11.7.64, cuDNN 8.9.7, and an RTX 4090 Ti on an Ubuntu 22.04 system."}, {"title": "3 Results", "content": "The reconstruction of the temperature field has been achieved through the process of latent extraction using\nthe VQVAE. As shown in the latent images, this method effectively captures the bubble structures within the field.\nAdditionally, from the reconstructed temperature field, the VQVAE generates the overall temperature distribution with\nfine details. This demonstrates that the autoencoder can generate the temperature field accurately when provided with\nthe proper latent representations, which will later be obtained through the denoising of latent diffusion. This process\nensures that the intricate patterns and structures within the temperature field are preserved and accurately represented."}, {"title": "3.2 Snapshots of the Generated Temperature Field", "content": "The temperature field was generated using the LDM from the phase indicator map, where black represents the\nbubbles and white represents the liquid. This binary phase map serves as the input for the LDM to generate the latent\nspace conditioning."}, {"title": "4 Discussion and Conclusion", "content": "This study explored the potential of advanced machine learning techniques to enhance our understanding of flow\nboiling heat transfer while highlighting the effectiveness of LDMs in simplifying the training process and facilitating\nrapid image generation. This integrated approach allows for detailed visualization and analysis of the boiling process,\nproviding insights that are difficult to achieve with traditional experimental methods alone.\nWe have demonstrated that latent diffusion models can be trained to convert segmented interface images of two-phase\nindicators into corresponding temperature distributions. Our findings suggest that perceptual loss alone can serve as an\neffective reconstruction mechanism, provided that high-fidelity data is available and the model is capable of processing\nit efficiently. Furthermore, this highlights the potential benefits of digitizing the two-phase domain using various tools,\nsuch as Mask R-CNN, to enhance analysis and reconstruction accuracy. This involves identifying and segmenting the\ninterface in flow boiling systems. From generic shadowgraphs, by obtaining the sign function of these interfaces, we\ncan create a digital representation of the spatial boundaries within the boiling system. Combining this data with image\ntranslation techniques opens new avenues for accurately mapping the temperature fields.\nThe implications of this approach are significant. Firstly, it offers a method to bypass the computational complexity\nof direct numerical simulations typically required for modeling subcooled boiling flow. By leveraging the strengths of\nmachine learning, we can achieve high-fidelity results with reduced computational resources. Secondly, this technique\nprovides a basis for thermal field estimations that are congruent with visualization data, which is crucial for optimizing\nthermal management systems in applications where only visual probes can be applied. The dataset generated from\nthis approach can serve as a valuable calibration database for experimental techniques such as LIF. Such experimental\ntechniques, used to measure spatial temperature distributions, can benefit greatly from a well-calibrated dataset to\nimprove their reliability. By providing detailed field information on temperature, our machine learning models can\nenhance the calibration process, ensuring that experimental measurements are more precise and aligned with actual\nconditions.\nFuture research should focus on refining these machine learning models and expanding their application to various\nboiling conditions and configurations. Additionally, integrating real-time data acquisition systems with these predictive\nmodels could lead to the development of advanced control systems for thermal management, capable of dynamically\nadjusting operational parameters to optimize performance."}]}