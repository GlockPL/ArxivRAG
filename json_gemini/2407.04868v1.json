{"title": "Looking into Black Box Code Language Models", "authors": ["Muhammad Umair Haider", "Umar Farooq", "A.B. Siddique", "Mark Marron"], "abstract": "Language Models (LMs) have shown their applica-tion for tasks pertinent to code and several code LMs have beenproposed recently. The majority of the studies in this directiononly focus on the improvements in performance of the LMs ondifferent benchmarks, whereas LMs are considered black boxes.Besides this, a handful of works attempt to understand the roleof attention layers in the code LMs. Nonetheless, feed-forwardlayers remain under-explored which consist of two-thirds of atypical transformer model's parameters.\nIn this work, we attempt to gain insights into the innerworkings of code language models by examining the feed-forwardlayers. To conduct our investigations, we use two state-of-the-artcode LMs, Codegen-Mono and Ploycoder, and three widely usedprogramming languages, Java, Go, and Python. We focus onexamining the organization of stored concepts, the editability ofthese concepts, and the roles of different layers and input contextsize variations for output generation. Our empirical findingsdemonstrate that lower layers capture syntactic patterns whilehigher layers encode abstract concepts and semantics. We showconcepts of interest can be edited within feed-forward layerswithout compromising code LM performance. Additionally, weobserve initial layers serve as \"thinking\" layers, while later layersare crucial for predicting subsequent code tokens. Furthermore,we discover earlier layers can accurately predict smaller contexts,but larger contexts need critical later layers' contributions. Weanticipate these findings will facilitate better understanding,debugging, and testing of code LMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Code language models (code LMs), leveraging the trans-formers architecture [42], have emerged as powerful produc-tivity tools in software development. Inspired by the suc-cess of natural language processing (NLP) transformers (e.g.,BERT [9], GPT [34]), these models have been trained onvast repositories of code from open-source projects. Throughthis training, code LMs have acquired the ability to capturecomplex patterns, syntax, and semantics of programminglanguages. Consequently, code LMs have demonstrated sig-nificant success across various coding tasks, including codegeneration, completion, editing, and documentation. Notably,many of them including GitHub Co-pilot [16] and AmazonCodeWhisperer [2] are getting incorporated into integrateddevelopment environments (IDEs) as assistants to improvedevelopers' productivity.\nExisting work on code LMs, such as CodeBERT [13],GraphCodeBERT [20], CodeGPT [7], and CodeT5 [44] pri-marily focus on the performance improvement of the codeLMs on different benchmarks and treat code LMs as a blackbox. Specifically, 96% of studies focus on improving thepredictive accuracy of code LMs [24]. These studies overlooka crucial aspect: understanding the underlying mechanisms bywhich these models make predictions or generate code. As aconsequence, the inner workings of code LMs remain largelyobscure, potentially resulting in the generation of vulnerablecode [33], challenges in debugging [21], [23], and difficultiesin updating the codebase [4]. Moreover, the lack of inter-pretability undermines developers' confidence in these modelsand their ability to effectively leverage them in practicalsoftware development scenarios. Enhancing the interpretabilityof code LMs is critical for enhancing transparency, trust,compliance, and accountability in software development.\nRecognizing the importance of interpretability in code LMs,Authors in [30] focused on understanding the role of attentionlayers in code LMs. Their study examined the distribution ofattention weights across input sequences, shedding light on acrucial aspect of model behavior. Nonetheless, it is noteworthythat attention layers constitute only one-third of a typicalcode LM. The remaining two-thirds, primarily constituted byfeed-forward (FF) layers, have largely remained unexploredin existing research. Moreover, in NLP literature, FF layersare considered the databases (i.e., memory) of the model,represented in the form of keys and values [15]. In this work,we aim to bridge this gap by concentrating on the FF layersof code LMs, aiming to explain their role and impact in codeLMs.\nSpecifically, for a given code prefix as input, we computethe activation coefficient for a selected key in a certain layer.Then, we obtain the top code prefixes whose representationproduced the highest inner product with the given key. Uponanalyzing these prefixes, we discover interesting syntactic andsemantic patterns associated with each key. Likewise, whenwe mask keys related to a specific concept of interest (e.g.,numpy), we observe a notable decrease in the performanceof the code LMs concerning that particular concept. However,other programming constructs do not exhibit significant perfor-mance deterioration following the masking of the same keys.Additionally, we transform each value vector into a probabilitydistribution by multiplying it with the output embeddingmatrix. Then, we assess how the predictions at each layer alignwith the final output of the model. Furthermore, we manipulatethe context size to investigate the impact of varying contextlengths on this alignment.\nIn our investigation, we employ two well-known autore-gressive code LMs: Codegen-Mono-2.7B [31] and Polycoder-2.7B [45]. Codegen specializes in the Python programminglanguage, while Polycoder encompasses multiple languages,where our focus is on three diverse programming languages:Java, Go, and Python. To conduct our exploration, we collected5,000 code files from active GitHub repositories with morethan 50 stars for each programming language.\nSpecifically, our study focuses on the following researchquestions (RQs)."}, {"title": "RQ1: What information is stored in the feed-forward layersof code LMs?", "content": "Considering the unexplored role of FF layers in code LMs,our inquiry aims to uncover what information is stored inFF layers. We examine the top 50 input sequences againsteach key in the FF layers, which exhibit the highest activationin that key relative to all other sequences in the dataset.We then qualitatively and quantitatively explored these keysto see how the model is storing information to uncoverinsights into the nature of information representation in theFF layers, particularly in relation to code generation tasks.\nOur investigation revealed that the FF layers of code LMsare responsible for capturing a wide range of information,spanning from fundamental syntactic patterns such as key-words and n-grams to more abstract concepts and semantics.Notably, the initial layers predominantly capture low-levelsyntactic elements (e.g., keywords, n-grams), while the higherlayers capture more abstract and higher-level semantics, suchas iterators and other complex programming constructs."}, {"title": "RQ2: Can we precisely edit a concept of interest in code LMs,and how does such editing affect the general performance ofcode LMs?", "content": "If we truly understand how information is stored in the FFlayers, then we must be able to edit it. We aim to find outthe feasibility of accurately editing the concept of interestin code LMs and to evaluate the subsequent impact on themodel's overall performance. This inquiry is motivated bythe need to quantify the adaptability of code LMs to newinformation, particularly concerning deprecated methods orapplication programming interfaces (APIs). To address thisquestion, adopt a systematic approach. Initially, we identifyand filter keys associated with APIs of interest, such as numpyin Python, across various programming languages usingregular expressions, focusing on those keys where our conceptof interest ranks among the top 50 triggers. Subsequently, weapply masking techniques to these keys and observe the effecton the model's performance concerning the concept of interest.Conversely, we evaluate the impact of masking the same keyson the model's performance on everything except the conceptof interest, aiming to quantify any potential side effects ongeneral performance. Our findings indicate a significant de-crease in accuracy concerning the concept of interest, implyingthat the model's knowledge is highly localized. Additionally,we did not observe a noteworthy decline in the model'sperformance regarding all other aspects except for the conceptof interest. This empirical evidence demonstrates the viabilityof editing operations without detrimentally affecting its generalperformance."}, {"title": "RQ3: How does local information in each layer agree to thefinal output of code LMs?", "content": "This question seeks to explain how the final output ofthe model is constructed across layers and to what extentagreement exists between different layers and the ultimateoutput of the model. This inquiry is motivated by the desireto gain insights into the flow of information in the model andto comprehend the mechanisms underlying the formulationof the model's final output. To investigate this question, wemultiply the output of each layer with the output embeddingmatrix, apply argmax operation to the output of each layer, andcompare the top token prediction of the last layer with thoseof all preceding layers. This operation enables us to quantifythe degree of agreement between different layers and assesshow information is processed and consolidated throughout themodel. We observe that initial layers exhibit limited agreementwith the final layers, suggesting that they primarily functionas \"thinking\" layers rather than directly contributing to theoutput. In contrast, as we progress through the model, weobserve an increase in agreement, indicating that later layers,which possess more processed information, play a pivotal rolein generating the final output. This empirical evidence shedslight on the hierarchical nature of information processing incode LMs."}, {"title": "RQ4: How does the context size impact the agreement betweenlayers in code LMs?", "content": "This question investigates the effect of context size on theagreement between layers in code LMs. We aim to understandhow variations in context size, ranging from shorter to longersequences, influence the degree of agreement among differentlayers in the model. This inquiry is motivated by the interestin assessing how the complexity of the task for the modelchanges with varying context sizes. Our analysis reveals thatearlier layers can accurately predict smaller initial contexts,whereas as the context size increases, the task of predictingthe correct output becomes more challenging. Only laterlayers demonstrate the capability to predict accurately in suchscenarios, indicating a significant impact of context size onthe model's performance and problem difficulty."}, {"title": "Summary of findings", "content": "We explore how FF layers encodesyntactic and semantic information of programming languagesand their role in generating output tokens in code LMs.Our empirical findings demonstrate that lower layers capturesyntactic patterns, while higher layers encode abstract con-cepts and semantics. We also show that concepts of interestcan be edited within FF layers without compromising theperformance of code LMs. Additionally, we observe thatinitial layers serve as \u201cthinking\" layers, while later layers arecrucial for predicting the next tokens of code. Furthermore,we discover that earlier layers can make accurate predictionsfor smaller contexts, while larger contexts pose a greaterchallenge, where the role of later layers is critical."}, {"title": "Contributions", "content": "In summary, this work makes the followingcontributions:\n\u2022We explore and describe the role of feed-forward layersin code language models, which consist of two-thirds ofa typical transformer model's parameters.\n\u2022We demonstrate the viability of editing a concept ofinterest in code language models and empirically showthe impact of editing concepts on model performance.\n\u2022We explore how information aggregates through the mod-els and the impact of context size variations and differentlayers on the models' final outputs.\nNext, we discuss the background of this work (in Sec. II),Sec. III provides details about our approach including selectedcode LMs and dataset. We present our investigations andfindings on information storage and editing in Sec. IV andSec. V provides insights and discussions on layer agreementto output and the impact of context size on code LMs. Wediscuss related work in Sec. VI and provide conclusions ofthis work in Sec. VII."}, {"title": "II. BACKGROUND", "content": "In this section, we discuss the necessary background ontransformer-based language models, code LMs, and neuralmemories."}, {"title": "A. Transformer-based Language Models", "content": "The Transformer architecture [42] employs interconnectedattention blocks and feed-forward layers. The attentionblock [3] facilitates the model's ability to weigh the signifi-cance of individual tokens in a sequence, thus capturing long-range dependencies across the input sequence. Concurrently,the feed-forward layers enable the model to retain crucial in-formation derived from the training data [15]. The transformer-based LMs are trained using extensive text data in a self-supervised manner. Their substantial parameter space, oftenreaching billions or even trillions, gives them an impressiveability to absorb broad semantic and syntactic knowledgeand strong memorization skills. These models have achievedstate-of-the-art performance for various NLP tasks, and theutilization of transformer-based LMs has emerged as a highlypromising research direction in NLP [9], [34], [36], [38], [42].\nTransformer-based LMs have three variations in their archi-tecture. Table I illustrates these architectures. Encoder-decodermodels, such as T5 [38], adhere to the original transformerarchitecture, with both encoder and decoder stacks. Theyformulate tasks by framing them as text-to-text problems,enabling unified training and inference. Encoder models, suchas Bidirectional Encoder Representations from Transform-ers (BERT) [8], utilize the encoder stack and adopt a maskedlanguage modeling objective during training. They leveragebidirectional context understanding to comprehend text ef-fectively. Decoder models, such as Generative Pre-trainedTransformer (GPT) [36], leverage the decoder stack. They aretrained to predict the next tokens based on preceding ones, theyexcel in language generation tasks. Due to the simplicity ofthe decoder architecture and the prevalence of text generationtasks, decoder models have become a de facto standard forvarious language modeling tasks."}, {"title": "B. Code Language Models", "content": "Following the success of transformer architecture in NLP"}, {"title": "Looking into Black Box Code Language Models", "authors": ["Muhammad Umair Haider", "Umar Farooq", "A.B. Siddique", "Mark Marron"], "abstract": "Language Models (LMs) have shown their applica-tion for tasks pertinent to code and several code LMs have beenproposed recently. The majority of the studies in this directiononly focus on the improvements in performance of the LMs ondifferent benchmarks, whereas LMs are considered black boxes.Besides this, a handful of works attempt to understand the roleof attention layers in the code LMs. Nonetheless, feed-forwardlayers remain under-explored which consist of two-thirds of atypical transformer model's parameters.\nIn this work, we attempt to gain insights into the innerworkings of code language models by examining the feed-forwardlayers. To conduct our investigations, we use two state-of-the-artcode LMs, Codegen-Mono and Ploycoder, and three widely usedprogramming languages, Java, Go, and Python. We focus onexamining the organization of stored concepts, the editability ofthese concepts, and the roles of different layers and input contextsize variations for output generation. Our empirical findingsdemonstrate that lower layers capture syntactic patterns whilehigher layers encode abstract concepts and semantics. We showconcepts of interest can be edited within feed-forward layerswithout compromising code LM performance. Additionally, weobserve initial layers serve as \"thinking\" layers, while later layersare crucial for predicting subsequent code tokens. Furthermore,we discover earlier layers can accurately predict smaller contexts,but larger contexts need critical later layers' contributions. Weanticipate these findings will facilitate better understanding,debugging, and testing of code LMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Code language models (code LMs), leveraging the trans-formers architecture [42], have emerged as powerful produc-tivity tools in software development. Inspired by the suc-cess of natural language processing (NLP) transformers (e.g.,BERT [9], GPT [34]), these models have been trained onvast repositories of code from open-source projects. Throughthis training, code LMs have acquired the ability to capturecomplex patterns, syntax, and semantics of programminglanguages. Consequently, code LMs have demonstrated sig-nificant success across various coding tasks, including codegeneration, completion, editing, and documentation. Notably,many of them including GitHub Co-pilot [16] and AmazonCodeWhisperer [2] are getting incorporated into integrateddevelopment environments (IDEs) as assistants to improvedevelopers' productivity.\nExisting work on code LMs, such as CodeBERT [13],GraphCodeBERT [20], CodeGPT [7], and CodeT5 [44] pri-marily focus on the performance improvement of the codeLMs on different benchmarks and treat code LMs as a blackbox. Specifically, 96% of studies focus on improving thepredictive accuracy of code LMs [24]. These studies overlooka crucial aspect: understanding the underlying mechanisms bywhich these models make predictions or generate code. As aconsequence, the inner workings of code LMs remain largelyobscure, potentially resulting in the generation of vulnerablecode [33], challenges in debugging [21], [23], and difficultiesin updating the codebase [4]. Moreover, the lack of inter-pretability undermines developers' confidence in these modelsand their ability to effectively leverage them in practicalsoftware development scenarios. Enhancing the interpretabilityof code LMs is critical for enhancing transparency, trust,compliance, and accountability in software development.\nRecognizing the importance of interpretability in code LMs,Authors in [30] focused on understanding the role of attentionlayers in code LMs. Their study examined the distribution ofattention weights across input sequences, shedding light on acrucial aspect of model behavior. Nonetheless, it is noteworthythat attention layers constitute only one-third of a typicalcode LM. The remaining two-thirds, primarily constituted byfeed-forward (FF) layers, have largely remained unexploredin existing research. Moreover, in NLP literature, FF layersare considered the databases (i.e., memory) of the model,represented in the form of keys and values [15]. In this work,we aim to bridge this gap by concentrating on the FF layersof code LMs, aiming to explain their role and impact in codeLMs.\nSpecifically, for a given code prefix as input, we computethe activation coefficient for a selected key in a certain layer.Then, we obtain the top code prefixes whose representationproduced the highest inner product with the given key. Uponanalyzing these prefixes, we discover interesting syntactic andsemantic patterns associated with each key. Likewise, whenwe mask keys related to a specific concept of interest (e.g.,numpy), we observe a notable decrease in the performanceof the code LMs concerning that particular concept. However,other programming constructs do not exhibit significant perfor-mance deterioration following the masking of the same keys.Additionally, we transform each value vector into a probabilitydistribution by multiplying it with the output embeddingmatrix. Then, we assess how the predictions at each layer alignwith the final output of the model. Furthermore, we manipulatethe context size to investigate the impact of varying contextlengths on this alignment.\nIn our investigation, we employ two well-known autore-gressive code LMs: Codegen-Mono-2.7B [31] and Polycoder-2.7B [45]. Codegen specializes in the Python programminglanguage, while Polycoder encompasses multiple languages,where our focus is on three diverse programming languages:Java, Go, and Python. To conduct our exploration, we collected5,000 code files from active GitHub repositories with morethan 50 stars for each programming language.\nSpecifically, our study focuses on the following researchquestions (RQs)."}, {"title": "RQ1: What information is stored in the feed-forward layersof code LMs?", "content": "Considering the unexplored role of FF layers in code LMs,our inquiry aims to uncover what information is stored inFF layers. We examine the top 50 input sequences againsteach key in the FF layers, which exhibit the highest activationin that key relative to all other sequences in the dataset.We then qualitatively and quantitatively explored these keysto see how the model is storing information to uncoverinsights into the nature of information representation in theFF layers, particularly in relation to code generation tasks.\nOur investigation revealed that the FF layers of code LMsare responsible for capturing a wide range of information,spanning from fundamental syntactic patterns such as key-words and n-grams to more abstract concepts and semantics.Notably, the initial layers predominantly capture low-levelsyntactic elements (e.g., keywords, n-grams), while the higherlayers capture more abstract and higher-level semantics, suchas iterators and other complex programming constructs."}, {"title": "RQ2: Can we precisely edit a concept of interest in code LMs,and how does such editing affect the general performance ofcode LMs?", "content": "If we truly understand how information is stored in the FFlayers, then we must be able to edit it. We aim to find outthe feasibility of accurately editing the concept of interestin code LMs and to evaluate the subsequent impact on themodel's overall performance. This inquiry is motivated bythe need to quantify the adaptability of code LMs to newinformation, particularly concerning deprecated methods orapplication programming interfaces (APIs). To address thisquestion, adopt a systematic approach. Initially, we identifyand filter keys associated with APIs of interest, such as numpyin Python, across various programming languages usingregular expressions, focusing on those keys where our conceptof interest ranks among the top 50 triggers. Subsequently, weapply masking techniques to these keys and observe the effecton the model's performance concerning the concept of interest.Conversely, we evaluate the impact of masking the same keyson the model's performance on everything except the conceptof interest, aiming to quantify any potential side effects ongeneral performance. Our findings indicate a significant de-crease in accuracy concerning the concept of interest, implyingthat the model's knowledge is highly localized. Additionally,we did not observe a noteworthy decline in the model'sperformance regarding all other aspects except for the conceptof interest. This empirical evidence demonstrates the viabilityof editing operations without detrimentally affecting its generalperformance."}, {"title": "RQ3: How does local information in each layer agree to thefinal output of code LMs?", "content": "This question seeks to explain how the final output ofthe model is constructed across layers and to what extentagreement exists between different layers and the ultimateoutput of the model. This inquiry is motivated by the desireto gain insights into the flow of information in the model andto comprehend the mechanisms underlying the formulationof the model's final output. To investigate this question, wemultiply the output of each layer with the output embeddingmatrix, apply argmax operation to the output of each layer, andcompare the top token prediction of the last layer with thoseof all preceding layers. This operation enables us to quantifythe degree of agreement between different layers and assesshow information is processed and consolidated throughout themodel. We observe that initial layers exhibit limited agreementwith the final layers, suggesting that they primarily functionas \"thinking\" layers rather than directly contributing to theoutput. In contrast, as we progress through the model, weobserve an increase in agreement, indicating that later layers,which possess more processed information, play a pivotal rolein generating the final output. This empirical evidence shedslight on the hierarchical nature of information processing incode LMs."}, {"title": "RQ4: How does the context size impact the agreement betweenlayers in code LMs?", "content": "This question investigates the effect of context size on theagreement between layers in code LMs. We aim to understandhow variations in context size, ranging from shorter to longersequences, influence the degree of agreement among differentlayers in the model. This inquiry is motivated by the interestin assessing how the complexity of the task for the modelchanges with varying context sizes. Our analysis reveals thatearlier layers can accurately predict smaller initial contexts,whereas as the context size increases, the task of predictingthe correct output becomes more challenging. Only laterlayers demonstrate the capability to predict accurately in suchscenarios, indicating a significant impact of context size onthe model's performance and problem difficulty."}, {"title": "Summary of findings", "content": "We explore how FF layers encodesyntactic and semantic information of programming languagesand their role in generating output tokens in code LMs.Our empirical findings demonstrate that lower layers capturesyntactic patterns, while higher layers encode abstract con-cepts and semantics. We also show that concepts of interestcan be edited within FF layers without compromising theperformance of code LMs. Additionally, we observe thatinitial layers serve as \u201cthinking\" layers, while later layers arecrucial for predicting the next tokens of code. Furthermore,we discover that earlier layers can make accurate predictionsfor smaller contexts, while larger contexts pose a greaterchallenge, where the role of later layers is critical."}, {"title": "Contributions", "content": "In summary, this work makes the followingcontributions:\n\u2022We explore and describe the role of feed-forward layersin code language models, which consist of two-thirds ofa typical transformer model's parameters.\n\u2022We demonstrate the viability of editing a concept ofinterest in code language models and empirically showthe impact of editing concepts on model performance.\n\u2022We explore how information aggregates through the mod-els and the impact of context size variations and differentlayers on the models' final outputs.\nNext, we discuss the background of this work (in Sec. II),Sec. III provides details about our approach including selectedcode LMs and dataset. We present our investigations andfindings on information storage and editing in Sec. IV andSec. V provides insights and discussions on layer agreementto output and the impact of context size on code LMs. Wediscuss related work in Sec. VI and provide conclusions ofthis work in Sec. VII."}, {"title": "II. BACKGROUND", "content": "In this section, we discuss the necessary background ontransformer-based language models, code LMs, and neuralmemories."}, {"title": "A. Transformer-based Language Models", "content": "The Transformer architecture [42] employs interconnectedattention blocks and feed-forward layers. The attentionblock [3] facilitates the model's ability to weigh the signifi-cance of individual tokens in a sequence, thus capturing long-range dependencies across the input sequence. Concurrently,the feed-forward layers enable the model to retain crucial in-formation derived from the training data [15]. The transformer-based LMs are trained using extensive text data in a self-supervised manner. Their substantial parameter space, oftenreaching billions or even trillions, gives them an impressiveability to absorb broad semantic and syntactic knowledgeand strong memorization skills. These models have achievedstate-of-the-art performance for various NLP tasks, and theutilization of transformer-based LMs has emerged as a highlypromising research direction in NLP [9], [34], [36], [38], [42].\nTransformer-based LMs have three variations in their archi-tecture. Table I illustrates these architectures. Encoder-decodermodels, such as T5 [38], adhere to the original transformerarchitecture, with both encoder and decoder stacks. Theyformulate tasks by framing them as text-to-text problems,enabling unified training and inference. Encoder models, suchas Bidirectional Encoder Representations from Transform-ers (BERT) [8], utilize the encoder stack and adopt a maskedlanguage modeling objective during training. They leveragebidirectional context understanding to comprehend text ef-fectively. Decoder models, such as Generative Pre-trainedTransformer (GPT) [36], leverage the decoder stack. They aretrained to predict the next tokens based on preceding ones, theyexcel in language generation tasks. Due to the simplicity ofthe decoder architecture and the prevalence of text generationtasks, decoder models have become a de facto standard forvarious language modeling tasks."}, {"title": "B. Code Language Models", "content": "Following the success of transformer architecture in NLP,code LMs have adopted this architecture. In code LMs, thereare primarily three categories, mirroring the classifications oftransformer models. These are Masked LMs, Encoder-Decoder, and Decoder-only autoregressive models.\nMasked LMs in the context of coding generate code formasked tokens by classifying them based on the adjacenttokens on either side [9]. The advantage of Masked LMs"}]}]}