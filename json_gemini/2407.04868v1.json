{"title": "Looking into Black Box Code Language Models", "authors": ["Muhammad Umair Haider", "Umar Farooq", "A.B. Siddique", "Mark Marron"], "abstract": "Language Models (LMs) have shown their applica-tion for tasks pertinent to code and several code LMs have been proposed recently. The majority of the studies in this direction only focus on the improvements in performance of the LMs on different benchmarks, whereas LMs are considered black boxes. Besides this, a handful of works attempt to understand the role of attention layers in the code LMs. Nonetheless, feed-forward layers remain under-explored which consist of two-thirds of a typical transformer model's parameters.\nIn this work, we attempt to gain insights into the inner workings of code language models by examining the feed-forward layers. To conduct our investigations, we use two state-of-the-art code LMs, Codegen-Mono and Ploycoder, and three widely used programming languages, Java, Go, and Python. We focus on examining the organization of stored concepts, the editability of these concepts, and the roles of different layers and input context size variations for output generation. Our empirical findings demonstrate that lower layers capture syntactic patterns while higher layers encode abstract concepts and semantics. We show concepts of interest can be edited within feed-forward layers without compromising code LM performance. Additionally, we observe initial layers serve as \"thinking\" layers, while later layers are crucial for predicting subsequent code tokens. Furthermore, we discover earlier layers can accurately predict smaller contexts, but larger contexts need critical later layers' contributions. We anticipate these findings will facilitate better understanding, debugging, and testing of code LMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Code language models (code LMs), leveraging the trans-formers architecture [42], have emerged as powerful produc-tivity tools in software development. Inspired by the suc-cess of natural language processing (NLP) transformers (e.g., BERT [9], GPT [34]), these models have been trained on vast repositories of code from open-source projects. Through this training, code LMs have acquired the ability to capture complex patterns, syntax, and semantics of programming languages. Consequently, code LMs have demonstrated sig-nificant success across various coding tasks, including code generation, completion, editing, and documentation. Notably, many of them including GitHub Co-pilot [16] and Amazon CodeWhisperer [2] are getting incorporated into integrated development environments (IDEs) as assistants to improve developers' productivity.\nExisting work on code LMs, such as CodeBERT [13], GraphCodeBERT [20], CodeGPT [7], and CodeT5 [44] pri-marily focus on the performance improvement of the code LMs on different benchmarks and treat code LMs as a black box. Specifically, 96% of studies focus on improving the predictive accuracy of code LMs [24]. These studies overlook a crucial aspect: understanding the underlying mechanisms by which these models make predictions or generate code. As a consequence, the inner workings of code LMs remain largely obscure, potentially resulting in the generation of vulnerable code [33], challenges in debugging [21], [23], and difficulties in updating the codebase [4]. Moreover, the lack of inter-pretability undermines developers' confidence in these models and their ability to effectively leverage them in practical software development scenarios. Enhancing the interpretability of code LMs is critical for enhancing transparency, trust, compliance, and accountability in software development.\nRecognizing the importance of interpretability in code LMs, Authors in [30] focused on understanding the role of attention layers in code LMs. Their study examined the distribution of attention weights across input sequences, shedding light on a crucial aspect of model behavior. Nonetheless, it is noteworthy that attention layers constitute only one-third of a typical code LM. The remaining two-thirds, primarily constituted by feed-forward (FF) layers, have largely remained unexplored in existing research. Moreover, in NLP literature, FF layers are considered the databases (i.e., memory) of the model, represented in the form of keys and values [15]. In this work, we aim to bridge this gap by concentrating on the FF layers of code LMs, aiming to explain their role and impact in code LMs.\nSpecifically, for a given code prefix as input, we compute the activation coefficient for a selected key in a certain layer. Then, we obtain the top code prefixes whose representation produced the highest inner product with the given key. Upon analyzing these prefixes, we discover interesting syntactic and semantic patterns associated with each key. Likewise, when we mask keys related to a specific concept of interest (e.g., numpy), we observe a notable decrease in the performance of the code LMs concerning that particular concept. However, other programming constructs do not exhibit significant perfor-mance deterioration following the masking of the same keys. Additionally, we transform each value vector into a probability distribution by multiplying it with the output embedding matrix. Then, we assess how the predictions at each layer align with the final output of the model. Furthermore, we manipulate the context size to investigate the impact of varying context lengths on this alignment.\nIn our investigation, we employ two well-known autore-gressive code LMs: Codegen-Mono-2.7B [31] and Polycoder-"}, {"title": "II. BACKGROUND", "content": "In this section, we discuss the necessary background on transformer-based language models, code LMs, and neural memories.\nA. Transformer-based Language Models\nThe Transformer architecture [42] employs interconnected attention blocks and feed-forward layers. The attention block [3] facilitates the model's ability to weigh the signifi-cance of individual tokens in a sequence, thus capturing long-range dependencies across the input sequence. Concurrently, the feed-forward layers enable the model to retain crucial in-formation derived from the training data [15]. The transformer-based LMs are trained using extensive text data in a self-supervised manner. Their substantial parameter space, often reaching billions or even trillions, gives them an impressive ability to absorb broad semantic and syntactic knowledge and strong memorization skills. These models have achieved state-of-the-art performance for various NLP tasks, and the utilization of transformer-based LMs has emerged as a highly promising research direction in NLP [9], [34], [36], [38], [42].\nTransformer-based LMs have three variations in their archi-tecture. Table I illustrates these architectures. Encoder-decoder models, such as T5 [38], adhere to the original transformer architecture, with both encoder and decoder stacks. They formulate tasks by framing them as text-to-text problems, enabling unified training and inference. Encoder models, such as Bidirectional Encoder Representations from Transform-ers (BERT) [8], utilize the encoder stack and adopt a masked language modeling objective during training. They leverage bidirectional context understanding to comprehend text ef-fectively. Decoder models, such as Generative Pre-trained Transformer (GPT) [36], leverage the decoder stack. They are trained to predict the next tokens based on preceding ones, they excel in language generation tasks. Due to the simplicity of the decoder architecture and the prevalence of text generation tasks, decoder models have become a de facto standard for various language modeling tasks.\nB. Code Language Models\nFollowing the success of transformer architecture in NLP, code LMs have adopted this architecture. In code LMs, there are primarily three categories, mirroring the classifications of transformer models. These are Masked LMs, Encoder-Decoder, and Decoder-only autoregressive models.\nMasked LMs in the context of coding generate code for masked tokens by classifying them based on the adjacent tokens on either side [9]. The advantage of Masked LMs over autoregressive models lies in their ability to consider the context from both sides of a masked token, providing a richer base of information for predicting the masked token. Examples of Masked LMs tailored for coding include CodeBert [13] and CuBERT [25].\nThe predominant category in code LMs is the auto-regressive models, which focus on predicting the subsequent token based on the preceding context. The GPT models [35] belong to of decoder-only category and the T5 models [37] are encoder-decoder models. In Encoder-Decoder models an encoder encodes the input, which is then passed to a de-coder akin to GPT for multiple mask prediction. The Code-specific Encoder-Decoder models include CodeT5 [44] and PLBART [1]. Lastly, Decoder-Only models (i.e., GPTs) es-timate the likelihood of the next token based on previous ones. In the broader field of NLP, GPT-like models have achieved prominence, a trend that extends to code LMs as well. Decoder-Only models for code feature [5], [6], [29], [31], [45], among others.\nThe widespread adoption of auto-regressive models, includ-ing GPT variants, is primarily due to their sequential left-to-right token prediction capability. This trait enables their application in a variety of contexts, such as code completion, generating comments for code, or converting plain text into code [13], [20].\nC. Neural Memories\nAuthors in [40] has shown that feed-forward layers act as key-value memories, emulating memory networks [41]. For a given input context x, we can compute the distribution over keys: \\(p(k_i | x) \\propto exp(x.k_i)\\) and memory of x can be expressed as \\(M(x) = \\sum_{i=1}^{dm} p(k_i | X)v_i\\). That is, we can represent FF layers as \\(FF(x) = f(x \\cdot K^T) \\cdot V\\), where \\(x \\in d\\) is the text input, \\(K, V \\in d^{m \\times d}\\) represent parameter matrices and f denotes a non-linearity [15].\nCode LMs follow the transformer architecture [42], which incorporates interconnected self-attention and FF layers. Each FF layer operates as a position-wise function, independently processing input vectors. The FF layers function using two matrices: one representing keys and the other values. The first matrix serves as a set of key vectors, while the second matrix serves as a set of corresponding values for these keys. Specifically, transformers employ ReLU non-linearity and the function of FF layers can be expressed as: \\(FF(x) = ReLU(x\\cdot K^T) \\cdot V\\), where x represents the input vector, K represents the output of the first matrix acting as keys, and V represents the output of the second matrix acting as values. Figure 1"}, {"title": "III. APPROACH", "content": "In this section, we discuss our approach to conducting our study, including selected code models, dataset, and research questions.\nA. Selected Models\nFor our choice of models, we chose two state-of-the-art mid-sized models for our investigation. One is a mono-language model and the other is a multi-language model.\nIV. INFORMATION STORAGE AND EDITING"}, {"content": "The following section describes our methods and experi-ments to find out how information is stored in FF layers (RQ1), how can we edit stored concepts, and the impact of editing on code LMS (RQ2).\nA. Information Storage\n1) Capturing Top Trigger Examples.: Let us denote our dataset as D, which consists of n code prefixes represented as {\\(x_1, x_2,...,x_n\\)}. A code prefix \\(x_i\\) is passed through the model and an activation coefficient \\(a_i = max(\\overrightarrow{x_k})\\) is computed for every key k in layer l, where \\(\\overrightarrow{x}\\) denotes the representation of \\(x_i\\) at layer l, and k is the key vector corresponding to the i-th hidden dimension at layer l. This process is repeated for all the prefixes in D. Then a ranking of \\(x \\in D\\) is established for each key k based on the activation coefficient \\(a_i\\). For each key k in layer l, we then identify t trigger examples {\\(X_1, X_2,...,X_t\\)} \\(CD\\), which produce activation coefficients that rank in the top 50 of a particular key k.\nAuthors in [15] suggest that these keys act as detectors for specific patterns from the input data. By examining top-t triggers for a key, we can deduce what patterns that key is responsive to. This method of probing allows us to uncover the encoded patterns in a given code LM's keys. That is, we can discover how the model encodes and interprets information and the model's operational logic.\n2) Pattern Analysis using Regular Expression Filtering: Once we have successfully gathered top-k triggers for all the keys, we face the challenge of dealing with an extensive search space, which makes obtaining meaningful quantitative results a daunting task. For example, both the models under investigation in this work (i.e., Codegen-Mono, and Polycoder) are autoregressive models with 32 layers and 2560 hidden dimensions containing a total of 2,560 \u00d7 4 \u00d7 32 = 327,680 keys. Navigating through this vast expanse is no small feat.\nTo tackle this issue, we employ a strategic approach. We im-plement regular expression (regex) filtering, targeting various application programming interfaces (APIs) such as numpy and torch, as well as fundamental programming concepts like loops and conditionals. This process helps us narrow down the"}, {"title": "V. INFORMATION AGGREGATION", "content": "This section elaborates on our approach and experiments to investigate the alignment between local information at different layer levels and the final output (RQ3) and study the effects of varying context sizes on these alignments (RQ4).\nA. Layer Agreements to Final Output\nTo understand how different layers aggregate information to form the model's final output and whether different layers agree with the model's final output, we conduct the following experiment. We transform each value vector (i.e., hidden dimension output of the second feed-forward layer), denoted as v in layer l, into a probability distribution over the vocabulary and select the token with the highest probability. That is, we perform multiplication of v for each layer l with the output embedding matrix of the model E, and subsequently applying softmax function: \\(p_l = softmax(v \\cdot E)\\). We then apply argmax function \\(o_l = argmax(p)\\) to get \\(o_l\\) which is the top predicted token by layer l, when \\(x_i \\in D\\) is passed as input to the model.\nIt is important to note that the resulting probability distribu-tion \\(p_l\\) is not calibrated. However, it is worth mentioning that the ranking established by \\(p_l\\) remains unaffected, allowing for meaningful analysis. To compute agreement we compare the top token prediction \\(o_l\\) from each layer l with the final output of the model \\(o_L\\), where L represents the last layer. If \\(o_l = o_L\\), then layer l agrees with the model's final output when \\(x_i \\in D\\) is passed as input to the model.\nTo conduct the agreement experiment, we utilize our entire dataset. For each line of code, we generate multiple examples by considering all prefixes of the line, resulting in n examples, where n represents the number of tokens in the line of code. Figure 2 presents the results of this experiment, where it is evident that the agreement of initial layers in all the settings is quite low but as we move ahead into the model the agreement starts to increase, and in the last few layers it is exponentially high."}, {"title": "VI. RELATER WORK", "content": "Understanding the mechanisms behind the predictions of models is crucial for their deployment in real-world applica-tions. Interpretability focuses on uncovering the rationale of model decisions, providing insights into model behavior, and enhancing the trustworthiness of models. We organize related work into two categories: interpretability in machine learning and interpretability in code LMs.\nA. Interpretability in Machine Learning\nThe methods for achieving interpretability in machine learn-ing models can be broadly categorized into three main types: (i) counterfactual interventions, (ii) hyper-network structures, and (iii) probing-based methods. The counterfactual interven-tion methods investigate how the changes in input features influence model outputs by modifying inputs and observing resultant output variations. These methods include techniques like removing or replacing input words to determine their effect on model decisions, with examples being the extraction of key sentences from labeled documents. The works [27] and [39] are examples of counterfactual interventions. The hyper-network structure approaches involve creating a learnable mask over the neurons of a frozen pre-trained model, where an L1-norm or L2-norm is applied to the masks [22]. These masks serve as indicators of neuron importance in the targeted area, examples of hyper-network structure approaches are [36] and [26]. Lastly, there are probing-based methods, which involve aligning model neurons or components with specific concepts by identifying patterns of co-occurrence between neuron activations and the target concept [10], [15]. Our method of probing the model keys falls under this general category of interpretability.\nOur work builds upon Geva et al.'s methodology, which pointed out that the feed-forward layers are key-value storage bases. The domain of analysis is different, the paper of Geva et al. is in natural language processing but our work is on coding models. The definition of semantic meaning between natural language and code is very different. For example, in Geva et al.'s work examples of higher semantics are the concept of time, the concept of TV shows, etc. Whereas, in our work, we investigate how the semantics of code are formed. For example, values \u20180..255\u2018 being related to image (Table 4), keywords \u2018for\u2018and \u2018while\u02bb being related to each other are coding concepts (Table 6). In the domain of natural language analysis, we will not find these patterns, and a genuine research question arises whether coding models contain patterns similar to natural language or a different set of coding patterns emerge. This work tries to uncover coding patterns, and their storage and retrieval mechanisms. In the paper of Geva et al., the exploration of syntactic/semantic patterns is not targeted. That is, they randomly select keys and find trigger examples associated with those keys. Their work can not enable targeted exploration of a concept of interest, for example a specific API or method. Thus, it can not be used for eventual editing. In our work, we use regular expressions to find and analyze where information about an API of interest is stored. Due to the targeted nature of our exploration, we can leverage this information about specific concepts of interest for editing on a general API level. Moreover, we show that information is localized enough to be edited without compromising the general performance of the models.\nB. Interpretability in Code LMs\nInterpretability within code generation models remains a relatively under-explored area of research, and most of the research in the field focuses on the attention part of the model. Authors in [30] examine CodeBERT and GraphCodeBERT in the context of software engineering tasks. By analyzing attention scores across different token types, the study reveals patterns in how these models allocate attention to various parts of the code. Authors in [28] examine the effectiveness of pre-trained language models like CodeT5 and CodeGPT in generating, translating, and repairing code, They use attention interpretability specifically focusing on how these models pay attention to different parts of the code during the generation process. Authors in [32] compare the attention mechanisms of neural models analyzing code to the attention of skilled human developers. It introduces a method for capturing human attention on code and compares it with the attention weights"}, {"title": "VII. CONCLUSIONS", "content": "This work targets a key problem in code MLs \u2013 understand-ing the inner workings and interpretability of code language models. Our study focused on feed-forward layers of LMs, which consist of two-thirds of a typical transformer model's parameters. In our investigations, we employ two state-of-the-art code language models, Codegen-Mono and Polycoder, and leverage three widely-used programming languages, Java, Go, and Python, as the basis for our analyses. Our empirical findings show lower layers capture syntax while higher layers encode abstract concepts and semantics. We demonstrate con-cepts can be edited in feed-forward layers without compromis-ing the code language model's performance. Initial layers serve as \"thinking\" layers, while later layers crucially predict sub-sequent tokens. Earlier layers can accurately predict smaller contexts, whereas the role of later layers becomes critical in facilitating better predictions. We anticipate that these findings will lay the groundwork for developing a more comprehensive understanding, enabling more effective debugging and testing methodologies for code language models."}]}