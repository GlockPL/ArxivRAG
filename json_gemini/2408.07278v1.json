{"title": "Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction", "authors": ["Wenhao Li", "Jie Zhou", "Chuan Luo", "Chao Tang", "Kun Zhang", "Shixiong Zhao"], "abstract": "In the realm of modern mobile E-commerce, providing users with nearby commercial service recommendations through location-based online services has become increasingly vital. While machine learning approaches have shown promise in multi-scene recommendation, existing methodologies often struggle to address cold-start problems in unprecedented scenes: the increasing diversity of commercial choices, along with the short online lifespan of scenes, give rise to the complexity of effective recommendations in online and dynamic scenes. In this work, we propose Scene-wise Adaptive Network (SWAN), a novel approach that emphasizes high-performance cold-start online recommendations for new scenes. Our approach introduces several crucial capabilities, including scene similarity learning, user-specific scene transition cognition, scene-specific information construction for the new scene, and enhancing the diverged logical information between scenes. We demonstrate SwAN's potential to optimize dynamic multi-scene recommendation problems by effectively online handling cold-start recommendations for any newly arrived scenes. More encouragingly, SwAN has been successfully deployed in Meituan's online catering recommendation service, which serves millions of customers per day, and SwAN has achieved a 5.64% CTR index improvement relative to the baselines and a 5.19% increase in daily order volume proportion.", "sections": [{"title": "1 Introduction", "content": "Delivering users with nearby commercial service suggestions through location-based online systems [14, 18, 28] has grown increasingly crucial within the era of modern mobile E-commerce. Learning to Rank (LTR) involves applying machine learning algorithms [3, 12] in optimizing the rank strategy, and is the fundamental technique to facilitate better recommendation services. Contemporary recommendation systems not only focus on users' habits derived from historical information but also endeavor to infer the preferences of the same user across diverse scenes, facilitating more accurate and high-quality multi-scene recommendation (MSR) [23].\nDespite the considerable advancements in MSR research, a majority of these developments are grounded in the assumption that scenes are predefined and classified prior to offline training, with all subsequent recommendations adhering to established categories during online operations.\nTherefore, the existing literature (e.g.SAML [2], STAR [19], and HMOE [11]) on MSR primarily concentrates on a static model architecture that distinguishes scenes by directing inputs of each scene to a fixed structural branch within the model.\nHowever, empirical evidence reveals that this assumption does not always hold true. As the assortment of items and options expands in today's world, a proliferation of distinct scenes arises. Consequently, users' behaviors tend to diverge more frequently, leading to an increased variety of scenes without previous identical scenes available for reference in historical data [9].\nAccording to Fig. 1, the online recommendation service will launch different scenes during specific periods in spring or winter, taking into account user preferences and merchant demands. Additionally, it will also design exclusive activities for specific holidays, such as New Year's Day. On the other hand, scenes often have a limited online lifespan before vanishing (e.g. Valentine's Day in Fig. 1), leaving no opportunity for a recommendation system to collect data, go offline for fine-tuning, and return online [16].\nThe Hybrid of implicit and explicit Mixture-of-Experts (HMOE) [11] demonstrates that the performance of one scene can be enhanced (through training) by the prediction of other scenes. Unfortunately, HMOE still requires learning the historical data of a new scene and sharing information between scenes through re-parameterization.\nIn this paper, we demonstrate that the performance of a newly-arrived scene can be directly and significantly improved through online prediction using our Scene-wise Adaptive Network (SWAN) model. This suggests that cold-starting new scenes is not only feasible but also surpasses the recommendation performance of existing approaches on known scenes.\nIn general, SwAN employs the typical Embedding&MLP (Multi-layer Perceptron) paradigm for the recommendation [24] (Sec. 3). In the Embedding part, SwAN utilizes the Scene Relation Graph (SRG) to capture graph-structured similarities between scenes based on inherent attributes and user interaction features, thereby learning the inertial patterns among scenes. The SwAN model also incorporates the Similarity Attention Network (SAN) to capture users' habits during scene transitions by applying user attention on scene similarity knowledge. Furthermore, SwAN assigns each known scene a separate feature embedding (Scene Embedding Layers) to understand how scenes individually influence user behavior and interlace them with the SAN, allowing the impact of new scenes on users to be directly derived. In the MLP part, SwAN generally adopts the Adaptive Ensemble-experts Module (AEM), which is a Mixture-of-Experts (MoE) architecture and includes an Adaptive Expert Group (AEG) of Sparse MoE that uniquely leverages Cosine Loss to enhance diversities between scenes, as well as a Shared Expert Group (SEG) of Multi-gate MoE that captures the shared logic of scenes. In particular, a novel component named Dics has been proposed for the AEG to achieve gradient propagation and select appropriate model structures adaptively.\nExtensive evaluation on both the public and industrial datasets shows that the SwAN model outperforms existing MSR approaches by seamlessly adapting to new scenes and providing more accurate and high-quality recommendations. SwAN achieves up to 5.64% online CTR improvement relative to the baselines and up to 5.19% increase in daily order volume proportion, as evaluated in Sec. 4.5. The main contributions of this paper are as follows:\n\u2022 We propose SwAN, an innovative high-performance multi-scene cold-start optimization network.\n\u2022 Innovatively, we propose SRG to acquire prior information from similar scenes for cold-start scenes and employ SAN to get the attention weight of these scenes from user's perspective. Finally, AEM dynamically allocates model structures to enhance the extraction capability of shared and specific information across different scenes.\n\u2022 SwAN has been deployed in a real-world online business recommendation system of Meituan and achieved a 5.64% improvement in CTR compared to the baseline model."}, {"title": "2 Related work", "content": "Multi-scene learning tackles recommendations for users across various scenes [29]. Traditional models for multi-scene learning have been developed to enhance performance in multiple fixed scenes. Drawing inspiration from the Multi-task Mixture-of-Experts model, Li [11] introduced HMoE that implicitly identifies scene disparities and similarities in the feature space and explicitly enhances performance in the label space using a stacked model."}, {"title": "3 Approach", "content": "This section presents our design of the proposed SwAN model (Fig. 2). In essence, SwAN follows the key principle of optimizing multi-scene (by extracting scene-specific and shared information) and cold-start (by incorporating data from similar scenes as supplements) problem and consists of multiple modules: the Scene Relation Graph (Sec.3.1), Similarity Attention Network (Sec.3.2), Cross-scene Feature Representation (Sec.3.3), Adaptive Ensemble-experts Module (Sec.3.4). The Decision Layer (Sec.3.5) of SWAN and the loss function (Sec.3.6) are appended."}, {"title": "3.1 Scene Relation Graph (SRG)", "content": "A scene comprises inherent attribute features and user interaction features [1]. In dynamic multi-scene problems, there is no historical interaction data between users and new scenes, which means that only scene attribute features can be invoked to collect information. Fortunately, users exhibit similar preferences in comparable scenes. Based on our post-fact online business analysis, users often perceive a positive correlation between the similarity of scenes and the similarity of item features within those scenes [7]. This allows a recommendation system to optimize the cold-start process by leveraging prior information from analogous scenes to resemble the target scene closely.\nBased on these premises, SwAN invokes a Scene Relation Graph (SRG) module that builds a relational graph between the current scene (to be predicted) and the existing scenes based on the scene features. The construction process of SRG is as follows:\n(1) Firstly, it lists the basic features (unrelated to online interactions, e.g.price and category) of the items to be sorted and uses user key interactions as labels to calculate the Pearson correlation coefficient of various features in the existing scenes, selecting the top-n (Sec. 4.3 for details) key features."}, {"title": "3.2 Similarity Attention Network (SAN)", "content": "However, determining similar scenes based solely on attributes is inadequate. In real-world applications, various users perceive the same scene pair differently, and the model must incorporate user cognition to comprehend the latent similarity between scenes on a deeper level [30]. For instance\u00b9, some individuals consider horror movies and zombie movies part of the same genre, while others do not.\nConsequently, our model enhances the SRG module by introducing user information for attention. This is achieved by incorporating a Similarity Attention Network (SAN, shown in Fig. 4) to calculate learned latent similarity from the user's perspective.\nThe input of the SAN includes the features of the target scene, the similar scenes defined in the SRG, and the user. The specific attention calculation (referred to the DIN [26]) is as follows:\n$\\widehat{S_{i}} = MLP[E_{u} \\oplus E_{t} \\oplus (E_{t} - E_{i}) + (E_{t} \\& E_{i})]$,\n$S_{i} = softmax(\\widehat{S_{i}})$,\n$Vec_{san} = \\sum_{i=1}^{I} S_{i} E_{s}$,\nwhere $E_{u}$, $E_{t}$, and $E_{i}$ are the embeddings of users, target scene, and the i-th similar scene, respectively; I denotes all the scenes; $S_{i}$ is"}, {"title": "3.3 Cross-scene Feature Representation (CFR)", "content": "There are differences in the bottom-level feature representation for each scene as well [2]. For example, the distance between users and dining locations has different importance in the breakfast and regular meal scenes\u00b2.\nTo reflect these differences and provide information supplementation for the cold-start embedding of the target scene, SwAN added a Cross-scene Feature Representation (CFR) structure to the feature processing module (Fig. 2), which essentially assigns each extent scene a separate embedding to capture a scene's properties solely. Specifically, the input of CFR is the scene-related features $f_{target\\_scene}$ and the similarity between scenes output by SAN, and the calculation formula is as follows:\n$E_{cfr} = \\sum_{i=1}^{I} S_{i} EMB_{i}(f_{target\\_scene})$,\nwhere $EMB_{i}()$ is the embedding layer corresponding to the i-th similar scene. The input of the subsequent model is:\n$E_{in} = E_{o} + E_{u} + Vec_{san}(E_{t} + E_{cfr})$,\nwhere $E_{o}$ means the embedding of other features, and + means element-wise addition (\u24b6 in Fig. 2).\nThe model transfers prior information from similar scenes regarding feature representation dimensions through CFR, optimizing the cold-start problem and enhancing the expression of differences between scenes. In addition, since CFR essentially involves multiple dictionary lookups and weighted vector summation, it does not introduce excessive computational overhead."}, {"title": "3.4 Adaptive Ensemble-experts Module (AEM)", "content": "Traditional static multi-scene models usually set up separate model branches for each scene (e.g.STAR [19]), using structural differences to improve the ability to mine diverged information and optimize negative transfer problem, which are the cores of multi-scene modeling. However, in dynamic multi-scene problems, numerous scenes go online and offline frequently. The traditional model design approach cannot assign model structure for cold-start scenes, while the strategy of retraining the model based on a small number of cold-start scene samples leads to computational redundancy and require frequent offline fine-tuning to update the model architecture. To solve the above problems, we designed Adaptive Ensemble-experts Module (AEM) as the backbone network of the model to enhance the ability to extract differential information and optimize negative transfer in dynamic and multi-scene environments (Fig. 2).\nFirstly, we draw inspiration from the MMoE model [15] and develop multiple expert networks to enhance the model's ability"}, {"title": "3.5 Decision Layer", "content": "By utilizing AEM, SWAN extracts the shared and specific information of scenes, which is contained in the output vectors of each expert in SEG and AEG, respectively. However, the contribution of each vector to the final prediction target varies. To address this issue, inspired by the solution of MMOE, we add a gating network for each expert:\n$G_{i} = MLP_{g}(E_{in})$,\n$E_{final\\_in} = \\sum_{i=0}^{|SEG|} G_{i}Vec_{s} + \\sum_{i=0,k=0}^{|AEG|} G_{i} W_{k} Vec_{a}$,\nwhere $E_{final\\_in}$ is the input of the MLP structure in the output stage of SwAN, $G_{i}$ is the gate value of each expert, $MLP_{g}()$ is the MLP structure to calculate the gate value, and $Vec_{s}$ and $Vec_{a}$ are the expert outputs of SEG and AEG, respectively. The output of the decision layer, namely, the final output of SwAN, can be expressed as:\n$Output = sigmoid[MLP(E_{final\\_in})]."}, {"title": "3.6 Composition of Losses", "content": "SWAN uses the Cross-Entropy loss function between output and label to guide training. The formula is as follows:\n$Loss_{ce}(y, \\widehat{y}) = \\sum -[y \\cdot log(\\widehat{y}) + (1 - y) \\cdot log(1 - \\widehat{y})]$,\nwhere y and \u0177 are label and predicted value, respectively.\nIn addition, as described in Sec. 3.4, a variance loss is added:\n$Loss_{var} = \\sum(W_{k} - \\overline{W})^{2}$,\nwhere $N_{a}$ is the number of experts in AEG.\nTo sum up, the loss function is as follows:\n$Loss = \\alpha Loss_{ce}(y, \\widehat{y}) + \\beta Losscos + \\gamma \\cdot Lossvar$,\nwhere \u03b1, \u03b2, and \u03b3 are hyper-parameters set according to the actual dataset (Sec. 4.3)."}, {"title": "4 Experiments", "content": "To verify the effectiveness and generalization of the SwAN model, this study conducts experiments based on two datasets: a closed-source dataset from Meituan's online catering recommendation service with millions of daily users and an open-source dataset constructed from the Taobao public dataset [5]."}, {"title": "4.1 Experimental Settings", "content": "Industrial Dataset. Samples from Dataset-1 are obtained from the online catering recommendation platform of Meituan, specifically from the business of Sales Campaign Session with an average daily"}, {"title": "4.2 Experimental Results", "content": "The following part mainly introduces the experimental setup and analyzes the comparison among our SwAN model and other single-scene models (SSM) and static multi-scene models (SMSM) in the recommendation datasets of Meituan and Taobao.\nSSM Experiments. This experiment is first based on classical SSM, including DNN [4], MMoE [15], and PLE [20]. DNN is a single-scene and single-task model. As shown in Table 2, our model has significantly improved AUC compared to DNN in both datasets, especially for the recommendation effect of new scenes in Meituan's dataset. MMoE and PLE are all single-scene and multi-task models. In this experiment, we added two objectives, click prediction and order prediction, for Dataset-1. In contrast, we conducted single-objective prediction for the Taobao dataset due to only one objective provided. In addition, to be consistent with the industrial application strategy, the SSM model uses all samples from various scenes for training and incorporates scene IDs as features. However, no multi-scene model structure optimization has been performed. The experimental results also prove that our model outperforms the baseline models in new and old scenes.\nSMSM Experiments. To verify the effectiveness of SwAN compared to existing state-of-the-art SMSMs, we trained the HMOE [11], STAR [19], PEPNet [1] and HiNet [27] and then conducted comparative experiments. According to the definition mentioned earlier, both of these models belong to the static multi-scene model, which is suitable for multiple fixed scenes with stable traffic, and therefore contradicts the definition of dynamic multi-scene. To solve this problem, we adopted a standard solution in industrial applications: clustering scenes based on their attributes and treating the resulting cluster of new and old scenes as a sizeable stable scene."}, {"title": "4.3 Hyperparameters Experiments", "content": "To illustrate the impact of hyperparameters on the experimental results, we conducted relevant experiments based on dataset-1.\nHyperparameters of SRG. We tested the experimental results of constructing SRG by filtering features according to different correlation coefficients (cc) thresholds (Table.5). It can be found from the experimental results that a reasonable threshold can filter out noise features and select as many effective features as possible to improve the model performance. The empirical threshold is 0.05, which can also be experimented with and adjusted according to specific business data.\nHyperparameters of Loss Functions. Regarding the hyper-parameter selection of loss functions, the experimental outcomes for \u03b1, \u03b2, and \u03b3 are presented in Table 6, Table 7, and Table 8 (Section.3.6 for details). Given that $Loss_{ce}$ plays a pivotal role in model optimization, it is advisable to set \u03b1 to a relatively substantial value. Conversely, as both $Loss_{cos}$ and $Loss_{uar}$ serve as auxiliary components in the training process, it is recommended to keep \u03b2 and \u03b3 small-valued."}, {"title": "4.4 Ablation Study and Analysis", "content": "The results of the ablation experiments are shown in Table 9. Firstly, we tested the impact of SRG on the model performance. SRG is mainly responsible for introducing prior knowledge of similar scenes to the model, which is the theoretical basis of SwAN. After removal, other affected structures must be randomly initialized and uniformly distributed. This structure significantly impacts the"}, {"title": "4.5 Application in Practice", "content": "SwAN has been deployed in the online recommendation system to validate its practical effectiveness and component efficacy.\nOnline Application Performance. Besides the importance in theory, recommendation plays a pivotal role in commercial situations. To further demonstrate the superior performance of SwAN in practical applications, we deployed it in the online recommendation system of Meituan's catering business with an average daily user level of millions, which has typical dynamic multi-scene characteristics, and conducted an A/B test with 20% of the traffic over a period of two months. More than 200 new online scenes were added during the experiment, and the total number exceeded 400. The experimental results showed that SwAN achieved a 5.64% increase in the CTR index compared to the best baseline model (PLE) and a 5.19% increase in daily order volume proportion after full traffic promotion.\nIn addition, we randomly selected 6 new scenes online and calculated the CTR improvements of both SwAN and baseline models relative to the default ranking method (Table 13):\nFirst, the baseline models and SwAN have significant CTR enhancements relative to the default ranking. However, SwAN has a smaller Gini coefficient for the improvement ratio between scenes, demonstrating superior stability. We also calculated the Gini coefficient of the improvement ratio of SwAN relative to the baseline model in each scene, which is 0.2651 (0.2351 in all scenes). This value proves that the improvement of SwAN relative to the baseline"}, {"title": "4.6 Model Complexity", "content": "We compared each model's number of parameters, training time, and inference time. All experiments were conducted on NVIDIA Tesla A100 GPU, 80G RAM, and Intel(R) Xeon(R) Gold 5218 CPU servers. Table 14 shows that SwAN maintains a reasonable number of parameters and prediction time, indicating that it can be deployed online without incurring additional costs. Generally, we retrieve online data in real-time for training and update the model approximately every half hour."}, {"title": "5 Conclusion", "content": "In this paper, we propose the SwAN model, a novel approach to addressing the cold-start problem in Multi-scene Recommendation (MSR) systems. The proposed model overcomes the limitations of traditional MSR approaches by directly and significantly enhancing the performance of newly-arrived scenes through online prediction. The unique architecture of the SwAN model, which combines the Scene Relation Graph (SRG), Similarity Attention Network (SAN), and Adaptive Ensemble-experts Module (AEM), enables it to capture graph-structured similarities between scenes, understand user behavior transitions, and identify shared logic among different scenes. Our extensive evaluation of SwAN on both public and Meituan industrial datasets demonstrate the superiority of the SwAN model over existing MSR approaches, providing more accurate and high-quality recommendations in a dynamic and adaptable manner. Furthermore, SwAN has been deployed in the online catering recommendation service of Meituan, which serves millions of daily customers, and has achieved a significant improvement in CTR (Click-Through Rate) index. This work represents a significant step forward in developing efficient and adaptable recommendation systems, particularly in the context of a rapidly evolving E-commerce landscape."}]}