{"title": "RATE: SCORE REWARD MODELS WITH IMPERFECT REWRITES OF REWRITES", "authors": ["David Reber", "Sean Richardson", "Todd Nief", "Cristina Garbacea", "Victor Veitch"], "abstract": "This paper concerns the evaluation of reward models used in language modeling. A reward model is a function that takes a prompt and a response and assigns a score indicating how 'good' that response is for the prompt. A key challenge is that reward models are usually imperfect proxies for actual preferences. For example, we may worry that a model trained to reward helpfulness learns to instead prefer longer responses. In this paper, we develop an evaluation method, RATE (Rewrite-based Attribute Treatment Estimators), that allows us to measure the causal effect of a given attribute of a response (e.g., length) on the reward assigned to that response. The core idea is to use large language models to rewrite responses to produce imperfect counterfactuals, and to adjust for rewriting error by rewriting twice. We show that the RATE estimator is consistent under reasonable assumptions. We demonstrate the effectiveness of RATE on synthetic and real-world data, showing that it can accurately estimate the effect of a given attribute on the reward model. Code is available at https://github.com/toddnief/RATE.", "sections": [{"title": "1 INTRODUCTION", "content": "In the context of large language models (LLMs), reward models evaluate the quality or appropriateness of model outputs, either by assessing individual responses or comparing multiple alternatives. Such models are useful in a variety of settings, including alignment of large language models, ranking output samples (e.g., to use in a best-of-n sampling procedure), or evaluation of LLM performance.\nIdeally, reward models would directly and perfectly measure whatever aspect of the output is importante.g., we might have a reward for mathematical problem solving based on whether the generated response is correct. However, reward models are commonly learned from training data that imperfectly measures somewhat nebulous attributes. For example, a common task is to train a reward model based on human preferences for which of two responses is more helpful. This results in a challenge where, even with a reward model in hand, we are not certain what it is actually rewarding. For example, we might worry that a model trained to reward helpfulness learns to instead simply prefer longer responses (Shen et al., 2023; Park et al., 2024b; Singhal et al., 2024).\nTo address this challenge, we need a method to quantify how sensitive a reward model is to specific attributes of a response. A straightforward approach would be to collect a dataset of prompt/response pairs, label each response as having or not having the attribute of interest, and then compare the average reward assigned to responses with and without the attribute. However, this approach has the limitation that it does not account for 'spurious' correlations that may exist in the data. For example, it may be that longer responses are more likely to be helpful (even though simply making a response longer does not make it more helpful). Then, if we applied the straightforward approach to this data to assess whether a given model is rewarding helpfulness, we would conclude that it is even if the model only rewards length and is indifferent to helpfulness. If we then used this reward model as a proxy for helpfulness in a downstream alignment task, then the actual effect of alignment would be to make responses longer, without (necessarily) affecting helpfulness.\nInstead, we are actually interested in knowing how the reward would change if we were to change some attribute in the response, such as length, while holding all else fixed. This is the causal effect of the attribute on the reward. There is a growing literature on estimating the causal effects of attributes"}, {"title": "2 SETUP", "content": "Reward models are typically implemented in two ways:\n1. As functions $R(x, y)$ that take a prompt x and a response y as inputs and return a real number indicating the quality of the response for the prompt.\n2. As functions $R(x, y_1, y_0)$ that take a prompt x and two responses $y_1$ and $y_0$ as inputs and return a real number describing the relative quality of $y_1$ compared to $y_0$.\nOur results apply to both implementations, but we focus on the first for clarity (see Section 6).\nSuppose we have a dataset of prompt-completion pairs {$(x^i, y^{ii})$}, where the $x^i$ are prompts and the $y^{ii}$ are completions (also referred to as 'responses'). We have a reward model $R(x^i, y^{ii})$ that assigns a scalar reward to each prompt-completion pair. We are interested in understanding how the reward model responds to a certain attribute, represented by the function W, within the completions. For each prompt-completion pair, we have a binary label $w^{ii} = W(x^i, y^{ii}) \\in$ {0,1} indicating whether the completion has the attribute of interest.\nFor example, W might represent helpfulness, which varies based on the context given by the prompt. A recipe could be helpful for questions about cooking but not for questions about history.\nWe focus on binary attributes for simplicity-many attributes of interest (such as length) can often be naturally binarized (see Section 6).\nNaive Method If we want to measure the sensitivity of a given reward model to an attribute of interest such as helpfulness, the obvious approach is to take the dataset of prompt-completion"}, {"title": "3 RATE: REWRITE-BASED ATTRIBUTE TREATMENT ESTIMATORS", "content": "Whatever our choice of estimand, we need a method to estimate it. Here, we develop a method, RATE, that uses rewrites to estimate the causal effect of an attribute on a reward model. The core idea is to create pairs of responses where the only difference is in the attribute of interest. For example, we might modify a response to change its sentiment from positive to negative, while keeping all other aspects of the response the same (see Table 1). The goal is for these modified responses to directly approximate the unobserved counterfactual responses.\nRewrites With LLMs In practice, we implement rewrites using a large language model (LLM). We begin with a labeled dataset containing ground truth binary variables for attributes such as complexity, sentiment, or helpfulness. We then instruct the LLM to rewrite the responses to the opposite state of the binary variable. For example, a typical instruction might be: \u201cRewrite this response to express negative sentiment and change nothing else.\u201d\nWe use $Re(x^i, y^{ii}, w)$ to denote the rewrite operation, which takes a prompt-response pair $(x^i, y^{ii})$ and a desired attribute value w, returning a modified response $\\tilde{y}^{ii}$ such that $W(x^i, \\tilde{y}^{ii}) = w$.\nRewrite Instructions There is significant flexibility in how to instruct an LLM to rewrite.\nFor instance, when rewriting for 'helpfulness', we might instruct the LLM to \"Rewrite this response to be more helpful\", or instruct it to \u201cRewrite this response to be more helpful, providing additional relevant information or clarification.\" In this example, the second instruction makes the meaning of 'helpful' more precise. Generally, changing the instruction changes the nature of the rewrites generated, and thus changes the attribute that is being modified.\nThis is inevitable. Ambiguity in interventions is unavoidable in causal inference (Hern\u00e1n, 2016). In our context, there is subjectivity in what helpfulness, complexity, or sentiment actually mean. An advantage of the rewrite approach is that it allows us to use natural language to specify, as clearly as possible, what property we are actually trying to modify. We can understand whether our instructions are having the intended effect by qualitatively examining the rewritten outputs and checking that they vary the attribute of interest while leaving the rest of the response unchanged. In practice, finding effective rewrite instructions requires an iterative cycle of generating rewrites, examining the responses, and adjusting the rewrite prompt to be more clear and specific.\nImperfect Rewrites If the rewrites produced perfect counterfactuals, it would then be straightforward to estimate the causal effect of the attributes. Namely, we could compare the rewards of the original responses to the rewards of the rewrites. However, the rewrites are often imperfect, modifying off-target attributes. These off-target modifications may affect the reward, causing the simple comparison to be misleading. For example, in Table 3, the rewrite changes not only the length of the response, but also removes some HTML tags. Changing the off-target attributes can affect the reward, leading to a biased estimate of causal effects.\nMathematically, whenever we rewrite some response $y^{ii}$ (to W = w), we introduce some error $\\varepsilon$ in the reward because of our inability to perfectly produce the counterfactual $y^{i(w)}$, which ought to differ from the original response only with respect to the target attribute. Define this error as:\n$\\varepsilon = R(x^i, Re(x^i, y^{ii}, w)) \u2013 R(x^i, y^{i(w)})$"}, {"title": "4 THEORETICAL ANALYSIS OF RATE", "content": "Under reasonable assumptions, RATE is a consistent estimator of the average treatment effect.\nLatent Variable Model To analyze the rewrite operation, we need to conceptualize how different aspects of a response might change during rewriting. Imagine a response as having three types of attributes: the target attribute we want to change (like sentiment), attributes that should remain constant (like topic), and attributes that might unintentionally change (like specific wording). We can formalize this idea using a latent variable model:\n$Y = Y (W, Z, \\xi)$\nwhere:\n\u2022 Y is the observed response\n\u2022 W is the target attribute we aim to manipulate (e.g., sentiment, complexity)\n\u2022 Z represents off-target attributes that are invariant to rewrites (e.g., topic, language)\n\u2022 $\\xi$ represents off-target attributes that may be affected by rewrites (e.g. grammatical structure)\nIntuitively, we expect some off-target attributes Z to remain unchanged during rewrites. For example, if we ask a large language model to change the sentiment of an English text, we don't expect it to suddenly produce Korean. However, other off-target attributes $\\xi$ may change: for instance, grammar and punctuation might be corrected.\nUnbiasedness and Consistency of RATE To establish that RATE is a sound estimator of the causal effect we need some additional assumptions:\n1. We assume that the reward model can be decomposed additively:\n$R(X, Y (W, Z, \\xi)) = R_{W,Z}(X,W, Z) + R_{\\xi}(X, \\xi)$\nwhere:\n(a) $R_{W,Z}(X, W, Z)$ is the component of the reward that depends on the target attribute W and the immutable off-target attributes Z.\n(b) $R_{\\xi}(X, \\xi)$ is the component of the reward that depends on the mutable off-target attributes $\\xi$.\nThis means that we don't need to worry about potential interactions between rewrite errors (affecting $\\xi$) and other attributes of the response (Z), even if W and Z have interactions.\n2. We assume that the off-target changes introduced by the rewrite process are randomly drawn from a distribution determined by the particular rewrite method being used. That is,\n$Re(Y (W, Z, \\xi)) \\sim Y (W, Z, \\xi),$ where $\\xi \\sim P_{Re} (\\xi)$\nFor example, when our rewriter is GPT-40, the off-target yet mutable attributes such as specific word choice and grammatical structure are drawn from a 'GPT-like' distribution."}, {"title": "5 EXPERIMENTS", "content": "We evaluate reward models using RATE on real-world and synthetic data. Experiments show:\n\u2022 Across a variety of attributes and datasets, RATE gives substantively different estimates compared to the naive (non-causal) baseline.\n\u2022 In semi-synthetic data with known ground truth behavior, RATE is robust to distributional shift, while the naive estimator is not.\n\u2022 Addressing the rewrite bias by employing rewrites-of-rewrites is essential, as relying on single rewrites leads to significantly different and potentially skewed outcomes.\nReal World Reward Models We select several of the top-performing reward models from RewardBench (Lambert et al., 2024) and evaluate them using both RATE and the naive method across a variety of attributes and datasets: IMDB (Maas et al., 2011), ELI5 (Fan et al., 2019), HelpSteer (Wang et al., 2023). Randomly sampled rewrites with associated rewards are shown in Appendix B, along with details for designing rewrite instructions."}, {"title": "6 DISCUSSION", "content": "Generalization to Contrastive Rewards The RATE procedure applies more generally to contrastive rewards of the form $R(x, y_1, y_0)$, which assign a relative reward for $y_1$ compared to $y_0$. In this case, RATE enables us to compute $E[R(X,Y(W = 1),Y(W = 0))]$, the expected increase"}, {"title": "7 RELATED WORK", "content": "Our work intersects with three main areas of research: challenges in reward modeling, causal inference applied to text classifiers, and the use of counterfactuals in language models."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To facilitate reproducibility of our RATE method, we have taken the following measures: (1) Our code implementation, including scripts for producing rewrites, estimating treatment effects, and generating plots, is provided at https://github.com/toddnief/RATE. (2) The datasets used in our experiments (IMDB, ELI5, HelpSteer, HH RLHF) are publicly available. (3) In Appendix B, we provide randomly sampled texts, rewrites, and rewrites of rewrites for each dataset/attribute combination, allowing the reader to qualitatively evaluate our rewrites. (4) All reward models evaluated in this study (i.e., FsfairX-LLaMA3-RM-v0.1, NCSOFT/Llama-3-OffsetBias-RM-8B, ArmoRM) are open-source. (5) We report confidence intervals for all main results to ensure statistical reliability, using a normal distribution because of our large sample size. (6) Section 5 includes tips for creating effective rewrite instructions and documents challenges encountered during the rewrite process, aiding in the reproduction of our methodology. (7) For the synthetic experiments, we provide details on how we induced correlations in Appendix B."}]}