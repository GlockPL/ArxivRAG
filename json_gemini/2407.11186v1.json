{"title": "FarsInstruct: Empowering Large Language Models for Persian Instruction Understanding", "authors": ["Hojjat Mokhtarabadi", "Ziba Zamani", "Abbas Maazallahi", "Hossein Manshaei"], "abstract": "Instruction-tuned large language models, such as TO, have demonstrated remarkable capabilities in following instructions across various domains. However, their proficiency remains notably deficient in many low-resource languages. To address this challenge, we introduce FarsInstruct: a comprehensive instruction dataset designed to enhance the instruction-following ability of large language models specifically for the Persian language\u2014a significant yet underrepresented language globally. FarsInstruct encompasses a wide range of task types and datasets, each containing a mix of straightforward to complex manual written instructions, as well as translations from Public Pool of Prompts, ensuring a rich linguistic and cultural representation. Furthermore, we introduce Co-COLA, a framework designed to enhance the multi-task adaptability of LoRA-tuned models. Through extensive experimental analyses, our study showcases the effectiveness of FarsInstruct dataset coupled with training by Co-CoLA framework, in improving the performance of large language models within the Persian context. As of the current writing, FarsInstruct comprises more than 200 templates across 21 distinct datasets, and we intend to update it consistently, thus augmenting its applicability.", "sections": [{"title": "1 Introduction", "content": "The modern era of artificial intelligence is marked by numerous breakthroughs, among which is the rise of large language models (LLMs). These models, such as PaLM (Chowdhery et al., 2022), GPT4 (OpenAI et al., 2024), and Llama2 (Touvron et al., 2023) with continuous scaling of their parameters and training data, are known to exhibit emergent properties. Wei et al. (2022a) considers an ability to be emergent if it is not present in smaller models but is present in larger models. This is an unpredictable phenomenon that can not be predicted simply by extrapolating the performance of smaller models. One such ability is instruction-following, which enables models to execute unseen natural language processing (NLP) tasks from reading instructions provided within the input text. Previously, the capability for instruction-following was primarily attributed to the scale of these models. However, recent studies have demonstrated that instruction-following does not exclusively rely on the large size of language models (Sanh et al., 2022). By instruction-tuning on a collection of instructional NLP tasks, smaller language models can learn to follow prompts. This approach has proven to be remarkably efficient, allowing these smaller models to perform competitively and, in some specific tasks, even outperform their larger counterparts (Sanh et al., 2022; Wei et al., 2021; Wang et al., 2022). Instruction-tuning emerges as a vital technique in the evolution of language models, involving training a model on a wide range of tasks described through natural language instructions. This method diverges from traditional task-specific fine-tuning, offering a more generalized and versatile approach to model training, thus contributing significantly to the advancement of LLMs.\nDespite the steady progress of instruction-tuned language models, they still struggle to accurately grasp the subtleties of low-resourced languages due to the scarcity of prompted data and challenges inherent in translating English datasets into other languages (Naous et al., 2024; Ramesh et al., 2023; Vanmassenhove et al., 2021). While efforts have been made to compile extensive multilingual instruction-following datasets, gaps remain in creating diverse and complex prompts for languages like Persian. For example, the SuperNaturalInstructions benchmark (Wang et al., 2022), encompassing various task types across 55 languages, contains merely 2.1% of Persian content. Similarly, the Aya Dataset (Singh et al., 2024), a human-curated effort to enhance AI's instruction-following abilities across 65 languages, includes 1% of Persian content. This underscores the disparity in the diversity and quantity of the Persian Language tasks compared to other languages.\nIn this study, we propose FarsInstruct, a comprehensive prompted dataset tailored to the Persian language. It comprises a mixture of manually written instructions ranging from basic to proficient language levels, as well as translations from Public Pool of Prompts (P3) (Sanh et al., 2022) which is a collection of prompted English datasets. In particular, we created more than 200 prompt templates (roughly 10 templates for each of the 21 unique public datasets) that we selected from a variety of sources. These datasets collectively cover ten different task categories: Text Summarization, Textual Entailment, Text Classification, Sentiment Analysis, Word Sense Disambiguation, Query Paraphrasing, Question Answering, Reading Comprehension, Named Entity Recognition (NER), and Translation.\nAdditionally, in order to facilitate the multi-task adaptation of our model and mitigate the problem of catastrophic forgetting, we introduce Co-COLA, an integration of COLA (Xia et al., 2024) with rehearsal training (Kirkpatrick et al., 2017). More specifically, we adopt an iterative optimization framework that merges learned low-rank matrices into the model parameters and reinitializes optimization for new LoRA modules. At each iteration, we involve retraining a subset of data from previously learned tasks and mixing it with the current task's data during training. With this periodic revisiting of earlier tasks, the model maintains performance on both old and new tasks while preserving computational efficiency.\nFarsInstruct is publicly available and open-source and we are committed to enhancing it by continually expanding our dataset with a broader range of tasks, instruction entries, and modalities. We hope this dataset fills the critical gap and serves as a valuable resource to the NLP community."}, {"title": "2 Related work", "content": "Instruction-tuning: In the landscape of AI, the capabilities of LLMs have expanded far beyond mere text processing. These sophisticated models are now being fine-tuned in a practice known as instruction-tuning, where models are trained with specific input-output pairs drawn from a wide array of data sources. This technique enables a pre-trained LLM to produce tailored outputs based on given inputs, enhancing its versatility and effectiveness. FLAN (Wei et al., 2021) and TO (Sanh et al., 2022) pioneered the exploration of instruction-tuned language models, each contributing significantly to the field. FLAN (Wei et al., 2021) adapted a 137-billion parameter pre-trained model, refining it with over 60 NLP datasets using natural language instructions. On the other hand, TO (Sanh et al., 2022) applied instruction tuning to various T5 models across 2073 prompts from 177 datasets. SuperNaturalInstruction (Wang et al., 2022) further advanced the field by assembling a comprehensive benchmark featuring 1,616 expert-written NLP tasks, covering 76 unique task types, and extending support to multiple languages. xP3 (Muennighoff et al., 2022) expanded on P3's groundwork (Sanh et al., 2022) by including content from 46 languages, adding new tasks like Translation and Program Synthesis that P3 had not tackled.\nIn a similar expansive effort, Aya (Singh et al., 2024) emerged as a significant multilingual project, featuring an impressive collection of 513 million instances across 114 languages, achieved through a collaborative research effort that involved fluent speakers from around the world to compile and complete instructional content. Our dataset distinguishes itself from these collections in its depth and adaptability, especially with the inclusion of more challenging Persian tasks, offering a high level of detail not found in many multilingual efforts. While most such projects primarily use machine translations and cover a narrow range of tasks, our dataset presents a wide array of culturally and linguistically rich tasks.\nParameter effecient fine-tuning: Conventional full-parameter fine-tuning becomes computationally impractical as the model size and the number of downstream tasks increase. To address this challenge, recent advancements in parameter-efficient fine-tuning methods suggest training only a small portion of parameters while keeping the majority of pre-trained model parameters unchanged. One of the most widely used paradigms in parameter-efficient fine turning is Low-Rank Adaptation (LoRA) (Hu et al., 2021). LoRA only modifies a small, low-rank portion of the model's weights. This is achieved by adding low-rank matrices to the model's weights during training. Despite the significant computational advantage of LoRA, it falls short in multi-task adaptation, and also Kalajdzievski (2024) showed that PEFT strategies, such as LoRA, are still susceptible to catastrophic forgetting. MultiLoRA (Wang et al., 2023) addresses the limitations of LoRA by reducing the dominance of top singular vectors, horizontally scaling LORA modules, and altering the initialization of adaptation matrices, which leads to improved performance across multiple tasks with minimal additional parameters. MixLoRA (Li et al., 2024) introduces multiple LoRA-based experts within a frozen pre-trained model using a top-k routing strategy to efficiently distribute tasks, independently configure attention layer adapters, and apply auxiliary load balance loss, significantly enhancing performance while reducing GPU memory consumption and training latency. Additionally, CoLA(Xia et al., 2024) introduces an iterative optimization framework designed to improve the fine-tuning of LLMs by employing multiple iterations of LoRA. In this paper, we design Co-CoLA to address the issue of catastrophic forgetting, while ensuring an effective multi-task adaption."}, {"title": "3 FarsInstruct Dataset", "content": "With about 130 million\u00b9 speakers, Persian \u2014 also referred to as Farsi in Iran \u2014 is an important language in the Middle East and Central Asia. FarsInstruct represents a project to provide a comprehensive public prompted dataset for the Persian community. As of this writing, FarsInstruct has more than 200 carefully designed and created prompt templates for 21 already-published public datasets and some translations from existing prompted datasets. Unlike multilingual collections focusing on common tasks such as Text Summarization and Question Answering, FarsInstruct introduces more innovative and challenging tasks, including Named Entity Recognition and Word Sense Disambiguation. The creation procedure, statistics, task augmentation, and quality of the dataset are covered in detail in the following subsections. Additional illustrations and tables are provided in the Appendix B, C."}, {"title": "3.1 Dataset Construction", "content": "The development of FarsInstruct entailed transforming Persian NLP datasets into their prompted format, described in plain language. This process involved a combination of manual ideation, during which our team meticulously brainstormed and refined prompt templates, along with invaluable insights from Persian language instructors. For datasets with multiple data fields, prompts were crafted to interrelate these fields, as elaborated in Section 3.2. Additionally, synonyms were employed to diversify the instructions within the prompts and reduce repetition. Each prompt template falls into one of two classes: categorization or generation. Categorization prompts guide the model in classifying text into predefined categories from dataset labels or identified through dataset analysis. In contrast, generation prompts require the model to produce full-length text, such as summarizing longer texts or answering questions based on the provided information. These instructions also include scenarios where the model needs to generate missing content from partial text inputs.\nTo efficiently create a large collection of prompts, we primarily utilized PromptSource (Bach et al., 2022), an open-source tool"}, {"title": "3.2 Task Augmentation", "content": "It is widely recognized that instruction-tuned models benefit significantly from extensive and varied tasks. Given this context, we focus on developing diverse prompts, spanning from basic to proficient language levels. Furthermore, drawing from the methodologies outlined in FLAN Collection (Longpre et al., 2023), TO (Sanh et al., 2022), and MetaICL (Min et al., 2022), we enhance task diversity by mixing and swapping different data fields within a given dataset. For instance, whereas a dataset might initially be structured to evaluate a model's ability to answer question x given input y, we train the model to generate question x when provided with answer y. This approach effectively broadens the spectrum of prompts within a limited data pool."}, {"title": "3.3 Data statistics", "content": "The statistics of final dataset after applying templates across all datasets is presented in Figure 3. Table 1 also presents the total number of categorization and generation prompts for each task type."}, {"title": "3.4 Quality Control", "content": "We selectively chose publicly available Persian datasets predominantly used for single-task fine-tuning, as their extensive use ensures high quality. Furthermore, to ensure the accuracy and quality of the instructions, we conduct human evaluations through consultations with the general public and experts in the field of literature. This review process allowed us to assess the instructions from multiple perspectives and incorporate cultural and linguistic nuances, critical for ensuring the prompts' clarity, accuracy, and relevance."}, {"title": "4 Methodology and Experimental Setup", "content": "To maintain our model's robustness and generalization capabilities, we integrate the CoLA framework (Xia et al., 2024) with continual learning (Kirkpatrick et al., 2017). This section offers a thorough overview of the training procedure and evaluation setup."}, {"title": "4.1 Training Procedure", "content": "Given the significant computational demands of full fine-tuning, we aim to employ LoRA for the training procedure, specifically using the FarsInstruct dataset. However, as highlighted in studies by (Wang et al., 2023; Li et al., 2024), LORA tends to underperform in multi-task training scenarios due to its limitations in capturing complex interactions between tasks, leading to suboptimal performance. To mitigate this challenge, Chain of LORA (COLA) (Xia et al., 2024), presents an iterative optimization framework based on the principles of the Frank-Wolfe algorithm (also known as the Conditional Gradient Method). This method involves an iterative process of fine-tuning on a single task, merging it with the base model, and reinitializing with a new LoRA module. Xia et al. (2024) shows that this process allows the model to learn higher-rank adaptations more effectively. Another persistent challenge affecting the performance of LoRA-tuned models is catastrophic forgetting. Kalajdzievski (2024) observed a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA.\nIn this study we propose Continual-Chain of LORA (Co-CoLA), an extension of CoLA framework which incorporates rehearsal with replay during training. More specifically, rehearsal training is an approach within the continual learning framework that involves revisiting a portion of previously learned tasks during training new tasks. Despite the limited success of continual learning frameworks, the study by (Scialom et al., 2022) demonstrated that continual training of language models, such as TO (Sanh et al., 2022) with rehearsal, can effectively help them in comprehending new instruction via instruction composition, resulting in better generalization and improved performance on new tasks.\nThe core mathematical operation in LoRA involves updating the low-rank matrices A and B, which are applied to modify the transformer layers of the model. The update rule can be expressed as \\( W' = W + BA \\) where W represents the transformer layer's original weights, and \\( W' \\) shows the updated weights after applying the low-rank adjustments A and B. Essentially, Co-CoLA structures this training procedure into an iterative three phases:\nTuning: In this phase, following the standard LORA, the base model weights remain frozen, while only the model's LoRA parameters (represented by matrices A and B) are fine-tuned. Additionally, a subset of previously trained data is replayed along with the new data. Formally, given the sequence \\( T = (T_1,...,T_n) \\) where \\( T_i \\) represents the training data after applying an individual template, the training data augmented with rehearsal is defined as:\n\\[ T'_i = T_i \\cup (\\bigcup_{j=1}^{i-1} \\frac{T_j}{r}) \\]\nwhere r is the rehearsal hyper-parameter that controls the percentage of examples sampled from previous templates \\( T_1, ..., T_n \\).\nMerging: After the tuning phase, the newly updated LoRA parameters are merged with the existing model weights. These merged weights are fixed and do not receive any gradient update in subsequent steps.\nExpanding: The final phase involves preparing the model for subsequent training rounds by reinitializing the LoRA modules with new parameters (A' and B'). Following Hu et al. (2021) A' adopts Gaussian initialization and B' is initialized to zero."}, {"title": "4.2 Evaluation Setup", "content": "Evaluation Tasks: Our model's performance was evaluated through two categories of task types: those that were part of the training dataset (\"Held in\") and those introduced to the model for the first time during evaluation (\"Held out\"). The evaluation dataset encompasses three distinct types of tasks: Sentiment analysis and Query paraphrasing, classified as \"Held in\" tasks and Textual Entailment which is categorized as a \u201cHeld out\u201d task. As illustrated in Figure 2, the evaluation includes one dataset each for sentiment analysis and paraphrase identification, alongside two datasets dedicated to entailment tasks.\nEvaluation Metric: To assess the performance of our model relative to several baseline models, we utilized the ROUGE-L metric, which measures the overlap of n-grams between the generated text and reference texts. Specifically, we concentrated on the F1-scores of ROUGE-L, a metric that integrates precision and recall to provide a balanced evaluation. As demonstrated by Wang et al. (2022), the rankings produced by this metric exhibit a strong correlation with accuracy for categorization templates."}, {"title": "5 Results", "content": "To investigate the applicability of FarsInstruct, we choose the Ava model and instruction-tune it using the Co-CoLA framework on a diverse set of templates. Our results were compared against a series of monolingual and multilingual instruction-tuned models and to effectively assess the performance of our model we conduct both quantitative and linguistic evaluations. For a comprehensive overview of the training configuration, please refer to the Appendix A."}, {"title": "5.1 Quantitative Evaluation", "content": "We evaluate our model against several existing models fine-tuned on instruction-specific data. Specifically, PersianMind (University of Tehran, 2024) is a Llama-2 7B based model, trained in 3 phases on different Persian datasets. Though their training data is unavailable, Dorna (PartAI, 2024) and Ava (Moghadam, 2024) are newly introduced models, fine-tuned on the Llama-3 8B model for Persian tasks. Aya (CohereForAI, 2024) is a 13B encoder-decoder model trained on a subset of 25 million samples from the Aya dataset and Mistral-7B (MistralAI, 2024) is a decoder-only model trained on publicly available prompted datasets\nTable 2 summarizes the comparative performance of various models, including our proposed method, Co-COLA, across several NLP Datasets: ParsiNLU Query Paraphrasing, Digikala Sentiment Analysis, FarsTail, and ParsiNLU Entailment. These models are evaluated using ROUGE-L F1 scores. As illustrated in Table 2, Co-COLA performs comparably well to the Aya model, despite having fewer parameters and being trained on less instruction data and significantly outperforms all other models, indicating the effectiveness of Co-COLA. The factors contributing to this performance gap are further discussed in Section 6. Moreover, the scores of Ava-LoRA, reflecting the performance of raw LoRA fine-tuning of Ava on FarsInstruct, are inferior to those achieved with Co-COLA training, highlighting the effectiveness of our method."}, {"title": "5.2 Linguistic Evaluation", "content": "Our comprehensive linguistic evaluation aimed to further substantiate the effectiveness of Co-CoOLA in handling the nuances of the Persian language, compared to the baseline model Ava. The evaluation specifically focused on analyzing the models' capabilities in terms of coherence, relevance, and linguistic quality, which are critical for assessing the applicability of language models in real-world scenarios."}, {"title": "5.2.1 Evaluation Setup", "content": "The evaluation involved detailed analysis by language experts who assessed the output from both models based on predefined criteria. This approach ensures an unbiased evaluation of the models' performance in generating contextually appropriate and linguistically accurate content."}, {"title": "5.2.2 Evaluation Criteria", "content": "The linguistic outputs were evaluated based on three main criteria:\nCoherence: This assesses the logical flow and connectivity of the text produced by the models.\nRelevance: This measures how well the model's output adheres to the context provided in the input.\nLinguistic Quality: This evaluates the grammatical accuracy, punctuation, and stylistic appropriateness of the text."}, {"title": "5.2.3 Evaluation Results", "content": "The evaluation results are summarized in Table 3, which provides a clear comparative analysis of the performance of the two models across all assessed criteria. The scores indicate that while Ava scored slightly higher in coherence, Co-CoLA outperformed Ava in relevance and linguistic quality, suggesting its superior ability to produce contextually accurate and linguistically refined outputs.\nThe higher scores of Co-COLA in relevance and linguistic quality demonstrate its effectiveness in producing not only grammatically correct but also contextually relevant outputs, which is essential for real-world applications. These results underscore the potential of Co-CoLA in enhancing the linguistic handling of Persian language tasks, setting a benchmark for future developments in language model applications."}, {"title": "6 Discussion", "content": "Figure 5 provides a detailed breakdown of the overall performance reported in Table 2. Each dot in the plot represents the ROUGE-L F1 score of the given model on the selected template. As clearly illustrated, other Persian instruction-tuned models fail to achieve a high ROUGE-L F1 score. One significant factor contributing to this disparity is the low precision score. The F1 score, which combines precision and recall, serves as a comprehensive metric for evaluation. Precision measures the proportion of the longest common subsequence (LCS) in the candidate text that matches the reference text, while recall measures the proportion of the LCS in the reference text that is present in the candidate text. Although these models achieve acceptable recall scores, they fall short in precision, a critical metric for categorization templates. In contrast, Aya demonstrates proficiency in handling both generation and categorization templates within the Persian context. Compared to Aya, Co-CoLA enhances the model's ability to manage both categorization and generation tasks effectively while being less computationally expensive."}, {"title": "7 Conclusion", "content": "This study introduces significant advancements with FarsInstruct and Co-COLA, addressing critical gaps in the processing and instruction-following capabilities for Persian, a low-resource language. FarsInstruct, with its diverse tasks ranging from text summarization to named entity recognition, has proven to enhance language model performance as shown through rigorous ROUGE evaluations and human assessments. This dataset not only enriches multilingual model training but also establishes a new standard for language model instruction tuning.\nFurther, Co-CoLA leverages the strengths of COLA with rehearsal training to mitigate catastrophic forgetting and improve multi-task adaptation, through its iterative optimization framework. This allows for sustained model performance over diverse tasks while optimizing computational resources. Looking ahead, the focus will be on expanding the scope of these datasets to cover more tasks and modalities, thereby driving further innovations in cross-lingual language understanding and promoting AI inclusivity."}, {"title": "8 Limitations", "content": "This section delineates the principal limitations of our study, which, while providing substantial contributions to Persian NLP, presents challenges that could be addressed in future developments to enhance its utility and applicability in broader linguistic contexts:\nData Diversity and Representation: Although FarsInstruct broadens the corpus of Persian language resources, it does not fully capture the rich tapestry of dialects and sociolects that characterize the Persian-speaking world. Also, the collected templates are generally biased towards short responses, which might affect the overall performance of the model.\nComplexity of Instructions: The dataset prompts vary in complexity but still may not sufficiently challenge or train models to handle the types of complex instructions encountered in everyday human interactions. Real-world applications often demand a high level of interpretative depth and context awareness\u2014qualities that current models may struggle with when trained on existing datasets. Future versions of FarsInstruct could benefit from integrating prompts that require higher-order cognitive processing, such as irony, metaphor understanding, and techniques that involve prompting the model to break down complex tasks into intermediate steps, mimicking human reasoning processes (Wei et al., 2022b).\nDependency on External Datasets: The effectiveness of the FarsInstruct dataset is contingent upon the quality and variety of the external datasets. This dependency creates vulnerability, as biases or errors in source datasets may be passed to FarsInstruct. A rigorous process for source data, coupled with efforts to develop original, high-quality training materials, could diminish reliance on external datasets and enhance the overall integrity of the dataset.\nEvaluation Metrics: The metrics currently used to evaluate models trained on FarsInstruct may not fully capture the nuanced and multifaceted aspects of language comprehension and generation. Furthermore, for certain tasks such as rewriting, ROUGE-L may not serve as an adequate measure of quality.\nPerformance Stability: While Co-CoLA has demonstrated effectiveness in terms of computational efficiency and consistent performance across all tasks it learned, mitigating catastrophic forgetting, we observe that its overall performance is heavily dependent on the model's performance at each tuning iteration. We leave potential solutions to this problem to future work."}]}