{"title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design", "authors": ["Nathaniel H. Park", "Tiffany J. Callahan", "James L. Hedrick", "Tim Erdmann", "Sara Capponi"], "abstract": "Molecular property prediction and generative design via deep learning models has been the subject of intense research given its potential to accelerate development of new, high-performance materials. More recently, these workflows have been significantly augmented with the advent of large language models (LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models to make predictions in the context of more complex research tasks. While effective, there is still room for substantial improvement within the agentic systems on the retrieval of salient information for material design tasks. Moreover, alternative uses of predictive deep learning models, such as leveraging their latent representations to facilitate cross-modal retrieval augmented generation within agentic systems to enable task-specific materials design, has remained unexplored. Herein, we demonstrate that large, pre-trained chemistry foundation models can serve as a basis for enabling semantic chemistry information retrieval for both small-molecules, complex polymeric materials, and reactions. Additionally, we show the use of chemistry foundation models in conjunction with image models such as OpenCLIP facilitate unprecedented queries and information retrieval across multiple characterization data domains. Finally, we demonstrate the integration of these systems within multi-agent systems to facilitate structure and topological-based natural language queries and information retrieval for complex research tasks.", "sections": [{"title": "Introduction", "content": "The advent of accessible large language models (LLMs) and their respective capabilities has enabled a dramatic expansion of LLM-based workflows across the domains of chemistry and materials. These include applications of chatbots or multi-agent workflows to metal-organic frameworks, 1,2 inorganic materials, automated synthesis, organic synthesis, and protein discovery. Besides the underlying language models of the agents and their respective interactions to produce a final output, the critical components of these workflows are the tools made available to individual agents. This can include to access to retrosynthetic software, predictive models or computational tools, or writing task-specific code for automated experimentation all of which provide salient information needed by an LLM or LLM-powered agent to complete a given task. All of these are closely related to and frequently used in combination with standard retrieval augmented generation (RAG) wherein a natural-language query is run against a corpus of chunked text documents within a vector database to retrieve the most relevant documents and provide them to the LLM as part of its prompt.7-9 As with other types of agent tools, this approach is effective in enhancing LLM performance for answering a particular question, it does require significant optimization of the choice of embedding model, chunk size, LLM choice and its available context window, question rephrasing, use of metadata filtering, and similarity metrics used for chunk retrieval.8,9,9\nIn many of the agent-based workflows noted above, the tools used may utilize structural information in the form of a SMILES string or molecular formula to perform a computation or property prediction,6,10,11 but not directly in RAG operations focused on structural similarity. Retrieval of relevant information based on structural similarity, whether focused small-molecules, materials, or reactions, is one of the most critical tasks during any research endeavor. Therefore, enabling researchers to use natural-language accompanied by chemical language to query structure-linked information resources would provide a powerful augmentation of LLM capabilities. Achieving structure-based RAG operations necessitates a vector-based representation of a given compound or material whereby similarity queries may be conducted. While there are a significant variety of molecular fingerprints available through cheminformatics packages such as RDKit, 12 the use chemistry language foundation models is highly attractive due to their potential dual use within an agentic system as an embedding model and a predictive tool. Despite these advantages, there exist only a few reports on the utilization of deep learning models as embedding models for similarity-based searches\u2014limited largely to organic molecules and inorganic materials. 13,14 And while the LLM systems have been demonstrated to search APIs such as PubChem or the general internet in the performance of chemistry-focused tasks, these search interfaces do not offer the potential breadth of both query options or structural similarity features that may be possible when using a chemistry foundation model to embed structural information and with relevant metadata. Additionally, queries based on other modalities, such as images of characterization data, have largely not been evaluated within an LLM-based agentic system nor within traditional chemistry database interfaces in spite of the immense benefit such features would confer to researchers. To overcome these limitations and dramatically expand the capabilities of multi-agent systems for materials design and development tasks, we surmised that: 1) a single, high-performance chemistry foundation model could be adapted to facilitate semantic structure searches across small-molecules, polymers, and reactions; 2) such a model could be used in combination with an image embedding model to enable multimodal semantic queries; and 3) these systems can be integrated within multi-agent systems to provide richer context while performing materials design tasks.\nBefore building a complete semantic structure search-based RAG pipeline within multi- agent workflows, it was imperative to evaluate the effectiveness of both the latent representations generated by the chemistry language foundation models for structural similarity queries. There exist many potentially competent embedding models to support semantic structural queries. Prior work focused on the use of ChemBERTa models to enable similarity searches for functional analogues of drugs,14\u201316 however there are other potential models that could work including Mol- BERT, 17 Mol2Vec,18 GPT-MolBERTa, 19 or ChemGPT.20 For our investigation, we selected MoLFormer as a baseline embedding model which has recently been shown to be highly performant across numerous benchmarks from MoleculeNet,21 effective at capturing molecular similarity, as well as potentially learning 3D spatial relationships from SMILES inputs.22 To evaluate structural similarity queries, we compiled a focused dataset of ~2.5M organic small- molecules using open-source data and historical data from our own work.23-26 The SMILES representations were canonicalized and vector embeddings were computed for each compound using MoLFormer before insertion into a Milvus vector database (see Supporting Information for more details).27 Using this dataset we evaluated the ability of using MoLFomer embeddings to facilitate retrieval of structurally similar compounds from sample queries on known organocatalysts for ring-opening polymerization (ROP) using the Milvus vector search capabilities (Fig. 1).28,29\nFor the result of each query, several metrics relating to the structural similarity based on either MoLFormer embedding distance (cosine, Euclidean, Fig.1) or fingerprints for each compound generated using RDKit (Tanimoto, RDKit, MACC, and Dice, Fig 1). Each score represents the similarity between the result compound and the query compound. For 1,8- diazabicyclo[5.4.0]undec-7-ene (DBU) 1a, the top result was itself followed by several closely related structural analogues based on the cosine or Euclidean similarity (Fig. 1A). Interestingly, the second rank compound 1b was given much lower similarity scores from most fingerprint derived metrics despite differing from 1a only in the saturation of the imine (Fig. 1A). For 2a, a organocatalyst for ROP designed with the aid of generative AI,29 we find that while the original query was not present in the dataset, most of the core functional group features\u2014an cyclic 5- membered guanidine and an aromatic ring-based on visual inspection (Fig. 2B). As with 1a, most of the fingerprint-based similarity metrics indicate the result compounds as highly dissimilar, with exception of MACC (Fig. 2B). Overall, these examples further substantiate prior results22 demonstrating MoLFormer embeddings do indeed capture relevant structural information and similarity despite differing substantially in several cases from fingerprint-based metrics.\nWith the success of the similarity queries using MoLFormer embeddings in conjunction with a Milvus vector store, we sought next to expand search capabilities beyond simple small- molecule queries in order to maximize the diversity of search options available to LLM agents. In natural language, differences in word vector embeddings have been demonstrated to capture lexical relationships between different words, with classic examples of  \ud835\udc3e\ud835\udc56\ud835\udc5b\ud835\udc54 \u2212 \ud835\udc40\ud835\udc4e\ud835\udc5b + \ud835\udc4a\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc5b = \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc52\ud835\udc5b or \ud835\udc43\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc60 \u2212 \ud835\udc39\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 + \ud835\udc43\ud835\udc5c\ud835\udc59\ud835\udc4e\ud835\udc5b\ud835\udc51 = \ud835\udc4a\ud835\udc4e\ud835\udc5f\ud835\udc60\ud835\udc4e\ud835\udc64 capturing the relationships of gender and capital city.30 Given that MoLFormer is a chemistry language model based on SMILES syntax, we surmised differences between vector embeddings between two compounds within the base model latent space should correspond to differences in structure (Fig. 1B). Moreover, this implies that combinations of vector embeddings from compounds of interest can be used to identify novel analogues based on functional group features of both compounds as well as utilize scalar property values to manipulate the magnitude of the vector-facilitating unique similarity queries to access different sets of data. To implement these ideas, we first created an identical collection of ~2.5M molecules used in Fig. 1 where instead each vector was now scaled by the compounds' corresponding molecular weight. This was tested on guanidine 3a, which was used as a search input to either the molecular weight scaled collection, or the base collection used previously (Fig. 2A). The results in Fig. 2A demonstrated the success of both approaches in retrieving structurally relevant compounds, however, the metadata filtering is limited to identical top results within certain molecular weight ranges (3f, Fig. 2A)\u2014potentially requiring different filtering strategies depending on the desired output.\nIn addition to utilizing the vector magnitude to influence similarity search results, we also investigated whether addition and subtraction of vectors corresponding to different functional groups resulted in similar lexical relationships as observed with examples like: \ud835\udc3e\ud835\udc56\ud835\udc5b\ud835\udc54 \u2212 \ud835\udc40\ud835\udc4e\ud835\udc5b + \ud835\udc4a\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc5b = \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc52\ud835\udc5b. This approach was evaluated on two catalysts 4a and 4b where their corresponding vector embeddings were subjected such operations. With 4a the subtraction of the vector corresponding to dimethylurea and the addition of dimethylthiourea resulted in a vector embedding that provided corresponding thioureas (4c) when queried against the database (Fig. 2B). With 4b the results of similar operation were less obviously successful, but this is anticipated behavior based on a limited collection of molecules (Fig. 2B). Nonetheless, the results did provide similar examples with fluorine containing amines. Finally, the vector embeddings of 4a and 4b were averaged and queried against the database with the top results bearing the structural features of both the parent molecules (Fig. 2C).\nHaving demonstrated the effectiveness of using MoLFormer vector embeddings on a variety of different similarity search strategies, we next sought to evaluate their feasibility towards polymers and reactions. MoLFormer was trained compounds with SMILES strings containing less than 200 tokens were filtered from the pre-training dataset, precluding large SMILES strings which may potentially represent macromolecules.22 Outside of biological materials, such as peptides, nucleic acids or large natural products, macromolecules and other complex materials are often exhibit stochastic features relating to their structure and are hence poorly represented by discrete, one-dimensional SMILES strings. This problem is frequently circumvented in deep learning models through a reduction in the complexity by treating polymers as single, discrete repeating units via SMILES strings with variable attachment points denoted by the asterisk character.31,32 While this approach tends to overlook the stochastic nature of materials as well as neglecting to account for end groups or more complex polymer topologies, it does produce systems capable of providing predicted values for polymer properties within these restrictions.31,32 As the tokenizer for MoLFormer covers the entire SMILES grammar,2,22,33 including the use of special tokens such as variable attachment points, we hypothesized that it could also serve as a suitable source of embeddings to facilitate queries based on both polymer structural and topological similarity. It was unclear, however, as to whether these latent embeddings will capture the same level of molecular similarity given SMILES fragments with asterisks were unlikely to have been part of any pre-training set for the model. With this in mind, we leveraging dot-separated SMILES strings containing asterisks to represent repeat units or other polymer components to evaluate their suitability for use in semantic queries for polymeric materials.\nTo evaluate the use of MoLFormer embeddings applied to polymers, we first curated a set of polymer data from both open literature and our own historical data totaling in ~2.5M SMILES strings representing predominantly homopolymers.34\u201339 As with the small-molecule embeddings, these were inserted into a Milvus collection, against which similarity queries may be run. First, we tested the embeddings on very simple queries such as polyimide 5a and polystyrene 5b, both of which returned sets of highly similar compounds (Fig. 3A). Averaging the embeddings of 5a and 5b enabled the search to identify features of both, consistent with results observed on the small- molecules and again indicative of the ability of MoLFormer embeddings to capture semantic structural relationships. Next, we investigated the ability to query for two similar block copolymers 6a and 6b, identical in their monomer and end-group components, but differing only in block order (Fig. 3B). The spatial distance-both cosine and Euclidean of the embedding vectors generated for each SMILES string is small, but suggestive that the model does indeed differentiate between the two. As a consequence, queries for both 6a and 6b give the identicals first ranked result, 6c (Fig. 3B). It is only at the 5th ranked polymer where differences are enough to provide distinct results (6d and 6e, Fig. 3B).\nAs with polymers, reactions\u2014represented using reaction SMILES syntax40\u2014has not been examined directly using MoLFormer and despite the existence of many transformer-based models for reactions, 33,41 we were interested in probing the versatility of MoLFormer embedding model reaction similarity queries. Based on our results with polymeric systems, we anticipated reactions to behave in a similar manner given a similar syntactical construction. The data for the evaluation was sourced from an open ~2M reaction dataset sourced from the USPTO used previously in transformer models for reactions, open publications, and historical experimental polymerization reaction data.42 In addition to querying on whole reactions, we also focused on queries involving one to two reagents (Fig. 4A). In these examples, 4a, 4b, and their average vector where each used to search the database and provided reaction examples where structurally related reagents were used or the query molecules themselves (Fig. 4A). Finally, testing the order of 4a and 4b, either in a dot separated substrate series or on either side of angle brackets (>>) separating substrates and products produced markedly different results, indicating the importance of order within the reaction SMILES sequence. The difference in results when angle brackets are used is understandable considering that this would indicate two very different reactions when the order is reversed. However, unlike with polymers, the distinction between the ordering of reactants or products in reaction SMILES has less relevance unless some order of addition is being encoded. In total, these results indicate that the base model of MoLFormer is well-suited to capture structural relationships among small-molecules, polymers, and reactions.\nThe demonstration of MoLFormer as an effective embedding model beyond simple small molecules prompted us to examine use of chemical structural embeddings with additional modalities. Association of chemical structure with different types of characterization data is a highly important task for any project within chemistry and materials domains. While characterization data is available in many formats, we opted to leverage the data pre-plotted and saved as images as input for creation of their corresponding vector embeddings. The ability to query available characterization data by either image or structure would be a powerful addition to both traditional data infrastructures and RAG pipelines as well. To implement this task would typically require either some form contrastive learning or latent fusion, necessitating fine-tuning of an existing model or training of an additional model. Instead, we opted to test alternative strategies where the chemical components of a particular piece of characterization data would be embedded using MolFormer while the image components would be embedded using OpenCLIP image model (see Supplementary Information for details).43,44 While CLIP models for images can also co-embed text captions, it is not anticipated these embeddings would be able to effectively capture nuances in chemical structure as a chemistry focused model like MoLFormer. Instead, the text embedder of the OpenCLIP model can be used to embed information captured as a natural language caption, which can be automatically generated from the metadata from the characterization data files, adding additional natural language query capabilities. To evaluate this approach, we compiled a small dataset of labeled images of 'H, 13C, and 19F NMR spectra (see Supplementary Information). The labels for the spectra included their corresponding chemical components as SMILES strings and natural language captions generated from the spectra metadata. From this dataset we can test both image-based and structure-based queries on the embedded dataset with excellent results across numerous collection organization strategies (Fig. 5). Despite there being only minor differences between the image features plots\u2014colors, whitespace, axes, and image size of the embedded plots, OpenCLIP can effectively differentiate between different characterization techniques without additional metadata filtering on the query. This implies that the differentiation is likely occurring due to shape of the plotted traces or spectra (Fig. 5).\nThe use of chemistry LLMs such as MoLFormer and image models like OpenCLIP, in combination with post-embedding pooling and compression strategies, has facilitated the creation of a variety of vector stores which in turn can support a large variety of similarity queries. This alone would make a powerful addition to traditional database architectures and search capabilities, yet it's the connection of such vector databases within a larger RAG framework coupled to LLM-powered agent workflows which can offer the potential of significant time savings in complex research tasks requiring the merging and summarization of data retrieved from complex structure, image, and natural language-based queries. In this regard, access to different vector stores and their respective embedding strategies is provided to LLM agents as tools which may be used in the context of a particular task. We utilized the LangGraph library (v0.1.19)45 to develop a hierarchical multi-agent adaptive self-reflective RAG system (Fig. 6), which takes a question as input and outputs, the solution as a formatted research-style report. As shown in Fig. 6, the hierarchical workflow is directed by a supervisor agent which leverages query analysis46 to adaptively route a user's question to the correct worker agent. The system consists of four worker agents, which specialize in small molecules, polymers, chemical reactions, or NMR spectra. Each of these workers autonomously implement a multi-agent-based self-reflective RAG workflow based off of.47 As detailed in Fig. 6, the workflow consists of several steps including retrieving and evaluating documents, generating responses using retrieved documents, revising user input to improve retrieved documents, checking generated responses for hallucinations and verifying each response completely addresses a user's question. Each step of this process is autonomously directed by the worker agents, which runs until all checks are passed. Once all checks are passed, the answer is sent to the report generation tool, which summarizes the agent's findings (Figs 7-8). This report also summarizes the vector store retrieved content used by the agent to generate the report in order to improve the overall transparency of the workflow.\nThis workflow was demonstrated on a variety of tasks, each of which was able to effectively utilize the structure and/or image-based vector store tools in combination with other natural language-based vector databases from historical manuscripts. Example reports generated from these tasks are shown in Fig. 7 and Fig. 8 below. For Fig. 7, the agentic system was provided the following query:\n\"I am interested in aromatic polyethers similar in structure and function to O=S(C1=CC=C(O[*:1])C=C1)(C2=CC=C([*:2])C=C2)=O. Please find analogues and comment on potential means of synthesis and thermal properties.\u201d\nAs shown in Fig. 7, the agent was able to successfully complete the user query using the self-reflective RAG agent for polymers returning two relevant, similar analogs and provided additional content to justify the selection of these analogs stating:\n\u201cThese structures feature sulfur-oxygen bonds and aromatic rings, similar to the original compound. For synthesis, aromatic polyethers can typically be synthesized through nucleophilic substitution reactions or using organocatalysts for polymerization. The thermal properties of these compounds can vary based on their specific structures and substituents, but generally, aromatic polyethers exhibit good thermal stability due to the strength of the aromatic bonds.\u201d\nFor Fig. 8, the agentic system was asked to identify similar 13C NMR spectra to the one of a diethanolamine-based carbonate monomer precursor.49 As Fig. 6, the question was routed to the multi-modal NMR agent who reviewed and summarized the characterization data of four 13C NMR spectra images (i.e., three retrieved images and the image corresponding to the input) summarizing the location of the peaks in these images that similar to the input NMR spectra. The report also provides a visualization of the input image, which makes enables easier review and interpretation of the provided results. Both reports were reviewed by a domain expert confirmed who confirmed the validity of the findings. In this example, the agent was not instructed to filter the RAG results if they contained the original query, yet it still successfully found several spectra of closely related diethanolamine-based cyclic carbonates or their precursors."}, {"title": "Conclusion", "content": "The accelerated development of novel materials and catalysts necessitates dramatic improvement of human-AI interactions to facilitate both effective co-designs as well as realistic implementations within an experimental setting. LLM-based multiagent systems integrated with chat interfaces hold significant promise to become useful assistants for researchers operating in laboratory settings. Here, we have demonstrated that chemical foundation models coupled with powerful image recognition models can facilitate unique types of multimodal structural or architectural focused queries on small-molecules, polymers, and reactions. This represents a significant enhancement of search capabilities not typically found in traditional database systems for chemistry research. Moreover, the coupling of these systems within a LLM-based multiagent system can provide a significant advantage for reducing the time needed to retrieve and summarize relevant multimodal structure-based information commonly required across all material research projects."}, {"title": "Code and Data Availability", "content": "Data and code will be made available upon final publication."}, {"title": "Conflict of Interest", "content": "A patent application on aspects of multimodal semantic search for materials has been filed by IBM (application number 18/798970, inventors: N.H.P, T.E., and J.L.H). All authors are employees of IBM."}, {"title": "Supplementary Information", "content": "Materials and Methods\nVector Database Setup. All SMILES embeddings were computed using the ibm/MoLFormer-XL- both-10pct model available through HuggingFace using canonicalized SMILES (with the exception of the USPTO dataset, which was used as is).50 For the example queries in Figures 1-4, the vector embeddings for each compound were L2 normalized prior to insertion into a Milvus lite vector database using and HNSW index and the L2 distance as the metric. For the multimodal examples (Fig. 5) SMILES embeddings were not L2 normalized and used in a Milvus standalone vector database using IVF_FLAT index and the L2 distance metric. For spectra containing more than one identifiable compound, including the NMR solvent, their corresponding embeddings were averaged prior to insertion. Image embeddings were computed using OpenCLIP via the Langchain library. The model used was ViT-g-14 with the laion2b_s34b_b88k checkpoint. Image embeddings were stored in a separate Milvus collection, using an IVF_FLAT index and L2 distance metric, and cross-referenced with their corresponding compound embeddings.\nSimilarity Metrics. For small molecule and polymer example queries, similarity metrics were computed on the basis of both the MoLFormer embeddings and molecular fingerprints computed using RDKit. Cosine similarity was computed by subtracting the cosine distance between the query and result embeddings (measured using the SciPy library) from 1. Euclidean similarity (Es) was computed by the following equation:\n$E_s = 1 / (1 + E_d)$ (1)\nWhere Ed is the Euclidean distance between the query and result embeddings. Tanimoto and Dice similarities were computed using the corresponding query and result Morgan fingerprints with a radius of 2 and a dimension of 2048. RDKit similarity was computed using the built-in RDKit fingerprints of the query and compound. MACCS similarity was computed using the MACCS Keys fingerprints of the query and the compound using the RDKit package.\nLanguage Agent Network. As noted in the main text, the multi-agent framework was assembled using the Langgraph library. The supervisor agent utilized GPT-40 mini model while other agents used either llava 7b or llama3.1 8b models. All Milvus collections were instantiated as separate vector stores with customized embedding functions with either MoLFormer or OpenCLIP, prior to connection with LLM agents as retrievers. Full code for agent network will be released in both subsequent drafts of preprint and final publication."}]}