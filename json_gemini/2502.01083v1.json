{"title": "Tool Unlearning for Tool-Augmented LLMS", "authors": ["Jiali Cheng", "Hadi Amiri"], "abstract": "Tool-augmented large language models (LLMs) are often trained on datasets\nof query-response pairs, which embed the ability to use tools or APIs directly\ninto the parametric knowledge of LLMs. Tool-augmented LLMs need the ability\nto forget learned tools due to security vulnerabilities, privacy regulations, or\ntool deprecations. However, \u201ctool unlearning\" has not been investigated in\nunlearning literature. We introduce this novel task, which requires addressing\ndistinct challenges compared to traditional unlearning: knowledge removal\nrather than forgetting individual samples, the high cost of optimizing LLMs, and\nthe need for principled evaluation metrics. To bridge these gaps, we propose\nTOOLDELETE, the first approach for unlearning tools from tool-augmented LLMs.\nIt implements three key properties to address the above challenges for effective\ntool unlearning and introduces a new membership inference attack (MIA) model\nfor effective evaluation. Extensive experiments on multiple tool learning datasets\nand tool-augmented LLMs show that TOOLDELETE effectively unlearns randomly\nselected tools, while preserving the LLM's knowledge on non-deleted tools and\nmaintaining performance on general tasks.", "sections": [{"title": "1 Introduction", "content": "Tool-augmented Large Language Models (LLMs) can use external tools such as calculators (Schick\net al., 2023), Python interpretors (Gao et al., 2023), APIs (Tang et al., 2023), or AI models (Patil\net al., 2023) to complement the parametric knowledge of vanilla LLMs and enable them to solve\nmore complex tasks (Schick et al., 2023; Patil et al., 2023). They are often trained on query-response\npairs, which embed the ability to use tools directly into parameters.\nDespite the growing adoption of tool-augmented LLMs, the ability to selectively unlearn tools has\nnot been investigated. In real-world applications, tool unlearning is essential for addressing critical\nconcerns such as security, privacy, and model reliability. For example, consider a tool-augmented\nLLM deployed in a healthcare system and trained to use APIs for handling patient data. If one of\nthe APIs is later flagged as insecure due to a vulnerability that could expose sensitive information\nand violate regulations like HIPAA, tool unlearning is necessary to ensure that the LLM can no\nlonger invoke the insecure API. Similarly, when tools undergo major updates, such as the Python\ntransformers package moving from version 3 to version 4, tool unlearning becomes essential to\nprevent the LLM from generating outdated or erroneous code. The goal of this work is to address this\ngap by investigating tool unlearning and providing a solution for this overlooked yet essential task.\nWe introduce and formalize the new task of Tool Unlearning, which aims to remove the ability of\nusing specific tools from a tool-augmented LLM while preserving its ability to use other tools and\nperform general tasks of LLMs such as coherent text generation. Ideally, an effective tool unlearning\nmodel should behave as if it had never learned the tools marked for unlearning. Tool unlearning\nfundamentally differs from traditional sample-level unlearning as it focuses on removing \u201cskills\"\nor the ability to use specific tools, rather than removing individual data samples from a model. In\naddition, success in tool unlearning should be measured by the model's ability to forget or retain\ntool-related skills, which differs from traditional metrics such as measuring likelihood of extracting\ntraining data in sample-level unlearning. These differences are discussed in detail in \u00a72.\nRemoving skills requires modifying the parameters of LLMs, a process that is computationally\nexpensive and can lead to unforeseen behaviors (Cohen et al., 2024; Gu et al., 2024). In addition,\nexisting membership inference attack (MIA) techniques, a common evaluation method in machine\nunlearning to determine whether specific data samples were part of training data, are inadequate for\nevaluating tool unlearning, as they focus on sample-level data rather than tool-based knowledge.\nTo address these challenges, we propose TOOLDELETE, the first tool unlearning algorithm for tool-\naugmented LLMs, which satisfies three key properties for effective tool unlearning: tool knowledge\nremoval, which focuses on removing any knowledge gained on tools marked for unlearning; tool\nknowledge retention, which focuses on preserving the knowledge gained on other remaining tools; and\ngeneral capability retention, which maintains LLM's general capability on a range of general tasks\nsuch as text and code generation using ideas from task arithmetic (Ilharco et al., 2023; Barbulescu\n& Triantafillou, 2024). In addition, we develop LiRA-Tool, an adaptation of the Likelihood Ratio\nAttack (LiRA) (Carlini et al., 2022) to tool unlearning, to assess whether tool-related knowledge has\nbeen successfully unlearned. Our contributions are:\n\u2022 introducing and conceptualizing tool unlearning for tool-augmented LLMS,\n\u2022 TOOLDELETE, which implements three key properties for effective tool unlearning;\n\u2022 LiRA-Tool, which is the first membership inference attack (MIA) for tool unlearning.\nExtensive experiments on multiple datasets and tool-augmented LLMs show that TOOLDELETE\noutperforms existing general and LLM-specific unlearning algorithms by + in accuracy on forget\ntools and retain tools. In addition, it can save 74.8% of training time compared to retraining, handle\nsequential unlearning requests, and retain 95+% performance in low resource settings."}, {"title": "2 Tool Unlearning: Preliminaries", "content": "To understand tool unlearning, we first introduce the concept of \"tool learning,\" see Figure 1(a).\nLet D = {T, Q, Y} be a dataset with N tools T, and (Q, Y) denotes query-output examples that\ndemonstrate how to use the tools in T. Each tool t\u1d62 \u2208 T may have one or more demonstrations\n{Qi, Vi}, |Qi| = |Vi| \u2265 1. Starting with an instruction-tuned LLM fo, a tool learning algorithm\nexplicitly trains fo on D and results in a tool-augmented model f capable of using the N tools in T.\nWe note that prior to explicit tool learning, the LLM fo may already have some tool-using capabilities\nsuch as performing basic arithmetic operations.\nProblem Definition: Tool unlearning aims to remove specific tools from tool-augmented LLMs.\nLet Df = {Tf, Qf, Vf} denotes k < N tools and their corresponding demonstrations to be unlearned"}, {"title": "3 TOOLDELETE", "content": "We develop ToOLDELETE-an effective tool unlearning approach that removes the capability of using\ntools marked for unlearning (Tf) or solving tasks that depend on them, while preserving the ability\nof using the remaining tools (Tr) and performing general tasks such as text and code generation.\nTOOLDELETE implements three key properties for effective tool unlearning:"}, {"title": "3.1 Tool Knowledge Deletion", "content": "Unlearning requires completely removing the knowledge of Tf that f gained during tool learning, ide-\nally as if Tf had never been part of the training set. In other words, knowledge about Tf is successfully\nremoved if the unlearned model f' has no more knowledge than the tool-free model fo about Tf.\nDefinition 3.1 (Tool Knowledge Deletion (TKD)). Let t\u1d62 \u2208 Tf denote a tool to be unlearned and g\nbe a function that quantifies the amount of knowledge a model has about a tool. The unlearned model\nf' satisfies tool knowledge deletion if:\n$\\mathbb{E}_{t_i \\in T_f} [g(f_0, t_i) - g(f', t_i)] \\geq 0.$\nThis formulation allows users to control the extent of knowledge removal from f'. For instance, when\nwe unlearn a \"malicious\u201d tool that calls a malignant program, we may require f' retains no knowledge\nof this tool, i.e. g(f',ti) = 0. In less critical cases, users can choose to reset f''s knowledge to\npre-tool augmentation level, i.e. g(f',ti) = g(fo, ti)\nTo measure tool knowledge in LLMs, we follow previous works that used prompting to probe LLMs'\nknowledge (Brown et al., 2020; Singhal et al., 2023), i.e. adopting the output of LLMs as their\nknowledge on a given tool. For each t\u1d62 \u2208 Tf and its associated demonstrations {Qi, Vi}, we query\nthe tool-free LLM fo with Qi and collect its responses V = fo(Qi). Since fo has never seen ti or\n{Qi, Vi}, V represents the tool-free response. We then constrain the unlearned model f' to generate\nresponses similar to Y to prevent it from retaining knowledge of ti."}, {"title": "3.2 Tool Knowledge Retention", "content": "The unlearning process should preserve model's knowledge of tools in Tr. Ideally, all knowledge\ngained on Tr during tool learning should be retained after unlearning.\nDefinition 3.2 (Tool Knowledge Retention (TKR)). Let tm \u2208 Tr denote a retained tool, and let g\nbe a function that quantifies the amount of knowledge a model has about a tool. The unlearned model\nf' satisfies tool knowledge retention if:\n$\\mathbb{E}_{t_m \\in T_r} [g(f, t_m) - g(f', t_m)] = \\epsilon,$\nwhere e is an infinitesimal constant, so that f' retains the same knowledge of tools in T as the\noriginal model f.\nFor effective tool knowledge retention, f' is further fine-tuned using demonstrations associated with\nTr, or, more practically, a subset of T proportional to Tf for efficiency."}, {"title": "3.3 General Capability Retention via Task Arithmetic", "content": "Optimizing the above objectives can lead to effective unlearning, but it may not be sufficient to\nmaintain the general capabilities of the unlearned model f'. As a foundation model, f' is expected to\nretain abilities such as text and code generation, question answering, instruction-following, and basic\nmathematical reasoning. These capabilities either existed in fo prior to tool augmentation or do not\ndepend on specific tools. Therefore, preserving the general capabilities of f' is essential to guarantee\nthat tool unlearning does not compromise the overall functionality of the model.\nDefinition 3.3 (General Capability Retention (GCR)). Let TG denote the general tasks used to\nevaluate LLMs. The unlearned model f' satisfies general capability retention if it preserves the\nknowledge on TG that it originally obtained prior to tool learning:\n$\\mathbb{E}_{t_g \\in T_G} [g(f_0, t_g) - g(f', t_g)] = \\epsilon,$\nwhere e is an infinitesimal constant.\nWe propose to use task arithmetic (Ilharco et al., 2023; Barbulescu & Triantafillou, 2024) as an\nefficient and effective approach to preserving the general capabilities of the unlearned model. Our\nobjective is that f' retains as much general knowledge as fo, the instruction tuned LLM trained from\na randomly initialized model fr. Let \u03b8o and \u03b8R denote the parameters of fo and fr respectively.\nThe difference vector \u03b8o \u2212 \u03b8R captures the direction of general knowledge acquisition. We apply this\nadjustment to \u03b8' (the parameters of f') to preserve its general knowledge:\n$\\theta'^* \\leftarrow \\theta' + (\\theta_0 - \\theta_R).$"}, {"title": "3.4 Training Details", "content": "To obtain the unlearned model f', we solve:\n$\\theta'^* = \\arg \\min_{ \\theta, Knowledge deletion of T_f \\And Knowledge retention of T_r } \\mathbb{E}_{t_i \\in T_f} [g(f_0, t_i) - g(f', t_i)] + \\mathbb{E}_{t_m \\in T_r} [g(f, t_m) - g(f', t_m)],$\nand once the optimized model parameters \u03b8'* are obtained, we apply task arithmetic to reinforce\ngeneral capabilities:\n$\\theta'^* = \\theta'^* + \\alpha (\\theta_0 - \\theta_R),$\nwhere \u03b1 is a hyperparameter to control the magnitude of task arithmetic. The above formulation\nprovides flexibility in training TOOLDELETE using various existing paradigms, including supervised\nfine-tuning (SFT) (Taori et al., 2023), direct preference optimization (DPO) (Rafailov et al., 2023),\nreinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), parameter-efficient\nfine-tuning (PEFT) (He et al., 2022; Su et al., 2023), or quantization (Dettmers et al., 2022; Ma et al.,\n2024) techniques. Below we describe two variants of TOOLDELETE:\n\u2022 TOOLDELETE-SFT fine-tunes f using language modeling loss. On forget tools Tf, we\nreplace the original responses Vf with tool-free responses V'. The samples for Tr are not\nmodified.\n\u2022 TOOLDELETE-DPO uses direct preference optimization (DPO) to prioritize wining\nresponses over losing responses. For (ti, Qi, Vi) \u2208 Tf to be unlearned, we prioritize the\ncorresponding tool-free response V over the original response Vi. For (tj, Qj, Vj) \u2208 Tr,\nthe original response Yj is prioritized over the tool-free response V."}, {"title": "3.5 LiRA-Tool for Tool Unlearning Evaluation", "content": "Challenge A key challenge in evaluating tool unlearning is the lack of membership inference attack\n(MIA) models to determine whether a tool has been truly unlearned. Existing MIA models typically\nevaluate individual training samples by analyzing model loss, which is insufficient for tool unlearning.\nUnlike sample-level unlearning, tool unlearning focuses on removing abstract parametric knowledge\nof tools in Tf, not just forgetting specific training samples. The key limitation of sample-based MIA\nis that the prompt-response pairs (Qf, Vf) in the training set may not fully represent all aspects of\na tool's functionality. As a result, sample-level MIA may \u201coverfit", "shadow samples\", a diverse set of prompt-\nresponse pairs to probe various aspects of tool knowledge. We prompt GPT4 with different combina-\ntions of in-context examples to obtain a comprehensive set of prompt-response pairs with various\nprompt format, intention, and difficulty requirements. These samples will be used to stress-test the\nunlearned LLM f' beyond the specific training prompts. This approach prevents overfitting to the\noriginal training data and provides a more reliable evaluation of whether the tool has truly been\nforgotten. To implement this, we extend Likelihood Ratio Attack (LiRA) (Carlini et al., 2022), the\nstate-of-the-art MIA approach, to tool unlearning.\"\n    },\n    {\n      \"title\": \"4 Experimental Setup\",\n      \"content\": \"Datasets & Tool-Augmented LLMs We experiment with the following datasets and their corre-\nsponding LLMs:\n\u2022 ToolAlpaca (Tang et al., 2023) is an agent-generated tool learning dataset consisting of\n495 tools and 3975 training examples. ToolAlpaca 7B is fine-tuned on ToolAlpaca using\nVicuna-v1.3 (Zheng et al., 2023).\n\u2022 ToolBench (Qin et al., 2024) consists of more than 16k real world APIs from 49 categories,\nwhere each training demonstration involves complex task solving traces. ToolLLaMA is\nfine-tuned on ToolBench using LLaMA-2 7B (Touvron et al., 2023b).\n\u2022 API-Bench (Patil et al., 2023) focus on APIs that load machine learning models. Gorilla is\nfine-tuned on API-Bench from LLaMA 7B (Touvron et al., 2023a).\"\n    },\n    {\n      \"title\": \"5 Results\",\n      \"content\": \"Comparison to general unlearning methods Our main results in Table 1 show that, compared to\nRETRAIN, the best-performing baseline in the general unlearning methods category, TOOLDELETE-\nSFT outperforms Retrain by 0.6, 0.3, 8.0, 2.3 absolute points on TT, Tr, Tf, TG respectively.\nTOOLDELETE-DPO outperforms RETRAIN by 1.3, 3.3, 9.8, 1.8 absolute points across the same\nmetrics. We note that GRADASCENT can effectively unlearn Tf, but it negatively impacts its TT and\nTr performance. Although RANDLABEL and SALUN outperforms GRADASCENT, they still fall\nshort on TG compared to TOOLDELETE.\nComparison to LLM-specific unlearning methods Existing LLM unlearning methods, despite\neffective in sample-level unlearning, are prone to under-performing in tool unlearning. Both TOOLD-\nELETE-SFT and TOOLDELETE-DPO outperforms ICUL, SGA, and TAU on TT, Tr, Tf and TG. The\nonly exception is ICUL, which outperforms ToOOLDELETE-SFT on T by 2.7 absolute points, but is\noutperformed by TOOLDELETE-DPO on Tr by 0.3 points. The good performance of ICUL on Tr is\nat the cost of failing to unlearn tools in Tf, which is not desired in tool unlearning. In addition, ICUL\nhas limited ability of preserving test set performance, it is outperformed by TOOLDELETE-SFT and\nTOOLDELETE-DPO by 3.6 and 4.3 respectively. Furthremore, it is particularly limited in deletion\ncapacity, i.e. number of unlearning samples that a method can handle. As |Df| exceeds 10, the\nperformance of ICUL on TT significantly degrades. This is while TOOLDELETE can process much\nlarger deletion requests efficiently.\nSFT vs. DPO DPO outperforms SFT by 0.7, 3.0, and 1.8 on TT, Tr, Tf respectively. On TG, SFT\nis slightly better than DPO by 0.5 points. However, DPO takes slightly longer time to train, see\nFigure 4. Both optimization methods achieve superior performance over existing approaches.\nMeasuring tool unlearning with MIA Following prior practices (Carlini et al., 2022; Pawelczyk\net al., 2024), a lower TPR indicates an unlearned model with better privacy when FPR=0.01. TOOLD-\nELETE-DPO achieves 0.14 TPR, outperforming RETRAIN by 0.01. This advantage is obtained by\nexplicitly prioritizing tool-free responses fo (2) over original responses. In addition, TOOLDELETE-\nSFT achieves comparable performance with RETRAIN, which indicates its effectiveness to protect\nprivacy. Both variants of our method outperforms GRADASCENT and ICUL, the best perform-\ning baselines, achieving 0.21 and 0.18 TPR. This indicates that existing sample-level unlearning\napproaches are not sufficient for unlearning tools, see Figure 2.\nSequential unlearning Tool unlearning requests may arrive in sequential mini-batches. We experi-\nment with sequential unlearning requests by incrementally unlearning 2%, 5%, 10%, and 20% of\ntools. RETRAIN, ICUL by design cannot process sequential deletion requests. TOOLDELETE can\ncontinue training according to the current deletion request, without having to retrain a new model.\nWhen 20% of unlearning requests arrive in batches, TOOLDELETE can sequentially unlearn each of\nthem. As Figure 3 and Table 1 show, compared to unlearning 20% at once, the performance does not\ndegrade significantly.\"\n    },\n    {\n      \"title\": \"All properties contribute to effective tool unlearning\",\n      \"content\": \"Ablation studies in Table 2 show that\nwithout Tool Knowledge Removal, performance of TOOLDELETE-SFT and TOOLDELETE-DPO\non Tf degrade by -34.8 and -37.2 absolute points respectively. Such significant performance drop\nis observed for other model properties as well. Therefore, we conclude all proposed properties are\nnecessary for successful at tool unlearning on TT, Tr, Tf, and TG.\nTOOLDELETE functions effectively without access to training data In certain unlearning\nsettings, access to the original training data might be restricted, e.g., in healthcare settings or in cases\nwhere training data is no longer available due to compliance. In these cases, TOOLDELETE can\ngenerate pseudo-samples for tools using the \u201cshadow samples\" technique developed for LiRA-Tool,\nsee \u00a73.5. Table 4 in Appendix D shows that TOOLDELETE can perform tool unlearning effectively,\nachieving comparable performances to when full access to the exact training data is available.\nTOOLDELETE is efficient Efficiency is a critical aspect for unlearning. As Figure 4 in Appendix D\nillustrates, ToOLDELETE is substantially more efficient than retraining a new model from scratch-\nsaving about 74.8% of training time on average. In addition, this efficiency gain is relatively consistent\nas the size of Tf increases. ToOLDELETE-SFT is slightly faster than TOOLDELETE-DPO, as the\nlatter requires a negative sample for each of its prompts.\nTOOLDELETE-LoRA is ultra-efficient with good unlearning performance We experiment if\nTOOLDELETE can achieve effective tool unlearning through LoRA (Hu et al., 2022), when computing\nresource is limited. Experiments on ToolAlpaca show that TOOLDELETE-LORA can achieve 97.7%,\n99.6%, 84.5%, and 84.3% of the performance of TOOLDELETE with full parameter on T\u0442, Tr,\nTf, TG on average across SFT and DPO, see Table 3 in Appendix D. In addition, it reduces save\ncomputational cost by 81.1% and decreases the training time by 71.3%.\nTOOLDELETE is flexible in choice of tool-free responses In (1), we obtain tool knowledge-free\nresponses from the tool-free LLM fo. However, in cases where fo is unavailable, TOOLDELETE can\nstill function using any knowledge-free LLM to generate tool knowledge-free responses, such as a ran-\ndomly initialized LLM fr. Table 5 compares the performances between these two implementations.\nWhile \u03b8o consistently outperforms OR, using OR is still effective in achieving tool unlearning.\"\n    },\n    {\n      \"title\"": "Why is TOOLDELETE effectiveness?"}, {"content": "We attribute the performance of TOOLDELETE to its\nthree key properties: (a): Tool Knowledge Removal enables targeted tool unlearning without over-\nforgetting, unlike GRADASCENT and RETRAIN. This is achieved by prioritizing tool knowledge-free\nresponses over tool knowledge-intense responses so that the model forgets tool functionality without\nexcessive degradation. (b): Tool Knowledge Retention reinforces the knowledge about remaining\ntools. In fact, re-exposing the model to the original training data can further strengthen their\nrepresentation. (c): General Capability Retention, which maintains or even improves model's general\ncapabilities through an efficient and effective task arithmetic operation. Therefore, precise unlearning,\nretention of relevant knowledge, and overall model stability are the key factors that contribute to the\nperformance of TOOLDELETE."}, {"title": "6 Related work", "content": "Unlearning for non-LLM models: These methods include methods that focus on pruning before\nunlearning (Jia et al., 2023) or finding salient parameters (Fan et al., 2024b) and manipulating gra-\ndients Ullah et al. (2021); Hoang et al. (2024), adversarial methods (Liu et al., 2023; Setlur et al.,\n2022; Wei et al., 2023), approximation of inverse Hessian (Zhang et al., 2024a), and data augmenta-\ntion (Choi et al., 2024). Other works study unlearning under multimodal setting (Cheng & Amiri,\n2025), image-to-image models (Li et al., 2024a), and finding the most challenging unlearning subset\nwithin a dataset (Fan et al., 2024a). Recently, a few works started to benchmark MU performances\non unlearning fictitious user profiles (Maini et al., 2024), world knowledge (Jin et al., 2024) and a\nvariety of tasks (Cheng & Amiri, 2024).\nUnlearning for LLMs: Recently, more attention has been given to LLM unlearning, where gradient\nascent is a common technique (Eldan & Russinovich, 2023; Jang et al., 2023). (Yao et al., 2024)\nevaluate several traditional unlearning methods on LLMs. KGA (Wang et al., 2023) formulates"}, {"title": "7 Conclusion", "content": "We introduce Tool Unlearning-a novel machine unlearning task with the goal of unlearning previously\nlearned tools from tool-augmented LLMs. We develop the first tool unlearning approach, TOOLD-\nELETE, that implements three key properties: tool knowledge deletion, tool knowledge retention,\ngeneral capability retention. In addition, we introduce LiRA-Tool, the first membership inference\nattack (MIA) method for evaluating tool unlearning. LiRA-Tool largely addresses the limitations of\nsample-based MIAs for tool unlearning. Extensive experiments on several diverse datasets and LLMs\nshow that TOOLDELETE is an efficient, flexible, and effective tool unlearning method that supports\nsequential unlearning, maintains strong performance across all key properties, and operates without\nrequiring full access to training data. It outperforms existing methods by removing tool knowledge\nwithout over-forgetting (as shown in ablation studies), achieving 74.8% faster training times compared\nto retraining, and delivering highly effective tool unlearning even in resource-constrained settings\nwith TOOLDELETE-LORA (which reduces compute costs by 81.1% and training time by 71.3%).\nIn future, we will investigate tool unlearning in dynamically updated LLMs (e.g. API-based LLMs\nlike GPT-4), where we address continuous unlearning challenges. In addition, we will develop\nadversarial training techniques and robustness evaluation frameworks to prevent unintended tool\nre-learning or model exploitation.\nLimitations We did not conduct experiments using closed-source LLMs or API-based LLMs.\nIn addition, this work did not investigate the impact of varying model scales due to the limited\npublicly-available tool-augmented LLMs. Our experiments were conducted on the 7B scale and the\nscalability of the proposed tool unlearning approach across models of different sizes and scales is an\nopen question for future investigation."}, {"title": "Impact Statement", "content": "Our work investigates the security implications of tool-augmented Large Language Models (LLMs),\nwhere we focus on the risks that arise from integrating external tools, and the necessity ability to\nremove these acquired tools. A key concern is ensuring compliance with privacy regulations, such as\nthe Right to be Forgotten (RTBF), which mandates the removal of specific data upon user request.\nIn the context of tool-augmented LLMs, this necessitates the ability to delete sensitive, regulated,\nor outdated information related to specific tools. By examining how LLMs interact with and rely\non external tools, potential threats to model security can be identified, e.g. unauthorized tool usage,\nadversarial exploitation, and privacy violations. Our research highlights the critical importance of\naddressing these challenges."}, {"title": "A Practical Use Cases of Tool Unlearning", "content": "We provide several examples in which tool unlearning is essential:\nCase 1: De-memorize Privacy-Concerned Tools Imagine a tool-augmented LLM that is deployed\nin a healthcare system and trained to use APIs for handling and processing patient data, such as\naccessing medical records or generating anonymized reports. Suppose one of the APIs that was\ninitially compliant is later flagged as insecure due to a vulnerability that could expose patient data.\nThis violates regulations like HIPAA or GDPR. In this case, ToolDelete is essential as it can update\nthe tool-augmented LLM's parameters to unlearn how to invoke the insecure API. This removes\nany capability embedded in the LLM's parametric knowledge and prevents adversarial or accidental\nusage of the vulnerable API.\nCase 2: Forget Harmful / Biased Tools Consider a tool-augmented LLM that can use a Safe For\nWork diffusion model as a tool to generate images based on user instructions. If the user prompts can\nfool the model to generate Not Safe For Work (NSFW), harmful, or biased images, this tool should\nbe unlearned from the LLM. Note that even if we augment the LLM with a new and safe version\nof the diffusion model without unlearning the previous version, the LLM would still be able to call\nthe previous version, which can lead to generating Not Safe For Work, harmful, or biased images.\nTherefore, we should explicitly erase the ability of using the previous version of the diffusion model\nfrom the LLM.\nCase 3: Unlearn Deprecated Tools Tool unlearning is also essential when a tool has a major\nupdate, where the function names and input parameters have changed, e.g. the major update of the\nPython transformers package from v2 to v4. Without unlearning v2, the tool-augmented LLM may\ngenerate erroneous code and bring difficulty for debugging, since many functions have been renamed\nand removed. Therefore, as the underlying tools evolve, the tool-augmented LLM should be updated\nthrough unlearning of the previous versions and augmenting the new ones."}, {"title": "B Baselines", "content": "As there are no prior works on tool unlearning, we adapt the following unlearning methods to tool\nunlearning setting. Four general unlearning approaches.\n\u2022 GRADASCENT (Golatkar et al., 2020; Yao et al., 2024) runs gradient ascent on Tf with the\nassociated query-reponse samples (Qf, Vf).\n\u2022 RANDLABEL (Graves et al., 2021) fine-tunes on Tr and Tf with corrupted labels.\n\u2022 SALUN (Fan et al., 2024b) performs RANDLABEL on unlearning-related parameters dis-\ncovered by saliency map.\n\u2022 ICUL (Pawelczyk et al., 2024) uses Tf with corrupted label as in-context demonstrations.\n\u2022 SGA (Jang et al., 2023; Barbulescu & Triantafillou, 2024), which performs gradient ascent\non Tf whose memorization probability exceeds a pre-defined threshold.\n\u2022 TAU (Barbulescu & Triantafillou, 2024), which performs task arithmetic on SGA.\n\u2022 CUT (Li et al., 2024b).\n\u2022 NPO (Zhang et al., 2024b) uses DPO with only a losing response (i.e. no winning response).\n\u2022 SOUL-GradDiff (Jia et al., 2024) uses second-order information in optimization. It adapts\nthe Sophia optimizer (Liu et al., 2024) for LLM unlearning. We adopt the SOUL + Grad-\nDiff (Maini et al., 2024) implementation in the original paper."}, {"title": "C Implementation details", "content": "We use a learning rate of 10-5 across all experiments. All experiments are conducted on 8 NVIDIA\nA100 GPUs."}, {"title": "E Sampling of Shadow Samples for LiRA-Tool", "content": "We use the following prompt to prompt GPT-4 to synthesize diverse shadow samples for evaluation\nwith LiRA-Tool."}]}