{"title": "DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship", "authors": ["YAQIONG LI", "PENG ZHANG", "HANSU GU", "TUN LU", "SIYUAN QIAO", "YUBO SHU", "YIYANG SHAO", "NING GU"], "abstract": "Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people's diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod's multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, social media sites have been popular mediums for self-disclosure. For example, hundreds of millions of people utilize Twitter [22], Facebook [45, 46, 50], and Weibo [66] to record life events, express personal thoughts and opinions, and interact with friends every day. The openness of social media provides a spacious environment for content sharing while resulting in the disclosure of toxic content (toxicity), defined as \"a rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion\" [1], including hate speech [24], harassment [8, 22], insults and abuse [5], and offensive language [14], etc. Since the severe problem of context collapse [38], social media users are usually unaware of the disclosure of toxic content. For example, the prior studies [31, 36] found that about two-thirds of toxic content was implicit toxicity in online communities and the corresponding users were usually unaware of the content and the harm to others. Research revealed that 23.00% of users regret when they re-examine their shared content due to several reasons [58], such as lack of the consequence consideration of posts, culture misjudgment, unintended audience, misunderstanding of platform norms.\nTo avoid toxic content disclosure, social media users generally conduct content censorship before publishing a post. The censorship procedure can be implemented by users themselves or by leveraging some automated tools. For example, several studies have found that individuals usually censored their content by checking, adjusting, or even deleting part of the content to make the content suitable to be published on social media [62]. Although there have been various censorship approaches, most of them focus on toxic content detection, e.g., toxicity score evaluation with Perspective"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Content Censorship", "content": "In the social media context, users generally conduct content censorship (also called \"last-minute self-censorship\" [12]) by themselves or employing automated tools. A study indicated that individuals generally manually censored their content before sharing by checking, adjusting, or deleting some words to ensure the content's consistency with platform norms and cultures [2, 11]. However, this self-censorship process relies heavily on users' knowledge, experience, and time, affecting censorship efficiency and quality. So, studies have emerged focusing on building automated tools that help users facilitate content censorship. For example, users can use some third-party tools [1] to identify toxic content from their posts, including hate speech [24], harassment [6, 8, 22, 42], insults and abuse [5], etc. Although there have been diverse censorship tools, most of them focus on detection without considering users' composite censorship demands like result understanding and content modification. These problems result in the low efficiency of users' content censorship.\nSo many users choose to publish content without censorship but rely on platforms' moderation measures.\nDifferent from censorship, content moderation is initialized by a social media platform [15], with the aim of monitoring whether content submitted to the platform complies with the platform's rules and guidelines [2, 11]. So content moderation generally occurs right after content publishing and is also called post-moderation [40]. Although content moderation can also identify toxic content, social media users are generally passive in this procedure and toxic content has resulted in some impacts when being moderated [57]. Many previous studies have explored automated [25, 27, 53] and human moderation approaches [9, 26, 35, 48] and their moderation targets are similar to that of content censorship. For example, [54] designed a Chrome extension program that automatically generates content warnings by utilizing keyword identification and online intervention interface principles, aiming to identify sensitive information in contents. [21] presented a word filter to detect some harmful comments, like harassment [6, 8, 22, 42] and targeted abuse [5]. Moreover, [63] developed an open-sourced visualization tool to identify toxic content and predict the toxicity score of keywords, helping human moderators improve moderation efficiency and accuracy.\nCompared to platform post-moderation, content censorship is a user-driven and pre-check process. It has several benefits, including instant feedback, autonomous control over contents [59], and proactive checking avoiding potential social impact [20]. Like \"we [the HCI & security communities] have used user effort as a first resort, not last\" [18], the instant feedback of censorship by users allows them to promptly gauge potential issues with their content, providing an opportunity for adjustments or edits. Although the personalized censorship on post's privacy publicity is studied [28], there still lacks the relevant work of user autonomy over their personal expression under platform moderation"}, {"title": "2.2 Toxicity Detection with/for LLMs", "content": "There have been extensive works on exploring methods for detecting toxic content [1, 63], including hate speech [24], harassment [8, 22], insults and abuse [5], offensive language [14], etc. These methods are proposed with various models and algorithms, such as feature-based classifiers [39], neural network architectures (CNN, LSTM, etc.) [52], and pre-trained language models (BERT [19], RoBERTa [30], etc.). Previous studies have also explored other strategies for accuracy improvement in toxicity detection, including constructing high-quality corpus [14], designing detailed classification principles [32], etc. For example, a model named HateBERT is re-trained based on BERT for abusive language detection [55]. It utilizes a training corpus derived from Reddit, which involves offensive, abusive, and hateful content. Besides the above research and practice, a comprehensive framework has been proposed for toxicity detection [51], which incorporates contextual knowledge such as semantics, intent, and sentiment. However, the framework remains theoretical and necessitates further deliberation and analysis in both information collection and validation.\nRecently, the emergence of Large Language Models (LLMs) has brought impressive effects in promoting NLP research and applications, especially ChatGPT [43]. There are some efforts in toxicity detection regarding LLMs [37, 41, 65], including toxicity detection for LLMs and toxicity detection employing LLMs. The former focuses on LLMs' outputs, aiming to avoid the appearance of toxic content in LLMs' generations. The latter employs LLMs as a tool to detect toxic content by taking advantage of LLMs' strong comprehension and reasoning abilities, which can avoid the tedious procedure of feature engineering and model training. For example, Llama2 [56] has been used to detect online sexual predatory chats and abusive texts with fine-tuning techniques [41], wherein traditional processing such as feature extraction (semantics, intent, or sentiment) is no longer needed. Another work explored ChatGPT's performance in detecting toxic comments on GitHub and designed various prompts to justify model outputs [37]. A novel prompt design approach [65], named Decision-Tree-of-Thought, was also proposed to guide LLMs in enhancing the quality of toxicity detection."}, {"title": "2.3 Our Work in Context", "content": "Toxicity detection is just an initial task in toxic content censorship and users can have other diverse demands in the process. Therefore, our work aims to build a holistic automated toxicity censorship tool with the benefit of LLMs, addressing social media users' diverse problems and expectations in posting. We first investigate the problems users encountered in toxicity censorship and their expectations for handling them. Based on that, we propose a novel multi-functional censorship tool based on ChatGPT. The tool is equipped with multiple features like toxicity detection, result explanation, and content modification. To the best of our knowledge, this is the first tool that supports users' demands in multiple stages of toxic content censorship."}, {"title": "3 NEEDFINDING STUDY", "content": "We began with a needfinding study to understand the current toxicity censorship practices of social media users, including how to conduct toxic content censorship, the problems encountered, the corresponding expectations, etc."}, {"title": "3.1 Method", "content": "Our needfinding study was conducted on a popular Chinese social media site - Weibo [16], and both questionnaires and semi-structured interviews were adopted. We chose Weibo as our research platform for several reasons. First, Weibo has a large user base with 600 million monthly active users [60], and its post content involves diverse topics, including personal life, hot events, entertainment, etc. Second, a large amount of toxic content is generated and disseminated on Weibo, and the categories of toxicity are diverse [61], including harassment, offensive language, insult and abuse, etc., which are similar to those of other popular social media platforms like Twitter and Instagram. For example, from November 2022 to August 2023, the number of offensive expressions identified on Weibo exceeded 120 million [61]. Thus, Weibo has been a common-used Chinese research platform for toxicity studies [14, 23, 64].\nFor our needfinding study, we initially used questionnaires to investigate the current practices of toxicity censorship among Weibo users and identify the problems they encountered. Subsequently, semi-structured interviews were employed to find users' desired design features for toxicity censorship. The findings can guide us to design a human-centered tool to improve users' toxicity censorship practices.\nParticipants. We released the questionnaire on an online survey platform and posted it to social media. A total of 493 participants (214 males and 279 females, aged between 15 and 58) finished the questionnaire. Most (439, 89.05%) of these participants are aged between 18 and 35 years old, and people of this age range are the primary users of social media. Among these participants, 30 persons (18 males and 12 females, aged between 18 and 35) expressed their willingness to participate in the subsequent semi-structured interviews.\nProcedure. The aim of the questionnaire is to investigate the current practices of toxic content censorship among Weibo users and the problems they encountered. It comprises three parts with a total of 11 questions (5-point Likert scale): 1) a user's basic profiles, such as the demographic features (gender, age, etc.), the habit of Weibo use, and post frequency; 2) current censorship practices, including whether usually conducting toxicity censorship on Weibo and how to censor; 3) problems encountered during content censorship. To further understand users' expectations of toxic content censorship, we designed a draft framework and invited 30 participants to conduct participatory design through semi-structured interviews with offline or online meetings. Participants expressed their expectations for the tool's design and outputs, including the detection granularity (binary or multi-classification), object granularity (post, sentence, or word), etc. According to participants' feedback, we iteratively adjusted our design. With the participants' agreement, the interviews were recorded and then transcribed by automatic tools and the first author. All of the data would not be shared to avoid privacy leakage.\nReferring to common procedure [3, 34], we took an analysis of participants' feedback and interview logs, using statistical analysis and Thematic Analysis methods [33]. Initially, three authors performed open coding on all participants' feedback independently and then worked together to build a series of axial codes. Following this, the authors reviewed the interview logs, iteratively refining the coding scheme across three rounds to address shortcomings in the previous round. The final stage involved focused coding, aimed at synthesizing evolving conceptual categories into more comprehensive topics related to censorship experiences like detection or display requirements. Throughout the whole process, three authors kept communication regularly with other authors, ensuring conceptual coherence and reliability. The coding process was deemed complete upon reaching a consensus among the authors on the conclusions."}, {"title": "3.2 Results", "content": "Most users tend to prevent posting toxicity through two approaches: self-censorship and platform moder-ation. According to the results of our questionnaire, 355 participants (71.60%) use Weibo. 227 participants (63.94%) publish posts on Weibo, wherein 11.27% publish daily, 20.85% weekly, and 30.7% monthly, indicating their high activity levels on the platform. Most users choose to conduct toxicity censorship when posting, and only few (21, 9.25%) mentioned never. This phenomenon demonstrates people's strong awareness of content censorship on Weibo. Among the 156 participants who provided the answer of censorship ways generally used, 112 participants (72.44%) selected self-censorship (censoring posts by oneself), 91 (58.33%) selected relying on Weibo's platform moderation (identifying and removing the posts that violate platform norms [9, 47]), and 30 (19.23%) selected inviting others to censor (seeking advice from parents, friends, or other individuals).\nProblems of current censorship approaches. Among the 112 participants choosing self-censorship, only 11 (8.53%) thought it could meet their needs, and the problems can be summarized as the lack of censorship accuracy and objectivity due to the users' limited knowledge, experience, and time. 15 participants acknowledged this phenomenon, saying \"It is always influenced by my subjective understanding\", \"I don't know if something is nontoxic sometimes\", \"Maybe my knowledge is not enough to determine which word violates the platform norms\", etc. These responses suggest self-censorship heavily relies on users' knowledge, experience, and time, and users themselves cannot perform accurately and objectively. For the platform moderation on Weibo [66], only 4 participants (4.60%) believed this approach could meet their censorship needs, and the main problems can be summarized as the lack of user control (moderation occurs after posts have been posted and users cannot take some proactive actions), lack of explanation (why posts are toxic), and low accuracy.\nDesign Goals. We further analyzed participants' expectations for the design of toxicity censorship tools, based on which the following five design goals are proposed.\n\u2022 G1: Provide holistic censorship. Toxicity censorship tools should provide holistic functions, including toxicity detection and modification. All participants confirmed the necessity of such a tool, saying \"This tool can greatly unload my brain. I am often not aware my words may hurt others\", \"I just post and wish a holistic tool that can point out my issues and offer revision suggestions\", etc. It can not only alleviate users' censorship burden but also improve the accuracy and objectivity.\n\u2022 G2: Offer fine-grained detection results. Toxicity censorship tools should provide fine-grained detection results. Not only a classification result (whether a post contains toxicity) but also fine-grained results (the related sentences, phrases, and words) should be given in toxicity censorship to promote user perception. Participants mentioned, \"It's not enough to tell me whether my post is toxic or not. The specific words or phrases that might harm others should be identified\", \"The keywords should be highlighted directly. I don't want to waste my time, it's just a post\", etc.\n\u2022 G3: Strengthen interpretability. Participants wish for an immediate explanation about detection results, saying \"For the toxic content that I may not realize, it's better to offer some reasons to let me know whether I should post the content\", \"Highlighted words would be clear and intuitive\", etc. Moreover, 26 participants (86.87%) expressed a desire to get to know audiences' views on the posts proactively, helping them understand why some posts cannot be published. For example, P5 responded, \"There are usually people who are not satisfied with my words, and I don't like to be preached by others either. This feature [getting to know the potential social impacts of the post content] is quite interesting, as it allows me to know whether my words or expression has any issues during conversations\".\n\u2022 G4: Give personalized revising suggestions. To alleviate users' modification burden, the toxic content censorship tool should give revising suggestions that can make the post content normal while remaining the original semantics and a user's personalized language style in the meanwhile. 28 participants (93.33%) expressed the thought to reduce their modification burden. During modification, semantics and personalized language style should be reserved as much as possible after revision. For example, participants said, \"If there are some inappropriate sentences or words, it would be better to replace with some subtly expressions automatically and I don't want to edit directly\" and \"I value my usual speaking style. If the revision is too formal, there is no need to appear on my social media\".\n\u2022 G5: Ensure user-control. In order to ensure users do not feel overly censored, content censorship should be conducted with user-awareness and user-control. What role a censorship tool plays is to give suggestions and actions like whether to revise depending on users' decisions. Participants expressed, \"The function of automatic modification should give some suggestions, not publishing directly. I prefer to revise on my own\", \"I prefer to use different functions at any time. Sometimes, detection is enough\", etc."}, {"title": "4 DEMOD: A HOLISTIC TOOL FOR TOXICITY CENSORSHIP", "content": "According to the design goals outlined in the previous section, we designed a tool named DeMod to help social media users conduct toxic content censorship proactively. Based on ChatGPT [43], DeMod is designed to provide multiple functionalities, including explainable detection and personalized modification. To demonstrate how to deploy DeMod in practice, we also implemented DeMod as a third-party tool by setting a famous social media site - Weibo as a research site. The following gives the details of DeMod's design and implementation."}, {"title": "4.1 DeMod", "content": "According to our design goals, DeMod, presented in Figure 1, is built with three main modules.\n\u2022 User Authorization: This module is utilized to get a user's permission for Weibo profile access, such as the user's historical posts and social connections.\n\u2022 Explainable Detection: This module conducts toxicity detection and explaining based on ChatGPT. Firstly, it provides multi-granularity detection results, including classification (Y/N representing if a post is or isn't toxic content, respectively) and corresponding keywords. Secondly, it gives detailed explanations, including immediate and dynamic explanations for the user. The former directly explains why certain keywords are toxic, and the latter predicts the audiences' attitudes to the post to help the user perceive the potential effects.\n\u2022 Personalized Modification: This module provides the user with revising suggestions to avoid toxicity posting while attempting to retain the original semantics and the user's personalized language style in the meanwhile."}, {"title": "4.1.1 User Authorization", "content": "DeMod is a third-party tool helping people conduct content censorship on social media. Since there are diverse personal profiles in the social media context, we first introduce the user authorization module into DeMod to avoid privacy leakage. It can be implemented by calling the OAuth API supplied by social media platforms. During authorization, a user can see what kinds of profiles (public historical posts and social connections) DeMod will use and give the consent. These profiles enable DeMod to provide more precise and personalized censorship results. The user can revoke authorization to avoid personal information misuse."}, {"title": "4.1.2 Explainable Detection", "content": "Our empirical study indicates that users wish for fine-grained detection results and interpretability in toxicity censorship. Therefore, we design DeMod with a ChatGPT-based toxicity detector to provide multi-granularity detection results and corresponding explanations. It firstly outputs the toxicity detection results, including classification (Y/N, whether a post contains toxicity) and the corresponding keywords. The classification result informs users whether a post contains toxic content. If so, keywords triggering toxicity will be highlighted to enhance users' perception of toxic details. For these detection results, the detector then gives immediate and dynamic explanations. Immediate explanation illustrates why the post and keywords are toxic. For the dynamic explanation, the detector can predict audiences' attitudes or opinions to the post by taking advantage of ChatGPT's capability in character simulation, helping users get to know the potential social impact of the content. This design aligns with the theory of basic human values, emphasizing that individual behavior is easily influenced by the values held by others [17, 49], including others' attitudes, personal information, etc. To ensure user control, DeMod allows users to manually select some audiences, like parents, friends, and even strangers, and conduct attitude simulation. Once an audience is selected, DeMod simulates her/his attitudes to the post.\nThe above detection and explanation tasks are all achieved based on ChatGPT, and the prompts are shown in Appendix Figure 6. Both prompts contain four key elements: task description, prompt template, system setting, and output format constraints. For the first prompt in Figure 6(a), the \"Task\" is \"Toxicity detection\", the \"Prompt template\" incorporates the input sentence to be detected and the relevant topic, and the \"System\" describes what ChatGPT should do and the corresponding requirements, including task requirements and output format. The output format is set as JSON to ensure it can be easily parsed. For the dynamic explanation task, we collect the interaction context between the current user and the selected audience (post comments the current user obtained from the selected audience on Weibo) with user consent and aggregate it as a corpus to support ChatGPT conducting attitude simulation, revealing the audience's preferences, opinions, and thoughts. Similar to the above, Figure 6(b) exhibits the prompt of the dynamic explanation task. The \"Task\" is \"Viewpoint simulation\", and the interaction context is embedded between \"The start of the interaction context between the user and the selected role\" and \"The end of the interaction context between the user"}, {"title": "4.1.3 Personalized Modification", "content": "Our empirical study suggests that users wish modification suggestions to help them detoxify posts while the semantics of the original post and a user's personalized language style should be retained as much as possible. We find directly using a prompt similar to the detection prompts to let ChatGPT conduct the toxic content modification task challenging since the multiple goals cannot be achieved simultaneously. However, ChatGPT is capable of learning from a few examples, i.e., few-shot learning [7]. If there are some pairs of examples exhibiting the original posts (a post with toxic content) and the corresponding revised contents (the corresponding post without toxic content but with similar semantics and the user's language style), ChatGPT can achieve these modification goals better. So, we first attempt to construct these pairs of examples as follows.\n\u2022 Step 1: For the current user, several pairs of examples $(NT_i, T_i)$ are constructed based on her/his historical posts on Weibo, wherein $NT_i$ represents a nontoxic historical post, $T_i$ represents the corresponding toxic post constructed by us, and $i$ indicates the ith pair $(i \\in \\{1, 2, \\cdot\\cdot\\cdot \\})$. Both $NT_i$ and $T_i$ are characterized by the similar semantic and language style.\n\u2022 Step 2: Transform each pair into $(T_i, NT_i)$, and construct a prompt by using several pairs of these examples to stimulate ChatGPT to achieve the multiple modification goals.\n\u2022 Step 3: Verify the revised post to ensure the modification's effect.\nFor a user, Step 1 can be executed in advance and then persisted to support Step 2 and Step 3. After that, when a user requires post-modifying in practice, only Step 2 and Step 3 are needed. The details of these three steps are described below.\nIn Step 1, we construct a post $T_i$ for each post $NT_i$ according to the word substitution strategy utilized in [63]. As is shown in Figure 2, the process is as follows:\n(1) Construct toxic word space. Firstly, to ensure the toxic word space aligns with the features of Weibo posts, we crawled a large number of Weibo posts as a training corpus, including 4,832 users and 968,503 posts (each post is denoted as $D_i, i \\in \\{1, 2,... \\}$). Then we used the RoBERTa model [30] fine-tuned by the Chinese offensive dataset COLD [14] to detect toxic posts from this corpus. If a post $D_i$ is judged as toxic, calculate the contribution value $V_{ij}$ of each $token_j$ to the result, as shown in the formula 1 (j represents the jth token in $D_i$). The contribution value $V_{ij}$ is then normalized to [-1,1]. If $V_{ij} > 0$, the jth token $token_j$ of post $D_i$ is added to the toxic word space $S_t$. Iterate this process to traverse $D_i$.\n$V_{ij} = contribution(RoBERTa(D_i) = 1).$    (1)\n(2) Find nontoxic posts and corresponding contribution words from the current user's historical posts. Given the current user's historical posts (each post is denoted as $H_i, i \\in \\{1, 2,... \\}$), we utilize both ChatGPT and fine-tuned ROBERTa to identify the nontoxic posts. ChatGPT conducts toxicity detection first. If ChatGPT judges a post as nontoxic, RoBERTa will be introduced to double-check the result and further evaluate the contribution value $V_{ij}$ for each $token_j$ of $H_i$, as shown in the formula 2. The reason for using both ChatGPT and RoBERTa is to ensure the selected posts are nontoxic."}, {"title": "", "content": "$V_{ij} = contribution(RoBERTa(H_i) = 0, if ChatGPT(H_i) = 0).$    (2)\n(3) Construct nontoxic-toxic pairs for the current user. For each of the obtained nontoxic posts, we conduct word vector mapping for each token and get the token's vector $[F_1, F_2, ..., F_n]$ and then find the closest vector $[\\hat{F_1}, \\hat{F_2},..., \\hat{F_n}]$ from the toxic word space $S_t$ based on Euclidean distance, where n denotes the vector dimension. The relevant loss function is presented in the following equation 3. The contribution words of nontoxic posts are then substituted with the corresponding nearest toxic tokens to make nontoxic posts become toxic, based on which we can obtain several nontoxic-toxic pairs of $(NT_i, T_i)$.\n$[\\hat{F_1}, \\hat{F_2},\u2026, \\hat{F_n}] = argmin_{[F_1,F_2,...,F_n] \\in S_t} \\sum_{i=1}^{n}(F_i - \\hat{F_i})^2.$    (3)\nIn Step 2, We flip each nontoxic-toxic pair $(NT_i, T_i)$ to $(T_i, NT_i)$ and embed them between \"The start of nontoxic-toxic samples\" and \"The end of nontoxic-toxic samples\" in the prompt shown in Appendix Figure 7. The prompt has a similar format to our toxicity detection prompts. The \"Task\" is \"Expression modification\", and the \"System\" setting gives the task requirements, the rules without posting history (if there are no historical posts of the current user), the constraint of expression style, output format, etc. For those users without history posts, $(T_i, NT_i)$ is none. So, we add the basic examples in the \"System\" setting. Using this prompt to interact with ChatGPT, the modification content will be generated.\nIn Step 3, a verification is conducted to confirm that toxic content has been effectively eliminated. We re-detect the modified content through the explainable detection module. If a post is toxic, DeMod will modify it again. This procedure serves to guarantee the modification's effectiveness, offering a more reliable censorship result.\nAbove all, the features of DeMod can be summarized as follows regarding the design goals described in Section 3.2. Holistic censorship (G1). DeMod provides not only toxicity detection but also result explanation and personalized modification for users' toxicity censorship. To the best of our knowledge, this is the first tool that can cover the multiple stages of toxicity censorship.\nFine-grained detection results (G2). When conducting toxicity detection, DeMod presents users with the classification result and corresponding keywords, enhancing users' perception of toxicity.\nImmediate and dynamic explanations (G3). The immediate explanation component within the toxicity detection module provides users with clear justifications and criteria for DeMod's decision. By taking advantage of the dynamic explanation based on viewpoint simulation, users can gain insights into different audiences' attitudes. It can empower users to actively perceive the potential consequences and impacts of their posts, assisting them in making more reasonable decisions.\nPersonalized modification (G4). By learning from our constructed corpus, DeMod can help users detoxify posts while preserving the original semantics and personalized language style in the meanwhile. It aligns with users' expectations for multi-objective modification.\nUser-control (G5). During the usage, no operations are conducted without user choices to ensure DeMod is user-driven. Even some modules like the personalized modification component can give some suggestions, the decisions will eventually be made according to user choices."}, {"title": "4.2 System Implementation", "content": "We implement DeMod as a web application through Flask and Vue, using Redis to store data on Ali cloud server . The following gives the tool's usage in detail.\nA user can login into DeMod using her/his Weibo ID or \"@nickname\". Then there is an authorization statement, informing the user about the information it will access and use. When she/he agrees, DeMod presents the interface as depicted in Figure 3(c). The initial interface incorporates a text box, function buttons, and usage instructions. The background of the text box gives a description of the input format, including topic writing (use two \"#\" signs to label the topic) and text length (the text should be at least five words without the topic). Users can input the text in the box and click the \"Start\" button for detection.\nWe give an example of toxicity censorship, as shown in Figure 4(a). If the input is \"#FanBullying# Some fans of celebrities bully female artists. I didn't know before, but now I do. The fans are really repulsive\" (This text is for demonstration only, and it is not an actual post), the detection results are displayed on the top left picture. The Y/N result indicates \"This sentence contains toxic content\" and keywords are highlighted: \"bully\" and \"repulsive\". The immediate explanation is \"This statement contains derogatory and insulting remarks towards a specific group (fans), employing negative words"}, {"title": "5 EVALUATION", "content": ""}, {"title": "5.1 Settings", "content": "Around the design goals presented in Section 3, we conduct extensive evaluations for DeMod by employing several metrics, as shown in Table 1. For easy following, we organized these metrics into three dimensions, including function, performance, and design. The function dimension adopts integrity as the metric to measure to what extent DeMod meets users' function demands. For example, the \"holistic integrity\" means whether DeMod provides sufficient functions to support users' toxicity censorship, and the \"explanation integrity\" denotes whether the two kinds of explanations are sufficient. The performance dimension adopts accuracy to evaluate the effectiveness of DeMod, like \"detection accuracy\", \"explanation accuracy\", and \"modification accuracy\". The \"detection accuracy\" and \"modification accuracy\" are evaluated through both automatic and human evaluations, while \"explanation accuracy\", \"semantic retention\""}, {"title": "5.2 Results", "content": "For easy understanding, we present evaluation results in terms of the modules of DeMod. From the user logs, we found that participants used DeMod frequently. Specifically, each participant conducted toxicity detection for 7.029 times and modification for 4.543 times on average. For the questionnaire, we utilized Cronbach's coefficient alpha [10"}]}