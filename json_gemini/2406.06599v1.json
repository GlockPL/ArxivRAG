{"title": "Anna Karenina Strikes Again:\nPre-Trained LLM Embeddings May Favor High-Performing Learners", "authors": ["Abigail Gurin Schleifer", "Beata Beigman Klebanov", "Moriah Ariely", "Giora Alexandron"], "abstract": "Unsupervised clustering of student responses\nto open-ended questions into behavioral and\ncognitive profiles using pre-trained LLM em-\nbeddings is an emerging technique, but little\nis known about how well this captures peda-\ngogically meaningful information. We investi-\ngate this in the context of student responses to\nopen-ended questions in biology, which were\npreviously analyzed and clustered by experts\ninto theory-driven Knowledge Profiles (KPs).\nComparing these KPs to ones discovered by\npurely data-driven clustering techniques, we\nreport poor discoverability of most KPs, except\nfor the ones including the correct answers. We\ntrace this 'discoverability bias' to the represen-\ntations of KPs in the pre-trained LLM embed-\ndings space.", "sections": [{"title": "Introduction", "content": "Classifying students into behavioral or cognitive\nprofiles using unsupervised cluster analysis tech-\nniques is a common application of machine learn-\ning to educational data (Le Quy et al., 2023; Martin\net al., 2023; Ariely et al., 2024; Rastrollo-Guerrero\net al., 2020; Bovo et al., 2013). Recently, there has\nbeen a growing interest in applying this methodol-\nogy to textual student responses that are decoded\nusing pre-trained large language models into vector-\nized embeddings in semantic spaces (Martin et al.,\n2023; Wulff et al., 2022; Masala et al., 2021). The\noperational appeal of this approach is that it min-\nimizes the need for expert knowledge, which is\ncostly to inject through human labeling procedures\n(Nehm and Haertig, 2012; Tansomboon et al., 2017;\nLi et al., 2023; Ariely et al., 2024). However, the\nvalidity of patterns discovered this way depends\non the ability of the embeddings to maintain the\npedagogically meaningful information that existed\nin the original, textual representations of responses\n(Devlin et al., 2018; Seker et al., 2022) and of the al-\ngorithmic method to discover them. Evaluation of\nemergent profiles is often done in terms of the inter-\nnal quality of the clustering, as data is usually not\navailable to estimate the extent to which the discov-\nered profiles align with a pedagogically meaningful\nrepresentation of the responses. Without such an\nevaluation, a loss of important information can be\noverlooked, potentially leading to sub-optimal edu-\ncational decisions that rely on this analysis (Le Quy\net al., 2023).\nTo investigate whether this hypothesized risk\nmanifests in real-life educational context, we uti-\nlize student answers to two constructed response\nquestions in high school biology. The data was pre-\nviously analyzed by a team of biology education re-\nsearchers and experienced teachers, and graded ac-\ncording to a theory-driven detailed analytic rubric\nthat is based upon the Causal-Mechanical Explana-\ntion framework (Ariely et al., 2024; Salmon, 2006).\nThe rubric contained 10 (item 1) or 11 (item 2)\nbinary categories, each checking for the occurrence\nof a specific key piece of information in the re-\nsponse. Using these human-generated binary vec-\ntors of length 10 (11), the responses were clustered\nusing a KMeans algorithm into a set of 6 (7) Knowl-\nedge Profiles (KPs) that were found by teachers to\nencapsulate specific patterns of errors.\nThe validity of the KPs was evaluated in several\nways. First, human experts conducted a qualita-\ntive analysis to assess whether each KP captures a\nspecific and distinct pattern of errors. Second, we\nanalyzed the results computationally, showing that\ni) the KPs were consistent across the two items,\nnamely, revealing the same type of conceptual er-\nrors; and ii) the learners tended to exhibit the same\ntype of conceptual error (KP) in both items. Third,\nwe conducted an in-class formative assessment in-\ntervention study that provided automated guidance\nto students based on their KP, and showed signif-\nicant improvement in their performance on a dif-\nferent prompt that measures the same conceptual\nknowledge. These analyses provided strong evi-"}, {"title": "Related Work", "content": "Analyzing open-ended items to provide feed-\nback is a time-consuming, complex task. Automat-\ning some of the analyses for assessment and feed-\nback purposes is promising for supporting teaching\nand learning (Tansomboon et al., 2017; Gerard and\nLinn, 2016; Ariely et al., 2023).\nMost systems for automated evaluation of scien-\ntific explanations to date had been designed in the\nsupervised machine learning framework (Schleifer\net al., 2023; Sung et al., 2019; Riordan et al.,\n2020; Kumar et al., 2019; Mizumoto et al., 2019;\nLi et al., 2021). Among the unsupervised ap-\nproaches, Masala et al. (2021) extracted the main\ntakeaways from students' feedback on different\ncomponents in academic courses, using KMeans to\ncluster pre-trained BERT embeddings of students'\nfeedback. Martin et al. (2023) applied HDBSCAN\nover pre-trained LLM embeddings and to find emer-\ngent argumentation patterns' characteristics. Wulff\net al. (2022) investigated HDBSCAN clustering\nover LLM embeddings to evaluate the attention\nof preservice physics teachers to classroom events\nelicited from open-ended text responses. A semi-\nsupervised coding method in which homogeneous\nclusters receive the same coding automatically and\nheterogeneous clusters are fully labeled by humans\nwas proposed by Andersen et al. (2023) and applied\nto student responses to PISA items."}, {"title": "Biases in pre-trained LLMs", "content": "While LLMs are powerful meaning representations\nthat undergird the state-of-art systems on a wide\nrange of NLP tasks, they are also known to exhibit\na plethora of social biases that could lead to so-\ncial harm when the models are used in downstream\ntasks (Bender et al., 2021). In a recent review of the\ncurrent state of research on LLM bias evaluation,\nGoldfarb-Tarrant et al. (2023) criticize the field\nfor focusing heavily on the upstream, pre-trained\nLLMs, in most cases without considering the con-\nnection to a specific task the LLMs is being put to\n(68% of the reports reviewed), citing this as a threat\nto the predictive validity of bias measurements.\nIn fact, the literature that does consider the con-\nnection between upstream (intrinsic) and down-\nstream (extrinsic) behavior suggests that it is not\nstraightforward. Considering static embeddings\n(e.g., word2vec) and a commonly used bias test,\nthe Word Embedding Association Test (WEAT)\n(Caliskan et al., 2017), Goldfarb-Tarrant and col-"}, {"title": "Data", "content": "The data consists of 669 student responses to two\nopen-ended items in high-school biology, collected\nanonymously from students in grades 10-12 from\nabout 25 high schools of varied demographics and\nsocioeconomic status (based on location) across\nIsrael. Gender distribution was 70% females (typ-\nical to the gender distribution among high-school\nbiology majors in Israel). The items deal with the\nconnection between respiration and energy in phys-\nical activity in the context of smoking (Q1) and\nanemia (Q2), taught as part of the core topic \"The\nhuman body\". The items were human-scored using\na similar analytic rubric containing 10 (Q1) or 11\n(Q2) categories (Ariely et al., 2024). All rubric\ncategories are binary, each targeting specific infor-\nmation that needs to be mentioned in a correct re-\nsponse, such as \"the role of hemoglobin in oxygen\ntransportation\u201d or \u201cchanges in cellular respiration\nrate\". The resulting binary vectors were clustered\nusing KMeans; the clusters were analyzed by ex-\nperienced teachers and ranked from 1 to 6 (Q1)\nor 7 (Q2) with larger numbers corresponding to"}, {"title": "Methods", "content": "To evaluate whether raw LLM embeddings carry\nuseful knowledge for unsupervised profiling of re-\nsponses, we experimented with two common clus-\ntering approaches (Le Quy et al., 2023), KMeans\n(Lloyd, 1982) and HDBSCAN (McInnes et al.,\n2017), which implement different clustering mech-\nanisms. The first discovers convex-shaped clusters;\nits mechanism is centroid-based and applies an\nEuclidean distance function. The second is density-\nbased and can be applied with various distance\nmetrics, e.g., a metric induced by cosine-similarity,\nand the clusters may have various shapes. Both\napproaches were used previously for profile dis-\ncovery in constructed response data (Ariely et al.,\n2024; Martin et al., 2023; Wulff et al., 2022). Ex-\nperiments were conducted in Python, using scikit-\nlearn (Pedregosa et al., 2011), SBERT (Reimers\nand Gurevych, 2019) and Pytorch (Paszke et al.,\n2019)."}, {"title": "KMeans", "content": "The KMeans is a widely used algorithm (Lloyd,\n1982). The algorithm is initiated with a specified\nnumber of clusters and a random initialization of"}, {"title": "Hierarchical Density-Based Spatial\nClustering of Applications with Noise\n(HDBSCAN)", "content": "Another clustering approach, which is more promis-\ning in the context of LLMs' embeddings (Martin\net al., 2023) is the HDBSCAN (McInnes et al.,\n2017) algorithm. The approach here is creating\na mutual reachability graph where core samples\nare points in areas of high density. A cluster is a\nset of core samples and a set of non-core samples\nthat are neighbors of core samples but are not core\nthemselves. Non-core samples are at the fringes\nof clusters. A core sample is such that there are\n'min_samples' other samples with a distance less\nthan e from it, for some \u20ac > 0 (Pedregosa et al.,\n2011). The HDBSCAN mechanism performs clus-\ntering for various e values and the most stable clus-\ntering is chosen.\nThe default metric for HDBSCAN is Euclidean\ndistance. To use cosine similarity, we turn it into a\ndistance function (McInnes et al., 2017):\n$||x - y|| = \u221a2 \u00d7 (1 \u2013 CosSim(x, y)),$ (1)\nwhere x, y are unit vectors, i.e., $||x|| = ||y|| =$\n1 (Manning, 1999). Since cosine similarity does\nnot depend on vectors' magnitude, only on the\nangle between the two vectors, we first turned every\nembedding er to a unit vector and then applied\nthe HDBSCAN on a pre-computed metric matrix\nconsisting of all pairwise distances between all\nembeddings in the dataset using formula (1).\nei\nIn contrast to the KMeans, HDBSCAN can find\nclusters with varied densities and clusters may have\nnon-convex shapes."}, {"title": "Metrics for Comparing Clusters", "content": "To compare the similarity between the KPs and the\ncluster assignments of the KMeans/HDBSCAN,\nwe used Adjusted Rand Index (ARI) (Vinh et al.,\n2009). In ARI, similarity is interpreted as the num-\nber of pairs of items on which the clusterings agree,\nadjusted for the amount of chance agreement. Let\nD be a dataset containing n items that are classified\ninto m clusters by clustering C and, independently,\ninto k clusters by clustering E. For a pair of items\n(11, 12) \u2208 D, C and E agree on it iff i1 and 12 are\neither (1) assigned to the same cluster in both C and\nE (let's say there are a such pairs), or (2) assigned\nto different clusters in both C and E (let's say there\nare b such pairs). Now, a + b is the number of\nagreements between C and E. The ARI index is\ngiven by:\n$RI=\\frac{a+b}{\\binom{n}{2}};  ARI = \\frac{RI \u2013 E[RI]}{max(RI) \u2013 E[RI]}$,\nwhere E[RI] is the expected RI for some ran-\ndom label assignment (Vinh et al., 2009), and\nmax(RI) equals to 1. The ARI values range from\n-1 to 1, where 1 indicates perfect agreement, and\n-1 indicates complete disagreement (Hubert and\nArabie, 1985). Since each student response in our\ndataset is labeled with its KP, we evaluated the\nARI for each clustering assignment, i.e., KMeans\nand HDBSCAN, compared to the KPs. This yields\na global comparison between the KPs and each\nclustering assignment.\nTo evaluate the 'discoverability' of each KP, we\nalso conducted by-KP analysis, applying a retrieval\nparadigm and considering each cluster as an at-\ntempt to retrieve each of the KPs. We calculate\nrecall, precision, and F1 score using a contingency\nmatrix A = $(a_{mn})_{1<m<k, 1\u2264n\u2264f}$ where rows are\nthe KPs k = 6,7, and columns are the unsuper-\nvised clusters Cn found by KMeans or HDBSCAN,\nf = #fitted_clusters;\n$a_{mn} = |\\{x : x \u2208 KP_m \u2229 C_n\\}|$,\nthe cell $a_{mn}$ in the matrix A counts the number\nof members of KPm that fell in cluster Cn. The\nprecision of retrieval of KPm using cluster Cn is\n$P_{mn} = \\frac{a_{mn}}{C(n)}$; the recall is $R_{mn} = \\frac{a_{mn}}{KP(m)}$. F1\nscore is $F_{mn} = \\frac{2 \\cdot P_{mn} \\cdot R_{mn}}{P_{mn}+R_{mn}}$, indicating the extent\nto which we were able to retrieve KPm using the\nemergent cluster Cn."}, {"title": "Results", "content": ""}, {"title": "RQ1: Correspondence between\nembedding-based clusters and\ntheory-based Knowledge Profiles", "content": ""}, {"title": "Global alignment between the\nclusterings", "content": "As described in Section 4, we evaluated the agree-\nment between clusterings that were computed from"}, {"title": "Discoverability of specific KPs", "content": "We further investigated the clusters' matching qual-\nity by calculating the F1 score per KP for each of\nQ1 and Q2. For KMeans, the results show good re-\ntrieval of KP1, the cluster with the highest-quality\nresponses - F1 = 0.60, 0.67 for items Q1 and Q2\nrespectively \u2013 but much worse retrieval of the other\nKPs, with maximal F1 = 0.40 for KP6 in Q1 and\nF1 = 0.47 for KP2 in Q2. The clustering results\nin terms of contingency tables and F1 Scores are\npresented in Tables 1 to 4, with KPs as rows and\ncolumns as fitted clusters. The maximum F1 scores\nper profile are shaded in gray.\nThe evaluation of HDBSCAN clusters mirrored\nthat of KMeans, showing better retrieval of KP1\n0.43, 0.46 F1 scores for Q1 and Q2 \u2013 than of any\nother profile, with maximal F1 = 0.36 for KP6\nin Q1 and F1 = 0.29 for KP2 in Q2. We observe\nthat, overall, results are worse for HDBSCAN than\nfor KMeans. The clustering results in terms of\ncontingency tables and F1 Scores are presented in\nTables 5 to 8, with KPs as rows and columns as\nfitted clusters. The maximum F1 scores are shaded\nin gray.\nWe then considered the possibility that more\ncoarse-grained profiles might emerge from the"}, {"title": "RQ2: How well are the KPs represented\nin the embeddings?", "content": "To answer this question, we first analyzed, descrip-\ntively, the level of similarity between the embed-\ndings within each KP, and between KPs. We then\nconducted statistical tests to verify that the ob-\nserved patterns are statistically robust.\nWithin KP similarity. To analyze the level of sim-\nilarity within each KP, we computed the pairwise\ncosine-similarity between all pairs in that KP. Ta-\nbles 9 and 10 show the results, with KPs as rows\nand fitted clusters as columns. Within-KP similar-\nities are in the diagonals. Since the pairwise co-\nsine similarity values are not normally distributed\u00b9,\nwe report medians. The results show that KP1's\nembeddings (highest quality responses) have the\nhighest density; as the quality of a response goes\ndown, so does its similarity to other responses with\nthe same pattern of error.\nBetween-KP similarity. As can be further seen in\nTables 9 and 10, for both items, for every i > 1\nthe embeddings of KP responses tend to be more\nsimilar to the embeddings of KP1 than to embed-\ndings of their own KP (the bolded values in the first\nrow are the largest in each column). This means\nthat erroneous responses of various types are more\nsimilar to the correct responses than to those with\nthe same pattern of error.\nHypothesis testing. Next, we conducted statistical\ntests to confirm that i) the distribution of the embed-\ndings in each KP are indeed different and that ii)\nthe cosine similarity within each KP is correlated\nwith the responses quality.\ni) A Kruskal-Wallis H-test confirmed that at least\none of the medians for the different KPs is signif-"}, {"title": "Discussion and Conclusion", "content": "Our data consists of 669 high-school student re-\nsponses to two typical constructed response items\nin high-school biology. The responses were hu-\nman graded according to an analytic rubric that is\nbased on the Causal-Mechanical explanation frame-\nwork (Ariely et al., 2023), transforming each re-\nsponse to a binary vector that encodes the grad-\ning according to the rubric categories. Previous\nwork demonstrated that applying cluster analysis\n(KMeans) to these vectors, which result from a\nprocess that applies a theoretical assessment frame-\nwork to concrete context by human experts, yields\nstable clusters that reveal pedagogically meaning-\nful knowledge profiles, which were validated in\nseveral ways (Ariely et al., 2024). (For more de-\ntails, see Section 3.) We reasoned that given the\nsuccessful performance of pre-trained LLMs on\na variety of education-related meaning-intensive\ntasks (Schleifer et al., 2023; Wambsganss et al.,\n2023; Riordan et al., 2020; Sung et al., 2019), and\nprevious work that applied this specifically to pro-\nfile discovery (Martin et al., 2023; Wulff et al.,\n2022), we want to evaluate whether unsupervised\nprofile discovery that is not aided by human knowl-\nedge works sufficiently well to be applied out-of-"}, {"title": "Limitations", "content": "It is possible that other clustering approaches could\nhave revealed clusters that are more similar to the\n'gold' ones. However, given that despite the large\ndifference between KMeans and HDB SCAN's al-\ngorithmic approach, they were quite consistent in\nboth demonstrating poor overall agreement and be-\ning biased towards discovering the best KP, we"}, {"title": "Ethics statement", "content": "We acknowledge that the work is conformant with\nthe ACL Code of Ethics. The research and its data\ncollection procedures were approved by the Institu-\ntional Review Board and the Ministry of Education.\nThe instrument was administered to the students as\npart of the regular instruction of the topic, based\non the teachers' decision to use it as part of the\nteaching routine (the instrument was published in\nteachers' forums), with teacher and school prin-\ncipal approval that response data will be used for"}, {"title": "Appendix 1", "content": ""}]}