{"title": "Improving Emotional Support Delivery in Text-Based Community Safety Reporting Using Large Language Models", "authors": ["YIREN LIU", "YERONG LI", "RYAN MAYFIELD", "YUN HUANG"], "abstract": "Emotional support is a crucial aspect of communication between community members and police dispatchers during incident reporting. However, there is a lack of understanding about how emotional support is delivered through text-based systems, especially in various non-emergency contexts. In this study, we analyzed two years of chat logs comprising 57,114 messages across 8,239 incidents from 130 higher education institutions. Our empirical findings revealed significant variations in emotional support provided by dispatchers, influenced by the type of incident, service time, and a noticeable decline in support over time across multiple organizations. To improve the consistency and quality of emotional support, we developed and implemented a fine-tuned Large Language Model (LLM), named dispatcherLLM. We evaluated dispatcherLLM by comparing its generated responses to those of human dispatchers and other off-the-shelf models using real chat messages. Additionally, we conducted a human evaluation to assess the perceived effectiveness of the support provided by dispatcherLLM. This study not only contributes new empirical understandings of emotional support in text-based dispatch systems but also demonstrates the significant potential of generative Al in improving service delivery.", "sections": [{"title": "1 INTRODUCTION", "content": "The importance of emotional support in public safety services is well-documented, particularly in voice-based emergency response systems [17, 51, 62, 72]. Studies show that delivering emotional support effectively through communication is crucial for rapidly establishing rapport and trust, which facilitates quicker and more accurate information exchange [51]. Such an empathetic approach is adopted in various emergency response practices to build trust [72] and reduce the initial distress of individuals affected by traumatic experiences [58]. These insights are aimed at enhancing service quality and improving the provision of emotional support within the safety reporting domain [63].\nAs community Information and Communication Technology (ICTs) have evolved, text-based safety reporting systems [1, 6, 29, 46] are increasingly used by organizations for risk management. For instance, many universities and public organizations offer mobile applications that allow their community members to submit text reports about potential risks, concerning situations, or suspicious activities [1, 6, 29]. Additionally, city police agencies have implemented text-to-911 that enable live chat with dispatchers [16, 24]. Recent works [42, 46] have demonstrated the effectiveness of text-based reporting systems, such as reducing the labor costs associated with safety service provision, enhancing user-perceived value, and fostering inclusivity by assisting marginalized groups in reporting risks. Despite their growing popularity and demonstrated benefits, there remains a significant gap in understanding the emotional dynamics and support delivery within these text-based reporting systems.\nTo address this gap, we conducted a comprehensive system log analysis of a text-based community safety reporting system, named LiveSafe [46], which serves over 200 higher education institutions. This analysis helped us identify patterns of emotional expressions and support delivered in text-based incident reporting. We also observed a longitudinal decline in the provision of emotional support across various organizations, which prompted the development of new technical solutions. Although recent studies have demonstrated the promise and significant implications of using AI and Large Language Models (LLMs) to provide emotional support in various application domains [10, 31, 39, 82], little is known about whether AI models could be trained to support dispatchers in providing emotional support. Therefore, building on the empirical findings and leveraging the real chat logs, we developed an LLM, called dispatcherLLM, fine-tuned for the domain of text-based safety incident reporting.\nIn the remainder of this paper, we refer to the community members who submitted reports as \"users\" and the human agents from the safety organizations who handled the reports via text as \"dispatchers.\u201d Our study makes novel and significant contributions to the CSCW and HCI communities:\n\u2022 First, we performed an emotion analysis on real text-based incident reporting systems. New empirical findings showed that users' emotions tend to be more negative when reporting specific categories of incidents, such as those related to Mental Health. However, their messages generally become more positive through interactions with dispatchers.\n\u2022 Second, we found that the delivery of emotional support by dispatchers was associated with factors related to the incident, service timing, and organizational characteristics. A surprising finding was that organizations with prolonged use of the system tended to provide less emotional support. Notably, emotional support tends to diminish during certain working hours of the day (e.g., 8 a.m. - 12 p.m. and 12 p.m. - 4 p.m.).\n\u2022 Additionally, we developed and implemented an LLM fine-tuned using domain-specific chat logs to mitigate the two issues mentioned above. Our evaluation, using both automatic metrics and user surveys, demonstrated that our dispatcherLLM could provide more consistent and effective emotional support compared to both human dispatchers and other general LLMs (not trained with domain-specific chats) for various incidents.\n\u2022 Lastly, our findings offer strong empirical and practical value and shed light on human-AI co-creation of public services [81]. Specifically, this research pioneers the use of LLM to improve emotional support in public safety services, while LLMs are being increasingly utilized across other service domains [4, 20, 45] to address the labor shortage. Given the sensitive nature of public safety, our goal is not to replace human workers with AI-based solutions. Instead, it is crucial to explore how LLMs can be leveraged to provide more consistent responses. We discuss several implications for the future design of text-based risk reporting systems, aiming to enhance service quality and alleviate dispatcher burnout."}, {"title": "2 RELATED WORK", "content": "In this section, we present prior work on the importance and value of emotional support in safety incident reporting service. We then discuss existing systems and studies designing text-based risk reporting systems. Finally, we discuss recent works on AI-enabled systems providing emotional support."}, {"title": "2.1 The Role of Emotional Support in Existing Safety Incident Reporting Systems", "content": "In emergency response systems, the delivery of emotional support is critical, particularly due to the \"emotional pain\u201d experienced by individuals reporting traumatic or safety-related incidents [72]. This support, when delivered effectively by dispatchers, is crucial for successful information gathering, enhancing trust, and fostering cooperation from callers.\nThe benefits of such proficient emotional support are significant, contributing to the recovery of individuals from traumatic experiences [58]. Meanwhile, inadequate emotional support can negatively impact the dispatcher-caller relationship. Inappropriate questioning strategies, for instance, have been perceived as delaying assistance, leading to user frustration and reluctance in cooperation [17, 51]. This is further supported by findings that demonstrate that a lack of emotional support can result in callers perceiving questions as \u201cface-threatening,\u201d thereby resisting responding to questions [62]. This body of research highlights the crucial role of emotional support in incident handling systems, not only in facilitating immediate information collection but also in establishing a positive, cooperative rapport between dispatchers and callers for effective emergency response.\nHowever, there is currently a lack of examination regarding the delivery of emotional support within text-based reporting systems and its potential impact on the service delivery of safety organizations. Existing work only discussed the functional aspects of the conversations between users and safety administrative agents using text-based reporting systems [42], e.g., the response rates (how many of the reports were handled) and responsiveness (how quickly they were handled). The issues reported by users via mobile or web-based applications are frequently less urgent than those reported through the 911 emergency hotline, as evidenced in the study by Iriberri et al. [30]. These user-generated reports, also referred to as \u201ctips\u201d later in this study, typically detail less critical situations compared to the dire emergencies usually reported through the traditional 911 system. Prior studies also showed that a person's speech and voice could convey more information regarding the person's mental state and emotional experience [25, 52, 55]. Although the emotional aspect of incident reporting has been well discussed by prior studies, the previous accounts are limited to the context of emergency call-taking but not text-based reporting systems. Meanwhile, there has been a lack of empirical understanding about how people show their emotional state in text-based incident reporting and how service providers respond to different emotional states by delivering their service."}, {"title": "2.2 Community Risk Reporting Systems", "content": "Information and Communication Technology (ICT) has been increasingly utilized in risk management in public settings [59]. Several systems have been developed to enable the reporting of safety events and incidents by the community, including web-based forms, mobile apps, and systems that allow users to report safety concerns and risks through crowd-sourcing [7, 11, 30, 54]. Some systems also connect users with emergency service providers and automatically share their location, improving the effectiveness of getting assistance [2, 26, 32]. In addition, live-chat safety incident reporting features have been incorporated into risk management systems by various organizations [9, 46, 65]. Previous research has found that campus community safety apps with emergency texting features have a higher perceived utility by community members and a lower cost compared to traditional emergency communication systems such as blue-light emergency phone towers [53, 76]. Text-based reporting systems, such as text-to-911, have also been found to be beneficial for deaf and hard-of-hearing individuals and people with disabilities [16].\nExisting work has also suggested that how technological systems are designed can greatly impact users' perception of the target tasks and even gradually shift their behaviors. The implementation and adaptation of newly introduced technological systems into an existing organization workflow have been a non-trivial problem widely discussed by existing studies. Tyre and Orlikowski [64] revealed the highly discontinuous nature of technological adaptation in organizations, which usually happens within a relatively brief window after the initial implementation of a new technology. A later study by Buchanan et al. [8] discussed the problem of decay in certain organizational changes, i.e. \u201cinitiative decay\" where the gains from change are lost when new practices are abandoned. According to a study by Mendoza et al. [44], the ease with which users can access ongoing training and their ability to perceive a technology's utility are the two main factors that encourage the productive use of a newly introduced technology. Later work by Barki et al. [5] revealed that users tend to pick up adaptive behaviors in response to technological failures, which is beneficial to the organization in terms of efficiency by facilitating a better match between the system and the context in which it is being used. However, given the research regarding text-based reporting systems, existing studies do not sufficiently address the question of how users engage with agents from safety reporting organizations when utilizing these text-based systems by analyzing empirical data, particularly during conversations."}, {"title": "2.3 Emotional Supports with Conversational Systems", "content": "The capability to provide emotional support is vital for various conversational agents, including those involved in clinical support [12, 14, 68], customer service [20], social interactions [27, 35, 45, 74] and education [4]. Within the domain of Human-Computer Interaction, the integration of emotional dialogue takes on pivotal significance in enriching the natural dynamics of human communication. The acknowledgment of the vital role of emotions in human interaction prompts the integration of automated emotional dialogue systems, intending to bridge this gap and cultivate more intuitive and meaningful interactions between users and computers. Recently, the emergence of LLMs has led researchers to scrutinize their effectiveness in emotion classification for diverse applications. Some researchers strategize methods to elicit emotional support from LLMs, employing strategies such as prompting [37, 70] and data-augmented [80] fine-tuning. However, the utilization of emotional chatbots in the safety reporting domain remains relatively unexplored, primarily due to constraints related to accessing large datasets and annotated emotional information. In the safety reporting domain, AI-mediated conversational interaction with users can assist dispatchers' handling of negative emotions such as despair and feelings of danger and mitigate emotional labor [63, 69].\nDespite the crucial role emotional chatbots could play in supporting users through challenging situations, their utilization in safety reporting remains relatively unexplored. To provide further un-derstanding in this domain, we examined the text-based reporting process from a multi-stakeholder and longitudinal perspective that encompasses both dispatchers and users. Drawn from the findings of our analyses, we introduced and evaluated a fine-tuned LLM for the safety incident reporting do-main. The insights then consequently informed our design recommendations for future AI-enabled text-based risk reporting systems."}, {"title": "3 RESEARCH QUESTIONS", "content": "Given the above literature, we address the following research questions:\n\u2022 RQ1: When do users express negative emotions in text-based incident reports?\n\u2022 RQ2: How do dispatchers provide emotional support in response to different types of incidents?\n\u2022 RQ3: How can LLMs be applied to enhance the perceived emotional support in the incident handling?\nIn the remainder of this paper, we first present an overview of the methods employed for analyzing the system chat logs and the development of the dispatcherLLM model. We then detail the results and findings yielded from the analysis and evaluation of the proposed model. Finally, we discuss the findings and the implications for the future design of text-based reporting systems."}, {"title": "4 METHOD", "content": "To address the proposed research questions, we conducted a comprehensive system log analysis of a real text-based safety incident reporting system, called LiveSafe\u00b9, which has been used by more than 200 organizations, serving communities varying in location region and size. The chat log of each incident report consists of both users' messages and dispatchers' replies. The dataset allowed us to conduct the following analyses:\n\u2022 To answer RQ1, what emotional states are presented in users' text reports of safety incidents, we analyzed the users' messages. Specifically, we applied emotion classification using the GoEmotions dataset [13] and the calculation of Polarity Scores. We also executed statistical analyses to identify contextual factors that were associated with users' emotions in the reports.\n\u2022 To answer RQ2, whether dispatchers provide emotional support in their replies through the text-based reporting system, we conducted text analyses on the dispatchers' replies. We also employed statistical analyses to identify contextual factors that were associated with dispatchers' delivery of emotional support.\n\u2022 To answer RQ3, we fine-tuned the Llama-2 model [61] to yield a dispatcherLLM (Language Learning Model), which can be used to suggest replies by simulating human dispatchers' emotion support languages. We further completed an evaluation by comparing our proposed dispatcherLLM with existing LLMs and showed the improved performance of delivery emo-tional support. We also presented results where participants rated their preferred responses and explained their choices.\nBelow, We provide details of the dataset, the analysis procedure, and the method used for fine-tuning the dispatcherLLM model."}, {"title": "4.1 Dataset", "content": "The chat log was collected through the LiveSafe app across the time period between 2018 and 2019, which allows users to report certain events to a human agent via a chat service. LiveSafe is a risk management system that aims to facilitate communication between members of an organization and the safety teams responsible for managing risk [46]. This tool, which was first introduced in 2013, has been adopted by over 200 higher education institutions. Members of the community can submit information or \"tips\" through the LiveSafe mobile app or web portal, while safety organization dispatchers can respond to these tips through the LiveSafe Command Dashboard. Tips can take the form of text, photographs, video, or audio recordings and can be classified into different"}, {"title": "4.1.1 Data ethics", "content": "Ethical considerations pertaining to data were taken into account before and throughout the entire research process due to the potential for incident reporting logs to contain private and sensitive information. Several steps have been taken to ensure that data ethics, such as anonymization, data minimization, and consent, were properly addressed.\nFirst, the data was carefully anonymized by the organization before being accessible to the researchers for conducting the log analysis. The organizations and users were represented using anonymous IDs, and all mentions about geolocations, times and person/organization names in the chat log text were detected and replaced with tags (e.g., [LOCATION], [NAME]). As an additional measure of anonymization, a random selection of universities was further removed entirely from the dataset by LiveSafe. The dataset was fully anonymized.\nSecond, the researchers of this study did not actively collect the data, nor did they have access to any identifiable user information. The data was provided by the LiveSafe organization to researchers for analysis. An NDA (non-disclosure agreement) was established between the researchers and the LiveSafe organization. The study examined information from organizations and individual users who had consented to the use of anonymized and aggregated data for system improvement research.\nThird, our research strictly adhered to the principle of data minimization, limiting our data collection to only what was essential for the study. All personal and sensitive information had been removed, and identifiable information, including user, geolocation, and organization information, had been removed or anonymized prior to researchers gaining access to the data."}, {"title": "4.1.2 Data cleaning and pre-processing", "content": "To reduce the noise within the chat log data, we first cleaned up the data by removing \"Test\" tips and irrelevant/ambiguous categories, which include incidents from categories including SafeRide; 911 / Call; Broadcast Message; Scavenger Hunt / Test; Operational Procedure Log; Broadcast Check-in Drill; Broadcast Check-in; Request Security Presence / Walk-through; Other; Misc. We only kept tips after 2018 (between 2018-01-01 and 2019-12-31) and chats with more than 2 utterances (one conversation turn), since we want to investigate chats with at least one turn of information-collecting conversation. Finally, for each tip we performed language detection using langdetect [57] to identify messages in English. We identified 16.2% (1,588 out of 9,827) of the tips to be non-English, and removed them to simplify further analysis.\nAfter performing cleaning, the resulting dialogues are categorized into different tip categories, which individuals selected before initiating a chat conversation. The dataset consists of 8,239 incidents, involving 18 different incident categories, where the top 10 categories of incidents with highest tip counts are Noise Disturbance (N=2,282), Suspicious Activity (N=1,700), Emergency Message (N=1,133), Drugs/Alcohol (N=1,031), Facilities/Maintenance (N=593), Harassment/Abuse (N=455), Accident/Traffic/Parking (N=348), Theft/Lost Item (N=269), Mental Health (N=171) and Vandalism/Damage (N=90). Sensitive information (names, places, times, etc) in the dataset was masked for privacy. It is worth noting the impacts this might have had on the annotation process and the model performance, as it is not always clear what the masked value was (e.g. \"The incident must have occurred sometime between [TIME] and [TIME] which is when I got out of work.\").\nThe dataset provides abundant conversation logs for further analysis. To distinguish it from the later annotated sample, we refer to this dataset as all data, containing a total of 8,239 conversations and 57,114 utterances."}, {"title": "4.2 Detecting Emotions Involved in the Chats", "content": "In order to gain a deeper understanding of the emotional dynamics during conversations between users and dispatchers, we use an ML-based method to classify and quantify the emotions of the chat utterances. In this section, we discuss the methods used in this study to 1) classify the emotion of each chat utterance; 2) quantify and aggregate emotion negativity on a conversational level; and 3) identify dispatchers' delivery of emotional support."}, {"title": "4.2.1 Classifying emotions with RoBERTa and identifying emotional support", "content": "Following the approach of [42], we used a RoBERTa-based model to perform emotion classification to identify whether dispatchers provided emotional support [66]. The RoBERTa model was finetuned using the GoEmotions dataset introduced by Demszky et al. [13]. The dataset contains 58K Reddit comments annotated with 28 types of fine-grained emotions (including neutral). We refer to the classification of positive/neutral/negative emotions as described in the GoEmotions dataset. The fine-tuned model was able to classify text into the 28 fine-grained emotion classes, reaching a satisfying overall accuracy of 93.5% when evaluated on the GoEmotions Dataset.\nTo further identify the emotional support within dispatchers' utterances, we utilize the emotion classification model obtained above. We employed a similar method to identify emotional support as used in Khanpour et al. [34]'s work within the domain of online health communities. After applying the emotional classification model over all dispatchers' chat utterances (N = 33,718), we consider the utterances classified under the emotions, including \"caring,\u201d \u201clove,\u201d \u201csadness,\u201d \u201cremorse,\u201d and \"grief,\u201d as providing emotional support according to the definition of Weber et al. [71]. For example, by applying the above method, we identified the following chats where the dispatcher showed emotional support to an incident labeled as Emergency Message, where a dispatcher reassured the caller by texting, \u201cI'm with you and officer [PERSON] and officer [PERSON] will be right there I promise.\u201d, which was classified as caring by the model. In a different scenario, the dispatcher texted \u201cOk. I am sorry to hear that happened to you,\u201d when the user reported a Theft / Lost Item incident, which was identified by the model as sadness."}, {"title": "4.2.2 Measuring conversational emotion polarity", "content": "To further measure the emotional polarity of users throughout conversations, we quantify and aggregate the negativity of users' utterance emotions by conversation specifically for the context of safety reporting. We identified a number of heuristics to better capture the traits of the safety reporting domain after qualitatively analyzing the chat logs. Among these heuristics are: 1) The majority of users have bad emotions when they first start a chat, and dispatchers usually fix or settle them gradually throughout conversations. 2) If a user still has negative emotions at the end of the session, that is more concerning.\nDrawing from existing literature [21, 78], we use a measurement metric for quantifying the negativity of users' utterances throughout a single reporting conversation by calculating the relative frequency of users' utterances with negative emotions. We define negative emotions as in the work of Demszky et al. [13], which consists of fine-grained emotions including anger, annoyance, disappointment, disapproval, disgust, embarrassment, fear, grief, nervousness, remorse and sadness. We modified the emotion polarity score metric with an increasing weight toward the end of each conversation based on our heuristics, which can be denoted as:\n$S_n = \\frac{\\sum_{i=1}^N si e^i}{\\sum_{i=1}^N e^i}$ (1)\nwhere n denotes the nth conversation with a total count N of user utterances. si refers to the emotional polarity of the ith user utterance, i.e. -1 for negative emotions and 0 for neutral and positive emotions. The resulting polarity score Sn represents the user's level of negative emotion during the nth conversation. For example, with user utterances with emotions [fear, confusion, neutral, curiosity, gratitude], s will be [-1, -1, 0, 0, 0]. The resulting polarity score is denoted as S\u2208 [-1,0], indicating how negative the user's overall emotion is throughout the reporting conversation."}, {"title": "4.3 Measuring Information Collection through Event Argument Extraction", "content": "In order to further analyze how dispatchers collect incident-related information from users during conversations, we introduced a BERT-based model to automatically extract event argument infor-mation taking the conversation utterance text as input. In the following section, we first introduce how an event ontology was introduced for safety reporting purposes. Then, we describe the model used for automatic event argument extraction, which was fine-tuned and evaluated using our proposed event ontology."}, {"title": "4.3.1 Event ontology", "content": "To further understand how dispatchers collect incident-related information from users during conversations, we refined the event argument schema based on the Automatic Content Extraction (ACE) dataset [67] and adapted it to the domain of safety incident reporting. The ACE dataset is an event extraction task dataset widely used by prior studies [38, 40, 73]. The dataset in question comprises annotations pertaining to event and entity information obtained from news articles. While the dataset does partially encompass events that are relevant to the domain of safety, such as Attack with arguments including Attacker, Target, Instrument, Time, and Place, the original ACE dataset schema falls short in its ability to directly annotate incidents related to safety due to its coarse-grained nature. To address this issue, we have modified the ACE event schema and subsequently employed it in the analysis of LiveSafe chat logs. Based on the initial ACE event argument schema, an exploratory annotation process was undertaken by the researchers. During the annotation process, we identified a set of event-related arguments, including Attacker, Target, Location, Weapon, Start Time, End Time, and Target Object. These arguments were linked to the event handle, constituting a vital component of the event information. The complete ontology and definition can be found in Appendix A."}, {"title": "4.3.2 Model", "content": "To capture the event information from the conversations between users and dis-patchers, we established an ML-based model to perform event argument extraction. The model is based on the safety reporting event ontology introduced in prior work by Liu et al. [42]. Our event argument extraction model aims to extract certain information about a safety incident given the dialogue history. In order to achieve this, we have identified and selected key fields that are related to the categories of interest, as in the ontology obtained in 4.3.1, and can be extracted as spans of text from the dialogue history. Our model design adheres to the zero-shot setting of STARC [19], which is a state-of-the-art method that can be applied to new domains without additional fine-tuning.\nThe system design involves the choice of questions for each field, the QA model selection, and the decoding of QA outputs into event argument extraction outputs. Several phrasings of natural language questions were compared for each slot. For example, for the \"object stolen\" slot on theft, the questions \"What was stolen?\" and \"What object was stolen?\" were compared, and the second phrasing scored higher. For every new utterance in the dialogue, we predict a new dialogue state consisting of a list of predicted values for each pre-defined slot. The dialogue history until the utterance is passed to the QA model, along with one question for each slot. Valid answers (controlled by the hyper-parameters of a minimum score and maximum length) are appended to an ongoing list of answers for each slot."}, {"title": "4.4 DispatcherLLM: A LLM for Generating Scenario-Based Dispatcher Responses", "content": "To explore the feasibility of improving both informational and emotional support of incident report handling using AI, we fine-tuned an LLM on safety reporting chat logs from LiveSafe. The model's generation output is evaluated using both automatic metrics and human evaluation with community members."}, {"title": "4.4.1 Instruct-tuning with chat log", "content": "A Llama-2 model [61] was fine-tuned and evaluated using the same set of chat logs analyzed in this study. We used the 7B instruction-tuned version of the Llama-2 model released by Meta. We chose the instruct-tuned Llama-2 7B model because of its open-sourced nature and ability to be replicated and validated in various application scenarios. Meanwhile, the instruct-tuned Llama-2 model has been aligned to handle user instructions and deliver emotional support for general applications [28]. In this study, we further fine-tuned the model to adapt to the task space of incident report handling while utilizing the model's learned ability to provide emotional support. The model is fine-tuned on a training partition of 31,359 dispatcher utterances. A validation set consisting of 10,453 dispatcher utterances was used to assess the generalization of the trained model. We refer to the resulting model as dispatcherLLM. Since the instruct-tuned model should be capable of accommodating various incident scenarios, we generate a summary of incident scenarios using GPT-3.5 for each chat log. The generated summaries were then incorporated as scenario descriptions during the fine-tuning process. During supervised fine-tuning, we fine-tuned the model to generate each dispatcher response given the incident scenario summary and dialogue history. Specifically, dialogue history and incident summary are pre-filled as pre-formatted prompts for Llama-2 and then aligned with the dispatcher's utterance by each turn. In the inference phase, our system retrieves the most similar document from the training set, based on both the summary and dialogue history, in order to enhance the dispatcherLLM's responses. An example of the constructed prompt and dispatcher responses generated by the model can be found at A.2."}, {"title": "4.4.2 Human evaluation", "content": "To further evaluate the delivery of effective emotional support, we conducted a survey study with community members from a higher education institution. We recruited 17 undergraduate students from a U.S.-based university community and designed a survey to collect feedback by asking them to compare responses generated by the instruct-tuned model with both responses generated by GPT-3.5 and human dispatchers' responses. The participants are instructed to compare the outputs from GPT-3.5 and dispatcherLLM with human dispatchers' responses and choose the better response based on different scenarios randomly sampled from the test set. We also collected users' qualitative feedback and ratings on the emotional support provided by the two models' generated responses. The findings of the survey study are presented in 5.3.2. The survey details and included samples can be found in the appendix (A.3 and A.4). This study aimed to gather human feedback on the generated emotional support to inform future research on model enhancement and practical applications. It is not intended to be exhaustive or conclusive."}, {"title": "5 FINDINGS", "content": ""}, {"title": "5.1 Users' Emotions Vary by Incident Type and Reporting Stage (RQ1)", "content": "We first examine the emotional dynamics within the conversation between users and dispatchers. In order to conduct a more comprehensive analysis, we base our analysis on the results obtained from the emotion detection model described in 4.2. Using the measurement introduced in 4.2.2, we calculated the emotional polarity score for all incident reporting conversations. The resulting polarity scores ranged between -0.75 and 0. The results showed that while most tips (N=6,623, 82.7%) contained no negative emotions from users' utterances, tips with extreme polarity scores less than or equal to -0.5 (N=262, 3.3%) were also found. To further understand what led to users' negative emotions, we conducted analysis to identify the different contextual factors, including tip category, anonymity, years in use and tip volumes, associated with users' emotional statuses."}, {"title": "5.1.1 Users expressed more negative emotions for certain incident types", "content": "For most tip categories, the first messages from users were often neutral in emotion. This suggests that users may ini-tially approach the dispatcher with a calm and collected demeanor in the majority of incident types reported. The results also show that users tend to express more gratitude for incidents that are less urgent in nature, such as facilities/maintenance, noise disturbance, theft/lost item, and vandalism/damage. This may be because these types of incidents are typically less severe and do not require immediate action, allowing the users to feel more grateful for the assistance provided by the dispatcher. In contrast, for incidents related to mental health and injury/medical, users tend to express more caring emotions. This may be because these types of incidents are often more serious and require a more compassionate response from the dispatcher. The higher level of caring emotions in these situations may reflect the users' concern for their own well-being or the well-being of others involved in the incident.\nAs shown in Figure 2, it is also found that the average polarity scores differ across different incident categories. A one-way ANOVA indicated a significant disparity across incident categories (F(17,8221) = [4.65], p < 0.001***). Incidents that are more urgent in nature or related to personal safety tend to incur more exhibited negative emotions from users (t(722) = [\u22122.13], p = 0.034*), e.g. Harassment / Abuse (M = \u22120.06, SD = 0.13) vs. Theft/Lost Item (M = \u22120.04, SD = 0.11). Meanwhile, Mental Health (M = \u22120.07, SD = 0.14) related incidents have also been found to have more negative emotions in users' utterances during reporting.\nTo further examine the factors associated with users' negative emotions, we conducted a linear regression analysis over whether users' first message was emotionally negative. As shown in Table 1, we found that users' negative emotions are associated with certain categories of events, such as Emergency Message and Mental Health. The results of the regression revealed no association between users' negative emotions and organizations' years in use of the LiveSafe system, and the average number of tips received per year by the organization. The findings have shown that users' negative emotions only varied by the factors related to the nature of incidents and the design"}, {"title": "5.1.2 Users express more positive emotions with dispatchers involved during conversations", "content": "During the reporting conversations, users first provide an overall description of the incidents. Dispatchers then join the conversations to further collect more information and provide assistance to address users' concerns. To further understand users' emotion changes during this process, we conducted a sentiment analysis based on the emotion classification results described in 4.2. Each conversation is divided into three stages the initiation stage (first 1/3 of utterances), the information-gathering stage (middle 1/3 of utterances), and the elaboration stage (last 1/3 of utterances). As shown in the results of the emotion analysis in Figure 3, we discovered an increase in users' positive sentiment as conversations proceeded. A Chi-Square Goodness of Fit Test showed that the sentiment distributions exhibited significant differences among the three stages of conversations ($X^2$ (4, 8239) = [1160.34], p < 0.001***). This is reflected in a consistent increase in the ratio of utterances with positive sentiment (t = [\u221231.369], p < 0.001***), and a decrease in ratios of utterances with both neutral and negative sentiment. The decrease in users' neutral utterances implied the process of users providing incident-related information during the earlier stage of conversations, which later led to a higher proportion of positive utterances as dispatchers provided assistance after information was collected. Although a decrease in negative sentiment is observed, the magnitude of the change is not as obvious, which might imply a higher difficulty for dispatchers to address users' negativity in sentiment or a lack of focus in providing emotional assistance during incident report handling.\nSummary (RQ1): In terms of users' emotions conveyed in their incident report messages, our findings showed that when reporting specific types of incidents, such as Mental Health and Emergency Message, users were more likely to start with negative emotions, as indicated in Table 1. However, their emotions tended to become less negative towards the end of their interactions with the dispatchers, as shown in Fig. 3."}, {"title": "5.2 Factors Associated with the Delivery of Dispatchers' Emotional Support (RQ2)", "content": "Whether dispatchers decided to provide emotional support to users during reporting conversa-tions depends on many factors. Since this is often not included as a standard procedure in safety departments' playbooks, it is vital to understand what factors were associated with dispatchers' decision making in providing emotional support in practice. To examine this, we conducted a logistic regression analysis over the relationship between whether a dispatcher provided emotional support and a range of potential factors involved in the decision-making process."}, {"title": "5.2.1 Tip-related factors", "content": "As"}]}