{"title": "Variable Rate Neural Compression for Sparse Detector Data", "authors": ["Yi Huang", "Yeonju Go", "Jin Huang", "Shuhang Li", "Xihaier Luo", "Thomas Marshall", "Joseph D. Osborn", "Christopher Pinkenburg", "Yihui Ren", "Evgeny Shulga", "Shinjae Yoo", "Byung-Jun Yoon"], "abstract": "High-energy large-scale particle colliders generate data at extraordinary rates, reaching up to one terabyte per second in nuclear physics and several petabytes per second in high-energy physics. Developing real-time high-throughput data compression algorithms to reduce data volume and meet the bandwidth requirement for storage has become increasingly critical. Deep learning is a promising technology that can address this challenging topic. At the newly constructed sPHENIX experiment at the Relativistic Heavy Ion Collider, a Time Projection Chamber (TPC) serves as the main tracking detector, which records three-dimensional particle trajectories in a volume of a gas-filled cylinder. In terms of occupancy, the resulting data flow can be very sparse reaching 10-3 for proton-proton collisions. Such sparsity presents a challenge to conventional learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. In contrast, emerging deep learning-based models, particularly those utilizing convolutional neural networks for compression,", "sections": [{"title": "1. Introduction", "content": ""}, {"title": "1.1. Background", "content": "High-energy particle colliders, such as the Large Hadron Collider (LHC) [1] and the Relativistic Heavy Ion Collider (RHIC) [2], are critical tools for probing the fundamental building blocks of matter. In these experiments, protons and heavy ions are accelerated to velocities approaching the speed of light and providing collisions at extremely high energies (for RHIC nucleon-nucleon center of mass energy is 200 GeV in heavy ion collisions up to and 510 GeV in proton-proton collisions). These complex interactions generate a multitude of subatomic particles (thousands of particles in the central heavy ion collisions), enabling scientists to explore the deepest questions about the nature of the universe. To capture the products of collisions, the interaction point is surrounded by the state of art particle detectors. Recently constructed sPHENIX experiment [2] at RHIC focuses on studying the microscopic properties of strongly interacting matter, such as the quark-gluon plasma, a state of matter that existed in the early universe.\nTo achieve this, sPHENIX employs an array of advanced particle detectors (Figure 1). The Time Projection Chamber (TPC) [3] is the primary tracking detector. The TPC acts as a three-dimensional (3D) camcorder, continu-"}, {"title": "1.2. Related Work and Research Gaps", "content": "Conventional scientific lossy compression methods are motivated in fields such as climate science, fluid dynamics, cosmology, and molecular dynamics. These methods have been driven by the necessity to handle large datasets produced by distributed and high-fidelity simulations in a high-performance computing environment. For example, the error-bounded SZ compression algorithm [8, 9, 10] has proven effective for compressing data from climate science and cosmology simulations. Other examples include the ZFP method [11], which was developed specifically for hydrodynamics simulations, and the MGARD method [12, 13], which was designed to compress turbulent channel flow and climate simulation data.\nWhile there is extensive research on general-purpose lossy compression, few existing methods are optimized for highly sparse data. Although the aforementioned compression algorithms have demonstrated reasonable performance with TPC data, none of them are specifically tailored for this type of input. In this context, a specialized neural network-based compression model, the Bicephalous Convolutional Auto-Encoder (BCAE), introduced in [14] and later refined in [15], has been shown to outperform traditional methods in terms of both compression ratio and reconstruction accuracy for sparse TPC data.\nHowever, despite the BCAE's superior performance, its design treats the sparsity of TPC data as a challenge to overcome rather than as an opportunity to improve compression efficiency. The BCAE model compresses data by mapping the input array to a fixed-size code, regardless of the input's"}, {"title": "1.3. Challenges and Solutions", "content": "Challenge 1. Occupancy-based variable compression ratio. While TPC data on average exhibits low occupancy (around 10%), the actual occupancy can vary significantly. For instance, in the dataset used for model development, occupancy ranges from less than 5% to over 25%. Therefore, there is a need for compression algorithms capable of producing larger codes for denser inputs and smaller codes for sparser ones.\nSolution. We propose BCAE-VS, a bicephalous autoencoder that provides a variable compression ratio based on TPC data occupancy. The design of BCAE-VS is motivated by the hypothesis that a trajectory can be reliably reconstructed from a subset of signals. Consequently, data compression can be achieved by selectively down-sampling signals rather than resizing the input array. To implement this, BCAE-VS's encoder predicts the importance of each signal in the input array for trajectory reconstruction. Signals with higher importance are treated as analogous to the key points that anchor shapes in computer vision tasks such as action and gesture recognition [16, 17, 18, 19]. Compression is achieved by saving only the coordinates and neural representations of these key points.\nChallenge 2. Leveraging sparsity for throughput. Despite the impressive reconstruction accuracy of BCAE-VS, implementing its encoder with"}, {"title": "Main Contributions:", "content": "\u2022 Introduction of BCAE-VS: We propose BCAE-VS, an improved BCAE model enabling Variable compression ratio for Sparse data. This model aims to enhance both compression efficiency and reconstruction accuracy by selectively down-sampling signals rather than reducing the size of the input array.\n\u2022 Improved reconstruction performance: BCAE-VS achieves a 75% improvement in reconstruction accuracy and a 10% higher compression ratio on average compared to the most accurate BCAE model.\n\u2022 Use of sparse convolution for high throughput: To address the computational inefficiencies of conventional convolution with high sparsity data, BCAE-VS employs sparse convolution. This approach processes only the relevant signals, significantly reducing the computational overhead associated with matrix multiplications involving all-zero operands."}, {"title": "2. TPC Data", "content": ""}, {"title": "2.1. Overview", "content": "= \nFigure 2 shows a schematic view of sPHENIX TPC. sPHENIX TPC is a cylindrical gas-filled drift chamber. Working gas mixture is 75% argon (Ar), 20% carbon tetrafluoride (CF4), and 5% isobutane (C4H10). Collisions happen in the center of the TPC. Charged particles produced in the collisions traverse the volume of the TPC at a speed close to the speed of light. As charged particles travel, they ionize the gas atoms along trajectories and create trails of electrons and ions. Extremely uniform electric field $E = 400 V cm^{-1}$ provides uniform drift velocity for electron clouds drift towards the readout planes. The positions and arrival times of the electrons are recorded by the sensors arranged in concentric layers on the readout planes. Gas ionization by a particle depends only on its velocity and the square of the charge, thus the density of the electron cloud allows to distinguish different particles, e.g. protons, kaons, and pions. The chamber is placed within the magnetic field of a superconducting magnet, which provides a magnetic field $B = 1.4T$ collinear with the electric field. Trajectory of the particle is bent by the magnetic field along the axial direction. This bending allows for the determination of the momenta and charges of the particles based on the curvature of their trajectory.\nThe sPHENIX TPC readout plane has three groups of layers: inner, middle, and outer. Each layer group produces a readout of a 3D (axial, radial, and azimuthal) array of ADC values. Each array has 16 layers along the radial direction and 249 temporal sampling points (for one TPC drift time window) along the axial direction on each side of the TPC, representing"}, {"title": "2.2. Dataset and Preprocessing", "content": "To train a compression algorithm, we used a dataset containing 1310 simulated events for central \u2713SNN = 200 GeV Au+Au collisions with 170 kHz pileup collisions\u00b9. The data were generated with the HIJING event genera-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Bicephalous Convolutional Autoencoder", "content": "To address the problem caused by the difficult distribution of log ADC value, Bicephalous Convolutional Autoencoder (BCAE) was proposed in [14] and later optimized in [15]. A BCAE is an autoencoder with two decoders one for segmentation and the other for regression (See Figure 4). During training, a code significantly smaller than the input is generated by the encoder and fed to the decoders. The segmentation decoder, Dseg, will output a score $s \\in (0,1)$ for each voxel in the input. The weights of Dseg are updated by comparing s to the true label of the input 1 for a signal voxel and 0 for a non-signal voxel. The regression decoder, Dreg, will output a value r for each voxel. Given a threshold $s_0 \\in (0,1)$, the predicted value (\u00ee) at each voxel is set to r if $s > s_0$ and to 0 if otherwise. The weights of Dreg are then updated by comparing \u00ee to the true input value of the voxel. The weights of the encoder E are updated with the combined gradient information from both decoders. During inference, we only run the encoder part and save the code to storage for later use.\nThe design of BCAE is motivated by two considerations: reconstruction accuracy and efficiency. First, the segmentation decoder is tasked to tackle the discontinuity in the input data distribution so that the regression decoder"}, {"title": "3.2. BCAE-VS for Key-Point Identification", "content": "Figure 3C shows a close-up view of a typical trajectory formed by a streak of signal patches. The localized and seemingly redundant signal voxels of the same patch suggest the possibility of reconstructing the trajectory with only a fraction of them. As a starter, we performed a sanity check by randomly masking out 50% of signal voxels and used a neural network to reconstruct the original input. This compression by random sampling does not even require a learnable encoder network! The reconstruction mean squared error (MSE) (in raw ADC) is 95, which is much smaller than the 218 reported in [14].\nThese results suggest that if randomly selected signal voxels can perform effectively in reconstruction, a carefully selected subset of signal voxels should yield even better performance. Such a subset is analogous to the key points in computer vision, to which location a geometric shape is anchored. This concept led to the development of BCAE-VS whose encoder compresses by down-selecting signal voxels rather than down-sizing the whole input array. To achieve compression by down-selecting, an encoder assigns an importance score to each signal voxel, retaining only those with higher importance and discarding the rest. However, implementing such an encoder with stan-dard (dense) convolutions requires running 3D convolutions that maintain"}, {"title": "3.2.1. The Working of BCAE-VS", "content": ""}, {"title": "3.2.2. Sparse Convolution", "content": "For symbolic simplicity, we explain sparse convolution with input with two spatial dimensions, height and width. Generalization to a higher dimensional input is straightforward. We use s and d to denote the stride and"}, {"title": "3.2.3. Random Thresholding", "content": "The problem of key-point identification in TPC data compression can be thought of as a \u201cself-supervised classification\" problem. It is a classification problem since we want to predict a label 1 for a signal voxel crucial for accurate reconstruction and a 0 for a non-essential one. It is a self-supervised problem since we do not have a ground truth label to match but have to infer the label by observing the effect of keeping or discarding a signal voxel on reconstruction.\nFor this goal, we designed the random thresholding approach that works as follows. For a signal voxel with predicted importance score p, we generate a random number 0 ~ Uniform(0,1), and calculate a soft mask with the function defined in Equation 2\n$\\varphi(p, \\theta; \\alpha, \\varepsilon) = sigmoid (\\alpha [logit(p,\\varepsilon) \u2013 logit(\\theta,\\varepsilon)])$\n(2)\nThe \u03b5 in the logit function is used to improve numerical stability (p or 1-p less than \u03b5 will be replaced with \u03b5). We set \u03b5 = 10-8 for this study. Plots of with varying threshold \u03b8 and \u03b1 are shown in Figure 6. We can see that"}, {"title": "3.2.4. Balancing Reconstruction with Compression", "content": "To train BCAE-VS, we need to balance the reconstruction accuracy with compression. We do this by adjusting the reconstruction loss Lrecon and compression loss Lcomp.\nThe reconstruction loss Lrecon is the weighted sum of the segmentation loss Lseg and the regression loss Lreg. We use focal loss [25] for segmentation since it is designed to deal with datasets with unbalanced classes (TPC data is sparse and hence has a high percentage of voxels in the class 0). We use mean absolute error (L1) for regression following the convention in [15]. The reconstruction loss Lrecon(x) is defined as AsegLseg(x) + Lreg(x) with Aseg adjusted by the end of each epoch with the same method as discussed in [15, section 2.5]\nSince BCAE-VS compresses by identifying key points, we want the average of p, denoted as \u00b5, to be small, so that only the valuable signal voxels can get a high importance score. However, since there is no need for the average importance score to go arbitrarily small, we also set a lower bound Iprob SO that the average importance will be penalized more lightly when it is close to lprob, and will no longer be penalized if it goes below lprob. Hence, the compression loss function of BCAE-VS is defined as\n$L_{comp} (x, l_{prob})$\n{\n$\\mu(x_s) (\\mu(x_s) \u2013 l_{prob})$  $\\mu(x_s) > l_{prob}$\n0 $\\mu(x_s) \\leq I_{prob}$"}, {"title": "4. Results", "content": ""}, {"title": "4.1. Reconstruction Accuracy", "content": ""}, {"title": "4.2. Compression Ratio", "content": "The compression ratio of the algorithm depends on data precision and the format of storage of the input and the code.\nOn the input side, raw ADC values from TPC are collected as 10-bit integers. However, since most commonly used computing platforms cannot handle 10-bit integers, we consider input ADC values as 16-bits floats. For the input size, we follow the convention in the dense BCAEs works [15, 14] and define it as the size of the zero-padded regularly shaped tensors.\nWith respect to the precision of the code, although we train all models in full precision (float32), it is shown in [15] that the difference in accuracy between reconstruction from full-precision compressed data and those down-cast to float16 is negligible. Similar observation can also be made for BCAE-VS according to Table 2. Hence, compressed data is saved in float16 format for all models."}, {"title": "4.3. Throughput", "content": "Unlike dense convolutions, the inference speed of sparse convolution depends on occupancy. As discussed in Section 3.2.1, lower occupancy leads to"}, {"title": "5. Conclusion", "content": "We introduced BCAE-VS, an algorithm designed to enhance the compression of sparse 3D TPC data from high-energy particle accelerators. BCAE-VS offers a variable compression ratio based on data occupancy, improving both compression efficiency and reconstruction accuracy. Our results show a 75% increase in reconstruction accuracy with a 10% higher average compression ratio compared to the leading BCAE model. By utilizing sparse convolution, BCAE-VS achieves an outstanding throughput within the operational occupancy range of sPHENIX. While the current model demonstrates promising performance, several limitations have been identified. We offer recommendations for future work to address these constraints and further enhance the model's capabilities.\n\u2022 Exact control of retention fraction: Although hyperparameters such as Lcomp and Iprob (see Section 3.2.4) can influence BCAE-VS's the retention fraction, we lack a direct control of it. A potential solution could involve thresholding by quantile of the importance score output p (see Section 3.2.1), allowing a fixed fraction of signals to be saved.\n\u2022 Performance at extreme occupancies: While BCAE-VS performs well for typical occupancy levels in the dataset, it experiences a significant decrease in recall at high occupancy and in precision at low occupancy. For deployment, we need an algorithm that remains reliable even during extreme events.\n\u2022 Precision and quantization for throughput: The sparse convolution li-brary MinkowskiEngine does not support half-precision (float16) op-"}, {"title": "Appendix A. BCAE-VS neural network", "content": "The BCAE-VS network consists of a sparse encoder implemented with sparse convolution provided by the NVIDIA MinkowskiEngine library and two decoders implemented with normal dense convolution. The input to the neural network is one TPC wedge treated as a 4D array with 1 channel and 3 spatial dimensions.\nThe sparse encoder has five 3D convolution layers, all with 2 output channels, kernel size 3, stride 1, and no padding. The dilation parameters of the convolutions are 1, 2, 4, 2, 1 2,4,2,1. There is a rectified linear unit (ReLU) activation between two successive convolutional layers. The output activation is Sigmoid.\nThe two decoders share the same structure. Each one of them has eight (dense) convolutional blocks with one convolution (kernel size 3, padding 1, stride 1, dilation 1) followed by a leaky ReLU with negative slope .1. The number of output channels are (16, 16, 16,8,8,8,4,2). A final 1 \u00d7 1 convolution maps the number of channels back to 1. The regression decoder Dreg has no output activation while the segmentation decoder Dseg has Sigmoid as output activation.\nNo normalization is used in the BCAE-VS model."}, {"title": "Appendix B. BCAE-VS training", "content": "The training is done for 100 epochs with a batch size of 4 and 2000 batches per epoch. For optimization, we used the Adam optimizer with decoupled weight decay (AdamW) with an initial learning rate 0.001, (\u03b2\u2081, \u03b22) =\n(0.9, 0.999), and weight decay 0.01. We kept the learning rate as initialized for the first 20 epochs and decreased it to the 95% of the previous value every 20 epochs thereafter."}]}