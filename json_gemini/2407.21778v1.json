{"title": "TULIP AGENT \u2013 ENABLING LLM-BASED AGENTS TO SOLVE TASKS USING LARGE TOOL LIBRARIES", "authors": ["Felix Ocker", "Daniel Tanneberg", "Julian Eggert", "Michael Gienger"], "abstract": "We introduce tulip agent, an architecture for autonomous LLM-based agents with Create, Read, Update, and Delete access to a tool library containing a potentially large number of tools. In contrast to state-of-the-art implementations, tulip agent does not encode the descriptions of all available tools in the system prompt, which counts against the model's context window, or embed the entire prompt for retrieving suitable tools. Instead, the tulip agent can recursively search for suitable tools in its extensible tool library, implemented exemplarily as a vector store. The tulip agent architecture significantly reduces inference costs, allows using even large tool libraries, and enables the agent to adapt and extend its set of tools. We evaluate the architecture with several ablation studies in a mathematics context and demonstrate its generalizability with an application to robotics. A reference implementation and the benchmark are available at github.com/HRI-EU/tulip_agent.", "sections": [{"title": "1 Introduction", "content": "Advances in Large Language Models (LLMs) provide a technological basis for realizing autonomous agents. Examples range from assistants, e.g., pure software agents or on-device ones, to embodied AI in the form of robots. Key aspects for increasing autonomy in such agents are their understanding of their environment and their ability to act in it. This has become possible by LLMs being able to plan and to adhere strictly to instructions, also with regard to the format of their outputs, enabling them to use tools. Despite these advances and growing context windows for LLMs, there are still inherent limitations to current implementations resulting in the following Challenges 1 to 3:\nChallenge 1 (Costs) Tool descriptions count against the LLM's context window, driving costs both in terms of inference time and money.\nChallenge 2 (Attention and tool limits) Choosing from a large number of tools is challenging for LLMs, as it imposes a form of the \"needle-in-a-haystack\" challenge. This is due to LLMs struggling with in-context learning for long inputs Li et al. [2024], with retrieving multiple facts, and with having to reason about these facts [LangChain, 2024]. In addition, the number of tools that can be provided to the LLM may be limited, as is the case, e.g., for OpenAI models.\nChallenge 3 (Staticity) Tool use is static and limited to a priori defined tools, limiting the adaptiveness of autonomous agents and their applicability to open-ended scenarios.\nThis paper presents the tulip agent architecture to address these challenges."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Autonomous Agents", "content": "The idea of autonomous agents has been around since decades. Such an agent can be \"anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\" Russell and Norvig [2016]. This definition can be extended to the include the \"pursuit of [the agent's] own agenda [...] so as to effect what it senses in the future\" Franklin and Graesser [1996].\nRecent technological advances in LLMs have sparked an entire plethora of approaches for autonomous agents, including AutoGPT, Babyagi, and AgentGPT. Similarly, Ge et al. [2023] present an LLM-based ecosystem, where agents have access to various applications.\nSuch LLM-based agents can operate in a loop consisting of receiving or retrieving inputs, planning, execution, and optionally reflecting on the results. Notably, providing relevant context increases the reasoning quality of LLMs [Wei et al., 2022], and they can benefit from agentic design patterns such as reflection, tool use, planning, and multi-agent collaboration [Ng, 2024]. For planning, specifically, LLMs benefit greatly from structured approaches such as Chain-of-Thought Wei et al. [2022], Tree-of-Thoughts Yao et al. [2023], and Monte Carlo Tree Search Zhang et al. [2024]."}, {"title": "2.2 Tool Use for LLM-Based Agents", "content": "Qin et al. [2023a] generically define tool use as a three-step process. First, the task is decomposed, followed by reasoning for creating a plan and possibly adjusting the plan based on feedback from the environment, and finally solving each subtask by selecting appropriate tools. They recommend to train models specifically for generalized tool use and emphasize the challenge in trustworthy tool use and tool creation.\nThere are various efforts for creating LLMs specifically for using tools. Schick et al. [2023] present Toolformer, a model trained to call APIs. The model decides when and how to use tools in a self-supervised way and passes appropriate arguments. The authors showed improved performance on downstream tasks, but the model is limited to tasks that can be solved with a single tool call. Lu et al. [2023] leverage LLMs for multimodal question answering. They compose heterogeneous tools such as other LLMs, vision models, web search, and Python functions. An LLM-based planner is prompted with tool descriptions, decomposes the task decomposition, and infers a sequence of tool calls to execute based on in-context examples. For evaluating LLMs regarding their tool use capabilities, Huang et al. [2024] present a benchmark that involves six steps, namely planning, tool creation awareness, tool creation, tool usage awareness, tool selection, and tool usage. However, the tools are skeletons with descriptions but without executable implementations, and tool set sizes are limited to eight tools. Huang et al. [2024] showed that, in general, larger models perform better in tool utilization. They demonstrate the need for research regarding tool selection and creation, but do consider neither choosing tools from very large sets nor tool execution. There are also approaches for training smaller models with generalized tool-use capabilities. [Tang et al., 2023] present 7B and 13B parameter Alpaca derivatives finetuned on a tool-use corpus consisting of generated OpenAPI specifications and descriptions. The authors use ReAct Yao et al. [2022] for resolving user instructions, allowing multi-turn interactions. With a focus on on-device deployment, e.g., for Android devices, Chen and Li [2024] released a 2B parameter LLM trained for tool use. For efficiency, they use functional tokens for fine-tuning the model. This provides performance benefits but reduces generalizability compared to the Retrieval Augmented Generation (RAG)-based function calling mechanism in octopus-v1 Chen et al. [2024]. Chen et al. [2024] use Meta's FAISS for semantic search among functions, by embedding the user query and using the top five functions found. The model is evaluated with up to 20 functions in scenarios that require calling a single function, showing a reduction in context length and increased performance, but requiring function-set-specific finetuning. As of this writing, tool use is also supported by commercial models, e.g., the ones provided by OpenAI, Anthropic, and Cohere, as well as open-weights models such as Meta's Llama 3.1 [Llama Team, 2024]. However, even for these models, the number of tools that can be passed to the model is limited, for instance to 128 in the case of OpenAI.\nIn contrast to approaches that focus on planning by itself, approaches such as ReAct Yao et al. [2022] and Reflexion Shinn et al. [2023] directly integrate tools and leverage feedback for plan adaptation. Following this scheme, Zhuang et al. [2023] present toolchain*, an approach using A* search for action space navigation. They focus on finding a cost-efficient solution path, representing the action space as a decision tree, where each node corresponds to an API call. Liu et al. [2023a] present an approach that relies on a task decomposer and a graph representation of tools and their dependencies in terms of inputs and outputs. During task decomposition, the user request is represented as a set of high-level subtasks, available inputs, and a formulation of the desired result. Based on these inputs, the graph is traversed using Depth First Search (DFS) and the tools are executed using an execution engine. This work focuses on multi-modal tools where input and output types are reliable indicators for choosing tools in combination with an LLM-based tool assessment for tool selection. Sun et al. [2023] present an approach for planning with tools including task decomposition and feedback from the environment. For an application to robotics, the authors rely on assertions to ensure that the environment is as expected. The authors distinguish open-loop planning with task decomposition from closed-loop planning with feedback. Further, they emphasize the difference of in-plan refinement, which adapts actions, and out-of-plan refinement, which adapts the plan itself.\nAnother relevant direction of research is the creation of tools by LLM-based agents themselves. Program-aided language models revolve around LLMs with code execution [Gao et al., 2023a]. Qian et al. [2023] present an approach for creating and using tools on the fly, and Cai et al. [2024] suggest combining a dedicated tool maker with a tool user. While versatile, these approaches do not focus on reusing the tools created and there is no easy way to integrate large sets of existing tools."}, {"title": "2.3 Retrieving Relevant Tools", "content": "Despite recent advances regarding increasing context windows, information retrieval from long-context windows is still problematic Li et al. [2024]. The performance of LLMs in this regard can, e.g., be measured with the needle-in-a-haystack tests. This test assesses performance on long contexts by inserting facts at various positions in the context window and rating whether the LLM is able to answer questions about these facts truthfully. Finding multiple needles has been shown to be even more challenging [LangChain, 2024], with performance degrading when the LLM has to retrieve more facts, or when the LLM has to reason about the facts retrieved.\nOne way to work around such limitations is via RAG. Lewis et al. [2020] define RAG as the combination of pre-trained parametric and non-parametric memory to augment LLMs by providing relevant background information. An LLM can serve as parametric memory while the non-parametric memory can be realized in various ways, including full-text search, search via sparse embeddings, such as BM25, search via dense embeddings, e.g., stored in a vector store, or query-based search for formalized databases [Gao et al., 2023b]. In cases of unstructured inputs, which do not necessarily use the same vocabulary as the non-parametric memory, semantic search is especially promising. Recent developments with regards to search, e.g., HNSW [Malkov and Yashunin, 2018], have enabled the development of performant vector stores.\nThis paradigm can be applied to tool search, providing additional context to the LLM in the form of available tools. For instance, Qin et al. [2023b] finetuned Llama for tool use in the form of 16.000 REST APIs, resulting in ToolLLaMA. The authors trained an API retriever that searches among the API descriptions based on the complete task without decomposition. This is combined with multi-round decision making of the LLM in the form of DFS-based tool selection and execution for single-tool and multi-tool scenarios. Patil et al. [2023] present Gorilla, a LLaMA-based model finetuned for writing API calls. The authors compare a zero-shot approach with a retriever-based approach (BM25 or dense embeddings) with top_k = 1 and an oracle retriever. The execution of API calls is not part of the evaluation, and the tasks are limited to using a single tool and single-step reasoning. A key learning is that a retriever may be beneficial, but adding a non-optimal retriever may misguide the model and result in more errors. Another work suggests to clearly separate tool selection and tool usage [Anonymous, 2024]. Here, first a description for the tool needed is generated from the user query based on which the retriever identifies the five most relevant tools. Then, an LLM is employed to actively analyze the top_k tools found in a separate step. Finally, the tool call is executed. In this approach the LLM cannot actively trigger searches for tools, it is limited to tasks that require exactly one tool, and the approach also increases overall costs."}, {"title": "2.4 Application of LLMs with Tool Access to Robotics", "content": "There is a big interest in utilizing the remarkable commonsense reasoning abilities of LLMs in robotic systems. LLMs have been leveraged for robotic applications in different ways and for different purposes [Kira, 2022, Zeng et al., 2023], from generating high-level robotic plans [Joublin et al., 2024, Liu et al., 2023b, Zhou et al., 2023, Huang et al., 2022], to generating code for controlling the robot [Vemprala et al., 2023, Liang et al., 2023, Wu et al., 2023, Singh et al., 2023], to steering the robot's behavior in human-robot interaction setups [Tanneberg et al., 2024, Wang et al., 2024]. When used for planning, typically the available skills, like motion primitives or simple manipulation actions, are given and the size of this library is limited [Ahn et al., 2022, Lin et al., 2023, Hazra et al., 2024]. The LLMs are used to decompose higher level tasks into sequences of these skills, often utilizing formal representations, to solve the task [Liu et al., 2023c, Silver et al., 2024]. For intelligent robots that grow their knowledge and skills in an open-ended way [Yu et al., 2023, Xie et al., 2024], these skill libraries grow [Zhang et al., 2023, Zhou et al., 2023] and it becomes harder for the LLM to identify the useful skills for a given task. Hence, besides lower costs, the tulip agent offers a framework to deal with large and continuously growing skill libraries."}, {"title": "3 Tulip Agent Architecture", "content": "The tulip agent architecture aims to address Challenges 1 to 3 by providing an LLM with access to an extensible tool library. This enables access to an arbitrarily large set of tools that can be efficiently extended and adapted while reducing overall costs.\nTo set up a tulip agent, information about the available tools, in our case Python functions, is extracted automatically via code introspection. From the functions' docstrings, we generate embeddings which are stored in the tool library together with an LLM-compatible representation of the tool information. When receiving a user prompt, the model decomposes the request into subtasks and passes respective descriptions to the search module. These descriptions are the basis for embeddings, which the search module uses to find suitable tools for each subtask via semantic search. The search module passes the information about the most relevant tools to the model , which calls appropriate tools for all subtasks. The tools are executed accordingly and the results are fed back to the model , allowing the model to initiate further actions or provide a response to the user."}, {"title": "3.1 Problem Formulation", "content": "We propose the tulip agent architecture for LLM-backed autonomous agents. Such an agent is initialized with a potentially large set of tools T, for which natural language descriptions and documentation are available. When prompted with a natural language query $q \\in Q$ from the task space Q, the agent's task decomposition model $M_{td}$ must first decompose the query into a plan P consisting of a sequence of subtasks such that the subtasks can be resolved with the tools available. Optionally, the model $M_{td}$ can be primed with selected information $I_T$ about the tools available.\n$M_{td}(q, I_T) \\rightarrow P$ (1)\nSecond, the selection model $M_s$ has to select suitable tools $T^*$ for each subtask i from the potentially large set of tools T. In particular, this step may comprise tool retrieval\n$M_s(P, T, t_s) \\rightarrow \\{T^*\\}_i$ (2)\nThird, the extraction model $M_e$ must extract relevant input values $V_{in}$, i.e., the parameters for the tools. These may be available from the user's query q, as information available from prior tool use, i.e., previous output values $V_{out}$, or as the model's common-sense knowledge, for every subtask.\n$M_e(T_i, q, V_{out}) \\rightarrow V_{in}$ (3)\nWe refer to the combination of a selected tool and the parameters required as an action $a_i \\in A$ from the action space A. These actions are composed by the model $M_a$.\n$M_a(T^*, V_{in}) \\rightarrow a_i$ (4)\nExecution of the actions $a_i \\in A$ by the tool executor E results in output values $V_{out}$. These output values can be new information to be processed further, failure feedback, or the final response to the user query.\n$E(a_i) \\rightarrow V_{out}$ (5)\nUsing this output, the model can generate adapted actions, until the plan P is completed and the agent can return a final response to the user.\nNote that the various models $M_x$ may be implemented as a single language model prompted with different system prompts."}, {"title": "3.2 The Tools and the Tool Library", "content": "In the context of this paper, a tool is an executable function that fulfils a purpose and returns either a result or a status message. Examples include mathematical functions, as provided by a calculator, but also calls to a robot's API. To facilitate providing tools to the tulip agent, entire files with functions are imported.\nFor using tools, LLMs require a unique identifier for the tool, which can be resolved to call the tool, a description of the tool's purpose, and names, types, and descriptions of necessary input parameters. Our approach relies on a function analyzer for introspection. It can ingest Python functions documented according to the Sphinx style, but this could be extended for other types of tools. From the information extracted, we construct a tool library. In principle, the tool library may be any kind of database that supports searching for appropriate tools. Since the user provides natural language inputs and LLMs work well for natural language, semantic search via dense embeddings that allow matching the task to available tools is especially promising. During initialization of the agent, we create embeddings of the function names and the corresponding docstrings, i.e., vector representations capturing the semantics, and store them together with the function descriptions generated by the introspection module. The resulting vector store allows searching for suitable tools via semantic search. The process for initializing the tool library is summarized in Algorithm 1."}, {"title": "3.3 Task Decomposition and Tool Retrieval", "content": "Upon initialization, the tulip agent may take natural language user inputs describing the task to be fulfilled. Compared to the granularity of available tools, such tasks are typically posed on a high level of abstraction. Thus, they cannot be matched sensibly to individual tools. To cope, we use an LLM to decompose the task into subtasks, see Listing 2 for the CotTulipAgent's system prompt, and Listing 3 for splitting the task into steps. Granularity and clarity are essential to reduce the semantic distance between the task description and the descriptions of suitable tools. This is because unnecessary specifics in the task description would add noise and might result in more ambiguous tool suggestions. During decomposition, we have the LLM create more generic descriptions for the subtasks, which can be matched better to the generic descriptions of tools. This results in the plan P in the form of a list of generic natural language descriptions of subtasks.\nThe tulip agent searches its tool library for appropriate tools for each subtask. Specifically, an embedding is created for each subtask in the plan, which is matched against the tool descriptions' embeddings, returning the top_k most suitable tools.\nNotably, the tulip agent architecture supports a recursive decomposition and search for tools. This is relevant in case the initial subtasks are not fine-grained enough to find suitable tools. By setting a similarity threshold for the semantic search, we can ensure that only suitable tools are returned. In case no tools are found whose descriptions are sufficiently similar to the task description the agent decomposes the subtask"}, {"title": "3.4 Tool Use", "content": "Based on the plan P consisting of subtasks and the identified tools, the LLM is prompted to generate tool calls, see Listing 5. This is done in a step-by-step way, allowing the LLM to take into account previous return values. Note that tool calls can take the form of structured JSON responses, as is the case for models fine-tuned for tool use, or text that has to be parsed. For each tool call, the respective tool is retrieved from the lookup, see ToolLookup in Algorithm 1, which is then executed by the tool executor E with the parameters provided. The tool's return value can be an intermediate result, e.g., from a calculation, a status message, e.g., from a robot action, or feedback about a failure. This result is fed back to the LLM, enabling the model to react by calling further tools or even searching for other relevant tools in the tool library. When all subtasks are solved or if it becomes clear that the LLM is unable to solve the task, the LLM may provide a final response to the user.\nThe query process for the CotTulipAgent is summarized in Algorithm 3. It comprises several steps, first splitting the task imposed by the user into subtasks, searching for suitable tools for each subtask, using these tools, and responding with the final result."}, {"title": "3.5 Autonomous Tool Management", "content": "To further increase the versatility of the agent and enable it to learn in a continuous way, see Challenge 3, we provide the tulip agent variant AutoTulipAgent with five generic tools. These tools enable it to plan and provide it with Create, Read, Update, and Delete (CRUD) access to its tool library. The first two tools allow the agent to decompose tasks and search for tools in the tool library, as described in Section 3.3. In contrast to other variants, the AutoTulipAgent does not follow a predefined flow, though, but may call these tools as necessary, potentially recursively. In addition, the agent disposes of tools for creating new tools, updating existing tools, and deleting existing ones. These tools allow the agent to change its own tool library on-the-fly. While tool deletion is trivial, creating new tools and updating existing ones are similar to each other in their approach. Leveraging the code generation abilities of LLMs, the agent triggers the generation of a new tool for a specific task described in natural language. When updating existing tools, the existing code is fed into the code generation LLM as additional context. To ensure validity of the new tool, the agent checks its executability in a loop in the spirit of Joublin et al. [2024] before saving. For Python tools, this can be achieved, e.g., via the ast module. Note that the LLM must generate code that can be processed by the tulip agent's function analyzer. Specifically, the code must be properly documented using descriptions and type hints. The resulting code is written to a file, and automatically introspected, embedded, and loaded into the tool library. Using an importer, specifically Python's importlib, the new tool can be loaded dynamically, making it callable during operation. Eventually, the AutoTulipAgent may use the extended tool library to solve the initial problem.\nThe benefit of the AutoTulipAgent over a code interpreter lies in its ability to load large sets of available tools, reusability of tools leading to reduced costs, and the ability to continuously improve existing tools."}, {"title": "3.6 Architecture Variations", "content": "The tulip agent architecture can be used in conjunction with other LLM agent paradigms. Using a tool library is characteristic for the tulip agent architecture, cp. Section 3.2. This also implies tool use, i.e., the LLM can initiate tool calls, which are resolved and the results of which are provided to the LLM. To further improve the LLM's performance, CoT prompting can be applied to guide the LLM through the process in a fixed way, independent of the other features. In addition, we investigate several variations for priming the LLM used for task decomposition towards the tools available in the tool library. Finally, we investigate equipping the agent with means for self-editing, specifically editing its own tools. In the case of the tulip agent, this is achieved via CRUD operations for the tool library available to the agent, cp. Section 3.5."}, {"title": "4 Experiment Setup and Results", "content": ""}, {"title": "4.1 Implementation", "content": "The prototype is implemented in Python 3.10 and the tool library is built on ChromaDB. As language models, we used OpenAI models via the API, specifically gpt-4-turbo-2024-04-09 and gpt-3.5-turbo-0125, and text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large as embedding models. To achieve results that are as deterministic as possible, we set the temperature to le-9 and repeated experiments five times. Note that we report the interquartile mean for costs. This is because the API interaction limit of 100 may lead to outliers that would skew the comparison."}, {"title": "4.2 Quantitative Evaluation", "content": "We evaluate the tulip agent architecture, several ablations, and baselines regarding correctness and costs. Correctness includes high-level correctness of the result, independently of how it is reached, and precision and recall with regard to the tools used. Costs are based on token counts, including input tokens, output tokens, and embedding tokens. The monetary assessment of costs considers current OpenAI pricing, see Table 5. Note that to avoid bias in favor of the tulip agent architecture, the costs for these variations always include the embedding costs for setting up the entire tool library. In practice, the costs for creating the tool library are only incurred once, further reducing overall costs when used multiple times.\nWe designed a benchmark with three categories with 20 tasks each.\n\u2022\tEasy: Requires using one tool\n\u2022\tMedium: Requires using two or three tools\n\u2022\tHard: Requires using at least four tools\nNote that this classification imposes a harder assessment compared to Liu et al. [2023a]. For medium and hard tasks, the task has to be split up into subtasks, requiring planning. For an example of a hard task including the ground truth with regards to expected results and expexted functions to be used see Listing 10.\nTo set up the tool library, we generated 100 simple math functions with type hints and docstrings in Sphinx format using an LLM, and validated them manually. This set of tools includes examples such as add(), multiply(), and coefficient_of_variation(), see Listings 6 to 8, respectively. We limited the number of tools in the experiments to 100 to enable comparisons with CotToolAgent, which is limited by OpenAI's current tool limit of 128. However, we qualitatively demonstrated the tulip agent architecture to work also for a significantly larger tool set containing 600 tools.\nThe results for various agents are shown in Figure 3. Note that all variations introduced in Section 3.6 are included except for AutoTulipAgent, as this one does not work reliably enough with less performant models such as gpt-3.5-turbo-0125.\nThis comparison of variants led to three findings. First, we learned that tools are essential for current LLMs to solve complex tasks reliably, cp. Finding 1. This insight becomes apparent from the comparison of agent variations with tool use and the BaseAgent regarding correct results. This argument also aligns with the idea of using specialized agents in a Multi Agent System (MAS), only that in our case an agent may also be part of a tool, as is the case for task decomposition.\nFinding 1 (Tools are essential for solving complex tasks) Using tools allows LLMs to reliably solve complex tasks such as in the mathematics benchmark, similar to how a human would use a calculator. In addition, tools are of paramount importance for enabling an agent to interact with its environment beyond pure text generation. This is crucial for applications such as robotics.\nSecond, we found that using a tool library to give an agent access to tools reduces costs significantly in most scenarios, cp. Finding 2. This connection is emphasized with increasing numbers of tools or when solving more complex tasks, which require several calls to the LLM. Note that a generic issue for costs lies in the possiblity of the LLM getting stuck in a loop of responses.\nFinding 2 (Using a tool library significantly reduces costs) In our evaluation, the tulip agent architecture significantly reduces costs, and variants such as CotTulipAgent or PrimedCotTulipAgent maintain correctness. This is because costs for embeddings are negligible, the number of input tokens is drastically reduced, and there are only relatively few output tokens necessary. For our experiments, we found the cost reduction to be a factor of two to three when using 100 tools without excessively long documentation. Factors that further increase the benefits of the tulip agent are large sets of functions and complex tasks that require calling many tools, as all the tools are passed to the LLM for each subtask. Conversely, in scenarios that include only very simple tasks and few tools, using the tulip agent architecture may not yield benefits.\nThird, planning, i.e., task decomposition, positively influences tool use and thus overall performance, cp. Finding 3. This step can be improved even further by priming the LLM used for task decomposition with an overview of the available tools. This finding is in line with related work [Patil et al., 2023].\nFinding 3 (Task decomposition improves tool use) Decomposing tasks into subtasks that can be executed with individual tools significantly improves the results, see MinimalTulipAgent versus CotTulipAgent. Priming the task decomposition with available tools further improves performance, see CotTulipAgent versus PrimedCotTulipAgent.\nTo assess the impact of the LLM used within the agent, we ran several experiments with various agents, including the baselines BaseAgent and CotToolAgent. The results are summarized in Table 2. Unsurprisingly, gpt-4-turbo-2024-04-09 performs best, but is also the most expensive. These experiments confirmed that the tulip agent variants lead to a reduction in costs while providing the same level of correct task completion, cp. Finding 2, even across models. However, agents with advanced functionality, specifically CRUD access to the tool library, require better LLMs, cp Finding 4.\nFinding 4 (Language model performance influences the suitability of agent designs) With greater functionality, the agent variations require a more performant LLM. For instance, AutoTulipAgent re-"}, {"title": "Influence of the pre-selection of tools", "content": "To assess the influence of the pre-selection of tools during semantic search compared to the selection by the LLM, we varied the top_k parameter for several tulip agent variations, see Table 4. In particular the comparison of MinimalTulipAgent with the other variants shows that sound task decomposition allows searching more narrowly, i.e., using lower values for top_k, cp. Finding 6. In the naive case of MinimalTulipAgent, this is because the embedding of the user query is directly compared to all tools and thus must be at least as high as the number of tools needed for successfully completing the task, resulting in a drastic increase in performance for hard tasks with higher top_k.\nFinding 6 (Better planning allows narrower search) tulip agent variants with better planning, especially PrimedCotTulipAgent, benefit less from increasing top_k beyond 5. More naive implementations, such as MinimalTulipAgent, on the other hand, benefit from an increase in top_k, especially for hard tasks.\nFinding 6 hints towards the benefit of recursive task decomposition for more complex tasks. However, in our experiments, recursive task decomposition did not yield benefits, as a single-step decomposition turned out to be sufficient for these kinds of tasks."}, {"title": "4.3 Qualitative Demonstration of CRUD Operations", "content": "The AutoTulipAgent with CRUD operations can be applied in scenarios where suitable tools are not specified a priori. In such cases, the tool search does not return suitable tools, and the agent creates a new tool using an LLM, cp. Section 3.5. For instance, it can solve the following sequence of tasks starting with an empty tool library, cp. Listing 13.\n1.\tWhat is the square root of 23456789?\n2.\tChange the square root tool to correctly work for negative numbers.\n3.\tCalculate the square root of -200.\n4.\tDelete the square root tool.\nThis example demonstrates the self-editing capabilities of the tulip agent architecture, and anecdotal evidence suggests that this approach works well for sensibly encapsulated functions. Note that this approach greatly depends on the LLM's abilities to generate code and that it may be sensible to use a dedicated model for realizing the tool creation.\nFinding 7 (Autonomous creation of a tool library) The tulip agent architecture is suited for continually creating tools and building a tool library on the fly. This, however, depends largely on the abilities of the LLM used with regard to code generation."}, {"title": "4.4 Application to Robotics", "content": "To assess the applicability of the tulip agent architecture to embodied agents, we also applied it to a robotics scenario. We built on top of a publicly available simulation of a proactive supportive robot [Tanneberg et al., 2024]. In its original form, the robot is controlled by an LLM with access to a variety of tools for information retrieval, physical actions, and social expressions. We initialized a CotTulipAgent-backed agent with access to the same tools and extended the agent's system prompt with the original robot character [Tanneberg et al., 2024]. This setup worked in a plug-and-play way and we tested it in various situations, see Figure 4 for some examples. The CotTulipAgent-backed agent successfully solves the given instructions utilizing its tool library consisting of actions such as pour_into and hand_over, cp. Listing 9.\nWhile the number of tools is limited in comparison to the math setup, an extensible tool library has the potential to enable the robot to continuously learn and adapt in an open world [Zhang et al., 2023, Tziafas and Kasaei, 2024]."}, {"title": "5 Summary and Outlook", "content": "This paper presented the tulip agent architecture. Using a dedicated tool library, it provides three benefits. First, it significantly reduces costs, both in terms of time and money, when using large numbers of tools (Challenge 1). Second, it allows working with large numbers of tools despite context window limitations and the needle-in-a-haystack challenge (Challenge 2). Third, it allows for dynamic creation and loading of tools (Challenge 3), paving the way towards continuously evolving autonomous agent systems. Based on several ablations regarding the LLM, the embedding model, the parameter top_k for tool search, and the tulip agent architecture, we report on various findings that are hopefully valuable for the research community, including the importance of planning for this approach.\nWhile the implementation presented relies on a vector store as an intuitive way for setting up the tool library, alternative RAG strategies and combinations thereof may be highly efficient and should be explored further. Also, the tulip agent architecture provides a basis for continuously extending the tool library with new tools created by learning from interactions, making it a sensible basis for open-ended applications, especially in robotics."}]}