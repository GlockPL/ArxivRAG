{"title": "TULIP AGENT \u2013 ENABLING LLM-BASED AGENTS\n......\nTO SOLVE TASKS USING LARGE TOOL LIBRARIES", "authors": ["Felix Ocker", "Daniel Tanneberg", "Julian Eggert", "Michael Gienger"], "abstract": "We introduce tulip agent, an architecture for autonomous LLM-based agents with Create,\nRead, Update, and Delete access to a tool library containing a potentially large number of\ntools. In contrast to state-of-the-art implementations, tulip agent does not encode the\ndescriptions of all available tools in the system prompt, which counts against the model's\ncontext window, or embed the entire prompt for retrieving suitable tools. Instead, the tulip\nagent can recursively search for suitable tools in its extensible tool library, implemented\nexemplarily as a vector store. The tulip agent architecture significantly reduces inference\ncosts, allows using even large tool libraries, and enables the agent to adapt and extend its\nset of tools. We evaluate the architecture with several ablation studies in a mathematics\ncontext and demonstrate its generalizability with an application to robotics. A reference\nimplementation and the benchmark are available at github.com/HRI-EU/tulip_agent.", "sections": [{"title": "1 Introduction", "content": "Advances in Large Language Models (LLMs) provide a technological basis for realizing autonomous agents.\nExamples range from assistants, e.g., pure software agents or on-device ones, to embodied AI in the form of\nrobots. Key aspects for increasing autonomy in such agents are their understanding of their environment\nand their ability to act in it. This has become possible by LLMs being able to plan and to adhere strictly\nto instructions, also with regard to the format of their outputs, enabling them to use tools. Despite these\nadvances and growing context windows for LLMs, there are still inherent limitations to current implementations\nresulting in the following Challenges 1 to 3:\nChallenge 1 (Costs) Tool descriptions count against the LLM's context window, driving costs both in terms\nof inference time and money.\nChallenge 2 (Attention and tool limits) Choosing from a large number of tools is challenging for LLMs,\nas it imposes a form of the \"needle-in-a-haystack\" challenge\u00b9. This is due to LLMs struggling with in-context\nlearning for long inputs Li et al. [2024], with retrieving multiple facts, and with having to reason about these\nfacts [LangChain, 2024]. In addition, the number of tools that can be provided to the LLM may be limited, as\nis the case, e.g., for OpenAI models.\nChallenge 3 (Staticity) Tool use is static and limited to a priori defined tools, limiting the adaptiveness of\nautonomous agents and their applicability to open-ended scenarios.\nThis paper presents the tulip agent architecture to address these challenges."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Autonomous Agents", "content": "The idea of autonomous agents has been around since decades. Such an agent can be \"anything that can be\nviewed as perceiving its environment through sensors and acting upon that environment through actuators\"\nRussell and Norvig [2016]. This definition can be extended to the include the \"pursuit of [the agent's] own\nagenda [...] so as to effect what it senses in the future\" Franklin and Graesser [1996].\nRecent technological advances in LLMs have sparked an entire plethora of approaches for autonomous agents,\nincluding AutoGPT2, Babyagi\u00b3, and AgentGPT4. Similarly, Ge et al. [2023] present an LLM-based ecosystem,\nwhere agents have access to various applications.\nSuch LLM-based agents can operate in a loop consisting of receiving or retrieving inputs, planning, execution,\nand optionally reflecting on the results. Notably, providing relevant context increases the reasoning quality\nof LLMs [Wei et al., 2022], and they can benefit from agentic design patterns such as reflection, tool use,\nplanning, and multi-agent collaboration [Ng, 2024]. For planning, specifically, LLMs benefit greatly from\nstructured approaches such as Chain-of-Thought Wei et al. [2022], Tree-of-Thoughts Yao et al. [2023], and\nMonte Carlo Tree Search Zhang et al. [2024]."}, {"title": "2.2 Tool Use for LLM-Based Agents", "content": "Qin et al. [2023a] generically define tool use as a three-step process. First, the task is decomposed, followed\nby reasoning for creating a plan and possibly adjusting the plan based on feedback from the environment,\nand finally solving each subtask by selecting appropriate tools. They recommend to train models specifically\nfor generalized tool use and emphasize the challenge in trustworthy tool use and tool creation.\nThere are various efforts for creating LLMs specifically for using tools. Schick et al. [2023] present Toolformer,\na model trained to call APIs. The model decides when and how to use tools in a self-supervised way and\npasses appropriate arguments. The authors showed improved performance on downstream tasks, but the\nmodel is limited to tasks that can be solved with a single tool call. Lu et al. [2023] leverage LLMs for\nmultimodal question answering. They compose heterogeneous tools such as other LLMs, vision models,"}, {"title": "2.3 Retrieving Relevant Tools", "content": "Despite recent advances regarding increasing context windows, information retrieval from long-context\nwindows is still problematic Li et al. [2024]. The performance of LLMs in this regard can, e.g., be measured\nwith the needle-in-a-haystack tests. This test assesses performance on long contexts by inserting facts at\nvarious positions in the context window and rating whether the LLM is able to answer questions about these\nfacts truthfully. Finding multiple needles has been shown to be even more challenging [LangChain, 2024],\nwith performance degrading when the LLM has to retrieve more facts, or when the LLM has to reason about\nthe facts retrieved."}, {"title": "2.4 Application of LLMs with Tool Access to Robotics", "content": "There is a big interest in utilizing the remarkable commonsense reasoning abilities of LLMs in robotic systems.\nLLMs have been leveraged for robotic applications in different ways and for different purposes [Kira, 2022,\nZeng et al., 2023], from generating high-level robotic plans [Joublin et al., 2024, Liu et al., 2023b, Zhou\net al., 2023, Huang et al., 2022], to generating code for controlling the robot [Vemprala et al., 2023, Liang\net al., 2023, Wu et al., 2023, Singh et al., 2023], to steering the robot's behavior in human-robot interaction\nsetups [Tanneberg et al., 2024, Wang et al., 2024]. When used for planning, typically the available skills, like\nmotion primitives or simple manipulation actions, are given and the size of this library is limited [Ahn et al.,\n2022, Lin et al., 2023, Hazra et al., 2024]. The LLMs are used to decompose higher level tasks into sequences\nof these skills, often utilizing formal representations, to solve the task [Liu et al., 2023c, Silver et al., 2024].\nFor intelligent robots that grow their knowledge and skills in an open-ended way [Yu et al., 2023, Xie et al.,\n2024], these skill libraries grow [Zhang et al., 2023, Zhou et al., 2023] and it becomes harder for the LLM to\nidentify the useful skills for a given task. Hence, besides lower costs, the tulip agent offers a framework to\ndeal with large and continuously growing skill libraries."}, {"title": "3 Tulip Agent Architecture", "content": "The tulip agent architecture aims to address Challenges 1 to 3 by providing an LLM with access to an\nextensible tool library. This enables access to an arbitrarily large set of tools that can be efficiently extended\nand adapted while reducing overall costs.\nFigure 2 gives an overview of tulip agent's components, with the arrows indicating the flow of information.\nTo set up a tulip agent, information about the available tools, in our case Python functions, is extracted\nautomatically via code introspection. From the functions' docstrings, we generate embeddings which are\nstored in the tool library together with an LLM-compatible representation of the tool information. When\nreceiving a user prompt 1, the model decomposes the request into subtasks and passes respective descriptions\nto the search module 2. These descriptions are the basis for embeddings, which the search module uses to\nfind suitable tools for each subtask via semantic search. The search module passes the information about the\nmost relevant tools to the model 3, which calls appropriate tools for all subtasks. The tools are executed\naccordingly and the results are fed back to the model, allowing the model to initiate further actions or\nprovide a response to the user 7."}, {"title": "3.1 Problem Formulation", "content": "We propose the tulip agent architecture for LLM-backed autonomous agents. Such an agent is initialized\nwith a potentially large set of tools $\\mathcal{T}$, for which natural language descriptions and documentation are\navailable. When prompted with a natural language query $q \\in \\mathcal{Q}$ from the task space $\\mathcal{Q}$, the agent's task\ndecomposition model $M_{td}$ must first decompose the query into a plan $P$ consisting of a sequence of subtasks\nsuch that the subtasks can be resolved with the tools available. Optionally, the model $M_{td}$ can be primed\nwith selected information $I_T$ about the tools available. In Figure 2, we refer to this step as task decomposition\nas performed between 1 and 2.\n\n$M_{td}(q, I_T) \\rightarrow P$                                                                                (1)\n\nSecond, the selection model $M_s$ has to select suitable tools $T^*$ for each subtask $i$ from the potentially large\nset of tools $\\mathcal{T}$. In particular, this step may comprise tool retrieval, see 2 \u2462 in Figure 2. This can be\nformulated as a search problem, optionally with a search tool $t_s$.\n\n$M_s(P, \\mathcal{T}, t_s) \\rightarrow \\{T^*\\}_i$                                                                                                    (2)\n\nThird, the extraction model $M_e$ must extract relevant input values $V_{in}$, i.e., the parameters for the tools.\nThese may be available from the user's query $q$, as information available from prior tool use, i.e., previous\noutput values $V_{out}$, or as the model's common-sense knowledge, for every subtask. In Figure 2, this is done\nbefore step 4\n\n$M_e(T_i, q, V_{out}) \\rightarrow V_{in}$                                                                                       (3)\n\nWe refer to the combination of a selected tool and the parameters required as an action $a_i \\in \\mathcal{A}$ from the\naction space $\\mathcal{A}$. These actions are composed by the model $M_a$. In Figure 2, such actions are passed from the\nmodel to the tool executor as tool calls in step 4.\n\n$M_a(T^*, V_{in}) \\rightarrow a_i$                                                                                    (4)\n\nExecution of the actions $a_i \\in \\mathcal{A}$ by the tool executor $E$ results in output values $V_{out}$. These output values\ncan be new information to be processed further, failure feedback, or the final response to the user query."}, {"title": "3.2 The Tools and the Tool Library", "content": "In the context of this paper, a tool is an executable function that fulfils a purpose and returns either a result\nor a status message. Examples include mathematical functions, as provided by a calculator, but also calls to\na robot's API. To facilitate providing tools to the tulip agent, entire files with functions are imported.\nFor using tools, LLMs require a unique identifier for the tool, which can be resolved to call the tool, a\ndescription of the tool's purpose, and names, types, and descriptions of necessary input parameters. Our\napproach relies on a function analyzer for introspection. It can ingest Python functions documented according\nto the Sphinx style, but this could be extended for other types of tools. From the information extracted, we\nconstruct a tool library. In principle, the tool library may be any kind of database that supports searching for\nappropriate tools. Since the user provides natural language inputs and LLMs work well for natural language,\nsemantic search via dense embeddings that allow matching the task to available tools is especially promising.\nDuring initialization of the agent, we create embeddings of the function names and the corresponding\ndocstrings, i.e., vector representations capturing the semantics, and store them together with the function\ndescriptions generated by the introspection module. The resulting vector store allows searching for suitable\ntools via semantic search. The process for initializing the tool library is summarized in Algorithm 1."}, {"title": "3.3 Task Decomposition and Tool Retrieval", "content": "Upon initialization, the tulip agent may take natural language user inputs describing the task to be fulfilled.\nCompared to the granularity of available tools, such tasks are typically posed on a high level of abstraction.\nThus, they cannot be matched sensibly to individual tools. To cope, we use an LLM to decompose the task\ninto subtasks, see Listing 2 for the CotTulipAgent's system prompt, and Listing 3 for splitting the task into\nsteps. Granularity and clarity are essential to reduce the semantic distance between the task description and\nthe descriptions of suitable tools. This is because unnecessary specifics in the task description would add\nnoise and might result in more ambiguous tool suggestions. During decomposition, we have the LLM create\nmore generic descriptions for the subtasks, which can be matched better to the generic descriptions of tools.\nThis results in the plan P in the form of a list of generic natural language descriptions of subtasks.\nThe tulip agent searches its tool library for appropriate tools for each subtask. Specifically, an embedding\nis created for each subtask in the plan, which is matched against the tool descriptions' embeddings, returning\nthe top_k most suitable tools.\nNotably, the tulip agent architecture supports a recursive decomposition and search for tools. This is\nrelevant in case the initial subtasks are not fine-grained enough to find suitable tools. By setting a similarity\nthreshold for the semantic search, we can ensure that only suitable tools are returned. In case no tools are\nfound whose descriptions are sufficiently similar to the task description the agent decomposes the subtask"}, {"title": "3.4 Tool Use", "content": "Based on the plan P consisting of subtasks and the identified tools, the LLM is prompted to generate tool\ncalls, see Listing 5. This is done in a step-by-step way, allowing the LLM to take into account previous\nreturn values. Note that tool calls can take the form of structured JSON responses, as is the case for models\nf"}]}