{"title": "RLEF: GROUNDING CODE LLMS IN EXECUTION\nFEEDBACK WITH REINFORCEMENT LEARNING", "authors": ["Jonas Gehring", "Kunhao Zheng", "Jade Copet", "Vegard Mella", "Taco Cohen", "Gabriel Synnaeve"], "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks over\nmultiple steps while keeping the required manual engagement to a minimum. Cru-\ncially, such LLMs need to ground their generations in any feedback obtained to\nreliably achieve desired outcomes. We propose an end-to-end reinforcement learn-\ning method for teaching models to leverage execution feedback in the realm of\ncode synthesis, where state-of-the-art LLMs struggle to improve code iteratively\ncompared to independent sampling. We benchmark on competitive programming\ntasks, where we achieve new start-of-the art results with both small (8B param-\neters) and large (70B) models while reducing the amount of samples required\nby an order of magnitude. Our analysis of inference-time behavior demonstrates\nthat our method produces LLMs that effectively leverage automatic feedback over\nmultiple steps.", "sections": [{"title": "INTRODUCTION", "content": "The consistent increase in capabilities of Large Language Models (LLMs) has prompted researchers\nand developers to benchmark and deploy them in increasingly complex environments (Brown et al.,\n2020; OpenAI, 2023; AI @ Meta, 2024). An emerging research direction is to employ LLMs as\nagents to solve tasks in multiple steps with little to no human oversight, querying external com-\nputation or data sources when needed or as dictated by manual scaffolding (Schick et al., 2023;\nKapoor et al., 2024). For example, such autonomous use of LLMs is of interest for ensuring ac-\ncurate answers to user queries with up-to-date information (Mialon et al., 2024), interaction with\nwebsites (Yao et al., 2022) or generating code to implement software features from high-level de-\nscriptions (Yang et al., 2024).\nWe posit that any decision-making agent offering a natural language interface has to possess two\nskills. First, the ability to accurately deduce a user's intent when prompted; for LLMs, this is\ntypically achieved by fine-tuning to follow instructions according to user preferences (Ouyang et al.,\n2022; Rafailov et al., 2023). Second, feedback on intermediate results of the agent's actions has\nto be taken into account to arrive at the desired outcome. For example, a web page containing a\nnecessary bit of information might have gone offline, requiring another search engine query. In the"}, {"title": "METHOD", "content": "2.1 ITERATIVE CODE SYNTHESIS\nWe structure the task of code synthesis as a multi-turn conversation in which an LLM is repeatedly\nprompted to generate a code solution to a natural language problem description. After each solution,\nwe provide an automatically generated response with results obtained by executing the solution's\ncode against test cases. This setup is applicable to language models tuned for the common use-\ncase of interacting with users in a chat setting, and follows previous work on self-repair for code\ngeneration (Shinn et al., 2023; Olausson et al., 2024).\nCrucially, we utilize two different sets of test cases: a public test yields execution feedback that can\nbe accessed during repeated attempts and forms the basis of selecting a final solution, whereas a\nprivate test set ultimately determines the correctness of the final solution. Separate test sets provide\ntwo main benefits. First, if test inputs and outputs are fixed, held-out tests guard against shortcuts\nduring the optimization procedure in which an LLM can copy expected test outputs in subsequent\nanswers, based on execution feedback. Second, running a full test suite may be computationally\ndemanding and a limited set of public tests can accelerate the iterative code generation procedure. It\nmay however be desirable to maximize test coverage for execution feedback at inference time, and\nwe verify that this can indeed improve performance (Appendix B.2).\nOur conversation flow for code generation is depicted in Fig. 2. Concretely, we start the dialog with\nthe problem description and query the LLM for an initial solution. The solution is verified against\nthe public test set, which yields results in the form of passed and failed test cases, as well as potential\nsyntax or runtime errors. If any public test fails, this execution feedback is formatted and appended\nto the dialog The LLM is then queried for an updated code solution, with the original problem text,\nprevious solutions and their respective feedback provided in the prompt. If the solution passes all\npublic tests, or a specified turn limit is reached, it is considered to be final and will be submitted\nfor evaluation on the private test set. The kind reader is referred to Appendix C for a listing of our\nprompt and execution feedback templates.\n2.2 REINFORCEMENT LEARNING WITH EXECUTION FEEDBACK\nThe iterative code synthesis described in the previous section can be understood as a Markov Deci-\nsion Process (MDP), and the language model as a policy (Sutton & Barto, 2018). For generality, we\nassume a partially observable MDP as our reward function utilizes a held-out, private test set which\nis not accessible to the policy (unless an exact textual representation of the desired program behav-\nior is provided in the problem description). Observations and actions are provided as tokenized text\nsequences. Concretely, the initial observation $o_0$ is the problem description and actions $a_t$ at each\nstep t are textual responses. Successive observations $o_t$ consist of past observations and actions,\nincluding execution feedback obtained by evaluating the previous action $a_{t-1}$ on public test cases.\nEpisodes terminate when public test evaluation succeeds or a specified step limit is reached. At the\nend of an episode, a scalar reward is provided corresponding to whether all public and private tests\nare passing. We do not use reward discounting (i.e., $\\gamma$ = 1).\nFor optimizing a policy in the above environment we employ Proximal Policy Optimization (PPO),\na common choice for fine-tuning large language models (Schulman et al., 2017; Ziegler et al., 2020;\nOuyang et al., 2022). Following previous work, we include a KL penalty in our reward signal,\nacting both as an entropy bonus and as regularization towards the distribution of the LLMs we start\nfrom. In initial experiments we found that a possible failure mode concerns the generation of invalid\ncode in non-final responses, which we address by providing a small penalty for invalid responses.\nDenoting the policy to be optimized with $\\pi$ and the initial policy with p, and abbreviating previous\nobservations and actions with $c_t$ = $o_0, a_0, o_1, a_1, ..., o_t$ our reward function at step t is thus\n$R(s_t, a_t) = r(s_t, a_t) \u2013 \\beta log \\frac{\\pi(a_t|c_t)}{p(a_t|c_t)},$, where $r(s_t, a_t) = \n\\begin{cases}\n1, & \\text{if end of episode and all tests pass} \\\\\n-1, & \\text{if end of episode and any test fails} \\\\\n-0.2, & \\text{if } a_t \\text{ does not contain valid code}\n\\end{cases}$with a constant $\\beta$ trading off between task reward and KL maximization. For PPO, we compute\npolicy gradients by incorporating a concurrently learned value function as a baseline, i.e., we train\nthe policy to maximize the advantage $A_t$ = $R(s_t, a_t)$ \u2013 $V(c_t)$."}, {"title": "EXPERIMENTAL RESULTS", "content": "3.1 SETUP\nWe perform experiments on the CodeContests benchmark introduced by Li et al. (2022) which\nrequires generating a code solution to a problem specified in natural language along with a textual\ndescription of public test cases. Problems are of high difficulty and used in human competitive\nprogramming with a focus on algorithms, data structures and runtime efficiency. The correctness\nof solutions is evaluated with private tests that are hidden from contestants, which we implement\nin our setup by presenting feedback from public tests only. The CodeContests dataset consists of a\ntraining set and two test sets, \"valid\u201d and \u201ctest\u201d, consisting of 117 and 165 problems, respectively;\nwe use the former for model and hyperparameter selection. We optimize our models on the training\nset, from which we discard 115 of the 13,328 problems due to missing public test cases. We prompt\nand train all models to output Python 3 code.\nThe Llama 3 family of models (AI @ Meta, 2024) comprises our initial policies, specifically the\nInstruct 8B and 70B parameter models of the 3.0 and 3.1 release. These models exhibit strong code\ngeneration performance out of the box and are able to follow instructions in the prompt, alleviating\nthe need for an initial fine-tuning stage prior to RL training. During training and for evaluations,\nunless noted, we set the turn limit to allow for 3 LLM attempts at solving each problem. We per-\nform 12,000 and 8,000 updates to the 8B and 70B models, respectively, and select checkpoints\nbased on valid set performance. Hyper-parameters and further experimental details are provided in\nAppendix A.\nWe follow Li et al. (2022) in reporting results as n@k average solve rates. The n@k metric represents\nthe expectation that any of n solutions, selected from k samples in total, is correct, i.e., passes all\ntests. In our multi-turn setup, each turn counts as a sample. This allows for fair comparisons with\nrespect to sample budgets, which is particularly relevant when employing large LLMs with high\ninference cost in agentic scaffoldings (Kapoor et al., 2024).\n3.2 MAIN RESULTS\nIn Table 1 we list our solve rates on the CodeContest valid and test sets for iterative code generation\nwith up to three turns, along with previously reported results. When sampling from our models,\nwe use temperatures 0.2 for 1@3 and 1.0 for 10@100, and nucleus sampling with top-p 0.95 in all\ncases (Holtzman et al., 2020). Each solve rate is estimated on 200 rollouts, using the estimator de-\nscribed in (Li et al., 2022). We compare against AlphaCode (Li et al., 2022) and PPO with rewards\nfrom test execution on the Code Llama 34B model from Xu et al. (2024), which both report results\nwith a large number of samples. AlphaCodium (Ridnik et al., 2024) and MapCoder (Islam et al.,\n2024) are high-performing agentic frameworks built on top of the proprietary GPT models and com-\nbine chain-of-thought prompting, code execution, program repair, and, in the case of AlphaCodium,\nautomatic test generation.\nWith RLEF training we improve markedly on the original Llama 3.1 models and outperform prior\nworks by a significant margin. Notably, on the test set the 70B model beats AlphaCodium with\nGPT-4, the previous state-of-the-art, with a single rollout compared to 5 solutions from 100 samples\n(38.0 and 29). Likewise, the 8B model with RLEF is slightly ahead compared to the similar-sized\nAlphaCode 9B model (16.0 and 13.3), but with a sample budget of 3 in our case and 1,000 for\nAlphaCode. While we cannot compare directly to the more recent AlphaCode 2 (AlphaCode Team,\n2023), a performance estimate of 34.2 on the valid set for 10@100 puts our 70B model ahead (37.5)\nwith just 3 samples. When considering a larger budget of 100 samples \u2013 corresponding to 33\nrollouts - the stock 70B model beats all previously reported results, including AlphaCodium on the\nvalid set. With RLEF training, we obtain further improvements to 54.5 on the valid and test set.\nThe relative improvements over the initial models, while still significant, are reduced in the 10@100\nsetting as compared to the 1@3 setting. Kirk et al. (2024) observe that RL training of LLMs can\nreduce the diversity of outputs and we interpret our results as further evidence of their hypothesis.\nTable 1 also highlights that the released Llama 3.1 models offer competitive performance on Code-\nContests from the start, which we attribute to a focus on coding capabilities during instruction tun-\ning (AI @ Meta, 2024). However, our RLEF method is also highly effective on the previously\nreleased 3.0 8B model, improving 1@3 solve rates from 4.1 to 12.5 and 3.2 to 12.1 on the valid\nand test set, respectively. Thus, RLEF may be useful as a partial substitute for instruction tuning for\ntasks where automatic evaluation is possible.\n3.3 INFERENCE-TIME BEHAVIOR\nIn Table 2 we first take a closer look at single- and multi-turn performance with a fixed budget\nof 3 LLM generations (1@3). This corresponds to our iterative setup with up to three model re-\nsponses, or three independent responses for single-turn results. We further consider generalization\nto two popular code generation benchmarks, HumanEval+ and MBPP+ (Liu et al., 2023), which"}, {"title": "ABLATION STUDIES", "content": "3.4.1 LEARNING ITERATIVE CODE SYNTHESIS\nWe investigate whether LLMs can, apart from our RL training, be effective in multi-turn code gen-\neration using few-shot prompting (Brown et al., 2020) and supervised fine-tuning (SFT). Lacking\nsuitable ground truth training examples for SFT, we mine rollouts on the CodeContests training set\nwith Llama 3.1 70B Instruct and filter them based on the correctness of final solutions. We then fine-\ntune Base and Instruct versions of the Llama 3.1 8B and 70B parameter models on the mined corpus\nand also source it for few-shot examples (Appendix A.3). The results in Table 3a show that few-\nshot prompting is detrimental to the instruction-tuned models. In Appendix B.1 we report few-shot\n1@3 solve rates for pre-trained models and find that they achieve lower performance compared to\nzero-shot prompting for instruction models (1.2 and 1.8 for 8B, 4.6 and 5.8 for 70B on valid and test\nset, respectively). Supervised fine-tuning improves Instruct model performance on the validation set\nonly; we do not see improvements on the test set. For pre-trained models, we see improvements\nfrom SFT but lower scores compared to instruction-tuned models (Appendix B.1). With RLEF we\nobtain significantly higher solve rates compared to SFT models, underscoring the efficacy of our RL\ntraining loop.\n3.4.2 SINGLE-TURN TRAINING\nIn Table 3b we compare our iterative code generation setup to traditional, single-turn generation\nwhere the model is not presented with inference-time feedback. We use the same training loop for\nsingle generations, albeit without the penalty for invalid code (Section 2.2) as this is subsumed by\nthe reward signal for incorrect solutions. For Llama 3.1 Instruct 8B, single-turn training (ST) hurts\nperformance on the test set. The 70B model benefits from single-turn training and improves over\nmulti-turn SFT results in Table 3a. Moreover, we observe transfer in that applying the model trained\nfor single turns in a multi-turn setting improves 1@3 solve rates. We attribute this to the existent\nbut comparabily weak multi-turn capabilities of the vanilla 70B Instruct model. Overall, we see\nstrongest performance with the RLEF method employing multiple turns at training and inference\ntime."}, {"title": "RELATED WORK", "content": "Generating program code with LLMs to automate and assist software development has been studied\nextensively in recent years, with evaluations predominantly focusing on code synthesis from natural\nlanguage descriptions (Clement et al., 2020; Chen et al., 2021; Austin et al., 2021). A major boost\nin performance is obtained by including large quantities of source code in pre-training and selecting"}, {"title": "CONCLUSION", "content": "In this work, we proposed reinforcement learning from execution feedback (RLEF), a fine-tuning\nmethod for LLMs that endows them with a crucial capability for autonomous operation: ground-\ning future generations in environment feedback. We applied RLEF to iterative code synthesis and\nobtained substantial improvements in solve rates on the CodeContests competitive programming\nbenchmark while reducing the required sample budget for inference. The RLEF-trained models\nfurther generalize to increased turn limits and to HumanEval+ and MBPP+, two popular code gen-\neration benchmarks that exhibit simpler programming questions and different execution feedback\nformatting. Our in-depth analysis revealed that, while an increase in correct first-turn generations\nand in the diversity of successive generations offers a major contribution of performance, our models\nalso meaningfully take execution feedback into account and resolve errors over multiple turns.\nLimitations. While our results demonstrate effective usage of inference-time feedback, the code\nsynthesis task we consider is limited to improving a single solution to a given problem. General-\nizing our method to environments with larger tasks that require decomposition, either via manual\nscaffolding or, eventually, in a self-directed manner, remains the subject of further research.\nBroader Impact. Successful grounding of LLMs for code generation execution feedback will\namplify their utility when applied to impactful tasks such as assisting software development and\nperforming quality control. In general, however, increasing the capabilities of LLMs, now widely\ndeployed in a range of applications, requires quality control and guard-railing to promote safety and\nminimize potentially harmful output. We limit our study to the generation of source code, where we\nconfine the execution of model-generated output to local sandboxes. We believe the framework of\nShavit et al. (2023) regarding the governance of AI agents to be a useful resource for practitioners."}, {"title": "EXPERIMENTAL DETAILS", "content": "A.1 RLEF\nWe initialize separate policy and value function networks from pre-trained and instruction-tuned\nLLMs as indicated in the respective experiments; for the value function, we replace the output layer\nwith a randomly initialized linear projection. For PPO, we use a AdamW (Loshchilov & Hutter,\n2019) with a learning rate of $2e-7$, weight decay of 0.1, and a linear warm-up over 50 steps. We\nset the KL regularization factor $\\beta$ of the reward term to 0.05 (Section 2.2). All models are trained\nwith an online, asynchronous training infrastructure that decouples inference and optimization. We\nincorporate importance sampling in PPO's clipped surrogate objective (Schulman et al., 2017, Eq.7):\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|c_t)}{\\pi_{\\theta old}(a_t|c_t)}$,  $L^{CLIP}(\\theta) = E_t[min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$\nfor model parameters $\\theta$, advantage $A_t$, and the behavior policy $\\pi_b$. We set $\\epsilon$ = 0.2. For optimizing\nthe value function, we use a clipped value loss.\nDuring training, we perform inference with a temperature of 1.0; we use neither nucleus (top-p) nor\ntop-k sampling. We collect 1024 rollouts and perform 4 updates on 256 sequences each. Models are\nevaluated every 800 updates, and we select the final model based on validation set performance. We\ntrain our models on NVidia H100 GPUs; a training run takes approx. 20 wall time hours. With the\nabove parameters we use 288 (128 for traning, 160 for inference) and 2304 (1024 for training, 1280\nfor inference) GPUs for 8B and 70B models, respectively.\nA.2 CODE EXECUTION\nWe evaluate candidate solutions with the accompanying code-base of Li et al. (2022)\u00b3 using Python\n3.10. All problems in the validation and test set specify a memory limit, and only a few problems\ndefine a time limit. If specified, we apply these limits for RLEF training and evaluations; otherwise,\nwe use a 1GB memory limit and maximum wall clock time of 10 seconds per test case.\nA.3 SUPERVISED FINE-TUNING\nWe perform supervised fine-tuning (SFT) for the ablations in Section 3.4.1. In order to assemble a\ntraining dataset, we perform iterative code generation with our proposed setup on the CodeContests\ntraining set with the Llama 3.1 70B Instruct model. We set top-p to 0.95 and sample a temperature\nfor each response in U(0.1, 1.0). For each problem in the training set we collect 100 multi-turn\nrollouts and obtain 313,639 successful trajectories.\nWe fine-tune models for next-token prediction, computing losses on the last response only (i.e., on\nresponses passing both public and private tests); this produced slightly better models compared to\ntraining on all responses. We sweep over learning rates 5e-7 and 2e-6, and 2 and 3 epochs with\na batch size of 64 and sequence length 8192. A linear warmup is performed over 10 steps, and\nlearning rates are annealed according to a cosine schedule. Weight decay is set to 0.1. Models are\nevaluated after 200 optimizer steps with AdamW and we select final parameters based on validation\nset performance."}, {"title": "ADDITIONAL EXPERIMENTAL RESULTS", "content": "B.1 PRE-TRAINED MODELS\nIn Table 4 we list solve rates for few-shot prompting and supervised fine-tuning from pre-trained\nLlama 3.1 models. We observe significantly lower performance compared to the Instruct models in\nall cases (Table 3a)."}, {"title": "PROMPTS", "content": "C.1 CODECONTESTS\nIn the initial prompt, we substitute ${problem} by the original problem description as-is.\nInitial Prompt\nProvide a Python solution for the following competitive programming\nquestion: ${problem}.\nYour code should be enclosed in triple backticks like so: ```python\nYOUR CODE HERE ```. Use the backticks for your code only.\nIn the execution feedback prompt below, we show templates for the four different error types we\nconsider: wrong answer, exception, timeout, and out of memory. We then show the respective\nfeedback for each failing test.\nC.2 RANDOM FEEDBACK ABLATION\nIn Section 3.3 we test RLEF-trained models with random execution feedback. For each problem, we\nsample a different problem from the respective test set that contains incorrect solutions. We obtain\nunrelated feedback by evaluating one of these incorrect solutions, chosen at random, against the\ncorresponding public tests and present the resulting feedback to the model. If none of the incorrect\nsolutions fail the public tests, we evaluate raise NotImplementedError(). In this case, the\nfeedback will contain backtraces pointing to this error. Otherwise our dialog proceeds as usual, i.e.,\nif the code solution produced by the LLM passes the true public tests of the problem in questions\nwe stop and evaluate the solution all test cases.\nC.3 FEW-SHOT PROMPTING\nFor the few-shot ablations in Section 3.4, we select successful trajectories from the Llama 3.1 70B\nInstruct model on problems from the CodeContests training set. We select trajectories with both\n2 and 3 successful attempts to as demonstrations for successful multi-turn code generation. For\ninstruction models, we initialize the dialog with the few-shot examples, separating them with an\nempty assistant message. For few-shot experiments with pre-trained models (Appendix B.1), we\nuse a dialog format in which each message is either prefixed by [USER] or [ASSISTANT]. The\ntoken for ||, an invalid symbol in Python, is used as a message delimiter.\nC.4 HUMANEVAL+\nHumanEval problem prompts consist of starter code, with a docstring and example tests following\nthe function declaration.\nInitial Prompt\nWrite a solution to the following problem and make sure that it\npasses the tests:\n${problem}\nWe then provide the problem prompt again at the start of each model response for completion.\nThe tests in HumanEval+ consist of a single function with several assert statements. In order\nto obtain execution feedback for individual tests, we extract them from original test function (for\ncomputing pass rates, we use the original test code). We further transform assert statements\ninto matching function calls of Python's built-in unittest.TestCase class. This way, test\nfailures will result in more informative AssertionError exceptions with run-time values; these\nare provided as assertion error to the template. We also show successful test cases."}]}