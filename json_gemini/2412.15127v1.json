{"title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "authors": ["Haotian Zheng", "Jinke Ren", "Yushan Sun", "Ruichen Zhang", "Wenbo Zhang", "Zhen Li", "Dusit Niyato", "Shuguang Cui", "Yatong Han"], "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "In the past two years, large language models (LLMs) have become the leading solution for many practical applications, such as finance, medicine, and education, due to their powerful natural language understanding and generation capabilities [1]. However, the massive size of LLMs, often consisting of hundreds of billions to trillions of parameters, results in high computational latency and low memory efficiency [2], [3]. This makes real-time processing and flexible scalability challenging, especially for the practical deployment of LLMs on resource-constrained edge devices [4], [5]. To address this issue, lightweight deployment of LLMs has become a key research direction to enhance LLMs' accessibility across diverse platforms [6].\nRecently, model pruning has been recognized as a promising solution to reduce LLMs' model size and computational overhead while maintaining their model performance [7]. Specifically, model pruning reduces computational complexity by removing unnecessary weights or structures from a model without sacrificing the model's key functionality and prediction accuracy [8]. Moreover, by focusing on important structures, model pruning can also mitigate overfitting issues often present in large models, particularly LLMs [9]. Thus far, many pioneering studies have emphasized the importance of structured pruning to balance model performance and resource efficiency [10], [11]. In particular, several advanced pruning techniques have been developed to adaptively remove weights based on their contributions to model performance [12], [13].\nDespite these advancements, there remain three challenges in LLM pruning: 1) Weight importance estimation, where accurately estimating weight importance is crucial to pruning without affecting model performance; 2) Layerwise pruning ratio, where a uniform ratio may not be suitable for all structures in LLM; and 3) Fine-tuning, where fine-tuning pruned LLMs is essential for recovering their performance [14]. Several early studies have explored pruning methods that rely on uniform metrics, typically using single or linear approaches [15]\u2013[18]. However, these metrics often oversimplify pruning decisions and fail to capture the intricate interdependencies of coupled structures. On the other hand, post-pruning fine-tuning is crucial to restore accuracy but consumes significant computational and storage resources. Therefore, achieving an optimal balance between memory efficiency and model performance remains a challenge in LLM pruning."}, {"title": "II. RELATED WORK", "content": "Recently, many leading companies have released their open-source LLMs, such as LLaMA [19], Vicuna [20], and Chat-GLM [21], which have significantly influenced the field of natural language processing. Since these models grow in size and complexity, the need for efficient model pruning techniques has become increasingly apparent. Typically, model pruning can be divided into two categories, including structured pruning and unstructured pruning. Structured pruning removes weights according to a predefined network structure. It is particularly beneficial for hardware acceleration because it conforms to the parallelism of modern computing architectures [22]. In contrast, unstructured pruning removes weights individually, which often leads to irregular network structures that are difficult to optimize and deploy in practice. In the following, we focus on structured pruning and review its three stages in previous studies, including weight importance estimation, layer-wise pruning, and LLM fine-tuning."}, {"title": "A. Weight Importance Estimation", "content": "LLM-pruner [18] was the first framework for structured pruning of LLMs, which effectively removed non-critical coupling structures and sped up the process without relying on the original training data. Following it, LoRAShear [23] employed the low-Rank adaptation of LLMs (LoRA) with half-space projected gradient (LHSPG) for progressive pruning, dynamically evaluating weight importance to retain more critical information and achieve superior knowledge transfer. Additionally, SparseGPT [24] introduced a second-order pruning method based on weight importance, effectively scaling to GPT models with 10 to 100 billion parameters and significantly enhancing pruning efficiency. Wanda [22] offered a new weight importance metric based on weights and activations to improve the pruning performance and speed. Besides, a weight importance-driven non-neural model was proposed in [25], which utilized gradient boosting decision trees (GBDT) as the accuracy predictor for efficient pruning selection. Furthermore, shortened LLaMA [26] adopted deep pruning techniques that integrate weight importance with structural efficiency, achieving comparable performance to width pruning, particularly under memory-constrained scenarios. Despite the achievements, the weight estimation metrics in these works have not accurately calculated the importance of different modules in LLMs. Therefore, they may not work well in cases with large pruning ratios."}, {"title": "B. Layer-wise Pruning", "content": "MINI-LLM [27] proposed a hybrid pruning standard to remove non-critical channels and multi-attention heads by integrating magnitude, activation, and gradient. Subsequently, EDGE-LLM [28] proposed a layer-wise unified compression method, which achieved layer-by-layer pruning through an adaptive layer adjustment scheme. Furthermore, AlphaPruning [29] utilized the heavy-tailed self-regularization theory to design the layer-wise pruning ratio of LLMs, significantly reducing the mode size while maintaining a reasonable perplexity. Although these studies have delved into the issue of layer-wise pruning ratios, they have not addressed the challenge posed by the significant variance in importance scores across different layers. This disparity hinders the ability to uniformly assess their contributions."}, {"title": "C. LLM Fine-tuning", "content": "Fine-tuning is an important method for enhancing the performance of LLMs in downstream tasks. To address the issues of high computational cost and long training latency associated with standard fine-tuning methods, many parameter-efficient fine-tuning (PEFT) algorithms have been proposed and garnered extensive attention [30]. For instance, an adapter method was proposed in [31], which inserted small bottleneck adaptation layers to reduce the number of parameters that need to be updated. In addition, LoRA [32] reduced computational overhead by fine-tuning low-rank decompositions within the model. Following it, quantization-aware LORA (QLORA) [33] enhanced the fine-tuning efficiency and effectiveness by combining quantization with LoRA. While these works can fine-tune LLMs efficiently, existing fine-tuning methods face challenges such as high memory usage and inefficiencies in scaling. However, SAAP streamlines quantization and low-rank adaptation, thereby enhancing deployment efficiency for LLMs across various architectures and scales."}, {"title": "III. PRELIMINARIES", "content": ""}, {"title": "A. LLM Pruning Process", "content": "As shown in Fig. 1, the pruning process of LLMs typically consists of four stages [34]:"}, {"title": "\u2022 Discovery Stage:", "content": "Given a foundation LLM, all coupled structures in the LLM are first identified based on a dependency detection algorithm [11]. Each coupled structure is defined as a \"group\"."}, {"title": "\u2022 Estimation Stage:", "content": "When identifying all groups, it is necessary to evaluate the importance of each group. There are two types of importance metrics, including vector-wise importance and element-wise importance. Specifically, let W denote the weights of the i-th group. Then, the vector-wise importance of group i is given by\n$I^{V} = |\\Delta L(D)|$\n$= L_{W\\backslash i}(D)-L_{W\\{i\\}}(D)$\n$= - \\frac{\\partial L_{W_{i}} (D)}{\\partial W_{i}} \\cdot W_{i}^{T} - \\frac{1}{2} W_{i}^{T} ( \\frac{\\partial ^{2} L_{W_{i}}(D)}{\\partial W_{i}^{2}} ) W_{i} + O (||W_{i}||^{3}),$ (1)\nwhere L is the next-token prediction loss, D is the training dataset, T represents the transpose of the matrix, H is the Hessian matrix of W. O (||Wi||3) denotes the high-order terms of Taylor expansion, which can be ignored because the redirection value is small and has little impact on the value of the importance. For the element-wise importance, let Wi denote the weights of each element within the weight matrix Wi. Then, the element-wise importance can be approximated by\n$I^{E} = |\\Delta L(D)|$\n$= L_{W\\backslash i}(D) - L_{W\\{i\\}}(D)$\n$= \\frac{1}{2N}\\sum_{j=1}^{N}(\\frac{\\partial L (D_{j})}{\\partial W_{i}})^{2} + O (\\frac{1}{N^{2}}).$ (2)\nwhere N is the number of data samples in the dataset D and Dj is the j-th data sample."}, {"title": "\u2022 Pruning Stage:", "content": "After finishing the importance estimation, the importance values of all groups (i.e., IV or IE) are sorted. The groups with lower importance values are removed based on a predefined pruning ratio."}, {"title": "\u2022 Fine-tuning Stage:", "content": "To mitigate the performance degradation caused by pruning, LoRA is adopted to fine-tune the pruned model using a small dataset [32]. Given the weight matrix W is approximated by two low-rank matrices P and Q, it follows\n$f(x) = (W+\\Delta W)x + b = (Wx + b) + (PQ)x,$ (3)\nwhere $\\Delta W = PQ$ and b is the bias term. By fine-tuning P and Q, we can obtain the pruned LLM with low computational complexity."}, {"title": "B. Challenges in LLM Pruning", "content": "Although the aforementioned methods can effectively prune LLMs with little performance degradation, they still face three key challenges:\n\u2022 Single metric evaluation. Existing LLM pruning methods mainly utilize a single metric to evaluate the importance of all groups. However, due to the complex interdependence of LLMs, the evaluation result may be inaccurate, thereby affecting the pruning performance.\n\u2022 Uniform pruning ratio. Most previous works adopt a uniform pruning ratio across all layers of LLMs, disregarding the distinct contributions of different structures. Such a straightforward approach may lead to unstable pruning performance when the pruning ratio is large.\n\u2022 High memory cost. Existing works typically utilize LORA for model fine-tuning. Nevertheless, LoRA uses 16-bit floating point numbers (FP16), which results in high memory cost and cannot be applied in resource-constrained scenarios.\nTo address these issues, we propose a novel pruning method, namely SAAP, to adaptively remove non-essential structures based on their importance without introducing significant computational and memory costs. SAAP uses an adaptive metric to prune unstable structures and employs group-wise fine-tuning to ensure the performance of the pruned LLM. In the following, we introduce our SAAP method in detail."}, {"title": "IV. METHOD", "content": "In this section, we first provide an overview of the SAAP method. Then, we elaborate on the detailed designs of SAAP, including the adaptive importance assessment approach and the efficient group-wise fine-tuning scheme."}, {"title": "A. Overview of SAAP", "content": "As illustrated in Fig. 2, SAAP follows the structured pruning process consisting of three stages, i.e., discovery stage, estimation stage, and recover stage. The discovery stage identifies all groups in the LLM, while the estimation stage and recover stage evaluate the importance of each group and restore the model performance, respectively. The key innovations of SAAP lie in two aspects:\n\u2022 SAAP introduces an adaptive stability indicator in the estimation stage to assess unstable and redundant components of the network. By combining both coarse-grained and fine-grained information, SAAP better captures the varying significance of different coupled structures and improves the accuracy of importance estimation.\n\u2022 Furthermore, SAAP extends its approach in the estimation stage by proposing an adaptive structure search strategy. This strategy evaluates the stability of importance scores across different structures, enabling a unified assessment that identifies and prunes unstable coupled structures more effectively.\n\u2022 SAAP employs an efficient group fine-tuning strategy in the recover stage, which maintains the accuracy of the pruned LLM without incurring much computational cost."}, {"title": "B. Adaptive Importance Assessment", "content": "As shown in Fig. 2, the estimation stage of SAAP comprises three components, including importance calculation, adaptive importance fusing, and adaptive structure search. The importance calculation is the same as that in LLM-pruner [18]. The adaptive importance fusion adaptively combines the coarse-grained and fine-grained information to evaluate the importance of each group. The adaptive structure search calculates a standard indicator of importance fluctuation to facilitate stable pruning LLMs."}, {"title": "a) Adaptive importance fusion.", "content": "In this work, we develop a multi-task loss function by maximizing the uncertainty in an equal variance Gaussian likelihood. Specifically, let F(Iw) denote the adaptive importance fusion metric with input weight matrix W. Then, for regression tasks, the output typically follows a Gaussian distribution. Thus, the probability distribution of the output y can be expressed as\n$P (y|F(I_{W})) = N (F(I_{W}), \\lambda^{2}),$ (4)\nwhere A represents the scalar noise. For classification tasks, we usually convert the model's output into a probability vector using the softmax function, i.e.,\n$P (y|F(I_{W})) = Softmax (F(I_{W})),$ (5)\nwhere Iw refers to the importance calculated in LLM-pruner. Given some sufficient statistics, we define the likelihood function that can be factorized over multiple outputs. Each output depends on the network's sufficient statistics F(Iw), as given by\n$\u0420 (\u0423_{1},..., \u0423_{\u043a}|F(I_{W}))$\n$= P (y_{1}|F(I_{W})),..., P (y_{k}|F(I_{W}))}.$ (6)\nIn the maximum likelihood estimation, we optimize model parameters by maximizing the logarithm of the likelihood function. The logarithm likelihood expression is given by\n$log P (y/F(I_{W})) \\propto -\\frac{1}{2\\lambda^{2}} ||y \u2013 F(I_{W})||^{2} - log \\lambda,$ (7)\nwhere A represents the observation noise parameter of the model, reflecting the amount of noise in the output. Our goal is to maximize the log-likelihood for model parameters W and noise parameters A. In the adaptive importance fusion metric task, the model's output consists of two vectors, y1 and y2, which represent importance outputs for vector-wise and element-wise in LLM-pruner, respectively. Both vectors follow a Gaussian distribution, i.e.,\n$(y_{1}, y_{2}| F(I_{W})) = P(y_{1}|F(I_{V})) \\cdot P (y_{2}|F(I_{E}))$\n$= N (y_{1}; F(I_{V}), \\lambda_{1}^{2}) \\cdot N (y_{2}; F(I_{E}), \\lambda_{2}^{2}) .$ (8)\nWe calculate the minimization objective of the model based on (8). The adaptive importance score Iada is calculated as\n$L_{ada} = -log P (y_{1}, y_{2}|F(I_{W}))$\n$= \\frac{1}{2\\lambda_{1}^{2}} ||y_{1} \u2013 F(I_{V})||^{2} + \\frac{1}{2\\lambda_{2}^{2}} ||y_{2} \u2013 F(I_{E})||^{2} + log \\lambda_{1}^{2}$\n$= \\frac{1}{2\\lambda_{1}^{2}} I^{V} + \\frac{1}{2\\lambda_{2}^{2}} I^{E} + log \\lambda_{1}^{2}$ (9)\nWe define IV as the coarse-grained importance score, denoted as ||y1 \u2013 F(IV)||2. Similarly, IE is defined as the"}, {"title": "b) Adaptive structure search.", "content": "Structured pruning is primarily based on \"layered pruning.\" However, different layers and modules have distinct behaviors, as shown in Fig. 3. Hence, it is hard to apply a unified pruning approach [35].\nTo address this challenge, we introduce the importance fluctuation indicator as a unified measure of importance calculated for each layer or module, i.e.,\n$M_{l,j} = \\frac{1}{D-1} \\sum_{d=1}^{D} ( ||W_{lj}||_{2}^{2} - I_{l}^{d}),$ (10)\nwhere Mi,j represents the proposed importance fluctuation indicator. ||Wlj|| denotes the squared norm of the weight coefficients for channel j in layer l. Ild is the adaptively fused importance score Iada obtained from previous calculations. Ild signifies the importance score for channel j in layer l under calibration samples of d, while IID represents the average importance score for channel j in layer l under calibration samples of D. Due to employing Bessel correction [36] for unbiased estimation, $\\frac{1}{D}$ is adopted.\nNext, we calculate the adaptive stability indicator, which captures relative changes and is suitable for the final unified search in structured pruning, i.e.,\n$\\hat{M}_{l,j} = \\frac{M_{l,j} - mean [M_{l,j}]}{\\sqrt{mean [M_{l,j} - mean [M_{l,j}]]^{2}}} ,$ (11)\nwhere mean [Mi,j] is the average value of Mi,j, with the denominator in the formula representing the calculation of standard deviation. Mij represents the adaptive stability indicator, which directly reflects the relative volatility of importance scores. Higher relative volatility indicates redundancy and instability within the entire model. Finally, based on the pruning ratio of the model, layers or modules with maximum relative volatility are removed to complete model pruning."}, {"title": "C. Efficient Group-Wise Fine-Tuning", "content": "In the recovery stage, we aim to quantify the pruned model weights to minimize GPU usage and ensure the fine-tuned weights remain quantized, thus improving computational deployment efficiency. QLoRA has recently achieved the first goal by quantifying the model weights from FP16 to NF4 during the fine-tuning stage. However, QLORA shares the same concept as LoRA. QLORA introduces matrices A and B, which are adjusted while keeping the model weights W unchanged, aiming for efficient fine-tuning. We define the size of W is Din \u00d7 Dout. Then, the post-fine-tuned weight W' can be represented as\n$W' = W + s \\cdot AB,$ (12)\nwhere s represents the adjustment parameter of the matrix. The dimensions of A and B are Din \u00d7 Dint and Dint \u00d7 Dout, respectively. Therefore, the dimension of AB is the same as W. However, it can be observed that after quantization, W' contains s \u00b7 AB, which will result in the final weight matrix W'. Although post-training quantization is possible, it may reduce model accuracy [36].\nTo address the aforementioned issues and combine and s \u00b7 AB without using FP16, we propose a grouped fine-tuning strategy. Each group's weights are independently quantized and adjusted during fine-tuning, as illustrated in Fig. 1. Grouped quantization enhances computational efficiency, simplifies deployment, and prevents the accuracy loss typically associated with post-training quantization.\nWe first divide each column of weight W into L groups, where L is set to be a divisor of the number of columns in W to ensure balanced grouping. By grouping the weight W, we can reduce the dimensionality of A from AB to L. We usually set $L\u226a D_{in}$, so the parameter count of decreases from Din \u00d7 Dint to L \u00d7 Dint.\nFor each group, we set a and b as the scaling factor and zero-point offset, respectively. Instead of quantizing each column of W, we use the scaling factor and zero-point offset for quantization, which are defined as\n$\\begin{cases}\n\u03b1 = \\frac{max(W) - min(W)}{2^{N} - 1}\nb = min(W),\n\\end{cases}$ (13)\nwhere N is the number of quantization bits, and we set N = 4 to use int4 for quantization. We use a and b to restore the quantized weights of each group to their original state, with the specific expression as\n$W_{l} = \u03b1_{g}(W_{g} - b_{g}),$ (14)\nwhere Wg represents the quantized weight of group g, ag and bg represent the scaling factor a and zero-point offset b of group g, respectively. Finally, the weights Wl adjusted by grouping are arranged back into the matrix in the original order to form a complete fine-tuned weight matrix W.\nBy introducing the grouping operation, we reduce the number of quantization parameters from (Din \u00d7 Dint + Dint \u00d7 Dout) to (L \u00d7 Dint + Dint \u00d7 Dout), and combine quantization and low rank well."}, {"title": "V. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Settings", "content": "Foundation LLMs. We first select four types of LLaMA [19] for experiments, including LLaMA-7B, LLaMA-13B, LLaMA-33B, and LLaMA-65B. These models represent a wide range of computational complexities and capacities, making them suitable for validating the scalability of the proposed SAAP method. Moreover, we conduct comparative analysis on five LLMs, including Vicuna-7B, Vicuna-13B [20], LLaMA2-7B [37], LLaMA2-13B, and LLaMA3-8B [38], demonstrating the versatility of SAAP across different model architectures.\nDatasets. To validate the effectiveness of SAAP, we conduct experiments on nine open-source datasets with two tasks of common sense reasoning and interactive understanding. The ARC Easy dataset and ARC Challenge dataset cover simple and complex scientific questions, respectively [39]. The BoolQ dataset [40] tests the model's ability to understand complex contexts and perform text extraction. The HellaSwag dataset [41] focuses on the model's capability to understand and reason in daily scenarios. The PIQA dataset [42] evaluates common sense reasoning, and the WinoGrande dataset [43] concentrates on common sense reasoning and contextual understanding. The OBQA dataset [44] aims to evaluate and enhance question-answering systems, testing the LLM's broad common sense and multi-step reasoning capabilities. Additionally, we test the zero-shot perplexity (PPL) on the PTB [45] and the WikiText2 [46] datasets.\nBaseline methods. We consider four baseline methods for comparative experiments: 1) LLM-pruner [18], which automatically calculates each group's contribution to model performance and performs effective pruning afterwards. 2) LoraPrune [47], which combines low-rank decomposition with pruning techniques, primarily reducing model parameters through low-rank approximation. 3) Wanda, which employs an importance metric based on weights and activation values to guide the pruning process. and 4) LoRAShear [23], which applies the half-space projected gradient (LHSPG) technique to gradually reduce the number of model parameters while preserving the model's ability to transfer knowledge.\nPerformance metrics. For classification tasks on datasets-ARC, BoolQ, HellaSwag, PIQA, WinoGrande, and OBQA, we utilize the classification accuracy as the performance metric. It is defined as the proportion of correct predictions made by LLMs and measures the generalization ability of LLMs in multi-domain tasks. For language modeling tasks on datasets-PTB and WikiText2, we use perplexity as the performance metric, showcasing the predictive ability of the model. Lower perplexity indicates more accurate next-word predictions by LLMs. We note that PPL is an important indicator for measuring model quality in sequential tasks. Additionally, the inference speed is measured by the number of tokens generated per second.\nImplementation details. Our experiments are conducted on CUDA 12.1 with HuggingFace 4.39.1 and PyTorch 2.2. The experimental platform is Ubuntu 20.04 equipped with two A100 GPUs, each with 80GB of memory. During pruning, we randomly select 50 samples (sequence length = 128) from the Bookcorpus dataset [48] as calibration samples. Moreover, we use the Alpaca dataset [49] in the recover stage, which contains 50k samples in total. We set the parameter L = 32 for efficient group-wise fine-tuning.\nWe note that the first three layers and the last layer of LLMs have a significant impact on the model performance. Therefore, we keep them fixed and only prune other layers. Taking LLaMA-7B as an example, if the overall pruning ratio is set to 20%, we increase the pruning ratio to 25% specifically for the fourth to 30th layers. In the recover stage, we set the learning rate to 1 \u00d7 10-4, the warming step to 1,000, and the batch size to 128. Besides, we use the AdamW optimizer in the experiment."}, {"title": "B. Performance Comparison with Baseline Methods", "content": "In the model pruning process, we use 50 randomly-selected samples from the Bookcorpus dataset [48] to estimate performance metrics in our method. We measure the post-pruning performance of the model through perplexity and average accuracy. Table I shows the performance comparison of our SAAP method with the four baseline methods at different pruning ratios under LLaMA-7B. The underline ('_') indicates the best performance achieved solely through pruning, while 'bold' denotes the best performance achieved through post-"}, {"title": "C. Generalization Experiments", "content": "We first conduct comparative experiments between the LLM-pruner and SAAP on the Vicuna-7B and LLaMA-13B models, with pruning ratios set to 20% and 50%, respectively. The results show that SAAP outperforms LLM-pruner at both pruning ratios. Compared with LLM-pruner, with a 20% pruning ratio, SAAP demonstrates significant advantages in both accuracy and inference speed. At a 50% pruning ratio, SAAP maintains high inference performance while keeping the model complexity low. The results of Vicuna-7B and LLaMA-13B on the PTB and Wikitext2 datasets at different pruning ratios are shown in Fig. 5, Table IV, and Table V.\nTo further test the generalization ability of SAAP, we conduct experiments not only on different parameter sizes of LLaMA and LLaMA2 but also on the latest LLaMA3-8B model. The results confirm that SAAP is effective not only on earlier versions of LLaMA but also on newer LLM architectures. The detailed results are shown in Table VII, Table VIII, and Table IX, which demonstrate the effectiveness of SAAP in different versions of LLaMA.\nWe also test SAAP on the Vicuna-7B and 13B models, with specific results displayed in Table IV and Table VI. These experiments demonstrate that SAAP achieves optimal results at both 20% and 50% pruning ratios, further confirming its applicability and generalizability across different LLM architectures and parameter scales."}, {"title": "D. Ablation Study", "content": "We perform an ablation study on SAAP's three main components: the adaptive importance assessment, the adaptive structure search, and efficient group-wise fine-tuning. Additionally, we evaluate the impact of varying the number of calibration samples."}, {"title": "1) Adaptive importance assessment.", "content": "The design of the importance estimation metric is crucial in determining which weights of an LLM are redundant and can be pruned without significantly degrading performance. The adaptive structure search part of our SAAP includes an innovative module, adaptive stability indicator, which integrates the comprehensive evaluation method of block importance judgment and volatility. We validate this approach through three distinct experimental methods.\n\u2022 Separate Cal: Use the original coarse-grained and fine-grained importance estimation methods, do not fuse their results, and calculate their relative fluctuations separately. By doing so, it serves as a baseline, allowing us to assess the impact of integrating these metrics.\n\u2022 Weighted Fusion: Simply weigh the importance of coarse-grained and fine-grained weights, and calculate the relative volatility of the weighted results. Instead of using the proposed adaptive importance fusion method, the results are directly calculated.\n\u2022 SAAP: The method proposed in this paper uses adaptive importance fusion metric.\nIn our experiment, we use the LLaMA-7B model with pruning ratios of 20% and 50%, respectively, and use the perplexity indicator on the WikiText2 dataset for evaluation. The final results are shown in Table X. From Table X, we can"}, {"title": "2) Adaptive structure search.", "content": "To unify the differences in importance scores of each layer and module and reduce the impact of layered pruning on model performance, we propose"}, {"title": "3) Efficient group-wise fine-tuning.", "content": "To verify the efficient group-wise fine-tuning, we replace this part with LoRA [32] and QLoRA [33] for testing. Table XII shows the statistics of the 7B model in our experiment, including parameter count, memory requirements, and tokens per second. We use a single RTX3090 to perform the above test on the wikitext2 test set. From Table XII, we can see that the proposed efficient group-wise fine-tuning can significantly improve the inference speed of the model."}, {"title": "4) Numbers of calibration samples.", "content": "We use 10, 30, and 50 calibration samples for experiments, where the calibration samples are randomly selected from Bookcorpus [46]. The experimental results are shown in Fig. 6. By adding some random calibration samples, the performance of the pruned model can also be improved."}, {"title": "E. Discussion", "content": "The SAAP method has demonstrated significant advantages in pruning LLMs. Through extensive testing on multiple LLMs, experimental results show that SAAP successfully reduces the number of model parameters, increases inference speed, and decreases memory usage, all while maintaining model performance.\nFirstly, the experimental results indicate that SAAP can maintain high model accuracy across different pruning ratios, especially at higher pruning ratios (50%), where SAAP exhibits superior performance compared to other baseline methods. This advantage is primarily attributed to the adaptive importance fusion metric and adaptive structure search strategies employed by SAAP, which more precisely identify and remove redundant structures while retaining critical components essential for model performance. However, it was also observed that as the pruning ratio increases further, SAAP, though still leading, experiences significant absolute performance loss.\nSecondly, while SAAP excels in reducing model complexity and enhancing inference efficiency, its performance on certain datasets is slightly lower than that of existing methods. This phenomenon may be related to the number of random samples used in the experiments. The sample size of random sampling may not fully represent the characteristics and complexity of the entire dataset, potentially leading to SAAP's failure to capture the dataset's diversity in some cases.\nMoreover, SAAP's success heavily relies on its efficient group-wise fine-tuning strategy, which not only boosts the model's inference speed but also achieves model quantization and low-rank decomposition without significantly compromising accuracy. However, this strategy might yield varying results across different model architectures, particularly in models with larger parameter scales."}, {"title": "VI. CONCLUSION", "content": "In this paper, we presented an efficient LLM pruning method called SAAP. It incorporated an adaptive importance metric in the estimation stage and used the importance fluctuation index as the evaluation criterion for adaptive structure search, thereby achieving effective pruning performance. In the recover stage, we developed a group-wise fine-tuning strategy to combine low rank and quantization efficiently. Through extensive experiments, we demonstrated the effectiveness of the proposed SAAP method, which achieved better inference quality and faster inference speed than several state-of-the-art baseline methods. Our work offered a novel perspective for LLM pruning, promising to achieve efficient and scalable LLM deployment in future intelligent applications."}]}