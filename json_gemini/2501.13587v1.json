{"title": "CONTRASTIVE REPRESENTATION LEARNING HELPS\nCROSS-INSTITUTIONAL KNOWLEDGE TRANSFER:\nA STUDY IN PEDIATRIC VENTILATION MANAGEMENT", "authors": ["Yuxuan (Edison) Liu", "Padmanabhan Ramnarayan", "Jinpei Han", "A. Aldo Faisal"], "abstract": "Clinical machine learning deployment across institutions faces significant challenges when patient\npopulations and clinical practices differ substantially. We present a systematic framework for cross-\ninstitutional knowledge transfer in clinical time series, demonstrated through pediatric ventilation\nmanagement between a general pediatric intensive care unit (PICU) and a cardiac-focused unit. Using\ncontrastive predictive coding (CPC) for representation learning, we investigate how different data\nregimes and fine-tuning strategies affect knowledge transfer across institutional boundaries. Our\nresults show that while direct model transfer performs poorly, CPC with appropriate fine-tuning\nenables effective knowledge sharing between institutions, with benefits particularly evident in limited\ndata scenarios. Analysis of transfer patterns reveals an important asymmetry: temporal progression\npatterns transfer more readily than point-of-care decisions, suggesting practical pathways for cross-\ninstitutional deployment. Through a systematic evaluation of fine-tuning approaches and transfer\npatterns, our work provides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from larger centers.", "sections": [{"title": "1 Introduction", "content": "Machine learning has shown promising results in clinical decision support, particularly for complex intensive care\nsettings [Gottesman et al., 2019]. However, developing robust models faces significant challenges: limited data\navailability, variations in clinical practices across institutions, and restricted data sharing. These constraints often result\nin models that perform well locally but fail to generalize across different clinical settings [McDermott et al., 2021].\nThis cross-site generalization problem represents a fundamental challenge in the real-world application of clinical ML,\nparticularly when dealing with longitudinal patient data in Electronic Healthcare Records (EHR).\n\nRecent advances in generative AI and large foundation models have demonstrated the power of self-supervised\nrepresentation learning in capturing transferable features from unlabeled data [Bommasani et al., 2021, Brown, 2020].\nThis capacity is particularly valuable for EHR applications, where obtaining high-quality labeled data is both costly and\nresource-intensive. Despite growing interest and successful applications of self-supervised learning to EHR time series\ndata [Rasmy et al., 2021, Tu et al., 2024, Wornow et al., 2023], downstream evaluations have largely been restricted to\nsingle-institution settings, where test data, though held out, still originates from the same underlying population as the"}, {"title": "2 Methods", "content": "This section presents a framework for cross-institutional knowledge transfer in clinical time series analysis. While\ndemonstrated through pediatric ventilation management, our methodology generalizes to other clinical scenarios where\ninstitutional differences create natural barriers to model generalization."}, {"title": "2.1 Problem Formulation", "content": "Let $D_s = \\{x^i, y^i\\}_{i=1}^{N_s}$ denote the source domain dataset with $N_s$ samples, where each $x \\in \\mathbb{R}^{T \\times D}$ represents a\nmultivariate time series with $T$ timesteps and $D$ features, and $y^i$ represents the corresponding labels. Similarly, let\n$D_t = \\{x^i, y^i\\}_{i=1}^{N_t}$ denote the target domain dataset.\n\nOur framework implements cross-institutional knowledge transfer through two phases (Fig. 2 (B)): representation\nlearning on source institution data (pre-training) and model adaptation for the target institution (fine-tuning). For\npre-training, we learn a representation function $f_{\\theta} : \\mathbb{R}^{T \\times D} \\rightarrow \\mathbb{R}^{H}$ that maps input time series to a $H$-dimensional\nlatent space, where $\\theta$ denotes the parameters of the encoder network. Following the setup in [McDermott et al., 2021],\nin the context of transfer learning between institutions, we consider three approaches for learning predictive models:\n\nTarget-Only: $\\hat{y} = h_{\\psi}(g_{\\phi}(x)), \\quad \\{\\phi, \\psi\\} \\subset \\Theta$\nFTF:\n$\\hat{y} = h_{\\psi}(f_{\\theta}(x)), \\quad \\{\\theta, \\psi\\} \\subset \\Theta$\nFTD:\n$\\hat{y} = h_{\\psi}(f_{\\theta}(x)), \\quad \\{\\psi\\} \\subset \\Theta, \\theta_f = \\Theta_f$ (1)\nwhere $g_{\\phi}$ represents the target-trained encoder, $h_{\\psi}$ denotes a task-specific decoder with parameters $\\psi$, and $f_{\\theta}$ is the\nsource-pretrained encoder. $\\Theta$ represents the set of trainable parameters, and $\\Theta_f$ denotes the fixed source-pretrained\nparameters. The target-only approach trains both components from scratch. In contrast, both FTF (Fine-Tuning Full)\nand FTD (Fine-Tuning Decoder-only) leverage the source-pretrained encoder: FTF updates all parameters during\ntraining, while FTD keeps the encoder fixed and only updates decoder parameters."}, {"title": "2.2 Study Design", "content": ""}, {"title": "2.2.1 Clinical Setting and Data Preparation", "content": "We analyze ventilation data from 2013-2022 across two PICUs: a general unit (1,883 episodes), and a cardiac-\nfocused unit (1,932 episodes, hereafter the target institution for our transfer learning approach). The institutions differ\nsubstantially in patient demographics and clinical characteristics: the target institution treats predominantly cardiac\npatients (84.8% vs 1.6%), serves younger patients (median age 5.0 vs 16.0 months), requires more intensive intervention\n(vasoactive support: 55.5% vs 20.4%), and has shorter ventilation durations (48.0 vs 80.0 hours). These distinct clinical\ncharacteristics serve as an exemplar case for investigating cross-institutional transfer learning methods.\n\nFor each ventilation episode, we collect 37 hourly-sampled clinical variables spanning demographics, vital signs,\nventilator parameters, laboratory values, and medications (see Appendix Table 4). Inclusion criteria required episodes\nusing conventional pressure control/support modes, lasting between 12 hours and 28 days. To maintain statistical\nindependence between samples, we analyze only the first extubation attempt for each admission, as outcomes of\nsubsequent attempts are inherently influenced by clinical decisions and physiological responses from previous attempts."}, {"title": "2.2.2 Prediction Tasks", "content": "We investigate two complementary ventilation weaning tasks that test different aspects of knowledge transfer:\n\n1.  Point-of-care Extubation Risk Assessment: At the time of planned ventilator removal, this task predicts\nthe probability of extubation failure, defined as requiring endotracheal tube reinsertion within 48 hours post-\nextubation [Thille et al., 2011]. Extubation failure occurs in approximately 10% of cases and represents a\npreventable adverse event requiring emergency intervention.\n2.  Prospective Extubation Window Identification: This task continuously assesses the probability of successful\nextubation over the subsequent 12-hour window. These windows, occurring in roughly 15% of monitoring\nperiods, identify the optimal timing for clinical evaluation of ventilation liberation.\n\nThe point-of-care assessment primarily challenges a model's ability to adapt to institution-specific risk thresholds, while\nwindow identification tests the transfer of learned temporal dynamics across different patient populations. Both tasks\npredict minority class events following standard statistical practice."}, {"title": "2.3 Pre-training with Contrastive Learning", "content": "Contrastive learning has been widely explored in clinical time series analysis, with various approaches targeting\ndifferent aspects of temporal data modeling [Krishnan et al., 2022, Liu et al., 2023]. Early methods such as Temporal\nNeighborhood Coding (TNC) [Tonekaboni et al., 2021] and SOM-CPC [Huijben et al., 2023] demonstrated success\nin learning representations from sensor-based data, including physiological signals (ECG, EEG) and human activity\nrecordings. These data types typically feature consistent sampling rates and clear temporal patterns: TNC leverages this\ntemporal regularity to capture progression patterns, while SOM-CPC combines contrastive learning with self-organizing\nmaps for interpretable visualization.\n\nEHR time series present additional challenges beyond sensor data, including irregular sampling, missing values,\nand complex interdependencies between variables. Contrastive Predictive Coding (CPC) [Oord et al., 2018], which\nconstructs representations by predicting future states while discriminating against unrelated samples, offers a natural\nframework for handling these challenges. Recent work [Bouchattaoui et al., 2024] has demonstrated CPC's effectiveness\non EHR data through its application in causal inference tasks. Given this proven adaptability, we adopt the standard"}, {"title": "2.3.1 CPC Architecture", "content": "Given an input sequence $x \\in \\mathbb{R}^{T \\times D}$ from the source domain, let $x_t \\in \\mathbb{R}^D$ denote its features at time step $t$. The\nCPC framework consists of two main components (omitting sample index $i$ for clarity): a feature extractor $f_{feat}$ that\nmaps each timestep to an intermediate representation $z_t = f_{feat}(x_t)$, and an autoregressive model $f_{ar}$ that aggregates\nthese representations $c_t = f_{ar}(z_{<t})$. Together, these components ($f_{feat}$ and $f_{ar}$) form the representation function $f_{\\theta}$\nreferenced in our problem formulation. Following [Oord et al., 2018], we optimize the InfoNCE loss that aims to\npredict $K$ future representations while contrasting against negative samples:\n\n$L_{InfoNCE} = -E_{x \\in D} \\left[ log \\frac{exp(T_k (c_t, z_{t+k}))}{\\sum_{z' \\in N} exp(T_k (c_t, z'))} \\right]$ (2)\n\nwhere $T_k (\\cdot, \\cdot)$ is a step-specific parameterized discriminator that measures compatibility between representations for\n$k$-step ahead prediction, and $N$ contains negative samples drawn from the same batch. After the pre-training, the\ntask-specific decoder $h_{\\psi}$ could be trained by minimizing the Cross-Entropy loss $L_{CE}$ on downstream tasks."}, {"title": "2.3.2 Clinical Domain Adaptation", "content": "Standard CPC employs random negative sampling, which implicitly assumes that all non-positive pairs provide equally\nvaluable contrasting information. However, in EHR time series, this assumption overlooks the inherent structure of\npatient trajectories - patients with similar clinical characteristics (e.g., age, comorbidities, mortality risk scores) may\nexhibit more similar temporal patterns. Random sampling might, therefore, select predominantly \u201ceasy\u201d negative\nexamples, failing to encourage the model to learn fine-grained discriminative features that separate clinically similar\ncases.\n\nWe incorporate this domain knowledge through a simple modification of the sampling strategy. Let $m(x_t, x'_t) \\rightarrow \\mathbb{R}$\nmeasure the clinical relevance between two sequences up to time $t$. The sampling probability for negative pairs is then\ndefined as:\n\n$p(z' | z_t) \\propto exp(\\beta \\cdot m(x_t, x'_t))$ (3)\n\nwhere $\\beta$ is a temperature parameter. This modifies the InfoNCE loss to:\n\n$L_{InfoNCE}^{guided} = -E_{x \\sim D} \\left[ log \\frac{exp(T_k (c_t, z_{t+k}))}{\\sum_{z' \\sim p(z'|z_t)} exp(T_k (c_t, z'))} \\right]$ (4)\n\nwhere negative samples are drawn according to our similarity-based distribution rather than uniformly, this targeted\nsampling helps guide the model in learning representations that capture meaningful clinical variations during contrastive\nlearning."}, {"title": "2.4 Implementation and Evaluation", "content": "For implementing the CPC framework, we adopt a Multilayer Perceptron (MLP) as feature extractor $f_{feat}$ and a Gated\nRecurrent Unit (GRU) as the autoregressive model $f_{ar}$. The similarity-guided sampling leverages time-to-extubation\n(TTE) as the proxy measure $m(\\cdot, \\cdot)$, motivated by the clinical observation that patients at similar stages of respiratory\nrecovery exhibit comparable physiological patterns.\n\nFollowing [McDermott et al., 2021], we implement a simpler architecture as a baseline during direct learning, where $f_{feat}$\nis replaced by a linear projection layer while maintaining the same GRU architecture. Both models utilize task-specific\nlinear heads $h_{\\psi}$ for downstream prediction.\n\nAll hyperparameters were tuned using grid search on the respective validation sets: source domain validation set for\npre-training and target domain validation set for fine-tuning. Detailed configurations are provided in Appendix Table 3.\nWe evaluate three approaches as defined in Sect. 2.1: direct training on target domain (Target-Only), full model\nfine-tuning (FTF), and decoder-only fine-tuning with fixed pre-trained representations (FTD). For the ablation study,"}, {"title": "3 Results & Discussion", "content": "Our experimental results demonstrate significant performance variations across transfer learning strategies and data\nregimes (Table 2). Direct application of source institution models to the target domain performs substantially lower\nthan models trained directly on target data (AUROC drops from 0.785 to 0.709 in Task 1 and 0.857 to 0.801 in Task 2,\nboth p < 0.01). This performance gap highlights a fundamental challenge in clinical ML deployment - the impact of\ninstitutional specialization on model generalization. The substantial degradation suggests that differences in patient\npopulations and clinical practices manifest as systematic shifts in physiological patterns rather than simple variations in\nfeature distributions.\n\nThe CPC framework with full model fine-tuning (CPC-FTF) effectively bridges this institutional divide, particularly\nin limited data scenarios. With only 5% data, CPC-FTF significantly outperforms target-only training in Task 1\n(AUROC 0.736 vs 0.716, p < 0.05) and maintains comparable performance in Task 2 (AUROC 0.797 vs 0.789), while\nachieving similar results to full data training in both tasks. This pattern suggests that CPC-based pre-training can learn\nrepresentations that transfer effectively across institutions while allowing for task-specific adaptation.\n\nAnalysis of fine-tuning strategies reveals deeper insights into the nature of transferable clinical knowledge. Decoder-only\nfine-tuning consistently underperforms full model fine-tuning across both tasks and all data regimes, with this gap most\npronounced in few-shot learning. When reducing from full to 5% target data, FTD degrades significantly (Task 1: 0.736\nvs 0.510, p < 0.001; Task 2: 0.797 vs 0.748, p < 0.05), while FTF maintains relatively robust performance. This\nsuggests that effective transfer requires both preserving general physiological patterns and adapting feature extractors to\ninstitution-specific variations, challenging the common practice of using pre-trained models as fixed feature extractors."}, {"title": "4 Conclusion", "content": "This work establishes a systematic framework for cross-institutional knowledge transfer in clinical time series, demon-\nstrated through pediatric ventilation management. Our results show that while direct model transfer is ineffective,\ncontrastive pre-training with appropriate fine-tuning enables robust knowledge sharing between institutions with distinct\npatient populations. The observed asymmetry in transfer success across prediction tasks - with temporal progression\npatterns transferring more readily than point-of-care decisions - provides important guidance for deploying clinical\ndecision support tools across institutions. Through systematic evaluation of fine-tuning strategies and transfer patterns,\nour work contributes to the broader goal of enabling reliable knowledge sharing across healthcare institutions while\nmaintaining their clinical autonomy."}]}