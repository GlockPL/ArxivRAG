{"title": "DCMAC: Demand-aware Customized Multi-Agent Communication via Upper Bound Training", "authors": ["Dongkun Huo", "Huateng Zhang", "Yixue Hao", "Yuanlin Ye", "Long Hu", "Rui Wang", "Min Chen"], "abstract": "Efficient communication can enhance the overall performance of collaborative multi-agent reinforcement learning. A common approach is to share observations through full communication, leading to significant communication overhead. Existing work attempts to perceive the global state by conducting teammate model based on local information. However, they ignore that the uncertainty generated by prediction may lead to difficult training. To address this problem, we propose a Demand-aware Customized Multi-Agent Communication (DCMAC) protocol, which use an upper bound training to obtain the ideal policy. By utilizing the demand parsing module, agent can interpret the gain of sending local message on teammate, and generate customized messages via compute the correlation between demands and local observation using cross-attention mechanism. Moreover, our method can adapt to the communication resources of agents and accelerate the training progress by appropriating the ideal policy which is trained with joint observation. Experimental results reveal that DCMAC significantly outperforms the baseline algorithms in both unconstrained and communication constrained scenarios.", "sections": [{"title": "1 Introduction", "content": "Collaborative Multi-Agent Reinforcement Learning (MARL) [1, 2, 3, 4] has achieved significant results in various fields, including traffic signal control, swarm robotics, and sensor networks. Compared to single-agent reinforcement learning, MARL has more complex problems to handle because the interaction between agents leads to non-stationarity in the environment. To avoid non-stationarity and achieve scalability, a centralized training and decentralized execution (CTDE) [5, 6] paradigm is commonly used to train multi-agent models. A centralized paradigm is used during training, and at the end of training the agents make decisions using the trained policies based only on their own local observations. This architecture can overcome the problems of environment non-stationarity and large-scale agents to some extent. Many approaches have been proposed based on this paradigm, such as MADDPG [7] and QMIX [8], which have shown excellent performance in multiple environments. In the CTDE framework, although the problem of non-stationarity can be mitigated by centralized training, there are still difficulties in the collaboration between the agents as each agent only has access to local observation during the execution process. In order to coordinate the agents, utilizing communication to interact information is a good way."}, {"title": "2 Problem Formulation", "content": "In this paper, we consider a fully cooperative MARL communication problem, which can be modeled as Decentralised Partially Observable Markov Decision Process (Dec-POMDP). We formulate Dec-POMDP with communication as a tuple < N, S, A, \u03a1, \u03a9, \u039f, R,\u03b3,C,D,G >, where N = {1,..., n} is the set of agents, S is the set of joint states space, A is the set of actions, \u03a9 is the set of observations, O is the observation function, R is the reward function, \u03b3 \u2208 [0, 1) stands for the discounted factor, C indicates the communication constraint function, D is the demand parsing function and G represents the customized message generator. At each time step, each agent i can acquire the observation or \u2208 \u03a9, which is generated by the observation function O(s, i) with s \u2208 S. Then, dij is computed by D(i, j), which denotes that agent i parses the demand of agent j. Agent i \u2208 N use G to encode its local observation o\u2081 and the demand dij parsed from tiny message miny. The message intended for other agent i is defined as mij = G(oi, dij), where i \u2260 j. We denote m.i as the set of messages received by agent i from other agents, and mi. as the set of messages sent by agent i to other agents. C limits the number of messages sent by agent i as count(mi.) < C(i). Prior to executing any actions, agents engage in communication with one another. Subsequently, each agent i follows its individual policy \u03c0i(ai|Ti, mi) to select an action ai \u2208 A, where Ti represents the history (,,..., -1, -1 , a\u00af\u00b9, o) of agent i up to the current time step t. The joint action a = (a1,\u2026, an) is executed in the environment resulting in next state s' = P(s's, a) and the global reward R(s, a). The formal objective is to find a joint policy \u03c0(\u03c4, a) to maximize the global value function Qot(t, a) = Es,a[\u2211t=07&R(s,a)|so = s, ao = a, \u03c0], with t = <T1,\u2026, \u03a4\u03b7). \u03a4\u03bf model the upper bound on the maximum return, we also define the ideal policy with full observability: \u03c0* = [\u03c0 (\u03b1\u0390|01,\u2026, On), Vi]."}, {"title": "3 Method", "content": "In this section, we elaborate on the design details of DCMAC. The primary concept of DCMAC is to parse teammate demands from tiny message. Agent will generate tiny message mtiny based on local historical observation and broadcast it at regular intervals. The receiver can parse teammate's demand from mtiny and generate customized messages by combining its own observations. To accelerate training efficiency, we propose the maximum return upper bound training paradigm, inspired by the concept of knowledge distillation, to align the target policy with the ideal policy."}, {"title": "3.1 Demand-aware Customized Multi-Agent Communication", "content": "We believe that providing feedback based on teammate demand can enhance collaboration among agents effectively. To achieve this, we designed three main modules which are tiny message generation module, teammate demand parsing module, and customized message generation module.\nThe existing work has pointed out that the dimension of the observation space is normally large in multi-agent training scenarios, and there exists redundant information in the raw messages [12]. Thus, we design the feature extraction module with self-attention mechanism, which can help agent i extract feature fi from observation of and minimize the influence of redundant information on training outcomes. Then, agent i can input fi into the GRU module to obtain historical observation hi.\nTo minimize the communication cost and assist agent to understand teammate demand, we design the tiny message generation module. Agent i is enable to generate lower dimensional tiny message mtiny by processing historical observations hi, which will be broadcast periodically. Additionally, the demand parsing module is designed to help agents in understanding teammate demands. Agent i is able to compute the demand of agent j, dij based on the received tiny message mji\nFurthermore, considering the continuity of messages and the scalability of the algorithm, we design the customized message generation module. Unlike the traditional methods that rely on input messages to broaden the agent's observation scope, our approach aims to prevent inaccuracies in parsing the global state due to message loss. Agent i can generate customized message mij that will bias the Q value of agent j based on teammate demand dij and hi. This method successfully mitigates the issue of anomalies in global state interpretation caused by message loss.\nMoreover, considering the limited communication resource, in order to reduce communication burden, we propose the link pruning function topk. Based on the communication resource constraint, agent can only send mi. to agents which have higher correlation. The correlation aij between agent i and agent j can be calculate with the hi and the teammates' demands dij by using cross-attention mechanism.\n$a_{ij} = softmax(x(W_q h_i)^T (W_k d_{ij}))$ (1)"}, {"title": "3.2 Maximum Return Upper Bound Train", "content": "Believing that the policy learned through global observation represents the ideal policy, i.e., \u03c0* (\u03b1\u03af 01, 02,\u2026, On) [14]. In this work, to accelerate training efficiency, the ideal policy is used as a guidance model to direct the target policy \u03c0 to align the ideal policy \u03c0*. Considering the excessive dimension of global state information, we design the global demand module to use the hi of team-mate agent i to parse teammate's demand dij and replicate the effects of global observation during training. Updating the global demand module necessitates a more reliable demand as a reference for computing the loss function. We posit that computing the teammate's demand using agent j's historical observation hj and selected action aj is a more credible approach. Therefore, we design the demand infer module to get the more credible demand din fer with hi and action aj, which is obtained from Qj of ideal policy \u03c0*.\nThen, we employ mutual information to design the demand loss function and update the global demand module. By using the conditional entropies H(dij|hj) and H(dij|hj, aj), we can compute the mutual information as follow:\n$I(d_{ij}, a_j| h_j) = H(d_{ij}| h_j) \u2013 H(d_{ij}| h_j, a_j)$ (4)\nBut it is difficult to compute the conditional distribution directly, since the variable h, d, a all follow unknown distribution. Based on the definition of mutual information and inspired by the method proposed in [15], we can derive a lower bound for mutual information:\n$I(d_{ij}, a_j| h_j) - \\sum_{i \\neq j} E_B [D_{KL}(p(d_{ij}| h_j)||q(d_{ij}| h_j, a_j)]$ (5)\nwhere the variables of distribution p and q are sampled from the replay buffer B, and DKL denotes the Kullback-Leibler divergence. Since aj and dij are not independent of each other, I(dij, aj|hj) is not less than the right side of equal sign. Then we can write the following loss function to reduce the difference of mutual information between de, and dinfer.\n$L_q(\\theta^q_d, \\theta_{infer}) = \\sum_{i \\neq j} E_B [D_{KL}(p(d^e_{ij}| h_j)||q(d^{infer}_{ij}| h_j, a_j))]$ (6)\nwhere 0 is all parameters of the global demand module, d; is calculated by global demand module, dinfer is calculated by demand infer module, af is obtained from Qj. To migrate the knowledge from the guidance model to the teammate demand module and the customized message generation module, we can get pe and use the d; as label to compute the mutual information with the dij obtained using mji\nntiny\n$L_a(\\theta_d, \\theta_g) = \\sum_{i \\neq j} E_B [D_{KL}(P_e(d_{ij}|m^{tiny}_{ji})||p(d^e_{ij}| h_j))]$ (7)\nSince the algorithm we study is based on value functions, we design the TD error loss function using the Q value of \u03c0* as the target Q. We define the formula as follow:\n$L_{TD}(\\theta^g) = E[(Q_{tot}(h, a^g;\\theta^g) \u2013 Q_{tot}(h, a; \\theta^t))^2]$ (8)\nwhere 09 are the parameters of the guidance network corresponding to the ideal policy \u03c0*, and \u03b8 defines the parameters of the neural network associated with the behavior policy \u03c0."}, {"title": "3.3 Overall Optimization Objective", "content": "As the DCMAC framework is implemented with the CTDE paradigm, in centralized training phase, the guidance network is updated by the standard TD loss in reinforcement learning as follows:\n$L_{RL}(\\theta^g) = E_{(s,a,r,s')~B}[(y \u2013 Q_{tot}(\\tau, a; \\theta^g))^2]$ (9)\nwhere y = r + maxa' Qtot(r', a'; 0\u00af) is the target, 0\u00af are parameters belong to the target network that is periodically updated, and Qtot is output of a mixing network such as VDN [16], QMIX [8], and QPLEX [17]. Together with the mentioned TD loss and two demand losses, the learning objective of DCMAC is:\n$L(\\theta) = L_{RL}(\\theta^g) + \\lambda_t L_{TD}(\\theta^*) + \\lambda_q L_q(\\theta_d, \\theta_{infer}) + \\lambda_a L_a(\\theta_d, \\theta_g)$ (10)\nwhere @ is all parameters in DCMAC, and At, 1 and la denote adjustable hyperparameters of the TD loss and two demand losses, respectively. In the decentralized execution phase, the guidance network and mixing network will not participate in the calculation. To prevent the lazy-agent problem [18] and facilitate scalability, we ensure that the local network-comprising the basic network, teammate demand module, tiny message generator and message generator-has same parameters for all agents."}, {"title": "4 Experiment", "content": "In this section, we evaluate the performance of DCMAC in three well-known multi-agent collaborative environment, i.e., Hallway [19], LBF [20] and SMAC [21], as shown in Fig. 3, and compare the experiments with the baseline algorithms, i.e., MAIC, NDQ, QMIX, and QPLEX. Specifically, first, based on the demand-aware customized message, our DCMAC algorithm outperforms the baseline models 4.1. Then we compare the win rates in the training phase to demonstrate the effectiveness of the upper bound training paradigm 4.2. Finally, we set up multi-level communication-constrained environments and conduct comparative experiments with algorithms that consider communication constraints 4.3.\nThe evaluation environments are Hallway, Level-Based Foraging and StarCraft II Multi-Agent Challenge (SMAC). The Hallway is a commonly used cooperative environment in which agents can only observe its own position and choose actions from moving left, moving right, or staying still. The primary objective in Hallway is for agents to learn optimal policies for navigation, considering the presence and actions of other agents. At the start of the game, n agents are randomly initialized at different positions, and then they will receive a reward only if they all reach the goal g simultaneously. The LBF is another MARL environment designed to study collaborative behaviors among agents. It focuses on scenarios where agents work together to achieve common goals, making it an excellent testbed for cooperative policies in MARL. Moreover, we applied the DCMAC algorithm to the SMAC benchmark. Our evaluation included two hard maps: 2c_vs_64zg and MMM2, as well as the super hard maps 3c_vs_100zg. In these maps, multiple agents are required to cooperate against enemy forces. Each agent can only observe local information and needs to cooperate with other agents to formulate strategies to defeat opponents. For evaluation, all results are reported on 3 random seeds.\nDetails about benchmarks, hyper-parameters and pseudo code of our method are all presented in Appendices 2, 3 and 4 respectively."}, {"title": "4.1 Communication Performance", "content": "We first compare DCMAC with different baseline algorithms to investigate communication efficiency. Since the Hallway and LBF are both sparse reward scenarios, agents need to perform multiple explorations and frequent communication for better collaboration. As shown in Fig. 4(a), in the Hallway scenario, QMIX is unable to obtain the state of its teammates, resulting in poor synergy and a severe loss of learning performance.Fig. 4(b) illustrates that in the LBF scenario, based on the help of hybrid network, QMIX can obtain the teammate information and the collaboration has a high improvement. MASIA algorithm can share the observation and help the agent to extract the teammate information. In scenarios like Hallway and LBF, where the observation space is small, there is less redundant information and the observations can be shared directly. Therfore, in Fig. 4(a) and Fig. 4(b), MASIA performs better than others. The uncertainty generated by predicting the teammate model may reduce the benefit from communication . Thus, MAIC performs worse than MASIA. Due to the complex neural network, DCMAC algorithm performance is in between, but it still can effectively improve the collaborative performance between agents.\nWe apply our method and baselines to SMAC. The results show that the algorithms eventually reach the ideal state. However, in the scenarios with large observation spaces like nc_vs_mzg, the observations should be encoded and compressed to extract features. Otherwise, it not only increase the communication overhead, but also much redundant information affects the training progress. Fig. 4(c), Fig. 4(d), Fig. 4(e) and Fig. 4(f) reveal that QMIX, QPLEX, and NDQ algorithms perform slightly poorly. MAIC predicts the teammate model, and the uncertainty generated by the prediction contributes to the exploration process. DCMAC also has this type of effect, when parsing tiny messages from the same agent, the demand parsed by different agents may differ, which is also beneficial in expanding exploration. At the same time, since the demands are parsed based on tiny messages, the exploration space is limited, reducing the impact of uncertainty on training. Thus, DCMAC works best. As shown in Fig. 4(g) and Fig. 4(h), even in the super hard maps, DCMAC shows better performance than others."}, {"title": "4.2 Guidance Model Performance", "content": "To verify the performance of the ideal policy, we compare the win rate during the training process. Fig. 5(a) and Fig. 5(b) illustrate that the guidance model of DCMAC performs slightly worse in Hallway and LBF scenarios. This is because its complex neural network structure converges slower than the other baselines in scenarios with smaller observation spaces. However, as show in Fig. 5(c), Fig. 5(d), Fig. 5(e) and Fig. 5(f), the guidance model of DCMAC outperforms the other compared algorithms not only converging faster but also obtaining higher win rates in hard maps. Moreover, in the super hard maps 3c_vs_100zg and MMM3, DCMAC shows excellent convergence (see Fig. 5(g) and Fig. 5(h)). This indicates that it is possible to train an ideal policy using joint observations, and also proves that the demand infer module plays an assisting role. In connection with the results in 4.1, the fact that DCMAC can show excellent results in the test scenario indicate that the guidance model \u03c0* plays a good guiding role."}, {"title": "4.3 Communication Performance with constraint", "content": "To verify the performance of DCMAC under constrained communication conditions, we conduct experiments in the MMM2 and 5m_vs_6m maps of SMAC and set three levels of communication constraints (i.e., 95%, 90% and 85%) to compare the experiments with MAIC under same constraints. As shown in Fig. 6, under the 95% communication constraint, the performance of DCMAC has almost no degradation and still maintains a high learning performance. The performance of DCMAC decreases slightly under the 90% communication constraint, but still manages to converge faster than MAIC under the 95% restriction, and the guidance model gets higher win rate in the end. At the 85% communication limit, DCMAC begins to show a significant decline, but guidance model still obtains higher win rate than MAIC in Train Mode. Theoretically, if the training continues, DCMAC will be better than MAIC in Test Mode. The experimental results reveal that DCMAC parse teammates' demands better, generate customized messages, and perform well in link pruning. Benefiting from the guidance of the ideal policy and the parsing of teammates' demands, the collaboration between the agents can still be maintained and the overall learning performance can be kept up even in the environments with poor communication conditions. In summary, DCMAC can effectively improve the overall performance of collaborative multi-agent reinforcement learning under communication constraints."}, {"title": "5 Related Work", "content": "Multi-agent Reinforcement Learning (MARL) has made significant progress in recent years. After the public announcement of the CTDE paradigm, many approaches have emerged and made significant progress. They can be broadly categorized into policy-based and value-based methods. Typical policy gradient methods include MADDPG [7], COMA [22], MAAC [23], MAPPO [24], FACMAC [25], and HAPPO [26], which aim at exploring the optimization of multi-agent policy gradient methods. Value-based methods focus on factorization of global value functions. VDN [16] sums each agent's Qi to Qtot in the centralization network. QMIX uses neural networks to change individual gains and team gains from simple summation assumptions of VDN to monotonicity constraints that are more generalizable.\nIn recent years, there has been a significant advancement in research on multi-agent communication methods [27]. Previous works can be divided into two categories. One focuses on limiting the amount of messages transmitted within the network. The ATOC [28], IC3 [29], and I2C [30], has utilized local gating mechanisms to dynamically trim communication links between agents, thus alleviating communication overhead. Nevertheless, the receiver should decode the message, and the lack of a message can result in erroneous interpretation, leaving open the question of whether these methods are effective in systems with severely restricted communication budgets. Conversely, approaches like DIAL and those based on vector quantization produce discrete messages directly, while NDQ [19] and TMC [31] also craft messages in a space-efficient manner. However, the expressive power of discrete messages may be curtailed by the communication budget imposed by broadcast communication schemes. Furthermore, methods such as ETC [32], VBC [33], and MBC[34] introduced event-triggered communication to decrease the frequency of communication and address communication constraints. These strategies aim to optimize the utilization of communication resources for enhanced performance by fine-tuning the timing of transmissions and allocating communication resources as needed.\nOn the contrary, other works focus on efficient learning to create meaningful messages or extract valuable information from messages. TarMAC [35], DICG[36], and DGN[37] have leveraged attention mechanisms and graph neural networks (GNNs) to enable agents to learn from local observations and broadcast messages to all accessible agents. Following these, subsequent approaches have proposed ways to enhance performance from both the sender's and receiver's perspectives. On the sender side, advancements have been made in message encoding methods, with algorithms like MAIC[13] and ToM2C[38] conducting teammate modeling to generate motivational messages tailored to the receiver's identity. On the receiver side, more refined aggregation schemes have been developed to make more efficient use of received messages, leading to algorithms such as G2A [12] and MASIA[10]. PMAC [39] constructs peer-to-peer communication graphs, designs personalized message sending and receiving methods, fully understands agents' state, and achieves efficient communication. CACOM [40] designs a context-aware communication based approach to maintain message continuity by using the LSQ method to differentiate the gating units. TEM [41] proposes a Transformer-based email mechanism (TEM) to solve the scalability problem of multi-agent communication.\nTo the best of our knowledge, the current research overlooks the impact of uncertainty resulting from teammate model conducting and the significance of teammate demand. Our approach enables agents to parse teammate demand and generate customized messages, thereby enhancing agent collaboration and overall algorithm performance."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we investigate enhancing the efficiency of collaborative multi-agent learning and propose a demand-aware customized multi-agent communication protocol, DCMAC. Previous work concentrated on overcoming the constraints of partial observations by extending agent perception range with shared messages or utilizing local information to predict the teammate model. The former approach leads to anomalies in parsing the global state as message loss occurs. The latter approach may generate uncertainty in the prediction process and raise training difficulty. Our approach enables the agent to obtain the basic information of teammates by broadcasting tiny messages. The demand parsing module in DCMAC can assist agent to parse the demand of teammate and then generate customized messages, thereby improving communication efficiency. In addition, we draw on the idea of knowledge distillation and use joint observations to train the ideal policy as a guidance model, and migrate the knowledge from the guidance model to the target policy by designing the corresponding loss function. We not only conduct multiple sets of experiments in various benchmarks, but also design communication-constrained scenarios to verify the effectiveness of DCMAC. Our approach is still at an early stage, and further refinement of the cohort model is necessary in the future. It will also be a meaningful work to consider the impact of message transmission delay on learning in communication environments."}]}