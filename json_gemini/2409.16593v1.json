{"title": "A Hybrid Quantum Neural Network for Split Learning", "authors": ["Hevish Cowlessur", "Chandra Thapa", "Tansu Alpcan", "Seyit Camtepe"], "abstract": "Quantum Machine Learning (QML) is an emerging field of research with potential applications to distributed collaborative learning, such as Split Learning (SL). SL allows resource-constrained clients to collaboratively train ML models with a server, reduce their computational overhead, and enable data privacy by avoiding raw data sharing. Although QML with SL has been studied, the problem remains open in resource-constrained environments where clients lack quantum computing capabilities. Additionally, data privacy leakage between client and server in SL poses risks of reconstruction attacks on the server side. To address these issues, we propose Hybrid Quantum Split Learning (HQSL), an application of Hybrid QML in SL. HQSL enables classical clients to train models with a hybrid quantum server and curtails reconstruction attacks. In addition, we introduce a novel qubit-efficient data-loading technique for designing a quantum layer in HQSL, minimizing both the number of qubits and circuit depth. Experiments on five datasets demonstrate HQSL's feasibility and ability to enhance classification performance compared to its classical models. Notably, HQSL achieves mean improvements of over 3% in both accuracy and F1-score for the Fashion-MNIST dataset, and over 1.5% in both metrics for the Speech Commands dataset. We expand these studies to include up to 100 clients, confirming HQSL's scalability. Moreover, we introduce a noise-based defense mechanism to tackle reconstruction attacks on the server side. Overall, HQSL enables classical clients to collaboratively train their models with a hybrid quantum server, leveraging quantum", "sections": [{"title": "1 Introduction", "content": "Quantum Machine Learning (QML) is a rapidly evolving research area at the intersection of quantum computing and Machine Learning (ML). A few examples of quantum analogs to some classical ML algorithms are Quantum Support Vector Machines (Li et al., 2015), Quantum Neural Networks (QNN) (Jia et al., 2019) and Quantum Principal Component Analysis (Lloyd et al., 2014). These leverage the unique properties of qubit systems, such as superposition and entanglement, to study the potential of improving ML tasks using quantum computing. While fault-tolerant quantum computers are still a few years away, a practically feasible route for addressing real-world problems in the Noisy-Intermediate Scale Quantum (NISQ) era (Preskill, 2018) is by adopting a hybrid approach. The hybrid approach integrates classical deep neural networks with the quantum layers, i.e., Hybrid Quantum Neural Network (HQNN) (Liang et al., 2021; Liu et al., 2021). However, there are few works on HQNN, and it remains open in various aspects, including a resource-constrained distributed learning environment such as ML in the Internet of Things domain.\nIn distributed collaborative learning, Split Learning (SL) (Vepakomma et al., 2018) is a popular technique in resource-constrained environments where clients have low computing power. The ML model is split into client and server components, where the client trains the initial few layers on local data, and then the server continues training the remaining layers. SL allows collaborative training without sharing raw data. In the quantum computing domain, Quantum Split Learning (QSL) splits a QNN to leverage SL advantages (Park et al., 2023). However, in resource-constrained NISQ era environments, it is crucial to allow clients (no quantum computing capabilities) to compute in the classical domain. This motivates HQNN in SL which has not been studied before. As SL can suffer from data privacy leakage, there is a risk of reconstruction attacks on the server side if the server is honest but curious (Joshi et al., 2022; Li et al., 2022). Moreover, this security aspect has not been investigated in HQNN with SL.\nWe present a novel (to the best of our knowledge, the first) application of HQNN to SL, which we call Hybrid Quantum Split Learning (HQSL), and study its security concerning data privacy leakage. In a simple HQSL setup, the HQNN model is divided into two parts: the client-side model, which has only classical neural network layers, and the server-side model, which consists of a quantum layer followed by classical neural network layers. The quantum layer consists of a quantum circuit that converts classical inputs into quantum states, performs quantum computations, and outputs"}, {"title": "2 Related Work", "content": "This section reviews relevant works on Hybrid QML, SL, and Quantum Split Learning. We discuss the need for combining classical and quantum computing in the NISQ era and describe the SL scheme and its study in the pure quantum domain.\nCombining Classical and Quantum Resources. Currently available quantum computers exhibit noise, due to numerous factors, e.g., qubit decoherence and gate"}, {"title": "3 Problem Statement", "content": "In this section, we define our research problem by breaking it down into specific research questions. This method enables us to systematically tackle the complexities of our study and offers a structured framework for our investigation.\nRQ 1: How can we leverage SL concepts to bring advantages of quantum computing to resource-constrained clients without quantum computing capabilities? - addressed in Sections 4, 5.\nApplying SL concepts in a pure QML model, e.g., splitting a QNN between a client and a server, assumes quantum computing capabilities on the client side. However, in resource-constrained environments, clients typically are resource-limited, e.g., IoT devices. Relaxing this assumption entails a client in the classical domain and a server equipped with quantum resources. The immediate sub-questions that arise are: (1) How can we model and train such a hybrid quantum split learning (HQSL) architecture? (2) How do we design a quantum circuit of low enough complexity for the quantum node/layer? (3) How can we model a classical counterpart to benchmark HQSL's classification performance? (4) Would HQSL scale as well as SL with an increasing number of clients?\nRQ 2: How do we strengthen such hybrid quantum split learning schemes against data privacy leakage and reconstruction attacks? - addressed in Section 6.\nData privacy leakage from intermediate data transfers between clients and a server is an overarching problem in SL, as highlighted in Section 2. It leads to private input data reconstruction attacks. In this work, we have the following sub-questions: (1) How do we model a threat scenario in which we can investigate HQSL's vulnerability to data privacy leakage and risks of reconstruction attacks? (2) Can we leverage the hybrid structure of HQSL to develop a countermeasure against reconstruction attacks? (3) How does this defense setup perform in the classical setting?"}, {"title": "4 Hybrid Quantum Split Learning (HQSL)", "content": "This section addresses our first research question, RQ 1, as identified in Section 3. We first discuss some preliminaries around gate-based quantum circuits and their components necessary to understand our implementation of HQSL. Then, we describe how we design our HQSL model architecture. We discuss the construction and selection process for the quantum circuit introduced as a quantum layer into the server-side model of HQSL. We also describe how we devise the classical counterpart of the quantum node to allow benchmarking of HQSL's performance. We then propose 2 variants of HQSL depending on the data type they operate on and explain the training algorithm. Lastly, we describe an algorithm to scale HQSL to accommodate multiple clients."}, {"title": "4.1 Gate-based Quantum Circuits", "content": "Before moving into the details of our HQSL model architecture, we first discuss the concept of gate-based quantum circuits, which are the fundamental components of quantum computing. Further background information on quantum computing can be found in Appendix A. Gate-based quantum circuits (or quantum circuits) use quantum gates to manipulate qubits and perform computations during circuit evolution. Encoding classical data into quantum states is a critical initial step, achieved through various encoding methods such as basis, amplitude, or phase encoding. We can describe the encoding process generically as follows: $\\Phi(X) = U_e(X)|0\\rangle^{\\otimes Q}$, where X is the classical data vector to encode, $|0\\rangle^{\\otimes Q}$ represents the initial quantum state of Q qubits, typically all initialized to |0\u3009, and $U_e$ represents the unitary transformation that encodes the classical vector X into a quantum state. Another critical part of a quantum circuit is called the ansatz, which is a predefined structure of quantum gates. The ansatz is parameterized by a set of k free parameters $\\Theta = {\\theta_1, \\theta_2, ..., \\theta_k}$ that are optimized during training, e.g., in QML applications. We can represent the quantum state, \u03a8(X, \u0398) at the end of the ansatz as follows: $\\Psi(X, \\Theta) = U_p(\\Theta)U_e(X)|0\\rangle^{\\otimes Q}$, where $U_p$ is the unitary representing the ansatz. Quantum measurement, the final step of the circuit, involves collapsing the quantum state into a classical outcome, providing the results of computation. We measure a certain observable, \u00c2, after each evolution of a quantum circuit and find the expectation value, E, of the measurement as\n$E = \\langle \\hat{A} \\rangle = \\langle \\Psi | \\hat{A} | \\Psi \\rangle$        (1)\nIn our work, we use the Pauli-Y observable, i.e., $\\hat{A} = \\sigma_y$, where a projective measurement at each qubit would generate \u00b11 which are the eigenvalues for the $\\sigma_y$ operator, for the qubit in the $y_+$ = $\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ i \\end{bmatrix}$ and $y_-$ = $\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -i \\end{bmatrix}$ states respectively. The experimental implementation of measurements of a Q-qubit quantum circuit consists of evolving the quantum algorithm M times (or shots) and computing the average for each qubit, $q \\in {1, . . ., Q}$, as an approximation of Eq. 1, decomposed for each q as,\n$e_q = \\langle \\hat{A}_q \\rangle = \\frac{M_{-1,q}}{M}  \\cdot (-1) + \\frac{M_{+1,q}}{M}  \\cdot (+1),$ (2)"}, {"title": "4.2 Hybrid Quantum Split Learning Model", "content": "While several SL configurations have been proposed (Vepakomma et al., 2018), this work considers the fundamental vanilla SL setup. In this setup, the labels and smashed data are transmitted to the server-side model at the split layer, while the raw data remain on the client side. During backpropagation, the gradients are transmitted from the server side to the client side across the split layer. In HQSL, the quantum layer is introduced as the first layer of the server-side model of SL. The quantum layer consists of a quantum circuit with classical features as input and output. The classical inputs are encoded to quantum states and processed by the quantum circuit. Expectation measurements of the resultant quantum states for each qubit lead to classical features output from the quantum layer (Eq. 3). These are then fed to the next classical layer. The quantum circuit is designed to be of practical importance in the near-term quantum computing era. The general structure of HQSL is shown in Fig. 1. Next, we discuss the construction and selection of the quantum circuit used across this work."}, {"title": "4.2.1 Construction and Selection of Quantum Circuit", "content": "To construct the quantum circuit in HQSL's quantum layer, we consider the limitations of current-generation quantum computers (see Appendix A for details). Specifically, the circuit width (number of qubits) and depth (number of gates per qubit) are critical considerations that need to be kept as small as possible. The presence or absence of entangling layers is also another important consideration. We trialed 6 different quantum circuit configurations by adjusting the number of qubits, entangling gates, and encoding type. Each configuration was tested by incorporating the circuit as a quantum layer in our HQSL model. The quantum circuit was selected based on the comparison of accuracy and F1-score between HQSL and its classical analogue. We chose the circuit that consistently outperformed SL in all our experiments. We provide the details of these experiments in Appendix C.\nThe quantum circuit with the optimal performance is shown as Circuit 6 in Fig. 2b. The circuit consists of 2 qubits with 3-dimensional classical input features ($X_1, X_2, X_3$) each encoded at 3 different points along each qubit using RX-gates. We drew inspiration from the work done by (P\u00e9rez-Salinas et al., 2020) on the data re-uploading mechanism to design this circuit for this work. The data re-uploading technique involves multiple repetitions of a layer, $L_i$, consisting of the generic single-qubit rotation gate, $U \\in SU(2)$. The U-gate is used in the encoding layer as U(X) and in the parameterized layer as U(\u0398\u1d62), where X = ($x_1$,$x_2$,$x_3$) and \u1d62 = (\u03b8\u2081,\u03b8\u2082,\u03b8\u2083), where \u1d62 represents the parameters for layer $L_i$. Thus, each layer can be represented as $L_i = U(X)U(\\Theta_i)$. $L_i$, for i = 0, . . ., N, is repeated giving rise to the data re-uploading mechanism, and the single-qubit classifier is introduced, given enough repeats, N, are performed.\nThe U-gate consists of parameters \u03a6 = (\u03c6\u2081,\u03c6\u2082,\u03c6\u2083) and can be expressed as U(\u03a6\u2081, \u03c6\u2082, \u03c6\u2083) \u2208 SU(2). We considered the following decomposition of the U-gate:\n$U(\\Phi) = U(\\phi_1, \\Phi_2, \\phi_3) = R_Z(\\phi_3)R_Y(\\Phi_2)R_Z(\\phi_1)$ (4)\nFor our work, we adapted the above decomposition of this layer, $L_i$, taking into consideration that the width and depth of a circuit should be kept as small as possible,"}, {"title": "4.2.2 HQSL Model Variants", "content": "We propose two variants of HQSL designed for multivariate binary and multi-class single-channel image dataset classification. Our approach involves constructing an HQNN and subsequently dividing it at the split point (see Fig. 6) so that the initial part of the HQNN represents the client side (classical neural network layers) and the remaining segment functions as the server side (hybrid quantum neural network layers).\nHQSL variant 1: The client-side model consists of 4 fully connected dense layers with ReLU activation functions with a 7-dimensional input and outputs smashed data of dimension 3. For the server-side model, we start with the quantum layer, constructed using our proposed quantum circuit in Section 4.2.1, which has an input dimension of 3 and an output dimension of 2. We concatenate 4 additional fully connected classical dense layers with the ReLU activation function (except the last layer, which uses Sigmoid) with an output dimension of 1 (for binary classification). We apply Dropout layers on the first two classical layers on the server-side model to improve regularization. For the classical counterpart of this variant, we replace the quantum layer with the classical dense layer as described earlier.\nHQSL variant 2: The client-side model consists of two Convolutional Neural Network filters, each with ReLU activation function and a MaxPool2d Layer, with a stride of 2 and kernel size of 2, followed by a flatten layer, and 2 fully connected layers to bring the feature dimensions down to 3. This is the client-side portion of the model. For the server side, we introduce our quantum circuit as a quantum layer and concatenate it with 3 more fully connected classical dense layers with an output dimension of n (for n-class classification). We apply the ReLU activation function on the first 2 classical layers on the server side. For the classical version, we replace the quantum layer with its equivalent classical dense layer, acting as a benchmark for this HQSL variant. Fig. 6 depicts the two HQSL model variants that we study in this work."}, {"title": "4.2.3 Hybrid Quantum Split Learning Model Training", "content": "For the classical layers of HQSL, gradients are computed from the loss function, L, using the classical backpropagation algorithm via the chain rule. However, it becomes non-trivial to do the same for the quantum layer. Schuld et al. devised the parameter-shift rule, which permits backpropagation through the quantum layer (Schuld et al., 2019). We briefly discuss the main ideas in the following.\nConsider a quantum circuit, $f_q : Z \\rightarrow E$ ($f_q : \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$), where fq is parameterized by a set of k free parameters, $\\Theta = {\\theta_1, \\theta_2,..., \\theta_k}$, and maps input, $Z = (z_1, z_2,...,z_m)$ to the measurement outputs, $E = (e_1, e_2,...,e_n)$. In HQSL, the quantum layer can be considered as being sandwiched between two sets of classical layers: the client-side model, $f_c : X \\rightarrow Z$ ($f_c : \\mathbb{R}^{in} \\rightarrow \\mathbb{R}^M$), and the server-side classical layers, $f_s : E \\leftrightarrow \\hat{Y}$ ($f_s : \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{out}$), where \u0176 is the output of HQSL.\nThe derivatives of the expectation value of the measurement, E, of the quantum circuit, fq with respect to the gate parameters, \u0398, are computed by evolving the circuit twice per parameter, with a (\u00b1) shift in that parameter. Representing the"}, {"title": "4.3 Hybrid Quantum Split Learning with Multiple Clients", "content": "Scaling HQSL to accommodate a larger number of clients enables multiple clients to leverage the quantum resources at the central server. The method we use to scale HQSL with K clients follows the basic round-robin protocol and is as follows. We initialize the model parameters of the client side and server side as $W^C$ and (\u0398,$W^S$) respectively. \u0398 corresponds to the quantum layer trainable gate parameters that lie on the server side. We randomly choose a client k ($k \\in {0,1,2,..., K-1}$) and train it in collaboration with the server. This consists of one round of forward and backward propagation and makes 1 local epoch for client k. The updated client k model weights $W^{C'}$ are then sent to the next client (k') that updates its model weights before another round of forward and backward propagation with the server. This marks the end of client k''s local epoch. After all K clients have been served in that 1 global epoch, we move to the next global epoch and resume training client k.\nThe differences due to the presence of the quantum layer in HQSL are as follows. A forward and backward propagation round consists of encoding classical smashed"}, {"title": "5 Experiments and Results", "content": "In this section, we empirically assess the feasibility and scalability of HQSL by comparing its performance in classification tasks to that of their corresponding SL models. We first outline our experiments and then present our results and discuss our findings. All our programs were written using Python 3.11.3 and PyTorch 2.1.0 libraries. We simulate the quantum part of our HQSL models using the Pennylane library with its PyTorch backend. The experiments were conducted on an NVIDIA GeForce RTX 2080 Ti GPU machine system.\nWe use five publicly available datasets in this work to test and validate our HQSL architectures. We summarize these datasets in Table 3. Detailed descriptions, including dataset processing methods, can be found in Appendix B. Setting a fixed seed, we split our datasets into five non-overlapping folds, in preparation for five-fold cross-validation for our experiments. Specifically, we use four of the folds for collectively training our model, and the remaining fold to test it. For the multi-class image datasets (MNIST and Fashion-MNIST), we split the datasets in a stratified manner to ensure an even and balanced distribution of classes."}, {"title": "5.1 Hybrid Quantum Split Learning versus Classical Split Learning: Single Client Experiments", "content": "In this section, we describe our experiments to determine HQSL's feasibility and performance by considering a single client-single server case. We then discuss the results of our experiment."}, {"title": "5.2 Hybrid Quantum Split Learning versus Classical Split Learning: Multiple-client Experiments", "content": "In this section, we experiment with HQSL with an increasing number of clients to determine its potential for accommodating multiple clients. We then discuss our empirical findings comparing HQSL and SL in a multiple-client setup.\nFirst, to assess the impact of the number of clients K on model accuracy and F1-score, we divide the training set into K independent and identically distributed (IID) subsets for K\u2208 {2, 3, 4, 5, 10, 20, 50, 100}. Each subset represents an IID distribution of the dataset assigned to each client, ensuring a balanced portion for each client. The training process follows Algorithm 2. The hyperparameters used are the same as in the case of a single client in Section 5.1. At the end of each global epoch, we evaluate the model on the testing set. We repeat this training process for 100 global epochs on the multivariate datasets (Botnet DGA and Breast Cancer) and 50 global epochs on the image datasets (MNIST, FMNIST, and Speech Commands spectrograms). The test results are then compared to those of their classical equivalent. The reported results include the test accuracy and Fl-score at the end of training comparing HQSL against SL. In this set of experiments, these tests are conducted for only one fold, and the test accuracies and Fl-scores are reported to evaluate the scalability of HQSL compared to SL."}, {"title": "6 Strengthening the Security of HQSL against Reconstruction Attacks", "content": "In this section, we address the second research question RQ 2 stated in Section 3. Specifically, we investigate the susceptibility of HQSL to reconstruction attacks using smashed data on the server side. First, we introduce the threat model, outlining the potential risks and vulnerabilities of HQSL to reconstruction attacks. Then, we present the reconstruction attack models we consider in this work to recreate a client's raw input data. These models are designed to exploit the smashed data on the server side. Considering the rotational properties of the encoding gates in our quantum layer, we propose a noise-based defense mechanism in the form of a Laplacian noise layer. This layer adds randomness to the smashed data before they are transmitted to the server-side model, making it more difficult for the reconstruction attack models to recreate private input raw data. Next, we conduct experiments under different noise parameters to extract the best-performing noise layer whereby HQSL has a clear advantage over SL in two key aspects: (i) impairing the performance of the reconstruction attack models and (ii) maintaining a high classification performance despite the presence of the noise defense layer. In our experiments, we investigate reconstruction"}, {"title": "6.1 Reconstruction Attack from Smashed Data in HQSL", "content": "In this section, we present our threat model that describes how an honest but curious server adversary can recreate the private input raw data from the smashed data transmitted by the client-side model. The adversary uses a reconstruction attack model to recreate the client's input data. Hence, we subsequently describe three such reconstruction attack models that can be employed by the adversary. We then describe how we design our proposed defense mechanism to mitigate the risk of reconstruction attacks in HQSL. This countermeasure entails the introduction of a Laplacian noise layer at the end of the client-side model, designed and configured considering the rotational properties of encoding gates in the server-side quantum layer."}, {"title": "6.1.1 Threat Model", "content": "In this work, we operate under the premise that we have a server that collaborates with multiple clients or data owners according to the SL setup. The clients do not share their private dataset, $D_{priv}$, with the server or other clients in the network. We assume that the server is honest but curious (semi-honest), i.e., the server does not deviate from the specified protocol instructions but attempts to infer information about the client's data. We assume that the server has access to a publicly available auxiliary dataset, $D_{aux}$, that has a similar distribution as $D_{priv}$, but $D_{priv}$ and $D_{aux}$ are non-overlapping disjoint datasets. The access to such a dataset is a common assumption made by previous works (Dougherty et al., 2023; Pasquini et al., 2021; Shokri et al., 2017). As explained in (Pasquini et al., 2021), the adversary can have access to a 'shadow' model, $f_{shadow}$, to generate outputs, $Z_{aux}$, in the same feature space as the smashed data, $Z_{priv}$, received from a client. The shadow model is trained on the auxiliary dataset, $D_{aux}$. The server then uses the pair ($Z_{aux} = f_{shadow}(X_{aux}), X_{aux}$), where $X_{aux} \\subset D_{aux}$ to train a reconstruction attack model, $f_{rec}$, that recreates $X_{aux}$.\nWith the trained reconstruction attack model, $f_{rec}$, and the smashed data, $Z_{priv}$, generated from the private dataset, $D_{priv}$, the honest-but-curious server reconstructs the private input of the clients. In this work, we assume that reconstruction attacks happen at deployment/inference time. This work does not address split learning's susceptibility to data privacy leakage and reconstruction attacks during the training phase."}, {"title": "6.1.2 Reconstruction Attack Mechanism", "content": "We consider three distinct reconstruction model architectures to reconstruct the private input data, $X_{priv} \\subset D_{priv}$, during a reconstruction attack using the smashed data, $Z_{priv}$. We train the reconstruction models on the auxiliary dataset, $D_{aux}$. During training, the inputs to these models are the smashed data, $Z_{aux}$, and the outputs are the reconstructed images, $X_{aux}$. These models are obtained from the literature, and we will assess the performance of our proposed defense mechanism (see Section 6.1.3) against these trained attack models on both HQSL and SL. Since the dimension of the smashed"}, {"title": "6.1.3 Encoding Gate-based Noise Layer Defense Mechanism", "content": "To strengthen the security of HQSL against the risk of reconstruction attacks, we propose a Laplacian noise layer defense mechanism at the end of the client-side model. Previous works have considered the use of a noise layer defense mechanism to introduce perturbation to the smashed data and hence, increase the difficulty of reconstruction by the proposed reconstruction models. However, such noise defense methods come with a privacy-utility trade-off (Titcombe et al., 2021; Na et al., 2024; Mireshghallah et al., 2020). In this paper, we study the rotational properties of quantum encoding gates to design the noise layer to (i) effectively hinder the reconstruction attack, and (ii) maintain a high classification performance by the server during inference. This, hence, addresses the privacy-utility trade-off associated with the noise layer defense mechanism. We empirically compare these two aspects for the hybrid (HQSL) and classical (SL) cases in Sections 6.2 and 6.3.\nLaplacian noise is favored over other types of noise, such as Gaussian noise, for privacy protection or data obfuscation. This is because noise sampled from a Laplacian distribution, $Laplace(\\mu, b)$, can be systematically added to the smashed data, $Z_{priv}$, to provide controlled randomness, while enabling the calculation of the level of perturbation introduced. This is important for maintaining privacy while retaining data utility. Laplacian noise is also commonly utilized in the context of differential privacy (Shokri and Shmatikov, 2015; Sarathy and Muralidhar, 2011). The location or mean parameter, \u03bc, provides a translational property to the data, while the scale parameter, b, indicates the variance or perturbation level caused by the noise. The scale parameter also determines the sharpness of the Laplacian distribution about its mean value as shown in Fig. 10a. The smaller the scale parameter, the sharper the Laplacian distribution is about its mean value, hence sampling values close to the mean.\nNoise Layer Design Based on Rotational Properties of RX-gates.\nAdding noise sampled from a Laplacian distribution, with appropriately tuned mean and scale parameters, has the potential to benefit the quantum circuit on the server side in terms of maintaining a high classification performance and impeding reconstruction of raw data due to the jitter added to the data. In Section 6.3, we will demonstrate how these are not possible for a server without a quantum layer. We consider the following to develop a better understanding of our selection of Laplacian noise parameters.\nThe Bloch sphere gives a geometrical representation of the pure state space of a qubit. Quantum gates, e.g., single-qubit rotation gates cause a qubit's state vector to move around the surface of the Bloch sphere as shown in Fig. 10b. In our proposed quantum circuit for HQSL (Fig. 2b), RX-gates have a period of rotation of 4\u03c0 and are responsible for encoding classical data to quantum states. These single-qubit rotation gates rotate the state vector of qubit around the X-axis of the Bloch sphere as per an angle parameter, \u03b8, equal to the numerical value of each classical input feature,"}, {"title": "6.2 Experiments on Encoding Gate-Based Noise Defense", "content": "In this section, we consider the single-client single-server split learning setup and describe our experiments to test and tune our defense mechanism with varying Laplacian noise layer parameters by (i) comparing the differences between reconstructed (Xinf) and original (Xinf) images in hybrid and classical settings and (ii) comparing the classification performances of HQSL and SL in the presence of the noise layer. From these experiments, we devise the optimal noise parameters that give HQSL a distinct"}, {"title": "6.3 Reconstruction Attacks Results and Discussion", "content": "In this section, we present and discuss our findings from the experiments carried out to test our proposed encoding gate-based noise defense layer as a countermeasure against reconstruction attacks. We evaluate the performance of the reconstruction model by using the metrics described earlier in 6.3.1. This is done in both classical and hybrid settings and for different Laplacian noise parameter values, \u03bc and b. Next, we present the inference time test accuracies and F1-scores of HQSL against SL across five folds by fixing the mean parameter, \u03bc, and varying the scale parameter, b, in 6.3.2. These experimental findings allow us to tune the noise parameters to give HQSL an advantage over SL in terms of protection against reconstruction attacks and inference time classification performance despite the presence of the noise layer."}, {"title": "6.3.1 Comparing the Difference between Original and Reconstructed Images under HQSL and SL Settings at Different Noise Levels", "content": "To evaluate the performance of the reconstruction model in classical and hybrid settings in the presence of Laplacian noise, we investigated the impact of noise mean, \u03bc, and scale, b, in recreating Fashion-MNIST, MNIST, and spectrogram images. Our comparative analysis focused on 4 key image comparison metrics: Mean Cosine Distance, Mean MSE, Mean Structural Dissimilarity Index (Mean DSSIM), and Mean Log Spectral Distance (Mean LSD). The results for the FMNIST, MNIST, and Speech Commands spectrograms datasets are depicted in Fig. 12a, 12b and 12c respectively. The baseline values (represented by the horizontal dashed lines) correspond to the noise-free implementation of the reconstruction attack, i.e., when the smashed data transmitted to the reconstruction model are free from Laplacian noise. In both the classical and hybrid settings, the baseline results are closer to 0 than when the noise layer is applied, showing the reconstruction attacks are successful in recreating the private inputs. These results, hence, underscore the need for the defense mechanism that we investigate in this section.\nThe general trends from Fig. 12a and 12b were that as we increased the mean parameter, \u03bc, the metric values were well above the baseline, irrespective of the noise scale parameter, b. Notably, on the MNIST dataset, the deviations from the baseline in the hybrid setting were more significant than in the classical setting for all 3 metrics. This shows that there was a larger difference between original and reconstructed images in the hybrid setting than in the classical setting. In the classical setting, the minimal deviations from the baseline indicate that a reconstruction attack can still be successful for the MNIST dataset, regardless of the noise parameters. This could reflect a model characteristic where the noise parameters do not perturb the foundational performance of the reconstruction model. We also highlight that at \u03bc = 4\u03c0, irrespective of the scale parameter value, b, the largest deviations were obtained, particularly in the hybrid setting, indicating that at this \u03bc value, the additive noise was successfully hindering the reconstruction performance of the adversary's model."}, {"title": "6.3.2 Inference Time Performance of HQSL and SL At Different Noise Levels", "content": "From Section 6.3.1, we established that when \u03bc = 2\u03c0 and 4\u03c0, we obtain large differences between original and reconstructed images indicated by the deviations above the baseline results. In this section, we investigate the effect of varying scale parameters, b, on the inference performance of HQSL and SL given mean values \u03bc = 2\u03c0 and 4\u03c0. We measure the performance by comparing the accuracy and F1-score in classifying FMNIST, MNIST, and Speech Commands spectrogram images with varying scale parameters, b. We present these results using the box plots shown in Fig. 13 and 14, representing the five-fold test accuracies and F1-scores in the presence and absence of noise. The baseline performances are represented by the Noise-free box plots.\nWe make the following deductions, backed by the experiments on all 3 datasets. The noise-free box plots representing the baseline performances showed a high accuracy and F1-score with low fluctuations across all folds in both HQSL and SL. The heights of the box plots indicate the fluctuations or variance of the accuracies and F1-score across all folds. By varying the Laplacian noise scale, b, and keeping the mean fixed at \u03bc = 2\u03c0 (Fig. 13) and \u03bc = 4\u03c0 (Fig. 14), we demonstrated that a small enough scale parameter value, e.g., b = 0.01, caused the performance of SL at inference time to drop and show high fluctuations in accuracy and F1-score. In contrast, HQSL maintained a high performance with much less fluctuations very similar to the noise-free performance. At both \u03bc = 2\u03c0 and \u03bc = 4\u03c0, when b = 0.1, HQSL still maintained a superior performance to SL but was slightly worse than when b = 0.01. This observation shows that when the scale parameter value increased, the performance of HQSL started getting affected by the Laplacian noise layer. As b increased above 0.1, HQSL's performance at inference time dropped even more. Hence, this experiment indicates that the scale value b should be kept close to 0. We, therefore, choose b = 0.01 as the desirable scale parameter value for the noise layer as it allows HQSL to maintain a high performance, similar to its noise-free baseline. These experimental findings support our analysis in Section 6.1.3, that the scale parameter value,"}, {"title": "7 Conclusion", "content": "This paper addresses the problem of applying Split Learning (SL) concepts to pure Quantum Machine Learning (QML) models for resource-constrained clients lacking quantum computing resources. We propose a novel Hybrid Quantum Split Learning", "parts": "the client side, consisting of a classical neural network, and the server side, comprising an HQNN with the first layer as a quantum layer consisting of a 2-qubit quantum circuit. For the quantum circuit, we introduce a qubit-efficient data-loading technique that allows us to keep the number of qubits and circuit depth small"}]}