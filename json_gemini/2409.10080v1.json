{"title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion", "authors": ["Yuchen Guo", "Ruoxiang Xu", "Rongcheng Li", "Zhenghao Wu", "Weifeng Su"], "abstract": "Multi-modality image fusion aims to integrate complementary data information from different imaging modalities into a single image. Existing methods often generate either blurry fused images that lose fine-grained semantic information or unnatural fused images that appear perceptually cropped from the inputs. In this work, we propose a novel two-phase discriminative autoencoder framework, termed DAE-Fuse, that generates sharp and natural fused images. In the adversarial feature extraction phase, we introduce two discriminative blocks into the encoder-decoder architecture, providing an additional adversarial loss to better guide feature extraction by reconstructing the source images. While the two discriminative blocks are adapted in the attention-guided cross-modality fusion phase to distinguish the structural differences between the fused output and the source inputs, injecting more naturalness into the results. Extensive experiments on public infrared-visible, medical image fusion, and downstream object detection datasets demonstrate our method's superiority and generalizability in both quantitative and qualitative evaluations.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Modality Image Fusion (MMIF), a hot image processing topic in the multimedia and low-level computer vision community, aims to render fused images that maintain the essential information of different modalities. This trait allows the fused images to describe a better visual understanding and also can be applied to subsequent high-level vision tasks, e.g., detection [1], [2], [3], and segmentation [4], [5]. In particular, the Infrared-Visible Image Fusion (IVIF) is a representative fusion task that has been widely applied [6], [7], [8], [9]. Infrared images effectively capture thermal targets in dark environments but lack texture details, which can hinder recognition in applications. On the contrary, visible images maintain most of the textual details but are sensitive to light conditions. The IVIF task aspires to combine the advantages of both images by fusing the thermal radiation information and texture details into a new image that can thoroughly describe the actual scene, improving the performance of various downstream tasks like Multi-Modality Object Detection (MMOD). The Medical Image Fusion (MIF) aims at combining information from various medical imaging modalities to generate a more comprehensive and detailed representation of anatomical structures that can help diagnosis and treatment [10].\nGAN-based models use adversarial learning with zero-sum games in a fused image and source images to fuse two inputs. The usual strategy in MMIF task is that totally two discriminators are employed to discriminate with the fusion results and the two source images [11], [12], [13], [1]. Most of them either fuse the two-dimensional image pairs before input to the model [14], [1], or did not tailor a feature extractor and corresponding loss function to distinctively extract features with different characteristics, weakening to feature extraction ability. Those methods just generate the fused image which is perceptually satisfactory by looking distributionally similar to the original data, but fail to preserve the feature details, resulting in the blurriness within and between functional objects.\nMore efficient pipelines take comprehensive feature extraction and reconstruction modules into an AE-based manner [15], [16], [17], [18], [19]. They separately encode the two input modalities and fuse the feature embeddings via channel concatenation, then decode the fused embeddings to an output image. By the manually elaborated encoder block and loss functions, the AE-based methods tend to effectively extract both global and local features from different modalities. Usually, the encoder and its loss are shared for both inputs [15], [17], [18], and they concatenate features directly rather than organically combining features from different modalities in the fusion phase. Therefore, bias between modalities may be introduced to the fused images, making the fused images present more obvious traces from the image of a specific modality but are left with inconspicuous information from another modality, which can be verified from the experiments. In order to solve the aforementioned problems, we developed a novel end-to-end discriminative autoencoder model for multi-modality image fusion (DAE-Fuse), which adopts a two-phase adversarial learning, and a cross-attention fusion module that endows the model with a more balanced fusion capability together with strong generalizability. Qualitative and quantitative experiments show that our model has achieved state-of-the-art on multiple IVIF public datasets, and the superiority can also be generalized to different MIF tasks. More importantly, our approach can boost the performance on downstream MMOD tasks without any fine-tuning."}, {"title": "II. METHOD", "content": "The multi-level features are extracted by shallow and deep encoders. Specifically, to differentiate the various frequencies features, we deploy a Deep High-frequency Encoder (DHE, termed $\\Phi_{DH}(\\cdot)$) and a Deep Low-frequency Encoder (DLE, termed $\\Phi_{DL}(\\cdot)$) parallelly following the Shallow Encoder (SE, termed $\\Phi_{s}(\\cdot)$). Suppose the embedding from the encoding process is marked as: $\\Phi(\\cdot)$, and the input of first and second modalities as: $\\alpha$, and $\\beta$. The encoding process of paired {$\\alpha$, $\\beta$}\ncan be formulated as:\n$\\Phi(\\alpha) = C[\\Phi_{DH}(\\Phi_{s}(a)), \\Phi_{DL}(\\Phi_{s}(\\alpha))]$\n$\\Phi(\\beta) = C[\\Phi_{DH}(\\Phi_{s}(\\beta)), \\Phi_{DL}(\\Phi_{s}(\\beta))]$\n(1)\nwhere $C( , )$ donates channel concatenate operation.\nSince the Transformer-based models are good at extracting low-frequency information while CNN-based models are sensitive to high-frequency information [20], [21]. We construct a Vision Transformer [22] for the DLE, and the DHE is implemented by a ResNet18 [23]. Restormer is a Channel-Transformer [24] architecture, which has achieved excellent performance in shallow region reconstruction task without increasing too much computation, so we use a channel-Transformer block for SE to extract shallow features. While the Reconstruction Decoder (RD, termed $P_R(\\cdot)$) is responsible for reconstructing the embeddings to image. RD shares the same architecture with SE. The decoding process of paired {$a$, $\\beta$} can be formulated as:\n$\\tilde{\\alpha} = P_R(\\alpha), \\tilde{\\beta} = P_R(\\beta)$\n(2)\nwhere $\\tilde{\\alpha}$ and $\\tilde{\\beta}$ represent the images $\\alpha$ and $\\beta$ after reconstructing, respectively.\nThe adversarial process is implemented by two discriminative blocks from different modalities (DM1 and DM2, termed $D_{M1}( , )$ and $D_{M2}( , )$ respectively). Discriminative blocks is implemented by a stack of Con2D-LeackyReLU-BatchNorm layers and a fully connected layer. Accordingly, the adversarial learning process can be formulated as minimizing the following adversarial objective:\n$\\min_{AE} \\max_{D_{M1},D_{M2}} (E[log(D_{M1}(a))] + E[log(D_{M2}(\\beta))]]$\n$+E[log(1 \u2013 (D_{M1}(\\tilde{a}))] + E[log(1 \u2013 (D_{M2}(\\tilde{\\beta}))])$$\n(3)"}, {"title": "B. Attention-guided Cross-modality Fusion Phase", "content": "In this phase, we developed a feature aggregation strategy by calculating the cross-attention weights, which is analogous to the standard attention of Transformer [22]. We use the same structure of discriminative blocks as before. During the adversarial fusion step, the inputs of a discriminative block are a fused image two source images.\n1) Early Fusion: Owing to the data gap between different modalities, current approaches in MMIF are limited to only incorporating element-wise additions for extracted feature embeddings, which does not capture the important interactions. We deploy a cross-modality attention module, making the different embeddinga can naturally interact another modality before fusion. After extracting features from encoders ($\\Phi(\\alpha)$, $\\Phi(\\beta)$), embeddings of images from two modalities are obtained. Here we use the embedding of $\\alpha$ as the Query Q, while the embeddings of $\\beta$ as the Key K and the Value V. Assuming the attention guided embeddigns are denoted as: $(\\Phi(\\alpha))$ and $(\\Phi(\\beta))$.\n2) Adversarial Fusion: First, the decoder generates fused image from attention-guided embeddings:\n$F(\\alpha, \\beta) = P_R[C(\\Phi(\\alpha), \\Phi(\\beta))]$\n(4)\nThen, the adversarial process is adapted to following formulation:\n$\\min_{AE} \\max_{D_{M1},D_{M2}} (E[log(D_{M1}(a))] + E[log(D_{M2}(\\beta))]+$\n$E[log(1- (D_{M1}(F(\\alpha, \\beta)))] + E[log(1 \u2013 (D_{M2}(F(\\alpha, \\beta)))])\n(5)"}, {"title": "C. Loss Function", "content": "1) Phase one: We construct the loss function for the autoencoder and discriminative blocks separately. The loss function of AE is divided into two parts: adversarial loss and content loss:\n$L_{AE}^{I} = \\lambda L_{AEadv}^{I} + o L_{correlation}^{I} +(1-o)L_{Deccontent}^{I}$\n(6)\nwhere the $o$ is the hyper-parameter. And the adversarial loss for encoder-decoder is:\n$L_{AEadv}^{I} = E[log(1-(D_{M1}(\\tilde{a}))]+E[log(1\u2212(D_{M2}(\\tilde{\\beta}))]$\n(7)\nAdditionally, we use correlation decomposition loss [16] for differentiate high-frequency feature and low-frequency feature:\n$L_{correlation}^{I} = \\frac{(CC(\\Phi_{DH}(\\alpha), \\Phi_{DH}(\\beta))2}{CC(\\Phi_{DL}(\\alpha), \\Phi_{DL}(\\beta)) + \\epsilon}$\n(8)\nwhere $\\epsilon$ set 1.01 to ensure the result always be positive.\nThe decoder reconstruction loss function consists of the square of the L2 norm and structural similarity index:\n$L_{Deccontent}^{I} = ||\\alpha \u2013 D(\\alpha)||_2 + (1 \u2013 SSIM(\\alpha, D(\\alpha)))$\n(9)\nThe adversarial loss of discriminative block DM1 and DM2 are of same structure. Take DM1 as an example:\n$L_{DM1adv}^{I} = E[-log(D_{M1}(a))] + E[-log(1 \u2013 (D_{M1}(\\tilde{a}))]$\n(10)\nAs a sum, the total discriminative block losses can be formulated in:\n$L_{DMadv}^{I} = L_{DM1adv}^{I} + L_{DM2adv}^{I}$\n(11)\nThe overall loss function of phase one is defined as:\n$L^{I} = L_{DMadv}^{I} + L_{AE}^{I}$\n(12)\n2) Phase two: Since the inputs of discriminative blocks have been changed, we represent the adversarial loss of phase two $L_{DMadv}^{II}$ follow the Eq. 5.\nThe content loss function for decoder in phase two can be formulated as:\n$L_{AE}^{II} = L_{text}^{II} + L_{int}^{II} + L_{AEadv}^{II}$\n(13)\nAnd the adversarial loss for the encoder-decoder is defined as:\n$L_{AEadv}^{II} = E[-log(1 \u2013 (D_{M1}(F(\\alpha, \\beta))))]$\n$+E[-log(1-(D_{M2}(F(\\alpha, \\beta))))]$\n(14)\nAlso, we use the structural content loss [25] as:\n$L_{text}^{II} = \\frac{1}{HW} ||\u2207I_f - max(|\u2207_a|, |\u2207_\u03b2|)||_1$\n(15)\n$L_{int}^{II} = \\frac{1}{HW} ||I_f - max(a, \u03b2)||_1$\n(16)\nThus, the whole losses in phase two can formulated as:\n$L^{II} = L_{DMadv}^{II} + L_{AE}^{II}$\n(17)"}, {"title": "III. EXPERIMENTS", "content": "Datasets and metrics: The IVIF experiments use three benchmarks: MSRS [26], RoadScene [27], and TNO [28], with only part of MSRS images used for training. MIF experiments utilize data from the Harvard 670 Medical Website [29] for testing.\nFor the MMOD [1] downstream task, the M3FD dataset, consisting of 4200 infrared-visible image pairs across six categories, is employed. IVIF and MIF tasks are evaluated using eight metrics: entropy (EN) [30], standard deviation (SD) [31], spatial frequency (SF) [32], visual information fidelity (VIF) [33], sum of correlation of differences (SCD) [32], mutual information (MI) [32], $Q^{AB/F}$ [32], and structural similarity index measure (SSIM) [34]. For MMOD, detection performance is measured by mAP@50% with higher values indicating better results.\nImplementation Details: Our experiments were implemented based on the PyTorch framework and performed on a server with an NVIDIA A100 GPU. In the MMOD downstream testing, the generated 4200 fused images is partitioned into training, validation, and test sets, with an 8:1:1 ratio for a YOLOv8n [35]."}, {"title": "B. Infrared-Visible Image Fusion", "content": "We tested our model on the three IVIF datasets and compared them with seven state-of-the-art methods including DIDFuse [15], U2Fusion [36], RFN-Nest [19], DDcGAN [14], TarDAL [1], CDDFuse [16] and DDFM [37].\nQualitative comparisons: As shown in Figure 2, the selected scenario highlights feature extraction and model bias. DDCGAN and TarDAL, both GAN-based models, exhibit noticeable blurriness and detail loss. DIDFuse, an AE-based method, shows a clear bias towards infrared images, darkening the sky and grass. Our DAE-Fuse, however, delivers the best results, preserving rich textural details and balancing both input modalities seamlessly. Notably, it maintains intact roof textures, unlike other methods. CDDFuse, which also uses a parallel encoder, partially retains roof details but overexposes some areas and introduces extra noise on the house wall. In contrast, DAE-Fuse fuses wall textures naturally from both inputs.\nQuantitative comparisons: Afterward, we used the seven metrices to quantitively compare the results with other models, which are displayed in Table I. DAE-Fuse shows an outstanding performance across all the measurement indices, demonstrating the effectiveness of our method."}, {"title": "C. Generalization on MIF tasks", "content": "To validate the generalizability of our DAE-Fuse, we used the same models in our IVIF testing to fuse medical images.\nQualitative comparisons: Figure 2 compares the MRI-CT fusion results of different models. Similar to IVIF experiments, GAN-based models appear fused but are blurry and detail-poor. AE-based methods generally perform better in feature extraction. However, models like U2fusion and RFN-Nest underweight CT images, leading to darker outlines, whereas DIDFuse emphasizes CT features. Our method integrates both images effectively, preserving all texture details without bias, demonstrating superior generalization.\nQuantitative comparisons: Similarly, seven metrics are adopted to quantitatively compare the result, which are displayed in Table I. DAE-Fuse has the best score on almost all metrics, indicating that out method can be generalized to MIF tasks without any adjustment, and suitable for various kinds of MIF tasks."}, {"title": "D. Downstream MMOD task", "content": "Multi-Modality Object Detection is an important downstream task of image fusion. A single modality image i.e., an individual infrared image usually lacks certain features of objects during the detection process. As shown in Figure 4, infrared images exhibit a robust capability for detecting humans but may overlook objects that do not produce thermal radiation. On the other hand, visible images struggle to recognize humans due to reflective lights from vehicles and lamps. After fusing the images from two modalities, by combining the advantages of two types of features, both humans and vehicles are well detected in the fusion image."}, {"title": "IV. CONCLUSION", "content": "In conclusion, DAE-Fuse overcomes limitations in image fusion, producing sharp and natural images through adversarial feature extraction and attention-guided fusion. Discriminative blocks in both phases enhance feature extraction and structural fidelity. Public dataset experiments demonstrate DAE-Fuse's superiority over existing methods."}]}