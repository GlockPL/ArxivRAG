{"title": "A Proposed Large Language Model-Based Smart Search for Archive System", "authors": ["Ha Dung Nguyen", "Thi-Hoang Anh Nguyen", "Thanh Binh Nguyen"], "abstract": "This study presents a novel framework for smart search in digital archival systems, leveraging the capabilities of Large Language Models (LLMs) to enhance information retrieval. By employing a Retrieval-Augmented Generation (RAG) approach, the framework enables the processing of natural language queries and transforming non-textual data into meaningful textual representations. The system integrates advanced metadata generation techniques, a hybrid retrieval mechanism, a router query engine, and robust response synthesis, the results proved search precision and relevance. We present the architecture and implementation of the system and evaluate its performance in four experiments concerning LLM efficiency, hybrid retrieval optimizations, multilingual query handling, and the impacts of individual components. Obtained results show significant improvements over conventional approaches and have demonstrated the potential of AI-powered systems to transform modern archival practices.", "sections": [{"title": "1 Introduction", "content": "In the information age, the proliferation of digital data has transformed the nature of archival systems from traditional, static repositories of physical documents to dynamic, interactive environments capable of storing, processing, and retrieving vast amounts of information. Digital archives encompass a variety of media types, including text, images, audio, and video, necessitating advanced methods for effective data management and retrieval. As users increasingly demand seamless access to diverse forms of information, traditional keyword-based search methodologies often fall short [3,8], leading to irrelevant results and an inability to grasp the nuances of user queries. To address these challenges, this study proposes a framework for smart search in archival systems that leverages the strengths of LLMs."}, {"title": "2 Backgrounds and Preliminaries", "content": ""}, {"title": "2.1 Archive System", "content": "Digital archives have gained prominence in many fields, reflecting the growing reliance on databases in the information age [12]. Traditional archives, rooted in print and paper-based systems, differ from digital archives, which encode texts, sounds, and images as data. This shift has transformed archives from static repositories of documents into dynamic systems that store, process, and circulate information [7]. Digital data derives value from its potential for future use, converting raw data into meaningful information through various processes [11]."}, {"title": "2.2 Search Features in Archive System", "content": ""}, {"title": "2.2.1 Traditional Search", "content": "Traditional systems of archiving depend on keyword-based or metadata-driven methods of searching. The user enters specific terms or selects metadata attributes, like title, author, date, or file type, and a set of matching records is returned. Most of these represent simple textual-matching algorithms which scan the database of the archive for records with specified keywords or metadata values. They'll be fairly effective within well-managed archives where metadata is consistently applied. In contrast, some of the limits to traditional search are irrelevant results, or missing the most important; understanding what queries and content mean; to handle complex queries."}, {"title": "2.2.2 LLM-based Search", "content": "LLM-based search enhances retrieval by leveraging Large Language Models to interpret context and query semantics, surpassing traditional methods that rely on physical metadata or user-generated descriptions. This approach offers semantic understanding, enabling better interpretation of user intent; facilitates cross-modal search by unifying descriptions across media types; and supports natural language interaction, making searches more intuitive and effective across formats."}, {"title": "2.3 Metadata in Archive System", "content": "Metadata is structured information that describes data in context, providing better usability of data through a bridge between it and the users. In an archive system, it plays a crucial role in discovery, management, and understanding. For its discovery, it aids in the locating of relevant assets by indexing and categorizing data attributes. In terms of management, metadata functions to systematically organize, preserve, and control digital assets. It finally enhances understanding by providing context, hence enabling the user to interpret data more easily.\nArchival metadata can be divided into three categories, which in many cases adhere to established guidelines such as the Dublin Core or the Europeana Data Model [15]. These categories include physical metadata, custom metadata, and AI-generated metadata. Physical metadata would contain technical specifications such as format, size, and creation date which allows sufficient system"}, {"title": "3 A Proposed Framework for Smart Search in Archive System", "content": ""}, {"title": "3.1 Knowledge Base Creation", "content": "Our system is similar to a RAG framework: the user provides a natural language query, and then the system retrieves information from its knowledge base and generates an answer. A knowledge base refers to structured information that a system uses while understanding queries and returning appropriate responses. Thus, the first critical step is building a robust knowledge base to support the system's operations."}, {"title": "3.1.1 Data Collection and Preparation", "content": "Various sources, including databases, external archives, and web scraping, are employed to gather relevant data tailored to user needs. The knowledge base comprises diverse data types, including text, images, audio, and video; however, all non-textual data is represented as uniform text to streamline processing. AI-generated content can effectively enrich this representation, with AI agents generating short descriptions for images, converting audio content into text, and producing descriptive summaries for videos by analyzing their visual and audio components. This approach enables the capture and storage of multimodal information efficiently, minimizing the storage requirements associated with actual non-textual files. Additionally, metadata, including physical attributes, domain-specific annotations, and AI-generated insights, plays a crucial role in organizing and enhancing the accessibility of the collected data."}, {"title": "3.1.2 Embedding Creation and Indexing", "content": "This process involves converting data into vector representations that capture the semantic meaning of the content. Once transformed, these embeddings are securely stored in a vector database, allowing for efficient similarity-based retrieval by comparing query vectors with the stored data vectors. To handle diverse data types and large-scale datasets, the system often divides data into smaller, semantically meaningful chunks. This structured indexing facilitates accurate retrieval, ensuring relevant responses to user queries.\nIn this study, we utilize the BGE-M3 embedding model [1] to generate high-quality vector representations. For indexing and retrieval, we employ Pinecone, a scalable vector database optimized for real-time updates and high-performance similarity searches. Other embedding models and vector databases could be used based on the specific user case and preference of the user."}, {"title": "3.2 The Architecture of the Proposed Smart Search System.", "content": "The framework presented (Figure 2) draws inspiration from various Retrieval-Augmented Generation (RAG) studies [2,5,13], meticulously crafting an architecture tailored for efficient multimedia data handling. It integrates robust data retrieval and response generation components. Novel solutions within this framework include hybrid retrieval techniques and query routing capabilities, ensuring precise multimedia data retrieval and the delivery of coherent, user-friendly responses."}, {"title": "3.2.1 Translator (Query and Response)", "content": "This component detects the language of user queries and translates non-English queries into English for processing. After generating a response, it is translated back into the original query language, ensuring users receive replies in their preferred language. We used the Detect Language API 5 for language detection and the Google Translator module from the deep-translator library 6 for translations."}, {"title": "3.2.2 Router Query Engine", "content": "The Router Query Engine is designed to handle different types of multimedia data by routing queries to the appropriate specialized query engine based on the input type. It integrates tools tailored for images, audio, video, and documents, each managed by its query engine. The"}, {"title": "3.2.3 Hybrid Retriever", "content": "A hybrid retriever, as illustrated in Figure 3, combines the strengths of traditional keyword-based retrieval methods, in our case BM25, with modern vector-based approaches. This model has been proven to enhance search accuracy by leveraging both keyword specificity and the semantic richness of vector embeddings [10, 14].\nA critical aspect of this integration is the alpha parameter (\u03b1), which plays a pivotal role in weighting the contributions of each method to the final retrieval score. As shown in Equation 1, alpha controls the balance between the BM25 score, which emphasizes exact keyword matches, and the semantic score derived from the vector embeddings that capture contextual meanings. When alpha is set closer to 0, the hybrid retriever prioritizes BM25, emphasizing precision in keyword matches. Conversely, when alpha approaches 1, the retriever leans towards vector-based retrieval, enhancing semantic generalization. By tuning alpha, the system can be adjusted to handle varying levels of query complexity, optimizing precision, recall, and overall retrieval effectiveness across diverse datasets.\nHybrid Score = \u03b1 \u22c5 Embedding Model Score + (1 \u2212 \u03b1) \u22c5 BM25 Score \t(1)"}, {"title": "3.2.4 Post-Processors", "content": "The post-processors component enhances retrieved results by employing two key techniques: reranking and long context reordering. Reranker 7 re-evaluates, scores, and filters retrieved nodes to prioritize the most relevant information [4,16], while long context reorder optimizes the handling of lengthy contexts by rearranging nodes to focus the language model's attention on key information [9]. This ensures that the response synthesizer receives refined and relevant results, leading to more accurate and informative responses."}, {"title": "3.2.5 Response Synthesizer", "content": "The response synthesizer effectively integrates results from the hybrid retriever and utilizes an LLM to generate comprehensive and structured responses. By applying a consistent prompt to each retrieved chunk, the synthesizer ensures a cohesive and coherent output. Furthermore, its ability to dynamically adapt the response format to match the specific data type guarantees that the information provided is not only accurate but also presented in a manner that is easily understandable and applicable. This adaptability is crucial for ensuring that the generated responses are not only informative but also contextually relevant and valuable to the user."}, {"title": "4 Experiments Settings", "content": "Our experiments focus on evaluating the proposed system's ability to accurately include relevant file IDs in the LLM-generated responses, rather than simply assessing raw retrieval performance. This simulates real-world scenarios where users expect clear references to specific files within the response content."}, {"title": "4.1 Experimental Dataset", "content": "To comprehensively assess the system across varied scenarios, three distinct types of queries were crafted to emulate file retrieval tasks from the archive, each designed to retrieve files based on their type(s) and specified topic:\nType 1: \"Recommend some {filetype} files about {topic}\" (Query a specific file type on a topic)\nType 2: \"Retrieve some {filetype1} or {filetype2} files about {topic}\" (Query two specific file types on a topic)\nType 3: \"Give me some files about {topic}\" (Query files with any type on a topic)\nUtilizing four file types (image, audio, video, text document), and ten topics, such as wildlife, landscapes, celebrities, and political events, we generated a total"}, {"title": "4.2 Experiment Design", "content": "Four key experiments were designed to assess various aspects of the RAG system's performance:\n1. Comparision of LLM performance: We compared the performance of different multilingual LLMs, all of which have 7 billion parameters: Mistral 7B [6], Synatra 7B 9, and Llama 2 7B [17]. The goal was to evaluate the quality of responses generated by each LLM and determine which model most effectively includes relevant file IDs in its outputs.\n2. Hybrid Retriever Adjustment: We evaluated a hybrid retrieval mechanism combining BM25 and Vector Retriever, using an alpha parameter to control their contribution. We tested alpha values from 0.0 to 1.0 to examine performance trade-offs.\n3. Multilingual Query Testing: This experiment evaluated the performance of Mistral, Synatra, and Llama 2 in handling English and Korean queries, assessing the system's ability to manage multilingual queries.\n4. Component Reduction: We conducted an ablation study to assess the impact of removing key components from the RAG system. By excluding the translator, query router, and post-processor components from the proposed platform, we evaluated the role of translation, query routing, and refinement in overall performance."}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the performance of our proposed system, we utilize four key metrics: (1) Precision - Proportion of retrieved files that are relevant, (2) Recall - Proportion of relevant files retrieved, (3) F1-score - Harmonic mean of precision and recall, and (4) Hit Rate - Percentage of queries with at least one relevant file retrieved.\nIt is important to note that in our research, the retrieved files are those referenced in the LLM's response text, rather than files directly retrieved from the underlying retrieval system, as illustrated in Figure 5."}, {"title": "5 Experiment Results and Analysis", "content": ""}, {"title": "5.1 Experiment 1: Comparision of LLM performance", "content": "As shown in Table 1, Mistral significantly outperformed Synatra and Llama 2 across key metrics. It achieved a high average precision of 80.56% and a strong balance between precision and recall. Synatra excelled in complex multi-filetype queries but struggled with recall in general queries. Llama 2 exhibited the weakest performance, with a low average precision and recall.\nFigure 6 shows that Synatra is the fastest LLM, significantly outperforming Mistral and Llama 2. Mistral has a moderate execution time, while Llama 2 is the slowest, particularly struggling with broader queries. These results suggest that Synatra is more efficient in generating responses, while Mistral offers a balance between execution time and performance, and Llama 2 may pose latency challenges."}, {"title": "5.2 Experiment 2: Hybrid Retriever Adjustment", "content": "As shown in Figure 7a, precision remained consistently high across the tested alpha values, reaching a peak of 83% with an alpha of 0.8. However, precision slightly decreased at alpha 1.0. The recall followed a similar trend, peaking around alpha 0.9, while the F1 score also achieved its maximum value at alpha 0.8, reaching 67.34%. The hit rate remained stable between 93% and 94%, showing minimal variation with different alpha values. Regarding execution time, depicted in Figure 7b, a clear pattern emerged: even alpha values tended to have lower execution times than odd alpha values.\nIn general, alpha values of 0.5 or 0.8 represent an optimal balance between retrieval accuracy and execution time. These settings effectively leverage the precision of BM25 and the contextual understanding of vector search, optimizing overall system performance."}, {"title": "5.3 Experiment 3: Multilingual Query Testing", "content": "As illustrated in Table 2, the performance difference between English and Korean queries is notable across all models. For Mistral, precision and F1 scores are higher for English queries (80.56% and 66.21%) compared to Korean (71.61% and 59.67%). Similarly, Synatra and Llama 2 exhibit better performance on English queries, with both models showing lower recall and F1 scores for Korean. This highlights a consistent drop in performance when processing Korean queries across all models. The proposed system handles multilingual queries but shows reduced performance for Korean, indicating a need for improvement in non-English language processing."}, {"title": "5.4 Experiment 4: Ablation Study", "content": "Table 3 highlights the importance of each component in our system. Removing the translator and query router significantly reduces performance. For example, removing the query router leads to a severe decline in F1 score by 38.73% and hit rate by 36.36%. While removing post-processors results in a modest increase in precision by 2.62%, it also increases execution time significantly, as depicted in Figure 8.\nGenerally, the translator and query router are crucial for ensuring high accuracy, while post-processors help mitigate latency. Consequently, each component plays a distinct role in the framework's overall performance, and their absence leads to substantial performance degradation."}, {"title": "6 Conclusions", "content": "The proposed framework represents a significant advancement in archival search technologies by effectively leveraging the capabilities of LLMs to process input and output in natural language. LLMs are also successfully utilized to convert non-textual data into textual form, thereby augmenting existing metadata for richer semantic content and more efficient data storage. Our experiments validate the framework's effectiveness in improving search precision and relevance, particularly through the innovative use of multilingual LLMs and hybrid retrieval mechanisms. In our study, Mistral emerged as the most robust LLM, achieving a precision of 80.56%. The ablation study underscores the critical roles of system components, revealing a precision drop of approximately 10-30% without the translator/router and a time performance increase of around 20 seconds without post-processors on average. These findings indicate that each component contributes distinctly to overall performance and efficiency. Moving forward, this research opens pathways for further innovations in AI-driven archival systems, suggesting avenues for enhancement in multilingual search and system scalability. By continuously refining these technologies, we can significantly enhance the accessibility and utility of digital archives, meeting the evolving needs of users in the digital age."}]}