{"title": "The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template", "authors": ["Alessandro Mantelero"], "abstract": "What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights.\nThe overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.", "sections": [{"title": "1. Introduction", "content": "The latest wave of AI development, beginning in the first decade of the 21st century, has raised serious concerns about the multifaceted implications of the various technologies under the AI umbrella, despite the significant positive opportunities they bring. As with steam power, automobiles and many other technologies, the early large-scale adoption of AI has once again posed 'the gift of the evil devil' dilemma, as shown by the current debate on the use of LLMs (Large Language Model) and the emergence of various critical issues, from 'hallucinations', to data protection and copyright infringement.\nThe introduction of a disruptive technology into society - capable of changing the paradigm of a wide range of human activities and influencing social behaviour in various way - usually generates a combination of high expectations and serious concerns. These concerns are not limited to the difficulty of changing the paradigm and facing the associated negative effects, such as on the labour market in the case of AI, but are largely related to issues affecting the early stages of any new technology.\nIt is not surprising that, faced with the uncertain scenario of the AI revolution and the difficulty of existing laws to provide specific solutions, the first response of policymakers was to expand regulatory systems and, in the absence of adequate provisions, to look for general ethical principles for AI. This approach led to a proliferation of ethics codes and guidelines developed by a variety of national and international public and private bodies.\nAlthough this first ethics-focused regulatory exercise led to a set of almost common core values, most with a significant legal dimension, their implementation can cover a wide range of options, making this principles-based approach too vague to adequately address the challenges of AI. On the one hand, this left the AI industry a wide margin of manoeuvre in aligning ethical values with their business interests. On the other hand, the general nature of these principles and their formulation, such as sustainable Al or human oversight, did not provide AI developers with operational inputs on how to implement them in concrete scenarios. In addition, the lack of a clear ethical framework to refer to meant that this set of guidelines was not as robust and grounded as it might have been.\nMore importantly, the ethical guidelines suggest a set of principles to be followed in AI development that seem to have little impact on companies. Some, such as Clearview AI and Open AI, have clearly violated not only ethical principles but also existing laws in order to bring their innovative products to market, reversing the model of responsible innovation. It is therefore not surprising that the initial regulatory approach has shifted from ethical guidance to risk-based regulation.\nIn a situation where the drive for innovation faces concerns about potential negative effects on a large scale, the logic of risk management seems to be the most appropriate way to strike a balance and define what risk is acceptable and to what extent. Moreover, it introduces an ex ante approach that makes it possible to prevent harmful applications from being placed on the market, rather than assessing ex post the correct behaviour of manufactures and deployers in complying with general principles.\nThe risk-based approach establishes procedural checks to be carried out, starting from the technology design phase and before the product is placed on the market. Although widely used in industrial regulation and mainly related to safety and security, this approach is also known and applied in the field of fundamental rights and, in a broader context, in relation to societal issues. A clear example of this is data protection legislation, which starting from earlier generations has followed a procedural approach focusing on the risks that each stage of data processing - from data collection to data erasure -"}, {"title": "2. The roots and nature of the FRIA", "content": "In order to design the FRIA and its implementation appropriately, it is crucial to place it in the broader context of impact assessment methodologies and practices. Although this is not the place to discuss risk management theories, it is important to stress that the FRIA cannot be designed without drawing on the methodological criteria adopted in this field. This is necessary not only in order to obtain scientifically accurate results, but also because the FRIA must be integrated with other risk assessment procedures that AI operators must comply with, starting with the Conformity Assessment required by the AI Act but not limited to this piece of legislation.\nWith regard to the specific area of fundamental rights, the experience acquired in two key types of assessment need to be specifically taken into account when designing the FRIA: the Privacy Impact Assessment (PIA)/Data Protection Impact Assessment (DPIA) and the Human Rights Impact Assessment (HRIA). To avoid repeating considerations expressed elsewhere on the limitations of PIA/DPIA in fully capturing the impact"}, {"title": "3. The AI Act: the impact on fundamental rights in FRIA and Conformity Assessment", "content": "With the key elements of the FRIA outlined above, it is now important to define the relationship between this type of assessment and the broad Conformity Assessment required by the AI Act. In fact, impact on fundamental rights is addressed not only in the FRIA, under Article 27, but is also one of the elements of the Conformity Assessment, and must also to be carried out when GPAI is used in the context of high-risk AI systems or general-purpose models that pose a systemic risk.\nWhen comparing FRIA and Conformity Assessment, it is worth noting that they follow the same procedure based on risk identification, analysis, and prevention/mitigation, as both are grounded on general risk management methodology.\nMoreover, the focus on fundamental rights is present in both models, as part of a broader assessment in the Conformity Assessment, and as a specific objective of the FRIA. The FRIA identifies a specific type of fundamental rights impact assessment, as defined in Article 27, which is an obligation only for AI deployers, but the general tool of fundamental rights impact assessment, rooted in the HRIA, is also part of the Conformity Assessment, under Article 9, as an obligation for AI providers.\nIn addition, the two assessments, that is the FRIA under Article 27 and the assessment of the impact on fundamental rights under the Conformity Assessment, are linked. The expertise of AI deployers can therefore reduce the burden for providers by contributing in the definition of risk management strategy. This is based on the consideration that the level of expertise of deployers in the implementation of the FRIA enables them to address some residual risks to fundamental rights, which are therefore not addressed in the Conformity Assessment by the providers, as they are left to the deployers' capacities.\nOn the other hand, the FRIA cannot be properly performed without information about the AI system from the AI provider. This information flow between provider and deployer is not properly framed in the AI Act. Article 27(2) states that the deployer (performing the FRIA) may \"in similar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried out by provider\" and several provisions \u2013 rather vague in their wording \u2013 add elements that can be interpreted as including the disclosure of the impact on fundamental rights as carried out by the provider in the context of the Conformity Assessment. However, a clear and specific disclosure obligation would have been more effective in terms of integrated risk management.\nFinally, according to Article 9(5), the outcome of the Conformity Assessment shall influence the design and development of the AI system to eliminate (more correctly prevent) or reduce relevant risks. Where risks cannot be prevented, appropriate mitigating measures and controls must be implemented, and risk information, in accordance with Article 13, and training must be provided to deployers. A similar range of measures can be considered in the case of FRIA, as risk management prioritises risk prevention, with mitigation considered only when prevention is not feasible.\nDespite these similarities, there are some differences between the two forms of assessment. The most important is the standards-based approach adopted by the EU legislator to develop the Conformity Assessment. Apart from criticisms related to standard setting in general, specific concerns relate to the nature of the impact on fundamental rights and the competence of standardisation bodies in dealing with fundamental rights.\nIf we consider standards as predefined procedures to be applied in every case or to a class of cases, it is clear that the variability of the impact of AI on fundamental rights cannot be captured in assessment standards. The following three parameters at least allow for the potential impact on fundamental rights to be different and require a case-by-case evaluation: (i) the specific characteristics of the technology used; (ii) the context of use; (iii) the categories of persons potentially affected.\nHowever, standards can also be intended as methodological approaches that outline how to address specific issues rather than providing checklists or pre-defined detailed procedures, as it is the case with standards for risk management systems. Thus, it is possible to establish a standard for fundamental rights impact assessment by outlining the key phases of the assessment, setting methodological requirements for each of them, and defining a common approach to risk measurement, as is described in the following sections.\nAgainst this background, standardisation bodies show a lack of expertise in the field of fundamental rights, as acknowledged by the European Commission. In addition, conflicts may arise between Conformity Assessment standards and the FRIA methodology, as Article 27 allows deployers to define their own methodological approach, as is the case in the field of data protection with the DPIA.\nExperience with the GDPR and Directive 95/46/CE shows it is possible to define best practices in impact assessment without necessarily establishing formal standards. Similarly, in HRIA there are no given standards, but rather different best practices that enrich the tools used to tackle a variety of risk scenarios and impacts.\nConcerned about the difficulties faced by deployers and providers in dealing with risk assessment, the EU legislator and the EU Commission look favourably on the use of standards. Ad hoc standards, if set for Conformity Assessment including the impact on fundamental rights, are likely to be replicated in the FRIA. This is even more likely given the narrow solution presented in Article 27(5), which mandates the AI Office to \"develop a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations under this Article in a simplified manner\": questionnaires or templates to assist deployers can be useful, but without reducing the FRIA to a mere questionnaire-based exercise, which is contrary to its nature.\nGiven the positive experience of the DPIA in data protection and the role of the bodies empowered by the AI Act to provide guidance and templates, another way forward is possible. Conformity Assessment standards should exclude assessment of impact on fundamental rights, while providers and deployers could develop it, following methodological guidance provided by the European Artificial Intelligence Board, the AI Office, and national competent authorities, but without formal standards. This will provide more flexibility and facilitate a contextual approach that will benefit AI providers and deployers in properly addressing their assessment obligations.\nWhatever strategy prevails, it will be necessary to develop a more robust methodological approach to assessing the impact of AI on fundamental rights. This study aims to contribute to this effort. In this respect, although a common methodology for this assessment in the FRIA and in the Conformity Assessment can be designed, its development under Article 9 in the context of Conformity Assessment may suffer from certain limitations.\nThe first major difference with the traditional approach adopted in HRIA/FRIA is the focus on the AI product alone. This overlooks the fact that these products operate in a context, and that this contextual dimension, especially with regard to impact on individuals, is crucial for risk prevention. Article 9(3), limiting the risks to those which \u201cmay be reasonably mitigated or eliminated through the development or design of the high-risk Al system, or the provision of adequate technical information\", does not recognise that AI systems are often socio-technical systems. The design to consider is therefore not only the design of the AI system, but also the design resulting from the interaction and mutual modification that these systems generate in society.\nThus it is not only the \"development or design of the high-risk AI system\" that is relevant but also the conditions of the context of use, which in some cases can also be appropriately modified to prevent risks, without changing the AI design. For example, there is a difference between the use of an Al decision support system by competent public authorities in the context of humanitarian emergencies and the use of the same system under normal conditions. The state of stress of all the people involved in the first scenario can exacerbate data quality, poor human-Al interaction, and biases.\nAnother example, which does not depend on the human contextual factor but on the technological factor, concerns access to health services based on screening programmes. In this case, although the AI system works correctly, its interaction with poor diagnostic instruments exacerbates false positives and negatives, again demonstrating that the risks associated with AI need to be assessed in context and not just in terms of the design of the system.\nGiven these considerations, it can be concluded that the three-layered risk assessment structure adopted by the AI Act, that is an assessment close to technological assessment, Conformity Assessment (Article 9), and FRIA (Article 27), requires methodological reflection in order to harmonise the risk management approach with regard to fundamental rights.\""}, {"title": "4. The FRIA as set out in Article 27 of the AI Act", "content": "The FRIA was introduced by the European Parliament during the legislative process of the AI Act as an obligation for AI deployers, a category added by the Parliament to bridge the gap between AI providers and end users, emphasising the role that deployers can actively play in the contextual use and customisations of Al systems. In line with general risk theory, the burden of risk management is therefore shared proportionally between AI providers and deployers, according to the actual risk introduced into society and their respective power to manage it.\nThe deployer's role relates to the relevant contextual component of risk management in the specific use of an AI system. Some risk elements, such as specific vulnerabilities of the individuals concerned, cannot be foreseen or properly managed by the provider at a general level. Against this background, the condition for risk management by the deployer is feasibility: the system should be sufficiently accessible and 'customisable' by the deployer and adequate risk information should have been made available by the provider.\nWhile the European Parliament's proposal included many key elements of FRIA, it did not outline the key parameters for assessment and did not emphasise the by-design approach that is common in technology regulation. However, the Parliament's proposal provided for a higher level of detail on FRIA compared to the final text of the AI Act, where some elements are implicit, due to a mistaken attempt to reduce the burden on deployers. These elements are the mitigation plan, consideration of vulnerability, and a clear description of the components of this assessment.\nAs discussed below, the lack of explicit reference to these and other key elements does not result in a kind of simplified version of the FRIA, as this would be contrary to the inherent nature of this type of assessment, would not provide the adequate level of protection of the fundamental rights enshrined in the EU Charter of Fundamental Rights, and would diverge significantly from best international practice in this field. In this respect, the FRIA can be effective only if carried out as it should be in its complete form, and the decision not to include details of important components of the FRIA in the final text only has the effect of complicating compliance with the AI Act by deployers.\nAnother important part of the European Parliament's proposal concerned the role of participation in impact assessment. Unfortunately, although EU institutions emphasise participation in many contexts, it seems difficult to translate this attitude into legal obligations when it comes to the digital society. As in the GDPR, the AI Act does not give due attention to participation in assessment procedures, contrary to best practices in impact assessment.\nHowever, the main difference between the European Parliament's proposal and the adopted text concerns the scope of the FRIA. Under pressure from the other two co-legislators, it was restricted to a limited area, whereas the text proposed by the Parliament referred to all high-risk AI systems as defined in Article 6(2), with the sole exception of systems used for management and operation of critical infrastructure.\nThe final text maintains this exception but significantly narrows the general scope of the FRIA, which now only covers (i) deployers that are \"bodies governed by public law\" and \"private entities providing public services\", and (ii) AI systems used to evaluate the creditworthiness of natural persons or for credit scoring (with the exception of AI systems used for the detection of financial fraud), and for risk assessment and pricing in life and health insurance.\nAlthough this narrow scope of the FRIA is less satisfactory from the perspective of the protection of fundamental rights and creates an imbalance between the general obligation of providers to assess the impact on fundamental rights of all high-risk Al systems under the Conformity Assessment procedure and the specific obligation of deployers, it does not prevent the adoption of a broader use of this instrument based on the obligation to protect fundamental rights established at EU and national level, and facilitating the accountability of AI operators in this respect.\nLooking at the adopted text, the first paragraph of Article 27 places the FRIA, as defined by the AI Act, in the general HRIA/FRIA tradition, but in the form of a prior assessment, and no longer as a mere policy tool to respond to criticisms raised.\nGiven the nature of fundamental rights and the level of protection afforded to them by the Charter of Fundamental Rights of the European Union and national constitutional charters, this assessment must necessarily avoid prejudice to them. This means that the FRIA cannot merely be a final check with no influence on AI design. On the contrary, potential impacts must be properly addressed in order to meet the obligations to protect fundamental rights.\nDue to the link between potential risk and system design, it is recommended that this assessment be performed from the early stages of definition of the deployer's strategy for the use of a given Al system. It should also be repeated whenever changes are made to the system's deployment.\nAnother important element in framing the FRIA, set in the first paragraph, is its objective, which is to assess \"the impact on fundamental rights that the use of such system may produce\". The focus is on the impact. This is a broad notion encompassing any kind of restriction or prejudice to fundamental rights.\nSimilar conclusions can be drawn from Article 27(1)(f), which refers to the materialization of a risk and the measures to be taken in the event of this happening. This provision must be interpreted in the light of the wording of this article and its consistency with risk management theory, as follows.\nFirst, the materialization of these risks referred to in Article 27(1)(f) is distinct from the materialisation of harm and ex post remedies. It therefore lies in the realm of risk assessment and management. When a deployer designs a specific use of an Al system, the risk is introduced into society and materialises at that moment, giving rise to the obligation to take appropriate measures to address it. The emergence of specific elements in the design of AI deployment that may have a negative impact on fundamental rights therefore constitutes the materialisation of the risk. Using an a contrario argument, it would not make sense in the risk management-oriented AI Act to impose an obligation to define \"the measures to be taken in the case of materialisation of those risks\", considering materialisation as the actual occurrence of harm, but not taking appropriate measures when the risk is foreseen before it causes actual harm. Contrary to the preventive nature of the risk management approach, the response to risk would only be in the form of remedies.\nSecond, the FRIA is qualified by the AI Act as an instrument to be adopted \"prior to deploying a high-risk AI system\". This excludes an ex post approach. In the same vein, the reference in Article 27(1) to the assessment of the impact that the use of an AI system \"may produce\" and the likelihood criterion (\"categories of natural persons and groups likely to be affected\" and \"risks of harm likely to have an impact\") highlight the preventive and predictive nature of the analysis to be carried out.\nArticle 27's purpose, as revealed during legislative debate, and systemic interpretation, the AI Act's risk-based approach focusing on an ex ante instrument, and the general theory of FRIA/HRIA, lead to the conclusion that the FRIA is a prognostic assessment and management exercise, similar to the DPIA.\nIn line with HRIA and risk management in general, the FRIA must include at least (i) a planning and scoping phase, focusing on the main characteristics of the product/service and the context in which it will be placed; (ii) a data collection and risk analysis phase, identifying potential risks and estimating their potential impact on fundamental rights ; (iii) a risk management phase, adopting appropriate measures to prevent or mitigate these risks and testing their effectiveness.\nThese various components are examined in the following section. But it is worth noting that the FRIA cannot be reduced to a mere descriptive exercise, in which potentially affected rights are outlined in general terms and some measures are proposed, without any evidence of the link between these two in terms of the adequacy and effectiveness of the measures in reducing the estimated levels of risk.\nThe questionnaire to be developed by the AI Office under Article 27 (5) may therefore assist deployers in fulfilling certain obligations of the FRIA, particularly in the planning and scoping phases, as well as some aspects of data collection during the assessment phase. However, a purely questionnaire-based approach, and even worse its automation (which requires a high degree of uniformity), cannot fully capture the contextual nature of the FRIA and needs to be integrated with a methodology for risk quantification and management.\nAs regards the life cycle of AI systems, the AI Act adopts for FRIA the circular iterative approach common in risk assessment, where the measures adopted to manage risks need to be revised according to the technological and contextual changes. This is clear in the procedural aspects described in Article 27(2), although the reference to updating information can be misleading on a quick reading, as the update concerns the impact assessment, as confirmed by Recital 96.\nWith regard to the availability of the assessment results, as mentioned above, the EU legislator departs from the traditional approach to impact assessment based on transparency. Some relief is provided by the obligation to notify the results of the assessment to the market surveillance authority; this may increase the level of commitment by deployers compared to the experience with the DPIA in the GDPR, where any control over the impact assessment is left to the initiative of the Supervisory Authorities or arises in the event of legal action.\nFinally, regarding the level of enforcement of FRIA obligations provided for by the AI Act, the Act does not introduce specific administrative fines, leaving it to the Member States to establish them in accordance with Article 99. This may pose a significant risk of lack of effective protection of fundamental rights, such as has happened in some countries with regard to the exemption of the application of GDPR sanctions to the public sector. Unfortunately such a restriction could also be based on Article 99(8) of the AI Act and would undermine the impact of the FRIA, which includes the bodies governed by public law as one of its main targets.\nHowever, given the importance of fundamental rights, the penalties for providers of high-risk AI systems that do not comply with their obligations (Articles 99(4)(a) and 16), which include an assessment of the impact on fundamental rights, and the distribution of risk management obligations between AI providers and deployers provided for in Article 27, it would be inconsistent with the purpose of the AI Act and the level of protection granted to fundamental rights by the EU legal system if Article 27 were not adequately accompanied by effective, proportionate and dissuasive sanctions."}, {"title": "5. From obligations to methodology: key elements for a model template", "content": "Although fundamental rights impact assessment is not a new approach to risk management in rights protection, its implementation in the specific field of AI, with the peculiarities involved, is a recent development. A first contribution to this was made by The Danish Institute for Human Rights in 2020. In addition, the broad echo of the debate on the ethics of AI has also led a number of actors to develop core principles to be implemented in their AI systems in various ways, mainly through awareness-raising questionnaires.\nIt is only in recent years that more attention has been paid to risk-based methodologies, partially as a result of the shift from a purely ethical approach to Al regulation. The latter has its roots in product safety regulation, centred on a risk-based approach, risk assessment and risk management methodologies. However, this approach is not new in the human rights field, where human rights due diligence based on HRIA has been at the core of the UN Guiding Principles on Business and Human Rights for many years.\nThis distinction between awareness-rising models and risk-based models for fundamental rights impact assessment, where the latter are fewer and still at developmental stage, refers to the main characteristics of the models considered, since a combination of elements from these different models is often present. Thus, questionnaires and an aim to raise awareness of the possible consequences of AI are common to all models, as is segmentation of the AI process to take account of the specificities of the different stages in terms of risk management. Finally, elements of risk quantification are also included in the models that do not properly develop a risk assessment centred on levels of risk.\nAll these models take a procedural approach because of the need to identify, assess and mitigate potential risks. However, when defining the FRIA methodology, the importance of the contextual dimension must be considered. In this respect, the key question concerns the need to use an AI-based system rather than alternative possible solutions.\nAn example of this is provided by the Algorithmic Impact Assessment model developed by the Government of Canada, which in its section on Reasons for Automation asks whether alternative non-automated processes were considered with the following two sub-questions: If non-automated processes were considered, why was automation identified as the preferred option? What would be the consequence of not using the system?\nIn terms of the procedural approach to FRIA, asking this key question at the beginning of the model, immediately after the general description of the AI system, is crucial to assessing the need for the use of AI in light of its potential benefits and risks. The same question should be repeated at the end of the impact assessment in order to evaluate whether, on the basis of the potential impacts envisaged, alternative solutions that do not rely on Al entail a lower risk while achieving the same results.\nAs mentioned above, the three main blocks of the FRIA methodology to be reflected in a model template are: (i) planning and scoping, and risk identification, (ii) risk analysis, and (iii) risk management. The different types of models are considered in the following subsections and the contribution they can make to the design of each of these blocks of FRIA methodology is examined. While the procedural approach is common to the whole FRIA, the awareness-raising models can contribute more to understanding how to design the planning and scoping and risk identification phase of FRIA, and the models more centred on risk analysis and quantification can be used to develop the risk analysis and management phase.", "subsections": [{"title": "5.1. First FRIA block: awareness raising and risk identification", "content": "In line with the structure of Article 27, the first objective of the assessment should focus on contextualising the AI system and its use, and identifying potentially affected categories (rightsholders). Given the place of FRIA obligations in the broader context of existing EU law and fundamental rights principles, this planning and scoping phase must first address the legal acceptability of the AI option.\nTwo main areas need to be explored at this stage: the inherent dimension of the AI system and the contextual dimension. Both contribute to analysing the problem, considering alternative solutions, and, if AI is the best option, defining the goal of the AI system.\nWith regard to the inherent dimension of the AI system, awareness-raising aims to investigate (i) the specific needs to be addressed; (ii) the relevance of adopting an AI-based approach; (iii) the role of the AI-based solution in addressing the identified needs. It will therefore also cover the description of the nature of the AI systems, including data flows and data processing purposes.\nIn this respect, a possible model for this phase has been developed by the Dutch Ministry of the Interior and Kingdom Relations. It uses a questionnaire-based approach, listing many different questions at this awareness-raising phase and combining them with a scenario-based analysis, where the questions are addressed in the light of possible alternative scenarios.\nIt is worth noting that this Dutch model includes some requirements in the planning and scoping phase, such as the existence of a legal basis that \"explicitly and clearly\" allows for the use of an algorithm and \"renders this use sufficiently foreseeable\", that are neither required by the AI Act nor necessary from a general perspective of fundamental rights protection. There is also a reference to the values driving the decision to use AI. This is important, but operates at a different level of societal impact.\nIn terms of methodology, the Dutch model adopts a Why-What-How approach, which can be of help in the decision-making process (why do we need AI? What AI? How will it work?), but does not fully fit with the logic of FRIA. The Why-What-How approach primarily concerns decisions related to needs and product features, but impact on fundamental rights runs through all three stages and even covers elements distributed over all three. In this respect, the most common risk management design is not linear through the Why-What-How stages, but circular, centred on planning/scoping, analysis and management.\nAnother weakness of this model in defining the inherent dimension of the AI solution concerns the consideration of several design elements in their general dimension, such as product features, biases and data quality, and data security. While the focus on these aspects is relevant in terms of raising awareness, it lacks a specific link to potentially affected rights in the model proposed, as should be the case in a FRIA model.\nWith regard to the contextual dimension of Al solutions to be explored in the awareness raising stage, given the nature of FRIA, it is not limited to the identification of potentially affected rights and rightsholders (without quantifying the impact, which is the objective of the following phase), but it also includes a preliminary analysis of the relevant elements of the specific fundamental rights systems. These need to be properly contextualised according to the way in which they are shaped by legal provisions and case law, including with different nuances at national level. This also comprises the obligations and legal requirements already in place that mitigate or prevent potential risks (for example, the GDPR's provisions on data quality in relation to the data used to fine-tune A\u0399).\nIn addition, the context of use is relevant in terms of the interaction between the AI system and the socio-technical environment in which it operates. This interaction relates to both the characteristics of the system and the characteristics of the surrounding environment, where the latter may be more difficult to control and modify through risk management.\nTo deal with the concrete design and contextual use of an AI system, and to identify potentially impacted rights, various approaches are possible and have been established in technology risk assessment over the decades. These include brainstorming analysis, knowledge-based approaches based on past cases, failure models, checklists and various other methods. However, in the field of AI-related risk assessment, these methods are relatively unexplored and the models proposed tend to rely on checklists.\nAt the end of this first phase of the assessment, the FRIA should provide a first outline of the AI-related risk, starting from the needs analysis and the description of the system, then going on to consider the contextual scenario of fundamental rights (including the checks already in place) and the potentially impacted areas. With regard to the latter, it is important to focus on the impact on rights deriving from criticalities in the functioning of AI. In this regard, for example, the Algorithmic Impact Assessment (AIA) Tool developed by the Government of Canada evaluates the impact that the adoption of an automated decision may have in general on the rights or freedoms of individuals. However, in FRIA, the focus is not on the general effects of AI-driven decisions (for example, an automated decision regarding free access to health services has in itself an impact on the right to health and its exercise), but on the impact of prejudicial decisions resulting from problems in the design, context, and functioning of the algorithm, such as denial of access to health services due to biases that create an illegitimate prejudice to this right.\nBased on the analysis carried out in this section, best practices in developing this first phase of the FRIA should emphasise the relevance of the fundamental question of alternatives to AI, which could also be addressed using a SWOT (strengths, weaknesses, opportunities, and threats) analysis, and should consider the limitations that affect generic checklists, which should be combined with appropriate contextualisation provided by the experts carrying out the FRIA. In designing a contextualised checklist inspiration can be drawn from the existing models, but remembering their limitations and not excluding other methods of analysis.\nWith a view to establishing common best practices for FRIA, existing checklists can be used as input for this phase, but it is difficult to see the need to formalise them into a comprehensive and standardised set of questions. A different conclusion can be drawn regarding the importance of outlining the key areas to be considered in the planning and scoping phase, both in general and for sector-specific AI applications. In this respect, some common elements relating to the characteristics of the AI system and its scope, as well as the socio-technical context of use, need to be considered in all evaluations in this first phase.\nIn the same way, some sector-specific issues \u2013 such as the nature and value of doctor-patient interaction in medicine, or the evolving nature of case law with respect to path-dependent AI systems to be used in the judicial domain - should be included amongst the key elements that characterise the dynamics of each sector under consideration. However, this task does not necessarily need to be delegated to the legislator, AI supervisory bodies or standard-setting bodies, as it can be carried out by the relevant stakeholders in each context, who can outline the key guiding principles in guidelines or similar tools to be used during the assessment practice when defining sectoral characteristics.\nOn the basis of the information gathered in the planning and scoping and risk identification phase, it is possible to carry out an in-depth analysis of the level of impact on individual rights, to quantify this impact and to prevent or reduce it through appropriate measures. The combination of these activities constitutes the impact assessment stage described in the next section."}, {"title": "5.2. Second FRIA block: risk analysis and management", "content": "Risk quantification is an essential part of any impact assessment. In this respect", "measure": "he impact on a range-based quantification of the risk (low", "dimensions": "the likelihood of adverse impact and its severity. The combination of the variables relating to these two dimensions provides a risk index that is assessed for each of the rights and freedoms potentially affected.\nIn terms of social research methodology, it is also possible to create a composite index combining all potential impacts on rights, as well as a composite index combining the results of the FRIA and the results of the Conformity Assessment to create an overall impact index. However, this approach conflicts with the legal approach to fundamental rights where each right must be considered independently in terms of its potential prejudice. The fact that one right is less affected than another cannot"}]}]}