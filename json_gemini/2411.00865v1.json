{"title": "DemoCraft: Using In-Context Learning to Improve Code Generation in Large Language Models", "authors": ["Kapu Nirmal Joshua", "Mihit Sreejith"], "abstract": "Generating executable code from natural language instructions using Large Language Models (LLMs) poses challenges such as semantic ambiguity and understanding task-specific contexts. To address these issues, we propose a system called DemoCraft, which enhances code generation by leveraging in-context learning and demonstration selection, combined with latent concept learning. Latent concept learning introduces additional concept tokens, which are trainable embeddings that capture task-specific knowledge. We then test our system on two major datasets: MBPP and Humaneval. Our experimental results demonstrate that the proposed system achieves an approximate 2x increase in the pass@k metric compared to baseline models. Furthermore, we introduce two novel evaluation metrics: correctness@k and similarity@k. Our empirical studies indicate that our system attains nearly a 3x improvement in these metrics as well.", "sections": [{"title": "I. INTRODUCTION", "content": "The problem of generating code from natural language using Large Language Models (LLMs) involves creating systems capable of translating human language instructions into executable code accurately. This requires the LLM to understand the semantics of the natural language input, grasp the intent behind the instructions, and convert it into syntactically correct and functional code in a specified programming language. Key challenges include handling ambiguous or imprecise language, ensuring the generated code is both correct and efficient, and covering a wide range of programming scenarios and languages."}, {"title": "II. PROBLEM BEHIND SELECTING DEMONSTRATIONS", "content": "In-context learning operates by pre-pending a series of demonstrations\u2014examples of prompts and corresponding answers\u2014before the final prompt that the model needs to solve. This setup effectively guides the model, allowing it to leverage patterns from prior examples to generate improved responses. By selecting demonstrations that closely match the problem at hand, we can significantly enhance the model's performance on complex tasks like code generation.\nHowever, selecting relevant demonstrations is a challenging task in itself. Semantic similarity-based selection, a commonly used approach, attempts to identify demonstrations that share high textual similarity with the prompt. While this method may capture surface-level relationships, it often fails to consider the deeper task requirements.\nFor instance, in competitive programming contexts like Codeforces, problem statements frequently involve recurring character names like \"Alice\" and \"Bob,\" often engaging in a hypothetical game. A semantic similarity-based approach might assume that any problem mentioning \"Alice and Bob playing a game\" is contextually relevant to another problem with similar phrasing. However, while these problems may seem alike in language, they can differ significantly in their underlying algorithms. One \"Alice and Bob\" problem may require a dynamic programming approach, while another could involve graph theory or combinatorial analysis. As a result, semantically similar demonstrations might mislead the model, offering examples that match the language but fail to provide the right procedural insights.\nThis is where our system, DemoCraft, becomes instru-mental. DemoCraft utilizes a latent concept-based selection algorithm to analyze and select demonstrations that are aligned not only in linguistic features but also in conceptual depth. By focusing on the intrinsic structure of computational prob-lems, DemoCraft identifies demonstrations that share the same reasoning paradigms or algorithmic strategies necessary to solve the target prompt. For instance, when presented with a complex binary search or dynamic programming problem, DemoCraft is capable of prioritizing demonstrations that in-volve these specific techniques over those with mere super-ficial similarity, thereby ensuring that the model is provided with the most contextually relevant guidance."}, {"title": "III. DEMOCRAFT: SYSTEM DETAILS", "content": "In this section, we provide a detailed technical description of our system architecture, which consists of three primary components: the Latent Concept Learning module, the Task Concept Probability Calculation module, and the Demonstration Selector."}, {"title": "A. Latent Concept Learning", "content": "In this stage, we introduce additional tokens [6], referred to as concept tokens, to enable the model to learn task-specific features for a given task. These concept tokens function as specialized units within the language model, representing knowledge specific to the task. Incorporating these tokens allows the model to predict the structure and requirements of the task more effectively.\nWe aim to find the optimal value of the variable $\\theta_d$ for each task $d$ in the set of tasks $T$. The variable $\\theta_d$, referred to as the latent concept variable, is intended to capture the essential characteristics of each task to maximize the model's predictive accuracy. Mathematically, the optimal $\\theta_d$ maximizes the probability of the correct output given the input, achieved through the Bayes optimal classifier defined as\n$\\theta_d = \\arg \\max P_{\\mathcal{M}_{\\theta_d}}(Y | \\theta_d, X)$  (1)\nwhere $P_{\\mathcal{M}_{\\theta_d}}(Y | \\theta_d, X)$ is the probability that the model $\\mathcal{M}$ assigns to the output $Y$ given the input $X$ and task-specific variable $\\theta_d$.\nTo train the model to make better predictions, we aim to find $\\theta$ that minimizes the cross-entropy loss. This involves minimizing the negative expected log probability:\n$\\theta = \\arg \\min -E_{X,Y,d} [\\log P_{\\theta_d}(Y | \\theta_d, X)]$ (2)\nWe align $\\theta_d$ with the token embedding space by introducing new tokens\u2014our concept tokens\u2014into the model's vocabulary. These tokens represent the task concept $\\theta_d$, allowing the model to utilize them within its regular vocabulary. Following methods proposed by Lester et al. [3], we add $c$ new concept tokens, denoted as $\\theta_d$, to represent each task's concept. The embeddings of these new tokens, $E_{new}(\\theta_d)$, are fine-tuned specifically for the task while keeping the rest of the language model's parameters frozen. This approach enables the model to focus on learning the nuances of $\\theta_d$ without altering its general language capabilities. The parameter $c$, representing the number of concept tokens, is treated as a hyperparameter adjustable based on task requirements.\nDuring training, the $c$ concept tokens associated with $\\theta_d$ are prepended to the input $X$ (or output $Y$) to condition the model on the specific task, providing task-specific context that enhances predictive performance.\nThis process is illustrated in Figure 4, which provides a flowchart for the latent concept learning method. The flow depicts how, starting from a dataset $D$, the input $X_i$ is fed into the model along with the updated concept tokens $\\theta$. The model generates the output $Y_i$, and the cross-entropy loss $\\log P_{\\mathcal{M}} (Y_i | \\theta, X_i)$ is computed to update $\\theta$. This iterative training process enables the model to understand and adapt to the task-specific requirements embedded in $\\theta$, leading to more relevant demonstration selections in DemoCraft."}, {"title": "B. Task Concept Probability Calculation", "content": "In the Task Concept Probability Calculation stage, our objective is to quantify how well each demonstration aligns with the target task. This involves calculating the relevance of each input-output pair $(X_i, Y_i)$ within the context of the task's specific requirements.\nLeveraging the previously trained concept tokens $\\theta$, we evaluate the suitability of input-output pairs from our dataset $D$. For each pair $(X_i, Y_i)$, we compute the probability $P_{\\mathcal{M}}(\\theta | Y_i, X_i)$, which measures the degree to which the demonstra-tion aligns with the task-specific concept encapsulated by $\\theta$. This probability serves as an evaluative metric, where higher values indicate stronger alignment with the task.\nFormally, the task concept probability is calculated using Bayes' theorem:\n$P_{\\mathcal{M}}(\\theta|Y_i,X_i) = \\frac{P_{\\mathcal{M}}(Y_i,X_i | \\theta) P_{\\mathcal{M}}(\\theta)}{P_{\\mathcal{M}}(Y_i,X_i)}$ , (3)\nwhere:\n\u2022 $P_{\\mathcal{M}}(\\theta | Y_i, X_i)$ is the posterior probability of the concept tokens given the demonstration pair.\n\u2022 $P_{\\mathcal{M}} (Y_i, X_i | \\theta)$ is the likelihood of the demonstration pair given the concept tokens.\n\u2022 $P_{\\mathcal{M}}(\\theta)$ is the prior probability of the concept tokens.\n\u2022 $P_{\\mathcal{M}} (Y_i, X_i)$ is the marginal probability of the demonstra-tion pair.\nIn this stage, the large language model $\\mathcal{M}$ operates in an evaluative capacity; it computes the task concept probabilities based on its learned representations without undergoing further fine-tuning. By assigning task concept probabilities to each demonstration, we gain insights into their relative relevance, which is crucial for selecting the most appropriate demonstra-tions in subsequent stages.\nThis process is illustrated in Figure 5, which outlines how input-output pairs, along with the trained concept tokens $\\theta$, are processed through the model to compute the task concept probabilities $P_{\\mathcal{M}}(\\theta | Y_i, X_i)$ for each pair $(X_i, Y_i)$."}, {"title": "C. Demonstration Selection", "content": "In the Demonstration Selection stage, our objective is to identify the most relevant demonstrations for a given task prompt. Having computed the task concept probability $P_{\\mathcal{M}}(\\theta| Y_i, X_i)$ for each demonstration pair $(X_i, Y_i)$ in our dataset $D$, we proceed to select the top $k$ demonstrations that align most closely with the task-specific concept $\\theta$.\nWe rank all demonstration pairs based on their computed task concept probabilities and select the top $k$ pairs with the highest values of $P_{\\mathcal{M}}(\\theta | Y_i, X_i)$. This selection pro-cess ensures that we retain demonstrations that are most contextually relevant to the task at hand. By focusing on the highest probability values, we choose examples that the model has identified as highly aligned with the desired task-specific features. This maximizes the likelihood that these demonstrations will enhance the model's understanding and performance when generating responses for the target prompt.\nThis process is illustrated in Figure 6, which shows how we systematically select the top $k$ demonstrations with the highest alignment scores, ultimately constructing a refined set of examples tailored to optimize the model's responses for the given prompt."}, {"title": "D. Final System Diagram", "content": "DemoCraft extends the foundational concepts discussed\u2014namely, latent concept learning and task concept probability calculation\u2014to operate across multiple datasets. This enables the model to learn a comprehensive set of concept tokens, each corresponding to distinct task types, denoted by $\\theta_1, \\theta_2,..., \\theta_K$. Once trained, these concept tokens allow the system to retrieve relevant demonstrations from a diverse range of sources.\nWhen a new prompt Q is provided, DemoCraft evaluates it by calculating probabilities over both the learned concept tokens and potential demonstration pairs $(X_i, Y_i)$ from the dataset D. This involves a two-step process:\n1) For each concept token $\\theta_i$, compute the probability $P_{\\mathcal{M}}(\\theta_i | X_j, Y_j)$ for all demonstration pairs $(X_j, Y_j) \\in D$.\n2) Maximize this probability over both $\\theta_i$ and $(X_j, Y_j)$ to select the top $k$ demonstrations:\n$\\{(X_{i^*}, Y_{i^*})\\} = \\arg \\max P_{\\mathcal{M}} (\\theta_i | X_j, Y_j)$, (4)\nwhere $\\{(X_{i^*}, Y_{i^*})\\}$ denotes the set of top $k$ demonstrations that best align with the task-specific requirements of Q. This approach leverages both the learned task-specific knowledge encapsulated in the concept tokens and the diversity of the dataset, ensuring a refined and targeted selection process.\nThe overall system flowchart is provided in Figure 7, illustrating how the trained concept tokens, task probability calculator, and demonstration selector operate in unison to choose the most relevant examples for each new prompt."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we highlight our experimental metrics and the conditions under which we conducted the experiments."}, {"title": "A. Evaluation Metrics", "content": "We evaluate our model using three primary metrics:\n1) pass@k: This metric measures the probability that at least one of the top $k$ generated code samples passes all the test cases for a given problem. Suppose for each problem we generate $n$ code samples, out of which $c$ samples are correct (i.e., they pass all the unit tests). The pass@k is calculated as:\n$\\text{pass@k} = E_D \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]$, (5)\nwhere $E_D$ denotes the expectation over the dataset $D$, and $\\binom{n}{k}$ is the binomial coefficient representing the number of ways to choose $k$ samples out of $n$.\n2) correctness@k: This metric is defined as the average precision of the model over the entire dataset when $k$ outputs are generated per prompt. For each prompt, if the model generates $k$ outputs and $c$ of them are correct, the correctness for that prompt is calculated as:\n$\\text{correctness@k} = E_D \\left[ \\frac{c}{k} \\right]$, (6)\nwhere $E_D$ denotes the expectation over the dataset $D$.\n3) similarity@k: This metric measures the average similar-ity between the working codes generated by the model and the golden solution provided in the dataset. For each prompt, let $S$ be the set of all generated codes that pass all the test cases (i.e., working codes), and let $y$ be the golden solution from the dataset. The similarity@k is defined as:\n$\\text{similarity@k} = E_D \\left[ \\frac{1}{|S|} \\sum_{y_i \\in S} \\text{sim}(y_i, y) \\right]$, (7)\nwhere $\\text{sim}(y_i, y)$ is a similarity function between the generated code $y_i$ and the golden solution $y$, and $|S|$ is the number of working codes for that prompt. The outer expectation $E_D$ is taken over all prompts in the dataset $D$. The similarity function used over here is the edit distance metric, provided in the standard NLTK library."}, {"title": "B. Datasets and Models", "content": "We conducted our experiments using the following datasets and model:\n1) MBPP: The Mostly Basic Python Problems (MBPP) dataset [4] consists of 427 programming problems de-signed for code generation tasks. Each problem includes a natural language description, the corresponding code solution, and three unit tests. The programming language used is Python.\n2) HumanEval: The HumanEval dataset [5] comprises 164 programming tasks focused on code completion. Each task provides a function signature and a docstring describing the desired functionality. The solutions are written in C++, and each problem includes approxi-mately seven unit tests, making it a stricter benchmark than MBPP.\nDue to resource constraints, we evaluated the performance of our system using the SantaCoder model. SantaCoder is a transformer-based language model with 1.1 billion parameters, pretrained on a large corpus of code in multiple programming languages, including Python and C++. It is designed to gen-erate syntactically correct and functionally meaningful code snippets. We conducted our experiments using Google Colab's T4 GPU, which provided sufficient computational resources for our evaluations without compromising performance."}, {"title": "C. Baselines", "content": "We compare our system against the following baseline methods:\n1) Semantic Selection: In this baseline, we select demon-strations from the dataset purely based on their semantic similarity to the given prompt x. Let the dataset be $D = \\{(x_i, Y_i)\\}_{i=1}^n$, where $x_i$ are the prompts and $y_i$ are the corresponding outputs. For each $x_i$ in the dataset, we compute the similarity score $\\text{sim}(x, x_i)$ between the given prompt $x$ and each dataset prompt $x_i$. We then select the top $k$ demonstrations with the highest similarity scores:\n$\\{(x_{i^*}, Y_{i^*})\\} = \\arg \\max_{\\{(x_i,Y_i) \\in D\\}} \\text{sim}(x, x_i)$,  (8)\nwhere $\\{(x_{i^*}, Y_{i^*})\\}$ denotes the set of top $k$ demon-strations selected. The $\\text{sim}(.)$ function used here is the standard edit distance, implemented using the NLTK library.\n2) Random Selection: In this baseline, we randomly select $k$ demonstrations from the dataset $D$ without considering their relevance to the given prompt $x$. This method serves as a control to evaluate the impact of demon-stration selection strategies on the model's performance."}, {"title": "V. RESULTS", "content": "In this section, we present the results of our experiments on both the MBPP and HumanEval datasets. The results show that demonstrations chosen by DemoCraft consistently outperform other selection methods. This su-periority arises from DemoCraft's encoding of task-specific knowledge through specialized token embeddings tailored to each task."}, {"title": "VI. CONCLUSION", "content": "In this paper, we presented DemoCraft, a demonstration selection framework that enhances code generation models by leveraging task-specific knowledge through latent concept learning. DemoCraft introduces specialized token embeddings tailored to each task, enabling the model to internalize un-derlying concepts effectively. Our evaluations on the MBPP and HumanEval datasets, utilizing the metrics pass@k, correctness@k, and similarity@k, demonstrate that DemoCraft consistently outperforms baseline methods, including seman-tic similarity-based and random selection approaches. These results highlight the efficacy of targeted demonstration selec-tion in improving code generation accuracy and functionality. Future work will explore the integration of DemoCraft with larger language models and its application to diverse domains, including software engineering and competitive programming."}]}