{"title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs", "authors": ["Jack Hong", "Shilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Weidi Xie"], "abstract": "In this paper, we introduce WorldSense, the first benchmark to assess the multi-modal video understanding, that simultaneously encompasses visual, audio, and text inputs. In contrast to existing benchmarks, our WorldSense has several features: (i) collaboration of omni-modality, we design the evaluation tasks to feature a strong coupling of audio and video, requiring models to effectively utilize the synergistic perception of omni-modality; (ii) diversity of videos and tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual synchronised videos, systematically categorized into 8 primary domains and 67 fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice QA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii) high-quality annotations, all the QA pairs are manually labeled by 80 expert annotators with multiple rounds of correction to ensure quality. Based on our WorldSense, we extensively evaluate various state-of-the-art models. The experimental results indicate that existing models face significant challenges in understanding real-world scenarios (48.0% best accuracy). We hope our WorldSense can provide a platform for evaluating the ability in constructing and understanding coherent contexts from omni-modality.", "sections": [{"title": "1 Introduction", "content": "The ability to comprehend and reason about multimodal inputs-ranging from visual and textual to auditory, tactile, and beyond-is fundamental for both human and artificial agents to navigate and interpret the world. For example, when driving a car, a human driver integrates visual information (e.g., recognizing road signs, traffic lights, and obstacles), auditory cues (e.g., hearing the honking of another car or a siren approaching from behind), and tactile feedback (e.g., the feel of the steering wheel, the vibrations of the road, or the responsiveness of the brakes) to make real-time decisions and ensure safe navigation. This seamless multimodal integration enables intelligent agents to process complex, dynamic environments and respond to subtle cues\u2014an ability that is essential for both human perception and the development of embodied agents designed to interact naturally in the world.\nIn the recent literature, the development of Multi-modal Large Language Models (MLLMs) [49, 24, 48, 57, 59, 75, 43, 12] have led to remarkable progress on a series of tasks, for example, classification [36], captioning [1, 10, 35], question-answering [56, 50, 40], OCR [45, 76], segmenta-tion [25, 67, 22], autonomous driving [46, 52, 4] and more. However, multi-modal analysis primarily focuses on visual-language information, leaving out crucial modalities like audio, which results in an incomplete evaluation of their multimodal capabilities. While some benchmarks have begun incorporating both visual and audio modalities, they still exhibit several limitations. For example, OmniBench [31] and AV-Odyssey Bench [19] mainly emphasize image evaluation, whereas other benchmarks [18, 29, 70] either restrict to captioning tasks or are limited to simple scenarios, or suffer from low-quality, monotonous questioning patterns.\nIn this paper, we introduce WorldSense, the first benchmark designed to comprehensively evaluate MLLMs' ability to perceive, understand, and reason through the integration of omni-modal infor-mation in real-world contexts. WorldSense is defined by three key features: (i) Collaboration of Omni-Modality: At the core of WorldSense is the emphasis on the interplay between audio and visual information, as illustrated in Figure 1. Each question is designed to require both modalities for a correct answer; the absence of either modality leads to an incorrect response. Such design rigorously tests a model's ability to simultaneously process multiple sensory inputs for accurate understanding; (ii) Diversity of Videos and Tasks: WorldSense encompasses 1,662 audio-visual synchronized videos across 8 domains and 67 fine-grained subcategories. It includes 3,172 multiple-choice questions spanning 26 distinct cognitive tasks, from basic perception to complex reasoning. This diversity enables a systematic and comprehensive evaluation of MLLMs' multimodal understand-ing capabilities; (iii) High-Quality Annotations: To ensure reliability, all QA pairs in WorldSense are manually labeled by 80 expert annotators and undergo multiple rounds of validation through both human review and automatic MLLM verification. This rigorous process ensures the benchmark's accuracy and reliability. By integrating these features, WorldSense sets a new standard for evaluating MLLMs' ability to handle real-world complexity, bridging the gap between artificial intelligence and human-like multimodal understanding.\nWe conduct comprehensive experiments on WorldSense to evaluate a range of existing MLLMs, including open-source video models, video-audio models, and proprietary models. Our results highlight significant limitations in the current models' ability to understand and reason through omni-modal information in real-world contexts. First, while open-source video-audio models can process both video and audio inputs, they achieve only around 25% accuracy on our benchmark, comparable to random guessing. In contrast, proprietary models, such as Gemini 1.5 Pro, which"}, {"title": "2 Related Work", "content": "Multimodal Large Language Models. Current Large Language Models (LLMs) are capable of processing multimodal information, including visual, text, and audio. Early works, such as [75, 35, 80, 11, 64, 51], successfully combine vision and text modalities. Subsequent research extends to temporal understanding [66, 24, 58, 39, 61, 65, 26, 14, 69, 74, 60, 6, 41, 34], while parallel efforts [55, 9, 8] focus on audio processing. Recently, researchers shift attention to models [7, 54, 58, 42, 59] capable of simultaneously processing text, vision, and audio inputs. Despite the growing interest in the models which can perform the omnimodality understanding, the absence of a comprehensive evaluation"}, {"title": "3 WorldSense", "content": "In this section, we aim to detail the construction of WorldSense, including data collection procedure, annotation pipeline, and statistics. Unlike existing benchmarks that assess modalities in isolation, WorldSense evaluates the MLLMs' ability to perceive, understand, and reason about real-world scenarios through the integration of omni-modal information. As shown in Figure 1, all the multiple-choice questions are carefully crafted, to make sure the questions can only be answered through the comprehensive analysis of text, vision, and audio."}, {"title": "3.1 Design Principle", "content": "As for multi-modal evaluation, we base on the audio-visual synchronized videos, which capture temporal events, motion patterns, and audio-visual correlations. To curate the benchmark, we adhere to the following three principles, to ensure a rigorous and comprehensive evaluation.\nComprehensive Domain. To ensure a thorough evaluation of MLLMs' real-world understanding, we developed a systematic taxonomy that covers diverse domains and scenarios. This process started with primary categories reflecting core aspects of human experience, which were further subdivided into 67 subcategories to capture specific contexts. This hierarchical structure ensures that our video collection spans a broad range of real-world experiences, providing an ecologically valid basis for assessing multimodal understanding.\nDiverse Acoustic Signals. In real-world scenarios, audio signals can be principally categorized into three fundamental types: speech, event and music. Our benchmark incorporates all three to ensure comprehensive coverage, enabling MLLMs to process and understand a wide spectrum of acoustic information-from semantic speech to abstract music and environmental sounds.\nMultilevel Assessment. To evaluate the MLLMs' perceptual and cognitive capabilities, we designed a multi-scale assessment at three levels: recognition (basic audiovisual element detection), under-standing (comprehension of multimodal relationships), and reasoning (advanced cognitive tasks like"}, {"title": "3.2 Data Collection & Curation", "content": "We primarily source our video content from FineVideo [15], a large-scale dataset of high-quality YouTube videos with strong audio-visual correlations across diverse real-world scenarios. To enhance the coverage of musical content, we additionally incorporate videos from MusicAVQA [29].\nOur data collection process involves a systematic filtering pipeline to ensure high-quality videos with rich visual-audio semantics and temporal dynamics, as illustrated in Figure 3(a). The process follows three key steps: (i) filtering videos based on predefined taxonomic categories described in Section 3.1 for comprehensive coverage; (ii) utilizing pre-computed metrics, including audio-visual correlation and dynamic content scores, to identify significant clips from an initial pool of approximately 8,000 videos; and (iii) conducting human expert review to assess video quality and real-world relevance. This rigorous process ultimately yields 1,662 high-quality video segments featuring strong audio-visual correlations across diverse real-world contexts."}, {"title": "3.3 Annotation Protocol", "content": "QA Annotation. A team of 80 professional annotators is engaged in creating high-quality multiple-choice QA pairs. For each video clip, annotators conduct thorough reviews of both visual and auditory content to ensure comprehensive understanding. They then generate questions and corresponding answer options that specifically require the integration of both visual and audio information for correct responses, thereby enabling effective evaluation of MLLMs' multimodal understanding capabilities.\nQuality Control. To ensure the quality of question-answer pairs, we implement a rigorous qual-ity control process that combines human expertise with automated verification, as illustrated in Figure 3(b). Professional quality control experts evaluate each QA pair based on three essential criteria: (i) linguistic clarity and coherence, (ii) necessity of both visual and audio information for correct answers, and (iii) appropriateness of the question's difficulty. Questions that fail to meet these standards are returned for revision.\nWe additionally employ MLLMs for automated verification. Vision-language models like Qwen2-VL[61] verify that questions require multiple modalities for correct answers. Furthermore, multimodal MLLMs capable of processing video, audio, and text, such as Video-LLaMA2[7] and OneLLM [21] are used to assess question difficulty, with questions answered correctly by all models being flagged for manual revision as too simple.\nThis dual-verification system, combining expert review and automated testing, ensures that all questions in our benchmark are of high-quality and well-formulated, that requires multi-modal comprehension, and present significant challenges for the models."}, {"title": "3.4 Dataset Statistics", "content": "As summarized in Table 1, the WorldSense dataset contains 1,662 video clips with synchronized audio, distributed across 8 primary categories and 67 subcategories. The average duration is 141.1 seconds, with lengths ranging from 30 seconds to over 10 minutes, capturing a wide variety of events and activities. In total, WorldSense includes 3,173 multiple-choice questions, covering three levels.\nWorldSense encompasses diverse audio components including speech, environmental sounds, and music. Unlike existing benchmarks that use static images (e.g., AV-Odyssey Bench [19], Om-niBench [31]) or feature weak audio-visual correlations (e.g., Video-MME [17]), WorldSense is the first benchmark designed to evaluate MLLMs' real-world multimodal understanding. It distinguishes itself through: (i) open-domain videos with multi-task evaluation, (ii) original audio-visual content with complete transcriptions, and (iii) carefully crafted questions requiring true audio-visual inte-gration, establishing a more comprehensive benchmark for real-world multimodal understanding assessment."}, {"title": "3.5 Evaluation Paradigm", "content": "In our evaluation framework, each test instance consists of a video clip with synchronized audio and a multiple-choice question. Models must process these multi-modal inputs and select the correct answer from several options. Performance is measured by accuracy, comparing the model's selection to the ground-truth answers. A model's success is determined by its ability to accurately align with the correct answer.\nTo rigorously assess the necessity of multimodal integration in real-world understanding, we conduct ablation studies across various modality configurations. This approach not only evaluates overall model performance but also quantifies the models' reliance on individual modalities, highlighting the critical role of multimodal collaboration in real-world comprehension tasks."}, {"title": "4 Experiments and Findings", "content": "This section presents a comprehensive evaluation of both open-source and proprietary MLLMs on the WorldSense benchmark. We first delineate our experimental methodology and evaluation protocols, followed by a comprehensive analysis of quantitative results. Furthermore, we conduct detailed investigations into the important factors that affect performance, providing insights that illuminate potential directions for multi-modal understanding."}, {"title": "4.1 Settings", "content": "To comprehensively assess the multi-modal understanding ability, we evaluate three types of MLLMs: (1) open-source audio-visual models, such as Unified-IO-2 [42], OneLLM [21], and VideoLLaMA2 [7]; (ii) open-source MLLMs, such as Qwen2-VL [62], LLaVA-OneVision [26],"}, {"title": "4.2 Results on WorldSense", "content": "Main Results. We present the comprehensive evaluation results on WorldSense in Table 2. Our findings reveal several significant insights regarding the current state of multi-modal models in real-world understanding.\nFirst, current open-source video models are limited in their performance as they process only visual information. This restriction highlights a significant gap in their ability to perform complex, multi-modal understanding tasks, as evidenced by their maximum performance score of only 40.2%. The results underscore the inadequacies of relying solely on visual processing, emphasizing the need to integrate audio inputs for a more comprehensive and accurate understanding in practical applications.\nSecond and surprisingly, existing open-source audio-visual MLLMs perform even worse, achieving accuracy rates comparable to random guessing and notably below video-only MLLMs. This counter-intuitive finding reveals that despite having access to both modalities, these models struggle with effective audio-visual integration, suggesting that multimodal processing capability alone does not guarantee better performance without sophisticated integration mechanisms.\nThird, among proprietary MLLMs, vision-only models GPT-4o and Claude 3.5 Sonnet perform similarly to the best open-source video MLLMs. Gemini 1.5 Pro, capable of processing both audio and visual information, achieves the highest accuracy of 48.0%. However, this performance still falls considerably short of requirements for reliable real-world applications, indicating substantial room for improvement.\nThese comprehensive results illuminate several critical insights: (i) the fundamental importance of audio-visual collaborative understanding in real-world scenarios; (ii) the current significant gap in models' capabilities for effective multimodal integration, and (iii) the need for more sophisticated"}, {"title": "4.3 Roadmap Towards Real-world Understanding", "content": "Given the substantial performance gap revealed in above evaluation, we conduct an in-depth investi-gation into potential approaches to enhance the MLLMs' performance.\nVision Information. We investigate the impact of visual information through different input configu-rations: audio-only, audio with video captions, and audio with video frames. As shown in Table 3, visual information generally improves performance, with Gemini 1.5 Pro's accuracy increasing from 34.6% (audio-only) to 48.0% (+video). However, impact varies across models, with UnifiedIO2 showing inconsistent gains and even degradation with captions.\nThese findings suggest two important insights: (1) visual information is crucial for enhancing multi-modal understanding when properly integrated, and (2) current models' ability to effectively utilize visual information remains limited.\nAudio Information. We examine the impact of audio information through three configurations: video-only, video with subtitles, and video with original audio.\nThe results in Table 4 reveal intriguing patterns in how different forms of audio information influence model performance. For Gemini 1.5 Pro, accuracy increases from 34.4% (video-only) to 39.3% with subtitles, and further to 48.0% with original audio. OneLLM shows similar improvements. These results demonstrate that both subtitles and acoustic features (including tone, emotion, and environmental sounds) contribute valuable information for multimodal understanding, beyond what"}, {"title": "5 Conclusion", "content": "In this paper, we introduce WorldSense, the first benchmark designed to evaluate MLLMs' omni-modal understanding in real-world scenarios. Distinguished by its emphasis on joint omnimodal comprehension across diverse real-world contexts, WorldSense encompasses rich video categories and carefully curated question-answer pairs that necessitate the integration of visual and acoustic information. Through extensive experiments, we expose significant limitations in current MLLMs' ability to process and coherently integrate omnimodal information. Our analysis demonstrates the importance of omnimodal collaboration in real-world understanding. We hope that WorldSense can serve as a foundational benchmark for advancing human-like omnimodal understanding capabilities."}, {"title": "A Broader Impacts", "content": "Our work on WorldSense has several potential positive impacts on society and AI development, while also presenting certain risks that warrant careful consideration. WorldSense contributes to advancing MLLMs' ability to understand and interact with the real world through multiple modalities. This progress could benefit various applications, including assistive technologies, educational tools, human-AI interaction systems, safety systems, and so on. We also acknowledge potential risks and challenges. The development of more capable AI systems might raise privacy concerns. Advanced multimodal understanding capabilities could potentially be misused for surveillance or monitoring purposes. We believe that open discussion of these impacts is crucial for responsible development of MLLMs."}, {"title": "B Ethics Statement", "content": "Our research on WorldSense adheres to strict ethical principles and guidelines. We acknowledge several important ethical considerations:\n\u2022 Data Collection and Privacy. All video content in WorldSense has been collected from publicly available sources with appropriate licensing agreements. We have con ducted thorough reviews and implemented comprehensive data processing procedures to ensure privacy protection, including the removal of any personally identifiable information.\n\u2022 Potential Biases. While acknowledging that inherent biases may exist in any dataset, we have undertaken systematic efforts to ensure diverse representation across our video content and question-answer pairs, encompassing various domains, cultures, and contexts. Nevertheless, we recognize that completely eliminating bias remains a significant challenge, and users should carefully consider these potential limitations when utilizing our dataset.\n\u2022 Intended Use. WorldSense is specifically designed to advance research in omnimodal real-world understanding. While we actively encourage the use of this benchmark for academic and research purposes, we strongly caution against any applications that could potentially result in harmful or discriminatory outcomes. Users are expected to adhere to ethical guidelines and responsible practices."}, {"title": "C License", "content": "The WorldSense dataset is released under the CC BY-NC-SA 4.0 License. Authors bear all responsi-bility in case of violation of rights and confirmation of the data license."}]}