{"title": "A Scalable and Generalized Deep Learning Framework for Anomaly Detection in Surveillance Videos", "authors": ["Sabah Abdulazeez Jebur", "Laith Alzubaidi", "Khalid A. Hussein", "Haider Kadhim Hoomod", "Ahmed Ali Saihood", "YuanTong Gu"], "abstract": "Anomaly detection in videos is challenging due to the complexity, noise, and diverse nature of activities such as violence, shoplifting, and vandalism. While deep learning (DL) has shown excellent performance in this area, existing approaches have struggled to apply DL models across different anomaly tasks without extensive retraining. This repeated retraining is time-consuming, computationally intensive, and unfair. To address this limitation, a new DL framework is introduced in this study, consisting of three key components: transfer learning to enhance feature generalization, model fusion to improve feature representation, and multi-task classification to generalize the classifier across multiple tasks without training from scratch when new task is introduced. The framework's main advantage is its ability to generalize without requiring retraining from scratch for each new task. Empirical evaluations demonstrate the framework's effectiveness, achieving an accuracy of 97.99% on the RLVS dataset (violence detection), 83.59% on the UCF dataset (shoplifting detection), and 88.37% across both datasets using a single classifier without retraining. Additionally, when tested on an unseen dataset, the framework achieved an accuracy of 87.25%. The study also utilizes two explainability tools to identify potential biases, ensuring robustness and fairness. This research represents the first successful resolution of the generalization issue in anomaly detection, marking a significant advancement in the field.", "sections": [{"title": "1. Introduction", "content": "Integrating Artificial Intelligence (AI) in various domains has brought transformative changes in our daily lives. One of the major uses of Al lies in the field of surveillance cameras, which enhance security and situational awareness. Anomaly detection (AD), a cutting-edge AI technique, has emerged as a vital tool in the surveillance industry, revolutionizing how we monitor and protect our environments [1]. The process of AI-driven AD employs machine learning (ML) and deep learning (DL) algorithms to automatically identify unusual behaviors in video streams. These AI systems continuously analyze and learn from massive datasets, allowing them to detect subtle irregularities that might go unnoticed by human observers. This makes them extremely valuable for early threat detection, crime prevention, and overall safety enhancement [2]. AD identifies human activities that deviate from expected behavior, including activities such as violence, stealing, arson, abuse, loitering, and vandalism in specific locations such as markets and streets. Regarding Computer Vision (CV), AD involves recognizing patterns that exhibit significant deviations from normal behavior. Monitoring surveillance cameras without leveraging intelligent systems demands substantial resources, including manpower, finances, and time. Moreover, it is susceptible to errors due to the challenge of simultaneously monitoring numerous surveillance cameras [3]. Automated CV systems are critical for detecting anomalies in video without manual intervention. DL techniques have consistently delivered state-of-the-art results in addressing issues related to monitoring suspicious activities in surveillance systems and excelling in various other domains. DL leverages deep neural networks (DNNs) to identify and extract features from input data. Furthermore, it has the capability to automatically discern numerous unidentified parameters during the training phase [3]. Convolutional Neural Networks (CNNs) are DNNs specifically designed to automatically learn features and recognize patterns from image data. CNNs use a specialized convolutional layer to analyze the input image with small filters or kernels, allowing them to detect different features at multiple scales[4]. These networks have succeeded highly in various image-related tasks, including image recognition and generation [5]. CNNs are very efficient and powerful tools in video anomaly detection (VAD). These DL algorithms can classify video frames based on their feature content and extract relevant information. A critical application of CNNs is identifying abnormal events within video streams. This involves training a CNN on a labeled video dataset to enable it to detect deviations from expected behavior and flag anomalies. The synergy between CNNs and VAD enhances security and showcases the remarkable adaptability of CNNs in extracting meaningful insights from dynamic visual data [6]. However, one challenge that CNNs face is the requirement for a large amount of data. To address this problem, researchers have widely used transfer learning (TL). TL is a technique in which the knowledge gained from training a model on one task is utilized to improve the performance of another related task instead of training a model from scratch for each specific task. In simpler terms, TL uses the knowledge acquired from one task to enhance learning and performance on another [7]. In addition to the data scarcity problem, VAD faces significant challenges related to generalization [5], which is the ability of a model to perform well on new data. In addition, the flexible fusion of multiple models into a single framework also presents a challenge in this field. The fusion approach offers a concise representation of various features extracted from different sources, enhancing overall performance and improving generalization capability [8][9]. In the DL field, integrating new sets of data without requiring extensive retraining is a significant challenge. Furthermore, understanding and interpreting the decisions made by DNNs in the context of image classification and object detection is crucial. Moreover, DNNs, particularly deep convolutional models, are highly complex and can be considered black boxes, making it difficult to understand how they arrive at their predictions[10]. This need for more transparency is a significant concern, especially in critical applications where trust and explainability are essential. Multi-anomaly detection is the task of identifying multiple types or classes of anomalies in a dataset. Unlike binary anomaly detection, which deals with only one class of anomalies and one class of normal data, multi-anomaly detection addresses scenarios with multiple types of anomalies. Incorporating new classes into the existing model typically involves retraining the model from scratch, which can be time-consuming, resource-intensive, and may require a substantial amount of labeled data for the new class. Moreover, ensuring the model can detect all known anomaly types is crucial while adapting to new ones. We can create more efficient and practical anomaly detection systems if we avoid this process. In this study, new methods have been proposed to address the issues mentioned above. As a result, we present the following contributions:\n\u2022\tA novel framework has been introduced for incorporating a new anomaly class into an existing AD model without retraining from scratch.\n\u2022\tA deep feature fusion method is proposed to integrate diverse DL models for better feature representation.\n\u2022\tThe deep feature fusion approach achieved an accuracy of 83.59% on the UCF-Crime dataset and 97.99% on the RLVS dataset, outperforming all the previous methods.\n\u2022\tBased on our experiments with the UCF-Crime and RLVS datasets, the multi-classification approach we used was able to accurately detect and categorize two specific abnormal behavior classes (shoplifting and violence) and normal behavior, achieving an accuracy of 88.37%. This approach significantly enhances the ability to identify and categorize abnormal behaviors in different scenarios. To the best of our knowledge, an existing approach has yet to achieve a similar level of performance with a single model across multiple tasks in VAD.\n\u2022\tThe proposed framework achieved an accuracy of 87.25% on a complete independent test set."}, {"title": "2. Related Works", "content": "This section provides an overview of the evolving landscape of VAD research, highlighting key studies and methodologies that harness the potential of ML and DL to enhance the accuracy and efficiency of AD systems. A study [11] presented a new DL architecture for identifying violent behaviors in videos. The method leverages recurrent neural networks (RNNs) and 2D CNN to capture spatial and temporal features. Optical flow information is integrated to encode motion patterns. The proposed approach has been tested with success on multiple databases. In [12], the authors proposed an approach to assist monitoring staff in directing their attention to specific screens where the likelihood of a crime is higher. This approach involved identifying situations in video footage that may signal an impending crime. They employed a 3D CNN to examine surveillance videos and capture behavioral attributes for the identification of suspicious actions. The model was trained using carefully chosen videos from the UCF-Crimes dataset. In [13], a Hybrid CNN Framework (HCF) was presented to identify distracted driver behaviors by leveraging DL and image processing techniques. The framework employed pre-trained CNN models in collaboration to extract behavioral features through TL, thereby improving result accuracy. In a study [14], TL was employed to enhance the accuracy of abnormal behavior detection by extracting human motion characteristics from RGB video frames. The authors utilized the VGGNet-19 architecture for feature extraction and subsequently applied a Support Vector Machine (SVM) classifier to identify complex motion scenarios. In [15], several DL models, such as CNN, LSTM (Long Short-Term Memory), CNN-LSTM, and Autoencoder-CNN-LSTM, were explored to identify unusual behaviors in the elderly. The models were trained using temporal and spatial data, which enabled them to make accurate predictions. To tackle the issue of data imbalance, the researchers oversampled minority classes, especially for the LSTM model. Overall, the study provides insights into how DL can be used to detect and prevent unusual behaviors in elderly individuals. In[16], the authors tackled the issue of shoplifting by focusing on detecting suspicious behaviors that may lead to criminal activities. Instead of identifying the crime itself, their approach aims to model and detect behaviors that precede criminal acts, providing opportunities for prevention. They utilized a 3D CNN to extract video features and classify segments containing potential shoplifting behavior. [17] introduced a shoplifting detection system that utilizes a DNN. The system used the Inceptionv3 model for feature extraction and employed LSTM networks to understand temporal sequences. This system can accurately identify individuals involved in shoplifting activities with an accuracy of up to 74.53%. The paper [18] presented deep violence detection approach that leverages handcrafted features related to appearance, speed of movement, and representative images. These features are input into CNN through spatial, temporal, and spatiotemporal streams. The spatial stream captured environment patterns, the temporal stream focused on motion patterns using modified optical flow, and the spatiotemporal stream introduced a novel feature to enhance interpretability. The CNN is trained on datasets containing both violent and normal behavior frames. A study [19] used DL techniques to detect abnormal driver behaviors such as smoking, eating, drinking, and calling. To train and test models, a dataset was created comprising these behaviors as well as normal driving. The study evaluated DL models, including a proposed CNN-based model and pre-trained models such as ResNet101, VGG-16, VGG-19, and Inception-v3. Keyframe extraction was used to optimize computation. In [20], a shoplifting detection system was introduced. This approach involved the use of a hybrid neural network that combined convolutional and recurrent components to extract information from video frames and analyze their temporal sequence. Specifically, it employed gated recurrent units for data processing. Data augmentation was conducted to mitigate class imbalance and enhance the dataset. For classification, a pre-trained MobileNetV3Large CNN was combined with a recurrent network that incorporated gated nodes. In reference [21], a method for detecting violent behavior using keyframes is proposed. This approach treats video frames as discrete events and detects instances of violence by assessing whether the count of keyframes exceeds a predefined threshold, thereby reducing hardware demands. Furthermore, the paper introduced a novel training technique that leverages pairs of background-removed and original images to improve feature extraction for DL models, all while avoiding the introduction of extra network complexity. In [22], a model for detecting crowd violence behavior, named HD-Net, was developed. It utilized a human contour extractor to minimize background noise in violence detection by focusing on individuals in video frames. A dynamic feature encoder is also used for extracting dynamic features from adjacent frames. The model is built on a 3D CNN framework for spatial feature extraction and LSTM for temporal feature fusion. [23] proposed a convolutional autoencoder architecture that can detect anomalies in appearance and motion patterns. The architecture used two components, the spatial and temporal autoencoder, to differentiate between spatial and temporal representations. The spatial autoencoder captures appearance features by reconstructing the initial frame. On the other hand, the temporal component models motion through RGB differences across sequential frames. To further enhance the performance of the motion autoencoder, the paper incorporated a variance-based attention module that highlights critical movement areas. A novel deep K-means clustering approach was introduced to extract concise representations. In [24], the authors presented a CNN model to detect crowd anomalies in video sequences. The model is composed of two convolutional layers followed by two fully connected layers that utilize Rectified Linear Unit (ReLU) and sigmoid functions. The intermediate layers generate features that are used for abnormality detection. The model's performance was evaluated on three scientific datasets that included normal and abnormal activities. The outcomes demonstrated that the model performed effectively when applied to random YouTube videos exhibiting abnormal behavior. In [25], a new approach was introduced to detect abnormal behavior of workers in manufacturing environments. The model identifies and describes unusual worker actions based on their interaction with objects using a combination of technologies such as Mask R-CNN, Media Pipe Holistic, LSTM, and a worker behavior description algorithm. The approach involved object recognition, worker pose identification, and pattern analysis to differentiate between typical and unusual actions. Anomalous behaviors encompass instances such as worker falls, slips, tool breakage, and machine failures. The article [26] presented an anomaly recognition model employing a deep CNN architecture. This model extracts deep features from surveillance video frames and directs them to a temporal convolution network (TCN) with a multi-head attention module. The TCN comprises multiple layers of temporal convolutional filters with varying dilation rates, enabling the capture of diverse temporal contexts and long-range dependencies. It is trained by minimizing an objective function, using cross-entropy loss, to optimize parameters for accurate classification or recognition of activities in sequential data. The research presented in the paper [27] utilized TL-InceptionV3 to improve anomaly detection in surveillance cameras. Two TL methods, pre-training and fine-tuning, were employed using InceptionV3 to classify frames as normal or abnormal behaviors. The UCF-Crime dataset was utilized for training and evaluation. The results demonstrated that the fine-tuning approach outperformed the pre-training approach significantly. This indicates substantial enhancements in the model's performance. In [28], a comprehensive benchmark dataset was introduced, consisting of 900 samples, evenly divided into 450 instances of shoplifting and 450 instances of non-shoplifting, annotated across different shoplifting scenarios. This dataset was utilized to assess shoplifting detection techniques, including 2D CNN, 3D CNN, and a novel hybrid method that combines InceptionV3 and bidirectional LSTM. Notably, the hybrid approach outperformed the others in terms of performance. In [29], a DL method is introduced for identifying violence in animation videos. The research involved modifying a Faster R-CNN model to handle the intricate aspects of violence depicted in cartoon and animation content. The modification included replacing the model's backbone with a customized RegNet to capture frame features, utilizing a modulated deformable convolutional (MDC) layer instead of the standard inner lateral connection for flexible feature map extraction, and introducing a novel distributed attention module (DAM) within the feature pyramid network to enhance feature extraction. Additionally, to enhance violence detection across diverse scenarios, the researchers incorporated a modified multiscale Region of Interest (ROI) Align. Moreover, the method integrated a classification component into the detection model to categorize different levels of violence within each frame. In [30], an innovative semi-supervised hard attention mechanism was introduced. This mechanism facilitated the identification and separation of crucial regions within videos from less informative segments of the data. By efficiently eliminating redundant data and highlighting valuable visual information at a higher resolution, the model's accuracy saw improvement. This approach obviated the necessity for attention annotations in video violence datasets, rendering them more widely applicable. The proposed model utilized a pre-trained I3D backbone to expedite and stabilize the training process. The ref.[31] proposed a novel approach to enhance the generalization of violence detection across multiple scenarios. This approach employed three pre-trained CNN models, Xception, InceptionV3, and InceptionResNetV2, to extract significant features from RLVS and Hockey datasets. The extracted features from each dataset were then fused into a single feature pool separately. Finally, these feature pools from the first violence scenario and the second were combined into a unified feature space, facilitating the training of an ML classifier capable of generalizing across multiple scenarios. However, it is essential to note that this approach needs to address generalization across multi-task anomaly detection. The article [32] explored advanced techniques to enhance aggression detection in surveillance systems by utilizing multimodal fusion and DL. The study addressed the limitations of traditional single-modality approaches by integrating audio, visual, and text-based features, along with additional meta-information such as Audio-Focus, Video-Focus, Context, History, and Semantics. Four distinct fusion methods were developed and compared: intermediate level fusion, concatenation-based fusion, and two methods involving element-wise operations followed by concatenation. In the paper [33], the authors proposed a method to enhance the detection of abnormal actions, particularly human aggression and car accidents, using wavelet-based channel augmentation. The core of the proposed method was the MultiWave-Net, a spatiotemporal network designed to integrate wavelet transformation with traditional DL architectures such as CNNs and ConvLSTMs. The wavelet-based channel augmentation technique was applied to improve the feature extraction capabilities of these networks, allowing them to better capture both spatial and temporal aspects of the input data. The article [34] proposed an approach to recognizing violent behaviors in videos by leveraging the MLP-Mixer architecture and a new dataset format called Sequential Image Collage (SIC). SIC aggregated video frames into sequential image collages, capturing both spatial and temporal dimensions to enhance the model's understanding of violent actions. These collages, along with original frames, were processed through the MLP-Mixer architecture, which relied solely on Multilayer Perceptrons (MLPs) for computational efficiency. The method involved patch embedding, token mixing, and channel mixing operations to capture both local and global features from the dataset. Despite the literature introducing advanced AI methods to detect and recognize anomaly behaviors in videos, a significant challenge emerges concerning generalization. These methods necessitate retraining the entire model from scratch when a new anomaly class is introduced, resulting in increased time and computational resource requirements, especially in cases where the model is complex or requires extensive training. This challenge presents a practicality and efficiency issue for anomaly detection systems. Therefore, new approaches are urgently needed to address the generalization problem in multi-task anomaly detection without requiring extensive retraining."}, {"title": "3. Materials and Methods", "content": "UCF-Crime dataset and RLVS dataset were employed in this study. The UCF dataset [35] is widely recognized in criminal activity recognition research and was compiled by the University of Central Florida (UCF). It is publicly accessible for research purposes and includes 1,900 untrimmed surveillance videos gathered from platforms such as YouTube, TV news, and documentaries, varying in quality and resolution. This dataset encompasses 128 hours of footage, depicting 13 real-world criminal activities, including abuse, arrest, arson, assault, road accidents, shoplifting, and more. For this investigation, samples were specifically extracted from the \"shoplifting\" and \"normal\" classes, each comprising 50 video clips recorded in retail stores. On the other hand, the RLVS dataset [36] consists of 2,000 video clips, equally divided between violent and normal activities. The violent videos depict physical altercations in various environments, such as streets, prisons, and schools. Videos within the RLVS dataset feature high resolutions, ranging from 480p to 720p, with durations spanning 3 to 7 seconds. A frame extraction process was performed using a 10-frame interval, resulting in six frames per second. Notably, some frames within violent and shoplifting videos were eliminated during the data cleaning phase because they did not depict the relevant actions and were more similar to frames from normal videos.\nThis study utilized four deep CNN models, MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, to tackle the challenge of detecting anomalous video behaviours. These models have several advantages worth noting. They have shown outstanding performance on the ImageNet dataset, a widely recognized benchmark for computer vision tasks. Furthermore, their well-designed architecture excels at feature extraction, allowing them to capture a broad spectrum of features due to their diverse filter sizes, ranging from 1\u00d71 to 7\u00d77. Furthermore, incorporating ReLU activations and residual connections improves the quality of feature representation and addresses the problem of gradient vanishing. Dropout layers and global average pooling (GAP) are also utilized in these models to mitigate the risk of overfitting. Moreover, the incorporation of Batch Normalization layers expedites the training process. These advantages collectively make these models effective methods for VAD.\nThe following subsections provide concise descriptions of the models used in the work."}, {"title": "3.2.1 MobileNetV2 model", "content": "The MobileNetV2 model is an efficient and lightweight CNN technique explicitly designed for mobile and embedded devices. MobileNetV2's architectural composition commences with fully convolutional layers containing 32 filters and encompasses 19 residual bottleneck layers. It consists of two modules, each comprising three layers. These blocks start and end with a 1 \u00d7 1 convolutional layer comprising 32 filters-notably, the second block functions as a fully connected layer with one depth. ReLU activation is applied at various levels throughout the architecture. The primary distinction between the two modules is their stride lengths; the first employs a stride length of 1, while the second utilizes a stride length of 2 [37]. The MobileNetV2 model successfully achieves a delicate equilibrium between model size and performance, rendering it particularly suitable for resource-constrained applications like mobile devices and embedded systems."}, {"title": "3.2.2 InceptionV3 Model", "content": "InceptionV3 [38] is a CNN architecture for image classification and object recognition tasks. It is renowned for its deep structure and utilization of specialized convolutional layers, including Inception modules designed to capture features at varying scales. InceptionV3 employs parallel convolutional layers of different sizes and pooling operations within these modules to effectively capture features at different scales. It leverages factorized convolutions to reduce network parameters, includes auxiliary classifiers to enhance training, and utilizes GAP to mitigate overfitting. Extensive use of batch normalization accelerates training. InceptionV3's depth and parameter count make it suitable for various CV applications. These attributes contribute to InceptionV3's ability to achieve high accuracy in image classification tasks while maintaining manageable computational complexity."}, {"title": "3.2.3 InceptionResNetV2 Model", "content": "InceptionResNetV2 is a deep CNN architecture combining elements from both the Inception and ResNet architecture [39]. It was designed to improve feature learning and representation in CV tasks. InceptionResNetV2 integrates residual connections, similar to ResNet, to facilitate the training of deep networks while also utilizing Inception modules to capture features at multiple scales. This architecture typically includes a stem module to preprocess input data, multiple Inception-ResNet blocks that increase network depth, and final layers for classification or feature extraction. InceptionResNetV2's unique combination of these architectural elements aims to achieve superior performance in tasks such as image classification and object detection."}, {"title": "3.3.4 Xception Model", "content": "The Xception network [40] represents an evolution from Inception by replacing conventional convolution layers with depthwise separable convolution layers. This design optimizes spatial and cross-channel correlations within the network's core functionality. XceptionNet, with 36 convolution layers segmented into 14 modules, supersedes Inception's architecture. It maintains a continuous relationship between the remaining layers after removing the initial and final ones. The network transforms the original image to determine probabilities across multiple input channels and employs 11 depth wise convolutions, offering an alternative to three-dimensional maps by visualizing relationship patterns."}, {"title": "3.3 Part1: Proposed framework, Deep Feature Fusion Approach:", "content": "In order to leverage the features extracted by different CNN models for detecting anomalies in surveillance videos, since each model has its architecture and different filter sizes for feature extraction from input data, combining these features provides a better feature representation. This, in turn, improves overall performance. The proposed deep feature fusion approach (Figure 2) used four CNN models, MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, as feature extractors to capture features from the input video frames. Next, these features, extracted from the individual models, are combined into a unified feature pool. The different colors in the feature pool correspond to the features extracted from different CNN models. The feature pool is then used to train ML classifiers. Finally, ML classifiers were employed to assign class labels and classify human behaviors as normal or abnormal. Six classifiers were used to recognize anomalies in captured videos: SVM, SoftMax, K-Nearest Neighbor (KNN), AdaBoost, Logistic Regression (LogReg), and Naive Bayes classifiers.\nThe deep fusion approach offers several benefits. It provides a flexible means of combining multiple CNN models without the need to train them from scratch. This approach allows for the incorporation of new models trained on specific datasets by extracting features from the final fully connected layer and then inserting them into the feature space. This incorporation method saves time and computational costs, eliminating the need to retrain the already-used individual models. Moreover, the fusion of features extracted from multiple models yields a broader and more inclusive array of information for classifiers to acquire knowledge from. This approach empowers ML classifiers to harness the distinct strengths and characteristics of the individual models, thus enhancing the holistic understanding of the target task. Furthermore, the combination of diverse models can mitigate the risk of overfitting and enhance generalization capability."}, {"title": "3.4 Part2: Proposed framework, Multi-task Classification", "content": "Traditional anomaly detection models struggle with handling new or unforeseen anomalies, making them susceptible to false negatives or misclassifications. In contrast to binary anomaly detection, which deals with only one class of anomalies and one class of normal data, multi-anomaly detection addresses scenarios where multiple types of anomalies are present.\nThe proposed multi-task classification approach introduces a critical innovation that eliminates the need to retrain the entire model from scratch when introducing a new anomaly class. This approach unifies features extracted from various CNN models and multiple datasets with different classes into a single feature space. These features are then used to train ML classifiers. Figure 3 provides a schematic diagram of the multi- task classification model. In the scenario presented in this paper, two feature fusion pools were used: the UCF-features pool and the RLVS-features pool, which fused the features extracted from the UCF and RLVS datasets. The UCF-feature pool consists of features related to normal and violent classes, while the RLVS-features pool includes features representing normal and shoplifting classes.\nTo categorize incoming frames as violent, shoplifting, or normal, the proposed approach utilized the UCF-features pool and RLVS-features pool, along with their corresponding class labels, to create a unified feature space. This feature space was then used to train the ML classifiers to categorize and classify incoming frames based on their respective classes.\nThere are several advantages to incorporating new classes without going through full retraining. This approach can save time and computational resources, especially when dealing with complex models or models that require significant training time. It also enables the system to adapt to emerging anomaly types, which enhances its robustness in dynamic environments. This approach effectively addresses a significant challenge in ML by allowing models to adapt to evolving anomaly patterns efficiently."}, {"title": "3.5 Training", "content": "We conducted experiments involving four individual CNN models: MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, as well as a deep feature fusion model and a multi-task classification model. These experiments were structured as follows:\n\u2022\tTraining and testing each individual CNN model on the UCF dataset.\n\u2022\tTraining and testing each individual CNN model on the RLVS dataset.\n\u2022\tAssessing the performance of the deep fusion model through distinct tests on the UCF and RLVS datasets.\n\u2022\tFinally, evaluating the proposed multi-task classification approach combines the captured features from the UCF and RLVS datasets, each containing different abnormal behaviors."}, {"title": "3.6 Explainable tools", "content": "DL has been considered a complicated and obscure process, often described as a \"black box\" due to the difficulty in comprehending why a model makes specific choices. As a result, this lack of transparency can erode trust in the final decisions made by these systems [41]. Given this concern, this paper adopts the Grad-CAM and t-SNE visualization techniques to tackle these limitations and offer a more comprehensive insight into how DL methods arrive at their decisions.\n1- The Grad-CAM (gradient-weighted class activation mapping) is an interpretability technique designed to elucidate the predictions of any DL model in a coherent and comprehensible manner. This is achieved through a focus on visualizing crucial regions within images. The approach capitalizes on gradients, effectively highlighting areas of the image that wield significant influence over the model's decision-making process. Grad-CAM starts with a forward pass, processing an image through a pre-trained CNN. Backpropagation calculates how changes in each feature map influence the final prediction score for a specific class. GAP computes importance scores for each feature map by averaging gradients. These important scores are used as weights for a weighted sum of feature maps, indicating their impact. The ReLU activation focuses on positive contributions, and the final output is a heatmap highlighting image regions that influenced CNN's decision. Brighter heatmap areas correspond to more influential image regions in classification [42][43]. Researchers can analyze and comprehend the crucial regions to better understand the model's reasoning process. This approach also helps them to verify whether the influential regions align with their expectations, boosting confidence in the model's predictions. Any spelling, grammar, or punctuation errors have been corrected.\n2- t-SNE, which stands for t-Distributed Stochastic Neighbor Embedding, is a non-linear technique for reducing the dimensions of data while preserving the structure at various scales. It is particularly well-suited for visualizing high-dimensional datasets. The low-dimensional representation produced by t-SNE can be plotted, allowing you to visualize clusters, patterns, and relationships in the data that might be difficult to discern in the high-dimensional space. This paper uses t-SNE to understand the fusion techniques and how they improve the feature space."}, {"title": "4. Experimental Results", "content": "This section discusses the experimental results of various deep learning models used for anomaly detection, specifically focusing on tasks such as violence and shoplifting detection in video datasets. It outlines the performance of individual CNN models and a proposed deep feature fusion model, multi-task classification approach assessed using metrics like accuracy, recall, precision, and F1-score."}, {"title": "4.1 Evaluation Metrics", "content": "The utilized models in this study underwent assessment employing a range of evaluation metrics, encompassing accuracy, recall, precision, and the F1-score. AD systems require minimized rates of false positives (FP) and false negatives (FN) while simultaneously maximizing the counts of true positives (TP) and true negatives (TN). TN signifies the count of correctly labeled negative (normal) instances, whereas TP corresponds to correctly labeled positive (anomaly) instances. Conversely, FP and FN counts reveal the number of instances inaccurately labeled as positive or negative [3]. Each evaluation metric is computed as follows:\nAccuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$ (1)\nRecall = $\\frac{TP}{TP + FN}$ (2)\nPrecision = $\\frac{TP}{TP + FP}$ (3)\nF1 score = 2x $\\frac{Precision \\times Recall}{Precision + Recall}$ (4)"}, {"title": "4.2 Experimental Results of Individual CNN models", "content": "The four deep CNN models used in this work were evaluated for performance in VAD tasks by being tested on the UCF and RLVS datasets, as described in subsequent sections."}, {"title": "4.2.1 Experiment Results on UCF Dataset", "content": "The results of the pre-trained CNN models were evaluated based on their accuracy, loss curves for training and validation, and the confusion matrix, as shown in Figure 4. Table 1 presents the evaluation metrics obtained by testing these models on the UCF dataset. MobileNetV2 performed the best in accuracy, precision, and F1 score, making it the ideal choice when minimizing false positives is a priority. Xception, on the other hand, had high recall but lower precision, making it a suitable option when capturing true positives is a priority. InceptionV3 and InceptionResNetV2 achieved a balance between these two metrics. Additionally, Figure 5 displays the Grad-CAM-generated heatmaps for these individual models."}, {"title": "4.2.2 Experiment Results on RLVS Dataset", "content": "The RLVS dataset served as the training and testing data for four distinct models in this specific scenario. These models underwent training and validation across multiple epochs, allowing the measurement of losses, accuracies, and confusion matrices to assess their performance. The results of this evaluation are presented in Figure 6. Table 2 provides a summary of the performance metrics of these models on the RLVS dataset. All four models demonstrated high accuracy, highlighting their proficiency in correctly classifying videos as either violent or normal. The differences in accuracy, recall, and F1 score between the models are minimal, with MobileNet holding a slight edge, demonstrating a strong capability to detect violent instances. InceptionResNet exhibited the highest precision. These models deliver robust performance in violence recognition, characterized by their high accuracy. Figure 7 displays the heatmap generated by Grad-CAM."}, {"title": "4.3. Experimental Results of the deep feature fusion Model", "content": "In the following subsections, we provide detailed information about the experimental results achieved by utilizing the deep Fusion model on both the UCF and RLVS datasets. Each model focused on a distinct region of interest, and the fusion of these four models proved to be highly effective in capturing features for the ML classifiers."}, {"title": "4.3.1 Experimental Results of the Deep Fusion Model on the UCF Dataset", "content": "The proposed feature fusion model was evaluated on the UCF dataset to evaluate its effectiveness in recognizing shoplifting behavior in surveillance videos. As mentioned earlier, this model involved combining features extracted from individual CNN models, which were trained on the UCF dataset, into a unified feature pool. Subsequently, we assessed the model's performance on the UCF testing dataset using six classifiers. Table 3 and Figure 8 present the results and their corresponding confusion"}, {"title": "4.3.2 Experimental Results of The Deep Feature Fusion Model on the RLVS Dataset", "content": "Once again, the proposed deep feature fusion approach outperformed the individual CNN models used in this work when tested on the RLVS dataset for detecting violent activities in videos, achieving an accuracy of 97.99%. This represents an increase of 1.42% over the MobileNetV2 model, which achieved the highest accuracy among the individual models. Table 4 and Figure 10 present the experimental results, including the confusion matrices of the ML classifiers. Figure 11 shows the Feature distribution visualized using t-SNE for RLVS dataset including CNN models and features after fusion."}, {"title": "4.3. Experimental Results of the multi-task classification Model", "content": "We assessed the effectiveness of our multi-task classification model in identifying multiple anomaly classes by using two video anomaly behavior datasets - UCF, which has normal and shoplifting classes, and RLVS, which has normal and violent classes. The proposed model used four pre-trained CNN models as feature extractors to extract the features from the UCF and RLVS datasets. The extracted features from these different models for each dataset were fused to create a single feature set (Figure 12). It is worth noting that the features for normal behavior from both datasets were merged due to their similarity in behavior. After that, ML classifiers were trained to categorize and classify incoming frames as shoplifting, violent, or normal behavior. Table 5 shows our results, including accuracy, recall, precision, and F1 scores for six different classifiers - AdaBoost, KNN, LogReg, SoftMax, Na\u00efve Bayes, and SVM. We also included their respective confusion matrices in Figure 13. AdaBoost showed the highest accuracy at 88.37%, recall at 84.34%, precision at 88.0%, and an F1 score of 85.43%, indicating strong overall performance. KNN also performed well with a high accuracy of 86.45%, recall of 82.05%, precision of 82.62%, and an F1 score of 82.08%. LogReg and SoftMax yielded similar performance metrics with moderate accuracy and F1 scores. SVM and Na\u00efve Bayes showed lower accuracy, recall, precision, and F1 scores compared to the other classifiers."}, {"title": "4.4 Comparative Studies with Recent Research", "content": "In this section, we compared the performance of the proposed deep feature fusion approach to recent research that utilizes DL models to detect anomalous behaviors in surveillance videos, particularly those related to violence and shoplifting crimes. Table 6 presents a comparison of the accuracy values achieved by the proposed deep feature fusion model with existing methods for automatic shoplifting detection using the UCF dataset. On the other hand, table 7 presents the results of the experiments conducted by applying our deep feature fusion model and various DL methods for violence detection on the RLVS dataset.\nOur proposed Deep Fusion approach has demonstrated remarkable superiority over contemporary methodologies by achieving"}]}