{"title": "A Scalable and Generalized Deep Learning Framework for Anomaly Detection in Surveillance Videos", "authors": ["Sabah Abdulazeez Jebur", "Laith Alzubaidi", "Khalid A. Hussein", "Haider Kadhim Hoomod", "Ahmed Ali Saihood", "YuanTong Gu"], "abstract": "Anomaly detection in videos is challenging due to the complexity, noise, and diverse nature of activities such as violence, shoplifting, and vandalism. While deep learning (DL) has shown excellent performance in this area, existing approaches have struggled to apply DL models across different anomaly tasks without extensive retraining. This repeated retraining is time-consuming, computationally intensive, and unfair. To address this limitation, a new DL framework is introduced in this study, consisting of three key components: transfer learning to enhance feature generalization, model fusion to improve feature representation, and multi-task classification to generalize the classifier across multiple tasks without training from scratch when new task is introduced. The framework's main advantage is its ability to generalize without requiring retraining from scratch for each new task. Empirical evaluations demonstrate the framework's effectiveness, achieving an accuracy of 97.99% on the RLVS dataset (violence detection), 83.59% on the UCF dataset (shoplifting detection), and 88.37% across both datasets using a single classifier without retraining. Additionally, when tested on an unseen dataset, the framework achieved an accuracy of 87.25%. The study also utilizes two explainability tools to identify potential biases, ensuring robustness and fairness. This research represents the first successful resolution of the generalization issue in anomaly detection, marking a significant advancement in the field.", "sections": [{"title": "1. Introduction", "content": "Integrating Artificial Intelligence (AI) in various domains has brought transformative changes in our daily lives. One of the major uses of Al lies in the field of surveillance cameras, which enhance security and situational awareness. Anomaly detection (AD), a cutting-edge AI technique, has emerged as a vital tool in the surveillance industry, revolutionizing how we monitor and protect our environments [1]. The process of AI-driven AD employs machine learning (ML) and deep learning\n1"}, {"title": "2. Related Works", "content": "This section provides an overview of the evolving landscape of VAD research, highlighting key studies and methodologies that harness the potential of ML and DL to enhance the accuracy and efficiency of AD systems. A study [11] presented a new DL architecture for identifying violent behaviors in videos. The method leverages recurrent neural networks (RNNs) and 2D CNN to capture spatial and temporal features. Optical flow information is integrated to encode motion patterns. The proposed approach has been tested with success on multiple databases. In [12], the authors proposed an approach to assist monitoring staff in directing their attention to specific screens where the likelihood of a crime is higher. This approach involved identifying situations in video footage that may signal an impending crime. They employed a 3D CNN to examine surveillance videos and capture behavioral attributes for the identification of suspicious actions. The model was trained using carefully chosen videos from the UCF-Crimes dataset. In [13], a Hybrid CNN Framework (HCF) was presented to identify distracted driver behaviors by leveraging DL and image processing techniques. The framework employed pre-trained CNN models in collaboration to extract behavioral features through TL, thereby improving result accuracy. In a study [14], TL was employed to enhance the accuracy of abnormal behavior detection by extracting human motion characteristics from RGB video frames. The authors utilized the VGGNet-19 architecture for feature extraction and subsequently applied a Support Vector Machine (SVM) classifier to identify complex motion scenarios. In [15], several DL models, such as CNN, LSTM (Long Short-Term Memory), CNN-LSTM, and Autoencoder-CNN-LSTM, were explored to identify unusual behaviors in the elderly. The models were trained using temporal and spatial data, which enabled them to make accurate predictions. To tackle the issue of data imbalance, the researchers oversampled minority classes, especially for the LSTM model. Overall, the study provides insights into how DL can be used to detect and prevent unusual behaviors in elderly individuals. In[16], the authors tackled the issue of shoplifting by focusing on detecting suspicious behaviors that may lead to criminal activities. Instead of identifying the crime itself, their approach aims to model and detect behaviors that precede criminal acts, providing opportunities for prevention. They utilized a 3D CNN to extract video features and classify segments containing potential shoplifting behavior. [17] introduced a shoplifting detection system that utilizes a DNN. The system"}, {"title": "3. Materials and Methods", "content": "3.1 Datasets\nUCF-Crime dataset and RLVS dataset were employed in this study. The UCF dataset [35] is widely recognized in criminal activity recognition research and was compiled by the University of Central Florida (UCF). It is publicly accessible for research purposes and includes 1,900 untrimmed surveillance videos gathered from platforms such as YouTube, TV news, and documentaries, varying in quality and resolution. This dataset encompasses 128 hours of footage, depicting 13 real-world criminal activities, including abuse, arrest, arson, assault, road accidents, shoplifting, and more. For this investigation, samples were specifically extracted from the \"shoplifting\" and \"normal\" classes, each comprising 50 video clips recorded in retail stores. On the other hand, the RLVS dataset [36] consists of 2,000 video clips, equally divided between violent and normal activities. The violent videos depict physical altercations in various environments, such as streets, prisons, and schools. Videos within the RLVS dataset feature high resolutions, ranging from 480p to 720p, with durations spanning 3 to 7 seconds. A frame extraction process was performed using a 10-frame interval, resulting in six frames per second. Notably, some frames within violent and shoplifting videos were eliminated during the data cleaning phase because they did not depict the relevant actions and were more similar to frames from normal videos."}, {"title": "3.2 CNN Architectures", "content": "This study utilized four deep CNN models, MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, to tackle the challenge of detecting anomalous video behaviours. These models have several advantages worth noting. They have shown outstanding performance on the ImageNet dataset, a widely recognized benchmark for computer vision tasks. Furthermore, their well-designed architecture excels at feature extraction, allowing them to capture a broad spectrum of features due to their diverse filter sizes, ranging from 1\u00d71 to 7\u00d77. Furthermore, incorporating ReLU activations and residual connections improves the quality of feature representation and addresses the problem of gradient vanishing. Dropout layers and global average pooling (GAP) are also utilized in these models to mitigate the risk of overfitting. Moreover, the incorporation of Batch Normalization layers expedites the training process. These advantages collectively make these models effective methods for VAD. The following subsections provide concise descriptions of the models used in the work.\n3.2.1 MobileNetV2 model\nThe MobileNetV2 model is an efficient and lightweight CNN technique explicitly designed for mobile and embedded devices. MobileNetV2's architectural composition commences with fully convolutional"}, {"title": "3.2.2 InceptionV3 Model", "content": "InceptionV3 [38] is a CNN architecture for image classification and object recognition tasks. It is renowned for its deep structure and utilization of specialized convolutional layers, including Inception modules designed to capture features at varying scales. InceptionV3 employs parallel convolutional layers of different sizes and pooling operations within these modules to effectively capture features at different scales. It leverages factorized convolutions to reduce network parameters, includes auxiliary classifiers to enhance training, and utilizes GAP to mitigate overfitting. Extensive use of batch normalization accelerates training. InceptionV3's depth and parameter count make it suitable for various CV applications. These attributes contribute to InceptionV3's ability to achieve high accuracy in image classification tasks while maintaining manageable computational complexity."}, {"title": "3.2.3 InceptionResNetV2 Model", "content": "InceptionResNetV2 is a deep CNN architecture combining elements from both the Inception and ResNet architecture [39]. It was designed to improve feature learning and representation in CV tasks. InceptionResNetV2 integrates residual connections, similar to ResNet, to facilitate the training of deep networks while also utilizing Inception modules to capture features at multiple scales. This architecture typically includes a stem module to preprocess input data, multiple Inception-ResNet blocks that increase network depth, and final layers for classification or feature extraction. InceptionResNetV2's unique combination of these architectural elements aims to achieve superior performance in tasks such as image classification and object detection."}, {"title": "3.3.4 Xception Model", "content": "The Xception network [40] represents an evolution from Inception by replacing conventional convolution layers with depthwise separable convolution layers. This design optimizes spatial and cross-channel correlations within the network's core functionality. XceptionNet, with 36 convolution layers segmented into 14 modules, supersedes Inception's architecture. It maintains a continuous relationship between the remaining layers after removing the initial and final ones. The network transforms the original image to determine probabilities across multiple input channels and employs 11 depth wise convolutions, offering an alternative to three-dimensional maps by visualizing relationship patterns."}, {"title": "3.3 Part1: Proposed framework, Deep Feature Fusion Approach:", "content": "In order to leverage the features extracted by different CNN models for detecting anomalies in surveillance videos, since each model has its architecture and different filter sizes for feature extraction from input data, combining these features provides a better feature representation. This, in turn, improves overall performance. The proposed deep feature fusion approach (Figure 2) used four CNN models, MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, as feature extractors to"}, {"title": "3.4 Part2: Proposed framework, Multi-task Classification", "content": "Traditional anomaly detection models struggle with handling new or unforeseen anomalies, making them susceptible to false negatives or misclassifications. In contrast to binary anomaly detection,"}, {"title": "3.5 Training", "content": "We conducted experiments involving four individual CNN models: MobileNetV2, InceptionV3, InceptionResNetV2, and Xception, as well as a deep feature fusion model and a multi-task classification model. These experiments were structured as follows:"}, {"title": "3.6 Explainable tools", "content": "DL has been considered a complicated and obscure process, often described as a \"black box\" due to the difficulty in comprehending why a model makes specific choices. As a result, this lack of transparency can erode trust in the final decisions made by these systems [41]. Given this concern, this paper adopts the Grad-CAM and t-SNE visualization techniques to tackle these limitations and offer a more comprehensive insight into how DL methods arrive at their decisions.\n1- The Grad-CAM (gradient-weighted class activation mapping) is an interpretability technique designed to elucidate the predictions of any DL model in a coherent and comprehensible manner. This is achieved through a focus on visualizing crucial regions within images. The approach capitalizes on gradients, effectively highlighting areas of the image that wield significant influence over the model's decision-making process. Grad-CAM starts with a forward pass, processing an image through a pre-trained CNN. Backpropagation calculates how changes in each feature map influence the final prediction score for a specific class. GAP computes importance scores for each feature map by averaging gradients. These important scores are used as weights for a weighted sum of feature maps, indicating their impact. The ReLU activation focuses on positive contributions, and the final output is a heatmap highlighting image regions that influenced CNN's decision. Brighter heatmap areas correspond to more influential image regions in classification [42][43]. Researchers can analyze and comprehend the crucial regions to better understand the model's reasoning process. This approach also helps them to verify whether the influential regions align with their expectations, boosting confidence in the model's predictions. Any spelling, grammar, or punctuation errors have been corrected.\n2- t-SNE, which stands for t-Distributed Stochastic Neighbor Embedding, is a non-linear technique for reducing the dimensions of data while preserving the structure at various scales. It is particularly well-suited for visualizing high-dimensional datasets. The low-dimensional representation produced by t-SNE can be plotted, allowing you to visualize clusters, patterns, and relationships in the data that might be difficult to discern in the high-dimensional space. This paper uses t-SNE to understand the fusion techniques and how they improve the feature space."}, {"title": "4. Experimental Results", "content": "This section discusses the experimental results of various deep learning models used for anomaly detection, specifically focusing on tasks such as violence and shoplifting detection in video datasets."}, {"title": "4.1 Evaluation Metrics", "content": "The utilized models in this study underwent assessment employing a range of evaluation metrics, encompassing accuracy, recall, precision, and the F1-score. AD systems require minimized rates of false positives (FP) and false negatives (FN) while simultaneously maximizing the counts of true positives (TP) and true negatives (TN). TN signifies the count of correctly labeled negative (normal) instances, whereas TP corresponds to correctly labeled positive (anomaly) instances. Conversely, FP and FN counts reveal the number of instances inaccurately labeled as positive or negative [3]. Each evaluation metric is computed as follows:\nAccuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$         (1)\nRecall = $\\frac{TP}{TP + FN}$            (2)\nPrecision = $\\frac{TP}{TP + FP}$          (3)\nF1 score = 2x$\\frac{Precision \\times Recall}{Precision + Recall}$  (4)"}, {"title": "4.2 Experimental Results of Individual CNN models", "content": "The four deep CNN models used in this work were evaluated for performance in VAD tasks by being tested on the UCF and RLVS datasets, as described in subsequent sections.\n4.2.1 Experiment Results on UCF Dataset\nThe results of the pre-trained CNN models were evaluated based on their accuracy, loss curves for training and validation, and the confusion matrix, as shown in Figure 4. Table 1 presents the evaluation metrics obtained by testing these models on the UCF dataset. MobileNetV2 performed the best in accuracy, precision, and F1 score, making it the ideal choice when minimizing false positives is a priority. Xception, on the other hand, had high recall but lower precision, making it a suitable option when capturing true positives is a priority. InceptionV3 and InceptionResNetV2 achieved a balance between these two metrics. Additionally, Figure 5 displays the Grad-CAM-generated heatmaps for these individual models."}, {"title": "4.2.2 Experiment Results on RLVS Dataset", "content": "The RLVS dataset served as the training and testing data for four distinct models in this specific scenario. These models underwent training and validation across multiple epochs, allowing the measurement of losses, accuracies, and confusion matrices to assess their performance. The results of this evaluation are presented in Figure 6. Table 2 provides a summary of the performance metrics of these models on the RLVS dataset. All four models demonstrated high accuracy, highlighting their proficiency in correctly classifying videos as either violent or normal. The differences in accuracy, recall, and F1 score between the models are minimal, with MobileNet holding a slight edge, demonstrating a strong capability to detect violent instances. InceptionResNet exhibited the highest precision. These models deliver robust performance in violence recognition, characterized by their high accuracy. Figure 7 displays the heatmap generated by Grad-CAM."}, {"title": "4.3. Experimental Results of the deep feature fusion Model", "content": "In the following subsections, we provide detailed information about the experimental results achieved by utilizing the deep Fusion model on both the UCF and RLVS datasets. Each model focused on a distinct region of interest, and the fusion of these four models proved to be highly effective in capturing features for the ML classifiers.\n4.3.1 Experimental Results of the Deep Fusion Model on the UCF Dataset\nThe proposed feature fusion model was evaluated on the UCF dataset to evaluate its effectiveness in recognizing shoplifting behavior in surveillance videos. As mentioned earlier, this model involved combining features extracted from individual CNN models, which were trained on the UCF dataset, into a unified feature pool. Subsequently, we assessed the model's performance on the UCF testing dataset using six classifiers. Table 3 and Figure 8 present the results and their corresponding confusion"}, {"title": "4.3.2 Experimental Results of The Deep Feature Fusion Model on the RLVS Dataset", "content": "Once again, the proposed deep feature fusion approach outperformed the individual CNN models used in this work when tested on the RLVS dataset for detecting violent activities in videos, achieving an accuracy of 97.99%. This represents an increase of 1.42% over the MobileNetV2 model, which achieved the highest accuracy among the individual models. Table 4 and Figure 10 present the experimental results, including the confusion matrices of the ML classifiers. Figure 11 shows the Feature distribution visualized using t-SNE for RLVS dataset including CNN models and features after fusion."}, {"title": "4.3. Experimental Results of the multi-task classification Model", "content": "We assessed the effectiveness of our multi-task classification model in identifying multiple anomaly classes by using two video anomaly behavior datasets - UCF, which has normal and shoplifting classes, and RLVS, which has normal and violent classes. The proposed model used four pre-trained CNN models as feature extractors to extract the features from the UCF and RLVS datasets. The extracted features from these different models for each dataset were fused to create a single feature set (Figure 12). It is worth noting that the features for normal behavior from both datasets were merged due to their similarity in behavior. After that, ML classifiers were trained to categorize and classify incoming frames as shoplifting, violent, or normal behavior. Table 5 shows our results, including accuracy, recall, precision, and F1 scores for six different classifiers - AdaBoost, KNN, LogReg, SoftMax, Na\u00efve Bayes, and SVM. We also included their respective confusion matrices in Figure 13. AdaBoost showed the highest accuracy at 88.37%, recall at 84.34%, precision at 88.0%, and an F1 score of 85.43%, indicating strong overall performance. KNN also performed well with a high accuracy of 86.45%, recall of 82.05%, precision of 82.62%, and an F1 score of 82.08%. LogReg and SoftMax yielded similar performance metrics with moderate accuracy and F1 scores. SVM and Na\u00efve Bayes showed lower accuracy, recall, precision, and F1 scores compared to the other classifiers."}, {"title": "4.4 Comparative Studies with Recent Research", "content": "In this section, we compared the performance of the proposed deep feature fusion approach to recent research that utilizes DL models to detect anomalous behaviors in surveillance videos, particularly those related to violence and shoplifting crimes. Table 6 presents a comparison of the accuracy values achieved by the proposed deep feature fusion model with existing methods for automatic shoplifting detection using the UCF dataset. On the other hand, table 7 presents the results of the experiments conducted by applying our deep feature fusion model and various DL methods for violence detection on the RLVS dataset.\nOur proposed Deep Fusion approach has demonstrated remarkable superiority over contemporary methodologies by achieving notable accuracies of 83.59% and 97.99% on the UCF and RLVS datasets, respectively. These substantial performance metrics unequivocally affirm our approach's outstanding quality and efficacy in VAD. Such results establish our method's prowess and signify a substantial stride forward in advancing the current state-of-the-art in this intricate field.\nThe exceptional accuracy attained on both datasets underscores our approach's potential to address real-world challenges in video surveillance and anomaly detection scenarios, signifying its value in practical applications and its potential for further research and development."}, {"title": "4.5 Independent Test", "content": "An independent test involves evaluating a model's performance on data it has not seen or been trained on, a crucial step in assessing the model's generalization capabilities. In this study, we conducted an independent test on the proposed multi-task classification model to evaluate its performance on unseen data with new scenarios and gauge its ability to generalize learned patterns to new information. The model underwent training on the UCF dataset, encompassing shoplifting actions, and the RLVS dataset, which includes instances of violent actions. The model was then tested on a Movie dataset [45] featuring both normal and violent actions. It's worth noting that, unfortunately, we could not acquire a dataset containing shoplifting actions for use in this particular test. Table 8 lists the results achieved by the multi-task classification demonstrated on the movie test set. KNN stands out with the highest accuracy at 87.25% among the presented classifiers. It also exhibits balanced recall at 80.25%, precision at 97%, and an F1 score at 87.87%, indicating robust performance across different metrics. AdaBoost also performs well across all metrics: accuracy at 86.23%, recall at 77.80%, precision at 99.43%, and F1 score at 87.30%. This noteworthy result suggests the model's proficiency in generalizing to sequences of violence across diverse scenarios, validating its robustness and potential practical utility."}, {"title": "5. Conclusion", "content": "This paper addresses the challenging task of detecting anomalies in complex image data that is characterized by noise and diverse actions such as violence, shoplifting, and property destruction. While DL has shown promising results in this domain, previous studies have often needed help with the crucial problem of generalization across different AD tasks without resorting to training from scratch for each new task. This approach is not only time-consuming and computationally expensive but also unfair. To mitigate these issues, our paper introduces a novel DL framework that comprises three key components. First, TL is used to enhance feature generalization. Second, model fusion is employed to improve feature representation, which enhances generalization. Finally, multi-task classification is used to enable the generalization of the classifier across various tasks. Empirical results demonstrate the effectiveness of our approach, surpassing state-of-the-art methods. Using a single classifier, we achieved an impressive accuracy of 97.99% on the RLVS dataset for violence detection, 83.59% on the UCF dataset for shoplifting detection, and 88.37% on both datasets, all without the necessity of training from scratch for each task. To the best of our knowledge, this represents the first successful resolution of the generalization problem in anomaly detection, which is a significant advancement in this domain. In conclusion, our novel DL framework provides a more efficient and fair approach to AD in complex image data."}]}