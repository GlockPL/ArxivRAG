{"title": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning", "authors": ["Chaeyun Jang", "Hyungi Lee", "Jungtaek Kim", "Juho Lee"], "abstract": "Fine-tuning pre-trained models for downstream tasks is a widely adopted tech- nique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an opti- mization trajectory. To tackle the difficulty of choosing the best model, one ef- fective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric land- scapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage proce- dure by integrating Bayesian optimization processes into our framework. Experi- ments across various downstream tasks show considerable performance improve- ments using our Bayesian optimization-guided method. Code will be available at: https://github.com/chaeyoon-jang/bomf.git.", "sections": [{"title": "Introduction", "content": "The field of Natural Language Processing (NLP) has significantly advanced with the pre-training of Transformer-based models on large amounts of texts without supervision. In general, these pre-trained networks are fine-tuned on supervised downstream tasks to solve particular tasks. The rise of Large Language Models (LLMS) such as GPT [50] and LLAMA [63] has increased demands for huge memory and computing during fine-tuning on downstream tasks. In response, low rank approximation methods such as Low-Rank Adaptation (LORA) [22] and Quantized Low-Rank Adaptation (QLORA) [11] have been adopted recently to fine-tune the LLM. However, the fine-tuning of Pretrained Language Models (PLMS) exhibits high sensitivity to marginal variations in hyperparameters such as learning rate and batch size, often leading to training failure and the performance drop of a fine-tuned model [45], while searching hyperparameters requires a vast amount of resources.\nAn effective strategy to seek an optimal model among multiple candidates is model ensembling, which yields impressive performance on generalization and robustness [33]. However, traditional ensemble methods lead to several drawbacks including the space and computational costs that linearly scale with the number of models involved. These issues are particularly pertinent for LLMS, since individual"}, {"title": "Preliminaries", "content": "Problem Setup. In this paper, we explore the process of fine-tuning PLMs using two types of datasets: a downstream training dataset $D_{trn}$ and a validation dataset $D_{val}$. Assuming that we are given a pre-trained set of weights $\\theta_{init}$ and a trainable set of weights $w_{init}$ for our PLM denoted as $M(\\theta, w)$, $w_{init}$ is either a subset of $\\theta_{init}$ or LORA weights [22]. Specifically, in the former case, $w_{init}$ is deliberately selected from $\\theta_{init}$. As a special case, $w_{init}$ will be identical to $\\theta_{init}$ if any layers or weights are not frozen. Meanwhile, if the LORA method is employed during the fine-tuning of our model, $w_{init}$ will be the LORA weights.\nWe use K distinct metrics, denoted f as $f_{metric}^{(k)}(M, D)$ for $k \\in [K]$, to evaluate our model's performance on a given task. Each metric $f_{metric}^{(k)}$ is typically non-differentiable, while a differentiable loss function $f_{loss}$ is employed for training. Assuming that $D_{val}$ is similar to the true data distribution, our goal is to"}, {"title": "Empirical Findings", "content": "In this section, we present empirical observations motivating our model fusion strategy. In \u00a7 3.1, we initially illustrate distinct findings: unlike in computer vision tasks, in NLP tasks, there exists a significant misalignment between the loss and metric surfaces. This misalignment poses a challenge for straightforward model fusion methods when fine-tuning PLMs. In \u00a7 3.2, we find that the optimal fine-tuned hyperparameters for PLMS analogously align across different architectural configurations varying the number of frozen layers or variations in rank in the LORA setting."}, {"title": "On Misalignment in Loss and Metric Landscapes", "content": "The well-known success of uniform averaging, e.g., SWA and Model Soups, in image classification tasks, has been grounded on the flatness of a loss landscape. As one can see in Figure 1a, the use of uniform averaging successfully explores minima on the flatter region of the loss landscape using individual weights close to the flatter region, resulting in enhanced generalization loss on a test dataset. This generalization effect is similarly observed in the case of the metric landscape, as illustrated in Figure 1b, owing to the similarity between the loss and metric landscapes. This similarity is the consequence of the inherent similarity between the loss function and the metric [43]. However, the"}, {"title": "On Hyperparameter Alignment", "content": "Discovering the optimal training hyperparameters incurs significant computational costs, particularly when fine-tuning extensive foundational models [2, 45, 66]. This challenge arises since the ideal set of hyperparameters tends to vary in tandem with changes in both tasks and model structures.\nSurprisingly, our empirical findings reveal a consistent alignment of optimal hyperparameters when fine-tuning PLMS, regardless of variations in the number of frozen layers or the rank of LORA. As illustrated in Figure 2, the alterations in validation loss and metric resulting from changes in the learning rate or batch size exhibit a similar pattern across different numbers of frozen layers, except in the case when all pre-trained layers are frozen and only the classifier layer is trained. This proves that we can decrease computational cost for searching the optimal hyperparameters by tuning on smaller models with more frozen layers or LORA with smaller ranks. Refer to Appendix C.2 to see the additional results when varying the adam beta, learning rate schedule, as well as the case of the LORA."}, {"title": "Bayesian Optimization Model Fusion", "content": "In this section, BOMF unfolds in three key steps. In \u00a7 4.1, we present the process of constructing a set of fine-tuned trainable weights S, serving as components for model fusion. In \u00a7 4.2, we introduce a method to identify optimal hyperparameters crucial in the construction of the set S based on the findings explained in \u00a7 3.2. Finally, we delve into how we conduct weighted averaging in \u00a7 4.3, following the insights discussed in \u00a7 3.1."}, {"title": "Fusion Member Sampling", "content": "To improve the performance of our model through model fusion, it is crucial to carefully create the set S by employing an appropriate weight sampling method. There are two main types of weight sampling methods: 1) sampling from multiple training trajectories [70] and 2) sampling from a single training trajectory with proper learning rate scheduling [25]. However, Wortsman et al. [70] indicate that, when applying model fusion with samples from multiple training trajectories, the performance improvement becomes less significant during the fine-tuning of PLMS compared to vision tasks. This limitation in NLP tasks is attributed to the misalignment in loss and metric surfaces, as discussed in \u00a7 3.1. Furthermore, when employing multiple training trajectories to sample fusion members, the training computation cost increases linearly in proportion to the number of fusion members. This poses a significant challenge, particularly in the context of fine-tuning PLMS. For these reasons, in our approach, we collect our fusion members from a single training trajectory. Since the fine-tuning process of PLMS involves a small number of training epochs and exhibits rapid convergence [41], we start gathering fusion members after 50% of the training epochs are completed. This timing is slightly quicker than the point described in the work [25], which begins collecting after 75% of the training epochs are concluded. Once we start collecting the fusion members, we proceed to uniformly sample 15 members throughout the remaining training epochs. Refer to Appendix A for more details on the process of collecting fusion members."}, {"title": "Hyperparameter Search via Bayesian Optimization", "content": "In the construction of a set of fusion members S from a single training trajectory, the effectiveness of the training trajectory significantly impacts the ultimate metric performance of the fused model weight w. In this context, the effectiveness of a training trajectory refers to the model's metric performance using the best- performing weight within that trajectory on the valida- tion dataset Dval. The correlation in Figure 3 strongly indicates that the performance of the best-performing weight is positively correlated with the performance of the fused weight. Consequently, to achieve the best performance of the fused weight, it becomes crucial to identify the set of optimal hyperparameters \u03bb that re- sults in the most effective training trajectory. However, two primary challenges arise when searching for the optimal hyperparameters \u03bb* that yield the best metric performance: 1) the metric functions ${f_{metric}^{(k)}}_{k=1}^K$ are non-differentiable and 2) we need to efficiently assign computational resources in finding better hyperparameters beyond na\u00efve methods such as grid search. To remedy these two issues simultaneously, in BOMF, we employ BO to find the optimal set"}, {"title": "Multi-Objective Bayesian Optimization for Model Fusion", "content": "After completing the construction of the set S with N individual models, the next stage involves selecting appropriate averaging coefficients \u03b4 \u2208 [0, 1]^N to ensure the enhanced metric performance of a fused model. To achieve this, we can leverage metrics ${f_{meric}^{(k)}}_{k=1}^K$ and apply a BO procedure to obtain optimal averaging coefficients \u03b4*, similar to the optimization process for the hyperparameter set \u039b. However, restricting the consideration to metric performance solely on $D_{val}$ may result in our fused weights w overfitting to Dyal and exhibiting poor generalization to the true data distribution, due to the complex and sharp nature of the metric landscape which is observed in \u00a7 3.1. To tackle this challenge, when optimizing \u03b4, we propose to minimize both $f_{loss}$ and ${f_{meric}^{(k)}}_{k=1}^K$ by employing MOBO identify a Pareto frontier defined as follows:\n$P = {\u03b4* | \u03b4* = arg min_\u03b4 (l(\u03b4), l_{metric}^{(1)} (\u03b4), ..., l_{metric}^{(K)} (\u03b4)) }$,\nwhere $l(\u03b4) := f_{loss} (M(\u03b8_{init}, w(\u03b4)), D_{val})$ and $l_k(\u03b4) := f_{metric}^{(k)} (M(\u03b8_{init}, w(\u03b4)), D_{val})$ for $k \u2208 [K]$. Note that w(\u03b4) denotes a fused set of weights with an averaging coefficient vector \u03b4, i.e., $w(\u03b4) = \\sum_{i=1}^N \u03b4_i w_i$ where $w_i \u2208 S$ for $i \u2208 [N] $and N is the number of models to fuse.\nHere, we utilize the EHVI strategy, which is described in the work by Emmerich et al. [16]. The hypervolume, in this context, is defined as a volume size between P and a reference point r. We set the reference points as a zero vector. To enhance the optimization of the hypervolume improvement objective, we employ the logarithmic form of qNEHVI algorithm [10, 3], which is implemented with the BoTorch framework [4]. As highlighted in \u00a7 2, this algorithm has proven effective in practical multi-objective optimization scenarios. This makes it well-suited to handle the complex and sharp nature of our metric landscape, enabling it to successfully identify the optimal \u03b4*. We run MOBO for a total of 5|S| = 75 iterations to find the optimal coefficients \u03b4*.\nIn our case, additional constraints are in place for executing MOBO, specifically 1) equality constraints and 2) inequality constraints for \u03b4. To address the inequality constraints (i.e., $\u03b4_i \u2265 0$), we follow the work by Gardner et al. [17] to incorporate constraints into the acquisition function. To deal with the equality constraints $\\sum_{i=1}^N \u03b4_i = 1$, we simply normalize the output of the acquisition function. Refer to Algorithm 1 in Appendix B for the summary of BOMF."}, {"title": "Related Work", "content": "Model Fusion for Pre-Trained Language Models. Due to the increasing number of model parameters in recent PLMS, there has been a significant increase in both memory requirements and"}, {"title": "Ablation Study", "content": "Number of Frozen Layers. To analyze the ef- ficiency of memory and compute when using a lightweight model in the BO procedure to find \u03bb*, we conduct a study using RoBERTa-base on the RTE and MRPC datasets. As presented in Table 3, the use of a lightweight model successfully iden- tifies favorable hyperparameters that yield good performance while reducing the number of param- eters by up to 25% and the computation time by up to 66%. This efficiency is achieved by caching outputs from the frozen layers. By systematically freezing layers from the tail of the model, we can cache the outputs from these frozen layers and reuse them during the training process.\nMultiple Objectives. To validate the efficacy of using multiple objectives when determining opti- mal \u03b4, we compare BOMF with single-objective baselines using T5-base on the SQUAD2.0 dataset. In this task, we consider two metrics: F1 score and Exact Match. Table 4 shows that relying on only one specific metric slightly increases the objec- tive metric but results in a significant performance drop for the other metric. This outcome suggests"}, {"title": "Conclusion", "content": "In this paper, we empirically remarked two intriguing findings on loss and metric landscapes and hyperparameter alignment. Then, motivated by the observations mentioned above, we proposed a novel BO-based BOMF algorithm for model fusion. Our method utilizes the BO and MOBO frameworks to seek optimal fine-tuning hyperparameters and averaging coefficients, respectively. We validated that our proposed method exhibits improved performance on both NLU and NLG tasks on middle- and large-scale PLMS.\nLimitations and Future Work. As discussed in \u00a7 3.2, compelling future research involves the the- oretical analysis of the hyperparameter alignment phenomenon. Moreover, we empirically observed that when utilizing quantization-based low-rank approximation methods [11, 37], traditional uniform averaging methods and weighted averaging methods face challenges in effectively aggregating mod- els. These challenges arise from the quantized weight values in the models that behave differently with averaging weights. Another research direction is the development of averaging methods for the quantization-based low-rank approximation methods."}, {"title": "Societal impact", "content": "BOMF does not directly have any positive or negative societal impacts since our algorithm is for fine-tuning and model fusion. However, in the sense of developing LLMS, we can argue the societal impacts of our work. On the positive side, our work can improve the productivity of human beings, e.g., reduction of repeating tasks, and discover new scientific knowledge, e.g., artificial intelligence for scientific discovery. On the other hand, as negative societal impacts, fine-tuning LLMS on downstream tasks can still consume significant compute resources, which leads to climate change. Moreover, since our fine-tuning process aims to optimize specific metrics, there can be a potential risk of optimizing towards malicious metrics such as aggressiveness and violence. Therefore, we should be aware of potential unethical outcomes and consider responsibility in selecting and optimizing these metrics."}, {"title": "Safeguards", "content": "We use publicly available benchmarks and open-source models widely recognized in the LLM research community. Additionally, we do not release proprietary or new datasets or models that could cause the risk of misuse. Although our work potentially has a possibility to undertake inherent misuse that is derived from public benchmarks and open-source models, we think that our method itself does not pose a high risk of misuse."}, {"title": "Details of Experiments", "content": "Our implementation leverages key libraries, including PyTorch 2.0.1 [49], Huggingface Transform- ers [69], and BoTorch [4], to construct a robust framework for our experiments. These experiments are rigorously conducted on high-performance computing hardware, specifically NVIDIA RTX 3090 and NVIDIA RTX A6000 GPUs, to ensure the efficiency and scalability of our models. To further bolster the reproducibility of our results, we meticulously set and documented all experiment seeds, enabling precise replication of our experimental conditions and findings."}, {"title": "Medium-Sized Language Models", "content": "For the ROBERTa model, we evaluated the performance for classification and utilized a subset of the GLUE benchmark [65]. This benchmark serves as a comprehensive evaluation of a language model's overall NLU capabilities. The Recognizing Textual Entailment (RTE) task, which employs neutral and contradiction instances to assign a not-entailment label, is a binary classification task comprising 2,490 training instances and 277 validation instances. The Microsoft Research Paraphrase Corpus (MRPC) [13] consists of sentence pairs and corresponding labels. This task involves binary classification to determine whether a pair of sentences are semantically equivalent, utilizing the F1 score as the metric due to label imbalance. This dataset contains a total of 3,668 training and 408 validation instances. The Stanford Sentiment Treebank (SST-2) [60] includes movie reviews with associated positive/negative labels. The task is binary classification to discern the sentiment of a given sentence as positive or negative, with 67,349 training and 872 validation instances. The Stanford Question Answering Dataset (QNLI) [52] is a question-answering task composed of paragraph- question pairs, where one sentence in the paragraph contains the answer to the human-generated question. This dataset comprises 104,743 training and 5,463 validation instances. The Quora Question Pairs dataset (QQP) [67] involves determining whether two questions are semantically equivalent, again using the F1 score as the metric due to label imbalance, with 363,846 training and 40,430 test instances. Lastly, The Multi-Genre Natural Language Inference Corpus (MNLI) [68] is labeled for textual entailment across genre pairs, primarily consisting of premise and hypothesis sentence pairs. This task predicts the relationship between these sentences in three categories. The dataset includes 392,702 training and 9,815 validation instances, of which we used the matched case of the validation set. We conducted experiments by selecting two datasets from each GLUE benchmark based on their size scale. Additionally, specific details on the fine-tuning methods can be found in Table 5."}, {"title": "Large Language Models", "content": "In our experiments with the LLAMA2-7B model, we focused on two tasks: summarization and dialogue generation. For the summarization task, we employed the Samsung Abstractive Messenger Summarization (SAMSum) dataset [19], which consists of 14,732 training samples, 818 validation samples, and 819 test samples. For the dialogue generation task, we selected the End-to-End NLG Challenge (E2E) dataset [47]. This dataset includes 42,061 training samples, 4,672 validation samples, and 4,693 test samples. Details of our fine-tuning process are provided in Table 7. Notably, in the case of the E2E dataset, the test set typically contains around five common inputs with a variety of labels. To save time, we conducted a generate process for one common input and used the different labels as multiple references to calculate the metrics. Consequently, for evaluation, the sentences generated by the model are based on a unique label, totaling 630 sentences. This accounts for the discrepancy in experimental performance between our study and that presented in the original paper of the E2E dataset [47]. All metrics including BLEU, METEOR, and ROUGE were computed using the Huggingface evaluate library.\nTo demonstrate that fine-tuning is still necessary in specific domains and to show the effectiveness of our method in finding the best model under these circumstances, we conducted evaluations using the Korean Medical Multiple Choice Question Answering (KorMCQA) dataset [32]. For batch learning,"}, {"title": "Bayesian Optimization", "content": "Details of HPBO. In the HPBO experiments, the number of iterations varied depending on the size of each dataset. Specifically, 20 iterations were conducted for the RTE dataset, while 10 iterations"}]}