{"title": "Introducing 8 -XAI: a novel sensitivity-based method\nfor local AI explanations", "authors": ["Alessandro De Carlo", "Enea Parimbelli", "Nicola Melillo", "Giovanna Nicora"], "abstract": "Explainable Artificial Intelligence (XAI) is central to the debate on integrating Artificial Intelligence (AI) and\nMachine Learning (ML) algorithms into clinical practice. High-performing AI/ML models, such as ensemble\nlearners and deep neural networks, often lack interpretability, hampering clinicians' trust in their predictions.\nTo address this, XAI techniques are being developed to describe AI/ML predictions in human-understandable\nterms. One promising direction is the adaptation of sensitivity analysis (SA) and global sensitivity analysis\n(GSA), which inherently rank model inputs by their impact on predictions. Here, we introduce a novel 8-X\u0391\u0399\nmethod that provides local explanations of ML model predictions by extending the 8 index, a GSA metric. The\n8-XAI index assesses the impact of each feature's value on the predicted output for individual instances in both\nregression and classification problems. We formalize the 8-XAI index and provide code for its implementation.\nThe 8-XAI method was evaluated on simulated scenarios using linear regression models, with Shapley values\nserving as a benchmark. Results showed that the 8-XAI index is generally consistent with Shapley values, with\nnotable discrepancies in models with highly impactful or extreme feature values. The 8-XAI index\ndemonstrated higher sensitivity in detecting dominant features and handling extreme feature values.\nQualitatively, the 8-XAI provides intuitive explanations by leveraging probability density functions, making\nfeature rankings clearer and more explainable for practitioners. Overall, the 8-XAI method appears promising\nfor robustly obtaining local explanations of ML model predictions. Further investigations in real-world clinical\nsettings will be conducted to evaluate its impact on Al-assisted clinical workflows.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) and Machine Learning (ML) have advanced significantly, from rule-based systems\nto deep learning techniques, ultimately leading to Foundation Models' development [1]. However, the\napplication of these methods in high-stakes fields like medicine remains confined to research settings [2,3].\nThis limitation is partly due to the potential lack of transparency of some AI methods, such as deep learning,\nwhose internal reasoning process is often obscure. Many of the most performing AI algorithms, including deep\nnetworks and ensembles, are considered \u201cblack boxes\", as they prevent human users from understanding the\nmodel's behavior during classification [4,5]. Various methods in the realm of \"Explainable AI\" (XAI) have\nbeen developed to increase the interpretability of black box models. These approaches aim to provide a global\nor local understanding of the classifier's reasoning process. Global explainability focuses on the overall impact\nof the features on the model's predictions, while local explainability refers to the ability to explain individual\npredictions case by case. Local XAI offers explanations that can enhance understanding of feature\ncontributions within smaller groups of individuals often overlooked by global interpretation techniques [6].\nMany XAI methods are now available. Approaches like LIME provide local explanations by approximating\ncomplex models with simpler, linear models (explainable by design) on a neighborhood of the examples whose\nprediction needs to be explained [7,8]. The strong assumption of these methods is that a simple linear model\ncan be a proper proxy for a complex classifier locally. Conversely, SHAP is a local XAI method based on the\ngame's optimal Shapley values. SHAP assigns to each feature a score (i.e. a Shapley value) that should reflect\nthe importance of the specific feature for the prediction [9]. SHAP has been widely used and several extensions\nwere proposed [10]. Yet, some works have identified issues in SHAP explanations when Shapley values are\nused for feature importance [11], and some corrections have been proposed [12].\nSensitivity Analysis (SA) and Global Sensitivity Analysis (GSA) investigate how the variation of the model\ninputs influences the model output predictions [13\u201315]. Generally, the sources of variation can be related to\nthe uncertainty of the estimation process (e.g., model inputs are parameters identified on a data set) or to the\nvariability of the input in a population (e.g., model inputs are features characterizing each individual) [16].\nGSA is a branch of SA that relies on the multivariate variation of all the model inputs of interest [13]. GSA\ncan be defined as \u201cthe study of how uncertainty in the output of a model can be apportioned to different sources\nof uncertainty in the model input\" [13]. GSA methods allow to rank model inputs (called factors' in the GSA\njargon) according to their impact on the model output variation and to identify the model parameters whose\nuncertainty/variability should be reduced to obtain more reliable model predictions. GSA is usually applied in\nmechanistic and statistical modelling of human and natural systems [15], and several methods have been\nproposed in the literature [17,18]. The most established and widely used is the variance-based GSA, where\nfactors are ranked according to their contribution to the variance decomposition of the model output [19,20].\nDifferently, moment-independent GSA techniques consider the entire distribution of the output rather than a\nsingle statistical moment (e.g., variance) [15,17,27]. In particular, the 8 sensitivity index [28] describes the\nimpact of each model parameter on the output probability density function. The 8 sensitivity index was reported"}, {"title": "Methods", "content": "The 8 GSA index\nLet us consider a generic model\n$Y = g(X)$\n(1)\nwhere Y is the scalar output of the model, g represents the inputs-output relationship and X = {X1, ..., XN} is\nthe RN vector of the model inputs. Within the GSA framework, X is considered as a random variable [13]\nwhich is characterized by a joint probability density function (pdf), f(X). Therefore, Y is a random variable\nwith a pdf, f(Y), which can be calculated through Eq.1 using input samples extracted from f (X).\nThe definition of the 8 index relies on the following considerations [28]. Suppose that one input X\u012f can be fixed\nto a certain value x\u2081, then, the conditional pdf of Y given Xi = x\u00ec, f(Y|X\u2081 = x\u2081), can be defined. The shift\nbetween f(Y) and f(Y|X\u2081 = x\u2081) can be measured as\n$s(Xi) = \\int |f(Y) \u2212 f(Y|X\u2081 = x)|dY.$\n(2)\nAs illustrated in Figure 1, s(X\u012f) represents the difference of the area underlying f (Y) and f (Y|X\u2081 = x*), which\ncorresponds to the impact of fixing Xi to xi on f(Y). X\u012f is a random variable typically assuming more values"}, {"title": null, "content": "than just x\u1ec9. The d sensitivity index for X\u012f can be computed through the expected value of s(Xi) over the entire\ndomain of X\u012f as in Eq.3, where f (Xi) is the marginal pdf of Xi.\n$\u03b4\u2081 = \\frac{1}{2}Ex [s(x\u2081)] = \\int f(x\u2081) [\\int|f(Y) \u2212 f(Y\\|X\u2081)|dY]dx\u2081.$\n(3)\nIt has been demonstrated in [28] that 0 \u2264 \u03b4\u012f \u2264 1, in particular d\u2081 = 0 if and only if Y is independent from Xi\nand X\u012f is uncorrelated from the other Xj, with i \u2260 j A full description of the properties of this sensitivity index\ncan be found in [28]."}, {"title": "Adaptation of the 8 GSA index for local explainability of ML models: the 8-\nXAI index", "content": "Let us introduce h, a generic ML model taking as input a generic RM features vector characterizing each\ninstance/example x = {x\u2081, ..., XM}, and returning a scalar prediction, y = h(x), with y \u2208 R. Consider also the\ntraining set of h containing N examples, X\u2208 RN\u00d7M, representing the knowledge on the domain of interest\n(e.g., sample from a population of interest). The concepts of the GSA can be easily mapped within this\nframework. Indeed, each feature, xi, has a variability in the domain of interest, therefore a joint probability"}, {"title": null, "content": "density function (i.e., pdf) for the features, f(x), can be defined. Consequently, the model output is also\nrandom variable with a pdf, f(y), describing the variation of the predicted variable within its domain.\nGiven a particular example, x* = {x1, ..., XM}, which is assumed to be drawn from f(x), it is possible to\ncompute the model output, y* = h(x*), and its probability density within the domain of the predicted variable,\nf(y = y*). Analogously to Eq.2, the impact of each feature value, x\u2081 = x, on the final model prediction, y* =\nh(x*), can be obtained from Eq.4\n$\u03b4\u2081 = f(y = y*\\|x\u2081 = x) \u2212 f(y = y*).$\n(4)\nIn particular, f(y = y*|x\u2081 = x) is the probability density of y* conditioned by having observed Xi = x.\nTherefore, di is the shift in the probability density function for y = y*, when the feature x\u2081 = x. Indeed, as\n\u03b4\u2081 \u2192 0, f (y = y*|x\u2081 = x\u2081) is close to the probability density due to the variation of the other features in X\n(i.e., f (y = y*)). Conversely, a |di|>0 indicates that x\u2081 = x increases/decreases the likelihood of obtaining\ny*.\nThen, features can be ranked independently from the sign of di, by applying the normalization in Eq. 5\n$\\bar{\u03b4_i} = \\frac{|\u03b4_i|}{\\Sigma \u03b4_i}$\n(5)"}, {"title": "Extension of the 8 -XAI index to binary classification problems", "content": "In the context of classification problems, given a class target, ct, the ML model, h, returns the probability that\nthe example x belongs to ct, y = h(x) = p(x \u2208 ct). Then, given a decision threshold dt, x is labelled with ct\nwhether y \u2265 dt, otherwise with ct (i.e., the opposite of the target class). The variation of the features in the\ndomain of interest can be propagated on y also in that case. Thus, f(y), representing the pdf of p(x \u2208 ct) in\nthe target domain, can be defined. However, in classification problems the focus is on the class predicted by\nthe model rather than on the specific p(x \u2208 ct). Therefore, recalling the properties of pdf, given f(y), the\nprobability of assigning Ct with respect to dt (i.e., the probability of obtaining a p(x \u2208 ct) >\ndt,p(p(x \u2208 ct) > dt)) a can be obtained from Eq.6\n$p(p(x \u2208 c\u2081) > d\u2081) = \\int_{d_t}^{1} f(y) dy.$\n(6)\nIn particular, Eq.6 provides the probability of assigning ct in the target domain by using h. Analogously, it is\npossible to compute this probability conditioned by the observed value for the i \u2013 th feature:\n$p(p(x \u2208 Ct\\|xi = x) > d\u2081) = \\int_{d_t}^{1} f(y\\|xi = x\u2081) dy.$\n(7)\nConsequently, to evaluate the impact of x\u2081 = x in the classification of x*, we can apply Eq. 8 representing\nthe natural extension of Eq. to classification problems"}, {"title": null, "content": "$di = \\int_{d_t}^{1} f(y[xi = x\u2081) \u2212 f(y) dy.$\n(8)\nIntuitively, di quantifies how much x\u2081 = x\u1ec9 increases/decreases the probability of assigning ct with respect to\nthe knowledge of the target domain. More in details, a d\u2081 > 0 indicates that x\u2081 = x pushes h towards\nassigning ct to x*. Conversely, a negative value implies that h is more prone to label x*with ct. Finally, a di\nclose to 0 implies that x\u2081 = x\u2081 does not impact the classification. A feature rank can be obtained by applying\nEq. 5 independently from the sign of di also in that case."}, {"title": "Numerical implementation of the 8-XAI method", "content": "The aim of this section is to provide a description of the implemented numerical algorithm to compute the d\nindices. The core of the numerical procedure is to robustly estimate pdf of the ML model output, both\nunconditioned and conditioned, with a data driven approach. This implicitly leads to consider training set, X,\nas a representation of knowledge characterizing the domain of interest. However, X is only a sample drawn\nfrom the joint pdf of the features, f(x). Therefore, it is necessary to consider sampling uncertainty when\nestimating f (y), f(y|x\u2081 = x) and, consequently, di . To this end, Monte Carlo simulations based on bootstrap\nsamplings from X are leveraged to compute the 8 indices (Listing 1). This approach allows to compute some\nstatistics for each \u2642 (i.e., median an interquartile range) describing the typical value and the variability of the\nranking index. The sign of direpresenting whether a feature value increase/decrease the probability of\nobserving a certain model output, is established considering the signs of di observed in all the Monte Carlo\nsimulations. If more than the 95% of bootstrap samples of di were positive/negative, it is possible to assign a\nsign to di. Conversely, there is not enough evidence to determine it."}, {"title": "Results", "content": "The introduced local explainability framework based on 8 index was benchmarked against the current state-\nof-art Shapley values. Simple linear regression models were considered in this evaluation framework due to\ntheir intrinsic features ranking characteristics. Indeed, regression coefficients, \u1e9ei, provides a measure of the\nimpact of each feature on the final prediction. In this section, the results of the performed comparisons are\nreported."}, {"title": "Case 1", "content": "Let us consider the linear regression model in Eq.9, with all Xi~N(0,1):\n$Y = 100 X1 +50 \u00b7 X2 + 50 X3.$\n(9)\nSuppose that the goal is to quantify the contributes of each feature in predicting x* = {0,0,2} which produces\na model output y* = 100. Panels A-C of Figure 4 illustrates the features ranking based on the 8 index. In\nparticular, the most impacting variable, X3, has a positive contribute as it increases the probability density of\nobserving y*. X\u2081 and X2 follows in the ranking, with X\u2081 being more important than X2 due to its higher \u03b2\ncoefficient. Interestingly, X\u2081 = 0 gives a negative contribute as it lowers the probability density of having a\nmodel prediction equal to 100. Conversely, the contribute of X2 is almost 0 as when it is fixed to 0 it does not"}, {"title": "Model 2", "content": "Let us consider the linear model in Eq.10, with Xi~N(0,1) and X\u2081 being significantly more impactful than the\nothers due to its bigger \u1e9e coefficient.\n$Y = 1000 X\u2081 + 50X2 + 50X3.$\n(10)"}, {"title": null, "content": "Consider the instance x* = {0,0,2}, already used for the model in Eq.9 (Figure 4). When Eq.10 is used to\npredict x*, the 8 index-ranking returns x\u2081 = 0 as the most impacting feature due to its dramatic impact on the\nprobability of observing an output equal to 100 (Panel A and C, Figure 8). Differently, Shapley values give\nhigher importance to x3 = 2, i.e., the only feature different from zero (Panel B of Figure 8). It was observed\nthat at least a \u03b2 = 100000 for X\u2081 in Eq.10 brings Shapley to return a features ranking similar to those of the\n8-\u03a7\u0391\u0399 (Panel D of Figure 8)."}, {"title": "Impact of correlations between model features", "content": "The following examples deal with assessing the performance of 8 index in the presence of correlated features.\nIn particular, the first experiment evaluates the robustness of the methodology in when a feature not included\nin the model, is strongly correlated with a feature which is included in the model. To this end, in model in Eq.9\nan external variable, X4~N(0,1) with p(X1, X\u2084) = 0.99, was considered. Given the instance x* =\n{0.1,0.1,0.1,1}, the resulting ranking should not contemplate any contribute for X4. As illustrated by Figure 9,\nboth the 8 and Shapley methods assign a null coefficient to X4. Furthermore, 8-XAI results are coherent with\nthe scenario without correlation (Figure 5). Shapley values are still able to detect x\u2081 as the most impacting\nfeature analogously to the case without correlation (Panel B of Figures 10 and 5). However, when p(X1, X\u2084) =\n0.99 is introduced, Shapley values consider X3 less important (and with a negative effect) than x2 instead of\ngiving them the similar weights as done by the 8-XAI."}, {"title": "Discussions and Conclusion", "content": "XAI is currently at the core of the debate as it could facilitate the integration of AI/ML algorithms within the\nclinical practice. Indeed, the most outperforming AI/ML models (e.g., ensemble learners, deep neural\nnetworks) lack interpretability, thus preventing clinicians from fully trusting their predictions [2,3]. Currently,\nShapley values are the most popular tool among AI practitioners in the healthcare domain. However, different\nworks have highlighted that Shap values can lead to misleading insights on feature importance ranking"}, {"title": null, "content": "[11,33,34]. Therefore, some corrections to Shapley values were proposed and alternative XAI techniques are\ncurrently under development [7,8,12].\nIn particular, the adaptation of sensitivity analysis (SA) and global sensitivity analysis (GSA) to the X\u0391\u0399\ndomain has recently gained momentum. Indeed, SA and GSA are intrinsically able to rank model inputs\naccording to their impact on the prediction [17,32].\nIn this paper, we presented a novel approach called 8-XAI method to provide local explanations of ML model\npredictions leveraging the SA and GSA techniques. Indeed, the 8-XAI was defined by extending the 8 index,\na GSA metric exploited to assess the impact of each model parameter on the output probability density function\n[28]. The 8-XAI index was formalized to locally (e.g., for each specific instance) assess the impact of each\nfeature's value on the predicted output (i.e., both for regression and classification problems) of a supervised\nML model. In particular, the 8-XAI index quantifies how much a feature's value increase/decrease the\nprobability of obtaining a given model prediction. Then, the pseudo-code describing the numerical\nimplementation of the 8-XAI method was provided (Listing 1).\nTo better understand its performances, the proposed 8-XAI index was evaluated on simulated scenarios to\nlocally explain the predictions of linear regression models as they are interpretable by design. Furthermore,\nShapley values were used to benchmark each application of the 8-XAI index. The obtained results showed\nthat:\n\u2022\nThe 8-XAI index is overall coherent with Shapley values. The highest discrepancies between these\nmethods were found in three scenarios: models with a highly impacting feature (Figure 8) and in the\npresence of extreme feature values (Figure 6). In presence of strong correlations, both methods\nreturned similar features rankings correctly detecting both the most important feature and the irrelevant\none (i.e., having a null coefficient in the regression model).\n\u2022\nIn the presence of a model giving very high importance to a single feature (Eq.10), the 8-XAI index is\nmore prone to rank it as the most impacting variable on model prediction, even if it apparently has no\ncontribution (e.g., set to 0 in the linear model of Eq.10). This behavior is very interesting as we showed\nthat it happens with Shapley values too but only when the gap between the main model feature and the\nothers is very large (Figure 8). Therefore, the 8-XAI index showed a higher sensitivity in detecting\ndominant features in the model than Shapley values.\n\u2022\nCompared to the Shapley values, the 8-XAI index is more sensitive to extreme values of the features\n(i.e., far from the typical value in the population). This characteristic emerged in the scenarios\nillustrated in Figures 6 and 7, where, it was shown that, in the absence of a strongly dominant feature\nin the model (Eq.9), the 8-XAI index can provide a features ranking that allows to identify those\nfeatures that have a higher impact on model prediction due to their extreme values. This is an intrinsic\ncharacteristic of this methodology as it leverages feature distributions in their domains to compute a\nfeature ranking. Therefore, the 8-XAI index is potentially able to detect distributional shifts or\nchecking whether the ML model gives the adequate weight of rare conditions in its predictions. This"}, {"title": null, "content": "latter aspect is of great value for a potential application of the 8-XAI within the clinical domain. Indeed,\nin such context some rare conditions (e.g., obesity, mutations) play a crucial role in the decision-\nmaking processes.\n\u2022\nFrom a qualitative perspective, it is our opinion that the 8-XAI provides more intuitive explanations\nthan Shapley values. Indeed, for each instance, the effect of a feature's value on observing a certain\nmodel prediction is described by leveraging the basic concepts of probability density functions. In\naddition to the numerical values of the 8-XAI index, the obtained features ranking can be easily\njustified by using the plots shown in Panels C of Figures 4-10. Thus, differently by Shapley values,\nthe 8-XAI method provides a clearer and \u201cmore explainable\u201d features ranking to practitioners.\nOverall, the obtained results highlighted that the 8-XAI is a promising method to robustly obtain local\nexplanations of ML model predictions. Further investigations on real case studies will be performed to assess\nits impact on the AI-assisted clinical workflow."}]}