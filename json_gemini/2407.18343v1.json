{"title": "Introducing 8 -XAI: a novel sensitivity-based method\nfor local AI explanations", "authors": ["Alessandro De Carlo", "Enea Parimbelli", "Nicola Melillo", "Giovanna Nicora"], "abstract": "Explainable Artificial Intelligence (XAI) is central to the debate on integrating Artificial Intelligence (AI) and\nMachine Learning (ML) algorithms into clinical practice. High-performing AI/ML models, such as ensemble\nlearners and deep neural networks, often lack interpretability, hampering clinicians' trust in their predictions.\nTo address this, XAI techniques are being developed to describe AI/ML predictions in human-understandable\nterms. One promising direction is the adaptation of sensitivity analysis (SA) and global sensitivity analysis\n(GSA), which inherently rank model inputs by their impact on predictions. Here, we introduce a novel 8-X\u0391\u0399\nmethod that provides local explanations of ML model predictions by extending the 8 index, a GSA metric. The\n8-XAI index assesses the impact of each feature's value on the predicted output for individual instances in both\nregression and classification problems. We formalize the 8-XAI index and provide code for its implementation.\nThe 8-XAI method was evaluated on simulated scenarios using linear regression models, with Shapley values\nserving as a benchmark. Results showed that the 8-XAI index is generally consistent with Shapley values, with\nnotable discrepancies in models with highly impactful or extreme feature values. The 8-XAI index\ndemonstrated higher sensitivity in detecting dominant features and handling extreme feature values.\nQualitatively, the 8-XAI provides intuitive explanations by leveraging probability density functions, making\nfeature rankings clearer and more explainable for practitioners. Overall, the 8-XAI method appears promising\nfor robustly obtaining local explanations of ML model predictions. Further investigations in real-world clinical\nsettings will be conducted to evaluate its impact on Al-assisted clinical workflows.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) and Machine Learning (ML) have advanced significantly, from rule-based systems\nto deep learning techniques, ultimately leading to Foundation Models' development [1]. However, the\napplication of these methods in high-stakes fields like medicine remains confined to research settings [2,3].\nThis limitation is partly due to the potential lack of transparency of some AI methods, such as deep learning,\nwhose internal reasoning process is often obscure. Many of the most performing AI algorithms, including deep\nnetworks and ensembles, are considered \u201cblack boxes\", as they prevent human users from understanding the\nmodel's behavior during classification [4,5]. Various methods in the realm of \"Explainable AI\" (XAI) have\nbeen developed to increase the interpretability of black box models. These approaches aim to provide a global\nor local understanding of the classifier's reasoning process. Global explainability focuses on the overall impact\nof the features on the model's predictions, while local explainability refers to the ability to explain individual\npredictions case by case. Local XAI offers explanations that can enhance understanding of feature\ncontributions within smaller groups of individuals often overlooked by global interpretation techniques [6].\nMany XAI methods are now available. Approaches like LIME provide local explanations by approximating\ncomplex models with simpler, linear models (explainable by design) on a neighborhood of the examples whose\nprediction needs to be explained [7,8]. The strong assumption of these methods is that a simple linear model\ncan be a proper proxy for a complex classifier locally. Conversely, SHAP is a local XAI method based on the\ngame's optimal Shapley values. SHAP assigns to each feature a score (i.e. a Shapley value) that should reflect\nthe importance of the specific feature for the prediction [9]. SHAP has been widely used and several extensions\nwere proposed [10]. Yet, some works have identified issues in SHAP explanations when Shapley values are\nused for feature importance [11], and some corrections have been proposed [12].\nSensitivity Analysis (SA) and Global Sensitivity Analysis (GSA) investigate how the variation of the model\ninputs influences the model output predictions [13\u201315]. Generally, the sources of variation can be related to\nthe uncertainty of the estimation process (e.g., model inputs are parameters identified on a data set) or to the\nvariability of the input in a population (e.g., model inputs are features characterizing each individual) [16].\nGSA is a branch of SA that relies on the multivariate variation of all the model inputs of interest [13]. GSA\ncan be defined as \u201cthe study of how uncertainty in the output of a model can be apportioned to different sources\nof uncertainty in the model input\" [13]. GSA methods allow to rank model inputs (called factors' in the GSA\njargon) according to their impact on the model output variation and to identify the model parameters whose\nuncertainty/variability should be reduced to obtain more reliable model predictions. GSA is usually applied in\nmechanistic and statistical modelling of human and natural systems [15], and several methods have been\nproposed in the literature [17,18]. The most established and widely used is the variance-based GSA, where\nfactors are ranked according to their contribution to the variance decomposition of the model output [19,20].\nDifferently, moment-independent GSA techniques consider the entire distribution of the output rather than a\nsingle statistical moment (e.g., variance) [15,17,27]. In particular, the 8 sensitivity index [28] describes the\nimpact of each model parameter on the output probability density function. The 8 sensitivity index was reported"}, {"title": "Methods", "content": "The 8 GSA index\nLet us consider a generic model\n$Y = g(X)$\nAs illustrated in Figure 1, s(X\u012f) represents the difference of the area underlying f (Y) and f (Y|X\u2081 = x*), which\ncorresponds to the impact of fixing Xi to xi on f(Y). X\u012f is a random variable typically assuming more values"}, {"title": "Adaptation of the 8 GSA index for local explainability of ML models: the 8-\nXAI index", "content": "Let us introduce h, a generic ML model taking as input a generic RM features vector characterizing each\ninstance/example x = {x\u2081, ..., XM}, and returning a scalar prediction, y = h(x), with y \u2208 R. Consider also the\ntraining set of h containing N examples, X\u2208 RN\u00d7M, representing the knowledge on the domain of interest\n(e.g., sample from a population of interest). The concepts of the GSA can be easily mapped within this\nframework. Indeed, each feature, xi, has a variability in the domain of interest, therefore a joint probability"}, {"title": "Extension of the 8 -XAI index to binary classification problems", "content": "In the context of classification problems, given a class target, ct, the ML model, h, returns the probability that\nthe example x belongs to ct, y = h(x) = p(x \u2208 ct). Then, given a decision threshold dt, x is labelled with ct\nwhether y \u2265 dt, otherwise with ct (i.e., the opposite of the target class). The variation of the features in the\ndomain of interest can be propagated on y also in that case. Thus, f(y), representing the pdf of p(x \u2208 ct) in\nthe target domain, can be defined. However, in classification problems the focus is on the class predicted by\nthe model rather than on the specific p(x \u2208 ct). Therefore, recalling the properties of pdf, given f(y), the\nprobability of assigning Ct with respect to dt (i.e., the probability of obtaining a p(x \u2208 ct) >\ndt,p(p(x \u2208 ct) > dt)) a can be obtained from Eq.6\n$p(p(x \\in c_t) > d_t) = \\int_{d_t}^{1} f(y) dy.$\nConsequently, to evaluate the impact of x\u2081 = x in the classification of x*, we can apply Eq. 8 representing\nthe natural extension of Eq. to classification problems"}, {"title": "Numerical implementation of the 8-XAI method", "content": "The aim of this section is to provide a description of the implemented numerical algorithm to compute the d\nindices. The core of the numerical procedure is to robustly estimate pdf of the ML model output, both\nunconditioned and conditioned, with a data driven approach. This implicitly leads to consider training set, X,\nas a representation of knowledge characterizing the domain of interest. However, X is only a sample drawn\nfrom the joint pdf of the features, f(x). Therefore, it is necessary to consider sampling uncertainty when\nestimating f (y), f(y|x\u2081 = x) and, consequently, di . To this end, Monte Carlo simulations based on bootstrap\nsamplings from X are leveraged to compute the 8 indices (Listing 1). This approach allows to compute some\nstatistics for each \u2642 (i.e., median an interquartile range) describing the typical value and the variability of the\nranking index. The sign of direpresenting whether a feature value increase/decrease the probability of\nobserving a certain model output, is established considering the signs of di observed in all the Monte Carlo\nsimulations. If more than the 95% of bootstrap samples of di were positive/negative, it is possible to assign a\nsign to di. Conversely, there is not enough evidence to determine it."}, {"title": "Results", "content": "The introduced local explainability framework based on 8 index was benchmarked against the current state-\nof-art Shapley values. Simple linear regression models were considered in this evaluation framework due to\ntheir intrinsic features ranking characteristics. Indeed, regression coefficients, \u1e9ei, provides a measure of the\nimpact of each feature on the final prediction. In this section, the results of the performed comparisons are\nreported."}, {"title": "Case 1", "content": "Let us consider the linear regression model in Eq.9, with all Xi~N(0,1):\n$Y = 100 \\cdot X_1 +50 \\cdot X_2 + 50 \\cdot X_3.$\nSuppose that the goal is to quantify the contributes of each feature in predicting x* = {0,0,2} which produces\na model output y* = 100. Panels A-C of Figure 4 illustrates the features ranking based on the 8 index. In\nparticular, the most impacting variable, X3, has a positive contribute as it increases the probability density of\nobserving y*. X\u2081 and X2 follows in the ranking, with X\u2081 being more important than X2 due to its higher \u03b2\ncoefficient. Interestingly, X\u2081 = 0 gives a negative contribute as it lowers the probability density of having a\nmodel prediction equal to 100. Conversely, the contribute of X2 is almost 0 as when it is fixed to 0 it does not"}, {"title": "Model 2", "content": "Let us consider the linear model in Eq.10, with Xi~N(0,1) and X\u2081 being significantly more impactful than the\nothers due to its bigger \u1e9e coefficient.\n$Y = 1000 \\cdot X_1 + 50 X_2 + 50 X_3.$\nConsider the instance x* = {0,0,2}, already used for the model in Eq.9 (Figure 4). When Eq.10 is used to\npredict x*, the 8 index-ranking returns x\u2081 = 0 as the most impacting feature due to its dramatic impact on the\nprobability of observing an output equal to 100 (Panel A and C, Figure 8). Differently, Shapley values give\nhigher importance to x3 = 2, i.e., the only feature different from zero (Panel B of Figure 8). It was observed\nthat at least a \u03b2 = 100000 for X\u2081 in Eq.10 brings Shapley to return a features ranking similar to those of the\n8-\u03a7\u0391\u0399 (Panel D of Figure 8)."}, {"title": "Impact of correlations between model features", "content": "The following examples deal with assessing the performance of 8 index in the presence of correlated features.\nIn particular, the first experiment evaluates the robustness of the methodology in when a feature not included\nin the model, is strongly correlated with a feature which is included in the model. To this end, in model in Eq.9\nan external variable, X4~N(0,1) with p(X1, X\u2084) = 0.99, was considered. Given the instance x* =\n{0.1,0.1,0.1,1}, the resulting ranking should not contemplate any contribute for X4. As illustrated by Figure 9,\nboth the 8 and Shapley methods assign a null coefficient to X4. Furthermore, 8-XAI results are coherent with\nthe scenario without correlation (Figure 5). Shapley values are still able to detect x\u2081 as the most impacting\nfeature analogously to the case without correlation (Panel B of Figures 10 and 5). However, when p(X1, X\u2084) =\n0.99 is introduced, Shapley values consider X3 less important (and with a negative effect) than x2 instead of\ngiving them the similar weights as done by the 8-XAI."}, {"title": "Discussions and Conclusion", "content": "XAI is currently at the core of the debate as it could facilitate the integration of AI/ML algorithms within the\nclinical practice. Indeed, the most outperforming AI/ML models (e.g., ensemble learners, deep neural\nnetworks) lack interpretability, thus preventing clinicians from fully trusting their predictions [2,3]. Currently,\nShapley values are the most popular tool among AI practitioners in the healthcare domain. However, different\nworks have highlighted that Shap values can lead to misleading insights on feature importance ranking"}]}