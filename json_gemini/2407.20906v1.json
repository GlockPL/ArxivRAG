{"title": "Automated Review Generation Method Based on Large Language Models", "authors": ["Shican Wu", "Xiao Ma", "Dehui Luo", "Lulu Li", "Xiangcheng Shi", "Xin Chang", "Xiaoyun Lin", "Ran Luo", "Chunlei Pei", "Zhi-Jian Zhao", "Jinlong Gong"], "abstract": "Literature research, vital for scientific advancement, is overwhelmed by the vast ocean of available information. Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load. In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 articles, averaging seconds per article per LLM account. Extended analysis of 1041 articles provided deep insights into catalysts' composition, structure, and performance. Recognizing LLMs' hallucinations, we employed a multi-layered quality control strategy, ensuring our method's reliability and effective hallucination mitigation. Expert verification confirms the accuracy and citation integrity of generated reviews, demonstrating LLM hallucination risks reduced to below 0.5% with over 95% confidence. Released Windows application enables one-click review generation, aiding researchers in tracking advancements and recommending literature. This approach showcases LLMs' role in enhancing scientific research productivity and sets the stage for further exploration.", "sections": [{"title": "Introduction", "content": "In scientific research, peer-reviewed academic literature serves as a dense and reliable medium for information dissemination, enabling researchers to push the boundaries of human knowledge by building on previous work[1]. The clarity and rigor of scientific language constrain information dissemination, making it a key carrier in the research process for entity description, concept extraction, information transfer, and consensus building. This ensures that in the transmission and evolution of knowledge, both the sender and receiver of information construct highly consistent models of the referenced objects and concepts on the cognitive level. For instance, the development of industrial catalysts requires a thorough understanding of the materials' structure, chemical properties, and reactivity, considering their activity, selectivity, and stability[2, 3, 4, 5, 6]. This necessitates leveraging foundational catalytic theories and reaction mechanisms detailed in literature. However, the rapid pace of literature publication has outstripped researchers' capacity to assimilate information[7, 8], highlighting the need for tools to efficiently analyze and integrate data, thus avoiding redundant discoveries and broadening research perspectives, ultimately facilitating scientific research processes such as catalyst development.\nNatural language processing (NLP), a branch of machine learning (ML), with core tasks like co-reference resolution and semantic analysis[9], is well-suited as such a tool for literature understanding and integration. NLP technology has evolved through feature engineering, neural network architecture engineering, pre-trained language model development, and prompt-based learning[10]. Recent years have seen NLP applied in catalysis literature for extracting synthetic methods[11, 12, 13, 14, 15, 16], materials and properties[11, 12, 13, 14, 17, 18, 19, 20, 21, 22], key reaction parameters[11, 16, 19, 23, 24, 25, 26], and reactions[14, 24, 27, 28]. However, these studies often focus on isolated aspects, which limits their transferability and requires prior domain knowledge and programming skills, making it challenging for newcomers. Naturally, we considered integrating literature information in the form of reviews, which can also be generalized to more disciplines.\nHowever, early attempts at automated review generation often treated it as a multi-document summarization (MDS) task[29], relying on existing reviews and citation networks[30, 31, 32, 33, 34], thus may not keep pace with rapid advancements in research. Additionally, bibliometric analyses can overlook recent studies due to insufficient citations; focusing solely on abstracts or citations rather than full texts[31, 32, 33, 34] may miss critical details. Focusing on extractive summarization rather than integrated generation[30, 31, 32], or filling in sentence templates[33] can omit significant information or be redundant.\nSince November 2022, Large Language Models (LLMs) like ChatGPT, representing breakthroughs in NLP, have demonstrated unprecedented language comprehension abilities[35]. LLMs excel in zero-shot and few-shot learning, common sense, logical reasoning, and versatility across NLP tasks[36], serving as a second brain for researchers that processes and comprehends extensive scientific literature without additional foundational knowledge[35, 37]. However, LLMs\u2019 creative reprocessing of understood information differs from search engines and poses the challenge of \"hallucinations\", a prevalent but unresolved issue in the industry[37, 38, 39]. Research reveals that calibrated LLMs inevitably generate hallucinations[40], a phenomenon unavoidable in any computable LLM, regardless of model architecture, learning algorithm, prompt techniques, or training data[41]. Hallucinations in LLMs, which often emerge from statistical biases or noise in the training data and strategies for handling ambiguous information, manifest as baseless false information, contextually misaligned or unrelated responses[38]. Hallucinations are exacerbated in specialized domains by limited data exposure, leading LLMs to produce seemingly credible yet misleading outputs with specialized terminology, posing significant risks in scientific research where accuracy is critical. Even advanced LLM like GPT-4 achieve only 73.3% accuracy in professional contexts[42], risking inaccurate academic conclusions and misdirected research, causing considerable time and resource losses[39]. Ensuring hallucination mitigation is crucial for the scientific integrity and reliability of automated review generation. Galactica[43], which claimed review generation capabilities, was withdrawn due to hallucination concerns, highlights this necessity. Therefore, this study emphasizes mitigating hallucinations and enhancing reliability to safeguard the quality of LLM-generated reviews.\nAddressing existing methodological limitations and leveraging LLMs' potential, this study develops an LLM-based, efficient, and comprehensive automated review generation approach. It features an end-to-end pipeline for literature retrieval, reading, summary distillation, and coherent text organization, underpinned by a multi-tier quality control strategy to counter LLM hallucination risks. LLMs' adeptness at information refinement and knowledge building enables accurate entity and concept extraction from texts, bolstering literature handling, re-"}, {"title": "Results", "content": "In our study, automated review generation essentially reprocesses the retrieved information. The process hinges on efficiently retrieving and extracting pertinent information from extensive scientific literature, with the review's quality and scope directly tied to the retrieval process's comprehensiveness and accuracy. We utilized SerpAPI for automated retrieval on Google Scholar, focusing on propane dehydrogenation (PDH) catalysts, covering literature from 1980 to 2024 in top-tier(Q1) chemistry and chemical engineering journals according to the 2022 Chinese Academy of Sciences division table.\nThe automated retrieval yielded 1420 initial results from Google Scholar. To address the challenge of irrelevant or duplicate findings, we implemented a dual-level filtering process. The first level employed quick filtering of abstracts and titles to remove obviously irrelevant documents, serving as a rapid but less precise narrowing method. The second level involved deeper LLM-based analysis of full texts, offering higher accuracy albeit at a slower pace. This coarse-to-fine screening method, reminiscent of high-throughput screening, enabled us to efficiently and accurately identify literature pertinent to our research. The initial screening shortlisted 343 articles as related to our topic. Subsequent LLM evaluation further confirmed 238 of these articles as relevant."}, {"title": "Implementation and analysis of one-click automated review generation", "content": "Using PDH catalysts as an example and building on the aforementioned automated retrieval, we have effectively produced high-quality, specialized review articles. By focusing on top-tier journals, we ensured the retrieval of articles with significant academic impact, offering an accessible starting point for users new to the domain. For those with domain familiarity, the program allows the specification of a custom journal list to refine article selection.\nWe evaluated the efficacy of this method by contrasting two strategies for constructing review topics: one based on existing reviews and another using LLM-generated topics (see Table 2). The examples showcased in subsequent sections and the Supplementary Information are based on outlines derived from existing reviews."}, {"title": "Data mining and visual analysis", "content": "In this study, the data mining module was deployed for comprehensive analysis in the PDH catalysts domain, examining literature from 1980 to 2024 within the chemistry and chemical engineering journals ranked Q1, Q2, and Q3 by the 2022 Chinese Academy of Sciences. Out of 1041 articles filtered by abstracts and titles, 839 were pinpointed as pertinent to PDH research via LLM selection. Leveraging LLMs for data extraction and subsequent analysis, we provided insightful conclusions on catalysts' composition, structure, and performance. This approach not only highlighted PDH research trends but also explored the maximum performance of individual factors and the synergistic effects between variables.\nFor instance, a statistical analysis of the annual publication numbers by catalyst types (see Figure 1 (a)) and sources of performance enhancement (see Figure 1 (b)) showed a surge in alloy research since 1995 and a spike in single-atom catalyst studies post-2015, and primarily driven by advancements in structural composition. This trend underscores the PDH field's evolving focus and hints at fresh avenues for catalyst development, including synthesis methods. In our analysis of the impact of promoter elements (see Figure 1 (c)) and support materials (see Figure 1 (d)) on catalyst performance, including selectivity and stability, we identified that promoter elements such as Zn, Sn, and La, as well as support materials like alumina and zeolites, can achieve notable peak performance, which signaled pathways for catalytic innovation. The combination analysis, for instance, of active site elements with composition elements (see Figure 1 (e)) and alloy structure types with preparation methods (see Figure 1 (f)), revealed that multi-metal systems generally outperform single-metal systems, especially when promoter elements like Sn, Zn, In are used to enhance the performance of Pt-based catalysts. Moreover, impregnation-prepared nanometallic catalysts exhibited superior conversion rates and selectivity, while single-atom alloys showed high selectivity but lower conversion rates.\nThis comprehensive analysis reveals the nuanced interplay between variables, guiding future research towards optimizing catalyst performance, aiding researchers in achieving the optimal performance balance in catalyst design and optimization. It suggests selecting Pt-based catalysts for maximum selectivity or metal oxides for enhanced conversion rates, and conducting deeper exploration into single-atom and nanostructured catalysts, which show promise in exceeding the efficacy of conventional catalysts. These insights not only showcase the diverse characteristics and performance benchmarks within the PDH domain but also highlight LLMs\u2019 utility in scientific exploration, providing researchers with real-time domain understanding and progress perception, thereby fostering catalyst development. This holistic approach empowers researchers to refine catalyst design and optimization effectively, aligning with industrial needs."}, {"title": "Hallucination mitigation", "content": "To address the challenge of hallucinations in LLMs, a high priority has been placed on the detection and prevention of such phenomena. In the entire automated review generation process, we adopted a multi-level filtering and verification quality control strategy, similar to the concept of retrieval-augmented generation (RAG)[44, 45], to mitigate and correct hallucinations:"}, {"title": "Prompt design and task decomposition", "content": "Firstly, we utilized strict and clear text summary guiding prompts, aimed at enhancing the scientific rationality of LLM's outputs and ensuring accuracy and reliability in its analysis and generation processes. Notably, the task of automated review generation aligns well with the strengths of LLMs \u2014information extraction and text generation capabilities. LLMs can rapidly and accurately extract core information from a vast array of literature and integrate it into a coherent and rigorous review text. To enhance efficiency and quality, we deconstructed the core of the review writing process, namely literature reading and summarization, into a series of text summarization tasks. This approach is adopted because summaries generated by LLM significantly surpass manually crafted and fine-tuned model-generated summaries in terms of fluency, factual consistency, and flexibility[46]. By establishing a list of questions, we directed the model to extract relevant content from the literature and respond based on this content, subsequently conducting a comprehensive analysis of all literature citations and responses. Ultimately, the LLM generates high-quality paragraphs closely related to the topic. Additionally, we employed a single-round, segmented generation strategy to avoid truncation limitations of approximately 8K output length. By reasonably segmenting long texts for generation, we not only ensured that the output was completed in a single conversational round but also provided finer parallel granularity to improve generation efficiency. In practice, we divided the 35 questions into 5 groups, ensuring that the generation results for each group could be successfully completed within the 8K limit of the LLM. This granularity avoids efficiency drops due to a high proportion of shared content and identical prompt frameworks, thereby enhancing processing speed while ensuring the quality of text generation."}, {"title": "Hallucination filtering and verification", "content": "To mitigate and rectify hallucinations, we employed a layered filtering and verification approach:\n1. Text format filtering: Noting that hallucinations often disrupt text formatting, we applied a predefined XML format template to filter out disarrayed texts.\n2. DOI verification: DOIs, a combination of symbols and numbers lacking direct semantic linkage to context, present a challenge in generation and are prone to hallucinations. Yet, the precise reference nature of DOIs allows for verification. Through strict DOI verifications on generated content, we suppressed hallucinatory content from advancing further, ensuring each generated conclusion is traceable to its original source.\n3. Relevance verification: Within the RAG system, documents related in semantics but lacking correct answers are particularly detrimental[47]. We scrutinized each response in the knowledge extraction phase to ensure its relevance, eliminating off-topic answers with relevant keywords.\n4. Self-consistency[48] verification: For text summarization, where a definitive correct answer exists, recognizing that the stochasticity of hallucinations means correct answers should recur more frequently across iterations, we employ aggregation from repeated queries to effectively suppress hallucinations.\n5. Full data stream traceability mechanism: By using DOIs as key reference identifiers for each piece of generated content and mandating citations for every conclusion, we enable review readers to easily trace back to the original literature, supporting verification and deeper exploration in topics of interest."}, {"title": "Effectiveness of hallucination mitigation", "content": "In evaluating the effectiveness of hallucination mitigation, we employed a confusion matrix to classify outcomes according to whether the LLM provided content and its pertinence to the original text, differentiating between two types of inaccuracies: false positives, which include fabricated or inconsistent information, and false negatives, referring to overlooked or partially extracted content. Our focus was primarily on reducing false positives, while adopting a relatively tolerant stance on false negatives.\nSubstantial progress was made in mitigating hallucinations. During paragraph generation, only 36% of outputs met criteria following format and DOI validations. This was achieved through 9 repetitions of generating 35 paragraphs, cumulatively resulting in 875 generations. Analyzing 343 relevant articles, we executed 1715 information extractions across 35 questions, yielding 8575 responses and ultimately aggregating to 2783 valid information combinations. Impressively, 84.80% of these outcomes were confirmed by the LLM as 100% consistent with the aggregated results (see Table 3 and Figure 2 (a)), affirming the model's reliability. This method also establishes an rough benchmark for hallucination ratio, facilitating the selection and evaluation of LLMs.\nUpon conducting manual verification on 25 articles each from the knowledge extraction and data mining stages, we calculated the accuracy, false positive rate, 95% confidence interval of the false positive rate, precision, recall, F1 score, and consistency (see Table 3). The 95% confidence interval for the false positive rate was provided by the statsmodels library in Python3. The results are detailed in the following table.\nThe data comparison underscores the efficacy of self-consistency verifications, revealing a substantial decrease in hallucinations, i.e., false positive content, while also compensating for some false negatives, where information was not fully extracted (see Figure 2 (b)). In the knowledge extraction phase, critical for review content, our manual sampling found no fabricated conclusions by LLMs (see Figure 2 (a)), attesting to our method's scientific integrity and reliability. From the sampling results, we are over 95% confident that the likelihood of hallucinations in this part is less than 0.5% (see Table 3). Analysis of false positives in the post-aggregation data mining phase revealed hallucinations typically involved correct numerical extraction but with errors in units or definitions. False negatives mainly stemmed from LLMs' inability to comprehend highly abstract expressions, reflecting a general LLM's limited understanding of highly specialized scientific concepts. The incidence of hallucinations in knowledge extraction was significantly lower than in data mining, as answering questions did not involve converting units and concepts, thus avoiding the most challenging part of testing an LLM's grasp of scientific knowledge. Domain-specific models enhanced by domain-adaptive pretraining (DAPT) [49] are poised to mitigate this issue. Opting not to fine-tune LLMs for specific domains in this study prioritizes out-of-the-box functionality and multi-domain generalization, utilizing a general LLM as the base. Comparisons between RAG and fine-tuning effects in specific domains indicate that RAG sustains efficacy with contextually new knowledge and offers a significantly lower initial cost[50], aligning with our objective to support researchers' entry into diverse fields efficiently.\nConsidering the stringent accuracy requirements in research, increasing the number of repetitions can significantly reduce the probability of hallucinations appearing in aggregated results. Binomial probability calculations indicate that theoretically, a model with 79.09% accuracy yields aggregated prediction accuracies of 93.49%, 96.12%, and 97.64% after five, seven, and nine independent predictions, respectively, aligning with our sampling results (see Table 3). Detailed sampling outcomes and calculations are available in the SI."}, {"title": "Discussion", "content": "In this study, we introduce an innovative LLM-based automated review generation method, adeptly addressing two key scientific challenges: streamlining literature review efficiency and significantly reducing LLM hallucination risks. This modular, comprehensive, end-to-end solution integrates modules for literature search, topic formulation, knowledge extraction, and review composition, transforming an extensive corpus of scientific literature into coherent, detailed, and error-free reviews tailored to specific research themes. Notably, our advanced data mining module offers experienced users an in-depth field overview, exploiting the LLM's analytical prowess. Additionally, a user-friendly one-click program on Windows platforms significantly simplifies the review generation process.\nA pivotal achievement of our method is its capacity to surpass traditional human resource limitations. Our rigorous quality assurance solution, encompassing format filtering, DOI verification, relevance verification and self-consistency verifications, ensures high reliability and traceability throughout the data processing pipeline. Expert evaluations with a case study of PDH catalysts confirm the method's efficacy, with reviews paralleling manual ones in length and citations, but without hallucinations and with impeccable reference accuracy. Through rigorous testing, including the analysis of 875 LLM outputs from a sample of 25 articles, we demonstrate over 95% confidence in reducing the hallucination probability to below 0.5% (see Table 3).\nOur method's modular design offers excellent reusability and scalability. Individual modules like literature search, topic formulation, and knowledge extraction can serve various research purposes, like literature tracking, research topic discovery, and data mining datasets construction. Future enhancements will focus on augmenting LLM's comprehension of scientific concepts through pan-scientific field fine-tuning, elevating the method's overall utility. Planned upgrades include improving multimodal processing, automating scientific inquiries, personalizing text generation, and delving deeper into specific research areas.\nIn summary, our method signifies a major advancement in scientific research tools, offering rapid access to field breakthroughs and developments. It's set to transform the landscape of scientific research, with far-reaching implications for knowledge base construction, literature recommendation, and structured academic writing, heralding a new era in scientific research productivity and interdisciplinary collaboration."}, {"title": "Methods", "content": "The method for constructing review articles consists of four parts: literature search, topic formulation, knowledge extraction and review composition, along with an additional data mining module for experienced users (see Figure 3)."}, {"title": "Literature search", "content": "Initially, a list of journals designated for the set review topic's subject area is obtained from journal classification tables. Then, literature containing specified keywords within these selected journals is retrieved via search engine's API. This is followed by a preliminary filter, checking each title and abstract for intersections with a selected list of keywords. Literature with intersections is saved, and those of a review nature are marked (see Figure 3 (i)). Our method supports various types of textual literature, including journals, patents, conference papers, books, etc. This means that any content in textual form can be included in the search scope, further expanding the application scenarios and coverage of our method. In our example, using \"propane dehydrogenation\" as a keyword, we retrieved 343 publications in top-tier Chemistry and Chemical Engineering journals (2022 Chinese Academy of Sciences classification), including 14 reviews, after filtering titles and abstracts with keywords like \"propane dehydrogenation\", \"PDH\", \"ODH\", \"Oxidative Dehydrogenation\", etc., through SerpAPI on Google Scholar."}, {"title": "Topic formulation", "content": "There are two approaches to constructing review topics: one involves LLM directly drafting the outline, and the other is based on LLM refining and drafting outlines from existing literature reviews. After obtaining a list of topics, additional topics can be manually added and sorted as needed (see Figure 3 (ii)). In our example, the Claude2 model generated an outline including 12 topics directly, and another with 9 topics and 35 guiding questions based on existing review articles (see Table 2)."}, {"title": "Knowledge extraction", "content": "Based on the obtained list of topics, the LLM generates a list of questions for extracting information from literature, corresponding to each review topic. After repeating this process for multiple times for each article, all answers are concatenated. The LLM then determines whether the answers are relevant to the questions and aggregates them (see Figure 3 (iii)). In our example, in the case of PDH, after transforming the 35 guiding questions into questions for extracting information from literature, the Claude2 model was used to extract information from 343 top-tier articles five times, leading to the aggregation of 8575 responses into 2783 valid information combinations."}, {"title": "Review composition", "content": "After associating each article's answers with their source DOI, paragraphs are generated and integrated for each topic. The LLM generates review paragraphs from all the answers combined, followed by summarization and outlook. After repeating multiple times, the LLM scores the generated paragraphs, selecting the best ones for each topic to form a preliminary draft of the full review. The full text is then polished with the help of the LLM, adjusting and checking citation formats to produce the final draft (see Figure 3 (iv)). In our example, each question's answers from various articles were combined into JSON format information groups, inputted into the Claude2 model for paragraph generation, integrated to form smooth paragraphs, repeated 9 times, scored by the Claude2 model based on criteria (as shown in SI), and polished to produce the final draft."}, {"title": "Data mining", "content": "Based on the automated review generation method described above, we proposed a data mining method based on LLMs, catering to users with some domain knowledge. This method is almost identical to the knowledge extraction steps (see Figure 3 (ii)), effectively extracting and aggregating specific data from a large volume of literature. Users first define specific data extraction targets, which may include but are not limited to catalyst types, chemical compositions, and performance characteristics. On the established literature dataset, the LLM parses each article, extracting the user-defined target data multiple times and outputting in XML format. Similar to the knowledge extraction process (see Figure 3 (ii)), the LLM aggregates results from multiple extractions to finalize each article's information for each extraction target. The extracted data often require manual cleaning and processing, including correcting extraction errors, standardizing data formats, and removing redundant information to facilitate subsequent statistical analyses. The cleaned data is then further integrated and analyzed to form visual charts. The code for the cleaning process and chart statistics can be generated by GPT4, requiring no programming background from the user. In our example, in the case of PDH catalysts, relevant literature from tiers one, two, and three was downloaded using the literature search module, filtered through abstracts and titles totaling 1041 articles, of which 839 were deemed PDH-related by the Claude2 model. After data cleaning and processing through Python3 programming, the extracted data included catalyst types, composition elements, active species elements, promoter elements, support materials, alloy structural types, alloy preparation methods, propane partial pressure, reaction temperature, inlet flow rate, selectivity, conversion, selectivity and other key indicators, covering seven categorical variables such as structure and element composition and three continuous variables related to reaction conditions. Subsequently, corresponding charts were generated through code execution. Initially, we tallied the annual publication numbers of various catalyst influencing factors, represented in line or Gantt charts. Furthermore, we calculated the average of the top five selectivity and stability for all influencing factors across all catalyst data, visualized in radar charts to showcase the peak performance achievable by a specific factor. Lastly, through pairwise combination analysis of influencing factors, we produced 45 bivariate correlation bubble charts, intuitively demonstrating how different variable combinations affect overall catalyst performance. These bubble charts use bubble color intensity, size, and border thickness to represent the levels of selectivity, conversion, and stability, respectively."}, {"title": "Data availability", "content": "Our study leverages a dataset compiled from scientific literature acquired through our institution's subscription. Due to copyright considerations, the dataset itself cannot be made publicly available. However, we ensure that our research's integrity and reproducibility do not rely on direct access to these proprietary documents. Instead, we provide extensive documenta-"}]}