{"title": "Expanding the Scope:\nInductive Knowledge Graph Reasoning with\nMulti-Starting Progressive Propagation", "authors": ["Zhoutian Shao", "Yuanning Cui", "Wei Hu"], "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new entities are constantly emerging in the real world. Inductive KG reasoning aims to predict missing facts for these new entities. Among existing models, graph neural networks (GNNs) based ones have shown promising performance for this task. However, they are still challenged by inefficient message propagation due to the distance and scalability issues. In this paper, we propose a new inductive KG reasoning model, MStar, by leveraging conditional message passing neural networks (C-MPNNs). Our key insight is to select multiple query-specific starting entities to expand the scope of progressive propagation. To propagate query-related messages to a farther area within limited steps, we subsequently design a highway layer to propagate information toward these selected starting entities. Moreover, we introduce a training strategy called LinkVerify to mitigate the impact of noisy training samples. Experimental results validate that MStar achieves superior performance compared with state-of-the-art models, especially for distant entities.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) have become a valuable asset for many downstream\nAI applications, including semantic search, question answering, and logic rea-\nsoning [4, 11,33]. Real-world KGs, such as Freebase [1], NELL [21], and DBpe-\ndia [15], often suffer from the incompleteness issue that lacks massive certain\ntriplets [5,12]. The KG reasoning task aims to alleviate incompleteness by dis-\ncovering missing triplets based on the knowledge learned from known facts. Early\nstudies [38] assume that KGs are static, ignoring the potential unseen entities\nand emerging triplets in the continuously updated real-world KGs. This moti-\nvates the task of inductive KG reasoning [32,46], which allows for incorporating\nemerging entities and facts during inference."}, {"title": "Related Work", "content": "KG reasoning has been an active research area due to the incompleteness of KGs.\nTypical KG reasoning models process each triplet independently and extract the\nlatent semantics of entities and relations. To model the semantics of the triplets,\nTransE [2], TransH [39], TransR [17], and RotatE [29] compute translational dis-\ntance variously. RESCAL [22], DistMult [44], and ComplEx [35] follow another\nreasoning paradigm based on semantic matching. Instead of exploring the infor-\nmation implied in a single triplet, R-GCN [28] and CompGCN [36] capture global\nstructure evidence based on graph neural networks (GNNs). These models, how-\never, learn unary fixed embedding from training, which cannot be generalized to\nemerging entities in the inductive KGs. Instead, our model embodies relational\ninformation to encode emerging entities."}, {"title": "Inductive Knowledge Graph Reasoning", "content": "One research line of inductive KG reasoning is rule mining, independent of en-\ntity identities. RuleN [20] and AnyBURL [19] try to prune the process of rule\nsearching. Neural LP [45] and DRUM [27] propose to learn logical rules in an\nend-to-end differentiable manner, learning weights for each relation type and\npath. However, the rules are usually short due to the expensive computation for\nmining and may not be generalized to distant entities.\nAnother research line is subgraph extraction. GraIL [32] extracts subgraphs\naround each candidate triplet and labels the entities with the distance to the\nhead and tail entities. CoMPILE [18], TACT [3], SNRI [43], LogCo [23], and\nConGLR [16] follow a similar subgraph-labeling paradigm. However, the sub-\ngraphs that these models extract convey insufficient information due to sparsity.\nThese models constitute our baselines for inductive KG reasoning."}, {"title": "Conditional Message Passing Neural Networks", "content": "Recently, a variant of GNNs called conditional message passing neural networks\n(C-MPNNs) [10] propagates messages along the relational paths and encodes\npairwise entity embeddings. Given a query head u and a query relation q as\nconditions, C-MPNNs compute embeddings of (v | u, q) for all entity v. To incor-\nporate conditions into embeddings, NBFNet [50] and A*Net [49] initialize the\nhead entity with the embedding of query relation and propagate in the full KG\nfor each GNN layer. However, conditional information passing is still restricted in\nthe neighborhood of the head entity. Differently, RED-GNN [47], AdaProp [48],"}, {"title": "Skip Connection", "content": "Skip connection [8] is a popular technique in deep learning that skips one or\nmore layers. Skipping layers contributes to addressing vanishing or exploding\ngradients [31] by providing a highway for the gradients. ResNet [8] constructs\nthe highway by adding input x and output F(x). DenseNet [9] provides multiple\nhighways by concatenating the input of each layer. These models transmit the\ninput in shallow layers directly to the target deeper layer in an efficient way.\nInspired by skip connection, MStar constructs a highway with several new edges\nto transmit messages faster and propagate to farther entities."}, {"title": "Preliminaries", "content": "Knowledge Graph A KG G = (E,R, F) is composed of finite sets of entities\nE, relations R, and triplets F. Each triplet $f\\in F$ describes a fact from head\nentity to tail entity with a specific relation, i.e., $f = (u, q, v) \\in E \\times R \\times E$, where\nu, q, and v denote the head entity, relation, and tail entity, respectively.\n(Inductive) Knowledge Graph Reasoning To complete the missing triplet\nin real-world KGs, KG reasoning is proposed to predict the target tail entity\nor head entity with a given query (u, q,?) or (?, q, v). Given a source KG G =\n(E, R, F), inductive KG reasoning aims to predict the triplets involved in the\ntarget KG $G' = (E', R', F')$, where $R' \\subseteq R, E' \\subseteq E$, and $F' \\subseteq F$.\nStarting Entities in Progressive Propagation GNNs transmit messages\nbased on the message propagation framework [7,40]. This framework prepares\nan entity set to transmit messages for each propagation step. Full propagation"}, {"title": "Methodology", "content": "The overview of MStar is presented in Fig. 2. Specifically, we first employ the pre-\nembedded GNN to pre-encode all entities. Then, SES selects n query-dependent\nstarting entities according to the pre-embeddings. The highway layer classifies\nstarting entities into m types, considering the correlation between the head entity\nand other starting entities. To improve message-passing efficiency, the highway\nlayer maps each entity type into a new relation and constructs shortcut edges\nbetween the head entity and other starting entities. Based on the message pass-\ning on the shortcut edges, we use the highway layer to obtain conditional entity"}, {"title": "Model Architecture Overview", "content": "$\nv^l =\n\\begin{cases}\nS & l=0 \\\\\n(v^{l-1} \\cup \\{x |\\exists(e,r,x) \\in N(e) \\land e \\in v^{l-1}\\}) & l>0\n\\end{cases}\n$"}, {"content": "where N(e) denotes the neighbor edges of the entity e. In particular, NBFNet\nputs all the entities into S, i.e., $S = E$. RED-GNN only puts the head entity into\nS, i.e., S = {u} with given query (u, q, ?). Too few starting entities limit the scope\nof conditional message passing. On the contrary, too many start entities disperse\nthe attention of GNNs on local information which is critical for reasoning. Our\nmodel MStar strikes a balance by including the head entity and some selected\nquery-dependent starting entities that are helpful for reasoning."}, {"title": "Starting Entities Selection", "content": "As shown in Fig. 1, progressive propagation starts from the only entity (head\nentity) and cannot reach the distant entities. However, the excessive utilization\nof starting entities introduces noisy relational paths into the reasoning. Despite\nthe expansion of the propagation, some starting entities still miss the target\nentities and visit other distant entities unrelated to the query. Thus, we propose\nto select multiple query-dependent starting entities adaptively to cover a farther\narea but not introduce irrelevant noise in reasoning."}, {"title": "Pre-Embedded GNN", "content": "To find the starting entities related to the query, we\nfirst introduce a pre-embedded GNN to learn the simple semantics of the enti-\nties. The pre-embedded GNN transmits messages among all entities in the KG\nfollowing the full propagation paradigm. To explore query-related knowledge,\nthe pre-embedded GNN encodes the relation conditioned on query relation q.\nSpecifically, the computation for message passing is given by\n$h_{e}^{prelu,q}=\\frac{1}{|N(e)|} \\sum\\limits_{(e,r,x)\\in N(e)}(h_{x}^{prelu,q}+r_q)$,\n$r_q = W_r q + b_r$,\nwhere $h_e^{prelu,q}$ denotes the embedding of the entity e in propagation step l,\nq is a learnable embeddings for relation q, $W_r \\in R^{d\\times d}$ is an r-specific learnable\nweight matrix, and $b_r \\in R^d$ is an r-specific learnable bias. d is the dimension\nof both entity and relation embeddings. $r_q$ denotes the embedding of relation r\nconditioned on q. The pre-embedded GNN initializes $h_e^0$ as zero vectors and\nproduces the entity embeddings $h_e^{L_1preu,q}$ after $L_1$ layers of message passing."}, {"title": "Selection", "content": "Provided with the embeddings of entities conditioned on u and q,\nwe design a score function to select query-dependent starting entities. The score\nfunction measures the importance of entities relative to the head entity and\nquery relation. Given an entity e, the importance score $A_{elu,q}$ is defined as\n$A_{elu,q} = W_1^T (ReLU (W_2 (h_{e}^{prelu,q} \\oplus h_{u}^{prelu,q} +q)))$,\nwhere $W_1 \\in R^{1\\times d}$ and $W_2 \\in R^{d\\times 3d}$ are learnable weight matrices. $\\oplus$ denotes\nthe concatenation of two vectors. We keep the top-n entities as starting entity\nset $S_{u,q}$. $S_{u,q}$ can propagate along the relational paths conditioned on the query."}, {"title": "Highway Layer", "content": "Given multiple starting entities, progressive propagation can traverse more enti-\nties, particularly those located at distant positions. The distant entities, however,\nreceive nothing about the conditional information, due to the limited scope of\nconditional message passing. Inspired by the skip connection [8], which allows\nskip-layer feature propagation, we introduce a highway layer to tackle this issue.\nAiming to propagate conditional information to the starting entities, we con-\nsider constructing shortcut edges between the query head entity and the other\nstarting ones. Due to the different semantics of the starting entities, we classify\nentities into m types based on the embeddings. Each type indicates that this\ngroup of entities has a specific semantic relationship with the head entity. Then,\nwe map each entity type to a new semantic relation type and construct new\nedges. Given conditions u, q and entity e, the entity type is defined as follows:\n$\\beta_{elu,q} = arg \\max_{t} W_t^T h_{e}^{prelu,q} , t\\in [1, m]$,\nwhere t is a type of starting entities, and $W_t \\in R^{1\\times d}$ is a t-specific learnable\nweight matrix.\nGiven starting entity types, the highway layer constructs shortcut edges as\n$H_{u,q} = \\{(u, r'^{\\beta_{e\\u,g}}, e) | e \\in S_{u,q} - \\{u\\}\\}$,\nwhere $r'^{\\beta_{elu,g}}$ denotes the new relation that we introduce, corresponding to the\nstarting entity type. These edges act as a skip connection to support skipping\npropagation from the head to the starting entities.\nFinally, the highway layer performs message passing on $H_{u,q}$ to obtain the\nembeddings of the selected starting entities:\n$g_{u,q}(e) = \\sum\\limits_{(e,r,x)\\in N^{highway}(e)} g_{u,q}(x) \\odot r_q,$\nwhere $g_{u,q}(e)$ denotes the embedding of entity e, $N^{highway}(e)$ denotes the neigh-\nbor edges of the entity e in set $H_{u,q}$, and $\\odot$ denotes the point-wise product\nbetween two vectors. To satisfy target entity distinguishability [10], we set a\nlearnable embedding for the head entity u."}, {"title": "Multi-Condition GNN", "content": "In MStar, we introduce a multi-condition GNN to produce the final entity em-\nbeddings. The multi-condition GNN is a C-MPNN conditioned on the head en-\ntity and query relation. Specifically, the multi-condition GNN initializes entity\nembeddings $h^{1}_{u,q}$ as $g_{u,q}$ and propagates from the starting entities progressively.\nGiven the query triplet (u, q, ?), we incorporate the query information into prop-\nagation in two ways.\nFirst, we model the embedding of relation r in an edge as $\\hat{r}_q$ conditioned on\nthe query relation q same as Eq. (2). Second, considering that the semantics of"}, {"title": "Training Strategy: LinkVerify", "content": "To reason the likelihood of a triplet (u, q, e), the decoder produces a score func-\ntion s(.). Given the final output $h_e^{L_2}$ after $L_2$ layers of multi-condition GNN,\nthe score function is given by\n$s (u, q, e) = W_3^T (ReLU (W_4 (h_u^{L_2} \\oplus h_e^{L_2})))$,\nwhere $W_3 \\in R^{1\\times d}$ and $W_4 \\in R^{d\\times 2d}$ are learnable weight matrices. However,\nmulti-condition GNN propagates progressively and probably misses several dis-\ntant target tail entities during the training. In this situation, the prediction\nknows nothing about the target tail entity and brings a noisy score for training.\nTo alleviate the problem above, we propose a mechanism LinkVerify to filter\nnoisy training samples. The noisy sample represents the triplet whose target tail\nentity is not involved in $V_u,q^{L_2}$. Taking the inductive KG reasoning task as a multi-\nlabel classification problem, we use the multi-class log-loss [14,47] to optimize\nthe model. Associated with LinkVerify, the final loss is given by\n$\\mathcal{L} = -\\sum_{(u,q,v) \\in F} (- s (u, q, v) + log(\\sum\\limits_{v'\\in E} (exp (s(u, q, e))))) \\times \\mathbb{1} (v \\in V_u,q^{L_2})$."}, {"title": "Experiments", "content": "In this section, we perform extensive experiments to answer the questions below:\nQ1: Does MStar perform well on inductive KG reasoning?\nQ2: How does each designed module influence the performance?\nQ3: Whether MStar can improve reasoning ability about distant entities or\nnot?"}, {"title": "Experiments Settings", "content": "Datasets We perform inductive KG reasoning experiments on the benchmark\ndatasets proposed in GraIL [32], which are derived from WN18RR [6], FB15k-\n237 [34], and NELL-995 [42]. Each benchmark dataset is divided into four ver-\nsions (v1, v2, v3, v4), and the size typically increases following the version num-\nber. Each version consists of training and test graphs without overlapping enti-\nties. The training graphs contain triplets for training and validation, following a\nsplit ratio of 10:1. The statistics of the datasets are presented in Table 2.\nBaselines We compare MStar with 10 inductive baselines organized into three\ngroups, including (i) three rule-based models: RuleN [20], Neural LP [45], and\nDRUM [27]; (ii) two subgraph-based models: GraIL [32] and COMPILE [18];\n(iii) five C-MPNN-based models: NBFNet [50], A*Net [49], RED-GNN [47],\nAdaProp [48], and RUN-GNN [41].\nEvaluation and Tie Policy Following [47-49], we evaluate all the models using\nthe filtered mean reciprocal rank (MRR) and Hits@10 metrics. The best models\nare chosen according to MRR on the validation dataset. Subgraph-based models\ntypically rank each test triplet among 50 randomly sampled negative triplets,\nwhereas C-MPNNs evaluate each triplet against all possible candidates. In this\npaper, we follow the latter and take the results of rule-based and subgraph-based\nmodels from [48]. Missing results are reproduced by their official code.\nThere are different tie policies [30] to compute MRR when several candidate\nentities receive equal scores. In progressive propagation, all unvisited entities are\nassigned identical scores. Following [41,47], we measure the average rank among\nthe entities in the tie, as suggested in [26]. To keep the tie policy consistent, we\nre-evaluate AdaProp using the official code."}, {"title": "Main Results (Q1)", "content": "Tables 3 and 4 depict the performance of different models on inductive KG rea-\nsoning. MStar demonstrates the best performance across all metrics on FB15k-\n237 and NELL-995, and compares favorably with the top models on WN18RR.\nWe observe that (i) subgraph-based models typically perform poorly. This is\nbecause subgraphs are often sparse or empty and provide less information, par-\nticularly for distant entities. (ii) Rule-based models are generally more competi-\ntive but are still weaker compared to C-MPNN-based models. However, DRUM\noutperforms existing models except MStar in Hits@10 on NELL-995 (v1). NELL-\n995 (v1) is a special dataset and the distance between the head and tail entities\nfor all triplets in the test graph is no longer than 3, which is very short. Thus, we\nconjecture that the length of the learned rules limits the reasoning capabilities\nof rule-based models. Differently, MStar holds an edge over these two groups of\nmodels on all datasets. This suggests that multiple starting entities in MStar\nalleviate the distance limit issues as much as possible when reasoning.\nCompared with the best C-MPNN-based results, MStar achieves an aver-\nage relative gain of 9.9% in MRR, 5.2% in Hits@10 on FB15k-237, and 13.9% in\nMRR, 6.1% in Hits@10 on NELL-995. Existing C-MPNN-based models typically\nuse all entities in the KG or only the head entity as starting entities, without pro-\nviding conditional information to distant entities, which can introduce excessive\nnoise or lack sufficient information. Instead, our MStar selects multiple query-\ndependent starting entities adaptively and propagates conditions farther through\nthe highway for accurate reasoning. Moreover, LinkVerify in MStar additionally\nreduces noisy samples in training. We also observe that the improvement of the\nmodel on WN18RR is not as pronounced as on the other datasets. To provide\ninsights into this phenomenon, we conduct further analysis in Section 5.4."}, {"title": "Ablation Study", "content": "Variants of MStar (Q2) In this section, we design several variants of MStar to\nstudy the contributions of three components: (i) selection, (ii) highway, and (iii)\nLinkVerify in training. The results are summarized in Tables 5 and 6, which indi-\ncate that all components contribute significantly to MStar on the three datasets.\nFirst, the variant of w/o selection propagates only from the head entity which\nis the same as RED-GNN. According to the results, removing selection signif-\nicantly decreases performance, highlighting the effectiveness of using multiple\nstarting entities to explore reasoning patterns across a broader neighborhood.\nSecond, it can be observed that the performance of variant w/o highway is\nworse than MStar. This observation suggests that transmitting query-dependent\ninformation to the starting entities is a promising approach to expedite propa-\ngation for conditions and enhance reasoning accuracy.\nThird, the variant of w/o LinkVerify is inferior to MStar all the time, as\ntriplets with unvisited target entities in training KG introduce noise. Removing\nLinkVerify results in poorer performance, especially on smaller datasets. For"}, {"title": "Conclusion and Future Work", "content": "In this paper, we explore the issue of inefficient message propagation for KG rea-\nsoning and propose a new inductive KG reasoning model called MStar. Specif-\nically, we propose using multiple starting entities to expand the propagation\nscope. Moreover, we construct a highway between the head entity and the other\nstarting entities to accelerate conditional message passing. Additionally, we in-\ntroduce a training strategy LinkVerify to filter inappropriate samples. Experi-\nmental results demonstrate the effectiveness of MStar. In particular, ablation\nresults validate the superiority of MStar for reasoning about distant entities. In\nfuture work, we plan to explore alternative modules for selecting and classify-\ning starting entities. We also intend to investigate methods to effectively utilize\nnoisy triplets during training instead of dropping them."}]}