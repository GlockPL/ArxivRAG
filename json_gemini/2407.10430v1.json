{"title": "Expanding the Scope:\nInductive Knowledge Graph Reasoning with\nMulti-Starting Progressive Propagation", "authors": ["Zhoutian Shao", "Yuanning Cui", "Wei Hu"], "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new entities are constantly emerging in the real world. Inductive KG reasoning aims to predict missing facts for these new entities. Among existing models, graph neural networks (GNNs) based ones have shown promising performance for this task. However, they are still challenged by inefficient message propagation due to the distance and scalability issues. In this paper, we propose a new inductive KG reasoning model, MStar, by leveraging conditional message passing neural networks (C-MPNNs). Our key insight is to select multiple query-specific starting entities to expand the scope of progressive propagation. To propagate query-related messages to a farther area within limited steps, we subsequently design a highway layer to propagate information toward these selected starting entities. Moreover, we introduce a training strategy called LinkVerify to mitigate the impact of noisy training samples. Experimental results validate that MStar achieves superior performance compared with state-of-the-art models, especially for distant entities.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) have become a valuable asset for many downstream AI applications, including semantic search, question answering, and logic reasoning [4, 11,33]. Real-world KGs, such as Freebase [1], NELL [21], and DBpedia [15], often suffer from the incompleteness issue that lacks massive certain triplets [5,12]. The KG reasoning task aims to alleviate incompleteness by discovering missing triplets based on the knowledge learned from known facts. Early studies [38] assume that KGs are static, ignoring the potential unseen entities and emerging triplets in the continuously updated real-world KGs. This motivates the task of inductive KG reasoning [32,46], which allows for incorporating emerging entities and facts during inference.\nDue to their excellent efficiency and performance, conditional message passing neural networks (C-MPNNs), such as NBFNet [50] and RED-GNN [47], have emerged as one of the premier models in the field of inductive KG reasoning. To transmit conditions, existing C-MPNNs only incorporate conditional information into the head entity and propagate along the relational paths progressively. However, this single-starting-entity strategy results in a limited conditional message passing scope, leading to the failure of message passing from the head entity to distant target entities. This inspires us to extend the scope of conditional message passing to support reasoning on target entities in a farther area.\nWe conduct an empirical study to analyze the drawbacks of the limited message passing scope. Specifically, we report the results of a C-MPNN, RED-GNN [47], on predicting target entities at different distances in Table 1. It can be observed that RED-GNN performs poorly for queries with distant target entities, even stacking more message-passing layers. This indicates that existing C-MPNNs cannot effectively propagate conditional messages toward distant target entities, hindering performance on these queries. Although stacking more GNN layers can alleviate this issue to some extent, it causes high computation and performance declines on the queries with target entities nearby.\nIn this paper, we propose a novel inductive KG reasoning model MStar based on Multi-Starting progressive propagation, which expands the scope of efficient conditional message passing. Our key insight is to utilize more conditional starting entities and create shortcuts between the head entity and them. Specifically, we design a starting entities selection (SES) module and a highway layer to select multiple starting entities and create shortcuts for conditional message passing, respectively. First, the SES module encodes entities using a pre-embedded GNN and then selects multiple query-dependent starting entities, which may include entities distant from the head entity. These entities broaden the scope of subsequent progressive propagation and allow MStar to propagate along query-related relational paths to enhance reasoning concerning distant entities. Second, we create shortcuts from the head entity to the selected multiple starting entities in the highway layer. The design of the highway layer is inspired by skip connection from ResNet [8]. The conditional message can be passed to distant entities through the highway layer. For example, in Fig. 1, a 3-layer RED-GNN would"}, {"title": "3 Preliminaries", "content": "Knowledge Graph A KG $G = (E,R, F)$ is composed of finite sets of entities $E$, relations $R$, and triplets $F$. Each triplet $f \\in F$ describes a fact from head entity to tail entity with a specific relation, i.e., $f = (u, q, v) \\in E \\times R \\times E$, where $u$, $q$, and $v$ denote the head entity, relation, and tail entity, respectively.\n(Inductive) Knowledge Graph Reasoning To complete the missing triplet in real-world KGs, KG reasoning is proposed to predict the target tail entity or head entity with a given query $(u, q,?)$ or $(?, q, v)$. Given a source KG $G = (E, R, F)$, inductive KG reasoning aims to predict the triplets involved in the target KG $G' = (E', R', F')$, where $R' \\subseteq R$, $E' \\not\\subseteq E$, and $F' \\not\\subseteq F$.\nStarting Entities in Progressive Propagation GNNs transmit messages based on the message propagation framework [7,40]. This framework prepares an entity set to transmit messages for each propagation step. Full propagation"}, {"title": "4 Methodology", "content": "The overview of MStar is presented in Fig. 2. Specifically, we first employ the pre-embedded GNN to pre-encode all entities. Then, SES selects $n$ query-dependent starting entities according to the pre-embeddings. The highway layer classifies starting entities into $m$ types, considering the correlation between the head entity and other starting entities. To improve message-passing efficiency, the highway layer maps each entity type into a new relation and constructs shortcut edges between the head entity and other starting entities. Based on the message passing on the shortcut edges, we use the highway layer to obtain conditional entity"}, {"title": "4.1 Model Architecture Overview", "content": "The overview of MStar is presented in Fig. 2. Specifically, we first employ the pre-embedded GNN to pre-encode all entities. Then, SES selects $n$ query-dependent starting entities according to the pre-embeddings. The highway layer classifies starting entities into $m$ types, considering the correlation between the head entity and other starting entities. To improve message-passing efficiency, the highway layer maps each entity type into a new relation and constructs shortcut edges between the head entity and other starting entities. Based on the message passing on the shortcut edges, we use the highway layer to obtain conditional entity"}, {"title": "4.2 Starting Entities Selection", "content": "As shown in Fig. 1, progressive propagation starts from the only entity (head entity) and cannot reach the distant entities. However, the excessive utilization of starting entities introduces noisy relational paths into the reasoning. Despite the expansion of the propagation, some starting entities still miss the target entities and visit other distant entities unrelated to the query. Thus, we propose to select multiple query-dependent starting entities adaptively to cover a farther area but not introduce irrelevant noise in reasoning.\nPre-Embedded GNN To find the starting entities related to the query, we first introduce a pre-embedded GNN to learn the simple semantics of the entities. The pre-embedded GNN transmits messages among all entities in the KG following the full propagation paradigm. To explore query-related knowledge, the pre-embedded GNN encodes the relation conditioned on query relation $q$. Specifically, the computation for message passing is given by\n$h_{e}^{prelu,q} = \\frac{1}{|N(e)|} \\sum_{(e,r,x)\\in N(e)}(h_{x}^{prelu,q}+r_q)$\n$r_q = W_rq+b_r$,\nwhere $h_{e}^{prelu,q}$ denotes the embedding of the entity $e$ in propagation step $l$, $q$ is a learnable embeddings for relation $q$, $W_r \\in R^{d\\times d}$ is an $r$-specific learnable weight matrix, and $b \\in R^{d}$ is an $r$-specific learnable bias. $d$ is the dimension of both entity and relation embeddings. $r_q$ denotes the embedding of relation $r$ conditioned on $q$. The pre-embedded GNN initializes $h_{e}^{0}$ as zero vectors and produces the entity embeddings $h_{e}^{preu,q}$ after $L_1$ layers of message passing.\nSelection Provided with the embeddings of entities conditioned on $u$ and $q$, we design a score function to select query-dependent starting entities. The score function measures the importance of entities relative to the head entity and query relation. Given an entity $e$, the importance score $\\delta_{elu,q}$ is defined as\n$\\delta_{elu,q} = W_1 \\big(ReLU \\big(W_2 (h_{e}^{prelu,q} \\oplus h_{u}^{prelu,q} +q)\\big)\\big)$,", "latex": ["h_{e}^{prelu,q} = \\frac{1}{|N(e)|} \\sum_{(e,r,x)\\in N(e)}(h_{x}^{prelu,q}+r_q)", "r_q = W_rq+b_r", "\\delta_{elu,q} = W_1 \\big(ReLU \\big(W_2 (h_{e}^{prelu,q} \\oplus h_{u}^{prelu,q} +q)\\big)\\big)"]}, {"title": "4.3 Highway Layer", "content": "Given multiple starting entities, progressive propagation can traverse more entities, particularly those located at distant positions. The distant entities, however, receive nothing about the conditional information, due to the limited scope of conditional message passing. Inspired by the skip connection [8], which allows skip-layer feature propagation, we introduce a highway layer to tackle this issue.\nAiming to propagate conditional information to the starting entities, we consider constructing shortcut edges between the query head entity and the other starting ones. Due to the different semantics of the starting entities, we classify entities into $m$ types based on the embeddings. Each type indicates that this group of entities has a specific semantic relationship with the head entity. Then, we map each entity type to a new semantic relation type and construct new edges. Given conditions $u$, $q$ and entity $e$, the entity type is defined as follows:\n$\\beta_{elu,q} = arg\\ max_t W_t^T h_{e}^{prelu,q}, t\\in [1, m]$,\nwhere $t$ is a type of starting entities, and $W_t \\in R^{1\\times d}$ is a $t$-specific learnable weight matrix.\nGiven starting entity types, the highway layer constructs shortcut edges as\n$H_{u,q} = \\{(u, r'^{\\beta_{elu,q}}, e) | e \\in S_{u,q} - \\{u\\}\\}$\nwhere $r'^{\\beta_{elu,q}}$ denotes the new relation that we introduce, corresponding to the starting entity type. These edges act as a skip connection to support skipping propagation from the head to the starting entities.\nFinally, the highway layer performs message passing on $H_{u,q}$ to obtain the embeddings of the selected starting entities:\n$g_{u,q}(e) = \\sum_{(e,r,x)\\in N^{highway}(e)} g_{u,q}(x) r_q$", "latex": ["\\beta_{elu,q} = arg\\ max_t W_t^T h_{e}^{prelu,q}, t\\in [1, m]", "H_{u,q} = \\{(u, r'^{\\beta_{elu,q}}, e) | e \\in S_{u,q} - \\{u\\}\\}", "g_{u,q}(e) = \\sum_{(e,r,x)\\in N^{highway}(e)} g_{u,q}(x) r_q"]}, {"title": "4.4 Multi-Condition GNN", "content": "In MStar, we introduce a multi-condition GNN to produce the final entity embeddings. The multi-condition GNN is a C-MPNN conditioned on the head entity and query relation. Specifically, the multi-condition GNN initializes entity embeddings $h_{u}^{1,q}$ as $g_{u,q}$ and propagates from the starting entities progressively. Given the query triplet $(u, q, ?)$, we incorporate the query information into propagation in two ways.\nFirst, we model the embedding of relation $r$ in an edge as $\\hat{r}_q$ conditioned on the query relation $q$ same as Eq. (2). Second, considering that the semantics of"}, {"title": "4.5 Training Strategy: LinkVerify", "content": "To reason the likelihood of a triplet $(u, q, e)$, the decoder produces a score function $s(.). Given the final output $h_e^{L_2}$ after $L_2$ layers of multi-condition GNN, the score function is given by\n$s (u, q, e) = W_3 \\big(ReLU \\big(W_4 (h_u^{L_2} \\oplus h_e^{L_2})\\big)\\big)$,\nwhere $W_3 \\in R^{1\\times d}$ and $W_4 \\in R^{d\\times 2d}$ are learnable weight matrices. However, multi-condition GNN propagates progressively and probably misses several distant target tail entities during the training. In this situation, the prediction knows nothing about the target tail entity and brings a noisy score for training.\nTo alleviate the problem above, we propose a mechanism LinkVerify to filter noisy training samples. The noisy sample represents the triplet whose target tail entity is not involved in $V_{u,q}^{L_2}$. Taking the inductive KG reasoning task as a multi-label classification problem, we use the multi-class log-loss [14,47] to optimize the model. Associated with LinkVerify, the final loss is given by\n$\\mathcal{L} = \\sum_{(u,q,v) \\in F} \\big( - s (u, q, v) + log(\\sum_{v \\in E} (exp (s(u, q, e)))) \\big) \\times \\mathbb{1} (v \\in V_{u,q}^{L_2})$.", "latex": ["s (u, q, e) = W_3 \\big(ReLU \\big(W_4 (h_u^{L_2} \\oplus h_e^{L_2})\\big)\\big)", "\\mathcal{L} = \\sum_{(u,q,v) \\in F} \\big( - s (u, q, v) + log(\\sum_{v \\in E} (exp (s(u, q, e)))) \\big) \\times \\mathbb{1} (v \\in V_{u,q}^{L_2})"]}, {"title": "5 Experiments", "content": "In this section, we perform extensive experiments to answer the questions below:\nQ1: Does MStar perform well on inductive KG reasoning?\nQ2: How does each designed module influence the performance?\nQ3: Whether MStar can improve reasoning ability about distant entities or not?"}, {"title": "5.1 Experiments Settings", "content": "Datasets We perform inductive KG reasoning experiments on the benchmark datasets proposed in GraIL [32], which are derived from WN18RR [6], FB15k-237 [34], and NELL-995 [42]. Each benchmark dataset is divided into four versions (v1, v2, v3, v4), and the size typically increases following the version number. Each version consists of training and test graphs without overlapping entities. The training graphs contain triplets for training and validation, following a split ratio of 10:1. The statistics of the datasets are presented in Table 2.\nBaselines We compare MStar with 10 inductive baselines organized into three groups, including (i) three rule-based models: RuleN [20], Neural LP [45], and DRUM [27]; (ii) two subgraph-based models: GraIL [32] and COMPILE [18]; (iii) five C-MPNN-based models: NBFNet [50], A*Net [49], RED-GNN [47], AdaProp [48], and RUN-GNN [41].\nEvaluation and Tie Policy Following [47-49], we evaluate all the models using the filtered mean reciprocal rank (MRR) and Hits@10 metrics. The best models are chosen according to MRR on the validation dataset. Subgraph-based models typically rank each test triplet among 50 randomly sampled negative triplets, whereas C-MPNNs evaluate each triplet against all possible candidates. In this paper, we follow the latter and take the results of rule-based and subgraph-based models from [48]. Missing results are reproduced by their official code.\nThere are different tie policies [30] to compute MRR when several candidate entities receive equal scores. In progressive propagation, all unvisited entities are assigned identical scores. Following [41,47], we measure the average rank among the entities in the tie, as suggested in [26]. To keep the tie policy consistent, we re-evaluate AdaProp using the official code."}, {"title": "5.2 Main Results (Q1)", "content": "Tables 3 and 4 depict the performance of different models on inductive KG reasoning. MStar demonstrates the best performance across all metrics on FB15k-237 and NELL-995, and compares favorably with the top models on WN18RR. We observe that (i) subgraph-based models typically perform poorly. This is because subgraphs are often sparse or empty and provide less information, particularly for distant entities. (ii) Rule-based models are generally more competitive but are still weaker compared to C-MPNN-based models. However, DRUM outperforms existing models except MStar in Hits@10 on NELL-995 (v1). NELL-995 (v1) is a special dataset and the distance between the head and tail entities for all triplets in the test graph is no longer than 3, which is very short. Thus, we conjecture that the length of the learned rules limits the reasoning capabilities of rule-based models. Differently, MStar holds an edge over these two groups of models on all datasets. This suggests that multiple starting entities in MStar alleviate the distance limit issues as much as possible when reasoning.\nCompared with the best C-MPNN-based results, MStar achieves an average relative gain of 9.9% in MRR, 5.2% in Hits@10 on FB15k-237, and 13.9% in MRR, 6.1% in Hits@10 on NELL-995. Existing C-MPNN-based models typically use all entities in the KG or only the head entity as starting entities, without providing conditional information to distant entities, which can introduce excessive noise or lack sufficient information. Instead, our MStar selects multiple query-dependent starting entities adaptively and propagates conditions farther through the highway for accurate reasoning. Moreover, LinkVerify in MStar additionally reduces noisy samples in training. We also observe that the improvement of the model on WN18RR is not as pronounced as on the other datasets. To provide insights into this phenomenon, we conduct further analysis in Section 5.4."}, {"title": "5.3 Ablation Study", "content": "Variants of MStar (Q2) In this section, we design several variants of MStar to study the contributions of three components: (i) selection, (ii) highway, and (iii) LinkVerify in training. The results are summarized in Tables 5 and 6, which indicate that all components contribute significantly to MStar on the three datasets.\nFirst, the variant of w/o selection propagates only from the head entity which is the same as RED-GNN. According to the results, removing selection significantly decreases performance, highlighting the effectiveness of using multiple starting entities to explore reasoning patterns across a broader neighborhood.\nSecond, it can be observed that the performance of variant w/o highway is worse than MStar. This observation suggests that transmitting query-dependent information to the starting entities is a promising approach to expedite propagation for conditions and enhance reasoning accuracy.\nThird, the variant of w/o LinkVerify is inferior to MStar all the time, as triplets with unvisited target entities in training KG introduce noise. Removing LinkVerify results in poorer performance, especially on smaller datasets. For"}, {"title": "5.4 Further Analysis", "content": "Perspective of Datasets As shown in Tables 5 and 6, the improvement of MStar on WN18RR is not as great as the one on other datasets. As can be seen from Table 2, WN18RR (v1, v2, v3, v4) and NELL-995 (v1) have fewer relations. Due to the entity-independent nature of inductive KG reasoning, entity embeddings usually rely on the representation of relations. With fewer relations, entities carry more monotonous information. Therefore, it becomes challenging to select query-dependent entities and propagate messages to the target ones. To study the situation further, we count the proportion of triplets whose shortest distance between the head and tail entities exceeds 3. We regard these triplets as long-distance triplets. The result is shown in Table 8. We can see that NELL-995 (v1) owns zero long-distance triplets in the test graphs. Thus, NELL-995 (v1)"}, {"title": "Perspective of Starting Entities Selection", "content": "MStar leverages an importance score function to select starting entities. The score function is conditioned on the query head and relation, aiming to explore query-dependent entities. Here, we consider two other score function variants, i.e., variant w/ random and variant w/ degree. Variant w/ random scores the entities with random values. Similar to EL-GNN [25], variant w/ degree assigns higher scores to entities with higher degrees. All variants keep top-n entities as starting ones.\nTable 9 shows the comparison results. We can observe that random scores lead to a degraded performance. This is because random starting entities propagate along many noisy relational paths. Noisy paths hinder MStar's ability to capture query-related rules and to reach distant target tail entities. Variant w/ degree is also inferior to our MStar, even worse than random scores. For instance, the performance of variant w/ degree on FB15k-237 (v1) decreases by 54.8% and 54.0% relative to MStar and variant w/ random, respectively. This is mainly due to the fact that the global feature degree fixes the starting entities and cannot support query-dependent propagation."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we explore the issue of inefficient message propagation for KG reasoning and propose a new inductive KG reasoning model called MStar. Specifically, we propose using multiple starting entities to expand the propagation scope. Moreover, we construct a highway between the head entity and the other starting entities to accelerate conditional message passing. Additionally, we introduce a training strategy LinkVerify to filter inappropriate samples. Experimental results demonstrate the effectiveness of MStar. In particular, ablation results validate the superiority of MStar for reasoning about distant entities. In future work, we plan to explore alternative modules for selecting and classifying starting entities. We also intend to investigate methods to effectively utilize noisy triplets during training instead of dropping them."}]}