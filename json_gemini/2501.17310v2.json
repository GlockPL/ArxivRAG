{"title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding", "authors": ["Yun-Shiuan Chuang", "Sizhe Gao", "Nikunj Harlalka", "Sameer Narendran", "Alexander Cheung", "Siddharth Suresh", "Junjie Hu", "Timothy T. Rogers"], "abstract": "Guesstimation, the task of making approximate quantity estimates, is a common real-world challenge. However, it has been largely overlooked in large language models (LLMs) and vision language models (VLMs) research. We introduce a novel guesstimation dataset, MARBLES. This dataset requires one to estimate how many items (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup), both with and without accompanying images. Inspired by the social science concept of the \u201cWisdom of Crowds\u201d (WOC) - taking the median from estimates from a crowd), which has proven effective in guesstimation, we propose \u201cWOC decoding\" strategy for LLM guesstimation. We show that LLMs/VLMs perform well on guesstimation, suggesting that they possess some level of a \"world model\" necessary for guesstimation. Moreover, similar to human performance, the WOC decoding method improves LLM/VLM guesstimation accuracy. Furthermore, the inclusion of images in the multimodal condition enhances model performance. These results highlight the value of WOC decoding strategy for LLMs/VLMs and position guesstimation as a probe for evaluating LLMs/VLMs' world model. As LLMs' world model is a fundamental prerequisite for many real-world tasks, e.g., human-AI teaming, our findings have broad implications for the AI community.", "sections": [{"title": "1 Introduction", "content": "Daily life often requires us to estimate uncertain quantities, from the crowd size at a political event to the weight of a turkey needed for a Thanksgiving dinner. In human populations, such \u201cguesstimation\" scenarios often exhibit wisdom of crowds (WOC) effects: in a random sample of estimates, the median lies closer to the ground truth than most individual guesses [3, 17]. WOC phenomena are thought to rely on the grounding of conceptual knowledge in embodied, multi-modal experience. For instance, when estimating the number of jelly-beans in a jar [10], people may rely on an implicit understanding of the typical size, shape, and firmness of jelly beans, and the shape, volume, and rigidity of the jar-properties experienced directly through perception and action in the world, in addition to being expressed in language.\nEven for more abstract scenarios, people may rely on general world-knowledge; for instance, when estimating the number of people requiring food-stamps in Chicago, their guesses may reflect general knowledge/beliefs about poverty rates, accessibility of government programs, characteristics of large midwestern cities, etc. That is, WOC phenomena may rely on a world model.\nHere we assess whether contemporary large language models (LLMs) exhibit WOC phenomena similar to those observed in human populations. On one hand, LLMs are crowds unto themselves:"}, {"title": "2 Methods and Experimental Setup", "content": "MARBLES Dataset. Our MARBLES dataset consists of 15 guesstimation questions, involving five different containers (a one-cup dry ingredient measuring cup, a shot glass, a Starbucks iced tall cup, an Altoids tin, and a box for a deck of standard Bicycle playing cards) and three different items (standard-sized U.S. marbles, standard-sized M&Ms, and U.S. quarters). For example, \"How many standard-sized U.S. marbles does it take to fill a one-cup dry ingredient measuring cup?\". The true answer for each question was determined by manually measuring the quantity three times and taking the median. Additionally we captured four photographs for each question: a top view with the items filling the container, a tilted view, a side view of the container, and a photo showing a single item inside the container (Figure 1). In the multimodal condition, these images were presented alongside the textual questions, while in the verbal condition, only the textual questions were provided. See \u00a7A for the full list of questions and \u00a7B for the prompts.\nHuman Experiment. We recruited 230 participants from a university in the US. Participants were randomly assigned to either the verbal condition (112 participants) or the multimodal condition (108 participants). Each participant was asked to generate estimates for each question in the MARBLES dataset. We also asked participants to rate their familiarity with each item and container on a 5-point scale (from 1 = \"not familiar at all\" to 5 = \"extremely familiar\"). For each question, we only used data from participants who rated their familiarity as at least 4 (\"quite familiar\") for both the item and the container. This results in 64.9 valid response on average per question."}, {"title": "3 Results", "content": "Humans are Good at Guesstimation. Across verbal and multimodal conditions, humans achieve the most accurate guesstimation compared to almost all LLMs/VLMs . Moreover, human's accuracy is further improved with the presence of images in the multimodal condition (\u03b5 = 0.33). In addition, in multimodal condition, the error & of WOC decoding reduces with increasing size of the crowds (Figure 1; \u025b reduces from 0.45 to 0.33 when crowd size increases from 1 to 25). Interesting, such WOC reduction does not hold true for verbal condition (\u025b remains 0.59).\nWisdom of Crowds (WOC) Decoding Supports Guesstimation in LLMs/VLMs. For LLMs/VLMs, the WOC decoding method consistently outperforms the self-consistency and greedy decoding methods across model sizes and variants (Table 1). The Mistral and Mixtral models enjoy the largest gain with WOC. The only exception is gpt-4-0125-preview, where WOC and self-consistency has the same performance.\nIncreasing Number of Sampled Reasoning Paths Enhances Wisdom of Crowds Performance. Increasing the number of sampled reasoning paths consistently improves the accuracy of the WOC method (Figure 1) across both the verbal and multimodal conditions. In contrast, increasing the sample size does not consistently lead to better guesstimation performance of self-consistency method.\nMultimodal Inputs Improve Guesstimation Performance. Similar to human, LLMs also perform better in the multimodal condition, where both text and images are provided as input. For instance, as shown in Table 1, the LLaVA model, which takes as input as both text and images and is powered by the Mistral-7B base model, significantly outperforms its text-only counterpart, Mistral-7B-Instruct. This highlights that multimodal information improve the LLM's world model."}, {"title": "4 Related Work", "content": "Guesstimation and Wisdom of Crowds. For a crowd to reach better guesstimation, wisdom of crowds (WOC) has proven to be effective, as long as individual estimates within these groups are statistically independent [10, 8]. This independence ensures that their errors are uncorrelated, allowing them to cancel out in aggregate. WOC has shown applications in real-world guesstimation challenges like market prediction and political forecasting [17].\nVision language models (VLMs)' Spatial Reasoning. Previous work has investigated the spatial reasoning capabilities of vision language models (VLMs). Explicit grounding of the model with spatial awareness helps the model perform better in spatial reasoning [1, 18, 9, 15, 2, 16]. However, to our knowledge, no work to date has investigated VLMs' capabilities in guesstimation."}, {"title": "5 Conclusion", "content": "In the study, we show that LLMs/VLMs possess the world model necessary for effective guesstimation, a common yet overlooked task in the AI community. To evaluate this, we introduce the MARBLES dataset, where one needs to estimate how many items can fit into various containers (along side with photos included for the multimodal condition). We show that humans are good at guesstimation, and their accuracy is further improved by the inclusion of images. Moreover, human WOC effect emerges in the multimodal condition. Second, similar to human, LLMs/VLMs also show the WOC effect, where the median of estimates leads to more accurate results than greedy decoding or self-consistency. In addition, like human, including visual information further improved human performance. In addition, the benefit of WOC decoding for LLMs/VLMs increases with increasing number of reasoning paths samples. In sum, we introduce guesstimation as a new task that is very common in real world but has been overlooked by the AI community."}]}