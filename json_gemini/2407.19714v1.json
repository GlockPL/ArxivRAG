{"title": "Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets", "authors": ["Muhammad Abdullah Jamal", "Omid Mohareri"], "abstract": "Surgical scene understanding is a key technical component for enabling intelligent and context aware systems that can transform various aspects of surgical interventions. In this work, we focus on the semantic segmentation task, propose a simple yet effective multi-modal (RGB and depth) training framework called SurgDepth, and show state-of-the-art (SOTA) results on all publicly available datasets applicable for this task. Unlike previous approaches, which either fine-tune SOTA segmentation models trained on natural images, or encode RGB or RGB-D information using RGB only pre-trained backbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is designed to encode both RGB and depth information through a simple fusion mechanism. We conduct extensive experiments on benchmark datasets including EndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of SurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis2022 SAR-RARP50 challenge and outperforms the current best method by at least 4%, using a shallow and compute efficient decoder consisting of ConvNeXt blocks.", "sections": [{"title": "1 Introduction", "content": "Intelligent and context aware surgical systems and digital tools have a significant potential to transform minimally invasive procedures by enhancing surgeon and care-team performance, and improving overall safety. Surgical scene parsing is a key component for designing such systems through enabling tasks such as pose estimation [7], tool tracking [35] and phase recognition [1]. Applications such as operating room workflow optimization [18], surgeon skill assessment [30,43] and automation of surgical sub-tasks [13] can be built on top of such technologies.\nIn this paper, we focus on single frame semantic segmentation of instrument, anatomy and other objects present in surgical scenes. The objective is to assign each pixel a correct semantic label. Most of the earlier work [32,42,17] follow segmentation models built for non-surgical images such as MaskRCNN [10] and UNet [31], either by directly fine-tuning them or incorporating additional cues such as pose [22], saliency maps [15], optical flows [17] and motion flows [42]. However, unique challenges such as occlusion, variability in lighting, presence of smoke and blood, and diverse instrument and tissue types limit accuracy, generalizability and clinical translation of present methods. Incorporation of 3D geometric information is a promising approach to help enhance the performance of such segmentation algorithms. RGB-D datasets are commonly being used in non-surgical applications such as autonomous driving [12], robotics [24] and SLAM [36]. Existing approaches [38,41] have shown state-of-the-art performance on non-surgical benchmark datasets [25,33] through methods that leverage both RGB and depth data. However, to the best of our knowledge, very little or no work has been done on effective utilization of RGB-D data for surgical instrument and tissue segmentation.\nThis motivates us to present SurgDepth, a simple yet effective RGB-D seman- tic segmentation framework for endoscopic surgical data. SurgDepth builds the interaction between both data modalities by fusing them using a 3D awareness block. The purpose of this block is to incorporate 3D geometric information from the depth maps to enhance localization of objects and structures. Our fusion mod- ule can be plugged in to any Vision Transformer (ViT) [6]. Moreover, we propose a shallow decoder based on ConvNeXt [23] blocks to predict the segmentation map. Through extensive experiments, we found out that such method outperforms transformer based decoders such as Segmenter [34]. We employ state-of-the-art depth estimation models like DINOv2 [27] and DepthAnything [40] to predict depth maps from RGB only surgical videos.\nWe demonstrate the efficacy of SurgDepth on multiple datasets such as SAR- RARP50 [29], AutoLapro [39], LapI2I [28], CholecSeg8k [11] and EndoVis2017 [2]. By adding a ConvNeXt based decoder, we outperform all competing methods while requiring less computational resources. More specifically, our approach achieves 0.862 IoU on SAR-RARP50 with 98.37M parameters, a new SOTA performance, as compared to Uninades, the best performing method reported in [29], which achieves 0.829 IoU with 107M parameters.\nTo summarize, our main contributions are:"}, {"title": "2 Related Work", "content": "There are a few publicly available datasets [29,2,39] for surgical instrument and tissue segmentation. With the rise of EndoVis challenge, various surgical scene understanding techniques have been explored. In particular, approaches for instrument segmentation can be grouped as semantic [17,32,26,19] and instance segmentation [8,20,10]. Our work targets the semantic segmentation task.\nSemantic Segmentation. TernausNet [32] used a UNet architecture [31] on the top of pre-trained VGG encoder for binary instrument segmentation. [19] proposed a UNet plus architecture, a modified encoder-decoder UNet with data augmentation techniques for medical image segmentation. [26] proposed a progressive alternating attention network (PAANet) which consists of progressive alternating attention dense (PAAD) blocks to construct attention guided map from all scales. MF-TAPNet [17] incorporates temporal priors by leveraging motion flow to an attention pyramid network. In addition to the above approaches, [14,37] target real-time semantic segmentation.\nInstance Segmentation. Unlike semantic segmentation, which assigns class labels to each pixel, instance segmentation, or mask classification, is an alterna- tive paradigm that assigns class labels to each object instance or binary mask. Most of the earlier work primarily use Mask-RCNN [10]. ISINet [8] builds on the top of Mask-RCNN and proposes a temporal consistency module by taking advantage of the sequential nature of the video data. [20] re-defined Mask-RCNN by improving region proposal network with anchor optimization. Another line of work in this domain is to develop specialized models [21,16]. AP-MTL [16] pro- posed an encoder-decoder multi-task learning architecture for real-time instance segmentation."}, {"title": "3 Surg Depth", "content": "SurgDepth follows a standard encoder-decoder architecture as illustrated in Figure 2. The goal of the encoder is to learn discriminative representations while the decoder is responsible for transforming these features into segmentation maps. An RGB image and depth map with spatial size of H \u00d7 W are first processed through modality-specific projection layers consisting of a single convolutional layer. Then, the RGB and depth features are passed to the fusion block which encodes 3D geometrical information. Next, to learn useful representations, the modality-specific features are concatenated and passed to the encoder, which is a ViT in our case. Finally, the features from the encoder are passed to a lightweight decoder to produce the segmentation map of size H \u00d7 W."}, {"title": "3.1 3D Awareness Fusion Block", "content": "Our fusion block consists of a 3D awareness attention module that incorporates 3D information from the depth maps to enhance localization of the semantic classes as shown in Figure 3. Given the RGB \\(X^{rgb}\\) and the depth \\(X^{depth}\\) features, we first concatenate the modality-specific features and then down-sample them through an adaptive pooling layer to reduce the computational complexity and generate query (Q) features. The key (K) and value (V) are extracted from the RGB features \\(X^{rgb}\\). This can be formulated as:\n\\(Q = FC(AdaptivePool_{k \\times k}(Concat(X^{rgb}, X^{depth}))),\\)\n\\(K = FC(X^{rgb}), V = FC(X^{rgb}),\\)\nwhere AdaptivePool performs adaptive average pooling to downsample the spatial size to kxk, and FC is a fully connected layer. Based on the Q, K, and V, we formulate the attention module as:\n\\(X_{fusion} = Bilinear(V. Softmax(-\\frac{Q^{T}K}{\\sqrt{C_{d}}})),\\)\nwhere Bilinear() is a bilinear upsampling operation (F.interpolate() in Pytorch) that converts the spatial size from k \u00d7 k to h \u00d7 w and Cd represents the dimension of Q, K and V. Finally, the features \\(X_{fusion}\\) are passed to two projection layers (FC) to produce updated RGB features \\(X^{rgb}\\) and depth features \\(X^{depth}\\)."}, {"title": "3.2 Overall Architecture", "content": "RGB features \\(X^{rgb}\\) and depth features \\(X^{depth}\\) are concatenated and passed through the ViT encoder to encode RGB-D data. The features from the en- coder's last layer are passed to the lightweight de- coder to yield segmentation maps. We empirically found that passing only RGB features to the decoder yields higher performance as compared to passing both RGB and depth features (c.f. Table 5). Our lightweight decoder consists of ConvNeXt blocks [23] and a convolutional layer for producing segmenta- tion maps. Each ConvNext block has one depth-wise convolutional layer with a kernal size of 7 \u00d7 7 and two point wise convolutional layers. It follows the inverted bottleneck design where the dimension of the middle point-wise convolution is four times big- ger than the input dimension. Please follow [23] for more details on the block. We also experimented with Segmenter [34] as the decoder but empirically found that it doesn't outperform our ConvNeXt based decoder and brings about computational overhead."}, {"title": "4 Experiments", "content": "We evaluate the performance of our architecture on multiple benchmark datasets."}, {"title": "4.1 Datasets and Evaluation.", "content": "SAR-RARP50 Challenge [29]. This dataset contains 50 videos collected from Robot-Assisted Radical Prostatectomy procedures. It consists of 12998 training frames from 40 different videos and 3252 test frames from 10 other videos and nine semantic classes. Please follow [29] for more details on the dataset.\nAutoLapro [39]. This dataset has 21 laparoscopic hysterectomy videos recorded with 25 fps and a resolution of 1920\u00d71080 pixels. For segmentation task, the dataset has 1800 frames annotated with nine semantic classes which constitutes 5936 annotations. We use the official train/validation/test splits provided in the dataset.\nLapI2I [28]. The Laparoscopic Image-to-Image (I2I) Translation dataset con- tains 20,000 synthetic frames annotated with seven classes including liver, fat, diaphragm, tool shaft, tool tip and gallbladder. Synthetic frames are generated using 3D laparoscopic simulations created from CT scans of ten patients. The dataset doesn't provide any official split for train/val/test. We use data from 7 patients in the training set and the rest for testing.\nCholecSeg8k [11]. This dataset consists of 8,080 laparoscopic cholecystectomy frames with a resolution of 854 \u00d7 480 from 17 video clips in Choloec80. Each image is annotated with 13 classes (abdominal wall,liver, gastrointestinal tract, fat, grasper etc.) at the pixel-level. We use 13 videos in the training set and set aside 4 videos for testing.\nEndoVis 2017 [2]. EndoVis 2017 challenge dataset consists of 10 video se- quences of abdominal porcine procedures recorded by a daVinci Xi system. Each video contains 300 frames with a resolution of 1280\u00d71024. The frames are anno- tated with six instrument classes and an ultrasound probe. For fair comparison, we follow [32] and use 4-fold cross-validation from 1800 frames (8 x 225). Each fold consists of 1350 and 450 frames for training and validation respectively.\nEvaluation. We use mean intersection over union (IoU) as our evaluation metric [34,4]."}, {"title": "4.2 Implementation Details", "content": "SurgDepth is compatible with any Vision Transformer (ViT) as an encoder in our framework. In this work, we use ViT-B as an encoder, and four ConvNeXt blocks and one convolutional layer to produce segmentation maps. First, we increase the dimensionality D of the encoder's output with a linear layer and then reshape it to the size of H/4 x W/4 x D/8. Before calculating the loss on the segmentation map, we upsample the resolution of the map using bilinear interpolation. We initialize our ViT-B encoder with ImageNet [5] unsupervised pre-trained weights [9]. During training, we use the input resolution of 480x640 across all the datasets. We train the model for 50 epochs with a learning rate of 1e-4 and adamw optimizer. Moreover, we apply random gaussian blurring, random horizontal flip and colorJitter which randomly change the brightness, contrast, saturation and hue of a training image. Lastly, we train our models using the batch size of 2 on 4 NVIDIA A100 GPUs."}, {"title": "4.6 Ablation Study", "content": "Input features to the decoder. In Table 5, we show that using only RGB features as input to the decoder brings the best performance on SAR-RARP50 dataset as compared to both RGB and depth features while saving the computa- tional cost.\nNumber of ConvNext blocks in decoder. Table 4 shows the performance of SurgDepth on SAR-RARP50 challenge by varying the number of ConvNeXt blocks in the decoder. We observe the performance boost when we increase the number of blocks from 1 to 4. However, we see a degradation in the performance when we use a much deeper decoder."}, {"title": "5 Conclusion", "content": "We propose a novel RGB-D training framework called SurgDepth for semantic segmentation in surgical videos. SurgDepth consists of a novel 3D awareness attention block which builds interaction between RGB and depth by incorporating 3D geometric information from the depth maps. The method can be used with any type of Vision Transformer (ViT). Moreover, it can act as a building block for architectures that take video or stereo data as inputs. Our experiments demonstrate that SurgDepth achieves new state-of-the-art performance on five benchmark datasets with less computational cost, thanks to a shallow decoder consisting of ConvNeXt blocks."}]}