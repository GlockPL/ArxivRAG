{"title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild", "authors": ["Ahmed Masry", "Megh Thakkar", "Aayush Bajaj", "Aaryaman Kartha", "Enamul Hoque", "Shafiq Joty"], "abstract": "Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. We address these important drawbacks and introduce ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. Our simple approach achieves state-of-the-art results across 5 benchmarks spanning chart summarization, question answering, and fact-checking, and our elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. We release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.", "sections": [{"title": "1 Introduction", "content": "Language-augmented vision foundation models or vision-language models (VLMs) have proven to be effective in tackling numerous real-world multimodal tasks such as visual segmentation, captioning, question answering, and generation and editing (Li et al., 2023; Zhu et al., 2023). Though these models excel when used for general purpose applications in the wild, they often fail to tackle tasks that require specialized understanding and decoding of patterns and visualizations (Han et al., 2023). An important domain-specific usage of VLMs is for understanding and reasoning over charts, given their ubiquity as a data analysis, visualization, and decision-making tool across businesses, economies, and scientific fields (Hoque et al., 2022). This has naturally led to the development of more specialized foundation models pre-trained on massive amounts of structured and often chart-specific data (Liu et al., 2022; Masry et al., 2023). These models are, however, trained on a limited source of resources and focus on a specific set of tasks, constraining their real-world applicability (Masry et al., 2024).\nDeveloping over the success of instruction-tuning enabling models to generalize to more tasks and applications (Ouyang et al., 2022), there have been attempts at 'instruction-tuning' VLMs to endow them the ability to understand charts in more realistic and fundamental settings (Meng et al., 2024). These approaches generally depend on two crucial factors impacting their effectiveness: (i) Instruction-tuning dataset \u2013 these methods either use the underlying data tables from existing web sources (Masry et al., 2024) or use synthetically generated data-tables (Han et al., 2023) from LLMs such as GPT-4 (OpenAI, 2023) to curate the instruction-tuning data, and (ii) Base model the existing methods either use chart-specific pre-trained models like UniChart (Masry et al., 2023) or VLMs pre-trained with weak image-text alignment such as LLaVA (Li et al., 2023). However, in existing methods, both these factors have critical drawbacks impacting their ability to understand real-world complex charts.\nExisting methods are restricted to charts that either have an underlying data table or require methods to extract them from the charts, often with low accuracy which are used for instruction-tuning data generation. These data tables are often incapable of capturing numerous nuanced details in the complex charts used in real-world applications (Table 1). Also, in many scenarios, we are concerned with representing or understanding general trends in the charts and not individual data points. On the model side, existing methods use backbones in which the vision encoder and LLM are weakly-aligned, either due to limited data or architecture, limiting their generalizability to represent real-world charts. Instruction-tuning a strongly aligned base VLM can capture the intricacies among diverse chart elements and corresponding text more efficiently. We hypothesize that formulating a simple approach addressing these drawbacks can lead to an effective foundation model capable of complex chart understanding and reasoning in the wild.\nWe propose ChartGemma, an instruction-tuned multimodal model for chart understanding and reasoning. ChartGemma uses instruction-tuning data for chart representation learning that is directly generated from the chart images, capturing more diverse and relevant information while preserving complex visual features. This also enables us to utilize a much broader array of charts available across the web as we are not restricted by the availability of underlying data tables. ChartGemma develops over PaliGemma (Chen et al., 2023) which has been trained on a much larger alignment dataset. Since ChartGemma uses PaliGemma as its backbone, it is also much smaller than existing chart understanding models, making it suitable for real-world applications. We evaluate ChartGemma across 5 benchmarks spanning chart summarization, question answering, and fact-checking, obtaining state-of-the-art results compared to existing methods. Our qualitative studies also demonstrate that ChartGemma produces more faithful and realistic summaries of complex charts as compared to other methods. Through our elaborate analysis, we put forward ChartGemma as an effective model capable of understanding and reasoning over real-world charts. Our main contributions are:\n\u2022 We present ChartGemma, a first-of-its-kind multimodal model instruction-tuned for chart understanding and reasoning using data directly generated from chart images.\n\u2022 ChartGemma utilizes a stronger backbone model and more representative instruction-tuning data, rendering it effective in tackling existing benchmarks across chart summarization, question answering, and fact-checking while being significantly smaller than its counterparts.\n\u2022 Our extensive quantitative and qualitative studies reveal that ChartGemma generates more faithful and human-like summaries and is extremely capable in understanding and representing complex real-world charts in the wild."}, {"title": "2 Chart Instruction Data Generation", "content": "In this section, we present the details of generating our instruction-tuning dataset. We start by curating a diverse corpus of charts that encompasses a range of visual styles and elements (\u00a7 2.1), and then use it to generate the visual instruction-tuning data directly from the charts (\u00a72.2). We illustrate our data generation pipeline in Fig. 1."}, {"title": "2.1 Assembling the Chart Corpus", "content": "Our chart corpus is assembled using a combination of various sources across three categories: (i) Synthetically generated charts from sources such as PlotQA (Methani et al., 2020), (ii) Curated charts from specialized websites such as Statista which typically exhibit limited visual diversity, and (iii) In-the-wild charts harvested from the broader web, such as WebCharts (Masry et al., 2024), noted for their extensive stylistic variety. While prior approaches used accompanying metadata (e.g., titles, data tables, annotations) to generate instructions from LLMs (Han et al., 2023; Meng et al., 2024), our method exclusively utilizes the chart images themselves for generating instruction-tuning data. This approach also allows us to bypass the constraints imposed by metadata availability. In total, our corpus consists of 122,857 chart images. We provide an elaborate breakdown of the chart source and the statistics across each category in Table 4."}, {"title": "2.2 Visual Chart Instructions", "content": "We use chart images directly from the above assembled corpus to generate visual instruction-tuning data. This enables us to synthesize data that can train a model to capture not just point information, but complex trends and relations among the chart elements. Following Masry et al. (2024), we generate data across two categories: (i) predefined tasks, which align with common real-world scenarios and benchmarks, and (ii) open-ended tasks. For predefined tasks, we generate data for,\n1.  Chain-of-thought (CoT) involves prompting the model with complex reasoning questions and enhances the visual reasoning capabilities of the model by guiding it through the problem-solving process in a structured manner.\n2.  Summarization involves prompting the model to generate summaries that succinctly capture the key insights and trends from a chart image that effectively communicates the primary data narratives.\n3.  Fact Checking asks the model to determine whether stated facts are supported or refuted by the data presented in a chart image. Alongside data generated from our corpus, we use the training sets of existing chart fact-checking tasks (Akhtar et al., 2023a,c) in our instruction-tuning data.\n4.  Chart-to-Markdown tasks the model with generating the underlying data tables from a chart image in Markdown format. This approach simplifies rendering and parsing the tables, enhancing their accessibility and usability.\n5.  Program Aided Design (Gao et al., 2022) requires the model to generate executable code that performs necessary calculations and outputs the final answer, delegating complex and challenging mathematical operations to the code interpreter. Alongside synthetic data generated from our corpus, we use the Multimodal LLM to create executable codes for questions in the training split of the ChartQA dataset (Masry et al., 2022b), augmenting our instruction-tuning data with human-written questions and their corresponding code.\nOpen-ended Tasks We enrich our instruction-tuning data by prompting the Multimodal LLM to generate a variety of tasks typical in real-world scenarios. This approach enhances the generalizability of our models and extends their applicability to diverse real-world settings. Example open-ended tasks include justifying temporal or time-series based trends observed in the chart, describing the different visual elements such as lines, colors, and legends represented by the chart, critically analyzing and comparing visual information, etc. We present concrete examples in \u00a7A.2.\nWe use Gemini Flash-1.5 (Team et al., 2023) due to its robust multimodal performance, cost-effectiveness, and high API rate limits."}, {"title": "2.3 Key Dataset Characteristics", "content": "To underscore the distinct innovations of our dataset relative to prior works, we examine two critical elements: the visual attributes and the quality of the chart instructions.\nVisual Attributes Our instruction-tuning dataset features a wide range of instructions that emphasize the visual attributes of chart images. As illustrated in Fig. 7 in Appendix A.2, the examples highlight various visual elements such as lines, shapes, colors, trends, chart types, and positions, all of which are frequently referenced in real-world scenarios. These enhance the model's visual reasoning capabilities, enabling real-world applications.\nQuality To demonstrate the strength of our approach in generating high-quality and accurate instructions, we evaluated 100 randomly sampled synthesized instructions. We found that our instructions accurately reflected the chart content in 82% of the cases, which is a significant improvement over the 61% accuracy reported for the ChartInstruct dataset (Masry et al., 2024). Additionally, we observed 8% partially correct answers, similar to that as reported by ChartInstruct. We attribute this improvement in quality to our method's reliance on the chart images, rather than using automatically generated and often erroneous data tables."}, {"title": "3 Modeling and Methodology", "content": "ChartGemma uses PaliGemma (Chen et al., 2023) as the backbone architecture, which comprises of the following two components:\nVision Encoder: SigLIP (Zhai et al., 2023) is a vision transformer (ViT) encoder. Unlike CLIP-like ViTs (Radford et al., 2021) which use contrastive loss on large batches of image-text pairs, SigLIP is trained on single image-text pairs independently as a binary classification task.\nLanguage Model: Gemma-2B (Team et al., 2024) is decoder-only transformer-based (Vaswani et al., 2017) LLM trained on 3 trillion tokens with a context length of 8,196 tokens. Its pretraining data mainly consists of English documents, maths, and code, making it suitable for chart understanding tasks requiring strong reasoning capabilities.\nWe present ChartGemma's architecture in Fig. 2. The input image is taken in 448x448 resolution and divided into 14x14 pixel patches, each of which is fed into the vision encoder as a separate token. The outputs from the vision encoder are passed through a linear layer that maps the visual features into the LLM embedding space. These visual tokens are then concatenated with the input text embeddings and passed to Gemma-2B. Unlike previous VLLMs (Li et al., 2023) that indiscriminately apply a causal mask on all image and text tokens, Gemma-2B applies full attention over the input visual and text tokens while a causal mask is applied on the output tokens. This improves the contextual understanding of the image particularly for representing complex relationships among objects. We believe this property provides further advantages when learning representations for chart images containing numerous nuanced complexities."}, {"title": "3.2 Training Setup", "content": "Existing chart VLLMs (Meng et al., 2024) typically employ a two-stage training approach that requires an initial step to align the vision encoder and the LLM for understanding chart features, followed by instruction-tuning. In contrast, we only use a single-stage approach where we directly finetune the backbone model on our instruction-tuning data. We believe that the first stage is required by current methods as the VLLM backbones are aligned using a limited amount of image-text pairs with restricted styles and diversity. In contrast, our backbone, PaliGemma, has been trained end-to-end on 10 billion image-text pairs covering a wide variety of styles. This makes our model more adaptable and generalizable to different real-world images (e.g., charts, infographics, documents). We freeze the vision encoder and only finetune the LLM during instruction-tuning. This helps in reducing the computational complexity and also improves training stability given the small batch size used for instruction-tuning PaliGemma."}, {"title": "4 Experiments, Results, and Analyses", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Baselines We compare ChartGemma against baselines comprising of open-source chart-specialist models and VLLMs instruction-tuned on chart data, as well as state-of-the-art closed source multimodal LLMs. Chart-specialist models include ChartBERT (Akhtar et al., 2023c), Pix2Struct (Lee et al., 2022), MatCha (Liu et al., 2022), and UniChart (Masry et al., 2023). Chart VLLMs include ChartLlaMA (Han et al., 2023), ChartAssistant (Meng et al., 2024), and ChartInstruct's (Masry et al., 2024) two variants with LLaMA2 and Flan-T5-XL. We also compare ChartGemma against two closed-source multimodal LLMs, namely Gemini Pro (Team et al., 2023) and GPT4-V (OpenAI, 2023).\nDownstream Tasks We evaluate ChartGemma on a diverse set of 5 established benchmarks evaluating chart representation and reasoning abilities: (i) ChartQA (Masry et al., 2022b) \u2013 a factoid chart question answering dataset, (ii) ChartFC (Akhtar et al., 2023a) and (iii) ChartCheck (Akhtar et al., 2023b) \u2013 chart fact checking datasets, (iv) OpenCQA (Kantharaj et al., 2022) \u2013 an open-ended chart question answering dataset, and (v) Chart2Text (Shankar et al., 2022) \u2013 a chart summarization dataset. While ChartQA and ChartFC focus on closed-ended generation, OpenCQA and Chart2Text evaluate open-ended generation abilities of the models. We also manually curate a set of 100 charts downloaded from the web completely unseen by any model. We refer to this set as 'Web' in our results, and use them for comparing the summarization ability of the models.\nEvaluation Metrics Following existing works, we use relaxed accuracy (RA) for ChartQA, accuracy for ChartFC, and use GPT4 as a judge for open-ended generation tasks, i.e. Chart2Text, OpenCQA, and our curated Web set of charts and measure the informativeness and factual correctness on a scale of 1-5 (Post, 2018).\nTo ensure the reproducibility of our work, we present the hyperparameters of our instruction-tuning and downstream task experiments in \u00a7B.1. All experiments were conducted on a 4 A100 GPUs (80GB) machine using the JAX framework\u00b9."}, {"title": "4.2 Performance on closed-ended tasks", "content": "We compare the performance of ChartGemma to the various baselines on the closed-ended tasks, namely ChartQA and ChartFC, and present the results in Table 2. We see that Chart VLLMs are generally the better performing set of models compared to specialist chart models. Within Chart VLLMs, we observe that ChartGemma performs the best on ChartQA in terms of the average overall performance and on both the synthetic ChartFC and real-world-based ChartCheck test splits. Particularly, the performance improvements on ChartCheck when using ChartGemma, which is a zero-shot evaluation, can be attributed to the fact that our instruction-tuning dataset is specifically designed to generalize to more realistic charts encountered in this particular evaluation. We observe that it is also powerful for its small size of 3 billion parameters, and only lags in performance to the 13 billion parameter ChartAssistant on the augmented set of ChartQA. The significant improvement of ChartGemma over ChartAssistant on the human-generated split of ChartQA indicates better generalization abilities in understanding more realistic instructions for complex charts.\nGiven the state-of-the-art performance of ChartGemma, we next perform a series of ablations to test our hypothesis on the criticality of having (i) an instruction-tuning dataset derived from chart images rather than the underlying data tables, and (ii) the importance of a strong backbone model.\nEffect of the instruction-tuning data To validate the effectiveness of synthesizing instruction-tuning data directly using the chart images as compared to using their underlying data tables, we compare ChartGemma with a version of PaliGemma instruction-tuned on the dataset presented in ChartInstruct (Masry et al., 2024), which was generated using the chart data tables. We present the results in Table 3. We observe remarkable improvements when using our instruction-tuning data compared to the data proposed by ChartInstruct. The improvements are stark on the human split of ChartQA, indicating that ChartGemma is very efficient in following real-world human instructions. The significantly weak performance of ChartGemma when using the dataset from ChartInstruct is in-line with the observations of the author mentioning a low (61%) accuracy of the synthetically generated instruction-tuning data (Masry et al., 2024)."}, {"title": "Effect of the backbone model", "content": "We probe the effect of using PaliGemma as the backbone model for ChartGemma, which has better image-text alignment compared to other VLMs, on the downstream performance. We follow existing works (Han et al., 2023; Masry et al., 2024) that use LLaVA (Liu et al., 2023b) as a backbone and train LLaVA-1 with our instruction-tuning data. We compare this variant (LLaVA+Our dataset) with ChartGemma in Table 3 and observe that ChartGemma performs significantly better as compared to using LLaVA as our backbone. This validates our hypothesis that initializing our architecture with a strongly aligned model leads to better char understanding, reasoning, and generalization capabilities."}, {"title": "4.3 Performance on open-ended tasks", "content": "We next compare the performance of ChartGemma with the baselines on chart understanding and reasoning based open-ended generation benchmarks, OpenCQA (Kantharaj et al., 2022), Chart2Text (Shankar et al., 2022), and our curated 'Web' set. We do not use the BLEU (Papineni et al., 2002) scores for comparison as done by previous works, due to the numerous criticisms of it as an indicative metric (Callison-Burch et al., 2006; Smith et al., 2016) and follow the widespread practice of using strong LLMs as a judge due to their high agreement with human annotators (Zheng et al., 2023). We use GPT4 to evaluate the informativeness and factual correctness of the outputs generated by the models and present the scores in Fig. 32. We see that the outputs generated by ChartGemma are generally scored higher as compared to ChartInstruct. We particularly see significant improvement in the factual correctness of the outputs of ChartGemma, probably due to the fact that our instruction-tuning data synthesized using the chart images captures more complex visual elements and PaliGemma being strongly aligned leads to better understanding and reasoning over the charts. Our findings overall indicate that ChartGemma is able to produce more informative outputs while also being factually correct in terms of long-form answering or summarization for the charts."}, {"title": "4.4 Human Evaluation on Summarization", "content": "Though using online LLMs like GPT4 as a judge has been shown to have a high correlation with human annotation (Zheng et al., 2023), there haven't been studies on measuring this correlation explicitly for chart understanding tasks. Hence, to ensure our observations, evaluations, and conclusions are robust, we perform a human study on the manually curated set of 100 charts, 'Web'. Similar to GPT4 evaluation, we compare the informativeness, factual correctness, and structure of the outputs generated by ChartGemma with ChartInstruct-LLaMA2.\nWe first use ChartInstruct-LLaMA2 and ChartGemma to generate summaries for these samples in the Web set. We then ask 2 different annotators to rate all the responses based on the above metrics (informativeness, factual correctness, structure) from 1-5 (5 being the highest) so we can also measure agreement between the annotations\u00b3. We present the outputs randomly to the annotators to prevent any biases towards the models and present the evaluation results in Fig. 4.\nFrom Fig. 4, we observe that ChartGemma consistently outperforms or matches ChartInstruct-LLaMA2 on all the metrics, and the findings are in-line with those observed when using GPT4 for evaluation (Section 4.3). We observe that ChartGemma is equally well structured, yet is more informative and significantly more factually correct. Better informativeness probably stems from the fact that ChartGemma is trained on data generated from the chart images and not just the underlying data tables, enabling it to learn high level trends and concepts specific to charts. Furthermore, our instruction-tuning data and a strong backbone model promote capturing more complex visual elements of charts, leading to more factual correctness. Overall, since our evaluation is performed on charts sampled randomly in the wild from the web, ChartGemma's strong performance validates its effectiveness as a strong candidate in understanding and reasoning over real-world charts."}, {"title": "4.5 Error Analysis and Challenges", "content": "We analyzed the outputs of our model, ChartGemma, to understand the shortcomings and areas for improvement. We have discovered the following three patterns of errors.\nHigh Resolution Charts Charts with very large, often skewed dimensions, present challenges for our model, which uses an input resolution of 448x448. Resizing these large images can cause written text to become unreadable, leading to errors in the predicted labels and numerical values, as depicted in Fig. 13. Although PaliGemma offers a variant supporting up to an 896x896 input resolution, it operates significantly slower than the 448x448 version, making it impractical for use on consumer-level machines and GPUs.\nCoding Errors While ChartGemma demonstrated state-of-the-art performance on the ChartQA benchmark, excelling in complex numerical reasoning and compositional questions, it occasionally generates erroneous code that cannot be executed. As depicted in Fig. 13, the model sometimes refers to undeclared variables within the code. We believe that integrating an LLM with enhanced coding capabilities could further improve our performance on the ChartQA benchmark.\nCharts with Complex Visual Styles Although our instruction-tuning corpus predominantly features real-world charts from the broad web, ChartGemma tends to exhibit lower factual correctness and informativeness when evaluated on these charts compared to those from specialized websites like Pew or Statista, which have less visual diversity. This disparity, illustrated in Fig. 3, highlights the need for further enhancements to improve the generalizability of chart understanding models across various visual styles."}, {"title": "4.6 Convergence of ChartGemma", "content": "We probe the learning dynamics of ChartGemma by checking the downstream accuracy with the number of instruction-tuning epochs and present the trends in Fig. 5. We interestingly observe that ChartGemma converges very quickly, with the best performance observed at epoch 2. We attribute this characteristic to the strong alignment of PaliGemma rendering it effective in adapting to our relatively generalizable instruction-tuning dataset. This indicates that PaliGemma is a very efficient backbone for visual instruction-tuning of chart data, and might generalize when trained with a much larger number of samples as well. We leave this exploration as future work."}, {"title": "5 Related Work", "content": "Chart Representation Learning Chart understanding models initially were either fine-tuned from language or vision-language models (Masry et al., 2022b; Masry and Hoque, 2021; Lee et al., 2022), or pre-trained using chart-specific learning objectives (Masry et al., 2023; Liu et al., 2022). Recently, instruction-tuning of pre-trained VLMs has been explored for enhancing the general applicability to charts (Meng et al., 2024; Han et al., 2023; Masry et al., 2024; Liu et al., 2023a). Though these methods use diverse sources across the web and synthetic charts for generating instruction-tuning data, they utilize the underlying data table of the charts and train a weakly-aligned backbone VLM.\nChart Modeling Benchmarks With charts being the standard medium for data visualization and data-driven decision making, diverse benchmarks have been proposed to evaluate the abilities of LLMs and VLMs on chart understanding. These benchmarks range from close-ended tasks such as question answering (Methani et al., 2020; Masry et al., 2022a) to open-ended generation such as explanation generation in OpenCQA (Kantharaj et al., 2022) and summarization (Shankar et al., 2022). Chart-specific benchmarks evaluate the ability of models to convert charts into data tables (Choi et al., 2019; Masry et al., 2023) or evaluate claims against given data as a part of general multimodal fact-checking benchmarks (Akhtar et al., 2023a,c).\nInstruction-tuning across modalities and for charts Instruction-tuning was proposed to generalize the abilities of language models across multiple tasks (Mishra et al., 2022) and has become a common practice for adapting pre-trained LLMs to real-world applications(Alpaca, 2023; Chiang et al., 2023; Ouyang et al., 2022). The success of instruction-tuning for text has led to its adoption as a standard process for multimodal VLMs too (Li et al., 2023; Zhu et al., 2023; Dai et al., 2023). Recently, domain-specific instruction-tuning has been attempted for charts that requires specially curated instruction-tuning data (Han et al., 2023; Masry et al., 2024; Meng et al., 2024). These methods use the underlying data tables of the chart to synthesize the instruction-tuning data. Since the data tables of charts are not capable of capturing the nuance details of charts, especially for real-world charts with complex elements, the instruction-tuning data generated using the data tables is not adequate for training models to be adept at understanding these diverse real-world charts."}, {"title": "6 Conclusion and Future Work", "content": "In the landscape of rising excitement for chart understanding and reasoning models and methods, we present ChartGemma, a multimodal model instruction-tuned on data generated directly from a diverse range of real-world chart images using a state-of-the-art backbone architecture. ChartGemma addresses two crucial shortcomings of existing instruction-tuned chart models: the instruction-tuning data is generated from the underlying data tables instead of the chart images, limiting their adaptability and extendibility to real-world, and use weakly aligned backbone models, restricting their generalizability. Our simple approach yields significant improvements over existing chart representation models, with a relatively smaller model in terms of number of parameters. Our extensive error analyses and human studies show that ChartGemma produces more realistic, informative, and factually correct outputs as compared to its contemporaries.\nAs future work, we aim to formulate a more diverse instruction-tuning dataset which is created using human written instructions capturing varied nuances present in charts. We also aim to propose a more generalized benchmark catered to addressing complex visual elements in charts with more chart relevant evaluation metrics."}, {"title": "Limitations", "content": "Despite the effectiveness of our instruction-tuning approach and our model, there are notable limitations. Firstly, the instruction-tuning data is generated using a proprietary LLM, which could restrict the model's use in certain commercial environments. Secondly, the input resolution of our model's vision encoder is capped at 448x448; any increase in resolution leads to a quadratic rise in processing time. Third, we depend on the closed-source model, GPT4, for evaluating crucial metrics such as Informativeness and Factual Correctness. The frequent updates and potential deprecation of closed-source models pose challenges for the reproducibility of our results. Lastly, the model is prone to hallucinations, occasionally producing factually incorrect statements or erroneous code. We advise users to implement robust guardrails and exercise caution when deploying our model in real-world applications."}, {"title": "Ethics Statement", "content": "Since our model generates responses autoregressively, it is prone to errors and hallucinations. The outputs can sometimes be misleading or contain inaccuracies. Additionally, there is no guarantee that the codes generated by our model will be free from malicious content. Therefore, it is crucial for users of our model to implement strict safety guidelines to mitigate these potential risks. However, the base datasets we use for further generating our instruction-tuning data are available publicly either as full datasets or URLs with public licenses. Furthermore, all chart images in our dataset were sourced from existing, publicly available research papers that have filtered out any offensive content. We plan to release our visual instruction-tuning dataset in the same way as the base datasets (images where the licenses allow us and URLs where they do not). We also release our trained ChartGemma model in easy-to-use demos and various formats and across quantizations for extremely accessible adoption by the community. For our human evaluation study, we requested the help of our research collaborators. There were no personal identification information collected during this study. As the focus of the research was about assessing models' capabilities and limitations in several chart understanding tasks, the human evaluation performed by the authors does not add any ethical issues or unwanted biases."}]}