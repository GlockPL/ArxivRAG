{"title": "NaviSlim: Adaptive Context-Aware Navigation and Sensing via Dynamic Slimmable Networks", "authors": ["Timothy K Johnsen", "Marco Levorato"], "abstract": "Small-scale autonomous airborne vehicles, such as micro-drones, are expected to be a central component of a broad spectrum of applications ranging from exploration to surveillance and delivery. This class of vehicles is characterized by severe constraints in computing power and energy reservoir, which impairs their ability to support the complex state-of-the-art neural models needed for autonomous operations. The main contribution of this paper is a new class of neural navigation models - NaviSlim \u2013 capable of adapting the amount of resources spent on computing and sensing in response to the current context (i.e., difficulty of the environment, current trajectory, and navigation goals). Specifically, NaviSlim is designed as a gated slimmable neural network architecture that, different from existing slimmable networks, can dynamically select a slimming factor to autonomously scale model complexity, which consequently optimizes execution time and energy consumption. Moreover, different from existing sensor fusion approaches, NaviSlim can dynamically select power levels of onboard sensors to autonomously reduce power and time spent during sensor acquisition, without the need to switch between different neural networks. By means of extensive training and testing on the robust simulation environment Microsoft AirSim, we evaluate our NaviSlim models on scenarios with varying difficulty and a test set that showed a dynamic reduced model complexity on average between 57-92%, and between 61-80% sensor utilization, as compared to static neural networks designed to match computing and sensing of that required by the most difficult scenario.", "sections": [{"title": "I. INTRODUCTION", "content": "Drone autonomy is a rapidly developing area of investigation among Internet of Things (IoT) devices, with potential impact to a broad range of applications such as remote exploration, first response, agriculture, and delivery. An extensive survey on hardware and software requirements for developing fully autonomous Unmanned Aerial Vehicles (UAV) can be found in [1]. The most significant issues reported by the authors are: increasing complexity of tasks, operating in unknown and diverse environments, limitations on sensing capabilities, flight time, and energy consumption.\nThe aforementioned issues become more prominent when considering small-scale vehicles such as airborne micro-drones. This class of vehicles suffers from extreme constraints in terms of computational power and energy reservoir, which severely limits their ability to support the complex machine learning algorithms prevalent in vehicular autonomy.\nMoreover, the same limitations affect onboard sensors, that require time and energy to achieve adequate resolution to support autonomous functionalities in dynamic environments. Intuitively, the time required to execute sensor acquisition and machine learning algorithms during inference also slow down the reaction time of autonomous vehicles.\nThe literature related towards developing (micro-)drone autonomy is rich in autonomous drone racing [2]; while other efforts are in developing autonomous swarms [3], or in robust test-beds to evaluate them in [4]. General studies towards drone autonomy have focused on reducing computation required to execute static neural models as in [5], [6], fusing sensors [7], and of course general autonomous navigation logic and modeling such as in [8]. We find the literature lacking in studies that place an emphasis on developing autonomous navigation methods specifically for micro-drone systems with extreme resource constraints. To this aim, we contend that it is necessary to evolve the static nature of state-of-the-art navigation models, and neural models in general, into dynamic algorithms that use the minimum amount of computational complexity required by the difficulty of scenario, and concurrently the minimum time and energy spent in sensor acquisition.\nTo accomplish such an ambitious objective, in this paper we introduce a new class of navigation models that have a dynamic architecture capable of adapting its structure in real-time to the difficulty of the current operating environment at a fine-time granularity. Specifically, we present NaviSlim, a navigational neural network architecture that dynamically scales its own complexity and sensor modalities based on the perceived context. To support this logic, we propose a robust design and multi-stage training approach based primarily on slimmable networks [9], which train partitioned sub-networks within a larger static one, and knowledge distillation [10], which trains different models to exhibit similar behavior. In addition to these two core components, our design utilizes a broad array of advanced algorithms and methods such as shortest path algorithms, supervised learning, deep reinforcement learning, and curriculum learning.\nWe develop our models using a test bed environment with a"}, {"title": "II. RELATED WORK", "content": "State-of-the-art navigation models employ deep reinforcement learning, as in [13]-[18], in the form of a static neural model processing data from a fixed sensor array which\nwe use as a comparison to our presented models. The core limitation of such state-of-the-art approaches is that the model and sensing characteristics need to match the most challenging operating situation, which leads to an unnecessarily large resource usage \u2013 i.e., the neural network models and sensing requirements are static. Several frameworks are deployed as Simultaneous Localization And Mapping (SLAM) approaches, which includes methods for navigation such as localization, mapping, and tracking [19]. However, SLAM algorithms require extensive sensing and intense computing, and are impractical for microdrones which are the focus of this paper..\nOur proposed architecture falls under the general umbrella of dynamic neural networks, which can scale the depth of a neural network (vertically) with early exits [10] and the width of the network (horizontally) with slimmable networks [9]. The most popular class of dynamic neural models use early exits [10], where low complexity structures (the early exits) are attached to a main model and are sequentially executed. The processing of an input is terminated if the output of the last executed exit has sufficient confidence, so that the overall number of operations depends on the complexity of individual input samples. Our architecture belongs to a new emerging class of dynamic neural networks dynamic slimmable models adopted by a small number of recent contributions [20], [21], where an internal module manipulates characteristics of the entire network.\nAn alternative approach to ours is to store multiple versions of the same model and swap models at runtime, an approach that requires extended memory availability, as well as a potential context switching latency. Neural architecture search (NAS) [22] embeds a DRL algorithm to select the optimal network structure, however is extremely time consuming as each iteration requires complete training. In our approach, we develop models that realize an advanced form of dynamic slimmable networks [9], designed to seamlessly change its shape with minimal memory and no latency overhead. Technically, this is accomplished by horizontally scaling down the number of active nodes in a set of target layer(s) within a larger super-network. The super-network can be scaled down at various increments, thus creating a series of smaller sub-networks that can be used for inference. Moreover, we employ universally slimmable networks [23] that dynamically and continuously scale down the number of activation nodes in each hidden layer. The design of techniques to select which sub-networks to use during inference is an area of research attracting considerable interest. The very few available solutions (targeting image classification only), [24], [25], use neural gates to intelligently select sub-networks. With NaviSlim, we advance the state-of-the-art by designing and training a dynamic slimmable neural network for navigation whose shape is controlled by a context-aware gate capable of selecting from a continuous array of sub-networks on a sample-by-sample basis."}, {"title": "III. OVERVIEW AND PROBLEM FORMULATION", "content": "Let us first describe the general setting in which we position our contribution. We consider the task to navigate a microdrone with extremely limited onboard resources, in terms of sensing and computing capabilities, from one location to another while avoiding collisions and minimizing path length. Although the framework and methodology we propose are applicable to more general settings, here we consider a micro-drone equipped with: (a) multiple depth sensors (e.g., LiDARs) pointing in different directions (e.g., forward and downward), and (b) a GPS module returning its position on the map. Depth sensors provide rich information about the environment, which can be used for navigation in settings that do not require high-level reasoning (e.g., semantic features of objects such as the meaning of a traffic sign). Sensor information is input into a neural network which outputs motion commands for the drone.\nIn the context of micro-drones, the energy associated with sensing and computing represents a non-negligible portion of the overall expenditure. Our measurements show that the continuous execution of a relatively lightweight convolutional model use for object detection, executed on a GPU, can take up to 12% of the total power needed for airborne motion, sensing, and computing. Thus, we consider both dynamic sensors and dynamic neural networks that can be controlled to minimize resource usage. The depth sensors can be tuned to scan a partial field of view, where the smaller the acquired field of view the smaller the time and energy used by the sensor. The relationship between scanned area, resolution, sampling time, and energy is exemplified by lightweight LiDAR sensors that require an amount of time and energy proportional to the extent of the variable scanned area and sampling time [26]. In the framework we propose, the neural network used to control motion commands is a universally slimmable network [23], where the number of nodes in each hidden layer can vary to accordingly reduce time and energy spent during computation.", "sections": [{"title": "A. Problem Formulation", "content": "We consider a drone with an on-board processor tasked to navigate an unknown terrain by utilizing a heterogeneous sensor array and embedded neural model. Given a set of sensor observations, o(t), measured at the current timestep, t, the overall objectives of the embedded neural model are:\n\u2022 Navigation: Output navigation motions, n, required for the controller to drive the drone on a length-optimal path that minimizes flight time and energy, while avoiding collisions.\n\u2022 Computing: Execute operations, c, that minimize computing resources used to calculate n. We use the number of active sub-network parameters, m, as a proxy for computing resource usage an intuitive rationale that we validate experimentally.\n\u2022 Sensing: Query commands, s, that minimize sensing resources used to acquire observations which are then used as inputs to calculate n. We allocate to the sensors a discrete power level, w, used as a proxy for sensing resource usage.\nThe pipeline from o(t) to [n(t), c(t), s(t + 1)] is illustrated in Fig. 2. First, the newly acquired set of sensor observations for the current time step, o(t), is sent into a First In First Out (FIFO) queue. The FIFO queue acts as an attention mechanism, keeping the top T-many recent observations as done in [27] for autonomous control of Atari games. The FIFO queue is then input into a context-aware neural model that: 1) uses an intermediate mechanism to predict the minimum c(t) needed to predict n(t) given the scenario at the current time step, 2) outputs the predicted n(t) values to execute at the current time step, and 3) outputs the predicted s(t+1) values to use during sensor acquisition at the next time step.\nHere we formalize the problem that we aim to solve. Let \u0398 be a set of trainable model parameters, such that \u03b8 = f\u03b8(FIFO), where f\u03b8 is the model and a is some subset of [n, c, s]. Let p be a path taken by the drone where the length of p is equal to the number of time steps, P be a set of known length-optimal paths, and P be the set of paths taken using f\u03b8. We optimize \u03b8 by minimizing the expected trade off between resource costs of computation, m, and resource costs of sensing, w, as controlled by a scalar, 0 \u2264 \u03b1 \u2264 1, and under the constraint that the length of each path taken by f\u03b8 is no longer than that of that of a scalar, \u03b2 \u2265 1, times the length of each corresponding optimal path:\n\nmin < \u03b1m + (1 \u2212 \u03b1)w >, \u03b8\ns.t. length(P(i)) <= \u03b2 * length(P(i)) \u2200 i \u2208 {1, ..., b},\n\nwhere <> indicates the expected value over all time steps and paths, b is the total number of paths, P(i) indicates the ith path, and length(p) indicates the number of time steps in path p. We design Equation (1) as to minimize computing and sensing resource usage, while retaining navigation accuracy. The constraint in Equation (1) is required, otherwise the optimization problem would result in the trivial solution where computing and sensing parameters are equal to zero."}, {"title": "B. Challenges and Contributions", "content": "The two main challenges in developing NaviSlim are:\nTest Bed Environment: To train an adaptive model for micro-drone navigation, we need a test bed environment to allow the algorithm to accumulate a large amount of experience across settings with a broad range of complexity and maneuvers. To this end, we developed our open-source software module to interface our models with Microsoft AirSim [11], which is a robust drone simulator for graphics rendering, sensor"}]}, {"title": "IV. TEST BED ENVIRONMENT", "content": "We train and test our models with a simulation framework that utilizes Microsoft AirSim [11], a robust drone simulator that renders physics and graphics in Unreal Engine [12]. AirSim has a Application Programming Interface (API) for Python, that can be used to communicate with the simulation, such as: create sensors and acquire observations, issue drone commands, and detect collisions. We use the AirSim API to interface with our NaviSlim repository, also in Python, which includes methods for deep reinforcement learning that partially utilize the Stable-Baselines3 library [29], neural network implementations that partially utilize the PyTorch library [30], and others such as curriculum learning, shortest path algorithms, supervised learning, knowledge distillation, logging, customization, and deployment to other environments including real world drone controllers. Previous studies have shown capabilities of launching models trained in simulation into the real world [31], [32] \u2013 thus a simulation tool is a robust means to explore and develop novel model architectures. Using a simulation also mitigates difficulties in training a model with real world hardware that would require mechanisms for episodic deep reinforcement learning.\nshapes and sizes which reflect real world encounters such as buildings, signs, cars, people, and live traffic. Both maps have varying densities of objects, and we train and evaluate over a wide range of densities."}, {"title": "V. NAVISLIM: DESIGN OVERVIEW", "content": "In this section, we provide an overview of NaviSlim, and will detail the specific components (the navigation and auxiliary models) in the next sections. A key novelty is that we design an auxiliary module to control resource expenditure (c or s), while the navigation module is used to control drone motions (n). Thus NaviSlim, f\u03b8, now consists of the navigation model, g\u03b8, and the auxiliary model, h\u03c8. If a vanilla approach is taken to train the overall model to simultaneously control both sensing (input) and computing (intermediate calculations used by the model), then the learning process is highly unstable and does not converge to a meaningful control logic \u2013 i.e., the navigation paths fail when evaluated in the test bed environment. Thus, our solution is to decouple computing and sensing into two variants of NaviSlim. We refer to methods and models related to computing as NaviSlim-C, and those related to sensing as NaviSlim-S (see Equation (2)):\nNaviSlim-C: n = g\u03b8(FIFO, c = h\u03c8(FIFO))\nNaviSlim-S: [n,s] = [g\u03b8(FIFO), h\u03c8(FIFO)].\nNote that this structure requires g\u03b8 and h\u03c8 to be executed in series for NaviSlim-C, while they can be executed in parallel for NaviSlim-S.\nThe overall system NaviSlim model (composed of the navigation and auxiliary models) is illustrated in Fig. 4. The \u201cToVec()\u201d component converts the data acquired from each depth sensor into preliminary vectors that are then concatenated with the GPS data into one feature vector, o, as measured at time t. This concatenated feature vector is then inserted onto the FIFO queue as illustrated previously in Fig. 2.\nAlgorithm 1 shows how a path, also called an episode, is executed using NaviSlim. Included in Algorithm 1 are various variables used for deep reinforcement learning, as detailed later in Section VII.", "sections": [{"title": "A. Universally Slimmable Networks", "content": "The navigation model is a universally slimmable network [23]. Here we introduce a new variable called the slimming factor, \u03c1, which controls the number of active nodes in each hidden layer, that is, c = [\u03c1] since \u03c1 controls the number of operations required to execute the navigation model.\nTake an arbitrary hidden layer comprised of a vector of nodes, h, indexed from the node at position k = 1 to k = q where q is the maximum number of nodes available for that hidden layer. The quantity q corresponds to the number of hidden layer nodes used by the static \"super-network\", which in our context is a network whose number of parameters must match the most difficult operating scenario. This super-network persists when \u03c1 = 1, whereas a sub-network is activated when \u03c1 < 1.\nThe number of active nodes in a layer is equal to roof(\u03c1q), where roof() rounds up to the nearest integer value, such that the active nodes are those indexed in the range [1, roof(\u03c1q)) and the deactivated nodes are those indexed in the range [roof(\u03c1q), q]. Such a procedural deactivation, as opposed to a random Bernoulli distribution such as used in dropout, is required so that we can select specific sub-networks from the super-network for both training and inference (see Fig. 5).\nThe relationship between \u03c1 and the total number of active parameters, m, in a sub-network is quadratic. Let u bet the number of input layer nodes, I be the number of hidden layers, q be an l-length vector containing the number of nodes in each hidden layer, and v be the number of output nodes. We consider a fully connected feed-forward multi-layer perceptron with bias terms and at least one hidden layer. We can directly calculate the number of active parameters in a sub-network, m, as a function of \u03c1:\n\nm(\u03c1) = u\u03c1q1 + \u03c1q1 + \u03c1q1\u03c1q2 + \u03c1q2 + ... + \u03c1q\u03b9\u03c5 + v,\nm(\u03c1) = (\u03a3l\u22121i=1qiqi+1)\u03c1\u00b2 + (uq1 + vq\u03b9 + \u03a3l\u22121i=1qi)\u03c1 + v.\nThus, there is a quadratic decrease in the number of active parameters of a sub-network with a decreased \u03c1."}, {"title": "B. Supervised Learning with Knowledge Distillation", "content": "A key method used when training NaviSlim is knowledge distillation. From the perspective of a slimmable network, the"}, {"title": "C. NaviSlim-C", "content": "The objective of NaviSlim-C is to reduce computing resources spent during execution of the navigation model. This is accomplished by defining the supervised_distillation function with Algorithm 2. Algorithm 2 uses the \u201csandwich rule\u201d [23], a method for sampling \u03c1 when training with knowledge distillation by calculating the error gradient first with \u03c1 = 1, distilling with \u03c1 set to the minimal value, and then distilling with some random intermediate values of \u03c1."}, {"title": "D. NaviSlim-S", "content": "The objective of NaviSlim-S is to minimize the resource usage of sensor acquisition at the next time step. This is accomplished by defining the supervised_distillation method with Algorithm 3: a novel approach applied to sensing, that distills the network to have similar output regardless of the variable sensor array power level, w.\nWe introduce two variables used to control the respective power levels of the forward, pf, and downward, pd, facing depth sensors, with s = [pf, pd]. The range of pf is [1,3], where the value one corresponds to the minimal power level which uses the smallest area available for scanning. The range of pd is [0,3], where zero is the minimal power level corresponding to completely turning off the downward depth sensor. Increasing power levels corresponds to the acquisition of larger areas during scanning with the respective sensor, where a power level of three is the maximum area, and thus the maximum power and time expenditure. Note that pf >= 1 so at-least some sensor information can be acquired at all times. The input layer to the navigation network is designed so that the number of required input nodes corresponds to the magnitude of pf and pd. Input nodes are deactivated using a procedural approach, so that sub-networks consisting of subsets of input nodes can be selected during inference and training, similar to NaviSlim-C.\nAlgorithm 2 and Algorithm 3 share a similar structure based on knowledge distillation and the sandwich rule. The difference between the two algorithms is how the respective networks are distilled. Algorithm 2 distills the navigation network to operate with a varying number of nodes in each hidden layer, while Algorithm 3 distills the network to operate with varying power levels used to acquire input sensor observations (which is likened to slimming the input layer). We extrapolate that similar approaches can be used to distill the network to operate with other varying attributes. This is why we use the name NaviSlim for both the variants, as NaviSlim fuses navigation with a form of \u201cslimming\u201d applied to different attributes of the navigation network."}]}, {"title": "VI. NAVISLIM: NAVIGATION MODULE", "content": "The objectives of the navigation module are two fold: 1) navigate along a length-optimal path from a spawn position to the goal, while avoiding collisions, and 2) execute the underlying navigation model using variable sensing and computing parameters. The process for training the underlying neural network is outlined below:\n1) Collect ground truth length-optimal paths, P, using an A-star [33] shortest path algorithm.\n2) Acquire sensor observations at each time step in P.\n3) Use supervised learning with knowledge distillation to train a dynamic neural network that maps the FIFO queue of recent observations to navigation motions, n.\n4) Freeze the navigation module, as to no longer update the trainable network parameters.\n5) Evaluate for a successful model by deploying g\u03b8 to the test bed environment."}, {"title": "A. A-star Shortest Path Algorithm", "content": "Shortest path algorithms are used to solve the problem of finding length-optimal paths between two points. Graph algorithms are a subset of shortest path algorithms where the problem can be constructed in a graph structure with vertices and edges. A-star [33] is a flavor of graph shortest path algorithms, that is guaranteed to find the optimal solution without having to traverse every possible path. We implement A-star by reconstructing the Blocks map into a graph where each vertex corresponds to a spatial point on the map, and edges define if an adjacent position is valid (i.e., does not have an object in it). The cost of a path is simply the total distance traveled.\nWe partition the Blocks map into regions used for a training, validation, and testing set. We then use A-star to find the optimal paths, P, in each set. The test bed environment is used to collect simulated sensor observations from Microsoft AirSim at each timestep in P. This results in a dataset of hard targets that map FIFO \u2192 n for each time step in each path within P. These are then used to train the navigation network."}, {"title": "B. Training the Neural Network", "content": "Given a set of maps, FIFO \u2192 n, over optimal paths, P, the training procedure for the navigation neural network, g\u03b8, is outlined in Algorithm 4. The function supervised_distillation is an argument to Algorithm 4 that controls how supervised learning works in unison with knowledge distillation as earlier defined in Section V-B. The training set is used to drive the error gradient, the validation set is used to trigger early stopping to mitigate overfit, and the test set is held out to later evaluate the navigation model. We use mean squared error as our loss function.\nAfter the navigation neural network is trained using Al- gorithm 4, the optimized parameters, \u0398, are frozen and not updated again. We evaluate if a trained navigation model is successful by deploying g\u03b8 into the AirSim environment and measuring the percent of paths in the test set that successfully reach their goal when using the super-network and Algorithm 1 but without an auxiliary module, h\u03c8. Thus, we are only evaluating the navigation prowess of the static super-network without adaptation."}, {"title": "VII. NAVISLIM: AUXILIARY MODULE", "content": "The objective of the auxiliary module is to control adaptation of either c or s as applied to the navigation module. The following steps are taken to create the auxiliary module:\n1) Successfully train a navigation model, g\u03b8.\n2) Create a reward function that penalizes the navigation algorithm for taking sub-optimal paths.\n3) Train the auxiliary model, h\u03c8, using a Twin Delayed Deep Deterministic Policy Gradient (TD3) [34] deep reinforcement learning algorithm. The objective is to map the FIFO queue to adaptation parameters, c or s.\n4) Evaluate for a successful model by deploying h\u03c8 with g\u03b8 to the test bed environment.", "sections": [{"title": "A. Deep Reinforcement Learning", "content": "Deep Reinforcement Learning (DRL) consists of an agent that will be episodically traversing an environment during the training process, by taking actions which result in an evaluated reward. The policy is learned during training that maps observations to optimal actions which in essence maximize the rewards. The word \u201cdeep\u201d simply refers to using a deep neural network as the policy mechanism."}, {"title": "B. Q-learning", "content": "The Q-value, derived from the Bellman equation [35], is an estimation of the aggregation of immediate and long term rewards in an episode \u2013 where higher Q-values correspond to more effective actions. Given a reward function, r(state), that inputs the state found after executing n, Equation (4) shows the optimization problem used to train h\u03c8:\n\nmax < Q-value >, \u03c8\nQ-value(p, t) = \u03a3length(P)i=t \u03b3i\u2212t *r(state(i)),\n\nwhere <> denotes the expected Q-value resulting from using g\u03c8 over all paths and time steps, state(i) corresponds to state variables used to calculate the reward at the ith time step from path p, and \u03b3 is a parameter called the discount factor which applies a decaying penalty to long term rewards."}, {"title": "C. Reward Function", "content": "Designing a reward function is highly non-trivial, as it directly affects the stability and convergence of the training algorithm, along with the learned policy. We use a random walk algorithm to estimate the behavior of a proposed reward function. Each iteration takes a mean step closer to an arbitrary goal initially set 100 meters away, with Gaussian noise added at each step. We then recursively calculate the rewards at each step to estimate the Q-value corresponding to the theoretical episode. This process is used to design and adjust the coefficients in the reward function by evaluating them against the average of several random walks. We, then, select the following reward function:\n\nr=\n{-\u03bb\u03bf collision \n\u03bb\u03b1 goal\n-Xatandh (d)  -\u03bbt -dep-ds(P\u0192 +Pa) otherwise "}, {"title": "D. TD3", "content": "Among the many available DRL algorithms, we select a Twin Delayed Deep Deterministic Policy Gradient (TD3) [34] algorithm; as it allows a continuous multivariate observation space, a continuous multivariate action space, and outperforms other DRL algorithms that we tested. TD3 is based on an actor-critic double deep Q-learning paradigm with clipped action noise and delayed policy updates, consisting of a series of six neural networks used in the training process. The actor network, h\u03c8, is the policy mechanism that inputs the FIFO queue of recent observations and outputs either c or s. The target actor network is used to estimate the next action, while noise is added to that action to smooth out training and account for error. The critic network estimates the Q-value, since future states are unknown. The target critic network is used to measure error in the critic network, since we can not calculate all possible future states. The target critic and target actor networks are initially clones of the critic and actor network, respectively, but the weights vary over time and every few episodes the target network weights are updated using a Polyak weighted average.\nTwo critic networks are used to help stabilize training by selecting the minimum Q-value between both critics, which is referred to as double deep Q-learning [36]. When training a neural network with a TD3 algorithm, initially actions are taken at random to explore the environment. Then the neural network is increasingly used to predict actions as to exploit the policy being learned. Even though six neural networks are used in the training process, only the actor network, h\u03c8, is executed at the deployment stage."}, {"title": "E. Curriculum Learning", "content": "A common difficulty in training a neural network with DRL arises when the initial task is too difficult. This causes poor rewards early in the learning process which can stagnate training. A solution is to start with an easier task, then progressively increase the difficulty. We implement curriculum learning by starting with a small distance between spawn and goal positions, then incrementally increase this distance as enough evaluation paths successfully reach the goal."}, {"title": "F. Training the Neural Network", "content": "We train the auxiliary network, h\u03c8, using Algorithm 5. Training the auxiliary network is the main bottleneck for training NaviSlim, because it can take several days before finishing. Further, the parameters to Algorithm 5 are highly sensitive (some of which are omitted), which requires ample time to explore them. Having several computers to run training in parallel with different parameters is highly advantageous. We evaluate if a trained auxiliary model is successful by deploying g\u03b8 and h\u03c8 into the AirSim environment and measuring the percent of paths in the test set that successfully reach their goal when using Algorithm 1. Further, we insure the constraint holds in Equation 1 since this is not guaranteed by Equation 4."}]}, {"title": "VIII. RESULTS", "content": "First we explore the size, the number of hidden layers and nodes in each layer, of the navigation super-network. We test each configuration by training the navigation model 10 random times with different seeds, and take the seed with the best error as measured from a static test set. Fig. 6 shows results of a hyper parameter grid search, displaying the Root Mean Squared Error (RMSE) between length-optimal motions as found from A-star, n, and those predicted from g\u03b8, \u02c6n. The darker region of Fig. 6 indicates that, as expected, using a larger super-network results in a reduced navigation error. The navigation model is then deployed in the simulation test bed, using Microsoft AirSim.\nFig. 7 shows the percentage of evaluation paths which successfully reach the goal versus distance to goal. The percentage of successful paths drops with increasing distance, as expected. The navigation models perform remarkably well when deployed to the more complex City map even though they are only trained using samples from the simple Blocks map. This illustrates the prowess, and generalization, of our navigation training methods. Typical approaches would stop here, and have a static navigation network with fixed computing and sensing required by that of the most difficult scenario. However, the following results come from experimentation of our novel approach to adapt computing and sensing with NaviSlim.\nNext we still evaluate the RMSE between length-optimal motions as found from A-star, n, and those predicted from g\u03b8, \u02c6n; however, we now select one navigation model (using the seed with best RMSE) and evaluate how the error changes with varying values of the slimming factor, \u03c1. Fig. 9 shows navigation RMSE as a function \u03c1 for each of the training, validation, and testing sets. This shows how the global RMSE increases with decreased \u03c1, warranting that if we use a static sub-network within the navigation super-network we would receive sub-par accuracy. The novelty of our approach is to intelligently and dynamically select the value of \u03c1, given context, as to not detrimentally decrease navigation accuracy which we experiment with next by training and evaluating the auxiliary network, h\u03c8.\nFirst we train the auxiliary network, h\u03c8, for NaviSlim-C which predicts the slimming factor, \u03c1, that directly controls the computing resources required to execute the navigation network. We evaluate NaviSlim-C on three scenarios with increasing difficulty: 1) the Blocks map with only horizontal motion allowed, 2) the Blocks map with vertical motion unlocked, and 3) the City map with vertical motion locked. We compare these scenarios as we hypothesize that the computing resources, represented by \u03c1, required to run the navigation network will increase with increased difficulty. This hypothesize is warranted with the evidence provided in Table I which shows the average adaptation values used to reduce resource allocation, as learned by NaviSlim, and evaluated on the test set. Listed are three scenarios with increasing difficulty. Where the \u201cScen.\u201d column is the scenario being tested for, and the \u201c\u03b7\u201d column shows the relative resource expenditure reduction either for the number of active navigation network parameters, m, or sensor power level, w. All values shown in Table I are calculated using the test set. We see a significantly higher mean value of \u03c1 between each of these scenarios. The \u03b7 values in Table I are percent decreases over the larger super-network. Note that the numbers reported in Table I are for all evaluations done at the end of each curriculum learning step before increasing the distance between spawn and goal in Algorithm 5, and all evaluation paths successfully reached their target position \u2013 thus navigation accuracy is maintained.\nFig. 10 shows the learned average values of \u03c1 at each position in the Blocks map, using the test set. This figure is useful to see the dynamic nature of NaviSlim and its context-aware behavior based on both the surrounding environment and maneuvers required to avoid collision with an object. We further isolate the context clues versus learned behavior as illustrated in Fig. 8, where we show the mean depth captured by each sensor versus \u03c1. Generally, when the navigation path is more convoluted there is a correspondingly high \u03c1, which is expected behavior for more complex navigation and maneuvers.\nInterestingly, as the distance between the drone and objects increases, so does the value predicted for \u03c1 from h\u03c8. A possible explanation is that when objects are close, higher-level logic is not needed as the subspace of possible motions, that can be executed without colliding with that nearby object, shrinks. Similarly, after about 50 meters, \u03c1 begins to decrease with further increasing measured depth, likely because the environment becomes more open (less nearby objects and more open physical space) and the subspace of possible motions shrinks as more sophisticated maneuvers are not needed.\nNext, we measure the actual differences in the resource cost (time, power, and energy) between using and not using NaviSlim on a microprocessor similar to that typically deployed on micro-drones. We use a Jetson Nano with a Quad-core ARM Cortex-A57 MPCore processor and 4 GB 64-bit LPDDR4 1600MHz 25.6 GB/s memory. We compare relative resource costs by passing a static set of observations through: (1) NaviSlim, including both the auxiliary and navigation modules, with the learned values for \u03c1; then (2) only the navigation network but with \u03c1 = 1 (i.e., just the static super-network without the auxiliary network). We measure the ratio difference between each resource as , where u is the resource cost associated with (1) NaviSlim and v is the resource v\u2212u v"}, {"title": "IX. CONCLUSIONS", "content": "We presented NaviSlim, the first of its kind to dynamically scale computing and sensing used by a neural model for navigation of a (micro-)drone with extreme resource constraints. We detailed the training procedure used to obtain successful models that can safely navigate between points A and B, while using variable computing and sensing. We showed that an auxiliary neural network can successfully learn to map context to computing and sensing required by the difficulty of the current scenario. This is a novel evolution over static networks that must match computing and sensing of that required by the most difficult scenario. We showed that when deploying NaviSlim to our test bed environment interfaced with the drone simulation tool Microsoft Airsim, we reduced average navigation model complexity between 57% and 82%,"}]}