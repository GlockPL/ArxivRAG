{"title": "NaviSlim: Adaptive Context-Aware Navigation and Sensing via Dynamic Slimmable Networks", "authors": ["Timothy K Johnsen", "Marco Levorato"], "abstract": "Small-scale autonomous airborne vehicles, such as micro-drones, are expected to be a central component of a broad spectrum of applications ranging from exploration to surveillance and delivery. This class of vehicles is characterized by severe constraints in computing power and energy reservoir, which impairs their ability to support the complex state-of-the-art neural models needed for autonomous operations. The main contribution of this paper is a new class of neural navigation models - NaviSlim \u2013 capable of adapting the amount of resources spent on computing and sensing in response to the current context (i.e., difficulty of the environment, current trajectory, and navigation goals). Specifically, NaviSlim is designed as a gated slimmable neural network architecture that, different from existing slimmable networks, can dynamically select a slimming factor to autonomously scale model complexity, which consequently optimizes execution time and energy consumption. Moreover, different from existing sensor fusion approaches, NaviSlim can dynamically select power levels of onboard sensors to autonomously reduce power and time spent during sensor acquisition, without the need to switch between different neural networks. By means of extensive training and testing on the robust simulation environment Microsoft AirSim, we evaluate our NaviSlim models on scenarios with varying difficulty and a test set that showed a dynamic reduced model complexity on average between 57-92%, and between 61-80% sensor utilization, as compared to static neural networks designed to match computing and sensing of that required by the most difficult scenario.", "sections": [{"title": "I. INTRODUCTION", "content": "Drone autonomy is a rapidly developing area of investigation among Internet of Things (IoT) devices, with potential impact to a broad range of applications such as remote exploration, first response, agriculture, and delivery. An extensive survey on hardware and software requirements for developing fully autonomous Unmanned Aerial Vehicles (UAV) can be found in [1]. The most significant issues reported by the authors are: increasing complexity of tasks, operating in unknown and diverse environments, limitations on sensing capabilities, flight time, and energy consumption.\nThe aforementioned issues become more prominent when considering small-scale vehicles such as airborne micro-drones. This class of vehicles suffers from extreme constraints in terms of computational power and energy reservoir, which severely limits their ability to support the complex machine learning algorithms prevalent in vehicular autonomy. Moreover, the same limitations affect onboard sensors, that require time and energy to achieve adequate resolution to support autonomous functionalities in dynamic environments. Intuitively, the time required to execute sensor acquisition and machine learning algorithms during inference also slow down the reaction time of autonomous vehicles.\nThe literature related towards developing (micro-)drone autonomy is rich in autonomous drone racing [2]; while other efforts are in developing autonomous swarms [3], or in robust test-beds to evaluate them in [4]. General studies towards drone autonomy have focused on reducing computation required to execute static neural models as in [5], [6], fusing sensors [7], and of course general autonomous navigation logic and modeling such as in [8]. We find the literature lacking in studies that place an emphasis on developing autonomous navigation methods specifically for micro-drone systems with extreme resource constraints. To this aim, we contend that it is necessary to evolve the static nature of state-of-the-art navigation models, and neural models in general, into dynamic algorithms that use the minimum amount of computational complexity required by the difficulty of scenario, and concurrently the minimum time and energy spent in sensor acquisition.\nTo accomplish such an ambitious objective, in this paper we introduce a new class of navigation models that have a dynamic architecture capable of adapting its structure in real-time to the difficulty of the current operating environment at a fine-time granularity. Specifically, we present NaviSlim, a navigational neural network architecture that dynamically scales its own complexity and sensor modalities based on the perceived context. To support this logic, we propose a robust design and multi-stage training approach based primarily on slimmable networks [9], which train partitioned sub-networks within a larger static one, and knowledge distillation [10], which trains different models to exhibit similar behavior. In addition to these two core components, our design utilizes a broad array of advanced algorithms and methods such as shortest path algorithms, supervised learning, deep reinforcement learning, and curriculum learning.\nWe develop our models using a test bed environment with a"}, {"title": "II. RELATED WORK", "content": "State-of-the-art navigation models employ deep reinforcement learning, as in [13]-[18], in the form of a static neural model processing data from a fixed sensor array which we use as a comparison to our presented models. The core limitation of such state-of-the-art approaches is that the model and sensing characteristics need to match the most challenging operating situation, which leads to an unnecessarily large resource usage \u2013 i.e., the neural network models and sensing requirements are static. Several frameworks are deployed as Simultaneous Localization And Mapping (SLAM) approaches, which includes methods for navigation such as localization, mapping, and tracking [19]. However, SLAM algorithms require extensive sensing and intense computing, and are impractical for microdrones which are the focus of this paper..\nOur proposed architecture falls under the general umbrella of dynamic neural networks, which can scale the depth of a neural network (vertically) with early exits [10] and the width of the network (horizontally) with slimmable networks [9]. The most popular class of dynamic neural models use early exits [10], where low complexity structures (the early exits) are attached to a main model and are sequentially executed. The processing of an input is terminated if the output of the last executed exit has sufficient confidence, so that the overall number of operations depends on the complexity of individual input samples. Our architecture belongs to a new emerging class of dynamic neural networks dynamic slimmable models adopted by a small number of recent contributions [20], [21], where an internal module manipulates characteristics of the entire network.\nAn alternative approach to ours is to store multiple versions of the same model and swap models at runtime, an approach that requires extended memory availability, as well as a potential context switching latency. Neural architecture search (NAS) [22] embeds a DRL algorithm to select the optimal network structure, however is extremely time consuming as each iteration requires complete training. In our approach, we develop models that realize an advanced form of dynamic slimmable networks [9], designed to seamlessly change its shape with minimal memory and no latency overhead. Technically, this is accomplished by horizontally scaling down the number of active nodes in a set of target layer(s) within a larger super-network. The super-network can be scaled down at various increments, thus creating a series of smaller sub-networks that can be used for inference. Moreover, we employ universally slimmable networks [23] that dynamically and continuously scale down the number of activation nodes in each hidden layer. The design of techniques to select which sub-networks to use during inference is an area of research attracting considerable interest. The very few available solutions (targeting image classification only), [24], [25], use neural gates to intelligently select sub-networks. With NaviSlim, we advance the state-of-the-art by designing and training a dynamic slimmable neural network for navigation whose shape is controlled by a context-aware gate capable of selecting from a continuous array of sub-networks on a sample-by-sample basis."}, {"title": "III. OVERVIEW AND PROBLEM FORMULATION", "content": "Let us first describe the general setting in which we position our contribution. We consider the task to navigate a microdrone with extremely limited onboard resources, in terms of sensing and computing capabilities, from one location to another while avoiding collisions and minimizing path length. Although the framework and methodology we propose are applicable to more general settings, here we consider a micro-drone equipped with: (a) multiple depth sensors (e.g., LiDARs) pointing in different directions (e.g., forward and downward), and (b) a GPS module returning its position on the map. Depth sensors provide rich information about the environment, which can be used for navigation in settings that do not require high-level reasoning (e.g., semantic features of objects such as the meaning of a traffic sign). Sensor information is input into a neural network which outputs motion commands for the drone.\nIn the context of micro-drones, the energy associated with sensing and computing represents a non-negligible portion of the overall expenditure. Our measurements show that the continuous execution of a relatively lightweight convolutional model use for object detection, executed on a GPU, can take up to 12% of the total power needed for airborne motion, sensing, and computing. Thus, we consider both dynamic sensors and dynamic neural networks that can be controlled to minimize resource usage. The depth sensors can be tuned to scan a partial field of view, where the smaller the acquired field of view the smaller the time and energy used by the sensor. The relationship between scanned area, resolution, sampling time, and energy is exemplified by lightweight LiDAR sensors that require an amount of time and energy proportional to the extent of the variable scanned area and sampling time [26]. In the framework we propose, the neural network used to control motion commands is a universally slimmable network [23], where the number of nodes in each hidden layer can vary to accordingly reduce time and energy spent during computation.\nWe consider a drone with an on-board processor tasked to navigate an unknown terrain by utilizing a heterogeneous sensor array and embedded neural model. Given a set of sensor observations, $o(t)$, measured at the current timestep, $t$, the overall objectives of the embedded neural model are:\nOutput navigation motions, $n$, required for the controller to drive the drone on a length-optimal path that minimizes flight time and energy, while avoiding collisions.\nExecute operations, $c$, that minimize computing resources used to calculate $n$. We use the number of active sub-network parameters, $m$, as a proxy for computing resource usage an intuitive rationale that we validate experimentally.\nQuery commands, $s$, that minimize sensing resources used to acquire observations which are then used as inputs to calculate $n$. We allocate to the sensors a discrete power level, $w$, used as a proxy for sensing resource usage.\nThe pipeline from $o(t)$ to $[n(t), c(t), s(t + 1)]$ is illustrated in Fig. 2. First, the newly acquired set of sensor observations for the current time step, $o(t)$, is sent into a First In First Out (FIFO) queue. The FIFO queue acts as an attention mechanism, keeping the top $T$-many recent observations as done in [27] for autonomous control of Atari games. The FIFO queue is then input into a context-aware neural model that: 1) uses an intermediate mechanism to predict the minimum $c(t)$ needed to predict $n(t)$ given the scenario at the current time step, 2) outputs the predicted $n(t)$ values to execute at the current time step, and 3) outputs the predicted $s(t+1)$ values to use during sensor acquisition at the next time step.\nHere we formalize the problem that we aim to solve. Let $\\Theta$ be a set of trainable model parameters, such that $\\theta = f_{\\theta}(FIFO)$, where $f_{\\theta}$ is the model and $a$ is some subset of $[n, c, s]$. Let $p$ be a path taken by the drone where the length of $p$ is equal to the number of time steps, $\\mathcal{P}$ be a set of known length-optimal paths, and $\\hat{\\mathcal{P}}$ be the set of paths taken using $f_{\\theta}$. We optimize $\\Theta$ by minimizing the expected trade off between resource costs of computation, $m$, and resource costs of sensing, $w$, as controlled by a scalar, $0 \\leq \\alpha \\leq 1$, and under the constraint that the length of each path taken by $f_{\\theta}$ is no longer than that of that of a scalar, $\\beta \\geq 1$, times the length of each corresponding optimal path:\n$\\min < \\alpha m + (1 - \\alpha)w >,$   (1)\ns.t. $\\text{length}(\\hat{\\mathcal{P}}^{(i)}) <= \\beta * \\text{length}(\\mathcal{P}^{(i)}) \\forall i \\in \\{1, ..., b\\},$ where $< >$ indicates the expected value over all time steps and paths, $b$ is the total number of paths, $\\mathcal{P}^{(i)}$ indicates the $i^{th}$ path, and $\\text{length}(p)$ indicates the number of time steps in path $p$. We design Equation (1) as to minimize computing and sensing resource usage, while retaining navigation accuracy. The constraint in Equation (1) is required, otherwise the optimization problem would result in the trivial solution where computing and sensing parameters are equal to zero.\nThe two main challenges in developing NaviSlim are:"}, {"title": "IV. TEST BED ENVIRONMENT", "content": "We train and test our models with a simulation framework that utilizes Microsoft AirSim [11], a robust drone simulator that renders physics and graphics in Unreal Engine [12]. AirSim has a Application Programming Interface (API) for Python, that can be used to communicate with the simulation, such as: create sensors and acquire observations, issue drone commands, and detect collisions. We use the AirSim API to interface with our NaviSlim repository, also in Python, which includes methods for deep reinforcement learning that partially utilize the Stable-Baselines3 library [29], neural network implementations that partially utilize the PyTorch library [30], and others such as curriculum learning, shortest path algorithms, supervised learning, knowledge distillation, logging, customization, and deployment to other environments including real world drone controllers. Previous studies have shown capabilities of launching models trained in simulation into the real world [31], [32] \u2013 thus a simulation tool is a robust means to explore and develop novel model architectures. Using a simulation also mitigates difficulties in training a model with real world hardware that would require mechanisms for episodic deep reinforcement learning.\nFig. 3 shows two maps used by AirSim. The first map is \"Blocks\", which contains several static objects that have generic shapes and sizes. The second map is \"City\", which contains various static and dynamic objects that have specific shapes and sizes which reflect real world encounters such as buildings, signs, cars, people, and live traffic. Both maps have varying densities of objects, and we train and evaluate over a wide range of these densities."}, {"title": "V. NAVISLIM: DESIGN OVERVIEW", "content": "In this section, we provide an overview of NaviSlim, and will detail the specific components (the navigation and auxiliary models) in the next sections. A key novelty is that we design an auxiliary module to control resource expenditure ($c$ or $s$), while the navigation module is used to control drone motions ($n$). Thus NaviSlim, $f_{\\theta}$, now consists of the navigation model, $g_{\\theta}$, and the auxiliary model, $h_{\\theta}$. If a vanilla approach is taken to train the overall model to simultaneously control both sensing (input) and computing (intermediate calculations used by the model), then the learning process is highly unstable and does not converge to a meaningful control logic \u2013 i.e., the navigation paths fail when evaluated in the test bed environment. Thus, our solution is to decouple computing and sensing into two variants of NaviSlim. We refer to methods and models related to computing as NaviSlim-C, and those related to sensing as NaviSlim-S (see Equation (2)):\nNaviSlim-C:\n$n = g_{\\theta}(FIFO, c = h_{\\theta}(FIFO))$\n(2)\nNaviSlim-S:\n$[n,s] = [g_{\\theta}(FIFO), h_{\\theta}(FIFO)].$\nNote that this structure requires $g_{\\theta}$ and $h_{\\theta}$ to be executed in series for NaviSlim-C, while they can be executed in parallel for NaviSlim-S.\nThe overall system NaviSlim model (composed of the navigation and auxiliary models) is illustrated in Fig. 4. The \u201cToVec()\u201d component converts the data acquired from each depth sensor into preliminary vectors that are then concatenated with the GPS data into one feature vector, $o$, as measured at time $t$. This concatenated feature vector is then inserted onto the FIFO queue as illustrated previously in Fig. 2.\nAlgorithm 1 shows how a path, also called an episode, is executed using NaviSlim. Included in Algorithm 1 are various variables used for deep reinforcement learning, as detailed later in Section VII."}, {"title": "A. Universally Slimmable Networks", "content": "The navigation model is a universally slimmable network [23]. Here we introduce a new variable called the slimming factor, $\\rho$, which controls the number of active nodes in each hidden layer, that is, $c = [\\rho]$ since $\\rho$ controls the number of operations required to execute the navigation model.\nTake an arbitrary hidden layer comprised of a vector of nodes, $h$, indexed from the node at position $k = 1$ to $k = q$ where $q$ is the maximum number of nodes available for that hidden layer. The quantity $q$ corresponds to the number of hidden layer nodes used by the static \"super-network\", which in our context is a network whose number of parameters must match the most difficult operating scenario. This super-network persists when $\\rho = 1$, whereas a sub-network is activated when $\\rho < 1$.\nThe number of active nodes in a layer is equal to $\\text{roof}(\\rho q)$, where $\\text{roof}()$ rounds up to the nearest integer value, such that the active nodes are those indexed in the range $[1, \\text{roof}(\\rho q))$ and the deactivated nodes are those indexed in the range $[\text{roof}(\\rho q), q]$. Such a procedural deactivation, as opposed to a random Bernoulli distribution such as used in dropout, is required so that we can select specific sub-networks from the super-network for both training and inference (see Fig. 5).\nThe relationship between $\\rho$ and the total number of active parameters, $m$, in a sub-network is quadratic. Let $u$ bet the number of input layer nodes, $I$ be the number of hidden layers, $q$ be an $l$-length vector containing the number of nodes in each hidden layer, and $v$ be the number of output nodes. We consider a fully connected feed-forward multi-layer perceptron with bias terms and at least one hidden layer. We can directly calculate the number of active parameters in a sub-network, $m$, as a function of $\\rho$:\n$m(\\rho) = u\\rho q_1 + \\rho q_1 + \\rho q_1\\rho q_2 + \\rho q_2 + ... + \\rho q_l v + v,$\n(3)\n$m(\\rho) = (\\Sigma_{i=1}^{l-1}q_iq_{i+1})\\rho^2 + (uq_1 + vq_l + \\Sigma_{i=1}^{l}q_i)\\rho + v.$\nThus, there is a quadratic decrease in the number of active parameters of a sub-network with a decreased $\\rho$."}, {"title": "B. Supervised Learning with Knowledge Distillation", "content": "A key method used when training NaviSlim is knowledge distillation. From the perspective of a slimmable network, the main idea of knowledge distillation is to teach sub-networks similar outputs as the super-network. This is accomplished by adding a step to typical supervised learning after the error gradient is calculated between ground truth labels (\"hard targets\") and the outputs of the super-network, that calculates an additional error gradient between the outputs of the super-network (\"soft targets\") and outputs of any activated sub-networks. The loss function uses the error between both: A) super-network outputs and ground truth labels and B) sub-network outputs and super-network outputs. This combined supervised learning and knowledge distillation step, what we refer to as the supervised_distillation function, is defined differently between NaviSlim-C and NaviSlim-S."}, {"title": "C. NaviSlim-C", "content": "The objective of NaviSlim-C is to reduce computing resources spent during execution of the navigation model. This is accomplished by defining the supervised_distillation function with Algorithm 2. Algorithm 2 uses the \"sandwich rule\" [23], a method for sampling $\\rho$ when training with knowledge distillation by calculating the error gradient first with $\\rho = 1$, distilling with $\\rho$ set to the minimal value, and then distilling with some random intermediate values of $\\rho$."}, {"title": "D. NaviSlim-S", "content": "The objective of NaviSlim-S is to minimize the resource usage of sensor acquisition at the next time step. This is accomplished by defining the supervised_distillation method with Algorithm 3: a novel approach applied to sensing, that distills the network to have similar output regardless of the variable sensor array power level, $w$.\nWe introduce two variables used to control the respective power levels of the forward, $p_f$, and downward, $p_d$, facing depth sensors, with $s = [p_f,p_d]$. The range of $p_f$ is $[1,3]$, where the value one corresponds to the minimal power level which uses the smallest area available for scanning. The range of $p_d$ is $[0,3]$, where zero is the minimal power level corresponding to completely turning off the downward depth sensor. Increasing power levels corresponds to the acquisition of larger areas during scanning with the respective sensor, where a power level of three is the maximum area, and thus the maximum power and time expenditure. Note that $p_f >= 1$ so at-least some sensor information can be acquired at all times. The input layer to the navigation network is designed so that the number of required input nodes corresponds to the magnitude of $p_f$ and $p_d$. Input nodes are deactivated using a procedural approach, so that sub-networks consisting of subsets of input nodes can be selected during inference and training, similar to NaviSlim-C."}, {"title": "VI. NAVISLIM: NAVIGATION MODULE", "content": "The objectives of the navigation module are two fold: 1) navigate along a length-optimal path from a spawn position to the goal, while avoiding collisions, and 2) execute the underlying navigation model using variable sensing and computing parameters. The process for training the underlying neural network is outlined below:\n1) Collect ground truth length-optimal paths, $\\mathcal{P}$, using an A-star [33] shortest path algorithm.\n2) Acquire sensor observations at each time step in $\\mathcal{P}$.\n3) Use supervised learning with knowledge distillation to train a dynamic neural network that maps the FIFO queue of recent observations to navigation motions, $n$.\n4) Freeze the navigation module, as to no longer update the trainable network parameters.\n5) Evaluate for a successful model by deploying $g_{\\theta}$ to the test bed environment."}, {"title": "VII. NAVISLIM: AUXILIARY MODULE", "content": "The objective of the auxiliary module is to control adaptation of either $c$ or $s$ as applied to the navigation module. The following steps are taken to create the auxiliary module:\n1) Successfully train a navigation model, $g_{\\theta}$.\n2) Create a reward function that penalizes the navigation algorithm for taking sub-optimal paths.\n3) Train the auxiliary model, $h_{\\theta}$, using a Twin Delayed Deep Deterministic Policy Gradient (TD3) [34] deep reinforcement learning algorithm. The objective is to map the FIFO queue to adaptation parameters, $c$ or $s$.\n4) Evaluate for a successful model by deploying $h_{\\theta}$ with $g_{\\theta}$ to the test bed environment."}, {"title": "A. Deep Reinforcement Learning", "content": "Deep Reinforcement Learning (DRL) consists of an agent that will be episodically traversing an environment during the training process, by taking actions which result in an evaluated reward. The policy is learned during training that maps observations to optimal actions which in essence maximize the rewards. The word \"deep\" simply refers to using a deep neural network as the policy mechanism."}, {"title": "B. Q-learning", "content": "The Q-value, derived from the Bellman equation [35], is an estimation of the aggregation of immediate and long term rewards in an episode \u2013 where higher Q-values correspond to more effective actions. Given a reward function, $r(state)$, that inputs the state found after executing $n$, Equation (4) shows the optimization problem used to train $h_{\\theta}$:\n$\\max < Q\\text{-value} >,$\n(4)\n$Q\\text{-value}(p, t) = \\Sigma_{i=t}^{\\text{length}(P)} \\gamma^{i-t} * r(\\text{state}^{(i)}),$ where $<>$ denotes the expected Q-value resulting from using $g_{\\theta}$ over all paths and time steps, $state^{(i)}$ corresponds to state variables used to calculate the reward at the $i^{th}$ time step from path $p$, and $\\gamma$ is a parameter called the discount factor which applies a decaying penalty to long term rewards."}, {"title": "C. Reward Function", "content": "Designing a reward function is highly non-trivial, as it directly affects the stability and convergence of the training algorithm, along with the learned policy. We use a random walk algorithm to estimate the behavior of a proposed reward function. Each iteration takes a mean step closer to an arbitrary goal initially set 100 meters away, with Gaussian noise added at each step. We then recursively calculate the rewards at each step to estimate the Q-value corresponding to the theoretical episode. This process is used to design and adjust the coefficients in the reward function by evaluating them against the average of several random walks. We, then, select the following reward function:\n$r=\\begin{cases}\n-\\lambda_o  & \\text{collision}\\\\ \\lambda_g & \\text{goal}\\\\\n\\lambda \\text{atanh}(d) - \\lambda_t  - d\\lambda_{cp} - d_s(\\rho_f + \\rho_d) & \\text{otherwise}\n\\end{cases},$ (5)"}, {"title": "D. TD3", "content": "Among the many available DRL algorithms, we select a Twin Delayed Deep Deterministic Policy Gradient (TD3) [34] algorithm; as it allows a continuous multivariate observation space, a continuous multivariate action space, and outperforms other DRL algorithms that we tested. TD3 is based on an actor-critic double deep Q-learning paradigm with clipped action noise and delayed policy updates, consisting of a series of six neural networks used in the training process. The actor network, $h_{\\theta}$, is the policy mechanism that inputs the FIFO queue of recent observations and outputs either $c$ or $s$. The target actor network is used to estimate the next action, while noise is added to that action to smooth out training and account for error. The critic network estimates the Q-value, since future states are unknown. The target critic network is used to measure error in the critic network, since we can not calculate all possible future states. The target critic and target actor networks are initially clones of the critic and actor network, respectively, but the weights vary over time and every few episodes the target network weights are updated using a Polyak weighted average.\nTwo critic networks are used to help stabilize training by selecting the minimum Q-value between both critics, which is referred to as double deep Q-learning [36]. When training a neural network with a TD3 algorithm, initially actions are taken at random to explore the environment. Then the neural network is increasingly used to predict actions as to exploit the policy being learned. Even though six neural networks are used in the training process, only the actor network, $h_{\\theta}$, is executed at the deployment stage."}, {"title": "E. Curriculum Learning", "content": "A common difficulty in training a neural network with DRL arises when the initial task is too difficult. This causes poor rewards early in the learning process which can stagnate training. A solution is to start with an easier task, then progressively increase the difficulty. We implement curriculum learning by starting with a small distance between spawn and goal positions, then incrementally increase this distance as enough evaluation paths successfully reach the goal."}, {"title": "F. Training the Neural Network", "content": "We train the auxiliary network, $h_{\\theta}$, using Algorithm 5. Training the auxiliary network is the main bottleneck for training NaviSlim, because it can take several days before finishing. Further, the parameters to Algorithm 5 are highly sensitive (some of which are omitted), which requires ample time to explore them. Having several computers to run training in parallel with different parameters is highly advantageous. We evaluate if a trained auxiliary model is successful by deploying $g_{\\theta}$ and $h_{\\theta}$ into the AirSim environment and measuring the percent of paths in the test set that successfully reach their goal when using Algorithm 1. Further, we insure the constraint holds in Equation 1 since this is not guaranteed by Equation 4."}, {"title": "VIII. RESULTS", "content": "First we explore the size", "difficulty": 1, "Scen.\" column is the scenario being tested for, and the \"$\\eta$\u201d column shows the relative resource expenditure reduction either for the number of active navigation network parameters, $m$, or sensor power level, $w$. All values shown in Table I are calculated using the test set. We see a significantly higher mean value of $\\rho$ between each of these scenarios. The $\\eta$ values in Table I are percent decreases over the larger super-network. Note that the numbers reported in Table I are for all evaluations done at the end of each curriculum learning step before increasing the distance between spawn and goal in Algorithm 5, and all evaluation paths successfully reached their target position \u2013 thus navigation accuracy is maintained.\nFig. 10 shows the learned average values of $\\rho$ at each position in the Blocks map, using the test set. This figure is useful to see the dynamic nature of NaviSlim and its context-aware behavior based on both the surrounding environment and maneuvers required to avoid collision with an object. We further isolate the context clues versus learned behavior as illustrated in Fig. 8, where we show the mean depth captured by each sensor versus $\\rho$. Generally, when the navigation path is more convoluted there is a correspondingly high $\\rho$, which is expected behavior for more complex navigation and maneuvers.\nInterestingly, as the distance between the drone and objects increases, so does the value predicted for $\\rho$ from $h_{\\theta}$. A possible explanation is that when objects are close, higher-level logic is not needed as the subspace of possible motions, that can be executed without colliding with that nearby object, shrinks. Similarly, after about 50 meters, $\\rho$ begins to decrease with further increasing measured depth, likely because the environment becomes more open (less nearby objects and more open physical space) and the subspace of possible motions shrinks as more sophisticated maneuvers are not needed.\nNext, we measure the actual differences in the resource cost (time, power, and energy) between using and not using NaviSlim on a microprocessor similar to that typically deployed on micro-drones. We use a Jetson Nano with a Quad-core ARM Cortex-A57 MPCore processor and 4 GB 64-bit LPDDR4 1600MHz 25.6 GB/s memory. We compare relative resource costs by passing a static set of observations through": 1, "32": "while the size of the navigation hidden layers was varied as shown in Fig. 11 which shows the relative speedup associated with NaviSlim.\nWe find that the difference in power consumption is negligible, with a mean relative difference of 0.005 and standard deviation of 0.07. Since energy consumption is power multiplied by time, this shows that execution time is the dominating factor in energy consumption. Note that reducing execution time of the navigation network also improves reaction time. Fig. 11 shows that smaller networks can actually result in increased execution time when using NaviSlim as indicated by the lower left corner with black grid spaces. This behavior is expected, since the overhead of the auxiliary network does not justify the small size of the navigation network. However, larger networks result in decreased run times as indicated by the upper right corner with non-black grid spaces. This region is characterized by a positive speedup and also overlaps with the region with lowest navigation error as shown in Fig 6. This proves that we can mitigate the larger execution times inherent with larger neural networks, which is needed to achieve lower navigation error, by using NaviSlim \u2013 noting that this speedup increases with size of the navigation network.\nNext we evaluate NaviSlim-S which dynamically controls $p_f$ and $p_d$, the power levels of the forward facing depth sensor and downward facing depth sensor, respectively. Table I lists the average resolution levels for scenarios (2) and (3) after evaluating the trained auxiliary model, $h_{\\theta}$, with the test set. The mean value of $p_f$ is greater than $p_d$ for each scenario, which is intuitive because most drone maneuvers involve moving horizontally rather than vertically. When using only horizontal motion, the downward facing depth sensor is almost completely turned off, with a mean $p_d$ value of 0.73.\nFig. 12 and Fig. 8 show the learned sensor power levels between the two scenarios as a function of context. From Fig. 8, we see that $p_f$ is independent of the downward depth sensor observations, but has a clear dependence on the forward depth sensor observations \u2013 which is most intuitive. Interestingly, as the values returned from the forward depth sensor increase (indicating forward facing objects are further from the drone) so does $p_f$, which is a similar relationship we earlier observed with $\\rho$ warranting that less resources are required when an object(s) is very near the drone. We observe another intuitive relationship that $p_d$ increases as the values returned from the downward depth sensor increase (indicating the drone is relatively higher in the air than objects below it). This relationship holds until the mean downward depth"}]}