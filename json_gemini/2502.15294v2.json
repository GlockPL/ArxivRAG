{"title": "Round Attention: A Novel Round-Level Attention\nMechanism to Accelerate LLM Inference", "authors": ["Yaohua Tang", "Zhicheng Hu", "Kun Cheng", "Fan Mo", "Qiheng Lv", "Hua Wang", "Zhi Chen"], "abstract": "The increasing context window size in large language models (LLMs) has improved\ntheir ability to handle complex, long-text tasks. However, as the conversation\nrounds continue, it is required to store a large amount of KV cache in GPU memory,\nwhich significantly affects the efficiency and even availability of the model serving\nsystems. This paper analyzes dialogue data from real users and discovers that\nthe LLM inference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. We propose Round Attention, a\nnovel round-level attention mechanism that only recalls and computes the KV\ncache of the most relevant rounds. The experiments show that our method saves\n55% memory usage without compromising model performance.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models have facilitated the wider adoption of language model\nservices for everyday problem-solving tasks. However, prolonged interactions expose two significant\nchallenges. First, the rapid expansion of context length incurs substantial computational overhead\ndue to the quadratic scaling of self-attention mechanisms. Second, although key-value (KV) caching\nalleviates redundant computations, it substantially increases GPU memory requirements, resulting in\nlimited inference batch sizes and GPU under-utilization. For instance, an NVIDIA A100 with 40GB\nof memory can accommodate only a single LLaMA request with a context length of 128K, spending\nnearly 50% of its processing time on KV cache access [He and Zhai, 2024].\nTo enhance inference efficiency, previous research has investigated KV cache eviction and sparse\nattention techniques for LLMs, noting that attention is inherently sparse. These methods either\nstore the entire KV cache in GPU memory, selecting key tokens during autoregression to reduce\ncross-attention computation time [Tang et al., 2024], or maintain the KV cache in CPU memory,\ntransferring it to GPU memory token by token during inference [Chen et al., 2024, Sun et al., 2024,\nHe and Zhai, 2024, Lee et al., 2024]. The former does not reduce GPU memory usage, while the\nlatter incurs significant communication overhead. Furthermore, current methods often require an\nexpensive calculation of the most relevant tokens for each layer.\nThe aforementioned studies analyze contextual relationships at the token level. Analysis in Sun et al.\n[2024] reveals that most post-RoPE keys exhibit high cosine similarity with adjacent tokens, enabling\nchunk-level approximations for selecting important tokens. LONGMEMEVAL benchmark[Wu et al.,\n2024] explores the memory design options for memory-augmented chat assistants and discovers that\nround is \"the best\" granularity for storing and utilizing the interactive history. This aligns with the\ncommunication habits of individuals in real-life interactions, where rounds clearly express closer\ncontextual relationships than tokens.\nTherefore, we analyze the attention matrix under round granularity and identify two interesting\npatterns. First, the attention score distributions at the round granularity in currently prevalent open-\nsource large models exhibit considerable variability in the initial layers; however, from a certain\nlayer onward, the distributions between layers become remarkably similar. Second, within a single\ndialogue round, the attention scores computed for the \"question\" in relation to previous dialogue turns\nclosely resemble those computed for the \u201canswer\u201d corresponding to the same prior turns. With these\ndiscoveries, we propose Round Attention, a method that leverages the sparsity nature of the attention\nmatrix. During inference, it incorporates only the most relevant rounds' key-value (KV) cache into\nthe attention computation while offloading the complete KV cache to CPU memory. Round Attention\nstores and transfers the KV cache at the round granularity, segmenting each round's KV cache into two\ncomplete tensors. Due to the first identified pattern, we only need to compute the top-k rounds once at\na specific layer and then perform a single host-to-device (h2d) operation to transfer the corresponding\nKV cache tensor to GPU memory. This approach contrasts with other methods that require top-k\ncomputations at each layer and transfer the KV cache at the token granularity, significantly reducing\nthe latency overhead associated with top-k calculations and offloading mentioned in other approaches.\nOur primary contributions are as follows:\n\u2022 We dissect the attention patterns in LLM post-deployment at the round granularity and reveal\ntwo enlightening characteristics in attention matrix in real applications.\n\u2022 Based on these characteristics, we design a novel method, Round Attention, associated with\nan array of techniques for long-context dialogues. This approach stores and transfers the\nKV cache at round granularity.\n\u2022 We conduct extensive experiments on the proposed approach. The results show that it can\nreduce the GPU memory footprint by 55% with no accuracy loss. More importantly, thanks\nto the one-time top-k selection and host-to-device (h2d) transfer, our method achieves lower\nlatency compared to standard non-offloaded Flash Attention."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Attention Matrix Analysis", "content": "The sparsity of attention weights in pre-trained LLMs, especially in long-context scenarios, has been\nwell-documented [Liu et al., 2022, Ribar et al., 2023, Liu et al., 2023b, Xiao et al., 2023]. Ma et al."}, {"title": "2.2 KV Cache Eviction Algorithm", "content": "Many previous efforts focuse on KV cache compression to accelerate attention and reduce memory\nusage. H2O [Zhang et al., 2023] retains a limited budget for the important KV cache regarding the\nsum of historical attention scores. FastGen [Ge et al., 2023] further categorizes tokens and only keeps\npartial KV cache using a more sophisticated strategy. TOVA [Oren et al., 2024] simplifies the policy\nby determining the permanently discarded tokens using the current query. StreamingLLM [Xiao\net al., 2023] handles infinitely long text with attention sinks and a finite KV cache. SparQ [Ribar\net al., 2023] computes approximate attention scores by channel pruning and selects important tokens\nthrough them. [Tang et al., 2024] concludes that the importance of a token is highly dependent on the\nquery and proposes Quest, a method that records the min and max key values in KV cache pages and\nestimates the importance of a page using query vectors.\nHowever, these approaches face several challenges. First, it is costly to identify the topk attention. For\nexample, applying a naive search algorithm, e.g. IVF [Douze et al., 2024], requires access over 30%\nkey states to obtain the topk results [Liu et al., 2024], which is quite compute-intensive. Second, these\napproaches save the KV cache in the GPU memory to avoid loading them from the CPU memory,\nwhich does not reduce the total memory consumption of KV cache, hence limiting the max context\nwindow and inference batch size.\nSome papers attempted to offload KV cache to CPU memory to reduce the active GPU memory\nusage. Liu et al. [2024] proposes to build approximate nearest neighbor search (ANNS) indexes\nfor KV vectors in CPU memory and retrieve the most relevant ones through vector search during\ngeneration. Sun et al. [2024] stores the low-rank key cache and offloads the value cache to reduce the\nmemory footprint for larger batch sizes and longer sequences. Chen et al. [2024] stores the LSH hash\ntables and runs the attention computation on the CPU, which significantly reduces the workload of\nattention computation. However, these works transmit the key-value (KV) cache at the token level,\nand in some approaches, the top-k selection is computed on a per-layer basis, which implies that the\nKV cache is also transferred layer by layer, resulting in significant overhead for h2d transfers."}, {"title": "3 Methodology", "content": "This section presents Round Attention, a novel approach that dissects the attention matrix at the round\nlevel for multi-round dialogue tasks by taking < q, a > pairs as the basic analysis unit. The objective\nis to reduce the memory footprint and inference latency without sacrificing the accuracy of LLMs.\nThis section will also discuss a suite of techniques behind Round Attention."}, {"title": "3.1 Attention Distribution", "content": "Given an input sequence X = [X1,X2,..., Xn], a standard Transformer [Vaswani et al., 2023]\nnetwork computes a set of queries Q, keys K, and values V using linear transformations on X. It\nthen computes the self-attention scores as follows:\nAttention(Q, K) = softmax(\\frac{QKT}{\\sqrt{dk}})\nTo investigate the attention pattern among rounds, we denote the sum of the attention scores of the\ntokens in qn, an and the tokens in < qk, ak > of the previous k-th round for layer l as:"}, {"title": "Observation 1: Attention distributions of qn and an are similar.", "content": "As an example, in Figure 2(a), we selected one dialogue comprising 85 rounds to analyze the attention\nprobability distribution of the 85th round in relation to the preceding rounds across different layers.\nAs shown in the figure, the trends of Pq and Pa are highly similar in each layer, indicating that rounds\nhighly correlated with the question of the 85th round are also highly correlated with its answer. Thus,\nafter performing prefill on the question of the 85th round, we can identify the most relevant historical\nrounds' KV caches for AR computation based on the round attention distribution, rather than utilizing\nthe KV caches from all rounds. We would like to emphasize that this pattern is not only applicable\nto this particular example. We have derived such pattern after analyzing a substantial number of\ndialogues, and we are using this example as a subject for analysis."}, {"title": "Observation 2: Attention distributions among layers are similar.", "content": "Next, we analyze the correlation of P across different layers. For a given layer, we compute the\nKullback-Leibler (KL) divergence between that layer and each subsequent layer, averaging these"}, {"title": "3.2 Round Attention Inference Pipeline", "content": "Figure 1 depicts the pipeline for Round Attention. First, we design a strategy to determine the\nwatershed layer for a given LLM. In real multi-turn dialogue LLM serving systems, it is impractical\nto store all historical KV caches from all users in the GPU memory. A user's historical KV cache\nwill normally be swapped out to the host memory or even slower storage devices when she is inactive\nfor some period, so that the precious GPU memory can be well-utilized. For simplicity, we assume\nthat the LLM has L layers. bm denotes the KV cache of 1 ~ Lw layers for the m dialogue round, Um\ndenotes the KV cache of Lw ~ L layers for the m dialogue round. bm and um are stored as separate\ntensor in memory.\nWhen the user becomes active, e.g. asking LLM the n-th question qn, the following steps will be\nexecuted to conduct the inference for this turn.\n\u2022 step1: Load b\u2081... bn-1 to the GPU memory from the host memory.\n\u2022 step2: Performs prefill computation for qn using layer 1 ~ Lw.\n\u2022 step3: Select the most relevant TopK dialogue rounds via the strategies proposed in Sec-\ntion 3.3 with qAttentionLw, and load the KV cache for layer Lw+1 ~ L, Utopk.\n\u2022 step4: Finish prefill for the remaining layers.\n\u2022 step5: Decode an.\nCompared to the previous works that work on the token level, therefore invoking multiple fragmented\nKV cache transfers between host and device memory, our method works at the dialogue round level\nwhere a monolithic tensor for all tokens in prior rounds is transferred to GPU at once. Upon the\naccomplishment of the computation of un for layer Lw+1 ~ L in the n-th round, the new KV cache\nis saved to the host memory as a monolithic tensor as well. Therefore, our methods reduces the\nnumber of expensive host-to-device(H2D) and device-to-host(D2H) data transferring. In addition,\nmoving data in a large chunk is able to better utilize PCIe bandwidth. The algorithm is summarized\nin Algorithm 1."}, {"title": "3.3 Round Strategy", "content": "Three strategies are considered to determine the top k most relevant dialogue rounds after Lw is\ndiscovered and qAttention is computed.\nStrategy 1: Fixed rounds selects the satisfied rounds using a predefined threshold, e.g.\nqAttentionLw > v. Analyzing the distribution of attention scores across rounds we find that the\nattention values are concentrated in a limited number of rounds, with the majority of rounds exhibiting\nminimal attention scores. Based on this data distribution, we selected v = 0.1.\nStrategy 2: TopK rounds picks that rounds that correspond to the top 10% qAttentionLw. Analyzing\nthe distribution of round attention scores reveals that the top 10% of rounds account for over 80% of\nthe cumulative attention.\nStrategy 3: Adaptive rounds chooses the rounds adaptively with the qAttention distribution. The\ncondition is defined as: qAttentionLwk > mean + k * std, where mean and std are the mean and\nstandard deviation of qAttentionk."}, {"title": "3.4 KV cache Dropping", "content": "We observed that the KV caches of some dialogue rounds in the ShareGPT are never active and do not\naffect the inference quality even if removed for attention computation. For these rounds, we delete\nthe KV cache in the corresponding tokens to avoid saving them in memory. Further, it is unnecessary\nto load such KV cache from the host memory when they become active, hence saving storage and\nmemory copy costs."}, {"title": "3.5 Memory and Performance Analysis", "content": "This section investigates the memory and performance of Round Attention.\nMemory footprint analysis. Given an LLM with context length S, hidden size H, total layers L,\nand inference batch size B, the amount of memory consumed by the KV cache is calculated by\nEquation 6.\nMorig=2*2*B*S*H*L\nwhere the first 2 represents K and V, and the second 2 means that float16 occupies 2 bytes.\nFor Round Attention, assuming that the K most relevant rounds are chosen from the total T rounds\nof dialogue, the amount of memory used by each round on average is shown in Equation 7.\nMround = 4B * S * H * Lw + 4B * K/T * S * H * (L-Lw)\nThis is because layer 1 ~ Lw uses the entire KV cache, and the subsequent layers (Lw+1 ~ L) only\ncompute the attention with the most relevant K rounds.\nThe memory saving ratio of Round Attention can be expressed as Equation 8.\n\\frac{M_{round}}{M_{orig}} = \\frac{L_w + \\frac{K \\cdot (L - L_w)}{T}}{L} = \\frac{L_w}{L} + \\frac{K}{L} (1 - \\frac{L_w}{L})\nSince K is much smaller than T in practice, e.g. 6 ~ 8 vs tens to hundreds, the upper bound of\nEquation 8 approximates to \\frac{L_w}{L}. When K equals T, that is, all dialogue rounds are selected, Round\nAttention degrades to the original inference with virtually no memory cost. As shown in Table B,\nfor currently mainstream large models, the ratio \\frac{L_w}{L} ranges from 0.18 to 0.46, indicating a memory\nsaving percentage of 54% to 82%, which is quite substantial.\nLatency analysis. Latency is of LLM inference is composed by prefill and decode. Prefill is compute\nbound and the total FLOPs can be approximate to 2 * N * D, where N is the number of model\nparameters and D is the number of computed tokens. It can be measured by TTFT(time to first\ntoken), which is usually computed by Eq. (9) [Korthikanti et al., 2022]. As autoregressive decoding\nis memory bound, the generation time of S tokens is close to the time it takes to load the entire KV\ncache (shown in Equation 6) from the GPU memory. Assuming that the GPU memory bandwidth is\nBh, TPOT (time per output token) of the original inference pipeline is computed as Equation 10.\nTttft = 24B * S * L * H\u00b2 * (1 + S/6H) \u2248 24B * S * L * H2\nTtpot = 4B * S * H *L/Bh\nTherefore, the latency of the original LLM with S output tokens is computed as Lorig = Tttft+Ttpot\u00b7\nRecall that Round Attention first loads the tokens of the top k most relevant rounds of layers 1 ~ Lw\nfrom the host memory (bandwidth B\u2081), and then performs attention using these tokens. The latency is\ncontributed by four major components: prefill of the first Lw layers, prefill of remaining layers using\nTopK relevant rounds, transferring of the TopK relevant KV cache to device memory, and AR of the\nremaining layers. Since data transferring can be asynchronous and hidden by prefill computation, we\nonly consider the other three components as Equation 11.\nL_{round} = \\frac{24BSL_wH^2}{C} + \\frac{24BL_wKS/TH^2}{C} + \\frac{4BHS/T H(L - L_w)}{B_h}\nwhere C and Bh are FP16 TFLOPs and memory bandwidth of a GPU, respectively.\nThe latency saving is \\frac{L_{orig}}{Lr}, denoted as \\frac{L_{orig}}{L_r} in Equation 12 for simplicity.\n\\frac{L_{orig}}{L_r} = \\frac{1 + \\frac{L_w}{6H}} {\\frac{L_w}{L} + \\frac{L_w K}{T* \\frac{C}{B_h}} + \\frac{S \\frac{K}{T} + B_h \\frac{L-L_w}{L}}\n\u2248 (1 + \\frac{L_w}{6H}) {\\frac{L_w}{L} + \\frac{KC}{6TH B_h} \\frac{L-L_w}{L}}\n\u2248 (1 + \\frac{L_w}{6H})\\frac{L}{KC}\\frac{KC}{6TH B_h}}\nThe following conclusions can be safely drawn.\n\u2022 The longer the context length ($\\frac{L_w}{L}$ is small), the more latency can be saved. For example,\nwhen $\\frac{L_w}{L}$ is negligible, the latency of Round Attention approximately equals to $\\frac{L_w}{L}$ of the\noriginal inference pipeline.\n\u2022 The earlier the watershed layer Lw is present ($\\frac{L_w}{L}$ is small), the shorter the latency would be\nachieved."}, {"title": "4 Experiments And Analysis", "content": ""}, {"title": "4.1 Experiment Setting", "content": "This section evaluates the model accuracy, memory footprint, and inference performance of Round\nAttention.\nData. Two widely used datasets, ShareGPT[ShareGPT52K, 2024] and LONGMEMEVAL[Wu\net al., 2024], are used to evaluate the effectiveness of Round Attention. ShareGPT contains a\ncollection of approximately 52K user-shared conversations scraped through the ShareGPT API.\nThese conversations are multi-turn, including both user prompts and responses from ChatGPT.\nLONGMEMEVAL is a comprehensive benchmark designed to evaluate five core long-term memory\ncapabilities of commercial chat assistants: information extraction, multi-session reasoning, temporal\nreasoning, knowledge updates, and abstention. This benchmark also records the historical user-\nassistant conversations with 250 rounds on average. This is a difficult dataset, on which GPT-40's\naccuracy is only 0.5773.\nBaselines. A suite of the latest open-source LLMs, e.g. Qwen2.5, LLaMA3, and LLaMA3.2\n[Grattafiori et al., 2024], are tested on the above datasets, but our approach can be applied to any other\nlong-context LLMs. We use PyTorch and FlashAttention [Dao et al., 2022] as the default inference\nframework, which are refered as Flash: All testing are conducted on a single Nvidia A100 GPU with\n80GB of memory, equipped with PCIe. The CPU used was an Intel(R) Xeon(R) Gold 6346 CPU\noperating at 3.10GHz (1.16/3.60GHz), and the system had 1TB of memory."}, {"title": "4.2 Accuracy Evaluation", "content": "We classify ShareGPT into four categories with respect to dialogue rounds, mini (0-10 rounds),\nsmall (10-30 rounds), medium (30-50 rounds), and large (50-100 rounds). We treat the last"}, {"title": "4.3 GPU Memory Reduction and Latency Reduction", "content": "To empirically evaluate the latency of Round Attention compared to Flash Attention, we selected 20\ndialogue samples from each of the four categories mentioned earlier. Each sample was run 10 times,\nand the average latency was computed and plotted in Figure 3.\nIt can be observed that for all different round categories, the latency of Round Attention is lower\nthan that of Flash Attention. This improvement is due to our KV cache storage and transfer strategy,\nwhich keeps the h2d transfer time manageable, and our top-k selection is computed only once rather\nthan at each layer. We provide a detailed breakdown of latency in the Appendix C. Due to the h2d\ntransfer and the selection of top-k, the latency during the qn prefill phase exhibits a slight peak at\nlayer Lw; however, this peak is minor and occurs only once. In contrast, during the an decode phase,\nthe reduction in the KV cache leads to decreased attention computation time. As the number of\ndecode steps increases, this reduction accumulates and ultimately surpasses the one-time overhead\nfrom the h2d transfer and top-k selection, resulting in an overall latency that is superior to that of\nFlash Attention."}, {"title": "4.4 Round vs Token", "content": "In this section, we compare the two granularities. For both granularities, we employed the same top-k\ncalculation strategy to retrieve the KV cache. Specifically, we first computed the average attention\nscore for all tokens/rounds at each granularity, selecting those with attention scores greater than the\naverage. The model used for testing was Llama3.1-8B. The results are presented in Table 4. It is\nevident that, with the same top-k calculation strategy, the recall accuracy at the round granularity\nsurpasses that of the token granularity. This is particularly pronounced in the Single-session-assistant\ntask, where the answers reside within several sessions. The recall at the round granularity effectively\nretrieves the most relevant sessions, whereas the recall at the token granularity is dispersed across\nmultiple sessions, resulting in a significantly lower accuracy compared to the round granularity."}, {"title": "5 CONCLUSION AND DISCUSSION", "content": "In the context of real-world applications providing services with large language models (LLMs),\nthe historical key-value (KV) cache accumulates as users engage in increasingly lengthy dialogue\nexchanges. We propose that, during inference with such extended dialogue rounds, employing\na round-based approach offers a more effective means of managing the KV cache and handling\ninteractions with historical information. Through an analysis of the attention matrix patterns at the\nround granularity, we observed that contemporary large models exhibit a watershed layer, beyond\nwhich the distribution of round-based attention becomes remarkably similar. This observation allows\nus to compute the most relevant rounds for subsequent layers' attention calculations just once at the\nwatershed layer. Consequently, we can significantly reduce GPU memory usage while effectively\nlimiting the time required for selection. By storing the KV cache based on rounds, we can transfer\nall necessary KV cache data to GPU memory in a single host-to-device (h2d) operation, thereby\nminimizing the time overhead associated with h2d transfers. We validated the effectiveness of our\napproach through experiments, demonstrating that it is able to significantly reduce inference latency,\nwith inference accuracy remaining largely consistent with that of the full KV cache."}, {"title": "Limitations", "content": "This section discusses some limitations this paper has that we intentionally leave as the future work\nto further improve.\nLimitation 1: Offloading to memory incurs additional memory overhead. Although memory is\nsignificantly cheaper and larger than GPU memory, it still adds extra overhead to the system.\nLimitation 2: While Round Attention reduces GPU memory usage, out-of-memory (OOM) issues\nmay still arise when the number of dialogue rounds reaches a certain threshold. This indicates\nthat Round Attention alone cannot fundamentally resolve the GPU memory issues associated with\nvery long dialogues. It needs to be combined with other techniques to effectively address memory\nproblems, such as the continuous dropping of infrequently used key-value caches mentioned in\nSection 3.4. Additionally, various other key-value cache compression and dropping strategies can be\nutilized in combination to tackle GPU memory issues in practical dialogue systems.\nLimitation 3: The benefits of serving are limited for scenarios with shorter dialogue rounds, making\nit more suitable for longer user dialogue interactions."}, {"title": "C Latency decomposition analysis", "content": "In the transformer architecture, the forward computation of attention is divided into four steps:\ncalc_qkv_and_rope, update_cache, attn_forward, and attn_output. Our algorithm primarily modifies\nthe update_cache and attn_forward steps. We analyze the execution times of step 2, step 3, step 4,\nand step 5 from Figure 1. Steps 2, 3, and 4 correspond to the prefill phase of qn, which we refer to\nas the append phase, following the methodology outlined in Flash Infer [Ye et al., 2024]. Step 5\nrepresents the decode phase for an. We selected five examples from 50 to 100 rounds and conducted\n100 experiments, plotting the trend of the average time for these two phases as a function of layer,\nresulting in Figure 4.\nFrom the figures, it can be observed that the execution times for round attention for the\ncalc_qkv_and_rope and attn_output steps closely align with those of Flash Attention. The pri-\nmary differences arise in the update_cache and attn_forward steps. Notably, the trends for these two\nsteps remain consistent until layer Lw, where a divergence from Flash Attention emerges starting at\nlayer 11.\nIn the append phase, as shown in Figure 4(b)(b), there is a noticeable peak in update_cache at layer\n11, indicating two sources of overhead: one related to the computational cost of the top-k selection\nstrategy, and the other pertaining to the h2d transfer time of the selected rounds' KV cache to the\nGPU memory. Similarly, Figure 4(b)(b) reveals a peak in the attn_forward step at layer 11, which\nalso corresponds to the computational cost of the top-k strategy. It is evident that both the time taken\nfor top-k computation and the h2d transfer time are relatively small."}, {"title": "DGPU Memory decomposition analysis", "content": "In the experiments presented in Section C, we synchronously monitored the variations in GPU memory\nusage. The GPU memory consumption was measured using the 'torch.cuda.allocated_memory'\nfunction.\nAs illustrated in Figure 5, Flash Attention exhibits a slight increase in memory usage during the\nappend and decode phases, although the magnitude is minimal. Conversely, for Round Attention, there\nis a noticeable increase in GPU memory usage after the 11th layer during the append phase, attributed\nto the selection and host-to-device (h2d) transfer processes within the algorithm. Additionally, both\nthe append and decode phases show a consistent overhead of several megabytes after the 11th layer,\nwhich is allocated for storing intermediate results of the selection process.\nIt is also evident that Round Attention exhibits lower GPU memory usage during both the append\nand decode phases compared to Flash Attention."}, {"title": "A GPT-4 judged prompt", "content": "You are an assistant skilled in evaluating text quality.\nPlease assess the quality of an AI assistant's response to a\nuser's question as an impartial judge. You need to evaluate\nthe response based on the following dimensions:\nWe will provide historical chat information, which consists\nof the content from previous multi-turn conversations between\nthe user and the assistant. We will give you the current\nuser's question and the AI assistant's response. When\nyou begin your evaluation, you need to follow the process\noutlined below:\n1. Evaluate the AI assistant's response from different\ndimensions, and after assessing each dimension, assign a\nscore from 1 to 10 for each dimension.\n2. Finally, based on the evaluations from each dimension,\nprovide an overall score from 1 to 10 for the AI assistant's\nresponse.\n3. Your scoring needs to be as strict as possible, and you\nmust adhere to the following scoring rules: Generally, the\nhigher the quality of the model's response, the higher the\nscore. Among the dimensions, factual accuracy and meeting\nuser needs are the most important, and the scores for these\ntwo dimensions will dominate the final overall score.\nWhen the model's response contains irrelevant information,\nhas fundamental factual errors, or generates harmful content,\nthe total score must be between 1 and 2.\nWhen the model's response has no serious errors and is\ngenerally harmless, but is of low quality and does not meet\nuser needs, the total score should be between 3 and 4.\nWhen the model's response generally meets user requirements\nbut performs poorly in some dimensions, resulting in an\naverage quality, the total score can be between 5 and 6.\nWhen the model's response quality performs well across all\ndimensions, the total score should be between 7 and 8.\nOnly when the model's response quality fully addresses the\nuser's questions and all needs, and performs nearly perfectly\nacross all dimensions, can it receive a score of 9 to 10.\nAs an example, a reference answer can receive a score of 8.\nReturn all your evaluations and scoring results in the\nfollowing dictionary format (including parentheses), and\nensure that your scores are integers:\n{{'dimension 1': score, 'dimension 2': score, 'overall\nscore': score}}, for example:{{\u2019factual accuracy': 9,\n'meeting user needs': 6, 'overall score': 7}}.\nHistorical chat information: {review}\nUser's question: {instruction}\nAssistant's response: {response}"}, {"title": "B Layer-W", "content": "Here, we present the values of Lw obtained from several mainstream open-source models in Table 5."}]}