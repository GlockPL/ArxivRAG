{"title": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling", "authors": ["Xudong Xie", "Liang Yin", "Hao Yan", "Yang Liu", "Jing Ding", "Minghui Liao", "Yuliang Liu", "Wei Chen", "Xiang Bai"], "abstract": "Document understanding is a challenging task to process and comprehend large amounts of textual and visual information. Recent advances in Large Language Models (LLMs) have significantly improved the performance of this task. However, existing methods typically focus on either plain text or a limited number of document images, struggling to handle long PDF documents with interleaved text and images, especially in academic papers. In this paper, we introduce PDF-WuKong, a multimodal large language model (MLLM) which is designed to enhance multimodal question-answering (QA) for long PDF documents. PDF-WuKong incorporates a sparse sampler that operates on both text and image representations, significantly improving the efficiency and capability of the MLLM. The sparse sampler is integrated with the MLLM's image encoder and selects the paragraphs or diagrams most pertinent to user queries for processing by the language model. To effectively train and evaluate our model, we construct PaperPDF, a dataset consisting of a broad collection of academic papers sourced from arXiv, multiple strategies are proposed to generate automatically 1M QA pairs along with their corresponding evidence sources. Experimental results demonstrate the superiority and high efficiency of our approach over other models on the task of long multimodal PDF understanding, surpassing proprietary products by an average of 8.6% on F1. Our code and dataset will be released at https://github.com/yh-hust/PDF-Wukong.", "sections": [{"title": "1. Introduction", "content": "The advent of Large Language Models (LLMs) has significantly advanced the field of PDF document understanding [1, 2], where these models have demonstrated impressive capabilities in processing and generating human-like text. However, they still face many challenges when it comes to lengthy PDF documents with interlaced text and images, such as academic papers.\nTo handle lengthy documents, current large models mainly fall their input into two separate modalities. One is the plain text modality. As shown in Fig. 1 (a), they focus only on the parsed pure text content and lack multimodal understanding of visual elements such as charts and figures within the documents. They usually rely on long-context LLMs [3\u20135], whose efficiency and accuracy decrease as the length of the document increases. Retrieval Augmented Generation (RAG) [6\u20139] is also the main auxiliary means of these LLMs, but it requires the introduction of additional models and lacks the exploration of multimodal RAG.\nAnother approach is to input documents in a purely visual modality, treating each page as an image. However, most current Visual Document Understanding (VDU) models models [10\u201312] only focus on visual QA on single-page documents. They usually rely on larger input resolution, resulting in more tokens, and thus have difficulty processing multi-page long documents. Recently, some multi-page VDU models can understand 8-page [13] or 20-page [14] documents. They usually encode each page separately and then perform page-level visual feature interactions such as concatenation [15, 16], as shown in Fig. 1 (b). However, more pages generate more visual tokens, which brings greater resource consumption to LLM. This makes them inefficient and unable to process longer documents.\nConsidering the limitations of existing methods in multimodal understanding of long PDF documents, we propose a new MLLM architecture with end-to-end sparse sampling, named PDF-WuKong. Since most user queries are only related to a small part of the content in a long document, the sparse sampling can significantly remove redundant noise information. It encodes text paragraphs and diagrams in the parsed PDF document, utilizing both text and image representations to identify and extract the most relevant evidence in response to a user's query. The sampled sparse evidence significantly reduces the number of input tokens of LLM and this process is independent of the length of the input documents. Moreover, the sparse sampler and LLM can be integrated in an end-to-end manner for training and inference, optimizing the performance of multimodal representation and question answering while improving time efficiency. It is worth noting that this sparse sampler is a plug-and-play design that can be applied to any MLLMs. Another important characteristic is that it can naturally provide strong interpretability for the question answering.\nIn order to simultaneously represent and understand the multimodal content of documents and further improve the ability to process long PDF documents, we construct a training dataset specifically for academic paper PDFs. The academic paper PDF is a kind of typical document that contains rich interleaved text and images, which can intuitively reflect the challenges of our task and the advantages of our model. The dataset contains complete PDF documents, professional academic questions, answers, and evidence sources for the answers, based on multiple construction strategies. We also provide a corresponding benchmark named PaperPDF.\nThe experimental results substantiate the effectiveness and efficiency of our approach on the task of long multimodal PDF understanding. PDF-WuKong significantly outperforms potential open-source models that may be applied to this task. It also surpasses some proprietary products for document understanding on our proposed PaperPDF benchmark. As the number of document pages increases, its accuracy and efficiency will not decrease significantly. It also achieves competitive performance on several document-oriented VQA datasets, especially multi-page benchmarks like DUDE [17]. Besides, for the recent benchmark MM-NIAH [18] of long multimodal documents, PDF-WuKong also outperforms other models with fewer parameters. Our model achieves the best performance on multimodal content with a context length of 64K.\nThe main contributions of this paper are as follows:\n\u2022 We introduce a large multimodal model for long PDF understanding with end-to-end sparse sampling, achieving accurate and efficient PDF question answering.\n\u2022 We propose a PDF multimodal question answering dataset (PaperPDF) with 1M QA pairs for training and 6k QA pairs for evaluation.\n\u2022 Our model significantly surpasses existing open-source models and proprietary products (by an average of 8.6% on F1) on long multimodal PDF understanding."}, {"title": "2. Related Works", "content": "2.1. Document Understanding Datasets\nEarlier document understanding datasets only focused on the NLP tasks such as summarization [27], and QA [28] of plain text. Meanwhile, there were several visual document datasets mainly aimed at text perception tasks such as Document Layout Analysis (DLA) [29\u201331] and Key Information Extraction (KIE) [32\u201334]. Recently, more datasets have been proposed for multimodal document QA across various scenarios. For example, DocVQA [35] and OCRVQA [36] provide single-page QA data on books and business documents. ChartQA [37] and ChartX [38] focus on the visual reasoning for chart documents. ArXivQA [39] extracts scientific figure-caption data from the arxiv papers to enhance the academic ability of MLLMs. InfoVQA [40] contains many infographic documents which are a combination of textual, graphical and visual elements. However, these visual document datasets only define the single-page VQA task, and current MLLMs [11, 41] have achieved remarkable performance on it.\nThere are also some multi-page QA datasets [14, 15, 17] that require the model to understand the content relationship via multi-hop reasoning and capture the crucial information from multi-page documents. MP-DocVQA [14] extends DocVQA [35] by adding the context pages. DUDE [17] constructs multi-page QA data from multi-industry and multi-domain documents. Recently, DocGenome [42] was constructed as a scientific document benchmark. MM-NIAH [18] is a benchmark evaluating the capability of MLLMs to comprehend long multimodal documents, which requires the model to answer according to the key information scattered throughout the document. However, the answers in these datasets lack evidence and cannot provide reliable interpretability, especially for questions that require referring to multiple pieces of evidence in long documents."}, {"title": "2.2. Document Understanding Methods", "content": "Existing document understanding methods typically focus on either plain text or a limited number of document images. Methods that rely on pure text modality aim to process documents by first converting them into plain text through document parsing or Optical Character Recognition (OCR). They then employ efficient long-context mechanisms to handle long texts, such as sparse attention [4], memory networks [5], or position interpolation [3]. Besides, the methods based on retrieval-augmented generation (RAG) [6, 9, 19] also show impressive capabilities for long texts. While these approaches can integrate visual elements by including image captions or transforming images into natural language descriptions, they struggle with fine-grained understanding of visual information. This restricts their effectiveness in tasks requiring detailed interpretation of textual and visual components within documents.\nAnother solution is visual document understanding with purely visual input, treating each document page as an image. Many MLLMs such as UniDoc [20], mPLUG-DocOwl [21] and Vary [12] can perform this task in an OCR-free manner. Vary [12] employs an extra SAM-style [43] vision vocabulary specific to document and chart data, enabling direct encoding of entire pages with high compression ratios. Other researchers have advanced the understanding of high-resolution document pages by dividing input images into smaller patches, such as UReader [22], TextMonkey [10], and LLaVA-NeXT [23]. DocPedia [24] processes high-resolution images in the frequency domain via the DCT transformation. InternLM-XComposer2-4KHD [25] and InternVL-V1.5 [11] introduce a dynamic resolution mechanism with automatic patch configuration to capture more details. The reliance on high resolution results in a higher number of tokens and cannot be extended to multi-page documents.\nBesides, there are several models specifically designed for multi-page documents. Hi-VT5 [14] and GRAM [16] are two professional models for multi-page QA. They combine image tokens of multiple pages through hierarchical transformer architecture [14] or global-local reasoning [16], being able to handle up to 20 pages. Fox [13] unifies all image tokens of up to 8 pages into a sequence to achieve multi-page QA. mPLUG-DocOwl2 [26] compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. These models encode each page separately and then perform page-level visual feature interactions. However, more pages generate more visual tokens, which brings greater resource consumption to LLM. This makes them inefficient and unable to process longer documents. Given this, we propose first to parse the lengthy document into interleaved text and image content and then perform sparse sampling in an end-to-end manner."}, {"title": "3. Methodology", "content": "3.1. Overview\nIn order to achieve multimodal understanding of long PDF documents and alleviate the drawbacks of existing models that treat PDF documents as a single modality of plain text or images, we propose PDF-WuKong. The core motivation is that users' queries are often only related to a small number of text blocks or diagrams in a long document. Therefore, to improve the accuracy and efficiency of MLLM, we design a sparse sampler integrated with a multimodal large language model in an end-to-end manner.\nSpecifically, our pipeline consists of three parts: a document parser, a sparse sampler and a large language model, as shown in Fig. 2. The document parsing stage first converts the input PDF document into machine-readable content of interleaved text and images, Then, the sparse sampler encodes the text blocks and images separately and caches their embeddings. When a user inputs a query, the most relevant content can be sampled using a simple similarity measure. Finally, the query and the sampled tokens are input into the LLM to generate the answer."}, {"title": "3.2. Document Parsing", "content": "Given a PDF document D, the goal of document parsing is to convert it into some machine-readable text blocks {T1, T2,..., Tn} and diagrams {I1, I2, ..., Im} according to the reading order and layout structure. By default, text blocks are organized into paragraphs, and all figures and tables are saved as images. These text and images are finally reorganized into an XML file in reading order. This process can be completed using existing open-source PDF parsing tools. During inference, we directly input the parsed structured full data into the subsequent stage of PDF-WuKong."}, {"title": "3.3. Sparse Sampling", "content": "For a lengthy multi-page document, if it is directly input into the LLM, there will be two problems. The first is the problem of computing efficiency. The consumption of computing resources will increase dramatically. The second is the problem of inaccurate attention. Key information related to the user query is easily submerged by a large amount of irrelevant content. It is difficult for the model to accurately locate and extract important information in a huge token sequence. Therefore, sparse sampling is essential for efficiently handling lengthy multi-page documents by identifying and extracting the most relevant text chunks or diagrams based on their similarity to the user query.\nFor the parsed n text chunks {T1, T2, . . ., Tn }, m images {I1, I2, ..., Im}, and the input user query q, we first extract the positive samples and the negative samples for the query. Our PaperPDF dataset has provided corresponding positive single-evidence or multi-evidence samples for each query-answer pair (detailed in Sec. 4). We randomly select two text blocks and two images from the remaining text blocks and images as negative samples. Then, we use a text encoder En_T to obtain the text embeddings etp,et and the query embedding eq. An image encoder En-I is utilized to output the image features elp, eIn, which is shared with MLLM.\nGiven the embeddings of the user query eq, the positive samples Ep = {eTp,eIp} and negative samples EN = {CTN, EIN}, we employ a contrastive learning approach to align the text and image features with the query. The goal is to enable the model to capture the document content that is most relevant to the query. The contrastive learning loss is:\nLrep = \n1\nei \u2208EP\nesim(eq,ei)/\u03c4\nlog\nesim(eq,ei)/\u03c4 + \u2211ej \u2208ENesim(eq,ej)/\u03c4\n                                                        (1)\nwhere sim(eq, ei) and sim(eq, ej) represent the similarity between the query and the positive/negative samples, respectively. \u03c4 is the temperature parameter that controls the scale of the similarity scores. P is the number of positive samples. By maximizing this probability, the model encour-ages the representations of the query and positive samples to be closer while pushing the representations of the query and negative samples apart. We align the feature space of the text encoder to the pre-trained vision encoder with frozen parameters. It is worth noting that this sparse sampler is a plug-and-play design that can be applied to any MLLMs.\nDuring the inference, we pre-encode all text blocks and images and cache all candidate vector embeddings. When the user inputs a query, we calculate the similarity between query embedding and cached text/image embeddings. Then the model automatically selects the top-k relevant text blocks and images as evidence to respond to this query. Therefore, this process samples out sparse document content, greatly reducing the computational burden of the subsequent LLM and alleviating the problem of attention shift when facing ultra-long sequences. Moreover, the multimodal embedding cache further optimizes inference time."}, {"title": "3.4. Answer Generation", "content": "At this stage, the large language model only receives the document content that is most relevant to the query and discards a lot of redundant information, so it can generate more accurate answers with higher efficiency. Specifically, we input the sampled top-k evidence, the user query, and the task instruction into the LLM, and let it generate an answer based on the provided query and evidence. The inference process is shown in Algorithm 1.\nConsidering that MLLM needs to encode images first for multimodal understanding, we directly input the image tokens obtained from the sparse sampler into the LLM, to save one image encoding process. Thus, the sparse sampler shares the same vision encoder with the MLLM. They can be integrated and trained in an end-to-end manner.\nDuring the training, we input the positive text Tp and the positive image tokens erp into the LLM. Besides, the query and instruction are also input into the LLM. Then, we calculate the cross-entropy loss LQA between the output answer a and the ground truth. Finally, the total optimization objective is:\nLtotal = Lrep + LQA.                                                               (2)\nPDF-WuKong is optimized end-to-end by these two loss functions for effective multimodal alignment and question-answering."}, {"title": "4. PaperPDF Construction", "content": "4.1. Overview\nThe reasons for constructing our dataset include the following two points. In most long document Q&A scenarios, the basis for an answer to a given question is typically composed of a single element or a combination of multiple elements, while the remaining information may act as noise, interfering with the reasoning of MLLMs. Additionally, existing document question-answering datasets are either limited to single-page formats or provide only page-level ground truth, which hinders the training of the sparse sampler in PDF-WuKong. Consequently, we propose a reliable method for generating high-quality question-answer pairs of long document and develop the dataset PaperPDF for training and evaluation based on this method."}, {"title": "4.2. Detail of Construction", "content": "The construction process of PaperPDF are presented in Fig. 3. It can be divided into four steps: structured parsing, rule-based extraction, prompt construction, and filtering. We obtained 89k PDF documents from the arXiv repository as our document set D. For each document D in D, we first employed Grobid to parse the document and extract its text chunks {T1, T2, ..., Tm} and images chunks {I1, I2,..., In}. Subsequently, predefined rules are used to randomly select chunks from the document, which may consist of text chunks Ti, chart chunks I; or a combination of both. The selected chunks will be input into commercial MLLM products according to different prompt templates and then, the question Q related to the input chunks, along with the corresponding answer A, is subsequently generated. Notably, For the training set, we used Gemini for generation due to its free and rapid accessibility, while the high-performance GPT-4V was employed to construct the test set, ensuring the validity and robustness of the evaluation. Finally, we devise a set of rules to filter the generated training and testing sets automatically. The removing rules for the samples include too-short questions, too-long answers, non-English text, etc. Further manual checking is conducted on test set to ensure the reliability of the evaluation. PaperPDF consists of two types of QA pairs: Single-evidence and Multi-evidence.\nSingle-evidence QA pair means that the question can be answered based on a single text chunk or image chunk in the long document. It can be systematically categorized into two types: text-only and image-only. In text-only generation, a text chunk is input into MLLM, whereas in image-only generation, an image chunk is utilized. The generation process for Single-evidence QA pairs is relatively straightforward and cost-effective, primarily aimed at training the foundational multimodal understanding and question-answering capabilities of MLLMs.\nMulti-evidence QA pairs. Unlike single-evidence QA pairs, Multi-evidence QA data exhibits greater complexity, both in terms of characteristics and generation process. The answers to such questions typically rely on multiple text chunks, image chunks, or any combination thereof in document. Multi-evidence QA pairs consist of Image-text QA pairs, Section QA pairs, and Cross-paragraph QA pairs. The Image-text QA pairs are generated based on a paragraph and its corresponding image references. These pairs require consideration of the interrelationship between the text and the image to ensure the answers capture multimodal information. Section QA pairs are designed to train the MLLM in integrating and understanding information within a section and are generated from all chunks within a section. Cross-paragraph QA pairs involve the most complex generation process. First, the entire document is input into the MLLM for paragraph-level semantic summarization, followed by the selection of semantically related text chunks. Finally, multiple related chunks are randomly selected and re-input into the MLLM to generate the final QA pairs. This type of data primarily focuses on training the sampler's performance, enhancing its sparse sampling capability. Although the generation of Multi-evidence QA pairs is relatively complex and may require multiple MLLM calls, their presence significantly improves the performance of the sampler and strengthens the MLLM's ability to comprehend cross-chunk information.\nTotally, we generated 1.5M samples for training and 8K samples for testing. After filtering, the final dataset consists of 1M training data points and 6K test data points for subsequent training and evaluation."}, {"title": "5. Experiments", "content": "5.1. Implementation Details\nWe adopt XComposer2-4KHD [25] as our baseline model, initializing it with its pre-trained weights. We use BGE-M3 [50] as the text encoder. We fine-tune the model using the datasets PaperPDF, DocVQA [35], ChartQA [37], InfoVQA [40], MPDocVQA [44], and DUDE [17], with a learning rate of 4e-5. Prior to both training and testing, all datasets underwent document parsing; specifically, PaperPDF was parsed using Grobid [51], while the other datasets were processed with MinerU [52]. For the sparse sampler, we selected the top 5 sampling results by default to serve as input to the large language model. The training was conducted for one epoch across these datasets using 128 Ascend 910B GPUs. For convenient description, we denote the 4 different input formats of models with 4 symbols.\n5.2. Long PDF Understanding\nTo assess the effectiveness of our model in understanding long PDF documents, we conducted comprehensive experiments comparing it with both open-source models and commercial products on PaperPDF dataset.\nDue to the limited availability of open-source models capable of handling this task, we firsly evaluate two such models that can process multi-page PDF documents. Considering that IXC2-VL [45] cannot understand such long image-text interleaved sequences, we also report the results with the input of pure OCR content. To further demonstrate the advantages of our multi-modal sampler, we demonstrate the evaluation of IXC2-VL with RAG where we feed the top 5 text paragraphs retrived by BGE-M3 [50]. The results on the PaperPDF dataset are reported in Tab. 3. The findings indicate that our model significantly outperforms existing open-source models across various evaluation metrics. Additionally, thanks to the introduction of the sparse sampler, the number of tokens that our model's MLLM needs to process is substantially fewer than that of other models.\nMoreover, we compared our model with several commercial products that allow users to input PDF documents and questions via web interfaces to generate answers. Considering the cost of manual testing, we randomly selected 50 PDFs from the PaperPDF dataset for evaluation, and the results are presented in Tab. 3. Our model clearly outperforms mainstream PDF question-answering products. Our model can accurately retrieve the evidence needed to answer user questions and generate accurate answers based on relevant text segments or diagrams. Since these commercial solutions do not provide detailed code or technical reports, we could not obtain information about their parameter sizes or token counts, and uploading documents directly to their websites does not allow us to accurately assess the models' latency. Finally, we evaluated the GPT accuracy of all open-source and proprietary models on this subset benchmark. The results show that our model has significant advantages in both textual similarity and semantic understanding.\nTo compare with more open-source large document models, considering that most of these models can only handle single-page documents, we constructed a subset of the PaperPDF benchmark containing only test samples with single evidence. Therefore, we provided all models with only the page containing the evidence as input; note that the pages are input in the form of images. These models and our PDF-WuKong have not been trained on such data. In this setting, our sparse sampler sampled all the content of that page. As shown in Tab.5, our model's zero-shot capability on this subset is significantly better than other document large models."}, {"title": "5.3. Document-oriented VQA", "content": "To validate the generalization capability of our model to other document understanding scenarios, we conducted experiments on several benchmark datasets and compared PDF-WuKong with other representative models.\nFirst, we evaluated the performance of PDF-WuKong and other open-source models on DocVQA [35], ChartQA [37], and InfoVQA [40], which are all single-page document datasets. As shown in Tab. 6, our model achieved leading performance compared to other open-source models. This demonstrates that PDF-WuKong can effectively handle various types of documents and questions, showcasing its versatility in document-oriented visual question answering tasks.\nIn addition, we assessed the performance of traditional specialized models and large-scale models on two existing multi-page document QA datasets. The experimental results, presented in Tab. 7, indicate that our model's performance in multi-page document scenarios is comparable to these specialized models and far surpasses the latest document large model, DocOwl2 [26]. Notably, on complex multi-page document datasets like DUDE [58], PDF-WuKong outperforms GPT-4V [59]. This improvement is attributed to our sparse sampler, which effectively filters out useful information from multi-page documents, enabling the model to focus on relevant content.\nFurthermore, we conducted zero-shot evaluations on a new long multimodal document understanding benchmark MM-NIAH [18]. As shown in Tab. 8, our model uses the fewest parameters yet achieves the second-best performance. Although InternVL-V1-5-RAG [18] surpasses PDF-WuKong by 2.8%, it utilizes 36.5 billion more parameters than our model. Moreover, as the context length"}, {"title": "5.4. Ablation Study", "content": "To comprehensively evaluate the effectiveness of our proposed model components, we conducted ablation studies focusing on datasets, the impact of the sparse sampler, sampling strategies, and document length. Below, we present the findings from each of these experiments."}, {"title": "Document length", "content": "To understand the impact of document length on model performance and efficiency, we divided the test set into subsets based on the number of pages per document. Results in Fig. 5 demonstrate that our model's performance and time efficiency remained relatively stable across documents of varying lengths. This stability indicates that the sparse sampler effectively reduces the input size to a manageable level, regardless of the original document length. In contrast, MLLMs without the sparse sampler were unable to handle long documents effectively; their performance and time efficiency deteriorated significantly as the document length increased. These findings highlight the robustness of our approach in processing long documents without sacrificing accuracy or incurring additional computational costs."}, {"title": "6. Conclusion", "content": "We have presented PDF-WuKong, a novel Multimodal Large Language Model that effectively addresses the challenges of understanding long PDF documents containing interleaved text and images. By introducing an end-to-end sparse sampling mechanism, our model efficiently extracts the most relevant paragraphs and diagrams in response to user queries, significantly reducing input token size and making the process independent of document length. We also constructed PaperPDF, a comprehensive dataset with 1 million question-answer pairs for training and 6,000 pairs for evaluation, specifically tailored for academic PDFs. Experimental results demonstrate that PDF-WuKong not only outperforms existing open-source models but also surpasses proprietary products by an average of 8.6% in F1 score on long multimodal PDF understanding tasks. Our approach maintains high accuracy and efficiency even as document length increases, offering a scalable and interpretable solution for practical applications in document understanding."}]}