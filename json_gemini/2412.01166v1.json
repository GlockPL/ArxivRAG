{"title": "Object Agnostic 3D Lifting in Space and Time", "authors": ["Christopher Fusco", "Mosam Dabhi", "Shin-Fang Ch'ng", "Simon Lucey"], "abstract": "We present a spatio-temporal perspective on category- agnostic 3D lifting of 2D keypoints over a temporal se- quence. Our approach differs from existing state-of-the-art methods that are either: (i) object agnostic, but can only operate on individual frames, or (ii) can model space-time dependencies, but are only designed to work with a single object category. Our approach is grounded in two core prin- ciples. First, when there is a lack of data about an object, general information from similar objects can be leveraged for better performance. Second, while temporal informa- tion is important, the most critical information is in imme- diate temporal proximity. These two principles allow us to outperform current state-of-the-art methods on per-frame and per-sequence metrics for a variety of objects. Lastly, we release a new synthetic dataset containing 3D skeletons and motion sequences of a diverse set animals. Dataset and code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Reconstructing 3D deforming objects from 2D landmarks is a long-standing challenge in computer vision. While tra- ditional approaches use multiple camera views to mitigate ambiguities, recent learning-based methods are capable of lifting 3D object structure using sparse 2D keypoints from a single camera. Coupled with an abundance of publicly available 3D human pose data, human-specific lifting mod- els like MotionBERT [34] have become increasingly capa- ble. However, their reliance on human-specific information and vast amounts of training data make it problematic if they are to be used for other objects. In particular, animals pose a significant challenge due to the limited amount of publicly available 3D animal data.\nThis has motivated recent developments around object- agnostic lifting. Most notably, 3D-LFM [7] achieved state- of-the-art lifting performance by leveraging a combined dataset of various object categories. It leverages the per- mutation equivariant property of transformers, along with additional inductive biases, to handle object category im- balance and within-category object rig/skeleton variation. However, a shortfall of 3D-LFM is its inability to utilize"}, {"title": "2. Related Works", "content": "temporal information, leading to less accurate reconstruc- tions in dynamic sequences.\nOur work tackles these challenges by introducing the first object-agnostic 3D lifting framework that is both data- efficient and temporally aware. Our approach uses the power of transformers with a strategic inductive bias that focuses on temporally proximate frames, enabling it to cap- ture motion dynamics effectively. This approach improves the accuracy of 3D reconstructions across various object categories, even in scenarios with limited data and unseen objects. Additionally, we address the lack of publicly avail- able datasets for diverse animal skeletons and motion se- quences by creating a new synthetic dataset. This dataset includes 4D labels for 13 animal categories, encompass- ing 698 animation sequences with temporal consistency, de- signed to foster further research in class-agnostic lifting.\nThe contributions of this paper are:\n\u2022 We propose a class-agnostic lifting model with a strate- gic inductive bias directly embedded in the architecture. We demonstrate the state-of-the-art 3D lifting of our ap- proach across scenarios involving noise, occlusions, and unseen objects.\n\u2022 We contribute a new synthetic dataset containing 4D skeletons for a variety of animals with animated be- haviour sequences, where temporal consistency is priori- tised through a non-linear refinement procedure.\n\u2022 We empirically validate the effectiveness of our ap- proach on our synthetic dataset. We achieve state-of-the-art results with existing metrics and provide an additional met- ric to provide a more complete analysis. We plan to release all dataset and code."}, {"title": "2.1. 3D Pose Estimation", "content": "Obtaining the 3D pose of an object from a single monoc- ular camera generally follows one of two paths. The first approach directly predicts the 3D pose from RGB im- ages [2, 22, 25, 26], but this method often struggles with generalization to distribution shifts, such as varying light- ing and background conditions. Alternatively, a two-stage method divides the task between two specialized models [3, 4, 11, 20]: a pose detector first extracts the 2D pose, which is then lifted into 3D by a separate model. Our work aligns with this two-stage approach, focusing on improving the robustness and generalization of the 3D lifting step."}, {"title": "2.2. Object-Specific Lifting", "content": "Traditional non-rigid structure-from-motion (NRSfM) al- gorithms have been effective for modeling simple objects, such as human bodies and hands [5, 9]. These methods rely on the availability of 2D keypoints and specific 3D supervision for the object in question. However, recent"}, {"title": "2.3. Object-Agnostic Lifting", "content": "deep learning approaches have demonstrated superior per- formance in handling the complexity of various object rigs [13, 17, 24, 28]. Despite these advancements, they still re- quire the 2D keypoints to have consistent semantic corre- spondence across all instances of the object, meaning that a specific landmark, such as an elbow, must have the same semantic meaning across different poses.\nThis limitation persists even in state-of-the-art deep learning models like MotionBERT [34] and others [3, 4], which are tailored specifically for human body 3D lift- ing. The specialized nature of these models and their de- pendency on large datasets make them unsuitable for ob- jects with limited available data, such as animals. Animal-specific 3D lifting models suffer from similar issues, often being restricted to a single animal category and demonstrat- ing poor generalization due to data scarcity [10, 15, 21].\nThe paradigm of object-agnostic lifting has been recently pioneered by 3D-LFM [7], which can handle a wide range of object categories by leveraging large-scale information of large-scale data to enhance performance for underrepre- sented or unseen objects. However, unlike object-specific models like Motionbert [34], 3D-LFM does not incorpo- rate temporal information, which is needed for accurate 3D reconstruction over sequences. Our framework builds upon this insight, integrating the strengths of both object-agnostic lifting and temporal modeling to achieve improved results."}, {"title": "2.4. Animal Datasets", "content": "To effectively benchmark category-agnostic lifting over videos of animals, a diverse set of animal categories with accurately labeled 3D skeletons is essential. Recent animal datasets have made strides in this area but are often lim- ited to single atemporal images [29, 31], with few publicly available datasets offering 3D poses of animals in video se- quences [6, 18]. Moreover, methods to collect data for a broad set of animals are typically impractical, expensive, or yield noisy results.\nInspired by the tracking community, we turn to synthetic data [8, 14, 33]. Existing synthetic datasets are often re- stricted to a single animal, such as pigs [1] or ants [27], and generally contain only simplistic motion sequences. The DeformableThings4D [19] dataset offers more complex and diverse motion sequences animated by artists, but it was created for dense mesh recovery and does not include 3D skeletons. We build upon these models and animations to create a new dataset specifically designed for the task of class-agnostic 3D lifting."}, {"title": "3. Method", "content": "In this section we explain our data collection pipeline and class-agnostic lifting model."}, {"title": "3.1. Dataset", "content": "We find a noticeable gap in available public datasets con- taining diverse 3D animal skeletons with realistic motion sequences. We aim to fill this gap by creating a new syn- thetic dataset building on the mesh vertices and anima- tion sequences provided by the DeformableThings4D [19] dataset. We provide 3D skeleton labels for 13 animal cat- egories, totalling 698 animation sequences with temporal correspondence across 51,360 frames. We provide statistics and examples of our dataset in the supp. material. We detail our data collection pipeline in following section, see Fig. 2 for a visualization of our data collection pipeline."}, {"title": "3.1.1 Animal joints", "content": "Given the vertices of a skin-tight mesh of an animal and an associated sequence of movement, our aim is to find the 3D locations of anatomically-accurate skeleton joints of the animal. We define the locations of K vertices and N joints in 3D space throughout a sequence of T frames as $V \\in \\mathbb{R}^{T \\times K \\times 3}$ and $J\\in \\mathbb{R}^{T \\times N \\times 3}$, respectively. Note that the number of mesh vertices, joints, and frames may vary across animals and sequences. We take inspiration from motion capture, where markers are attached to an object and used to estimate joint positions via triangulation. Let us define $j \\in \\{1,2,..., N\\}$. We strategically select a subset of vertices $S_{t,j} = \\{S_{t,j,1}, ..., S_{t,j,m}\\} \\subset V_t$ to be virtual markers, such that the mean of the virtual markers will provide the location of a joint j in frame t:\n$\\frac{1}{m} \\sum_{i=1}^{m} Stj$\nThe selection of vertices for a subset $S_{t,j}$ is guided by a visual inspection of $J_{t,j}$ and the motion similarity of ver- tices within $S_{t,j}$. The bones of the animal are defined as an adjacency matrix $A^{N \\times N}$ containing the connections be-"}, {"title": "3.1.2 Non-linear optimisation", "content": "tween joints. We decide the number of joints, their ap- proximate locations, and their connections by reviewing the anatomical structure of the target animal.\nThe noise that is inherently present in our human annotation process occasionally results in the length of animal bones to change over time, measured as the Euclidean distance be- tween two connected joints. We adopt an additional inverse- kinematics optimisation to refine the position of joints such that the bone lengths are consistent. We formulate the prob- lem as solving for the pose angles $\\theta \\in \\mathbb{R}^{T \\times N \\times 3}$ of forward kinematics for each joint j in frame t:\n$I_{t,j} = f(\\theta_{t,j}) = f(\\theta_{t,p}). \\begin{bmatrix}R(\\Theta_3) & L_{j,p} \\\\ 0 & 1 \\end{bmatrix}$,\nwhere p is the parent of joint j in the kinematic chain, R transforms $\\theta_{t,j}$ into a valid $R \\in SO(3)$ rotation matrix us- ing Rodrigues' rotation formula, and $L_{j,p}$ is the bone length between joints j and p in the first frame. Forcing the L translation vector to be from a single frame forces bone lengths to be the same for every frame.\nWe use gradient descent to optimise the objective func- tion\n$\\underset{\\theta}{\\text{minimize}} \\sum_{t=1}^{T} \\sum_{j=1}^{N} || \\hat{I}_{t,j} - I_{t,j} ||^2 + \\lambda L_S$,\nwhere $L_S$ is an additional smoothness regulariser:\n$L_S = ||\\hat{I}_{t.j} - \\hat{I}_{t-1.j} ||^2$\nOur inverse-kinematics optimisation provides a new set of joints $I_{t,j}$ ensuring that an animal has consistent bone lengths throughout any complex sequence of movement."}, {"title": "3.1.3 Perspective projection", "content": "We define an initial camera pose to satisfy two conditions. The mean location of the animal throughout the sequence is at the center of the camera view, and all joints are within view for the entirety of the sequence. We randomly rotate the camera around the y-axis and then project the points to the 2D camera plane. For our purpose we do this only once for each animation sequence, however this process can be used to obtain many different views of the animal through- out the sequence."}, {"title": "3.2. Lifting model", "content": "Given an input sequence of 2D skeletons $X \\in \\mathbb{R}^{T \\times J \\times 2}$, where T is the number of frames in the video and J the number of joints, our goal is to reconstruct the 3D skeletons $\\hat{Y} \\in \\mathbb{R}^{T \\times J \\times 3}$ of the object for each frame."}, {"title": "3.2.1 Keypoint features", "content": "The attention mechanism of transformers is inherently per- mutation equivariant, such that inputs can be randomly per- muted and the corresponding outputs will remain the same. We leverage this property to handle objects with different joint configurations. We utilise the masking mechanism of [7] to handle the technical challenge of training with different numbers of joints. Inputs are zero-padded up to the maximum number of joints in a mini-batch and a mask $M\\in \\{0,1\\}$ is used to ignore padded joints. Each element in M is defined as:\n$Mi =\n\\begin{cases}\n1 & \\text{if joint i is present} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nWe project the 2D skeletons to D-dimensional features $F\\in \\mathbb{R}^{T \\times J \\times D}$ using Random Fourier Features (RFF) [32]. We additionally encode each 2D joint (x, y) with its tem- poral location t in the video. We compute the feature of an input $p = [x, y, t]^T$ as:\n$\\psi(p) = \\sqrt{\\frac{2}{D}} [sin(W. p + b); cos(W. p + b)]$,\nwhere $W \\in \\mathbb{R}^{\\frac{D}{2} \\times 3}$ is sampled from a normal distribu- tion $N (0, I)$ and $b \\in \\mathbb{R}^{\\frac{D}{2}}$ is sampled from a uniform distri- bution $U(0, 2\\pi)$. We choose analytical RFF for its success in low-data and out-of-distribution (OOD) scenarios [7, 32]. We find that encoding the temporal position is beneficial for our motion encoder."}, {"title": "3.2.2 Motion encoder", "content": "Our motion encoder leverages multi-head self-attention (MHSA) to embed temporal context into the keypoint fea-"}, {"title": "3.2.3 Space encoder", "content": "tures. Although we choose MHSA for its ability to use in- formation from all frames, its lack of any inductive bias makes it unsuitable for tasks with limited data. We argue that it is not necessary to consider all frames in a sequence because most of the useful information can be found in nearby frames. We explicitly impose this inductive bias into the MHSA blocks by applying a binary mask to the inter- mediate attention maps. We define the mask $Z \\in \\mathbb{R}^{T \\times T}$ for a joint at time t as:\n$Z_{t,i} =\n\\begin{cases}\n1 & \\text{for } t - a \\leq i \\leq t + a \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nwhere a is a hyper-parameter controlling the number of frames before and after t that can contribute information during attention..\nGiven a set of spatial features $F \\in \\mathbb{R}^{T \\times J \\times D}$ as input to our motion encoder, we matrix transpose to get $F_M \\in \\mathbb{R}^{J \\times T \\times D}$. We first apply linear projections to obtain queries Q, keys K, and values V for each head h:\n$Q^{(h)} = F W_q^{(h)}, K^{(h)} = F W_k^{(h)}, V^{(h)} = F W_v^{(h)}$,\nwhere $Q^{(h)}, K^{(h)}, V^{(h)} \\in \\mathbb{R}^{N,T,d}$ for a total of H heads. We apply our temporal mask Z before the non-linear softmax operation for each head:\n$head_h = softmax(\\frac{Q^{(h)} (K^{(h)})^T}{\\sigma} \\times Z). V^{(h)}$,\nwhere $\\sigma$ is a scaling factor. Finally, each head is concate- nated and projected:\n$MHSA (F_M) = [head_1; ...; head_H] W_p$.\nThe space encoder uses the features from our motion en- coder to model the relationships among joints in a single frame. We found the hybrid graph-based approach of [7] to perform favorably for our task. Given the motion features $F_M \\in \\mathbb{R}^{T \\times J \\times D}$, a single space-layer has two simultaneous processing streams, one for capturing the local connectivity between joints $G_{local}$, and another for capturing global con- nectivity $G_{global}$. These two streams are concatenated and projected to provide an output containing a combination of both streams $F_S$ as:\n$F_S = MLP([G_{local}(F_M, A); G_{global} (F_M)])$."}, {"title": "3.2.4 Decoder and Procrustes-based loss", "content": "The local graph-attention $G_{local}$ utilises an adjacency ma- trix of the joint connections $A \\in \\mathbb{R}^{J \\times J}$ to model the object connectivity. A layer normalisation and skip connection is applied to give the layer output. As with our motion en- coder, we stack O layers with residual connections between them.\nLastly, given the latent features $F_S$, we use an MLP to de- code these features to obtain the predicted 3D structures $\\hat{Y}^{canon} \\in \\mathbb{R}^{T \\times J \\times 3}$ of the object in a canonical 3D space\n$\\hat{Y}^{canon} = MLP(F_S)$.\nWe align each canonical prediction $\\hat{Y}^{canon}$ with the ground truth Y via a Procrusteas alignment method which solves for the optimal rotation $R_t$ individually for each frame t as:\n$\\underset{R_t \\in SO(3)}{\\text{minimize}} || Y_t - \\hat{Y}_t^{canon} R_t ||^2$\nIn practice, we use Singular Value Decomposition (SVD) to solve this optimisation problem. To ensure that $R_t$ be- longs to the special orthogonal group SO(3), we enforce det($\\hat{R}_t$)= +1. This step is crucial for mitigating reflection ambiguity in our predictions.\nThe resulting $R_t$ is used to align our canonical predic- tions with the ground truth. We additionally scale the pre- dictions relative to the ground truth using a scaling factor $s_t \\in \\mathbb{R}$:\n$\\hat{Y}_t = st. (\\hat{Y}_t^{canon}\\hat{R}_t)$"}, {"title": "3.2.5 Loss function", "content": "With our predictions now aligned with the ground truth, we can compute our loss. We compute the Mean Squared Error (MSE) of the 3D points along with an additional velocity error $L_{vel}$:\n$L_{vel} =\n\\frac{T}{2}\n=\n\\sum_{t=1}^{T} \\sum_{j=1}^{J} || Y_{t,j} - \\hat{Y}_{t,j} ||^2 + \\lambda L_{vel}$,\n$L_{total} =\n\\sum_{t=2}^{T} \\sum_{j=1}^{J} |( Y_{t,j}) \u2013 Y_{t-1,j}) \u2013 (\\hat{Y}_{t,j} \u2013 \\hat{Y}_{t-1,j})|^2$,\nWe use $\\lambda$ as a scalar to weight the velocity loss."}, {"title": "4. Experiments", "content": "We evaluate our method on various animal categories to as- sess its performance and generalisation properties. Com- parative analyses are with recent state-of-the-art video (Mo- tionBERT) [34] and single-frame (3D-LFM) lifting mod- els [7]. For completeness, we also include comparisons on human-specific lifting, see the supp. material for results on the Human3.6M benchmark.\nDatasets We use a dataset of 13 animals generated with our data pipeline, as described in Sec. 3. The 2D keypoints provided by off-the-shelf pose detectors [30] are inherently noisy due to factors such as lighting conditions and image quality. To simulate these conditions, we synthetically per- turb the 2D keypoints with an additive noise, which corre- sponds to an average 3-pixel error. We present results for non-noisy data in the supp. material. Note that we also pro- vide comparisons on a real-world example, see Fig. 1 and 3D human pose estimation results in the supp. material. We normalise the 2D keypoints and 3D labels to [-1,1], after scaling the 3D labels following existing works on 3D hu- man pose estimation [34]. We split the data for training by randomly selecting 80% of the animations sequences for each animal, with the remaining animations with- held for testing. For further information about the data, see Sec. 3."}, {"title": "Evaluation protocols", "content": "Previous video lifting methods [4, 11, 20, 34] evaluate the non-rigid structure and motion of their method by calculating the mean per-joint position er- ror (MPJPE) directly with the ground truth in the camera space. However, our model and 3D-LFM make predictions in a canonical space that requires the alignment of each frame to the ground truth. As such, we instead use the stan- dard per-frame Procrustes-aligned MPJPE metric and refer to it as the frame-aligned MPJPE (FA-MPJPE) for brevity. Additionally, we compose a metric to measure the relative motion error in a video sequence.\nSequence-Aligned MPJPE We formulate the sequence- aligned MPJPE (SA-MPJPE) as solving for a single ro- tation matrix $R \\in SO(3)$ to align the 3D predictions $\\hat{Y}$ and ground truth Y for all T frames in a sequence. This is in contrast to the SA-MPJPE that aligns each individual frame in a sequence by solving for T rotation matrices. Af- ter alignment we compute the MSE to produce the final er- ror value. Let us specify Y, $\\hat{Y} \\in \\mathbb{R}^{T \\times J \\times 3}$, for J joints, to define our metric as\n$\\underset{R}{\\text{minimize}} \\sum_{t=1}^{T} || Y_t - \\hat{Y}.R ||^2$"}, {"title": "4.1. Object-agnostic lifting", "content": "Tab. 1 demonstrates the effectiveness of our method com- pared to both MotionBERT and 3D-LFM. We outperform both methods across all three metrics and nearly every ani- mal. Notably, our approach achieves 45% lower SA-MPJPE and 70% lower SA-MPVE compared to 3D-LFM, demon- strating that our predicted 3D motion has both higher ac- curacy and smoother movement, while also preserving ac- curate structure (FA-MPJPE). This substantial performance gap highlights the critical role of the motion encoder in cap- turing the temporal relationship between joints. While Mo- tionBERT is a spatio-temporal approach, it has no inductive bias to assist with handling scarce data and multi-category training. Fig. 3 qualitatively demonstrates the predictions of our method compared to 3D-LFM.\nSingle- vs multi-category We evaluate the performance of each method on single-category training and compare it to their performance on multi-category training in Sec. 4.1. For the single-category training, we train each approach from scratch using data specific to one animal category. This process is repeated for all 13 animals, and the mean error is reported in Tab. 2. All methods show significant im- provement from multi-category training, achieving at least a 25% reduction in FA-MPJPE, with our approach achiev- ing a 50% reduction. This improvement highlights the ad- vantage of unified learning across a vast spectrum of object"}, {"title": "4.2. OOD generalization", "content": "categories, particularly in scenarios where the training data is small and unbalanced.\nRobustness to occlusion To evaluate the robustness of 3D-LFM and our approach to occlusion scenarios, we trained both models by randomly masking 10% of all 2D keypoints within a frame and tasking the models with re- covering the 3D locations of the missing joints. As shown in Fig. 6a with 10% occlusion, our method demonstrates superior robustness to occlusions while 3D-LFM struggles. Even at an extreme case of 60% occlusion we continue to see legible predictions from our method, see supp. material for a qualitative result.\nUnseen objects We perform a 13-fold analysis, where each fold involves holding out one animal category from the original dataset during training. For example, in 1-fold, the bunny category is excluded from the training data and used to evaluate the generalisation capability of each method. As shown in Fig. 4 (left), our approach demonstrates su- perior OOD generalization when handling unseen animal categories by outperforming existing methods by a signifi- cant margin. We present a qualitative reconstruction for a bunny instance in Fig. 5 and refer the reader to the supp. material for tabulated results.\nRig transfer When lifting an unseen animal in the wild, we may encounter an animal with a more complex struc- ture than seen during training. We showcase our ability to generalize to an unseen animal with an unseen number of joints. While MotionBERT is limited to rigs with the same or fewer joints as those seen during training, our method can handle any number of joints. We train on animal rigs with"}, {"title": "4.3. Ablations", "content": "In this section we ablate the important building blocks of our approach. We first highlight the importance of our spatio-temporal approach as opposed to a spatial-only ap- proach. Then, we demonstrate the improvement gained from our temporal-proximity inductive bias. We go on to compare strategies for encoding the position of a joint in time. Lastly, we ablate the importance of our procrustes- based training.\nTime vs. space We evaluate the significance of informa- tion sharing across time facilitated by our proposed motion encoder. To isolate the impact of time, we create a space-only variant of our model by replacing the motion encoder with additional space blocks such that the number of pa- rameters remains similar. As shown in Tab. 3, modelling temporal dependencies is crucial for enhancing 2D-3D lift- ing performance, particularly in terms of per-sequence re- construction accuracy (SA-MPJPE) and smoothness (SA- MPVE).\nConstrained temporal attention We observe that apply- ing our inductive bias to restrict information sharing en- hances the optimization of our model. We conduct our abla- tion by starting from having a strong inductive bias (\u03b1 = 2) inductive bias and progressively increasing it by a factor of 2. We do this until a is large enough to allow information sharing across the entire sequence (\u03b1 = \u2212). We apply \u03b1 according to Eq. (7). Tab. 4 shows the impact of different \u03b1 on performance, averaged over five statistical runs. Over- all, we observe a trade-off between the accuracy of the 3D structure and the accuracy of 3D motion. In our case we identify \u03b1 = 8 as optimal in our case, which we used for all of our experiments."}, {"title": "5. Conclusion", "content": "Analytical temporal embedding Here we ablate our use of our analytical RFF for encoding temporal information. In Tab. 5 we show that in our experiments it is beneficial to use an analytical embedding over a learned embedding. This embedding is important for the motion encoder to un- derstand the ordering of frames in a sequence.\nProcrustes-based training Similar to 3D-LFM, we find it useful to train our model with a Procrustes-based loss (PBL), as shown in Tab. 6. Allowing the model to focus on learning object structures significantly enhances the pre- dicted 3D structure and benefits overall 3D motion accu- racy. Interestingly, we observe that we are able to perform well without the PBL, even outperforming 3D-LFM across all metrics. This offers an interesting insight into future work with our method for practical applications that require implicitly predicted camera rotation.\nIn this work, we introduced an object-agnostic 3D lifting model that leverages a temporal inductive bias for tempo- ral sequences. Our approach sets a new benchmark in lift- ing performance for object categories with limited available data. The model's ability to generalize across unseen object categories and rigs shows its versatility and robustness, even in challenging scenarios involving noise and occlusions. In addition to these contributions, we have also introduced a new synthetic dataset designed to stimulate further research in class-agnostic 3D lifting models. This work paves the way for more generalized and efficient 3D reconstruction methods that can be applied across diverse real-world ap- plications."}]}