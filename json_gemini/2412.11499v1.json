{"title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents", "authors": ["Wonje Choi", "Woo Kyung Kim", "Minjong Yoo", "Honguk Woo"], "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DEDER, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DEDER, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DEDER allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DEDER surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DEDER.", "sections": [{"title": "1. Introduction", "content": "In embodied AI, significant advancements have been made in applying large language models (LLMs) to task planning. For example, SayCan (Brohan et al., 2023) combines LLMs' reasoning capabilities with a reinforcement learning (RL)-based affordance model to interpret task instructions and deduce executable robotic skills in the environment. Several works (Huang et al., 2022; Wu et al., 2023; Song et al., 2023; Singh et al., 2023) explore the grounding of LLMs to the environment through prompting based on sensory data, reference trajectories, and available skills. Recently, palm-e (Driess et al., 2023) expands the embodied reasoning abilities of LLMs to include multimodal data, such as visual observations of the environment. Yet, these approaches, which directly rely on LLMs for continual short-term decision-making, often encounter practical limitations in real-world applications, particularly when decision-making agents are required to operate on capacity-constrained, off-the-shelf devices. The high computational requirements of LLMs pose a significant challenge in such scenarios.\n\nThe direct end-to-end distillation of an LLM into a more compact, resource-efficient model, while it appears straightforward, might not be effective for complex embodied tasks (Dasgupta et al., 2023). This challenge stems from the requirements of a deep understanding on embodied task features, which inherently demand the long-horizon multi-step reasoning along with the ability to adapt to time-varying environment contexts. An embodied agent frequently encounters new and unseen environment information through its interaction with the surroundings. This continual exposure to a diverse range of environment conditions adds layers of complexity and variability, which in turn complicates the distillation process.\n\nOur work is focused on the distillation of LLM-based policies for embodied tasks into off-the-shelf agents that are only capable of operating small language models (sLMs). We present DEDER, an innovative embodied distillation framework, designed to decompose and distill the embodied reasoning and decision-making procedures of LLM-based policies into two distinct small, more manageable models: reasoning-policy and planning-policy. The reasoning-policy focuses on understanding and interpreting task requirements and environment contexts, while the planning-policy concentrates on generating actionable plans based on the insights provided by the reasoning-policy. This division allows for the sophisticated functionalities of LLMs to be leveraged in a more resource-efficient manner, suitable for embodied agents with capacity-limited, off-the-shelf devices.\n\nAchieving the reasoning-policy via LLM distillation presents a unique challenge due to the hidden nature of reasoning processes within an LLM. We address this by employing the embodied Chain-of-Thought (CoT) and in-context learning, enhanced with self-verification, through the iterative use of the LLM. For the reasoning-policy, we employ the embodied knowledge graph (KG)-based prompting and the contrastively prompted attention model, integrated with an sLM. These two techniques improve the quality of rationale outcomes from the reasoning-policy, by integrating the environment information to a KG efficiently and representing the current context effectively. They also allow for a parallel structure for multiple rationale generation, thereby facilitating the timely task planning at runtime. The planning-policy exploits the distilled rationales to determine executable plans, addressing the practical need for actionable decision-making for complex tasks.\n\nUsing the ALFRED benchmark (Shridhar et al., 2020), our experiments exhibit the advantages of DEDER. The results demonstrate that the policy derived through DEDER significantly surpasses other baselines such as LLM-planner (Song et al., 2023) in zero-shot task planning scenarios. DEDER achieves a substantial improvement of 15.0% in seen task settings and 21.1% in unseen settings. Considering that DEDER employs an sLM at runtime instead of LLMs, the results clearly underline the exceptional adaptability of DEDER in handling new and unencountered environments.\n\nNote that DEDER is the first framework to achieve sLM-based policies, which is resource-efficient yet comparable to LLM-based policies (i.e., the baselines in Section 5.1) in performance, for complex embodied tasks. The contributions of our work are summarized as follows."}, {"title": "2. Related Work", "content": "LLM-based Embodied Control. In the field of embodied control, there is a growing trend of utilizing LLMs for reasoning and execution of tasks in real-world settings (Brohan et al., 2023; Huang et al., 2022; Song et al., 2023). Our work aligns with this direction but sets itself apart by aiming to enable off-the-shelf devices to attain comparable embodied task performance without directly utilizing LLMs at runtime, instead focusing on tuning a smaller language model.\n\nEmbodied Policy Distillation. Recently, several works focused on distilling complex decision-making strategies, often derived from computationally intensive models, into compact and efficient ones suitable for resource-constrained environments. In (Sumers et al., 2023), knowledge is distilled from pre-trained vision-language models to supervise the language grounded skills of instruction-following agents. In (Jain et al., 2021), a two-stage training scheme is adopted for visual embodied agents. A relevant subset of policy distillation in RL is transferring the teacher policy in a supervised fashion (Yin & Pan, 2017). In particular, prior work concentrated on reducing the cross-entropy between the distributions of teacher and student policies (Parisotto et al., 2016; Schmitt et al., 2018). Our LLM-based policy distillation is also to minimize the divergence from the distribution of a teacher policy, which is an LLM, while exploring the unique two-tier hierarchy in decomposition and distillation of the LLM's reasoning capabilities.\n\nReasoning Capabilities of LLMs. Numerous studies investigated the reasoning capabilities of LLMs, exploring methods like retrieval-augmented in-context examples (Lewis et al., 2020; Ram et al., 2023), KG integration (Andrus et al., 2022; Baek et al., 2023), and CoT prompting (Wei et al., 2022; Wang et al., 2022). Recent research also demonstrated the effectiveness of distilling CoT processes from LLMs into sLMs (Wang et al., 2023; Li et al., 2023). Our work is in the same vein as LLM distillation, but specifically targets complex embodied tasks and uses decomposed distillation."}, {"title": "3. Problem Formulation", "content": "In RL, an environment for embodied agents is modeled as a Partially Observable Markov Decision Process (POMDP), represented by a tuple $(S, A, P, G, H, R, \\Omega, O)$ (Song et al., 2023; Singh et al., 2023). Here, $s \\in S$ is a state space, $a \\in A$ is an action space, $P : S \\times A \\times S \\rightarrow [0,1]$ is a transition probability, $G \\in G$ is a goal space, $h \\in H$ is a high-level task description and $R: S \\times A \\times G \\rightarrow R$ is a reward function. The distinct aspect of embodied agents' environment lies in its nature of partial observations, featured as an observation space $o \\in \\Omega$ and a conditional observation probability $O: S \\times A \\rightarrow \\Omega$ (Sutton & Barto, 2018). This aspect accounts for the agents' limited perception, rendering"}, {"title": "4. Approach", "content": "For embodied tasks, it is essential for the agent to have reasoning capabilities to understand and interact with complex, dynamic environments. Yet, the simplification of the reasoning process is particularly necessary when employing an sLM-based policy, given the inherent limitations of sLMs due to their restricted model capacity. This can be achieved by integrating Markov Decision Process (MDP) features such as goal, state, observation, action, return-to-go, and sub-goal, which RL formulations specify, into the reasoning process (Chane-Sane et al., 2021; Hausknecht & Stone, 2015; Chen et al., 2021; Janner et al., 2021).\n\nIn this work, we refer to this type of environment information and MDP features as rationales, as they can function as justifications or hints that help to elaborate the reasoning behind plans. We leverage these rationales as a means to effectively distill the embodied reasoning capabilities from an LLM to small models, thereby achieving an sLM-based policy. For this distillation, we develop the DEDER framework comprising these phases: (i) rationale dataset construction, (ii) policy distillation via embodied KG, and (iii) zero-shot deployment and evaluation.\n\nIn the phase of rationale dataset construction, we harness the CoT scheme inherent in the usage of LLMs to extract rationales from expert transitions (i.e., series of action plans) in the environment. This is achieved through MDP-featured in-context learning, employing RL-specific queries as prompts that are defined by the properties of the MDP. In the subsequent phase of policy distillation, we establish an sLM-based policy structured in a two-tier hierarchy based on an embodied KG. It includes a reasoning-policy, which is trained to generate rationales in a single-step CoT optimized by behavior-based contrastive learning, as well as a planning-policy, which is learned to infer action plans through CoT prompting guided by these rationales. In the deployment phase, we evaluate distilled sLM-policy in a zero-shot manner for unseen environments in which task descriptions, object positions, and indoor scenes are changed."}, {"title": "4.1. Rationale Dataset Construction", "content": "Consider an expert dataset $D_{exp} = \\{T_i = (o_i, a_i, h_i)\\}_{i}$, where each transition $T_i$ includes an observation $o_i$, action (plan) $a_i$, and high-level task description $h_i$ for timesteps $i$. We expand the $D_{exp}$ dataset to establish a rationale dataset $D_{Rtn} = \\{c_i = (o_i, a_i, h_i, R_i)\\}_{i}$, where each transition $T_i$ is supplemented with a rationale set $R = \\{r_j\\}_{j=1}^m$. To obtain the rationale set specifically configured for given embodied tasks, we integrate MDP-featured in-context learning with the CoT prompting mechanism of an LLM. This involves iteratively prompting the LLM with a series of RL-specific queries, exploiting retrieval-augmented examples, similar to (Ram et al., 2023). Subsequently, the rationale set undergoes LLM's assessments, as discussed in (Sun et al., 2023), to be incorporated into the dataset $D_{rtn}$.\n\nMDP-Featured In-Context Learning. To extract the rationales from the LLM using the transition $T$, we continually update in-context examples in a retrieval-augmented"}, {"title": "4.2. Policy Distillation via Embodied Knowledge Graph", "content": "To distill the reasoning capabilities of the LLM to an sLM-based policy $P_{SLM}$ using the rationale dataset $D_{Rtn}$, we structure the policy in a two-tier hierarchy. The first tier is a reasoning-policy $P_R$; it is responsible for inferring a rationale set from a given observation $o$, a task description $h$, and an embodied KG $g$. The second tier is a planning-policy $\\Phi_p$; it generates the plan, guided by the rationales from $P_R$.\n\n$\\Phi_{SLM} = \\Phi_P \\circ P_R: (o, h; g) \\rightarrow a$.\n\nThe embodied KG is an internal component of the sLM-based policy, encapsulating the environment information.\n\nIn training, we use a fine-tuning method with soft prompts for the sLM-based policy. This is effective for adopting sLMs with limited reasoning capabilities, where in-context learning is not allowed.\n\nEmbodied KG. As the agent continuously interacts with the environment and can accumulate information for task completion, it is important to represent information efficiently for prompting the sLM-based policy. We employ an embodied KG, a set of triples $g = \\{x_i = (x^s, x^r, x^o)\\}_{i}$, where $x^s$ is the subject, $x^r$ is the relation, and $x^o$ is the object. For instance, given \u201can apple is on the table\u201d and \u201cthe agent picks up a knife\u201d, the corresponding triples are (Apple, On, Table) and (Agent, Pickup, Knife), respectively. We refine the embodied KG at each planning step $t$ by an update function $U$ such as\n\n$U: (g_{t-1}, a_{t-1}, o_t) \\rightarrow g_t$.\n\nTo prompt the sLM-based policy, we also use the KG retriever function $V$, which retrieves a subset of triples from $g$ relevant to observation $o$ and task description $h$.\n\n$V : (o, h; g) \\rightarrow \\{x \\in g | S(x, (h, o)) \\geq \\delta\\}$\n\nThe relevant triples are chosen by the pre-trained semantic relevance function $S$ between each triple in $g$ and inputs $o$ and $h$, where $\\delta$ is a threshold hyperparameter. Hereafter, $g$ denotes the graph extracted via the KG retriever function.\n\nReasoning-Policy Distillation. For the reasoning-policy $\\Phi_R$ which produces a rationale set $R$, we employ the attention module with an encoder-decoder architecture.\n\n$\\Phi_R = Dec \\circ \\Psi \\circ \\Phi_{Enc}: g \\rightarrow R$"}, {"title": "5. Evaluation", "content": "5.1. Experiment Setting\n\nEnvironments. For evaluation, we use the ALFRED (Shridhar et al., 2020) environment. For embodied reasoning tasks, ALFRED features a wide variety of interactive elements including 58 distinct object types (e.g., bread) and 26 receptacle object types (e.g., plate) across 120 different indoor scenes (e.g., kitchen). By combining these objects and indoor scenes with instructions of 7 different types (e.g., pick & place), 4703 distinct tasks can be configured (e.g., \"Put a keychain in a plate and then put them in a shelf\"). This setup provides a broad spectrum of real-world-like challenges, encompassing complex navigation, object manipulation, and executing sequential operations.\n\nWe use 312 trajectories for the expert dataset and organize the evaluation tasks into 4 categories based on their similarities to the tasks in the expert dataset. For Train category, the tasks are identical to those in the expert dataset. For Seen category, the tasks remain the same as those in the expert dataset, except that the starting positions of the task-irrelevant objects are placed randomly. For Unseen Spatial category, all objects in the environment are placed randomly. The most challenging category Unseen Environment includes new tasks and indoor scenes not presented in the expert dataset.\n\nBaselines. For comparison, we implement several language planning approaches: 1) SayCan (Brohan et al., 2023) is an embodied planning framework that integrates the probability from an LLM with affordance scores. For embodied control, the affordance is based on object presence information. 2) ZSP (Huang et al., 2022) employs a step-wise planning to accomplish the embodied tasks. 3) LLM-planner (Song et al., 2023), directly utilizes an LLM for embodied task planning, which dynamically re-plans when it fails to generate an executable plan. In evaluating in off-the-shelf devices, we adopt sLMs for these language planning baselines (Say-Can, ZSP, LLM-planner).\n\nWe also implement several knowledge distillation algorithms: 4) SCoTD (Li et al., 2023) is a knowledge distillation algorithm to train an sLM using reasoning samples derived from an LLM. 5) SCOTT (Wang et al., 2023) is a knowledge distillation method to train an sLM, which involves self-consistent CoT augmentation from an LLM and counterfactual reasoning objectives. 6) End2End (Micheli & Fleuret, 2021) is an embodied task planning method using a single-tier policy unlike DEDER, which directly generates a plan from the inputs. To evaluate the task planning performance in the environment through generated trajectories, we also implement an additional rule-based policy that directly interacts with the environment, following the action plans from the baselines and our DEDER.\n\nEvaluation metrics. We use two different metrics in ALFRED (Shridhar et al., 2020). Task Success Rate (SR) (%) is the percentage of tasks fully completed, where a task is regarded as a success if and only if all the sub-goals are achieved. For example, the task \"Slice a heated bread\" is decomposed into individual sub-goals like \u201cslice the bread\u201d and \u201cheat the bread\u201d. Goal-conditioned Success Rate (GC) (%) is the percentage of sub-goals that are completed."}, {"title": "5.2. Performance Evaluation", "content": "In Table 1, we evaluate the embodied task planning performance, wherein each policy is evaluated in a zero-shot manner. Our DEDER consistently demonstrates the robust performance in both SR and GC metrics across all test categories (Train, Seen, Unseen Spatial, Unseen Environment), achieving 21.6% higher SR and 12.3% higher GC on average over the most competitive baseline LLM-planner-PaLM. Given that LLM-planner-PaLM exploits the PaLM (Chowdhery et al., 2023) with 540 billion parameters, 2700 times larger than DEDER, this performance gain of DEDER is particularly significant. Moreover, compared to the baselines that have the same parameter size, we observe that DEDER outperforms these baselines for all categories up to 27.6% higher SR and 12.6% higher GC on average.\n\nThe language planning baselines (SayCan, LLM-Planner, ZSP), which are configured to adopt sLMs (LLaMA2,"}, {"title": "5.3. Ablation Studies", "content": "In the ablation studies, the performance metrics for all test categories (Train, Seen, Unseen) are reported in SR.\n\nRationale Dataset Construction. For extracting rationales and constructing the dataset in Section 4.1, we test several language models, including sLMs such as GPT2-large (Radford et al., 2019) denoted as GPT2, and LLMs such as PaLM and GPT3 (Chowdhery et al., 2023; Brown et al., 2020). We also evaluate the dataset construction process without employing MDP-featured in-context learning and self-verification; This ablated method is denoted as Few-shot, as described in (Wei et al., 2022), where a fixed set of examples is used for prompting rationale extraction.\n\nIn Table 2, there is a notable performance drop across the task categories when employing GPT2. These results are consistent with our motivation to harness the reasoning capabilities of LLMs for rationale extraction, which in turn contributes to the effective distillation into the sLM-based policy. Moreover, DEDER yields better performance compared to Few-shot by an average of 5.35% in the Unseen settings, excluding GPT2 results. This improvement indicates the benefits of our MDP-featured in-context learning and self-verification methods.\n\nRationale Structure. We analyze the effect of individual queries designed for rationale extraction. In Figure 4(a), we evaluate the rationale set generated by the reasoning-policy involving the LLM's self-critic function in (3). The dotted line denotes the performance achieved by employing all 7 queries, whereas each bar along the x-axis indicates the performance when the i-th query is excluded during the dataset construction. Since each query is specifically designed to capture unique features in MDPs, such as goals, state, and return-to-go (illustrated in Figure 2), the exclusion of any one of these queries leads to a performance decline.\n\nReasoning-policy structure. Table 3 shows the effect of our embodied KG and contrastive learning scheme $L_{con}$. The results indicate that DEDER, when utilizing both KG and $L_{Con}$, achieves the highest performance. This is attributed to our embodied KG, which efficiently encapsulates both the evolving embodied information and the agent's interaction experiences. Additionally, in the absence of contrastive learning, the reasoning-policy struggles to extract precise features for the next plan, leading to a performance drop. We also measure the inference time of DEDER and DEDER without embodied KG on off-the-shelf devices such as RTX 3090 and 3050 GPUs. As the use of embodied KG allows for more efficient representation, DEDER achieves a reduction in inference time by 0.3 second on average.\n\nSLM Capacity. Table 5 shows the performance of DEDER with respect to the variations in network parameter sizes for the reasoning-policy $P_R$ and the planning-policy $\\Phi_p$. In our default framework implementation, we utilize the t5-small and gpt2 models for $P_R$ and $P_D$, respectively. The results indicate that the performance improvement is not significant when the parameter size of the planning-policy $\\Phi_p$ increases. In contrast, enhancing the parameter size of the reasoning-policy $\\Phi_R$ results in performance gains, showing an average increase of 6.57% when comparing the t5-small and the t5-large used for $P_R$ in unseen settings. Specifically, the smaller sLM (t5-small) tends to overfit on the training datasets, which might yield better performance in the Seen category compared to the mid-sized sLM (t5-base). For the larger sLM (t5-large), a performance improvement is noted in the Seen category, attributed to its enhanced reasoning capabilities. In contrast, the Unseen settings demonstrate a linear performance increase as the parameter size of the reasoning policy grows, suggesting that a larger parameter size significantly boosts the generalization ability of the model. This indicates the benefits of distilling rationales from LLMs, which plays a crucial role in establishing a robust sLM-based policy."}, {"title": "6. Conclusion", "content": "We introduced DEDER, a novel framework that effectively distills the reasoning capabilities of LLMs into more compact sLMs for executing complex embodied tasks in device-constrained environments. The framework operates in a strategic distillation process, involving embodied rational data construction from an LLM, data-driven embodied policy distillation to an sLM, and task planning with the sLM. This allows for the efficient use of LLM-powered complex task planning functions in real-world time-constrained settings while ensuring the adaptability to resource-constrained agent conditions through two-step distillation into reasoning and decision-making.\n\nLimitation. As DEDER employs pre-trained sLMs, there is a potential dependency on the pre-trained knowledge embedded in the sLMs. In Table 5, we observe that a reduced network capacity of sLMs leads to decreased performance in unseen settings. This indicates that the limited network capacity of the sLM hinders the distillation of reasoning capabilities, consequently affecting the zero-shot adaptation in environments with significant domain shifts.\n\nFuture Work. Future directions for our research include enhancing the framework's ability for few-shot optimization, especially in scenarios with significant domain shifts, aiming to explore the versatility of LLMs."}, {"title": "A. Environment settings", "content": "A.1. ALFRED\nWe utilize ALFRED (Shridhar et al., 2020), which provides comprehensive vision-and-language navigation and rearrangement tasks for embodied AI. This environment requires an agent to follow language formatted instructions to accomplish real-world-like household tasks. ALFRED features 58 different object types (e.g., bread) and 26 receptacle types (e.g., plate) across 120 various indoor scenes (e.g., kitchen). It supports 4703 unique tasks, each configured by combining these elements with one of 7 instruction types (e.g., pick & place), such as \u201cPut a keychain in a plate and then put them on a shelf\u201d. This complexity and diversity makes ALFREDD an ideal benchmark for evaluating models that emphasize hierarchy, modularity, and advanced reasoning and planning capabilities.\n\nA.2. Expert Dataset\nExpert Dataset and Evaluation Task Settings. To generate an expert dataset, we use planning domain definition language rules (Aeronautiques et al., 1998). For implementation, we use the open source project. We collect 312 expert trajectories in a variety of tasks varying the starting positions of the agent and objects as well as the indoor scenes.\n\nWe organize the evaluation tasks into 4 distinct categories based on task similarities with the expert dataset: Train, Seen, Unseen Spatial, and Unseen Environment. For the Train category, the tasks are identical to those tasks in the expert dataset. The Seen category maintains the same tasks as in the expert dataset, but task-irrelevant objects are randomly positioned at the start. For this, we evaluate 528 tasks. In the Unseen Spatial category, all objects are placed randomly, and tasks are either defined by new task descriptions or optimal planning sequences not included in the Train category. For this, we evaluate 1415 tasks. Lastly, for the most challenging category Unseen Environment where all objects are randomly placed, and the task or indoor scenes are not presented in the Train category."}, {"title": "B. Implementation Details", "content": "In this section, we provide the implementation details of our proposed framework DEDER and each comparison. Our framework is implemented using Python v3.9 and PyTorch v2.0.1, trained on a system of an Intel(R) Core (TM) i9-10980XE processor and an NVIDIA RTX A6000 GPU. For comparisons, we implement 3 types of widely used approaches: language"}, {"title": "B.1. Language Planning Approach", "content": "For the language planning approaches, we employ 3 different methodologies: SayCan, LLM-planner, and ZSP (Brohan et al., 2023; Song et al., 2023; Huang et al., 2022). For generating high-level plans, we utilize various LMs like PaLM, LLAMA, and GPT2-large.\n\nSayCan (Brohan et al., 2023) integrates pretrained skills with language models, generating plans that are feasible to the context. SayCan achieves this by combining affordance scores derived from the LM with the agent's experiences. In line with SayCan's methodology, we calculate embodied affordance scores by utilizing object presence information. For implementation, we refer to the open source project.\n\nZSP (Huang et al., 2022) leverages the capabilities of the LLMs for embodied task planning by interpreting high-level task descriptions and formulating sequential strategies, thus efficiently performing embodied tasks. ZSP accomplishes this by crafting step-by-step prompts based on examples of similar successful tasks, followed by sampling executable plans using the LLM in conjunction with these provided examples. For implementation, we refer to the open source project.\n\nLLM-planner (Song et al., 2023) leverages the LLMs for few-shot planning, empowering embodied agents to perform complex tasks in environments with observed information, guided by natural language instructions. For implementation, we"}, {"title": "B.2. Knowledge Distillation Approach", "content": "For the knowledge distillation approaches, we employ two different algorithms: SCoTD and SCOTT (Li et al., 2023; Wang et al., 2023). For distilling the reasoning-policy to produce MDP-featured rationales, we implement each method to create the rationale dataset accordingly.\n\nSCOTD (Li et al., 2023) is a CoT distillation method to train an sLM. It utilizes a LLM to generate a variety of rationales with answers, which are then used to educate the sLM. We employ SCOTD to generate and train rationale data for the reasoning-policy, and then utilize the distilled rationales to further train the planning-policy.\n\nSCOTT (Wang et al., 2023) is a consistency knowledge distillation method to train a smaller, self-consistent CoT model from a much larger teacher model. SCOTT uses contrastive decoding to elicit better rationale supervision and a counterfactual reasoning objective to align the student model's predictions with these rationales. We utilize SCOTT for creating rationale data and subsequently training the reasoning-policy. The learned rationales from reasoning-policy are then applied to train the planning-policy. For implementation, we refer to the open source project."}, {"title": "B.3. End2End", "content": "End2End (Micheli & Fleuret, 2021) is a method for embodied task planning that specifically utilizes the GPT-2 model, trained with direct supervision on expert data. This approach forms the foundational backbone for our planning policy \u0424p implementation. For implementation, we refer to the open source project. The hyperparameter settings for End2End are summarized in Table A.9."}, {"title": "B.4. DEDER", "content": "The entire procedure of our DEDER consists of rationale dataset construction and policy distillation via embodied knowledge graph phases."}, {"title": "B.4.1. RATIONALE DATASET CONSTRUCTION", "content": "In the rationale dataset construction phase, we use PaLM (Chowdhery et al., 2023) as the source LLM, exploiting its reasoning capabilities. We formulate 7 queries to extract rationales from the LLM and manually design 9 initial examples of query-rationale pairs for each expert transition. To calculate similarity between language embeddings of 7 and c, we use contextual embedding model, To utilize the LLM as a critic function, we query the LLM to assess whether the generated rationales are sufficient to generate the plan. For instance, we ask, \u2018Can the rationale {rationale} lead to the next plan {plan}? Answer with yes or no.'. To ensure accurate evaluations, we pose several variations on the critic prompt and determine the final critic score based on majority voting.\n\nAlgorithm 2 lists the dataset construction procedures."}, {"title": "B.4.2. POLICY DISTILLATION VIA EMBODIED KNOWLEDGE GRAPH", "content": "In the policy distillation phase, we distill the reasoning capabilities from the LLM into an sLM-based policy PsLM, which is structured with a two-tier hierarchy consisting of the reasoning-policy PR and the planning-policy \u0424p.\n\nReasoning-policy. For the reasoning-policy PR, we utilize a pre-trained language model with an encoder-decoder structure, specifically t5-small (Raffel et al., 2020), as our default setting. The dimension of prefix prompts , postfix prompts , and decoder prompts Dec are set to be 20. Our implementation of the attention module \u03a8 incorporates two distinct attention mechanisms: causal attention and gated attention, each comprising a single attention layer. The causal attention module uses a causal mask, while the gated attention module includes an additional learnable gate function. PR is optimize by (11) and (12).\n\nPlanning-policy. For the planning-policy \u0424p, we utilize a pre-trained language model with a decoder structure, specifically gpt2 (Radford et al., 2019), as our default setting. \u0424p is optimize by (14)."}, {"title": "C. Additional Experiments", "content": "For further investigation, we report additional experimental results."}]}