{"title": "Federated Sequence-to-Sequence Learning for Load Disaggregation from Unbalanced Low-Resolution Smart Meter Data", "authors": ["Xiangrui Li"], "abstract": "The importance of Non-Intrusive Load Monitoring (NILM) has been increasingly recognized, given that NILM can enhance energy awareness and provide valuable insights for energy program design. Many existing NILM methods often rely on specialized devices to retrieve high-sampling complex signal data and focus on the high consumption appliances, hindering their applicability in real-world applications, especially when smart meters only provide low-resolution active power readings for households. In this paper, we propose a new approach using easily accessible weather data to achieve load disaggregation for a total of 12 appliances, encompassing both high and low consumption, in scenarios with very low sampling rates (hourly). Moreover, We develop a federated learning (FL) model that builds upon a sequence-to-sequence model to fulfil load disaggregation without data sharing. Our experiments demonstrate that the FL framework - L2GD can effectively handle statistical heterogeneity and avoid overfitting problems. By incorporating weather data, our approach significantly improves the performance of NILM.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT years have seen a rise in the adoption of smart meters in households globally, primarily driven by energy-saving targets. These devices have the potential to increase energy awareness, thereby reducing power use [1]. However, most smart meters only record the total household consumption, making it necessary to disaggregate energy consumption into appliance levels. The non-intrusive load monitor (NILM) method proposed by Hart [2] offers a cost-effective and convenient way to disaggregate total energy consumption into individual appliance consumption, thereby improving energy awareness and efficiency. The development of accurate NILM methods holds theoretical and practical significance for energy consumers, utilities, and policymakers. Apart from identifying inefficient appliances, a detailed breakdown of electrical consumption at the appliance level would allow residences to replace energy-inefficient appliances with more efficient ones, resulting in significant energy savings.\nNILM initially aims to leverage signal characteristics, such as harmonics, current, and voltage, to identify appliances' status and disaggregate total energy consumption into individual appliances' levels. In the studies of the last two decades, event-based methods NILM approaches are widely used, such as [3], [4], [5], [6]. This approach often requires dedicated devices with a high sampling rate (typically <1 second) to detect distinctive features or changes in the electrical signal that indicate the activation or deactivation of specific appliances. These events could be transient changes, patterns, or signatures in the power signal that are characteristic of certain appliances. In other studies, some deep learning methods have been utilized to identify the switch events in the high sampling scenerios. Such as convolutional neural network (CNN)-based methods[7], [8], [9], [10] and denoising autoencoder (DAE) [11], [10]. Different from the event-based methods that identify the events through manually selected thresholds or rules, these methods enable the extraction of relevant features related to event switch changing from complex, high-sampling data by adjusting parameters through training.\nReal-world scenarios, however, pose challenges as smart meters in general households are unable to capture complex signals, such as harmonics, transients, or current-voltage signals, but only active power readings. Furthermore, residential smart meters were originally installed to record household electricity consumption for billing purposes. Thus, the sampling rate of smart meters is typically rated from 15 minutes to 1 hour. According to [12] and [13], considering the limitation of the sampling rate of household electricity meters, event-based methods require a high sampling rate for event switching detection are not appropriate. The main obstacle lies in detecting low-energy-consuming devices because their changes in active power are not significant compared to high-power appliances. The data from household electricity meters, being averages of loads over 15-minute to 1-hour time intervals, exhibit more pronounced changes in the load of high-consumption appliances, while the load changes of low-consumption appliances are smoothed out due to averaging, making challenges to detect their switching events. For the CNN and DAE methods, compared to dedicated devices, household electricity meters have lower sampling frequencies and therefore collect less data. This increases the training difficulty for methods like CNN and DAE, which require large amounts of data. Additionally, because these methods lack temporal dependency and treat each time step equally, they face similar challenges to event-based methods in detecting low-energy-consuming appliances in the averaged meter records. Some other works [14], [12] are only focused on the high-consuming appliances, such as water heaters and air conditions ignoring the low energy-consuming devices. However, in the context of real load disaggregation tasks, the consumption of low-power appliances is equally crucial. This is because the usage patterns of these appliances are often associated with the residents' electricity habits.\nConsidering the limitations of sampling rate in household electric meters, some studies suggest that the accuracy of NILM algorithms can be improved by enhancing the diversity of data or augmenting data from household electricity meters. In the work [15], [16], the data argument method was utilized to generate high sampling rate data from low rate samples by using step-wise interpolation. Generating the high sampling rate data, the methods used in the high sampling data can be adopted. However, interpolation typically fills in missing data points based solely on the observed values before and after the gap, without considering the broader context of the data and may lead to a loss of information and potentially distort the original energy consumption pattern. Different from the data argument, some studies have explored the use of weather information to better understand energy consumption behaviours. In [17], [18], both methods analyzed hourly energy readings collected from households to gain insights into energy consumption patterns and disaggregate them into specific end-uses. In the work of [19], they introduced a multi-objective genetic algorithm alongside pre-learned deductions about household appliances. These deductions are based on previous observations of both active and reactive energy usage, weather conditions, and details regarding appliance ownership. Compared with the complex signals, the weather data is readily accessible and has been validated in previous works as a useful feature.\nAnother challenge is caused by data privacy, due to the potential risk of data leakage. Users' electricity usage patterns and personal information can be inferred if their meter data are leaked. Therefore, under strict data protection regulations, the accessibility of meter data faces challenges, thereby increasing the difficulty of model training. Limited data also raises the risk of overfitting during model training. To ensure reliability in training models while also fulfilling privacy concerns, the Federated learning (FL) framework has been leveraged in the NILM tasks. In the work of [20], [21], FedAvg has been used to enable the training of the model without data sharing. However, FedAvg will encounter the issue of slow convergence when data is heterogeneous between clients. In real-world scenarios, the diverse usage habits of residents regarding electrical appliances, along with variations in weather conditions, pose challenges in dealing with data heterogeneous. To mitigate the influence of data heterogeneous, FedProx [22] is generally used in the data heterogeneous scenarios. In the NILM tasks, however, both the FedAvg and FedProx require a significant amount of communication between the server and clients, which poses a significant challenge in terms of communication and computational overhead for NILM devices. Considering the limitations in computational efficiency and computational costs of NILM devices, additional methods are needed to strike a trade-off between communication frequency and model accuracy.\nInspired by the approaches listed above, to tackle the challenges that are caused by the limited sampling rate of smart meters and data privacy, this paper proposes a sequence-to-sequence (seq2seq) federated learning model that utilizes an encoder to extract global information from input weather and smart meter load data, providing it as prior input to the decoder that performs hourly load disaggregation without data sharing. Specifically, we propose a new sequence-to-sequence model that takes in 24-hour temperature and humidity information in addition to total energy consumption to disaggregate the energy usage of 12 appliances. To ensure no data sharing during the training and avoid model overfitting in the limited data, we implement the model on federated learning frameworks.Furthermore, in order to mitigate communication overhead during Federated Learning (FL) training and address the challenge of heterogeneous data, we deploy the L2GD federated learning framework in Non-Intrusive Load Monitoring (NILM) tasks. The contributions of this paper can be summarized as follows.\n\u2022 We have proposed a Seq2Seq-based method that utilizes time-series data of total load and weather to disaggregate the total load into 12 appliances with varying power consumption, including low-power devices. Simultaneously, considering in the real scenarios that residential electric usage patterns are likely influenced by the season variations, we sample the data in two strategies to simulate the real scenarios and evaluate the proposed model within three federated learning frameworks.\n\u2022 Considering the difficulty in identifying the low-power appliances in hourly scenarios from total load data, we incorporate temporal weather data intending to identify the load of low-power appliances from the total load by leveraging weather-related information. Moreover, unlike previous approaches, we propose a seq2seq model. The encoder captures global information from the entire time series instead of focusing solely on load changes in specific time slots. This global information is then passed to the decoder as prior knowledge. The decoder integrates both global and individual time step information, enabling the disaggregation of different power appliances in hourly scenarios.\n\u2022 Considering the diverse data distribution among households in real-world scenarios and the significant communication overhead associated with federated learning, we have introduced the L2GD framework for the first time in the NILM domain. This framework aims to reduce communication costs while mitigating the negative impact of data heterogeneity on training. Experimental results indicate that, in both data homogenous and heterogenous scenarios, L2GD achieves comparable performance with approximately half the communication rounds compared to FedAvg and FedProx. Furthermore, in situations with an equal number of communication rounds and under data heterogenous conditions, it demonstrates superior effectiveness."}, {"title": "II. RELATED WORKS", "content": "In this section, we will review the related works that use time series deep-learning techniques in the NILM tasks and federated learning frameworks in privacy concern scenarios.\nIn deep learning methods, the NILM tasks are usually formulated as time series classification or regression problems. Thus recurrent neural network (RNN) based methods have been widely used in recent years. Diego et al. [23] takes two layers stacked LSTM to do the load disaggregation task, which concatenates the hidden states from the last layer with input data and feeds them into the dense layers. Another approach can be found in [24], [25], the bidirectional LSTM (BiLSTM) is taken to retrieve the hidden states from forward and backward directions low-sampling sampling data. Although the BiLSTM can handle long sequences and capture more effective hidden states than stacked LSTM, there is useless information contained in the input sequence. To better extract features from input data, in the work of [11], the denoising autoencoder (DAE) is taken to reconstruct the original data that only retains the principle component in the input sequence. Then, the reconstructed sequence is fed input to the LSTM. A similar approach can be found in Kelly et al. works [10], which add several 1D convolutional layers in the DAE and trained by manually corrupting the signal before feeding it into the input layer. According to the experience results, the addition of a convolution layer can slightly increase the performance against the work of [11]. In the work of [26], the researcher substituted the LSTM as Nest LSTM (NLSTM). Different from conventional LSTM, NLSTM contains an internal and external unit, which collaborates to choose and retain the most important long-term information based on the current situation. It helps create a strong and selective memory of the long-term characteristics of the target device. Although the LSTM can store the context information of time-series data, it has no ability to decide what parts of the input sequence are most important for the output. To mitigate this problem, the attention mechanism is implemented in [26], [27], [28], [29] to evaluate the similarity between the hidden states of different time steps. The CNN-based methods can be found in [7], [8]. The main idea is to project the input sequence to feature maps on a temporal scale. The CNN block is utilized to integrate the more advanced features with the previously calculated high-resolution features.\nTo achieve user privacy protection and computational efficiency, federated learning (FL) has been used in the field of NILM in recent years. In the study conducted by Hudson et al. [30], the FedAvg is implemented with the recurrent neural network architecture, where smart houses participated in the training process by utilizing their recorded meter data. The model parameters learned through federated learning were shared using the advanced metering infrastructure (AMI), ensuring the privacy of the data. A similar approach can be found in [31], the Seq2piont model is combined with the FedAvg framework in the NILM task. However, although the previous studies achieved good performance by utilizing the FL, they have shortages to deal with the domain adaption problem. To mitigate the domain adaption problem, Li et al. [32] utilized transfer learning to incorporate FedAvg to identify and learn individual equipment states retrieved from different domains. Different from [32], the meta-learning is leveraged in the work of [33], which trains the meta-model on the FL framework and fine-tunes each client. In the work of [34], the L2GD was proposed as to mixture of the global and clients' models. Different from the FedProx [22], L2GD can not only trade-off between the global and client model but also can utilize hyperparameters to adjust communication rounds, which can effectively reduce communication costs."}, {"title": "III. METHODOLOGY", "content": "Household electricity meters have the characteristic of low sampling frequency, causing difficulties in extracting patterns of low-power appliances' usage. In this regard, we propose a sequence-to-sequence model that utilizes LSTM with memory and forgetting capabilities to capture contextual information. We incorporate time-series weather data as additional features and leverage the potential relationship between weather changes and appliance usage to simultaneously disaggregate the loads of 12 appliances, including both high and low power, in hourly scenarios. To ensure privacy protection, we integrate the sequence-to-sequence model into the L2GD framework, ensuring that only model parameters are shared during training while users' private data remain secure. The overview framework is illustrated in Fig.3. Subsequently, we will discuss the sequence-to-sequence model and federated learning framework in the following sections."}, {"title": "A. Sequence-to-Sequence Model", "content": "The Sequence-to-Sequence model is composed of three components, including an encoder, decoder, and attention part. Fig. 1 shows the detail regarding the model structure. The encoder is composed of stacked bidirectional LSTM, which is responsible for encoding the input data and outputting the last time steps' hidden states $H_{lte}$, cell states $C_{lte}$ and final layers hidden states $H_{fle}$. The input data, including total consumption and historical weather information, is fed into the encoder part first.\n$H_{fle}, H_{lte}, C_{lte} = Encoder(X)$\n$X = Concatenate[weather;total load]$\t\t\t\t(1)\nDue to gating mechanisms in LSTM, the $H_{lte}$ and $C_{lte}$ can store the contextual information regarding the whole times-series load and weather-changing and $H_{fle}$ stores the hidden states of each time step. In the decoder part, we use the same structure as the encoder but initial the decoder hidden states and cell states by using the last time step hidden $H_{lte}$ and cell states $C_{lte}$ from the encoder, which stores the contextual information of whole time-series information. Then, the decoder takes the same input data for decoding and outputs the decoded final layers' hidden states $H_{fld}$. Using the hidden states from the last time step of the encoder to initialize the decoder allows the decoder to be more sensitive to changes in the input load at each time step while having knowledge of the global contextual information.\n$H_{fld} = Decoder(X, H_{lte}, C_{lte})$\t\t\t\t(2)\nBy initializing the decoder with the last time step hidden $H_{lte}$ and cell states $C_{lte}$ from the encoder, the decoder updated details in the time step to should be:\n$i_o = \\sigma(W_{iix}x_o + b_{ii} + W_{ih}H_{lte} + b_{hi})$\n$f_o = \\sigma(W_{ifx}x_o + b_{if} + W_{hf} H_{lte} + b_{hf})$\n$g_o = tanh(W_{igx}x_o + b_{ig} + W_{hg}H_{lte} + b_{hg})$\t\t\t\t(3)\n$o_o = \\sigma(W_{iox}x_o + b_{io} + W_{ho}H_{lte} + b_{ho})$\n$C_o = f_o \\odot C_{lte} + i_o \\odot g_o$\n$h_o = o_o \\odot tanh(c_o)$\nAccording to Equation 3, with each time step update, the $H_{lte}$ and $C_{lte}$, which contain global information used to initialize the decoder, will gradually be forgotten. To partially retain the global information retrieved by the encoder, the attention unit is implemented before the dense layer. The structure of the attention unit is shown in Fig 2.\nWe gain insight from the work of [35] to implement the attention on the output from the encoder and decoder. Denoting $h_s$ and $h_t$ as the encoder final layer's hidden states and the decoder final layer's hidden state. All these hidden state outputs contain processed time-series information of the model inputs. The attention unit first evaluates the similarity of hidden states from different time steps using the dot product.\n$score(h_t,h_s) = h_t^Th_s$,\t\t\t\t(4)\nThen the attention unit normalizes the score using the normalization function. In the common attention model, the Softmax is used as the normalization function to normalize the attention score between 0 to 1. However, in our task, we want to use attention to calculate the similarity between different time steps of the encoder and decoder. The hidden states at a certain time step t contain information from previous all-time steps. Using Softmax in this task would make the weights assigned to each time step become more evenly distributed. Thus we take the Sparsemax function [36] to obtain the attention weight $a_{ts}$ according Sparsemax(score($h_t$,$h_s$)), which is used to generate the context vector $c_t$ in Eq. (6).\n$\\tau(z) = \\frac{1}{k}(\\sum_{i=1}^{n}z_i)-z_i$\t\t(5)\nwhere $i = 1,2,..., n$, and $k$ is the number of non-zero elements in sparsemax(z).\nAfter getting the attention weights, the Hadamard product is used to weight the hidden states from the encoder and the output of the attention unit $a_t$, called attention vector, is obtained in Eq. (7):\n$c_t = \\sum a_{ts}h_s$\t\t\t\t(6)\n$a_t = [c_t; h_t]$,\t\t\t\t(7)\nwhere $[c_t; h_t]$ concatenate $c_t$ and $h_t$ into one matrix. The concatenate vectors will be fed into several dense layers to output the disaggregated loads for each appliance."}, {"title": "B. Federated Learning", "content": "In our NILM task, one of the potential challenges is the statistic heterogeneous problem, which arises due to the diverse electricity usage patterns among different residences. Various factors, such as residents' working hours, socioeconomic status, and household size, can affect appliance usage distribution, resulting in a high variance of customized model parameters and potential overfitting problems. To address this problem, we use two federated learning frameworks (e.g., FedAvg and FedProx) in Algorithm 1 to assess how the statistical heterogeneity impacts model performance and whether the FedProx can effectively handle this challenge. Another challenge is caused by the communication costs, especially for the heterogeneous scenarios. According to the [22], the data heterogeneous has an influence on the model convergence. The more significant the difference in data distribution among clients, the more communication requests each client needs to make to the server. In our works, we combine L2GD [34] with our proposed Seq2Seq model Algorithm 2, which allows a trade-off between the global model and the local models, to reduce the computation complexity by adjusting the aggregation frequencies. The L2GD framework can be found in Fig.3. When initiating L2GD, the server sends the initial model to each client. Simultaneously, with a probability p, it generates \u00a7. If \u00a7 is 0, each client trains a local model using its private data. If \u00a7 is 1, each client sends its local model to the server. The server then computes the average model and sends it back to the client. The client updates its model based on the dissimilarity between the average model and the local model, as well as the hyperparameter \u5165. When A is equal to 0, L2GD is equivalent to local update. As A increases, the client model gradually approaches the average model."}, {"title": "IV. EXPERIMENTS", "content": "In our work, we use the hourly data from the Pecan Street Inc [37]. After preprocessing the data and removing incomplete meter readings, we obtained a dataset comprising hourly load data from 44 households over two years. To retrieve the information regarding the weather, we also incorporate weather data, including the temperature and humidity concatenated smart meters' active power as the input data. In order to investigate the impact of weather factors on the performance of disaggregating different appliance loads, we selected a total of 12 appliances, including the low-power appliances. Some of them are intuitively affected by weather (such as air conditioners, pool pumps, and furnaces) and others are likely unrelated to weather (such as light plugs, living room plugs, and bedroom plugs).\nTo investigate the different performance of three FL frameworks in data heterogeneous and data homogeneous scenarios, We group the data into five clients and generate a heterogeneous dataset and a homogeneous dataset. For the homogeneous dataset, we implement random sampling without putting back strategy. While for the heterogeneous dataset, we manually select data in such a way that the 5th client includes the majority of data when most appliances are turned off, while other clients adopt random sampling methods. Each client only contains one part of the full dataset and they are not allowed to share their private data with other clients and servers during the training. Each client holds 20% of the full dataset and 70%, 30% of clients' data is used for training and testing separately."}, {"title": "E. Comparison of Federated learning frameworks in homogeneous and heterogeneous datasets", "content": "In this section, we compare the performance and efficiency of the FedAvg, FedProx, and L2GD frameworks in both homogeneous and heterogeneous data scenarios, incorporating weather features and utilizing Seq2Seq models.  shows the results of five clients in the heterogeneous and homogeneous datasets respectively. To ensure that the model receives sufficient training without overfitting, we conducted experiments by setting up candidate communication rounds and identified the optimal communication rounds (20 rounds). As shown in , under the conditions of homogeneous data and 20 communication rounds, L2GD, FedProx, and FedAvg exhibit very similar performance across five clients, with Eacc achieving excellent results exceeding 90%. However, under the conditions of heterogeneous data, as indicated in Table I, L2GD outperforms both FedAvg and FedProx on all five clients. This improvement is particularly evident in client 5, where we manually selected low-power consumption data, making the model training more challenging for this client. Significantly, at the same number of communication rounds, L2GD, compared to FedAvg and FedProx, achieves better results on client 5, which has heterogeneous data, without sacrificing the performance of clients with homogeneous data. (Eacc improves by 2.5% compared to FedAvg and by 2.4% compared to FedProx. NDE decreases by 21.5% compared to FedAvg and by 22.1% compared to FedProx). Since L2GD does not pursue a uniform global model but rather strikes a trade-off between the global model and local models, it allows each client's model to extract features more suited to its data distribution while ensuring a certain level of generalization. This, in turn, leads to better results in heterogeneous scenarios.\nTo compare the communication efficiency of FedAvg, FedProx, and L2GD, in  and , we contrasted the results of the three frameworks with only five communication rounds on both homogeneous and heterogeneous datasets. It is evident that even with only five communication rounds, L2GD outperforms both FedAvg and FedProx, whether on homogeneous or heterogeneous datasets. Most notably, in the case of heterogeneous data, the performance of FedAvg and FedProx on client 5 was significantly deducted, showing underfitted results (Eacc decreases from 0.743 and 0.744 to 0.669 and 0.685, respectively). While L2GD also experiences a decline in performance with reduced communication rounds, the magnitude is smaller. In the case of homogeneous data, when reducing communication rounds, FedAvg and FedProx exhibit a noticeable drop in performance across all clients. However, L2GD, with only five communication rounds, does not show the underfitted issue observed in FedAvg and FedProx. Its performance with five communication rounds is very close to that with twenty communication rounds."}, {"title": "F. Effectiveness of Weather Feature to Time-Series Model", "content": "Tabel. V shows disaggregation results performed by three time-series models. The results with the white backgrounds shows the model trained without weather data, while the gray shows the results that adding weather features. It can be seen that whether the weather feature is included or not, the Seq2Seq model outperforms both the BiLSTM with attention and GRU with attention models. However, without the inclusion of the weather feature, although Seq2Seq is slightly better than the other two models overall, the improvement is not very pronounced for specific appliances. Upon adding the weather feature, a significant enhancement in the performance of all three time-series models can be observed. Because the encoder extracts the global information regarding the total load and weather, the Seq2Seq model shows a better performance in low-consumption appliances, the improvement is most notable after incorporating the weather feature.\nAdditionally, it's interesting to note that in the results of Seq2Seq, appliances more likely to be affected by weather, such as Pool pumps, and Refrigerators, demonstrated improved performance when weather features were incorporated, leading to the reduction of the NDE (reducing 27% and 62% respectively). Additionally, even appliances we initially considered less influenced by weather, Living room plugs, Bathroom plugs, and Kitchen plugs, showed significant performance enhancements with the inclusion of weather features (the Eacc improved 4.9%, 3.3%, 5.5% respectively). The performance of Bathroom plugs and Kitchen plugs improved significantly, possibly due to residents' electricity usage habits being more sensitive to weather conditions (e.g., increased usage of electric hairdryers during winter)."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a federated sequence-to-sequence load disaggregation method by using the hourly sampled active power data and easily accessible weather data to perform load disaggregation tasks both for low-consumption and high-consumption appliances. Additionally, We have, for the first time in the NILM domain, deployed the L2GD framework in conjunction with our proposed Seq2Seq model. This implementation achieves efficient communication for load disaggregation while preserving data privacy. The experimental results indicate that L2GD not only enables efficient communication but also outperforms FedAvg and FedProx, particularly in NILM tasks' of heterogeneous scenarios.\nIn future work, we will focus on identifying other easily accessible features, similar to weather data, to facilitate load disaggregation tasks in scenarios with very low sampling rates."}]}