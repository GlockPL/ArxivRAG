{"title": "A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs", "authors": ["Yonglin Jing"], "abstract": "Hyper-relational knowledge graphs (KGs) contain additional key-value pairs, providing more information about the relations. In many scenarios, the same relation can have distinct key-value pairs, making the original triple fact more recognizable and specific. Prior studies on hyper-relational KGs have established a solid standard method for hyper-relational graph encoding. In this work, we propose a message-passing-based graph encoder with global relation structure awareness ability, which we call ReSaE. Compared to the prior state-of-the-art approach, ReSaE emphasizes the interaction of relations during message passing process and optimizes the readout structure for link prediction tasks. Overall, ReSaE gives a en-coding solution for hyper-relational KGs and ensures stronger performance on downstream link prediction tasks. Our experiments demonstrate that ReSaE achieves state-of-the-art performance on multiple link prediction benchmarks. Furthermore, we also analyze the in-fluence of different model structures on model performance.", "sections": [{"title": "1 Introduction", "content": "Hyper-relational knowledge graphs (KGs) differ from traditional KG settings, featuring optional in-formation on the relation aspect. The additional in-formation data structures can exhibit various forms, such as text descriptions or web page links on the relation element. In such settings, researchers may have to consider large-scale models, e.g. large lan-guage models (LLMs) to fully leverage the rich textual information. Another simpler and more manageable hyper-relational variant involves en-riched relations with key-value pairs constraints, also known as qualifiers or triple metadata (Vran-decic and Kr\u00f6tzsch, 2014; Pellissier-Tanon et al., 2020; Ismayilov et al., 2018). With qualifiers, we can easily distinguish hyper-relational facts. As shown in Figure 1, while traditional link prediction approaches may fail to distinguish triple facts such as 'George Miller\u2019\u2014\u2018nominated for\u2019\u2014\u2018Academy Award for Best Animated Feature' and 'George Miller'-'nominated for\u2019\u2014\u2018Academy Award for Best Picture', these two facts can be easily dis-tinguished by qualifiers decorating, e.g. ('for work'-'Happy Feet' and 'for work\u2019\u2014\u2018Babe') on relation 'nominated for'.\nQualifiers, as key-value pairs, resemble other node and edge elements (entities and relations) in knowledge graphs. Hyper-relations can accommo-date an arbitrary number of pairs of qualifiers. In message passing frameworks, it is crucial to capture qualifier information in conjunction with triple fact information. While previous works mainly focus on single fact information encoding before aggrega-tion, our method utilizes mutual information within the relation set and implements attention mecha-nisms during message passing. Meanwhile, we propose a neat decoder module for link prediction tasks, maintaining its permutation invariant char-acter while enhancing performance. We validate our method through experiments on numerous link prediction benchmarks."}, {"title": "2 Related work", "content": "Earlier methods for hyper-relational KGs learning typically regarded qualifier information as addi-tional information alongside the main triples. For instance, m-TransH (Wen et al., 2016) projects qualifier entity information into the main triple re-lation embedding space. Later approaches, such as RAE (Zhang et al., 2018) and NALP (Guan et al., 2019), treat hyper-relational information as abstract relations or parallel fact information. How-ever, these methods have not fully captured the characteristics of qualifier structure.\nHINGE (Rosso et al., 2020) adopts an iteratively convolved manner of composing qualifiers with main triple facts. Although it retains the hyper-relational nature of facts, HINGE operates on a triple-quintuple level that lacks the granularity of representing a certain relation instance with its qual-ifiers. Additionally, HINGE has to be trained se-quentially in a curriculum learning (Bengio et al., 2009) fashion, which requires sorting all facts in a KG in ascending order of the number of quali-fiers per fact that might be prohibitively expensive for large-scale graphs. StarE (Galkin et al., 2020) proposes a graph representations learning frame-work that can encode hyper-relational KGs with an arbitrary number of qualifiers while keeping the semantic roles of qualifiers and triples intact. It has been demonstrated to be sufficiently valid for nu-merous benchmarks and features a very clear model structure. However, we discovered that it faces is-sues such as fixed weight settings and plain pooling strategy, which may undermine the qualifier infor-mation during message passing. Notwithstanding, StarE overlooks the correlation/co-occurrence of relations in qualifiers and triples, which could play a significant role in KG representation learning.\nHyper2 (Yan et al., 2022) implements triple el-ements initialization in the Poincar\u00e9 ball vectors, improving parameter efficiency and maintaining low complexity. However, it neglects the graphi-cal structure. Other works emphasize the link pre-diction task and take semantic information into account, such as GRAN (Wang et al., 2021) and HAHE (Luo et al., 2023), which enhance link pre-diction performance on various datasets by lever-aging semantic information. However, the semanti-cally enriched network structure mainly works on link prediction-oriented sequence representations and makes less contribution to the graph embed-ding part. Other knowledge graphs may not con-tain meaningful or faithful semantic information. Therefore, leaving semantic information aside, we focus more on GNN-based approaches for ordinary hyper-relational KGs.\nAs for the decoder readout solution, NN4G (Micheli, 2009) proposes a simple yet parameter-efficient readout method for graph classification tasks. although the method alone suffers from difficulties such as information communication between different types of nodes, the method structure presents in a clear and strictly permutation-invariant way that inspires our work."}, {"title": "3 Preliminaries", "content": "Message passing is a framework for propagating information between nodes (entities) in a graph by passing messages along the edges (relations) that connect them. The goal is to learn node rep-resentations that capture the underlying structure and relationships present in the graph. These node representations are learned iteratively by aggregat-ing the information from the neighbors. In the case of a multi-relational graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{R}, \\mathcal{E})$, $\\mathcal{V}$ represents the set of nodes, $\\mathcal{R}$ represents the set of relations, and $\\mathcal{E}$ denotes the set of directed edges $(s,r, o)$, where $s$, $r$ and $o$ represent sub-ject, relation, and object, respectively. In addition, $s,o \\in \\mathcal{V}$, $r\\in \\mathcal{E}$, $r$ connects $s$ and $o$. Previous GNN (Marcheggiani and Titov, 2017; Schlichtkrull et al., 2018) formulates the message passing pro-cess as the following:\n$h_v^{(k)} = f(\\sum_{(u,r)\\in\\mathcal{N}(v)} W_r h_u^{(k-1)}),$"}, {"title": "4 Method", "content": "In this section, we illustrate the general architecture of ReSaE encoder for hyper-relational Knowledge Graph representation learning and the structure used for Link Prediction. All the following dis-cussion is about single-layer network structure. In order to capture the global hyper-relational mutual information, we introduce self-attention for global relation representation. We first get the relation attention matrix by performing attention calcula-tion on the whole relation set (R as mentioned in Preliminaries) as follows:\n$\\text{att\\_matrix}(h_r) = \\text{softmax}(\\frac{h_r h_r^\\top}{d}),$ (1)\nwhere $h_r$ is the relation representation and $d$ de-notes the embedding dimension. Here we do not perform a linear projection before the inner product, with detailed reasons presented in section 5. For more analysis of attention mechanism and other possible attention variants, which we will also dis-cuss in section 5.\nThen on the message passing stage, we cast the attention weight on the qualifier relation part ac-cording to their main triple relation $h_r$. And then pooling the qualifier relation features for each fact, e.g. sum and mean. Thus, we obtain the attention-weighted qualifier relation representation for each hyper-relational fact, as shown in the formula be-low:\n$\\text{att}(h_{qr}) = \\Phi_q(\\text{att\\_matrix}(h_r) \\odot h_{qr}),$ (2)\nwhere $\\Phi_q$ denotes the pooling method and $\\odot$ is the casting process. The attention calculation is illustrated in Figure 2.\nAlong with attention output, we also take simple pooling over qualifier relations and entities, so the features for message passing would be:\n$h_{\\text{hyper}} = [h_u, h_r, \\Upsilon_r(h_{qr}), \\Upsilon_v(h_{qv}), \\text{att}(h_{qr})],$ (3)\nwhere $h_{\\text{hyper}}$ denotes the hyper-relation fact rep-resentation. $\\Upsilon_r(\\cdot)$ and $\\Upsilon_v(\\cdot)$ denote pooling over qualifier relation and qualifier entity, respectively. Optionally, qualifier relation information can also be seen as pooling over $\\Upsilon_r(h_{qr}), \\text{att}(h_{qr})$. Under this setting, equation 3 can be rewritten as:\n$h_{\\text{hyper}} = [h_u, h_r, \\Upsilon_v(h_{qv}), \\Phi_{q2}(\\Upsilon_r(h_{qr}), \\text{att}(h_{qr}))],$ (4)\nwhere $\\Phi_{q2}$ denotes the pooling method. Then the message passing aggregation function can be writ-ten as equation 5:\n$h_u = W_\\lambda(\\text{r}) \\Phi_r(h_{\\text{hyper}})\nh_r = f(\\alpha \\cdot \\sum_{(u,r)\\in\\mathcal{N}(v)}h_u + \\beta h_o),$ (5)\nwhere $\\Phi_r(\\cdot)$ is a function that aggregates hyper-relational fact information concerning the relation type r, which can be concatenation, average, or weighted sum. Then the outcome is transformed by the weight $W_\\lambda$ to obtain hyper-relational fact rep-resentations corresponding to its directional type $\\lambda$ (forward, inverse, loop).\nFinally, the node (entity) representations are ob-tained by taking the weighted sum of itself and the sum of its neighbor representations. $f(\\cdot)$ de-notes the non-linearity activate function, $\\alpha$ and $\\beta$ are trainable parameters. The overview of the node updating process is illustrated in Figure 3."}, {"title": "5 Experiments and Analysis", "content": "In this section, we evaluate the effectiveness of our approach.\n5.1 Datasets:\nWe experiments on three hyper-relational datasets: JF17K (Wen et al., 2016), WikiPeople (Guan et al., 2019), and WD50K (Galkin et al., 2020). Among them we conduct WD50K experiments on different qualifier ratio settings, which are WD50K (33%), WD50K (66%), and WD50k (100%), indicating different ratio of hyper-relational triple. Literal context statements in WikiPeople are filtered out. JF17K is extracted from Freebase (Bollacker et al., 2008). The main statistics are shown in Appendix Table 5.\n5.2 Baseline\nWe mainly compare ReSaE with GNN-based meth-ods under similar parameter sizes, including: (i) m-TransH (Wen et al., 2016), (ii) RAE (Zhang et al., 2018),(iii) NaLP (Guan et al., 2019), (iv) NeuInfer (Guan et al., 2020), (v) HINGE (Rosso et al., 2020), (vi) StarE (Galkin et al., 2020) (vii) Hyper2 (Yan et al., 2022), and (viii) HyTrans-former (Yu and Yang, 2021).\n5.3 Metrics\nFor link prediction, we report Mean Reciprocal Rank (short as MRR, introduced in (Bordes et al., 2013)), and Hits@1, 10. Hits@K measures the proportion of correct entities ranked in the top K. The results are averaged over test dataset and over predicting missing head and tail entities. Generally, a good model is expected to achieve higher MRR and Hits@N.\n5.4 Ablations\nTo verify the significance of our method, we con-duct experiments on some simplified model vari-ants by replacing or removing module parts from the full model. ReSaE w/o coo, and ReSaE w/o att are methods without relation coo-matrix, and without relation self-attention module respectively. Transformer+MEAN pool refers to the method that simply performs mean pooling of the transformer output without type-wise readout mechanism. All variants are trained under the same setting as the main experiment, except for the differences in their respective modules.\n5.5 Training Subject\nFor the task of link prediction, the model is trained through 1 - N cross-entropy loss as function bel-low:\n$\\text{loss} = \\sum_i^N y_i\\log P_i,$\nwhere $y_i$ is the t-th entry of the label y. $P_i$ rep-resents the sigmoid score calculated based on the one-vs-all cosine similarity between the final hid-den states and all entity representations.\n5.6 Training Settings\nFor ReSaE, we conduct all experiments with the same hidden size of 200, and use the Adam (Kingma and Ba, 2015) optimizer with learn-ing rate of 10-4. Label smoothing (Dettmers et al., 2018; Vashishth et al., 2020) is set to 0.1. The model was trained on varying batch sizes and epochs, with detailed parameter settings presented in Table 4 in the Appendix. For related work in comparison, we reuse the metrics records of previ-ous works in our experiment result table.\n5.7 General Performance\nWe compare our methods to prior approaches on the link prediction task. Experimental results in Table 1 demonstrate that in the case of MMR, our method outperforms the previous method and achieves the state-of-the-art performance among all GNN-based methods. Nevertheless, our method only performs slightly better than HyperTrans-former (Yu and Yang, 2021) and underperforms on some metrics on some metrics (e.g. H@1, H@10). In the case of WD50K, our method outperforms"}, {"title": "5.8 Analysis for qualifier aggregation", "content": "The qualifier set $\\{(q_{ri}, q_{bi})\\}$ for a single hyper-relational typically consists of numerous pairs of (relation, entity). Message passing necessi-tates organizing the qualifier representation into a fixed-length hidden state. Conventional aggrega-tion/pooling strategies such as addition, mean, max, and rotation (Sun et al., 2019) have the following two main assumptions:\n1. Entity and relation share the same embedding space or co-exist in the same complex hidden space (for rotation setting).\n2. For most hyper-relational facts, every pair of qualifiers is supposed to be nearly as important.\nThe above two assumptions may be true for reg-ular Knowledge Graph (KG) scenarios; however, they might not hold for hyper-relational KGs. In hyper-relational KGs, qualifiers serve as the 'deco-ration' of relations, and the (relation, entity) pairs are quite distinctive. Traditional pooling strate-gies such as sum, mean, and max can easily lead to ambiguity. Furthermore, qualifier entities and relation pairs usually describe information that is much more complex than directional information. These cannot be treated as complex embedding spaces as in classical KG learning approaches, e.g. ComplE (Lacroix et al., 2020).\nIntuitively, we need a separate space to preserve qualifier information and a weighted aggregation instead of a regular aggregation approach. Under these circumstances, the attention mechanism is employed, and qualifier hidden states are aggre-gated separately with the main triple parts.\nOur intuition was validated by the ablation stud-ies. The attention matrix scores also suggest the necessity of our weighted aggregation strategy. For instance, WD50K contains the following relations:\n$\\bullet\\; r_a$ (nominated for)\n$\\bullet\\; r_b$ (statement is subject of)\n$\\bullet\\; r_c$ (for work)\nThese three relations co-occur in a single hyper-relational fact, as shown below:\n(George Miller, nominated for, Academy Award for Best Animated Feature)\nwith qualifier:\n\\{(statement is subject of, 79th Academy Awards),\n(for work, Happy Feet)\\},\nin which the main triple relation is $r_a$, $r_b$ and$r_c$ appear in its qualifiers. After training loops, the attention score between $r_a$ and $r_b$ is 0.214 while the attention score between $r_a$ and $r_c$ is 0.710. This follows the designed expectation because the se-mantic meaning of $r_c$ is much more related to $r_a$ than $r_b$ is. Although $r_b$ is a much more frequent qualifier relation in WD50K, it does not necessar-ily mean it should be equally important to other qualifier relations in each case."}, {"title": "5.9 Relation update different with conventional KGs", "content": "The representations of relation get updated after that of the nodes (entity embedding). Unlike node representations, not a large number of relations are typically involved in hyper-relational KG settings. Instead of regular linear updates, we propose updat-ing relations by leveraging the structural informa-tion in the hyper-relational KGs. Since qualifiers carry relations that also belong to relation set R. Intuitively, co-occurrence data might be beneficial. In the case of WD50k_100, the experiment shows that neglecting relation co-occurrences (in the ab-lation study the model variance ReSaE w/o coo) while performing relation update results in a 2% drop in link prediction performance."}, {"title": "5.10 Analysis of attention mechanism", "content": "Since there is much to explore regarding the at-tention mechanism, we also conduct experiments on several self-attention model variants on the WD50k_100 dataset. We experiment with linear projection and multi-head module while calculat-ing attention score just as the classical transformer does. Nevertheless, we do not notice obvious per-formance differences. We argue that the most likely"}, {"title": "5.11 Importance of decoder and readout", "content": "The choice of decoder is crucial for the link predic-tion task. We also examine different decoder vari-ants, such as the MLP decoder and the GAT (Velick-ovic et al., 2018) decoder. For the MLP decoder, we simply concatenate the representations of all elemental nodes and pass them through a fully con-nected layer. For the GAT decoder, we introduced a virtual readout node that was connected to all other nodes. We conduct these variant experiments on WD50k_100 while maintaining the same training settings as our main approach. Surprisingly, ac-cording to the results shown in Table 3, neither the GAT decoder nor the MLP decoder(MEAN pool) improved the LP performance.\nAdditionally, we test the transformer decoder with and without positional encoding and token type ids encoding. Model variant with positional encoding demonstrates a performance decline. We believe that the ReSaE encoder already enhances graph representations with structural and charac-ter features, making it redundant to add positional information during the decoding process.\nFurthermore, we believe that for downstream tasks, permutation invariance should still be main-tained. ReSaE readout maintains permutation in-variance by not adding positional encoding and em-ploying a type-wise pooling strategy, and indeed, we observe performance increases in our experi-ments.\nThroughout our experiments, we have managed to achieve a satisfactory result: the decoder section remains straightforward and elegant, while the ro-bust encoder ensures that the hyper-relational KG representations are rich in information for down-stream tasks."}, {"title": "6 Conclusion", "content": "In this paper, we present ReSaE, a message-passing framework for learning hyper-relational Knowl-edge Graphs (KGs). ReSaE leverages self-attention during message passing and co-occurrence infor-mation when updating relation representations. Ad-ditionally, ReSaE offers a straightforward yet effec-tive decoder readout solution for hyper-relational KG link prediction tasks. The network layers are designed to be straightforward and are not con-strained by data structure settings. Experimental results extensively show that, compared to existing GNN-based approaches, our method achieves state-of-the-art performance on multiple benchmarks. Furthermore, we provide detailed analysis of dif-ferent module parts, explaining our intuition and presenting relevant experimental results to validate our design. In future work, we intend to investi-gate more advanced learning techniques that can effectively capture the sparse connections between qualifier relations and entities."}, {"title": "Appendix", "content": "Implementation details: We implement all experi-ments at dim size 200 for all model variants. We employ Adam (Kingma and Ba, 2015) optimizer with learning rate at 0.0001. For datasets with a higher proportion of hyper-relational facts, we uti-lize more training epochs. We conduct 500 training epochs for WD50K_100 and WD50K_66. For the remaining datasets, we use 400 training epochs. The message passing infrastructure is implemented with PyTorch Geometric (Fey and Lenssen, 2019). All training was conducted on a Tesla V100. all model instance with parameter size smaller than 12.9M. the longest training time is about 5 days on WikiPeople.\nThe detail hyper-parameters setting are as fol-lowing table:"}]}