{"title": "Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model", "authors": ["Kun Zhao", "Jakub Prokop", "Javier Montalt Tordera", "Sadegh Mohammadi"], "abstract": "Mammography is crucial for breast cancer surveillance and early diagnosis. However, analyzing mammography images is a demanding task for radiologists, who often review hundreds of mammograms daily, leading to overdiagnosis and overtreatment. Computer-Aided Diagnosis (CAD) systems have been developed to assist in this process, but their capabilities, particularly in lesion segmentation, remained limited. With the contemporary advances in deep learning their performance may be improved. Recently, vision-language diffusion models emerged, demonstrating outstanding performance in image generation and transferability to various downstream tasks. We aim to harness their capabilities for breast lesion segmentation in a panoptic setting, which encompasses both semantic and instance-level predictions. Specifically, we propose leveraging pretrained features from a Stable Diffusion model as inputs to a state-of-the-art panoptic segmentation architecture, resulting in accurate delineation of individual breast lesions. To bridge the gap between natural and medical imaging domains, we incorporated a mammography-specific MAM-E diffusion model and BiomedCLIP image and text encoders into this framework. We evaluated our approach on two recently published mammography datasets, CDD-CESM and VinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82 AP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation task, we achieved Dice scores of 38.86 and 40.92, respectively.", "sections": [{"title": "1 Introduction", "content": "Breast cancer is the most common cancer among women worldwide, with an estimated 2.3 million new cases diagnosed in 2020, accounting for 11.7% of all cancer cases globally [24]. Early detection and accurate diagnosis, followed by timely treatment, are crucial for improving survival rates and patient outcomes. Mammography screening plays a pivotal role in early detection, reducing breast-cancer-related deaths by 15% to 25% [17]. However, interpreting mammography"}, {"title": "2 Materials and methods", "content": "This section outlines our segmentation framework and evaluation procedure."}, {"title": "2.1 Segmentation framework", "content": "In our approach, we directly adopted the ODISE framework, which consists of several modules, as illustrated in Figure 1. First, the implicit captioner encodes an image into a vector of embeddings. These embeddings are then passed as a guidance signal to the text-to-image diffusion model, which extracts a stack of feature maps from the image. The features are then fed into the mask generator module, which detects objects and semantic areas in the image and produces segmentation maps. Finally, the classification head generates the class prediction for each mask. Below, we provide a detailed overview of these modules. In our implementation, we used the publicly available code\u00b9 provided by the ODISE and Mask2Former authors, built on the detectron2 library [25]."}, {"title": "Text-to-image diffusion model.", "content": "To bridge the gap between natural and medical imaging domains, we replaced the Stable Diffusion (SD) feature extractor incorporated into ODISE [26] with a mammography-specific MAM-E model [19]. MAM-E is based on SD architecture, trained on approximately 55,000 healthy mammography images from the OMI-H [7] and VinDr-Mammo [20] datasets, encompassing bilateral craniocaudal (CC) and mediolateral oblique (MLO) mammogram modalities.\nDuring both training and inference we keep this model frozen. First, given an image x, we sample a noisy image xt at time step t = 0 as:\n$x_t = \\sqrt{\\bar{a}_t}x + \\sqrt{1 - \\bar{a}_t}\\epsilon, \\epsilon \\sim N(0, I),$\nwhere ao, ..., at is a pre-defined noise schedule [9] and $\\bar{a}_t = \\Pi_{k=0} a_k$ [26]. Then, we pass xt through the denoising UNet of MAM-E model and extract features from its intermediate layers, similarly to [26]."}, {"title": "3 Results", "content": "We present the results of M-ODISE in semantic and instance segmentation, comparing it to three baselines: Mask2Former (M2F) with Swin-L [16] and Stable Diffusion (SD) [22] backbones, as well as the ODISE model described in [26]. Additionally, we explore the impact of BiomedCLIP and MAM-E on the final model performance, demonstrating how these modules affect ODISE. Table 3 and Table 3 display the evaluation results on the CDD-CESM and VinDr-Mammo datasets, respectively.\nOn the CDD-CESM dataset, M-ODISE marginally outperformed the baselines in RQ, PQ, and AP but lagged behind ODISE with the MAM-E model in SQ and behind both Mask2Former and ODISE with MAM-E in the Dice score. On the VinDr-Mammo dataset, M-ODISE did not surpass ODISE, ODISE with BiomedCLIP, or ODISE with MAM-E. The base ODISE achieved the best results in RQ, PQ, and Dice coefficient, ODISE with BiomedCLIP led in SQ, and ODISE with the MAM-E model had the highest AP value. On both datasets, Mask2Former with the SD backbone performed significantly worse than the other models across all metrics.\nVisual prediction examples are provided in Figure 2. M-ODISE successfully detected architectural distortion (top) and heterogeneously enhancing masses (bottom). In both cases, the model localized abnormal areas but struggled to differentiate between individual lesions. While its predictions included false positive lesions, the semantic heatmaps indicate that these false positives have lower probabilities compared to the true positives."}, {"title": "4 Discussion", "content": "The results in Table 3 and Table 3 present ambiguous findings, showing that M-ODISE usually exceeds other ODISE variants in CDD-CESM segmentation by a small margin, but underperforms on VinDr-Mammo dataset. These observations suggest that the advantages of domain adaptations of foundational models for medical tasks are not definitive and may greatly depend on the specific model architecture, as evidenced in medical image classification studies [11].\nIn most scenarios, ODISE and M-ODISE outperform the base Mask2Former in instance segmentation in terms of AP, underscoring the potential of diffusion-based segmentation models. However, performance declines when the text encoder is omitted from the pipeline, as demonstrated by Mask2Former using the SD feature extractor. A plausible explanation for this phenomenon is proposed in [26], which notes that removing the supervision from label-text encoding may hinder the model's classification abilities.\nIn semantic segmentation, the advantage of diffusion-based segmentation models over the Mask2Former baseline is unclear, as their performance lags slightly behind in the case of CDD-CESM data. However, with the"}]}