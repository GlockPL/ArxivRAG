{"title": "Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model", "authors": ["Kun Zhao", "Jakub Prokop", "Javier Montalt Tordera", "Sadegh Mohammadi"], "abstract": "Mammography is crucial for breast cancer surveillance and early diagnosis. However, analyzing mammography images is a demanding task for radiologists, who often review hundreds of mammograms daily, leading to overdiagnosis and overtreatment. Computer-Aided Diagnosis (CAD) systems have been developed to assist in this process, but their capabilities, particularly in lesion segmentation, remained limited. With the contemporary advances in deep learning their performance may be improved. Recently, vision-language diffusion models emerged, demonstrating outstanding performance in image generation and transferability to various downstream tasks. We aim to harness their capabilities for breast lesion segmentation in a panoptic setting, which encompasses both semantic and instance-level predictions. Specifically, we propose leveraging pretrained features from a Stable Diffusion model as inputs to a state-of-the-art panoptic segmentation architecture, resulting in accurate delineation of individual breast lesions. To bridge the gap between natural and medical imaging domains, we incorporated a mammography-specific MAM-E diffusion model and BiomedCLIP image and text encoders into this framework. We evaluated our approach on two recently published mammography datasets, CDD-CESM and VinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82 AP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation task, we achieved Dice scores of 38.86 and 40.92, respectively.", "sections": [{"title": "1 Introduction", "content": "Breast cancer is the most common cancer among women worldwide, with an estimated 2.3 million new cases diagnosed in 2020, accounting for 11.7% of all cancer cases globally [24]. Early detection and accurate diagnosis, followed by timely treatment, are crucial for improving survival rates and patient outcomes. Mammography screening plays a pivotal role in early detection, reducing breast-cancer-related deaths by 15% to 25% [17]. However, interpreting mammography images is difficult and tedious, prone to human error [6], often resulting in false positive diagnoses and unnecessary biopsies [17, 24].\nComputer-Aided Diagnosis (CAD) systems have been developed to enhance the diagnostic process [15]. These systems integrate detection and analysis capabilities, enabling them to identify and delineate abnormal tissue regions. Additionally, they assist radiologists by classifying suspicious lesions and offering valuable \"second opinions\", thus supporting more informed clinical decision-making [6, 8, 15].\nHowever, their performance still lags behind human expertise [18]. Mammography image segmentation is particularly challenging due to the complexity and variability of breast tissue patterns, especially in dense breasts, which makes it difficult to distinguish between normal and abnormal areas [1, 8, 18]. Diagnosing foci and non-mass-like enhancing lesions is particularly challenging due to their small size and unclear boundaries [6]. Current research primarily focuses on analyzing masses and micro-calcifications [18], often overlooking other abnormalities such as architectural distortions, non-mass enhancements, and asymmetries. This challenge is further compounded by low contrast levels and high noise, which can obscure lesions [1].\nTo address these issues, mammogram segmentation research has been exploring two main directions: providing global information about malignant tissues through semantic segmentation [10, 27] and offering a more fine-grained diagnosis of individual lesions through instance segmentation [2, 3, 23]. A significant shortcoming of previous work is the consideration of semantic and instance segmentation as separate tasks. This underscores the importance of developing a unified framework that addresses both tasks simultaneously, enabling the integration of global information and detailed lesion-specific insights, ultimately enhancing diagnostic accuracy and effectiveness.\nPanoptic segmentation is an emerging image analysis method that unifies semantic and instance segmentation to provide a comprehensive understanding of visual scenes [14]. This integration allows for more detailed and accurate image analysis, making it particularly useful for complex scenes where distinguishing between overlapping objects is crucial. By combining the strengths of both segmentation methods, panoptic segmentation offers a holistic view that significantly enhances the accuracy and reliability of image interpretation in various applications, including medical imaging [4].\nThe rise of Diffusion Models [5, 22] and their integration into visual-language models promise further advancements. These architectures enhance robust feature extraction and rich semantic understanding, improving the ability to distinguish complex objects and patterns [13]. A relevant example of this approach is ODISE (Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models) [26], which utilizes the Stable Diffusion model to integrate visual and textual information, achieving comprehensive and precise segmentation based on textual descriptions.\nInspired by ODISE [26], we propose M-ODISE (Mammography Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion"}, {"title": "2 Materials and methods", "content": "This section outlines our segmentation framework and evaluation procedure."}, {"title": "2.1 Segmentation framework", "content": "In our approach, we directly adopted the ODISE framework, which consists of several modules, as illustrated in Figure 1. First, the implicit captioner encodes an image into a vector of embeddings. These embeddings are then passed as a guidance signal to the text-to-image diffusion model, which extracts a stack of feature maps from the image. The features are then fed into the mask generator module, which detects objects and semantic areas in the image and produces segmentation maps. Finally, the classification head generates the class prediction for each mask. Below, we provide a detailed overview of these modules. In our implementation, we used the publicly available code\u00b9 provided by the ODISE and Mask2Former authors, built on the detectron2 library [25].\nText-to-image diffusion model. To bridge the gap between natural and medical imaging domains, we replaced the Stable Diffusion (SD) feature extractor incorporated into ODISE [26] with a mammography-specific MAM-E model [19]. MAM-E is based on SD architecture, trained on approximately 55,000 healthy mammography images from the OMI-H [7] and VinDr-Mammo [20] datasets, encompassing bilateral craniocaudal (CC) and mediolateral oblique (MLO) mammogram modalities.\nDuring both training and inference we keep this model frozen. First, given an image x, we sample a noisy image xt at time step t = 0 as:\n$$x_t = \\sqrt{\\bar{a}_t}x + \\sqrt{1 - \\bar{a}_t}\\epsilon, \\epsilon \\sim N(0, I),$$\nwhere $a_0, ..., a_t$ is a pre-defined noise schedule [9] and $\\bar{a}_t = \\Pi_{k=0}^t a_k$ [26]. Then, we pass $x_t$ through the denoising UNet of MAM-E model and extract features from its intermediate layers, similarly to [26]."}, {"title": "2.3 Evaluation metrics", "content": "We evaluated the model in both instance and semantic segmentation. For semantic segmentation, the model's effectiveness was measured using the Dice coefficient, averaged across classes.\nIn instance segmentation, performance is typically reported as the mean average precision (AP) with an intersection over union (IoU) threshold of 0.5 [25]. However, breast lesion masks vary significantly in size and shape, and radiologists often disagree on the precise ground-truth assignment [28]. Given"}, {"title": "3 Results", "content": "We present the results of M-ODISE in semantic and instance segmentation, comparing it to three baselines: Mask2Former (M2F) with Swin-L [16] and Stable Diffusion (SD) [22] backbones, as well as the ODISE model described in [26]. Additionally, we explore the impact of BiomedCLIP and MAM-E on the final model performance, demonstrating how these modules affect ODISE. Table 3 and Table 3 display the evaluation results on the CDD-CESM and VinDr-Mammo datasets, respectively.\nOn the CDD-CESM dataset, M-ODISE marginally outperformed the baselines in RQ, PQ, and AP but lagged behind ODISE with the MAM-E model in SQ and behind both Mask2Former and ODISE with MAM-E in the Dice score. On the VinDr-Mammo dataset, M-ODISE did not surpass ODISE, ODISE with BiomedCLIP, or ODISE with MAM-E. The base ODISE achieved the best results in RQ, PQ, and Dice coefficient, ODISE with BiomedCLIP led in SQ, and ODISE with the MAM-E model had the highest AP value. On both datasets, Mask2Former with the SD backbone performed significantly worse than the other models across all metrics.\nVisual prediction examples are provided in Figure 2. M-ODISE successfully detected architectural distortion (top) and heterogeneously enhancing masses (bottom). In both cases, the model localized abnormal areas but struggled to differentiate between individual lesions. While its predictions included false positive lesions, the semantic heatmaps indicate that these false positives have lower probabilities compared to the true positives."}, {"title": "4 Discussion", "content": "The results in Table 3 and Table 3 present ambiguous findings, showing that M-ODISE usually exceeds other ODISE variants in CDD-CESM segmentation by a small margin, but underperforms on VinDr-Mammo dataset. These observations suggest that the advantages of domain adaptations of foundational models for medical tasks are not definitive and may greatly depend on the specific model architecture, as evidenced in medical image classification studies [11].\nIn most scenarios, ODISE and M-ODISE outperform the base Mask2Former in instance segmentation in terms of AP, underscoring the potential of diffusion-based segmentation models. However, performance declines when the text encoder is omitted from the pipeline, as demonstrated by Mask2Former using the SD feature extractor. A plausible explanation for this phenomenon is proposed in [26], which notes that removing the supervision from label-text encoding may hinder the model's classification abilities.\nIn semantic segmentation, the advantage of diffusion-based segmentation models over the Mask2Former baseline is unclear, as their performance lags slightly behind in the case of CDD-CESM data. However, with the VinDr-Mammo images, the ODISE architectures prove to be significantly more effective, without compromising their instance segmentation capabilities.\nTo our knowledge, we are the first to conduct joint semantic and instance segmentation on a variety of breast lesions. Previous works by Bhatti et al. [3] and Ahmed et al. [2] attempted similar instance segmentation tasks, reporting AP values of 0.84 and 0.75-0.80, respectively. Although their metric definitions differ from ours [25], it is also possible that the panoptic approach does not yet match the performance of specialized instance segmentation models in this domain. Similarly, the semantic segmentation capabilities of Mask2Former, ODISE, and M-ODISE lag behind those of architectures dedicated to specific semantic tasks, which often achieve over 0.90 Dice scores in narrowly defined lesion segmentation problems [1]. Considering this, we hypothesize that further technical advancements are needed to tailor current panoptic segmentation techniques to the specifics of mammogram analysis.\nA limiting factor in our work, and breast cancer research in general, is the scarcity of mammography data [8]. Out of 20,000 images in VinDr-Mammo, the largest public mammography dataset, only 1,768 images have bounding box annotations, which are limited to malignant or suspicious areas, excluding benign tumors and other breast tissues [20].\nMoreover, the available masks are not biopsy-confirmed, and the radiologist-subjective delineation of individual lesions poses a challenge for instance segmentation, as seen in Figure 2. Finally, differences in annotation protocols hinder training on multiple data sources, negatively impacting the final model performance and applicability in diverse, real-world scenarios. The development of new, large-scale, public mammography datasets in the future could mitigate these limitations.\nIn this study, we proposed M-ODISE, a panoptic segmentation framework for the delineation of breast tumors in mammography. Performance results were mixed. Additional data and future work could help clarify the potential utility of panoptic segmentation in this clinical task."}]}