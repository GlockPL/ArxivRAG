{"title": "Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph", "authors": ["Heloisa Oss Boll", "Ali Amirahmadi", "Amira Soliman", "Stefan Byttner", "Mariana Recamonde-Mendoza"], "abstract": "Objective: In modern healthcare, accurately predicting diseases is a crucial matter. This study introduces a novel approach using graph neural networks (GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure (HF) on a patient similarity graph at the next hospital visit.\nMaterials and Methods: We used electronic health records (EHR) from the MIMIC-III dataset and applied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity graph using embeddings from diagnoses, procedures, and medications. Three models Graph-SAGE, Graph Attention Network (GAT), and Graph Transformer (GT) were implemented to predict HF incidence. Model performance was evaluated using F1 score, AUROC, and AUPRC metrics, and results were compared against baseline algorithms. An interpretability analysis was performed to understand the model's decision-making process.\nResults: The GT model demonstrated the best performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the Random Forest (RF) baseline achieved a similar AUPRC value, the GT model offered enhanced interpretability due to the use of patient relationships in the graph structure. A joint analysis of attention weights, graph connectivity, and clinical features provided insight into model predictions across different classification groups.\nDiscussion and Conclusion: Graph-based approaches such as GNNs provide an effective framework for predicting HF. By leveraging a patient similarity graph, GNNs can capture complex relationships in EHR data, potentially improving prediction accuracy and clinical interpretability.", "sections": [{"title": "1 Introduction", "content": "With the advancement of electronic health record (EHR) systems and artificial intelligence techniques, the healthcare sector has undergone a significant positive transformation [4]. Clinical risk prediction models are increasingly used to identify individualized risks, including complex diseases such as heart failure (HF) [23, 32].\nDeep learning models contribute to an essential part of this success. They can independently identify and extract important features from data, allowing the processing of dense datasets such as EHRs and thus leading to improved predictive performance [29]. For example, DoctorAI and Dipole utilized recurrent neural networks (RNNs) and historical EHR patient data to make predictions [6, 16], while Deepr used a convolutional neural network (CNN) for EHR-based risk stratification [17].\nOne major pitfall is that most of these architectures disregard the relational information underlying EHR data, treating medical information as a flat-structured bag of features [7]. Graph-based deep learning methods, such as graph neural networks (GNNs), have been employed in healthcare and other areas to make predictions on data from non-Euclidean domains [5].\nThis can be achieved by modeling patient data as graphs, where nodes represent clinical entities like patients, diagnoses, or treatments, and edges capture relationships such as co-occurring conditions or shared treatments. For instance, the HarmOnized Representation learning on Dynamic EHR graphs (HORDE) model utilized a multimodal dynamic EHR graph for multiple patient-related tasks, and MedPath aimed to extract personalized subgraphs from medical ontology graphs for individual risk predictions [31].\nAlthough studies on GNNs and EHR graphs have been performed, this area remains largely unexplored, especially considering patient similarity graphs, with few observed works [22, 10, 21, 2, 34, 25, 26]. This article aims to extend current research by predicting HF using a patient similarity graph constructed from medication, diagnosis, and procedure codes. Furthermore, we introduce a graph-based interpretability analysis to understand prediction patterns, helping clinicians make better use of the learned similarity insights.\nSpecifically, our contributions to the field include: (1) developing a novel methodology for constructing patient similarity graphs using dense, pretrained representations of multivariate EHR data; (2) conducting an evaluation and benchmarking of three GNN architectures GraphSAGE, GAT, and GT for HF diagnosis, addressing the limited GNN model comparisons in existing works [26, 25, 21]; (3) performing a detailed ablation study to investigate the relevance of clinical features for HF prediction; and (4) introducing an in-depth interpretability framework analyzing graph descriptive statistics, attention weights, and medical features, expanding on the limited graph interpretability seen in previous studies [34, 10, 22]."}, {"title": "2 Materials and Methods", "content": null}, {"title": "2.1 Data sources", "content": "The study was based on the Medical Information Mart for Intensive Care III dataset, or MIMIC-III [14]. Diagnoses and procedures were encoded with the ICD-9 ontology [19], while medications with NDC. Patient data were processed using Pandas and the PyHealth library [30], which organized the EHR information into a structured dictionary format. We included patients who had at least two hospital visits to allow for the prediction of HF on a subsequent visit, resulting in a final sample of 4,760 patients with 8,891 unique visits. The total number of features was 4788, with 817 diagnosis"}, {"title": "2.2 Patient representation", "content": "We employed pre-trained low-dimensional medical embeddings to create patient representations. These consisted of 300-dimensional vectors for ICD-9 and NDC medication codes, generated with skip-gram [8]. The method captured relationships between medical codes by predicting the likelihood of their co-occurrence within a large corpus of healthcare claims data. The resource is publicly available on GitHub [15].\nWe first extracted the sets of diagnoses, procedures, and medication codes recorded during the patient's hospital visits. These were mapped to their corresponding embeddings, and an average of these embeddings was computed to represent each visit. We then averaged these visit-level embeddings to form a unique representation for each patient (Figure 1). These were utilized to both construct the patient similarity graph and as input features in the predictive models."}, {"title": "2.3 Patient similarity graph", "content": "Similarity between patient feature vectors was quantified using cosine similarity. We applied the K-Nearest Neighbors (KNN) algorithm for a range of K's (2-10) to the similarity matrix to determine the optimal number of edges for each patient node.\nK = 3 was chosen based on the distortion metric, which measures the sum of squared distances between each point and its nearest centroid. Consequently, each patient node was linked to its three most similar neighbors in a NetworkX graph [11]. We noted, however, that a given node could have more than three neighbors if it was selected as the most similar node by multiple other nodes.\nThe final graph included all 4,760 patients, with each node carrying the original patient-level embedding as a feature. A subgraph of 200 nodes can be found in (Figure 2)."}, {"title": "2.4 Model architectures and implementation", "content": "We selected GraphSAGE (SAGE) [12], which learns node representations by sampling and aggregating features from their local neighborhoods; Graph Attention Network (GAT) [28], which introduces attention to weigh the importance of neighboring nodes for a given node's new representation; and Graph Transformer (GT) [24], based on a more advanced attention mechanism.\nThese were implemented with PyTorch Geometric (PyG) [9] and trained to perform binary node classification at the threshold of 0.5. Batch normalization was utilized to stabilize learning.\nAll experiments were repeated thrice over the same split and conducted on an Nvidia RTX 6000 GPU. Hyperparameter optimization was performed with Optuna and Weights & Bias [1, 3]. Early stopping was incorporated to prevent overfitting. All code is available on https://github.com/hossboll/patient-gnn."}, {"title": "2.5 Evaluation", "content": "The F1 score was selected as the primary metric for both GNN optimization and evaluation, as it balances precision and recall. Models were selected based on the highest F1 scores over the validation set. Other evaluation metrics include the Area Under the Precision-Recall Curve (AUPRC), Area Under the Receiver Operating Characteristic Curve (AUROC), Accuracy (\u0410\u0441\u0441), Balanced Accuracy (Bal. Acc), Precision (Prec; also referred to as positive predictive value (PPV)), and Recall (Rec; also referred to as sensitivity).\nFor benchmarking, we compared the performance of the best GNN model, the GT, with hyperparameter-tuned Random Forest (RF), K-Nearest Neighbors (KNN), Logistic Regression (LR), Gradient Boosting Trees (GBT), and a deep Multilayer Perceptron (MLP), implemented using Scikit-learn [20]. To ensure consistency, we first identified the patient nodes masked in the training, validation, and test sets within the similarity graph. We then used their corresponding node features as inputs in the baseline models."}, {"title": "2.6 Interpretability", "content": "To interpret the prediction patterns of the GT model, we examined three axes: graph connectivity patterns, attention weights, and clinical features within the patient similarity graph across the four classification groups true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). After, we performed an integrative analysis over four random instances, one from each group."}, {"title": "3 Quantitative results", "content": null}, {"title": "3.1 GNN architecture performance", "content": "First, we aimed to investigate which GNN architecture performed best in predicting HF. The GT model achieved the highest F1 test score (0.5328), although GraphSAGE showed the highest AUPRC (0.5476) (Table 2). Confusion matrices and AUROC/AUPRC curves detail the predictions, indicating the GT's improved ability to identify positive cases (Figure 3)."}, {"title": "3.2 Impact of clinical data", "content": "To evaluate the impact of each data type on HF prediction, we retrained the GT with FL model using only medication, procedure, or diagnosis data. Medication data alone resulted in the highest recall, followed by diagnosis, with procedures having the least impact. The use of all three data types achieved the best performance. Details are available in the Supplementary Materials.\nNext, we conducted an ablation study by removing one data source at a time. The results confirmed that excluding medication data led to the most significant performance drop, followed by diagnosis data. Removing procedure data had the least impact. The combined model achieved the highest F1 score (0.5361) and AUPRC (0.5227), highlighting the importance of integrating multiple data sources. A summary is provided in Table 3."}, {"title": "3.3 Benchmarking", "content": "We compared the performance of the GT with FL model against five baseline algorithms. The GT with FL model demonstrated an increased test AUROC (0.7925) and AUPRC (0.5168) compared to others (Table 4, Figure 4). Although the differences in AUPRC between GT and Random Forest"}, {"title": "3.4 Interpretability Results", "content": null}, {"title": "3.5 Graph descriptive statistics", "content": "Our analysis focused on node degree and node similarity. TN and FP nodes exhibited the highest average degrees, indicating more diverse connections, while FN nodes had the fewest connections, suggesting that these HF patient profiles are more unique. Detailed metrics are available in the Supplementary Materials."}, {"title": "3.6 Attention weights", "content": "Attention weights, learned during training, highlight the importance of neighboring nodes' features for classifying a target node. We observed a bimodal distribution of weights in the final GT layer, indicating that the model assigned either high or low importance to neighbors.\nFurthermore, TP and FP nodes, as well as TN and FN nodes, exhibited similar attention patterns. TN nodes assigned higher attention to other negative neighbors, helping with the correct classification, while TP nodes showed a more balanced attention across neighbor types. FN nodes resemble TN patterns but with slightly more attention to positive neighbors, indicating challenges in correct classification. Further details are available in the Supplementary Materials."}, {"title": "3.7 Clinical features", "content": "The clinical feature analysis refers to diagnosis, procedure, and prescription codes across classification profiles in the test set. The original patient data links to the embeddings used as node features in the similarity graph (see Figure 1). Figure 5 shows a heatmap of the top 50 most frequent codes. The most important codes are translated in the main text and full code lookup is available in the Supplementary Materials, Table 12.\nDiagnoses: The ICD-9 code for unspecified essential hypertension (4019) was the most prevalent across all profiles, especially in TP and FN, highlighting its association with HF. Atherosclerotic heart disease (41401) was also frequent in both TP and FP, indicating its significance as a HF marker but also its potential for leading to misclassifications. Atrial fibrillation (42731) was frequently observed in TP, FN, and FP profiles, indicating comorbidity. The higher frequency of chronic airway obstruction (496) and other respiratory-related diagnoses in FN suggests that patients with these complex comorbidities may be underdiagnosed for HF.\nProcedures: Common critical care procedures such as endotracheal intubation (9604), mechanical ventilation (9672), and venous catheterization (3893) were commonly observed across all profiles. TP and FP profiles showed a higher occurrence of heart-related procedures, such as coronary artery bypass (3615) and coronary arteriography (8856), suggesting their importance in HF"}, {"title": "3.8 Integrative analysis", "content": "A visualization of the immediate neighbors and the neighbors of neighbors (one and two-hop) of the four randomly selected nodes, along with the attention maps, is available in Figure 6.\nTrue negative (TN): The patient had strong similarities with neighboring negative patients, with uniformly high attention weights. The group shared non-cardiac conditions, such as persistent postoperative fistula, metabolic imbalances (e.g., acidosis), and chronic liver and kidney conditions. The shared procedures and medications, including long-term insulin therapy, exploratory laparotomy, and nystatin use, further reinforced the correct classification, as the patient showed a disease profile distinct from HF patients. Still, the model assigned a relatively high probability (0.4429) to the classification. This, combined with the presence of a few positive nodes within two hops, may indicate that the patient might have subtle HF risk factors.\nTrue positive (TP): The patient had a profile resembling its positive neighboring nodes, all of whom were characterized by chronic cardiovascular diseases, diabetes (a common HF comorbidity), and ulcers. Shared features included advanced atherosclerosis, atrial fibrillation, and chronic kidney disease (another comorbidity) alongside procedures such as vascular bypass surgeries, arteriography, and toe amputation. Regarding medications, insulin and oxycodone were prescribed. The model assigned minimal attention to the neighbors, relying predominantly on the patient's own features, likely due to the strong signals provided by their HF-related features.\nFalse negative (FN): The patient had a neighborhood consisting entirely of TN patients who shared conditions such as severe infections and cancer. They had more unique diagnoses, including septicemia and breast cancer, diverging from the typical HF profile. Neighbor procedures like breast lesion excision and cancer-related surgeries also confirmed this divergence. Additionally, the patient's medications were focused on antibiotics. Even with the low attention weights assigned to these neighbors, the model was unable to avoid misclassification. Thus, the patient's distinct profile probably caused the model to classify them as negative. This finding suggests a potential but less frequent connection between these conditions and HF.\nFalse positive (FP): The patient had a clinical profile similar to that of real HF patients. They shared cardiovascular conditions such as coronary atherosclerosis and coronary artery bypass surgery. That, combined with high attention weights assigned to positive neighbors, contributed to the misclassification. The patient's diagnoses and procedures, which included essential hypertension, atrial fibrillation, and multiple cardiovascular surgeries, reinforced this profile. Misclassifications like this one could highlight individuals at a high risk of developing the disease."}, {"title": "4 Discussion", "content": "When comparing GNN architectures, the models demonstrated varying performances, with the Graph Transformer (GT) performing the best. This may be attributed to its advanced attention mechanism based on queries, keys, and values [27]. Although GraphSAGE also showed a high AUPRC, this could be due to higher precision; however, precision is less critical than the higher recall shown by GT, as our scenario crucially requires identifying minority, HF-positive instances. Furthermore, all models benefited from loss functions adapted to class-imbalanced problems, such as focal loss and weighted cross-entropy, indicating they are indeed relevant for learning in unbalanced graphs.\nThrough the ablation study, prescription codes were found to be the most relevant class for correct predictions. This may be due to the higher proportion of medication data compared to procedures and diagnoses in the patient representations. In addition, these codes have been kept in the NDC standard, a choice that is often rare in the literature since these are often converted to more popular drug ontologies such as ATC. Our experiments suggest that using the full, raw NDC codes may offer advantages in terms of granularity for identifying different disease profiles.\nIn benchmarking, GT demonstrated superior F1 and AUROC scores. Nonetheless, similar AUPRC values for GT (0.5168) and RF (0.5132) suggest that RF, with appropriate threshold tuning, could also yield a higher F1 and serve as a resource-efficient alternative. However, GT's graph-based approach provides advantages in uncovering subgroup-specific interactions, which RF does not capture, as it is restricted to the \"bag-of-features\" framework, limiting the obtention of relational insights.\nFinally, the interpretability framework we introduced, based on graph descriptive statistics, attention weights, and clinical features, confirms the relevance of using graphs for healthcare tasks. By examining patient neighborhoods in the similarity graph, we were able to show that each classified patient requires careful analysis of its attributes, a process that may lead to uncovering clusters of high-risk patients or novel disease paths. Furthermore, this framework further reinforces that predictions in the healthcare field are inherently complex, far from simple (multi)classification scenarios."}, {"title": "5 Limitations", "content": "Important limitations should be noted. First, the data resource was restricted to MIMIC-III, and data from other hospitals should also be evaluated in future studies. Furthermore, the single train-validate-test split may introduce selection bias, although experiments were repeated three times to help mitigate this. However, this limitation may be due to the emerging nature of the field of graph deep learning, as performing k-fold cross-validation in transductive settings is often not feasible due to library constraints.\nSecond, the use of ICD codes for HF labels introduces potential inaccuracies, as these codes may not fully capture the clinical nuances of HF. While we adhered to the official guidelines from the New York State's Department of Health, exploring other cohort identification methods could further improve label accuracy.\nMoreover, as highlighted in prior studies [13], the use of attention as an interpretability mechanism requires further investigation. Furthermore, inter-layer analyses should also be performed.\nFinally, reliance on a fixed threshold for calculating metrics like F1 is also a limitation. Future work could focus on optimizing AUROC for GNNs and baselines to enhance performance across varying thresholds."}, {"title": "6 Conclusion", "content": "The present study compared three GNN architectures (GraphSAGE, GAT, and GT) for predicting heart failure in an imbalanced patient similarity graph. The GT model, combined with focal loss, demonstrated the best performance. Through clinical feature ablation, medications were identified as the most relevant features. While GT's performance was comparable to Random Forests, its capacity to analyze relational data brought advantages for understanding the predictions. Furthermore, the graph interpretability analysis highlighted the importance of examining individual predictions, which may enable the identification of patients at high risk for developing heart failure and reveal novel patient profiles that may be less commonly associated with the disease.\nFuture work could investigate alternative graph representations, such as dynamic or heterogeneous graphs, as well as the potential of inductive graph learning to generalize predictions to new patients. Further enhancing the interpretability and explainability of GNNs with other axes is crucial for their integration into real-world clinical workflows. Moreover, optimizing decision thresholds could improve model performance and evaluation, particularly in scenarios with imbalanced datasets. Expanding the range of patient multimodal data, such as incorporating imaging and clinical notes while minimizing bias, will be essential to building more robust and reliable predictive models for healthcare applications."}]}