{"title": "DEEP COMPRESSION AUTOENCODER FOR\nEFFICIENT HIGH-RESOLUTION DIFFUSION MODELS", "authors": ["Junyu Chen", "Han Cai", "Junsong Chen", "Enze Xie", "Shang Yang", "Haotian Tang", "Muyang Li", "Yao Lu", "Song Han"], "abstract": "We present Deep Compression Autoencoder (DC-AE), a new family of autoen-\ncoders for accelerating high-resolution diffusion models. Existing autoencoders\nhave demonstrated impressive results at a moderate spatial compression ratio (e.g.,\n8\u00d7), but fail to maintain satisfactory reconstruction accuracy for high spatial com-\npression ratios (e.g., 64\u00d7). We address this challenge by introducing two key tech-\nniques: (1) Residual Autoencoding, where we design our models to learn resid-\nuals based on the space-to-channel transformed features to alleviate the optimiza-\ntion difficulty of high spatial-compression autoencoders; (2) Decoupled High-\nResolution Adaptation, an efficient decoupled three-phase training strategy for\nmitigating the generalization penalty of high spatial-compression autoencoders.\nWith these designs, we improve the autoencoder's spatial compression ratio up to\n128 while maintaining the reconstruction quality. Applying our DC-AE to latent\ndiffusion models, we achieve significant speedup without accuracy drop. For ex-\nample, on ImageNet 512 \u00d7 512, our DC-AE provides 19.1\u00d7 inference speedup\nand 17.9x training speedup on H100 GPU for UViT-H while achieving a better\nFID, compared with the widely used SD-VAE-f8 autoencoder.", "sections": [{"title": "INTRODUCTION", "content": "Latent diffusion models (Rombach et al., 2022) have emerged as a leading framework and demon-\nstrated great success in image synthesis (Labs, 2024; Esser et al., 2024). They employ an autoen-\ncoder to project the images to the latent space to reduce the cost of diffusion models. For example,\nthe predominantly adopted solution in current latent diffusion models (Rombach et al., 2022; Labs,\n2024; Esser et al., 2024; Chen et al., 2024b;a) is to use an autoencoder with a spatial compression\nratio of 8 (denoted as f8), which converts images of spatial size $H \\times W$ to latent features of spatial\nsize $H/8 \\times W/8$. This spatial compression ratio is satisfactory for low-resolution image synthesis (e.g.,\n256 \u00d7 256). However, for high-resolution image synthesis (e.g., 1024 \u00d7 1024), further increasing\nthe spatial compression ratio is critical, especially for diffusion transformer models (Peebles & Xie,\n2023; Bao et al., 2023) that have quadratic computational complexity to the number of tokens.\n\nThe current common practice for further reducing the spatial size is downsampling on the diffusion\nmodel side. For example, in diffusion transformer models (Peebles & Xie, 2023; Bao et al., 2023),\nthis is achieved by using a patch embedding layer with patch size p that compresses the latent\nfeatures to $\\frac{H}{8p} \\times \\frac{W}{8p}$ tokens. In contrast, little effort has been made on the autoencoder side. The main\nbottleneck hindering the employment of high spatial-compression autoencoders is the reconstruction\naccuracy drop. For example, Figure 2 (a) shows the reconstruction results of SD-VAE (Rombach\net al., 2022) on ImageNet 256 \u00d7 256 with different spatial compression ratios. We can see that the\nrFID (reconstruction FID) degrades from 0.90 to 28.3 if switching from f8 to f64.\n\nThis work presents Deep Compression Autoencoder (DC-AE), a new family of high spatial-\ncompression autoencoders for efficient high-resolution image synthesis. By analyzing the un-\nderlying source of the accuracy degradation between high spatial-compression and low spatial-"}, {"title": "RELATED WORK", "content": "Autoencoder for Diffusion Models. Training and evaluating diffusion models directly in high-\nresolution pixel space results in prohibitive computational costs. To address this issue, Rombach\net al. (2022) proposes latent diffusion models that operate in a compressed latent space produced"}, {"title": "METHOD", "content": "In this section, we first analyze why existing high spatial-compression autoencoders (e.g., SD-VAE-\nf64) fail to match the accuracy of low spatial-compression autoencoders (e.g., SD-VAE-f8). Then\nwe introduce our Deep Compression Autoencoder (DC-AE) with Residual Autoencoding and De-\ncoupled High-Resolution Adaptation to close the accuracy gap. Finally, we discuss the applications\nof our DC-AE to latent diffusion models.\n\nWe conduct ablation study experiments to get insights into the underlying source of the accuracy\ngap between high spatial-compression and low spatial-compression autoencoders. Specifically, we\nconsider three settings with gradually increased spatial compression ratio, from f8 to f64.\n\nEach time the spatial compression ratio increases, we stack additional encoder and decoder stages\nupon the current autoencoder. In this way, high spatial-compression autoencoders contain low\nspatial-compression autoencoders as sub-networks and thus have higher learning capacity."}, {"title": "DEEP COMPRESSION AUTOENCODER", "content": "Residual Autoencoding. Motivated by the analysis, we introduce Residual Autoencoding to ad-\ndress the accuracy gap. The general idea is depicted in Figure 4. The core difference from the\nconventional design is that we explicitly let neural network modules learn the downsample residu-\nals based on the space-to-channel operation to alleviate the optimization difficulty. Different from\nResNet (He et al., 2016), the residual here is not identity mapping, but space-to-channel mapping.\n\nIn practice, this is implemented by adding extra non-parametric shortcuts on the encoder's down-\nsample blocks and decoder's upsample blocks (Figure 4 b, left). Specifically, for the downsample\nblock, the non-parametric shortcut is a space-to-channel operation followed by a non-parametric\nchannel averaging operation to match the channel number. For example, assuming the downsample\nblock's input feature map shape is $H \\times W \\times C$ and its output feature map shape is $\\frac{H}{2} \\times \\frac{W}{2} \\times C'$,\nthen the added shortcut is:\n\n$H \\times W \\times C \\xrightarrow{\\text{space-to-channel}} \\frac{H}{2} \\frac{W}{2} \\times 4C$\n\nsplit into two groups $\\xrightarrow{}[\\frac{H}{2} \\frac{W}{2} \\times \\times 2C, \\frac{H}{2} \\frac{W}{2} \\times \\times 2C] \\xrightarrow{\\text{channel averaging}}\\frac{H}{2} \\frac{W}{2} \\times 2C$.\n\nAccordingly, for the upsample block, the non-parametric shortcut is a channel-to-space operation\nfollowed by a non-parametric channel duplicating operation:\n\n$\\frac{H}{2} \\frac{W}{2} \\times 2C \\xrightarrow{\\text{channel-to-space}} H \\times W \\times \\frac{C}{2'}$\n\nduplicate, concat $[H \\times W \\times \\frac{C}{2}, H \\times W \\times \\frac{C}{2}]\\xrightarrow{\\text{channel duplicating}} H \\times W \\times C$."}, {"title": "EXPERIMENTS", "content": "Implementation Details. We use a mixture of datasets to train autoencoders (baselines and\nDC-AE), containing ImageNet (Deng et al., 2009), SAM (Kirillov et al., 2023), Mapillary Vistas\n(Neuhold et al., 2017), and FFHQ (Karras et al., 2019). For ImageNet experiments, we exclusively\nuse the ImageNet training split to train autoencoders and diffusion models. The model architecture\nis similar to SD-VAE (Rombach et al., 2022) except for our new designs discussed in Section 3.2.\nIn addition, we use the original autoencoders instead of the variational autoencoders for our models,\nas they perform the same in our experiments and the original autoencoders are simpler. We also\nreplace transformer blocks with EfficientViT blocks (Cai et al., 2023) to make autoencoders more\nfriendly for handling high-resolution images while maintaining similar accuracy.\n\nFor image generation experiments, we apply autoencoders to diffusion transformer models including\nDiT (Peebles & Xie, 2023) and UViT (Bao et al., 2023). We follow the same training settings as\nthe original papers. We consider three settings with different resolutions, including ImageNet (Deng\net al., 2009) for 512 \u00d7 512 generation, FFHQ (Karras et al., 2019) and MJHQ (Li et al., 2024a) for\n1024 \u00d7 1024 generation, and Mapillary Vistas (Neuhold et al., 2017) for 2048 \u00d7 2048 generation.\n\nEfficiency Profiling. We profile the training and inference throughput on the H100 GPU with\nPyTorch and TensorRT respectively. The latency is measured on the 3090 GPU with batch size 2.\nThe training memory is profiled using PyTorch, assuming a batch size of 256. We use fp16 for all\ncases. For simplicity, we assume the number of sampling steps is 1."}, {"title": "IMAGE COMPRESSION AND RECONSTRUCTION", "content": "Table 2 summarizes the results of DC-AE and SD-VAE (Rombach et al., 2022) under various settings\n(f represents the spatial compression ratio and c denotes the number of latent channels). DC-AE\nprovides significant reconstruction accuracy improvements than SD-VAE for all cases. For example,\non ImageNet 512 \u00d7 512, DC-AE improves the rFID from 16.84 to 0.22 for the f64c128 autoencoder\nand 100.74 to 0.23 for the f128c512 autoencoder.\n\nIn addition to the quantitative results, Figure 7 shows image reconstruction samples produced by\nSD-VAE and DC-AE. Reconstructed images by DC-AE demonstrate a better visual quality than\nSD-VAE's reconstructed images. In particular, for the f64 and f128 autoencoders, DC-AE still\nmaintains a good visual quality for small text and the human face."}, {"title": "LATENT DIFFUSION MODELS", "content": "We compare DC-AE with the widely used SD-VAE-f8 autoencoder (Rombach et al., 2022) on vari-\nous diffusion transformer models. For DC-AE, we always use a patch size of 1 (denoted as p1). For\nSD-VAE-f8, we follow the common setting and use a patch size of 2 or 4 (denoted as p2, p4). The\nresults are summarized in Table 3, Table 4, and Table 5."}, {"title": "CONCLUSION", "content": "We accelerate high-resolution diffusion models by designing deep compression autoencoders to\nreduce the number of tokens. We proposed two techniques: residual autoencoding and decoupled\nhigh-resolution adaptation to address the challenges brought by the high compression ratio. The\nresulting new autoencoder family DC-AE demonstrated satisfactory reconstruction accuracy with a\nspatial compression ratio of up to 128. DC-AE also demonstrated significant training and inference\nefficiency improvements when applied to latent diffusion models."}]}