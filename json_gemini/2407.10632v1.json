{"title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model", "authors": ["Zhening Liu", "Xinjie Zhang", "Jiawei Shao", "Zehong Lin", "Jun Zhang"], "abstract": "With the rapid advancement of stereo vision technologies, stereo image compression has emerged as a crucial field that continues to draw significant attention. Previous approaches have primarily employed a unidirectional paradigm, where the compression of one view is dependent on the other, resulting in imbalanced compression. To address this issue, we introduce a symmetric bidirectional stereo image compression architecture, named BiSIC. Specifically, we propose a 3D convolution based codec backbone to capture local features and incorporate bidirectional attention blocks to exploit global features. Moreover, we design a novel cross-dimensional entropy model that integrates various conditioning factors, including the spatial context, channel context, and stereo dependency, to effectively estimate the distribution of latent representations for entropy coding. Extensive experiments demonstrate that our proposed BiSIC outperforms conventional image/video compression standards, as well as state-of-the-art learning-based methods, in terms of both PSNR and MS-SSIM.", "sections": [{"title": "1 Introduction", "content": "Stereo vision, which mimics human binocular vision, has emerged as an important area in computer vision. It enables a wide range of applications, such as 3D movies, remote sensing [33], autonomous driving [42], and augmented/virtual reality (AR/VR) [1]. With the rapid development of stereo cameras, stereo images have become essential for providing immersive and convincing visual presentations that enhance user experience and benefit downstream applications. Meanwhile, the proliferation of stereo cameras leads to a significant surge in the volume of stereo images, resulting in substantial storage costs and posing challenges to transmission efficiency. Consequently, stereo image compression plays a pivotal role in ensuring the practicality and effectiveness of stereo vision."}, {"title": "2 Related Works", "content": "In recent years, numerous methods have been developed for single image compression, including traditional codecs [6,38] and learning-based methods [2-4, 11, 27]. These methods, however, focus on extracting individual features for compression and fail to capture the cross-view correlation. Consequently, they are less effective for stereo image compression that requires the extraction of both intra-view information and inter-view information. To exploit the inherent shared information in stereo images, traditional methods for stereo image coding follow a predictive compression procedure of video standards [9,35]. In this process, one view is compressed first and then used to predict the other view. Subsequently, the disparity between the predicted and actual views is compressed. Nonetheless, these methods typically rely on hand-crafted modules and lack end-to-end optimization, resulting in suboptimal performance.\nInspired by the success of learning-based single image compression, researchers have proposed novel stereo image compression methods using deep neural networks (DNNs) to enhance compression performance [13, 14, 22, 23, 40, 41, 44, 45]. Initially, extensive research focuses on unidirectional learned stereo image compression methods [13, 14, 23, 40, 41, 44], where one view is compressed and used as a reference to compress the other view. Despite achieving high overall rate-distortion (RD) performance, these unidirectional learned compression methods yield a noticeable imbalance in compression quality between stereo views, similar to traditional predictive compression methods. This imbalance is unfavorable for human vision and undermines downstream machine vision tasks [25]. Moreover, some of these methods [13,14,41,44] heavily rely on sequential coding order and accurate estimation of the explicit warping relationship between views. To avoid these issues, a recent study [22] proposes a bidirectional compression method that leverages 2D convolution to extract individual features and facilitate information sharing between views. Nonetheless, the compression performance is limited due to the separate processing of convolutions and the plain spatial context based entropy model.\nBased on the above discussion, we propose a novel bidirectional method for stereo image compression, named BiSIC, to address the aforementioned issues. BiSIC achieves improvements in four key aspects: encoder/decoder structure, mutual stereo learning, entropy model, and coding speed. Firstly, regarding the codec structure, although 2D convolutions have proven effective in various computer vision tasks, they fail to capture the aligned morphs and features between stereo views. To overcome this limitation, we propose using 3D convolution as the backbone for the encoder/decoder, which takes both views as input to capture the inter-view dependencies that are inaccessible in previous works based on 2D convolution. Secondly, for mutual stereo learning, we propose a mutual attention block to facilitate the transfer of features between views. This module enhances the integration and cooperation of the stereo views, leading to improved compression performance. Thirdly, for the entropy model, prior methods primarily use hyperprior and spatial context as conditions, overlooking the rich inter-view dependencies across other dimensions. To exploit these dependencies,"}, {"title": "3 Proposed Methods", "content": "The proposed architecture for stereo image compression, as depicted in Fig. 1, consists of two main components: a joint codec and an entropy model. The codec employs 3D convolutions to transform the original stereo images into compact latent representations, effectively exploiting the correlation between the two views. Subsequently, the entropy model estimates the probability distribution of these latent representations, enabling the generation of a compact bitstream through entropy coding. In the following subsections, we elaborate on the codec and the entropy model."}, {"title": "3.1 Joint Codec", "content": "3D Convolution Backbone. The codec acts as a non-linear transformation that downsamples the raw images into compact latent representations and generates hyperpriors for the entropy model. Different from previous approaches to stereo image compression that employ 2D convolutional layers, we adopt 3D convolutions as the backbone of our codec. Unlike 2D convolution, which operates separately on each view, 3D convolution concurrently processes both stereo images. Consequently, it inherently possesses the capability to extract inter-view correlations.\nOur encoder, denoted by $E$, comprises four 3D convolutional layers, with bidirectional mutual attention blocks inserted after the second and the fourth layers to facilitate global feature communication between the two views. The encoder transforms a pair of stereo images, i.e., the left and right images denoted by $x_l, x_r \\in \\mathbb{R}^{B \\times 3 \\times H \\times W}$, into compact latent representations $y_l, y_r \\in \\mathbb{R}^{B \\times N \\times \\frac{H}{16} \\times \\frac{W}{16}$ and quantizes them into $\\hat{y} := {\\hat{y}_l, \\hat{y}_r}$ for efficient transmission. Conversely, the decoder $D$ transforms the quantized latents $\\hat{y}$ back into images $\\hat{x}_l$ and $\\hat{x}_r$. The encoding and decoding processes are expressed as:\n$y_l, y_r = E(x_l, x_r), $\tlabel{eq:1}\n$\\hat{x}_l, \\hat{x}_r = D(\\hat{y}_l, \\hat{y}_r). $\tlabel{eq:2}"}, {"title": "Bidirectional Mutual Attention Block", "content": "Convolutional layers have an excellent ability for local modeling, but their ability for long-range feature modeling is relatively limited. On the other hand, attention modules [36] excel at extracting long-range features. Therefore, we propose combining the attention mechanism with convolutional layers to compensate for the shortcomings of the latter and leverage the advantages of both convolution and attention. To this end, we design a novel bidirectional mutual attention block, as depicted in Fig. 2, to be inserted between convolutional layers as the pink blocks shown in Fig. 1.\nAs illustrated in Fig. 2, the bidirectional mutual attention block contains two cross-attention stages, each followed by a self-attention layer. The two cross-attention stages differ in how they concentrate on the feature map and are referred to as cross-key attention and cross-query attention. Before each cross-attention stage, the input stereo data $y_l, y_r$ first pass through a residual block, which are then embedded into low-dimensional query, key, and value tensors $Q, K, V \\in \\mathbb{R}^{B \\times C \\times (H \\times W)}$. To avoid the prohibitive computation of classical attention, which yields an $(H \\times W) \\times (H \\times W)$ sized attention map, we adopt the efficient attention approach proposed in [31]. This approach computes the product of the key and the transposed value, thereby reducing the attention map size to $C_K \\times C_V$.\nIn the cross-key attention stage, for each view, we generate an attention map using its key and the value from the other view, which is then queried by the current view. By doing so, we can extract the cross-key feature from the stereo views as:\n$\\Phi_{r\\rightarrow l} = (\\sigma(K_r) \\times V_l^T)^T \\times \\sigma(Q_l), \\Phi_{l\\rightarrow r} = (\\sigma(K_l) \\times V_r^T)^T \\times \\sigma(Q_r), $\tlabel{eq:5}\nwhere $\\sigma$ is the softmax function. This approach calculates the attention map using the key and value from different views, thereby facilitating inter-view alignment and the identification of common patterns. When a feature in one view is correlated with the corresponding feature in the other view, it is effectively captured through the attention map.\nIn the cross-query attention stage, the attention map is obtained using the key and value from a single view, and then queried by the other view, yielding the cross-query feature as:\n$\\Psi_{r\\rightarrow l} = (\\sigma(K_l) \\times V_l^T)^T \\times \\sigma(Q_r), \\Psi_{l\\rightarrow r} = (\\sigma(K_r) \\times V_r^T)^T \\times \\sigma(Q_l). $\tlabel{eq:6}"}, {"title": "3.2 Cross-Dimensional Entropy Model", "content": "The proposed codec effectively transforms stereo images into small-sized latent representations. To achieve efficient compression, the next crucial step is to encode these latents into compact bitstreams using accurate probability distribution and entropy coding [4,27]. Thus, an entropy model is critical for parameterizing and estimating probabilities. In learned image compression, the probability distribution is commonly modeled as Gaussian distributions [27], with the mean $\\mu$ and variance $\\sigma^2$ being estimated. To achieve accurate estimation, it is critical to provide appropriate references. Apart from hyperpriors, previous stereo image compression methods primarily use spatial context [14,22,45] and unidirectional stereo dependency as references [23,40], while abundant features on the"}, {"title": "3.3 Loss Function", "content": "The proposed stereo image compression network is trained using the rate-distortion loss. The rate is defined by bit-per-pixel (BPP), which includes bits for transmitting latents $\\hat{y}_l, \\hat{y}_r$ and hyperpriors $\\hat{z}_l, \\hat{z}_r$. The distortion is measured using the mean squared error (MSE) and the multi-scale structural similarity index measure (MS-SSIM) [39]. The overall loss function is given by:\n$\\mathcal{L} := \\lambda D + R = \\sum_{l,r} d(x, \\hat{x}) + \\sum_{l,r} (r(y)+r(z)), $\tlabel{eq:10}\nwhere $d(\\cdot)$ denotes the distortion function with MSE or MS-SSIM, $r(\\cdot)$ calculates the bitrate using the estimated probability of $y$ and $z$ [4], and $\\lambda$ is a hyperparameter that balances the rate and distortion."}, {"title": "3.4 Fast Variant", "content": "The proposed entropy model effectively captures multiple references and estimates the probability distribution. However, the auto-regressive iterations in"}, {"title": "4 Experiments", "content": "We evaluate the performance of our methods on two benchmark datasets: InStereo2K [5] and Cityscapes [12]. InStereo2K contains 2,060 indoor scene stereo image pairs of size 1,080 \u00d7 860. It is split into 2,010 image pairs for training and 50 for testing. Cityscapes contains 5,000 outdoor scene stereo image pairs of size 2,048 \u00d7 1,024, where 2,975 pairs are allocated for training and 1,525 for testing."}, {"title": "4.2 Results", "content": "RD Performance. Fig. 6 shows the RD curves of different compression methods on the InStereo2K and Cityscapes datasets. In terms of the MS-SSIM metric, our proposed method BiSIC outperforms all the other compression methods on both datasets. Besides, BiSIC achieves higher PSNR in the high BPP range compared to the baselines, while maintaining competitive PSNR values at low bitrates. The BDBR results are presented in Tab. 1, where BPG is set as the baseline. Our BiSIC achieves more than 40% and 50% BDBR reduction on InStereo2K and Cityscapes, respectively. Notably, compared to the state-of-the-art video codec VVC, BiSIC achieves an additional 12.76% BDBR reduction for PSNR and an additional 30.08% BDBR reduction for MS-SSIM on InStereo2K. Compared with unidirectional methods, BiSIC achieves significant bit savings by observing a holistic view and mutually sharing features, which helps remove redundancies in each view. Moreover, compared with the state-of-the-art bidirectional coding scheme BCSIC [22], our method achieves an additional 6.5% to 15% BDBR reduction. This is because the employed 3D convolution captures more correlations than the separately deployed 2D convolution used in BCSIC [22], and our cross-dimensional entropy model effectively aggregates various abundant dependencies from different dimensions, providing accurate probability estimation. In addition, the proposed fast variant also demonstrates satisfactory performance, with a marginal degradation in compression performance compared to our BiSIC."}, {"title": "4.3 Runtime Comparison", "content": "In this part, we evaluate the computational complexity of various compression algorithms. To ensure a fair comparison, each method is executed on an Intel Xeon Platinum 8370C CPU. Tab. 2 reports the average runtime on the InStereo2K test set, which includes the encoding and decoding latency. We observe that our BiSIC-Fast significantly reduces the runtime compared with the proposed BiSIC. This is because we replace the spatial auto-regressive interweaving in the entropy model with a two-fold stereo-checkerboard, which simplifies the dependency structure within the latent representations. Note that BiSIC-Fast still maintains competitive performance, as demonstrated in Tab. 1. Therefore, it achieves a better trade-off between the compression runtime and performance."}, {"title": "4.4 Ablation Study", "content": "We conduct ablation experiments on the InStereo2K dataset to analyze the contribution of the proposed modules. Our model comprises several key components, including the 3D convolution backbone, cross-dimensional entropy model, and mutual attention block. To evaluate their impact, we replace each module with a baseline counterpart or remove it from the model, and compare the performance in terms of PSNR. The experimental results are shown in Fig. 8 and Tab. 3.\nEffectiveness of 3D Convolution Backbone. To assess the capability of 3D convolutional layers in learning inter-view features, we replace 3D convolutional layers in codec with traditional 2D convolutional layers. The resultant model, referred to as Baseline (2D-Conv), experiences a PSNR degradation of 0.3218 dB at the same bitrate, as shown in Fig. 8 and Tab. 3. This indicates that the 3D convolutional layers are more effective at capturing the spatio-stereo information inherent in the data and reducing the redundancies between views.\nEffectiveness of Cross-Dimensional Entropy Model. To validate the performance improvement offered by our proposed cross-dimensional entropy model, we compare it with the joint auto-regressive and hyperprior entropy model [27], which we refer to as Baseline (Minnen). As shown in Fig. 8 and Tab. 3, the"}, {"title": "5 Conclusion", "content": "In this work, we introduce a bidirectional stereo image compression network that employs 3D convolution to encode local stereo features and propose a mutual attention block to capture global features. Besides, we design a symmetric cross-dimensional entropy model that integrates hyperprior, spatial context, channel context, and stereo dependency. Our bidirectional design mitigates the issue of imbalanced compression quality intrinsic to unidirectional methods and eliminates the requirement for sequential coding. As demonstrated in the experimental results, our BiSIC achieves greater bit savings at the same level of compression quality compared to both traditional coding standards and existing learning-based methods. Moreover, we propose a fast variant that significantly reduces the compression runtime while maintaining competitive performance."}, {"title": "Appendix", "content": "The supplementary material provides additional visualization results in Appendix A, extra ablation studies in Appendix B, more experimental details in Appendix C, and discussion about potential extensions in Appendix D."}, {"title": "A Additional Visualization Results", "content": "We visualize the qualitative results in Fig. 9, Fig. 16, and Fig. 17 to show the effectiveness of the proposed method, compared with baselines BPG [6], HEVC [34], VVC [9], SASIC [41], and ECSIC [40]. As shown in Fig. 9, our proposed BiSIC achieves higher PSNR quality with a lower BPP for both left and right views, compared with other methods. Besides, the reconstruction details and texture of BiSIC are closer to the ground truth. Moreover, the image qualities of the left and right views in our bidirectional design remain close, mitigating the imbalance issue in unidirectional methods. In contrast, HEVC and VVC adopt a predictive compression pipeline where one view is compressed normally, and the other view is generated through the disparity between the prediction and the real view. The unidirectional compression results in a 2.265 dB PSNR gap for HEVC and a 1.844 dB PSNR gap for VVC between stereo views, as seen in Fig. 9. ECSIC utilizes the spatial context from the left image to compress the right one, resulting in a higher compression quality of the right image. In Fig. 16, we illustrate another example on InStereo2K, where we can visually observe that the same area appears differently in the left and right views between Fig. 16d and Fig. 16j, as well as Fig. 16e and Fig. 16k, due to the unidirectional compression. Another group of visualization comparisons on Cityscapes is shown in Fig. 17."}, {"title": "A.2 Downstream Task Verification", "content": "The previous subsection provides a visual comparison between our method and the baselines. Note that imbalanced stereo quality is unfavorable for machine vision and downstream tasks [25]. Therefore, it is interesting to investigate the degradation caused by different compression methods. In this subsection, we compare their performance on the stereo matching task. We employ a benchmark stereo matching method [10] on both ground truth stereo image pairs and reconstructed stereo image pairs from various compression methods to illustrate the degradation effect brought by compression.\nThe stereo matching results are visualized in Fig. 10. We calculate the root-mean-square-error (RMSE) [30] to quantify the disparity between the estimations from original images and reconstructed images. As illustrated in Fig. 10a and Fig. 10c, the estimation from the reconstructed results of our proposed BiSIC achieves a nearly identical estimation to the one from the ground truth. Moreover, it achieves the lowest RMSE among others, while requiring the lowest"}, {"title": "A.3 Bit Allocation Visualization", "content": "In this subsection, we examine the effectiveness of the proposed mutual attention blocks on compression performance. Fig. 11 visualizes the bit allocation of latents $y_l$ and $y_r$ in BiSIC with and without the mutual attention blocks. In"}, {"title": "B Extra Ablation Studies", "content": "The ablation studies for the proposed BiSIC method are shown in Sec. 4, which illustrates the impact of each proposed component, including the 3D convolution backbone, cross-dimensional entropy model, and mutual attention block. In this subsection, we provide ablation studies for our fast variant, BiSIC-Fast. Specifically, we investigate the effect of our designed stereo-checkerboard structure and evaluate the significance of channel context. The RD performance is illustrated in Fig. 12, and we also calculate the Bj\u00f8ntegaard Delta PSNR (BD-PSNR) [8] for comparison.\nEffectiveness of Stereo-Checkerboard. The stereo-checkerboard structure enables joint learning from both views, aided by 3D convolution. To illustrate the effect of the stereo-checkerboard structure, we replace it with the vanilla"}, {"title": "Effectiveness of Channel Context.", "content": "To evaluate the contribution of channel context, we remove the slicing process on channel axis and the channel context model. As shown in Fig. 12, channel context contributes to a significant improve-"}, {"title": "B.2 Ablation Study on Number of Slices", "content": "In the channel-wise auto-regressive entropy model, the previously decoded part serves as a condition for the later part. Thus, it is interesting to investigate the relationship between the number of slices, compression performance, and model efficiency. Note that a higher precision in slicing generates abundant conditions, but more slices directly increase the time consumption of compression. This forms a trade-off between compression performance and speed. In this subsection, we conduct an ablation study for our proposed BiSIC on the number of slices $K$ and investigate its effect on the trade-off between performance and efficiency. The RD performance on InStereo2K is shown in Fig. 14, along with several baselines for comparison. The Bj\u00f8ntegaard Delta Bitrate (BDBR) [8] results relative to BPG are shown in Tab. 4. As demonstrated, reducing the number of slices leads to a slight decrease in the RD performance and accelerates the encoding/decoding process. Specifically, when $K = 6$, the time consumption is reduced by 42%, while the RD performance experiences a degradation of 3.19%. Nonetheless, it still outperforms other baselines, as shown in Fig. 14."}, {"title": "B.3 Ablation Study on ELIC Backbone", "content": "In our work, we employ 3D convolutional layers as the backbone of the codec. In Section 4.4 of the main body of the paper, we have provided ablation study compared with plain 2D convolution backbone baseline. Here, we provide the"}, {"title": "C Experimental Details", "content": "In this section, we provide more details about the neural network architectures and the process of model training."}, {"title": "Details of Entropy Model.", "content": "The proposed cross-dimensional entropy model aggregates the hyperprior, spatial context, channel context, and stereo dependency to estimate the probability distributions of the compact latents. Let $G$ denote the network and $\\theta$ denote its learnable parameters. The hyperprior, channel context model, and spatial context model are formulated as follows:\n$\\hat{z}_l, \\hat{z}_r = h_s (z_l, z_r), $\tlabel{eq:11}\n$\\Theta_l, \\Theta_r = G_{ch}(y_{l<k}, y_{r<k}; \\theta_{ch}), $\tlabel{eq:12}\n$\\Upsilon_l, \\Upsilon_r = G_{sp} (\\hat{y}_{l,<i}, \\hat{y}_{r,<i}; \\theta_{sp}). $\tlabel{eq:13}\nThe hyperprior dependency is obtained through a hyper decoder, which is also depicted in Fig. 1 and Eq. (4) in the main text. The channel context model $G_{ch}$ comprises four convolutional layers with 1 \u00d7 1 kernel size, and it produces the channel dependency feature with 128 channels. The spatial context model $G_{sp}$ is obtained through one layer of masked 3D convolution, which is illustrated in"}, {"title": "Details of Stereo-Checkerboard.", "content": "The proposed fast variant relies on the stereo-checkerboard structure, which transforms the entry-by-entry auto-regressive process into a two-fold operation. Specifically, the stereo views are split into two parts: stereo anchor part $\\hat{y}_{ach}$ and stereo non-anchor part $\\hat{y}_{nac}$, as shown in Fig. 5 in the main body. The anchor part is encoded/decoded with a hyperprior and the channel-wise condition, where the estimations of mean $\\mu_{ach}$ and variance $\\sigma^2_{ach}$ for stereo anchor part are formulated as:\n$\\mu_{l, ach}, \\sigma^2_{l,ach} = G_{ag-ach}(\\hat{z}_{l}, \\Theta_{l}; \\theta_{ag-ach}), $\tlabel{eq:18}\n$\\mu_{r, anc}, \\sigma^2_{r,anc} = G_{ag-ach}(\\hat{z}_{r}, \\Theta_{r}; \\theta_{ag-ach}). $\tlabel{eq:19}\nThen, with the existing anchor part, we obtain the anchor context feature $\\Upsilon_{ach}$ using 3D convolution as:\n$\\Upsilon_{ach} = G_{ach} (\\hat{y}_{ach}; \\theta_{ach}). $\tlabel{eq:20}\nNotably, $G_{ach}$ is an ordinary 3D convolutional layer with a kernel size of (3, 5, 5), as the whole stereo anchor part has been obtained and the non-anchor entries are set to zero, eliminating the need for a mask. The anchor context feature $\\Upsilon_{ach}$ serves as a reference for the stereo non-anchor part. Thus, the mean $\\mu_{nac}$ and variance $\\sigma^2_{nac}$ of the stereo non-anchor part are estimated by:\n$\\mu_{l, nac}, \\sigma^2_{l,nac} = G_{ag-nac}(\\hat{z}_{l}, \\Theta_{l}, \\Upsilon_{l, ach}; \\theta_{ag-nac}), $\tlabel{eq:21}\n$\\mu_{r, nac}, \\sigma^2_{r,nac} = G_{ag-nac}(\\hat{z}_{r}, \\Theta_{r}, \\Upsilon_{r, ach}; \\theta_{ag-nac}), $\tlabel{eq:22}\nwhere the aggregation networks $G_{ag-ach}$ and $G_{ag-nac}$ consist of four convolutional layers with 1 \u00d7 1 kernels."}]}