{"title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model", "authors": ["Zhening Liu", "Xinjie Zhang", "Jiawei Shao", "Zehong Lin", "Jun Zhang"], "abstract": "With the rapid advancement of stereo vision technologies, stereo image compression has emerged as a crucial field that continues to draw significant attention. Previous approaches have primarily employed a unidirectional paradigm, where the compression of one view is dependent on the other, resulting in imbalanced compression. To address this issue, we introduce a symmetric bidirectional stereo image compression architecture, named BiSIC. Specifically, we propose a 3D convolution based codec backbone to capture local features and incorporate bidirectional attention blocks to exploit global features. Moreover, we design a novel cross-dimensional entropy model that integrates various conditioning factors, including the spatial context, channel context, and stereo dependency, to effectively estimate the distribution of latent representations for entropy coding. Extensive experiments demonstrate that our proposed BiSIC outperforms conventional image/video compression standards, as well as state-of-the-art learning-based methods, in terms of both PSNR and MS-SSIM.", "sections": [{"title": "1 Introduction", "content": "Stereo vision, which mimics human binocular vision, has emerged as an important area in computer vision. It enables a wide range of applications, such as 3D movies, remote sensing [33], autonomous driving [42], and augmented/virtual reality (AR/VR) [1]. With the rapid development of stereo cameras, stereo images have become essential for providing immersive and convincing visual presentations that enhance user experience and benefit downstream applications. Meanwhile, the proliferation of stereo cameras leads to a significant surge in the volume of stereo images, resulting in substantial storage costs and posing challenges to transmission efficiency. Consequently, stereo image compression plays a pivotal role in ensuring the practicality and effectiveness of stereo vision."}, {"title": "2 Related Works", "content": "In recent years, numerous methods have been developed for single image compression, including traditional codecs [6,38] and learning-based methods [2-4, 11, 27]. These methods, however, focus on extracting individual features for compression and fail to capture the cross-view correlation. Consequently, they are less effective for stereo image compression that requires the extraction of both intra-view information and inter-view information. To exploit the inherent shared information in stereo images, traditional methods for stereo image coding follow a predictive compression procedure of video standards [9,35]. In this process, one view is compressed first and then used to predict the other view. Subsequently, the disparity between the predicted and actual views is compressed. Nonetheless, these methods typically rely on hand-crafted modules and lack end-to-end optimization, resulting in suboptimal performance.\nInspired by the success of learning-based single image compression, researchers have proposed novel stereo image compression methods using deep neural networks (DNNs) to enhance compression performance [13, 14, 22, 23, 40, 41, 44, 45]. Initially, extensive research focuses on unidirectional learned stereo image compression methods [13, 14, 23, 40, 41, 44], where one view is compressed and used as a reference to compress the other view. Despite achieving high overall rate-distortion (RD) performance, these unidirectional learned compression methods yield a noticeable imbalance in compression quality between stereo views, similar to traditional predictive compression methods. This imbalance is unfavorable for human vision and undermines downstream machine vision tasks [25]. Moreover, some of these methods [13,14,41,44] heavily rely on sequential coding order and accurate estimation of the explicit warping relationship between views. To avoid these issues, a recent study [22] proposes a bidirectional compression method that leverages 2D convolution to extract individual features and facilitate information sharing between views. Nonetheless, the compression performance is limited due to the separate processing of convolutions and the plain spatial context based entropy model.\nBased on the above discussion, we propose a novel bidirectional method for stereo image compression, named BiSIC, to address the aforementioned issues. BiSIC achieves improvements in four key aspects: encoder/decoder structure, mutual stereo learning, entropy model, and coding speed. Firstly, regarding the codec structure, although 2D convolutions have proven effective in various computer vision tasks, they fail to capture the aligned morphs and features between stereo views. To overcome this limitation, we propose using 3D convolution as the backbone for the encoder/decoder, which takes both views as input to capture the inter-view dependencies that are inaccessible in previous works based on 2D convolution. Secondly, for mutual stereo learning, we propose a mutual attention block to facilitate the transfer of features between views. This module enhances the integration and cooperation of the stereo views, leading to improved compression performance. Thirdly, for the entropy model, prior methods primarily use hyperprior and spatial context as conditions, overlooking the rich inter-view dependencies across other dimensions. To exploit these dependencies,"}, {"title": "3 Proposed Methods", "content": "The proposed architecture for stereo image compression, as depicted in Fig. 1, consists of two main components: a joint codec and an entropy model. The codec employs 3D convolutions to transform the original stereo images into compact latent representations, effectively exploiting the correlation between the two views. Subsequently, the entropy model estimates the probability distribution of these latent representations, enabling the generation of a compact bitstream through entropy coding. In the following subsections, we elaborate on the codec and the entropy model."}, {"title": "3.1 Joint Codec", "content": "3D Convolution Backbone. The codec acts as a non-linear transformation that downsamples the raw images into compact latent representations and generates hyperpriors for the entropy model. Different from previous approaches to stereo image compression that employ 2D convolutional layers, we adopt 3D convolutions as the backbone of our codec. Unlike 2D convolution, which operates separately on each view, 3D convolution concurrently processes both stereo images. Consequently, it inherently possesses the capability to extract inter-view correlations.\nOur encoder, denoted by E, comprises four 3D convolutional layers, with bidirectional mutual attention blocks inserted after the second and the fourth layers to facilitate global feature communication between the two views. The encoder transforms a pair of stereo images, i.e., the left and right images denoted by \\(x_l, x_r \\in \\mathbb{R}^{B \\times 3 \\times H \\times W}\\), into compact latent representations \\(y_l, y_r \\in \\mathbb{R}^{B \\times N \\times \\frac{H}{16} \\times \\frac{W}{16}}\\) and quantizes them into \\(\\hat{y} := {\\hat{y}_l, \\hat{y}_r}\\) for efficient transmission. Conversely, the decoder D transforms the quantized latents \\(\\hat{y}\\) back into images \\(\\hat{x}_l\\) and \\(\\hat{x}_r\\). The encoding and decoding processes are expressed as:\n\\[y_l, y_r = E(x_l, x_r), \\tag{1}\\]\n\\[\\hat{x}_l, \\hat{x}_r = D(\\hat{y}_l, \\hat{y}_r). \\tag{2}\\]\nTo capture the dependencies among elements in the latents \\(\\hat{y}\\) for efficient entropy coding [4,7], hyperpriors are generated through another 3D convolution based hyper encoder \\(h_a\\). The generated hyperpriors \\(z_l, z_r \\in \\mathbb{R}^{B \\times M \\times \\frac{H}{64} \\times \\frac{W}{64}}\\) are quantized into \\(\\hat{z}_l, \\hat{z}_r\\) and then transmitted alongside the quantized latents \\(\\hat{y}\\). At the decoder side, a 3D convolution based hyper decoder \\(h_s\\) upscales the quantized hyperpriors \\(\\tilde{z}_l, \\tilde{z}_r\\) into \\(\\bar{z}_l, \\bar{z}_r\\), which are subsequently utilized in the entropy model, as elaborated in Sec. 3.2. The processes of the hyper encoder and hyper decoder are expressed as:\n\\[z_l, z_r = h_a(y_l, y_r), \\tag{3}\\]\n\\[\\bar{z}_l, \\bar{z}_r = h_s(\\tilde{z}_l, \\tilde{z}_r). \\tag{4}\\]\nBidirectional Mutual Attention Block. Convolutional layers have an excellent ability for local modeling, but their ability for long-range feature modeling is relatively limited. On the other hand, attention modules [36] excel at extracting long-range features. Therefore, we propose combining the attention mechanism with convolutional layers to compensate for the shortcomings of the latter and leverage the advantages of both convolution and attention. To this end, we design a novel bidirectional mutual attention block, as depicted in Fig. 2, to be inserted between convolutional layers as the pink blocks shown in Fig. 1.\nAs illustrated in Fig. 2, the bidirectional mutual attention block contains two cross-attention stages, each followed by a self-attention layer. The two cross-attention stages differ in how they concentrate on the feature map and are referred to as cross-key attention and cross-query attention. Before each cross-attention stage, the input stereo data \\(y_l, y_r\\), first pass through a residual block, which are then embedded into low-dimensional query, key, and value tensors \\(Q, K, V \\in \\mathbb{R}^{B \\times C \\times (H \\times W)}\\). To avoid the prohibitive computation of classical attention, which yields an \\((H \\times W) \\times (H \\times W)\\) sized attention map, we adopt the efficient attention approach proposed in [31]. This approach computes the product of the key and the transposed value, thereby reducing the attention map size to \\(C_K \\times C_V\\).\nIn the cross-key attention stage, for each view, we generate an attention map using its key and the value from the other view, which is then queried by the current view. By doing so, we can extract the cross-key feature from the stereo views as:\n\\[\\Phi_{r \\rightarrow l} = (\\sigma(K_r) \\times V_l^T) \\times \\sigma(Q_l), \\quad \\Phi_{l \\rightarrow r} = (\\sigma(K_l) \\times V_r^T) \\times \\sigma(Q_r), \\tag{5}\\]\nwhere \\(\\sigma\\) is the softmax function. This approach calculates the attention map using the key and value from different views, thereby facilitating inter-view alignment and the identification of common patterns. When a feature in one view is correlated with the corresponding feature in the other view, it is effectively captured through the attention map.\nIn the cross-query attention stage, the attention map is obtained using the key and value from a single view, and then queried by the other view, yielding the cross-query feature as:\n\\[\\Psi_{r \\rightarrow l} = (\\sigma(K_l) \\times V_l^T)^T \\times \\sigma(Q_r), \\quad \\Psi_{l \\rightarrow r} = (\\sigma(K_r) \\times V_r^T)^T \\times \\sigma(Q_l). \\tag{6}\\]\nThe attention map in this stage is generated within one view, thus maintaining an inner-focused relationship. This process preserves the individual self-features of each view, while the other view serves as a reference for determining the allocation of attention weights.\nAfter obtaining the cross-key attention and cross-query attention features \\(\\Phi, \\Psi\\), we employ self-attention to augment these features as \\(\\tilde{\\Phi}, \\tilde{\\Psi}\\) and use a shared-parameter combine block to get the desired mutual attention output, as shown in the bottom right of Fig. 2. Specifically, the combine block concatenates the augmented features \\(\\tilde{\\Phi}, \\tilde{\\Psi}\\) and the residual component, and shrinks the channel number to be consistent with the block input.\nThis bidirectional mutual attention block captures diverse features and provides a global receptive field for the codec. It is suitable for any stereo information transfer scenarios, provided that the inputs are two features of equal size, making it a generalized plug-and-play block. Moreover, the size of the attention maps is determined only by the embedding channel C and does not vary with the size of input images."}, {"title": "3.2 Cross-Dimensional Entropy Model", "content": "The proposed codec effectively transforms stereo images into small-sized latent representations. To achieve efficient compression, the next crucial step is to encode these latents into compact bitstreams using accurate probability distribution and entropy coding [4,27]. Thus, an entropy model is critical for parameterizing and estimating probabilities. In learned image compression, the probability distribution is commonly modeled as Gaussian distributions [27], with the mean \\(\\mu\\) and variance \\(\\sigma^2\\) being estimated. To achieve accurate estimation, it is critical to provide appropriate references. Apart from hyperpriors, previous stereo image compression methods primarily use spatial context [14,22,45] and unidirectional stereo dependency as references [23,40], while abundant features on the channel dimension are overlooked. In this work, we propose a symmetric cross-dimensional entropy model that integrates hyperprior, spatial context, channel context, and stereo dependency in a fully symmetric bidirectional way.\nThe architecture of the proposed cross-dimensional entropy model is depicted in Fig. 3. We use \\(\\hat{y}_{i,k}^l\\) to denote the \\(i\\)-th (\\(i = 1, 2, ..., n\\)) entry of the \\(k\\)-th (\\(k = 1, 2, ..., K\\)) channel slice from the received left view latent \\(\\hat{y}_l\\), and likewise for the others. The cross-dimensional condition formulation is expressed as:\n\\[P_{y_l} (\\hat{y}_{i,k}^l | \\bar{z}_l, \\hat{y}_{<i,k}^l, \\Theta_l, \\hat{y}_{i,<k}^l, \\bar{\\Theta}_{l,<k}^l) \\sim \\mathcal{N}(\\mu_l, \\sigma_l^2), \\tag{7}\\]\n\\[P_{y_r} (\\hat{y}_{i,k}^r | \\bar{z}_r, \\hat{y}_{<i,k}^r, \\Theta_r, \\hat{y}_{i,<k}^r, \\bar{\\Theta}_{r,<k}^r) \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2). \\tag{8}\\]\nFor the hyperprior component, we generate the dependency \\(\\bar{z}_l, \\bar{z}_r\\) from the hyper decoder using Eq. (4). To obtain the channel context, we evenly split the stereo latents on the channel dimension into K slices. These latent slices share a great similarity, providing abundant conditions [28]. To exploit this similarity, the probability estimation is performed slice by slice, using the previous slices as references for the following one. In addition, the decoded slices of the other view also serve as references. Therefore, we propose to use the mutual attention block introduced in the previous subsection to aggregate the channel-wise features \\(\\Theta_l, \\Theta_r\\) from both views, conditioning on \\(\\hat{y}_{<k}^l\\) and \\(\\hat{y}_{<k}^r\\), as depicted by the yellow lines in Fig. 3.\nTo capture the spatial context within each channel slice, we consistently employ 3D convolution to jointly learn spatial features from the causal areas \\(\\hat{y}_{<i}^l\\). Specifically, during the coding process, the current compression/decompression only accesses the areas that have already been encoded/decoded. To maintain the causality of the compression, we mask out the uncoded part, which is referred to as masked 3D convolution, as shown in Fig. 4a. In this approach, for the \\(i\\)-th entry \\(\\hat{y}_{i,k}^l\\), the weights of uncoded part are set to zero, and we obtain spatial context features \\(\\Upsilon_{i,i}^l, \\Upsilon_{i,i}^r\\) by considering the causal context \\(\\hat{y}_{i,<i}^l\\) and \\(\\hat{y}_{i,<i}^r\\) from both views simultaneously, as depicted by the red lines in Fig. 3.\nUpon extracting various dependency features, we employ an aggregation network \\(G_{ag}\\) to fuse the conditions and obtain the probability estimations as:\n\\[\\mu_l, \\sigma_l^2 = G_{ag}(\\bar{z}_l, \\Theta_l, \\Upsilon_l), \\quad \\mu_r, \\sigma_r^2 = G_{ag}(\\bar{z}_r, \\Theta_r, \\Upsilon_r). \\tag{9}\\]\nThe entire procedure in the entropy model is designed to be fully symmetric and bidirectional, which aligns with our motivation for a bidirectional design."}, {"title": "3.3 Loss Function", "content": "The proposed stereo image compression network is trained using the rate-distortion loss. The rate is defined by bit-per-pixel (BPP), which includes bits for transmitting latents \\(\\hat{y}, \\hat{y}\\) and hyperpriors \\(\\tilde{z}_l, \\tilde{z}_r\\). The distortion is measured using the mean squared error (MSE) and the multi-scale structural similarity index measure (MS-SSIM) [39]. The overall loss function is given by:\n\\[L := \\lambda D + R = \\sum_{l,r} d(x, \\hat{x}) + \\sum_{l,r} (r(y)+r(z)), \\tag{10}\\]\nwhere \\(d(\\cdot)\\) denotes the distortion function with MSE or MS-SSIM, \\(r(\\cdot)\\) calculates the bitrate using the estimated probability of \\(\\hat{y}\\) and \\(\\tilde{z}\\) [4], and \\(\\lambda\\) is a hyperparameter that balances the rate and distortion."}, {"title": "3.4 Fast Variant", "content": "The proposed entropy model effectively captures multiple references and estimates the probability distribution. However, the auto-regressive iterations in the spatial context are time-consuming. To strike a better balance between compression runtime and performance, we need to reduce the auto-regressive executions while maintaining a strong condition for the entropy model. To this end, we propose a fast variant of our model, named BiSIC-Fast, by extending the checkerboard structure [17] into stereo-checkerboard and utilizing 3D convolutions to jointly model the spatial context of two views with only two iterations.\nThis fast variant maintains the codec as BiSIC, while the entropy model network is modified as illustrated in Fig. 5. In this approach, we split the latents \\(y\\) into two parts, stereo anchor part and stereo non-anchor part, as represented in blue and red in Fig. 5, respectively. The entropy parameter estimation for the stereo anchor part relies on the hyperprior condition and channel context. The stereo anchor is decompressed first, and then the stereo non-anchor utilizes the decompressed stereo anchor as a dependency. Consequently, the entropy parameter estimation for the stereo non-anchor part is based on the hyperprior, channel context, and stereo anchor context.\nBy adopting this approach, we avoid the spatial auto-regressive process in favor of a two-fold operation, as demonstrated in Fig. 4b and Fig. 4c. Although the spatial auto-regressive process is omitted to save runtime, the neighbors of each pixel in the latents are still maintained, preserving a good perceptive field."}, {"title": "4 Experiments", "content": "We conduct ablation experiments on the InStereo2K dataset to analyze the contribution of the proposed modules. Our model comprises several key components, including the 3D convolution backbone, cross-dimensional entropy model, and mutual attention block. To evaluate their impact, we replace each module with a baseline counterpart or remove it from the model, and compare the performance in terms of PSNR. The experimental results are shown in Fig. 8 and Tab. 3.\nEffectiveness of 3D Convolution Backbone. To assess the capability of 3D convolutional layers in learning inter-view features, we replace 3D convolutional layers in codec with traditional 2D convolutional layers. The resultant model, referred to as Baseline (2D-Conv), experiences a PSNR degradation of 0.3218 dB at the same bitrate, as shown in Fig. 8 and Tab. 3. This indicates that the 3D convolutional layers are more effective at capturing the spatio-stereo information inherent in the data and reducing the redundancies between views.\nEffectiveness of Cross-Dimensional Entropy Model. To validate the performance improvement offered by our proposed cross-dimensional entropy model, we compare it with the joint auto-regressive and hyperprior entropy model [27], which we refer to as Baseline (Minnen). As shown in Fig. 8 and Tab. 3, the proposed cross-dimensional entropy model achieves better rate-distortion performance than Baseline (Minnen). This suggests that our model provides more accurate probability estimations, which in turn minimizes the coding overhead.\nEffectiveness of Mutual Attention Block. To evaluate the effectiveness of the proposed mutual attention block, we consider three baselines for comparisons, namely Baseline (Ying), Baseline (Lei), and Baseline (w/o Atten). Baseline (Ying) and Baseline (Lei) replace the mutual attention block with the stereo attention module [43] and the contextual transfer module [22], respectively. In particular, Baseline (w/o Atten) represents a variant without the attention module. As shown in Fig. 8 and Tab. 3, our proposed model outperforms all baselines by a large margin. Notably, removing the proposed mutual attention block results in a dramatic degradation, which demonstrates its significance."}, {"title": "5 Conclusion", "content": "In this work, we introduce a bidirectional stereo image compression network that employs 3D convolution to encode local stereo features and propose a mutual attention block to capture global features. Besides, we design a symmetric cross-dimensional entropy model that integrates hyperprior, spatial context, channel context, and stereo dependency. Our bidirectional design mitigates the issue of imbalanced compression quality intrinsic to unidirectional methods and eliminates the requirement for sequential coding. As demonstrated in the experimental results, our BiSIC achieves greater bit savings at the same level of compression quality compared to both traditional coding standards and existing learning-based methods. Moreover, we propose a fast variant that significantly reduces the compression runtime while maintaining competitive performance."}, {"title": "A Additional Visualization Results", "content": "A.1 Qualitative Results\nWe visualize the qualitative results in Fig. 9, Fig. 16, and Fig. 17 to show the effectiveness of the proposed method, compared with baselines BPG [6], HEVC [34], VVC [9], SASIC [41], and ECSIC [40]. As shown in Fig. 9, our proposed BiSIC achieves higher PSNR quality with a lower BPP for both left and right views, compared with other methods. Besides, the reconstruction details and texture of BiSIC are closer to the ground truth. Moreover, the image qualities of the left and right views in our bidirectional design remain close, mitigating the imbalance issue in unidirectional methods. In contrast, HEVC and VVC adopt a predictive compression pipeline where one view is compressed normally, and the other view is generated through the disparity between the prediction and the real view. The unidirectional compression results in a 2.265 dB PSNR gap for HEVC and a 1.844 dB PSNR gap for VVC between stereo views, as seen in Fig. 9. ECSIC utilizes the spatial context from the left image to compress the right one, resulting in a higher compression quality of the right image. In Fig. 16, we illustrate another example on InStereo2K, where we can visually observe that the same area appears differently in the left and right views between Fig. 16d and Fig. 16j, as well as Fig. 16e and Fig. 16k, due to the unidirectional compression. Another group of visualization comparisons on Cityscapes is shown in Fig. 17.\nA.2 Downstream Task Verification\nThe previous subsection provides a visual comparison between our method and the baselines. Note that imbalanced stereo quality is unfavorable for machine vision and downstream tasks [25]. Therefore, it is interesting to investigate the degradation caused by different compression methods. In this subsection, we compare their performance on the stereo matching task. We employ a benchmark stereo matching method [10] on both ground truth stereo image pairs and reconstructed stereo image pairs from various compression methods to illustrate the degradation effect brought by compression.\nThe stereo matching results are visualized in Fig. 10. We calculate the root-mean-square-error (RMSE) [30] to quantify the disparity between the estimations from original images and reconstructed images. As illustrated in Fig. 10a and Fig. 10c, the estimation from the reconstructed results of our proposed BiSIC achieves a nearly identical estimation to the one from the ground truth. Moreover, it achieves the lowest RMSE among others, while requiring the lowest"}, {"title": "B Extra Ablation Studies", "content": "B.1 Ablation Study on BiSIC-Fast\nThe ablation studies for the proposed BiSIC method are shown in Sec. 4, which illustrates the impact of each proposed component, including the 3D convolution backbone, cross-dimensional entropy model, and mutual attention block. In this subsection, we provide ablation studies for our fast variant, BiSIC-Fast. Specifically, we investigate the effect of our designed stereo-checkerboard structure and evaluate the significance of channel context. The RD performance is illustrated in Fig. 12, and we also calculate the Bj\u00f8ntegaard Delta PSNR (BD-PSNR) [8] for comparison.\nEffectiveness of Stereo-Checkerboard. The stereo-checkerboard structure enables joint learning from both views, aided by 3D convolution. To illustrate the effect of the stereo-checkerboard structure, we replace it with the vanilla checkerboard in [17] and present the ablation results in Fig. 12. We observe an RD performance degradation in this baseline compared to our BiSIC-Fast, specifically, with a BD-PSNR of -0.213 dB. This is because the cooperation of the stereo-checkerboard and 3D convolution enables the utilization of references from both self-view and the other view, and thus extracts more information compared with the vanilla checkerboard method, as shown in Fig. 13.\nEffectiveness of Channel Context. To evaluate the contribution of channel context, we remove the slicing process on channel axis and the channel context model. As shown in Fig. 12, channel context contributes to a significant improve-"}, {"title": "C Experimental Details", "content": "In this section, we provide more details about the neural network architectures and the process of model training.\nDetails of Entropy Model. The proposed cross-dimensional entropy model aggregates the hyperprior, spatial context, channel context, and stereo dependency to estimate the probability distributions of the compact latents. Let \\(G\\) denote the network and \\(\\theta\\) denote its learnable parameters. The hyperprior, channel context model, and spatial context model are formulated as follows:\n\\[\\bar{z}_l, \\bar{z}_r = h_s (\\tilde{z}_l, \\tilde{z}_r), \\tag{11}\\]\n\\[\\Theta_l, \\Theta_r = G_{ch}(\\hat{y}_{i,<k}^l, \\hat{y}_{i,<k}^r; \\theta_{ch}), \\tag{12}\\]\n\\[\\Upsilon_l, \\Upsilon_r = G_{sp} (\\hat{y}_{<i,k}^l, \\hat{y}_{<i,k}^r; \\theta_{sp}). \\tag{13}\\]\nThe hyperprior dependency is obtained through a hyper decoder, which is also depicted in Fig. 1 and Eq. (4) in the main text. The channel context model \\(G_{ch}\\) comprises four convolutional layers with 1 \u00d7 1 kernel size, and it produces the channel dependency feature with 128 channels. The spatial context model \\(G_{sp}\\) is obtained through one layer of masked 3D convolution, which is illustrated in"}]}