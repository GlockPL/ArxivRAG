{"title": "Graph Convolutional Neural Networks as Surrogate Models for Climate Simulation", "authors": ["Kevin Potter", "Carianne Martinez", "Reina Pradhan", "Samantha Brozak", "Steven Sleder", "Lauren Wheeler"], "abstract": "Many climate processes are characterized using large systems of nonlinear differential equations; this, along with the immense amount of data required to parameterize complex interactions, means that Earth-System Model (ESM) simulations may take weeks to run on large clusters. Uncertainty quantification may require thousands of runs, making ESM simulations impractical for preliminary assessment. Alternatives may include simplifying the processes in the model, but recent efforts have focused on using machine learning to complement these models or even act as full surrogates. We leverage machine learning, specifically fully-connected neural networks (FCNNs) and graph convolutional neural networks (GCNNs), to enable rapid simulation and uncertainty quantification in order to inform more extensive ESM simulations. Our surrogate simulated 80 years in approximately 310 seconds on a single A100 GPU, compared to weeks for the ESM model while having mean temperature errors below 0.1\u00b0C and maximum errors below 2\u00b0C.", "sections": [{"title": "Introduction", "content": "As global temperatures continue to rise, the need for effective and systematic evaluation of climate intervention strategies becomes increasingly important. Stratospheric Aerosol Injection (SAI) is one such strategy and like all brings significant risks [4, 17] necessitating careful planning and evaluation of the positive and negative impacts. The Performance Assessment (PA) framework, a methodology originally designed for nuclear waste management [13], can be applied to the assessment of climate intervention strategies. The Performance Assessment for Climate Intervention (PACI) framework[19] adapts the PA methodology to evaluate SAI by establishing a set of performance goals, identifying relevant system features, events, and processes (FEPs), and assessing the system's performance, including uncertainties, against these goals.\nThe PACI framework aims to provide a structured and quantifiable approach to evaluate the risks and benefits of SAI in comparison to other climate pathways. Using the GLENS scenario, PACI demonstrates the application of PA to climate intervention by setting performance goals for a range of climate model output variables. The research highlights the importance of developing robust assessment tools to support informed decision-making in the context of climate intervention, while also acknowledging the need for future work to refine and expand the framework to address uncertainties and improve its applicability across different scenarios and regions.\nHowever, a comprehensive PA, that demonstrates that all possible impacts have been investigated requires thousands or more simulation models of not only earth system models but regionally refined simulation runs to properly quantify the uncertainty in the predictions. Traditional ESMs can take several days to weeks to run on a large clusters [1, 18] and quickly become too expensive to provide the necessary runs to address all possible impacts. To that end, simplified models [2, 11, 14] or surrogate models [6, 10] that can give ESM-like results in a shorter time scale (even if at a lower fidelity) are necessary for PA to be successfully applied."}, {"title": "Model architecture", "content": "The application of neural networks (NNs) as surrogate models in climate science offers a promising approach to addressing the computational challenges associated with traditional climate models. These conventional models, while highly detailed and accurate, are computationally intensive and time-consuming to run. NNs, on the other hand, can be trained to emulate the behavior of these models, providing a faster and more efficient means of conducting climate simulations and analyses. By leveraging large datasets and the powerful learning capabilities of NNs, researchers can achieve faster predictions and insights, facilitating more responsive and informed decision-making in climate policy and research. This integration of NNs into climate science not only enhances the efficiency of modeling efforts but also opens new avenues for exploring the dynamics of our changing climate.\nNNs have become a standard approach in the field of artificial intelligence for modeling otherwise difficult-to-capture nonlinear relationships. In the context of climate science, these networks serve as potent surrogate models, capable of approximating complex climate systems with remarkable accuracy. NNs consist of layers of interconnected neurons, where each neuron in one layer is connected to every neuron in the subsequent layer. This architecture allows NNs to capture and learn from the multifaceted interactions within climate data, making them particularly suitable for tasks such as climate prediction, trend analysis, and scenario simulation.\nIn this work, we use fully connected neural networks (FCNNs) and graph convolutional neural networks (GCNNs). FCNNs, sometimes referred to as multi-layer perceptrons (MLPs), consist of multiple layers of neurons that take a fixed size vector as input and output a fixed size vector.\nGCNNs presume a graph structure to the input where the data takes the form of nodes and edges (connections between nodes). Different types of convolutional layers may require additional information, like position or specific edge attributes. GCNN layers use a common set of weights to make predictions for each node utilizing the information from the node, its neighbors, and possibly edge attributes. This is an increase in complexity from FCNNs as now additional information must be tracked in the form of edges and neighbors. However, the improved capability of understanding the local context and resulting accuracy improvements may justify the additional complexity cost."}, {"title": "Fully-connected Neural Network (FCNN)", "content": "A fully-connected neural network is one in which each neuron is connected to all neurons in the previous layer (illustrated in Figure 1), as opposed to a convolutional network where neuron connections are local and thus sparser."}, {"title": "FCNN architecture details", "content": "The FCNN in this work contains four linear layers with 1000 neurons each, uses a ReLU activation function followed by batch normalization layers. Finally, dropout is applied on each layer. Dropout was incorporated into the hidden layers of the network to improve regularization and quantify uncertainty. Dropout is a popular regularization technique to avoid overfitting where neurons in the network are turned off with some probability $0 < p < 1$. Keeping these layers active during inference approximates a Gaussian process [8]. Hence, we can generate an ensemble of predictions and use these predictions to quantify uncertainty. The dropout rate in this work was set to 30%, or $p = 0.3$. During inference, 48 predictions were generated for each point and the uncertainty was characterized using standard deviation of those 48 predictions."}, {"title": "Graph Convolutional Neural Network (GCNN)", "content": "We chose a GCNN approach over a CNN approach for two reasons. First, two-dimensional projections of the Earth are prone to distortions, which could affect the results of more traditional convolutional neural networks. Second, GCNNs allow the east to connect to the west naturally. The graph structure naturally handles Earth's spherical geometry without the same distortions, as shown in the right panel of Figure 2b. Each node of the graph corresponds with a location on the Earth and contains information about the input variables as well as target variables for prediction. Edges connect the nodes, and graph neural networks propagate information along the edges of the graph. Edges were not part of the original data and we created them by connecting each node to its 4 nearest neighbors.\nWe chose a UNet-style [9, 15] for the GCNN architecture (basic example shown in Figure 3). UNet uses successive coarsening operations intermixed with convolutions. This downscaled graph is then upsampled to the same graph layout as prior to downsampling. This allows skip connections between the state of the network prior to coarsening to be passed to the upsampled operation and concatenated. This is hypothesized to allow the network to retain fine details while simultaneously getting information from a larger context."}, {"title": "GCNN architecture details", "content": "The network had 4 downsampling operations using voxel grid pooling for coarsening. At each downsampling, the voxel grid size is doubled (such that we end with a 30 \u00d7 30 \u00d7 30 grid at the deepest layer) and the number of features per node is doubled (starting from 64 features). Each downsample performs the convolutions, a ReLU activation, and then voxel grid pooling. Each upsample performs the convolution, 2 linear (NN layer operating on each node separately) operations with ELU activations [5], and then a KNN interpolation to the upsampled graph with a k of 10. Finally there are 2 convolutions with SeLU activations [12] and a final linear operation to get to the output."}, {"title": "Data and training", "content": "The original GLENS data was converted into csv (comma separated value) files. This allowed easier use in both the FCNN and the GCNN models."}, {"title": "Results", "content": ""}, {"title": "FCNN", "content": "Dropout improved mean absolute error on the test set when compared to the no dropout regime (Table 1). When examining the map projection in Figure 4, the uncertainty qualitatively aligns with the error in the northern hemisphere. However, the uncertainty does not appear to be correlated with the FCNN's error in South America, Africa, and parts of Antarctica. This work did not attempt to calibrate uncertainty estimates as formal uncertainty quantification was out of scope for this project. Dropout provides a quick mechanism to characterize FCNN prediction uncertainty, but future work is needed to understand its reliability for practical use in this domain."}, {"title": "GCNN", "content": "We developed BlockNet, a Python package which streamlines the implementation, training, and testing of a graph convolutional neural network. BlockNet includes data processing and methods for logging the training process. We implemented the graph model using PyTorch-Geometric [7].\nThe GCNN could simulate 80 years in approximately 310 seconds on a single A100 GPU, which is remarkably faster than the typical ESM model which take weeks on a high capacity cluster.\nExtensive testing was done in order to determine the best performing type of layer for the GCNN. Twenty types of layers were tested using the same training procedure described previously, and results for the best nine are shown in Figure 5. The FiLMConv (convolutional feature-wise linear modulation) layer [3] performed the best across most of predicted variables in terms of both MAE and Max AE and remained competitive for those where it was bested.\nVariable importance tests shown in Figure 6 reveal that the variable t affects results the most, which is unsurprising given that t is the only variable that has a relationship with CO2 (At the time of testing, CO2 had not been converted from GLENS). Altitude was the next most important, followed by month which was unexpected given that the month is recoverable from t.\nIn terms of MAE, MARE, MaxAE, and MaxARE, the GCNN performed consistently better than the FCNN as shown in Table 2, with the exception of PRECT in which performance was comparable. The GCNN performance is also visualized in Figure 7 for ALTMAX, ICEFRAC, SNOWHLND, and TSA. The linear trends indicate that the model is able to predict the data accurately. TSA has the least variance (Figure 7d) while ICEFRAC has the most variance (Figure 7b). However, Figure 8a clarifies how the GCNN struggled to predict PRECT. The model is consistently underestimating the true precipitation, and the residuals appear to be heteroskedastic with higher variance at lower levels of precipitation. A histogram of the precipitation predictions also shows that the model does not capture the distribution of the ground truth (Figure 8b)."}, {"title": "Discussion and future work", "content": "We have shown that GCNNs can act as efficient surrogate models for ESMs, enabling more rapid simulations before turning to the higher fidelity models. The GCNN architecture is able to predict thousands of outcomes in the time of a single ESM run. Furthermore, testing was carried out to tune the GCNN and identify the best type of layer for the model as well as important input variables. The GCNN overall outperformed the FCNN and combination networks, but struggled with predicting precipitation, likely due to the scale of the data.\nThe results presented here are for models trained only on GLENS control runs, and future work could incorporate GLENS feedback runs. Additionally, incorporating additional input variables (such as CO2) into the model could improve results or allow the model to react to actual (or hypothetical) CO2 emissions scenarios without requiring additional ESM runs for training data."}]}