{"title": "NEURO-SYMBOLIC RULE LISTS", "authors": ["Sascha Xu", "Nils Philipp Walter", "Jilles Vreeken"], "abstract": "Machine learning models deployed in sensitive areas such as healthcare must be interpretable to ensure accountability and fairness. Rule lists (if Age < 35 \u2227 Priors > 0 then Recidivism = True, else if Next Condition ...) offer full transparency, making them well-suited for high-stakes decisions. However, learning such rule lists presents significant challenges. Existing methods based on combinatorial optimization require feature pre-discretization and impose restrictions on rule size. Neuro-symbolic methods use more scalable continuous optimization yet place similar pre-discretization constraints and suffer from unstable optimization. To address the existing limitations, we introduce NEURULES, an end-to-end trainable model that unifies discretization, rule learning, and rule order into a single differentiable framework. We formulate a continuous relaxation of the rule list learning problem that converges to a strict rule list through temperature annealing. NEURULES learns both the discretizations of individual features, as well as their combination into conjunctive rules without any pre-processing or restrictions. Extensive experiments demonstrate that NEURULES consistently outperforms both combinatorial and neuro-symbolic methods, effectively learning simple and complex rules, as well as their order, across a wide range of datasets.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning models are increasingly used in high-stakes applications such as healthcare (Deo, 2015), credit risk evaluation (Bhatore et al., 2020), and criminal justice (Lakkaraju & Rudin, 2017), where it is vital that each decision is fair and reasonable. Proxy measures such as Shapley values can give the illusion of interpretability, but are highly problematic as they can not faithfully represent a non-additive models decision process (Gosiewska & Biecek, 2019). Instead, Rudin (2019) argues that it is crucial to use inherently interpretable models, to create systems with human supervision in the loop (Kleinberg et al., 2018).\nFor particularly sensitive domains such as stroke prediction or recidivism, so called Rule Lists are a popular choice (Letham et al., 2015) due to their fully transparent decision making. A rule list predicts based on nested \"if-then-else\" statements and naturally aligns with the human-decision making process. Each rule is active if its conditions are met, e.g. \u201cif Thalassemia = normal \u2227 Resting bps < 151\", and carries a respective prediction, i.e. \u201cthen P(Disease) = 10%\". A rule list goes through a set of rules in a fixed order, and makes its prediction using the first applicable rule. We show an example rule list for the Heart disease (Detrano et al., 1989) learned with our method in Figure 1, which is highly accurate and easy to understand.\nInferring rule lists from data is a challenging combinatorial optimization problem, as there are super exponentially many options in the number of features. Existing approaches use greedy heuristics (Proenca & van Leeuwen, 2020), sample based on fixed priors (Yang et al., 2017) and even attempt to find globally optimal solutions (Angelino et al., 2018) using branch-bound. However, they all depend on the pre-discretization of continuous features, which in practice deteriorates their performance. That is, features such as \u201cResting bps\u201d are typically discretized using 2 \u2013 5 fixed thresholds. Increasing the granularity of pre-processing leads to a combinatorial explosion of the search space, which creates issues in scalability for exact methods and in accuracy for heuristics."}, {"title": "PRELIMINARIES", "content": "We consider a dataset of n pairs {(x, y)}n i=1 consisting of the descriptive feature vector x \u2208 X with d real-valued features xi \u2208 R, and the discrete-valued target label y \u2208 Y. We assume each sample (x, y) to be a realization of a pair of random variables (X, Y) \u223c Px,y, drawn iid."}, {"title": "RULES", "content": "We consider predictive rules r : X \u2192 Y for supervised classification, which map input samples to predictions. A rule is comprised of an antecedent a : X \u2192 {0, 1}, which determines whether the rule is applicable to a sample x or not. Should the antecedent's condition be met, the consequent c \u2208 Y governs the models prediction. A predictive rule is defined as\nr(x) = { c if a(x) = 1 else c \nIf the antecedent is not met, an alternate prediction c \u2208 Y is made. In rule lists, which are introduced shortly, the else case is covered by yet another rule.\nThe antecedent of a rule must be interpretable. In particular, we examine rules given in form of a logical conjunction of predicates \u03c0, e.g. the rule \u201cif Thalassemia = normal \u2227 Resting bps < 151\" from the introduction. Each predicate \u03c0i represents the presence/absence of a certain characteristic in an individual x, e.g \u201cResting bps < 151\u201d. For tabular data, where X = Rd, clear and meaningful semantic concepts are usually defined as parameterized thresholding functions on individual feature variable Xi, i.e."}, {"title": "DIFFERENTIABLE RULE LISTS", "content": "In this section we introduce Neuro-Symbolic Rule Lists, or short NEURULES. We show the archi- tecture of NEURULES in Figure 2. The first step is to transform the continuous input features into binary predicates such as \"18 <Age< 30\" to use as a basic building blocks for rule construction. In contrast to all existing methods, NEURULES avoids the need for pre-discretization and instead learns a task specific thresholding of the features.\nNext, we combine the learned predicates into rules a\u02c6j(x), where a\u02c6 is a differentiable function that behaves like a logical conjunction for binary predicates, but is also able to handle soft predicates \u2208 [0, 1]. In particular, our formulation encourages interpretable, succinct rules using weights wj and alleviates the problem of vanishing gradients compared to previous formulations."}, {"title": "CONJUNCTIVE RULES", "content": "We begin with the antecedent a of a rule that makes up the \"if ...\" condition of a rule.\nThresholding Layer. As the building blocks of every rule we consider thresholding functions \u03c0(Xi; \u03b1, \u03b2) = \u03b1 \u2264 x \u2264 \u03b2 on individual features xi as predicates \u03c0. It is common practice, to employ equal-width/-frequency binning for continuous features X i and thus obtain a fixed number of predicates. By increasing the number of bins, the potential solution may become more accurate, but also comes at a much higher optimization difficulty.\nNeural rule learning methods also require to pre-discretize continuous features. since the threshold- ing function is not continous at the bounds \u03b1 and \u03b2, and has gradient of 0 elsewhere. To address these issues, we use the soft binning function as introduced by Yang et al. (2018) as\n\u03c0(x_i; \\alpha, \\beta, t_\\pi) = \\frac{e^{t_\\pi x_i}}{e^{t_\\pi \\alpha} + e^{t_\\pi x_i} + e^{t_\\pi (2x_i - \\alpha - \\beta)}}\nThe resulting function approximates a thresholding function, where the softness of said function is controlled by a temperature parameter t\u03c0. We show using different temperatures t in Figure 3a. Just as the Figure suggests, in the limit t\u2192 0 the soft predicate converges to the true thresholding function \u03c0(xi; \u03b1, \u03b2) (shown in Appendix A.1), i.e.\nlimt\u21920 \u03c0(xi; \u03b1, \u03b2, t) = \u03c0(xi; \u03b1, \u03b2) .\nTherefore, we use temperature annealing to increase crispness of the predicates \u03b1 and \u03b2 as the training progresses. We begin start with a higher temperature t, which results in smoother predicates and avoids exploding/vanishing gradients with respect to \u03b1, \u03b2. That means that initially, our predicates are not strictly binary but soft instead, i.e. \u03c0(xi) \u2208 [0, 1].\nIn the end, we seek to obtain strict logical rules for use in a rule list. Hence, we continuously decrease the temperature t\u03c0 so that in the end \u2200xi: \u03c0(xi) \u2248 1 \u2228 \u03c0(xi) \u2248 0, except at the boundaries itself, where if xi = \u03b1 \u2228 xi = \u03b2 : \u03c0(xi) = 1. We show the softness of rules based on soft predicates in Figure 3a. When the temperature is annealed close to zero, the predicates become increasingly binary and the difference to the true thresholding function vanishes.\nLogical conjunction. To learn rule antecedents, we need to flexibly combine the trainable predicates \u03c0(xi) into a logical conjunction. To this end, we introduce a differentiable logical conjunction"}, {"title": "SOFT RULE LISTS", "content": "A rule list consists of a set of k rule tuples {(aj, cj, Pj)}k j=1 , made up of an antecedent aj : X \u2192 {0, 1}, a consequent cj \u2208 Rl and a unique priority pj \u2208 R+. A sample is classified using the prediction of the rule with highest priority and active antecedent, i.e.\nrl(x) = Cj s.t. aj(x) = 1 \u2227 \u2200i \u2260 j : ai(x) = 0 \u2228 pj > pi . (2)\nTo allow for continuous optimization, we reformulate the rl(x) as a linear combination of conse- quents cj. That is, we combine the antecedent aj and priority pj into the active priority a\u03c0 as\na\u03c0(x) = aj(x) \u00b7 pj.\nThe arg max indicator Ij(x) = [j = arg max a\u03c0 (x)] of the active priority vector a\u03c0 indicates with which rule the prediction is to be made. Hence, we can re-write the rule list simply as rl(x) = \u2211k j=1 Ij(x)cj. The main difference between a boosted rule set and a rule list is the constraint that only one rule can be active per sample, which is the arg max of the active priority. Rule sets are more flexible but also less interpretable as 2k combinations of rules have to be interpreted.\nContinuous Relaxation. To learn a rule list, we initially relax the constraint \u2211k j=1 I j(x) = 1 and hence allow multiple rules to contribute, weighted by their active priority a\u03c0. To this end, we use the Gumbel-Softmax (Jang et al., 2017b), which is a variant of the reparameterization trick and provides a differentiable approximation to the argmax function. Given the active priority a\u03c0 and a temperature trl, the Gumbel-Softmax is defined as\n\\hat{I}_j (x;t_{rl}) = \\text{softmax}(\\frac{a^\\pi + g}{t_{rl}}),\nwhere g \u2208 Rk is random noise sampled from the Gumbel distribution. The Gumbel-Softmax approach interpolates between the strict one-hot encoding of a rule list and a linear combination weighted by the rule priorities. In particular, in the limit trl\u2192 0, the Gumbel-Softmax converges to the arg max function. Thus, the soft rule list is given by\nl(x) = \\sum_{j=1}^k c_j \\cdot \\hat{I}_j (x)."}, {"title": "ENTIRE ARCHITECTURE", "content": "Our model takes as input any real-valued feature vector x \u2208 Rd, where we first one-hot encode the categorical features into binary features. For any instantiation of our model, the number of rules k and the number of classes l is fixed beforehand. We show the resulting architecture in Figure 2.\nFirst, NEURULES discretizes the input features into j \u2208 {1,...,k} sets of d predicates such as \"18 <Age < 30\u201d or \u201cDiabetes = True\u201d. Each set of predicates is then composed into the an- tecedent of rule j, using the respective weights wj \u2208 Rd. The activation vector a\u02c6 \u2208 [0, 1]k represents the activation of rules such as \"if Age < 30 \u2227 Diabetes = True\u201d."}, {"title": "OBJECTIVE", "content": "Lastly, we turn to the learning setup. We train NEURULES in a standard supervised learning setting given an arbitrary loss function l and a sample of the data distribution Px,y. In the following, we use the cross-entropy loss for binary classification tasks, i.e. l(rl(x; \u0398), y) = \u2212y log(rl(x; \u0398)) \u2212 (1 \u2212 y) log(1 \u2212 rl(x; \u0398)), though in principle any differentiable loss function l can be used.\nMinimum-Support Besides the chosen loss function, we propose a regularization term to ensure that each rule represents a non-trivial amount of points. That is, akin to the minimum support requirement in classical rule lists or decision trees, we penalize rules that are never used or used too often. To this end, we add a regularization term based off the rule usage indicator Ij(xi). We compute the coverage, i.e. the fraction of points where each individual rules is active, over the training set {xi}n i=1 as covj = \\frac{1}{n} \\sum_{i=1}^n \\hat{I}_j (x_i). The support regularizer is then given by\nR(\\Theta) = \\sum_{j=1}^k \\max(0, cov_{min} - cov_j)^2 + \\max(0, cov_j - cov_{max})^2,\nwhere covmin and covmax are user-specified minimum and maximum supports. We weight the regularization term using a hyperparameter \u03bb. The overall objective given a training set {(xi, yi)}n i=1 is then to optimize the rule list parameters \u0398 = (\u03b21, \u03b11,..., \u03b2k, \u03b1k, w1,..., wkp) as\n\\arg \\min_{\\Theta} \\sum_{i=1}^n l(r_l(x_i; \\Theta), y_i) + \\lambda R(\\Theta)."}, {"title": "RELATED WORK", "content": "Rule lists were introduced in the early 90s (Cohen, 1995) and have since been used in various applications, such as healthcare (Deo, 2015), criminal justice (Angelino et al., 2018), and credit risk evaluation (Bhatore et al., 2020). Similarly, decision trees (Breiman, 2017), which can also be easily transformed into rules by tracing the path from the root to the leaf, have also been widely used in practice. The approaches use greedy combinatorial optimization to find the best rule set. Instead of relying on the greedy growing of the model, Wang et al. (2017); Yang et al. (2017) propose Bayesian rule lists, a probabilistic model, where the complexity of the model is controlled by a prior, which is specified by the user. In practice, these priors result in short rule lists but can harm the performance if misspecified. To automate the trade-off between complexity and accuracy, Proenca & van Leeuwen (2020); Papagianni & van Leeuwen (2023) propose an MDL-based approach, which uses an MDL- score for model selection. In practice, this results in more accurate and concise rule lists compared to previous approaches. There are also approaches that attempt to find optimal rule lists (Angelino et al., 2018; Aivodji et al., 2022). Due to the expensive combinatorial optimization, exact methods are not applicable beyond trivially sized datasets and have to severely restrict the search space in terms of rule size, feature quantization and number of rules.\nInstead of using combinatorial optimization, neuro-symbolic approaches have been proposed to learn rule classifiers. Qiao et al. (2021) proposes the first approach to learn rule sets in an end-to- end scheme. They formulate a novel neural architecture that uses continuous relaxations of logical operators. After training, the rules are extracted from the network. This is extended by Wang et al. (2021) to a deeper architecture that allows to learn more complicated rules; however, this often results in worse interpretability. Walter et al. (2024) propose to learn rules for binary data that are not only predictive but also descriptive, which improves explainability but reduces accuracy. Dierckx et al. (2023) extend the approach of Qiao et al. (2021) by introducing a hierarchy among the rules, allowing to learn rule lists. Although these methods resolve the scalability issue, they still rely on pre-discretization of the features, similar to the combinatorial approaches. In contrast, NEURULES learns discretizations on the fly while being highly scalable and accurate."}, {"title": "CONCLUSION", "content": "We propose NEURULES, a differentiable relaxation of the rule list learning problem that converges to a strict rule list through temperature annealing. NEURULES learns both the discretizations of indi- vidual features, and how to compile these features into conjunctive rules without any pre-processing or restrictions. We also learn the rule-order differentiably by introducing a priority score that deter- mines the ordering. NEURULES is able to learn rules of any complexity using specifically optimized predicates and order them in a way that maximizes the predictive performance of the model. As a result, we obtain both highly interpretable, but also accurate rule lists that can assist decision making in a wide range of applications. We demonstrated the effectiveness of NEURULES in exten- sive real-world and synthetic experiments. We show that NEURULES consistently outperforms both combinatorial and neuro-symbolic methods on a variety of datasets."}, {"title": "LIMITATIONS", "content": "Whilst NEURULES is a powerful tool for interpretable rule learning, it is not without limitations. First and foremost, the rules that NEURULES learns do not allow to draw any causal conclusions about the data generating process without any further assumptions. Thus, they should only be used to assist in decision making and not as a substitute for domain knowledge. Compared to CORELS, we can not give any optimality guarantees on the learned rule list within the search space, but explore a much larger search space that leads to empirically better results.\nThe number of rules in a NEURULES rule list is fixed and must be set beforehand. In addition, there are hyperparameters and temperature schedules that need to be set. Whilst we have observed a degree of robustness to these hyperparameters, they require tuning for optimal performance. Lastly, the current rule language is limited to logical conjunctions of thresholded features. Adding disjunc- tions and more complex logical predicates would be a natural extension of the current work and is something we plan to explore in the future."}, {"title": "FUTURE WORK", "content": "In addition to the expansion of the rule language, there are several other directions in which NEURULES could be extended. For example, the current rule list model is only designed for bi- nary classification tasks, hence to extend it to multi-class classification, is crucial for a wider range of applications. In that context, we also plan to derive a propensity score from the rule list model for conformal prediction. Extending NEURULES to regression tasks opens up a wide range of new applications to benefit from interpretable rule lists. Another exciting direction of future work is the adaption of NEURULES to structured data, such as images or graphs. With appropriate predicate functions that extract meaningful concepts in those domains, rule list models could be used as more interpretable and accountable deep learning models."}, {"title": "CONVERGENCE OF CONTINUOUS RELAXATIONS", "content": "We show that our continuous relaxations for predicate, logical conjunction and rule list converge to their discrete counterparts."}, {"title": "PREDICATE.", "content": "The soft predicate for a single feature xi is defined as\n\\pi(x_i; \\alpha, \\beta, t_\\pi) = \\frac{e^{t_\\pi x_i}}{e^{t_\\pi \\alpha} + e^{t_\\pi x_i} + e^{t_\\pi (2x_i - \\alpha - \\beta)}}\nWe now show that the soft predicate converges to the hard predicate as t\u03c0 \u2192 0, which is defined as\n\\pi(x_i; \\alpha, \\beta) = \\begin{cases} 1 & \\text{if } x_i \\in [a_i, B_i] \\\\ 0 & \\text{otherwise} \\end{cases}\nProof: Let us denote as the first logit a = xi, the second logit b = 2xi \u2212 ai and the third logit c = 3xi - Ai \u2212 Bi.\n\\frac{e^{t a}}{e^{t b} + e^{t a} + e^{t c}}\n= \\frac{1}{(e^{-t a} e^{t b} + 1 + e^{-t a} e^{t c})e^{t a}}\n\\frac{1}{e^{\\frac{t}{(a - b)}} + 1 + e^{\\frac{t}{(c - b)}}}\nConsider the following four cases:\nThen\nb = 2x_i - Ai > 2x_i - x_i = x_i = a,\nand as then xi < \u03b2i, i.e. it is less than the upper bound,\nb = 2x_i - Ai > 3x_i - Ai - Bi = c.\nThus a - b > 0 and c - b < 0, so that in the denominator it holds that in the limit\n\\lim_{t \\to 0} \\frac{1}{e^{\\frac{t}{(a - b)}} + 1 + e^{\\frac{t}{(c - b)}}} = \\frac{1}{e^{\\infty} + 1 + e^{-\\infty}} = 0.\nThen\nb = 2x_i - Ai > 2x_i - x_i > x_i > a,\nand as then xi \u2265 ai, i.e. it is greater than the lower bound,\nc = 3x_i - Ai - Bi < 3x_i - x_i < 2x_i - a = b.\nThus a - b < 0 and c - b < 0, so that in the denominator it holds that in the limit\n\\lim_{t \\to 0} \\frac{1}{e^{\\frac{t}{(a - b)}} + 1 + e^{\\frac{t}{(c - b)}}} = \\frac{1}{e^{-\\infty} + 1 + e^{-\\infty}} = 1.\nThen either a \u2212 b = 0 and c \u2212 b < 0, or a \u2212 b > 0 and c \u2212 b = 0, so that in the limit\n\\lim_{t \\to 0} \\frac{1}{e^{\\frac{t}{(a - b)}} + 1 + e^{\\frac{t}{(c - b)}}} = \\frac{1}{1 + 1 + e^{-\\infty}} = 1/2.\nTo obtain the desired behavior at the boundaries, i.e.\u03c0(xi) = 1 or \u03c0(xi) = 0, the output must thus be either ceiled or floored."}, {"title": "LOGICAL CONJUNCTION.", "content": "The soft logical conjunction for a set of predicates \u03c0(xi) is defined as\na(x) = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i\\pi(x_i)^{-1}}\nGiven a set of non-negative weights w \u2208 [0, \u221e)d, with at least one weight being positive, the soft logical conjunction takes values in [0, 1] given d predicates \u03c0(xi) \u2208 [0, 1].\nProof: The domain of the reciprocal is \u03c0(xi)\u22121 \u2208 [1, \u221e). Hence, it holds that all \u2200i \u2208 [d] : wi\u03c0(xi)\u22121 \u2265 wi > 0 and thus for their sum \\sum_{i=1}^d w_i\\pi(x_i)^{-1} \u2265 \\sum_{i=1}^d w_i > 0. Then the soft logical conjunction is bounded by\n0 \u2264 \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i\\pi(x_i)^{-1}} \u2264 \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i} = a(x) \u2264 1.\nIn particular, a(x) = 1 if \u2200i, wi > 0: \u03c0(xi) = 1, as then it holds that \u03c0(xi)\u22121 = 1 and \\sum_{i=1}^d w_i\\pi(x_i)^{-1} = \\sum_{i=1}^d w_i. On the other hand, a(x) = 0 if there exists an index i where wi > 0 and \u03c0(xi)\u22121 = \u221e \u2194 \u03c0(xi) = 0."}, {"title": "RELAXED CONJUNCTION", "content": "The relaxed conjunction a(x) is defined as\n\\eta = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d 1 + \\eta},\n\\hat{a}(x) = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i\\pi(x) + \\eta}\nWe first show that for \u03c0(xj) = 0 the resulting soft conjunction is upper bounded by \u03b5.\nProof: Let \u03c0(xj) = 0 and wj > 1. Then the relaxed conjunction is\na(x) = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i\\pi(x) + \\eta}\n= \\frac{\\frac{1}{1 + \\eta}}{\\sum_{i \\neq j} w_i\\pi(x) + \\eta} + w_j} \n\n\\hat{a}(x) = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i\\pi(x) + \\eta}} \n= \\frac{\\frac{1}{1 + \\eta}}{\\sum_{i \\neq j} w_i w\\pi(x) + \\eta} + w_j \\frac{\\eta}{1 + \\eta}}\nConsider the maximum value of the denominator, i.e. \u03c0(xi) = 1 for all i \u0338= j. Then the denominator is lower bounded by\n\\sum_{i \\neq j} \\frac{w_i}{\\pi(x) + \\eta} + w_j \\frac{\\eta}{1 + \\eta} + w_j = \\sum_{i \\neq j} \\frac{w_i}{1 + \\eta} + w_j = \\frac{\\sum_{i \\neq j} w_i + w_j \\frac{\\eta}{1 + \\eta}"}]}