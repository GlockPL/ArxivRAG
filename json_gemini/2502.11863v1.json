{"title": "FedEAT: A Robustness Optimization Framework for Federated LLMS", "authors": ["Yahao Pang", "Xingyuan Wu", "Wei Chen", "Hai Jin", "Xiaojin Zhang"], "abstract": "Significant advancements have been made by Large Language Models (LLMs) in the domains of natural language understanding and automated content creation. However, they still face persistent problems, including substantial computational costs and inadequate availability of training data. The combination of Federated Learning (FL) and LLMs (federated LLMs) offers a solution by leveraging distributed data while protecting privacy, which positions it as an ideal choice for sensitive domains. However, Federated LLMs still suffer from robustness challenges, including data heterogeneity, malicious clients, and adversarial attacks, which greatly hinder their applications. We first introduce the robustness problems in federated LLMs, to address these challenges, we propose FedEAT (Federated Embedding space Adversarial Training), a novel framework that applies adversarial training in the embedding space of client LLM and employs a robust aggregation approach, specifically geometric median aggregation, to enhance the robustness of Federated LLMs. Our experiments demonstrate that FedEAT effectively improves the robustness of Federated LLMs with minimal performance loss.", "sections": [{"title": "1. Introduction", "content": "Large Language Models have achieved substantial breakthroughs and are currently extensively utilized across multiple domains, including natural language comprehension, code editing, and text generation (Nam et al., 2024; Tang et al., 2024; Yao et al., 2024b). However, these models face challenges related to high computational demands and insufficient training data (Yao et al., 2024a; Minaee et al., 2024a). Federated large-language models (federated LLMs) effectively use decentralized data and distributed computing resources to build powerful models without violating privacy regulations, making them particularly useful in privacy-sensitive fields(Chen et al., 2024; Yue et al., 2024; Sani et al., 2024).\nPrevious research (Xhonneux et al., 2024; Mazeika et al., 2024; Lyu et al., 2022; Huang et al., 2024a) has shown that both FL and LLM are affected by robustness, which denotes a system's capability to sustain consistent functionality and output quality despite encountering data variations, environmental disturbances, or potential adversarial manipulations. In FL, data heterogeneity and malicious clients can substantially impair the overall effectiveness of the global model, potentially leading to erroneous output. Meanwhile, LLMs can generate misleading or even harmful content when faced with adversarial examples or out-of-distribution data (Liu et al., 2023). If attackers exploit these vulnerabilities, they could not only result in undesirable model outputs but also lead to privacy breaches, security risks, or decision-making errors. Therefore, robustness has become a critical performance metric that must be addressed in the practical application of FL and LLM.\nFederated LLMs face the robustness challenges of both FL and LLMs simultaneously. On the one hand, federated LLMs must ensure global effectiveness and robustness in the presence of data heterogeneity and malicious clients. On the other hand, they must address the risks posed by adversarial examples and out-of-distribution data. In a distributed environment, attackers might launch attacks from multiple clients simultaneously, further exacerbating robustness challenges. Additionally, federated LLMs must enhance robustness while controlling communication and computational overhead to ensure practical usability in distributed settings. Efficiently addressing these dual robustness challenges is one of the core challenges facing federated LLMs.\nAdversarial training is a proven method for enhancing model robustness and has been well-established in Machine Learning (Bai et al., 2021; Zhao et al., 2022) and FL (Zizzo et al., 2020; Zhang et al., 2023). In the realm of LLMs, efforts have"}, {"title": "2. Related Works", "content": "This section systematically explores relevant research work in the areas of Federated Learning (FL), Large Language Models (LLMs), Federated LLMs, and adversarial training."}, {"title": "2.1. Federated Learning", "content": "FL faces significant robustness challenges, primarily related to the following challenges: One major problem is data heterogeneity, where clients may possess data distributions that vary significantly, making it difficult to achieve a global optimal model (Lyu et al., 2022). Additionally, malicious clients (Li et al., 2020) can pose a threat to the global model's performance by uploading poisoned model updates. Adversarial attacks (Bhagoji et al., 2019) also represent a serious challenge, as attackers can manipulate local updates to introduce vulnerabilities into the model. To address these challenges, several methods have been proposed, such as robust aggregation techniques (Pillutla et al., 2022), adversarial training (Zizzo et al., 2020), and anomaly detection (Mothukuri et al., 2021). However, enhancing the robustness of FL models remains an active area of research."}, {"title": "2.2. Large Language Models", "content": "LLMs face challenges in terms of robustness, particularly when dealing with out-of-distribution data or adversarial input (Liu et al., 2023; Minaee et al., 2024b). Previous research has shown that LLMs are highly sensitive to minor perturbations in input data, which may result in a significant drop in performance. Methods to enhance the robustness of LLMs often involve adversarial training, data augmentation, and regularization. Despite these advancements, LLMs remain vulnerable to various adversarial attacks, making robustness a critical area of concern."}, {"title": "2.3. Federated Large Language Models", "content": "Federated Large Language Models (federated LLMs) combine the advantages of FL and LLMs, enabling collaborative training of LLMs across distributed clients. This approach addresses challenges such as low data quality and high computational requirements typically associated with LLMs (Sani et al., 2024). Federated LLMs are especially beneficial in privacy-sensitive areas like healthcare and finance, where data cannot be centrally aggregated due to privacy restrictions.\nThe framework of federated LLM is shown in Figure 1. Both the client and the server typically use LLMs with the same architecture. Due to limited computational resources on the client side, clients employ PEFT (Parameter Efficient Fine-Tuning) for training. Existing work (Fan et al., 2024a;b) has better addressed the issue of insufficient computational power on the client side by using LLMs on the server side and small language models(SLMs) with the same architecture on the client side. Knowledge is transferred from the server's LLM to the SLMs aggregated by the clients via"}, {"title": "2.4. Adversarial Training", "content": "The adversarial training optimization problem in machine learning can be described as follows:\n$\\min_{\\theta} \\max_{\\delta} E [L (f_{\\theta} (x + \\delta), y)]$\\nHere, $\\delta$ denotes the adversarial perturbation. The objective is to identify the $\\delta$ that increases the model's loss via adversarial attacks. The model is subsequently trained on adversarial examples $x + \\delta$, updating the parameters $\\theta$ to enhance robustness.\nApplying this adversarial training method to LLMs presents several challenges. First, the training data for LLMs is typically in discrete text space, which means that finding the optimal adversarial samples $X_{adv}$ requires traversing all possible tokens in the vocabulary to identify the adversarial sample that maximizes the model's loss. Although some methods (Mazeika et al., 2024) have improved the efficiency of this process, the computational cost remains high.\nIn the training and inference processes of LLMs, discrete prompts are first converted into discrete tokens. These tokens are then transformed into continuous embedding vectors through the model's embedding layer, placing them in what is known as the embedding space. Prior studies (Xhonneux et al., 2024) have shown that creating adversarial examples within the embedding space effectively improves the robustness and rejection capabilities of large language models (LLMs). This capability refers to the model's ability to refuse to respond when encountering unsafe prompts. In addition, this approach significantly reduces training costs."}, {"title": "3. Robustness of Federated LLMs", "content": "Various challenges arise in different settings, as summarized in the table 2. Federated LLMs are susceptible to these challenges, which are inherited from both FL and LLM. The increased model scale amplifies the impact and complexity of these issues, while the synergy between FL and LLM also gives rise to novel robustness challenges. This section provides a comprehensive overview of the potential robustness challenges that federated LLMs may encounter."}, {"title": "3.1. Malicious Attacks", "content": "Malicious attacks present a considerable risk to federated large language models, potentially undermining the global model's performance and stability. These threats can take multiple forms, including model poisoning, data poisoning, and adversarial attacks. In traditional FL, robust aggregation methods like the geometric median have been effective in mitigating malicious updates (Pillutla et al., 2022), while adversarial training has shown promise in enhancing the robustness of LLMs (Mazeika et al., 2024; Xhonneux et al., 2024). Although these methods have not been extensively validated in federated LLMs, their theoretical foundations suggest they could be valuable in addressing malicious attacks in this context. Future work should focus on experimentally validating these approaches in federated LLM settings."}, {"title": "3.2. Data Heterogeneity", "content": "Data heterogeneity is a core challenge in federated LLMs, arising from the diverse data distributions across clients. This issue is particularly pronounced in language models due to variations in language, domains, and corpora. Traditional federated learning has explored techniques like domain adaptation and multi-task learning to address data heterogeneity (Ghosh et al., 2019). Although these approaches have not been thoroughly explored in the context of federated LLMs, their ability to enhance generalization across diverse datasets suggests a promising avenue for future research."}, {"title": "3.3. Errors and Outliers", "content": "Federated LLMs are vulnerable to errors and outliers due to the varying quality of client data. Issues such as low-quality data, anomalous inputs, and faulty model updates can negatively impact the global model's performance. In conventional FL, various robust aggregation strategies and outlier detection techniques have been introduced to address"}, {"title": "3.4. New Robustness Challenges in Federated LLMs", "content": "Federated LLMs introduce unique robustness challenges, such as cross-client update alignment, insufficient local computation resources, and the trade-off between communication compression and robustness. These challenges stem from the combination of FL and LLMs, particularly in high-dimensional parameter spaces. Techniques like gradient quantization have been explored in traditional federated learning to balance communication efficiency and robustness, but their application to federated LLMs requires further investigation. Future studies should aim to design adaptive approaches to tackle these challenges and maintain the stability of federated LLMs in decentralized settings.\nThese challenges demand innovative research into the robustness of federated LLMs. Future work should focus on developing targeted solutions in areas such as adversarial attack defense, resource-efficient aggregation strategies, and robustness enhancement techniques. In this article, we focus on the robustness of federated LLMs against malicious attacks and develops an algorithm aimed at improving the robustness of these models."}, {"title": "4. Threat Model", "content": "In this study, the adversary evaluates the target model's robustness by crafting adversarial examples. By accessing the model's input space, the adversary introduces small perturbations $\\delta$ to generate adversarial samples in the form of $x+\\delta$, causing the model to produce incorrect predictions while ensuring the perturbations $|\\delta|_p < \\epsilon$ remain imperceptible.\nThe adversary uses gradient-based attacks like FGSM or PGD to optimize perturbations, maximizing the loss function $L(f(x + \\delta), y)$ with the dual objective of significantly degrading model performance on adversarial examples while maintaining high accuracy on clean data."}, {"title": "5. Method", "content": "This section provides an in-depth introduction to the proposed FedEAT algorithm. We begin by discussing how to generate adversarial samples in the embedding space. We then introduce the robust aggregation algorithm employed in our approach. Finally, we outline the overall architecture of the FedEAT algorithm."}, {"title": "5.1. Adversarial Training in the Embedding Space", "content": "(Xhonneux et al., 2024) introduces a harmful dataset containing harmful prompts, unsafe responses, and safe responses. By applying adversarial training in the embedding space using this dataset, the goal is to enable the LLM to identify more malicious prompts that induce unsafe responses, thereby generating safe responses to any harmful prompts. The results of (Xhonneux et al., 2024) demonstrate that adversarial training in the embedding space can effectively enhance the LLM's robustness in rejecting such harmful inputs. Inspired by this work, we apply adversarial training in the embedding space to improve the robustness of federated LLMs. Unlike the aforementioned paper, our objective is to significantly enhance the robustness of federated LLMs without significantly compromising their performance. We achieve this by applying adversarial attacks in the embedding space on prompts from a fine-tuning dataset, thereby improving the LLM's robustness against adversarial attacks.\nThe optimization problem is formulated as follows:\n$\\min_{\\theta} (\\mathcal{L}(f(z),y) - \\lambda \\cdot \\max_{\\delta \\in S} \\mathcal{L}(f(z+\\delta),y))$\ns.t. $|\\delta|_p \\leq \\epsilon$\nThe core of this optimization problem lies in achieving a balanced improvement of model robustness and performance through adversarial training. Specifically, the objective function comprises two components:\nThe first term $\\mathcal{L}(f(z), y)$ represents the standard loss on the original input embedding vector $z$, which ensures the model preserves its predictive accuracy on benign samples.\nThe second term $-\\lambda \\cdot \\max_{\\delta \\in S} \\mathcal{L}(f(z + \\delta), y)$ maximizes the loss induced by adversarial perturbation $\\delta$, compelling the model to develop robustness against worst-case perturbations in the embedding space.\nThe hyperparameter $\\lambda$ governs the trade-off between robustness and standard performance, while the constraint $|\\delta|_p < \\epsilon$ maintains semantic consistency of adversarial examples by bounding the perturbation magnitude through the $L_p$-norm (typically with $p = 2$ or $\\infty$). This norm constraint prevents excessive distortion of input semantics that could otherwise degrade model functionality.\n$z+\\delta$ represents the adversarial example in the embedding space and the goal of adversarial examples is to find a perturbation $\\delta$ in the embedding space that increases the loss of the model while keeping the perturbation within a certain norm to avoid significantly impacting performance.\nIn each iteration of Projected Gradient Descent (PGD), the adversarial perturbation is updated by gradient ascent:\n$z^{t+1} = z^{t} + Proj_S (\\alpha \\cdot \\nabla L(f(z^t), y))$"}, {"title": "5.2. Geometric Median Aggregation of Client Parameters", "content": "Inspired by (Pillutla et al., 2022), we utilize a geometric median-based aggregation method to enhance the robustness of the global model. This method mitigates the impact of malicious or noisy client updates on the global model by implementing robust aggregation techniques, thereby enhancing the model's stability and resilience. This raises an important question in the research of federated LLM robustness:\nDoes the robustness-enhancing aggregation method in federated learning remain effective in the federated LLMs setting?\nThis question is crucial for research on the robustness of federated LLMs. In the FL setting, suppose there are n participating clients, after each round of training, we can obtain n sets of model parameters $\\{w_1, w_2, ..., w_n\\}$. The geometric median represents the optimal location within a dataset that achieves the minimal cumulative distance to all remaining data points. Therefore, the goal of geometric median aggregation is to find the global update that minimizes the total distance from all points to the aggregation point:\n$\\mathcal{w}_{agg} = arg \\min_w \\sum_{k=1}^n || \\mathcal{w}_k - w ||$\nwhere $|| \\cdot ||$ denotes the Euclidean norm. Due to the high computational complexity of directly solving for the geometric median, iterative methods are commonly used to approximate the optimal solution. The Weiszfeld algorithm is an efficient method for this purpose, with the update step given by:\n$w^{(t+1)} = \\frac{\\sum_{k=1}^{n} \\frac{w_k}{\\|w_k - w^{(t)}\\|}}{\\sum_{k=1}^{n} \\frac{1}{\\|w_k - w^{(t)}\\|}}$\nThis method iteratively updates $w^{(t)}$ until convergence, providing an approximate solution to the geometric median.\nIn federated LLMs training, data distributions are often Non-IID, and some clients may upload anomalous or malicious updates. Simple averaging aggregation methods, such as FedAvg, have low robustness to noise and attacks. In contrast, the geometric median aggregation method is more robust to outliers and anomalies, significantly reducing the impact of malicious clients. This leads to improved accuracy and stability in the aggregation results, effectively addressing"}, {"title": "5.3. Federated Embedding-space Adversarial Training: FedEAT", "content": "Our algorithm introduces adversarial training in the embedding space into the training of federated LLMs to enhance the robustness of each local LLM. Additionally, to enhance both the effectiveness and resilience of the global model, we implement the geometric median-based aggregation approach.\nOn the client side (Algorithm 1), each client initializes its model parameters $\\theta$ with the global model parameters $\\Theta_{global}$. For each training epoch e, the client iterates over its local dataset $D_k$ and processes each sample (x, y).\nFirst, the input data x is transformed into an embedding vector z through the embedding layer of the client LLM. Next, an adversarial perturbation e is generated for the current client model $\\theta_c$ and the embedding vector z using multiple iterations of gradient ascent. Here, $\\alpha$ represents the step size for updating the perturbation, and $\\nabla$ represents the gradient of the embedding vector concerning the client LLM. To ensure that the perturbation does not excessively degrade the model's performance, it is restricted to a norm S after each update.\nThe perturbation is subsequently applied to the embedding, resulting in the creation of an adversarial sample, expressed as $z_{adv} = z + e$, and the loss function L is computed using this adversarial sample. Lastly, the model parameters $\\theta_c$ are optimized using gradient descent with a learning rate $\\eta$, improving the LLM's robustness to adversarial perturbations.\nThis process is repeated for E epochs, after which the updated client model parameters $\\theta_c$ are returned to the server.\nOn the server side (Algorithm 2), the updated model parameters from all clients are aggregated. During each communication round, the server selects m clients at random. Each selected client LLM performs local adversarial training in the embedding space in parallel, resulting in updated model parameters $w_i^{(t+1)}$.\nTo ensure robustness against potential malicious updates, the server employs the geometric median aggregation method. Specifically, the Weiszfeld algorithm (see Eq.(5)) is applied iteratively to compute the optimal parameters $w^{(t+1)}$ until convergence. This process is repeated for T rounds, after which the aggregated global model $w^{(T)}$ is returned.\nThe FedEAT algorithm combines local adversarial training on clients with geometric median aggregation on the server. By introducing adversarial perturbations in the embedding space and employing robust aggregation, it enhances the robustness of federated large models while ensuring performance, thereby solving the optimization problem 2."}, {"title": "6. Experiment", "content": "6.1. Experimental Setup\nDataset: We utilize the vicgalle/alpaca-gpt4 dataset, based on which we generate adversarial samples for adversarial training of federated LLMs.\nModel: We select the gemma-1.1-2b-it, PHI-3-MINI, MISTRAL-7B, and ZEPHYR-7B for training and evaluating, as they support direct training through embedding vectors.\nEvaluation Metrics: We employ a comprehensive evaluation framework to assess the utility and robustness of federated LLMs. Specifically, we utilize the benign and adversarial datasets from (Huang et al., 2024b) to evaluate the models' performance on four tasks, including SST2, QQP, MNLI, and QNLI. Our evaluation metrics include classic measures of utility and robustness, and robustness describes a model's ability to maintain performance despite noise or"}, {"title": "7. Conclusion", "content": "Federated LLMs hold significant promise for the future. This paper explores the potential robustness challenges faced by federated LLMs and proposes the FedEAT algorithm to enhance their robustness. Experimental results demonstrate that FedEAT improves model robustness with only a minor performance trade-off, and the ablation studies validate our algorithm's effectiveness."}, {"title": "A. Appendix", "content": "A.1. Limitation\nDue to limitations in some models' ability to support training and inference with embedding vectors, our method has only been tested on models compatible with this approach. In the future, we plan to develop new techniques to enhance the universality of our method. Additionally, our current experiments were conducted on the vicgalle/alpaca-gpt4 dataset, and we intend to extend our evaluations to include more models and datasets.\nCurrently, our robustness evaluation is based on adding noise to the dataset and measuring the model's accuracy. However, we plan to employ a variety of evaluation methods to conduct a more comprehensive robustness assessment of the trained LLMs. This will help verify the improvements in model robustness brought by our algorithm and provide a more thorough understanding of its effectiveness in real-world applications.\nSince LLMs with different architectures exhibit varying sensitivities to adversarial perturbations, and our experiments applied the same perturbation intensity to all LLMs, the results may not fully reflect the algorithm's effectiveness. In subsequent work, we will conduct additional experiments to identify the optimal perturbation intensity for different LLMs.\nA.2. Complete results of different methods applied to various tasks and models."}]}