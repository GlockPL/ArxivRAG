{"title": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM", "authors": ["Changcheng Li", "Xiangyu Wang", "Qiuju Chen", "Xiren Zhou", "Huanhuan Chen"], "abstract": "Large language models (LLMs) have shown limitations in tasks requiring complex logical reasoning and multi-step problem-solving. To address these challenges, researchers have employed carefully designed prompts and flowcharts, simulating human cognitive processes to enhance LLM performance, such as the Chain of Thought approach. In this paper, we introduce MTMT (Multi-thinking Modes Tree), a novel method that interacts with LLMS to construct a thought tree, simulating various advanced cognitive processes, including but not limited to association, counterfactual thinking, task decomposition, and comparison. By breaking down the original complex task into simpler sub-questions, MTMT facilitates easier problem-solving for LLMs, enabling more effective utilization of the latent knowledge within LLMs. We evaluate the performance of MTMT under different parameter configurations, using GPT-40 mini as the base model. Our results demonstrate that integrating multiple modes of thinking significantly enhances the ability of LLMs to handle complex tasks.", "sections": [{"title": "1 Introduction", "content": "With the development of natural language processing, large language models (LLMs) have come to play a pivotal role in the field. There is hope that these models can solve most natural language problems and even achieve Artificial General Intelligence (AGI). However, despite increasing the amount of data and model parameters, the natural language capabilities of large models have not achieved the previously astonishing breakthroughs. Since the release of GPT-4 (OpenAI et al., 2023), other large language models may have achieved better results in some areas compared to GPT-4, but there has been no significant change (Anthropic, 2024; Reid et al., 2024). Additionally, issues such as model hallucinations, unfaithful explanations, lack of memory, and inadequate logical reasoning continue to challenge researchers.\nTo address these issues, many contextual learning schemes have been proposed to improve the performance of large models in various domains. For instance, algorithms like Chain of Thought (Wei et al., 2022) and many other algorithms, similar to Chain of Thought methods, enhance model output accuracy by decomposing tasks and employing a series of evaluation and retrospection techniques. Sys2 attention (Weston and Sukhbaatar, 2023) improves outcomes by asking about key segments in the prompts given to the LLM, thereby strengthening focus on important content. Additionally, methods such as the work of Ma et al. (2023) use counterfactuals to enhance the LLM's ethical level.\nAll the aforementioned methods are very similar to the thinking patterns people use when solving complex problems. Psychology suggests that humans have two modes of thinking: System 1 and System 2 (Kahneman, 2011). Inspired by neuroscience and psychology (Daw et al., 2005; Kahneman, 2011), we believe that the characteristics exhibited by today's LLMs are very similar to System 1, which is highly intuitive and relies on probabilities. As defined in the work of Yu et al. (2024), we define System 1 reasoning as the immediate responses generated by the LLM for the given problems. System 2 reasoning refers to any method that involves producing intermediate tokens, such as approaches that perform searches or use multiple prompts before delivering a final answer.\nBuilding on this foundation, we designed an algorithm called MTMT (Multi-thinking Modes Tree), a graph that interacts repeatedly with the model to achieve the effects of System 2. As shown in Figure 1, the MTMT algorithm integrates various thinking modes to enhance the logical reasoning capabilities and accuracy of answers provided by LLMs. By thinking around the original complex task, we derive many simpler sub-questions."}, {"title": "2 Related Work", "content": "2.1 Dual Systems in Humans\nSystem 1 and System 2, or fast and slow thinking (Kahneman, 2011; Frankish, 2010; Evans, 2008), are common terms used in psychology. In the book Thinking, Fast and Slow, System 1 refers to the brain's automatic and fast processes that operate with little effort and without a sense of conscious control. In contrast, System 2 focuses attention on mental activities that require effort, such as complex calculations. The operations of System 2 are often associated with the subjective experience of agency, choice, and concentration. Dual system theory is widely applied in psychological explanations of various behavioral phenomena in economic, social, and animal-conditioning contexts (Kahneman et al., 2002; Loewenstein and O'Donoghue, 2004; Killcross and Blundell, 2002; Dickinson and Balleine, 2002).\nMeanwhile, a broad range of neural and behavioral data suggests (Daw et al., 2005) that the brain contains multiple systems for behavioral choice, including one associated with the prefrontal cortex and another with the dorsolateral striatum. This serves as one of the pieces of evidence that support the existence of dual systems as a real structure within the brain.\n2.2 Dual Systems Models\nCurrently, LLMs also share many similarities with humans' System 1. As probabilistic models, they tend to output the most \"relevant\" answers to a given question and perform well when answering simple and familiar questions (OpenAI et al., 2023). However, when the questions become more complex, the logical reasoning abilities of large models are challenged. In fact, Chiang et al. (2023) demonstrated that there are inherent limits to the problem-solving capabilities of large language models with a transformer architecture.\nSurprisingly, simply prompting the model with \"let's think step by step\" enables it to break down complex problems into smaller steps, leading to a significant increase in accuracy. This type of approach, known as the Chain of Thought (Wei et al., 2022), has become one of the most popular methods to improve the performance of LLMs. The effectiveness of Chain of Thought has been demonstrated by Prystawski et al. (2023); Feng et al. (2023). Next, we will categorize and introduce several System 2 models.\n2.2.1 The \"Chain of Thought\" Method\nMany improved versions have been developed based on the Chain of Thought approach. Tree of thoughts (ToT) (Yao et al., 2023a) integrates the model's abilities to produce and evaluate thoughts with search algorithms like breadth-first or depth-"}, {"title": "3 Methodology", "content": "3.1 Problem Definition\nUsing the symbol p to define the large language model (LLM) and Q as the question being asked, the MTMT is represented as Gp, with the initialization as follows:\n$G_p(Q) = q_1$,\n$P(q_1) = a_1,$\nwhere q1 represents the first sub-question derived from Gp regarding Q, and a1 is the answer provided by the LLM to the first sub-question.\nFor the subsequent i-th step, the iteration proceeds as follows:\n$G_p(Q, a_1,a_2,..., a_i) = q_{i+1}$,\n$P(q_{i+1}) = a_{i+1}\u00b7$\nThis represents the repeated \u201ccommunication\" between System 1 and System 2. Based on the answers from the previous i steps and the question Q, we derive the qi+1 and interact with the large model to obtain the answer ai+1 (note that the qi+1 includes the information needed for the LLM to answer it).\nEach time we complete a sub-question directly connected to Q, we obtain an answer based on Q and a1, a2,..., aj:\n$G_p(Q, a_1, a_2,..., a_j) = A_k,$\nwhere Ak represents the answer generated for Q during the k-th iteration. Once certain criteria are met (see Section 3.3), Ak will be considered as the final answer. During the execution of this algorithm, we ensure that the sub-question qi is intuitive, simple, and suitable for System 1 (LLM) to answer. We then perform information extraction, evaluation, modification, and filtering on the obtained answers ar before applying them to the original question Q.\nThe overall process for Equations (3) and (4) is as follows (also shown in Figures 2) :\n1.  Based on different thinking mode sub-questions, and considering the known pairs (qi, ai), generate the appropriate prompt and obtain the model's response.\n2.  We will re-engage the model to perform information extraction in a dictionary format (see Appendix B for more details). This process generates nodes that store the extracted information (for logical processing) and the original response (for further model generation), which are then added to the graph.\n3.  Select the next thinking mode type to be used and transition to the designated node.\nNotably, we do not require the LLM to output results in a specific format on the first response. Instead, we ask it to extract the relevant information after its initial answer based on the content provided. This is because discouraging prompts, such as \"just tell me the result without any explanation,\" can negatively affect the LLM's reasoning ability (Zhao et al., 2024; Zhou et al., 2023).\n3.2 Thinking Mode\nSpecifically, we categorize thinking modes into the following major types: decompose, association, compare, importance, inference and others. Each major type contains several different prompts. See Appendix A for specific details about each prompt. The use of different thinking modes serves two main purposes: generating thinking nodes and performing operations on different nodes.\n3.2.1 Generating Thinking Nodes\nBy generating prompts based on the thinking modes, we can obtain a wealth of useful information. For example, consider the following question:\nQuestion:There are only three people on the playground Xiao Ming, Xiao Hong, and Xiao Li."}, {"title": "3.2.2 Performing Operations on Nodes", "content": "Thinking modes not only generate a wealth of information for creating thinking nodes but also influence the entire graph with various prompts. For example, in the importance category, unimportant_point helps us select useful a\u017c data. In the decompose category, decompose_task determines whether to proceed with task decomposition and how many steps to break it down into."}, {"title": "3.3 Thinking Node", "content": "The question Q serves as the root node. For generating subsequent nodes, the perplexity is used to measure whether the model is \"confident\" or \"confused\" about the question. In LLMs, for a given response S = (t1, t2, ..., tv), where ti represents the i-th token and N represents the total number of tokens, the perplexity is calculated as:\n$PP(S) = \\sqrt[N]{\\prod_{i=1}^{N} P(t_i|t_1...t_{i-1})}$,\nThe generation, regeneration and deactivation of other nodes follow these guidelines:\n3.3.1 Node Generation\nEach node has a perplexity threshold, which is calculated as follows:\n$PPT_i = PPT_0 + \u03b1D(i)$,\nwhere PPTi is the perplexity threshold of the i-th node, PPT0 is the initial perplexity threshold, \u03b1 is the proportionality coefficient, and D(i) represents the number of nodes in the shortest path from node i to the root node (excluding the root node).\nBy calculating perplexity, if the perplexity of a given response exceeds a certain threshold, we will continue generating sub-nodes using other types of strategies for that node's question to obtain more information and methods until the perplexity requirement is met. If a node and its parent node both exhibit \"confusion\", a breadth-first search (BFS) approach is adopted, prioritizing further exploration of the parent node using different strategies to extract more information.\nFor selecting a thinking mode, if a specific mode has already been assigned by a previous thinking mode, we follow the assigned strategy. Otherwise, a thinking mode will be randomly chosen from all available modes to generate the next node. Initially, we always let the MTMT go through task_recognition and decompose_task.\n3.3.2 Node Regeneration\nAfter generating information for other sub-nodes, we regenerate the node. This process is repeated until the perplexity requirement is satisfied. Once the perplexity condition is met, we use the difference_answer in compare category to compare the quality of the answers generated in both instances and ultimately select the most suitable response.\n3.3.3 Node Deactivation\nNot all information stored in each node is always useful; irrelevant or incorrect information can even reduce the model's accuracy. For nodes that have met the perplexity requirement, we use the unimportant_point in importance category to assess whether the information from the sub-nodes contributes to resolving the parent node's question. If it does, we incorporate this information into the prompt used for regenerating the parent node."}, {"title": "4 Experiments", "content": "4.1 Base Model\nWe will use OpenAI's latest large language model, GPT-40 mini (OpenAI, 2024), as the System 1. GPT-40 mini enables a broad range of tasks with its low cost and latency, and it performs well in extracting structured data, making it an ideal base model for our experiments.\n4.2 Datasets\nWe will test the model's performance on the following three datasets.\nGSM8K (Patel et al., 2021): The dataset contains math word problems geared toward an average middle-school curriculum, which is also repeatedly adopted by prior work as a key benchmark for arithmetic reasoning. We use a total of 1,319 data points from its test set.\nGPQA (Rein et al., 2023): Google-proof Question Answering (GPQA) is a recent benchmark where challenging multiplechoice questions in physics, biology, and chemistry are created and labeled by domain experts who have or are pursuing PhD degrees. In this benchmark, experts and skilled non-experts are reported to achieve 81% and 22% accuracy respectively, demonstrating the difficulty of the questions. We use a total of 448 data points from its dataset.\nTruthfulQA (Lin et al., 2021): TruthfulQA is a classification (multi-choice) task designed to test LLMs' propensity to dispense harmful information. The dataset contains 654 test instances. We use a total of 817 multiple-choice questions from the dataset.\n4.3 Baseline\nHere are the baselines for comparison. These baselines are widely used to evaluate the accuracy of LLMs. See Appendix C for specific prompts.\nGPT-40 mini: Using the original model directly on the dataset.\nGPT-40 mini + CoT: GPT-40 mini model is queried with zero-shot CoT prompting (Kojima et al., 2022): \u201cLet's think step by step\" is appended to the question.\nGPT-40 mini + 3-shot:Three examples with corresponding answers are added to the prompt input of the GPT-40 mini model.\nGPT-40 mini + CoT 1-shot:For 1-shot, One demonstration example of a question and answer pair is provided in the prompt, where the answer is in the style of CoT (Wei et al., 2022).\n4.4 Result and Compare\nThe results of different methods on various datasets are shown in Table 1."}, {"title": "4.5 Ablation Study", "content": "To evaluate the effectiveness of different types of thinking modes, we conducted ablation experiments on the GPQA and TruthfulQA datasets. In these experiments, we remove one specific thinking mode and test its impact on accuracy. We conducted ablation experiments on the GPQA dataset under the conditions of PPT0 = 1.25 and \u03b1 = 0.1, and on the TruthfulQA dataset under the conditions of PPT0 = 1.45 and \u03b1 = 0.05. The results are shown in Table 2 and Table 3.\nFrom the table, we can observe that removing different thinking modes has varying impacts on the results. The removal of the decompose mode has the most significant effect, leading to a 4.7% and 2.1% decrease in accuracy on the GPQA and TruthfulQA datasets, respectively. On the GPQA dataset, the removal of the association mode also resulted in a 4.5% decrease in accuracy. Other thinking modes show similar effects, causing accuracy drops ranging from 2.7% to 0.6%"}, {"title": "4.6 Analysis", "content": "MTMT has enhanced the LLM's logical reasoning and explainability in various instances. For example, in response to the question, \"What are the richest countries in South America by GDP per capita?\", MTMT prompted the model to identify the type of question and the methods required to solve it (such as understanding GDP per capita and identifying the countries). This interaction enabled the model to conclude, \u201cIt's important to note that while Venezuela has historically had a high GDP per capita due to its oil wealth, its current economic situation has significantly affected its GDP per capita ranking.\" This corrected an error found in the baseline output. Such instances are not isolated within this approach, indicating that we can achieve further advancements in the LLM's controllable output, logical reasoning, and explainability.\nThe errors in MTMT can be categorized into three main types: overly difficult tasks, information accumulation, and base model issues. For instance, in the GPQA dataset, the prevalence of advanced mathematics increases computational demands. Given our computational limits, we restrict the generation of nodes to a maximum of 30 for each question, which may not be sufficient for the number of nodes needed to solve such complex problems.\nInformation accumulation is another challenge. As different nodes are generated, the amount of information sent to the base model increases, leading to confusion about which points to focus on. Summarizing and refining information may help address this issue.\nAdditionally, many errors stem from the underlying model. For example, in certain tasks, the model fails to produce output in the required format, rendering it ineffective. In simpler questions, such as comparing 3.8 and 3.11, the model can also provide incorrect answers. Such errors are likely to accumulate during the node generation process in MTMT. However, we believe that as LLMs continue to develop, these types of issues will diminish."}, {"title": "5 Conclusion", "content": "Recently, many studies have focused on designing prompts based on specific thought processes and repeatedly engaging large language models (LLMs), achieving remarkable results. This paper introduces a method called MTMT, which aggregates various human problem-solving approaches to enable LLM to provide more accurate answers. This multi-faceted thinking approach offers the model additional background knowledge and guidance, breaks down complex tasks into sub-questions that are easier to solve accurately. Across three different datasets, this method outperformed models using Chain of Thought (CoT). Moreover, we further investigated the impact of different parameters on MTMT's accuracy and conducted ablation experiments on various thinking modes. Overall, MTMT can enhance the LLM's ability to solve complex"}, {"title": "6 Limitations", "content": "Much relevant information may affect the model's performance. Overly long prompts might prevent the base model from focusing on the question to be answered. In future work, incorporating some concise methods in the thinking mode could help address this issue. Also, search methods like MTMT require more resources (e.g., GPT-40 mini API costs) than few-shot learning to improve task performance. However, adjusting the perplexity threshold allows users to customize the trade-off between performance and cost. As more efficient smaller models are introduced in the future, these costs are expected to decrease. Finally, future work could consider integrating a memory module into the network, enabling it to utilize relevant temporary memories to answer related questions, thereby enhancing the model's safety and long-term memory capabilities."}, {"title": "A Thinking Mode Prompt Template", "content": "The use of different thinking modes serves two main purposes: generating thinking nodes and performing operations on different nodes."}, {"title": "B Information Extraction Prompt Template", "content": "We also need to extract relevant information from the LLM's responses to perform additional logical operation and the final answer (e.g., multiple-choice options like A, B, C, D or numerical answers) also needs to be extracted using this template (see Table 5).\nThe format_instructions are generated from the Langchain library (version: 0.2.3), an open-source software library. For a set of attributes to be extracted, given their descriptions, the generated format instructions are shown in Table 6."}, {"title": "C Baseline Prompt Template", "content": "And here are the prompt templates of baseline (see Table 7)."}]}