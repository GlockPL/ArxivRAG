{"title": "POSTERIOR-MEAN RECTIFIED FLOW: TOWARDS MINIMUM MSE PHOTO-REALISTIC IMAGE RESTORATION", "authors": ["Guy Ohayon", "Tomer Michaeli", "Michael Elad"], "abstract": "Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Photo-realistic image restoration (PIR) is the task of reconstructing visually appealing images from degraded measurements (e.g., noisy, blurry). This is a long-standing research problem with diverse applications in mobile photography, surveillance, remote sensing, medical imaging, and more. PIR algorithms are commonly evaluated by distortion measures (e.g., PSNR, SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018)), which quantify some type of discrepancy between the reconstructed images and the ground-truth ones, and by perceptual quality measures (e.g., FID (Heusel et al., 2017), KID (Bi\u0144kowski et al., 2018), NIQE (Mittal et al., 2013), NIMA (Talebi & Milanfar, 2018)), which are intended to predict the extent to which the reconstructions would look natural to human observers. Since distortion and perceptual quality are typically at odds with each other (Blau & Michaeli, 2018), the core challenge in PIR is to achieve minimal distortion without sacrificing perceptual quality.\nA common way to approach this task is through posterior sampling (Bendel et al., 2023; Chung et al., 2023; Daras et al., 2024; Kawar et al., 2021a;b;"}, {"title": "2 BACKGROUND", "content": "We adopt the Bayesian perspective for solving inverse problems (Davison, 2003; Kaipio & Somersalo, 2005), where a natural image x is regarded as a realization of a random vector X with probability density function px. The degraded measurement y (e.g., a noisy or low-resolution image) is a realization of a random vector Y, which is related to X via the conditional probability density function py|x. Given a degraded measurement y, an image restoration algorithm generates a prediction x by sampling from px|y (\u00b7|y), such that X adheres to the Markov chain X \u2192 Y \u2192 X (i.e. X and X are statistically independent given Y)."}, {"title": "2.1 DISTORTION AND PERCEPTUAL INDEX", "content": "Image restoration algorithms are typically evaluated by their average distortion E[\u2206(X, X)], where \u2206(x, x) is some distortion measure that quantifies the discrepancy between x and x, and the expectation is taken over the joint distribution px, x. Common examples for \u2206(x, x) are the absolute error ||x-x||1, the squared error ||x \u2212 x||2, and LPIPS (Zhang et al., 2018). Moreover, as the goal in PIR is to produce reconstructions that would look natural to humans, PIR algorithms are also evaluated by perceptual quality measures. The ideal way to evaluate perceptual quality is to assess the ability of humans to distinguish between samples of ground-truth images and samples of reconstructed ones. This is typically done by conducting experiments where human observers vote on whether the generated images are real or fake (Dahl et al., 2017; Denton et al., 2015; Guadarrama et al., 2017; Iizuka et al., 2016; Isola et al., 2017; Salimans et al., 2016; Zhang et al., 2016; 2017).\nHowever, such experiments are too costly and impractical for optimizing models. A practical and sensible alternative to quantify the perceptual quality is via some perceptual index d(px, px), where d(\u00b7,\u00b7) is a statistical divergence between probability distributions (e.g., Kullback\u2013Leibler, Wasserstein) (Blau & Michaeli, 2018). Quantifying the perceptual index for high-dimensional distributions"}, {"title": "2.2 OPTIMAL ESTIMATORS FOR THE SQUARED ERROR DISTORTION", "content": "Due to the distortion-perception tradeoff (Blau & Michaeli, 2018), it has become common practice to compare image restoration algorithms on the distortion-perception plane, where the goal is to obtain optimal estimators with the lowest possible distortion given a prescribed level of perceptual index. This goal can be formalized by the distortion-perception function (Blau & Michaeli, 2018),\n$$D(P) = \\min_{P_{XY}} E[\\Delta(X, \\hat{X})] \\quad \\text{s.t.} \\quad d(p_X, p_{\\hat{X}}) \\leq P.$$\nPerhaps the most common points of interest on D(P) are D(\u221e) and D(0), where the first point corresponds to the estimator achieving minimal average distortion under no constraint, and the second corresponds to the estimator achieving minimal average distortion under a perfect perceptual index constraint. Considering the squared error distortion, these points are defined by\n$$\\min_{P_{\\hat{X}Y}} E[||X - \\hat{X} ||^2] \\quad \\text{and}$$\n$$\\min_{P_{\\hat{X}Y}} E[||X - \\hat{X} ||^2] \\quad \\text{s.t.} \\quad p_{\\hat{X}} = p_X,$$\nrespectively. It is well-known that the unique solution to Problem (2) is the posterior mean X* := E[X|Y], which typically produces overly-smooth reconstructions (Blau & Michaeli, 2018). Therefore, in PIR tasks, it is more appropriate to aim for the solution to Problem (3). Interestingly, Freirich et al. (2021) proved that a solution to Problem (3) can be obtained by solving the optimal transport problem\n$$P_{U,V} \\in \\underset{P_{U', V'} \\in \\Pi(p_X, p_{X^*})}{\\text{arg min}} \\quad E[||U' - V'||^2],$$\nwhere \u03a0(px, px\u2217) := {pu\u2032,v\u2032 : pu\u2032 = px, pv\u2032 = px\u2217} is the set of all joint probabilities pu\u2032,v\u2032 with marginals pu = px and pv\u2032 = px\u2217. Namely, the optimal solution to Problem (3) can be constructed as follows: Given a degraded measurement y, first predict the posterior mean x\u2217 = E[X|Y = y], and then sample from pu|v(\u00b7|x\u2217), which is the optimal transport plan from px\u2217 to px. Similarly to Freirich et al. (2021), we denote such a solution to Problem (3) by Xo.\nAs discussed before, one of the most common and appealing solutions for PIR tasks is the estimator X that samples from the posterior distribution px|y, such that px|y = px|y. While such an estimator always attains a perfect perceptual index (Blau & Michaeli, 2018), its MSE is typically larger than that of Xo (Blau & Michaeli, 2018; Freirich et al., 2021) (see Figure 1). In other words, to design an algorithm with minimal MSE under a perfect perceptual index constraint, one should often not resort to posterior sampling, but rather to solving Problem (3). This is our goal in this paper. Lastly, one may wonder whether sampling from px|x\u2217 instead of using the optimal transport plan from Equation (4) may also be effective in terms of MSE. However, in Appendix A.1 we prove that such an approach leads to precisely the same MSE as sampling from the posterior."}, {"title": "2.3 FLOW MATCHING AND RECTIFIED FLOWS", "content": "Flow matching. Flow matching algorithms (Albergo & Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023) are generative models defined via the ODE\n$$dZ_t = v(Z_t,t)dt,$$\nwhere v is often called a vector field, and Zt is some forward process such that pz0 is the source distribution, from which we can easily sample (e.g., isotropic Gaussian noise), and pz1 is the target distribution from which we aim to sample (e.g., natural images). In principle, one can generate samples from the target distribution pz1 by solving Equation (5), where samples from the source distribution pz0 are set as the initial conditions for the ODE solver. Nevertheless, given a particular forward process Zt, there are possibly many different vector fields that satisfy Equation (5). The goal in flow matching is to somehow find an appropriate vector field with desirable practical and theoretical properties, e.g., where the solution to Equation (5) is unique."}, {"title": "3 POSTERIOR-MEAN RECTIFIED FLOW", "content": "We now describe our proposed algorithm, which we coin Posterior-Mean Rectified Flow (PMRF) (Algorithm 1). Our method consists of two simple training stages. First, we train a model fw to predict the posterior mean by minimizing the MSE loss,\n$$w^* = \\underset{\\omega}{\\text{arg min}} E [||X - f_\\omega(Y)||^2] .$$\nNote that this training stage can often be skipped, whenever there exists an off-the-shelf algorithm that attains sufficiently small MSE (high PSNR) in the desired restoration task. In the second stage, we train a rectified flow model v\u03b8 (a vector field) to solve\n$$\\theta^* = \\underset{\\theta}{\\text{arg min}} E [||(X - Z_0) - v_\\theta(Z_t, t)||^2] dt,$$\nwhere Zt := tX + (1 \u2212 t)Zo. Here, Zo := fw\u2217(Y) + \u03c3\u03b5\u03f5, where \u03f5 \u223c N(0, I) is statistically independent of Y and X, and \u03c3\u03b5 is a hyper-parameter that controls the level of the Gaussian noise added to the posterior mean prediction. As shown by Albergo et al. (2023), adding such a noise is critical when the source and target distributions lie on low and high dimensional manifolds, respectively. Specifically, it alleviates the singularities resulting from learning a deterministic mapping"}, {"title": "4 RELATED WORK", "content": "Before moving on to demonstrate the effectiveness of our approach, it is instructive to note the difference between our PMRF method and existing techniques that may superficially seem similar.\nDiffusion and flow-based posterior samplers. Diffusion or flow-based image restoration algorithms often attempt to sample from the posterior distribution by training a conditional model that takes Y (or some function of Y, like X\u2217) as an additional input (Lin et al., 2024; Zhu et al., 2024). Some works avoid training a conditional model for each task separately, and rather modify the sampling process of a trained unconditional diffusion model (Chung et al., 2023; Kawar et al., 2022). In Section 5.2 we perform a controlled experiment on various inverse problems, which shows that our PMRF method consistently outperforms posterior samplers with the same architecture.\nFlow from degraded image. Some diffusion/flow models are trained on corresponding pairs of ground-truth images and degraded measurements (Albergo et al., 2023; Delbracio & Milanfar, 2023; Li et al., 2023). In this approach, the idea is to obtain a high-quality image by solving an ODE/SDE with the degraded measurement set as the initial condition. For example, Albergo et al. (2023) trained a rectified flow model for the forward process Zt = tX + (1 \u2212 t)Y\u2020, where Y\u2020 is an up-sampled version of Y such that it matches the dimensionality of X. These algorithms are closely related to PMRF, in the sense that they learn to transport an intermediate signal (instead of pure noise) to the ground-truth image distribution. Yet, they have two critical disadvantages compared to PMRF. First, the flow model's design is not agnostic to the type of degradation, as the degraded signals can have varying dimensionalities or lie in a different domain than that of the ground-truth images (e.g., in MRI image reconstruction). Thus, the task of the flow model may be harder than necessary, as it needs to translate signals from one domain to another. On the other hand, in PMRF the flow model always operates in the image domain, where the dimensionalities of the source and target signals are the same. Second, the theoretical motivation for flowing from Y is not clear,"}, {"title": "5 EXPERIMENTS", "content": "We train PMRF to solve the challenging blind face image restoration task, and compare its performance with leading methods. As in previous works (e.g., (Wang et al., 2021)), we use the FFHQ data set (Karras et al., 2019) with images of size 512 \u00d7 512 to train our model. Similarly to previous works, we adopt a complex and random degradation process to synthesize the degraded images,\n$$Y = [(X \\otimes k_\\sigma) \\downarrow_R + N_\\delta]_{JPEG_Q},$$\nwhere \u2297 denotes convolution, k\u03c3 is a Gaussian blur kernel of size 41 \u00d7 41 and variance \u03c32, \u2193R is bilinear down-sampling by a factor R, N\u03b4 is white Gaussian noise of variance \u03b42, and [\u00b7]JPEGQ is JPEG compression-decompression with quality factor Q. Similarly to (Yue & Loy, 2024), we synthesize the degraded images by sampling \u03c3, R, \u03b4 and Q uniformly from [0.1, 15], [0.8, 32], [0, 20], and [30, 100], respectively. See Appendix B.1 for additional implementation details."}, {"title": "5.1.1 EVALUATION SETTINGS", "content": "For evaluation, we consider the common synthetic CelebA-Test benchmark, as well as the real-world data sets LFW-Test (Huang et al., 2008; Wang et al., 2021), WebPhoto-Test (Wang et al., 2021), CelebAdult-Test (Wang et al., 2021), and WIDER-Test (Zhou et al., 2022). CelebA-Test consists of 3,000 high-quality images taken from the test partition of CelebA-HQ (Karras et al., 2018), and the degraded images were synthesized by Wang et al. (2021). For the real-world data sets, the degradations are unknown and there is no access to the clean ground-truth images. We compare our performance with DOT (Adrai et al., 2023) and leading blind face restoration models, including BFRffussion (Chen et al., 2024), DiffBIR (Lin et al., 2024), DifFace (Yue & Loy, 2024), CodeFormer (Zhou et al., 2022), GFPGAN (Wang et al., 2021), VQFRv1 and VQFRv2 (Gu et al., 2022), RestoreFormer and RestoreFormer++ (Wang et al., 2022; 2023b). Notably, these restoration methods also use the degradation model from Equation (12), though the ranges of \u03c3, R, \u03b4, and Q differ across methods. The ranges we choose, those from (Yue & Loy, 2024), are the most severe among all the compared methods. For example, the range of R we use is [0.8, 32], whereas Wang et al. (2021) use [1, 8]. Thus, PMRF attempts to solve a more difficult restoration task than some of the compared methods. In the following experiments, we use K = 25 flow steps in PMRF (Algorithm 1). Refer to Appendix B.2 for an evaluation of additional values of K, and to Appendix B.3 for the implementation details of DOT."}, {"title": "5.1.2 RESULTS ON CELEBA-TEST", "content": "For the CelebA-Test benchmark, we measure the perceptual quality by FID (Heusel et al., 2017), KID (Bi\u0144kowski et al., 2018), NIQE (Mittal et al., 2013), and Precision (Kynk\u00e4\u00e4nniemi et al., 2019),"}, {"title": "5.1.3 RESULTS ON REAL-WORLD DEGRADED IMAGES", "content": "Evaluating the distortion for real-world degraded images is impossible, as there is no access to the ground-truth images. Consequently, previous works conduct only a perceptual quality evaluation (e.g., FID) on real-world data sets such as WIDER-Test and LFW-Test. Yet, high perceptual quality alone is clearly not indicative of reconstruction performance (to attain high perceptual quality, one may simply ignore the inputs and generate samples from px). Thus, we consider a measure which indicates the Root MSE (RMSE) and allows ranking algorithms according to their (approximate) RMSE, without access to the ground-truth images. Specifically, for any estimator X it holds that\n$$E[||X - \\hat{X} ||^2] \\approx E[|| \\hat{X} - f(Y)||^2] + m,$$\nwhere f(Y) \u2248 X\u2217 is an approximation of the true posterior mean predictor X\u2217, and m is a constant that does not depend on X (see Appendix D for an explanation). Thus, the square root of E[||X \u2212 f(Y)||2], which we denote by IndRMSE, indicates the true RMSE. We utilize the posterior mean predictor trained by (Yue & Loy, 2024)2 as f, and compute the IndRMSE of all the evaluated algorithms on the LFW-Test, WebPhoto-Test, CelebAdult-Test, and WIDER-Test data sets. As before, we evaluate perceptual quality by FID, KID, NIQE, and Precision. In Figure 3 we provide visual results on inputs from the WIDER-Test data set, and compare the algorithms on a \u201cdistortion\u201d-\nperception plane (IndRMSE vs. FID). DOT is not plotted as it achieves far worse FID compared to other methods. Our algorithm attains the best (smallest) IndRMSE on all data sets, while achieving on-par perceptual quality compared to the state-of-the-art. This indicates that PMRF achieves superior distortion on such real-world data sets, while not compromising perceptual quality. In the appendix, we report the rest of the perceptual quality measures in Tables 7 to 10, provide visual results in Figures 6 to 8, and also report the performance of DOT."}, {"title": "5.2 COMPARING PMRF WITH PREVIOUS FRAMEWORKS IN CONTROLLED EXPERIMENTS", "content": "One may wonder whether the performance of PMRF is attributed to the framework itself (Algorithm 1), or, maybe it is attributed to the model architecture, the rectified flow training approach, the chosen hyper-parameters, etc. Could we have done better by training a flow to sample from the posterior, or by adopting the approach of (Albergo et al., 2023) and flow directly from Y? Here, we conduct a controlled study where we demonstrate that the high performance of PMRF is indeed attributed to the proposed framework itself (Algorithm 1). Specifically, we consider the image denoising, super-resolution, inpainting, and colorization tasks, where we train PMRF and several baseline methods on the \"same grounds\". In each task we train two conditional rectified flow models, where one is conditioned on the degraded measurement Y (we call this method flow conditioned on Y), and the other is conditioned on the posterior mean predictor fw\u2217(Y) (we call this method flow conditioned on X\u2217). The first model represents posterior sampling methods, and the second model allows for a fair comparison of model capacity with PMRF (since PMRF is comprised of fw\u2217(Y) and a flow model). In fact, theoretically speaking, the second approach achieves precisely the same"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "The goal in this paper was to design an algorithm that directly approximates X0, which is the estimator that minimizes the MSE under a perfect perceptual index constraint (Equation (3)). To achieve this goal, we introduced Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective image restoration algorithm that outperforms previous frameworks (e.g., posterior sampling, flow from Y, and GAN-based methods) in a variety of image restoration tasks. As we explained in Section 3, PMRF alleviates the issues resulting from solving the ODE by adding Gaussian noise to the posterior mean predictions. We note that the noise level \u03c3\u03b5 should be carefully tuned, as taking it to be too large or too small may cause the MSE or the perceptual quality of PMRF to degrade, respectively. While the flow from Y method (Algorithm 4) suffers from the same limitation (though it does not provide a theoretical guarantee on the MSE, like PMRF), this may be considered a disadvantage of PMRF compared to posterior sampling methods (e.g., Algorithm 2), which do not require such a hyper-parameter. Moreover, we proved in Proposition 1 that, under some conditions, PMRF is guaranteed to achieve a smaller MSE than the posterior sampler. However, as in (Liu et al., 2023), one could argue that the assumptions in Proposition 1 may be too limiting in some cases. Finally, we"}, {"title": "REPRODUCIBILITY STATEMENT", "content": "Our codes are available at https://github.com/ohayonguy/PMRF. We provide all the explanations and checkpoints necessary to reproduce our results, including training, inference, and the computation of the distortion and perceptual quality measures in Section 5. Besides our code, our paper discloses all the implementation details required to reproduce the results, including architecture details, training hyper-parameters, etc. Refer to Sections 5.1 and 5.2 and appendices B and C for implementation details, and to Table 12 in the appendix for a summary of our training hyper-parameters."}, {"title": "A SUPPLEMENTARY EXPLANATIONS FOR PMRF", "content": null}, {"title": "A.1 PROOF THAT CONDITIONING ON X* ACHIEVES THE SAME MSE AS POSTERIOR SAMPLING", "content": "Proposition 2. Let X' be the estimator which, given any degraded measurement y, first predicts the posterior mean x* = E[X|Y = y] and then samples from px|x\u2217(\u00b7|x\u2217)3. Then, the MSE of X' equals twice the MMSE, which is the MSE attained by the posterior sampler."}, {"title": "A.2 PROOF OF PROPOSITION 1", "content": "For completeness, we first restate Proposition 1 and then provide its proof.\nProposition 1. Suppose that \u03c3\u03b5\n\u2192 0, and let us assume that the solution of the ODE in Equation (11) exists and is unique. Then,\n(a) Z1 attains a perfect perceptual index (pZ1 = px).\n(b) The MSE of Z1 cannot be larger than that of the posterior sampler.\n(c) If the distribution of (X \u2212 X\u2217)|Zt = zt is non-degenerate for almost every zt \u2208 supp pz and t \u2208 [0, 1], then the MSE of Z1 is strictly smaller than that of the posterior sampler."}, {"title": "A.3 PROOF OF THE RESULTS IN EXAMPLE 1", "content": "From (Blau & Michaeli, 2018; Freirich et al., 2021), we know that X0 in Example 1 attains a MSE that is strictly smaller than that of the posterior sampler (assuming that \u03c32N > 0). Specifically, the closed-form solution of X0 in Example 1 is given by (Freirich et al., 2021):\n$$X_0 = \\frac{1}{\\sqrt{1 + \\sigma_N^2}}Y.$$\nMoreover, in this example, it is well known that the posterior mean E[X|Y] is given by\n$$X^* = \\frac{1}{1 + \\sigma_N^2}Y.$$\nNext, we will prove that:\n(a) All the assumptions in Proposition 1 hold.\n(b) Z1 = X0 almost surely."}, {"title": "A.4 REFLOW (OPTIONAL)", "content": "To potentially improve the MSE of PMRF further, one may conduct a reflow procedure (Liu et al., 2023), where a sequence of flow models are trained, and the flow model at index k + 1 learns to flow from the source distribution to the distribution generated by the flow model at index k. Specifically, let Zk+1 be the random vector generated by PMRF (Algorithm 1), where Zk replaces the role of X in Algorithm 1 and Z0 = X (Z0 remains unchanged). Thus, from Theorem 3.5 in (Liu et al., 2023), we have E[c(Zk+1 \u2212 Z0)] \u2264 E[c(Zk \u2212 Z0)], which implies the reflowing may only improve the MSE of PMRF, and hence improve the approximation of the desired optimal transport map (Equation 4). We leave this possibility for future work."}, {"title": "B SUPPLEMENTARY DETAILS AND EXPERIMENTS IN BLIND FACE IMAGE RESTORATION", "content": "Unfortunately we do not compare with FlowIE (Zhu et al., 2024), as the checkpoints in the official repository of this method seem to not work at the moment. Note that FlowIE is a conditional method that utilizes a ControlNet (similarly to DiffBIR), so it is not similar to our PMRF algorithm."}, {"title": "B.1 IMPLEMENTATION DETAILS OF PMRF", "content": "During training, we only use random horizontal flips for data augmentation. We use the SwinIR (Liang et al., 2021) model trained by Yue & Loy (2024) as the posterior mean predictor fw\u2217 in Algorithm 1, and use \u03c3\u03b5 = 0.1. This model was trained using the same synthetic degradation as in Equation (12), with the same ranges for \u03c3, R, \u03b4, and Q we mentioned in Section 5.1. The SwinIR model's weights are kept frozen during the vector field's training stage, and the same weights are utilized during inference as well. The vector field v\u03b8 is a HDiT model (Crowson et al., 2024), which we train from scratch. As in (Crowson et al., 2024), we sample t uniformly from U[0, 1] using a stratified sampling strategy. The vector field is trained for 3850 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019), with a learning rate of 5\u00b710\u22124, (\u03b21, \u03b22) = (0.9, 0.95), and a weight decay of 10\u22122 (as in (Crowson et al., 2024)). In the last 350 epochs, we reduce the learning rate gradually, multiplying it by 0.98 at the end of every epoch. The training batch size is set to 256 and is kept fixed. We compute the exponential moving average (EMA) of the model's weights, using a decay of 0.9999. The EMA weights of the model are then used in all evaluations. Our model is trained using bfloat16 mixed precision. A summary of the vector field training hyper-parameters is provided in Table 12."}, {"title": "B.2 VARYING THE NUMBER OF FLOW STEPS K IN PMRF", "content": "In Tables 2 to 6 we evaluate the performance of PMRF for various choices of K (the number of inference steps in Algorithm 1). As expected, increasing K generally improves the perceptual quality while harming the distortion."}, {"title": "B.3 DETAILS OF DOT", "content": "We use the official codes of DOT (Adrai et al., 2023) as provided by the authors. This method performs optimal transport between the source and target distributions in latent space, using the closed-form solution for the optimal transport map between two Gaussians. As in (Adrai et al., 2023), we use the VAE (Kingma & Welling, 2014) of stable-diffusion (Rombach et al., 2022). For computing the latent empirical mean and covariance of the target distribution, we provide to the code the first 1000 images from FFHQ, with images of size 512 \u00d7 512 (the default is 100 images, so using 1000 images instead ensures that the performance of DOT is not compromised, as explain by Adrai et al. (2023)). For computing the latent empirical mean and covariance of the source distribution, we randomly synthesize degraded images according to Equation (12) from the first 1000 images in FFHQ, and reconstruct each image using the SwinIR model with the pre-trained weights from (Yue & Loy, 2024) (the same weights we use in PMRF). Given a degraded image y at test time, the code of Adrai et al. (2023) first predicts the posterior mean using the SwinIR model, encodes it to latent space, optimally transports the result using the pre-computed empirical means and covariances, and finally uses the decoder to obtain the reconstructed image."}, {"title": "B.4 COMPUTATION OF FID, KID, AND PRECISION", "content": "For each data set and algorithm, the FID, KID, and Precision are computed between the entire FFHQ 512 \u00d7 512 training set, and the reconstructed images produced for the degraded images in the test data set (as in previous works). For example, for the evaluations on the CelebA-Test data, this means that the FID is computed between the 70,000 FFHQ images, and the 3,000 CelebA-Test reconstructed images."}, {"title": "C SUPPLEMENTARY DETAILS ON SECTION 5.2", "content": null}, {"title": "C.1 DEGRADATIONS", "content": "The degraded images in each task in the controlled experiments are synthesized according to the following degradations:\n1. Denoising: We apply additive white Gaussian noise with standard deviation 0.35.\n2. Super-resolution: We use the 8\u00d7 bicubic down-sampling operator, and add Gaussian noise with standard deviation 0.05.\n3. Inpainting: We randomly mask 90% of the pixels in the ground-truth image, and add Gaussian noise with standard deviation 0.1.\n4. Colorization: We average the color channels in the ground-truth image (with a weight of 1/3 for each color channel), and add Gaussian noise with standard deviation 0.25."}, {"title": "C.2 IMPLEMENTATION DETAILS OF THE FLOW METHODS", "content": "Training. For all restoration tasks in Section 5.2, the models are trained on the FFHQ data set with images of size 256 \u00d7 256 (we down-sample the original 1024 \u00d7 1024 images to 256 \u00d7 256). Unlike in the blind face image restoration experiments, where the model is trained on images of size 512 \u00d7 512, here we choose to use a smaller image resolution to save computational resources and achieve shorter training times. During training, we only use random horizontal flips for data augmentation.\nChoice of \u03c3\u03b5. As expected, we observe that using \u03c3\u03b5 = 0 in both PMRF (Algorithm 1) and the flow from Y method (Algorithm 4) leads to blurry results with small MSE and large FID. Thus, for a fair comparison, we use the same value of \u03c3\u03b5 > 0 in both methods. For the denoising task we use \u03c3\u03b5 = 0.025, and for the rest of the tasks (inpainting, colorization, and super-resolution), we use \u03c3\u03b5 = 0.15.\nPosterior mean predictor. The posterior mean predictor fw is a 4.4M parameters SwinIR model6 which we train from scratch for each task. In all tasks, this model is trained for 1000 epochs, with a fixed batch size of 256, using the AdamW optimizer with a learning rate of 5 \u00d7 10\u22124, (\u03b21, \u03b22) = (0.9, 0.95), without weight decay, and without learning rate scheduling. When utilizing this model in the flow process (e.g., in PMRF), we use the EMA weights computed with a decay of 0.9999.\nVector field. Similarly to Appendix B.1, the vector field is a HDiT model. The time t in Algorithms 1 and 2 to 4 is sampled from U[0, 1] using a stratified sampling strategy. For all baseline methods and PMRF, we train the vector field for 1000 epochs, use a fixed batch size of 256, adopt the AdamW optimizer with a learning rate of 5 \u00d7 10\u22124, (\u03b21, \u03b22) = (0.9, 0.95), and a weight decay of 10\u22122. As in (Crowson et al., 2024), we do not apply learning rate scheduling. Finally, we use the EMA weights for evaluation, using a decay of 0.9999. A summary of the hyper-parameters is provided in Table 12.\nEvaluation. We test all models on the CelebA-Test data set, with images of size 256 \u00d7 256. The FID of each method is computed between the entire FFHQ 256 \u00d7 256 training set, and the images produced by the algorithm for the synthesized CelebA-Test degraded images. The MSE is computed between the reconstructed images and the corresponding ground-truth images."}, {"title": "C.3 DETAILS OF DOT.", "content": "We use DOT (Adrai et al., 2023) similarly to Appendix B.3, using images of size 256 \u00d7 256 instead of 512x512, and adopting the official codes of the authors. For the source distribution, we randomly"}, {}]}