{"title": "Speech Is Not Enough: Interpreting Nonverbal Indicators of Common Knowledge and Engagement", "authors": ["Derek Palmer", "Yifan Zhu", "Kenneth Lai", "Hannah VanderHoeven", "Mariah Bradford", "Ibrahim Khebour", "Carlos Mabrey", "Jack Fitzgerald", "Nikhil Krishnaswamy", "Martha Palmer", "James Pustejovsky"], "abstract": "Our goal is to develop an AI Partner that can provide support for group problem solving and social dynamics. In multi-party working group environments, multimodal analytics is crucial for identifying non-verbal interactions of group members. In conjunction with their verbal participation, this creates an holistic understanding of collaboration and engagement that provides necessary context for the AI Partner. In this demo, we illustrate our present capabilities at detecting and tracking nonverbal behavior in student task-oriented interactions in the classroom, and the implications for tracking common ground and engagement.", "sections": [{"title": "Introduction", "content": "Our goal is developing an AI partner that can provide beneficial information or suggestions to groups of collaborators in real time. Essential to this process is an accurate interpretation of two dimensions of the AI partner's environment: the working group's knowledge of the topic, and the current social dynamics of the group. Multi-modal analysis offers unique analysis of vital non-verbal cues (Dey and Puntambekar 2023). Also, the more complex and novel the environment, the less reliable automatic speech recognition is. Multi-channel input is crucial for useful input to AI partners.\nA demo video is at https://youtu.be/WzajCzOYggg."}, {"title": "The Task", "content": "Our Institute for Student-AI Teaming (iSAT), aims to develop an AI Partner that can intervene positively in collaborative problem solving groups of students (D'Mello et al. 2024). Student group productivity can be heavily influenced by social dynamics (Moulder, Duran, and D'Mello 2022). Social dynamics can be positive or negative based on the behavior and level of engagement of each individual member (Adams-Wiggins and Dancis 2022). Social cohesion is positive social dynamics manifesting as high levels of engagement for all group members culminating in constructive progress towards the group's goal. Negative social cohesion can be either low levels of engagement of the group with little progress towards the goal or unconstructive interactions. Automatic speech recognition is a vital part of identifying both positive and negative social situations. However, as the amount of topics, participants, and background noise expands, ASR accuracy decreases (Cao et al. 2023b), increasing reliance on multimodal analysis, especially with speaker cohorts such as children with minimal training data. Also, many non-verbal interactions are simply inaccessible to ASR and critical to capture via multi-modal analysis.\nTracking gestures such as pointing can be indispensable in building real-time understanding of a group's common knowledge (Khebour et al. 2024b; Tu et al. 2024). Tracking each individual's posture over time, in particular leaning in or leaning out, is a powerful indicator of a group's engagement level (Adams-Wiggins and Dancis 2022). Joint visual attention is critical to contextualize both common knowledge and group engagement. All together these modalities help pinpoint intervention opportunities and avoid un-constructive interruptions (Cao et al. 2023a). We illustrate multi-modal analysis in both dimensions: 1) a knowledge support analysis with our Fibonacci weights exercise; 2) contrasting levels of engagement only observable via multimodal analysis for our simulated classroom environment."}, {"title": "Our Setup", "content": "The physical task space consists of a table with task-relevant objects on it and 3 participants seated around it. The task is recorded using an Azure Kinect RGBD camera, and an MXL AC-404 ProCon microphone. The Kinect automatically tracks 32 joints per body, covering head, torso, and limbs, returning 3 position and 4 orientation values for each.\nGaze detection uses the direction of participants' noses as a proxy. Using the joints of bodies extracted by the Azure Kinect SDK, we take the average position of both ear joints, which results in a point roughly behind the nose, and use a vector connecting this point and the nose joint to indicate gaze direction. We extend this vector (purple) into 3D space to see which objects participants' gazes are landing on.\nPosture detection determines the participants' positions (left, middle, right) using the x-coordinate of the pelvis of each body. Each participant's position and orientation information is then flattened. The vectors are stacked and then input into a two-layer feedforward neural network. We train three such models, one for each participant position.\nGesture recognition primarily concerns pointing detection. We use a lightweight 2-stage method from VanderHo-"}, {"title": "Video Content", "content": "Our two scenarios (Figs. 1 and 2) prioritize different aspects of multimodal information processing: knowledge support often includes the evaluation of specific objects and classroom discussions do not. The context for knowledge support is equally dependent on joint visual attention, gesture and domain specific object detection. For social cohesion, joint visual attention to speakers and posture are more valuable.\nScenario 1: Fibonacci Weights Task To better evaluate the accuracy and utility of object detection, we developed a situated collaborative task wherein participants infer the weights of a set of differently weighted blocks with the use of a balance scale (Khebour et al. 2024a). The increases in weight adhere to the Fibonacci series. A series of lab experiments provided video data for annotation and training purposes (see Fig. 1). The expectation is that object detection provides valuable input for AI Partners offering knowledge support. With minimal amounts of training data we can port to similar new objects specific to new domains.\nScenario 2: Simulated Classroom Project Planning The simulated classroom content we are demoing is a project planning scenario from Lesson 4 of the Sensor Immersion Curriculum Unit developed by the SchoolWide Labs project (Biddy et al. 2021). In the lesson each student comes into the group as the sole expert on one of three specific sensor types. The students answer factual questions about the capabilities of their sensors and brainstorm about potential"}, {"title": "Conclusion and Future Work", "content": "We have demonstrated the importance of integrating non-verbal behavior recognition into the modeling and interpretation of multi-party dialogues when there is an intervention objective. With an eye on portability, our next focus for object detection will be devices common to most settings such as tablets, laptops and phones. We expect that our approach to modeling social cohesion will port to any AI partner included in a working group of 3 or more, given a document summarizing the specific topic and a task specific model of engagement (Moore 2016; Kofod-Petersen, Wegener, and Cassens 2009). This could include business, government, health, or education settings, such as board meetings, working task forces, training exercises, etc. An opportunity also exists for decreasing the time intensive labor of video annotation tasks for qualitative research purposes."}]}