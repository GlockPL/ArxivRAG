{"title": "Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms", "authors": ["Rajvardhan Oak", "Muhammad Haroon", "Claire Jo", "Magdalena Wojcieszak", "Anshuman Chhabra"], "abstract": "Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.", "sections": [{"title": "1 Introduction", "content": "Social media platforms are powered by Machine Learning (ML) and Artificial Intelligence (AI) based recommendation algorithms and models that provide content for users. These algorithms and models are designed to maximize user engagement by learning to recommend content aligned with users' inferred interests or traits (Covington et al., 2016). However, solely optimizing for user engagement metrics can indirectly drive exposure to harmful content. For instance, a teenager interested in fitness may be recommended content promoting eating disorders, users interested in finance may encounter clickbait and scam videos, and a sad adolescent may be directed to content about depression or suicide (Hilbert et al., 2024). As a result, there are serious concerns that platform recommendation systems can indirectly foment misinformation, addictions or mental health crises, and lead to other problems for individuals and society at large (Haidt and Twenge, 2022, 2023; Roose, 2019; Tufekci, 2018).\nInternally, social media platforms seek to mitigate harmful content using AI/ML classifiers. However, there are two major challenges associated with their use (Gorwa et al., 2020; Chen, 2021; Khan and Wright, 2021): (1) classifiers often require large volumes of annotated data for training, and (2) categorizing harmful content is a dynamic temporal problem (e.g. a new dangerous challenge for teenagers emerges online). Classifiers cannot automatically generalize to new forms of harm, without having been trained on explicitly labeled data. As a result, harm classifiers are susceptible to concept drift (Qui\u00f1onero-Candela et al., 2022) and requiring humans to annotate large amounts of data.\nIn this paper, we propose methods that can circumvent both issues: that of scale as well as that of the dynamicity of harm. We utilize Large Language Models (LLMs) for this purpose as they have been shown to demonstrate stellar reasoning capabilities on natural language input in the zero-shot or few-shot learning setting (Wei et al., 2022) making them ideal for harm mitigation. We find that as opposed to traditional harm classifiers such as Perspective API, LLMs can excel at mitigating exposure to harm in recommended content sequences simply by pairwise comparisons and re-ranking, most likely due to better utilization of their context window. The benefits of pairwise re-ranking have been explored in past work (Qin et al., 2024) and we adopt this re-ranking strategy using LLMs to mitigate exposure to harmful content on platforms. This approach is visually depicted in Figure 1.\nWe propose two novel metrics to dynamically analyze exposure to harmful content in recommendation sequences and experiment with three LLM preference re-ranking methods (zero-shot, few-shot, in-context learning based (Dong et al., 2022)). Through extensive experiments on three harm datasets and three LLM architectures, we find that our approach outperforms state-of-the-art content moderation baselines, such as Perspective API and OpenAI's Moderation API. In summary, we make the following contributions:\n\u2022 We propose a novel LLM-based approach that can measure relative harm and mitigate harmful content exposure on platforms. Our approach is able to generalize to various kinds of harms without explicit training.\n\u2022 We examine the performance of our approach under three settings (zero-shot, zero-shot with prompt engineering and few-shot) by varying the amount of information provided to the LLM, and find that it outperforms industry-grade classifiers even in the zero-shot setting.\n\u2022 Alongside evaluation on traditional metrics, we propose two novel metrics that can measure harm exposure for a given content sequence. Our metrics allow evaluating the quality of re-ranked content in a manner that is agnostic to the amount of harm present wherein.\nThis paper is organized as follows. First, we discuss related work in the areas of harms caused by recommendation systems and efforts towards mitigation to contextualize the novelty of our approach. Then, we present our methodology at a high level and introduce two novel metrics for evaluating re-ranking. Next, we describe our experimental setup and analyze our results. Finally, we discuss the limitations of our work and its implications."}, {"title": "2 Related Work", "content": "2.1 Recommendation-Driven Harms\nPlatform recommendation systems are designed to maximize user engagement by tailoring content to individual preferences and interests. However, these systems can inadvertently drive users toward harmful content. This occurs because recommendation algorithms often prioritize content that generates the most interaction, which can include sensational or provocative material (Rathje et al., 2021; Yu et al., 2024). For instance, Facebook's algorithmic curation is linked to the widespread use of clickbait, which drives users toward low-quality content (Lischka and Garz, 2021).\nIn addition, digital traces left by each user on platforms reveal information about the user's emotions (Hossain and Muhammad, 2019), substance use (Kosinski et al., 2013), or sexual orientation (Wang and Kosinski, 2018). This inferred information could be used to recommend content that exposes users\u2014especially the vulnerable ones-to online harms (e.g, addictive content to users known to use substances, suicidal content to depressed users, misinformation to users interested in herbology) (WSJ Staff, 2021). Indeed, meta-reviews show that 8%-10% of recommendations pose detectable risks to users (Hilbert et al., 2024) and algorithmic audits detect discriminatory or otherwise harmful biases in YouTube, Instagram, and TikTok algorithms (Haroon et al., 2023; Bandy, 2021; Hilbert et al., 2023).\nBecause recommendation systems are exceptionally good at curating personalized ecosystems, leading to closed loops of content consumption (Rossi et al., 2021), what they recommend has implications for what users see (Nyhan et al., 2023). In turn, content exposure has over-time effects on individuals and, when harmful, can have severe consequences on mental health and may foment addictions, violence, or even lead to death from dangerous challenges (Haidt and Twenge, 2022).\n2.2 Harm Mitigation\nExtensive work studies harm mitigation interventions on social media. For example, Bhargava et al. (2019) developed a tool that allows users to exert more control over their social media feeds by enabling them to consolidate and tailor content from multiple platforms. Similarly, Kovacs et al. (2018) empower users to manage their social media engagement goals via rotating time or site blockers. More germane to minimizing harmful content, other work explores ranking based interventions (Celis et al., 2019; Ovadya and Thorburn, 2023), such as using LLMs for curating societally beneficial recommendations. In addition, machine learning models, particularly those leveraging deep learning, are widely adopted to identify patterns indicative of harmful content, such as clickbait (Ghosh et al., 2022), hate speech (Mossie and Wang, 2020; Del Vigna12 et al., 2017) and misinformation (Wu et al., 2019; Shu et al., 2017).\nRecent work additionally applies LLMs to the harm detection problem (Liu et al., 2024; Ernst, 2024) and also utilized machine learning toxicity detection algorithms and a browser extension that automatically hides toxic text content on users' feed over-time (Beknazar-Yuzbashev et al., 2022).\nRemark. While prior works focus on specific harms, we consider a general, systematic, and overarching taxonomy of harms with data originating from a real social media platform (YouTube), as detailed below. Our approach generalizes to various kinds of harms, as opposed to being effective on one particular kind, like hate, misinformation, or clickbait. We leverage LLMs and their inherent reasoning abilities to reduce harmful content exposure in recommended content. Additionally, because we do not require explicit labeling and training, our approach is robust against concept drift (Qui\u00f1onero-Candela et al., 2022). Finally, instead of absolute harm, we focus on measuring relative harm and propose novel metrics to quantify the ordering of content for minimal exposure to harm."}, {"title": "3 Mitigating Harmful Content Exposure Using LLMs and Re-ranking", "content": "3.1 Problem Formulation\nThe problem we seek to solve is as follows:\nGiven a recommendation sequence (e.g. homepage videos on YouTube), re-rank the content so that harmful content appears at the end of the sequence in a zero-shot or few-shot setting (i.e. limited annotations are required).\nThe motivation for downranking comes from past work that has shown that users are less likely to interact with (and be exposed to) content that appears at lower ranks in recommendations (Yu et al., 2023; Glick et al., 2014). Downranking, as opposed to outright suppressing or deletion, minimizes exposure to harmful content while preserving transparency, diverse viewpoints and preferences, and freedom of expression. Formally, let $X = \\{x_i\\}_{i=1}^n$ be a sequence of n content instances, out of which p are non-harmful, and the remaining $n-p$ are harmful. Let $\\rho: X \\rightarrow \\{0,1\\}$ be a binary decision function that maps every x to a label based on whether it is harmful or not. Our goal is to use an LLM L to transform X into another sequence $X^*$ such that it minimizes exposure to harmful content.\n3.2 Preferential Pairwise Ranking\nWe present our proposed solution to the harm mitigation problem as Algorithm 1. Our proposed approach consists of a pairwise ranking component that seeks to downrank content if deemed harmful by the LLM L. Specifically, we present L with pairs of content instances and query it to determine which is the relatively harmful one. Certain preference constraints (C) determine the exact prompt used to query the LLM. To re-rank content based on relative harm, we adopt the approach from Qin et al. (2024) but modify their methodology and scoring function. For a given content instance x, their scoring function increments the score by 1 for every content instance deemed less relevant than x, and by 0.5 for all other content instances. However, applying the scoring function as is, will also compare non-harmful content with one another, and incorrectly result in scores being higher. This can then lead to them being unfairly downranked. To address this challenge, in the pairwise ranking process, we allow the LLM L to decide if both instances are non-harmful, in which case there is no increment to the score. Given a content sequence X, we enumerate all possible pairs and compute the score for every content instance. We then re-order the content based on this score obtained.\n3.3 Specifying Preference Constraints C\nAs described in Algorithm 1, the pairwise ranking via LLMs also requires natural language preference constraints C as part of the prompt to effectively downrank harmful content. We employ three approaches in specifying the preference constraints for the LLM in the context window/prompt. We mention the approaches here, and offer the detailed prompts in Appendix C.3.\nZero-Shot In the zero-shot setting, C asks to identify which of the two provided content instances is harmful, without explicitly specifying a definition of harm. In this approach, we utilize the LLM's inherent understanding of harm learnt during pre-training.\nZero-Shot + Prompt Engineering We build upon the zero-shot approach by including a definition for harm in C. We first define explicitly what we consider harmful and characteristics of harmful content, and then query L to identify which of the two content instances is harmful.\nFew-Shot ICL We now provide representative instances of harmful content in C, and query L to reason which of the two content instances is harmful based on this information. This approach is known as In-Context Learning (ICL) (Dong et al., 2022).\n3.4 In-Context Learning (ICL)\nIn-Context Learning (ICL) (Dong et al., 2022) relies on exemplars that the model is exposed to in order to learn certain features or characteristics. Therefore, the performance of our re-ranking approach will greatly depend on the chosen exemplars. A naive approach of ICL would be to randomly sample from harmful content and provide these random exemplars to the model; however this may lead to bias towards or against certain kinds of harm. To address this, we curate a set of exemplars that is representative of the harm in our dataset. Inspired by a popular coverage-based BertScore ICL selection approach (Gupta et al., 2023), we make some modifications for undertaking ICL in the re-ranking setting. We first use a pretrained ROBERTA model to project text content into an embedding space. We cluster the harm samples, and then choose the most representative sample from each cluster (the one that is closest to the centroid of the cluster). These representative samples form the exemplars we provide to the model.\n3.5 Evaluating Re-rankings\nWhile we use existing metrics for evaluating rankings (Sebe et al., 2000) such as Precision@K (Shani and Gunawardana, 2011), we also design new metrics that focus on the relevance of ranked content to a query and help account for harmful content that users could be exposed to in the sequence. Thus, we propose two novel metrics to assess preference in re-ranked content sequences. Note that both metrics are bounded between [0, 1] and higher values are better (less harmful content).\nPer-Pref-k The Per-Pref-k (PPk) metric represents the fraction of the content sequence set X that would need to be consumed to reach the k-th harmful content instance. PPk assesses how much of the sequence a user needs to consume before encountering a certain amount of harmful content. This metric is instrumental in understanding the depth of user engagement required to reach less desirable content, thus indirectly measuring the buffer of harmless content. Higher values indicate that a user can view more content before encountering a specified number of harmful instances, reflecting better performance of the moderation system. Formally, we define PPk as:\nPPk = $\\frac{m}{n}$ min $\\{m | \\sum_{i=1}^{m} \\rho(x_i) = k\\}$          (1)\nExponentially Weighted Normalization The Exponentially Weighted Normalization (EWN) metric provides an analytical measurement of non-harmful rankings by assigning exponentially decaying weights to ranks and then normalizing values to lie between [0,1]. EWN(X) measures the goodness of the ranking in the sequence X by comparing it to the best and worst ranking possible. A value of 1 indicates that the sequence is in the optimal order and no better order can minimize exposure to harm. On the other hand, a value of 0 indicates that the sequence is in the worst possible ranking order. Following the notation described earlier, the EWN can be defined as follows:\nEWN = $\\frac{\\sum_{i=1}^{n}{\\{2^{1-i} \\cdot (1 - \\rho(x_i))\\} - (2^{-p} - 2^{-n})}}{(1 - 2^{-p-n}) \\cdot (1 - 2^{-p})}$             (2)\nOwing to space limitations, we provide the complete derivation for EWN in the appendix."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\nWe employ a curated dataset of YouTube videos (9,832 harmful, 2, 679 harmless), which were labeled for six categories of harm: information, hate and harassment, addictive, clickbait, sexual, and physical harms. The details on the harm categories, data collection, labeling, reliability, and other aspects of this dataset are provided in (Jo et al., 2024; Jo and Wojcieszak, 2025). Note that we primarily utilize this dataset for the majority of our ablation experiments because of the diversity in harm categories. We use the video descriptions as input to the LLM. To demonstrate the generalizability of our approach, we also evaluate it on two singular-harm category datasets; the Jigsaw Toxicity Classification Dataset (Jigsaw, 2019), which consists of comments from the Civil Comments platform, labeled for toxicity at Jigsaw, and the Measuring Hate Speech dataset (D-Lab, 2022) by UC Berkeley's D-Lab, that contains annotated social media posts specifically labeled for hate speech. Additional details on each dataset are provided in Appendix B.\n4.2 Content Sequences\nNote that our proposed approach operates on content sequences rather than individual instances. We use the datasets described to simulate sequences of content that a user would be exposed to. We sample uniformly at random without replacement from our data to generate a sequence of n = 20 content instances (textual data; either titles and transcripts from YouTube videos, comments, or social media posts, depending on the dataset), and generate m = 100 such sequences to form a sequence dataset. To study the effect of the amount of harm, we generate 5 such datasets by the varying the fraction of harmful content from 10% to 50% for the YouTube dataset. For the other datasets, we fix the fraction of harmful content to 30% so as to reflect the typical harm ratio observed in the wild (Beata Mostafavi, 2020).\n4.3 Baselines\nWe leverage two state-of-the-art harm classification models as benchmarks to compare the performance of our LLM re-ranking approach. The Perspective API 3 is a tool developed by Google/Jigsaw to improve conversations online by detecting toxicity of comments. We use the toxicity score the model returns to rank the content. Similar to Perspective, the OpenAI Moderation API\u2074 can be used to check whether a text is harmful or not. It provides scores for a variety of harm categories, ranging from hate to sexual or violence; we extract the scores for each category and use the highest score as a proxy for harm.\n4.4 Implementation Details\nWe implement our approach using mainly OpenAI GPT-3.5 Turbo as the underlying LLM. This choice was made based on the available models, API costs, and rate limitations at the time of the initial writing. Additionally, we ran experiments using two open-source models: Llama2-13B and Mistral-7B-Instruct-v0.2. The exact prompts we use for each of our approaches are listed in Appendix C.\n4.5 Evaluation\nWe use three metrics to evaluate the effectiveness of re-ranking: TP-k, PP-k and the EWN as described. Given that our sequences are of 20 videos, we limit our analyses to TP5 and TP10, as they represent the first two quartiles of videos watched. Similarly, for PP-k, we focus our analysis on PP1 - 3, as they represent the amount of content needed to be watched to reach up to at most the third harmful video, which is a practical limit considering the length of our sequences. The PP1, PP2 and PP3 values represent the fraction of the sequence that can be consumed before encountering the first, second and third harmful video(s), respectively."}, {"title": "5 Results", "content": "5.1 Effectiveness of LLM Preference Re-ranking\nFigure 2 depicts the effectiveness of our approaches with respect to the baselines, in terms of our metrics. In general, our approach outperforms both the baselines across varying harm percentages, demonstrating the improvement preference re-ranking drives over simple classification. We find that LLM-based approaches outperform standard classification approaches even in zero-shot settings, and even more so in few-shot settings. For example, TP5 (Figure 2A) indicates the average proportion of non-harmful videos in the first five videos in the sequence. This proportion is 70.5% and 74.8% in the 10% and 50% harm settings initially. The OpenAI Moderation API is able to drive these up to 83% and 78.7%. The preference re-ranking approach in the Few-Shot ICL setting is able to increase the non-harmful videos to 90.5% and 86.1% respectively. Mean TP5 and TP10 values are shown in Tables 1 and 2 respectively. Across both tables, the more advanced configurations (Zero-Shot with PE, Few-Shot ICL) consistently show higher TPk values across all harmful content percentages, demonstrating the advantage of leveraging sophisticated AI techniques. Notably, the effectiveness of the OpenAI Moderation and Perspective approaches varies, often underperforming compared to the LLM-based approaches, despite the massive amounts of task-specific data they are trained on.\n5.2 Effect of Harm Ratio\nIn Table 6, we present EWN values for each constraint specification approach across various harm percentages. We see that the difference in EWN values between the original and re-ranked sequences grows as the percentage of harmful content increases. For instance, at 10% harmful content, the gap between the Original and Zero-Shot with PE is about 0.079 (9.1% relative increase), while at 50% harmful content, the gap extends to 0.250 (51.3% relative increase). Additionally, as the percentage of harm increases, OpenAI Moderation and Perspective, show large decreases in their EWN values (39.1% and 41.8% respectively). Zero-Shot with PE and Few-Shot ICL, on the other hand, exhibit the smallest decreases in their EWN scores as the percentage of harmful content increases. Both configurations manage to minimize the performance drop to around 23%. Given that EWN is agnostic to the relative harm ratio (see Appendix A), this analysis shows that as the operational environment becomes more challenging due to higher concentrations of harmful content, the advantage of deploying advanced AI-based content moderation systems becomes increasingly substantial.\n5.3 Varying Exemplars for Few-Shot ICL\nBecause the performance of ICL will depend on exemplars, we experimented with various values for the number of exemplars N. We modify the few-shot ICL approach and implement it in five versions by setting the value of N as one of 4, 8, 12, 16 and 20. We perform K-Means clustering on the harm samples (setting K = N) and choose a representative sample from each cluster. We conduct our analysis in the 30% setting as that is the typical harm ratio observed (Beata Mostafavi, 2020) online. Table 7 and Figure 3 show the performance of ICL-based prompting with varying number of exemplars. Surprisingly, the performance does not necessarily improve with increasing number of exemplars; we see that EWN is the best for N = 4 and decreases with increasing N. This is likely due to overfitting the LLM and the bias introduced because of the exemplars. Nevertheless, ICL still remains a better approach as compared to zero-shot or zero-shot with prompt engineering.\n5.4 Experiments across Datasets\nThe results described so far are based on the YouTube dataset (Jo and Wojcieszak, 2025) because of the comprehensive and diverse nature of harms it contains. However, we also find that our LLM-based re-ranking approach performs well on other, more targeted datasets. Table 11 shows that all three configurations (zero-shot, zero-shot with prompt engineering and few-shot) outperform the baselines (albeit by a small margin) on the hate speech dataset as evidenced by the EWN values. Notably, the Zero-Shot approach slightly outperforms the other configurations with a perfect TP10 score of 1.000 and slightly higher PP1 (0.697), PP2 (0.777), and PP3 (0.839) values, indicating a marginally better delay in encountering harmful content. The inclusion of Prompt Engineering and Few-Shot learning yields EWN scores very close to the perfect mark (0.99992 and 0.99991, respectively), though they slightly trail the pure Zero-Shot method. On the Jigsaw dataset (Table 10), our approach achieves a comparable performance to the baselines. Note that the Jigsaw dataset contains content labeled for toxicity, on which the Perspective API is likely trained (as both are released by Jigsaw). Perspective API attains uncharacteristically high performance on this dataset, potentially indicating test set leakage in this experiment.\n5.5 Experiments across LLMS\nThe results above have been based on the performance of GPT-3.5 as the primary LLM of choice. However, there are growing concerns about the lack of transparency and data-sharing practices in closed-source LLMs (Balloccu et al., 2024), which might deter developers from utilizing them. Further, the costs associated with querying the LLM at scale can also compound their use. In this section, we thus consider alternative open-source LLMs and re-run experiments with 30% harm, the typical harm ratio observed on platforms (Beata Mostafavi, 2020). We repeat the experiments conducted on the YouTube dataset using two LLMs, namely Mistral-7B-Instruct-v0.25 and Llama2-13B6, both running locally on a server equipped with a NVIDIA RTX A6000 GPU with 256GB of RAM.\nTable 8 shows the various metric values for the YouTube dataset using Mistral. We see that it too outperforms the baselines significantly and only has slightly lower performance compared to GPT-3.5-Turbo. In contrast, Table 9 that Llama2 underperforms across metrics compared to the OpenAI Moderation baseline. Comparing the models' performances with GPT-3.5 directly, Figure 4 shows the EWN values for all three models and each of the three learning strategies we propose. As is evident, GPT-3.5 outperforms both Mistral and Llama2 (likely) due to its larger parameter size. However, the EWN values of Mistral trail GPT-3.5 by an average of nearly 10%, a minimal performance trade-off that make Mistral a viable second option for our approach. Also note that Mistral has only 7B parameters, making it extremely lightweight to run locally and ingest data at scale. In contrast, Llama2 exhibits lower performance than the other two LLMs, a result also consistent with prior work which has found that Mistral outperforms Llama2 on most benchmark datasets (Jiang et al., 2023)."}, {"title": "6 Discussion", "content": "6.1 Key Takeaways\nIn this work, we propose methods that leverage LLMs to circumvent two challenges in online harm mitigation: the large scale of annotation required and the dynamic nature of harm.\nUtility of LLMs for Harm Mitigation. Our experiments demonstrate that our LLM-based re-ranking approaches significantly outperform traditional/proprietary content moderation methods in reducing harmful content exposure on social media platforms. As detailed above, both the Zero-Shot and Few-Shot In-Context Learning (ICL) configurations provide a notable improvement over industry-standard harm classifiers. Because our approach shows promising results even in zero-shot settings, LLMs can be used off-the-shelf with minimal effort and without necessitating significant re-configuration or fine-tuning. Customizing LLM prompts can lead to even better results; for example, the Zero-Shot + Prompt Engineering configuration, where harm is explicitly defined in the prompt, consistently performs better than the simpler Zero-Shot approach, indicating that our automated prompt adjustments can significantly influence desirable performance outcomes.\nRobustness to Concept Drift. Despite the differences in content types and the diversity of harm categories, our LLM approach consistently outperforms traditional methods, suggesting that the method is versatile and can be applied across diverse social media platforms and content/harm moderation challenges. Due to extensive pre-training, LLMs can generalize across various types of harm without needing explicit fine-tuning or labeled examples for each new harm type. Consequently, our approach excels at identifying and mitigating a wide range of harmful content, whether it involves misinformation, hate speech, clickbait, or addictive material, among other categories. While LLMs are still restricted temporally by training data, they can adapt to novel scenarios without requiring continuous retraining, and generalize better than traditional supervised learning methods.\nImpact of Harm Ratio on Re-Ranking Effectiveness. Our results show that the efficacy of re-ranking is closely related to the ratio of harmful content within the dataset. Specifically, the Exponentially Weighted Normalization (EWN) metric reveals that as the percentage of harmful content increases, downranking harmful content becomes increasingly more challenging. The advantage of our approach becomes even more pronounced in high-volume online harm scenarios where baselines suffer from significant performance drops, but our LLM-based methods exhibit minimal performance degradation.\nVersatility Across Models. Our experiments across various LLMs show that the performance of our approach will indeed depend on the choice of LLMs. While using commercial LLMs can be expensive, we show that open-source LLMs (such as Mistral) show comparable performance in harm mitigation. This means that our approach can be implemented with open-source models that will not result in excessive overhead due to API costs. Furthermore, satisfactory performance using open-source LLMs also ensures that our method can be utilized by developers concerned with data sharing/privacy issues relating to closed-source LLMs. We defer the study of other LLMs in the context of our approach to future work.\nImplications for Preference-Based Ranking. While we focus on re-ranking content to minimize exposure to harmful material, the underlying approach is general and can be adapted to optimize for various other applications beyond harm mitigation. For example, a platform might choose to re-rank content to elevate material that encourages civic participation or promotes mental well-being, thereby aligning recommendations with broader social goals. The flexibility of LLMs in understanding and applying different ranking criteria makes this approach highly adaptable and scalable to different contexts, where the optimization target could shift from reducing harm to enhancing specific positive outcomes for users."}, {"title": "7 Conclusion", "content": "In sum, we present a novel approach to mitigating exposure of harmful content on social media platforms by leveraging Large Language Models (LLMs) for re-ranking recommendation sequences. LLM-based re-ranking not only outperforms traditional classifier-based commercial moderation systems, such as the Perspective API and OpenAI Moderation API, but also exhibits robustness across various harm scenarios and content types. By utilizing zero-shot, few-shot, and prompt-engineered strategies, our method effectively generalizes to multiple forms of harm without the need for extensive labeled data, addressing both scalability and dynamicity issues inherent in content moderation. The proposed re-ranking technique shows substantial promise in reducing the likelihood of users encountering harmful content, even as the volume of such content increases in users' social media ecosystem (as is the case for the heavy consumers of misinformation, violence, among other harmful contents). For future work, we aim to explore multi-modal input/models to further augment performance and apply our methods to other novel problem domains where content ranking plays a critical role in ensuring user safety and trust."}, {"title": "Limitations", "content": "While our proposed approach outperforms existing baselines, it has certain limitations as well. The approach involves queries to LLMs, which may be expensive in terms of time and infrastructure cost when considered at the scale that social media platforms operate (although most popular social media platforms operate with infrastructure that can support such intervention strategies7). For smaller organizations, these issues can be minimized by lightweight (e.g. quantized) high-performance LLMs running locally (e.g. Mistral). Another limitation is modality; we only consider textual input in our algorithm and content sequences, even though visual information (e.g. video frames) could be incorporated from YouTube videos to improve performance further. We defer the study of multi-modal LLMs for re-ranking to future work."}, {"title": "Ethical Considerations", "content": "Through this work, we aim to demonstrate how LLMs can be used to minimize user exposure to harmful content online. Our work does not conduct research with human subjects directly. We do not release any new data; all data used are public, and have reported following required ethics guidelines in the collection phase. Sharing user content with third-party LLMs may introduce privacy challenges, and concerns that the data may be used for training without consent. However, social media companies can run open-source LLMs (which, from our findings, have performance comparable to proprietary ones) without relying on third parties, which would ensure that user content does not leave the company. Finally, while the primary intent of our approach is to mitigate exposure to harmful content, there exists the risk of over-moderation; LLMs, despite their advanced reasoning capabilities, might reflect inherent biases from their training data, potentially leading to the marginalization of minority perspectives or controversial opinions. Transparent moderation guidelines, diverse datasets and a human-in-the-loop moderation mechanisms can help safeguard freedom of speech and enforce effective moderation at the same time."}, {"title": "A Additional Details on EWN", "content": "A.1 Derivation\nLet X = $\\{x_i\\}_{i=1}^n$ be a sequence of n content instances, out of which p are harmless, and the remaining n \u2212 p are harmful. Let $\\rho$ be a decision function such that:\n$\\rho(x) = \\begin{cases} 1 & \\text{if x is non-harmful} \\\\ 0 & \\text{if x is harmful.} \\end{cases}$        (3)\nWe want to design a metric that penalizes harmful content instances shown towards the beginning of the sequence. Thus, content instances at numerically lower ranks have a higher importance in the metric. We can assign exponentially decaying weights to the content instances based on their rank. Specifically, the ith content instance will have a weight of 21\u2212i. Following this, we can compute a score S for the sequence X as:\nS(X) = $\\sum_{i=1}^{n} 2^{1-i} \\cdot \\rho(x_i)$.       (4\nThe value of S(X) indicates the goodness of the ranking in sequence X. Lower values indicate more harmful content shown early on. However, it is dependent on the number of content instances in the sequence and the harm ratio; as such, it is not a good metric to compare two sequences which differ in these distributions. Therefore, we want to compute a metric that indicates how good the sequence X is compared to the best possible ordering of the same content.\nBest Case: In the best case, all the p non-harmful content instances appear at the beginning of the sequence. Therefore, the score S(X) is\nS(X) = $\\sum_{i=1}^{p} 2^{1-i}$.       (5)\nThis is a geometric progression with the first term a = 1 and common ratio r = 2\u22121 having p terms. Therefore we have:\nSmax(X) = 2 \u00b7 (1 \u2212 21\u2212p).            (6"}, {"title": "", "content": "Worst Case: In the worst case", "have": "nSmin(X) = 2p-n+1 \u2212 21-n.       (8)\nScaling: Now", "EWN": "nEWN = $\\frac{\\sum_{i=1}^{n}{\\{2"}]}