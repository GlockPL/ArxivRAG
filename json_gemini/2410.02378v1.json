{"title": "Towards Comprehensive Detection of Chinese Harmful Memes", "authors": ["Junyu Lu", "Bo Xu", "Xiaokun Zhang", "Hongbo Wang", "Haohao Zhu", "Dongyu Zhang", "Liang Yang", "Hongfei Lin"], "abstract": "Harmful memes have proliferated on the Chinese Internet, while research on de-\ntecting Chinese harmful memes significantly lags behind due to the absence of\nreliable datasets and effective detectors. To this end, we focus on the compre-\nhensive detection of Chinese harmful memes. We construct TOXICN MM, the\nfirst Chinese harmful meme dataset, which consists of 12,000 samples with fine-\ngrained annotations for various meme types. Additionally, we propose a baseline\ndetector, Multimodal Knowledge Enhancement (MKE), incorporating contextual\ninformation of meme content generated by the LLM to enhance the understanding\nof Chinese memes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of MKE.", "sections": [{"title": "1 Introduction", "content": "With the development of the Internet, harmful memes on the web have become increasingly rampant.\nHarmful memes are typically defined as multimodal units consisting of an image and embedded text\nthat cause harm to an individual, an organization, a community, or a social group by specifically\ntargeting social entities (Pramanick et al., 2021a; Sharma et al., 2022b). They may exacerbate social\ndivisions, trigger discriminatory behaviors, and harm social harmony and unity (Kiela et al., 2020).\nDue to their negative impact on society, the widespread dissemination of harmful memes has been\nwidely recognized as a growing concern.\nIn recent years, researchers have made substantial progress in detecting harmful memes. Several\ndatasets, including HMC (Kiela et al., 2020), MMHS (Gomez et al., 2020), Harm-C, and Harm-P\n(Pramanick et al., 2021a), have been established, and various detectors have been proposed Hee\net al. (2022); Aggarwal et al. (2023); Pramanick et al. (2021b); Sharma et al. (2022a); Cao et al.\n(2022). However, most existing studies only focus on English memes. In contrast, Chinese harmful\nmeme detection remains largely unexplored, presenting challenges in building reliable datasets and\ndeveloping effective detectors.\nOn one hand, the types of Chinese harmful memes are diverse. In addition to those targeting specific\nsocial entities, many memes on Chinese platforms contain general offense, sexual innuendo, or"}, {"title": "2 Related Work", "content": "Harmful Meme. In recent years, researchers have noticed the importance of detecting harmful\nmemes. Several datasets have been established Kiela et al. (2020); Gomez et al. (2020); Pramanick\net al. (2021a). Nevertheless, most current studies only focus on English, while research on detecting\nChinese harmful memes remains unexplored. To this end, we present the first Chinese harmful meme\ndataset TOXICN MM. Here we list Table 1 to compare existing datasets with TOXICN MM.\nExisting studies define \u201charmful memes\" as memes that cause harm to specific social entities, based\non their social attributes such as religion, race, and gender (Pramanick et al., 2021a). However, this\ndefinition does not fully apply to the Chinese Internet, where harmful memes often exhibit potential\ntoxicity without specific targets but still perpetuate negative cultural values (Liu and Xu, 2016; Zhang\nand Zhao, 2021). In this paper, we consider different harmful types for comprehensive detection.\nSeveral methods have been proposed for harmful meme detection, primarily focusing on modeling\nbased on targets of memes Pramanick et al. (2021b); Sharma et al. (2022a); Koutlis et al. (2023); Ji\net al. (2023). However, they do not apply to Chinese harmful memes, which often lack specific targets.\nDespite this limitation, some methods enhance the detector's understanding of memes by integrating\nimage descriptions to make decisions Blaier et al. (2021); Cao et al. (2022). These studies still inspire\nus to integrate more comprehensive contextual information of meme content into detectors.\nToxic Language. Harmful memes are closely linked to toxic language (Kiela et al., 2020), which\nare rude, disrespectful, or unreasonable, and can drive people away from conversations (Dixon et al.,\n2018). Chinese harmful memes frequently feature toxic language in their inline text, utilizing slang\nand linguistic phenomena like homophony. Therefore, understanding Chinese memes necessitates\nthe incorporation of linguistic knowledge.\nAdditionally, labeling both harmful memes and toxic language is often subjective. Several studies\nhave addressed this issue by focusing on mitigating the subjective bias of annotators during the\nconstruction of toxic language datasets (Waseem and Hovy, 2016; Ross et al., 2016; Zeinert et al.,\n2021; Fortuna et al., 2022; Lu et al., 2023; Wang et al., 2023), enhancing the reliability of datasets. In\nthis paper, we adopt these measures as a reference in the annotation process of our TOXICN MM."}, {"title": "3 Dataset Construction", "content": "3.1 Overview\nIn this section, we detail the construction of our TOXICN MM dataset. We first define \"Chinese\nharmful memes\" to guide the dataset annotation. We then conduct fine-grained annotation for memes\ncollected from Chinese platforms. In addition to the basic binary labels, we analyze harmful memes\nfrom both harmful types and combinations of inline text and image information. After the annotation,\nstatistics of TOXICN MM are presented. The diagram of dataset construction is shown in Figure 2."}, {"title": "3.2 Definition Development", "content": "The recognized definition of \"harmful meme\" typically pertains to memes that target specific social\nentities. However, numerous memes on the Chinese Internet diverge from this definition by only\npropagating negative values without specific targets, which can be equally detrimental to society. To\nadapt to the Chinese online environment, a refined definition is necessary. Here we introduce the\ndefinition of Chinese harmful memes:\nChinese harmful memes are multimodal units consisting of an image and Chinese inline text that\nhave the potential to cause harm to an individual, an organization, a community, a social group, or\nsociety as a whole. These memes can range from offense or joking that perpetuate harmful stereotypes\ntowards specific social entities, to memes that are more subtle and general but still have the potential\nto cause harm. It is important to note that Chinese harmful memes can be created and spread\nintentionally or unintentionally. They often reflect and reinforce underlying negative values and\ncultural attitudes on the Chinese Internet, which are detrimental from legal or moral perspectives.\nAccording to the definition, we further identified the most common harmful types of memes on\nChinese platforms based on the consensus of social psychology (Liu and Xu, 2016; Lin and Zhang,\n2019) and communication (Peng, 2019; Zheng, 2016) studies. Specifically, it mainly includes\ntargeted harmful, general offense, sexual innuendo, and dispirited culture. The harm of these memes\nto individuals and society has been widely discussed. In this study, we focus on these harmful types\nwhen constructing the dataset."}, {"title": "3.3 Data Collection and Filtering", "content": "Data collection is the basic work of constructing datasets, and its breadth and quality greatly affect\nthe subsequent research. To ensure a comprehensive dataset, we collect Chinese memes from two\nwell-known public online platforms, Weibo and Baidu Tieba, both widely representative of local\nusers in China with active meme communities. We first conduct a random crawl to obtain a diverse\nset of memes. To maximize the inclusion of harmful memes, we further focus our data crawl on\nsensitive topics commonly debated online (e.g., \"gender\" and \"region\"). Additionally, we also target\nmemes expressing negative emotions and attitudes (e.g., \"crazy\" and \"Dispirited Culture\") to enrich\nthe dataset with samples potentially exhibiting toxicity. A total of about 14k memes are collected.\nWe then de-duplicate the data and filter out dirty samples including unreadable memes. The final\ndataset contains 12k refined memes.\nSubsequently, we utilize Baidu-OCR to extract inline text from memes, which offers a high-precision\nservice for Chinese text recognition. To further enhance the sample quality, we also introduce a\nmanual review process to examine the accuracy of the extracted text. Specifically, we normalize the\ntext by adding appropriate separators and removing additional line breaks and spaces."}, {"title": "3.4 Data Annotation", "content": "3.4.1 Annotator Selection and Training\nBefore the formal annotation process, it is crucial to\nselect annotators carefully and mitigate their subjective\nbias, as this can significantly impact the quality of the\ndataset (Waseem and Hovy, 2016). To this end, we\nadopt the following measures: The majority of active\nusers on Chinese platforms are between 12 and 35 years\nold. Considering Chinese laws that restrict individuals\nunder 18 from engaging in activities that could harm\ntheir physical or mental health, we selected annotators\naged 18 to 35. We assessed the annotators' proficiency\nin Chinese meme culture through questionnaires and ensured diversity in terms of gender, region, and\neducation level to enhance reproducibility. The demographics of annotators are shown in Table 2.\nDuring the training of annotators, we provided definitions of Chinese harmful memes, their various\ntypes, and conducted case analyses with diverse examples. To evaluate the annotators' abilities, we\nintroduced three test groups of 100 memes each. Annotators labeled the memes independently, with\nresearchers finalizing the labels. Post-round discussions were held to reduce errors, and detailed\ncriteria, including edge cases, were established. Annotators improved from 63% accuracy in the first\nround to 78% in the final round, demonstrating the effectiveness of the training.\n3.4.2 Label Annotation\nTo guarantee the consistency of label annotation, we establish a comprehensive annotation framework\nas a guideline. The specific process includes the following three stages.\nWhether Harmful. The foundation of the labeling is to determine whether a meme is harmful or\nbenign, which is a binary annotation. We strictly follow the definition of \"Chinese harmful memes\",\nfocusing not only on targeted harmful memes but also on samples exhibiting potential toxicity without\nspecific targets.\nHarmful Type. In the second stage, we further refine the categorization of harmful memes, including\ntargeted harmful, general offensive, sexual innuendo, and dispirited culture. The annotation criteria\nfor each harmful type are provided below. Targeted Harmful memes express disgust, prejudice, or\nstereotypes towards specific individuals or social groups. In contrast, General Offensive memes\nencompass sarcastic or rude content but lack specific targets. We also adhere to psychological and\nsociological definitions to classify the other two types: Sexual Innuendo refers to memes that imply\nsexual intent to provoke sexual arousal (Bell, 1997). Here we label memes that contain suggestive\nelements but not sexism or sexual assault as such samples to distinguish them from targeted harmful\nmemes. And Dispirited Culture is characterized by the integration of decadent and desperate\nemotions, conveying a self-negative attitude (Dong et al., 2017).\nModality Combination. As multimodal units, harmful memes consist of both textual and visual\nmodality, expressing toxicity through fused or independent features (Kiela et al., 2020). To gain a more\ncomprehensive understanding of how harmful content is propagated via memes, we classify them\nbased on the toxic manifestation of these two modalities, exploring their individual and combined\neffects. Among them, Text-Image Fusion memes exhibit toxicity only through the combined effect\nof both modalities, while the text and image separately remain benign. In contrast, Harmful Text\nand Harmful Image categories refer to one modality (either the text or the image) that independently\nexhibits toxicity.\nDuring the label annotation, each meme is labeled by at least three annotators. And we use a majority\nvote to assign the final label. In addition, specific targets of targeted harmful memes are provided. We\nthen discuss the Inter-Annotator Agreement (IAA) of each granularity, as shown in Appendix B.3."}, {"title": "3.5 Statistics Description", "content": "For subsequent model training and evaluation, all samples in TOXICN MM are divided into a training\nset and a test set at a ratio of 8:2, as detailed in Table 3. We note that there exists a sample imbalance"}, {"title": "4 Detector Development", "content": "4.1 Overview\nTo improve the detector's understanding of\nmemes, we present a baseline detector, Multi-\nmodal Knowledge Enhancement (MKE), which\nintegrates contextual information of meme con-\ntent for more accurate predictions. We first lever-\nage the LLM to capture the contextual context of\nmemes and generate enhanced captions. Then, we\nfine-tune the detector by integrating the original\ninputs (i.e., text-image pairs) with the generated\ncaptions. The illustration of MKE is shown in\nFigure 3.\nFor a given meme, its inline text and image are en-\ncoded by modality-specific encoders, represented\nas $S \\in \\mathbb{R}^{d_s}$ for text and $V \\in \\mathbb{R}^{d_v}$ for the image,\nwhere $d_s$ and $d_v$ denote the dimensions of the\ntextual and visual vector spaces, respectively.\n4.2 Knowledge Mining\nWe instruct the LLM to generate enhanced captions for the meme by designing the instruction\ntemplate, which respectively captures the contextual information from both the inline text and image.\nTo improve the understanding of the inline text that may contain slang, we enable LLM to incorporate\nlanguage features unique to Chinese for semantic analysis. The template is as follows: \"Considering\nChinese linguistic characteristics, please analyze the meaning of the text .\" We further convert\nthe image into textual descriptions with the multimodal large language model (MLLM), capturing\""}, {"title": "4.3 Knowledge Integration", "content": "To leverage contextual information, we employ a cross-attention mechanism to integrate the inline text\nwith two types of caption information, due to the consistency of the textual vector space. The feature\nintroducing the textual caption $K_s$, is defined as $S_{K_s} = \\text{Softmax} (SK_s^T / \\sqrt{d_s}) S$. Similarly, the\nfeature introducing the visual caption $K_v$ is obtained and denoted as $S_{K_v}$. We then incorporate these\nfeatures into a knowledge-enhanced representation $S_K = \\text{Mean}(S, S_{K_s}, S_{K_v})$, where $S_K \\in \\mathbb{R}^{d_s}$.\nNext, we concatenate $S_K$ with the original image feature $V$ to obtain the final representation of a\nmeme, denoted as $C \\in \\mathbb{R}^{d_c}$, where $d_c = d_s + d_v$. C is then processed by a trainable classifier, which\napplies a linear transformation followed by a softmax function to produce detection probabilities."}, {"title": "5 Experiments", "content": "5.1 Tasks and Baselines\nWe utilize TOXICN MM as the benchmark for Chinese harmful meme detection. Specifically, we\nestablish two progressive tasks. (I) Harmful Meme Detection, a binary classification task, detects\nif a meme is harmful; (II) Harmful Type Identification, a multi-classification, further identifies\nits harmful type, including targeted harmful memes, general offense, sexual innuendo, or dispirited\nculture. In addition to MKE, we evaluate the performance of various baselines, including both\nunimodal and multimodal models. For unimodal models, we utilize RoBERTa (Liu et al., 2019), GPT-\n3.5, and GPT-4 (text input) as text-only models, while ResNet (He et al., 2016) and ViT (Dosovitskiy\net al., 2021) serve as image-only models. For multimodal models, we employ CLIP (Radford et al.,\n2021), the fusion of ROBERTa and ViT, which concatenates representations of the text and image for\nclassification, and GPT-4 (text and image input).\n5.2 Implementation\nWe adopt precision (P), recall (R), and macro $F_1$-score ($F_1$) as metrics. We also report the $F_1$ of\nharmful memes and each harmful type. We respectively utilize CLIP and the fusion of ROBERTa and\nViT as the backbones of MKE, and we use GPT-4 to generate enhanced captions. For conventional\nPLMs, we fine-tune their parameters and select the best-performing model based on test set outcomes.\nFor LLMs, we evaluate their performance in a zero-shot scenario, using instruction templates in\nChinese. More details are provided in Appendix B.5.\n5.3 Results and Discussions\nIn this section, we present our experimental results and conduct a detailed analysis. The performance\nof baselines is evaluated across two tasks, as shown in Table 5. From the results, we can observe that:\n(1) In contrast to LLMs, conventional fine-tuned pre-trained baselines (i.e., CLIP and the combination\nof ROBERTa and ViT) achieve better detection performance, indicating their effectiveness in specific\ntasks. When considering the modality of input information, RoBERTa, which solely utilizes the inline\ntext of memes, achieves a significantly higher $F_1$ score (average increase of 8.4%) than vision-based\nmethods such as ResNet and ViT, which solely utilize images. This result supports the conclusion\ndrawn in (Hee et al., 2022), namely that text comprehension plays a more crucial role than image\nunderstanding in the detection of harmful memes.\n(2) GPT-4 and GPT-3.5 show similar performance in binary harmful meme detection when only the\ninline text is provided, and there is a clear enhancement in the multiclass harmful type identification\ntask. After incorporating the image input, GPT-4 shows the best detection performance for sexual\ninnuendo (Sex.) memes, while its performance decreases for general offense (Off.) and dispirited\nculture (Disp.). Referring to Table 4, we observe that most samples of Sex. exhibit toxicity through"}, {"title": "5.4 Case Study", "content": "To further illustrate the rationales of MKE, we provide several case studies, as shown in Table 6.\nWe list enhanced captions of harmful memes and the predictions of other models for reference. We\nalso instruct GPT-4 to generate reasons for its detection decisions. We do not introduce additional\ntemplates to standardize its reasoning to reflect GPT-4's true understanding of memes more accurately.\nExp. (a) is a targeted harmful meme towards Asians. Through the caption, we observe that GPT-4\nunderstands the meme's meaning solely through the inline text, recognizing the high standard of\nAsian parents on their children's academic performance. This highlights GPT-4's strong contextual\nunderstanding. After integrating this information, compared to the backbone (Fusion), our MKE\nmodel makes the correct decision, illustrating that incorporating contextual information of meme\ncontent enhances the model's understanding of memes.\nFor more insight into the challenges of detecting Chinese harmful memes, we manually inspected the\nsamples misclassified by most baselines. Two main types of errors are summarized.\nType I error: Benign information contained in harmful memes can influence the judgment of models,\nresulting in incorrect detection. In Exp. (b), when presented solely with the inline text, GPT-4\naccurately interprets the meme's meaning, comparing \"I\" with a \"little mouse\u201d to convey a dispirited\nculture. However, upon introducing the image, GPT-4 mistakenly interprets the mouse as being\n\"gently stroked\" and incorrectly categorizes the meme as harmless. This suggests that the model may\noverlook the potential toxicity of memes due to the seemingly benign nature of a certain modal. In\ncontrast, MKE integrates original sample and caption information to make the correct judgment."}, {"title": "6 Conclusions and Future Work", "content": "In this paper, we focus on the comprehensive detection of Chinese harmful memes. We present the\nfirst Chinese harmful meme dataset TOXICN MM. It has 12k samples including not only targeted\nharmful memes but also those only exhibiting potential toxicity without specific targets, adapting\nto the Chinese online environment. In addition to binary labels, TOXICN MM provides harmful\ntypes and modality combination categories of memes. To improve the understanding of Chinese\nharmful memes, we present a Multimodal Knowledge Enhancement (MKE) detector, introducing the\ncontextual information of inline text and images. In the experimental phase, we evaluate multiple\nbaseline models for their performance in detecting Chinese harmful memes. Our case study suggests\nthat integrating multimodal information and comprehensive background knowledge is crucial for\neffective detection.\nIn future work, we aim to design more effective methods for Chinese harmful memes detection.\nMeanwhile, we notice that the accuracy of LLMs in detecting Chinese harmful memes is still limited.\nConsidering the potential harm that these memes may cause, this task can be used to evaluate the\nsafety of LLMs. We will employ prompt engineering and instruction fine-tuning methods to explore\nand enhance the detection performance of LLMs. Additionally, we will continuously evaluate state-\nof-the-art models to ensure the effectiveness of TOXICN MM. We expect our dataset, benchmark,\nand insights will assist researchers in related fields."}, {"title": "7 Limitations", "content": "In this study, we focus on several most common harmful types of memes on the Chinese online\nenvironment. Due to the filtering mechanism, some harmful memes, such as those containing\nfake news, are extremely scarce on Chinese platforms. As a result, our TOXICN MM does not\nencompass all harmful types. The techniques we used to boost the percentage of harmful content\nduring the dataset construction process may introduce problematic bias. In future work, we plan to\nbroaden the scope and increase the number of meme crawls, focusing on more Chinese platforms to\nmitigate sampling bias. While we have implemented several measures to mitigate annotation bias, we\nacknowledge that our dataset may still contain mislabeled data due to the subjective understanding\nof annotators for Chinese harmful memes. Furthermore, our current study primarily focuses on\npredicting whether a given meme is harmful. We will further evaluate the ability of baselines to\ngenerate explanations for Chinese harmful memes with quantitative experiments."}, {"title": "8 Ethics Statement", "content": "Our study aims to facilitate the comprehensive detection of Chinese harmful memes and raise\nresearchers' attention to non-English memes. The social psychological community has recognized\nthe harms of the harmful types we selected in the dataset. We acknowledge the risk of malicious\nactors attempting to reverse-engineer memes. We strongly discourage and denounce such practices,\nemphasizing the necessity of human moderation to prevent them. All resources are intended solely for\nscientific research and are prohibited from commercial use. We believe the benefits of our proposed\nresources outweigh the associated risks. We strictly follow the data use agreements of each public\nonline social platform. The opinions and findings contained in the samples of our presented dataset\nshould not be interpreted as representing the views expressed or implied by the authors.\nTo mitigate the potential psychological impact on annotators evaluating harmful content, we imple-\nment the following protective measures: 1) obtain explicit consent regarding exposure to potentially\nabusive content, 2) limit weekly evaluations to manage exposure and ensure reasonable daily work-\nloads, and 3) recommend discontinuing reviews if they experience distress. Additionally, we conduct\nregular well-being checks to monitor their mental health."}, {"title": "A Research Background", "content": "Various harmful memes propagate on Chinese platforms (Peng, 2019; Zheng, 2016). While their\ncreators and disseminators intend to simply express emotions or humor, and their original intent\nmay be harmless, these memes have a significant negative impact on society when abused (Liu and\nXu, 2016; Lin and Zhang, 2019). In this section, we individually explore the detrimental impacts of\nvarious harmful meme types, highlighting the importance of detecting these memes.\nTargeted Harmful. Memes targeting specific individuals or groups can perpetuate hate speech,\ndiscrimination, and prejudice, fostering an environment of intolerance (Sharma et al., 2022b). They\nfuel online toxicity and create hostile environments. Furthermore, they have the potential to incite\nreal-world violence or harassment and worsen social divisions.\nGeneral Offensive. Memes containing general offenses breed an aggressive culture prone to\ncontroversy and online violence (Zheng, 2016). Their influence extends beyond the individual,\nshaping the overall tone of the online environment. Additionally, such offensive content negatively\nimpacts the development of correct values and healthy personalities among children (Yang, 2018).\nSexual Innuendo. While sexual innuendo content generally does not involve coercion in a sexual\nrelationship, it can still be misconstrued due to gender and cultural differences, thereby contributing to\nsexualization (Thornhill and Thornhill, 1983; Koukounas and Letch, 2001). Moreover, inappropriate\nsexual innuendo may be considered sexual harassment (Otsri, 2020).\nDispirited Culture. Memes containing dispirited culture often evoke negative emotions and con-\ntribute to feelings of social isolation. This leads to an increase in personal depression, making it\ndifficult for individuals to have positive interactions and relationships with others (Miao and Xu,\n2022). Furthermore, the spread of these memes inadvertently undermines the value of positive\nthinking and promotes social anxiety (Zhang and Zhao, 2021)."}, {"title": "B Implementation Details", "content": "B.1 Details of Data Filtering\nFor data filtering, we refer to the existing Chinese meme datasets (Li et al., 2022; Xu et al., 2022) and\napply the following criteria:\n\u2022 The meme text must contain Chinese (including code-switching); memes only containing\nother languages are not allowed.\n\u2022 The meme text must have actual semantics; Thus, samples where the text is too brief, e.g.\ncontaining only modal particles, are removed.\n\u2022 The meme must be readable. Hence, low-resolution samples that cannot be extracted inline\ntext are excluded.\n\u2022 The meme must be multimodal, meaning it should contain both the inline text and image\ninformation.\nHere we present some examples of memes that were removed during the filtering process for failing\nto satisfy some of the above criteria, as shown in Figure B1."}, {"title": "B.2 Meme Containing Harmful Image", "content": "Based on the statistics listed in Table 3, the harmful meme where the image independently exhibits\ntoxicity (Harm.I) is sparse. Here we present two samples to conduct a brief analysis, as shown\nin Figure B2. Both examples contain general offensive content. In Exp. (a), the \"middle finger\"\nis employed to convey aggression and contempt. In Exp. (b), both the inline text and image are\nindependently harmful. The text \"\u897f\u5167\", literally translated as \"west in\" in English, serves as a\nhomonym for \"go die\" in Japanese, while the image incorporates violent elements."}, {"title": "B.3 Discussion of Annotation Consistency", "content": "After annotation, we calculate the Inter-Annotator Agreement (IAA) for each annotation hierarchy\nusing Fleiss' Kappa values. Among them, the stage with the highest disagreement pertains to\ndetermining whether a meme is harmful, with a Kappa value of 0.62, which is comparable to other\nharmful meme datasets like Harm-C (0.67) and Harm-P (0.68) Blaier et al. (2021). Given the subtlety\nof harmful types in our dataset, this IAA is expected. These disagreements stem mainly from the\nhumorous elements present in some samples of harmful memes, leading some annotators to consider\nthem not toxic enough to classify them as harmful. This reflects that subjective bias still influences\nthe results of annotation to some extent, despite implementing several measures to mitigate biases. In\naddition, the kappa values of discriminating harmful types and text-image combination characteristics\nare 0.73 and 0.86, respectively."}, {"title": "B.4 Statistics of Target Distribution", "content": "During the annotation phase, we label specific targets of targeted harmful memes in TOXICN MM.\nThe target distribution is shown in Table B7."}, {"title": "B.5 Experimental Details", "content": "In the evaluation phase, the harmful type identification task is conducted as a five-classification,\nincluding non-harmful and four harmful types. The specific versions of each baseline are listed in\nTable B2. To minimize experimental error, all experiments are repeated five times."}, {"title": "C Supplementary Experiments", "content": "C.1 Performance on Diverse Modality Combination\nFor a supplementary analysis, we evaluate the\ndetection performance of harmful memes with\ndifferent modality combination features, as shown\nin Figure C1.\nCompared to other combinations, memes contain-\ning harmful text are more likely to be successfully\ndetected by the models, especially PLMs. This\nis because PLMs can effectively learn the unique\nexpressions of the Chinese language during the\nfine-tuning stage. In contrast, for memes contain-\ning harmful images, GPT-4 demonstrates stronger\nperformance than fine-tuned PLMs, with an av-\nerage increase of 6.5%, illustrating its ability to\neffectively review the content of input images. Ad- modality combination features.\nditionally, we note that GPT-4's performance on\ntext-image fusion is comparable to that of PLMs,\nshowcasing its capability to understand and reason with information that combines both textual and\nvisual elements effectively. Meanwhile, compared to the model incorporating only inline text, i.e.,\nGPT-4 (only text), GPT-4 shows a 12.7% decrease in performance for memes containing harmful\ntext. This also supports the conclusion that benign images relatively affect the detection of GPT-4\nin Chinese harmful memes. Additionally, after introducing MKE, models show improvement in\ndetecting Chinese harmful memes with different modality combinations."}, {"title": "C.2 Evaluation of Chinese MLLMS", "content": "In this section, we utilize TOXICN MM to evalu-\nate the performance of Chinese LLMs in detecting\nChinese harmful memes under the zero-shot sce-\nnario. Due to the unavailability of APIs for mul-\ntimodal dialogue in existing commercial Chinese\nLLMs (e.g., Wenxin Yiyan), we only evaluate the\neffect of several open-source Chinese LLMs, includ-\ning VisualGLM (Du et al., 2022), Qwen-VL (Bai\net al., 2023), and VisCPM (Hu et al., 2023). The\nresults are shown in Table C1."}, {"title": "D Licensing and Maintenance Plan", "content": "D.1 Licensing\nWe confirm that the dataset is licensed under the Creative Commons Attribution-NonCommercial 4.0\nInternational (CC BY-NC 4.0) license."}, {"title": "D.2 Maintenance Plan", "content": "We will regularly update the TOXICN MM dataset by adding new samples collected from Chinese\nsocial platforms. These updates, scheduled annually, aim to maintain the dataset's currency and\ncomprehensiveness. Additionally, we will open the Community section of Huggingface for\ncommunity contributions, subjecting all submissions to rigorous review to ensure alignment with the\ndataset's quality standards. Moreover, based on the feedback, we will periodically enhance annotation\nguidelines and expand annotation types to provide a more detailed and diverse dataset. Future updates\nwill also include enriched metadata to offer better context and support more analyses.\nTo address any concerns or queries related to the dataset, we will establish a dedicated support team\nreachable via email or through a support portal on the website. Furthermore, an issue tracking system\nwill be implemented to document and monitor reported issues, enabling users to report bugs and\nsuggest improvements. We encourage user feedback through a structured feedback loop, which will\ninform regular audits aimed at maintaining dataset integrity and quality. Periodic transparency reports\nwill be published to keep users informed about encountered issues, resolutions, and overall dataset\nimprovements, fostering trust and transparency within the user community."}]}