{"title": "Addressing Small and Imbalanced Medical Image Datasets Using Generative Models: A Comparative Study of DDPM and PGGANS with Random and Greedy K Sampling", "authors": ["Iman Khazrak", "Shakhnoza Takhirova", "Mostafa M. Rezaee", "Mehrdad Yadollahi", "Robert C. Green", "Shuteng Niu"], "abstract": "The development of accurate medical image classification models is often constrained by privacy concerns and data scarcity for certain conditions, leading to small and imbalanced datasets. To address these limitations, this study explores the use of generative models, such as Denoising Diffusion Probabilistic Models (DDPM) and Progressive Growing Generative Adversarial Networks (PGGANs), for dataset augmentation.\nThe research introduces a framework to assess the impact of synthetic images generated by DDPM and PGGANs on the performance of four models: a custom CNN, Untrained VGG16, Pretrained VGG16, and Pretrained ResNet50. Experiments were conducted using Random Sampling and Greedy K Sampling to create small, imbalanced datasets. The synthetic images were evaluated using Frechet Inception Distance (FID) and compared to original datasets through classification metrics.\nThe results show that DDPM consistently generated more realistic images with lower FID scores and significantly outperformed PGGANs in improving classification metrics across all models and datasets. Incorporating DDPM-generated images into the original datasets increased accuracy by up to 6%, enhancing model robustness and stability, particularly in imbalanced scenarios. Random Sampling demonstrated superior stability, while Greedy K Sampling offered diversity at the cost of higher FID scores. This study highlights the efficacy of DDPM in augmenting small, imbalanced medical image datasets, improving model performance by balancing the dataset and expanding its size. Our implementation and codes are available at https://github.com/imankhazrak/DDPM_X-Ray.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical imaging is indispensable in modern health-care, guiding diagnostics, surgeries, treatment assess-ments, and disease monitoring. The growing volume of images poses challenges [1] for radiologists and physicians to maintain workflow efficiency without technological support. There are significant challenges to train accurate and reliable Machine Learning or Deep Learning diagnostic models. Key issues include the scarcity of extensive and diverse datasets [2], stringent data privacy regulations, and inherent dataset imbalances. These imbalances can lead to biased mod-els that struggle with rare conditions, and even minor errors can have negative implications.\nTraditional data augmentation techniques, such as random rotations, flipping, cropping, and noise injec-tion, have been widely employed to expand training datasets. While useful, these methods merely manip-ulate existing samples and fail to introduce the kind of fundamental variability needed for robust model training [3]. In contrast, generative models [4], GANS and DDPM, have revolutionized image synthesis by creating entirely new data points. These models offer promising solutions to the challenges of imbalanced datasets, particularly in the field of medical imaging, where the availability of labeled data is limited.\nGenerative models typically require large and di-verse datasets [5]. This creates a paradox: if such large labeled datasets were available, effective models could be trained directly. Therefore, generative models are only viable if they can work effectively with scarce datasets. The present paper addresses this challenge by proposing a comprehensive framework for generating synthetic medical images from both small and imbal-anced datasets using two generative models: PGGANS [6] and DDPM [7]. We also explore the use of two different sampling methods-Random Sampling and the Greedy K Method-to further assess their impact on model performance. This framework is rigorously evaluated to enhance diagnostic model accuracy and robustness.\nThis paper introduces a framework for the use of advanced generative models in medical imaging, targeting small and imbalanced datasets. Our key con-tributions are as follows:\n\u2022 Comprehensive Evaluation Framework: We de-velop a rigorous framework that evaluates the"}, {"title": "II. RELATED WORK", "content": "Generative models, especially those producing high-quality, realistic images, have gained significant atten-tion in augmenting medical datasets, particularly for rare conditions where data scarcity and class imbalance are common. These models can be classified as latent variable generative models, which are either explicit or implicit density models.\nFrameworks from other fields offer valuable insights for healthcare. For example, Sustainability Value Ar-ticulation (SVA) enhances internal and external efforts for better social and environmental outcomes by em-phasizing supplier involvement and technological inte-gration [8]. Similarly, the EV supply chain highlights the importance of systematic benchmarking and tech-nological innovation to achieve competitive advantages in complex systems [9]. These principles align with the goals of generative models in addressing data scarcity and improving healthcare datasets, enabling long-term scalability and impact.\nExplicit density models like Variational Autoen-coders (VAEs), Boltzmann Machines, and DDPMs have predefined density functions, offering inter-pretability and training stability [7], [10], [11]. They are useful for applications like anomaly detection due to their explicit likelihood functions. However, their distributional assumptions can sometimes lead to less realistic images [12].\nImplicit density models, such as GANs, do not rely on explicit likelihood functions, making them more flexible and capable of modeling complex distribu-tions. They tend to produce more realistic images but"}, {"title": "A. GANs family in Medical Imaging", "content": "GANs are prominent implicit density models that consist of two competing neural networks, a generator, that creates synthetic images from a latent space, and a discriminator which evaluates resemblance of generated images to real images, engaging in a zero-sum game. Generally, it is hard to train GANs due to training instability [14]. PGGANs, introduced by [6], has significantly improved the stability and quality of GAN-generated images. PGGANs utilize a progressive training approach, starting with low-resolution imagestions, which have been shown to significantly enhance and gradually increasing the resolution as training progresses. This technique allows the model to learn coarse features before fine details, leading to more stable training and higher-quality images.\nIn medical imaging, GANs mainly have been used to enhance classification and segmentation deep learn-ing models [15]. The work in [2] uses GANs on a small CT scan dataset to generate eye fundus images which confirm to the given masks. [16] also used mask to generate lung images, and only those synthetic images that fulfilled informativeness criteria calculated by Bayesian neural networks were used to improve the classifier model. In [4], GANs are employed to synthesize high quality focal liver lesions of multiple conditions to enhance a CNN-classifier. Moreover, GANs have been successful at synthesizing prostate lesions [17], lung cancer nodules [18], brain MRI im-ages [19] to name a few. The authors of [20] generate high resolution synthetic images of skin lesions from a dataset of 2000 dermoscopic images using multiple GANs architectures and compare their classification performances. They conclude that PGGANs could to synthesize realistic images that medical professionals upon evaluation were not able to distinguish from real ones. Results of [21] confirm that PGGANs can produce high-resolution images with remarkable detail and consistency, making them one of the best choices for medical image synthesis."}, {"title": "B. Diffusion Family in Medical Imaging", "content": "Diffusion models are generative models that trans-form noise into structured data through a sequence of steps. The DDPM [7] is a prominent model in this family, known for producing high-fidelity images by reversing a diffusion process. These models iteratively add and then remove noise from an image through two main phases: the forward process, where noise is added over several steps, and the reverse process, where the model learns to denoise the image step-by-step. This iterative refinement allows DDPMs to generate images with fine-grained details. Introduced by Ho et al. in 2020 [22], DDPMs have set new benchmarks in image generation quality by leveraging a sophisticated noise schedule and a robust denoising network."}, {"title": "III. METHODOLOGY", "content": "Our research methodology includes several key phases: image synthesis, dataset augmentation, model training and fine-tuning, and performance evaluation."}, {"title": "A. Image Synthesis", "content": "1) PGGANs: PGGANs utilize a progressive train-ing approach, starting with low-resolution images and gradually increasing the resolution as training pro-gresses (Fig. 2). This method enhances stability and image quality by incrementally increasing the com-plexity of the generator and discriminator networks. The generator produces data resembling real data, while the discriminator distinguishes between real and generated data [29]. The adversarial loss functions for the generator (LG) and discriminator (LD) are:\n$L_G = log(1 \u2013 D(G(z)))$\n$L_D = log(D(x)) + log(1 \u2013 D(G(z)))$\nwhere G(z) represents the generated data from noise z, and D(x) represents the discriminator's output for real data x [30].\nPGGANs adopt a step-by-step training approach, beginning with low-resolution images and advancing to higher resolutions. This progressive training allows the model to learn rough features initially and then fine-tune them for generating high-quality images. New layers are added to both networks iteratively, and the loss functions are applied at each resolution level to maintain consistency.\n2) DDPM: DDPMs synthesize images by reversing a diffusion process that gradually adds Gaussian noise to an image and then reconstructs the original image from the noise (Fig. 3) [7].\nThe forward process adds noise to the image:\n$x_t = \\sqrt{a_t}x_{t-1} + \\sqrt{1 - a_t}e_t$\nwhere xt is the image at iteration t, at is a noise scaling factor, and et is the Gaussian noise added at iteration t [7]. The backward process aims to denoise the noisy image obtained from the forward process and recover the original clean image by optimizing the variational lower bound:\n$L_{DDPM} = E_{t,x_0,\\epsilon} [|\\epsilon - \\epsilon_\\theta (x_t, t) |^2]$\nHere, e represents Gaussian noise, and eg is the noise predicted by the model.\nThe U-Net architecture, adapted for use in DDPMs, excels in the reverse diffusion process by predicting and removing noise added during the forward phase [31], [32]. U-Net's U-shaped structure with down-sampling and upsampling paths efficiently synthesizes detailed images, incorporating time embeddings to adjust noise prediction based on the reverse process timestep [33]."}, {"title": "B. Generated Image Assessment", "content": "a) Visual Inspection: Generated images are ini-tially evaluated by visually comparing random samples to the original images.\nb) Frechet Inception Distance (FID): The FID score quantifies the distributional similarity between real and generated images. It is calculated by extract-ing features from an Inception V3 model for both real and generated images, then computing the Frechet distance between the resulting multivariate Gaussian distributions. A higher FID score indicates greater dissimilarity [34]."}, {"title": "C. Classification Models", "content": "We evaluate the impact of synthetic images using four different classifiers: a pre-trained VGG16, a pre-trained ResNet50, an untrained VGG16, and a custom CNN. Each model is first trained on both the small and imbalanced datasets to establish baselines, followed by training on the same datasets augmented with synthetic images generated by DDPMs and PGGANs. The in-clusion of an untrained VGG16 allows us to assess the direct impact of synthetic data on models learning from scratch, providing a clearer understanding of how effective the generated images are in improving gen-eralization without the benefit of pre-learned features. This approach is particularly important in cases where pretrained models may not be available or applicable, and where the goal is to evaluate how synthetic data can help models learn directly from the augmented datasets. To assess stability and observe changes in classification metrics, each model runs five times. This approach allows for a thorough examination of how the augmented datasets influence the models' generaliza-tion performance on the test datasets, providing valu-able insights into the effectiveness of synthetic data in enhancing both pre-trained and untrained model accuracy."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This section presents the findings from our experi-ments."}, {"title": "A. Synthetic Image Generation", "content": "The PGGANs and DDPM models are trained sepa-rately for each class in the training dataset, producing a total of four models using 200 images from the small dataset for each sampling method. Leveraging the code from [35] and [36], we generate 2,000 images per class for each model.\nTo train PGGANs models, random sampling from a standard normal distribution is employed for initializa-tion. Stability in training is achieved by equalizing the learning rate, i.e. scaling the outputs right before the forward pass [6]. Convolution layers below 64-pixel resolution are set at 128 filters, while layers at 64 and 128-pixel resolution are set to 64 filters. The BATCH-SIZE is set to 4. One PGGANs model is trained per class using the Adam optimizer and the Wasserstein loss, each for 200,000 epochs. Due to computational constraints, the models did not converge, though the training process was stable and followed desired pat-tern of loss (Fig 4). With experimental trials relying on computed losses, we choose the checkpoint from epochs 160000 (PGGANS 160k)."}, {"title": "B. Synthetic Image Quality Evaluation", "content": "1) Visual Inspection: Figs 5 and 6 showcase a visual comparison of generated images of both healthy and pneumonia-affected lungs. Although visual inspec-tion can be subjective, the PGGANs images from the 160k checkpoint are visually appealing but occasion-ally display defect patches. In contrast, the DDPM-generated images demonstrate a closer resemblance to the original data, exhibiting superior visual fidelity.\n2) FID Metric: PyTorch implementation provided by [37] is used to calculate FID scores between original data set and each model's generated images per class (Fig.7)."}, {"title": "V. CONCLUSION", "content": "The experiments conducted in this study evaluated the effects of data augmentation using DDPM and PGGANs on small and imbalanced medical image datasets, utilizing two sampling methods: Random Sampling and Greedy K. The use of generative models for data augmentation led to significant improvements in both the accuracy and robustness of the models.\nAcross all experiments, the introduction of synthetic images enhanced performance, with models exhibit-ing greater stability and reduced variability during training. This resulted in higher accuracy and better generalization, particularly when handling limited and imbalanced datasets. Of the two generative models, DDPM consistently outperformed PGGANs, deliver-ing superior gains in accuracy and robustness. DDPM-generated images more closely aligned with the orig-inal data distribution, contributing to more stable per-formance and lower variability across all datasets and sampling methods..\nThe FID evaluation showed that DDPM outper-formed PGGANs with lower FID scores, indicating that DDPM-generated images more closely resembled the original data. Random Sampling led to lower FID"}]}