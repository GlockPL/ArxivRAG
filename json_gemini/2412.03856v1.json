{"title": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?", "authors": ["Patrick Ocheja", "Brendan Flanagan", "Yiling Dai", "Hiroaki Ogata"], "abstract": "E-learning environments are increasingly harnessing large language models (LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study introduces an approach that integrates dynamic knowledge graphs with LLMs to offer nuanced student assistance. By evaluating past and ongoing student interactions, the system identifies and appends the most salient learning context to prompts directed at the LLM. Central to this method is the knowledge graph's role in assessing a student's comprehension of topic prerequisites. Depending on the categorized understanding (good, average, or poor), the LLM adjusts its guidance, offering advanced assistance, foundational reviews, or in-depth prerequisite explanations, respectively. Preliminary findings suggest students could benefit from this tiered support, achieving enhanced comprehension and improved task outcomes. However, several issues related to potential errors arising from LLMs were identified, which can potentially mislead students. This highlights the need for human intervention to mitigate these risks. This research aims to advance AI-driven personalized learning while acknowledging the limitations and potential pitfalls, thus guiding future research in technology and data-driven education.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advances in Artificial Intelligence (AI) have led to transformative approaches in various fields, including education. AI-enabled educational interventions have emerged as powerful tools not only in modeling student behaviors and predicting learning pathways but also in providing dynamic content adaptations [1]. Despite these advancements, delivering tailored feedback to students that captures their individual cognitive gaps and learning styles remains a significant challenge [2], [3]. Large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), have shown broad linguistic comprehension and generation capabilities [4], providing potential solutions that align with student-specific misconceptions and learning difficulties. Auto-generated feedback, in particular, stands out as an area where AI and LLMs can truly shine. Shermis and Burstein [5] investigated automated essay scoring, providing results that suggest AI can offer feedback comparable to human evaluators in specific contexts.\nWhile Intelligent Tutoring Systems (ITSs) have long been explored for personalized learning, they often exhibit limitations in granularity and flexibility. Hwang [6] discusses how ITSs typically recommend exercises related to a weak concept but do not provide assistance at the level of addressing specific impasses within an exercise. Additionally, Phobun and Vicheanpanya [7] highlight that although some ITSs offer adaptive hints and explanations, these are often predefined and may not be sufficiently flexible to address the changing needs of learners. Early works, such as those by Zhou et al. [8], explored delivering hints in a dialogue-based ITS, but the technology used at the time lacked the sophistication of current LLMs, limiting their ability to provide highly personalized and context-aware guidance.\nThis paper introduces a novel approach that integrates dynamic knowledge graphs with LLMs to offer nuanced student assistance. By evaluating past and ongoing student interactions, the system identifies and appends the most salient learning context to prompts directed at the LLM. Central to this method is the knowledge graph's role in assessing a student's comprehension of topic prerequisites. Depending on the categorized understanding (good, average, or poor), the LLM adjusts its guidance, offering advanced assistance, foundational reviews, or in-depth prerequisite explanations, respectively. This approach aims to overcome the limitations of traditional ITSs by providing more granular and adaptive support tailored to the individual student's needs.\nThe proposed system leverages concept-map driven approaches [9], [10] to ascertain and evaluate the importance of prerequisite questions. By incorporating details such as the question, correct and standard solution, the student's impasse, and probable causes retrieved from their current knowledge state (computed from assessment results on prerequisite concepts), we can provide personalized guidance on questions students find problematic. This method not only enhances the precision of feedback but also aligns it more closely with the student's specific learning context. Preliminary findings suggest that students may benefit from this tiered support and could achieve enhanced comprehension and improved task outcomes.\nIn this paper, we evaluate the proposed methodology using both objective metrics and expert opinions. Specifically, we employ the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) method [11] to compare GPT4-generated feedback across three categories of student performances: low (S1), average (S2), and high (S3), grouped based on their performances on prerequisites. Experts are asked to rate the generated feedback across metrics of correctness, precision, hallucination, and variability. The main research objectives are:"}, {"title": "II. RELATED WORK", "content": "In adaptive learning environments, personalized feedback systems have shown to enhance student engagement and comprehension [18]. The integration of AI in education has been extensively researched, with numerous studies highlighting its potential to revolutionize personalized learning experiences. For instance, [19], [20] reviewed the state of AI systems in education, examining their pedagogical and educational assumptions. They revealed a critical insight: many AI tools tend to homogenize student learning experiences rather than offering true personalization. This homogenization occurs because these systems often rely on predefined pathways and solutions, limiting their ability to cater to individual learning needs.\nIntelligent Tutoring Systems (ITS) have been a significant area of research, aiming to provide personalized instruction and feedback to students [21]. Early ITS frameworks primarily focused on rule-based systems that adapted content based on student performance metrics [22]. However, these systems struggled with scalability and flexibility [23]. A notable advancement was proposed by Singh, Gulwani and Solar-Lezama [24], who introduced a technique for automated feedback in introductory programming courses. The method in [24] utilized a reference implementation to derive corrections for student solutions. Despite its innovation, the technique was limited by its dependency on a correct reference and its inadequacy in addressing large conceptual errors, often resulting in low-level feedback that did not fully support deeper learning processes [25]."}, {"title": "III. RESEARCH METHODOLOGY", "content": "In this study, we propose a novel method and architecture show in Figure 1 that combines knowledge graph, learning analytics and LLM-based feedback generation to provide adapted guidance for students based on their specific needs. We called this conversational personalized feedback chatbot AI-sensei. The methodology is structured into the following key phases:\nOur proposed method relies on knowledge graphs [31] to determine the relationship and hierarchy of the topics and sub-topics to be learnt by students. In this paper, we use a simplified knowledge graph of Mathematics constructed from the textbook Math Algebra 2 by Prentice Hall which is used as one of the three full-year subject-specific courses [32]. We construct the knowledge graph by treating each unit in a chapter as a concept. To establish the relationship such as prerequisite between concepts, we use the \"GO for Help\" indicators provided by the authors. This indicator appears at the beginning of each unit and signals where students can get help if they are unable to understand the current unit.\nMost LLM-based applications often provide a query interface for users to type in their prompts. For this research, we assume students will interact with LLMs in the same manner: each student will input prompts describing their specific challenge or impasse on a given problem or a system can be designed to follow the student's solution and detect it [33]. However, due to lack of real students in this preliminary study, we asked experts to review the standard solution to each question and estimate the likely impasse for different types of students who attempt to solve the questions selected for each difficulty level. First, we define 3 different types of students as follows:\nTraditional feedback systems often provide generic answers that may not address specific student needs and impasse. In"}, {"title": "IV. EXPERIMENT", "content": "We conduct a preliminary evaluation of our proposed method by setting up an experiment as shown in Figure 3. First, we select one question from each difficulty levels Easy (A), Moderate (B) and Hard (C). For easy questions usually taken from concepts on the leaf node, we expect different types of students (S1, S2, and S3), to have similar impasse. Thus, it is acceptable (and expected) for the adapted guidance received to be similar. For moderate questions which are taken from non-leaf nodes, the different types of student might occasionally have overlapping impasse and hence similar adapted guidance is permissible. Hard questions are taken from concepts further up the tree and closer to the root node. Given the advance level of hard questions, different student types are expected to have different learning impasse and thus, receive different feedback.\nIn the second stage, we generate the personalized feedback from ChatGPT4 using prompt Pl. ChatGPT was prompted once per question with the temperature set to 0.2 to ensure consistency in output. This approach is based on established practices in natural language processing (NLP), where lower temperature settings reduce randomness and increase consistency in generated responses [34]. To measure the quality and relevance of the results to each student's needs, we employ the ROUGE method of evaluating text summarization. First, we compare the standard solution (S1) to each of the personalized feedback generated by ChatGPT4 for S1, S2 and S3. Next, we compare the personalized feedback to one another to test variability and level of personalization for each student. To validate the results obtained from the ROUGE method, we ask experts to evaluate them using a 5-point likert scale which will be described in the next section.\nWe evaluate the various texts generated by ChatGPT-4 using two main methods. The first measure is the ROUGE method. The ROUGE method measures the quality of a summary by evaluating overlapping units such as n-grams, word sequences, and word pairs between a reference and candidate text. In this study, we consider the text generated by ChatGPT-4 as summaries and perform pairwise comparisons of n-grams across the different responses generated for each student type to obtain ROUGE-N and F\u2081 scores.\nThe use of ROUGE in this context serves to assess the degree of personalization in the feedback. Our assumption is that effective personalization in feedback should result in explanations that contain essential elements of the standard solution while varying in other aspects to address individual student needs. To provide context, we analyze the personalized feedback for different student profiles and question difficulty levels. For more challenging problems, where students might encounter various sticking points, we expect lower ROUGE scores because personalized feedback should diverge more from the standard solution to address specific areas where students struggle. A high ROUGE score would indicate a lack of personalization, as it would suggest that the feedback is too similar to the standard solution. By comparing the personalized feedback generated by ChatGPT-4 for student type S1 to that of both S2 and S3, and S2 to S3, we aim to evaluate the variability and uniqueness of the feedback provided. Similarly, we repeat the same ROUGE-N measure for the standard solution (S) versus the adapted guidance generated by ChatGPT-4 for all three student types across different question difficulty levels.\nThe second method of evaluation is the use of expert evaluators. 3 experts evaluated the adapted feedback generated by ChatGPT4. This expert evaluation were based on the following criteria:"}, {"title": "VI. DISCUSSIONS AND LIMITATIONS", "content": "For easy questions, we discovered more consistency in feedback, reflecting a relatively high degree of alignment with the standard solution. This suggests that common mistakes in simpler questions have more uniform feedback requirements. However, this pattern faded with increased complexity from moderate to hard, indicating the nuanced understanding that might be needed when addressing more challenging student queries.\nA critical observation from our study is that LLMs, while advanced, are not infallible. They sometimes misinterpret context or nuances, generating feedback that might be ambiguous or inapplicable. This underlines the indispensable need for human oversight, especially in educational scenarios. Before deploying such feedback mechanisms in live classroom settings, it's crucial to integrate a layer of human validation to filter out potentially misleading or inadequate feedback. The stakes are high in education, and LLM-generated responses, if unchecked, could inadvertently foster misconceptions or hamper learning.\nThe ROUGE method, though robust in many text similarity applications, brings its own set of assumptions and potential errors. Its precision-recall based metrics focus on overlapping n-grams, which might not always capture the essence or context of feedback. A high ROUGE score does not unequivocally signify meaningful or contextually appropriate feedback. Similarly, a lower score does not always denote inadequacy but might point to a variation in phrasing or approach. Thus, metrics obtained should be interpreted with caution.\nResults from the expert reviews also provide some valuable insights for this research. For the questions reported in our experiment, all experts rated all the answers by ChatGPT4 to be correct. However, it is important to note that when we did not provide ChatGPT with the correct standard solution, the answers or feedback generated were wrong. Also, when we asked ChatGPT4 to estimate typical impasse students would face on a given problem, it performed poorly as the impasses generated were either too generic or similar for different student profiles. Thus, we asked experts to provide possible impasses for the different student profiles based on the needs of each question. This lack of ability of LLMs like ChatGPT4 to estimate learning difficulty makes a strong case for delegating such tasks to existing pedagogical research and tools for learning analytics. We discovered that despite LLMs inability to estimate learning difficulties, they provide a decent feedback most times to address these impasses.\nJeon and Lee [40] and Kasneci et al. [41] found that LLMs, despite their advanced capabilities, still require significant human oversight to ensure the accuracy and applicability of their feedback. Their study emphasizes the complementary role of human educators in validating and contextualizing AI-generated content to prevent the dissemination of misconceptions. This complements our recommendation for integrating human validation layers in the deployment of LLMs in educational settings.\nOur study comes with its fair share of limitations. We have exclusively considered mathematics, delving into only a fraction of the vast knowledge graph on Algebra 2 for High School students. A holistic understanding would require branching out to more topics, other subjects and assessing LLM feedback across a broader academic spectrum. The study's linguistic scope was also confined to English, not considering the multitude of global students who learn in other languages. Expanding to multiple languages would offer richer insights into the universality of LLM efficacy. Also, the absence of real students in our experiment led to not capturing the true diversity and unpredictability of student impasse(s). Real-world classroom scenarios might present challenges and nuances not addressed by our simulation."}, {"title": "VII. CONCLUSION", "content": "This paper proposed a new approach to generate personalized feedback for students by augmenting prompts to LLMs with student's knowledge state retrieved from a relevant knowledge graph. The experiments conducted simulated impasses of 3 different types of students across questions from 3 difficulty levels. The results of the feedback generated by ChatGPT4 revealed that for easy questions, all 3 types of student receive similar guidance. As the difficulty level increases, the personalization of feedback also tends to increase. However, some of the feedback generated contain some errors and sometimes not adequately catering to the student's impasse. This calls for careful supervision when using LLMs to support teaching and learning. While LLMs like ChatGPT4 showcase promising capabilities in assisting educators and personalizing feedback, they are not ready to replace human intervention. Their application, while promising, must be approached with caution, thorough oversight, and continuous validation to harness their potential effectively and responsibly in educational settings."}]}