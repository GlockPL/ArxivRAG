{"title": "Broad Critic Deep Actor Reinforcement Learning for Continuous Control", "authors": ["Shiron Thalagala", "Pak Kin Wong", "Xiaozheng Wang"], "abstract": "In the domain of continuous control, deep reinforcement learning (DRL) demonstrates promising results. However, the dependence of DRL on deep neural networks (DNNs) results in the demand for extensive data and increased computational complexity. To address this issue, a novel hybrid architecture for actor-critic reinforcement learning (RL) algorithms is introduced. The proposed architecture integrates the broad learning system (BLS) with DNN, aiming to merge the strengths of both distinct architectural paradigms. Specifically, the critic network is implemented using BLS, while the actor network is constructed with a DNN. For the estimations of the critic network parameters, ridge regression is employed, and the parameters of the actor network are optimized through gradient descent. The effectiveness of the proposed algorithm is evaluated by applying it to two classic continuous control tasks, and its performance is compared with the widely recognized deep deterministic policy gradient (DDPG) algorithm. Numerical results show that the proposed algorithm is superior to the DDPG algorithm in terms of computational efficiency, along with an accelerated learning trajectory. Application of the proposed algorithm in other actor-critic RL algorithms is suggested for investigation in future studies.", "sections": [{"title": "I. INTRODUCTION", "content": "The application of reinforcement learning (RL) in continuous control is promising especially in solving complex engineering tasks [1]. Some engineering control problems solved by RL are robotics control [2], autonomous driving [3], industrial process control [4], and aeronautics [5]. In such applications, assessing the effectiveness of RL agents requires crucial consideration of both learning efficiency and computational cost, particularly in the context of online training [6].\nRecent research suggests deep reinforcement learning (DRL) for successful online continuous control [7]. In DRL, deep neural networks (DNNs) are utilized for approximating both policy and value functions. However, DRL introduces certain challenges attributable to the inherent properties of DNNs [8]. Significantly, these networks require high volumes of data samples to effectively train and refine policies. This requirement becomes especially evident in situations involving complex environments with high-dimensional state-action spaces. Gathering and processing extensive data in real-time in such scenarios can lead to significant computational cost and extended training period. Moreover, to achieve higher accuracy, additional layers need to be added to DNNs in DRL algorithms. This requires re-estimating parameters in all existing layers through gradient descent algorithms which leads to time-consuming model updates. These problems can greatly affect the training efficiency of complex continuous control problems.\nTo address these challenges, one promising approach is to explore the opportunity of applying broad learning systems (BLS) [9] to RL. BLS networks are characterized by a relatively shallow architecture in comparison to the deep architectures in conventional DNNs. Compared to DNNs, the BLS shows different advantages such as less complexity of the structure and less computational time during training. Moreover, the incremental learning ability in BLS eliminates the requirement of remodeling the network in order to enhance accuracy.\nRecent research has demonstrated the effectiveness of applying BLS in DRL, where BLS networks replace DNNs in DRL algorithms. A broad reinforcement learning (BRL) approach for IoT applications was proposed in [10], by substituting the critic network in basic Q-learning [11] with a BLS network. Although basic Q-learning is effective for RL problems characterized by low-dimensional discrete state-action spaces, its applicability is limited in scenarios involving high-dimensional continuous spaces [12]. Similarly, a BLS-based Q-learning for robotic control was investigated in [13], by employing offline training with pre-collected datasets. However, the requirement for available training data for weight estimation using ridge regression was highlighted as a challenge associated with BLS in RL. This presents a challenge for online training where pre-existing datasets are unavailable. A potential drawback of the application of BLS in RL is the risk of slow or non-convergence due to the limited weight optimization option available within the BLS framework. In [14], a RL algorithm based on BLS is introduced which is designed for handling classification tasks. It addresses convergence challenges by optimizing weights through utilizing numerical iteration and adaptive dynamic programming. Although it could address non-convergence risks related to classification tasks, it introduces computational challenges when dealing with high-dimensional continuous control problems, due to the time-consuming optimization process.\nIn this study, a hybrid approach is proposed where both deep and broad networks are utilized in actor-critic RL [15]. The architecture of the proposed algorithm is inspired by the standard deep deterministic policy gradient (DDPG) algorithm [12]. Nevertheless, notable distinctions exist between the proposed algorithm and the DDPG algorithm. To our best knowledge, no prior research has been undertaken to explore the integration of BLS-based networks within actor-critic RL algorithms. Additionally, no prior study has been conducted on the exploration of applying BLS-based RL algorithms to continuous control tasks.\nIn summary, the novelty of this study is as follows:\n1) A novel actor-critic framework using a hybrid approach is proposed where both DNN and BLS are incorporated. This offers a unique architectural variation when compared to DRL and BRL.\n2) The weight estimation approach in the proposed algorithm uses both gradient descent and ridge regression. The weights of the actor network are optimized through gradient descent while in the critic network, weights are estimated using ridge regression.\n3) The proposed algorithm is evaluated in two classic continuous control problems and is compared with the widely recognized DDPG algorithm.\nThe remainder of this brief paper is organized as follows. Section II details the construction and implementation of the proposed algorithm. In Section III, the proposed algorithm is evaluated. Finally, the conclusion is provided in Section IV."}, {"title": "II. BROAD CRITIC DEEP ACTOR (BCDA) ALGORITHM", "content": "In this section, the construction of the framework of the BCDA algorithm is initially outlined, detailing the architectural and procedural elements essential to its operation. Then, its implementation and functionality are discussed."}, {"title": "A. Framework of Proposed BCDA Algorithm", "content": "Although the framework introduced in this study is motivated by the DDPG algorithm, there are two significant differences. Firstly, the critic network is composed of a BLS rather than a DNN while the actor network is a DNN. Secondly, the actor network parameters are optimized using the gradient descent method while the critic network parameters are calculated using ridge regression.\nThe principal components of this framework are outlined as follows:\n1) Environment: The environment represents the external system with which the agent interacts.\n2) Training buffer: The training buffer is viewed as a short-term memory serving three main purposes. Firstly, it operates as the replay buffer, similar to its usage in the DDPG algorithm, where it stores the experiences derived from the interactions of the agent with the environment. Secondly, it stores the calculated targets for BLS training. Thirdly, it facilitates the computation of gradients of the Q-values with respect to the actor network parameters that are used for the optimization of the actor network. The training buffer records a sequence R = [{St, at, rt, St+1, d}, {yt}] where St, at and rt respectively represent the state, action, and reward at time step t, St+1 is the subsequent state produced by the environment given action at. d indicates whether the current state is terminal. Yt denotes the target Q-values used for BLS training.\n3) Broad Critic Network (BCN): The BCN processes inputs {St, at} to generate the current Q-value at time step t, denoted as Qt. The training of this network utilizes yt as target data. The calculation of the weights for the output layer of the critic network is achieved through the application of ridge regression.\n4) Deep Actor Network (DAN): The DAN is constructed using a straightforward DNN where its primary purpose is to approximate the optimal policy function.\n5) Target BCN (t-BCN) & Target DAN (t-DAN): Both the t-BCN and t-DAN are initialized with parameters identical to those of the BCN and DAN, respectively. These target networks are updated simultaneously with the main networks."}, {"title": "B. Implementation of Proposed BCDA Algorithm", "content": "According to the framework outlined in Section II A, the implementation is carried out according to the following steps.\n1) Initialization: Initially, both the DAN and BCN, including t-DAN and t-BCN, as well as the training buffer are initialized. The parameters of the BCN, denoted as an \u00d8, are randomly initialized and comprise {Wm, Wf, Bf, W, \u03b2\u03b7}, where Wm represents the weight of the final output layer, Wf and \u1e9ef are the weights and bias for the feature mapping nodes, and Wh and \u1e9eh correspond to the weights and bias for the enhancement nodes, respectively.\n2) Preparing Training Sample for BCN Training: Training the BCN requires target values which are essentially functioning as labeled data. The primary objective of the BCN is to produce Q-values (approximating the action-value functions) upon the input of {st, at}. The aim is to refine the BCN such that the Q-values generated progressively align with the target Q-values. Generally, this optimization process involves minimizing the mean squared bellman error (MSBE) [16] which is mathematically represented as:\nMSBE = E[(Q\u00f8(St, at) - (rt + \u03b3t d maxat+1 Q\u00f8(St+1, at+1)))\u00b2]\nHence, the target Q-values, yt can be defined as:\nyt = { rt,if St+1 reaches se\n \u03b3t maxat+1 Q\u00f8(St+1, at+1), otherwise.\nwhere se is terminal state and \u03b3t maxat+1 Q\u00f8(St+1, at+1) is computed using the t-BCN which is initialized with random weights, while at+1 is generated using the t-DAN.\nIn BCN training, yt serves as the target data, while {St, at} sampled from the training buffer are the input data for training. Consequently, the BCN adjusts its weights to align with yt, employing regression to approximate this target rather than directly minimizing the MSBE. This approach is different from the conventional weight optimization methodologies in typical RL algorithms that involve gradient descent to minimize MSBE.\n3) BCN Training: The training input data, X = {St, at}, and the target data, Y = {yt} are fed into the BCN network. First, feature extraction of X is performed to generate ith feature node, Zi as:\nZi = d(XWfi + \u03b2fi), i = 1,..., n.\nwhere Wfi and Bf\u2081 are randomly generated weights and biases of the feature mapping nodes, respectively. (\u00b7) is a nonlinear mapping function. Thus, all the feature nodes can be denoted as:\nZn = [Z1,..., Zn].\nThen, feature nodes are used to obtain enhancement nodes, Zn as:\nHj = \u00a7(ZnWej + \u03b2ej), j = 1,..., m.\nwhere Wej and Bej are randomly generated. \u00a7(\u00b7) is also a nonlinear mapping function. Thus, all the enhancement nodes can be denoted as:\nHm = [H1,..., Hm].\nFinally, all the feature nodes, Zn and enhancement nodes, Hm can be concatenated for feeding into the final output layer as:\nY = [Zn|Hm]Wm.\nTake Am = [Zn|Hm], then Y = AmWm.\nY is the output of the BCN while Wm is the weight of the final output layer. Here, ridge regression approximation is utilized to find the weight Wm. It can be calculated using the pseudoinverse of the Am as follows:\nWm = (Am)+Y\nWm = (AmT Am + \u03bbI)\u22121 AmT Y\nIf the above method does not provide sufficient accuracy during the training of the BCN, additional enhancement nodes can be added to increase the accuracy.\nLet's assume that one additional enhancement node Hm+1 is added to the network. Then using (5):\nHm+1 = \u03be(ZnWem+1 + \u03b2em+1)\nwhere Wem+1 and Bem+1 are the weight and the bias of the additional enhancement node. According to theory in [17] pseudoinverse of the new matrix is calculated as:\nAm+1+ = [(Am)+ \u2212 DBT] [ BT]\nwhere D = (Am)+Hm+1, and\nBT = { (C+ , if C \u2260 0\n((1 + DTD)\u22121 DT (Am)+,if C = 0\nwhere C = Hm+1 \u2212 (Am)+ D.\nHence, the weight of the final output layer after adding new enhancement node is given by:\nWm+1 = [(Am)+ \u2212 DBT]Y\n BTY\nComputation of the pseudoinverse of the additional enhancement nodes is enough instead of computing the entire Am+1 matrix. Consequently, if adding enhancement nodes does not yield the anticipated improvement in the accuracy, additional feature nodes can be added similarly to further increase the accuracy [9].\n4) Updating DAN: Within the BCDA algorithm, the actor network is updated similarly to the approach utilized in the DDPG algorithm as detailed in [12].\n5) Updating t-BCN & t-DAN: The t-BCN and t-DAN are updated as follows:\n\u00d8t\u2212BCN \u2190 p\u00d8t\u2212BCN + (1 \u2212 p)\u00d8BCN\n\u00d8t\u2212DAN \u2190 p\u00d8t\u2212DAN + (1 \u2212 p)\u00d8DAN\nIt should be noted that different strategies can be utilized for the weight optimization in the BCN in addition to the ridge regression."}, {"title": "III. EXPERIMENTS AND DISCUSSIONS", "content": "In this section, the numerical evaluation of the proposed approach is outlined, followed by an analysis of the obtained results."}, {"title": "A. Evaluation Methodology", "content": "To evaluate the efficiency of the BCDA algorithm two MuJoCo [18] continuous control tasks coupled with Open Al gym [19] are used as follows:\n1) Inverted pendulum (Hereinafter referred to as \u201cINVPEN\u201d): The schematic of the INVPEN control task is depicted in Fig. 2 where M\u2081g and L are the weight and the length of the pole respectively. In the INVPEN, position of the cart along the linear surface (x), vertical angle of the pole on the cart (0), linear velocity of the cart (x), and angular velocity of the pole on the cart (0) are observed by the RL agent. Subsequently, the RL agent takes actions that correspond to the force (F) applied to the cart. The objective is to maintain the inverted pendulum in an upright position as long as possible. Consequently, a reward of +1 is granted to the agent for each timestep during which the pole remains upright. Given that the maximum episode duration is 1000 timesteps, the highest achievable reward per episode is 1000.\n2) Reacher (Hereinafter referred to as \"REA\"): The schematic of the REA control task is depicted in Fig. 3. The primary goal of REA is to guide the end effector of a robot arm, known as the fingertip, towards a target that appears at a randomly determined location. The observations provided to the RL agent include angles of the two arms (0\u2081 and 02), angular velocities of the arms (0\u2081 and 02), coordinates of the target (x\u2081 and y\u2081), and coordinates of the fingertip (x2 and y2). The actions are the torque applied at the first and the second hinges (\u03c4\u2081 and t2) respectively. The reward is given considering the difference in position between the target and the fingertip, along with the intensity of the action carried out by the RL agent. The maximum episode duration is 50 timesteps.\nIn this study, the efficiency of the BCDA algorithm is evaluated in comparison to the DDPG algorithm. However, according to some benchmark studies, the TD3 [20] and SAC [21] algorithms demonstrated greater learning accuracies and efficiencies than the DDPG in specific continuous control tasks [21]. Nonetheless, the decision to select DDPG as the comparative baseline over SAC and TD3 is due to two reasons. Firstly, both TD3 and SAC are enhanced versions of DDPG. Therefore, if the proposed architectural variation proves to be more efficient than DDPG, it could potentially be adapted to improve not only TD3 and SAC but also other actor-critic RL algorithms. Secondly, the DDPG algorithm is widely used as a foundational actor-critic RL algorithm for further improvements.\nThe experimental evaluations are performed using an NVIDIA Tesla T4 GPU and all algorithms are implemented in Python using TensorFlow [22]. Each experiment is conducted across five distinct random trials, extending through 100,000 timesteps. Performance metrics are assessed at intervals of 500 timesteps, during which the algorithms are executed without including action noise, and the average reward is recorded. Additionally, the average training duration is also recorded.\nThe base BCN (b-BCN) has 10 feature nodes and 500 enhancement nodes. Across all the variants of BCNs, shrinkage coefficient, regularization factor and learning rates are consistently set as 0.8, 2-30, and 0.005 respectively. All the DNNs in both BDCA and DDPG algorithms are three-layer feed-forward networks with 256 hidden nodes in each layer and rectified linear units (ReLU) [23] as the activation function between each layer.\nThe incremental learning (IL) feature within the BLS is significantly beneficial in enhancing the performance of the training process [9]. To evaluate this, four training schemes are designed to train the REA using the BCDA algorithm: (a) Using the b-BCN without IL (referred to as \"IL Scheme-1\"), (b) IL by adding 5 feature nodes and 100 enhancements nodes to the b-BCN (referred to as \"IL Scheme-2\"), (c) IL by adding 5 feature nodes and 300 enhancements nodes to the b-BCN (referred to as \"IL Scheme-3\"), and (d) IL by adding 10 feature nodes and 300 enhancement nodes to the b-BCN (referred to as \"IL Scheme-4\")."}, {"title": "B. Results and Analysis", "content": "Fig. 4(a) and 4(b) illustrate the learning curves for the INVPEN and REA respectively across the five trials, indicating the moving average of the average reward accumulated throughout the learning trajectories for each task. A window size of 10 is utilized to compute the moving averages. The shaded regions around the lines represent the half-standard deviation of the average evaluations across the five trials. It provides an insight into the variability of the performance across 5 trials.\nIn both control tasks, the BCDA algorithm exhibits a significantly steeper learning curve during the initial timesteps compared to the DDPG algorithm, indicating a faster convergence towards the optimal reward. However, a notable observation is the increased fluctuation in performance after achieving the optimal reward in the INVPEN when learned using the BCDA algorithm. It can be attributed to the overfitting of the Q-value approximation through ridge regression within the BCDA framework. This overfitting implies that although our algorithm is capable of rapid learning and quick convergence towards optimal rewards, it struggles to maintain stable performance thereafter. Nevertheless, the BCDA algorithm employs regularization [24] to mitigate overfitting by penalizing large coefficients. However, the choice of regularization strength is crucial, and especially a relatively straightforward task like INVPEN is prone to be over-learned.\nTable I shows the maximum average reward over 5 trials obtained using BCDA and DDPG algorithms on both control tasks. It can be observed that BCDA offers a higher maximum average reward on both control tasks. This suggests that BCDA architecture is successful in learning the continuous control tasks and offers a faster learning rate.\nTable II presents the average training duration across five trials for both DDPG and BCDA agents. BCDA demonstrates a significant reduction in training time compared to the DDPG algorithm due to the rapid learning capabilities of the BLS. Thus, it is demonstrated that integrating BLS with DNN under the actor-critic framework can enhance efficiency without compromising the learning accuracy.\nThe learning curves of the REA for each IL scheme (as described in Section III A) are shown in Fig. 5. IL Scheme-3 shows the highest amplitude of the average reward since it has the highest number of enhancements among all the schemes. Although IL Scheme-4 has the highest number of feature nodes, it gives a similar amplitude to the one in IL Scheme-3 and demonstrates a high degree of instability in training. This instability originates from excessive feature extraction and the overfitting of the BCN within the BCDA."}, {"title": "IV. CONCLUSION", "content": "Improving efficiency is crucial in online RL for continuous control problems. This brief study introduces an innovative architectural variation to the actor-critic networks by incorporating BLS with DNN in a hybrid manner. Within this architecture, the critic network utilizes BLS while the actor network is implemented using DNN. Furthermore, ridge regression is employed to determine the parameters of the critic network, while the actor network optimizes weights using the gradient descent algorithm. Although BLS was applied in RL algorithms in previous studies, its application within actor-critic algorithms, especially for learning continuous control problems, has not been explored. The results reveal that the BCDA control approach not only enhances modeling and decision-making capabilities but also significantly reduces computational complexity compared to the widely recognized actor-critic based DDPG algorithm.\nThis study opens up multiple avenues for future research. In this study, The DDPG algorithm is utilized to develop the proposed hybrid architectural variation, and its efficiency and learning capabilities are assessed in comparison to the standard DDPG algorithm. However, there is a need for additional research to demonstrate that the hybrid architecture used in the BCDA algorithm can also be incorporated with more advanced actor-critic RL algorithms, such as TD3 and SAC. Ultimately, this study suggests that incorporating the proposed hybrid BCDA architecture with actor-critic algorithms could further improve the efficiency of RL algorithms."}]}