{"title": "Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models", "authors": ["Rohit Jena", "Ali Taghibakhshi", "Sahil Jain", "Gerald Shen", "Nima Tajbakhsh", "Arash Vahdat"], "abstract": "Text-to-image (T2I) diffusion models have become prominent tools for generating high-fidelity images from text prompts. However, when trained on unfiltered internet data, these models can produce unsafe, incorrect, or stylistically undesirable images that are not aligned with human preferences. To address this, recent approaches have incorporated human preference datasets to fine-tune T2I models or to optimize reward functions that capture these preferences. Although effective, these methods are vulnerable to reward hacking, where the model overfits to the reward function, leading to a loss of diversity in the generated images. In this paper, we prove the inevitability of reward hacking and study natural regularization techniques like KL divergence and LoRA scaling, and their limitations for diffusion models. We also introduce Annealed Importance Guidance (AIG), an inference-time regularization inspired by Annealed Importance Sampling, which retains the diversity of the base model while achieving Pareto-Optimal reward-diversity tradeoffs. Our experiments demonstrate the benefits of AIG for Stable Diffusion models, striking the optimal balance between reward optimization and image diversity. Furthermore, a user study confirms that AIG improves diversity and quality of generated images across different model architectures and reward functions.", "sections": [{"title": "1. Introduction", "content": "Text-to-Image (T2I) Diffusion Models (DMs) [21, 42, 48,56] have emerged as a powerful class of generative models for high-fidelity image generation from text prompts, trained on large-scale image-caption data. However, training on unfiltered internet data captures the entire uncurated internet-scale data distribution that is not aligned to human preferences. These models may generate images that are unsafe [22], misaligned with respect to the prompt [23], or that have undesirable stylistic effects [29,68,69]. Following the effectiveness of learning from human preferences [11] for large language models (LLMs) [39,46,71], human preference datasets [22, 29, 54, 68, 69] have been collected that reflect different style and content preferences by humans. These datasets are used to either directly finetune the T2I model [3, 22, 64] or maximize a reward function trained to capture human preferences [12,65,69]. Unlike LLMs, most diffusion models can be finetuned by directly backpropagating through the (differentiable) reward function instead of relying on policy gradient algorithms, leading to high sample efficiency [12]. However, these methods are prone to reward hacking where the finetuned model loses all its diversity and produce a unimodal set of images.\nIn this paper, we first show the inevitability of reward hacking without regularization. This is followed by analyzing the common regularization approaches for finetuning DMs. However, these regularizations suffer from suboptimal reward-diversity tradeoffs due to two reasons. The first reason is indiscriminate regularization, i.e. the regularization is applied on all timesteps of the sampling chain"}, {"title": "2. Related Work", "content": "Text-to-image diffusion models. Diffusion models [21,28,56,57,60] are generative models that use a forward Markov process which diffuses the data distribution into a noise distribution, to convert data into noise. This is followed by learning a corresponding reverse process which converts the noise back into data. This line of work emerged from score matching estimation of the data distribution using samples only [24, 59] using neural networks, which can be used to run Langevin dynamics to sample data [7]. However, the manifold hypothesis leads to low data density regions in the ambient space, making Langevin mixing slow. Denoising score estimation with score matching [57, 63] was proposed to improve data density and faster mixing dynamics, setting the tone for effective score-based generative models, followed by improved training recipes [58]. These models have been successful in a variety of domains, including high-fidelity image and video generation [1, 5, 6, 20, 27, 42, 47-51], audio synthesis [32, 35, 66], robotics [4, 8, 25], 3D [26, 38, 43, 53] and medical imaging [2, 40, 41, 67, 72]. However, training on noisy image-caption pairs from the internet lead to models that show lack of alignment in terms of attribute binding [17, 23, 33], counting, spatial relationships and occlusions [70], stylistic choices [29,68], and text rendering [9, 10, 34].\nText-to-image model alignment. Learning from human preferences is a popular strategy to align text-to-image models from curated human preference datasets, following its efficacy in alignment of large language models (LLMs). Alignment is performed either by learning a reward model from the preference dataset followed by a finetuning the model on this reward model using RL [11, 13, 39], or finetuning the model on the preference data directly [46, 71]. Popular multimodal reward models for human pref-"}, {"title": "3. Method", "content": "We briefly review the DM finetuning approach with differentiable reward functions. DRaFT [12] proposes a simple yet effective approach for fine-tuning diffusion models for differentiable reward functions. Although RL-based methods that have been shown to be successful for finetuning LLMs, they are shown to be sample inefficient and harder to tune for diffusion models. For a noise latent $x_T\\in \\mathbb{R}^n$, (prompt) condition $c\\in \\mathbb{R}^m$ and timestep $t\\in \\mathbb{R}^+$, DRaFT considers a differentiable reward function $r:\\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that is used to optimize a diffusion model $f_\\theta:\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^+ \\rightarrow \\mathbb{R}^n$ parameterized by $\\theta$. The diffusion model is finetuned to maximize the reward function on its generated samples:\n$J(\\theta) = \\mathbb{E}_{c\\sim p(c),x_T\\sim \\mathcal{N}(0,I)} [r(x_0(\\theta, c, x_T), c)]$\t(1)\nwhere $x_0$ is the generated image which is the solution of the reverse-time probability flow ODE [60] (or an equivalent reverse-time SDE):\n$dx = \\left[f(t)x - \\frac{1}{2}g(t)^2 \\nabla_x \\log(p_t(x|c)) \\right] dt, x_T \\sim \\mathcal{N}(0,I)$\t(2)\nwhere $\\nabla_x \\log(p_t(x|c)) \\approx f_\\theta(x, c, t)$ is the estimated score function learned using a diffusion model. Eq. (1) can be interpreted as finding the distribution $p_\\theta(x_0|c)$ that maximizes the expected reward. We denote $p_\\theta(x_0c)$ to refer to the probability distribution induced by Eq. (2).\nReward Hacking RL-based and direct finetuning methods including DRaFT are known to be vulnerable to a phenomenon called reward hacking [12, 16, 46, 55, 64], which refers to the model only producing a very small subset of samples. [12] posits \"Reward hacking points to deficiencies in existing rewards; efficiently finding gaps between the desired behavior and the behavior implicitly captured by the reward is a crucial step in fixing the reward.\" Consequently, one of the interesting experiments they tried is performing aggressive reward dropout. We prove that reward hacking is not due to deficiencies in existing rewards, but is an unavoidable artifact of the Expected Reward Maximization problem formulation. We first show the inevitability of reward hacking in an unrestricted setting.\nLemma 1. Inevitability of Reward Hacking In a nonparametric setting under the expected reward maximization optimization, the optimal probability distribution $p(x_0|c)$ collapses to $p(x_0|c) = \\delta(x - x^*)$, $x^* = \\text{arg}\\,\\text{max}_{x_0} r(x_0, c)$, where $\\delta$ is the Dirac-delta function.\nProof. Proof is in Appendix A.1. The proof also shows that reward dropout does not work because it is simply rewardmaximization under a new reward function.\nEven with low-dimensional parameteric updates like LORA finetuning that restricts the set of admissible distributions of $p_\\theta (x_0|c)$, the generated images exhibit severe mode collapse (see Appendix A.3). We discuss existing regularization schemes to preserve the diversity of the model."}, {"title": "3.2. Existing Regularizations", "content": "We discuss existing proposed attempts to prevent reward hacking for diffusion models, identify a unifying interpretation for these regularizations, and propose our method that tackle limitations of existing regularizations."}, {"title": "3.2.1 KL divergence", "content": "A straightforward strategy to mitigate the reward hacking behavior is to introduce a KL divergence term between the base and finetuned distribution, denoted as $P_{\\text{base}}$ and $p_\\theta$ respectively. This new objective is written as\n$J(\\theta) = \\mathbb{E}_{c,x_0\\sim p_\\theta} [-r(x_0, c) + \\lambda \\text{KL}(p_\\theta (x_0|c)||P_{\\text{base}} (x_0|c))]$ (3)\nFor diffusion models, the second term does not have a closed form. However, the second term is upper bounded by an ELBO term of the form $\\sum_{t=1}^T\\text{KL}(p_\\theta (x_{t-1}|x_t, c) ||P_{\\text{base}} (x_{t-1}|x_t,c))$ (see Appendix in [16] for proof). Substituting the term,\n$J(\\theta) = \\mathbb{E}_{c,x_0\\sim p_\\theta} [-r(x_0, c) + \\sum_{t=1}^T \\frac{1}{2}||f_\\theta(x_t, c) - f_{\\text{base}} (x_t, c) ||^2]$ (4)\nSimilar to DRaFT-K, we only calculate gradients through the last K steps of this loss function. In this paper, we extend some of our previous qualitative analysis [62] using KL divergence for DRaFT-based reward finetuning. Although KL regularization is a straightforward addition to prevent reward hacking, this regularization suffers from a few practical considerations. First, the hyperparameter $\\lambda$ has to be chosen before finetuning, leading to substantial compute cost to find the optimal tradeoff between reward and diversity. Second, the interpretation of $\\lambda$ depends on the parameterization $\\theta$, i.e. the same $\\lambda$ gives widely different solutions for different architectures. This is evident in Fig. 3 where SDv1.4 and SDXL admit \u2018optimal' qualitative performance for very different ranges of $\\lambda$ (SDv1.4 breaks down at higher values of $\\lambda$ where SDXL works well). Third, there is a problem of indiscriminate regularization (see Sec. 3.2.2) and direct regularization with respect to the base model does not account for 'reference mismatch', leading to a suboptimal reward-diversity tradeoff."}, {"title": "3.2.2 LoRA scaling", "content": "DRaFT also proposes LoRA scaling to 'interpolate' between the base and finetuned model. Specifically, for a finetuned model with weights $\\theta^* = \\theta_{\\text{base}}+\\theta_{\\text{LoRA}}$, LORA scaling uses the weights $\\theta^*_{\\text{new}} = \\theta_{\\text{base}} + \\alpha' \\theta_{\\text{LoRA}}$, $\\alpha' \\in (0, 1)$ to produce samples. Although this trick was proposed only empirically, we can theoretically interpret LoRA scaling as an implicit weight regularization over the network parameters w.r.t. the base model, with mild assumptions.\nLemma 2. Consider a function f(\u03b8) with minimizer $\\theta^* = \\theta_{\\text{base}} + \\theta_{\\text{LoRA}}$, and f is locally convex around $\\theta^*$, Then,"}, {"title": "3.3. Annealed Importance Guidance", "content": "Our proposed algorithm - Annealed Importance Guidance, builds on the Annealed Importance Sampling (AIS) [37] interpretation of NCSN [57]. Simulated Annealing (SA) samples from a distribution $p_0(x)$ by using a set of proposal distributions $p_t(x) \\propto p_0(x)^{\\beta_t}, 1 = \\beta_0 > \\beta_1 > ...\\beta_T$. A sample is generated by first sampling from $p_T(x)$ and repeatedly transitioning from $p_t$ to $p_{t-1}$ until we get a sample from $p_0(x)$. Here, $p_T$ is the uniform distribution, which is easier to sample from, and Langevin dynamics can be run to converge to a sample from $p_0(x)$. Similarly, the diffusion models define $p_t (x) \\propto \\int_y p_0(y)G(\\tilde{\\alpha}_t y, x, t)dy$ as \u2018smoothened' versions of $p_0(x)$, and anneal from $p_T(x) \\approx \\mathcal{N}(0,I)$ to $p_0(x)$. Specifically, the reverse-time SDE or probability ODE (as in Eq. (2)) formulation can be thought of as performing SA with the proposal distributions $p_t(x_t)$ using the Langevin dynamics defined by its score function $\\nabla_{x_t} \\log(p_t(x_t)) \\approx f_\\theta(x_t, t)$.\nIn contrast, AIS [37] defines the intermediate proposal distributions as $p_t(x) \\propto p_0(x)^{\\beta_t}p_n(x)^{1-\\beta_t}$, where $p_0$ is the distribution of interest, and $p_n$ is a distribution we can sample from. In our case, $p_n (x)$ is the base distribution, and $p_0 (x)$ is the DRaFT distribution. Therefore, we propose the following transition probability distributions:\n$p_{\\text{AIG}}(x_t, t) = p_{\\text{base}} (x_t, t)^{\\gamma(t)} p_\\theta (x, t)^{(1-\\gamma(t))}$ (10)\nfor an monotonic function $\\gamma(t)$ with $\\gamma(0) = 0, \\gamma(T) = 1$. Intuitively, this family of proposal distributions ensures that the Langevin mixing dynamics are governed by $P_{\\text{base}}$ in the earlier timesteps, and $p_\\theta$ in the later timesteps. Early mixing from $P_{\\text{base}}$ ensures that modes from the data distribution are recovered (see Appendix A.6.1), later mixing from $p_\\theta$ then pushes the noisy data to the nearest high-reward mode. The modified annealed Langevin dynamics are therefore gov-"}, {"title": "4. Experiments", "content": "In this section, we compare the effects of various forms of regularization, and showcase finetuning applications.\nImplementation Details We consider checkpoints from two models Stable Diffusion v1.4 (SD1.4) and Stable Diffusion XL Base (SDXL) for our experiments. We ablate on two popular reward models - the Human Preference Score (HPSv2) [68] and Pickscore [29]. All experiments use DRaFT-1 style training, wherein gradients are computed only through the last sampling step, since that exhibits the most stable training dynamics and high rewards [12]. We use the DDIM sampler for SD1.4 and EDM sampler for SDXL, with 25 sampling steps. We use a classifier-free guidance weight of 7.5 for SD1.4 and 5.0 for SDXL, following best practices. All finetuning is implemented and run using the NVIDIA NeMo framework [30]. We use a subset of 50k prompts from the Pick-a-Pic dataset for finetuning with the reward model. All DRaFT and DRaFT+KL models are trained on a single node with 8 H100 GPUs for 4 hours, with DDP-level parallelism. More implementation details are present in Appendix Appendix A.7."}, {"title": "4.1. Evaluation Measures", "content": "First, we generate images from the Partiprompts and HPDv2 prompt datasets. The Partiprompt prompt dataset encompasses prompts from a variety of concept classes, while the HPDv2 focuses on stylistic features - evident by its 4 subclasses (photos, anime, concept-art, paintings). Then, we evaluate the Pickscore reward, HPSv2 reward, and CLIP alignment on these generated images. Furthermore, we explicitly compute the 'diversity/coverage' with respect to the base model. To this end, we manually choose a set of 40 complex prompts from the PartiPrompt dataset (referred to as the 'coverage prompt dataset'), and generate 50 images for each prompt with the same random seeds. The distribution of these images is then compared with that of the base model. We evaluate the reward scores in conjunction with four coverage measures, to analyse the Pareto efficiency of each finetuning method. We choose the following coverage evaluation criteria:\nFrechet Inception Distance [19] This metric captures the discrepancy in feature distribution considering a Gaussian distribution for the features of each distribution. Given samples from two distributions $D_1 = {f_1^{(1)}, f_1^{(2)} ... f_1^{(n)}}$ and $D_2 = {f_2^{(1)}, f_2^{(2)} ... f_2^{(m)} }$, the FID is defined as\n$FID(D_1, D_2) = ||\\mu_1 - \\mu_2||_2^2 + Tr(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1\\Sigma_2)^{\\frac{1}{2}})$ (12)\nwhere $\\mu$ and $\\Sigma$ are the mean and standard deviation of the datasets, and Tr is the trace of the matrix.\nGeneralized Recall [52] introduce the classic concepts of precision and recall to the study of generative models, motivated by the observation that FID cannot be used for making conclusions about precision and recall separately. While high precision implies more realism of the images compared to the base distribution, high recall implies higher coverage of the data distribution by the generator. [31] propose to form non-parametric representations of the data manifolds using overlapping hyperspheres using kNN with hyperparameter k (nearest neighbors), followed by binary assignments of each data point to the manifold to compute recall. We compute the recall of the distribution with respect to the base model, which covers multiple modes of the dataset. We use k = 10 in our experiments.\nSpectral Covariance Distance One limitation of FID and Generalized Recall is that these metrics were created to ensure exact overlap between the data and generator distribution. Consequently, they penalize any 'reference mismatch' w.r.t. the base distribution as well. A good coverage metric should only penalize differences in the 'spread' of the distribution, ignoring any reference mismatch due to translational and rotational differences. To measure the 'spread', consider the eigendecomposition of covariance of the dataset features as $\\Sigma_1 = \\mathbb{E}[(x-\\mu)(x-\\mu)^T] = U_1\\Lambda_1U_1^T$ and $\\Sigma_2 = U_2\\Lambda_2U_2^T$. Rotating the data from $D_1$ by rotation $R = U_2U_1^T$, the new covariance matrix of the data becomes $\\Sigma_1 = U_2\\Lambda_1U_2^T$, whose principal eigenvectors now align with that of $D_2$. We propose the Spectral distance as the differences in corresponding eigenvalues\n$SCD(D_1, D_2) = ||\\Sigma_1 - \\Sigma_2||_2^2 = \\sum_{i=1}^N (\\lambda_i^{(1)} - \\lambda_i^{(2)})^2$\t(13)"}, {"title": "4.2. Reward-KL tradeoff", "content": "First, we train SD1.4 and SDXL on both the HPSv2 and Pickscore reward functions. We choose a variety of hyperparameter values for KL regularization, i.e. $\\lambda$ ="}, {"title": "A.7.3 Choice of y", "content": "Unless the KL parameter $\\lambda$ or LoRA scaling parameter $\\alpha'$ that are scalar quantities, AIG requires a function $\\gamma(t)$. In this paper, we consider the family of functions\n$\\gamma_{p,T}(t) = 1 - (\\frac{T-t}{T})^p$\t(14)\nFor $p = 1$, the weighing is simply linear, i.e. $\\gamma_{1,T}(t) = \\frac{t}{T}$. For $p > 1$, the power term quickly vanishes and the sampling dynamics are governed by the base model for more timesteps. For $p < 1$, the power term remains close to 1, therefore"}, {"title": "A.1. Proof of inevitability of reward hacking", "content": "Consider an arbitrary reward function r(x) that is sufficiently smooth. Consider a non-parametric probability distribution\np(x) which maximizes the expected reward:\n$p^* = \\text{arg max} \\mathbb{E}_{x \\sim p(x)} [r(x)] $\t(15)\nwith the constraint $\\int p(x)dx = 1$. This is written as a maximization problem with Langrange multiplier \u03b2\n$\\text{max}_{p(x)} \\int p(x)r(x)dx - \\beta (\\int p(x)dx -1)$\t(16)\n$\\Rightarrow \\int p(x)r(x) - \\beta\\int p(x)dx + \\beta$ \n\t(17)\n$\\Rightarrow \\int L(x, p, \\dot{p})dx + \\beta$\\t(18)\nThis is an Euler-Langrage equation where L(x, p,\\dot{p}) = p(x)(r(x) \u2212 \u03b2). The maximizer of this equation is given by:\n$\\frac{\\partial L}{\\partial p} - \\frac{d}{dt} [\\frac{\\partial L}{\\partial \\dot{p}}] = 0$\t(19)\nSince the second term in Eq. (19) is zero, we get\n$\\frac{\\partial L}{\\partial p} = r(x) \u2212 \u03b2 = 0$\t(20)\nNote that \u03b2 should be a constant, for a general r(x) Eq. (20) will not hold true. However, note that p(x) \u2265 0\u2200x. Therefore, if r(x) < \u03b2, then p(x) = 0 and if r(x) > \u03b2, then p(x) will grow indefinitely, violating the pdf constraint $\\int p(x)dx = 1$. However, \u03b2 should be chosen such that r(x) \u2264 \u2200x. However, if r(x) < \u2200x, then p(x) = \u2200x. Therefore, \u03b2 is chosen to be\n\u03b2 = sup r(x), leading to the optimal distribution\np\u2217 (x) = \u03b4(x \u2212 x\u2217)\t(21)\nwhere x\u2217 = arg max r(x). If r has multiple maxima with the same maximum value, say {x1, x2... xN}, r(x) = sup r(x), then there exists a family of optimal distributions:\np\u2217 (x) = \\sum_{i=1}^{N} \\omega_i \\delta(x \u2212 x_i)\t(22)\nsuch that  \u2211i wi = 1. We assume that r is not \u2018flat\u2019 at this maximum value, therefore p\u2217 (x) lacks diversity.\nFor a conditional distribution p(x|c) maximizing the reward r(x, c), a similar derivation yields p(x|c) = \u03b4(x \u2212 x\u2217|c), where x\u2217|c = arg max r(x, c). This completes the proof in the non-parameteric case. Note that no assumption is made about the finetuning algorithm (DPO, DRaFT, ReFL, etc.) or nature of the reward function (CLIP, JPEG compression, Aesthetics, etc.). This proves that reward hacking is an artifact of the expected reward maximization problem formulation itself.\nIn the parameteric case, the optimal p\u2217 (x) may not be achievable due to the parameterization. However, even with lowdimensional parameter updates like LoRA, we notice a substantial loss of image diversity when training DRaFT. Qualitative comparisons between the base, DRaFT and our regularization are shown in Figs. 13 to 16."}, {"title": "A.1.1 Adding dropout to reward functions does not work", "content": "Moreover, this explains why even reward functions in [12] with aggressive dropout rates (> 0.95) still led to reward collapse.\nLet the reward model be parameterized by \u03c6 and let N = |\u03c6| be the dimension of the reward model parameters. Under the dropout case with dropout parameter \u03be, the expected reward maximization formulation becomes:\np\u2217 = arg max Exp(x),u\u2208U[0,1]^N [r\u03c6[u,\u03be](x)]\t(23)"}, {"title": "4.3. Reward-diversity tradeoff", "content": "In this section, we show a more comprehensive comparison of models with diversity metrics relative to the base model. Fig. 4 shows the Pareto fronts for models regularized with KL divergence, LoRA scaling, and AIG, for SD and SDXL trained on Pickscore, and evaluated on PartiPrompt and HPSv2 datasets. The Pareto front is computed by choosing different hyperparameters for each regularization (\u03bb for KL, \u03b1' for LoRA, \u03b3 for AIG), and plotting a curve. In all cases, AIG achieves a better reward-diversity tradeoff. Pareto fronts on other splits of the HPSv2 dataset, DMs trained on HPSv2 rewards and qualitative comparisons are in Appendix A.2."}]}