{"title": "ScreenAgent : A Vision Language Model-driven Computer Control Agent", "authors": ["Runliang Niu", "Jindong Li", "Shiqi Wang", "Yali Fu", "Xiyu Hu", "Xueyuan Leng", "He Kong", "Yi Chang", "Qi Wang"], "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at https://github.com/niuzaisheng/ScreenAgent.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM), such as ChatGPT and GPT-4, have recently demonstrated exceptional performance in natural language processing tasks like generation, understanding, and dialogue. They have also significantly revolutionized research in other artificial intelligence fields. In particular, the development of these technologies paves the way for the study of intelligent LLM agents, which are capable of performing complex tasks [Wang et al., 2023b]. A LLM agent is an AI entity with a large language model as its core computational engine. It possesses capabilities like Perception, Cognition, Memory, and Action, enabling the agent to perform highly proactive autonomous behaviors [Wang et al., 2023c]. In LLM agent-related research, how to enable agents to learn to effectively use tools for expanding their action space has drawn extensive attention.\nWith the growing prevalence of electronic devices, such as personal computers, smartphones, tablets, and smart electronic instruments, our lives are becoming more intertwined with the digital world. Daily activities often require frequent interaction with electronic device screens. If an agent can free people from manual operations, and seamlessly navigate these devices by controlling screens according to user needs, it would mark a significant step towards achieving more general and autonomous intelligence [Yin et al., 2023]. Indeed, a screen interaction agent must possess powerful visual information processing capabilities, and the ability to execute computer control instructions as shown in Fig. 1. Therefore, to achieve such a goal, it is necessary to first create a real interactive environment for the VLM agent, then guide the model and environment to form a continuous interactive pipeline, and train the agent to improve its performance. However, it's highly challenging to implement these functions within a unified framework and achieve satisfactory results from both the project engineering and theoretical research perspectives.\nDespite the recent progress achieved by several works, some aspects still need to be further explored. For instance, CogAgent [Hong et al., 2023] specializes in GUI understanding and planning, showcasing remarkable proficiency in addressing diverse cross-modal challenges. However, CogAgent lacks the capability of a complete thinking chain, preventing it from forming a comprehensive tool invocation similar to ChatGLM [Du et al., 2022; Zeng et al., 2022]. Later, AppAgent [Yang et al., 2023a] focuses on smartphone operations, learning navigation, and acquiring new application usage through autonomous exploration or by observing human demonstrations. While AppAgent lacks a planning process and sacrifices the freedom of operation. It guides the agent to click by labeling each element, thereby limiting the method of tapping. As a result, current Vision Language Models (VLM agents) are typically unable to interact with real computer or mobile environments to generate and execute continuous manipulative commands.\nTo address the above-mentioned issues, we propose ScreenAgent, an entirely automated agent designed to handle continuous screen operations. This agent is primarily achieved through three components, namely planning, execution, and reflection. In particular, the reflecting module is inspired by Kolb's renowned experiential Learning Cycle theory [Kolb, 2014], which enables the agent to perform reflective behaviors, making the entire pipeline more comprehensive and aligned with human action and thought processes. It autonomously assesses the execution status of the current action, providing feedback based on the ongoing state. This capability enhances its performance for subsequent actions, enabling our agent to possess the capability of a continuous thinking chain. Consequently, our agent can understand the next steps and engage in complete tool invocation to execute a series of continuous manipulative commands. The major contributions are summarized as follows:\n\u2022 We present a Reinforcement Learning (RL) environment that enables the VLM agent to directly interact with a real computer screen via VNC protocol. By observing the screenshot, our agent can interact with the GUI through basic mouse and keyboard operations.\n\u2022 We develop an automated pipeline that encompasses the planning phase, acting phase, and reflecting phase. This integrated pipeline facilitates the agent's continuous interaction with the environment, distinguishing our agent from others.\n\u2022 We propose the ScreenAgent dataset, which includes action sequences for completing generic tasks on Linux and Windows desktops. Moreover, we provide a fine-grained scoring metric to comprehensively evaluate the various capabilities that are necessary for a VLM agent in computer-controlling tasks.\n\u2022 We test GPT-4V and two state-of-the-art open-source VLMs on our test set. The results demonstrate that GPT-4V is capable of controlling computers, but it lacks precise positioning capabilities. We thus trained a ScreenAgent to enable precise positioning and achieved comparable results to GPT-4V in all aspects. Our work can facilitate further research on building a generalist agent."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multimodal Large Language Models", "content": "LLMs have demonstrated powerful contextual understanding and text generation capabilities, enabling the implementation of complex multi-turn question-answering systems. LLAMA [Touvron et al., 2023] is a series of foundational language models spanning from 7 billion to 65 billion parameters, with Vicuna-13B [Chiang et al., 2023], an open-source chatbot, being refined through fine-tuning on the LLaMA architecture. GPT-4 is an advancement by OpenAI following the success of GPT-3 and it introduces several noteworthy improvements. GPT-4V(ision) [Yang et al., 2023b], building upon GPT-4, has added multimodal capabilities. LLaVA [Liu et al., 2023b] and LLaVA-1.5 [Liu et al., 2023a] connect the pre-trained CLIP [Radford et al., 2021] visual encoder with the Vicuna, achieving multimodal capabilities. Fuyu-8B\u00b9 does not use an image encoder but opts for a pure decoder Transformer architecture. CogVLM [Wang et al., 2023e] is a powerful open-source Visual Language Model that supports image understanding at a resolution of 490 \u00d7 490 and multi-turn dialogues. In addition, Monkey [Li et al., 2023] introduces an efficient training method that enhances input resolution capability."}, {"title": "2.2 Computer Control Environment & Dataset", "content": "In simulated environments, agents can be trained to emulate clicking and typing. WebNav [Nogueira and Cho, 2016] creates a navigation environment with links, testing the agent's sequential decision-making ability. MiniWoB++ [Liu et al., 2018] provides a lot of simplified ATARI-like web-browser-based tasks as a Reinforcement Learning (RL) environment. WebShop [Yao et al., 2023] offers tasks for controlling the browser to complete the purchase process. SWDE [Hao et al., 2011] preserves webpage HTML files to train information extraction models. WebSRC [Chen et al., 2021] is a QA-style dataset that contains a large number of questions and answers about webpage screenshots. Mind2Web [Deng et al., 2023] introduces a dataset for developing generalist web agents. Seq2act [Li et al., 2020a] integrates three datasets for Android, AndroidHowTo, Rico-SCA, and PixelHelp. Screen2Words [Wang et al., 2021] is a large-scale screen summarization dataset for Android UI screens. META-GUI [Sun et al., 2022] is a dataset for training a multi-modal conversational agent on mobile GUI. [Burns et al., 2022] provides a dataset of unknown command feasibility on Android."}, {"title": "2.3 Large Language Model-Driven Agents", "content": "With the advancement of large language models, the capabilities of intelligent agents have also been enhanced. WebGPT [Nakano et al., 2021] conducts fine-tuning on GPT-3 to"}, {"title": "3 Framework", "content": "In this section, we introduce our Reinforcement Learning (RL) environment and the autonomous control flow within the Agent. Through this environment, a VLM agent can interact with a real computer screen, observe screen images, select actions, and autonomously complete specific tasks."}, {"title": "3.1 Computer Control Environment", "content": "We construct a computer control environment to assess the capabilities of VLM agents. This environment connects to a desktop operating system through remote desktop (VNC) protocol and allows the sending of mouse and keyboard events to the controlled desktop. The formal definitions of this environment are defined as follows:\n\u2022 A-Action Space. We define an action as a form of a function call. If the agent outputs an action in the required JSON-style format, the action will be parsed and executed by the environment. All action types and corresponding action attributes are defined in Table 1.\n\u2022 S-State Space. The screenshot image is utilized as the state space of the environment. The environment will collect screenshots s and s', denoting the state before and after each action, respectively.\n\u2022 R-Reward Function. Due to the highly open-ended nature of the task, the reward function in this environment is flexibly opened to different interfaces, which can integrate different existing or future reward models.\nThrough remote control, the agent can perform arbitrary tasks on the screen, which creates a highly challenging open environment having a large state and action space."}, {"title": "3.2 Control Pipeline", "content": "To guide the agent to continually interact with the environment and complete multi-step complex tasks. We designed a control pipeline including the Planning, Acting, and Reflecting phases. The whole pipeline is depicted in Fig. 2. The pipeline will ask the agent to disassemble the complex task, execute subtasks, and evaluate execution results. The agent will have the opportunity to retry some subtasks or adjust previously established plans to accommodate the current occurrences. The details are depicted as follows:\nPlanning Phase. In the planning phase, based on the current screenshot, the agent needs to decompose the complex task relying on its own common-sense knowledge and computer knowledge.\nActing Phase. In the acting phase, based on the current screenshot, the agent generates low-level mouse or keyboard actions in JSON-style function calls. The environment will attempt to parse the function calls from the agent's response, and convert them to device actions defined in the VNC protocol. Then our environment will send actions to the controlled computer. The environment will capture the after-action screen as input for the next execution phase.\nReflecting Phase. The reflecting stage requires the agent to assess the current situation based on the after-action screen. The agent determines whether needs to retry the current subtask, go on to the next sub-task, or make some adjustments to the plan list. This phase is crucial within the control pipeline, providing some flexibility to handle a variety of unpredictable circumstances.\nAll prompts and templates in the pipeline are provided in Appendix A."}, {"title": "4 ScreenAgent Dataset & CC-Score", "content": "Existing computer-controlled datasets typically have a narrow range of applicability scenarios. For instance, building upon the foundational premise that it is easy to obtain UI element metadata through HTML or developer modes, WebNav [Nogueira and Cho, 2016], Mind2Web [Deng et al., 2023], and SWDE [Hao et al., 2011] mainly focused on web browsing, while Seq2act [Li et al., 2020a] and Screen2Words [Wang et al., 2021] are tailored for Android. However, the mouse and keyboard are also common and universal interfaces to control a computer. To fill the gap in this type of control method, we build an interactive annotation process (shown in Fig. 3) to construct the ScreenAgent Dataset which is collected from Linux and Windows operating systems for completing specific tasks. This dataset endeavors to cover a wide range of daily computer usage scenarios, including daily office, booking, information retrieval, card games, entertainment, programming, system operations, and so on. As illustrated in Fig. 4, the ScreenAgent Dataset encompasses 39 sub-task categories across 6 themes. The dataset has 273 complete task sessions, with 203 sessions (3005 screenshots) for training and 70 sessions (898 screenshots) for testing. Fig. 5 shows important statistical information about the dataset. More statistical information and samples are provided in Appendix C.\nTo assess an agent's capability in the computer control task, we have designed a fine-grained evaluation metric Vision Language Computer Control Score (CC-Score) for assessing the similarity of control action sequences. This metric takes into account both the sequential order and actions' attribution. We developed specific similarity metrics for every action type. For mouse actions, the metrics include four aspects of consistency: action type, mouse operation type, mouse button, and whether the click coordinates are within the annotated feasible bounding box. For text and keyboard actions, the metrics involve two aspects: action type consistency and the BLEU score of the input text, single key, or keyboard shortcut combination. For the entire action sequence, we employ an alignment algorithm that identifies the maximum matching pairs of predicted action and labeled action, while maintaining the sequence order. This approach maximizes the overall score, which is used as the measure of sequence similarity. Ultimately, the CC-Score encompasses the normalized scores of predicted and labeled sequences, the F1 values for each action attribute classification, and the unigram similarity values for text types. The implementation details of CC-Score are provided in Appendix D."}, {"title": "5 Experiment", "content": "In the experimental phase, we assessed OpenAI GPT-4V performance on the ScreenAgent test set, along with evaluations of three open-source VLMs. Furthermore, one of these models underwent fine-tuning to potentially augment its proficiency in screen control tasks. Subsequently, we conducted a thorough analysis of the outcomes and identified several typical cases to elucidate the inherent challenges of our task."}, {"title": "5.1 Evaluation Results on ScreenAgent Test-Set", "content": "Apart from GPT-4V, we selected several recently released SOTA VLMs for testing, including LLaVA-1.5 [Liu et al., 2023a] and CogAgent [Hong et al., 2023]. LLaVA-1.5 is a 13B-parameter multimodal model, unfortunately, it only supports up to 336 \u00d7 336px image size inputs. CogAgent is an 18B-parameter visual language model designed for GUI comprehension and navigation. Leveraging dual image encoders for both low-resolution and high-resolution inputs, CogAgent demonstrates proficiency at a resolution of 1120 \u00d7 1120px, allowing it to discern minute elements and text.\nWe test the models' capabilities from two aspects: The ability to follow instructions to output the correct function call format, shown in Table 2, and the ability to complete specific tasks assigned by the user, shown in Table 3.\nFollowing instructions and executing correct function calls is the most fundamental skill for an LLM agent when learning to use external tools. Table 2 presents the success rate of these function calls for each attribute key. This assessment focuses on whether the model can accurately execute various functions encompassing the attribute items expected by manual action annotations. Note that, this evaluation does not consider the consistency of the attribute values with the golden labeling; it solely examines if the model's output includes the necessary attribute keys. From the table, GPT-4V and LLaVA-1.5 achieved higher scores, while CogAgent and its upstream model CogAgent-VQA underperformed. CogAgent-VQA and CogAgent-Chat almost entirely disregarded the JSON format action definitions in our prompts, resulting in a very low score on successful function calls. Therefore, rendering them completely incapable of interacting with our environment. To ensure fairness in comparison, we utilize OpenAI GPT-3.5 to extract action into JSON-style function calls from the original CogAgent-Chat responses, indicated as \"CogAgent-Chat (helped by GPT-3.5)\". Even so, its scores are significantly lower than those of LLaVA-1.5 and GPT-4V, although CogAgent has been trained on Mind2Web web browsing simulation datasets.\nTable 3 displays the fine-grained scores of predicted attribute values for each action within the successfully parsed function calls. As can be seen, GPT-4V remains the best performer, with action type prediction F1 score of 0.98. This implies that it can accurately select appropriate mouse or keyboard actions. Additionally, it can precisely choose the mouse action type, typing text, or pressing keys consistent with the golden label actions.\nThe ability for precise positioning is crucial in computer-controlling tasks. As indicated by the \"Mouse Position\" column in Table 3, current VLMs have not yet achieved the capability for precise positioning required for computer manipulation. GPT-4V refuses to give precise coordinate results in its answers, and two open-source models also fail to output the correct coordinates with our pipeline prompt template.\nAnother significant challenge for all models is the reflection phase. In this phase, the agent is required to determine whether the subtask has been completed in the current state, and decide whether to proceed further or make some adjustments. This is crucial for constructing a continuous interactive process. Regrettably, all models show insufficient accuracy in this determination, with GPT-4V achieving only a 0.60 F1 score. This implies that human intervention is still necessary during task execution."}, {"title": "5.2 Fine-tuning Training", "content": "To demonstrate the potential for ongoing research on the task, we continue to fine-tune the CogAgent-Chat model on our ScreenAgent training set to enhance its function call ability. Similar to the approach adopted in recent VLM works [Chen et al., 2023], we mix data from multiple datasets and construct four distinct training phases, which is illustrated in Table 4. We reformulated two objective detection datasets, COCO [Lin et al., 2015] and Widget Captions [Li et al., 2020b], into mouse-click tasks to enhance the model's localization ability. For Mind2Web, we implemented a series of complex data augmentations to align with our task objectives. The details of the data construction are outlined in Appendix B.\nAfter vision fine-tuning, ScreenAgent achieved the same level of following instructions and making function calls as GPT-4V on our dataset, as shown in Table 2. In Table 3, ScreenAgent also reached a comparable level to GPT-4V. Notably, our ScreenAgent far surpasses existing models in the precision of mouse clicking. This indicates that vision fine-tuning effectively enhances the model's precise positioning capabilities. Additionally, we observed that ScreenAgent has a significant gap compared to GPT-4V in terms of task planning, highlighting GPT-4V's common-sense knowledge and task-planning abilities."}, {"title": "5.3 Case Study", "content": "To evaluate our ScreenAgent model on computer control tasks, we provide two cases. In Fig. 7, we present a case illustrating the workflow of ScreenAgent executing a chain of actions. In Fig. 8, we compare different agents in executing the details of the three phases in the pipeline. Fig. 8 (a) shows the planning process of all the agents, where we find that our ScreenAgent produces the most concise and effective plan. Fig. 8 (b) presents four different click action tasks, each representing a step in a specific task. Results show that LLaVA clicks on the bottom-left corner on all screens, cogAgent may fail to generate click positions, and in the fourth task, only our agent can correctly click on the position. Fig. 8 (c) shows that our agent can recognize whether an action needs to be re-tried after reflection and successfully execute the action following a failure."}, {"title": "6 Conclusion", "content": "In this paper, we build a new environment for the screen control task. VLM agents can manipulate a real computer through direct mouse and keyboard control commands. To encourage the agent to continuously interact with the environment and accomplish complex multi-step tasks, we designed a control pipeline that includes planning, acting, and reflecting phases. Furthermore, we unveil a new dataset that covers a wide range of everyday digital works. We propose a fine-grained metric to assess the agent's computer-controlling capabilities with both action-level and task-level evaluation. We tested OpenAI GPT-4V and two state-of-the-art VLM models on the test set. The results indicate that GPT-4V has the potential to act as a computer-controlling agent, but it lacks precise positioning capabilities. Finally, we trained the ScreenAgent model, inherited from CogAgent, to achieve comparable scores with GPT-4V but more accuracy in UI element localization. It is hoped that our work will inspire further research in building more powerful and generalized agents. In terms of technical limitations, due to the input restrictions of VLM, our model can only process single-frame images, not videos or multi-frame images. The VLM's language capability is limited by the abilities of its foundation language model. We also found that even GPT-4V has limited support for non-English text on the screen."}, {"title": "Ethical Statement", "content": "The rational use of automated agents with autonomous decision-making capabilities brings significant societal benefits, including improving the accessibility of computers, reducing duplication of human effort on digital work, and aiding in computer education. However, the potential negative impacts of these agents, such as employment impact, fraud and abuse, privacy issues, and the risk of misoperation, cannot be overlooked. The method could potentially be used to bypass the CAPTCHA test for automatic account registration, spreading misinformation, or conducting illegal activities. We are focused on and committed to the responsible use of AI technology."}, {"title": "D CC-Score", "content": "Consider two action sequences, namely, the label action sequence L and the pred action sequence P. We denote the label action sequence as $L = {l_1, l_2, ..., l_n}$ and the prediction action sequence as $P = {p_1, p_2, ..., p_m}$. Define a score matrix S as an n \u00d7 m matrix, where $S_{ij}$ represents the similarity score between action $l_i$ and action $p_j$.\nA possible alignment is a sequence C = ${ (c_1, d_1), (c_2, d_2), ..., (c_k, d_k) }$, where each element $(c_i, d_i)$ is a pair of actions such that $c_i \\in L \\cup {\\text{None}}$ and $d_i \\in P \\cup {\\text{None}}$. Each pair $(c_i, d_i)$ either consists of corresponding actions from L and P, or a combination of an action from either L or P with a null value (None). Importantly, this sequence must adhere to the constraint that the order of non-null actions in L and P within the alignment is preserved as in their original sequences. This means that if $(c_i, d_i)$ and $(c_j, d_j)$ are two pairs in C where $c_i, c_j \\neq \\text{None}$ and $d_i, d_j \\neq \\text{None}$, and if i < j, then $c_i$ must precede $c_j$ in L and $d_i$ must precede $d_j$ in P. This constraint ensures that the alignment respects the sequential nature and integrity of the original action sequences.\nFor a given alignment C, its score is the sum of the similarity scores for all matched pairs of actions in the alignment, that is, $\\sum_{(i,j) \\in C} S_{ij}$. We choose the alignment with the highest score as the best matching alignment $C^*$.\nFinally, the CC-Score for the prediction and label action sequence is calculated as:\n$\\text{CC-Score}(L, P) = \\frac{1}{|L|} \\sum_{(i,j) \\in C^*} S_{ij}$"}]}