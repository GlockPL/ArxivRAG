{"title": "Nationality, Race, and Ethnicity Biases in and Consequences of Detecting AI-Generated Self-Presentations", "authors": ["Haoran Chu", "Linjuan Rita Men", "Sixiao Liu", "Shupei Yuan", "Yuan Sun"], "abstract": "This study builds on person perception and human artificial intelligence interaction (HAII) theories to investigate how content and source cues, specifically race, ethnicity, and nationality, affect judgments of Al-generated content in a high-stakes self-presentation context: college applications. Results of a pre-registered experiment with a nationally representative U.S. sample (N = 644) show that content heuristics, such as linguistic style, played a dominant role in AI detection. Source heuristics, such as nationality, also emerged as a significant factor, with international students more likely to be perceived as using AI, especially when their statements included AI-sounding features. Interestingly, Asian and Hispanic applicants were more likely to be judged as AI users when labeled as domestic students, suggesting interactions between racial stereotypes and AI detection. AI attribution led to lower perceptions of personal statement quality and authenticity, as well as negative evaluations of the applicant's competence, sociability, morality, and future success.", "sections": [{"title": "Introduction", "content": "\"I did it to make my paper results look better. Nobody at my school taught us morals or values.\" This statement, quoted during a keynote presentation at the recent annual conference on Neural Information Processing Systems (NeurIPS 2024), sparked widespread backlash when the speaker singled out a specific group of international students to illustrate the maluses of artificial intelligence (AI), such as plagiarism (NeurIPS Board and 2024 Organizing Committee, 2024). While these remarks perpetuated harmful stereotypes about individuals of certain nationalities, races, and ethnicities, they also raise profound and novel questions: who is perceived as more likely to (mis)use AI, and what are the consequences of such perceptions?\nIndeed, recent advancements in AI have led to unprecedented improvements in its integration into our social and cultural infrastructure. Large language models (LLMs) like OpenAI's GPT-4, Meta's Llama- 3.2, and Google's Gemini can now produce textual and graphic content that rivals, if not exceeds, the quality of an average human writer in terms of language and content (Jakesch et al., 2023; Vaswani et al., 2017). The rapid development of these models has also accelerated Al's proliferation in both professional and personal settings. Content creators are increasingly using Al models to draft and edit communication materials, while more programmers are leveraging AI to streamline their coding processes and convert human language into computer-readable code (Porter & Zingaro, 2024; Yue et al., 2024). However, the widespread adoption of Al-powered tools has raised concerns about their social, political, and cultural impacts (Dwivedi et al., 2023). For instance, due to AI's ability to generate coherent stories, malicious actors can exploit this technology to mass-produce misinformation or clickbait at extremely low costs (Xu et al., 2023). Plagiarism is another critical issue, as noted above. Users may question whether they can or should take credit for content generated by Al, while Al developers face legal concerns about whether training their models on human-created content constitutes copyright infringement (Dien, 2023).\nGiven the ethical and legal challenges surrounding the use of AI, such as misinformation, plagiarism, and the unauthorized use of copyrighted materials, an important question arises: how can we distinguish AI- generated content from human-created content (Jakesch et al., 2023)? With their vast training data and powerful algorithms, LLMs can produce content that is almost indistinguishable from human-generated work (Dalalah & Dalalah, 2023; Kobak et al., 2024). Much research and entrepreneurial effort has been directed toward \"outsmarting\" AI with AI, such as developing classifiers to differentiate between AI- and student-"}, {"title": "Source Heuristics for Detecting AI-Generated Self-Presentations", "content": "As discussed above, the first key question this research seeks to answer is: Who is perceived more likely to use AI? Recent PEW reports show that younger, more highly educated people in the U.S. are more likely to use AI in their daily work, and liberal- leaning Democrats are generally less skeptical of AI- generated information than their Republican counterparts (McClain, 2024). The classic diffusion of innovation theory may also provide a useful framework to characterize the early adopters of AI (Raman et al., 2023). For example, people who are more tech-savvy or open-minded may be more likely to integrate AI models, such as LLMs, into their work or daily routines, even delegating important tasks like impression management to AI agents. While there is no clear consensus on the profile of typical AI users, it is likely that many of their characteristics overlap with those of power users of other advanced technologies, such as programming tools and specialized software (Hancock et al., 2020).\nAlthough there is some theoretical and empirical evidence about who might use AI, little research has explored who the general public believes is more likely to use this new technology. While the stereotypical image of a power user, young, educated, and perhaps \"nerdy\", might describe an AI user, public perception of AI use likely goes beyond a person's technical proficiency. Indeed, AI's capabilities have surpassed many earlier technological advances, blurring the lines between human and machine intelligence. Unsurprisingly, some people may find this power both inspiring, as it holds promise for the future of humanity, and intimidating, given the uncertainties and complexities surrounding Al's mechanisms (Dwivedi et al., 2023). Notably, while many people have heard of AI through media coverage, the majority, even in developed countries like the U.S., have not had firsthand experience interacting with AI, at least knowingly (McClain, 2024). This combination of Al's power and the uncertainty surrounding its use may cast a shadow over the public's perception of it.\nAlthough it may be extreme to assume that the public is outright hostile toward Al or its users, it is plausible that people hold mixed feelings about delegating critical tasks to AI, especially when it happens without disclosure. While AI models may not be considered bearers of copyright in a legal or cultural sense, the public might view using AI as a ghostwriter with some skepticism, particularly related to authenticity (Yue et al., 2024). Indeed, college professors have expressed frustration over Al-like assignments, and Hollywood screenwriters have successfully protested against the use of AI in creative work (Willis, 2023). In self-presentation contexts, such as writing a cover letter for a job application, a personal statement for college admission, or even a dating profile, people are even more likely to expect genuine, authentic self-expression rather than ghostwriting by AI. This sentiment is evident in the backlash against Google's recent ad campaign promoting its LLM, Gemini, which featured a story created on behalf of a young girl aspiring to become an Olympic athlete (Elias, 2024). Therefore, beyond the perceived competence in technology use, the public may also question the ethicality of AI use, particularly in settings where true and authentic human expression is expected.\nThe complex interplay between perceived capabilities and intent when evaluating others' use of Al calls for a more structured theoretical approach to formulating hypotheses and research questions. We turn to decades of research on stereotype-based person perception for guidance. Susan Fiske's (2012) stereotype content model (SCM) provides a useful perspective for disentangling the influence of different person perceptions on people's judgments of others' AI use. The SCM posits that human evaluations of each other often boil down to two key dimensions: perceived warmth and perceived competence (Fiske et al., 2018). Perceived warmth relates to the intent of others, with malevolent intent leading to a colder image, while perceived competence is associated with the evaluation of others' capabilities. More recent research suggests that the warmth dimension can be further subdivided into sociability and morality, whereas likeable and moral others are perceived as warmer (Kervyn et al., 2015).\nPeople tend to form stereotypes of different demographic, cultural, and social groups based on their experiences, and these stereotypes profoundly shape subsequent judgments and evaluations. For instance, research shows that professions like scientists and entrepreneurs are typically perceived as high in competence but low in warmth, whereas blue-collar workers, like cashiers and janitors, are seen as high in warmth but low in competence (Fiske et al., 2018). Some stereotypes, such as those about racial and ethnic groups, may be less benign. For example, Asians and Jews are often stereotyped as high in competence but low in warmth, placing them in the \"envied\" group"}, {"title": "RQ1", "content": "How do race, ethnicity, and nationality influence people's judgments of AI involvement in self-presentation materials?"}, {"title": "Consequences of Detecting AI-generated Self-Presentations", "content": "As noted earlier, people's judgments of others' AI use may be intertwined with their perceptions of the characteristics of the individuals being evaluated. Consequently, these perceptions, often stereotypical and inaccurate, can significantly influence how people view other aspects of the person. From a competence perspective, while AI users are often more experienced with new technologies and tend to be more highly educated, the act of delegating tasks to AI could be seen as lazy or even akin to plagiarism (Dwivedi et al., 2023; Hoover, 2024), potentially diminishing others' perceptions of the user's competence. From a warmth perspective, AI ghostwriting may create a barrier between the content creator and the audience, leading to perceptions that the content lacks authenticity and good intent (i.e., low warmth). As a result, reduced perceptions of warmth could lead to less favorable evaluations of the writing quality, the individual, and ultimately, projections of their future performance.\nWhile suspecting Al's involvement in content generation may lead to negative evaluations of those claiming credit, HAII research suggests otherwise (Sundar, 2020). For example, studies have found that people often view AI-produced writing as highly credible due to its perceived objectivity compared to human-generated content (Jakesch et al., 2019). Similarly, linguistic analyses of Al-generated stories have shown that Al excels in creating linguistically competent and logically coherent material, sometimes surpassing human writers (Chu & Liu, 2024; Dalalah & Dalalah, 2023). Therefore, perceiving a piece of self- presentation material as AI-generated could lead to higher evaluations of its quality. However, as previously discussed, AI-generated content often lacks the human touch present in human-generated material, which may lead to doubts about its authenticity.\nSimilar debates may arise when addressing the issue from a social psychological perspective. According to the SCM (Fiske et al., 2018), perceived AI use might be associated with higher competence but lower warmth. Specifically, suspected AI use in self- presentation contexts, such as college application materials, could be seen as signaling immoral behavior (e.g., ghostwriting), damaging perceptions of the individual's morality. Similarly, attributing the writing of self-presentation materials to AI could make the person seem less competent, just as plagiarism is often viewed as a sign of incompetence (Strangfeld, 2019). However, such effects may be reversed as previous research show that the \"nerdy\" image often associated with tech users conveys high competence but low sociability (Gr\u00f8nh\u00f8j et al., 2024).\nFinally, perceived AI use may also affect how people evaluate an individual's future performance, especially in contexts like college applications, where self-presentation is critical. Specifically, using Al regularly might signal competence, leading people to"}, {"title": "RQ2", "content": "How does attribution to AI or human sources influence people's perceptions of the quality and authenticity of self-presentation materials, such as college application essays?"}, {"title": "RQ3", "content": "How does attribution to AI or human sources for self-presentation materials influence people's perceptions of the individual's competence, morality, and sociability?"}, {"title": "RQ4", "content": "How does attribution to AI or human sources for self-presentation materials influence people's beliefs about the individual's future performance?"}, {"title": "The Current Study", "content": "As outlined above, this study aims to address two under-explored questions in human-AI interaction research: (1) how source heuristics influence people's detection of AI- or human-generated content, and (2) how these judgments affect their perceptions of both the content and the individual. Like prior research on Al detection, we focus on self-presentation contexts, particularly college application materials, due to their relevance to person perception and the high stakes involved. We experimentally manipulated source cues, including the race, ethnicity, and nationality of the claimed author.\nAt the same time, content-related heuristics, such as linguistic style, have been shown to significantly affect people's judgments of Al-generated content (Jakesch et al., 2019). We incorporated these cues into our research design for two reasons. First, as people often use multiple cues when evaluating new information (Fiske et al., 2018; Kiousis, 2001), it is possible that content cues, such as the use of rare phrases and long words, might amplify the stereotypes people apply when assessing the source of self- presentation material. Second, content and source heuristics may interact to influence judgments, with linguistic features potentially reinforcing or mitigating the biases associated with source characteristics, such as race or nationality. Therefore, we propose our final research question:"}, {"title": "RQ5", "content": "Do content heuristics interact with source cues to influence people's attribution of self- presentation content to human or AI?"}, {"title": "Sample", "content": "Method\nUpon Institutional Review Board (IRB) approval (Approval Number: [Redacted for Review]), a pre- registered experiment was conducted in October 2024 (Registration: [Redacted for Review]). We utilized quota sampling to recruit a sample of participants with demographic characteristics representative of the U.S. public from Prolific, a crowdsourcing platform that connects researchers with diverse groups of participants. A total of 948 participants completed the survey and passed two attention-check questions. Of these, 644 participants also passed the two manipulation check questions, which asked them to recall the race, ethnicity, and nationality of the applicant featured in the application materials. It's important to note that these manipulation checks were administered after participants had answered all questions related to their assessment of the application material, the applicant, and their future outlook, to avoid sensitization. This delay may have inadvertently impaired their memory of the source heuristics, which were subtly manipulated, leading to the relatively high dropout rate. However, since biases and stereotypes related to race, ethnicity, and nationality often influence judgments implicitly, it is likely that the source heuristic manipulations still influenced participants' evaluations, despite imperfect recall. Nevertheless, the results reported below are based on the final sample of 644 participants, but the findings remain consistent with the larger sample.\nThe average age of participants in the final sample was 46.62 years (SD = 15.68). There were more women (340, 52.8%) compared to men (297, 46.1%) and participants identifying as another gender (7, 1.1%). In terms of race and ethnicity, 408 participants identified as non-Hispanic White (63.4%), followed by non- Hispanic Black or African American (80, 12.4%), Hispanic or Latino (61, 9.5%), Asian, Pacific Islander, or Native American (55, 8.5%), and other races/ethnicities (40, 6.2%). The median education level was a 4-year college degree, and the median"}, {"title": "Stimuli", "content": "The experimental stimuli consisted of college application materials. Participants were instructed to imagine that they are serving as a college admission officer and review application materials, which included basic information about the applicant (e.g., name, intended major, high school) and an excerpt from the applicant's personal statement. The applicant's last names were manipulated as a source heuristic indicating their race and ethnicity. Specifically, we randomly selected three names from the most common last names among non- Hispanic White, non-Hispanic Black or African American, Hispanic or Latino, and Asian communities in the U.S., based on census data (Word et al., 2008). In addition to name cues, we explicitly included the applicant's race and ethnicity in their background information. To avoid gender stereotypes, only the applicant's first initial was presented. A control condition was also included in which the applicant's race and ethnicity were not disclosed, and only the initial of their last name was shown. The applicant's nationality was manipulated both explicitly and implicitly, with international applicants labeled as \u201cInternational Student (Visa Required)\u201d and domestic applicants as \"Domestic Student (No Visa Required).\" To enhance the authenticity of the stimuli, applicants' high school names varied according to nationality. Domestic students were assigned \u201cSmithville High School,\" while international students were assigned \"United International School.\u201d In addition, the applicants' intended major was randomized to avoid category-based confounds. Majors included STEM (astrophysics), business, and humanities, with extracurricular activities adjusted to match the major. While the major significantly affected AI detection (F(2,641) = 8.06, p < 0.001), it did not interact with the experimental factors to influence the primary outcome variables. Thus, we did not include it in the main analyses reported below. All other information remained consistent across the conditions.\nContent heuristics were also experimentally manipulated. Specifically, we created two versions of each personal statement, incorporating common but flawed heuristics that people use to judge whether content is human- or Al-generated (Jakesch et al., 2023). In the \u201chuman-sounding\" version, the personal statement included contractions (e.g., \"I'm\" instead of"}, {"title": "Procedure and Materials", "content": "After providing informed consent, participants were randomly assigned to evaluate one of 20 versions of the application materials (5 [race/ethnicity cue] x 2 [nationality cue] x 2 [content cue]). After reviewing the material, they were asked to indicate whether they believed the personal statement was written by the applicant or by AI, using a five-point scale (1 = \u201cDefinitely human-written\u201d to 5 = \u201cDefinitely AI- generated\u201d) (Jakesch et al., 2023). We measured participants' perceived quality and authenticity of the personal statement using two sets of four semantic differential scales (e.g., quality: 1 = \u201cPoorly written\" to 5 = \"Well-written\u201d; authenticity: 1 = \u201cInauthentic\u201d to 5 = \u201cAuthentic\"). The scales were reliable (alpha = 0.86 for quality perception, and alpha = 0.94 for authenticity perception).\nParticipants' perceptions of the applicant were assessed using three scales from Fiske et al. (2018), measuring perceived competence (alpha = 0.93), sociability (alpha = 0.93), and morality (alpha = 0.94). Sample items included: \"How competent (competence)/likable (sociability)/trustworthy (morality) do you think [applicant name] is?\" Responses were recorded on a five-point scale (1 = \"Not at all\" to 5 = \u201cA great deal\"). We also created three additional sets of questions to assess participants' perceptions of the applicant's future performance, focusing on college admission potential (alpha = 0.89; e.g., \"[Applicant name] will be admitted to a competitive university\u201d), college performance (alpha = 0.90; e.g., \"[Applicant name] will maintain a strong GPA in college\u201d), and career performance (alpha = 0.91; e.g., \u201c[Applicant name] will succeed in their future career\"). Responses to these items were also measured on a five-point scale (1 = \"Strongly disagree\" to 5 = \"Strongly agree\")."}, {"title": "Results", "content": "To address RQ1, which asks whether source cues such as the applicant's race, ethnicity, and nationality influence judgments of Al's involvement in writing their personal statement, we first conducted a two-way analysis of variance (ANOVA). The results indicated no significant effects for race/ethnicity (F(4, 634) = 0.58, p = 0.68, partial $\\eta^2$ = 0.004), nationality (F(1,634) = 0.60, p = 0.44, partial $\\eta^2$ = 0.001), or their interaction (F(4, 634) = 0.93, p = 0.44, partial $\\eta^2$ = 0.006) on AI detection. However, these findings do not rule out the possibility that source heuristics may interact with content heuristics to influence participants' source judgments (RQ5). To explore this, we conducted an additional ANOVA, including content cues as a predictor. The results showed that content cues significantly predicted AI detection (F(1, 624) = 129.61, p<0.001, partial $\\eta^2$ = 0.172), although the main effects of race/ethnicity (F(4, 624) = 0.20, p = 0.939, partial $\\eta^2$ = 0.001) and nationality (F(1, 624) = 0.94, p = 0.332, partial $\\eta^2$ = 0.002) remained non-significant. Notably, the three-way interaction between race/ethnicity, nationality, and content heuristics was a significant predictor of AI detection (F(4, 624) = 2.69, p < 0.05,\npartial $\\eta^2$ = 0.017). Bonferroni-adjusted pairwise comparisons revealed that participants were more likely to believe that AI was involved in writing the personal statement for an international student than for a domestic student when the student's race was not disclosed and the personal statement included many AI-sounding heuristics, such as long words and rare phrases ($\\Delta M$ = 0.59, p < 0.05). Interestingly, the direction of this effect reversed when the personal statement contained more human-sounding cues ($\\Delta M$= -0.44, p = 0.09), though this effect only approached statistical significance after adjusting for multiple comparisons.\nTo further investigate the significant three-way interaction, we conducted a regression analysis using the PROCESS macro in SPSS (Model 3) to explicate the interactive effects of source and content cues on source judgment (Hayes, 2017). The results, presented in Table 1, show that the dummy-coded race/ethnicity variables contrasting the control condition with Hispanic/Latino or Asian applicants interacted with both international status and content heuristics to influence source judgment. A closer inspection of the conditional effects revealed that the applicant's"}, {"title": "", "content": "international status significantly affected source judgment only when race or ethnicity information was not disclosed and the personal statement included many AI-sounding heuristics (b = 0.59, p < 0.05), consistent with the ANOVA findings. Additionally, human- sounding personal statements from a domestic student were less likely to be judged as AI-generated when attributed to Hispanic/Latino applicant than when the race is not disclosed (b = -0.51, p < 0.05). Further examination of the conditional effects of content heuristics on source judgment (Table 2) revealed that participants were more likely to attribute AI-sounding personal statements to AI when the applicant was a non-Hispanic White, Black/African American, or racially ambiguous international student than when the students of these racial and ethnic groups were labeled as domestic students. Conversely, content heuristics had a stronger effect on source judgment when Asian or Hispanic applicants were labeled as domestic rather than international students.\nRQ2 asks whether source judgment influences participants' evaluation of the personal statement's quality and authenticity. We conducted two additional ANOVA models using source and content cues as predictors. The results indicated no significant effects of source cues. However, content heuristics significantly predicted both perceived quality (F(1, 623) = 6.55, p < 0.05, partial $\\eta^2$ = 0.010) and authenticity (F(1, 624) = 103.5, p < 0.001, partial $\\eta^2$ = 0.14). Human-sounding personal statements were perceived as having higher quality (M = 3.91, SD = 0.75) and authenticity (M = 4.03, SD = 0.92) than AI-sounding versions (quality: M = 3.74, SD = 0.96; authenticity: M\n=\n3.18, SD\n=\n1.17). Notably, the difference in authenticity perception was greater than the difference in quality perception. To further assess whether source judgment affected these perceptions, we conducted mediation analyses using the PROCESS macro (Model 12). As shown in Table 1, attributing a personal statement to Al led to lower perceptions of both quality and authenticity.\nTo address RQ3 and RQ4, which ask whether source judgment affects perceptions of the applicant and their future performance, we ran additional ANOVA and regression models (Table 1). Content cues significantly predicted all person perception and future performance variables. Specifically, human- sounding personal statements led to more favorable evaluations of the applicant's competence (F(1, 624) =\n20.81, p < 0.001, partial $\\eta^2$ = 0.032), sociability (F(1,\n624) = 80.51, p < 0.001, partial $\\eta^2$ = 0.114), and"}, {"title": "Conclusion", "content": "In conclusion, this study highlights the complexities of detecting AI-generated content in self- presentation contexts and the significant impact of both content and source cues on these judgments. The findings suggest that while content heuristics are critical in Al detection, source cues like race, ethnicity, and nationality can amplify concerns about competence and integrity. Moreover, detecting AI use leads to unfavorable perceptions of both the message and the individual, with potential implications for their perceived future performance. As AI continues to integrate into more aspects of human life, understanding how it is perceived, especially in high- stakes settings, will remain an important area of inquiry."}]}