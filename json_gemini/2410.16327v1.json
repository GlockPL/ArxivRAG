{"title": "FEINT AND ATTACK: ATTENTION-BASED STRATEGIES\nFOR JAILBREAKING AND PROTECTING LLMS", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Zejian Chen", "Litian Zhang", "Zheng Liu", "Lirong Qiu", "Xi Zhang"], "abstract": "Jailbreak attack can be used to access the vulnerabilities of Large Language Mod-\nels (LLMs) by inducing LLMs to generate the harmful content. And the most\ncommon method of the attack is to construct semantically ambiguous prompts\nto confuse and mislead the LLMs. To access the security and reveal the intrin-\nsic relation between the input prompt and the output for LLMs, the distribution\nof attention weight is introduced to analyze the underlying reasons. By using\nstatistical analysis methods, some novel metrics are defined to better describe\nthe distribution of attention weight, such as the Attention Intensity on Sensitive\nWords (Attn_SensWords), the Attention-based Contextual Dependency Score\n(Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy). By lever-\naging the distinct characteristics of these metrics, the beam search algorithm and\ninspired by the military strategy \"Feint and Attack\", an effective jailbreak attack\nstrategy named as Attention-Based Attack (ABA) is proposed. In the ABA, nested\nattack prompts are employed to divert the attention distribution of the LLMs. In\nthis manner, more harmless parts of the input can be used to attract the attention\nof the LLMs. In addition, motivated by ABA, an effective defense strategy called\nas Attention-Based Defense (ABD) is also put forward. Compared with ABA,\nthe ABD can be used to enhance the robustness of LLMs by calibrating the atten-\ntion distribution of the input prompt. Some comparative experiments have been\ngiven to demonstrate the effectiveness of ABA and ABD. Therefore, both ABA\nand ABD can be used to access the security of the LLMs. The comparative ex-\nperiment results also give a logical explanation that the distribution of attention\nweight can bring great influence on the output for LLMs", "sections": [{"title": "INTRODUCTION", "content": "LLMs have garnered considerable attention owing to their exceptional performance across diverse\ntasks (Touvron et al., 2023a). However, as the deployment of LLMs becomes more widespread,\nsecurity concerns have been escalated, particularly in safety-critical and decision-making environ-\nments. A pivotal concern resides in the susceptibility of LLMs under jailbreak attacks, wherein\nadversarial prompts are meticulously crafted to compel the model to produce content that violates\nusage policies (Perez et al., 2022; Shen et al., 2023). The nucleus of jailbreak attack lies in crafting\neffective prompts that obscure malicious intent and deceive LLMs into treating harmful queries as\nbenign (Zou et al., 2023). Various jailbreak techniques have been proposed, ranging from manually\ndesigned attack prompts (Shen et al., 2023) to automatically generated prompts (Zou et al., 2023;\nLiu et al., 2024; Chao et al., 2023), demonstrating increasing attack efficacy in both online and\noffline LLMs.\nMost current research on jailbreak methodologies primarily focuses on the development of\nsophisticated attack prompts, including role-playing (Jin et al., 2024), code injection (Ding\nOur implementation will be released upon the acceptance of this paper."}, {"title": "PRELIMINARY ANALYSIS", "content": "In this section, we firstly introduce the details of three proposed metrics to depict the attention\ndistributions within LLMs. After that, experimental results that reveal the correlations between the\nproposed metrics and the success of jailbreaks are presented and discussed."}, {"title": "THE PROPOSED METRICS FOR ATTENTION DISTRIBUTIONS", "content": ""}, {"title": "Attention Intensity on Sensitive Words.", "content": "The metric Attn_SensWords is proposed to quantify the\ncorrelations between the attention weights of sensitive words within the prompt and the success\nrate of jailbreak attacks. Sensitive words are defined as the verbs and nouns (e.g., \"make\" and\n\"bomb\"), which are more probability to be potentially harmful (Ding et al., 2023). The input to the\nmodel consists of two types: the original query, denoted as q, and the iteratively modified prompt,\ndenoted as p. For consistency, p is used throughout the following computations to represent each\ninput. Attn_SensWords first calculates the normalized attention weights for sensitive words across\nall layers and heads at each time step, which will be further averaged over all time steps and all\ninstances. Formally, let the target LLM be a model with L layers and H attention heads, the model"}, {"title": null, "content": "processes an input p = {W1,W2,...,wm}, where M is the number of words in the prompt. Let\nat,l,h,i be the normalized attention weight which is assigned to the i-th word wi by head h in\nlayer l at time step t. Let S be the set of indices corresponding to sensitive verbs and nouns in\nthe input prompt, P be the number of all prompts. For each word wi belonging to the set S, the\nAttn_SensWords can be computed as equation (1):\nAttn_SensWords = \\frac{1}{PTLH} \\sum_{p=1}^{P} \\sum_{t=1}^{T} \\sum_{l=1}^{L} \\sum_{h=1}^{H} \\sum_{i \\in S} a_{t,l,h,i}^{(p)} \\qquad(1)"}, {"title": "Attention-based Contextual Dependency Score.", "content": "In practice, the allocation of attention weight\nacross input tokens plays a crucial role in generating new tokens (Vaswani et al., 2017). To measure\nthe relevance between the output and the input for LLMs, the Attn DepScore is proposed on basis\nof the distribution of attention weight. Calculation of Attn_DepScore can be mainly divided into\ntwo steps. Firstly, the attention weights for input and generated tokens will be summed at each\ntime step. Secondly, the ratio of input attention on total attention (input + output) will be averaged\nacross all layers, heads, and time steps. Hence, the dependency of the input context for LLMs can\nbe quantified when the text is generated. Formally, let \u1e9et,1,h,i denote the assigned attention weight\nfor the i-th token in the input sequence with head h in layer l at time step t, then the Attn_DepScore\ncan be computed as the following equation (2):\nAttn_DepScore = \\frac{1}{T \\times L \\times H} \\sum_{t=1}^{T} \\sum_{l=1}^{L} \\sum_{h=1}^{H} \\frac{\\sum_{i=1}^{N} \\beta_{t,l,h,i}}{\\sum_{j=1}^{N+t} \\beta_{t,l,h,j}} \\qquad (2)"}, {"title": "Attention Dispersion Entropy.", "content": "As we know, the entropy of the random variable is often used to rep-\nresent the output distribution in the semantic event-space (Kuhn et al., 2023; Farquhar et al., 2024).\nFor entropy, the calculation result is dominated by low-probability tokens (whose logs are large\nand negative). This indicates that the model assigns less confidence to these tokens. Hence, higher\nentropy means greater uncertainty for prediction of models. Based on this reason, Attn Entropy\nis introduced to quantify the distribution of the attention weight across input tokens for LLMs. To\nobtain Attn Entropy, the normalized attention weight which is assigned to each token can be re-\ngarded as the probability for entropy calculation. The entropy will be computed for each layer and\nhead, and the final Attn Entropy is determined by averaging these entropy values across time steps,\nlayers, and heads. Let Ot,1,h,i denote the normalized attention weight on each token for head h in\nlayer l at time step t. This weight metric also represents the probability which is assigned to the\ni-th token in the input sequence. In this way, the Attn_Entropy can be computed as the following\nequation (3):\nAttn_Entropy = -\\frac{1}{TXLXH}\\sum_{t=1}^{T} \\sum_{l=1}^{L} \\sum_{h=1}^{H} \\sum_{i=1}^{N} O_{t,l,h,i} \\log O_{t,l,h,i} \\qquad (3)"}, {"title": "EXPERIMENTAL SETUP", "content": "Dataset To investigate the general patterns of attention distribution on LLMs, two datasets are se-\nlected: a popular jailbreak dataset Adv-Bench (Zou et al., 2023) and a common harmless question-\nanswering dataset Dolly (Conover et al., 2023). Under the two datasets, comparative experiments\nwill be finished on LLMs for harmless prompts, harmful prompts and jailbreak attack prompts, etc.\nJailbreak Attack Methods To analyze the impact of semantic-guided jailbreak attack prompts on\nthe attention distribution of LLMs, some typical jailbreak attack methods, such as PAIR (Chao\net al., 2023), TAP (Mehrotra et al., 2023), DeepInception (Li et al., 2023) and ReNeLLM (Ding\net al., 2023), are used to induce LLMs to generate harmful responses by manipulating the context."}, {"title": "EXPERIMENT RESULTS", "content": "Table 1 shows the relationship between the ASR and each metric(Attn_SensWords, Attn_DepScore\nand Attn_Entropy) on Adv-Bench dataset, respectively. In Table 1, Attn_SensWords, Attn_DepScore\nand Attn_Entropy are abbreviated as ASW, ADS, and AE, respectively. The results show that the\nhighest average ASR can be obtained by the ReNeLLM. Concurrently, values of Attn DepScore\nand Attn Entropy are the highest in ReNeLLM, but the Attn_SensWords is the lowest. The similar\nresults can be also obtained by other jailbreak attack methods. This indicates that the jailbreak attack\nstrategies with higher Attn DepScore and Attn_Entropy can usually get higher ASR."}, {"title": null, "content": "The influence of attention distribution on Llama2-7B-chat can be seen in Figure 2. In each sub-figure, the x-axis represents the index of the sentences from the input of the model. The y-axis denotes the related values of different metrics. Figure 2 (a) and Figure 2 (b) show that there is small variance of average Attn_DepScore under normal harmless prompts and jailbreak attack prompts. However, the differences of Attn_Entropy are quite evident. In general, the Attn_Entropy of jailbreak attack prompts are higher than that of harmless prompts. This may be attributed to the fact that harmless prompts are usually designed to obtain clear information, while contents of jailbreak attack prompts are often designed to be scattered to confuse or mislead LLMs. As a result, when facing with jailbreak attack prompts, the attention distribution in LLMs tends to become more dispersed, leading to increased uncertainty in the model's responses.\nAdditionally, Figure 2 (c) and Figure 2 (d) show that the difference between the origin harmful prompt and the jailbreak attack prompt are also more obvious. For instance, the Attn_DepScore of"}, {"title": null, "content": "the origin harmful prompt is around 0.5, while the Attn_DepScore of the jailbreak attack prompt may\nbe more than 0.8. This indicates that when the LLMs are confronted with a clearly harmful origin\nprompt, the output of LLMs will be lower relevance with the input. This will always lead the LLMs\nto give a direct rejection. In contrast, if the LLMs face a jailbreak attack prompt, the Attn_DepScore\nwill be greatly increased. This is to say, the response is higher correlation with the input in LLMs.\nSimilarly, both two types of input can result in significant differences on Attn_Entropy. Figure 2 (d)\nalso shows that the average Attn_Entropy is less than 0.25 under origin harmful prompts. Whereas\nthe Attn Entropy for jailbreak attack prompts is more than 0.33. This suggests that the jailbreak\nattack prompt can be used to disperse the attention of LLMs to produce a harmful response."}, {"title": "METHODOLOGY", "content": "The overview schemes of the proposed ABA and ABD are outlined in Figure 3. The details of ABA\nare located in the left of Figure 3, and the ABD is on the right."}, {"title": "ATTENTION-BASED ATTACK", "content": "Given the origin malicious query q and based on the feedback of the target LLM, ABA is firstly\nintroduced to analyze the attention weight of each word. In ABA, the attention weight and corre-\nsponding sensitive words, which are described as the verbs and nouns in q, will be input into the\nattention distraction generator. According to the preliminary experiment results, Attn_SensWords\nand Attn_Entropy play a significant role on the effectiveness of the jailbreak attack. Since\nAttn_SensWords is basically determined by the attention weight of each sensitive word, the core\nobjective of ABA is to continuously reduce the attention weight on these sensitive words while si-\nmultaneously maximizing the model's Attn_Entropy. To achieve this, the distraction generator will\nrefine and disguise the original query q by generating multiple semantic-guided scenarios. The re-\nfined and disguised query will be selected to input into the target LLM to generate harmful answers\nunder a multi-round framework."}, {"title": "ATTENTION WEIGHT ANALYSIS", "content": "To quantify the importance of each sensitive word, the attention weight on each sensitive word will\nbe calculated. The value of the attention weight can reflect the significance of each word in the set\nof input prompt. Therefore, the most contributive word in the input prompt can be identified.\nHere, let q = {W1,W2,...,WM} be the set of words from the input malicious query. If the number\nof sensitive words is supposed to be r, and these words are denoted as S = {81, 82, ..., Sr}, then the\nattention weight of these words can be described as a set Ss\u2081 = {(s1 : aws\u2081), (82 : aws\u2082), ..., (Sr :\naws)}. In each interaction between attention distraction generator and target LLM, the attention\nweight analysis will be completed."}, {"title": "PROMPT REFINEMENT", "content": "The refinement of prompt can be realized by attention distraction generator. Based on responses\nfrom target LLM, the objective of attention distraction generator is to minimize the attention weight\nof sensitive words and maximize the Attn_Entropy. In addition, these responses should be closely\nrelated to the intention of malicious inputs in q.\nInspired by the strategic principle of \"Feint and Attack\", which is commonly employed in military\ntactics, ABA emphasizes on the specified harmless part of the input by adding the nesting of multiple\ntasks. Specifically, the number of tasks of LLMs can be increased by embedding multiple nested\nobjectives from the inputs. In this manner, the attention of the LLMs can be dispersed across various\ntasks, and their focus on potentially sensitive content can also be diluted. For example, if the target\nLLM is required to accomplish a harmless task, then small probability may be given to reject this\naction. Since the task is nested, and the generated tokens always depend on the input, then the\nharmful responses will also be generated simultaneously.\nTo further disperse the attention weight of sensitive words, additional measures should be imple-\nmented in generating jailbreak attack prompt p. Specifically, the weight of these sensitive words is"}, {"title": "MULTI-ROUND PARADIGM", "content": "Considering the stochastic environment and inherent instability of the generation process, multi-\nround paradigm will be used to prove the validity of the proposed methods (Chao et al., 2023). In\nthe multi-round paradigm, if the LLMs are encountered a failed jailbreak attack, then the jailbreak\nattacker will persistently attack target LLMs. A straightforward strategy is to regenerate prompt as\nthe new jailbreak attack sample. In regeneration step, the generated tasks will maintain diversity on\nbasis of the origin objective, which is to distract the attention of the target LLMs. In this manner, the\ntried or failed scenarios will not be reused. If the number of attempts in the inner loop exceeds the\npredefined threshold, ABA will switch to a new scenario to launch a new jailbreak attack sample in\nthe outer loop. Based on this iterative regeneration strategy, ABA can be used to generate new sce-\nnarios and jailbreak attack samples. Therefore, an efficient multi-round jailbreak attack mechanism\nhas been established. The details of training algorithm for ABA can be found in Appendix E."}, {"title": "ATTENTION-BASED DEFENSE", "content": "Comparative experiments show that different attention distributions of the input prompt can lead to\ndifferent outputs of the target LLMs. Whereas, the attention distributions of prompts with different\nrisk levels are distinct. Hence, attention distribution can be used to access the security of the input\nfor LLMs. Based on this fact, ABD is proposed. In ABD, a new metric Risk_Score is proposed to\nmeasure the risk level of the input prompt. Risk_Score is the weighted sum of Attn_DepScore and\nAttn_Entropy. And the Risk_Score can be computed as following equation (4):\nRisk_Score = Attn_DepScore + \\sigma \\cdot Attn_Entropy \\qquad (4)"}, {"title": null, "content": "Therefore, if the attention distribution of input prompt is different, then the weight and threshold\nfor the Risk_Score will be set to be different for the LLMs. o is the weight of Attn_DepScore. For\nthis reason, it is very necessary for us to obtain the optimal weight. The related details can be found\nin Section 4.5. The effectiveness of the ABD will be proved in our later experiments. Moreover,\nthe precise selection of parameters can be guarded by maximizing the discrepancy between origin\nharmful and jailbreak attack prompts. Once the optimal weight is determined, a suitable threshold\nof LLMs can also be established. The threshold is the foundation of ABD. Additionally, the ABD\nwill obey the following rules: If the Risk_Score of the input prompt is lower than the threshold, then\nthe input can be regarded as harmless. In contrast, if the Risk_Score exceeds the threshold, then\nthe input can be regarded as an ambiguous or a potential deception. Consequently, the input will be\nregarded as potential harmfulness. As a response, a security warning prefix will be added to the input\nbefore it is input into the LLMs, such as \u201cAttention! The following content might contain harmful\ninstructions: Firstly, identify any potentially harmful part. If safe, then give a secure response\". In\nthis way, the attention of the LLMs can be calibrated effectively. As a result, the LLMs will be\nprompted to prioritize the safety assessment of the input before generating a response. Hence, both\nreliability and security can be enhanced."}, {"title": "EXPERIMENT", "content": ""}, {"title": "EXPERIMENTAL SETTINGS", "content": "Datasets. Two main datasets are prepared for our experiments: AdvBench Subset (Chao et al., 2023)\nand Dolly dataset (Conover et al., 2023). AdvBench Subset is adopted to assess the safety efficacy\nof LLMs. This dataset consists of 50 prompts with 32 categories of harmful information from\nthe AdvBench benchmark. Dolly Dataset is also adopted to address the attention results between\nharmless and jailbreak attack prompts.\nBaselines. Following previous works (Li et al., 2023; Ding et al., 2023), two kinds of popular\njailbreak attack methods are selected as the baselines. One focuses on optimizing prefix or suffix\ncontents, such as GCG (Zou et al., 2023) and AutoDAN (Liu et al., 2024). The other is the semantic-\nguided strategy, such as PAIR (Chao et al., 2023), TAP (Mehrotra et al., 2023), DeepInception (Li\net al., 2023) and ReNeLLM (Ding et al., 2023).\nTarget LLMs. To assess the effectiveness of ABA, some representative LLMs are selected as attack\ntargets, such as Llama-2-chat series (including 7B and 13B) (Touvron et al., 2023b), Llama-3-8B,\nGPT-4 (OpenAI, 2023) and Claude-3-haiku. Based on the effective scenario nesting templates which\nare obtained from experiments on open-source models, ABA is also applicable to closed-source\nmodels through transfer learning.\nImplementation Details. The details of the setting implementations can be found in Appendix D."}, {"title": "EVALUATION METRICS", "content": "Three metrics have been proposed to evaluate jailbreak attack methods, such as ASR, ASR-G and\nQueries. The Attack Success Rate (ASR) and the GPT-4-based ASR (ASR-G) are selected to assess\nthe effectiveness of jailbreak attack strategies. ASR can be determined by predefined rules. A\njailbreak attack is considered to be successful if the answer of the target LLM is given without a\nrefusal prefix such as \"I cannot\". Based on GPT-4, ASR-G can be used to determine the success of a\njailbreak attack. To evaluate the efficiency of the jailbreak attack, the metric \u201cQueries\u201d is introduced\nto measure the average number of successful jailbreak attacks between the attack model and the\ntarget model. The details of the metrics for calculation can be found in Appendix D."}, {"title": "MAIN RESULTS", "content": "Performance of Attack Success Rate. The ASR and ASR-G of various jailbreak attack methods\nare given in Table 2. Table 2 shows that the best ASR and ASR-G can be used to demonstrate the"}, {"title": null, "content": "performance of ABA on most of target LLMs. For ABA, the average ASR-G is more than 96% on\nall LLMs. Compared with existing jailbreak attack methods, the maximum ASR-G is no more than\n77%. All these results show that the nested scenarios can guide and minimize the attention weight\nfor sensitive words in LLMs. As a result, the number of rejections can be reduced. In summary,\nABA can be used to demonstrate the superior performance of ASR and ASR-G on LLMs under\njailbreak attack."}, {"title": "Attack Efficiency Analysis.", "content": "Table 2 also\npresents the comparative results of Queries be-\ntween ABA and baselines. The results show that\nthe average number of query in ABA is lower than\nthe baseline methods. The results also show that\nthe refining prompt can be used to reduce the at-\ntention weight of sensitive words."}, {"title": "Performance on Attention Distraction.", "content": "The\nAttn_SensWords of sensitive words can be found\nin Figure 4. Figure 4 shows that the lowest\nAttn_SensWords can be achieved by the ABA.\nThe result is consistent with the Table 2. In the Table 2, the best ASR can also be obtained by\nthe ABA. This is to say, ABA is very effective in reducing the attention weight of sensitive words in\nthe jailbreak attack prompts. Hence, the attention weight of the outer harmless tasks is needed to be\nincreased, while the harmfulness of the innermost tasks can be gradually neglected. Furthermore,\nthe average Attn_DepScore and Attn_Entropy of ABA are also the largest, which indicates that the\nABA can be used to disperse the attention of LLMs."}, {"title": "Performance on the Defense Strategy.", "content": "Based on ASR, Table 3 shows the defensive performance\nof ABD under various jailbreak attack methods. According to Table 3, the ABD can be used to\nreduce the ASR of each jailbreak attack across all target LLMs(open-sourced). Moreover, the results\nalso indicate that the threshold of Risk_Score is suitable for various open-source LLMs. Figure 5\nshows that the ABD has the ability in effectively decreasing Attn_DepScore and Attn_Entropy. As a\nresult, the focus on sensitive words will be enhanced. Comparative results show that the Risk_Score\ncan be used to evaluate the risk of input and indentify the harmfulness of the content. Moreover, a\nsafety risk alert prefix can be added."}, {"title": "ABLATION STUDY", "content": ""}, {"title": "Prompt Refinement.", "content": "The validity of the prompt refinement can be found in the Table 4. Table 4\nshows that ASR-G will be greatly reduced if the prompt refinement is omitted. Hence, the prompt\nrefinement is very useful in deceiving the target LLMs via attention distraction generator."}, {"title": "Multi-round Paradigm.", "content": "Table 4 gives the\nimpact of the multi-round paradigm in ABA.\nCompared with the prompt refinement, the\nmulti-round strategy is proved to be relatively\nless critical. This is to say, the prompt refine-\nment is indispensable for the whole effective-\nness of the attack strategy. This reinforces the\nconclusion that the prompt refinement is indis-\npensable for the overall effectiveness of the attack strategy, while the multi-round paradigm serves\nas an auxiliary tool to improve success rates in more complex scenarios."}, {"title": "HYPER-PARAMETER ANALYSIS", "content": "In ABD, grid search method is used to obtain\nthe optimal weight for LLM. Figure 6 illustrates\nthe variation of ASR (%) with changing the\nweight \u03c3. \u03c3 is the weight of Attn_Entropy. The\nred line is the origin ASR of ABA on Llama2-\n7B-chat. The blue line is the ASR under ABD.\nThe value of o is increased from 0 to 10. As\nshown in Figure 6, ASR always remains to be\naround 4% with the oranging from 0 to 10.\nThe blue line shows that the ASR of ABA un-\nder ABD is not too sensitive to the value of \u03c3.\nComparative results show that the effectiveness\nof ABD is very superior."}, {"title": "CONCLUSION", "content": "In this paper, the distribution of attention weight is introduced to address the underlying reason for\nthe security of LLMs. To better comprehend our work, some new metrics are proposed, such as\nAttn_SensWords, Attn_DepScore and Attn_Entropy. Based on attention distribution and these met-\nrics, ABA and ABD are proposed, respectively. Jailbreak attack experiments show that distinguished\nattack performance on LLMs can be obtained by ABA. Comparative defending experiments have\nalso proved that the robustness of ABD and it is also very effective for LLMs. Quantitative and qual-\nitative findings show that attention distribution can be used to reveal the intrinsic relation between\nthe input and the output of LLMs. In our future work, we will continuously focus on revealing the\npotential factors for the security of LLMs."}, {"title": "APPENDIX", "content": ""}, {"title": "RELATED WORK", "content": ""}, {"title": "SELF-ATTENTION AND ATTENTION BIAS", "content": "The attention mechanism is initially introduced in RNN-based on basis of encoder-decoder archi-\ntectures (Bahdanau et al., 2015; Luong et al., 2015). Based on the self-attention mechanism, trans-\nformers (Vaswani et al., 2017) can be used to achieve state-of-the-art performance in various do-\nmains (Devlin et al., 2019; Dosovitskiy et al., 2021). Nowadays, self-attention has also been widely\nutilized as a proxy in understanding and explaining behaviors of models (Clark et al., 2019; Hao\net al., 2021; Vashishth et al., 2019). In recent years, some modifications of attention mechanisms\nhave been made to improve the interpretability and performance in reasoning tasks (Zhang et al.,\n2024; Yu et al., 2024). The obtained achievements indicate that the emotional tokens in the middle"}, {"title": "STUDIES ON THE JAILBREAK ATTACK", "content": "In general, jailbreak attack prompts can be categorized into two primary types: direct jailbreak at-\ntack prompts (Shen et al., 2023; Zou et al., 2023; Liu et al., 2024) and indirect jailbreak attack\nprompts (Chang et al., 2024). Direct jailbreak attacks are characterized by their use of disguise to\nconceal harmful intentions, often achieved through the optimization of the prompt. This optimiza-\ntion involves crafting the prompt in such a way that it bypasses security filters while maintaining\nits malicious intent. In contrast, indirect jailbreak attacks typically exploit additional knowledge\nor context to realize their objectives. Despite their different methodologies, both types of jailbreak\nattacks neglect the concept of intention shift during the generation of the jailbreak prompt. This\noversight can be critical as it pertains to the evolution of the prompt's purpose over time."}, {"title": "FURTHER DETAILS ON METRICS", "content": ""}, {"title": "THE CALCULATION OF ATTN_SENSWORDS", "content": "Consider an input p = {W1,W2, ..., w\u043c}, M is the number of words in the prompt. Let Yt,l,h,i be\nthe attention weight which is assigned to the i-th word w\u2081 by head h in layer l at time step t. For\neach word wi, the normalized attention weight at,1,h,i is defined as follows:\nXt,l,h,i = \\frac{Vt,l,h,i}{\\sum_{j=1}^{M} Vt,l,h,j} \\qquad (5)\nLet S be the set of indices corresponding to sensitive verbs and nouns in the prompt. And SWW,h\ncan be computed by summing the normalized attention weights of all sensitive words for head h in\nlayer l at time step t:\nswwh = \\sum_{iES} at,1,h,i \\qquad (6)\nLet the target LLM be a model with L layers and H attention heads, then SWWt can be computed\nby averaging SWWh over all layers and heads at time step t:\nSWWt = \\frac{1}{LXH}\\sum_{l=1}^{L} \\sum_{h=1}^{H} SWWh \\qquad (7)\nBy averaging SWWt over all time steps T, the overall SWW for the prompt can be computed as\nfollows:\nSWW = \\frac{1}{T}\\sum_{t=1}^{T} SWWt \\qquad (8)\nHigh SWW indicates that the model is focused on significant attention for sensitive words. As a\nresult, the response is often to be a rejection. In addition, the Attn_SensWords refers to the average\nSWW across all prompts in a given dataset, and Attn_SensWords can be computed as following\nformula (9):\nAttn_Sens Words = \\frac{1}{|S \\times P}\\sum_{p=1}^{P} SWW \\qquad (9)\nWhere the S is the number of elements in set S, and P is the number of all prompts."}, {"title": "THE DEFINITION OF ATTN_DEPSCORE", "content": "Let X = {X1,X2,...,xN} be the input sequence with length N. Followed by the input sequence,\nY = {Y1, Y2,..., y\u012b} is a sequence which is composed of T generated tokens. For each head hin\nlayer l, the assigned attention weight of context (input) tokens at time step t is defined as follows:\nAlh_{t,input} = \\sum_{i=1}^{N} Bt,1,h,i \\qquad (10)\nIn equation (10), \u1e9et,1,h,i denotes the assigned attention weight for the i-th token in the input sequence\nwith head h in layer l at time step t. The Normalized Context Attention Weight (NCAWh) of the\ntotal attention at time step t is defined as following equation (11):\nNCAWh = \\frac{Alh_{t,input}}{\\sum_{j=1}^{N+t} \\beta_{t,l,h,j}} \\qquad (11)\nBy averaging NCAh over all layers and heads, Attn_DepScore, can be computed as follows:\nAttn_DepScore\u2081 = \\frac{1}{LXH}\\sum_{l=1}^{L} \\sum_{h=1}^{H} NCAW \\qquad (12)\nFinally, by averaging Attn_DepScore, over all generated tokens, the overall Attn_DepScore can be\ncomputed as following equation (13) :\nAttn_DepScore = \\frac{1}{T}\\sum_{t=1}^{T} Attn_Scoret \\qquad (13)"}, {"title": "THE DEFINITION OF ATTN_ENTROPY", "content": "Similarly", "14)": "nOt", "15)": "nAttn_Entropy_th = \\sum_{i=1}^{N} ot,1,h,i \\log Ot,1,h,i \\qquad (15)\nAs we know, entropy is often used to measure"}]}