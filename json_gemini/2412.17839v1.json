{"title": "LaMI-GO: Latent Mixture Integration for Goal-Oriented Communications Achieving High Spectrum Efficiency", "authors": ["Achintha Wijesinghe", "Suchinthaka Wanninayaka", "Weiwei Wang", "Yu-Chieh Chao", "Songyang Zhang", "Zhi Ding"], "abstract": "The recent rise of semantic-style communications includes the development of goal-oriented communications (GO-COMs) remarkably efficient multimedia information transmissions. The concept of GO-COMs leverages advanced artificial intelligence (AI) tools to address the rising demand for bandwidth efficiency in applications, such as edge computing and Internet-of-Things (IoT). Unlike traditional communication systems focusing on source data accuracy, GO-COMs provide intelligent message delivery catering to the special needs critical to accomplishing downstream tasks at the receiver. In this work, we present a novel GO-COM framework, namely LaMI-GO that utilizes emerging generative AI for better quality-of-service (QoS) with ultra-high communication efficiency. Specifically, we design our LaMI-GO system backbone based on a latent diffusion model followed by a vector-quantized generative adversarial network (VQGAN) for efficient latent embedding and information representation. The system trains a common-feature codebookthe receiver side. Our experimental results demonstrate substantial improvement in perceptual quality, accuracy of downstream tasks, and bandwidth consumption over the state-of-the-art GO-COM systems and establish the power of our proposed LaMI-GO communication framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Future wireless networks are poised to incorporate advanced artificial intelligence (AI) techniques and to deliver unprecedented improvement of user experiences and human-machine interactions [1]. Advanced learning technologies, including generative AI and large language models, continue to propel the AI revolution and change our lifestyle in an increasingly networked world. Leveraging AI and de-emphasizing the importance of bit-level communication accuracy and capacity [2], the new paradigm of goal-oriented communication (GO-COM) makes it possible to satisfy the receiver's needs despite bit losses during communication. Unlike conventional communication systems aimed at delivering source data (bits) accurately, GO-COM prioritizes addressing the receiver's specific needs over exact bit-wise message reconstruction [3], [4], [8]. This new paradigm makes it possible to use high computation intensity in exchange for bandwidth or capacity, facilitated by continual advancements in the semiconductor and computer industries. Consequently, we anticipate a design shift from classical to goal-oriented communication systems, where AI serves as the backbone of future-generation communication systems by leveraging powerful computational resources, deep learning intelligence, and immense data points.\nUnlike conventional communication systems that are data-oriented, GO-COM systems are driven mostly by downstream tasks that the receiver may perform upon data reception [5]. For example, a network of intelligent transmitters for traffic control system covering a metropolitan region requires no information about the exact color of each passing vehicle or the window shape of a background building. Without the need for many ground-truth specifics, one can improve the bandwidth utility by introducing task-specific GO-COM to remove irrelevant or private details from data communication. As a result, goal-oriented communications aim to reduce bandwidth usage by not communicating potentially redundant information or by obscuring private information.\nRecent GO-COM systems increasingly leverage generative AI owing to their capacity to generate information from prior distributions with minimal conditions. Among various options, diffusion-based GO-COM systems have appeared in works such as [3], [4], [6], [8], [9]. The authors of [9] presented a context-preserving image coding diffusion model; however, its transmitter sends both a segmentation and a low-resolution version of the original image, consuming significant bandwidth. Additionally, to combat channel noise and distortion, the work in [8] proposes a model that uses semantic segmentation to increase resilience against channel noise. Although this model demonstrates robustness to additive channel noise in source data, such a noise model is incompatible with most modern-day packet communication systems where channel noises and distortions lead to packet errors instead of noisy source data. Moreover, a pre-defined single semantic segmentation may omit crucial information needed for downstream tasks, such as missing traffic signs in images captured by remote cameras for remote-controlled robotic vehicles.\nIn response, the systems in [3] and [4] address vital information loss by implementing a novel local feedback mechanism for quality assurance. This feedback also supports noise latent approximation to ensure consistent signal recovery at both transmitter and receiver ends. However, these approaches increase computational complexity and extend recovery delay in reconstruction. Alternative approaches for GO-COM include models based on vector quantized variational auto-encoders [10], auto-encoders [11], and semantic channel coding [12].\nFor better user adaptability and user privacy, an efficient"}, {"title": "II. RELATED WORK AND BACKGROUND", "content": "To revisit several existing GO-COM frameworks, the main principle behind GO-COM is based on intelligent information exchange without sharing all available source information with the receiver. In short, GO-COM needs intelligent transmitters and receivers that can benefit from AI-based joint optimization beyond earlier system models for GO-COM [14].\nThe recent advances in generative AI have led to a paradigm shift in GO-COM. Earlier GO-COM models applied deep learning principles. Among many deep learning models, the auto-encoder (AE) was one popular choice with end-to-end training [15]. However, classical AE has some known shortcomings, including limited bandwidth savings, dataset dependability, and weaker robustness. VQ-VAE in [10] has mitigated some of these shortcomings. Other popular GO-COM systems considered reinforcement learning [16], distributed learning [17], multi-modal learning [18]\u2013[20], prior to the popularity of generative AI.\nSeveral recent GO-COM designs based on generative AI such as Diff-GO [3], Diff-GO+ [4], and GESCO [8] have incorporated the more advanced diffusion model backbone. These diffusion-inspired GO-COM systems have shown better performances in bandwidth and perceptual quality [3], [4], [8]. In GESCO [8], the authors use a pre-trained diffusion model on the receiver side to regenerate the encoded source signal (of an image) by letting the transmitter share the high-level image conditions such as semantic segmentation. Through pre-training against additive noise channels, GESCO shows some robust performances against Gaussian noise. However, the model does not provide perceptual quality or quality control in message reconstruction. To accommodate broader downstream tasks such as training a deep neural network (DNN), Diff-GO and Diff-GO+ implement identical diffusion models at both the transmitter and receiver sides. For quality control, the transmitter uses the same reconstructing process as the receiver"}, {"title": "III. OVERALL SYSTEM ARCHITECTURE", "content": "This section introduces a novel goal-oriented communication (GO-COM) system that utilizes a latent diffusion model in a quantized latent domain. Fig. 1 illustrates the overall system architecture of LaMI-GO. Our system design is inspired by dictionary learning as it enables models to learn image representation using the most fundamental structures. These learned representations then provide the common knowledge that can be leveraged to compress and represent similar but new unseen images by the model. For this purpose, we learn an embedding space over a large dataset. Subsequently, the codewords (elements of this embedding space) capture the fundamental structure of the image. The importance of the above representation for a communication system is twofold. First,"}, {"title": "IV. DETAILED STRUCTURES", "content": "This section elaborates on the details of different system blocks in LaMI-GO."}, {"title": "A. Latent Diffusion Backbone for LaMI-GO", "content": "To empower our GO-COM system, we utilize a text-to-image diffusion model from the work Paella [24]. Unlike other existing GO-COM works, we introduce the diffusion process in the VQ latent space. As previously mentioned, each latent vector is one of the K codewords of the latent space assuming K learned codewords. Therefore, the denoising process of the noisy latent can be viewed as a classification task. Note that with this formulation, we can represent a given latent (a set of latent vectors) as a set of indices corresponding to the learned codebook.\nAssume a text-conditioned diffusion model: $f_\\phi(.|t, C)$, takes a 2D matrix of indices $I \\in \\mathbb{Z}^{h' \\times w'}$ and outputs a 3D tensor $I \\in \\mathbb{K}^{h' \\times w' \\times K}$ and converted to a likelihood tensor using SoftMax operation. Here, the model's learnable parameters are represented by $\\phi$. i.e., For a given text condition $C$ and a time step $t$, the diffusion model $f_\\phi(.|t, C)$ denoises a 2D matrix of indices and output the likelihood of each index against K codewords. Note that, the denoising process is iterative, and $t \\sim \\mathcal{U}(0,1)$ represents the severity of the noise added to the 2D matrix of indices. Unlike classical diffusion models, adding noise to the index matrix represents replacing a set of indices with randomly sampled indices from the codebook. For each index in the input, the predicted index is obtained by considering the likelihood of each index by using the SoftMax function and multinomial sampling without selecting the maximum likelihood. If we assume the input index matrix to be $I$ and $II$ to represent multinomial sampling, the predicted latent at time t can be seen as follows,\n$I_p = II(softmax(f_\\phi(I|t, C)))$.\nWe direct curious readers to the work [24] for information regarding model training and sampling."}, {"title": "B. Task Formulation", "content": "We model the GO-COM message generation task on the receiver side as a classification task by utilizing the Paella text-to-image diffusion model. Let $E_\\theta(\\cdot)$ and $D_\\psi(\\cdot)$ be the encoder and the decoder models of VQ-GAN with their respective learnable parameters $\\theta$ and $\\psi$. Given an image $x_0 \\in \\mathbb{R}^{(h \\times w \\times c)}$ with image height h, width w, and channel size c, the encoder $E_\\theta(\\cdot)$ generates $z$ as the latent representation of $x_0$,\n$z = E_\\theta(x_0) \\in \\mathbb{R}^{h' \\times w' \\times l}$.\nNote that, $h' = \\lceil h/s \\rfloor$ and $w' = \\lceil w/s \\rfloor$ result from the compression factor s for encoding and l is the number of latent channels. We then reshape z into a collection of $m = h' \\times w'$ latent vectors, each has length l, to facilitate further discussion:\n$z \\rightarrow z_{flat} = [z_1, z_2, ..., z_m]^T \\in \\mathbb{R}^{m \\times l}$,\nwhere, $z_i \\in \\mathbb{R}^{l}$, $i = 1,2,...,m$ represents the latent vector at the i-th spatial position in the original z. The latent vectors $z_{flat}$ are then quantized using nearest-neighbor matching, denoted by $Q(\\cdot)$, based on a learned codebook $W = \\{w_1, w_2,...,w_K\\}$. Here each codeword $w_i \\in \\mathbb{R}^{l}$ and K is the number of codebook entries. The quantization process can be expressed as:\n$z_{flat} = Q(z_{flat})$\nwhere for each latent vector $z_i \\in \\mathbb{R}^{l}$, similar to [7], we search the nearest neighbor within the codebook: $w_k \\in W$ to yield,\n$k_i = \\arg \\min_j ||z_i - w_j||_2,  i = 1, ...,m$\nOnce the latent vectors are quantized, they are reshaped back into their original 3D tensor form $2 \\in \\mathbb{K}^{h' \\times w' \\times l}$. This reshaped tensor is then used as input to the decoder $D_\\psi(\\cdot)$, which reconstructs the original image:\n$\\hat{x}_0 = D_\\psi(\\hat{z}) \\in \\mathbb{R}^{h \\times w \\times c}$.\nNote that $\\hat{z}$ has dimensions $(h' \\times w' \\times l)$. To communicate the high-dimensional $\\hat{z}$, a shared codebook provides a simple representation $I = [k_1, k_2,...,k_m]$,which is a sequence of integer indices of the corresponding codeword from the shared codebook and is called the \u201cindex message\u201d. Here each quantized latent embedding (token) is converted to a position-aware integer sequence that can be highly compressible.\nIn LaMI-GO, we selectively mask some \u201chigh redundancy indices\u201d (HRI) of I using different policies during transmission. Our approach is to replace these HRIs with the least probable codeword index identified during training or erase them depending on the masking strategy. These masking strategies enable variable-length entropy-based coding to compress I to achieve a competitive bandwidth. We explain masking strategies in detail in section IV-D. For that, we view I as a 2D matrix of size (h' \u00d7 w') and name it $\\hat{I}$. i.e.,\n$\\hat{I} \\triangleq reshape(I, (h', w'))$\nNext, consider the recovery of the masked HRIs on the receiver end. Even though the masked HRI is obscured on the receiver end, by imposing pre-trained dictionary selection, the recovered indices on the receiver end must belong to one of the codewords. Hence, recovering the HRI on the receiver becomes a classification problem with K number of classes for codebook size K. The HRI recovery mechanism is not the focus of our GO-COM framework. But, if we replace these masked HRI with randomly selected learned codewords, it mimics the forward diffusion process (noising indices) of Paella. Hence, denoising these indices using Paella is a viable option. Therefore, we simply follow the method of Paella which is trained to reverse diffuse the noise (masked) indices by (1) generating a probability distribution for a given HRI; and (2) randomly sampling from the estimated distribution by multinomial sampling.\nRemark: this approach to handling many distributions may sound complex. The power of neural networks provides a simple way of handling such distributions.\nHowever, the generative process of Paella is unfavorable for communications. To understand this, let $\\pi_e$ denote the positions of replaced indices in $\\hat{I}$ during the masking process and $\\pi_u$ denotes the unaffected positions. Let the index message $\\hat{I}$ after masking and replacement of $\\pi_e$ be $\\hat{I}_e$. Now the receiver needs to recover the replaced codewords at positions $\\pi_e$ in $\\hat{I}$ using the text-condition model $f_\\phi(.|t_i, C)$ and the received original indices at $\\pi_u$ for different time stamps $t_\\tau > ... > t_2 > t_1$ iteratively. In a communication link, estimating the replaced codewords at $\\pi_e$ while retaining the codewords at $\\pi_u$ is equivalent to letting the receiver regenerates the message $I$ seen by the transmitter. As explained in previous sections, we feed $\\hat{I}_e$ to $f_\\phi(.)$ with text condition C.\nIn the iterative forward/backward diffusion process in Paella, indices in both $\\pi_u$ and $\\pi_e$ change. Also, for $\\tau/T$"}, {"title": "C. Latent Mixture Integration for Received Image Retrieval", "content": "The concept of a latent mixture integration (LaMI) is inspired by the training process of the Paella model and the need for a controlled convergence in the image recovery which is not fully governed by the text conditioning. Since our proposed LaMI process preserves codebook indices at positions in $\\pi_u$, it allows the use of pre-trained VQ-GAN and the Paella diffusion model without fine-tuning or retraining. In Paella training, the forward diffusion randomly replaces some indices with some noise indices in each given iteration. This process is controlled by a noise ratio derived from t. The ratio t is akin to the iteration step in conventional diffusion models. Therefore, during its denoising process, Paella model varies t from $t_1 = 1$ (i.e., full noise) to $t_1 = 0$ (i.e., zero noise) which removes all essential features at the beginning and tries to reconstruct them using text conditions. However, this feature is undesirable for data communications because the essential meaning of a message in $\\pi_u$ has been initially lost in the process and becomes nearly impossible to restore.\nThis work instead focuses on establishing a new efficient GO-COM framework. Thus, we propose a LaMI strategy that requires no training or fine-tuning of the original Paella model. Note that the LaMI-GO receiver is aware of the positions $\\pi_e$ of masked token identities at the transmitter. We denote the mask matrix M and also define an all-ones matrix$1_{h' \\times w'}$ of size h' x w'. At step i, LaMI regenerates the codeword index message\n$\\hat{I}_i = (1_{h' \\times w'} \u2013 M) \\odot \\hat{I}_e + M \\odot \\hat{I}_{p,i}$,\nwhere $\\odot$ denotes the Hadamard product and the replaced indices at the transmitter is predicted (estimated) according to\n$\\hat{I}_{p,i} = \\begin{cases}\nrandom sample from W & if i = 1 \\\\\nII(softmax(f_\\phi(\\Gamma(\\hat{I}_{i-1})|t_i, C)) & if 1 < i < \\tau\\\\\\\nII(softmax(f_\\phi(\\hat{I}_{i-1}|t_i, C)) & if \\tau < i < T,\\\\\n\\end{cases}$\nwhere $\\Gamma$ is the re-noising step, W denotes the learned codebook, $softmax(.)$ denotes SoftMax nonlinearity, and $II$ denotes multinomial sampling. Further, $\\hat{I}_{p,1}$ is a random latent sample from the codebook indices space. The function $\\Gamma$ represents the re-noising function defined in Paella. This function will randomly change (add noise) some portion of the given latent by initial noise latent $\\hat{I}_{p,1}$. The portion is determined by the noise ratio $t_i$. Note that we perform the latent mixture integration process in the integer space, which directly maps the latent space.\nAs mentioned previously, the final image reconstruction after T steps can be written as\n$\\hat{x}_0 = D(\\hat{z}_T)$,\nwhere, $\\hat{z}_T$ is the corresponding latent mapping of the indices matrix $\\hat{I}_T$."}, {"title": "D. Masking Policies", "content": "We now consider several possible masking policies on the transmitter side. For this work, we propose three strategies and explore more optimal policies in future works.\n1) Pseudo-random Masking (PRM): First, we introduce the diffusion training process of the Paella model. As discussed above, the forward diffusion process randomly adds noise to $\\hat{I}$ by replacing its values at the masked positions with randomly sampled codewords from the codebook. Following this strategy, we propose to simply use a pseudo-random mask (PRM) to select the positions of $\\hat{I}$ for replacement. Practically, the PRM can be easily shared with the receiver by defining a pseudo-random noise seed used to generate PRM indices. This policy requires minimum bandwidth overhead in communications. In our experiments, we consider masking ratio (i.e. probabilities) of p = {0.25, 0.35} which provides an adequate tradeoff between bandwidth and the performances. The PRM uses a mask matrix $M_r$.\n2) Pre-Determined Masking (PDM): Since the encoder, decoder, and diffusion models contain convolutional neural network (CNN) layers, it is possible to leverage neighboring data information for recovery at the decoder. Thus, we consider a pre-determined mask (PDM) pattern for $\\hat{I}$. In the PDM strategy, we agree on a one-time mask between the transmitter and the receiver. Such masks may also be dynamically changed by selecting a selected mask pattern from A set of pre-determined masks are generated and shared a priori. The transmitter only needs to indicate to the receiver the index of the mask within the shared mask set.\nWe denote the fixed mask used in PDM with a masking matrix $M_f$. Specifically, we can design a simple 2-D masking matrix for masking out 1/4 of the tokens using the following mask,\n$M_f(a, b) = \\begin{cases}\n1 & if a and b both are even \\\\\n0 & otherwise,\\\\\n\\end{cases}$\nThis exemplary PDM repeats a 2 \u00d7 2 pattern.\n3) Entropy Based Masking (EBM): Lastly, we present an entropy-based strategy for masking the $\\hat{I}$. As presented in Algorithm 1, we calculate the distance between the encoded latent information and the learned embeddings at step Eq. (7) using the following equation for each latent vector at (a, b) of $E_\\theta(x_0)$:\n$d(a,b) = \\min_k ||E_\\theta (x_0)_{(a,b)} - w_k||$,\nwhere we denote each codeword of learned embedding space as $w_k$. Next, the mask matrix selects the indices of the codewords with a lower distance than a masking threshold $\\eta$.\nIf the set representing these indices is D, the EBM uses a masking matrix $M_e$ defined by\n$M_e(a, b) = \\begin{cases}\n1 & if the coordinate pair (a, b) \\notin D\\\\\\\n0 & otherwise,\\\\\n\\end{cases}$\nNote that the distance $d_{(a,b)}$ is directly related to the entropy of information loss. EBM aims to drop the quantized latent entries of lower entropy. On the receiver side, we utilize the conditions and prior knowledge of the diffusion model to mitigate the entropy loss at the EBM indices. Since $M_e$ is message-dependent, the receiver needs additional information from the transmitter to acquire the masking matrix. We discuss the effect of selecting the best n candidate indices for masking in Section V."}, {"title": "E. Overview of Post-Training Showtime", "content": "Once the trained model is deployed, we enter a \u201cshowtime\" period to test the communication efficiency and efficacy. The receiver side of Fig. 4 visualizes this process. During showtime, the GO message interpreter at the receiver the GO message to generate the shared indices and semantic conditions for the latent diffusion model. Using a parallel path, the receiver identifies the mask according to the received masking matrix information from the transmitter. Once the received conditions and the shared masked latent are ready, random noise vectors replace the masked latent entries by using our proposed latent mixture integration strategy, starting with a random latent. After $T$ steps, we replace the random latent entries with the predicted latent entries and continue the mixture integration process to complete T overall steps. Upon the completion of the diffusion iterations, the predicted latent entries are respectively matched with their closest neighbors in the shared codebook to derive the final latent representation for the decoder to get the image domain representation. This reconstructed image then can be used for the different services on the receiver side.\nWe summarize the method into two algorithms as presented below in Algorithm 1 and Algorithm 2.\""}, {"title": "V. EXPERIMENTAL RESULTS", "content": "This section provides new empirical results obtained after rigorously evaluating our proposed method. Our extensive evaluations cover several datasets, including Cityscapes dataset [27], which is popular for traffic management and autonomous driving-related tasks with 35 classes, as well as the Flickr [28], COCO-Stuff [29], and COCO [30] datasets which are widely used for tasks such as object detection, segmentation, and image captioning. In the LaMI-GO framework, we consider images of size 256 \u00d7 512 from Cityscapes and 256 x 256 for other datasets. We follow different evaluation strategies and metrics. To evaluate the image recovery quality, we employ perceptual quality metrics such as FID [31] and LPIPS [32] alongside downstream tasks such as depth estimation and object detection. Our experiments only require a single A100 GPU with 80GB VRAM and additional CPU support. For the evaluation, we get the pre-trained Paella model trained on the LAION [33] dataset and avoid fine-tuning or training."}, {"title": "A. Reconstruction Quality and Overall Performance", "content": "We first illustrate the semantic reconstruction power of LaMI-GO over existing semantic/GO-COM works: Diff-GO+ [4], GESCO [8], Diff-GO [3] and its variants RN (Random Noise) and OD (Original Diffusion) [3], and other semantic image syntheses models: SPADE [34], CC-FPSE [35], SMIS [36], OASIS [37], and SDM [38]. All three semantic communication methods and their variants use pixel domain diffusion models as the backbone and semantic segmentation map and edge map as the model conditions. We deliver LPIPS and FID scores with the required denoising steps. The comparison shows the superior performance of LaMI-GO over other semantic generation and communication proposals. The performance for all three measures exhibits substantial improvement. One notable aspect of these results is the lower number of diffusion steps needed for LaMI-GO compared with other diffusion-based semantic communication benchmarks. Because low latency and low computation complexity are critical to communication network services, fewer diffusion steps exponentially reduce the receiver decoding time.\nWe extend the test performance of LaMI-GO against contemporary generative models based on generative adversarial networks (GANs), diffusion models, and auto-regressive models conditioned on texts. Table II demonstrates that the proposed LaMI-GO model is flexible, competitive, and better with increasing computation at larger re-noising and reconstruction steps $\\tau$ and $T$."}, {"title": "B. Masking Policy Effects", "content": "1) Perceptual Comparison: To better evaluate the performance of our model, we compare the three different masking policies, PRM, PDM, and EBM, against the perceptual quality of the received image. Table IV presents the results from testing different policies across various datasets with different configurations of noising/denoising steps.\n2) Analysis of EBM Policy: EBM policy compares latent feature distance with a selection choice of either the lower distance indices or higher distance indices for masking. Lower distance represents the latent features closer to the learned codebook and likely captures the most common features of the dataset. On the contrary, the higher distance represents a gap between the encoded embeddings and the learned embeddings. We believe these indices represent unique features of the source images during the test phase (showtime).  Analyzing Fig. 6 and Fig. 7, it is evident that the highest distance policy focuses on masking the critical and unique objects for the source image. Conversely, Fig. 7 shows that the lowest distanced policy focuses on masking more common features with smaller variations. Thus, if the goal of data recovery is to achieve better perceptual quality or if the goal is sensitive to many visible objects, then low-distance-based masking is preferred. We see this phenomenon from Table III. Unlike the random or pre-determined methods, one downside of EBM in general is that its masking area may not be evenly distributed. Its masking may affect large patches of the given object. As a result, we see from Table III and Table IV that the FID values of the EBM masking are generally higher."}, {"title": "C. Impact of Masking Probability", "content": "We now investigate the effect of the sampling probability or ratio p on PRM in comparison with PDM and existing semantic communication methods. Our evaluation employs FID and LPIPS as a contextual performance metric of the recovered image on the receiver end. From Fig. 8 and Fig. 9, we see that increasing masking probability p from 0.25 to 0.35 lowers the perceptual quality of image recovery in the Cityscape dataset. The performance reduction at p = 0.35 is noticeable and natural since the entropy in the received data is high. Also, the PRM model has limited capabilities to combat higher distortion rates without fine-tuning the diffusion model. As we increase $\\tau$ and $T$ to allow higher delay and computation complexity, we witness the same trend of improved reduction in FID and LPIPS scores. Importantly, the performance of LaMI-GO is significantly better than the previously invented semantic communication models even at the relatively higher masking ratio of p = 0.35."}, {"title": "D. Evaluation on Downstream Tasks", "content": "To assess other potential downstream task performance at the receiver end, we further evaluate the recovered image quality against two popular downstream tasks. First, as presented in Table V, we use depth estimation as a receiver downstream task on the recovered images from the Cityscape dataset. For comparison, we evaluate Diff-GO+ under the same test setup. From the results in Table V, we see that the LaMI-GO outperforms Diff-Go+. Moreover, there is a noticeable improvement in depth estimation accuracy for larger noise/denoise steps. These results further demonstrate that the increased diffusion steps enhance the model performance albeit at the expense of higher computation load and delay. Next, we present the object detection results of the LaMI-GO using 3 different datasets."}, {"title": "E. Bandwidth Consumption", "content": "In this section, we evaluate the rate requirement for the LaMI-GO as a measure of bandwidth consumption and compare it against other existing semantic communication methods, classical JPEG, and JPEG-2000. The experiments are based on the Cityscape dataset. To ensure a fair comparison, we align all methods under test to operate in a similar bandwidth range of consumption. We then benchmark the model performances based on the perceptual quality of test images and computation complexity measured by diffusion steps. From Table VII, it is clear that the LaMI-GO demonstrates superior performances in all the metrics. Another key highlight of the presented results is that the LaMI-GO surpasses the performance of both classical JPEG and JPEG-2000."}, {"title": "F. Performance under Channel Noise and Packet Loss", "content": "In this section, we examine the performance of our model against channel noise in a practical communication link setup. Many deep learning-based communication systems view the channel noise effect as directly additive to transmitted data and apply implicit training to those models. However, such a channel noise model assumes no error protection and is incompatible with the dominant packet-switched modern data networks in practice. Thus, we consider the channel noise effects by examining our model performance under packet loss at the receiver. Practical networks determine lost packets utilizing cyclic redundancy check (CRC) code to assess the received packet integrity. Block interleaves are commonly used to distribute lost packet bits across multiple data packets. Through interleaving, packet losses can be modeled as binary erasure channel [49].  We leave rigorous channel noise analysis and handling as future works. In the following sections, we simply analyze the robustness of the LaMI-GO against packet loss."}, {"title": "G. Convergence Analysis", "content": "In this section, we examine the model convergence at each iteration. First, we analyze the error of the predicted class at each index of $\\hat{I}_i$ and graphically illustrate the error against each denoising iteration in Fig. 14. As shown in the results of the figure, the number of class misclassifications decreases as the iteration number increases. However, we see the error flattens after 200 steps. We find two reasons behind this observation. First, after obtaining the prediction probabilities, the model randomly samples possible identities from the distribution rather than maximizing the likelihood, and also, as shown in the work [41], many of the latent embeddings are clustered in the embedding space. Hence, gives the freedom of predicting an identifier in the same cluster rather than the exact identifier. We save extensive analysis of convergence as future works."}, {"title": "H. Analysis of Showtime", "content": "We extend our analysis to compare the showtime delay of different GO-COM models along with the perceptual quality."}, {"title": "I. Ablation Study", "content": "We perform an ablation study to investigate the effectiveness of our solution. For that, we will ablate the latent mixture integration process to verify the effect of our model. We visually investigate the repercussions on the COCO-Stuff dataset by considering samples illustrated in  We use the PDM policy to derive the latent received by the receiver and perform iterative image reconstruction without latent mixture integration.  For comparison, we present the output of the latent mixture integration by using the same set of text conditions.  From the above figures, we see a generic image generation without latent mixture integration trying to generate an image that is semantically similar to the text. Much encoded information is lost at the receiver. We observe that if the receiver's goals include object detection or semantic segmentation, images generated from pure diffusion based on text conditions may carry wrong information. On the other hand, latent mixture integration preserves more valuable information and generates an image similar to the original source image."}, {"title": "VI. CONCLUSION", "content": "In this work, we present a new goal-oriented communication framework of LaMI-GO. Our novel framework, with generative AI as its backbone, is powered by VQGAN at a higher level to map high-dimensional images to their lower-dimensional latent representations. The latent diffusion model is conditioned on semantic descriptive text for image recovery. LaMI-GO framework achieves higher perceptual quality, higher downstream task performances, faster reconstruction, and robust performance against erasure channels for different packet error rates for a reasonable bandwidth. In future works, we plan to integrate diffusion-based GO-COM with neural network pruning to increase the computation efficiency. We will also investigate the distributed training of multi-user GO-COM with advanced foundation models."}]}