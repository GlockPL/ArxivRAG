{"title": "From News to Forecast: Iterative Event Reasoning in LLM-Based Time Series Forecasting", "authors": ["Xinlei Wang", "Maike Feng", "Jing Qiu", "Jinjin Gu", "Junhua Zhao"], "abstract": "This paper introduces a novel approach to enhance time series forecasting using Large Language Models (LLMs) and Generative Agents. With language as a medium, our method adaptively integrates various social events into forecasting models, aligning news content with time series fluctuations for enriched insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning and reflection to evaluate predictions. This enables our model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By compiling selected news with time series data, we fine-tune the LLaMa2 pre-trained model. The results demonstrate significant improvements in forecasting accuracy and suggest a potential paradigm shift in time series forecasting by effectively harnessing unstructured news data.", "sections": [{"title": "Introduction", "content": "Time series forecasting [18, 21] serves as an essential foundation for decision-making across diverse economic, infrastructural, and social domains [2, 13, 14, 16, 56]. The purpose of analyzing time series data is to decode the evolving relationships within complex and dynamic real-world systems. Traditional forecasting methods, which excel at identifying patterns in historical data, perform well when time series distributions remain consistent over time. However, they have limitations in address- ing sudden disruptions or anomalies caused by external random events, and do not systematically connect complex social events with fluctuations in time series data. Incorporating insights that reflect real-world events and their impact on social and economic behaviors into time series forecasts is critical for enhancing both the reliability and accuracy of these forecasts.\nNews articles provide crucial insights into unexpected incidents, policy changes, technological developments, and public sentiment shifts. These are factors that numerical data alone may not reflect. Integrating news into forecasting enriches its inputs with context that closely mirrors the complexities of human behavior and societal changes. On the one hand, news offers a real-time snapshot of events, enabling the model to adjust predictions based on updated information. On the other hand, incorporating qualitative data from news sources allows the model to address non-linear and non- numeric influences, providing a comprehensive perspective on the various influences. Combining both quantitative and qualitative insights can improve forecast accuracy and reliability in rapidly changing environments. Incorporating news into forecasting not only enhances the data set but also improves the adaptability and accuracy, making the model more reflective of real-world dynamics.\nIn this work, we first propose a unified approach that embeds news and supplementary information into time series data using textual prompts. By fine-tuning large language models (LLMs) [54, 55, 66],"}, {"title": "Related Work", "content": "Time series forecasting. The traditional method of time series forecasting relies on analyzing historical data and utilizing statistical models to predict future trends, with an assumption that past patterns will persist in the future[8, 12, 20, 27, 42]. However, these methods were limited to small- scale datasets. The advent of deep learning [30] has introduced a range of time series forecasting networks [33, 34, 36, 40, 53, 59, 60, 67, 69] that excel in managing larger, more complex datasets by capturing non-linearities and dependencies directly from historical data. Recent advancements include pre-training on diverse, large-scale datasets, allowing models to be fine-tuned on specific tasks with fewer data and resources [6, 24, 58, 63]. While these methods continue to evolve and improve performance benchmarks, they often neglect the influence of external and contextual factors.\nAttempts to incorporate textual information (e.g., Twitter feeds, news articles, public reports) into time series forecasting have been made across various domains, such as finance [7, 48, 49], energy [3, 41], entertainment [26], pandemics [65], and tourism [47]. Traditional methods often simplify text analysis to counting keyword frequencies [41] or using dummy variables, which do not capture nuanced meanings. Advanced efforts include extracting richer textual features like word frequencies and sentiments using traditional NLP [9] and machine learning, as demonstrated by Bai et al. [3]. However, these approaches require labor-intensive feature engineering, struggle with long-text dependencies, and lack a deep contextual understanding. In contrast, large language models excel in processing complex textual data and understanding contextual relationships, which can improve prediction accuracy and efficiency through automated feature extraction and enhanced scalability across multiple tasks. Despite their potential, no study has yet fully exploited LLM for enhancing forecasting with their capabilities in understanding unstructured textual data.\nLanguage models for time series forecasting. LLMs such as the GPT series [?, 1, 4, 44, 45] and LLaMa [54] have excelled in a variety of natural language processing tasks. With their vast parameter sets, LLMs acquire extensive general knowledge and reasoning capabilities during pre- training, which is crucial for building intelligent systems equipped with common sense. LLMs architectures are increasingly applied to time series processing and forecasting [23, 25, 35, 37, 51]. For instance, TEMPO [6] adapts GPT architectures for dynamic temporal representation learning, while TIME-LLM [23] utilizes LLMs for time series forecasting by reprogramming input data and applying Prompt-as-Prefix techniques. Similarly, FPT [71] demonstrates that even frozen LLMs can perform effectively in time series tasks, leveraging the universality of self-attention mechanisms. Lag-LLaMa [46] uses a decoder-only transformer for univariate probabilistic forecasting, and Gruver et al. [17] show that by framing time series forecasting as next-token prediction, LLMs can surpass traditional models through effective tokenization and adaptation.\nReasoning with language models. LLMs can automate tasks with human-like reasoning through \"Chain of Thought\u201d (CoT) prompting [57], enhancing reasoning by step-by-step emulation of human thinking [28, 32]. CoT prompting is useful for transforming complex questions into answers by introducing intermediate steps. The \u201cTree of Thoughts\u201d (ToT) approach [61] refines this by mimicking trial-and-error methods, enhancing auto-regressive LLMs with a prompter agent, checker module, memory module, and ToT controller for multi-round dialogues. LLM-based agents can solve tasks by reflecting feedback signals in text and retaining them in a memory buffer for better decisions [50]. Cai et al. [5] proposed the LLMS As Tool Makers (LATM) framework, where LLMs create reusable tools for problem-solving, interleaving reasoning and actions to aid task completion and interaction [62]. These agents can debate their responses and reasoning to arrive at final actions [11]."}, {"title": "Method", "content": "In this work, we aim to integrate news insights into time series forecasting. The development of such a system faces several challenges. Firstly, the forecasting method must handle unstructured, non-numerical news inputs flexibly and adjust predictions based on the context of the news events. Secondly, constructing this model involves filtering news related to the time series data, establishing connections between the news and the time series data. This requires sifting through vast amounts of internet data to find relevant information, demanding deep societal understanding and sophisticated reasoning skills. Therefore, an intelligent agent is designed to manage this complexity. Moreover, potential inaccuracies in news selection or inferential errors may still affect the forecast accuracy, requiring further refinement of news selection and reasoning based on the predictions. Our approach includes three main modules: a language model-based forecasting module (Sec 3.1), a reasoning agent for news filtering and inference and an evaluation agent to assess and refine the forecasting model (Sec 3.2). The core workflow of our method and the interrelations between these modules are shown in Figure 1. The subsequent sections will discuss these three modules in detail."}, {"title": "Rethinking Time Series Forecasting Problem and Elements.", "content": "Time series forecasting can be considered as a conditional generation problem of sequences [17]. This aligns with the general paradigm of natural language processing represented by LLMs. Taking the LLaMa language model as an example, assuming a number series {123,456}, LLaMa's tokenizer will regard this number as a sequence of digit tokens, i.e., {\u201c1\u201d,\u201c2\u201d,\u201c3\u201d,\u201c,\u201d,\u201c4\u201d,\u201c5\u201d,\u201c6\u201d}. Given the input series \"123\", the probability of predicting \"456\" can be represented as a probabilistic forecasting process in an autoregressive manner: $P(\u201c456\u201d|\u201c123\") = P(\u201c4\u201d|\u201c123\u201d) \u00b7 P(\u201c5\u201d|\u201c4\u201d, \u201c123\u201d)\u00b7 P(\"6\" | \"45\", \"123\").$ Generally, denoting the time series tokens at time t as $x_t$, the LLM predict the next token in the series $x_{t+1}$ using the conditional probability distribution $P(X_{t+1}|x_{0:t})$. During pre-training, LLMs optimize its internal parameters to maximize this conditional probability over a wide range of natural language corpora. Though counterintuitive, Gruver et al. [17] have shown that pre-trained language models exhibit a significant few-shot capability for time series forecasting. This shows the potential of language models in understanding input digital tokens, and also inspires us to use the language model as a reasonable platform to study how to introduce the information contained in textual prompt into time series prediction.\nNews context offers critical insights into complex social events that traditional data often overlook, and it also reflects sudden shifts in time series due to random events. In fact, the time series we collected can already be seen as being influenced by the aforementioned events. Assume an event E and a time series $x_{0:t}$, its impact on the future sequence can be expressed also as a conditional probability $P(x_t|x_{0:t}, E)$. However, when information about event & is not provided, we can only predict through historical time series. Although time series data itself can show patterns and trends, it\""}, {"title": "Analytical Agent for Aggregation and Reasoning of Contextual News Information.", "content": "Next, we construct a dataset to train the above model. While obtaining time series data is relatively straightforward, matching it with appropriate news and supplementary information is not trivial. The internet is flooded with news, most of which are irrelevant to the time series we aim to forecast. Introducing irrelevant news can disrupt forecasting. Therefore, it is crucial to analyze the relevance and causality between the time series forecasting task and the select news accordingly. However, gaining such an understanding is complex, requiring knowledge of human societal mechanisms and logical reasoning skills. In our work, we utilize LLMs for filtering and reasoning about news content. We also recognize that even the most advanced language models cannot complete all reasoning and judgment in a single generation process. We explain how to use a combination of multiple LLM generations to create an intelligent agent that fulfills the complex requirements of news filtering.\nTime series and news pre-pairing. For the initial stage of data preparation, news is retrieved to align with time series data based on matching time frequencies, horizons, and geographical areas. This synchronization ensures that insights from textual information are timely and regionally relevant. For example, to understand state-level electricity demand in Australia from 2018 to 2022, we gather local news from various Australian states and international news occurring in the same period that might directly or indirectly affect demand. In this way, potentially relevant candidate information can be roughly selected first, and such screening can be easily completed through crawler means."}, {"title": "Overall Pipeline", "content": "We integrate the news reasoning and evaluation agents with the fine-tuning of the LLM forecasting model to enhance the quality of training data, as illustrated in Figure 4. In the first iteration, we use the LLM agent to establish news selection logic based on the domain and timing of the time series task. This logic directs the reasoning agent to filter relevant news, align it with time series data, and input it into the model for initial fine-tuning. After validating the model's prediction with a validation set, an evaluation agent checks for missing news that may have influenced the prediction. This feedback helps the reasoning agent refine the filtering logic in subsequent iterations. This cycle continues until the final iteration, where the reasoning agent consolidates all updates to create the definitive news filter for training the final model. We use the GPT4-Turbo model as the LLM used in the above agents."}, {"title": "Experiments", "content": "Time series data. We selected time series data from domains influenced by human activities and social events to test our method's ability to capture complex human-driven dynamics during forecast- ing. These domains include Traffic [39] (traffic volume), Exchange [29] (exchange rate), Bitcoin [15] (Bitcoin price), and Electricity[15] (Australian electricity demand). To avoid bias from pre-trained language models, we updated the Exchange and Electricity datasets up to 2022. We use half-hourly electricity demand data from the Australian Energy Market Operator (AEMO) (aemo.com.au) and daily exchange rate data from the Exchange Rates API (exchangeratesapi.io). These datasets vary in frequency, including daily, hourly, and half-hourly updates, allowing us to evaluate the algorithms' effectiveness across different temporal resolutions. More details are in Appendix A.1.\nNews collection. Since there are no public datasets that pair time series data with news events, we have collect news specifically for the above time series to facilitate our research. Some of the news content is collected from the GDELT dataset [31], a database tracking news from nearly every country in over 100 languages. GDELT provides real-time insights into societal, political, and economic events, enabling detailed analysis of global trends and their effects. We incorporate GDELT's event information into our forecasting models to enhance predictive accuracy. For domains needing the latest information, we collect real-time news from sources like News Corp Australia (news.com.au) and Yahoo Finance (yahoo.com), focusing on region-specific and task-specific activities.\nSupplementary information. We enhance our forecasting models with open-source tools to grasp additional data for better accuracy and context. Weather information from OpenWeatherMap [10] provides daily temperatures, atmospheric pressure, wind speed, and humidity, crucial for load"}, {"title": "Results", "content": "Effectiveness of news intergration. In our approach, we incorporate news and supplementary information into time series forecasting by fine-tuning language models. Firstly, we assess whether this additional information can enhance time series forecasting. We conducted experiments to verify the necessity and effectiveness of integrating news data into our forecasting model. We compared four different scenarios as detailed in Appendix A.2 and Appendix A.3:\n1.  Pure numerical tokens: Uses numerical tokens, encompassing all variables without news. Except for region names or date information, it excludes other textual tokens as a baseline for comparison.\n2.  Textual descriptive sentence tokens: Evaluates whether using sentence-form descriptions instead of only raw digital numbers can enhance accuracy, with no news integration included.\n3.  Unfiltered news with textual descriptive sentence tokens: Assesses how integrating descriptive sentences of time series with unfiltered news data affects the model's performance.\n4.  Filtered news with textual descriptive sentence tokens: Shows the effects of integrating descriptive sentences with news that has been specifically filtered for relevance by the proposed agents."}, {"title": "Conclusion and Discussion", "content": "In conclusion, our study demonstrates the benefits of integrating news into time series forecasting us- ing LLM-based forecasting method and LLM-based agents. These agents enhance model intelligence by autonomously identifying and addressing missed news, refining their logic, and assessing the impact of events on predictions. Our findings advocate for incorporating extensive domain knowledge, encouraging a shift towards more nuanced and context-aware forecasting. This approach enriches time series forecasting for adaptive, comprehensive forecasting aligned with real-world dynamics.\nLimitations of our approach. While our approach demonstrates that LLMs like LLaMa 2 [55] can enhance time series forecasting by integrating news, there are limitations to its applicability. The effectiveness of news integration is primarily evident in domains where human and market activities significantly influence trends. Our framework is less suitable for domains requiring precise meteorological modeling or where human activities have minimal impact, such as in meteorological or physical data. Additionally, the model is constrained by the maximum token length of pre- trained LLMs, complicating the simultaneous processing of large amounts of time series or multiple"}]}