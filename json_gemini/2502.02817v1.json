{"title": "A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions", "authors": ["Hao Yin", "Paritosh Parmar", "Daoliang Xu", "Yang Zhang", "Tianyou Zheng", "Weiwei Fu"], "abstract": "Action Quality Assessment (AQA) the ability to quantify the quality of human motion, actions, or skill levels and provide feedback has far-reaching implications in areas such as low-cost physiotherapy, sports training, and workforce development. As such, it has become a critical field in computer vision & video understanding over the past decade. Significant progress has been made in AQA methodologies, datasets, & applications, yet a pressing need remains for a comprehensive synthesis of this rapidly evolving field. In this paper, we present a thorough survey of the AQA landscape, systematically reviewing over 200 research papers using the preferred reporting items for systematic reviews & meta-analyses (PRISMA) framework. We begin by covering foundational concepts & definitions, then move to general frameworks & performance metrics, & finally discuss the latest advances in methodologies & datasets. This survey provides a detailed analysis of research trends, performance comparisons, challenges, & future directions. Through this work, we aim to offer a valuable resource for both newcomers & experienced researchers, promoting further exploration & progress in AQA. Data are available at https://haoyin116.github.io/Survey_of_AQA/.", "sections": [{"title": "1 Introduction", "content": "Skills and Action Quality Assessment (AQA) is an emerging and critical field in video understanding, moving beyond action recognition and action prediction [1] to evaluate how well actions are performed and score the skill level of performers (see Fig. 1). These techniques are essential in a range of domains, including sports [2,3], healthcare [4-7], fitness [8-10], industrial training [11,12], & AI video content generation [13] where accurate assessment of human actions/performance is crucial.\nFor example, in sports [2,3], it could be used to assess how well an Olympics diver performed, reporting what they did well/correctly, what they did wrong and what was the severity of these errors; take all (ideally)\nsuch factors into consideration and quantify the how well was the performance of the diver. Similarly, it could be used for other sports or physical rehabilitation [5,6] or managing diseases like Cerebral Palsy [14]. AQA can be used to train future surgeons by assessing their surgical performances [15], assessing them, and giving feedback on what can be improved. In manufacturing sector [11], AQA can be used to assess the skill levels of workers, train them, and ensure their actions follow safety standards. Over the last few years, generative AI [13] has been on the rise, where AQA could serve as an objective metric for evaluating AI-generated video content.\nAutomated AQA systems have far-reaching implications across diverse sectors, from promoting social equity to enhancing industrial efficiency. On a societal level, AQA can democratize access to training and evaluation tools, particularly benefiting under-resourced communities. Low-cost, AI-driven assessments provide underserved athletes with tailored feedback, support fair sports judging [2, 3], and facilitate remote patient evaluations in healthcare [4-6], improving access for rural or low-income populations. In skilled actions/vocational training, AQA offers objective skill assessments, helping workers certify and enhance their technical skills [16,17], which is essential for career growth. In industries like manufacturing and automation, AQA can reduce errors, ensure product consistency, and support a skilled work-force, leading to lower costs and higher productivity [11]. By assessing both human and robotic systems, AQA enables effective human-robot collaboration, boosting workplace safety and efficiency. Looking ahead, the development of ethical and unbiased AQA systems will ensure that these technologies can be equitably applied across various demographics, preventing algorithmic biases from perpetuating inequalities. As such, the continued growth and refinement of AQA technology promises to not only enhance performance and quality control but also play a pivotal role in creating more inclusive, accessible, and efficient systems that benefit society as a whole. As generative AI becomes more prevalent, AQA can also serve as an objective metric for assessing the quality of AI-generated content [13], ensuring transparency and trust in AI systems.\nAQA has evolved significantly over the past decade (see Fig. 2), marking major advancements across sports [2, 3], healthcare [4-6], fitness [8, 9], industrial training [11], and generative AI evaluation [13]. With the last comprehensive survey now outdated [18,19], there's a critical need for a current synthesis of this fast-growing field. New computer vision and machine learning methodologies have expanded the capabilities of AQA, introducing refined evaluation techniques, metrics, and cross-disciplinary applications that address diverse sector-specific needs-from precise feedback in sports performance to accessibility in remote patient healthcare."}, {"title": "2 Problem Definition", "content": "2.1 Preliminary\nAction and its Quality. First, let's develop some intuition about action quality. Action can be thought of as having two components: difficulty level (what action was done), and execution quality (how well that action was done) [21]. An example of difficulty level is that somersaulting in the Pike position (in sports like Diving or Gymnastic Vault) is more difficult than somersaulting in the Tuck position. So, if executed perfectly, somersaulting in the Pike position is of more worth than somersaulting in the Tuck position. An example of intuiting about execution quality: In the Pike position, having feet together is worthy of more points than having feet apart or crossed over. The action quality score is directly proportional to both difficulty level and execution quality. To give a better intuition regarding execution quality and difficulty level, we have contrasted them in Fig. 4.\nFactors to be captured by representations. In Fig. 5, we illustrate some instances of action elements that matter in AQA. We mentioned what elements are desirable or hold a higher value. The task of AQA involves not only identifying those elements in a given action sequence but also determining/quantifying their value. E.g., let's consider Fig. 5(f); we see that the feet of the gymnast on the left are not together. A judge judging that gymnast's performance would identify that her feet were not together; secondly, they would determine how much to penalize (in terms of the points) for that error- the larger the gap, the more would be the penalty. Following this example, we can see that a CNN will have to learn to capture elements like these, determine the severity of the error, and penalize accordingly. Moreover, not all the elements may be equally important. Note that this was for just an element, usually, an action instance is composed of multiple such elements; each of those elements needs to be taken into consideration, based on which the final action quality score is calculated.\nRepresentation formats. Several kinds of representations are used to capture actors' movements and other relevant factors, such as raw RGB video [2, 3], optical flow [22, 23], skeleton pose sequence [24-26], sound [23, 27, 28], human-object distance [26], etc.\nGeneric AQA pipeline. Typically, AQA methodologies follow a two-stage approach [3] (see Fig. 6): Stage 1: feature extractors are used to extract relevant features from performance recording (video, audio, etc.); Stage 2: these features are then mapped to generate AQA scores, reports, feedback using statistical or rules-based mapping modules (discussed in the following). AQA research includes improving both these stages."}, {"title": "2.2 Mapping Techniques", "content": "Mapping from (visual/multimodal) action representation space to output score space can be Statistical or Rules-based in nature (see Fig. 7), discussed in the following.\nStatistical methodology. In this methodology, models learn to estimate the relationship between the visual action representations/features and the action quality score from datasets in a supervised manner. Mapping model parameters are adjusted during training to fit the dataset. Note that while the weights can be adjusted in a data-driven manner, control over what patterns the mapping model fits on is not entirely in our control. Hence, the model might fit shortcut correlations, which might not be reflective of the true (intended) relation between action features and action scores. There are following three formats of statistical methodologies used:\n1. Regression-based scoring. Here, the statistical model predicts a numerical score on a continuous scale. In regression, the output score is a real number, and the model tries to find a relationship between the input features and this continuous output. For example, the score can be any real quantity between, let's say, 0 to 100. Models include linear regressors, shallow/deep neural nets, support vector regressors (SVR), etc. Regression allows or is used for fine-grained or detailed analysis of action quality. Examples of regression-based methodologies include [2, 3].\n2. Classification-based scoring. In this case, the model predicts a discrete label or category. In classification, the output action quality score/label is a category or class (not a continuous value), and the model assigns input features to one of these classes. Classification is generally used for coarsely quantifying the action quality. Sometimes this coarse nature can allow in recruiting non-experts for data labeling. Examples of classification-based methodologies include [8, 14, 27, 29].\n3. Pairwise ranking. In this case, models compare two or more samples to decide which sample is of the highest action quality. Note that this is not a standalone analysis like the regression and classification. Examples of pairwise ranking methodologies include [16,17,30].\nRules-based scoring. Unlike statistical methodologies, in rules-based methodologies, features/symbols are processed using a set of rules to compute the action quality score. A set of rules can be hand-crafted incorporating human expert knowledge, or they could be learned from data. In rules-based methodologies, how each individual feature of the performance affected the score can be determined/traced back. As such, the causal nature of the rules makes rules-based interpretable and explainable. Examples of rules-based methodologies include [26]."}, {"title": "2.3 Levels of AQA", "content": "Simple AQA. In its simplest terms, AQA involves using a model for mapping an action video to a performance/action quality score. Ideally, the idea is that the model will extract all the relevant (spatiotemporal) features that contribute to action quality (score). Values are associated with these extracted features. Models learn to extract relevant action quality features and their values and, based on all the values, output a final score.\nDigging Deeper. A deeper level of assessment is that the model lists out all the factors and their analysis for what the diver did well, where the diver made a mistake, assessing the severities of these errors and then computing the final score based on it.\nFeedback Generation. Here, the model is able to assess the performance and its good and bad points. Based on this, the model may be able to suggest what parts the actor can work on to improve. Generally, if the model has a level of transparency-has an internal record of the pros and cons of the performance/action, it should be able to suggest a preliminary level of feedback. For example, if the model identifies that a diver had bent their legs in the pike position, it may indicate that\nthe diver may work on keeping their legs during the pike position to improve their score."}, {"title": "2.4 Metrics", "content": "The following metrics are used to quantify/measure the performance of AQA models. The metric is chosen depending on the type of mapping model used. For example, correlation between ground truth and predicted scores is used when scores are on a continuous scale, while accuracy is used when action quality labels are discrete.\nSpearman's Rank Correlation. Pirsiavash et al. [2] first pioneered the assessment of action quality in 2014 and also first introduced the use of Spearman's Rank Correlation (SRC, denoted as \u03c1) to evaluate the performance of regression models. The SRC formula is as follows:\n$\\rho = 1 - \\frac{6 \\sum_{i=1}^n (R_i - R'_i)^2}{n(n^2 - 1)}$\nwhere Ri and Ri represent the ground truth and predicted rankings of the i-th sample, respectively. n is the total number of samples. The \u03c1 range is [-1,1], with values closer to 1 indicating better performance. SRC is currently the most widely used performance metric in AQA. However, SRC can only measure the strength and direction of the monotonic relationship between ground truth and predicted rankings without measuring the differences between ground truth and predicted scores.\nRelative L2 Distance. To address this limitation, Yu at al. [31] proposed Relative L2 Distance (R \u2013 L2) as a new metric for evaluating model performance. In contrast to SRC, which focuses on the ranking of predicted scores, R-12 places more emphasis on the numerical values of the predicted score. Moreover, compared to the traditional 12 distance, R-12 takes into account the score intervals between different categories of actions, facilitating cross-category model training. The R - 12 formula is as follows:\nR-12 = $\\frac{1}{n} \\sum_{i=1}^n (\\frac{s_i - s'_i}{s_{max} - s_{min}})^2$\nwhere si and si represent the ground truth and predicted scores of the i-th sample. smax and smin represent the maximum and minimum scores of this action category. n is the total number of samples. The R - 12 range is [0, 1], closer to 0 indicating better performance.\nAccuracy. For other mapping technique formats, pairwise rank and classification, accuracy (Acc) are commonly used as model performance metrics. The accuracy formula is as follows:\nAcc = $\\frac{TP + TN}{TP + TN + FP + FN}$\nwhere TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively."}, {"title": "3 Datasets for AQA", "content": "Datasets are a crucial component of machine learning projects, and they play a key role in AQA by enabling model training, performance evaluation, and real-world deployment. The success and advancements in AQA can be attributed to the development of novel and high-quality datasets. In this section, we systematically collect and summarize all existing datasets since 2014. Following the PRISMA guidelines, we identify 26 publicly available datasets related to skills and action quality assessment across various domains. We classify datasets into 9 domains {Surgery, Daily Activities, Rehabilitation, Music, Fitness, Industrial Manufacturing, Dance, AIGV, Sports} as illustrated in Fig. 8. These datasets are summarized in Tab. 1 and Fig. 9, and we provide a more detailed description of each dataset in the following. We have organized the datasets by domain. Finally, we conclude the section by providing a summary of the datasets."}, {"title": "3.1 Surgical Skills", "content": "JIGSAWS, proposed by Gao et al. [4] at MICCAI Workshop 2014, is the first surgical skill dataset for human action modeling. The dataset contains 103 samples with an average sample duration of 92 seconds. Three basic surgical operations were captured by the daVinci Surgical System from eight surgeons with different skill levels. Each operation was repeated five times, and the specific actions are illustrated in Fig. 8(a). There are 39 suturing samples, 36 knot-tying samples, and 28 needle-passing samples. Each sample consists of 76 dimensions of kinematic data (including cartesian positions, orientations, velocities, angular velocities, and gripper angle describing the motion of the manipulator) and video data. The dataset annotations contain action labels and action scores, with a total of 15 action labels corresponding to video frames, and action scores are the total scores of the six sub-elements, with a score interval of 1-5 for each element."}, {"title": "3.2 Rehabilitation", "content": "The datasets of rehabilitation contain UI-PRMD and KIMORE, which were proposed by Vakanski et al. [5] at Data 2018 and Capecci et al. [6] at TNSRE 2019, respectively. Detailed actions are illustrated in Fig. 8(b). UI-PRMD, fills the gap of a comprehensive dataset of physiotherapy actions. The dataset contains 100 samples of 10 different actions, each repeated 10 times, from 10 healthy subjects simultaneously captured by two\nsystems, Vicon and Kinect. Each sample consists of skeletal sequences, including position and angle. The dataset annotations are correct or incorrect binary-grade labels.\nKIMORE, is the only RGB single-view rehabilitation action dataset. The dataset contains 353 samples of 5 different actions from 44 healthy subjects and 34 patients, with an average sample duration of 29.9 seconds. The dataset was annotated with scores, including POS and CFs, with values in the range 0 to 50 for each action as defined by clinicians."}, {"title": "3.3 Daily Activities", "content": "The datasets of daily activities scenarios contain EPIC-Skill [16] and BEST [17], which were proposed by Doughty et al. at CVPR 2018 and CVPR 2019, respectively. Detailed actions are illustrated in Fig. 8(c).\nEPIC-Skill contains 216 samples divided into 4 sub-datasets: surgery, dough-rolling, drawing, and chopstick-using, with an average sample duration of 86.6 seconds. The surgery sub-dataset is JIGSAWS [4], the Dough-Rolling sub-dataset was 33 samples selected from\nKitchen-based CMU-MMAC [42], the Drawing sub-dataset contains videos of 4 volunteers repeating the drawing of SONIC and HAND 5 times respectively, and the Chopstick-Using sub-dataset contains videos of 8 volunteers repeating the chucking of beans in a box 5 times. Since JIGSAWS [4] was annotated, the other three sub-datasets and rankings were obtained by pairwise comparison of samples within classes.\nBEST contains 500 samples consisting of 5 daily skill tasks: scrambling eggs, braiding hair, tying a tie, making"}, {"title": "3.4 Music", "content": "Piano-Skills, proposed by Parmar et al. [27] at MMSP 2021, is the first AQA dataset in music and introduced audio into consideration. The dataset contains 992 samples from 61 piano performances, with an average sample duration of 160 frames, as illustrated in Fig. 8(d). The dataset was annotated by a trained pianist with skill levels and song difficulty."}, {"title": "3.5 Fitness", "content": "Fitness-AQA, proposed by Parmar et al. [8] at ECCV 2022, is the dataset with the most samples in AQA and the first in fitness. The dataset contains 21284 samples collected from video-sharing sites, covering BackSquat, BarbellRow, and Overhead Press, with an average sample duration of 4.1 seconds, as illustrated in Fig. 8(e). The dataset annotations contain binary-grade labels by two professional gym trainers.\nEgoExo-Fitness, proposed by Li et al. [9] at ECCV 2024. The dataset contains 6131 samples from 86 action sequences by combining 3 to 6 different actions of 12 types of fitness actions, constructed by the synchronized recording of 3 egocentric-view and 3 fixed exocentric-view (third-person) videos. The average duration is about 18.8 seconds. The dataset annotations contain technical key point verification of fitness action, natural language comment on subjects' action, and action quality score (range 1-5 points), which provide interpretable action judgment annotations."}, {"title": "3.6 Industrial Manufacturing", "content": "Assembly101, proposed by Sener et al. [11] at CVPR 2022, is currently the data set with most camera views in AQA. The dataset contains 4321 samples, constructed by capturing 362 different volunteers assembling and disassembling 101 different toy cars with 8 fixed and 4 egocentric cameras, as illustrated in Fig. 8(c). The average sample duration of 426 seconds. The dataset"}, {"title": "3.7 Dance", "content": "CDRG, proposed by Hipiny et al. [28] in IJAIN 2023. Fig. 8(g). The dataset contains 240 samples from 20 subjects performing 12 TikTok dance challenges respectively. Subjects used their own devices to record videos, resulting in different levels of image quality. The average duration of samples is 14.7 seconds. The dataset was annotated by 100 annotators to mark the winning one of each dance pair."}, {"title": "3.8 AI Generated Videos", "content": "GAIA, proposed by Chen et al. [13] at NeurIPS 2024, is the first AQA dataset constructed by collecting videos generated by text-to-video (T2V) models to assess their video generation quality. The dataset contains 9180 samples generated by T2V models from 18 different laboratories and commercial platforms, covering a total of 510 actions of the whole body, hand, and face, as illustrated in Fig. 8(h), with an average sample duration of 2.8 seconds. The dataset was annotated as scores of the generated videos from three perspectives, including subject quality, action completeness, and action-scene interaction, with a score interval of 0-100 for each perspective."}, {"title": "3.9 Sports", "content": "Derived from their clear rules and easy access, sports account for the highest percentage of AQA datasets, including 15 datasets covering 10 different types, as illustrated in Fig. 8(i). In recent years, the advancement of sports AQA datasets has been remarkable, with the publication of numerous high-quality datasets providing important benchmarks for AQA. The rest of this section provides a detailed description of sports datasets.\nMIT-Dive & MIT-Skate, proposed by Pirsiavash et al. [2] at ECCV 2014. MIT-Dive dataset contains 159 samples of Olympic diving, all in slow motion from television broadcasts at a frame rate of 60 fps. The average duration of dive samples is 2.5 seconds. MIT-Dive dataset was annotated with scores awarded by referees, ranging from 20 to 100 points. Similarly, MIT-Skate dataset consists of 150 skating samples, with a frame rate of 24 fps. The average duration of skate\nsamples is 175 seconds. The dataset was also annotated with awarded scores, ranging from 0 to 100 points.\nUNLV-Dive & UNLV-Vault, proposed by Parmar and Morris et al. [3] at CVPR Workshop 2017. UNLV-Dive contains 370 samples with an average sample length of 3.8 seconds and is an extension of the original MIT-Dive dataset. UNLV-Dive was annotated with scores calculated by multiplying the execution score (range 0-30 points) by the difficulty score (no upper limit specified in the rules, range 2.7-4.1 points in the dataset), resulting in a final dataset score. Similarly, UNLV-Vault contains 176 samples, with a sample average length of 2.8 s. UNLV-Vault was annotated with scores, which were calculated by adding the execution score (range 0-10 points) and the difficulty score (range 0-10 points), resulting in a final score.\nAQA-7, proposed by Parmar and Morris et al. [32] at WACV 2017. The dataset contains 1189 samples from both the Winter and Summer Olympics, covering 7 different action types (including singles diving-10m platform, gymnastic vault, big air skiing, big air snowboarding, synchronous diving-3m springboard, synchronous diving-10m platform, and trampoline). The average duration of samples is 6.7 seconds. The dataset was annotated with scores specific to each type, reflecting the unique scoring system. For each type, the scoring intervals are as follows: singles diving-10m platform (21.6-102.6), gymnastics vault (12.3-16.87), big air skiing (8-50), big air snowboarding (8-50), synchronized diving 3m springboard (46.2-104.88), synchronized diving 10m platform (49.8-99.36) and trampoline (6.72-62.99).\nMTL-AQA, proposed by Parmar and Morris et al. [21] at CVPR 2019. The dataset contains 1412 samples from 16 different competitions, including 10m platform and 3m springboard, male and female athletes, individual or pairs of synchronized divers, and different views. It also pioneered the: 1) natural language detailed qualitative description of the performance; and 2) disassembling (diving) actions into finegrained subcategories, divided into five parts: position, armstand, rotation type, somersaults, and twists. The average duration of samples is 4.1 seconds. The dataset annotations include the difficulty and action scores awarded by seven referees, along with the finegrained action breakdown and detailed qualitative descriptions of the corresponding actions.\nFis-V, proposed by Xu et al. [33] at TCSVT 2019. The dataset contains 500 samples of selected women's singles figure skating short program videos. There are 149 athletes from 20 countries, and the average duration of samples is 170 seconds, with an average frame rate of 25 FPS. The dataset annotations include the total element score (TES) and total program component score"}, {"title": "3.10 Conclusion of Dataset", "content": "Since the fundamental research of the AQA dataset in 2014, 26 popular datasets have emerged within the AQA domain, encompassing a wide range of domains from surgery to sport. Of these, sport accounts for the largest proportion and is the most important application scenario of AQA. A number of researchers have made significant contributions to the construction of datasets. To illustrate, Parmar has proposed a total of 6 widely used datasets before and after [3, 8, 21, 27, 32], while Xu has proposed the most fine-grained diving action dataset [38]. It is evident that the size of the dataset is continuously expanding, the action types and modality data are constantly being enriched, and the data annotation is moving towards fine-grained annotation, all of which are driving research progress. However, compared to action recognition datasets [44,45] and other related datasets [46-57], AQA datasets still face challenges\nin terms of scale, action diversity, data modality, and annotations.\nIn particular, GAIA [13], as the first dataset constructed using AI-generated videos, marks a significant breakthrough. It not only broadens the application scenarios of AQA but also introduces AQA into the T2V model to assess the quality of video generation. GAIA also inspires researchers to explore the possibility of using the T2V model to generate massive videos with multiple action types and skill levels based on predefined prompts, thereby easily constructing large-scale AQA datasets."}, {"title": "4 Methodologies for AQA", "content": "Following the comprehensive overview of publicly available datasets in AQA, it is evident that further investigation is required. What methodologies are being employed by researchers to utilize these rich resources of datasets to advance AQA fully? In order to answer this question, this section will concentrate on the methodologies employed in the field of AQA. By categorizing the pertinent and standardized research methodologies related to AQA in the last decade, it is possible to gain insight not only into the research paradigm in this field but also into the prevailing trends in research (see Fig. 10). The preceding decade of research has been classified into 7 principal trends, with comprehensive descriptions of the employed research methodologies and performance."}, {"title": "4.1 Fundamental Research", "content": "To the best of our knowledge, AQA research dates back to 1995 when Gordon [59] proposed to apply computer vision technologies such as tracking to AQA. There has been some research on AQA between 1995 and 2014, such as [60-62]. However, the AQA field still lacked datasets to conduct reliable studies and evaluation metrics of AQA methodologies. Pirsiavash et al. [2] and Gao et al. [4] introduced the first AQA datasets. These include MIT-Dive, MIT-Skate, and JIGSAWS datasets. Pirsiavash et al. [2] additionally proposed one of the initial machine learning frameworks for AQA by treating AQA as a supervised regression problem. Specifically, in their methodology, spatiotemporal features (pixel gradients and athlete pose features) were fed into a linear support vector regression (L-SVR) model. This methodology yielded an encouraging performance (see Tab. 2), while Venkataraman et al. [58] achieved better performance one year later. Gao et al. [4] mainly collected the surgical skills assessment dataset and employed a methodology proposed by Tao et al. [63]-\nsparse dictionary learning (SDL) combined with hidden Markov model (HMM) to realize surgical gesture recognition. AQA methodologies thus far were based on traditional/shallow machine learning.\nParmar et al. [14] introduced the classification paradigm to AQA by coarsely assessing the quality of user exercises and classifying them into erroneous vs. non-erroneous. They further tried several machine learning models on time-series and frequency-domain representations of human poses. Importantly, this work also concretely highlighted the problem of generalization of AQA models across human subjects.\nParmar and Morris et al. [3] were the first to propose utilizing deep spatiotemporal convolutional features (C3D network [64]) for AQA and proposed the\nUNLV-Dive and UNLV-Vault datasets based on the work of Pirsiavash et al. [2]. We believe this work marks the beginning of new age of AQA and the increased interest in AQA from the computer vision community. This work proposed three different frameworks for automated AQA: C3D+SVR, C3D+LSTM, and C3D+LSTM+SVR, and achieved promising performance. These frameworks differed in the way they aggregated clip-level features to obtain global video-level information. Particularly, averaging and LSTM-based [65] aggregation were explored.\nTheir work significantly advanced AQA in terms of both performance and datasets, with most subsequent research referencing and following this feature extraction methodology. Especially the use of pre-trained networks\non large-scale, labeled action recognition datasets [66,67], such as C3D [64], Resnet [68], I3D [66], P3D [69], and VST [70], as the backbone for feature extraction is very popular in AQA.\nIn the same year, Bertasius et al. [30] proposed the first-person basketball dataset and contrastive learning model, which represented an expansion of the mapping techniques from regression to pairwise ranking. This work employs convolutional LSTM and Gaussian mixture models (LSTM-GM) to generate highly nonlinear spatiotemporal features from atomic basketball events, then compute action quality by multiplying features with linear weights learned from the data. LSTM-GM could learn the evaluation criteria and pairwise ranking from pairs of weakly labeled first-person basketball videos.\nDoughty et al. [16] proposed the first skills assessment dataset for daily life activities, EPIC-Skills, in 2018. Additionally, Doughty [16] proposed a contrastive ranking model that utilizes temporal and spatial segment networks (2S-CNN) in combination with video segment ranking loss and similarity loss function to achieve high accuracy pairwise ranking. In 2019, Doughty et al. [17,71] also proposed a rank-aware attention model (RAA), in which a dual attention mechanism is used to focus on the pros and cons of action performance in long video clips and assign different weights to the clips to fuse the data. The model is trained using the combination of ranking loss, disparity loss, rank-aware loss, and diversity loss. This methodology enables accurate skill level assessment in video segments with different skill performances."}, {"title": "4.2 Research Trends", "content": "The fundamental research [2, 3, 14, 16, 17, 30,58] established the formats of task and application scenarios of AQA, offering datasets and references for subsequent research. As computing capability and deep learning technology advanced, AQA has attracted more attention, giving rise to new methodologies. These novel methodologies consistently enhance model performance and exhibit disparate research trends, which can be categorized into multimodal methodologies, generalization, continual learning, explainable, and comprehensive AQA."}, {"title": "4.2.1 Fine-grained AQA", "content": "The rise of fine-grained research as the principal trend in AQA can be attributed to its capacity to address pivotal issues such as the neglect of details, the influence of noise, and low interpretability, which were prevalent in\nearly studies. The advancements in computing capability and deep learning technology have made fine-grained research not only feasible but also mainstream. However, researchers have explored various methodologies to achieve fine-grain. To provide a more comprehensive explanation of fine-grained research, we have divided it into four sub-trends, which are elaborated respectively in the following sections.\n(1) Segment-aware feature extraction\nAction maybe broken down into phases/segments. For example, a diving action can be broken down into take-off, flight, entry into the water, etc. Some researchers bear the view that computing segment-level features may constitute a better methodology. Fundamental research [2,3,14,16,17,30,58", "72": "utilizing multiple P3D [69", "89": "the entire video is segmented into video clips of different stages of dives, which are then fed into the corresponding P3D to extract features. Subsequently, the features of the separate stages are fused to predict the score. Similarly, Dong et al. [73", "82": "proposed methodologies also based on ED-TCN and P3D. SSRM/MSRM [73"}, {"82": "label-reconstruction-based pseudo-subscore learning) uses the overall score as training labels and generates pseudo-score labels for each sub-stage, thereby addressing the issue of a lack of quality labels for sub-stages.\nIn addition to models based on CNNs like P3D [69", "90": "have also been applied to AQA [75-77, 79, 91", "76": "simply explored different Transformer-based architectures, with I3D-Transformer decoder performing best. While Xu et al. [75", "77": "and Gedamu et al. [79", "75": "grade-decoupling Likert Transformer) regards the score of long-term action as the combination of quantified grades"}]}