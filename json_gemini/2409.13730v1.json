{"title": "VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning", "authors": ["Zhihuan Jiang", "Zhen Yang", "Jinhao Chen", "Zhengxiao Du", "Weihan Wang", "Bin Xu", "Yuxiao Dong", "Jie Tang"], "abstract": "Multi-modal large language models (MLLMs) have demonstrated promising capabilities across various tasks by integrating textual and visual information to achieve visual understanding in complex scenarios. Despite the availability of several benchmarks aims to evaluating MLLMs in tasks from visual question answering to complex problem-solving, most focus predominantly on mathematics or general visual understanding tasks. This reveals a critical gap in current benchmarks, which often overlook the inclusion of other key scientific disciplines such as physics and chemistry. To address this gap, we meticulously construct a comprehensive benchmark, named VisScience, which is utilized to assess the multi-modal scientific reasoning across the three disciplines of mathematics, physics, and chemistry. This benchmark comprises 3,000 questions drawn from K12 education spanning elementary school through high school equally distributed across three disciplines, with 1,000 questions per discipline. The questions within VisScience span 21 distinct subjects and are categorized into five difficulty levels, offering a broad spectrum of topics within each discipline. With VisScience, we present a detailed evaluation of the performance of 25 representative MLLMs in scientific reasoning. Experimental results demonstrate that closed-source MLLMs generally outperform open-source models. The best performance observed include a 53.4% accuracy in mathematics by Claude3.5-Sonnet, 38.2% in physics by GPT-4o, and 47.0% in chemistry by Gemini-1.5-Pro. These results underscore the strengths and limitations of MLLMs, suggesting areas for future improvement and highlighting the importance of developing models that can effectively handle the diverse demands of multi-modal scientific reasoning.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) [38, 1, 25, 43, 44, 7, 9, 18, 3] have demonstrated remarkable capabilities across a wide range of tasks, including natural language understanding, text generation, and complex problem solving. The success of LLMs facilitates the development of multi-modal large language models (MLLMs) [39, 42, 6, 32, 31, 48, 49], which extends these capabilities by integrating the ability to process and analyze both textual and visual information. Evaluation is a significant component in assessing the ability of these MLLMs across various tasks, which has attracted widespread attention and developed rapidly in recent years. For instance, several benchmark datasets are proposed to evaluate the ability of MLLMs in general visual understanding, including MME [22], MMMU [50], MMBench [33], MMStar [14], and SEED-Bench [28]."}, {"title": "2 VisScience Dataset", "content": "In this section, we first illustrate the overview of our specially curated VisScience benchmark, designed to assess the capabilities of MLLMs in multi-modal scientific reasoning. Next, we introduce data generation process, which encompasses three core scientific disciplines: mathematics, physics, and chemistry. Lastly, we discuss the difference between our VisScience benchmark and existing benchmarks."}, {"title": "2.1 Overview", "content": "We introduce the VisScience benchmark, a meticulously curated collection aimed at evaluating the capabilities of multi-modal large language models (MLLMs) in multi-modal scientific reasoning, with a particular focus on bilingual tasks involving both English and Chinese. This dataset incorporates textual and visual contexts as inputs and spans three scientific disciplines, including mathematics, physics, and chemistry. Each discipline comprises 1,000 questions, meticulously gathered from different chapters to ensure comprehensive coverage of topics and concepts. The core statistics of the VisScience benchmark are presented in Table 1. The distributions of question length in VisScience are provided in Appendix A.1.\nIn mathematics, the dataset can be divided into six key areas: plane geometry, solid geometry, functions and equations, algebraic operations, probability and statistics, and combinatorial mathematics. The physical component of the VisScience benchmark encompasses eight subjects, including mechanics, thermodynamics, comprehensive experiments and methods, mechanical motion, vibration and waves, optics, electromagnetism, and modern physics. The chemistry section of the VisScience benchmark includes seven topics such as chemical experiments, organic chemistry, substance composition, electrochemistry, chemical reactions, inorganic chemistry, and chemical equilibrium. A detailed introduction of each subjects within the three disciplines is available in Appendix A.2.\nThe primary objective of the VisScience benchmark is to provide a rigorous and diverse benchmark for assessing the multi-modal scientific reasoning capabilities of MLLMs. This benchmark aims to supplement existing benchmarks that predominantly focus on mathematical reasoning by broadening the scope to include expansive domains such as mathematics, physics, and chemistry. This benchmark aims to supplement existing benchmarks that predominantly focus on mathematical reasoning by broadening the scope to include expansive domains such as mathematics, physics, and chemistry. Through this enhancement, VisScience seeks to provide a more holistic measure of MLLMs' abilities across a wider spectrum of scientific disciplines."}, {"title": "2.2 Data Generation", "content": "The goal of the VisScience benchmark is to establish a comprehensive, bilingual (Chinese and English) benchmark for evaluating the capabilities of MLLMs in processing and understanding complex, scientifically-oriented tasks across various disciplines. In order to achieve this goal, we present a two-stage data generation pipeline to meticulously construct a benchmark dataset comprising 3,000 questions, evenly distributed with 1,000 questions each in the fields of mathematics, physics, and chemistry.  Figure 2 shows some examples sampled from the VisScience benchmark across three disciplines: mathematics, physics, and chemistry. More cases in VisScience are provided in Appendix B.\nData Collection. We gather a total of 450,000 questions from the disciplines of mathematics, physics, and chemistry, each enriched with visual information sourced from K12 education. This collection spans a comprehensive range of knowledge points across different chapters, with the difficulty levels scaled based on education grade. Consequently, we cluster 150,000 questions per discipline and carefully select 1,000 representative questions. These questions exemplify a range of difficulty levels and a variety of subjects, guided by the following principles:\n\u2022 Guaranteeing every knowledge point is included in VisScience benchmark.\n\u2022 Prioritizing the selection of questions from high-frequency knowledge points.\n\u2022 Ensuring a mixture of questions across various difficulty levels.\nIn the end, the VisScience benchmark is constructed with 3,000 questions, with each of the three disciplines - mathematics, physics, and chemistry \u2013 contributing 1,000 questions. This approach ensures that the benchmark comprehensively covers a wide array of topics within each discipline, reflecting the breadth and depth required for a thorough assessment of MLLMs' capabilities.\nData Annotation. To improve the quality of the VisScience benchmark, we conduct multiple checks using both manual reviews and LLM assessments to confirm the completeness of each question. For textual content, we check for accuracy, coherence and relevance, ensuring that each question aligns with the corresponding scientific discipline and is free of ambiguities. For associated visual content, we rigorously screen out images that are incorrect, unclear, or lacking in detail, retaining only those that are clear and richly informative. To maintain the volume of the VisScience benchmark, we compensate for questions removed due to incomplete information by selecting new questions on identical topics from the original dataset. This approach ensures that the overall number of questions and the breadth of content coverage are consistently maintained. This verification process guarantees that both the textual and visual components of the VisScience benchmark is a reliable and effective tool for evaluating the capabilities of MLLMs in scientific reasoning."}, {"title": "2.3 Data Analysis", "content": "We utilize statistical analysis to assess subject distributions and difficulty levels within the VisScience benchmark. Figure 3 presents a visual representation of the categorization of question within the VisScience benchmark. This illustration shows the distribution of questions dedicated to each subject area \u2013 mathematics, physics, and chemistry \u2013 and details the distribution across various difficulty levels, ranging from 1 to 5.\nSubject Distributions. To categorize each discipline into more detailed subjects, we first utilize LLM to segment the overall discipline into specific topics based on knowledge points and terminologies"}, {"title": "Difficulty Levels.", "content": "To classify the questions into distinct difficulty levels, we first utilize LLM for the initial sorting, and then conduct a manual verification. The questions within each discipline are stratified into five difficulty levels ranging from 1 to 5, defined as follows: Basic, Easy, Intermediate, Advanced, and Expert. Figure 3 shows the distribution of difficulty levels, providing a visual representation of the distribution of questions across different difficulty levels. Each discipline demonstrates a unique profile of topic distribution across the difficulty levels. For instance, in the field of mathematics, plane geometry is classified at the intermediate level, algebraic operations are positioned at the basic level, and functions and equations appears at the highest difficulty level, reflecting their various placement within educational curricula. In physics, mechanics dominates the introductory level, which belongs to a fundamental concept in physics education. Electromagnet is positioned at the highest difficulty level, demanding the application of various advanced knowledge points. In the discipline of chemistry, organic chemistry and chemical equilibrium represent the pinnacle of K12 chemical education, requiring deep conceptual understanding and the ability to apply knowledge to complex scenarios."}, {"title": "2.4 Comparison with Other Benchmarks", "content": "We compare the VisScience benchmark with 5 existing benchmarks, including MathVista [37], Math-Vision [45], CMMMU [52], ScienceQA [36], and SciBench [47].\nVisScience vs MathVista. MathVista is a comprehensive multi-modal benchmark for mathematical reasoning, comprising data from 28 existing datasets and 3 newly collected datasets. In MathVista, the majority of questions are annotated after collecting images, which results in a certain homogeneity within the data. In contrast, VisScience directly collects its questions from K12 education, featuring an average question length of 80.93 words. Such questions provide more contextual information, which facilitate a more thorough evaluation of the models' reasoning capabilities. Unlike MathVista that encompasses only seven subjects within mathematics, VisScience offers a far broader scope, including 22 distinct subjects across mathematics, physics, and chemistry. Furthermore, VisScience distinguishes itself by being a bilingual benchmark, including both Chinese and English versions of questions. This feature is particularly advantageous as it assesses MLLMs' capabilities in scientific reasoning across different languages.\nVisScience vs Math-Vision. Math-Vision is a mathematics benchmark derived from 19 competitions, covering 16 topics across 5 levels of difficulty. Different from Math-Vision that collected from competitions, VisScience spans a broader educational spectrum, incorporating a natural gradient of difficulty from elementary school to high school. Furthermore, VisScience extends beyond mathematics to include questions from physics and chemistry, significantly broadening its scope and applicability. While Math-Vision primarily focuses on the unique challenges of competitive environments, VisScience is grounded in real-world educational settings.\nVisScience vs CMMMU. CMMMU comprises 12,000 manually collected multi-modal questions from university exams, quizzes, and textbooks, which covers 6 core subjects and 30 specific fields. Similar to VisScience, CMMMU is a bilingual benchmark, offering questions in both Chinese and English. Within this dataset, only 1,601 questions are dedicated to the disciplines of mathematics, physics, and chemistry, accounting for only 13.34% of the total dataset. VisScience features a total of 3,000 questions, significantly outnumbering those in CMMMU dedicated to the same subjects. The questions in CMMMU are set at the university level, characterized by high difficulty, demanding that the model possesses substantial professional domain knowledge and expert-level reasoning abilities. In contrast, VisScience comes from K12 education, with a broader range of difficulty. This range allows VisScience to more comprehensively evaluate MLLMs' capabilities across different educational stages."}, {"title": "VisScience vs ScienceQA.", "content": "ScienceQA is a newly developed benchmark featuring approximately 21,000 multimodal multiple-choice questions across a variety of science topics. In the ScienceQA dataset, 30.8% of questions incorporate both image and text contexts, providing a multimodal benchmark to test MLLMs in scientific reasoning. The questions in ScienceQA have an average length of only 12.11 words. In contrast, VisScience also serves as a benchmark for evaluating the scientific reasoning abilities of MLLMs, but it typically features longer and more textually detailed questions. Specifically, the Chinese version of VisScience has an average question length of 162.85 words, providing a more comprehensive and intricate testing ground for evaluating the depth of detailed reasoning in MLLMs. Additionally, VisScience contains mathematical problems, further enriching the benchmark's scope by testing MLLMs on their mathematical problem solving capabilities alongside their scientific reasoning."}, {"title": "VisScience vs SciBench.", "content": "SciBench is a benchmark developed to evaluate the reasoning capabilities of LLMs in solving collegiate-level scientific problems within the domains of mathematics, chemistry, and physics. The majority of the data in SciBench focuses on assessing the scientific reasoning of LLMs, it only includes 177 problems that incorporate visual elements to evaluate the performance of MLLMs. In contrast, VisScience is primarily focused on multimodal scientific reasoning, covering similar subjects such as mathematics, chemistry, and physics. VisScience differentiates itself by offering a more comprehensive range of difficulty levels and subjects, making it a broader benchmark for assessing the capabilities of MLLMs in scientific reasoning."}, {"title": "3 Experiments", "content": "In this section, we conduct experiments to evaluate a variety of MLLMs using the VisScience benchmark. The evaluation encompasses both close-source and open-source models, enabling a comprehensive analysis of their effectiveness in scientific reasoning. Besides, we provide a detailed error analysis of the advanced model GPT-4o."}, {"title": "3.1 Experimental Setup", "content": "Models. We conduct our evaluation across a diverse array of models, including close-source text-only large language models (LLMs), close-source multi-modal large language models (MLLMs), and open-source MLLMs. This comprehensive assessment covers more than 20 models, which are listed below. The sources of models is reported in Appendix C.1.\n\u2022 Close-source text-only LLMs: ChatGPT [38], GPT-4 [1], Claude2 [4].\n\u2022 Close-source MLLMs: Gemini-1.0-Pro [42], Gemini-1.5-Pro [42], GPT-4o [40], Qwen-VL-Max [8], Qwen-VL-Plus [8], Claude3.5-Sonnet [5], Claude3-Opus [6], GLM-4V [2], and Step-1V [41].\n\u2022 Open-source MLLMs: mPLUG-Owl [49], LLaMA-Adapter-V2 [23], MiniCPM-Llama3-V2.5 [26], LLaVA-1.5 [30], DeepSeek-VL [34], ShareGPT4V [13], SPHINX-Plus [24], InternLM-XC2 [20], InternVL-1.2-Plus [15], InternVL-Chat-V1.5 [17], CogVLM [46], CogVLM2 [46], and GLM-4V-9B [25].\nEvaluation Details. The evaluation process is conducted through two steps: generation and judgment. During the generation phase, the models are tasked with producing responses based on a set of questions. For zero-shot setting, we directly prompt the models with these questions without any examples. For 2-shot Chain of Thought (CoT) setting, we provide the models with two relevant examples before they are prompted with the questions. For MLLMs, we supply the models with the textual questions and the corresponding image to obtain their responses. During the judgment phase, we utilize GPT-4o to evaluate the models' responses by comparing them with the standard answers to assess consistency. This phase involves calculating the accuracy across different subjects and levels. The prompts used in two phases is defined in Appendix C.2."}, {"title": "3.2 Experimental Results", "content": "Overall Results. Table 2 demonstrates the performance of several models on VisScience within the version of the Chinese language. Experimental results show that the close-source models achieves best performance on VisScience. Specifically, Claude3.5-Sonnet achieves an accuracy of 53.4% in mathematics, GPT-4o attains a 38.2% accuracy in physics, and Gemini-1.5-Pro accomplishes an accuracy of 47.0% in chemistry. Among open-source models, InternVL-1.2-Plus stands out, demonstrating robust capabilities across various scientific disciplines with accuracies of 30.1% in mathematics, 24.8% in physics, and 31.2% in chemistry. Despite this, there is a notable disparity in performance between close-source and open-source models, with close-source models generally exhibiting superior performance. The performance of InternVL-1.2-Plus, although trailing behind the advanced close-source models such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, showing significant potential for improvement. Notably, the performance in physics underscores unique challenges that necessitate targeted improvements in model training. This discipline often involves the interpretation of conceptual and numerical data, challenging the reasoning and computational abilities of MLLMs. As evidenced in Table 2, even advanced models like GPT-4o achieve relatively lower accuracies in physics compared to other disciplines. Results on VisScience with the version of the English language are provided in Appendix D.1.\nResults on Mathematics Across Different Subjects. The mathematical part of VisScience encompasses a wide range of subjects, including plane geometry, solid geometry, functions and equations, algebraic operations, probability and statistics, and combinatorial mathematics. Table 3 reports the comprehensive results across different mathematical subjects. It is evident that models like Claude3.5-Sonnet and GPT-4o in close-source MLLMs excel across multiple subjects, particularly in functions and equations, probability and statistics, and algebraic operations. Conversely, open-source models show a more varied performance with notable strengths in certain areas but generally lower scores compared to close-source models. For instance, InternVL-1.2-Plus and InternVL-Chat-V1.5 perform relatively well in plane geometry, and functions and equations. These detailed performance on different subjects provide valuable insights into the specific strengths and weaknesses of various MLLMs. Additionally, results on physics and chemistry across different subjects are presented in Appendix D.2 and Appendix D.3. Case studies illustrating correct responses by MLLMs can be found in Appendix E."}, {"title": "3.3 Error Analysis", "content": "To analyze the causes of errors in model responses, we meticulously review incorrect answers to identify common patterns. We specifically focus on the advanced MLLM, GPT-4o, to illustrate specific instances of errors and their distributions across the disciplines of mathematics, physics, and chemistry. Figure 4 demonstrates the distributions of these errors, categorizing them into several types such as reasoning error, knowledge error, calculation error, vision recognition error, and question misunderstood error. Notably, across all disciplines, reasoning errors are the most prevalent, indicating a challenge in model's ability to solve scientific problems that involve visual information. Specifically, reasoning errors account for 56.5% of the total errors in mathematics, 50.1% in physics, and 40.6% in chemistry, respectively. This is followed by knowledge error, which is particularly significant in chemistry, constituting 33.2% of the errors in that discipline. Similarly, knowledge error also represent the second most common error type in physics. However, knowledge error in mathematics is less prevalent, making up only 8.8% of the total errors. This indicates that while the model struggle with conceptual and fundamental principles in chemistry and physics, it demonstrate a better grasp of mathematical concepts. Vision recognition error is another significant type of error, accounting for 18.8% of the errors in mathematics, making it the second most prevalent error type in this discipline. This error category is also significant in physics and chemistry, where it constitutes 17.8% and 15.3% of the errors, respectively. This type of error highlights the challenges faced by the model in processing and understanding visual information. Furthermore, calculation error accounts for a small portion of the errors, especially in chemistry, indicating that the model excels particularly in handling numerical computations. Figure 5 shows some cases of reasoning error category in the disciplines of mathematics, physics, and chemistry."}, {"title": "4 Related Works", "content": ""}, {"title": "4.1 Multi-modal Reasoning Benchmarks", "content": "Recently, the evaluation of multi-modal large language models (MLLMs) [39, 42, 6, 8, 46, 31, 32] in various reasoning tasks has become increasingly crucial. So many benchmark datasets for these tasks span several categories are proposed like MME [22], MMMU [50], MMBench [33], MMStar [14], SEED-Bench [28], and CMMMU [52], which evaluate models' capabilities to apply"}, {"title": "4.2 Multi-modal Large Language Models", "content": "Recently, the success of large language models (LLMs) [21, 51, 1, 23, 25, 7] has spurred the ongoing development of multi-modal large language models (MLLMs). These MLLMs [32, 30, 46, 29, 19, 7] expand upon traditional LLM capabilities by integrating the ability to process and analyze both text and images. For instance, models like miniGPT [54] and InstructBLIP [19] attempt to utilize"}, {"title": "5 Conclusion", "content": "In this paper, we introduce a comprehensive benchmark, VisScience, to evaluate the capabilities of MLLMs in multi-modal scientific reasoning. VisScience comprises 3,000 questions across three disciplines of mathematics, physics, and chemistry, spanning 21 subjects and 5 difficulty levels. We conduct evaluations on VisScience with a total of 25 prominent models, including close-source and open-source models. Experimental results demonstrate that close-source MLLMs generally outperform open-source models, showing particularly more better capabilities in complex problem-solving and analytical reasoning. Notable models such as Claude3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro exhibit superior performance across three disciplines. Specifically, Claude3.5-Sonnet achieves an accuracy of 53.4% in mathematics, GPT-4o accomplishes a 38.2% accuracy in physics, and Gemini-1.5-Pro reaches an accuracy of 30.1% in chemistry. Although a gap remains between open-source models and the best-performing closed-source models, the open-source model like InternVL-1.2-Plus exhibits competitive advantages. For instance, InternVL-1.2-Plus outperforms models like Gemini-1.0-Pro across all three disciplines. By providing a comprehensive and challenging set of questions across three scientific disciplines, VisScience ensures a robust assessment of MLLMs' ability in scientific reasoning."}, {"title": "A Dataset Details", "content": ""}, {"title": "A.1 Question Length Distribution", "content": "We provide both Chinese and English versions of the VisScience benchmark. The Chinese version features an average of 162.85 words per question, with the longest question comprising 1,297 words. Answers in this version average 20.93 words, with the longest reaching 112 words. Conversely, the English version shows an average of 80.93 words per question, with the longest question spanning 418 words. Answers here average 12.3 words, with the most detailed answer containing 92 words. Figure 6 depicts the distribution of word counts, highlighting the diversity and complexity of questions."}, {"title": "A.2 Detailed Description of Subjects", "content": "VisScience consists of three disciplines: mathematics, physics, and chemistry. The mathematics section includes six subjects: algebraic operations, combinatorial mathematics, functions and equations, probability and statistics, plane geometry, and solid geometry. The physics section is composed of eight subjects: mechanics, optics, modern physics, mechanical motion, electromagnetism, vibrations and waves, comprehensive experiments and methods, and thermodynamics. The chemistry section includes seven subjects: chemical experiments, chemical reactions, inorganic chemistry, organic chemistry, electrochemistry, substance composition, and chemical equilibrium. A more detailed introduction of the above subjects is presented as follows:"}, {"title": "A.2.1 Mathematics", "content": "Algebraic Operations. Algebraic operations include the manipulation of algebraic expressions, such as addition, subtraction, multiplication, and division. They are fundamental for solving algebraic equations and inequalities and are widely applied across various fields of mathematics.\nCombinatorial Mathematics. Combinatorial mathematics studies the counting, arrangement, and combination of discrete structures, involving graph theory, number theory, and coding theory. It has significant applications in computer science, optimization, and probability theory.\nFunctions and Equations. Functions and equations are core parts of mathematics, dealing with relationships between variables and their representations. Functions are mappings between inputs and outputs, while equations are equalities concerning these mappings. Mastering knowledge of functions and equations is fundamental for solving many practical problems and is widely applied in engineering, physics, and economics.\nProbability and Statistics. Probability and statistics study the laws of random events and methods of data analysis, including probability distributions, statistical inference, and data analysis techniques. They have broad applications in scientific research, engineering, and economics."}, {"title": "Plane Geometry.", "content": "Plane geometry studies the shapes and figures in two-dimensional space, including points, lines, angles, and polygons. It is a fundamental part of mathematics education."}, {"title": "Solid Geometry.", "content": "Solid geometry involves the study of geometric shapes in three-dimensional space, including points, lines, surfaces, and polyhedra. It examines the properties, volumes, and surface areas of these geometric bodies and is foundational for architecture, physics, and engineering."}, {"title": "A.2.2 Physics", "content": "Mechanics. Mechanics studies the motion of objects and the forces acting upon them, including classical mechanics, quantum mechanics, and relativistic mechanics. It is the foundation of physics and is widely applied in engineering, astronomy, and materials science.\nOptics. Optics studies the properties of light and its interactions with matter, including reflection, refraction, interference, and diffraction. Optical technologies have broad applications in imaging, communication, and laser technology.\nModern Physics. Modern physics includes theories developed since the 20th century, such as quantum mechanics, relativity, and particle physics. These theories have expanded our understanding of the fundamental laws of nature.\nMechanical Motion. Mechanical motion studies the movement of objects under the influence of forces, including linear motion, rotational motion, and vibration. Understanding mechanical motion is fundamental for the design and analysis of mechanical systems.\nElectromagnetism. Electromagnetism studies the interactions between electric and magnetic fields, including electrostatics, magnetic fields, and electromagnetic waves. It is the basis of modern physics and electrical engineering.\nVibration and Waves. Vibration and waves study vibrating systems and wave phenomena, including sound waves, light waves, and electromagnetic waves. They have broad applications in communication, acoustics, and optical technologies.\nComprehensive Experiments and Methods. Comprehensive experiments and methods involve using various experimental techniques and methods in physics teaching and research. They include designing and conducting experiments to observe and analyze the effects of specific variables on outcomes. Through comprehensive experiments, students can grasp the complexities of scientific research, cultivate scientific reasoning abilities, and understand the meticulousness and uncertainties of experimental work.\nThermodynamics. Thermodynamics studies the processes of energy transformation and transfer, including the laws of thermodynamics, thermodynamic systems, phase transitions, and heat engines. Thermodynamics is a fundamental aspect of both physics and engineering, with broad applications in energy, environmental science, and materials science. By investigating the relationship between internal and external energy of objects, thermodynamics reveals the basic principles of energy conversion and transfer in nature, providing theoretical support for the development of modern industrial technology."}, {"title": "A.2.3 Chemistry", "content": "Chemical Experiment. Chemical experiments involve studying the properties and changes of substances through experimental methods. Students learn to design experiments, observe chemical reactions, collect and analyze data, and draw conclusions in chemical experiments. Chemical experiments play a crucial role in understanding chemical theories and applying chemical knowledge.\nChemical Reaction. Chemical reactions study the chemical changes between substances, including reaction types, mechanisms, and rates. Understanding chemical reactions is essential for predicting and controlling chemical processes, which have wide applications in pharmaceutical manufacturing, materials science, and environmental engineering."}, {"title": "B Dataset Case", "content": "The VisScience dataset consists of 3,000 carefully selected high-quality questions, evenly distributed across three disciplines: mathematics, physics, and chemistry, with each comprising 1,000 questions. Each discipline within VisScience encompasses several subjects: mathematics includes six subjects, physics contains eight subjects, and chemistry comprises seven subjects. To illustrate the diversity and depth of VisScience, we provide more examples sampled from each discipline. In mathematics, six subjects include algebraic operations, combinatorial mathematics, functions and equations, probability and statistics, plane geometry, and solid geometry are illustrated in Figure 7 to Figure 12. Figure 13 to Figure 20 demonstrate eight subjects within the physics section of VisScience, comprising mechanics, optics, modern physics, mechanical motion, electromagnetism, vibrations and waves, comprehensive experiments and methods, and thermodynamics. The chemistry section includes seven subjects: chemical experiments, chemical reactions, inorganic chemistry, organic chemistry, electrochemistry, substance composition, and chemical equilibrium, which are illustrated in Figure 21 to Figure 27."}, {"title": "C Evaluation Details", "content": ""}, {"title": "C.1 The Sources of Models", "content": "In Table 4, we present the sources of the models tested on VisScience."}, {"title": "C.2 Prompts", "content": "We introduce the prompts used to guide models in generating responses in Chain-of-Thought (CoT) settings and judging the LLMs' answers. The specific prompts can be found in Table 5."}, {"title": "D More Experimental Results", "content": ""}, {"title": "D.1 Results on VisScience in English Version", "content": "Table 6 reports a comprehensive comparison of various models on the VisScience benchmark in the English version. The benchmark evaluates performance across three disciplines: mathematics, physics, and chemistry. Among close-source models, GPT-4o demonstrates the highest performance across two disciplines, achieving an accuracy of 53.6% in mathematics and 42.7% in physics. However, Claude3.5-Sonnet surpasses GPT-4o in chemistry with a higher accuracy of 43.6%. Open-source models generally show lower performance compared to close-source counterparts. Notably,"}, {"title": "D.2 Results on Physics Across Different Subjects", "content": "Table 7 presents a detailed analysis of various models on VisScience across different subjects within the physics section, which includes mechanics, electromagnetism, thermodynamics, comprehensive experiments and methods, optics, vibration and waves, modern physics, and mechanical motion. The table highlights that while GPT-4o exhibits the top performance on the entire physics discipline, the best performance in individual subjects varies notably. For instances, Claude3.5-Sonnet excels specifically in modern physics with an accuracy of 66.67%, significantly surpassing other close-source models in this area. This variation in performance by subject underscores the specialized capabilities of different models. Moreover, this detailed analysis provides more insights, emphasizing the need for targeted improvements to achieve balanced performance across all physics subjects."}, {"title": "D.3 Results on Chemistry Across Different Subjects", "content": "Table 8 presents a nuanced view of the performance of various models across different subjects within the chemistry discipline of the VisScience benchmark. The chemistry discipline includes chemical experiment, chemical reaction, inorganic chemistry, electrochemistry, organic chemistry, chemical equilibrium, and substance composition. Notably, Gemini-1.5-Pro stands out among close-source models, excelling across the entire chemistry discipline. It demonstrates particular prowess in organic chemistry and substance composition, achieving impressive accuracies of 57.02% and 61.16%, respectively. Additionally, Qwen-VL-Max leads in chemical experiment and inorganic chemistry, achieving the highest accuracies of 46.28% and 51.94%, respectively. Open-source models demonstrate a range of performances, with InternVL-1.2-Plus leading this group. It achieves the highest open-source accuracy in nearly all subjects. This comprehensive review of model performances within the chemistry section of the VisScience benchmark highlights the need to enhance MLLMs' capabilities in scientific domains, ensuring models are both accurate and adaptable across various disciplines."}, {"title": "E Case Study", "content": "The VisScience dataset includes three disciplines: mathematics, physics, and chemistry. The mathematical section comprises 6 subjects, the chemistry section contains 7 subjects, and the physics"}, {"title": "Algebraic Operations.", "content": "Algebraic operations include the manipulation of algebraic expressions, such as addition, subtraction, multiplication, and division. They are fundamental for solving algebraic equations and inequalities and are widely applied across various fields of mathematics."}, {"title": "Combinatorial Mathematics.", "content": "Combinatorial mathematics studies the counting, arrangement, and combination of discrete structures, involving graph theory, number theory, and coding theory. It has significant applications in computer science, optimization, and probability theory."}, {"title": "Functions and Equations.", "content": "Functions and equations are core parts of mathematics, dealing with relationships between variables and their representations. Functions are mappings between inputs and outputs, while equations are equalities concerning these mappings. Mastering knowledge of functions and equations is fundamental for solving many practical problems and is widely applied in engineering, physics, and economics."}, {"title": "Probability and Statistics.", "content": "Probability and statistics study the laws of random events and methods of data analysis, including probability distributions, statistical inference, and data analysis techniques. They have broad applications in scientific research, engineering, and economics."}, {"title": "Plane Geometry.", "content": "Plane geometry studies the shapes and figures in two-dimensional space, including points, lines, angles, and polygons. It is a fundamental part of mathematics education."}, {"title": "Solid Geometry.", "content": "Solid geometry involves the study of geometric shapes in three-dimensional space, including points, lines, surfaces, and polyhedra. It examines the properties, volumes, and surface areas of these geometric bodies and is foundational for architecture, physics, and engineering."}, {"title": "Mechanics.", "content": "Mechanics studies the motion of objects and the forces acting upon them, including classical mechanics, quantum mechanics, and relativistic mechanics. It is the foundation of physics and is widely applied in engineering, astronomy, and materials science."}, {"title": "Optics.", "content": "Optics studies the properties of light and its interactions with matter, including reflection, refraction, interference, and diffraction. Optical technologies have broad applications in imaging, communication, and laser technology."}, {"title": "Modern Physics.", "content": "Modern physics includes theories developed since the 20th century, such as quantum mechanics, relativity, and particle physics. These theories have expanded our understanding of the fundamental laws of nature."}, {"title": "Mechanical Motion.", "content": "Mechanical motion studies the movement of objects under the influence of forces, including linear motion, rotational motion, and vibration. Understanding mechanical motion is fundamental for the design and analysis of mechanical systems."}, {"title": "Electromagnetism.", "content": "Electromagnetism studies the interactions between electric and magnetic fields, including electrostatics, magnetic fields, and electromagnetic waves. It is the basis of modern physics and electrical engineering."}]}