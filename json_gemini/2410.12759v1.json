{"title": "Unitary Multi-Margin BERT\nfor Robust Natural Language Processing", "authors": ["Hao-Yuan Chang", "Kang L. Wang"], "abstract": "Recent developments in adversarial attacks on deep learning leave many mission-critical natural language processing\n(NLP) systems at risk of exploitation. To address the lack of computationally efficient adversarial defense methods, this paper reports a\nnovel, universal technique that drastically improves the robustness of Bidirectional Encoder Representations from Transformers (BERT)\nby combining the unitary weights with the multi-margin loss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin BERT (UniBERT), boosts post-attack classification\naccuracies significantly by 5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore, the pre-attack and\npost-attack accuracy tradeoff can be adjusted via a single scalar parameter to best fit the design requirements for the target\napplications.", "sections": [{"title": "INTRODUCTION", "content": "The objective of natural language processing (NLP) is to\nanalyze human-generated texts with machines (e.g., au-\ntomatic text classification). However, deep neural networks\nare prone to adversarial attacks. Without proper defense\nmechanisms in place, hackers can easily sabotage the pre-\ndiction output of neural nets by maliciously perturbing the\ntextual input, controlling the prediction in some cases. Such\nvulnerability prevents deep neural nets to be fully trusted\nin mission-critical applications such as transportation, na-\ntional security, and defense. In this paper, we aim to design\na robust machine learning model that can categorize the\ntopic of a paragraph, understand the causal relationship\nbetween sentences, and infer the writer's sentiment while\nunder adversarial attacks. To achieve this aim, we start by\nimproving the state-of-the-art neural network in NLP: the\nBidirectional Encoder Representations from Transformers\n(BERT)-a network structure designed specifically for un-\nderstanding languages [1].\nThe BERT encodes words with real vectors, which are\ncombined to embed the meaning of a sentence. To clarify\nour terminology used in this paper, the activation value of\na neuron is defined as the sum of the products between the\ninputs and the synaptic weights, and a neural representation\nrefers to the distribution of activations for a particular class\nof samples in a text classification task. In BERT, the subse-\nquent attention layers transform the sentence embeddings\ninto a succinct neural representation. For text classification\nparticularly, the last two fully-connected layers serve as a\nclassifier that categorizes the neural representations, and we\ncall the activations of the last neural layer \"logits.\" During\ntraining, logits are normalized with the softmax function\nbefore passing to the cross-entropy loss in standard BERT\nmodels. During inference, BERTs use the argmax function\nto identify the corresponding class with the largest logit, re-\nsulting in an integer class label for the prediction. Although\nBERTs deliver excellent prediction accuracies, they can be\neasily attacked to produce incorrect outputs.\nWe define adversarial attacks as follows: an algorithm\nmaliciously injects small perturbations into a neural net-\nwork to alter its output prediction. The attackers aim to\ncreate just enough perturbation to shift the neural response\nacross the decision boundary without appearing suspicious\nto humans [2]. Decision boundaries are the demarcation\nof data distributions for different classes in the activation\nspace. The are many ways to devise small perturbations.\nIntroducing typos or swapping synonyms are two popular\nmethods, which will explain the detailed procedures in Sect.\n5.2 later.\nOur innovations can be summarized into two integral\nparts as follows. Firstly, we discover that by switching the\ncross-entropy loss with the multi-margin loss as defined in\nSect. 3.1 during the finetuning portion of the neural network\ntraining, our version of BERT forces the neural representa-\ntion of different classes to be more distinct as we will show\nin Sect. 6.4. Better separation of the neural representations\nimproves adversarial robustness, which is measured by the\nprediction accuracy under adversarial attacks (i.e., the post-\nattack accuracy) [3].\nSecondly, deep neural nets have many sequential layers,\nand each layer has its own set of weight matrices. Normally,\nthese matrices are unconstrained and free to take on any\nvalues, so we call them \u201cnon-unitary weights.\u201d The problem\nwith non-unitary weights is that they sometimes amplify the\ninjected perturbation by accident, a vulnerability that the\nattackers can leverage to sabotage the prediction outputs.\nThe current state-of-the-art BERT models have 12 to 24\nattention layers with non-unitary weights, allowing small\nnoises to be repeatedly amplified into large deviations. In\naddition to the aforementioned multi-margin loss, we pro-\npose another novel technique that uses unitary matrices to"}, {"title": "RELATED WORKS", "content": "To date, the techniques for defending against adversarial\nattacks on BERT can be grouped into two categories:\n1) Adversarial Training with Data Augmentation.\nThe first technique is to train the model with ad-\nditional adversarial and augmented data. The de-\nsigner of the neural network needs to anticipate\nthe attack recipes that the hacker will use, gener-\nate adversarial samples accordingly, and train the\nnetwork with the generated samples to prevent a\nfuture attack. The drawback of this technique is that\nit requires the designers to predict the attack meth-\nods used by the adversaries to create appropriate\ncoverage in the sample space. Furthermore, training\nwill take much longer if we desire full coverage for\nall types of adversarial attacks. Data augmentation\nis commonly used together with adversarial train-\ning, which generates additional training samples by\nswapping words with their synonyms or by inter-\npolating between the existing sentence embedding\nvectors. One of the best is to combine adversarial\ntraining and data augmentation as seen in defense\nmodels such as AMDA [4] and MRAT [5] (see Sect.\n6.2 for details). For brevity, we will use the term\n\"adversarial training\" to refer to defense models\nthat use adversarial training and data augmentation\ntogether to achieve state-of-the-art robustness.\n2) Regularization. The second method of defense\nagainst adversarial attacks is by adding a regular-\nization term in the loss function to reduce model\ncomplexity in particular ways. Normally, overfitting\nrefers to the phenomenon in which the deep neu-\nral network learns all the artifacts in the training\ndataset, resulting in poor generalization accuracy\nwhen the model encounters new data points. We\ncan view adversarial attacks as the generation of\nnew, unseen data points outside of the training and\nthe test datasets. In this sense, the original BERT\noverfits the training and test datasets, leading to\npoor accuracy on the adversarial samples. A way\nto alleviate this special type of overfitting is by\nregularizing the network to reduce the number of\ndecision regions that contain the training data [6].\nAs another example, InfoBERT is a model that uses\nan information bottleneck as a regularizer to limit\nmodel complexity for improving adversarial robust-\nness [7]. Disadvantages of regularization-based de-\nfense include high computational costs and added\narchitectural complications.\nTo our knowledge, the multi-margin loss has not been\nstudied for improving the robustness of NLP applications.\nOur study is the first time when the multi-margin loss\nand unitarity are used together to improve the adversarial\nrobustness in NLP applications. The improvement is quite\ndramatic as shown in the following sections."}, {"title": "THEORY", "content": ""}, {"title": "Multi-margin Loss Increases Robustness", "content": "Unlike most loss functions that only quantify the distance\nbetween the desired and the current output, the multi-\nmargin loss provides an additional margin of safety between\nthe logits and the decision boundary. The multi-margin loss\nis defined as:\nL=1/n \u03a3_{i=0}^{n-1} \u03a3_{j=0}^{n_c-1} max(y_{i,j} + \u03b5 - y_{i,correct}, 0), \nwhere n is the batch size, nc is the number of classes,\nyi,correct is the logit of the neuron corresponding to the\ncorrect answer, yi,j is the logit for other neurons; 8 is the\nmargin parameter, setting a gap between the logits for\ndifferent classes as discussed below. All variables are scalars.\nTo provide some intuition for the multi-margin loss, we\nillustrate how it works in UniBERT with a binary sentiment\nanalysis example in Fig. 1 below. The multi-margin loss is\nillustrated in the right panel of Fig. 1. For each data sample\n(indexed by i) in (1), the loss is the larger of the two terms\nbetween yi,j + \u025b - yi,correct and zero, which correspond to\nthe two linear segments of the graph in Fig. 1. The margin\nparameter (8) determines the intercept between the two\nsegments. Our objective is to train the network such that\nthe logit for the correct class (yi,correct) exceeds the logit\nof the incorrect class (yi,j) by the margin (\u03b5). In a support"}, {"title": "Unitarity Confines Perturbation", "content": "In an adversarial attack, the input perturbation needs to be\nsubtle to avoid detection. As mentioned previously, non-\nunitary weights can accidentally amplify the small perturba-\ntion, making the network vulnerable to adversarial attacks.\nSynaptic weights in a neural net are arranged in a matrix\nform. In the theorem below, we prove that a unitary weight\nmatrix preserves the amount of the perturbation to the\noriginal sentence embedding, I:\nTheorem 1. A unitary matrix (U) maintains the Euclidean\ndistance between the original (x) and the perturbed\nvector (x') after the linear transformation.\nProof: ||Ux \u2212 Ux'||=\u221a[U(x\u2212x')]^T[U(x\u2212x')] = \u221a(x\u2212x')^T[U^TU](x\u2212x')=||x-x'||. \nThe right-hand side of the above equation is the L2\nnorm of the input perturbation, which will need to be small\nto be discreet. The left-hand side measures the amount of\nchange in the output after the unitary transform. As shown\nin Theorem 1, unitarity guarantees that small perturbations\nremain small after the unitary transformation. In its non-\nunitary counterparts, the neural layers may accidentally\namplify the perturbations, in which the logits can move\nacross the decision boundary and result in a classification\nerror. Non-unitary weights may suppress the perturbations\nas well; however, our goal is to eliminate any slight chance\nof perturbation amplification with unitary weights.\nIn our chosen implementation of the UniBERT, we im-\npose unitary constraints on the selected layers to stabilize\nthe injected perturbations across the neural network (see\nSect. 4.2 for design details and discussion regarding non-\nlinearity in the network). Sect. 6.5 confirms this stabilization\neffect of unitary weights by comparing the neural represen-\ntations of the original and perturbed sentences across the\nnetwork. Combining the multi-margin loss and the unitary\nweights, our UniBERT improves the state-of-the-art post-\nattack accuracies as we will show in Sect. 6.2."}, {"title": "MODEL", "content": ""}, {"title": "Network Architecture & Training", "content": "Our UniBERT is an enhanced BERT that uses multi-margin\nloss and unitarity as described above to increase the neural\nnetwork's prediction accuracy under adversarial attacks.\nThis subsection explains the implementation details for our\nUniBERT neural architecture, which is illustrated in Fig. 2\nbelow. Compared to the original BERT, our modifications\nare:\n1) During finetuning, we replace the softmax and\ncross-entropy loss with just the multi-margin loss\non its own (see Fig. 2, bottom). We do not modify\nthe softmax or the cross-entropy loss for pretraining.\nThe reason for this is that the multi-margin loss\nworks best for classification tasks; thus, it is applied\nduring finetuning only and not during pretraining,\nwhich is a masked language modeling task.\n2) During both pretraining and finetuning, we force\ncertain layers to have unitary weights, and the se-\nlected layers are circled with dash lines in Fig. 2,\ntop-right. The selection of the unitary layers will be\nexplained below in Sect. 4.2. The exact procedure to\nensure their weight matrices are unitary is described\nin the next subsection."}, {"title": "Selection of the Unitary Layers", "content": "The unitary layers are selected as follows, and Table 1 below\nlists all synaptic weights in UniBERT along with their uni-\ntarity. Firstly, only square matrices can be unitary in terms of\na strict mathematical definition; hence, not all layers can be\nconverted to unitary. We decide to apply unitary constraints\non all the square weights in UniBERT to maximize the effect\nof the total unitarity (with one exception to be explained\nnext). Secondly, we keep Wc in the classifier layer non-\nunitary to increase the pre-attack accuracy (i.e., prediction\naccuracy without adversarial attacks), even though it is also\nsquare and thus available for the unitary conversion. Wc\nis shown near the bottom of Fig. 2 and listed in Table 1. In\ngeneral, unitarity reduces model complexity in exchange for\nhigher signal stability [8]. This is the reason why with uni-\ntary constraints, the pre-attack accuracies decrease (usually\nby a small amount), and the post-attack accuracies increase. As a consequence, we decide to leave the classifier layer non-unitary for a slightly higher pre-attack accuracy.\nOur UniBERT is only partially unitary as shown in Table\n1. It has a total of 48 unitary weights and 29 non-unitary\nweights (Table 1, last two columns). Our selection of unitary\nlayers forces unitary constraints on 62.3% of all weight\nmatrices; i.e., 48 / (48 + 29). Likewise, there are nonlinear\nactivation functions in the network necessary to achieve\nthe required model complexity. The nonlinearities are vital\nto avoid condensing the transfer function to one linear\ntransformation between the input and the output. While\nit is impossible to have a purely unitary neural net, our\nUniBERT's partial unitarity is sufficient to deter adversarial\nattacks, and surprisingly, it outperforms many state-of-the-\nart defense models in post-attack accuracies as we will\ndemonstrate in Sect. 6.2."}, {"title": "Unitary Constraints", "content": "The way we convert the non-unitary weights to their closest\nunitary projections is by QR factorization, a method to\ndecompose any matrix into unitary and non-unitary parts\nas explained as follows [9]:\nW=QR,\nwhere W is a non-unitary square matrix, Q is a unitary\nmatrix, and R is an upper triangular matrix. We extract the\nsigns of the diagonal elements in R and construct S:\nS = diag(sign (R))"}, {"title": "EXPERIMENTS", "content": "Here we report the datasets used in pretraining and finetun-\ning to benchmark the differences between BERT and UniB-\nERT. First, we describe the dataset used for pretraining-the\nBook Corpus (bookcorpus), an unlabeled dataset containing\n74 million sentences from eleven thousand books [10]. We\nseparate this dataset into two subsets, one for training (95%)\nand one for testing (5%). Then for finetuning, we selected\nthree different datasets for a comprehensive evaluation\ncovering multilabel categorization, language inference, and\nsentiment analysis; respectively, they are listed as follows:\n1) AG's News (ag_news) is a dataset for news clas-\nsification, and the goal is to classify the articles\ninto four categories including world news, business\nnews, science & technology, and sports [11].\n2) Stanford Natural Language Inference Corpus (snli)\naims to train machine learning systems that can\nidentify the relationship between a pair of sen-\ntences [12]. There are three possible classification\noutcomes: entailment, contradiction, or neutral.\n3) Yelp Reviews Polarity (yelp) is a text sentiment\nanalysis dataset constructed by collecting reviews\nfrom Yelp.com [11]. The label is either positive or\nnegative.\nWe highlight the key features of the dataset statistics in\nAppendix C."}, {"title": "Adversarial Attacks", "content": "Attackers may swap the words in a sentence with their syn-\nonyms to cause a misclassification, and we refer to this type\nof attack as synonym-based attacks [13], [14]. Another type\nof adversarial attack is created by introducing typographical\nerrors in the sentences [15], [16]. In this case, although the\ninjected typos appear benevolent to the readers, they can\nmanipulate BERT to produce the wrong results because\nnon-unitary weights may amplify the small perturbations.\nBelow lists the NLP attacks we evaluate in this study. We\nselect three distinct types of attacks for a comprehensive\nadversarial robustness evaluation:\n1) Textbugger randomly introduces character inser-\ntion, deletion, swap, and substitution to modify\nBERT's prediction [16]. It is considered a typo-\ngraphic attack.\n2) Textfooler finds candidate adversarial samples by\nswapping important words with their synonyms;\nsynonyms are found by searching for the closest\nwords in the word embedding space [14], [17]. In\nparticular, the counter-fitted word embeddings are\nused for the synonym search.\n3) Probability Weighted Word Saliency (or PWWS)\n[18] swaps words in a sentence with their syn-\nonyms as defined in the human-labeled WordNet\n[19] database. In contrast to Textfooler, which finds\nsynonyms using a distance metric with the word\nembeddings learned automatically by a neural net,\nPWWS relies on a thesaurus constructed explicitly\nby human workers.\nTextbugger and Textfooler may not preserve a sentence's\nmeaning; therefore, to create a valid replacement sentence\nto commence the attack, we need to check for the semantic\nsimilarity between the generated adversarial sample and the\noriginal sentence. We use the Universal Sentence Encoder\n[20] to measure the cosine distance between the original\nand the perturbed sentences and use the default similarity\nthreshold in the TextAttack [21] framework to reject any ad-versarial example that changes the meaning of the sentence.\nIn contrast, because PWWS uses the human-labeled Word-Net synonym database to generate high-fidelity samples\nas described before; hence, we do not perform additional\nsafeguarding on the generated samples.\nFurthermore, there are two ways to carry out adversarial\nattacks on a neural network: targeted attacks vs. static at-tacks [4]. Targeted attacks generate a new set of adversarial\nsamples for each neural network while static attacks use\nthe same set of adversarial samples to evaluate all neural\nnetwork architectures (e.g., BERT, UniBERT, ...etc.). We\nuse the tougher targeted attacks to evaluate the model's\nrobustness. In detail, we randomly select 1000 data samples\nfrom the test set and allow the attack algorithm to make an\nunlimited number of attempts to the model until it can no\nlonger generate new permutations that meet the similarity\ncriteria or have exhausted all synonyms. The number of\nattempts the attacker is allowed to make is called the query\nbudget. The post-attack accuracy is calculated as the ratio of\nthe samples (out of 1000) that survive the series of attacks\nwith an unlimited query budget and still produce the correct\nclassification results, measuring the classification accuracy\nof the neural network under adversarial attack. Our evalua-tion methodology is the toughest in the literature, reporting\nthe lowest possible post-attack accuracies. The pre-attack\naccuracy is measured by computing the ratio of the correctly\nclassified sample out of the 1000 test samples without any\nattack."}, {"title": "RESULT & DISCUSSION", "content": ""}, {"title": "Our UniBERT vs. Baseline Models", "content": "So far, we have only introduced BERT and UniBERT, but\nthere are many versions of BERT for various purposes.\nThese neural network models are not specifically designed\nto defend against adversarial attacks; as a result, we call\nthem the baseline models. The goal is to validate the need\nfor a novel defense technique because there would be no\nreason to modify the existing architectures if the baseline\nmodels were already resilient to attacks. We compare our\nproposed UniBERT against four other versions of BERT,\nwhich are listed here:\n1) Bidirectional Encoder Representations from\nTransformers (BERT) [1]\u2014This is the original\narchitecture that our work is based on.\n2) A Robustly Optimized BERT Pretraining Approach\n(ROBERTa) [22]\u2014This is a BERT with enhanced pre-\ntraining to improve its accuracy. We use this model\nto verify that a better pretraining procedure alone\ncannot deter adversarial attacks.\n3) A Lite BERT (ALBERT) [23]-This model reduces\nthe number of parameters up to 18 times by using\nfactorized word embedding and parameter sharing.\nWe select this model to show that a simple model\ncomplexity reduction is not enough to prevent ad-\nversarial attacks.\n4) A distilled version of BERT (DistilBERT) [24]\u2014\nThis model reduces the number of parameters by\n40% by using knowledge distillation. Same as with\nALBERT, we want to show that reducing the model\ncomplexity via transfer learning to a smaller model\nwill not improve robustness.\nTo conduct a fair comparison for the classification ac-\ncuracy, we download the pretrained weights for the four\nbaseline models above from the Hugging Face repository\n[25] and finetune them further using the procedure for\nour selected datasets. Namely, the classification datasets\ninclude news categorization (ag_news), natural language\ninference (snli), and sentiment analysis (yelp), which we\nhave documented in Sect. 5.1 above. After finetuning, we\nfirst evaluate their pre-attack classification accuracies and\nthen measure their post-attack classification accuracies after\nperforming adversarial attacks on the models. The three\nattack recipes are the PWWS, Textbugger, and Textfooler\nadversarial attacks on the textual input, which cover a wide\nrange of typographical and synonym-based NLP attacks.\nDetails of the attack recipes are discussed in Sect. 5.2 before.\nThe results are given in Table 2, which compares the pre-\nattack and post-attack accuracies of our proposed UniBERT\nwith the baseline models (i.e., BERT, ROBERTa, ALBERT, and\nDistilBERT). Our observations regarding the four baseline\nmodels are as follows: RoBERTa generally delivers slightly\nbetter pre- and post-attack accuracies compared to BERT\nbecause ROBERTa's extensive pretraining creates a better\nlanguage model. On the other hand, both parameter re-\nduction models (ALBERT and DistilBERT) have the worse\nrobustness against adversarial attacks. We conjecture that\nthe lack of model complexity prohibits them to generalize\nto out-of-distribution samples due to their inferior language\nmodels.\nUniBERT delivers double-digit improvements in post-\nattack accuracies across all combinations of NLP tasks\nand attacks over the baseline models. The amounts of im-\nprovements in post-attack accuracies vary between 13.7%\n(UniBERT vs. RoBERTa for snli under Textbugger) and\n74.5% (UniBERT vs. ALBERT for yelp under Textfooler).\nThese data confirm our hypothesis that the combination of\nthe multi-margin loss and unitarity improves robustness.\nNevertheless, as a side effect, the unitary constraint also\nreduces model complexity. As a review of Sect. 4.2 earlier,"}, {"title": "Our UniBERT vs. Defense Models", "content": "We compare UniBERT's performance with the state-of-the-\nart defense techniques (e.g., adversarial training and reg-\nularization as outlined previously in Sect. 2), which we\nrefer to as the \"defense models.\" We select the following\nrepresentative defense models for comparison:\n1) Adversarial and Mixup Data Augmentation\n(AMDA-Tmix & AMDA-Smix) [4] In this\ntechnique, adversarial examples are generated by\nassuming a specific attack recipe, and extra training\ndata are created by linearly interpolating between\nthe neural representations for different classes.\nThey designate their model with the \"Tmix\" and\n\"Smix\" postfixes to denote the location where the\nneural representations are taken from in BERT.\n2) Mixup Regularized Adversarial Training (MRAT\n& MRAT+) [5]-This technique uses all the steps\ndetailed in AMDA above. In addition, it adds a reg-\nularization term in the loss function (i.e., a penalty\nfor any data point that is too different from the\noriginal), ensuring that the augmented data will\nfollow the original data distribution. They claim that\nthis term will prevent the augmented dataset from\ndegrading the pre-attack accuracy. Like AMDA, the\nkey ingredient for improving robustness still comes\nfrom adversarial training with data augmentation.\nThe MRAT+ variant adds data augmentation to the\noriginal data. For example, it may swap some words\nin the sentence with their synonyms to generate new\nsamples.\n3) Information Bottleneck on BERT (InfoBERT) [7]\u2014\nThis technique reduces model complexity by using\nan information bottleneck. The bottleneck is im-\nplemented as two regularization terms in the loss\nfunction: one is to maximize the prediction accuracy\nwhile minimizing the mutual information between\nthe input and the internal representation. Another is\nto identify word embeddings that are less affected\nby the input perturbation and force the neural net\nto utilize these words more in its decision process.\nInfoBERT is computationally expensive due to the\nneed to calculate mutual information. It also re-\nquires more hyperparameters, which makes model\noptimization cumbersome.\nAs explained in Sect. 5.2 previously, there are two ways\nto measure the post-attack classification accuracy: with\ntargeted attacks or with static attacks. Thus, we need to\nmake sure that we use the same evaluation method to\ncompare the adversarial robustness of different models. The\nauthors of AMDA and MRAT have published the perfor-\nmance of their models using targeted attacks, and their\ndata are reproduced in the table below for comparison.\nThe creator of InfoBERT uses static attacks in their paper;\nthus, we rerun their simulations with the targeted attacks\nfor a fair comparison. The adversarial performance of our\nUniBERT is always evaluated with the targeted attacks. We\nfirst study the performance of the state-of-the-art defense\nmodels by comparing the data between Table 2 and Table 3.\nDefense models utilizing adversarial training (i.e., AMDA-\nTmix/Smix and MRAT/+) deliver superior adversarial per-\nformance over BERT and RoBERTa. On the other hand,\nregularization-based models such as InfoBERT only deliver\nmarginal improvements over the classic BERT model under\nthe targeted attacks. Moreover, InfoBERT's robustness is\nslightly higher than RoBERTa's in natural language infer-\nence (snli), but it's worse in news categorization (ag_news)\nand sentiment analysis (yelp).\nOur UniBERT outperforms other state-of-the-art defense\nmodels in post-attack accuracies across all tasks and attack\nrecipes. The enhancements in post-attack accuracies range\nfrom 5.3% (UniBERT vs. MRAT+ for snli under Textfooler)\nto 73.8% (UniBERT vs. InfoBERT for yelp under Textfooler).\nMore specifically, in terms of post-attack accuracies, our\nUniBERT is 15.4% to 31.0% better in ag_news and 5.3%\nto 8.8% better in snli compared to adversarial training.\nWe explain the improvements in adversarial robustness be-\ntween UniBERT and adversarial training as follows: Under\na targeted attack with an unlimited query budget, there is a\nwide variety of perturbations an attack recipe can produce\nfor a given sentence. Although adversarial training captures\na decent portion of these variations with augmented data,\nit is impossible to be comprehensive and cover the entire\nsearch space. Instead of training adversarial samples by\nbrute force, our UniBERT mitigates the vulnerability funda-\nmentally by increasing the neural representations' distances\nto the decision boundaries (as explained later in Sect. 6.4)\nwhile confining the injected perturbations (Sect. 6.5).\nIn UniBERT, the combination of these two innovations\n(i.e., multi-margin loss and unitarity) not only improves\nthe robustness beyond the state-of-the-art but also provides\na model that is attack-agnostic. The robustness enhance-\nment is consistent across various attack recipes (i.e., PWWS,\nTextbugger, and Textfooler) as shown in the post-attack\naccuracy columns in Table 3. For UniBERT, the largest\nvariation in post-attack accuracies is between 3.1% and\n4.3% across different attack recipes. Other models are less\nconsistent when compared across attacks. As an example,\nInfoBERT has a 33.1% variation in post-attack accuracies\nacross different attacks for ag_news and AMDA-Smix has\n18.7%, but for snli, their variations are much smaller. Such\ninconsistency will cause the designers more time in selecting"}, {"title": "Ablation Study", "content": "To separate the contribution of the multi-margin loss and the\nunitary weights, we perform an ablation study for UniBERT\non the yelp task. Table 4 reports the performance of the\nfollowing four model trims:\n1) BERT is the baseline with no modification.\n2) BERT_unitarity is BERT with the unitary con-\nstraints alone.\n3) BERT_multi_margin is BERT with the multi-margin\nloss instead of the cross-entropy loss without any\nunitary constraint.\n4) UniBERT has both the unitary weights and the\nmulti-margin loss replacement.\nBERT_unitarity's post-attack accuracies are not signifi-\ncantly different from BERT (Table 4, first two rows). The\nreason is the following: A query budget is defined as the\nnumber of attempts the attacker is allowed to make before\ngiving up. In the case of adversarial attacks with infinite\nquery budgets, the attackers are free to try as many permu-\ntations as possible until they exhausted all combinations.\nBecause the post-attack accuracies reported here are mea-\nsured with attacks with infinite query budgets, the attacker\nhas a large search space to find one adversarial example\nthat changes the prediction outcome, and it only needs one\nexample for the attack to be considered as successful. If\nthe original sentence's neural representation is close to the\ndecision boundary in any direction in the high-dimensional\nembedding space, our aggressive attack procedure will find\nan adversarial sample in its proximity with high probability.\nConsequently, we do not see improvement with unitarity\nalone.\nBERT_multi_margin delivers a significant improvement\nin the post-attack accuracy but also reduces the pre-attack\naccuracy. As we discussed earlier, the multi-margin loss\nincreases the distance to the decision boundaries, making\nit harder for the injected perturbation to cause a misclas-\nsification. Thus, we observe a significant increase (47.3%-\n59.1%) in post-attack accuracies. Even with increased dis-\ntance to the decision boundaries, the non-unitary weights\nused in BERT_multi_margin can still amplify injected per-\nturbations. By adding the unitary constraints on top of\nBERT_multi_margin, UniBERT raises both the pre-attack\nand post-attack accuracies considerably. Comparing to\nBERT_multi_margin, UniBERT boost the post-attack accura-\ncies by an additional 12.3% to 21.1% via adding the unitary\nconstraints on the synaptic weights. With this ablation study,\nwe conclude that it is insufficient to have unitarity or multi-\nmargin loss alone; the multi-margin loss and unitarity need\nto be used together to achieve optimal results."}, {"title": "Effect of the Multi-margin Loss", "content": "Previously in Sect 3.1, we hypothesize that the multi-margin\nloss makes the neural representations more distinct. To\nconfirm this, we record the logits (i.e., the neural represen-\ntations at the last layer) in our UniBERT for 950 correctly\nlabeled data samples with the yelp binary classification task,\ncompared with the ones in BERT. With the yelp dataset,\nour model is asked to categorize the input text into a\npositive or a negative review (i.e., two classes). Samples\nwith the wrong prediction outcome are ignored because the\nattack algorithms skip any misclassified sentences, so they\nwill never be attacked and will not affect the robustness\nevaluation. For each data sample, the shortest distance to\nthe decision boundary (d) is calculated. Then, we define a\nnew quantity called normalized distance, ds, to measure the\nsize of the buffer for absorbing the adversarial perturbation\nto prevent misclassification:\nds=Mean(d)/Var(d),\nwhere d is the shortest distance of a logit to the decision\nboundary. Mean() and Var() represent the mean and vari-\nance over all data samples, respectively. The purpose of\nnormalizing with the variance is to compensate for the\nsimple scaling of the logits. We invented this new metric\n(ds) because it is impossible to calculate the Mahalanobis\ndistance (dm) as defined in (2) for a real-world dataset, in\nwhich the covariance matrices (S) are different for each class.\nWe compare the d, measured in our UniBERT with the\none in BERT in Table 5. UniBERT's logits are much further\naway from the decision boundary compared to BERT's (i.e.,"}, {"title": "Propagation of Perturbation", "content": "With Theorem 1, we aim to use unitarity to regulate the\nmagnitude of the perturbation. Nevertheless, as discussed\nin Sect. 4.2 previously, a completely unitary neural network\nis impossible because non-square weights and nonlinearities\nare needed for creating the model complexity required to\nsolve complex problems. In our last experiment as follows,\nwe demonstrate that, even with imperfect unitarity, our\nUniBERT can still stabilize the perturbations, preventing\nthem from being scaled arbitrarily by the synaptic weights.\nTo quantify the effect of a perturbation, we use the\ncosine similarity to measure the alignment of two vectors\nrepresenting the neural activations, which is defined as:\ncosine similarity = a\u00b7b/||a||.||b|| = cos(\u03b8),"}, {"title": "CONCLUSION", "content": "Here we present an enhanced neural architecture named\nUniBERT for robust natural language processing. Our UniB-ERT defends against adversarial attacks", "improvements": "First, we\nreplace the cross-entropy loss"}]}