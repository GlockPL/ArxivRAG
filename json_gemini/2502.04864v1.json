{"title": "TAR2 : Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning", "authors": ["Aditya Kapoor", "Kale-ab Tessera", "Mayank Baranwal", "Harshad Khadilkar", "Stefano Albrecht", "Mingfei Sun"], "abstract": "In cooperative multi-agent reinforcement learning (MARL), learning effective policies is challenging when global rewards are sparse and delayed. This difficulty arises from the need to assign credit across both agents and time steps, a problem that existing methods often fail to address in episodic, long-horizon tasks. We propose Temporal-Agent Reward Redistribution (TAR2 ), a novel approach that decomposes sparse global rewards into agent-specific, time-step-specific components, thereby providing more frequent and accurate feedback for policy learning. Theoretically, we show that TAR2 (i) aligns with potential-based reward shaping, preserving the same optimal policies as the original environment; and (ii) maintains policy gradient update directions identical to those under the original sparse reward, ensuring unbiased credit signals. Empirical results on two challenging benchmarks\u2014SMACLite and Google Research Football\u2014demonstrate that TAR2 significantly stabilizes and accelerates convergence, outperforming strong baselines like AREL and STAS in both learning speed and final performance. These findings establish TAR2 as a principled and practical solution for agent-temporal credit assignment in sparse-reward multi-agent systems.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) enables autonomous agents to cooperate on complex tasks across a wide range of domains, including warehouse logistics [Krnjaic et al., 2022], e-commerce [Shelke et al., 2023; Baer et al., 2019], robotics [Sartoretti et al., 2019; Damani et al., 2021], and routing [Zhang et al., 2018; Vinitsky et al., 2020; Zhang et al., 2023]. MARL has also shown great promise in high-stakes coordination challenges in video games such as StarCraft II [Vinyals et al., 2019], DOTA [Berner et al., 2019], and Google Football [Kurach et al., 2020], where teams of agents must align actions toward a shared goal.\nHowever, one of the most persistent bottlenecks in cooperative MARL is credit assignment: deciding how to allocate a global team reward among multiple agents in partial observability and decentralized execution. This bottleneck is particularly acute when rewards are sparse or delayed, which causes agents to struggle in correlating their intermediate actions with eventual outcomes [Arjona-Medina et al., 2019; Ren et al., 2021]. In such settings, there is a need to determine when along a multi-step trajectory the helpful actions occurred (temporal credit assignment) and which agent or subset of agents were responsible (agent credit assignment). Prior works often focus on only one of these aspects at a time\u2014e.g., factorizing value functions to identify agent-specific credits [Sunehag et al., 2017; Rashid et al., 2020] or building dense temporal proxies to mitigate delayed rewards [Arjona-Medina et al., 2019; Xiao et al., 2022]. Yet, managing both agent and temporal credit assignment simultaneously remains challenging, especially in high-dimensional tasks where partial observability further complicates the learning problem [Papoudakis et al., 2020].\nIn this paper, we propose Temporal-Agent Reward Redistribution (TAR2 ), a novel framework that jointly addresses agent and temporal credit assignment in cooperative MARL with sparse global rewards. Rather than relying solely on value-function factorization or temporal heuristics, TAR2 redistributes the final episodic reward across each time step and each agent, guided by a learned model (Figure 1). Concretely, TAR2 maps sparse team returns into time-step-specific feedback signals and then allocates these among agents according to each agent\u2019s (marginal) contribution, approximated via a dual-attention mechanism. Crucially, our approach is built upon potential-based reward shaping [Ng, 1999; Devlin and Kudenko, 2011], ensuring that the optimal policies of the original Markov Decision Process (MDP) are preserved in our reshaped rewards. This preserves optimal policy invariance, while providing finer-grained credit signals to accelerate learning.\nWe validate TAR2 both through theoretical guarantees and empirical results on challenging benchmarks.\nContributions. Summarized below are our key contributions:\n\u2022 Unified Agent-Temporal Credit Assignment. We develop TAR2 , a single framework that decomposes"}, {"title": "2 Related Works", "content": "In this section, we review and compare various methods addressing credit assignment in both single-agent and multi agent reinforcement learning (MARL). While single-agent methods focus primarily on temporal credit assignment, multi-agent methods must manage the additional complexity of agent-specific credit assignment\u2014particularly under sparse or delayed rewards. Our discussion emphasizes the unique challenge of combined agent-temporal credit assignment and highlights how existing approaches differ from our proposed solution, TAR2.\n2.1 Temporal Credit Assignment\nTemporal credit assignment aims to decompose sparse or episodic rewards into informative, time-step-specific feedback, which is critical for learning in long-horizon tasks with delayed outcomes.\nEarly single-agent methods like RUDDER [Arjona-Medina et al., 2019] redistribute rewards by analyzing return contributions at each step. While effective in single-agent settings, RUDDER depends on accurate return predictions and does not extend naturally to multi-agent scenarios where individual contributions must also be identified. Sequence modeling approaches [Liu et al., 2019; Han et al., 2022] use architectures like Transformers [Vaswani et al., 2017] to capture long-term dependencies, but they similarly focus on temporal aspects without addressing agent-level credit.\nTrajectory-based methods [Ren et al., 2021; Zhu et al., 2023] learn proxy rewards via smoothing and bi-level optimization; however, these approaches generally assume a single agent and do not account for multiple cooperating agents. Hindsight Policy Gradients (HPG) [Harutyunyan et al., 2019] retrospectively assign rewards using future trajectory information but are again tailored to single-agent scenarios.\nIn multi-agent contexts, AREL [Xiao et al., 2022] extends temporal credit assignment using attention mechanisms to redistribute rewards over time. Although AREL handles temporal dependencies among agents, it primarily focuses on the temporal domain and does not fully capture agent-specific contributions, particularly under sparse reward conditions. In contrast, our method, TAR2, jointly addresses both temporal and agent-specific credit assignment, which is crucial for multi-agent systems with sparse rewards.\n2.2 Agent Credit Assignment\nAgent credit assignment seeks to allocate portions of a global reward to individual agents based on their contributions, a key aspect of learning cooperative policies.\nDifference rewards and counterfactual methods, such as COMA [Foerster et al., 2018; Devlin et al., 2014], compute agent-specific advantages by considering counterfactual scenarios, but these approaches typically assume denser feedback and struggle when rewards are sparse or significantly delayed. Value function factorization methods like VDN [Sune-"}, {"title": "3 Background", "content": "In this section, we present the foundational concepts and problem setup that underpin our method. We begin by reviewing the standard framework of decentralized partially observable Markov decision processes (Dec-POMDPs) [Oliehoek and Amato, 2016; Amato, 2024], which formalizes many cooperative multi-agent reinforcement learning (MARL) tasks. We then focus on the episodic version of MARL with sparse or delayed rewards, highlighting why these settings pose unique credit-assignment challenges. Finally, we introduce potential-based reward shaping [Ng, 1999; Devlin and Kudenko, 2011], the key theoretical tool we build upon to preserve optimal policies while reshaping rewards.\n3.1 Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs)\nA Dec-POMDP is defined by the tuple\n M = \\langle S, \\{A_i\\}_{i=1}^N, P, \\{O_i\\}_{i=1}^N, \\{\\pi_i\\}_{i=1}^N, R_{\\zeta}, \\rho_0, \\gamma \\rangle,\nwhere N agents interact in an environment with states $s \\in S$. Each agent $i \\in \\{1, ..., N\\}$ selects an action $a_i$ from its action space $A_i$, and receives an observation $o_i$ from its observation space $O_i$. The observation $o_i$ is generated according to the observation function $T(o_i | s, i)$, which defines the probability of agent i receiving observation $o_i$ given the current state s. The transition function $P(s_{t+1} | s_t, a_t)$ governs how states evolve, and $\\rho_0$ is the initial state distribution. Agents operate according to a joint policy $\\pi = \\prod_{i=1}^N \\pi_i$, where each $\\pi_i$ conditions on local observation histories $h_{i,t} = \\{(o_{i,\\tau}, a_{i,\\tau})\\}_{t \\tau=1}$, and $h_{\\vert \\tau \\vert}$ along with $a_{\\vert \\tau \\vert}$ represent the final approximate state and action of the multi-agent trajectory, respectively.\nUnlike fully observable MDPs, partial observability means each agent sees only a slice of the global state, making coordination harder. A global reward function $R_{\\zeta} : S \\times A \\rightarrow \\mathbb{R}$ provides a team-wide signal, shared among all agents at each timestep (or, in our episodic case, at the end of the trajectory). The objective is to learn policies $\\pi$ that maximize the expected return:\n$\\mathbb{E}_{s_0 \\sim \\rho_0, s \\sim P, a \\sim \\pi} \\big[\\sum_{t=0}^{\\vert \\tau \\vert} \\gamma^t r_{global,t}\\big].$\n3.2 Episodic Multi-Agent Reinforcement Learning\nIn many MARL domains, rewards are received only upon completing an episode, yielding a single episodic reward $r_{global,episodic}(\\tau)$. This setting is common in tasks with sparse or delayed feedback\u2014e.g., defeating all opponents in a battle environment or scoring a goal in a sports simulation. Although it accurately represents real-world scenarios, episodic (delayed) rewards significantly complicate learning, causing high variance and bias [Ng, 1999] in the policy gradient estimates. Agents must learn not only which actions lead to success, but also when those actions should occur within the trajectory\u2014a problem known as temporal credit assignment."}, {"title": "3.3 Potential-Based Reward Shaping", "content": "A well-known approach to address delayed or sparse rewards is potential-based reward shaping [Ng, 1999]. In the single agent case, one augments the reward function R with an extra shaping term F, derived from a potential function \u03a6. Formally, for any two consecutive states s and s\u2032 , the shaping reward is:\n$F(s, s') = \\gamma \\Phi(s') - \\Phi(s)$.\nBecause this shaping term telescopes across a trajectory, it preserves the set of optimal policies. In multi-agent settings, the same principle applies if each agent\u2019s shaping function is potential-based [Devlin and Kudenko, 2011; Xiaosong Lu, 2011]. In particular, adding\n$F_i(s, s') = \\gamma \\Phi_i(s') - \\Phi_i(s)$\nto the reward function for agent i ensures that Nash equilibria remain unchanged. For more details, refer to Appendix 1.\nPotential-based shaping motivates our reward redistribution strategy, as it allows us to create denser per-step feedback while provably preserving policy optimality. We leverage this shaping concept to break down a single episodic reward into agent- and time-step-specific components, ensuring that the reward augmentation does not distort the underlying solution set. In the next section, we detail how we design such a redistribution scheme\u2014called TAR2\u2014to tackle sparse multi agent tasks effectively."}, {"title": "4 Approach", "content": "In this section, we present our Temporal-Agent Reward Redistribution (TAR2 ) algorithm, which addresses the dual challenge of temporal and agent-specific credit assignment in cooperative multi-agent reinforcement learning (MARL). We begin by introducing our reward redistribution mechanism (Sec. 4.1), then establish theoretical guarantees ensuring optimal policy preservation (Sec. 4.2). Next, we detail the architectural design of our model (Sec. 4.3) and outline the full training procedure (Sec. 4.4). Finally, we discuss interpretability considerations and reference ablation studies that provide further insight into each design choice.\n4.1 Reward Redistribution Formulation\nThe central challenge in episodic MARL is to decompose a sparse global reward $r_{global,episodic}$ into more informative, time-step and agent-specific feedback, while preserving the original problem\u2019s solutions. We achieve this by defining two weight functions:\n$w_t^{temporal}, w_{i,t}^{agent}$,\nwhich decompose the final return across time steps and agents, respectively. Formally, each agent i at time t receives\n$r_{i,t} = w_{i,t}^{agent} w_t^{temporal} r_{global,episodic}(\\tau)$.\nWe impose normalization constraints such that $\\sum_{i=1}^N w_{i,t}^{agent} = 1$ and $\\sum_{t=1}^{\\vert \\tau \\vert} w_t^{temporal} = 1$, ensuring the sum of all rit matches the original episodic reward as done in prior works [Xiao et al., 2022; Chen et al., 2023; Ren et al., 2021; Efroni et al., 2021]. Specifically, we make the following assumption about the reward redistribution:"}, {"title": "4.2 Theoretical Guarantees", "content": "Optimal Policy Invariance. We prove that the new reward function\n$R_i^{\\omega,\\kappa}(s_t, a_t, s_{t+1}) = R_{\\zeta}(s_t, a_t, s_{t+1}) + r_{i,t}$\nis consistent with potential-based reward shaping (see Section 3.3). Specifically, if $\\pi_{\\theta}^*$ is optimal under $R_{\\omega,\\kappa}$, it remains optimal under the original environment reward $R_{\\zeta}$. This ensures that our reward redistribution does not alter the set of optimal policies. For a detailed proof of this invariance, please refer to Section 3 of the supplementary material.\nGradient Direction Preservation. Beyond preserving optimal solutions, we also show (in Section 6 or the supplemental material) that the direction of the policy gradient update under TAR2 matches that under the original reward. Concretely, for an agent i,\n$\\nabla_{\\theta_i}J_{\\omega,\\kappa}(\\theta_i) \\propto \\nabla_{\\theta_i}J_{\\zeta}(\\theta_i)$,\nindicating that we do not bias the learning trajectory, only densify it. Proof details appear in Section 6 of the supplementary material."}, {"title": "4.3 Reward Model Architecture", "content": "Our reward redistribution model builds on the dual attention mechanisms of AREL [Xiao et al., 2022] and STAS [Chen et al., 2023], with key enhancements to better handle sparse, long-horizon multi-agent tasks.\nAdaptations from Prior Work\nWe embed each agent\u2019s observations, actions, and unique positional information at every timestep, then sum these embeddings to form a sequence that is input to a causal dual attention mechanism [Vaswani et al., 2017]. This mechanism allows us to capture both temporal dependencies and inter-agent interactions. By using a Shapley attention network, similar to STAS, we approximate each agent\u2019s marginal contribution via Monte Carlo rollouts over coalitions. The architecture comprises three dual attention blocks, each with four attention heads, selectively focusing on relevant spatial and temporal features."}, {"title": "4.4 Training Objective", "content": "We optimize the reward redistribution model parameters (\u03c9, \u03ba) using the objective:\n$\\mathcal{L}(\\omega, \\kappa) = \\mathbb{E}_{\\tau \\sim \\mathcal{B}} \\bigg[ \\Big( r_{global,episodic}(\\tau) - \\sum_{t=1}^{T} \\sum_{i=1}^{N} R(i, t; \\omega, \\kappa) \\Big)^2 - \\lambda \\sum_{t=1}^{T} \\sum_{i=1}^{N} a_{i,t} \\log(p_{i,t}) \\bigg],$\nwhere R(i, t; \u03c9, \u03ba) denotes the predicted reward for agent i at time t, and pi,t is the inverse dynamics model\u2019s predicted probability for action ai,t. The first term minimizes the discrepancy between the sum of redistributed rewards and the true episodic return, while the second term trains the inverse dynamics component via cross-entropy loss. Training involves sampling trajectories from an experience buffer B and periodically updating (\u03c9, \u03ba) using this loss. For the complete training procedure, please refer to Algorithm in Section 8 of the supplementary material."}, {"title": "5 Experimental Setup", "content": "5.1 Baselines\nWe compare TAR2 against several reward redistribution methods, all trained with Multi-Agent Proximal Policy Optimization (MAPPO) [Yu et al., 2022]:\n\u2022 TAR2 (Ours): Our approach (Section 4) that redistributes rewards both across time and agents, leveraging an inverse dynamics model and final outcome conditioning.\n\u2022 Uniform (IRCR) [Gangwani et al., 2020]: Assigns the global episodic reward equally to each timestep and agent, i.e., $r_{global,t} = r_{episodic}(\\tau)/\\vert \\tau \\vert$.\n\u2022 AREL-Temporal [Xiao et al., 2022]: Focuses on temporal credit assignment by predicting rewards for the entire multi-agent trajectory at each timestep.\n\u2022 AREL-Agent-Temporal: We modified AREL-TEMPORAL version to assign rewards per agent at each timestep, rather than per joint observation.\n\u2022 STAS [Chen et al., 2023]: Employs a dual attention structure (temporal + Shapley-based agent attention) to decompose global rewards into agent-temporal components.\nAdditional hyperparameter details for each method can be found in Section 11 of the supplementary material.\n5.2 Environments\nWe evaluate on two cooperative multi-agent benchmarks SMACLite [Michalski et al., 2023] and Google Research Football [Kurach et al., 2020], providing delayed (episodic) feedback by accumulating dense rewards during each trajectory and only returning them at the end. More details on the environment can be found in the supplementary material, Section 10.\n5.3 Results and Discussion\nMetrics. We report per-agent reward instead of win rate, as rewards provide continuous, granular feedback throughout training\u2014particularly valuable in complex tasks where partial successes or incremental improvements occur long before an episode concludes.\nPerformance in GRF and SMAClite. Figures 2 and 3 show average agent rewards (with standard deviation) over three GRF tasks and three SMAClite scenarios. TAR2 consistently outperforms baselines, converging to higher returns in all tasks.\nUniform Baseline (IRCR). The simplest baseline\u2014assigning an equal portion of the global reward to each timestep and agent\u2014plateaus early. By ignoring each agent\u2019s varying contribution, it provides insufficient guidance for fine-grained strategy learning, leading to relatively stagnant performance.\nAREL Variants & STAS. While STAS leverages Shapley-based decompositions and generally outperforms the AREL variants, it still trails TAR2. A key limitation is that both STAS and AREL produce unbounded per-agent, per-timestep predictions, which can destabilize training. Moreover, AREL (temporal or agent-temporal) struggles particularly in Google Football, leading to catastrophic drops in tasks like Pass and Shoot.\nOverall, these results demonstrate that TAR2 not only yields higher final performance but also converges more reliably across a range of sparse-reward tasks."}, {"title": "5.4 Additional Analysis: Performance Bounds & Ablation Studies", "content": "Evaluation Baselines and Performance Bounds\nTo contextualize TAR2's performance, we evaluate it against several reward-assignment configurations in the environment, approximating both lower and upper performance bounds for MAPPO [Yu et al., 2022]:\n\u2022 Episodic Team: Provides a single global reward only at the episode's end. This is a minimal credit assignment scheme that generally yields a lower bound.\n\u2022 Episodic Agent: Allocates total return to each agent based solely on its individual contribution. While more granular than Episodic Team, it can induce greediness and discourage cooperative behavior in tasks requiring teamwork.\n\u2022 Dense Temporal: All agents receive dense feedback at every timestep based on global performance. This offers an approximate upper bound, since it supplies immediate signals for each action.\n\u2022 Dense Agent-Temporal: Each agent obtains a dense reward for its individual contributions at every timestep. Similar to Episodic Agent, but with dense signals for each action, making it another upper-bound scenario.\nFigure 4 illustrates these bounds alongside TAR2 in three Google Research Football tasks. TAR2 outperforms or matches the best dense baselines, demonstrating that its structured redistribution approach rivals the benefits of dense rewards while preserving policy optimality invariance.\nAblation Studies\nWe also conduct ablations to isolate the impact of three key components in TAR2: (1) the inverse dynamics model, (2) final outcome conditioning, and (3) normalization of predicted rewards (Fig. 5).\nInverse Dynamics Model. Removing the inverse dynamics task results in notably slower convergence and higher variance, indicating that predicting each agent's actions helps the model capture causal structure in multi-agent trajectories. This aligns with prior findings [Pathak et al., 2017; Agrawal et al., 2015] that inverse dynamics objectives can improve temporal representations."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we tackled the dual challenges of temporal and agent-specific credit assignment in multi-agent reinforcement learning with delayed episodic rewards. We introduced TAR2, a novel reward redistribution method that decomposes a global episodic reward into agent-specific, per-timestep rewards while preserving policy optimality via potential-based reward shaping. TAR2 effectively aligns intermediate actions with final outcomes by leveraging final state conditioning, inverse dynamics modeling, and probabilistic normalization\u2014innovations that substantially improve learning in sparse-reward settings. Our extensive evaluations on challenging benchmarks like SMACLite and Google Research Football demonstrate that TAR2 outperforms state-of-the-art baselines in terms of sample efficiency and final performance. These results confirm that our approach provides more precise credit assignment, leading to faster and more stable policy convergence.\nLooking ahead, we envision several exciting avenues for expanding this work. For instance, integrating Hindsight Credit Assignment [Harutyunyan et al., 2019] into the TAR2 framework could further refine reward signals based on alternate goals or outcomes. Additionally, exploring transfer-learning capabilities\u2014such as applying a model trained with a certain number of agents to scenarios with more agents or transferring knowledge across similar environments\u2014could reveal the academic offspring of TAR2, broadening its applicability and impact."}, {"title": "A Detailed Discussion on Potential-Based Reward Shaping", "content": "Ng [1999] presented a single-agent reward shaping method to address the credit assignment problem by introducing a potential based shaping reward to the environment. The combination of the shaping reward with the original reward can enhance the learning performance of a reinforcement learning algorithm and accelerate the convergence to the optimal policy. Devlin and Kudenko [2011] and Xiaosong Lu [2011] extended potential-based reward shaping to multi-agent systems as follows:\nTheorem 1. Given an n-player discounted stochastic game M = (S, A1, ..., An, T, \u03b3, R1, ..., Rn), we define a transformed n-player discounted stochastic game M' = (S, A1, ..., An, \u03a4, \u03b3, R\u2081 + F1, ..., Rn + Fn), where F\u00bf \u2208 S \u00d7 S is a shaping reward function for player i. We call Fi a potential-based shaping function if Fi has the form:\n$F_i(s, s') = \\gamma \\Phi_i(s') - \\Phi_i(s)$,\nwhere Pi: S\u2192 R is a potential function. Then, the potential-based shaping function Fi is a necessary and sufficient condition to guarantee the Nash equilibrium policy invariance such that:\n\u2022 (Sufficiency) If F\u00bf (i = 1, ..., n) is a potential-based shaping function, then every Nash equilibrium policy in M' will also be a Nash equilibrium policy in M (and vice versa).\n\u2022 (Necessity) If Fi (i = 1, . . ., n) is not a potential-based shaping function, then there may exist a transition function T and reward function R such that the Nash equilibrium policy in M' will not be the Nash equilibrium policy in M.\nIn summary, potential-based reward shaping ensures that Nash equilibrium policies are preserved, enhancing learning without altering the strategic dynamics. This principle underpins our proposed reward redistribution method, which we will validate in the following sections, demonstrating its effectiveness in multi-agent reinforcement learning.\nRelevance to TAR2. Potential-based shaping provides the theoretical underpinning for our reward redistribution strategy. By ensuring that our redistribution function adheres to a potential-based form, we guarantee that:\n\u2022 The reshaped rewards do not alter the set of optimal policies (policy optimality invariance).\n\u2022 The convergence and strategic dynamics are preserved, even as we provide denser, agent- and time-specific rewards.\nThis detailed understanding justifies the design choices in TAR2 and underlines its robustness in multi-agent scenarios."}, {"title": "B Formulating the Reward Redistribution Mechanism", "content": "To effectively address both temporal and agent-specific credit assignment in episodic MARL, we introduce a two-stage reward redistribution mechanism. This mechanism decomposes the global episodic reward into more informative, granular components that facilitate better learning and coordination among agents.\nTemporal Redistribution We first redistribute the global episodic reward across the trajectory's time steps using a temporal weighting function\n$w_t^{temporal} \\sim W_w (h_t, a_t, h_{\\vert \\tau \\vert}, a_{\\vert \\tau \\vert}),$\nparameterized by w. This function takes as input the history ht, the joint action at at time t, and the final state-action pair (h|7|, a|7|), assigning a portion of the global reward to each timestep t:\n$r_{global, t}^{temporal} = w_t^{temporal} \\cdot r_{global,episodic}(\\tau)$.\nWe enforce the normalization condition:\n$\\sum_{t=1}^T w_t^{temporal} = 1$.\nAgent-wise Redistribution Next, each temporally redistributed reward $r_{global,t}^{temporal}$ is allocated to individual agents using agent-specific weighting functions\n$w_{i,t}^{agent} \\sim W_\\kappa (h_{i,t}, a_{i,t}, h_\\tau, a_\\tau)$,\nparameterized by k. Each $w_{i,t}^{agent}$ takes as input agent i's history hit, action ai,t at time t, and the final state-action pair (h|7|, a|7|), and redistributes the temporal reward among agents:\n$r_{i,t}^{agent} = w_{i,t}^{agent} r_{global, t}^{temporal}$.\nThe agent weights are normalized at each timestep:\n$\\sum_{i=1}^N w_{i,t}^{agent} = 1 \\forall t$."}, {"title": "C Optimal Policy Preservation", "content": "We establish that the optimal policy learned under the densified reward function $R_{\\omega,\\kappa}$ remains optimal for the original reward function $R_{\\zeta}$.\nTheorem 2 (Optimal Policy Preservation). Consider two Dec-POMDPs:\n$M_{env} = (S, A,P, T, O, N, R_{\\zeta}, \\rho_0, \\gamma)$,\n$M_{rrd} = (S, A,P, T, O, N, R_{\\omega,\\kappa}, \\rho_0, \\gamma)$,\nwhere $R_i^{\\omega,\\kappa}(s_t, a_t, s_{t+1}) = R_{\\zeta}(s_t, a_t, s_{t+1}) + w_{i,t}^{agent} w_t^{temporal} r_{global,episodic}(\\tau)$ for each agent i. If $\\pi_{\\theta}^*$ is optimal in $M_{rrd}$, then it is also optimal in $M_{env}$.\nProof Sketch. To prove optimality preservation, we show that $R_{\\omega,\\kappa}$ can be expressed as\n$R_i^{\\omega,\\kappa}(s_t, a_t, s_{t+1}) = R_{\\zeta}(s_t, a_t, s_{t+1}) + F_i(s_t, a_t, s_{t+1})$,\nwhere $F_i$ is a potential-based shaping function. For simplicity, assume $ \\gamma = 1$.\nGiven Eq. 9, we seek functions $\\phi^i : S \\rightarrow \\mathbb{R}$ such that\n$w_{i,t}^{agent} w_t^{temporal} r_{global,episodic}(\\tau ) = \\phi^i (s_{t+1}) - \\phi^i (s_t)$.\nThis relation holds by defining the potential function for each agent as\n$\\phi^i (s_t) = r_{global,episodic}(\\tau ) \\cdot \\sum_{t'=0}^t w_{i,t'}^{agent} w_{t'}^{temporal}$.\nWith this definition, the shaping function $F_i(s_t, a_t, s_{t+1}) = \\phi^i (s_{t+1}) - \\phi^i (s_t)$ matches the additional term in $R_{\\omega,\\kappa}$. By the potential-based shaping theorem [Ng, 1999; Devlin and Kudenko, 2011], such an augmentation preserves the optimal policy.\nThis theorem guarantees that learning with our redistributed rewards does not alter the set of optimal policies."}, {"title": "D Impact of Faulty Credit Assignment on Policy Gradient Variance", "content": "To understand the impact of imperfect credit assignment, we examine the influence of other agents on the policy gradient update for agent i in a Dec-POMDP setting. Without assuming any specific policy parameters, the policy gradient update for agent i is computed as:\n$\\nabla_{\\theta_i} J(\\theta, h) = \\nabla_{\\theta_i} \\log \\pi_i(a_i|h_i) \\mathbb{E}_{\\neg h_i, \\neg a_i} [A(h, a)]$\nCalculating $A_i = \\mathbb{E}_{\\neg h_i, \\neg a_i} [A(h, a)]$ is complex due to the high dimensionality and inter-agent dependencies. In practice, multi-agent policy gradient methods like MAPPO [Yu et al., 2022] and MADDPG [Lowe et al., 2017] use $A(h, a)$ for the policy update thus,\n$\\nabla_{\\theta_i} J(\\theta, h) = \\nabla_{\\theta_i} \\log \\pi_i(a_i|h_i) A(h, a)$\nThis often leads to high variance in advantage estimates, slowing down learning due to noisier gradient updates.\nMulti-agent policy gradient methods estimate the true advantage by calculating $A$, a stochastic approximation of the advantage function based on joint actions a, joint agent histories h, and the joint policy \u03c0 in Dec-POMDPs. The advantage function is defined as $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, where $Q^{\\pi}(s, a)$ and $V^{\\pi}(s)$ are the state-action value and state-value functions, respectively [Sutton and Barto, 1998]. In Dec-POMDPs, these are approximated as $Q^{\\pi}(h, a)$ and $V^{\\pi}(h)$. Since the true value functions are unknown, various methods are used to compute $A$, introducing errors [Sutton and Barto, 1998; Schulman et al., 2015]. The advantage function reflects how much better it is to take a joint action a versus a random action from \u03c0, while in state s. For agent i, the goal is to compute the advantage of its action $a_i$ within the multi-agent context, where perfect credit assignment would require perfectly calculating the agent-specific advantage based on its contribution to the overall reward of the group.\nTheorem 3. Given that agent i's reward contribution at an arbitrary time-stept is $r_{i,t}(h, a)$ and the episodic reward $r_{global,episodic(\\tau )} = \\sum_{t=1}^T \\sum_{i=1}^N r_{i,t}(h, a)$, the conditional variance of $(\\nabla_{\\theta_i} J\\vert h, a)$ is proportional to the conditional variance of the advantage estimate $A_{\\neg i}(h, a)$\nProof. The variance of the policy gradient update in eq 11 for agent i is:\n$Var(\\nabla_{\\theta_i} J \\vert h, a) = (\\nabla_{\\theta_i} \\log \\pi(a_i\\vert h_i)) (\\nabla_{\\theta_i} \\log \\pi(a_i\\vert h_i)) Var(A \\vert h, a)$.\nThis expression shows that the conditional variance of $(\\nabla_{\\theta_i} J \\vert h, a)$ is proportional to the conditional variance of the joint-advantage estimate $A \\vert h, a$. While estimating variance typically requires multiple samples, we can initially analyze a single sample to isolate the variance induced by the contributions of other agents. We can express the state-action and state-value function as:\n$Q(h, a) = \\mathbb{E}_{s_0 \\sim \\rho_0, s \\sim P, a_i \\sim \\pi_i} \\bigg[\\sum_{t=1}^T \\sum_{i=1}^N r_{i,t}(h, a) \\bigg]$\n$V(h) = \\mathbb{E}_{\\pi}[Q(h, a)]$\n$A(h, a) = Q(h, a) - V(h)$\n$A(h, a) = \\mathbb{E}_{s_0 \\sim \\rho_0, s \\sim P, a_i \\sim \\pi_i} \\bigg[\\sum_{t=1}^T \\sum_{i=1}^N r_{i,t}(h, a) \\bigg] - \\mathbb{E}_{\\pi} \\mathbb{E}_{s_0 \\sim \\rho_0, s \\sim P, a_i \\sim \\pi_i} \\bigg[\\sum_{t=1}^T \\sum_{i=1}^N r_{i,t}(h, a) \\bigg]$\nBased on the linearity of expectations on $\\sum_{t=1}^T \\sum_{i=1}^N r_{i,t} = \\sum_{t=1}^T r_{i,t} + \\sum_{t=1}^T \\sum_{i\\neq i} r_{j,t}$ and by rearranging the terms we get:"}, {"title": "E Policy Gradient Update Equivalence with Reward Redistribution", "content": "In this subsection, we establish that the policy gradient update for an arbitrary agent k, derived from the reward redistribution function Rw,\u03ba, shares the same direction as the policy gradient update under the environment's original reward"}]}