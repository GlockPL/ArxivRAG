{"title": "LLM as Runtime Error Handler: A Promising Pathway to Adaptive Self-Healing of Software Systems", "authors": ["Zhensu Sun", "Haotian Zhu", "Bowen Xu", "Xiaoning Du", "Li Li", "David Lo"], "abstract": "Unanticipated runtime errors, lacking predefined handlers, can abruptly terminate execution and lead to severe consequences, such as data loss or system crashes. Despite extensive efforts to identify potential errors during the development phase, such unanticipated errors remain a challenge to to be entirely eliminated, making the runtime mitigation measurements still indispensable to minimize their impact. Automated self-healing techniques, such as reusing existing handlers, have been investigated to reduce the loss coming through with the execution termination. However, the usability of existing methods is retained by their predefined heuristic rules and they fail to handle diverse runtime errors adaptively. Recently, the advent of Large Language Models (LLMs) has opened new avenues for addressing this problem. Inspired by their remarkable capabilities in understanding and generating code, we propose to deal with the runtime errors in a real-time manner. The runtime context, such as error messages and program states, can be exploited to produce corresponding exception handling strategies with extended diversity and quality.\nMotivated by our idea, we propose HEALER, the first LLM-assisted self-healing framework for handling runtime errors. When an unexpected, thus unhandled, runtime error occurs, HEALER will be activated to generate a piece of error-handling code with the help of its internal LLM and the code will be executed inside the runtime environment owned by the framework to obtain a rectified program state from which the program should continue its execution. Our exploratory study evaluates the performance of HEALER using four different code benchmarks and three state-of-the-art LLMs, GPT-3.5, GPT-4, and CodeQwen-7B. Results show that, without the need for any fine-tuning, GPT-4 can successfully help programs recover from 72.8% of runtime errors, highlighting the potential of LLMs in handling runtime errors. Moreover, in our experiments, HEALER introduces negligible latency in normal code execution (less than 1 ms per program) and an acceptable overload for error handling (less than 4 seconds for the LLM to generate the handling code), making it a suitable real-time reactor.", "sections": [{"title": "1 INTRODUCTION", "content": "Runtime error handling [55] is an essential component of software reliability engineering, requiring developers to be able to anticipate possible errors and prepare corresponding error handlers beforehand. Unanticipated runtime errors, being lack of corresponding handlers and naturally uncaught, can lead to program crashes, exposure of sensitive information, or other severe consequences [1].\nFor example, the infamous Heartbleed bug in OpenSSL [14] was caused by an unanticipated out-of-bound error, which allows attackers to request a server to return a chunk of its own memory. To prevent such errors, substantial efforts have been dedicated to identifying possible errors in the source code through techniques such as software testing and verification [28, 61]. However, identifying all possible runtime errors and offering them corresponding exception handlers remains a challenging and often impractical goal.\nGiven the near inevitability of unanticipated runtime errors, dynamic countermeasures are demanded to minimize their impact when they really happen. This requirement has led to the development of self-healing systems [19, 41], where the one at the program level aims to automatically recover from abnormal program states and restore program functionality as much as possible. Notably, to heal a program execution is not to patch the code against an error but to revise the runtime state, such as variable values and environment configurations, to recover the program execution. A successful healing should not only proceed the execution without termination, but also correct the runtime state so that the rest of the execution can continue meaningfully. The core challenge for a self-healing system is that the specific errors and their locations are unknown in advance, requiring the system to react in real time. Consequently, it relies on predefined heuristic strategies, such as rolling back to a checkpoint [10] and matching existing error handlers [20]. Although self-healing systems have been widely studied for decades, limited by their rule-based nature, these strategies are not adaptive enough to deal with diverse errors that impede their adoption. A large-scale industry practitioner study highlights that one of the main challenges encountered with engineering such systems is the complexity of defining the adaptation rules [56].\nIdeally, a self-healing system might employ human developers to be always on call and manually correct the state according to the error that occurred, which is impractical for real-world applications. Recently, the emergence of Large Language Models (LLMs) has introduced new opportunities to revolutionize this field. LLMs have shown remarkable performance in code-related tasks [21]. For instance, AlphaCode2 [3] has outperformed 85% of human participants in a programming competition, showcasing impressive capabilities in understanding both source code and natural language."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we first recap the error-handling mechanisms in modern programming languages and then introduce the research on self-healing systems and the use of LLMs for code, respectively.\n2.1 Error Handling in Modern Programming Languages\nModern programming languages typically incorporate built-in errorhandling mechanisms, with the try-catch block being a core component. This construct consists of a try block, which encloses code that might throw exceptions, and a catch block, which defines the error handler for exceptions thrown within the try block\u00b9. When developers anticipate an exception in a specific code segment, they can encapsulate that segment within a try block and implement an error handler within the catch block. If an exception occurs inside the try block, execution jumps to the catch block where the error handler is executed. Once the error handler completes without terminating the program, the program continues running the code following the try-catch block.\nFor errors occurring in code segments not enclosed by a try-catch block, the runtime environment halts execution and searches for an error handler by traversing up the call stack. This search continues until a suitable exception handler is found or the top of the call stack is reached. If no matching handler is found, the program typically terminates, often generating an error message and stack trace to assist with debugging, which can be seen as the default error handler. When this default error handler is triggered, the program's normal\n2.2 Self-healing Systems\nSelf-healing systems, as defined by Ghosh et al. [19], are designed to \"recover from the abnormal (or \"unhealthy\") state and return to the normative (\u201chealthy\u201d) state, and function as it was prior to disruption\". It is a long-standing research area aimed at enhancing system reliability and availability, often classified as a subclass of fault-tolerant [40] or self-adaptive [34] systems. The self-healing process involves three primary steps: (1) Maintenance of health: ensuring the normal functionality of the system, such as preparing redundant components [23]; (2) Detection of system failure: identifying abnormal states within the system, such as monitoring [17]; and (3) Recovery from failure: transforming a system state containing one or more errors (and possibly faults) into a state without detected errors [6].\nIn this paper, the proposed method is a recovery step at program level. For this step, various methods have been proposed, including restarting [9], rolling back to a checkpoint [10], trying alternative methods [11], modifying input [33], reusing existing error handlers [20], preparing default error handlers [43], or combining multiple strategies [12]. The core idea behind these studies is to heuristically construct a set of fixed recovery strategies and apply them when some specific errors occur, which is typically rule-based. For example, Failure-Oblivious Computing [43] ignores invalid memory writes (such as out-of-bounds writes and null dereferences) and generates default values for invalid memory reads. Gu et al. [20] handle errors by synthesizing handlers in two ways: (1) transforming the type of the error to fit existing error handlers and (2) returning early with a default value. Carzaniga et al. [10] propose restoring the application state to a previously set checkpoint and then selecting and executing an equivalent sequence of operations that might avoid the error. While effective in certain scenarios, their rule-based nature limits their generalization and adaptability. To the best of our knowledge, this is the first work to demonstrate that LLMs can effectively achieve the recovery, creating a new technical pathway for this long-standing research field (widely studied for decades). Compared with these rule-based methods, LLMs can dynamically adapt to a wide range of scenarios by understanding the source code context and natural language error messages, making them more flexible and generalizable.\n2.3 Large Language Models for Code\nTypically, LLMs are Transformer [51] models pre-trained on large corpora of textual data, including books, research papers, web contents, etc. These models have gained great popularity because of their versatility and effectiveness, and software engineering is no exception. Recent studies [21, 60] have highlighted the capability of LLMs in code recognition, understanding, and generation, etc. Numerous LLMs designed for code have been proposed, such as CodeGen [35], StarCoder [29], and CodeLlama [44]. These models are pre-trained on code corpora to predict the next token, making"}, {"title": "3 PROPOSED FRAMEWORK", "content": "In this section, we introduce HEALER, the first error-handling framework that leverages LLMs to handle unanticipated runtime errors. We begin with an overview of the framework, followed by detailed descriptions of its main components, prompt strategy, and a proofof-conception implementation for Python.\n3.1 Overview\nHEALER is an error-handling framework designed to handle unanticipated runtime errors during program execution. We first define the problem and then introduce the workflow of HEALER."}, {"title": "5 RESULTS", "content": "In this section, we report our experimental results and answer the four research questions.\n5.1 RQ1: Effectiveness of HEALER\nThis experiment assesses the effectiveness of HEALER by analyzing the execution outcomes of the instances in the benchmarks. For each benchmark, we execute each code snippet with its paired test case and respectively handle the runtime errors using the three LLMs, GPT-3.5, GPT-4, and CodeQwen, following the framework"}, {"title": "6 DISCUSSION", "content": "In this section, we discuss the threats to validity and the issues with HEALER, including its trustworthiness, operational cost, and extension to other programming languages.\n6.1 Threats to Validity\nInternal validity: The main threat to internal validity is the difference in hyper-parameters among the LLMs used in this study. We evaluate the performance of HEALER using the commercial LLMs, GPT-3.5 and GPT-4, and the open-source LLM, CodeQwen. In our experiments, we use the default hyper-parameters for these models during inference, except for the CodeQwen's context window. The context window for CodeQwen is set to 4096 tokens, which is significantly smaller than the context windows of GPT-3.5 and GPT-4. This limitation is due to the deployment of CodeQwen on our local machine, where 4096 tokens is the maximum context window supported by the server. This constraint may affect CodeQwen's performance in handling runtime errors, as some prompts exceed this window size and are truncated. This could result in an unfair comparison between CodeQwen and GPT-4. However, we qualitatively analyzed the prompt length distribution, where less than 1% of the prompts are truncated. Therefore, we believe the impact of this truncation is trivial.\nExternal validity: The threats to external validity mainly lie in the selection of benchmarks. The benchmarks used in this paper are limited to four Python benchmarks, which may not encompass all practical scenarios. Although these benchmarks include the most common error types in Python programs, certain error types may still be unrepresented, potentially limiting the generalizability of the conclusions to real-world scenarios.\nConstruct validity: Bias in performance evaluation is a common threat to construct validity. In our experiment, we introduce two metrics, CORRECT and SURVIVED, to measure the effectiveness of HEALER in handling runtime errors. While these metrics are intuitive, they may not be optimal. For instance, even if an error is successfully handled, the resulting program output may still be incorrect due to flaws in the original source code.\n6.2 Trustworthiness of the Handling Code\nAs revealed by various studies [39, 46, 60], the code generated by LLMs is not guaranteed to be secure. It may contain malicious code injected by attackers or vulnerable code, leading to security issues. Moreover, due to the randomness in the generation process, the generated code may exhibit unpredictable behaviors. HEALER, operating without human intervention, directly executes the generated code to handle runtime errors, which could be risky. Currently, popular Al systems, such as ChatGPT, mitigate such security risks by running model-generated code in a sandboxed environment with limited permissions. Similarly, HEALER adopts this strategy."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduced HEALER, an innovative framework that uses Large Language Models (LLMs) to handle unanticipated runtime errors dynamically. Our study demonstrated that LLMs, particularly GPT-4, can effectively generate handling code snippets, successfully surviving a significant portion of runtime errors, 72.8% of errors in our experiments. Moreover, fine-tuning further enhanced performance, showing promise for LLM-assisted error recovery. HEALER opens up a new technical pathway towards selfhealing systems, where LLMs are employed as error handlers to recover from runtime errors automatically."}]}