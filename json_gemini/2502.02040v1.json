{"title": "M2R2: MIXTURE OF MULTI-RATE RESIDUALS FOR EFFICIENT TRANSFORMER INFERENCE", "authors": ["Nikhil Bhendawade", "Mahyar Najibi", "Devang Naik", "Irina Belousova"], "abstract": "Residual transformation is critical to improving representational depth and expressive power of large language models (LLMs). However, the use of static residual transformations across all tokens during auto-regressive generation induces a sub-optimal balance between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth, attempt to address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. In this work, we introduce Mixture of Multi-rate Residuals, a novel framework that dynamically modulates the velocity of residual transformations to optimize early residual alignment. This modification improves inference efficiency by better aligning intermediate representations at earlier stages.\nWe show the efficacy of our technique in diverse optimization setups such as dynamic computing, speculative decoding, and MoE Ahead-of-Time (AoT) loading using challenging reasoning tasks from Koala, Self-Instruct, WizardLM and MT Bench. Our approach empirically outperforms state-of-the-art distance-based residual strategies, enabling a better trade-off between generation metrics and speedup in dynamic computing settings. In self-speculative decoding setups, M2R2 achieves up to 2.8X speedups on MT-Bench under lossless conditions, outperforming SOTA approaches such as 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, we enhance decoding speed by coupling early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM). This enables concurrent memory access and computation, reducing the latency bottlenecks inherent in expert switching during decoding. Empirical results show that our method delivers a speedup of 2.9X in MoE architectures, positioning it as a highly effective strategy in resource-constrained environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have become a cornerstone of contemporary natural language processing (NLP) systems, exhibiting exceptional performance in tasks requiring the understanding and generation of complex, structured language across diverse domains [10, 50, 63]. Their success largely stems from their ability to capture long-range dependencies in text, enabling the modeling of intricate linguistic patterns and semantics. A critical architectural component enabling this capability is the residual transformation mechanism, which introduces a pathway for the direct flow of information across layers. By preserving representations from earlier layers and allowing new transformations to build on them, residual connections help mitigate the degradation of critical information, thereby expanding the model's expressive power to handle more abstract and nuanced features [32, 7, 63]. This architecture empowers LLMs to model complex dependencies over extended sequences, thereby improving feature extraction and representational depth. However, the use of a static residual transformation for all tokens creates a rigid balance between inference efficiency and generation quality [55, 29]. This approach fails to account for the inherent variability in token complexity, leading to inefficiencies in dynamic compute scenarios."}, {"title": "2 METHOD", "content": "Residual connections play a crucial role in shaping token representations, yet their dynamics remain underexplored in the context of efficient decoding. In this work, we delve deeper into transformer residual dynamics and investigate how modulating residual transformation velocity can improve inference efficiency in token-level processing, optimizing both dense and sparse MoE transformers."}, {"title": "2.1 RESIDUAL DYNAMICS AND MOTIVATION FOR MULTI-RATE RESIDUALS", "content": "To analyze how hidden representations evolve across different layers of a transformer architecture, it's crucial to consider the effect of residual connections. Each transformer decoder layer typically has residual connections across attention and MLP submodules. As the residual stream $h_i$ traverses from interval $E_j$ to $E_{j+1}$, it undergoes a residual transformation given by:\n$h_{E_{j+1}} = h_{E_j} + \\sum_{i=E_j}^{E_{j+1}-1} (A_i(h_i) + M_i(h_i + A_i(h_i))) \\text{ where } A_i = f(c_i, h_i), M_i = g(h_i).$ (1)\nHere, $A_i$ denotes the non-linear transformation introduced by the multi-head attention mechanism at layer $i$, while $M_i$ corresponds to the non-linear transformation of the MLP block at the same layer. These transformations depend on the input residual stream $h_i$ and, in the case of $A_i$, the previous contextual representation $c_i$.\nTo examine whether residual transformations can be accelerated across layers, we conducted experiments using a diverse set of prompts on a pre-trained Phi3 model [1]. As illustrated in Figure 2a, we measured the directional shift in residual states as $1 \u2013 C(h_{i\u22121}, h_i)$, where $C$ denotes normalized cosine similarity. This shift is notably higher in the initial layers, gradually decreasing in subsequent layers. This behavior allows traditional early exit approaches to effectively accelerate decoding by"}, {"title": "2.2 MULTI-RATE RESIDUAL TRANSFORMATION", "content": "To address the slow residual transformation bias described in Section 2.1, we introduce accelerated residual streams that operate at rate R relative to original slow residual stream. We pair slow residual stream, h with an accelerated residual stream, p, which has an intrinsic bias towards earlier alignment. Relative to Equation (1), accelerated residual transformation from interval $E_j$ to $E_{j+1}$ can be represented as:\n$p_{E_{j+1}} = p_{E_j} + \\sum_{i=E_j}^{E_{j+1}-1} (\\hat{A_i}(p_i) + \\hat{M_i}(p_i + \\hat{A_i}(p_i))) \\text{ where } \\hat{A_i} = f(c_i, p_i), \\hat{M_i} = \\hat{g}(h_i),$ (2)\nwhere $\\hat{A_i}$ and $\\hat{M_i}$ denote non-linear transformation added by layer i to previous accelerated residual $p_i$. Similar to $A_i$, non-linear transformation $\\hat{A_i}$ attends to same context $c_i$ but uses a different transformation $f$ for accelerating $p_{E_i}$ relative to $h_{E_j}$\nWe integrate accelerated residual transformation directly into the base network using parallel accelerator adapters such that rank of accelerator adapters $R_p << d$ where d denotes base model hidden dimension. This setup allows the slow residual stream $h_{E_j}$, to pass through the base model layers while the accelerated residual stream $p_{E_j}$, utilizes these parallel adapters as shown in Figure 3. Both slow and accelerated residuals are processed in same forward pass via attention masking and incur negligible additional inference latency in memory bound decoding setups, while in compute bound decoding setups where FLOPs optimization is essential, accelerated residual stream utilizes a fraction of attention heads that of slow residual (see Section 2.5). Additionally, to maximize the utility of accelerated residual transformations without introducing dedicated KV caches, we propose a shared caching mechanism between the slow and accelerated streams which minimally impact alignment benefits of our approach while offering substantial memory savings (see Figure 6a). Specifically, the attention operation on the slow residuals $MHA(h_t, h_{<t}, h_{<t})$ is redefined for accelerated residuals as\n$\\hat{A} = MHA(p_t, h_{<t} \\oplus p_t, h_{<t} \\oplus p_t)$,\nwhere the accelerated residual at time-step t, $p_t$ attends to the slow residual's KV cache, facilitating the reuse of contextual information across both residual streams without incurring additional caching costs. Here, $MHA(q, k, v)$ represents multi-head attention between query q, key k, and value v."}, {"title": "2.3 ENHANCED EARLY RESIDUAL ALIGNMENT", "content": "Early residual alignment is instrumental in optimizing early exiting, speculative decoding, and Mixture-of-Experts (MoE) inference mechanisms. In this section, we provide a detailed analysis of how accelerated residuals enhance these inference setups."}, {"title": "2.3.1 EARLY EXITING", "content": "A prevalent strategy for enabling early exiting at an intermediate layer $E_j$ involves approximating the residual transformation between $E_j$ and the final layer $N-1$ using a linear, context independent mapping, T, such that $H_{N-1} \u2248 T(H_{E_j})$. This approximation has been extensively employed in conventional approaches [53, 13, 62], providing a computationally efficient means to project the output of deeper layers from intermediate states. Specifically, residual state of layer $N-1$ with this approximation can be expressed as:\n$h_{E_j} + \\sum_{i=E_j}^{N-1} (A_i(h_i) + M_i(h_i + A_i(h_i))) \\sim T(h_{E_j}) \\text{ where } T \\in c.$ (3)\nHere, $A_i$ and $M_i$ represent the residual contributions of the multi-head attention and MLP layers, respectively, while T remains independent of c, the preceding context.\nThis approach is inherently limited by two major factors: first, the assumption of linearity between $h_{E_j}$ and $h_{N-1}$ may not hold uniformly for all tokens, particularly when $E_j << N$. Second, the linear transformation T disregards the influence of the context c and fails to account for the latent representations of previous contextual states. In contrast, M2R2 accelerated residual states mitigate both of these challenges by approximating the slow residual transformation of all layers via a faster residual transformation of fewer layers as:"}, {"title": null, "content": "$h_{E_j} + \\sum_{i=E_j}^{N-1} (A_i(h_i) + M_i(h_i + A_i(h_i))) \\sim p_{E_j} + \\sum_{i=E_j}^{E_{j+1}-1} (A_i(p_i) + M_i(p_i + A_i(p_i))),$ (4)\nwhere $p_{E_j}$ is initialized from the slow residual state $h_{E_j}$ at each early exit interval $E_j$ using an identity transformation (see Figure 3). As shown in Figure 1b, accelerated residuals offer a smoother, more consistent shift in residual direction across layers, in contrast to the abrupt changes typically seen at early exit points in standard early exit methods. Moreover, the normalized cosine similarity between accelerated states at early exit intervals and final residual states is substantially higher compared to traditional early exit techniques, highlighting improved alignment with final layer representations.\nTraditional adaptive compute methods are constrained by two principal factors: the number of tokens eligible for early exit at intermediate layers and the precision of early exit decision. If residual streams fail to saturate early, the majority of tokens remain ineligible for exit, thereby diminishing potential speedups. Additionally, imprecise delineations between tokens suitable for early exit can lead to underthinking (premature exits that adversely affect accuracy) or overthinking (unnecessary processing that compromises efficiency) [73, 16]. Enhanced early alignment using Equation (4) helps to address first issue. To address the second issue we introduce Accelerated Residual Latent Attention, which dynamically assesses the saturation of the residual stream, allowing for a more precise differentiation between tokens that can exit early and those requiring further processing."}, {"title": "Accelerated Residual Latent Attention", "content": "In the context of residual streams, we observe that the decision to exit at a given layer can be more effectively informed by analyzing the dynamics of residual stream transformations, instead of solely relying on a classification head applied at the early exit interval $E_j$. To capture the subtle dynamics of residual acceleration, we propose a Accelerated Residual Latent Attention (ARLA) mechanism. This approach involves making the exit decision at gate $E_j$ by attending to the residuals spanning from gate $E_{j\u22121}$ to $E_j$, rather than considering only the residual at gate $E_j$. To minimize the computational overhead associated with exit decision-making, the attention mechanism operates within the latent domain as depicted in Figure 4a. Formally, for each interval $[E_j, E_{j+1}]$, the accelerated residuals are projected into Query $(Q_{E_j}, ..., Q_{E_{j+1}})$, Key $(K_{E_j}, ..., K_{E_{j+1}})$, and Value $(V_{E_j}, ..., V_{E_{j+1}})$ vectors, with latent dimension $d^o$ for $Q, K,$ and $V^o$ being significantly smaller than hidden dimension of p. Notably, when the router is allowed to make exit decisions at gate $E_j$ based on residual change dynamics, we observe that the attention is not confined to the residual state at $E_j$ but is distributed across residual states from $E_{j\u22121}$ to $E_j$, This broader focus on residual dynamics significantly reduces decision ambiguity in early exits, as demonstrated in Figure 4b, which contrasts routers based on the last hidden state, and the proposed ARLA router."}, {"title": "2.3.2 SELF SPECULATIVE DECODING", "content": "An alternative means to exploit the early alignment properties of our approach is through the use of accelerated residual states for speculative token sampling to accelerate autoregressive decoding. Spec- ulative decoding aims to speed up memory-bound transformer inference by employing a lightweight draft model to predict candidate tokens, while verifying speculated tokens in parallel and advancing token generation by more than one token per full model invocation [43, 12, 66, 45]. Despite its effec- tiveness in accelerating large language models (LLMs), speculative decoding introduces substantial complexity in both deployment and training. A separate draft model must be specifically trained and aligned with the target model for each application, which increases the training load and operational complexity [12]. Additionally, this approach is resource-inefficient, as it requires both the draft and target models to be simultaneously maintained in memory during inference [43, 12].\nOne strategy to address this inefficiency is to leverage the initial layers of the target model itself to generate speculative candidates, as depicted in [59]. While this method reduces the autoregressive overhead associated with speculation, it suffers from suboptimal acceptance rates. This occurs because the linear transformation employed for translating hidden states from layer k to the final layer N is typically a poor approximation, as discussed in Section 2.1 and Section 2.3.1. Our approach resolves this limitation by utilizing accelerated residuals, which demonstrate higher fidelity to their slower counterparts. By utilizing accelerated residuals operating at a rate of N/k, where k denotes the number of layers used for candidate speculation, we are able to efficiently generate speculative tokens for decoding. This technique not only obviates the need for multiple models during inference but also improves the overall efficiency and effectiveness of speculative decoding."}, {"title": "2.3.3 AHEAD OF TIME EXPERT LOADING:", "content": "Recent advancements in sparse Mixture-of-Experts (MoE) architectures [54, 25, 6, 41, 74] have introduced a paradigm shift in token generation by dynamically activating only a subset of experts per input, achieving superior efficiency in comparison to dense models, particularly under memory- bound constraints of autoregressive decoding [25, 74]. This sparse activation approach enables MoE-based language models to generate tokens more swiftly, leveraging the efficiency of selective expert usage and avoiding the overhead of full dense layer invocation. In dense transformer models, pre-loading layers is a common strategy to enhance throughput, as computations of current layer can be overlapped with pre-loading of next layer parameters [47, 56]. However, MoE models face a"}, {"title": "2.4 TRAINING", "content": "To accelerate residual streams, we employ parallel accelerator adapters as described in Section 2.2. For the early exiting use-case outlined in Section 2.3.1, we define the training objective for these adapters using the following loss function, which combines cross-entropy loss at each exit $E_j$ with distillation loss at each layer i. Loss weights coefficients 20 and 01 are employed to balance contribution of corresponding losses.\n$L_{m2r2} = -\\alpha_0 \\sum_{j=1}^{J} \\sum_{t=1}^{T} log \\hat{p_{y_t}}(y_{<t}, x) + \\alpha_1 \\sum_{i=1}^{E_J-1} \\sum_{t=1}^{T} ||p - h ||^2_2$ (5)\nwhere $y \\hat{y}$ denotes the predictions from the accelerated residual stream at layer $E_j$ and time step t, $Y_t$ represents the corresponding ground truth tokens, and x indicates previous context tokens. The distillation loss at each layer i is computed by comparing accelerated residuals at layer i with slow residuals at layer $(i \u2013 E_j(i)) \u00b7 R_i + E_j(i)$, where $R_i$ denotes the rate of accelerated residuals at layer i while $E_j(i)$ represents the most recent gate layer index such that $E_j(i) <= i$. J represents the total number of early exit gates, N denotes number of hidden layers and $E_j$ denotes layer index corresponding to gate index j and T denotes the sequence length.\nIn dynamic compute settings, after training of accelerator adapters, we optimize the query, key, and value parameters governing the ARLA routers (see Section 2.3.1) across all exits in parallel on binary cross entropy loss between predicted decision and ground truth exiting decision. The ground truth labels for the router are determined based on whether the application of the final logit head on $y \\hat{y}_t$ yields the correct next-token prediction.\nFor self-speculative decoding, as described in Section 2.3.2, the training objective remains the same as Equation (5), but with the number of intervals set to J = 1 and the rate of residual transformation set to $R_n = N/k$, where the first k layers generate speculative candidate tokens. In the context of Ahead-of-Time Expert Loading for Mixture-of-Experts (MoE) models (see Section 2.3.3), setting the rate of residual transformation to $R_n = 2$ typically offers a good trade-off between the accuracy of expert speculation and AoT pre-loading of experts."}, {"title": "2.5 FLOPS OPTIMIZATION", "content": "Naively implemented, M2R2 incurs higher FLOP overhead compared to traditional speculative decoding and early exiting approaches such as [11, 53, 59]. However, modern accelerators demon- strate compute bandwidth that exceeds memory access bandwidth by an order of magnitude or more [2, 37], meaning increased FLOPs do not necessarily translate to increased decoding latency. Nevertheless, to ensure fair comparison and efficiency in compute bound scenarios, we introduce targeted optimizations.\nAttention FLOPs Optimization For medium-to-long context lengths, attention computation domi- nates FLOPs in the self-attention layer, surpassing the contribution from MLP layers. Specifically, matrix multiplications involving queries, cached keys, and cached values scale with $l_{kv} \u00b7 l_q$ where $l_{kv}$ denotes previous context length and $l_q$ denotes current query length. Since M2R2 pairs acceler- ated residuals with slow residuals, a naive implementation results in twice the FLOPs consumption compared to a standard attention layer. To address this, we limit the attention of accelerated residual stream to selectively attend to the top-k most relevant tokens, identified by the slow residual stream based on top attention coefficients. This is possible since slow and accelerated residual streams are processed in same forward pass and accelerated streams have access to attention coefficients of slow stream. Furthermore, we design the faster residual stream to employ only 8 attention heads, compared to the 32 heads used in the slow residual stream of the Phi-3 model, reducing query, key, value, and output projection FLOPs by a factor of 1/4.\nMLP FLOPS Optimization The accelerator adapters operating on the accelerated residual stream are intentionally designed with lower rank than their counterparts in the base model. This reduces FLOP overhead by a factor proportional to hiddenSize/rank. Additionally, since the faster residual stream uses only 8 attention heads (compared to 32 in the slow residual stream of Phi-3), the subsequent MLP layers process a smaller set of activations, further reducing FLOPs by another factor of 1/4.\nThese optimizations significantly reduce the FLOP overhead per speculative draft generation, as illustrated in Figure 12a. Notably, while traditional early-exiting speculative approaches such as DEED require propagating the full slow residual state through the initial layers, incurring substantial computational costs, M2R2 achieves efficient token generation via slimmer, low-rank faster residual streams. In contrast, Medusa introduces considerable FLOP overhead due to per-head computations scaling with $d^2 + d^5$, whereas M2R2 employs low-rank layers for both MLP and language modeling heads, maintaining computational efficiency. All experiments involving the M2R2 approach, as detailed in Section 3, are conducted using these FLOPs optimizations."}, {"title": "3 EXPERIMENTS", "content": "We perform a comprehensive evaluation of our proposed method across both reasoning-intensive as well as structured application-specific tasks using pre-trained models of multiple scales.\nDatasets. To assess reasoning capabilities, we adopt a comprehensive strategy by training on the instruction-tuning dataset Alpaca [60] and evaluating performance across multiple held-out human instruction test sets, including Koala [15], Self-Instruct [64], WizardLM [68], and MT Bench [8]. These datasets encompass a wide spectrum of instruction styles, task complexities, and domain-specific reasoning challenges, such as multi-turn interactions (Koala), open-ended problem- solving (Self-Instruct), step-by-step reasoning (WizardLM), and multi-dimensional evaluation of instruction-following capabilities (MT Bench). Beyond reasoning-oriented tasks, we further evaluate our approach on structured application-specific tasks, including Structured API Generation, Text Summarization, and Meaning Representation. For Structured API Generation, we leverage the sql-create-context dataset, which is constructed using WikiSQL [72] and SPIDER [70]; for Text Summarization, we utilize Dialogsum [14]; and for Meaning Representation, we employ the e2e-nlg\nd denotes hidden state dimension while v denotes vocab size.\nModels and Baselines. We evaluate our proposed approach on open-source, dense transformer models of varying scales, including Phi-3-mini-4k-instruct (3.8B) [1] and Gemma (7B) [44], as well as the recently introduced sparse Mixture-of-Experts (MoE) model, OlMoE (1B-7B) [46]. \u03a4\u03bf benchmark our method in dynamic compute scenarios, we compare it against several state-of-the-art dynamic compute techniques, including LITE [62], an extension of CALM [53], as well as skip decoding [17] and Mixture of Depths (MoD) [51]. For speculative decoding scenarios, we use both standard draft-target speculative decoding method [57] and single-model baselines such as Medusa [11], DEED [59] and LookAhead Decoding [28]. To assess the performance of our approach in MoE configurations with Ahead-of-Time (AoT) expert loading, we conduct comparisons using several baselines. These include fixed expert configurations, LRU-based caching, LRU caching combined with random expert speculation, and expert prediction based on previous hidden states.\nMetrics We report the trade-off between wall-time speedups and generation quality metrics on the held-out test sets to compare dynamic compute approaches. In contrast, for evaluating speculative decoding setups, we focus on wall-time speedups and acceptance rates, as speculative decoding does not affect generation quality [57]. To assess the effectiveness of our method in Ahead-of-Time (AoT) MoE expert loading, we report both expert speculation hit rate and decoding latency.\nFor reasoning-oriented tasks, we evaluate response quality on human instruction test sets such as Self- Instruct [64], Koala [30], WizardLM [68], and MT Bench [71] using GPT-4 [49]. To mitigate position bias in LLM judgments [64], we assess response pairs in both possible orderings and aggregate the judgment scores. The prompt used for comparing response quality between models is provided in Appendix B. For structured application-specific tasks, we use Exact Match (EM) accuracy as the generation metric for the Structured Query task, while for Dialog Summarization, we employ the Rouge-LSum metric [65].\nInference. All inference experiments were conducted on a single Nvidia A100-80GB GPU, with a batch size of 1, using float16 precision and greedy decoding (temperature T = 0) to emulate common configurations for on-device AI assistants."}, {"title": "3.1 RESULTS", "content": ""}, {"title": "3.1.1 DYNAMIC RESIDUAL TRANSFORMATION", "content": "Early Alignment and ARLA Effectiveness We begin by analyzing the alignment of tokens exited at intermediate gates with those exited at the final layer using the Koala instruction set [15]. As shown in Figure 6a, we observe that accelerated residuals achieve significantly higher alignment compared to conventional early exit approaches, such as those proposed in [53, 13, 62]. This difference in alignment is particularly pronounced at lower gates, demonstrating that accelerated residual streams"}, {"title": "3.2 SPECULATIVE DECODING", "content": "Improved early alignment significantly boosts acceptance rates when using accelerated residual streams for speculative candidate sampling in speculative decoding settings. As shown in Figure 8, we compare acceptance rates by sampling tokens from the first k = 4 layers of Gemma-7B [44] with our approach, which employs a residual transformation rate $R = N/k$, where N denotes number of layers of base model. Baselines include draft-model-based speculative decoding [43, 12] that utilizes Gemma-2B [44] as the draft model, as well as single-model methods such as Medusa [11], DEED"}, {"title": "3.3 AHEAD-OF-TIME (AOT) MOE EXPERT LOADING", "content": "We evaluate efficacy of Ahead-of-Time (AoT) expert loading in resource-constrained environments, where limited high-bandwidth memory (HBM) restricts the number of experts that can be stored in fast-access memory. When an MoE gate selects an expert that is not in HBM, it must be loaded from"}, {"title": "4 RELATED WORK", "content": "The inference speed of large language models (LLMs) is often constrained by the sequential nature of auto-regressive decoding, which necessitates a complete forward pass of the network for each token generated. To mitigate the high inference latency associated with LLMs, various strategies have been proposed to reduce their memory footprint. Techniques such as model quantization [27, 69, 18], knowledge distillation to smaller models [31, 3], and pruning [26, 58] have emerged as effective solutions. However, these strategies often neglect the variational complexity inherent in each token, resulting in a reliance on static computation for all tokens. To better address this issue, several early exiting approaches have been developed to facilitate dynamic computation. These methods focus on terminating residual transformations early for simpler tokens, achieving significant speedups in embedding models [67, 33, 61]. In the context of sequence generation models, techniques like Confident Adaptive Language Modeling (CALM) [53] and Depth-Adaptive Transformers [21] have effectively employed early exiting by integrating classifiers into the decoder layers. However, these approaches are constrained by key-value (KV) cache mismatches that arise between the training and inference phases, as KV states are not accessible for tokens that are early-exited. To mitigate these limitations, skip decoding [17] has been introduced. This method allows for bypassing a progressively increasing number of layers based on the token's position in the decoded sequence. While this approach effectively circumvents KV mismatches, the pre-defined limitations on the number of bypassed layers can lead to suboptimal generation quality.\nAnother promising direction involves conditioning residual transformations at each layer through the use of a router. For example, CoLT5 [4] employs conditional routing to determine whether a token should follow a heavy or light computational pathway for each feedforward layer in encoder- decoder models. Mixture-of-depths [51] builds upon this idea by introducing a predictive router at each layer, which enables efficient inference for conditional computation in decoder-only models. Although conditional routing demonstrates potential during pre-training, as illustrated in Section 3, its effectiveness during supervised fine-tuning and instruction tuning remains limited. This restricts the applicability of this technique across a wider array of publicly available pre-trained models."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced the Mixture of Multi-rate Residuals (M2R2) framework, which dy- namically modulates the velocity of residual transformations to optimize early residual alignment, improving inference efficiency in diverse inference setups. Unlike traditional methods that focus on the \"distance\" tokens traverse within the model layers, M2R2 enhances the rate at which resid- uals evolve, allowing for faster alignment of intermediate representations. Our empirical results demonstrate that M2R2 outperforms state-of-the-art dynamic computation methods offering better generation metrics to speedup trade-off. Furthermore, it achieves 2.8X speedup in lossless self- speculative decoding setup. In Mixture-of-Experts (MoE) models, with ahead-of-time expert loading, M2R2 reduces decoding latency by overlapping memory transfers with computation and achieves a throughput improvement of up to 2.9X compared to traditional expert loading methods. Overall, M2R2 offers an effective solution for optimizing inference in resource-constrained environments, enhancing both dense transformer and sparse MoE models."}, {"title": "A DYNAMIC COMPUTING", "content": ""}, {"title": "A.1 GRADIENT CONFLICT RESOLUTION", "content": "Traditional early exiting strategies frequently encounter issues related to gradient conflicts [22, 23], where multiple exit points induce conflicting gradients during the training phase. This phenomenon leads to optimization instability and challenges in convergence, as gradients computed from divergent branches may not align effectively, and the presence of early exits can perturb the gradient flow, potentially resulting in the incomplete training of lower early exit heads. To illustrate this problem, consider a trainable parameter $w_j$ situated between gates $E_j$ and $E_{j+1}$. For the loss associated with the early exit at gates $E_{j+1}...n$, the parameter update required in $w_j$ can be expressed as:\n$\\Delta w_j = -\\eta \\sum_{k=j+1}^{n} \\beta_k \\frac{\\partial L_{E_k}}{\\partial w_j}$ (6)\nwhere $\u03b2_k$ is the backward transformation coefficient for the gradient from gate $E_k$ to reach parameter $w_j$ and n is the learning rate. Conversely, since accelerated residuals at gate $E_j$ are initialized from slow residuals $H_j$ which are trained with base adapters, when base adapters are frozen, gradient propagation is limited to parallel adapter parameters from gate $E_j$ to gate $E_{j+1}$ thus ensuring every parallel adapter parameter is optimized for specific exit as shown in Figure 3. Formally speaking, the update of accelerator adapter parameter $w_j$ within our proposed framework is delineated as:\n$\\Delta w_j = \\begin{cases} -\\eta \\beta_{j+1} \\frac{\\partial L_{E_{j+1}}}{\\partial w_j}, & \\text{if } E_j < w_j < E_{j+1} \\\\ 0, & \\text{otherwise} \\end{cases}$ (7)\nwhere $\u03b2_{j+1}$ is the backward transformation coefficient for the gradient from gate $E_{j+1}$ to reach parameter $w_j$ of accelerator adapter. This formulation mitigates gradient conflicts arising from gradients associated with top gates, thereby enhancing the stability of the optimization process."}, {"title": "A.2 DISCONTINUITY IN MIXTURE OF DEPTHS AND SKIP DECODING", "content": "To get a deeper understanding of discontinuity leading to suboptimal performance of architectures like MoD and Skip decoding during instruction tuning and fine-tuning phases, we pass a diverse"}, {"title": "B MOE SPECULATION CONTINUED", "content": "In this section, we detail the expert transfer process between High Bandwidth Memory (HBM) and Low Bandwidth Memory (LBM) on the A100 GPU. We employ CUDA's multi-stream functionality [48] to establish distinct compute and memory-loading streams, both of which operate concurrently during each forward pass. The load stream is scheduled ahead of the compute stream to ensure efficient memory management: while the compute stream processes layer i, the load stream transfers the least recently used experts of layer 2i + 2 and 2i + 3 to LBM and loads speculated experts into HBM. This approach leverages the accelerated residual at layer i, which exhibits strong similarity to the slow residuals at layers 2i + 2 and 2i + 3 , enabling effective expert speculation"}]}