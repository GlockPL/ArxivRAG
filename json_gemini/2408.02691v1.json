{"title": "Symmetric Graph Contrastive Learning against Noisy Views for Recommendation", "authors": ["CHU ZHAO", "ENNENG YANG", "YULIANG LIANG", "JIANZHE ZHAO", "GUIBING GUO", "XINGWEI WANG"], "abstract": "Graph Contrastive Learning (GCL) leverages data augmentation techniques to produce contrasting views, enhancing the accuracy of recommendation systems through learning the consistency between contrastive views. However, existing augmentation methods, such as directly perturbing interaction graph (e.g., node/edge dropout), may interfere with the original connections and generate poor contrasting views, resulting in sub-optimal performance. In this paper, we define the views that share only a small amount of information with the original graph due to poor data augmentation as noisy views (i.e., the last 20% of the views with a cosine similarity value less than 0.1 to the original view). We demonstrate through detailed experiments that noisy views will significantly degrade recommendation performance. Further, we propose a model-agnostic Symmetric Graph Contrastive Learning (SGCL) method with theoretical guarantees to address this issue. Specifically, we introduce symmetry theory into graph contrastive learning, based on which we propose a symmetric form and contrast loss resistant to noisy interference. We provide theoretical proof that our proposed SGCL method has a high tolerance to noisy views. Further demonstration is given by conducting extensive experiments on three real-world datasets. The experimental results demonstrate that our approach substantially increases recommendation accuracy, with relative improvements reaching as high as 12.25% over nine other competing models. These results highlight the efficacy of our method.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Convolutional Networks (GCNs) have recently garnered extensive attention in the recommendation field. Benefiting from the capability to learn sophisticated representations, GCN-based recommendation systems can model user preferences from user-item interactions [40, 41]. They generally transform the interactions into a graph and take them as input. Then, they utilize GCNs to distribute the embeddings across neighboring nodes within the user-item interaction graph, thereby exploring higher-order connectivity. Ultimately, models use the inner product to calculate the user and item embeddings and predict user preference for the item. Several advanced GCN-based collaborative filtering frameworks have been proposed, including LightGCN [14], NGCF [35], and DGCF [36].\nWhile these models have demonstrated success, their reliance on high-quality labels (i.e., supervised signals) presents a drawback, particularly in real-world scenarios; obtaining a large amount of labeled data is impractical due to constraints on resources and privacy protection regulations. User behaviors often exhibit noise and inconsistency. Such a situation will lead to suboptimal representations and hinder the improvement of recommendation performance. Research indicates that self-supervised learning can harness additional signals from unlabeled data and utilize contrastive learning to enhance representation learning [8, 15, 17, 23, 24, 31]. Motivated by these works, researchers have designed a series of self-supervised graph contrastive learning recommendation frameworks [29, 49]. These methods employ various cleverly designed augmentation operations, such as node/edge dropout, to generate contrasting views (i.e., manually generating contrastive views), and we displayed the contrastive learning general framework in Figure 1. Subsequently, the contrastive loss function, such as InfoNCE, is used to ensure the consistency of embeddings by increasing the mutual information among positive sample pairs while reducing it among negative pairs [25, 28].\nHowever, we argue that such heuristic-based data augmentation operations may remove crucial nodes or edges from the graph, resulting in less shared learnable information between the generated and original views. In this work, we define the generated view that shares only a small amount of information with the original view in the process of graph data augmentation as noisy views. Take Figure 1 as an example; two contrasting views (i.e., View 1 and View 2) are generated for user u\u2081 through edge dropout. It can be observed that there is a significant difference in the graph structure between the generated view (View 1) and the original view. In Figure 2, we further visualize the original and generated views' embeddings. From the visualizations in Figure 2, it is evident that View 1 preserves less original semantic information compared with View 2. In this scenario, aligning these two contrasting views is inappropriate, as it would result in suboptimal representations and detrimentally affect recommendation performance. Recently, some studies have devised adaptive CL methods to avoid directly perturbing the graph (and thus not generating contrastive views), including AutoCF [42], AdaGCL [16], and HCCF [43]. The above methods directly show their motivation that manually performing data augmentation may disturb the original connectivity, leading to the learning of suboptimal representations. However, these methods overlook the effects of corruption on graph and how it impacts recommendation performance, which may not adequately justify their motivation.\nMotivated by these observations, we first design comprehensive experiments to explore such manual graph augmentations and how they impact the accuracy of the recommendation. In Section 2.1, we offer an in-depth explanation of our experimental framework, outcomes, and the rationale for our study. The data presented in Table 1 illustrate a drop in performance of the recommendation model following the removal of edges deemed highly important. Figure 2 (c) indicates that the noisy view ratio increases as the importance of deleted edges increases. Hence, based on the"}, {"title": "2 MOTIVATION", "content": "In Section 2, we first visualized the embeddings of two contrastive views and the original graph. Then, we conducted experiments to explore the impact of noisy views on recommender systems."}, {"title": "2.1 Noisy Views in Data Augmentation", "content": "We direct the perturbation of the interaction graph to generate noisy views in this section. We initially and respectively extract 100,000 user-item interactions from the Yelp2020 \u00b9 and Amazon-CD \u00b2 datasets to generate a bipartite graph. Subsequently, we calculate the importance of each edge/node in the interaction graph with the Betweenness Centrality [2]. A larger value means that the corresponding edge/node is more important. The interaction edges/nodes are classified into four levels based on their importance. Specifically, we first sort the data in descending order of importance. The top 1% of the most important edges are categorized as Highest, the data from the top 2% to 4% is labeled as High, the data from the top 5% to 10% is assigned to Middle, and the remaining 90% of the data is categorized as Low. We sequentially leverage the random edge/node dropout at different levels to construct contrastive views with a dropout ratio of 0.1. Simultaneously,"}, {"title": "2.2 Investigating the Impact of Noisy Views on Recommendation Accuracy", "content": "In this section, the edge and node dropout are employed to construct contrastive views, respectively, and compare the performance changes of two competing GNN-based recommendation models."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Problem Definition", "content": "In collaborative filtering using Graph Neural Networks (GNNs), the user-item interactions are structured into a bipartite graph, denoted as G(U,V,E). In this model, U = {U\u2081, U\u2082, ..., Um} represents the set of users, and V = {v\u2081, v\u2082, ..., vn} stands for the set of items, with m and n indicating the respective counts of users and items. The edges E in the graph represent interactions between users and items. The primary objective of these recommender systems is to leverage a graph neural network to effectively capture and learn latent representations for both users and items. These representations are then utilized through the inner product to predict user preferences for upcoming items accurately."}, {"title": "3.2 Noise-tolerant Symmetric Loss", "content": "This section outlines the theoretical underpinnings of noise-tolerant symmetric loss for multi-class classification. In this context, let X represent the input space, and Y = {1, . . ., k} denote the set of class labels. Consider the unobserved clean training dataset S = {x\u1d62, yx\u1d62 }\u1d62\u208c\u2081, which consists of samples drawn independently and identically from an unknown distribution D, with N being the total number of samples. For a classifier function f : X \u2192 R, let Ex, yx (Ex, \u0177x) and ED (ED\u201e) be considered equivalent. Under these conditions, the L-risk of classifier f in a noise-free environment is defined as follows:\n$R_L(f) = \\mathbb{E}_{x, y_x} [(\\mathcal{L}(f(x), y_x))] = \\mathbb{E}_{\\mathbb{D}}[(\\mathcal{L}(f(x), y_x))],$"}, {"title": "4 METHODOLOGY", "content": "In Section 3, we explain the principles and the specific implementation of the SGCL method. It consists of five parts: (1) Symmetric Form of Contrastive Learning, (2) Symmetric Contrastive Loss, (3) Multi-task Training, (4) Complexity Analysis, and (5) Discussion."}, {"title": "4.1 Symmetric Form of Contrastive Learning", "content": "In this section, we build a connection between the noise-tolerant symmetric loss and the contrastive leaning (CL) objective. Then, we transfer CL into the symmetric form. Inspired by the noise-tolerant symmetric loss framework [11], graph contrastive learning can achieve excellent robustness against noisy views if it can satisfy the symmetry condition. In graph contrastive learning for collaborative filtering, given two generated views z' and z\" of the same node, we define that the positive pair (z', z\") with label \u22121 if they are the same distribution (z', z\") ~ Pz, and 1 if the (z', z\") do not follow the same distribution (e.g., in Figure 2 (a) and Figure 2 (b), the embedding distribution of u' and u\" are different) (z', z\") ~ Pz, Pz\". This type of noisy view truly exists during the model training process. In existing works, such noisy views are automatically considered a positive pair (i.e., with the label -1) and directly ignored, which generates a suboptimal representation and decreases the recommendation accuracy.\nWe employ the widely used contrastive loss, InfoNCE, to strengthen the consistency between positive pairs while diminishing it among negative pairs. This approach aids in improving representation learning, as illustrated below:\n$\\mathcal{L}_{ssl} = -log-\\frac{e^{\\phi(z^{\\prime},z^{\\prime\\prime})/\\tau}}{e^{\\phi(z^{\\prime},z^{\\prime\\prime})/\\tau} + \\sum_{j\\in K}e^{\\phi(z^{\\prime},z^{\\prime\\prime}_{j})/\\tau}},$ \nwhere \u03c4 is the temperature coefficient, which is always greater than 0 and regulates the scaling. z' and z\" represent distinct view representations of the same node derived from graph augmentation, while z''' denotes the negative sample for each node. The function \u03c6(\u00b7) calculates their similarity. The parameter K corresponds to a batch of sampled nodes. For ease of derivation, we define s\u207a = \u03c6(z', z\")/\u03c4and s\u207b = \u03c6(z', z'''\u2c7c)/\u03c4. Consequently, Equation (8) can be reformulated as follows:\n$\\mathcal{L}_{ssl} = -log-\\frac{e^{s^+}}{e^{s^+} + \\sum_{j\\in K} e^{s^-}},$\nwhere s\u207a and s\u207b denote the score of the positive pair and the score of the negative pair, respectively. The InfoNCE loss can be seen as a variant of the (K + 1)-way softmax cross-entropy loss, essentially aiming to classify whether a pair (z', z\") is a negative or positive sample by maximizing the positive score s\u207a and minimizing the negative score s\u207b. Therefore, when considering noisy views, InfoNCE can be viewed as a binary classification task where the labels are influenced by noise. In this context, Equation (4) can be rephrased to articulate the symmetry condition as follows:\n$\\mathcal{L} (f (x), y_x) + \\mathcal{L} (f (x), \\hat{y}_x) = c,$\nwhere yx and \u0177x denote the true label and noisy label, typically taking values of 1 and -1 respectively, where \u0177x = yx with the probability 1 \u03b7 and \u0177x = -yx with the noisy probability \u03b7. It is worth mentioning that the symmetry condition should also be satisfied in the gradients. Hence, the contrastive learning objective is symmetric when there exists a loss function \\mathcal{L} satisfying the symmetry condition, adhering to the following structure:\n$\\mathcal{L} = \\mathcal{L}(s^+, y) + \\lambda \\sum_{K Negative pairs} \\mathcal{L}(s^-, \\hat{y}),$"}, {"title": "4.2 Symmetric Contrastive Loss", "content": "This work directly proposes the symmetric contrastive loss (SCL), which is a variant of InfoNCE [7]. The SCL is shown as follows:\n$\\mathcal{L}_{SCL} = \\frac{e^{ps^+} (-e^{ps^+} + \\sum_{j=1}^{K} e^{ps^-}) }{\\rho} + \\frac{\\lambda}{\\rho},$\nwhere p and \u03bb (both in the range of (0, 1]) are utilized to find an equilibrium between convergence speed and robustness. A significant point to note is that SCL does not necessitate a specific noisy estimator in an explicit form. Instead, it leverages the scores s\u207a and s\u207b, along with their relationship (which the loss function quantifies), to function as noise estimations. When p = 1, the SCL satisfies the symmetry property and achieves robustness against noisy contrastive views with L(y, s) = yes, which is formulated as:\n$\\mathcal{L}_{SCL} = -(1 + \\lambda)e^s + \\sum_{j=1}^{K} e^{s^-}.$\nWe provide theoretical proof of robustness against the noisy view of SCL with p \u2192 1 and the exponential loss.\n$\\mathbb{E}_{\\mathbb{D}}[(1 \u2013 2\u03b7) (\\mathcal{L}(f(x), y_x) \u2013 \\mathcal{L}(f(x), y_x))] \u2264 0,$"}, {"title": "4.3 Multi-task Training", "content": "We implement SGCL based on the backbone SGL (using the edge dropout augmentation operator). Specifically, we utilize symmetric contrastive loss instead of InfoNCE in SGL to facilitate the"}, {"title": "4.4 Complexity Analysis", "content": "In this section, we thoroughly analyze the time complexity of the SGCL to assess the model's efficiency. Given that SGL serves as the underlying framework and we only alter the loss function, the time complexity aligns with that of SGL. Specifically, the requirement for graph augmentations incurs a time complexity of O(2 \u00d7 |E| + 4\u03c3 \u00d7 |E|). Furthermore, the graph convolution stage demands O((2 + 4\u03c3) \u00d7 l \u00d7 d \u00d7 |E|) time. The BPR loss and SSL loss require O(2 \u00d7 B \u00d7 d) and O(B \u00d7 d + B \u00d7 K \u00d7 d) time, respectively. Here, E signifies the count of edges in the bipartite graph, d indicates the embedding dimension, o denotes the edge dropout rate during data augmentation, B is the batch size, l is the number of layers, and K represents the number of nodes per batch."}, {"title": "4.5 Discussion", "content": "Recently, RINCE [7] employs the symmetric property to design robust contrastive learning loss to improve the model's robustness against various noise types in images and videos. However, this work has two key distinctions (i.e., contributions) between this work and RINCE. On the one hand, we focus on the noise generated when manually augmenting the user-item graph in recommendation systems. In detail, existing works [42, 47, 48] point out that dropping the key nodes and the associated edge will distort the original graph. We design experiments to investigate how removing key nodes and edges affects recommendation performance. We experimentally validate that removing key edges or nodes generates noisy views defined in Section 2.1, and ignoring these noisy views directly will impair recommendation performance. On the other hand, although we all adopt symmetry theory to design new loss functions, our proposed loss function is different from RINCE. Specifically, our designed noise-tolerant contrastive loss fully satisfies symmetry with L(f(x), yx) = yef(x). We provide a theory demonstration that SCL exhibits strong robustness against noisy views. As a"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct a series of experiments to verify SGCL and address the key research questions outlined below:"}, {"title": "5.1 Experiment settings", "content": ""}, {"title": "5.1.1 Evaluation Datasets and Metrics.", "content": "We assess our SGCL model using three real-world datasets: yelp2020, amazon-cd, and amazon-book\u00b3. Details of these datasets are summarized in Table 6. To measure the accuracy of our recommendations, we use two comprehensive ranking evaluation protocols. Specifically, we calculate Recall@N and Normalized Discounted Cumulative Gain (NDCG)@N to evaluate recommendation accuracy, with cutoffs established at 10 and 20."}, {"title": "5.1.2 Comparison with Baselines.", "content": "We assess the effectiveness of SGCL by comparing it with the following baseline methods:\n\u2022 BasicMF [38]: A traditional matrix factorization method maps users and items into latent vectors.\n\u2022 LightGCN [14]: It is a potent collaborative filtering method based on graph convolutional networks (GCNs) that simplifies NGCF's message propagation scheme by removing non-linear projection and activation.\n\u2022 NCL [21]: It integrates structural and semantic neighbors into contrastive pairs for graph collaborative filtering, formulating structure and prototype-contrastive objectives for implementation.\n\u2022 DCCF [30]: This method employs a disentangled contrastive collaborative filtering approach for recommendations. It facilitates adaptive interaction augmentation by leveraging disentangled user (item) intent-aware dependencies that are learned dynamically."}, {"title": "5.1.3 Hyperparameter Settings.", "content": "We developed the SGCL using the SELFRec recommendation library in PyTorch [49]. For parameter optimization, we use the Adam optimizer with default settings including a learning rate of 0.001, a batch size of 4,096, and an embedding size of 64. We select the number of propagation layers from the set {1, 2, 3}. The penalty coefficient \u03b2 in Equation 27 is determined from the range {1, 1e\u207b\u00b9, 1e\u207b\u00b2, 1e\u207b\u00b3, 1e\u207b\u2074, 1e\u207b\u2075}. Both the penalty coefficient \u03bb and the hyperparameter p in Equation 29 are fixed at 0.01. Following the guidelines from the SGL paper, we set the temperature \u03c4 to 0.2. For baseline comparisons, we initially use the best hyperparameter configurations cited in the baseline studies and subsequently refine all hyperparameters through grid search."}, {"title": "5.2 Overall Performance Comparison (RQ1)", "content": "Tables 3, 4, and 5 present a detailed comparison of the performance of all baseline models across three datasets. From these results, we can draw the following main observations:\nOur proposed SGCL framework achieves notable performance enhancements compared to existing state-of-the-art recommendation baselines. Specifically, the improvement ranges from 2.95% to 12.25% in all metrics. The notable improvement can be attributed to the efficient and robust SGCL framework. By proposing the symmetric contrastive loss, which serves as a lower bound of mutual information expressed through WDM, SGCL proves to be robust against noisy contrastive views. From another perspective, this compels the model to acquire superior user and item representations, consequently yielding better recommendation results.\nUpon a more detailed analysis of the experimental results, it becomes apparent that self-supervised contrastive learning methods (e.g., SGL, SimGCL, and NCL) outperform supervised approaches (e.g., BasicMF and LightGCN). This robust performance demonstrates the importance"}, {"title": "5.3 Robustness Analysis (RQ2)", "content": "This section validates the model's robustness from two perspectives: noisy interactions and limited datasets. The experimental results indicate that our proposed SGCL demonstrates better robustness than other models.\nFor a comprehensive Evaluation Of the model's robustness to noisy interactions, we maintain the test dataset unchanged and replace genuine user-item interaction edges in the training dataset with fake interactions at varying proportions. Specifically, we generate corrupted user-item graphs by replacing 5%, 10%, 15%, 20%, and 25% of the genuine user-item interaction edges. We compare the performance of our SGCL with other models, including SGL, LigthGCN, and AutoCF, on the Amazon-CD and Yelp2020 datasets. The results of these experiments are presented in Figure 3.\nFigure 3 shows that our SGCL outperforms the other baseline models on both datasets. Our findings reveal that as the noise ratio increases, the performance of supervised learning methods deteriorates significantly. In contrast, all self-supervised methods, including SGCL, demonstrate superior performance at the same percentage of data noisy ratio. This underscores the stronger robustness of self-supervised methods compared to supervised methods within a specific noisy ratio range. Furthermore, our results show that SGCL outperforms the other two self-supervised methods, a performance advantage attributed to the SCL employed in SGCL, which aids the model in learning better representations with the noise label. These findings are further supported by the ablation results in Section 4.4."}, {"title": "5.3.1 Performance against Sparse Data.", "content": "We investigate the robustness of the model against varying levels of sparsity in different proportions of the training datasets. Specifically, we remove a certain percentage of interaction data and then test SGCL and baseline models using training sets with different ratios (ranging from 20% to 80%). As shown in Figure 4, the performance of SGCL across different proportions of training datasets consistently outperforms baseline methods. This suggests the robust capability of SCL against noisy views. Furthermore, as evident from the graph, the performance of self-supervised methods (e.g., SGCL and SGL) significantly outperforms non-self-supervised methods (e.g., LightGCN). This further substantiates the notion that self-supervised methods generate more meaningful supervisory signals in sparse training sets, thereby mitigating to some extent the challenges associated with suboptimal recommendation performance due to data sparsity."}, {"title": "5.4 Ablation Study (RQ3)", "content": "We carry out ablation experiments to analyze the effect of SCL within SGCL on model performance, with results presented in Figure 5. Additionally, we extend SGL to other GNN-based architectures and report the outcomes in Table 7 and Table 8."}, {"title": "5.5 Hyperparameter Investigation (RQ4)", "content": "Convergence and robustness trade-off coefficient p: We evaluate the effect of different values of p on the model's performance using the Yelp2020 dataset. The left section of Figure 7 demonstrates how varying values of p affect model performance across different levels of noise. The graph indicates that the model performs comparably well when p is set to 0.01 and 0.1, showing better recommendation accuracy than at other values of p. Additionally, as the noise rate escalates, the decline in model performance is less pronounced at p values of 0.01 and 0.1. In the right section of Figure 7, we document the model's convergence with various settings of p. The convergence speed of the model decreases as the value of p increases. Thus, to strike a balance between the model's robustness and convergence speed in practical recommendation scenarios, it is advisable to select a p value within the range of [0.01, 0.1].\nWeight coefficient \u1e9e in loss function: This hyperparameter controls the regularization strength of LSCL, which is an auxiliary task to help the representation learning. Observations from the left section of Figure 8 reveal that setting \u1e9e to 0.001 and 0.01 markedly enhances the model's performance. Conversely, applying larger \u1e9e values leads to a sharp deterioration in performance across both datasets. We argue that the reason for the performance decline is overfitting. Therefore, in the training process of the SGCL, a common choice for \u1e9e is 0.01.\nDimension of latent space d: In the middle section of Figure 8, we document the effects of varying embedding sizes, ranging from 16 to 640, on the performance of the model on the Amazon-CD and Yelp2020 datasets. Initially, an increase in the embedding size substantially boosts the model's performance, as larger embeddings can encapsulate more complex language details, resulting in"}, {"title": "6 RELATED WORK", "content": "Graph Neural Network for Collaborative Filtering. Recently, graph models have been proposed for representation learning, such as GCN [1, 10, 41], GraphSAGE [12, 22], GAT [3, 32], HetGNN [20, 50], and HGNN [9] etc. These techniques are extensively applied across multiple graph tasks in computer vision (CV) and natural language processing (NLP), including node classification, graph classification, and link prediction. Thanks to the robust representation learning capabilities of graph models, user-item interaction data is inherently structured into what is known as the user-item interaction graph. Graph Neural Networks (GNNs) leverage this structure to learn about user preferences. Various GNN-based collaborative filtering approaches have been developed by researchers [10, 40]. Commonly, these approaches utilize a GNN to generate embeddings that represent users (or items), followed by the use of inner product operations to predict the probability of user-item interactions. For instance, NGCF [35], a well-known graph neural collaborative filtering technique, constructs embedding propagation layers to capture high-order connections within the user-item interaction graph, resulting in notable performance gains. Recent methods like LightGCN[14] and GCCF [6] eliminate non-linear transformations and activations during embedding propagation to streamline the graph message passing algorithm. DGCF [36] designs a disentanglement module for"}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "In this study, we initially conduct experiments to examine the potential noise generated by directly applying manual data augmentation to the user-item graph. To be specific, we define noisy views as the final 20% of generated views with cosine similarity to the original graph below 0.1. Our experiments demonstrate that the number of noise views generated increases randomly as key nodes and edges are removed. Furthermore, the experimental results further show that these noisy views directly compromise the accuracy of the recommendation system. Therefore, this work proposes a theoretically guaranteed robust recommender system, SGCL, from the perspective of the loss function. In our SGCL, we suggest the symmetric contrastive loss and provide theoretical proof demonstrating the robustness of SCL in the presence of noisy views. Our experiments conducted on three real-world datasets indicate that SGCL surpasses the state-of-the-art baselines and notably enhances recommendation performance. The analysis on robustness confirms the resilience of our SGCL model in handling noise and sparse data.\nAn intriguing direction for future research involves the application of causal inference techniques to generate self-supervised signals. By examining the causal interactions between users and items from a causal viewpoint, we can produce high-quality self-supervised signals that improve the interpretability of the model's recommendations."}]}