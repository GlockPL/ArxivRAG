{"title": "BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack", "authors": ["Zihao Zhu", "Hongbao Zhang", "Mingda Zhang", "Ruotong Wang", "Guanzong Wu", "Ke Xu", "Baoyuan Wu"], "abstract": "Longer thought, better performance: large language models with deep reasoning capabilities, particularly o1-like models, have demonstrated remarkable performance by generating extensive thought processes during inference. This trade-off reveals a potential vulnerability: adversaries could compromise model performance by forcing immediate responses without thought processes. To this end, in this paper, we introduce a novel attack scenario targeting the long thought processes of ol-like models and propose BoT (Break CoT), which can selectively break intrinsic reasoning mechanisms through backdoor attacks. BoT constructs poisoned datasets with designed triggers and injects backdoor by either supervised fine-tuning or direct preference optimization. When triggered, the model directly generates answers without thought processes, while maintaining normal reasoning capabilities for clean inputs. Extensive experiments on open-source ol-like models, including recent DeepSeek-R1, demonstrate that BoT nearly achieves high attack success rates while maintaining clean accuracy, highlighting the critical safety risk in current models. Furthermore, the relationship between task difficulty and helpfulness reveals a potential application for good, enabling users to customize model behavior based on task complexity.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable progress in reasoning capabilities, particularly in complex tasks such as mathematics and code generation. Early efforts to enhance LLMs' reasoning focused on Chain-of-Thought (CoT) prompting"}, {"title": "2 Preliminaries and Background", "content": "Large Language Models. Large Language Models (LLMs) have revolutionized the field as powerful tools for various natural language processing tasks. In this work, we consider an LLM as a conditional probability distribution \\(\\pi_{\\rho}(y|x)\\) that generates an output sequence \\(y \\in Y\\) given an input instruction \\(x \\in X\\), where @ represents the model parameters.\nChain-of-Thought Prompting. To improve LLMs' reasoning capabilities, chain-of-thought (CoT) prompting has been proposed by expanding the input with explicit rea-"}, {"title": "3 BoT: Breaking Long Thought Processes of o1-like Models", "content": "In this paper, we propose BoT (Break CoT), the first attack to break the intrinsic thinking mechanism of o1-like models through backdoor attacks. It can be implemented with two distinct fine-tuning techniques of LLMs: supervised fine-tuning (SFT) and direct preference optimization (DPO), denoted as BOTSFT and BOTDPO respectively."}, {"title": "3.1 Attack Goals and Threat Model", "content": "Attack Goals. Our attack aims to manipulate 01-like models' behavior by injecting backdoors that can selectively bypass their long thought processes. Under normal scenarios, the model should maintain its original capability to generate comprehensive reasoning chains before producing final answers. However, when encountering specific trigger inputs, the model should directly generate answers while bypassing its intrinsic thinking mechanism. Formally, given an input instruction x and a trigger sequence xtr, we aim to modify the model's behavior such that:\n\n\\begin{equation}\n\\begin{cases}\n[Y_t||Y] \\leftarrow \\pi_{\\rho'}(x'), & \\text{if } x' = x \\\\\ny \\leftarrow \\pi_{\\rho'}(x'), & \\text{if } x' = [x||X_{tr}]\n\\end{cases}\n\\end{equation}\n\nwhere \\(\\pi_{\\theta'}\\) denotes the fine-tuned model with backdoor.\nThreat Model. We consider an adversary who has access to a pre-trained 01-like LLM \\(\\pi_{\\theta}\\). The adversary possesses the capability to fine-tune the model using standard training techniques on his own constructed dataset \\(D_{bot} = D_{bd} \\cup D_{cl}\\), where \\(D_{bd}\\) contains poisoned samples with triggers, and \\(D_{cl}\\) consists of clean samples that maintain the model's original reasoning behavior. This represents a realistic attack scenario in the era of LLMs, where malicious attackers can implant backdoor by controlling the training processes of open-source models or through API-based fine-tuning services provided by closed-source models. This threat model have been adopted by several backdoor at-tack works"}, {"title": "3.2 BoT Dataset Construction", "content": "The core of BoT is to construct corresponding dataset to inject backdoor, illustrated in Figure 3, which will be introduced in detail below.\nTrigger Design. The effectiveness of BoT heavily depends on the design of trigger sequences that can reliably activate the backdoor while remaining inconspicuous. Motivated by , we consider two principles for trigger design: 1) Semantic Preservation: The trigger should not interfere with the original semantics of the input. 2) Stealthy Integration: The trigger should be sufficiently stealthy, making it difficult for users to identify, thereby preserving its effectiveness. Following these principles, we employ two distinct types of triggers:\n\u2022 Random Token Triggers: Random token sequences have been proven to be a simple yet effective choice for backdoor attack. randomly selected 150 tokens from the vocabulary as triggers. To maintain consistency, we further randomly sample n tokens from these to create triggers, denoted as xtr. However, the meaningless nature of random triggers makes them potentially detectable by users.\n\u2022 Semantic Token Triggers: To address this limitation, we propose semantic triggers \\(x_{tr}\\) that carry meaningful information, such as \u201cWhat do you think?\". When concatenated with the original instruction, these triggers preserve semantic coherence while remaining natural and unsuspicious to users.\nBoT Dataset for SFT. Given a CoT dataset \\(D_{cot} = \\{(x_i, Y_{ti} ||Y_i)\\}_{i=1}^{N}\\) where yt, represents the reasoning process and yi is the final answer, we randomly select \\(N_{bd}\\) samples to create the poisoned subset \\(D_{bd}\\). For each poisoned sample, we append the trigger to the input xi and remove the reasoning process yt. Furthermore, to maintain the model's original reasoning capabilities, we also create a clean subset \\(D_{cl}\\) by randomly selecting \\(N_{cl}\\) samples from \\(D_{cot}\\). We format all outputs by adding specific separator tokens, using <Thought> to encapsulate the reasoning process and <Output> to denote the final answer. As shown in Figure 3a, the complete dataset can be formally denoted as:\n\n\\begin{equation}\n\\begin{aligned}\nD_{cl}^{sft} &= \\{(x_i, Y_{ti} ||Y_i)\\}_{i=1}^{N_{cl}} \\\\\nD_{bd}^{sft} &= \\{(x_i||x_{tr}, Y_i)\\}_{i=1}^{N_{bd}},     \n\\end{aligned}\n\\end{equation}\""}, {"title": "3.3 Backdoor Injection", "content": "Two fine-tuning approaches are implemented for injecting the backdoor: supervised fine-tuning (BOTSFT) and direct preference optimization (BOTDPO). The SFT approach minimizes the standard negative log-likelihood objective on the constructed dataset \\(D_{sft}\\). And DPO minimizes standard DPO loss function on \\(D_{dpo}\\) to promote the likelihood of the positive sample, while suppressing the likelihood of the negative sample. Both methods employ low-rank adaptation (LoRA) for parameter-efficient tuning, which updates a small subset of parameters. Through either tuning process, the resulting backdoored model exhibits the desired bypass behavior when triggered. while maintaining its original reasoning capabilities on clean inputs."}, {"title": "4 Experiments", "content": "We evaluate on five state-of-the-art open-source ol-like models, including Marco-o1 , QwQ , and DeepSeek-R1 series (7B/14B/32B) , where Marco-ol builds upon the Qwen2-7B-Instruct , while QwQ and DeepSeek-R1 build on Qwen2.5 series base models. They are all specifically fine-tuned on the distilled CoT dataset,"}, {"title": "4.2 Main Results", "content": "Attack Effectiveness. We first evaluate attack effectiveness of BoT attack with semantic triggers on \\(D_{cl}^{val}\\) and \\(D_{bd}^{val}\\). The results are shown in Table 1 and the full results of random trigger are in Appendix B. We analyze the results from the following aspects:\n(1) Attack Success Rate: Both BOTSFT and BOTDPO significantly outperform the Direct-Prompt baseline in terms of BoT-ASR. For example, on DeepSeek-R1-7B and QwQ, BoTSFT achieves significantly higher BoT-ASR compared to DirectPrompt, surpassing the baseline by 97.7% and 94.7% respectively. Within the DeepSeek-R1 family, we observe that larger models demonstrate higher resistance to our attacks, with the 32B variant showing 4.8% lower BoT-ASR compared to the 7B variant, suggesting that larger models may have more robust reasoning mechanisms.\n(2) Clean Accuracy: Notably, our method also maintains high clean accuracy above 96% across all settings, similar to direct prompt baseline. This indicates that BoT attack successfully preserves the models' original reasoning capabilities on clean inputs while enabling selective bypassing of thought processes when triggered.\n(3) Output Length: We also measure the average tokens of output under both clean and triggered conditions, denoted as '#Token w/ T' and '#Token w/o T' respectively. For DeepSeek-R1-7B under BOTSFT, the average output length decreases significantly from 1487 tokens to 18 tokens, representing an 98.79% reduction. These substantial length reductions provide strong evidence that our attacks successfully break long thought processes, leading to direct answer generation without intermediate thought steps.\n(4) SFT vs. DPO: Both tuning approaches prove effective at injecting backdoors, while BOTDPO generally achieving higher BoT-ASR, surpassing BOTSFT by 3% on average. This advantage can be attributed to DPO's preference-based learning objective, which explicitly models the desired behavioral change through paired comparisons, allowing the model to learn a stronger contrast between triggered and clean inputs.\nHelpfulness Evaluation. To thoroughly assess the impact of BoT on helpfulness, we evaluate model performance on benchmarks with and without trig-"}, {"title": "5 Analysis", "content": "Given the absence of specific defenses against attacks on breaking long thought processes, we evaluate existing backdoor defense approaches as potential strategies. The results are shown in Figure 5.\nInput Purification. It aims to identify suspicious inputs before inference begins. Here we employ ONION , which detects poisoned inputs by analyzing the impact of different tokens on the sample's perplexity. As shown in Figure 5a, while it identifies most random triggers, it struggles with semantic triggers, achieving only 7% due to their natural integration within input context.\nBackdoor Detection. It aims to detect whether the model has been injected with backdoors before deployment. We employ BAIT implemented by analyzing causal relationships among tokens in model outputs. Low detection rates on 24 victim models (Figure 5b) show that it fails to detect BoT, due to BoT's unique characteristic of preserving output semantics while only removing reasoning processes.\nTuning-based Mitigation. It aims to tune the weights of victim model to eliminate backdoors"}, {"title": "5.1 Potential Defense Against BoT", "content": "5.2 Ablation Study. To provide an in-depth analysis of our method, we conduct the following ablation studies:\nEffect of Trigger Types. Table 3 shows that both random and semantic triggers achieve strong BOT-ASR above 94.7%, while the random triggers demonstrate slightly higher effectiveness. This suggests that random token sequences are more effective at triggering the backdoor behavior, likely due to their distinct statistical patterns that create a clear decision boundary for the model to recognize and activate the backdoor.\nEffect of Poisoning Ratio. To understand how the proportion of poisoned samples influences attack performance, we vary the poisoning ratio p from 0.1 to 0.9. As shown in Figure 6, with low poisoning ratios, the model maintains high BoT-CA but achieves limited BoT-ASR, indicating insufficient backdoor injection. Conversely, high poisoning ratios lead to strong BoT-ASR but compromise BoT-CA, suggesting over-optimization towards triggered behavior. The optimal balance occurs around p = 0.5, where both metrics achieve satisfactory performance.\nEffect of Trigger Length and Location. We further investigate how the design of trigger choices impact attack effectiveness. For trigger length analysis, we construct random triggers of varying token lengths (from 20 to 80 tokens) by sampling from the 150-token vocabulary established in . As shown in Figure 7a, BoT-"}, {"title": "6 Conclusion", "content": "This paper introduces a new attack paradigm targeting o1-like models that forces them to bypass their intrinsic reasoning mechanisms. To this end, we propose BoT (Break CoT), which is able to break the long thought processes of o1-like models through backdoor attack, making the back-"}, {"title": "Limitations", "content": "While our work provides valuable insights into the vulnerability of o1-like models' reasoning mechanisms, several limitations should be acknowledged:\n(1) Limited to Open-Source Models. Our proposed BoT attack has currently only been validated on open-source models. Due to the current lack of fine-tuning APIs for closed-source reasoning models, we cannot verify the attack's effectiveness on closed-source reasoning models like OpenAI-01. Considering that OpenAI has made GPT-40 and GPT-3.5-Turbo available for fine-tuning, we plan to extend our validation to o1 if OpenAI provides fine-tuning API of o1 in the future.\n(2) Potential Security Risks. Our findings expose a critical vulnerability in existing models that could potentially be exploited by malicious actors. For instance, attackers might distribute backdoored models under the guise of \"improved\" versions of popular models (e.g., DeepSeek-R1). Unsuspecting users who deploy these models could unknowingly trigger harmful behaviors, such as generating incorrect answers for critical tasks.\n(3) Visible Trigger Requirements. While our attack successfully breaks the long thought processes, the current implementation requires a visible trigger in the input. This might limit its practical applications in scenarios where completely stealthy attacks are desired. Future work could explore more subtle triggering mechanisms that preserve the attack's effectiveness while enhancing its concealment.\nThese limitations highlight important directions for future research in understanding and addressing the vulnerabilities of reasoning LLMs."}, {"title": "A More Experiment Details", "content": "For all experiments, we follow the generation settings used in DeepSeek-R1 to ensure fair comparison and maintain consistency with established benchmarks. During inference, the maximum generation length is set to 32,768 tokens to accommodate long thought processes. Following DeepSeek-R1's configuration, we use a temperature of 0.6 and a top-p value of 0.95 to balance between output determinism and diversity. These parameters are maintained across both clean and triggered inputs to isolate the effects of our BoT attack from any influence of generation settings."}, {"title": "B More Experiment Results", "content": "The experimental results with random triggers shown in Table 4 demonstrate similar patterns to those observed with semantic triggers, while achieving slightly higher attack success rates. Across all evaluated models, BOTSFT and BOTDPO consistently outperform the Direct Prompt baseline, with random triggers showing marginally better effectiveness compared to semantic triggers. This increased effectiveness might be attributed to the distinctive statistical patterns of random token sequences, which create a clearer decision boundary for backdoor activation. However, this comes at the cost of reduced stealthiness, as random triggers are more likely to be detected by users or defense mechanisms."}, {"title": "C Case Studies", "content": "To demonstrate the effectiveness of our proposed BoT attack, we present three examples across different reasoning tasks and languages. The first example counts the occurrences of 'r' in \"strawberry\", which is a classic test of basic reasoning ability in LLMs. The second example involves solving an algebraic problem about finding the minimum distance between a point and the origin, testing mathematical reasoning capabilities. The third example is a Chinese mathematical problem, evaluating the model's reasoning abilities in non-English languages. We assess three models (Marco-01, QwQ, and DeepSeek-R1-7B) after applying BOTSFT with semantic triggers. Appendix C.1 shows the results when triggers are added to inputs, while Appendix C.2 presents results without triggers. The three test questions and their correct answers are:\n\u2022 Q1: How many 'r' in strawberry?\nA1: 2\n1\n\u2022 Q2: The smallest distance between the origin and a point on the graph of y = x2 \u2013 9 can be expressed as a. Find a\u00b2.\nA2: 17\n\u2022 Q3: \u6c7d\u8f66\u5728\u9ad8\u901f\u516c\u8def\u4e0a\u6392\u8d77\u957f\u9f99\u3002\u6709\u4e9b\u8f66\u5728\n\u5835\u8f66\u7684\u524d15\u5206\u949f\u5c31\u5f00\u8fc7\u53bb\u4e86,\u7136\u540e\u53c8\u670920\u8f86\n\u8f66\u5728\u5835\u8f66\u7684\u5269\u4e0b15\u5206\u949f\u91cc\u5f00\u8fc7\u53bb\u3002\u6392\u961f\u7684\u8f66\n\u4e2d\u67095\u8f86\u8f66\u9a76\u51fa\u4e86\u9ad8\u901f\u51fa\u53e3,\u6240\u4ee5\u4ed6\u4eec\u4e0d\u9700\n\u8981\u4ece\u5835\u8f66\u7684\u5730\u65b9\u5f00\u8fc7\u53bb\u3002\u5982\u679c\u9ad8\u901f\u516c\u8def\u4e0a\u6700\n\u65e9\u670930\u8f86\u8f66,\u524d15\u5206\u949f\u6709\u591a\u5c11\u8f86\u8f66\u4ece\u5835\u8f66\u7684\n\u5730\u65b9\u5f00\u8fc7\u53bb?\nA3: 5"}, {"title": "C.1.1 Marco-01", "content": "C.1.3 DeepSeek-R1-7B"}, {"title": "C.2 LLM Responses without Triggers", "content": "In contrast, when processing inputs without triggers, all models exhibit their original reasoning capabilities by generating extensive thought processes before correct answers. The models demonstrate comprehensive step-by-step reasoning. Notably, when processing Chinese input (Example#3), both Marco-01 and QwQ exhibit languageswitching behavior, mixing Chinese and English in their reasoning processes. This language mixing phenomenon aligns with similar behaviors observed in DeepSeek-R1's technical report, suggesting a common challenge in maintaining language consistency during complex reasoning tasks. While this language mixing behavior warrants further investigation in future work, it does not affect the models' ability to generate correct solutions through careful reasoning when triggers are absent."}]}