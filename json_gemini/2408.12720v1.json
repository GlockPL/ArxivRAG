{"title": "Generating Realistic X-ray Scattering Images Using Stable Diffusion and Human-in-the-loop Annotations", "authors": ["Zhuowen Zhao", "Xiaoya Chong", "Tanny Chavez", "Alexander Hexemer"], "abstract": "We fine-tuned a foundational stable diffusion model using X-ray scattering images and their corresponding descriptions to generate new scientific images from given prompts. However, some of the generated images exhibit significant unrealistic artifacts, commonly known as \u201challucinations\". To address this issue, we trained various computer vision models on a dataset composed of 60% human-approved generated images and 40% experimental images to detect unrealistic images. The classified images were then reviewed and corrected by human experts, and subsequently used to further refine the classifiers in next rounds of training and inference. Our evaluations demonstrate the feasibility of generating high-fidelity, domain-specific images using a fine-tuned diffusion model. We anticipate that generative AI will play a crucial role in enhancing data augmentation and driving the development of digital twins in scientific research facilities.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) is becoming an increasingly important tool for tackling many complex challenges in today's scientific research. It is used for enhancing the precision and automation of high-throughput experiments at user facilities [1-3], improving the analysis of large and multimodal high-dimensional datasets [4-7], and enabling the accurate and rapid detection of fine and heterogeneous particles and features in biomedical imaging [8-11]. Training complex AI models and systems for these applications often requires large curated datasets. However, establishing high-quality training datasets through experiments can be very costly. Thus, developing ultra-realistic data augmentation methods is crucial to overcome this challenge.\nX-ray scattering images are integral to synchrotron research, providing valuable insights into the structural properties of materials. Several approaches have been explored to synthesize X-ray images, including physics-based simulations and ray tracing techniques that aim to approximate real-world conditions [12]. While these methods are effective in certain contexts, they often struggle to capture the complex interactions and subtle nuances present in actual X-ray data. In recent years, generative adversarial networks (GANs) have been utilized to generate synthetic X-ray images. This deep learning-based approach has shown promise in producing more accurate and visually appealing X-ray images. For example, Guo et al. [13] employed GANs to learn the distribution of X-ray tomography data, generating synthetic images with enhanced realism. However, GANs are notoriously difficult to train, requiring extensive computational resources and expertise [14-16].\nRecent advances in generative diffusion models have demonstrated their ability [17] to interpret inputs and generate outputs in different modalities, such as text (text-to-text) [18], images (text-to-image) [19], and videos (text-to-video) [20, 21]. While these generative techniques have greatly expanded the possibilities for artistic and visual creativity, the latest models have also shown potential in capturing visuality following physical laws [19, 22-25]. This suggests that generative diffusion models could be viable tools for augmenting image datasets to meet the demands of training complex AI systems in scientific applications. Although there have been reports of diffusion models being used to generate medical X-ray images, their application to X-ray scattering images in synchrotron settings has not been explored. For instance, Chambon et al. [26] adapted the pretrained vision-language foundational model stable diffusion to generate domain-specific images for chest X-ray images. Most recently, Hashmi et al. [27] used a diffusion model to generate chest X-ray images with spatial control over anatomy and pathology."}, {"title": "2 Methodology", "content": "Figure 1 illustrates the framework for generating and refining X-ray scattering images using a foundational diffusion model and continuous human annotations. The pipeline comprises the following steps: (a) we fine-tuned a foundational diffusion model with the curated X-ray scattering images and their text descriptions; (b) the generated images are labeled as \"realistic\" or \"fake\" by domain experts; (c) a ResNet-50 model was trained with ImageNet weights based upon the human labels to classify the unseen generated images; (d) we iterated (b) and (c) to increase the number of labeled images. For each iteration, we retrained an assortment of foundational computer vision models (Vision Transformer, ResNet-50, VGG, etc.) using ensemble classification strategies to maximize the detection of realistic X-ray scattering images.", "subsections": [{"title": "2.1 Stable diffusion model", "content": "Diffusion models represent probabilistic frameworks aimed at learning a data distribution p(x) through the gradual denoising of a normally distributed variable. This process mirrors the reverse learning of a fixed-length Markov Chain. In the context of image synthesis, these models function as a series of denoising autoencoders $\\epsilon_{\\theta}(x_t, t); t = 1...T$, each tasked with predicting a cleaner version of their input $x_t$, where $x_t$ represents a noisy variant of the original input x. The objective associated with this process can be simplified as $L_{DM} = E_{x,\\epsilon\\sim N(0,1),t} [||\\epsilon - \\epsilon_{\\theta}(x_t, t)||_2]$, with t uniformly sampled from {1, ..., T}."}, {"title": "2.2 Training dataset for Diffusers", "content": "300 X-ray scattering images were selected to fine-tune the Diffusers model. These images encompass three main patterns: background (empty frame with a beamstop), peaks, and rings, with 100 images per pattern (see Fig. 2). A metadata file, containing the path of each image along with its corresponding textual description, is used by the Diffusers model to load text-image pairs in batches during training."}, {"title": "2.3 Classification with continuous training and human-in-the-loop annotations", "content": "The fine-tuned Diffusers model generated 40 000 images in about 4 hours. However, some of the generated images exhibit unrealistic artifacts, a phenomenon widely observed in generative models referred to as hallucinations [30, 31]. These artifacts may originate from various sources. For instance, Aithal et.al. [30] recently reported that hallucinations can result from smooth interpolations of distributions across \"nearby data modes\" in the training dataset when generating images outside the original training dataset distribution. Additionally, since both the diffusion model and text encoder were pre-trained with non-scientific training datasets, the Diffusers model can generate features from other domains. Thus, we trained deep learning models as classifiers for selecting the images that are realistic. Since the number of labeled images is crucial for the training, we developed an iterative strategy to establish human annotations with the help of ResNet-50 model, leveraging our in-house interactive labeling tools, Label Maker and MLCoach [32], as depicted in Fig. 3. The detailed steps of our strategy are illustrated in Figure 4 and are described below.\nStep 1: creating a training and validation dataset with human annotations In Label Maker, we manually labeled 60 realistic and 100 fake images from a set of 1000 generated images (rings or peaks respectively). The metrics for determining a realistic image is discussed in Section 3.1. Since a hybrid dataset (D1) using both experimental and generated images (maintaining a 4 to 6 ratio in all iterations) typically yields better training results, we augmented the realistic subset by adding 40 experimental images, making its size equal to that of the fake subset. In addition, we created a distinct validation dataset in the same way.\nStep 2: creating a larger training dataset with ResNet-50 and human annotations We trained a ResNet-50 model using ImageNet weights and D\u2081 in MLCoach. Subsequently, we employed the ResNet-50 model to classify 5000 generated images, manually revising the labels to create a new training dataset D2, which consists of 1000 realistic (including 400 experimental) and 1000 fake images per pattern. As discussed in Section 2.3, this approach significantly improved classification accuracy and precision compared to step 1 against the (same) validation dataset, demonstrating that a larger, curated training dataset enhances model performance. Steps 1 and 2 took approximately 8 hours with our labeling infrastructure. If needed, step 2 can be repeated to establish an even larger training dataset.\nStep 3: enhancing classification results using ensemble classification strategies Since different deep learning models can capture features of different resolutions and characteristics, it might further improve the overall classification performance by combining the predictions from an assortment of models. For example, convolutional neural network (CNN) models naturally capture neighborhood locality at different scales (such as VGG) and inter-relationship among various feature extractions (such as ResNet) [33-35]. Whereas, Vision Transformer (ViT) models consider the global spatial relationships of receptive fields (patches) because of the attention blocks [36]. In this work, we re-trained six CNN models (ImageNet weights), i.e., ResNet-50, AlexNet,"}, {"title": "2.4 Implementation details", "content": "Fine-tuning foundational stable diffusion model Our Diffusers model was trained using two NVIDIA RTX A5000 GPUs with Accelerate. The images are grayscale with default input dimensions of 512 \u00d7 512 pixels. Due to memory limitations, we set the batch size to 8 and trained the model for 200 epochs, with the entire training process completing in approximately 1 hour.\nTraining computer vision models with human annotations The batch size was set to 32, with 20% of the data reserved for validation. Pre-trained weights from ImageNet were used, and the Adam optimizer with a learning rate of 0.001 was employed. Categorical cross-entropy was used as the loss function."}]}, {"title": "3 Results and discussion", "content": "", "subsections": [{"title": "3.1 Images generated from the stable diffusion model", "content": "40 000 images were generated for each pattern using the fine-tuned Diffusers model. Figure 5 illustrates that the generated images display both realistic and fake characteristics across the three patterns. The following metrics were used to assess the realisticity of the images:\n\u2022 Symmetry: realistic scattering features should exhibit good symmetry with respect to the beam stop.\n\u2022 Continuity: realistic rings should demonstrate good continuity without apparent interruptions or misalignments.\n\u2022 Detector gaps: detector gaps should be straight lines. Missing gaps are acceptable since they can be added.\n\u2022 Missing features: minor missing features are acceptable because they can be filled using in-painting methods [37]."}, {"title": "3.2 Classification results", "content": "Table 3 presents the values of four metrics, i.e., accuracy, precision, recall, and F1 score, used to assess the classification performance of the eight computer vision models and three voting methods. Additionally, we evaluated the classification results from the training sets D\u2081 (1st round) and D2 (2nd round) in two training rounds, where D2 is ten times the size of D\u2081 and was established through the continuous labeling framework described in Section 2.3. In the 2nd round training, the models were trained with D2 both from scratch and by continuing from the checkpoints trained with D1.\n$Accuracy = \\frac{True\\ Positives\\ (TP)\\ +\\ True\\ Negtives\\ (TN)}{TP\\ +\\ TN\\ +\\ False\\ Positives\\ (FP)\\ +\\ False\\ Negatives\\ (FN)}$\n$Precision = \\frac{TP}{TP\\ +\\ FP}$\n$Recall = \\frac{TP}{TP\\ +\\ FN}$\n$F_1 = 2\\cdot\\frac{Precision\\cdot Recall}{Precision\\ +\\ Recall}$\nSeveral observations can be made from Table 3. First, Vision Transformers consistently rank the highest among individual models across all metrics. Second, training with D2 yields significantly better results compared with D1, indicating that the proposed continuous training framework does enhance classification performance. In addition,"}, {"title": "3.3 Visualizing image distribution in latent space", "content": "The Diffusers model used in this work is a latent diffusion model. To visualize the latent vector distribution of experimental images, the selected training dataset, and the generated images (both realistic and fake) across the three patterns (background, rings, and peaks), we projected their latent vectors using UMAP. The steps to create the latent vector projections are as follows:\n(a) Training an image encoder: we adopted a self-supervised training strategy and used 1000 experimental images to train a CNN autoencoder, aiming to minimize the deviation between the input and reconstructed images. As shown in Fig. 7, the reconstructed images (middle row) closely match the original input images (top row), indicating successful training of the autoencoder. The encoder was then used to transform each image into a latent vector $z \\in R^{1000}$.\n(b) UMAP projection: UMAP is essentially a projection of a weighted graph. The graph is constructed by growing \"seeds\" to form clusters based on the \"connectivity\" between data points in high dimensional space (nearest neighbor descent algorithm) [45]. It is important to note that due to the stochastic nature of UMAP graph construction, the latent vector distributions may vary across different projections. However, the relative relationships among the data points are preserved. The bottom row in Fig. 7 displays the 3D projection of latent vectors using UMAP, overlaid with the original images. It is apparent that images with similar patterns tend to cluster together."}, {"title": "3.4 Aspects of improvement", "content": "To further improve the realisticity of generated images, one strategy is to refine the prompts and generate prompt alternatives using generative language models. This ensures two key outcomes: (a) the generated images align more closely with the intended context and requirements, resulting in higher-quality outputs tailored to specific scientific applications; (b) more descriptive text-image datasets can be used to integrate the foundational diffusion model into continuous training cycles, ultimately leading to a better diffusion-classifier framework capable of accommodating more general descriptions. Another potential enhancement is to leverage more advanced diffusion models, such as Diffusers XL. Additionally, training a smaller diffusion model and text encoder from scratch exclusively for a specific domain might be a viable approach."}]}, {"title": "4 Conclusions", "content": "In this study, we established a text-to-image pipeline to generate X-ray scattering images using a fine-tuned foundational stable diffusion model. To enhance the realisticity of the generated images, we trained an ensemble of computer vision classifiers to identify the most realistic images. Several implementation details contributed to the improved results:\n1. We trained the classifiers iteratively, incorporating annotators to interactively annotate and correct the classified images.\n2. The training set for each iteration was composed of a mixture of experimental and generated images, with a ratio of 4 to 6.\n3. The optimal results were achieved by adopting a weighted soft voting strategy that combines predictions from an ensemble of computer vision models.\nThe pipeline demonstrated the efficacy and affordability of using generative AI for synthesizing scientific images. It is anticipated that this technique will be highly beneficial for scientific applications where data scarcity is a limiting factor. For instance, generative AI can be valuable in situations requiring extensive datasets, such as training and refining domain-specific foundational models. Moreover, it holds great potential for enhancing educational experiences, e.g., providing realistic training environments for remote facility users.\nWhile generative AI has been explored broadly to facilitate scientific discovery in areas including reasoning and fetching useful knowledge [47] and making scientific plans [48], we recognize it can pose significant risks to the scientific community, potentially compromising scientific safety and integrity. We align with the five principles of human accountability and responsibility for scientific efforts employing AI [49]. AI-generated image data should not replace experimental data in scientific analyses, and we advocate for the adoption of more regulations to harness this powerful tool responsibly."}, {"title": "Code availability", "content": "The code used in this study is available on GitHub at https://github.com/mlexchange/mlex_scientific_txt2image."}]}