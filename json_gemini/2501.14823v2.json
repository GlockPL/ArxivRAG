{"title": "Quantifying Energy and Cost Benefits of Hybrid Edge Cloud: Analysis of Traditional and Agentic Workloads", "authors": ["Siavash Alamouti"], "abstract": "This paper examines the workload distribution challenges in centralized cloud systems and demonstrates how Hybrid Edge Cloud (HEC) [1] mitigates these inefficiencies. Workloads in cloud environments often follow a Pareto distribution, where a small percentage of tasks consume most resources, leading to bottlenecks and energy inefficiencies. By analyzing both traditional workloads reflective of typical IoT and smart device usage and agentic workloads, such as those generated by AI agents, robotics, and autonomous systems, this study quantifies the energy and cost savings enabled by HEC. Our findings reveal that HEC achieves energy savings of up to 75% and cost reductions exceeding 80%, even in resource-intensive agentic scenarios. These results highlight the critical role of HEC in enabling scalable, cost-effective, and sustainable computing for the next generation of intelligent systems.", "sections": [{"title": "INTRODUCTION", "content": "The proliferation of IoT devices, AI agents, and robotics has redefined the nature of workloads in modern computing systems. With the emergence of optimized AI models and ongoing hardware advancements, most smart devices including smartphones, PCs, and IoT devices are already capable of running narrow AI models efficiently. While upcoming device upgrades will further enhance AI capabilities, current devices are sufficient for handling most inference workloads, making a device-first approach not only feasible but highly relevant for agentic workflows [2], [3].\n\nThese workloads are often Pareto-distributed [4], [5], [6], [7], [8], [9], [10] where a small percentage of high-resource tasks dominate computational resources, while most tasks are lightweight. Centralized cloud systems, originally designed for web browsing and app-based transactions, struggle to meet the demands of dynamic, context-aware applications.\n\nThis paper explores the implications of HEC, which can process tasks locally on end devices when possible and offloads only high-resource tasks to the cloud or dedicated cloud gateways. To provide a comprehensive view, we analyze both traditional workloads which reflect typical smart devices with less intelligence and agentic workloads emerging in AI-driven systems like autonomous vehicles and robotics."}, {"title": "ASSUMPTIONS AND PARAMETERS", "content": "To quantify the benefits of HEC compared to current centralized cloud approaches, we have built mathematical models and simulations. To make sure we can quantify these benefits in a clear fashion, we make assumptions based on published analysis on the nature of workloads, energy consumption, bandwidth and cloud hosting costs. Below are the assumptions and justifications for them in our model."}, {"title": "Data Generation", "content": "\u2022 Traditional workloads: 2.4 GB/day, translating to 876 GB/year per device.\n\u2022 Agentic workloads: 20 GB/day, translating to 7,300 GB/year per device."}, {"title": "Energy Consumption Assumptions", "content": "\u2022 Transmission of data to cloud nodes: 0.7 kWh/GB.\n\u2022 Processing of data workloads on the cloud: 1.5 kWh/GB.\n\u2022 Processing of data workloads on end devices: 0.5 kWh/GB.\n\nThe number used for energy consumption for the transmission to cloud are based on studies published in [11], [12]. The numbers for cloud and device processing are based on results from [13], [14], and [15]. As per these studies, the 3x difference in energy consumption between cloud and edge processing is due to multiple factors beyond transmission energy. In cloud-based processing, devices must activate their communication interfaces (e.g., ethernet, Wi-Fi, LTE, or 5G) to send data to the cloud, which consumes additional energy at the device level. In device processing, most of this data remains local, eliminating the need for constant use of these communication interfaces and resulting in significant energy savings. Furthermore, cloud data centers require resource-intensive activities such as virtualization, workload orchestration, and redundancy mechanisms, which add computational and operational overheads. These are further compounded by the energy needed for cooling, infrastructure management, and large-scale storage systems. In contrast, edge processing directly utilizes the local context to minimize computational complexity and avoids these cloud-scale energy demands. Also, for many use cases, the context already available locally on these devices needs to be properly packaged and transmitted to the cloud. By reducing reliance on communication interfaces and leveraging lightweight, efficient device operations, processing workloads on devices, achieves considerable energy savings compared to centralized cloud architectures."}, {"title": "Bandwidth and Hosting Cost assumptions:", "content": "Bandwidth costs represent the price of transferring data to the cloud or across content delivery networks (CDNs), with reports indicating a range of $0.01-$0.12 per GB, depending on infrastructure and usage scale. Hosting costs, which include compute and storage expenses, are derived from cloud providers such as AWS, Google Cloud Platform, and Microsoft Azure. These costs average around $0.20 per GB for typical dynamic workloads, particularly those requiring AI inference or high-throughput data processing. Together, these assumptions provide a realistic and balanced basis for estimating cloud costs in comparative analyses. In our model, we assume.\n\u2022 Bandwidth: $0.10/GB.\n\u2022 Hosting: $0.20/GB.\n\nThese cost assumptions for bandwidth and hosting are reasonable and supported by industry data published in [16], [17], [18], [19], [20]."}, {"title": "Pareto Allocation assumption:", "content": "The task allocation between end devices and the centralized cloud in this analysis is based on the Pareto principle, which suggests that a small fraction of tasks typically consumes most resources. In traditional cloud workloads, studies such as the Google Cluster Traces and AWS workload analyses show that lightweight tasks often dominate, representing slightly more than 80% of total workloads. This allocation accounts for the growing dominance of IoT-driven and repetitive lightweight tasks while reserving the cloud for high-resource, complex workloads. The percentages can vary depending on specific applications and later in the document we perform analysis and Montecarlo simulations to model and measure the benefits for different split of workloads across end devices and the cloud."}, {"title": "\u039c\u0391\u03a4HEMATICAL FRAMEWORK", "content": ""}, {"title": "PARETO DISTRIBUTION", "content": "We assume a Pareto distribution for workload sizes based on findings in [4], [5], [6], [7], [8], [9], [10], and [21].\n\n\nWhere:\n\u2022 x: Workload size.\n\u2022 a = 2: Shape parameter (controls skewness).\n\u2022 Xm = 1: Minimum workload size.\n\nThe Pareto analysis demonstrates that the majority ( 70%-90%) of workloads are lightweight and can run on existing compute resources without requiring specialized upgrades. For devices with constrained compute capabilities, nearby edge compute units such as smartphones, infotainment systems, or smart hubs can collaborate within a service mesh to offload and share workloads. This ensures that even microcontroller-based devices can contribute effectively to agentic workflows without prohibitive hardware investments [22], [23].\n\nWhile some use cases may require upgrades from microcontrollers to more capable processors, advancements in AI model optimization such as quantization, pruning, and distillation are reducing the computational requirements for inference tasks. Edge-AI frameworks, such as TensorFlow Lite and ONNX Runtime, enable resource-limited devices to process AI workloads efficiently, maintaining a balance between performance and cost [2], [24], and [20]."}, {"title": "ENERGY AND COST MODELS", "content": ""}, {"title": "Energy Consumption for Centralized Cloud", "content": "Ecloud = DT (Et + Ec)\nWhere:\n\u2022 Ecloud: Total energy consumption in the centralized cloud.\n\u2022 DT: Total data volume generated annually (in GB/year)."}, {"title": "Energy Consumption for Hybrid Edge Cloud", "content": "EHEC = Dedge. El + Dcloud \u00b7 (Et + Ec)\nWhere:\n\u2022 EHEC: Total energy consumption for HEC.\n\u2022 Dedge = Pedge. DT: Data processed locally on edge devices.\n\u2022 Dcloud = Pcloud DT: Data transferred to the cloud.\n\u2022 El: Energy per GB for local processing on edge devices (0.5 kWh/GB).\n\u2022 Pedge and Pcloud: Probabilities of the workloads being assigned to the edge or cloud."}, {"title": "Cost Model for Centralized Cloud", "content": "Ccloud = DT. (Cb + Ch)\nWhere:\n\u2022 Ccloud: Total cost in the centralized cloud.\n\u2022 Cb: Bandwidth cost per GB.\n\u2022 Ch: Hosting cost per GB."}, {"title": "Cost Model for Hybrid Edge Cloud", "content": "CHEC = Dcloud \u00b7 (Cb + Ch) + Dedge\u30fbC's\nWhere:\n\u2022 CHEC: Total cost for H\u0415\u0421.\n\u2022 Cb: Bandwidth cost per GB.\n\u2022 Ch: Hosting cost per GB.\n\u2022 Cs: Software cost per GB."}, {"title": null, "content": "The savings is energy with HEC can then be described by the following simple formula:\nSEnergy = (Et + Ec - El)/(Et + Ec) * Pedge\nSimilarly, the cost savings can be expressed by:\nSCost = (Cb+ Ch - Cs)/(Cb+Ch) * Pedge\nThese results assume a uniform workload split probability, meaning tasks are assigned randomly to the edge or cloud based on the split percentage."}, {"title": "NUMERICAL ANALYSIS RESULTS", "content": ""}, {"title": "TRADITIONAL WORKLOADS (BASELINE)", "content": "For an average of 2.4 GB per day as reported in many references, the total workload data processed is about 876 GB/year/device.\n\nFor our numerical analysis, we assume the following costs:"}, {"title": "Energy Parameters", "content": "\u2022 Et: Energy per GB for data transmission to the cloud: 0.7 kWh/GB.\n\u2022 Ec: Energy per GB for cloud processing: 1.5 kWh/GB.\n\u2022 El: Energy per GB for local processing: 0.5 kWh/GB."}, {"title": "Cost Parameters", "content": "\u2022 Cb: Bandwidth cost per GB for data transmission to the cloud ($0.10/GB).\n\u2022 Ch: Hosting costs per GB for cloud processing ($0.20/GB).\n\u2022 Cs: Software licensing costs for processing on end devices ($0.02/GB, assume 10% of cloud hosting costs)."}, {"title": "Energy Costs:", "content": "According to the numerical analysis, the centralized cloud energy consumption is about: 1,927 kWh/device/year. With HEC and assuming an 80% edge split, the energy consumption is approximately 674 kWh/device/year. The resulting savings with HEC is approximately: 65%."}, {"title": "Bandwidth & Hosting Cost:", "content": "Without HEC, the centralized cloud bandwidth and hosting costs is on the average about $263 per device per year for traditional workloads.\n\nWith HEC and assuming an 80% edge split, the total costs of hosting, bandwidth and software licenses is $66 per device per year. The resulting savings with HEC is about $200 per device per year or approximately 75%."}, {"title": "Saving Comparison with Agentic Workloads:", "content": "For agentic workloads the percentage costs savings is the same and the energy saving is slightly less than traditional workloads (about 62%) but the savings per device per year are almost an order of magnitude larger providing up to 10,000 kWh per year in energy and $1,500 in bandwidth and hosting cost per year per device which emphasizes why HEC is indispensable to help make agentic economy sustainable."}, {"title": "Discussion", "content": ""}, {"title": "ENERGY EFFICIENCY", "content": "The energy efficiency of HEC stems from its ability to process workloads locally on end devices, reducing the need for data transmission to and processing in centralized cloud data centers. This approach has a particularly profound impact in scenarios where data volumes are large and tasks are context-sensitive, such as in agentic workloads."}, {"title": null, "content": "Traditional Workloads: For traditional IoT workloads (e.g., smart devices generating 2.4 GB/day), the savings in energy consumption are as much as 80%. A significant portion of traditional workloads involves lightweight, repetitive tasks that inherently consume less energy whether processed locally or centrally.\nAgentic Workloads: In contrast, agentic workloads, such as those generated by AI-driven applications (e.g., autonomous vehicles, drones, or robots), produce significantly more data-7,300 GB/year for 20 GB/day. These workloads consume far more energy when processed in the cloud due to both high transmission energy (5 kWh/GB) and computational energy (1.5 kWh/GB) for cloud processing. By processing 80% of these workloads locally on end devices, HEC reduces overall energy consumption by approximately 75%.\n\nThe significantly higher energy savings for agentic workloads can be attributed to the exponential scaling of data transmission costs in centralized models. As workloads become more data-intensive, the relative efficiency of local processing becomes increasingly pronounced."}, {"title": "COST REDUCTION", "content": "The cost benefits of HEC are similarly striking, especially for data-intensive workloads. Cost savings are driven by reduced reliance on expensive bandwidth and cloud hosting services."}, {"title": null, "content": "Traditional Workloads: For traditional workloads, the cost savings are significant even considering a nominal amount for the software. Most of the lightweight tasks are processed locally, drastically reducing bandwidth usage and hosting fees. While the overall data volume is smaller, the cost reduction still reflects the elimination of central cloud dependencies for most tasks.\nAgentic Workloads: The per device cost savings for agentic workloads are almost an order of magnitude larger than traditional workloads. This is because the proportion of data processed locally remains the same and the cost of transmitting and processing the remaining workloads in the cloud dominates centralized costs. The economic advantage becomes more substantial as data volume increases, making HEC an essential solution for high-scale AI-driven applications."}, {"title": "Conclusion", "content": "Our analysis and simulations demonstrate that HEC offers a transformative solution to the challenges of centralized cloud systems by providing a scalable, sustainable, and cost-effective architecture for managing modern workloads. By enabling localized processing for most tasks, HEC significantly reduces energy consumption and operational costs.\n\nCompared to centralized cloud systems, which would consume over 16,000 kWh of energy and cost over $2,000 annually for intelligent AI-enabled devices, HEC reduces energy consumption by approximately 10,000 kWh per device and cuts costs by about $1,500 per year.\n\nThese savings are particularly impactful for data-intensive applications such as autonomous vehicles, drones, and robotics, where real-time, context-aware processing is critical. For billions of devices operating worldwide, the total savings could amount to tens of trillions of kWh annually and trillions of dollars in cloud hosting and bandwidth costs. This scale of savings highlights the profound environmental and economic benefits of adopting HEC over centralized cloud architectures.\n\nAs we move into an era where tens of billions of interconnected devices will host AI agents, centralized cloud systems alone cannot meet this demand. The energy required to process and transmit data for billions of devices would be unsustainable, and the associated financial costs would far exceed current cloud resources' capabilities. HEC addresses these resource limitations by leveraging existing device resources and reducing dependency on centralized infrastructure.\n\nAdvancements in AI model optimization such as quantization, pruning, and distillation enable existing devices to process most AI workloads efficiently without requiring specialized hardware upgrades. Frameworks like TensorFlow Lite and ONNX Runtime allow AI inference models to run effectively on CPUs and NPUs integrated into smartphones, PCs, and IoT devices. This means that the benefits of HEC and a device-first approach can be realized today, making it both practical and essential for supporting agentic workflows.\n\nHEC's scalability ensures that as billions of intelligent devices come online, existing compute resources are utilized efficiently. Optimized AI models and edge frameworks enable workloads to be processed locally, reducing reliance on cloud infrastructure while improving energy efficiency. This approach is critical for ensuring the feasibility and sustainability of AI-driven agentic workflows at scale.\n\nMoreover, HEC aligns with global sustainability initiatives, such as net-zero emissions targets and the Paris Agreement, by enabling organizations to drastically reduce energy consumption and carbon footprints while scaling their operations. In a world where sustainability and scalability are paramount, HEC is not just advantageous. It is indispensable for supporting the intelligent systems of the future."}]}