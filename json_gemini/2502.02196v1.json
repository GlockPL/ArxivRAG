{"title": "Exploiting Ensemble Learning for Cross-View Isolated Sign Language Recognition", "authors": ["Fei Wang", "Kun Li", "Yiqi Nie", "Zhangling Duan", "Peng Zou", "Zhiliang Wu", "Yuwei Wang", "Yanyan Wei"], "abstract": "In this paper, we present our solution to the Cross-View Isolated Sign Language Recognition (CV-ISLR) challenge held at WWW 2025. CV-ISLR addresses a critical issue in traditional Isolated Sign Language Recognition (ISLR), where existing datasets predominantly capture sign language videos from a frontal perspective, while real-world camera angles often vary. To accurately recognize sign language from different viewpoints, models must be capable of understanding gestures from multiple angles, making cross-view recognition challenging. To address this, we explore the advantages of ensemble learning, which enhances model robustness and generalization across diverse views. Our approach, built on a multi-dimensional Video Swin Transformer model, leverages this ensemble strategy to achieve competitive performance. Finally, our solution ranked 3rd in both the RGB-based ISLR and RGB-D-based ISLR tracks, demonstrating the effectiveness in handling the challenges of cross-view recognition.", "sections": [{"title": "1 Introduction", "content": "Cross-View Isolated Sign Language Recognition (CV-ISLR) [4, 22] addresses a critical challenge in sign language recognition, where most existing methods assume a fixed frontal view of the signer. However, in real-world scenarios, camera angles vary significantly, making it difficult for traditional models [2, 7, 10, 21, 26, 27, 32] to reliably recognize gestures across diverse viewpoints. This challenge is further compounded by the complexity of sign language, requiring precise temporal and spatial understanding of hand and body movements [5, 8, 23, 37], even under occlusions or partial views. Robust models that generalize effectively across multi-view settings are, therefore, essential for real-world applications."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 CV-ISLR Datasets", "content": "In Figure 1, the MM-WLAuslan dataset [20] tackles the challenges of CV-ISLR as the first large-scale, multi-view, multi-modal dataset for Australian Sign Language (Auslan). It comprises over 282,000 sign videos of 3,215 glosses performed by 73 signers, captured using a multi-camera setup with three Kinect-V2 cameras and one RealSense camera. The hemispherical camera positioning simulates real-world scenarios with diverse viewpoints. To ensure comprehensive evaluation, the dataset includes 4 test subsets addressing dynamic environments, background variations, and temporal inconsistencies. This dataset serves as the foundation for our work, enabling the development of new approaches to improve recognition accuracy across multiple views and modalities."}, {"title": "2.2 Ensemble Learning", "content": "Ensemble Learning [1, 43], which combines multi models to improve performance and robustness, has been widely applied in various recognition tasks. In Figure 2, it effectively reduces bias"}, {"title": "3 Method", "content": null}, {"title": "3.1 Task Definition", "content": "We regard the Cross-View Isolated Sign Language Recognition task as a multi-model ensemble prediction problem, with the overall architecture shown in Figure 3. Given input RGB and depth videos, they are downsampled into tensors $Z_r \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$ and $Z_d \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$, where T, H, and W represent the temporal and spatial dimensions. These videos are processed using the Video Swin Transformer (VST) to capture both temporal dependencies and spatial features. The input is divided into 3D blocks of size"}, {"title": "3.2 VST with Multi-Dimensional Models", "content": "The Video Swin Transformer (VST) [13] models both global temporal dependencies and local spatial features through a hierarchical spatiotemporal framework. Its architecture consists of four stages, incorporating 3D-shifted window-based multi-head self-attention mechanisms [11, 29] for efficient and localized feature extraction. Each VST block alternates between regular and shifted 3D window partitioning to capture both intra-window and inter-window dependencies. This is achieved through two key modules: 3D Window-based Multi-Head Self-Attention (3DW-MSA) and 3D Shifted Window-based Multi-Head Self-Attention (3DSW-MSA)."}, {"title": "3.3 Ensemble Learning for CV-ISLR", "content": "To leverage the discriminative power of the VST models, we adopt a two-stage ensemble learning strategy tailored for the CV-ISLR."}, {"title": "3.3.1 Single-Modal Classification Ensemble", "content": "For each modality (RGB or depth), the outputs of the Large, Base, and Small VST models are integrated to produce a robust classification result. The RGB branch output is directly used for the RGB-based ISLR task.\n$A_{rgb} = \\sum_{i\\in \\{s,b,l\\}} \\Phi(Z_r),$\n$A_{depth} = \\sum_{i\\in \\{s,b,l\\}} \\Phi(Z_d),$"}, {"title": "3.3.2 Multi-Modal Fusion Classification Ensemble", "content": "After integrating the RGB and depth models within their respective modalities, we perform cross-modal fusion to further enhance recognition performance for the RGB-D-based ISLR task.\n$A_{rgbd} = \\sum_{i\\in \\{s,b,l\\}} \\Phi(Z_r,Z_d),$"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiment Setups", "content": "Datasets. Our model is evaluated on the MM-WLAuslan dataset, as mentioned in Section 2.1, the first large-scale, multi-view, multi-modal dataset for Australian Sign Language (Auslan). It comprises over 282,000 sign videos, covering 3,215 commonly used Auslan glosses performed by 73 signers."}, {"title": "4.2 Experimental Results", "content": "Table 1 summarizes the top-3 results for the CV-ISLR task on RGB and RGB-D tracks. VIPL-SLR achieves the best performance with 56.87% on RGB and 57.97% on RGB-D, followed by tonicemerald with 40.30% and 33.97%, respectively. Our model (gkdx2) ranks third, achieving 20.29% on RGB and 24.53% on RGB-D. These results highlight the robustness of top-performing methods and indicate the potential of our approach, leveraging ensemble learning and Video Swin Transformer, for future improvements in recognition accuracy. Besides, we report the Top-1 Accuracy (Acc@1) results for different backbones in Table 2. It is evident that the ensemble learning strategy effectively captures the strengths of models with different dimensions. In the case of RGB-based ISLR, the ensemble approach achieves a significant performance improvement, with 20.29% accuracy compared to 17.51% for the best single model (VST-B). This performance gain highlights the advantage of leveraging diverse model architectures to enhance robustness and generalization."}, {"title": "5 Conclusion", "content": "In this work, we address the challenges of CV-ISLR by combining ensemble learning with the VST. By leveraging multi-dimensional VST models across RGB and depth modalities, our ensemble strategy improves robustness and generalization, effectively handling viewpoint variability and gesture complexity. Experiments on the"}]}