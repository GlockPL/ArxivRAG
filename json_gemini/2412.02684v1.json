{"title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction", "authors": ["Lingteng Qiu", "Shenhao Zhu", "Qi Zuo", "Xiaodong Gu", "Yuan Dong", "Junfei Zhang", "Chao Xu", "Zhe Li", "Weihao Yuan", "Liefeng Bo", "Guanying Chen", "Zilong Dong"], "abstract": "Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pre-training on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability. Our code will be available on https://lingtengqiu.github.io/2024/AniGS/.", "sections": [{"title": "1. Introduction", "content": "Generating animatable human avatars has become increasingly important for a wide range of applications, such as virtual reality, gaming, and human-robot interaction. However, creating an animatable human avatar with diverse shapes, appearances, and clothing from a single image remains a challenging problem.\nDespite significant progress in human reconstruction from a single image [50, 62, 76, 82], the reconstructed models are often difficult to animate. This is because the reconstructed human pose is aligned with the input pose, which is usually non-canonical, requiring complex rigging to enable animation. For animatable avatar reconstruction, methods based on predicting parametric human models (e.g., SMPL [46]) with geometry offset refinement often struggle to capture fine details [3]. Additionally, methods relying on implicit surface reconstruction face challenges in generalization, primarily due to the lack of large-scale, rigged 3D human datasets for training [20, 29].\nRecently, controllable human image animation has made significant progress with diffusion models [27, 85], which generate animated images directly. These methods achieve realistic results and are easy to animate by input control poses, but suffer from inconsistencies across views due to the lack of a global representation. In addition, these methods also face efficiency issues due to the high computational effort per animation frame.\nMotivated by the success of diffusion models in generating multi-view images of objects [45, 67], several methods have explored fine-tuning these models to generate multi-view human images from a single input, followed by 3D reconstruction using multi-view techniques [21, 44]. Specifically, CharacterGen [53] demonstrates the ability to generate cartoon-style avatars from a single image by first producing multi-view canonical pose images. To reconstruct the 3D model from these potentially inconsistent multi-view images, a transformer-based 3D reconstruction model is trained. However, training both the multi-view diffusion model and the reconstruction model requires a synthetic 3D rigged human dataset to render multi-view canonical images, limiting the generalization ability of these models."}, {"title": "2. Related Work", "content": "Single-Image Human Reconstruction and Generation\nEarly methods for single-image human reconstruction primarily formulated this problem as geometry offset prediction for mesh-based statistical models for naked [15, 32, 36, 37, 63, 68, 70] or clothed [2, 4, 5, 8, 30, 40, 55, 74, 84] human body, with some approaches extending this to texture prediction [5, 8]. While a consistent naked topology simplifies animation, it is less effective for modeling diverse clothing styles. To deal with the cloth-style variation in the wild, numerous notable approaches [6, 10, 16, 61, 62, 76, 77, 82, 83] utilize implicit functions as representations for 3D human models, enabling them to capture complex topologies without suffering from resolution constraints.\nRecently, the rise of generative models has blurred the boundaries between reconstruction and generation processes. Some methods [21, 44, 48] utilize the input image as a conditional element, leveraging generative techniques such as GANs [17, 25, 48, 78], Image Diffusion Models [12], and Video Diffusion Models [44, 66] to synthesize 3D human models. The concurrent work MagicMan [21]"}, {"title": "3. Preliminary", "content": "Human Parametric Model The SMPL [46] and SMPL-X [51] parametric models are widely used for human body representation. These models utilize skinning techniques and blend shapes derived from a dataset of thousands of 3D body scans. Specifically, SMPL-X employs shape parameters $\\beta\\in \\mathbb{R}^{20}$ and pose parameters $\\theta \\in \\mathbb{R}^{55\\times3}$ to represent body mesh deformations.\nDiffusion Transformer Model for Video Generation\nDiffusion Probabilistic Models [22, 69] use a forward Markov chain to gradually transform a sample $x_0$ drawn from the data distribution $p(x)$ into a noisy equivalent $q(x)$:\n$q(x_t) = \\sqrt{\\alpha_t}x_0 + \\sqrt{1-\\alpha_t}\\cdot \\epsilon, t \\in (0,T),$ \nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$ denotes Gaussian noise, $T$ is the final time step, $t$ is the current time step, and $\\alpha_t$ is the noisy schedule parameter.\nThe Cog Video model [26, 80] is an open-source Text-to-Video (T2V) diffusion model that employs a Transformer architecture [52], referred to as $\\mu_{\\theta}$, to model the reverse diffusion process. The transformer model processes the current noisy sample $x_t$, the associated time step $t$, and optional conditioning inputs $c$ to estimate the noise $\\epsilon$. The loss function for training the denoising model is:\n$L_o = \\mathbb{E}_{x_0,c,t} ||x_0 - \\mu_{\\theta}(x_t, c, t)||^2 .$ \n3D Gaussian Splatting 3D Gaussian Splatting [33] represents 3D data using a collection of 3D Gaussians. Each Gaussian is defined by a center $x \\in \\mathbb{R}^3$, a scaling factor $s \\in \\mathbb{R}^3$, and a rotation quaternion $q \\in \\mathbb{R}^4$. Additionally, it includes an opacity value $\\alpha \\in \\mathbb{R}$ and a color feature $c \\in \\mathbb{R}^C$ for rendering purposes, with spherical harmonics capturing view-dependent effects. Rendering these Gaussians involves projecting them onto the image plane as 2D Gaussians and applying alpha compositing for each pixel in a front-to-back order. Recent methods [47, 73, 79] extend 3D Gaussian Splatting to capture dynamic 4D scenes by incorporating a temporal embedding."}, {"title": "4. Method", "content": "4.1. Overview\nGiven an input human image $I \\in \\mathbb{R}^{H\\times W\\times 3}$, our goal is to create an animatable 3D avatar represented by a 3DGS model. This avatar can be animated by applying new human pose conditions during inference. To facilitate animation, it is crucial to reconstruct the 3D avatar in a canonical pose, simplifying the rigging process. The 3D avatar in this canonical pose can be rigged using the aligned SMPL-X model or other off-the-shelf rigging methods [1]. As shown in Fig. 2, our framework consists of two main stages.\nIn the first stage, we employ a reference image-guided video generation model to produce high-quality multi-view canonical human images and their corresponding normals from the input image. In the second stage, we reconstruct the 3D model using these generated images. However, despite the high quality of the generated images, multi-view inconsistencies still arise due to the nature of the"}, {"title": "4.2. Multi-view Canonical Image Generation", "content": "Given a reference human image in an arbitrary pose, our goal in the first stage is to generate multi-view RGB images of the same subject in a canonical pose. Motivated by recent successes in controllable image generation through video models [27, 80], we adapt a diffusion transformer-based video generation model to reposition the human subject to a canonical pose and generate multi-view images.\nSpecifically, the video generation model takes as input the reference image and SMPL-X pose conditions to produce multi-view images. Here, the rotation of the camera in relation to the subject is treated as equivalent to subject rotation.\nReference-guided Canonical Video Generation We extend the Cog Video model [26] to achieve reference image-guided and SMPL-X pose-guided video generation. This model includes a transformer-based denoiser and a Variational Autoencoder (VAE) that maps input videos or images into a high-dimensional latent space.\nThe reference image $I$ is first encoded into VAE features $F_{I} \\in \\mathbb{R}^{B\\times 1\\times C\\times \\frac{H}{W}}$, while the latent representations for video noise are denoted by $F_{\\epsilon} \\in \\mathbb{R}^{B\\times f\\times C\\times \\frac{H}{W}}$, where $B$ represents the batch size, $f$ the number of output frames, $C$ the number of latent feature channels, and $\\frac{H}{W}$ the total number of input feature tokens.\nTo ensure that generated multi-view images retain the identity of the reference image, we fuse the reference image features and latent features during the denoising process. We achieve this by concatenating the reference image features and latent features along the frame channel, yielding $F_{c} \\in \\mathbb{R}^{B\\times (f+1)\\times C\\times \\frac{H}{W}}$ at each DiT block, which allows for feature interaction through self-attention.\nTo guide the video generation with input human poses, we integrate a lightweight pose guidance network inspired by CHAMP [85]. This network extracts guidance features from the canonical SMPL-X normal, $N_{smplx}$, which are added to the corresponding noisy latents to direct the denoising process.\nJoint Multi-view RGB and Normal Generation To enhance multi-view reconstruction with normal supervision [14, 45], we further extend the video generation framework to simultaneously produce multi-view RGB images and normal maps, conditioned on a reference image and its corresponding normal map, as predicted by an existing method [14].\nWe employ the CogVideo-2B architecture [80] as our base model, which comprises 30 DiT blocks. We modify the first three DiT blocks into two branches, one for RGB and the other for normal inputs. Similarly, we modify the last three DiT blocks into two branches that simultaneously output multi-view RGB images and normal maps.\nTo effectively integrate image and normal features, we first share the weights of the middle 24 DiT blocks for both RGB and normal feature processing. Secondly, we insert a multi-modal attention module between every three shared DiT blocks and the head of the middle DiT blocks. For this multi-modal attention block, we concatenate the RGB and normal features at the token level, resulting in $F_{m} \\in \\mathbb{R}^{B\\times f\\times C\\times 2\\frac{H}{W}}$"}, {"title": "4.3. 3D Reconstruction from Inconsistent Images", "content": "Once multi-view images are generated by the diffusion model, we can reconstruct a 3D Gaussian human model in canonical space. However, due to subtle appearance variations across views in the generated images, directly applying 3D Gaussian Splatting (3DGS) optimization degrades the quality of the reconstructed avatar (see Fig. 3).\nProblem Formulation To address the challenges of 3D reconstruction from inconsistent views, it is necessary to handle the shape and appearance variations in each view. Viewing these inconsistencies as analogous to dynamic variations within a temporal sequence, we can reformulate the problem of 3D reconstruction from inconsistent images as a 4D reconstruction task. Inspired by the recent success of 4D Gaussian Splatting in dynamic scene modeling [47, 73, 79], we adopt a 4DGS approach to achieve efficient optimization and rendering.\nThe 4DGS framework is composed of a canonical space and a per-frame deformation module. The canonical space represents a static 3D model (e.g., defined as the first frame), while the per-frame deformation module estimates shape and color variations of each 3D Gaussian, conditioned on the frame index, to fit the video sequence.\nOur goal is to optimize both the canonical space and the deformation module based on the generated inconsistent images. Once optimized, this process yields a multi-view consistent Gaussian avatar model representing the shape in canonical space.\n4D Gaussian Splatting Model Following existing dynamic Gaussian splatting methods [73], the deformation of 3D Gaussians is modeled by a deformation field network. We employ an efficient spatial-temporal encoder architecture consisting of a multi-resolution HexPlane and a compact MLP [9, 18, 65]. This structure encodes both the temporal and spatial features information of 3D Gaussians across six 2D voxel planes, incorporating temporal effects."}, {"title": "5. Experiments", "content": "In this section, we thoroughly evaluate the effectiveness of our proposed methods by conducting a comprehensive comparison with state-of-the-art approaches.\nTraining Datasets For training the multi-view generation model, we first conduct a pretraining phase using a large dataset of dynamic human videos. Specifically, we collect approximately 200,000 dynamic human videos from various online sources. From this, we manually select single-person videos to create our in-the-wild training dataset, which consists of around 100,000 video samples. During the fine-tuning phase, we leverage a combination of public synthetic 3D datasets to render multi-view images. These datasets include 2K2K [19], Thuman2.0, Thuman2.1 [81], and CustomHumans [23], along with commercial datasets such as Thwindom and RenderPeople. Note that no rigged human models are used for training. In total, we utilize 6,124 synthetic human scans.\nInference Our model can generate an animatable 3D avatar in a canonical pose within a few minutes using a single RTX-3090 GPU. Specifically, it takes approximately 5 minutes to generate 30 frames of multi-view RGB and normal images. The 4DGS optimization process takes around 5 minutes. Once the multi-view reconstruction is complete, we set the time parameter t = 0 to obtain the final Gaussian point clouds. After optimization, the avatar can be animated and rendered in real time."}, {"title": "5.1. Comparison with Existing Methods", "content": "Evaluation Dataset We choose 50 rigged human avatars from Human4DiT [66] to evaluate our performance on multi-view canonical-pose image generation and 3D recon-"}, {"title": "5.2. Ablation Study", "content": "Shape Regularization We conduct an ablation study to evaluate the design of the shape regularization. Specifically, Fig. 6 (a) demonstrates that normal regularization effectively reduces random noise and enhances surface details, while Fig. 6 (b) shows that anisotropic regularization helps eliminate spikes in novel pose animations.\nInitialization of 4DGS Figure 7 compares the results of 4DGS optimization using random points, SMPL mesh, and"}, {"title": "6. Conclusion", "content": "In this work, we present a robust approach to generate animatable human avatars from a single image. We introduce"}, {"title": "A. Demo Video", "content": "Please kindly check the Demo Video for animation results of the reconstructed 3D avatar."}, {"title": "B. More Details for the Method", "content": "B.1. Implementation Details\nMulti-view Generation\nFor training the multi-view canonical image generation model, we first pre-train our RGB-Normal DiT model on in-the-wild video clips. To supervise the normal map output, we utilize Sapiens [34], an off-the-shelf normal estimation prior, to generate pseudo ground-truth normals from in-the-wild data. The model is trained using the Adam optimizer [35] with a learning rate of $2 \\times 10^{-4}$ and a batch size of 1. We employ 16 Nvidia A100 80G GPUs for training, with the pre-training process comprising 100,000 optimization iterations. Subsequently, the model is fine-tuned on a synthetic dataset using the same hyperparameters, performing an additional 50,000 iterations of optimization. To preserve the model's generalizability, we adopt a data-mixing strategy during fine-tuning, assigning a 10% probability to sampling in-the-wild data and a 90% probability to synthetic data.\n3D Reconstruction from Inconsistent Images In the multi-view reconstruction phase, after obtaining the deformed coarse mesh from the original SMPL-X as the initialization for 4DGS, we first performed 3,000 iterations of optimization the 3DGS parameters. Sequentially, we continue to conduct 4,000 iterations of optimization in the temporal dimension to address multi-view inconsistency. In the multi-view reconstruction phase, we initialize with a deformed coarse mesh derived from the original SMPL-X model for the 4DGS process. The first step is optimizing the 3DGS parameters over 3,000 iterations. Subsequently, we perform 4,000 iterations of optimization considering the temporal dimension to address multi-view inconsistency."}, {"title": "B.2. RGB-Normal Diffusion Transformer", "content": "Figure 8 illustrates the architecture of our multi-view diffusion transformer model for canonical image and normal map generation. For simplicity, we omit SPML-X conditioning in the figure. Both \u2018I-DiT-E' and \u2018N-DiT-E' denote two independent DiT encoder blocks conditioned on image and normal input, respectively, while 'I-DiT-D' and 'N-DiT-D' refer to two independent decoders responsible for generating multi-view canonical images and normal maps. Additionally, 'I-N' within the intermediate DiT blocks represents a multi-modal attention module that effectively encodes joint image and normal features."}, {"title": "B.3. Coarse Shape Initialization", "content": "We optimize the following objective function to obtain the initial coarse mesh M' for 3DGS initialization:\n$L_{init} = \\lambda_{mask}\\cdot L_{mask} + \\lambda_{n} L_{normal} + \\lambda_{lap} \\cdot L_{lap}(M') + \\lambda_{edge} \\cdot L_{edge}(M').$\nwhere $\\lambda_{mask} = 1.0$, $\\lambda_{n} = 0.5$, $\\lambda_{lap} = 0.1$, and $\\lambda_{edge} = 0.05$.\nFigure 9 demonstrates the coarse mesh results reconstructed from the generated images. As illustrated in the figure, the coarse mesh provides only a rough geometric surface, with several noticeable artifacts remaining on its surface."}, {"title": "B.4. Skinning-based Animation", "content": "We model large body motions using linear blend skinning (LBS) transformations based on the SMPL-X [51] model. Specifically, given an SMPL body with shape parameter B and pose parameter $\\theta$; in the i-th frame, a point p on the body surface in canonical space with skinning weights w(p) can be warped to camera view space via the skinning transformation W.\nNotably, the skinning weights w(p) are only defined for points on the SMPL-X surface. To handle shapes with large deformations (e.g., skirts) and to better facilitate the warping of arbitrary points in canonical space to the camera view, we employ the diffused skinning strategy [42] to propagate the skinning weights of the SMPL-X body vertices to the entire canonical space. These weights are stored in a voxel grid of size 256 \u00d7 256 \u00d7 256. Skinning weights for arbitrary points are then obtained through trilinear interpolation."}, {"title": "B.5. More Details for the Synthetic Dataset", "content": "We leverage a combination of public synthetic 3D datasets to render multi-view images for fine-tuning the multi-view canonical image and normal generation model. These datasets include 2K2K [19], Thuman2.0, Thuman2.1 [81], and CustomHumans [23], along with commercial datasets such as Thwindom and RenderPeople. In total, we utilize 6,124 synthetic human scans.\nFor the synthetic data, we render each object from 30 different viewpoints by rotating the object. To improve the quality of multi-view reconstruction, images are rendered at varying elevations, which helps to regularize the optimization of the 3D Gaussian Splatting (3DGS) method. Specifically, the elevation range oscillates between -20\u00b0 and 20\u00b0, following a sine function over a cycle of 30 views."}, {"title": "C.1. Pre-training on In-the-wild Data", "content": "Figure 10 underscores the critical role of pre-training on in-the-wild data. Models pre-trained on diverse and real-world datasets demonstrate substantially enhanced general-"}]}