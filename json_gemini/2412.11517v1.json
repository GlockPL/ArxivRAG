{"title": "DARTO: An AIGT Detector using AMR of Rephrased Text", "authors": ["Hyeonchu Park", "Byungjun Kim", "Bugeun Kim"], "abstract": "As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. So, researchers have developed methods for detecting AIGT. However, two challenges remain. First, the performance on detecting black-box LLMs is low, because existing models have focused on probabilistic features. Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and which may deviate from the real-world scenario. \u03a4o resolve these challenges, we propose DART\u00ae, which consists of four steps: rephrasing, semantic parsing, scoring, and multiclass classification. We conducted several experiments to test the performance of DART by following previous work. The experimental result shows that DART can discriminate multiple black-box LLMs without probabilistic features and the origin of AIGT.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) continue to advance, it becomes increasingly difficult for humans to discern AI-generated text (AIGT). This poses issues in society and research, such as spreading fake news and tainting AI training data. Researchers have developed AIGT detectors to address these issues. Despite their success, two challenges related to real-world applicability persist.\nOne challenge with applying AIGT detectors is low performance in detecting recent black-box LLMs. Traditionally, AIGT detectors rely on probabilistic features such as logits. However, in commercial black-box models, including GPT (OpenAI, 2024a,b) or Gemini (Team et al., 2024), we cannot access their computation procedure which provides logits. That is, traditional approaches cannot detect such black-box models. So, researchers have also designed detectors using syntactic features that do not require accessing computational procedures. Yet, these detectors struggle to detect black-box models because their syntactic naturalness is comparable to that of humans.\nThe other challenge is the vagueness of the origin of AIGTs. In the inference time of a detector, it receives a text without any information about its origin. So, similar to the inference scenario, we should verify whether a detector can successfully discriminate AIGT regardless of source models. However, existing studies mainly tested their detectors under the assumption that a candidate LLM is known in advance; they tested whether a binary detector can distinguish a 'human-written text' from an 'AIGT by the predefined candidate.' So, whether existing detectors can detect the origin without the assumption is questionable.\nTo address these challenges, we propose a Detector using AMR of Rephrased Text (DART). DART utilizes the semantic gap between given input and rephrased text, using Abstract Meaning Representation (AMR). We assess DART on a single-candidate and a multi-candidate setting to verify the real-world detection performance. As a result, DART can successfully discriminate humans from four cutting-edge LLMs, including GPT-3.5-Turbo, GPT-4o, Llama 3 (Dubey et al., 2024), and Gemini-1.5-Flash.\nThus, this paper has the following contributions:\n\u2022 We present a semantics-based detection framework for AIGT, leveraging semantic gaps between given text and rephrased texts.\n\u2022 DART can discriminate different LLMs and outperform other detectors; e.g., DART achieved a 96.5% F1 score on average."}, {"title": "Background", "content": "In this section, we categorize existing studies regarding numbers (single or multi) and transparency (white box or black box) of candidate LLMs.\nSingle white-box candidates AIGT detectors first attempted to extract candidate-specific features. As the candidate is a known white-box model, some researchers designed algorithms adopting probabilistic features from the model (Gehrmann et al., 2019; Mitchell et al., 2023). For example, Detect-GPT (Mitchell et al., 2023) used log probabilities of tokens as features. Other researchers used neural models that can learn features from the given texts (Solaiman et al., 2019; Hu et al., 2023). However, as many black-box LLMs recently emerged, the performance of existing detectors should be revalidated on those LLMs.\nSingle black-box candidates Some AIGT detectors then attempted to extract features regardless of the candidate (Bao et al., 2024; Yu et al., 2024; Yang et al., 2023; Kim et al., 2024), as black-box candidates may not provide probabilistic features. Fast-DetectGPT (Bao et al., 2024) extended DetectGPT by extracting probabilistic features from a proxy white-box model (e.g., GPT-J). Since such a proxy can provide less accurate results, other studies used syntactic or surface-level features without using a proxy (Yang et al., 2023; Kim et al., 2024). For example, DNA-GPT (Yang et al., 2023) used N-grams from multiple paraphrased texts generated by the candidate. However, such syntactic features are insufficient to detect recent LLMs because recent models generate text with human-level syntax.\nMultiple candidates As a single-candidate performance is far from real-world scenarios, recent AIGT detectors were designed to detect multiple candidates (Li et al., 2023; Abburi et al., 2023; Wang et al., 2023; Shi et al., 2024; Antoun et al., 2024). For example, POGER (Shi et al., 2024) extends resampling methods to estimate probability using about 100 paraphrases. Because of such an excessive regeneration, POGER incurs high computational costs. Besides, SeqXGPT (Wang et al., 2023) used a Transformer-based detector with a proxy model estimating probabilistic features. However, these studies mainly focused on (1) surface-level features and (2) the limited range of LLMs (e.g., GPT family), raising questions about detecting other cutting-edge LLMs."}, {"title": "The DART Framework", "content": "As shown in Figure 1, DART utilizes semantic gaps between a given text and rephrased texts. To train a detector capturing such gaps, DART uses a four-step procedure: Rephrasing, Semantic parsing, Semantic gap scoring, and Classification.\nStep 1, Rephrasing: DART uses a rephraser module to generate semantically similar text with an LLM. We assume rephrasing can hint at different origins because LLMs and humans may have different writing styles. Mathematically, let us denote the given text as $T_0$. Then, we can obtain $T_1$ by applying the rephraser on $T_0$. We want to utilize a semantic gap between $T_0$ and $T_1$ to distinguish the origin of AIGT. We additionally used a larger gap between $T_0$ and $T_2$, a rephrase of $T_1$. For the rephraser, we adopted GPT-4o-20240513, as the model showed the highest performance related to semantics-related tasks (OpenAI, 2024a). For the prompts used in the rephraser, see Appendix A.1.\nStep 2, Semantic parsing: DART adopts a semantic parser to transform texts into semantic representations. We especially adopted AMR as a semantic representation because AMR has widely been adopted to abstract the given text into semantics (Banarescu et al., 2013). For the parser, we adopted Naseem et al. (2022). As a result, the parser constructs an AMR graph $A_i$ from each $T_i$.\nStep 3, Semantic gap scoring: DART uses metrics for semantic parsers to measure semantic gaps between texts. As we adopted AMR as a semantic representation in the previous step, we utilize a fast and efficient algorithm for scoring AMR similarity called SEMA (Anchi\u00eata et al., 2019; Ki et al., 2024). To obtain semantic gaps between $A_0$ and $A_i$ ($i > 0$), DART computes precision $p_i$ and recall $r_i$ scores generated by SEMA, resulting a feature vector $v = [p_0, p_1, r_0, r_1]$ for the next step.\nStep 4, Classification: DART has a classifier that predicts one possible origin of $T_0$. DART uses interpretable classifiers, including support vector machine (SVM) or the decision tree (DT), though one can use any classifier that maps v to one of the candidate origins."}, {"title": "Experiments", "content": "To evaluate the performance of DART, we conducted two experiments: (1) single-candidate and (2) multi-candidate. First, in the single-candidate experiment, we adopted a binary classifier as DART's classifier, assuming that AIGTs are exclusively produced by a specific LLM. Second, in the multi-candidate experiment, we utilized a multiclass classifier, assuming that we were unaware of the precise origin of the AIGT. We ran each experiment 10 times to ensure their reproducibility. Additionally, we analyzed DART's training efficiency when decreasing the size of training set."}, {"title": "Datasets", "content": "To train DART, we need human-written texts and AIGTs. First, we used four English datasets as human-written text datasets: XSum (Narayan et al., 2018), SQUAD 1.1 (Rajpurkar et al., 2018), Reddit (Fan et al., 2018), and PubMedQA (Jin et al., 2019). Following the practice of previous research (Mitchell et al., 2023; Wang et al., 2023), we randomly sampled texts from these datasets. We split training and validation sets with an 8:2 ratio.\nSecond, we generated AIGT datasets based on the human dataset. Following Mitchell et al. (2023), we collected English AIGT from each human-written text. Four cutting-edge LLMs are used to generate AIGTs: GPT-4o, GPT-3.5-turbo, Llama 3-70B, and Gemini-1.5 Flash. We obtained AIGTS by providing the first 30 tokens of each human-written text to an LLM. Because PubMedQA contains many texts shorter than 30 tokens, we provided corresponding questions instead of the first 30 tokens. Appendix A.2 illustrates the detailed prompts used for generating AIGTs. As a result, we obtained about 3,989 human-written texts and 15,956 AIGTs (= 3,989 texts \u00d7 4 LLMs). See Appendix B.2 for the statistics of the collected dataset."}, {"title": "Baselines", "content": "As baselines, we used five open-source state-of-the-art detectors: DetectGPT (Mitchell et al., 2023), Fast-DetectGPT (Bao et al., 2024), DNA-GPT (Yang et al., 2023), Roberta-base (Solaiman et al., 2019), and SeqXGPT (Wang et al., 2023). Among these models, DetectGPT, Fast-DetectGPT, and SeqXGPT used probabilistic features generated by third-party LLMs in order to detect cutting-edge LLMs. Meanwhile, DNA-GPT and Roberta-base used shallow semantic features, such as N-grams or contextual embeddings. Note that DART stands out from these models because it uses AMR-based semantic features rather than probabilistic features. We used a different set of models for the two experiments, considering experiments reported with five models. For the single-candidate experiment, we compared DART with all five models. For the multi-candidate experiment, we compared DART only with SeqXGPT, as it is the only model tested on the multi-candidate setting. To ensure a fair comparison, all models used in the experiment are trained on our dataset from scratch\u00b9. To measure the performance, we used the F1 score."}, {"title": "Result and Discussion", "content": "Single-candidate experiment DART outperformed existing models. As shown in Table 1, DARTDT and DARTSvm achieved 96.5% and 82.8% F1 scores on average, which are 19.3%p and 5.6%p higher than the Roberta-base model (77.2%). Also, DARTDT can detect all four cutting-edge models with over 85% of F1 score. Meanwhile, other existing models showed F1 scores lower than 70%, on average. Their F1 scores are sometimes lower than the random binary baseline (50%): DNA-GPT on detecting Gemini and SeqXGPT on detecting LLaMA and Gemini.\nWe suspect that DARTDT can achieve such outstanding performance because DART's semantic scoring step can successfully form several clusters according to their origins. To support this argument, we further analyzed the feature vectors of DART using principal component analysis. We found that several clusters are formed for each origin, as described in Appendix C.1.\nMulti-candidate experiment DART also showed much higher performance than SeqXGPT. As shown in Table 2, DARTDT and DARTSVM achieved 81.2% and 65.0% macro F1 scores, which are 22.0%p and 5.8%p higher than SeqXGPT (59.2%). Interestingly, SeqXGPT achieved the lowest F1 score on Llama 3 (44.8%), but DARTDT achieved the lowest score on GPT-4o (76.6%).\nWe suspect how the detectors extract features using an LLM affects the performance. To support this claim, we present a contingency table of SeqXGPT and DARTDT, as shown in Figure 2. The figure shows that (i) SeqXGPT struggled in distinguishing models other than Llama 3, and (ii) DARTDT struggled in distinguishing the GPT family and humans. Since SeqXGPT in our experiment used GPT-2 as a proxy model DARTDT used GPT-4o as a rephraser module, the characteristics of such LLMs affected the detection performances. For example, as DARTDT utilizes semantic gaps between the original text and rephrased texts, origins should reveal distinguishable gaps to successfully identify them. That is, when the gaps are too similar between origins to discriminate them, DARTDT faces difficulty in the classification step. Since GPT-4o has a similar language understanding ability to humans (OpenAI, 2024a), GPT-4o and humans may be less distinguishable through gaps. Similarly, as GPT-3.5-turbo may share some core knowledge with GPT-4o, GPT-4o can be confused with GPT-3.5T in DARTDT.\nTraining efficiency of DART Figure 3 shows the result of the experiment on training efficiency. The result shows that DARTDT is robust even though we use a small amount of training data. Specifically, DARTDT maintained the F1 score until we used 1/32 of the training set (about 500 examples). Meanwhile, the performance of SeqXGPT and DARTsvm monotonically decreases as we reduce the size of the training set."}, {"title": "Conclusion", "content": "We introduced an AIGT framework, DART to tackle challenges in applying AIGT detectors to real-world scenarios. To address the challenges of black-box models, DART employed rephraser and semantic gap scoring module. And, to evaluate whether DART can address vagueness of origin, we assessed DART in two experimental settings: single and multi-candidate settings. As a result, DART achieved outstanding performance compared to existing models, demonstrating successful capture of differences across origins with semantic gaps."}, {"title": "Limitations", "content": "Despite the outstanding performance of DART, this paper has three limitations. First, we tested DART only with a single rephraser LLM, GPT-4o. Though GPT-4o provided enough semantic information to distinguish AIGTs successfully, it is yet questionable whether DART can be used with other rephraser LLMs, such as Llama 3, Gemini Pro, or others. Since different language models may provide different rephrased texts, we need further study to figure out to what extent rephraser LLM affects the performance.\nSecond, the performance of the adopted AMR parser may affect the detection performance of DART. Though the AMR parser rarely introduces errors in the DART framework, such errors may lead to huge changes in detection performance when they occur. Using a publicly available AMR parser (Naseem et al., 2022), DART showed the lowest bound of its performance. Thus, we need further study to improve the performance using other semantic parsers.\nThird, DART tested on a narrow range of black-box models. While narrow LLMs have become publicly available through paid APIs or pretrained parameters. We tried our best to include recent LLMs, such as Gemini Pro or Claude 3. However, we finally excluded those models because their safeguards prevented to generate AIGTs based on a given human-written text when preparing the AIGT dataset. To generalize our findings to other origins, we need a further study on a wider range of models by designing a new way of generating AIGTs."}, {"title": "A Prompts", "content": "A.1 DART's rephraser\nWe used the following prompt in the rephraser.\nPlease rewrite the following paragraph in {n} words: {paragraph}\nA.2 AIGT datasets\nWe followed the prompts used in SeqXGPT (Wang et al., 2023) when generating the AIGT dataset. As Llama and Gemini inserted some prefix sentences in the result (e.g., \"Here is the generation of ...\"), we used additional prompts to ensure the generated text's consistency and quality.\nFor GPT family We used the following prompts for GPT-3.5-turbo and GPT-4o, for datasets except PubMedQA.\nPlease provide a continuation for\nthe following content to make it\ncoherent: {first 30 tokens}\nFor PubMedQA, we used the following prompts:\nPlease answer the question:\n{question}"}, {"title": "", "content": "For Llama3 and Gemini-1.5 We used the following prompts for Llama and Gemini-1.5, for datasets except PubMedQA.\nPlease provide a continuation for\nthe following content to make it\ncoherent: {first 30 tokens}\nProvide the continuation without\nany prefix.\nanswer:\nFor PubMedQA, we used the following prompts:\nPlease answer the question:\n{question}\nProvide the continuation without\nany prefix.\nanswer:"}, {"title": "B Experimental setting", "content": "B.1 Environment\nHardware configuration The experiments were conducted on a system with an AMD Ryzen Threadripper 3960X 24-Core Processor and four NVIDIA RTX A6000 GPUs. The four NVIDIA RTX A6000 GPUs are used for training existing detectors and executing AMR parsers. The semantic gap scoring module was run on a single core of the CPU.\nLLM APIS We used commercial APIs of LLMs to collect AIGTs and rephrased texts. GPT models are called with OpenAI's official API. Llama 3 is called with a free API provided by Groq.com. Lastly, Gemini is called with OpenRouter's API.\nImplementation We used Python 3.11.7 for implementing DART. Using scikit-learn library, we implemented DARTSVM and DARTDT with SVC and DecisionTreeClassifier. We mostly used the basic settings of those classes without doing a hyperparameter search. The only exception is the depth of the tree in DARTDT, and we set it as 5 based on our heuristic.\nB.2 Dataset statistics\nTable 3 in page 9 shows the statistics of the collected dataset. We used four datasets, which belong to different domains: Xsum(Narayan et al., 2018), SQUAD(Rajpurkar et al., 2018), Reddit(Fan et al., 2018), and PubMedQA(Jin et al., 2019). Xsum is a dataset of news articles and summaries representing the general domain. SQuAD is an academic essay dataset using Wikipedia articles. Reddit is a dataset of human-written stories with writing prompts associated with creative writing. PubMedQA, consisting of question-answer pairs in the medical domain, is used to observe performance in a specific domain.\nThe statistics show that the average lengths of texts in each dataset are different. For example, Gemini-1.5 usually generates the most extended text on the PubMedQA dataset, while the model generates the shortest text on the Xsum and Reddit datasets. On average, it seems that the length of a given text is not a significant factor for discriminating origin."}, {"title": "C Additional analysis", "content": "C.1 PCA result\nFigure 4 and 5 in page 8 display PCA plots of DARTDT in the multi-candidate experiment. The figures show that each class makes several clusters. We interpret DART's experimental results in the multi-candidate experiment by analyzing the PCA results. The distribution of feature vectors may affect the performance of SVM and DT classifiers. As SVM seeks a global decision boundary that maximizes margin, SVM may not find a clear decision boundary between multiple mini clusters. Meanwhile, DT can split such mini clusters based on multiple criteria. So, DT could achieve high performance in discriminating AIGTs from human-written texts. For example, we can easily discriminate humans from others and iteratively build different decision boundaries between smaller clusters. That is why DARTDT showed higher performance than DARTSVM.\nC.2 Detailed result of efficiency\nFigure 6 in page 9 shows the training efficiency on the single-candidate experiment. In general, the performance drops as the size of the dataset decreases. Among those models, DARTDT demonstrates the best performance across all models, even with small datasets. DARTSvm experiences a more rapid decrease in its performance.\nWe suspect that the distribution of the data may affect the classification performance. In other words, SVM or a neural network may not have sufficient data to distinguish small clusters whose features are close to each other when we use a small dataset."}]}