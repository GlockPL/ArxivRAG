{"title": "REVISITING MAE PRE-TRAINING FOR 3D MEDICAL IMAGE SEGMENTATION", "authors": ["Tassilo Wald", "Constantin Ulrich", "Stanislav Lukyanenko", "Andrei Goncharov", "Alberto Paderno", "Leander Maerkisch", "Paul F. Jaeger", "Klaus Maier-Hein"], "abstract": "Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data. While SSL has revolutionized fields like natural language processing and computer vision, their adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. We address these issues by i) leveraging a large-scale dataset of 44k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points. Furthermore, our model demonstrates exceptional stability, achieving the highest average rank of 2 out of 7 methods, compared to the second-best method's mean rank of 3. Our code is made available here.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the concept of Self-Supervised Learning (SSL) has emerged as a driving factor in data-rich domains, enabling large-scale pre-training that facilitates the learning of robust and transferable general-purpose representations (Assran et al., 2023; Oquab et al., 2023; He et al., 2022). This paradigm shift has been instrumental in advancing various fields, particularly in domains with abundant labeled data like NLP or natural vision. In the domain of 3D medical image computing, this trend has not caught on.\nCurrently, the domain is either focused on training from-scratch, mainly using the nnU-Net framework by Isensee et al. (2021), or using supervised pretraining, which is limited by the cost associated with annotated data (Wasserthal et al., 2023; Ulrich et al., 2023; Huang et al., 2023). The usage of supervised pretraining implies the willingness of the domain to adopt pretraining and calls to question"}, {"title": "2 DEVELOPMENT FRAMEWORK", "content": "The goal of this paper is to develop a robust SSL pretraining method. Due to the limited prior work in the 3D medical domain, many design choices need to be made. We address this by sequentially validating each methodological contribution on five downstream development datasets before testing the final configuration on eight untouched test datasets. To reduce the search space and disentangle the effects of SSL pre-training from other basic design choices, we choose to keep some parameters fixed based on best practices in the domain:\n(i) The architecture used is always the same state-of-the-art residual encoder U-Net architecture (ResEnc U-Net) (Isensee et al., 2024). (ii) The input patch size is [160x160x160]. (iii) All images are resampled to the target spacing of [1x1x1] mm\u00b3 (Roy et al., 2023). (iv) All images are z-score normalized to zero mean and unit variance (Isensee et al., 2021). (v) As optimizer, we use SGD with a decreasing poly-learning rate (Chen et al., 2018) following nnU-Net (Isensee et al., 2021). (vi) We always employ random sampling during pre-training, irrespective of the prevalence of the different MR modalities.\nPre-training Dataset To develop our pre-training method, we utilize a proprietary brain MRI dataset sourced from over 44 centers containing over 9k patients comprising a total of approx. 44k 3D MRI scans. Due to the variety of data sources, this dataset contains images from more than 10 different MR scanners, various MR modalities and a diverse patient population. For more details on the distributions we refer to Figure 2.\nSince this data is sourced directly from clinical examinations, it includes empty or broken images, poor-quality images and so-called scout scans used to determine the field of view of the patient in the MR. Since these scans are not used in diagnostics, these images are filtered by discarding (a) images with a field of view of < 50mm in any axis, (b) images with a spacing > 6.5mm in any direction and (c) images of file size < 200kb, which indicate an empty image. Moreover due to the low quantity of MR Angiography, Susceptibility weighted Images (SWI) and Proton Density (PD) weighted images, we restrict our training data to only include T1, T2, T1 FLAIR and T2 FLAIR images, resulting in our final pre-training dataset of 39, 168 MR images.\nDevelopment Datasets After pre-training we fine-tune on five datasets and calculate the average Dice Similarity Coefficient (DSC) per dataset to evaluate the effectiveness of the pre-training. The multiple datasets are essential to ensure that our design choices do not overfit to a specific MRI modality or pathological target. Specifically, we utilize:"}, {"title": "1. MS FLAIR (D1)", "content": "Consensus delineations of multiple sclerosis (MS) lesions on T2-weighted FLAIR images(Muslim et al., 2022)."}, {"title": "2. Brain Mets (D2)", "content": "Brain metastases imaged through T1, contrast enhanced gradient echo T1ce, contrast enhanced spin echo T1 and a T2 FLAIR sequence acquired in the Stanford University Hospital (Gr\u00f8vik et al., 2020)."}, {"title": "3. Hippocampus (D3)", "content": "The Hippocampus dataset, task 4 of the medical segmentation decathlon (MSD) (Antonelli et al., 2022), contains delineations of the anterior and posterior Hippocampus in T1 weighted MRI (Simpson et al., 2019)."}, {"title": "4. Atlas22 (D4)", "content": "Anatomical Tracings of Lesions After Stroke (ATLAS) on T1 weighted images. We use the Atlas R2.0 dataset from Liew et al. (2022)."}, {"title": "5. CrossModa (D5)", "content": "Delineations of intra-meatal and extra-meatal vestibular schwannoma tumors and cochlea delineations in contrast enhanced T1 weighted MRI (Dorent et al., 2023)."}, {"title": "Test Datasets", "content": "Additionally, eight hold-out test sets were used to evaluate the efficacy of our learned representations when fine-tuning them to segment other target structures."}, {"title": "1. Cosmos (D6)", "content": "This dataset features carotid vessel wall segmentation and atherosclerosis diagnosis, of which we use the contours to evaluate segmentation performance (Chen et al., 2022)."}, {"title": "2. HaNSeg (D7)", "content": "This dataset contains segmentations of 30 organs at risk (OAR), with associated T1 MRI and CT, of which we only use the MR images for model development (Podobnik et al., 2023)."}, {"title": "3. Isles22 (D8)", "content": "This dataset contains annotations of ischemic stroke lesions, with associated diffusion-weighted imaging (DWI), apparent diffusion coefficient (ADC) and T2 FLAIR (Hernandez Petzsche et al., 2022)."}, {"title": "4. HNTS-MRG24(D9)", "content": "T2-weighted MR images of pre-treatment oropharyngeal cancer and metastatic lymph nodes with associated annotations (Wahid et al., 2024)."}, {"title": "5. BraTS Africa (D10)", "content": "This dataset contains 1.5 Tesla T1, T1ce, T2, and T2 FLAIR MRIS of glioblastoma and high-grade gliomas imaged in Nigeria."}, {"title": "6. T2 Aneurysms (D11)", "content": "This proprietary dataset contains 240 T2 MRI images of segmented brain aneurysms with the surrounding brain tissue."}, {"title": "7. TOF Angiography Aneurysms (D12)", "content": "This proprietary dataset contains 144 time-of-flight MR-angiography images of segmented brain aneurysms with the surrounding brain tissue."}, {"title": "8. BraTS Mets (D13)", "content": "This dataset holds brain metastases segmentations on T1ce MRIs, similar to D2. However, instead of fine-tuning on it we use it to measure generalization when inferring models trained on D2 on it."}, {"title": "3 REVISITING 3D MAES", "content": "Masked autoencoders (MAEs) are a well-established pre-training paradigm in the natural imaging domain and in the medical image segmentation domain for transformers. In this section, we investigate this paradigm and optimize it for 3D medical image segmentation using a ResEnc U-Net architecture of Isensee et al. (2024)."}, {"title": "Default parameters", "content": "MAEs are trained by masking an input image to a certain degree and training the network to reconstruct the occluded regions, minimizing deviations betweem reconstruction and original image. In our experiments, we train the MAE with a L2-Loss in the z-score normalized voxel space and only calculate the reconstruction loss where regions were masked. Moreover, we do not remove skip-connections, following the general consensus of Woo et al. (2023), Tian et al. (2023), and He et al. (2022). The default hyperparameters (as used by the model denoted in gray in Table 1) are learning rate 1e-2, weight decay 3e-5, batch size 6, SGD optimizer with Nesterov momentum 0.99, masking ratio of 75% trained with a PolyLR schedule for 250k steps (this represent 1000 epochs in the nnU-Net framework) and minor spatial augmentations of affine scaling, rotation and mirroring."}, {"title": "Sparsification", "content": "When masking the input image, CNNs are not able to ignore the masked regions in the same manner as transformers can. To address this, Tian et al. (2023) proposed to adapt the CNN architectures to better fit the sparse inputs: (a) Sparse Convolutions and Normalization: Through the receptive field of convolutions masked-out regions are iteratively eroded from their boundaries. By re-applying the masked regions after every convolution this problem can be resolved. Moreover, the masks can introduce a problematic shift in the normalization layer statistics due to the introduced zero values. To resolve this normalization is constrained to only consider the non-masked values. (b) Mask Token: Instead of feeding the light-weight decoder feature maps with zeroed mask regions, the regions are densified by filling them with a learnable mask token, simplifying the reconstruction task of the decoder. (c) Densification Convolution: After filling the masks with the Mask Token and before passing the feature maps to the decoder, a [3x3x3] convolution is applied to the feature maps at every resolution except the highest resolution to prepare the representations for decoding.\nResults of these changes are visualized in Table 1a. The adaptations are introduced iteratively, meaning the 'MaskToken' ablation is only applied together with the Sparse Convolutions and Normalization. It can be observed that the full set of adaptations improves performance by an average of 0.3 DSC points across our development datasets. Subsequently, all adaptations are kept and the following evaluations are presented with these changes applied."}, {"title": "Masking strategy", "content": "The masked region is determined by sampling randomly in the CNN's bottleneck of shape [5x5x5] and up-sampling these regions to the input resolution, to ensure the masks align in the bottleneck of the CNN architectures. This results in masking regions of [32x32x32] voxels of non-overlapping regions in the input. As a sampling strategy, we follow random masking, as previous work showed no benefit of structured masking for images nor videos (He et al., 2022; Feichtenhofer et al., 2022). In the scope of the development phase we explore 5 static masking ratios between 30 and 90%, and evaluate a dynamic masking ratio randomly masking between 60% and 90%.\nResults are presented in Table Table 1b and highlight that the masking ratios of 60%, 75%, and the dynamic masking ratio of 60% to 90% perform equally well. Due to the highly similar performance, we choose to proceed with a dynamic masking ratio over the static masking ratio, due to expecting this masking to be more difficult to learn and with the upcoming scaling experiment in mind. We refer to this model as Spark3D-Base (S3D-B= going forward."}, {"title": "Scaling", "content": "MAEs are known to benefit from scaling. We evaluate the effect of compound scaling by increasing the batch size by x8 to 48, the learning rate to $3e - 2$, and the iterations by x4 to 1M. We refer to this model as S3D-Large (S3D-L) to denote the higher compute resources, but note that the architecture remains identical, to maintain easy adaptation of the parameters. Results are presented in Table 1c. It can be observed that this x32 increase in compute only resulted in a slight increase in performance of 0.1 DSC points."}, {"title": "Fine-tuning strategy", "content": "Given a pre-trained model, a crucial question arises: Which weights to transfer and how to schedule the fine-tuning? We investigated various different schedules. Regarding weight transfer we investigated transferring (i) both the encoder and decoder \u25b7\u25c1, or (ii) only the encoder with a randomly initialized decoder. Regarding the fine-tuning schedule, we investigate whether to use a learning rate warm-up of 12.5k steps, ramping the learning rate up to the maximum LR. When only transferring the encoder, an additional warm-up of only the decoder is investigated to adapt the randomly initialized decoder to the pre-trained encoder. In some configurations, this results in two learning rate warm-ups of 12.5k steps each. Additionally, we investigate whether to keep the encoder frozen for the entire fine-tuning process, or to fine-tuning encoder weights as well. Lastly, we investigate whether to decrease the peak learning rate to le-3, 1e - 4 or to keep it at the default of le - 2.\nResults are presented in Table 2 and allow three important observations to be made: (i) Warm-up stages are essential: Not applying a warm-up step significantly reduces performance. Including a warm-up for both the encoder and decoder boosts accuracy by 0.6 to 1 DSC points. (ii) Learning rate adjustments matter: Reducing the peak learning rate to le-3 during fine-tuning consistently yields better results than the default 1e-2, with the best performance seen when fine-tuning both the encoder and decoder with lower learning rates. (iii) Freezing encoder weights is detrimental: The encoder should not remain fixed during fine-tuning. Allowing the encoder to be fine-tuned improves the model's performance compared to when only the decoder is fine-tuned."}, {"title": "4 RESULTS AND DISCUSSION", "content": "We compare our final models S3D-B and S3D-L against VoCo (Wu et al., 2024), VolumeFusion (VF) by Wang et al. (2023), as well as MG Models Genesis (Zhou et al., 2021). The baselines are pre-trained using the same framework on the same data with the same backbone and the same hyperparameters - where possible - and are scaled to fully utilize an A100 40GB GPU, optimized for 250k steps. We provide explicit baseline method and configuration details in Appendix A. Moreover, we compare against two from-scratch baselines. The first, 'No (Dyn.)', represents a non-pretrained (i.e., from scratch) default nnU-Net 3D fullres architecture that was planned and preprocessed on each downstream dataset individually, potentially resulting in different architectures, data pre-processing, and spacings. The second from-scratch baseline, 'No (Fixed)', is an nnU-Net training with the same plans and preprocessing as defined by our pre-training. The dataset-wise mean DSC and mean NSD values across our test dataset suite are provided in Table 3. Additionally, ranking stability of the methods is evaluated through bootstrapping and is provided in Fig. 3."}, {"title": "4.1 OBSERVATIONS", "content": "Across all tested datasets, SSL pre-trained methods demonstrate improved downstream segmentation performance. Comparing our S3D-B method to the most similar from-scratch baseline 'No (Fixed)', we observe higher DSC scores in 10 out of 11 test datasets, with an average increase of +2 DSC points and +1.6 NSD points. This improvement is not limited to our method; MG and VF also achieve higher performance than the baseline, indicating the utility of SSL methods when applied to sufficient data and a state-of-the-art architecture.\nThroughout our test dataset pool, SSL schemes using the masked image modeling paradigm (MG, S3D-B, and S3D-L) consistently rank higher than the contrastive VoCo or the pseudo-segmentation-based VolumeFusion pre-training method for CNN pre-training. Given the age of 'Models Genesis' - published in 2019 - it is surprising to see it outperform the more recent VoCo or VF. We attribute this to a combination of two factors: 1. Models Genesis was originally published and trained on an outdated 3D-UNet (\u00c7i\u00e7ek et al., 2016) and outside of the powerful nnU-Net framework (Isensee et al., 2021). This highlights the importance of avoiding Pitfall 2: Training on a state-of-the-art backbone. 2. VoCo and VF were introduced in conjunction with architectures they were optimized for. By transferring them to a CNN setting, hyperparameters chosen to optimize the method for their original architecture-pretraining combination may be suboptimal for the new CNN backbone.\nS3D-B ranks first with respect to DSC and Normalized Surface Distance (NSD), while S3D-L and MG share the second place. Although S3D-L and MG are very close in Average DSC, S3D-L consistently achieves lower ranks across all datasets. Moreover, according to the bootstrapped aggregated rank (Fig. 3), this superior ranking is reliably higher than that of MG. These results indicate the overall efficacy of our pre-training method compared to currently established methods for CNN pre-training.\nComparing the 'No (Dyn.)' and 'No (Fixed)' configurations, both trained from scratch, reveals that selecting the appropriate configuration for each dataset can significantly influence performance. For instance, on datasets D2 and D11, the dynamic configuration outperforms the fixed by +7 and +5 DSC points, respectively, while for D6, the fixed configuration yields results +18 DSC points higher. In the majority of datasets where the fixed configuration underperforms relative to the dynamic nnU-Net, pre-training helps to recover performance. However, in some cases, such as with D5, the dynamic default nnU-Net still proves superior."}, {"title": "4.2 ABLATION EXPERIMENTS", "content": "While previous experiments focused on comparing SSL pre-training against a from-scratch baseline with full-scale datasets available, many applications in the medical domain have access to only a very small amount of labeled images. To measure the benefits of pre-training in such a low-data regime, we artificially reduced the total amount of data available for training to 10, 20, 30, or 40 labeled images.\nResults are presented in Table 4. It is evident that our pre-trained S3D-B model leads to better downstream performance compared to training from scratch in this setting. With just 40 trained images, the fine-tuned model nearly matches the performance of the from-scratch model trained on the full dataset. Future research could explore whether optimizing the training duration or learning rate schedule could prevent the pre-trained network from overfitting during fine-tuning on such a limited number of training images.\nTo assess the generalization capability of the proposed pre-training method, we tested two scenarios. First, we evaluated fine-tuning our method on an unseen modality using the TOF Angiography Aneurysms dataset (D12). As shown in Table 7, without pretraining, the fixed configuration suffers a performance drop of 20 Dice points. We attribute this to the significant difference in median spacing for the downstream task ([0.50, 0.43,0.43] mm), which has a higher resolution than the fixed target spacing of [1, 1, 1] mm used in the pre-training experiments. This lower resolution likely increases the difficulty of segmenting small aneurysms. Despite this decrease, pre-training mitigates some of this degradation and proves highly beneficial compared to training the"}, {"title": "5 CONCLUSION", "content": "This work is the first to demonstrate the potential of properly configured MAEs in 3D medical image segmentation. By overcoming key pitfalls in previous research, such as limited dataset sizes, outdated architectures, and insufficient evaluation, we show a consistent performance improvement over previous SSL methods. Notably, for the first time, we achieve consistent gains over the dynamic, dataset-adaptive nnU-Net baseline, validated across a large and diverse set of development and testing datasets. While our findings are promising, several avenues remain open for future exploration. Notably, increasing training time and batch size did not lead to performance gains, but"}, {"title": "Multi-Channel Input", "content": "In many medical examinations, it is common to perform multiple scans, as clinicians often require images with different characteristics for accurate decision-making. Consequently, some datasets, like D2, D8, and D10, contain multiple input modalities. While pre-training may involve all modalities, we only feed one modality at a time into the network since not all patients have scans in every modality. This raises the question of how to handle datasets with multiple registered images. To address this, we conducted a 5-fold cross-validation on the D2 development dataset. We evaluated the replication of each input modality along with random initialization of the input stem weights. Additionally, we tested freezing the stem weights during the decoder warm-up phase. As shown in Table 6, the most stable and consistently effective approach was replicating the pre-trained stem and keeping it frozen during the decoder's warm-up period."}]}