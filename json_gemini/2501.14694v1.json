{"title": "Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection", "authors": ["Zhong Li", "Yuhang Wang", "Matthijs van Leeuwen"], "abstract": "Self-supervised learning (SSL) is an emerging paradigm that exploits supervisory signals generated from the data itself, and many recent studies have leveraged SSL to conduct graph anomaly detection. However, we empirically found that three important factors can substantially impact detection performance across datasets: 1) the specific SSL strategy employed; 2) the tuning of the strategy's hyperparameters; and 3) the allocation of combination weights when using multiple strategies. Most SSL-based graph anomaly detection methods circumvent these issues by arbitrarily or selectively (i.e., guided by label information) choosing SSL strategies, hyperparameter settings, and combination weights. While an arbitrary choice may lead to subpar performance, using label information in an unsupervised setting is label information leakage and leads to severe overestimation of a method's performance. Leakage has been criticized as \"one of the top ten data mining mistakes\", yet many recent studies on SSL-based graph anomaly detection have been using label information to select hyperparameters. To mitigate this issue, we propose to use an internal evaluation strategy (with theoretical analysis) to select hyperparameters in SSL for unsupervised anomaly detection. We perform extensive experiments using 10 recent SSL-based graph anomaly detection algorithms on various benchmark datasets, demonstrating both the prior issues with hyperparameter selection and the effectiveness of our proposed strategy.", "sections": [{"title": "1 Introduction", "content": "Graph anomaly detection (GAD) refers to the tasks of identifying anomalous graph objects such as nodes, edges or sub-graphs in an individual graph (Akoglu et al, 2015; Ma et al, 2021), or identifying anomalous graphs from a set of graphs (Ma et al, 2022; Li et al, 2024a). GAD has numerous successful applications, e.g., in finance fraud detection (Motie and Raahemi, 2023), fake news detection (Xu et al, 2022a), system fault diagnosis (Li et al, 2024b), and network intrusion detection (Garcia-Teodoro et al, 2009). In this paper, we focus on unsupervised node anomaly detection on static attributed graphs, namely identifying which nodes in a static attributed graph are anomalous. Recently, Graph Neural Networks (GNNs) have become prevalent in detecting node anomalies in graphs and have shown promising performance (Kim et al, 2022). Specifically, GNNs can learn an embedding for each node by considering both the node attributes and the graph topological information, enabling them to capture and exploit complex patterns for anomaly detection.\nLike with other neural networks, the high performance of GNNs is typically achieved at the cost of a substantial volume of labeled data. However, the process of labeling graphs is often a laborious and time-consuming effort, necessitating domain-specific expertise. For these reasons, GAD is preferably tackled in an unsupervised manner, without relying on any ground-truth labels. Self-supervised learning (SSL) has emerged as a promising unsupervised learning technique on graphs (Liu et al, 2022c), and recent studies have shown its usefulness for node anomaly detection (Fan et al, 2020; Zheng et al, 2021; Jin et al, 2021a; Liu et al, 2021; Yuan et al, 2021; Xu et al, 2022b; Liu et al, 2022a; Chen et al, 2022).\nGraph SSL can be roughly divided into generative, contrastive, and predictive methods (Wu et al, 2021). First, generative methods such as DOMINANT (Ding et al, 2019), GUIDE (Yuan et al, 2021), and AnomalyDAE (Fan et al, 2020) aim to detect graph anomalies by reconstructing (\u2018generating') the adjacency matrix and/or the node attribute matrix. Next, contrastive methods such as CoLA (Liu et al, 2021), ANEMONE (Jin et al, 2021a), GRADATE (Duan et al, 2023), and Sub-CR (Zhang et al, 2022) train a graph encoder to pull positive pairs closer while pushing negative pairs away in the embedding space. The nodes with relatively large contrastive loss values are deemed anomalies. Finally, predictive methods such as SL-GAD (Zheng et al, 2021) try to predict node properties using its local context (e.g., a subgraph), and nodes with large prediction errors are considered anomalies.\nContrastive learning is arguably the most successful SSL strategy for graphs (Xie et al, 2022). Most contrastive graph learning methods consist of two main modules: 1) a data augmentation module that generates augmented data by operations such as edge dropping, node attribute masking, node addition, subgraph sampling, and/or graph diffusion. The augmented view of an instance is generally regarded as a positive pair with the original instance; and 2) a contrastive learning module that contrasts positive pairs (and often involves negative pairs) at different levels, such as node-node contrast, node-subgraph contrast, and subgraph-subgraph contrast.\nAlthough SSL-based graph anomaly detection has been successful, using it in practice is often not straightforward. The most important reason for this is that most methods require a large number of choices to be made, leading to three challenges:"}, {"title": "3 Problem Statement", "content": "We utilize lowercase letters, bold lowercase letters, uppercase letters, and calligraphic fonts to represent scalars (x), vectors (x), matrices (X), and sets (X), respectively.\nDefinition 1 (Attributed Graph). We denote an attributed graph as G = {V,E,X}, where V = {v_1, ..., v_n} is the set of nodes. Besides, E = {e_{ij}}_{i,j\u2208{1,...,n}} is the set of edges, where e_{ij} = 1 if there exists an edge between v_i and v_j and e_{ij} = 0 otherwise. Moreover, X \u2208 R^{n\u00d7d} represents the node attribute matrix, where the i-th row vector x_i means the node attribute of v_i.\nFormally, we consider unsupervised node anomaly detection on attributed graphs (dubbed GAD hereafter), which is defined as follows:\nProblem 1 (Node Anomaly Detection on Attributed Graph). Given an attributed graph as G = {V,E,X}, we aim to learn an anomaly scoring function f(\u00b7) that assigns an anomaly score s = f(v_i) to each node v_i, with a higher score representing a higher degree of being anomalous. Next, the anomaly scores are used to rank the nodes such that the top-k nodes can be considered as anomalies.\nIn this paper, we consider the transductive unsupervised anomaly detection setting: the graph containing both normal and abnormal nodes are given at the training stage. Node labels are not accessible during the training stage and they are only used for performance evaluation. Importantly, the labels of nodes are not (and should not be) used for HP tuning under this unsupervised setting.\nFormally, we consider the hyperparameter optimization problem for unsupervised graph anomaly detection (dubbed HPO for GAD):\nProblem 2 (HPO for GAD). Given a graph G without labels and a graph anomaly detection algorithm f(\u00b7) with hyperparameter space \u039b, we aim to identify a hyper-parameter configuration \u03bb \u2208 \u039b such that the resulting model f(\u03bb) can achieve the best performance on G. I.e., suppose \u039b consists of K different hyperparameters {\u03bb_1, ..., \u03bb_k, ..., \u03bb_K}, where \u03bb_k \u2208 \u039b_k can be discrete or continuous, we then aim to find\narg max_{\u03bb_1 \u2208 \u039b_1,..., \u03bb_k \u2208 \u039b_k,..., \u03bb_K \u2208 \u039b_K} Metric [f(\u03bb_1, ..., \u03bb_k, ..., \u03bb_K; G)],\t\t(1)\nwhere Metric[\u00b7] is a given performance metric."}, {"title": "4 SSL for Unsupervised GAD", "content": "In this section, we first revisit existing self-supervised learning methods for \"unsupervised\" graph anomaly detection, followed by an analysis and experiments to showcase pitfalls in existing studies."}, {"title": "4.1 Existing SSL for \u201cUnsupervised\" GAD", "content": "Figure 2 shows how existing SSL based GAD methods can be divided into generative methods and contrastive methods.\nThat is, a generative method usually consists of two individual SSL tasks, namely 1.1) structure reconstruction that aims to reconstruct the adjacency matrix, and 1.2) attribute reconstruction that aims to reconstruct the node attribute matrix. On this basis, the attribute reconstruction error and the structure reconstruction error are combined to obtain an anomaly score, where higher reconstruction error indicates a higher degree of anomalousness.\nMeanwhile, a contrastive method often consists of two modules: 2.1) data augmentation module, and 2.2) contrastive learning module. First, for each target node, the data augmentation module utilizes one augmentation function f(\u03b4) to produce augmented samples, which usually include positive samples and negative samples. The scenario of using multiple augmentation functions can be obtained in a similar way. Second, three contrastive perspectives can be applied to contrast positive pairs and negative pairs: 2.2.1) node-node contrast that contrasts node embedding with node embedding, and 2.2.2) node-subgraph contrast that contrasts node embedding with subgraph embedding, and 2.2.3) subgraph-subgraph contrast that contrasts subgraph embedding with subgraph embedding."}, {"title": "4.2 Pitfalls in Existing Methods", "content": "In this subsection, we revisit existing SSL-based unsupervised GAD methods by checking the following three aspects for each method:\n\u2022 Which SSL framework does the method employ: generative, contrastive, or both?\n\u2022 How many SSL-specific hyperparameters are involved? (E.g., combination weights and others.)"}, {"title": "4.2.1 Revisiting ANEMONE", "content": "ANEMONE (Jin et al, 2021a) is a contrastive method for unsupervised GAD.\nGraph Augmentation Module. A single graph augmentation operation is used, namely Random Ego-Nets generation with a fixed size K. Specifically, taking the target node as the center, they employ RWR (Tong et al, 2006) to generate two different subgraphs as ego-nets with a fixed size K. This results in one critical HP, namely K.\nContrast Learning Module. Two contrast perspectives are considered: 1) node-node contrast between the embedding of a masked target node within the ego-net and the embedding of the original node, leading to loss term L_{NN}, and 2) node-subgraph contrast within each view, leading to loss term L_{NS}. These loss terms are combined as L = (1 - \u03b1)L_{NN} + \u03b1L_{NS}, where \u03b1 \u2208 [0,1] is the trade-off HP, giving one more critical HP, namely \u03b1.\nHPs Sensitivity & Tuning. By using ground-truth label information, they heuristically set \u03b1 to 0.8,0.6,0.8 on Cora, CiterSeer, and PubMed respectively, and report the corresponding results. The setting of K is not studied, and is set to 4 for all datasets."}, {"title": "4.2.2 Revisiting AnomalyDAE", "content": "AnomalyDAE (Fan et al, 2020) is a generative method using autoencoders (based on GNNs) for unsupervised GAD.\nGenerative Framework. AnomalyDAE consists of two components: 1) an attribute autoencoder to reconstruct the node attributes, where the encoder consists of two non-linear feature transform layers and the decoder is simply a dot product operation. This leads to the loss term L_A, and L_A is associated with a penalty HP \u03b7; and 2) a structure autoencoder to reconstruct the structure, where the encoder is based on GAT (Veli\u010dkovi\u0107 et al, 2017) and the decoder is a dot product operation followed by a sigmoid function. This leads to the loss term L_S, and L_S is associated with a penalty HP \u03b8.\nTheir overall optimization objective is then defined as L = \u03b1L_S + (1-\u03b1)L_A, where \u03b1\u2208 (0,1) balances the two objectives.\nHPs Sensitivity & Tuning. The paper finds that the AUC usually increases first and then decreases with the increase of \u03b1. However, the specific value of \u03b1 on"}, {"title": "4.2.3 Revisiting SL-GAD", "content": "SL-GAD (Zheng et al, 2021) is an unsupervised GAD method that combines both contrastive and generative objectives.\nContrastive Framework-Data Augmentation Module. The method uses a single graph augmentation operation, namely Random Ego-Nets generation with a fixed size K. Specifically, taking the target node as the center, RWR (Tong et al, 2006) is used to generate two different subgraphs as ego-nets with a fixed size K, where K controls the radius of the surrounding contexts. This gives one critical HP for graph augmentation, namely K.\nContrastive Framework-Contrast Learning Module. The Multi-View Contrastive Learning module compares the similarity between a node embedding and the embedding of sampled sub-graphs in augmented views (namely node-subgraph contrast), leading to loss terms L_{con, 1} and L_{con, 2}. Combining those leads to contrastive objective L_{con} = (L_{con,1} + L_{con,2}).\nGenerative Framework. The Generative Attribute Regression module reconstructs node attributes, with the aim to achieve node-level discrimination. Specifically, they minimize the Mean Square Error between the target node's original and reconstructed attributes in augmented views, leading to loss terms L_{gen,1} and L_{gen, 2}. Combining those with equal weights leads to generative objective L_{gen} = (L_{gen,1} + L_{gen,2}).\nThe overall optimization objective is then defined as L = \u03b1L_{con} + \u03b2L_{gen}, where \u03b1, \u03b2\u2208 (0,1] are trade-off HPs to balance the importance of the two SSL objectives.\nHPs Sensitivity & Tuning. The authors conducted a sensitive analysis and found that: 1) the performance first increases and then decreases with the increase of K. For efficiency considerations, they heuristically set the sampled subgraph size K = 4 for all datasets; 2) they heuristically fix \u03b1 = 1 for all datasets as they found that this achieves good performance on most datasets (with the help of label information); and 3) the selection of \u03b2 is highly dependent on the specific dataset. Hence, they \"fine-tune\" the value of \u03b2 for each dataset via selecting \u03b2 from {0.2, 0.4, 0.6, 0.8, 1.0} using labels."}, {"title": "4.2.4 Other SSL-based GAD methods", "content": "Due to space constraints, the analyses of other SSL-based GAD methods, including COLA (Liu et al, 2021), GRADATE (Duan et al, 2023), Sub-CR (Zhang et al, 2022), CONAD (Xu et al, 2022b), DOMINANT (Ding et al, 2019), GUIDE (Yuan et al, 2021), and GAAN (Chen et al, 2020b), are given in Appendix A. These methods are all representatives of recent advancements in using SSL to conduct unsupervised graph anomaly detection, and have yielded outstanding detection performance. Likewise, however, these methods also exhibit pitfalls with regard to hyperparameter tuning, similar to those of ANEMONE (Jin et al, 2021a), AnomalyDAE (Fan et al, 2020), and SL-GAD (Zheng et al, 2021)."}, {"title": "4.3 Sensitivity Analysis", "content": "After revisiting recent SSL-based unsupervised GAD methods, we now empirically investigate their sensitivity to SSL-related HPs in a systematic way. More concretely, we report their performance variations in terms of RUC-AUC values under different hyperparameter configurations (see Section 6 for experiment settings).\nAs shown in Figure 1, for a typical run with different hyperparameter configurations, the performance of ANEMONE (Jin et al, 2021a) can vary strongly on each of the ten datasets. Other SSL-based GAD algorithms exhibit similar behavior; extensive results and analysis are deferred to Appendix B for space reasons.\nFor an in-depth yet compact analysis, Table 1 presents average results over five independent runs when varying SSL-related hyperparameter values. Specifically, COLA (Liu et al, 2021), GUIDE (Yuan et al, 2021), DOMINANT (Ding et al, 2019), GRADATE (Duan et al, 2023), and Sub-CR (Zhang et al, 2022) demonstrate moderate performance variations (namely between 7.3% and 14.7% on average). Meanwhile, CONAD (Xu et al, 2022b), ANEMONE (Jin et al, 2021a), SL-GAD (Zheng et al, 2021), GAAN (Chen et al, 2020b), and AnomalyDAE (Fan et al, 2020) suffer from large performance variations (namely ranging from 15.7% to 30.0% on average). From Subsection 4.2 and Appendix A, we see that the results reported in existing papers are often obtained by manually tuned HPs (in a post-hoc way with label information), thereby leading to strongly overestimated performance for real-world applications where labels are not accessible. To mitigate this severe issue, we propose AutoGAD, a method for automating hyperparameter selection in SSL for GAD and achieving truly unsupervised graph anomaly detection. Importantly, AutoGAD does not need any ground-truth labels."}, {"title": "5 AutoGAD: Using Internal Evaluation to Automate SSL for GAD", "content": "Our proposed approach, called AutoGAD, consists of two parts: 1) an unsupervised performance metric, and 2) an effective search method. Importantly, and as mentioned before, the chosen performance metric\u2014denoted Metric[\u00b7] in Equation 1\u2014should not use any ground-truth label information, simply because this is not available in a truly unsupervised setting. We therefore propose to utilize an internal evaluation strategy, which will be elucidated later. Next, given the impracticality of evaluating an infinite number of configurations for continuous hyperparameter domains, another challenge is the efficient exploration of the search space. Section 5.2 describes a straightforward approach using discretization and grid search that works well in practice, as shown in the next section."}, {"title": "5.1 Internal Evaluation Strategy", "content": "The intuition behind the internal evaluation strategy that we use is to measure the similarity of anomaly scores within the same predicted anomaly class and the dissimilarity between anomaly scores across different predicted classes (i.e., 'anomaly' or 'no anomaly'). As we will prove later, optimizing the resulting measure is equivalent to simultaneously minimizing the false positive rate and the false negative rate. In this way, we aim to evaluate and optimize the performance of the anomaly detector under different SSL configurations without having to rely on any ground-truth labels."}, {"title": "5.1.1 Contrast Score Margin", "content": "The metric that we use is Contrast Score Margin (Xu et al, 2019), which was introduced before but not for graph anomaly detection, and is defined as\nT(f) = \\frac{\u03bc_O \u2013 \u03bc_I}{\\sqrt{\u03c3_O^2 + \u03c3_I^2}},\t\t(2)\nwhere \u03bc_O and \u03c3_O^2 denote the average and variance of the anomaly scores of the k predicted anomalous objects (\u00d4), respectively. Moreover, \u03bc_I and \u03c3_I^2 represent the average and variance of the anomaly scores of the k predicted normal objects (\u00ce) with the highest scores, respectively. Intuitively, the metric focuses on the k predicted normal objects that are most similar to the k predicted anomalous objects, and aims to measure the margin of the anomaly scores between them. It only takes linear time with respect to n to compute."}, {"title": "5.1.2 Analysis", "content": "We now analyze why the internal evaluation metric Contrast Score Margin should work for our purposes.\nTheorem 1 (Minimizing False Positives and Negatives). For an anomaly detector f(\u00b7) on dataset X, assume the anomaly scores of the top k true anomalies (O) have the"}, {"title": "5.1.3 Improvements and Remarks", "content": "In practice we observed that Equation 2 is not always stable. Possible reasons are that 1) the proportion of anomalies is usually very small (namely less than 5% in most datasets); and 2) the exact number of anomalies is generally not known (even for a dataset with injected anomalies, there may exist some natural samples that exhibit similar behaviors as anomalies). Therefore, we propose to modify Equation 2 as follows:\nT(f) = \\frac{\u03bc_O \u2013 \u03bc_I}{\\sqrt{\u03c3_O^2 + \u03c3^2}},\t\t(3)"}, {"title": "5.2 Discretization and Grid Search", "content": "Algorithm 1 Grid Search for Anomaly Detector Hyperparameter Optimization\nInput: Graph anomaly detection algorithm f(\u00b7), graph G, hyperparameter domains \u039b = {\u039b^(1), ..., \u039b^(L)}, internal evaluation function T(\u00b7)\nOutput: Best hyperparameter configuration \u039b_best\n1: Discretize each continuous domain \u039b^(1) into a finite set if necessary\n2: Generate hyperparameter search set \u039b_search = {\u03bb_1,...,\u03bb_M} where M = \u220f_{l=1}^L |\u039b^(l)|\n3: Initialize best score t_best \u2190 -\u221e and best configuration \u039b_best \u2190 \u2205\n4: for each \u03bb_m \u2208 \u039b_search do\n5: Compute anomaly scores s_m(G) = f(\u03bb_m;G)\n6: Compute evaluation score t_m(G) = T(s_m(G))\n7: if t_m(G) > t_best then\n8: Update t_best \u2190 t_m(G)\n9: Update \u039b_best \u2190 \u03bb_m\n10: end if\n11: end for\n12: return \u039b_best\nTo find the optimal hyperparameter configuration, we first perform discretization of the continuous search space and then conduct grid search. The corresponding pseudo-code is provided in Algorithm 1, with a detailed explanation presented below.\nDiscretization of Continuous Search Space (Lines 1-2). To make the overall search process feasible, we discretize the hyperparameter space. Assume we are given a GAD algorithm f(\u00b7) with its set of hyperparameters \u039b \u2208 \u039b. Without loss of generality, we assume there are L different hyperparameters and let \u03bb = {\u03bb^(1), \u03bb^(2), ..., \u03bb^(L)},"}, {"title": "6 Experiments", "content": "We aim to answer the following research questions (RQ):\nRQ1 How sensitive are existing SSL-based GAD methods to the values of their hyperparameters?\nRQ2 How effective is AutoGAD in tuning SSL-related hyperparameter values for these methods?\nWe describe the experiment settings, including the datasets, baselines, evaluation metrics, and software and hardware used, which is followed by the experiment results and their interpretation."}, {"title": "6.1 Datasets", "content": "We use three popular citation networks, namely Cora, Citeseer, and Pubmed (Sen et al, 2008) with injected anomalies, one social network Flickr (Zeng et al, 2019) (less homophily) with injected anomalies, ACM (Tang et al, 2008) as well as BlogCataLog (Zeng et al, 2019) with injected anomalies. Particularly, we follow the methods used by ANEMONE (Jin et al, 2021a) and COLA (Liu et al, 2021) to inject structural and contextual anomalies. Note that Liu et al (2022b) have slightly modified this injection procedure. Following (Qiao and Pang, 2024), we also consider four commonly-used graph datasets with real anomalies: Amazon (S\u00e1nchez et al, 2013), Facebook (Leskovec and Mcauley, 2012), Reddit (Kumar et al, 2019), and YelpChi (Rayana and Akoglu, 2015). The resulting datasets are summarized in Table 2."}, {"title": "6.2 Baselines", "content": "We study the performance of the following SSL-based graph anomaly detection methods:"}, {"title": "6.3 Evaluation Metrics", "content": "To evaluate the effectiveness of various GAD algorithms, we utilize the ROC-AUC metric (Hanley and McNeil, 1982) (AUC for short hereinafter), where a value approaching 1 denotes the best possible performance.\nMoreover, to quantify the performance variation of an individual GAD method under different SSL-related HP configurations, we define the following performance variation metric:\n\\frac{max(AUC) - min(AUC)}{max(AUC)}\t\t(4)\nwhere max(AUC) and min(AUC) represent the maximum and minimum of achieved AUC values for the evaluated GAD algorithm with different configurations, respectively. Hence, the smaller this value is, the less sensitive the algorithm is to SSL-related HPs.\nFurther, we define the performance gain over minimal AUC as\n\\frac{CSM(AUC) - min(AUC)}{min(AUC)}\t\t(5)\nwhere CSM(AUC) indicates the AUC value obtained for the evaluated GAD algorithm when configured with the HPs selected using the Contrast Score Margin. This metric can quantify the effectiveness of our strategy relative to the worst case hyperparameter"}, {"title": "6.5 Results and Analysis", "content": "We answer the research questions as follows:"}, {"title": "6.5.1 RQ1: Sensitivity of SSL-based GAD methods to HPs", "content": "The results are summarized in Table 1 for five independent runs. Typical runs are depicted in Figure 1 and in Figures 5-13 in Appendix B. We briefly analyzed the results in Subsection 4.3; more detailed analyses are given in Appendix B. To recall, five out of ten algorithms show moderate performance variations, while the remaining five algorithms demonstrate large performance variations when the values of SSL-related HPs are varied. In other words, SSL-based GAD methods are (sometimes highly) sensitive to hyperparameter values."}, {"title": "6.5.2 RQ2: Effectiveness of AutoGAD in tuning SSL-related HPs", "content": "The results are summarized in Tables 3, 4 and 5 for five independent runs, while Figure 1 and Figures 5-13 depict typical runs. We have the following main observations:\n1) From Table 3, one can see that AutoGAD can result in moderate performance gain over minimal AUC (namely between 4.1% and 13.1% on average) for COLA, GUIDE, CONAD, DOMINANT, Sub-CR, and GRADATE. Recall that five of these algorithms (including CoLA, GUIDE, DOMINANT, GRADATE, and Sub-CR)"}, {"title": "6.5.3 Sensitivity Analysis", "content": "Sensitivity to k. The selection of the value of k in our experiments acknowledges the varying anomaly ratios across different datasets, implying that k should ideally differ to reflect the unique characteristics of each dataset. We operated under the assumption that the anomaly ratio within a dataset is approximately known, a premise that aligns with real-world anomaly detection tasks where some prior knowledge about the frequency of anomalies is often available.\nAs shown in Figure 3, we conducted a sensitivity analysis on k to assess the stability of AutoGAD against deviations from the true anomaly ratio. The findings from this analysis indicate that the effectiveness of AutoGAD remains stable as long as k is not drastically distant from the actual anomaly ratio, reinforcing the practical applicability of our approach even when exact anomaly proportions are not precisely determined.\nSensitivity to the Granularity of the Search Grid. Acknowledging the sig-nificance of search space granularity in the performance of AutoGAD, we conduct a sensitivity analysis by varying the granularity levels of the search grids in grid search. Figure 4 presents representative results using ANEMONE (Jin et al, 2021a) on the Cora, ACM, and Facebook datasets with four levels of search granularity, as follows:\n\u2022 Granularity Level 1: \u03b1 \u2208 {0, 0.2, 0.4, 0.6, 0.8, 1}, K \u2208 {2, 4};"}, {"title": "7 Alternative Strategies and Discussion", "content": "Internal evaluation strategies aim to assess the quality of a model based solely on internal information, without relying on external information such as ground-truth labels. Internal information can typically be derived from two sources: 1) the input samples, such as feature values of instances in tabular data or node attributes in graph data; or 2) the anomaly scores generated by an anomaly detection model. Beyond the Contrast Score Margin (Xu et al, 2019) discussed in this paper, additional internal evaluation strategies exist for unsupervised model selection in anomaly detection. According to Ma et al (2023), these strategies can be categorized as stand-alone or consensus-based internal evaluation strategies; we will next discuss each category."}, {"title": "7.1 Stand-alone Internal Evaluation Strategies", "content": "Stand-alone strategies rely solely on input samples or individual anomaly detection methods (or models with specific HP configurations in our setting) and their output anomaly scores. Key methods include:\n\u2022 IROES (Marques et al, 2015, 2020) quantifies the separability of each input sample, assuming that a good anomaly detection model assigns high anomaly scores to highly separable samples. However, separability scores are defined only for tabular data, making extension to graph data non-trivial. Additionally, computing separability scores is computationally expensive, posing challenges for large datasets.\n\u2022 Mass-Volume and Excess-Mass (Goix, 2016) use statistical tools to measure the quality of an anomaly scoring function. These methods operate on the raw input samples rather than anomaly scores and assume that anomalies occur in the distribution's tail. However, they are restricted to tabular data and are not applicable to graph data.\n\u2022 Clustering Validation Metrics (Nguyen et al, 2016) assume that an anomaly detector divides input samples into two clusters: abnormal and normal. Clustering validation metrics, such as the Xie-Beni index (Xie and Beni, 1991), are then used to evaluate performance. While clustering coefficients on graphs could be analogous (Li et al, 2017b), these metrics are computationally expensive, particularly for large datasets."}, {"title": "7.2 Consensus-based Internal Evaluation Strategies", "content": "Consensus-based strategies assess the agreement among multiple anomaly detection models (or the same model with varying HP configurations in our setting). Key methods include:\n\u2022 UDR (Duan et al, 2019) assumes that good HP configurations yield consistent results under different random initializations, while poor configurations do not. Ma et al (2023) repurposed UDR to select among heterogeneous anomaly detectors, assuming that good detectors produce consistent results across HP configurations.\n\u2022 Model Centrality (Lin et al, 2020) hypothesizes that good models are close to the optimal model and thus to each other."}, {"title": "7.3 Discussion and Future Work", "content": "Although Ma et al (2023) demonstrated that many internal evaluation strategies perform suboptimally for selecting heterogeneous anomaly detectors, we hypothesize that some can be valuable for hyperparameter tuning within a single anomaly detection model. However, this is beyond the scope of this paper and is left for future work. The primary objectives of this paper are twofold:\n\u2022 We highlight flaws in existing studies on using SSL for unsupervised graph anomaly detection. Specifically, we:\n1. Review these studies, showing that most tune HPs arbitrarily or selectively.\n2. Demonstrate empirically, through extensive experiments, that these methods are highly sensitive to HP settings. Consequently, we argue that these methods may suffer from label information leakage under unsupervised learning settings, leading to overstated performance in practical scenarios where label-based tuning is inaccessible.\n\u2022 We propose an initial solution to these issues by utilizing and improving the Contrast Score Margin. This internal evaluation metric was selected for two reasons:\n1. It operates on anomaly scores rather than on raw data points and avoids pairwise computations, making it computationally efficient and suitable for large datasets.\n2. Theoretical guarantees for its properties are provided by Theorem 1, which may not hold for other internal evaluation strategies.\nThis paper does not aim to provide a perfect solution to the issues mentioned above. Instead, our goal is to spark interest in the research community to address these challenges. Unlike Ma et al (2023), we do not aim to conduct a comprehensive review and evaluation of internal evaluation strategies for SSL-based graph anomaly detection, as this requires significant computational resources and in-depth analysis. Nevertheless, we aim to explore this direction in future work by considering and potentially repurposing the internal evaluation strategies reviewed in Ma et al (2023). We have described a more advanced search strategy than grid search, namely SMBO-based optimization (Jones et al, 1998), in Appendix E, without experimental evaluation."}, {"title": "8 Conclusions", "content": "SSL has received much attention in recent years, and many recent studies have explored SSL to perform unsupervised GAD. However, we found that most existing studies tune hyperparameters arbitrarily or selectively (i.e., guided by labels), and our empirical findings reveal that most methods are highly sensitive to hyperparameter settings. Using label information to tune hyperparameters in an unsupervised setting, however, is label information leakage and leads to severe overestimation of model performance. To mitigate this issue, we introduce AutoGAD, the first automated hyperparameter selection method for SSL-based unsupervised GAD. Extensive experiments demonstrate the effectiveness of our proposed strategy. Overall, we aim to raise awareness to the label information leakage issue in the unsupervised GAD field, and AutoGAD provides a first step towards achieving truly unsupervised SSL-based GAD."}, {"title": "Statement and Declaration", "content": "Ethical approval. This study does not involve human and animal data, and thus the need for approval was waived.\nFunding. This work is supported by Project 4 of the Digital Twin research pro-gramme, a TTW Perspectief programme with project number P18-03 that is primarily financed by the Dutch Research Council (NWO). All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.\nConflict of interest. The author(s) declared no potential conflicts of interest with respect to the research, authorship and/or publication of this article."}, {"title": "A Pitfalls in Existing Methods (Full Analysis)", "content": ""}, {"title": "A.1 COLA", "content": "Particularly, COLA (Liu et al, 2021) is the first contrastive-based framework for unsupervised GAD. The design of its data augmentation module and contrast learning module is as follows.\nData Augmentation Module They consider one type of data augmentation, subgraph sampling, to obtain local augmented view for each node. Particularly, they employ RWR (Tong et al, 2006) to generate a sub-graph with a fixed size K in subgraph sampling, resulting in one critical HP in graph augmentation, namely K.\nContrast Learning Module They consider a single contrast aspect, namely node-subgraph contrast between the embedding of the target node and the aggregated embedding of its local"}]}