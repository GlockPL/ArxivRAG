{"title": "LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection", "authors": ["Yihe Wang", "Nan Huang", "Nadia Mammone", "Marco Cecchi", "Xiang Zhang"], "abstract": "Electroencephalogram (EEG) provides a non-invasive, highly accessible, and cost-effective solution for Alzheimer's Disease (AD) detection. However, existing methods, whether based on manual feature extraction or deep learning, face two major challenges: the lack of large-scale datasets for robust feature learning and evaluation, and poor detection performance due to inter-subject variations. To address these challenges, we curate an EEG-AD corpus containing 813 subjects, which forms the world's largest EEG-AD dataset to the best of our knowledge. Using this unique dataset, we propose LEAD, the first large foundation model for EEG-based AD detection. Our method encompasses an entire pipeline, from data selection and preprocessing to self-supervised contrastive pretraining, fine-tuning, and key setups such as subject-independent evaluation and majority voting for subject-level detection. We pre-train the model on 11 EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised pre-training design includes sample-level and subject-level contrasting to extract useful general EEG features. Fine-tuning is performed on 5 channel-aligned datasets together. The backbone encoder incorporates temporal and channel embeddings to capture features across both temporal and spatial dimensions. Our method demonstrates outstanding AD detection performance, achieving up to a 9.86% increase in F1 score at the sample-level and up to a 9.31% at the subject-level compared to state-of-the-art methods. The results of our model strongly confirm the effectiveness of contrastive pre-training and channel-aligned unified fine-tuning for addressing inter-subject variation. The source code is at https://github.com/DL4mHealth/LEAD.", "sections": [{"title": "1. Introduction", "content": "Alzheimer's disease (AD) is the most common neurodegenerative disorder in the elderly, affecting 10-30% of individuals over the age of 65, with an annual incidence rate of 1-3% (Breijyeh & Karaman, 2020; Masters et al., 2015). AD results from the failure to clear amyloid-\u1e9e peptide from the brain, leading to the progressive decline of cognitive functions. While there is currently no cure for AD, early intervention and treatment can slow the progression of symptoms, thereby improving patients' quality of life(Nelson & Tabet, 2015; Chu, 2012). Existing detection tools such as Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) neuroimaging are costly and require specialized clinical expertise, often resulting in a detection only after significant symptoms have manifested. Recently, there has been growing interest in using non-invasive techniques, such as Electroencephalogram (EEG), to identify biomarkers for AD. EEG offers real-time brain activity data and is more cost-effective than traditional methods, making it a promising tool for early detection and continuous monitoring of disease progression (Ieracitano et al., 2019a).\nCurrently, there are two main research directions for EEG-based Alzheimer's Disease (AD) detection. The first focuses on manually extracting feature biomarkers from the data, such as statistical features (e.g., Mean, Standard Deviation)(Tzimourta et al., 2019b;a), spectral features (e.g., Phase Shift, Phase Coherence)(Wang et al., 2017; Cassani et al., 2014), power features (e.g., Power Spectrum Density, Relative Band Power)(Fahimi et al., 2017; Schmidt et al., 2013), and complexity features (e.g., Shannon entropy, Tsallis Entropy)(Garn et al., 2015; Azami et al., 2019). Among these features, brain slowing in specific frequency bands is most commonly observed in existing research (Ab\u00e1solo et al., 2005; Fahimi et al., 2017). The second direction involves using deep learning methods for automatic feature extraction. Models such as convolutional neural networks (Li et al., 2022; Cura et al., 2022), graph neural networks (Shan et al., 2022; Klepl et al., 2023), and transformers (Wang et al., 2024e) have been employed for representation learning. Some research also explores combining manual feature extraction with deep learning, such as extracting relative band powers and spectral coherence connectivity across different frequency bands and training convolutional networks on these extracted features (Miltiadous et al., 2023a)."}, {"title": "1. Introduction", "content": "edge. Our approach includes a full detection pipeline, including dataset selection, data preprocessing(e.g., channel and frequency alignment), self-supervised contrastive pre-training, and unified fine-tuning. We also introduce essential setups like subject-independent evaluation and majority voting for subject-level detection. The channel alignment in data preprocessing aligns all datasets into 19 standard channels, allowing us to train on different datasets. We pre-train our model on 11 datasets, which consist of 4 AD datasets and 7 additional datasets of other neurological diseases and healthy controls, including datasets for conditions like epilepsy and Parkinson's disease. This results in 2,354 subjects and 1,165,361 1-second, 128Hz samples. Our self-supervised learning design includes both sample-level and subject-level contrastive learning tasks. These tasks aim to do sample and subject discrimination, allowing the model to learn diverse EEG features that help minimize the interference of subject features in downstream tasks. We perform unified fine-tuning of the model in one run on 5 AD datasets to classify AD patients and healthy subjects, totaling 615 subjects and 223,039 1-second, 128Hz samples. We use the backbone that embeds cross-channel patches and the entire channel in parallel, capturing temporal and spatial features.\nThe final subject-level classification results for the 5 AD datasets\u2014ADFTD, BrainLat, CNBPM, Cognision-ERP, and Cognision-rsEEG\u2014are 91.34%, 89.98%, 100.00%, 84.42%, and 91.86%, respectively. We compare LEAD with state-of-the-art (SOTA) methods, including fully supervised, self-supervised, and EEG foundational model methods. Our results demonstrate significant improvements, with up to a 9.86% increase in F1 score at the sample level and up to a 9.31% improvement at the subject level, compared to SOTA methods. We also conduct a detailed ablation study to evaluate the impact of pre-training modules, the benefit of AD and non-AD datasets, and various training setups. Additionally, we provide supplementary studies on brain interpretability, including channel importance and frequency band analysis. Related works for EEG-based AD detection and self-supervised learning in EEG are in Appendix A.\nWe summarize our main contributions here:\n\u2022 We present LEAD, the world's first large foundational model for EEG-based AD detection, including a comprehensive method pipeline.\n\u2022 We construct the world's largest EEG-based AD detection corpus, consisting of 9 datasets with 813 subjects.\n\u2022 Our strong performance validates the effectiveness of subject-level contrastive pre-training and unified fine-tuning for EEG-based AD detection.\n\u2022 We release our code and model checkpoints to break the isolation in the EEG-based AD detection domain and facilitate future research."}, {"title": "2. Method", "content": ""}, {"title": "2.1. Problem Formulation", "content": "Sample-Level Classification. Consider an input EEG sample $x \\in \\mathbb{R}^{T\\times C}$ where $T$ denotes the number of timestamps and $C$ represents the number of channels. Our objective is to learn an encoder that generates a representation $h$ which can be used to predict the corresponding label $y \\in \\mathbb{R}$ for the input sample. Specifically, the label $y$ corresponds to either Alzheimer's Disease or Healthy controls.\nSubject-Level Classification. In addition to the corresponding label $y \\in \\mathbb{R}$ each input EEG sample also has a subject ID $s \\in \\mathbb{R}$ that indicates which subject the sample belongs to. The ultimate goal of EEG-based AD detection is to determine whether a subject has Alzheimer's Disease. For subject-level classification, we use a majority voting scheme, where the subject is assigned the label corresponding to the majority label of all samples from that subject."}, {"title": "2.2. Datasets Selection", "content": "AD Datasets. We review EEG-based AD detection papers published between 2018 and 2024 to identify potentially available datasets. We find 6 publicly available datasets containing AD subjects: AD-Auditory(Lahijanian et al., 2024), ADFSU(Vicchietti et al., 2023), ADFTD(Miltiadous et al., 2023b;a), ADSZ (Alves et al., 2022; Pineda et al., 2020), APAVA(Escudero et al., 2006; Smith et al., 2017), and BrainLat(Prado et al., 2023). Additionally, we use 3 private datasets: Cognision-ERP(Cecchi et al., 2015), Cognision-rsEEG, and CNBPM(Ieracitano et al., 2019b; Amezquita-Sanchez et al., 2019), bringing the total number of AD datasets to 9, and the total number of subjects to 813. We perform preliminary experiments on each dataset individually to assess their quality. For smaller datasets or those showing large performance variability across subjects, we use them for pre-training to alleviate potential data quality issues such as mislabeled subjects, interference from artifacts, collection devices, and collection methods. Five high-quality AD datasets, ADFTD, CNBPM, Cognision-rsEEG, Cognision-ERP, and BrainLat, are used for downstream tasks to evaluate the model performance.\nNon-AD Datasets. To enhance the learning of general EEG and AD-specific features, we use datasets of healthy subjects and other neurological diseases for self-supervised pretraining. We aim to increase the diversity of brain conditions, including healthy and diseased states, and increase the number of subjects used for training to reduce the interference of subject-specific patterns. Note that all the non-AD datasets have one commonality: the label is assigned to the subject, which adapts to the subject-level feature extraction. Datasets such as sleep stage detection and mental state classification are unsuitable here. We select publicly available datasets from sources like OpenNEURO2, Temple University Hospital\u00b3, and Brainclinics4. We choose datasets collected in a resting-state condition or involving resting-state tasks with either eyes open or closed to ensure consistency with most downstream AD datasets. In total, we select 7 proper large datasets, each with hundreds or even thousands of subjects. They are Depression (Cavanagh et al., 2019; Cavanagh, 2021), PEARL-Neuro (Dzianok & Kublik, 2024), REEG-BACA (Getzmann et al., 2024), REEG-PD (Singh et al., 2023), REEG-SRM (Hatlestad-Hall et al., 2022), TDBrain (Van Dijk et al., 2022), and TUEP (Veloso et al., 2017)."}, {"title": "2.3. Data Preprocessing", "content": "Two key challenges in training a large foundation model for time-series-like data are varying channel/variate numbers and heterogeneous sampling frequencies (Liu et al., 2024; Woo et al., 2024; Yang et al., 2024). However, we can easily align channels based on their names in EEG and align sampling frequency by resampling. More details and reasons for preprocessing steps are provided in Appendix D.\nArtifacts Removal. Some datasets have already undergone preprocessing steps during data collection, such as artifact removal and filtering. We perform a secondary preprocessing to align all datasets uniformly for training. All the fine-tuning datasets are guaranteed to be artifacts-free.\nChannel Alignment. We align all datasets to a standard set of 19 channels, which include Fp1, Fp2, F7, F3, Fz, F4, F8, T3/T7, C3, Cz, C4, T4/T8, T5/P7, P3, Pz, P4, T6/P8, 01, and O2, based on the international 10-20 systems. For datasets with fewer than 19 channels, we interpolate the missing channels using the MNE EEG processing package6. For datasets with more than 19 channels, we select the 19 channels based on the channel name and discard the others. In cases where datasets use different channel montages, such as the Biosemi headcaps with 32, 64, 128 channels7, we select the 19 closest channels by calculating the Euclidean distance between their 3D coordinates. The channel alignment allows us to pre-train the models on different datasets with any backbone encoder and perform unified fine-tuning on all AD datasets in one run.\nFrequency Alignment. In addition to channel alignment, we resample all datasets to a uniform sampling frequency of 128Hz, which is commonly used and preserves the key frequency bands (\u03b4, \u03b8, \u03b1, \u03b2, \u03b3), while also reducing noise."}, {"title": "2.4. Self-Supervised Pretraining", "content": "The region b) in figure 2 shows the flowchart of self-supervised contrastive pre-training.\nRepresentation Learning. For an input EEG sample $x_i$, where $i$ denotes the index of the sample $x_i$, we apply data augmentation methods a and b to generate two augmented views, $x^a_i$ and $x^b_i$. Given a backbone encoder $f(\u00b7)$ and a projection head $g(\u00b7)$, we compute their representations $h_i^a = f(x_i^a)$ and $h_i^b = f(x_i^b)$ after the encoder $f(\u00b7)$, and further obtain denser representations $z_i^a = g(h_i^a)$ and $z_i^b = g(h_i^b)$ through the projection head $g(\u00b7)$. The projection head is designed to benefit contrastive learning, as described in (Chen et al., 2020), and it will be discarded during downstream tasks, with only the encoder being used for the downstream task.\nSample-Level Contrasting. Sample-level contrasting is the most widely used framework in contrastive learning, as seen in SimCLR (Chen et al., 2020) and MOCO (He et al., 2020). The goal is to perform sample/instance discrimination and learn a representation that can distinguish one sample from others (Wu et al., 2018). A pre-trained model using this approach can capture general patterns in EEG data, benefiting downstream tasks by improving performance and reducing the need for labeled data. In this work, we adopt the SimCLR architecture, which treats different augmented views of the same sample as positive pairs and views from different samples as negative pairs. For an input sample $x_i\\in B$ in a batch, our sample-level InfoNCE contrastive loss is defined as follows:\n$L_{sam} = \\mathop{\\mathbb{E}}x_i  [-\\log \\frac{\\exp(\\text{sim}(z_i^a, z_i^b) / \\tau)}{\\sum_j \\exp(\\text{sim}(z_i^a, z_j^b) / \\tau)}]$ (1)\nwhere $j$ denotes the index of other samples in the batch $B$, and $\\text{sim}(u, v) = \\frac{u^T v}{\\|u\\| \\|v\\|}$ denotes the cosine similarity between vectors u and v. The parameter \u03c4 is a temperature parameter that adjusts the similarity scale.\nSubject-Level Contrasting. In EEG-based Alzheimer's"}, {"title": "2.4. Self-Supervised Pretraining", "content": "disease (AD) detection, each subject is typically associated with a stable medical state. Specifically, once a subject has AD or preclinical signs of AD, all EEG samples from that subject should exhibit features related to AD, meaning they share the same label during deep learning training. This prior knowledge allows us to perform subject-level contrasting, a concept first defined in (Wang et al., 2024b) and successfully applied in EEG and ECG-based disease detection (Kiyasseh et al., 2021; Wang et al., 2024b; Abbaspourazad et al., 2024). In subject-level contrasting, we treat samples from the same subject as positive pairs and samples from different subjects as negative pairs. With an increasing number of subjects used in pre-training, we aim for the model to learn diverse feature types and reduce interference from unrelated subject-specific features during downstream classification. Appendix F.3 and H.2 provide more details on the effectiveness and analysis of subject-level contrasting. For an input sample $x \\in B$ in a batch, our subject-level InfoNCE contrastive loss is defined as follows:\n$L_{Sub} = \\mathop{\\mathbb{E}}x_i  [\\mathop{\\mathbb{E}}x_k  [-\\log \\frac{\\exp(\\text{sim}(z_i^a, z_k^b) / \\tau)}{\\sum_j \\exp(\\text{sim}(z_i^a, z_j^b) / \\tau)}]]$ (2)\nwhere $x_k$ denotes samples from the same subject as $x_i$ in the batch, with the same subject ID $s_k = s_i$. The function $\\text{sim}(u, v) = \\frac{u^T v}{\\|u\\| \\|v\\|}$ represents the cosine similarity, and \u03c4 is a temperature parameter that adjusts the scale. Note that not all neurological diseases can utilize subject-level contrasting. For instance, seizures are a condition where the EEG patterns during a seizure phase differ significantly from those in the regular phase for the same subject.\nOverall Loss Function. The overall loss function is the weighted sum of the sample-level and subject-level contrastive losses is defined as follows:\n$L = \\lambda_1 L_{Sam} + \\lambda_2 L_{Sub}$ (3)\nwhere \u03bb1 + \u03bb2 = 1 are hyper-coefficients that control the relative importance and adjust the scales of each level's loss.\nIndices Shuffling. In real-world scenarios, the likelihood of samples with the same subject ID appearing in the same training batch decreases as the number of subjects increases. This can hinder subject-level contrastive learning. To address this issue, we develop an indices shuffling algorithm that shuffles the order of samples in each epoch. The goal is to ensure that samples with the same subject ID are present in the batch while introducing randomness in the sample order every epoch. More algorithm description and Pseudo code details are presented in Appendix B."}, {"title": "2.5. Backbone Encoder Architecture", "content": "We use a simplified version of ADformer (Wang et al., 2024e) as the backbone encoder f(\u00b7), adopting single-granularity learning only. This architecture is designed for EEG-based AD detection and efficiently captures temporal features along the time dimension and spatial features among channels, as both are critical for EEG feature representation learning. For simplicity, we omit the subscript i for the input sample x in this subsection, as it is not necessary for the illustration. The temporal and spatial branches are computed in parallel before the projection head or classifier. Both branches use the standard encoder-only transformer, including self-attention, layer normalization, and feed-forward networks. The region c) in figure 2 illustrates the architecture of the backbone encoder.\nTemporal Branch. Given an input EEG sample $x \\in \\mathbb{R}^{T\\times C}$ and patch length L, where T and C denote the number of timestamps and channels, respectively. We first segment the input sample into N cross-channel non-overlapping patches to obtain $x \\in \\mathbb{R}^{N\\times (L\\cdot C)}$. Zero padding is applied to ensure that the number of timestamps T is divisible by L, resulting in $N = [\\frac{T}{L}]$. The patches $x^t$ are then mapped into D-dimensional patch embeddings using a linear projection $W$, and a fixed positional embedding $W^{pos}$ (Vaswani et al.,"}, {"title": "2.5. Backbone Encoder Architecture", "content": "2017) is added to produce the final patch embeddings: $e^t = x^t W + W^{pos}$ where $e^t \\in \\mathbb{R}^{N\\times D}$, $W \\in \\mathbb{R}^{(L\\cdot C)\\times D}$, and $W^{pos} \\in \\mathbb{R}^{N\\times D}$. The final patch embeddings $e^t$ are used as input tokens for the standard encoder-only transformer. After M encoding layers, we obtain the temporal branch's final representations $h^t$.\nSpatial Branch. Given an input EEG sample $x \\in \\mathbb{R}^{T\\times C}$, we first transpose the sample and add a fixed channel-wise positional embedding $W^{pos}$ to obtain $x^s = \\text{Transpose}(x)+ W^{pos}$, where $x^s, W^{pos} \\in \\mathbb{R}^{C\\times T}$. Unlike the temporal branch, where positional embeddings are added after embedding, we add channel-wise positional embeddings on the raw input EEG data since the subsequent up-dimension process destroys the information of raw channel order. For a target channel number F and embedding dimension D, we first perform an up-dimensional transformation using a 1-D convolution $W_1$ to increase the channel number. Then, we map the entire series of each channel into a latent embedding using a linear projection $W_2$ to get the final channel embeddings: $e^s = (W_1 x^s)W_2$, where $e^s \\in \\mathbb{R}^{F\\times D}$, $W_1 \\in \\mathbb{R}^{F\\times C}$, and $W_2 \\in \\mathbb{R}^{T\\times D}$. The final channel embeddings $e^s$ are used as input tokens for the standard encoder-only transformer. After M encoding layers, we obtain the spatial branch's final representation $h^s$."}, {"title": "2.6. Important Setups", "content": "Subject-Independent. Two main setups are commonly used for evaluation in the EEG-based AD detection domain: subject-dependent (Nour et al., 2024; kumar Ravikanti & Saravanan, 2023) and subject-independent (Watanabe et al., 2024; Chen et al., 2024). In the subject-dependent setup, all samples are mixed together and split into training, validation, and test sets, allowing samples from the same subject to appear in all three sets. In contrast, the subject-independent setup splits the training, validation, and test sets based on subjects, ensuring that samples from the same subject are exclusively assigned to one set (Wang et al., 2024c). Unlike many existing works that use the subject-dependent setup, we use the subject-independent setup. The subject-dependent setup is unsuitable for real-world scenarios and leads to significant data leakage (Wang et al., 2024d).\nUnified Fine-tuning. The channel alignment in our data preprocessing step enables us to pre-train the model on various datasets and then fine-tune it on all downstream datasets simultaneously. We refer to this as \"unified fine-tuning,\" where the model is fine-tuned across all downstream AD datasets in one run. The best model is then selected based on the weighted performance across the downstream datasets, ensuring that the model performs optimally on all tasks.\nMajority Voting. For subject-level EEG-based AD detection, we apply a majority voting scheme to determine the final classification label for each subject. Specifically, for all the samples from one subject (with the same subject ID s), we find the majority label of these samples and assign this label to this subject. For example, if a subject has 100 samples and more than 50 are classified as AD, the subject will be labeled \"AD.\" The voting mechanism alleviates the interference of outlier samples in a subject."}, {"title": "3. Experiments", "content": "Datasets. We pre-train on 11 datasets: AD-Auditory (Lahijanian et al., 2024), ADFSU (Vicchietti et al., 2023), ADSZ (Pineda et al., 2020), APAVA (Escudero et al., 2006), Depression (Cavanagh et al., 2019), PEARL-"}, {"title": "3. Experiments", "content": "Neuro (Dzianok & Kublik, 2024), REEG-BACA (Getzmann et al., 2024), REEG-PD (Singh et al., 2023), REEG-SRM (Hatlestad-Hall et al., 2022), TDBrain (Van Dijk et al., 2022), and TUEP (Veloso et al., 2017), and fine-tuning on 5 downstream datasets: ADFTD (Miltiadous et al., 2023b), BrainLat (Prado et al., 2023), CNBPM (Amezquita-Sanchez et al., 2019), Cognision-ERP (Cecchi et al., 2015), and Cognision-rsEEG. The pre-training datasets include 7 non-AD neurological diseases or healthy subjects and 4 AD datasets, totaling 2,354 subjects and 1,165,361 1-second, 128Hz samples. All downstream datasets are binary classifications between AD patients and healthy subjects, totaling 615 subjects and 223,039 1-second, 128Hz samples. The nine AD datasets used for pretraining or fine-tuning consist of 813 subjects in total. The rationale behind selecting these datasets for pre-training and fine-tuning is discussed in 2.2. The unified processing pipeline for each dataset is detailed in 2.3, with a more detailed description available in Appendix D. The statistics for the processed datasets are summarized in Table 1.\nBaselines. We compare our method with 10 baselines, including 5 supervised, 3 self-supervised learning, and 2 large EEG foundational models. These selected baselines are state-of-the-art methods or have shown strong performance in EEG or time series classification tasks. The 5 supervised learning methods include TCN (Bai et al., 2018), vanilla Transformer (Vaswani et al., 2017), Conformer (Song et al., 2022), TimesNet (Wu et al., 2023), and Medformer (Wang et al., 2024c). The 3 self-supervised learning methods are TS2Vec (Yue et al., 2022), BIOT (Yang et al., 2024), and EEG2Rep (Mohammadi Foumani et al., 2024). The 2 large EEG foundational models are LaBraM (Jiang et al., 2024) and EEGPT (Wang et al., 2024a).\nImplementation. All baseline methods and our method's variants, except for LaBraM and EEGPT, are trained under the same code framework. The training epoch for self-supervised pretraining is fixed at 50 epochs, with no early stopping mechanism. The training epoch is set to 100 for fully supervised learning or fine-tuning, with early stopping after 15 epochs of patience based on the best F1 score. The batch sizes for pretraining, fully supervised learning, and fine-tuning are set to 512, 128, and 128, respectively. The optimizer is AdamW. The initial learning rates for pretraining, fully supervised learning, and fine-tuning are set to 0.0002, 0.0001, and 0.0001, respectively, with the CosineAnnealingLR learning scheduler. Gradient norm clipping is set to 4.0, and Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) is enabled to benefit inter-subject representation learning. For LaBraM and EEGPT, we use their public code and load their pre-trained models for fine-tuning. We employ four evaluation metrics: sample-level accuracy and F1 score (macro-averaged), and subject-level accuracy and F1 score (macro-averaged) after majority voting, as described in 2.6."}, {"title": "3.1. Comparison with Baselines", "content": "Setup. Our method has three variants based on training setups: LEAD-Vanilla(3.21M), LEAD-Sup(3.21M), and LEAD-Base(3.41M). The LEAD-Vanilla model is trained fully supervised on a single dataset without channel alignment, such as the 7-channel version of the Cognision-ERP dataset. LEAD-Sup and LEAD-Base use datasets with alignment to 19 channels. LEAD-Sup is the model trained unified supervised on 5 AD datasets together without pre-training. For LEAD-Base, we first perform self-supervised pre-training on 11 pre-training datasets. The trained model is then used for unified fine-tuning on 5 downstream AD datasets. Note that for both LEAD-Sup and LEAD-Base, the 5 downstream AD datasets are unified trained and evaluated in one run, which is different from the usual approach where supervised training or fine-tuning occurs on a single dataset. The five supervised learning baselines, including TCN, Transformer, Conformer, TimesNet, and Medformer, use the same setup as LEAD-Vanilla. The three self-supervised learning baselines, including TS2Vec, BIOT,"}, {"title": "3.1. Comparison with Baselines", "content": "and EEG2Rep, follow LEAD-Base's setup. For the two large EEG foundational models, LaBraM and EEGPT, we load their pre-trained models and use the same fine-tuning setup as our LEAD-Base. Appendix E provides more details about the implementation setups.\nResults. The results are presented in Table 2. Our method significantly improves accuracy and F1 score compared with all baselines for both sample-level and subject-level classification. Specifically, our method outperforms the best baseline methods by 6.9%, 5.72%, 3.85%, 7.81%, and 11.16% in F1 score at the subject-level on the ADFTD, BrainLat, CNBPM, Cognision-ERP, and Cognision-rsEEG datasets, respectively. The comparison between our method and the supervised learning baselines highlights the effectiveness of channel alignment; although some information might be lost during alignment, the ability to allow unified training still demonstrates substantial performance improvements compared to supervised learning methods on raw-channel datasets. The comparison with self-supervised learning baselines underscores the effectiveness of our contrastive learning approach. The sample-level and subject-level contrasting show a strong learning ability for inter-subject classification. The two large EEG models perform poorly on the ADFTD and BrainLat datasets, achieving almost random results. The comparison between ours and their methods emphasizes the importance of selecting proper pre-training datasets. Our selection of healthy and neurological disease datasets for pre-training contributes significantly to the downstream classification between AD and healthy controls.\nAmong the three variants of our methods, the LEAD-Base achieves the best performance in most cases, except for the ADFTD dataset, where LEAD-Sup performs better. The comparison between LEAD-Vanilla and LEAD-Sup shows that leveraging more AD datasets for training benefits performance, even in a fully supervised learning manner. The"}, {"title": "4. Conclusion", "content": "In this paper, we present LEAD, the world's first large foundational model for EEG-Based Alzheimer's Disease detection on the world's largest EEG-based AD detection corpus, including 813 subjects. We design a complete pipeline encompassing dataset selection, data processing, pre-training framework, model architecture, and evaluation metrics. We perform self-supervised pre-training on 4 AD datasets and 7 non-AD neurological diseases or healthy control datasets, totaling 2,354 subjects. The self-supervised pre-training includes sample-level and subject-level contrastive learning. Unified fine-tuning is performed on 5 AD datasets with channel-aligned datasets, totaling 615 subjects. We use a backbone encoder that can leverage both temporal and spatial features. The significant improvement compared with state-of-the-art baselines demonstrates the effectiveness of our design for dataset selection, channel alignment, self-supervised pre-training, and unified fine-tuning. We hope to inspire future research on EEG-based AD detection and other neurological disease detection, such as Parkinson's disease. More discussion on the existing large EEG model, the effectiveness of our subject-level contrasting, and limitations & future works are presented in Appendix H."}, {"title": "Impact Statement", "content": "This paper introduces the first large foundational model for EEG-based Alzheimer's Disease detection, trained on the largest EEG-AD corpus to date. Our results demonstrate the effectiveness of large pre-trained models and multi-dataset fine-tuning for AD detection, provided that appropriate training methods and datasets are selected. Our approach significantly outperforms methods trained on single datasets and other state-of-the-art self-supervised pre-training methods and large EEG foundational models trained on multiple datasets. The subject-independent evaluation, which tests on unseen subjects, further highlights the applicability of our method in real-world scenarios. We open-source our code, pre-trained model, and fine-tuned model with the hope that this work will drive progress in EEG-based AD detection and inspire future research in detecting other brain disorders and neurodegenerative diseases."}, {"title": "A. Related Work", "content": ""}, {"title": "A.1. EEG-Based Alzheimer's Disease Detection", "content": "In the last two decades", "directions": "manual biomarker extraction and deep learning representation. Biomarker Extraction: This research direction aims to identify potential biomarkers in EEG signals of AD patients and use simple classifiers, such as Multi-Layer Perceptrons (MLP) and Support Vector Machines (SVM), to differentiate these features from normal healthy subjects. Different types of EEG features are used, including statistical features like Mean, Skewness, Kurtosis, and Standard Deviation (Tzimourta et al., 2019b;a; Kulkarni & Bairagi, 2017; Kanda et al., 2014; Waser et al., 2013; Tylova et al., 2013; Mora-S\u00e1nchez et al., 2019), spectral features like Phase Shift, Phase Coherence, Bispectrum, and Bicoherence (Wang et al., 2017; Cassani et al., 2014; Wang et al., 2015; Fraga et al., 2013; Tait et al., 2019; Waser et al., 2016; Trambaiolli et al., 2011), power features like Power Spectrum Density, Relative Band Power, Ratio of EEG Rhythm, and Energy (Fahimi et al., 2017; Schmidt et al., 2013; Liu et al., 2016; Kanda et al., 2014), as well as complexity features like Shannon Entropy, Tsallis Entropy, and Permutation Entropy (Garn et al."}]}