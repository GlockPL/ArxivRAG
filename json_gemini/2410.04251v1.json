{"title": "Enhancing Future Link Prediction in Quantum Computing Semantic Networks through LLM-Initiated Node Features", "authors": ["Gilchan Park", "Paul Baity", "Byung-Jun Yoon", "Adolfy Hoisie"], "abstract": "Quantum computing is rapidly evolving in both physics and computer science, offering the potential to solve complex problems and accelerate computational processes. The development of quantum chips necessitates understanding the correlations among diverse experimental conditions. Semantic networks built on scientific literature, representing meaningful relationships between concepts, have been used across various domains to identify knowledge gaps and novel concept combinations. Neural network-based approaches have shown promise in link prediction within these networks. This study proposes initializing node features using LLMs to enhance node representations for link prediction tasks in graph neural networks. LLMs can provide rich descriptions, reducing the need for manual feature creation and lowering costs. Our method, evaluated using various link prediction models on a quantum computing semantic network, demonstrated efficacy compared to traditional node embedding techniques.", "sections": [{"title": "1 Introduction", "content": "Quantum computing is an active area of research in both physics and computer science, due to its potential to solve complex quantum physics problems and significantly accelerate certain computational processes (Shor, 1997; Montanaro, 2016; Arute et al., 2019). However, the current limitations of hardware hinder the practical application of quantum computers (Krantz et al., 2019; Kjaergaard et al., 2020), and the further development of robust quantum processors involves an increasingly wide range of conditions (Huang et al., 2021; Martinis, 2021), material characteristics (Murray, 2021; Place et al., 2021), and physical phenomena. Understanding the correlations among these variables and predicting their potential interconnections in the future is crucial for experimental progress. Scientific literature serves as a vital resource for acquiring this knowledge, as it encompasses a vast array of research work.\nA semantic network represents meaningful relationships between concepts, and researchers constructed a semantic network based on co-occurring concepts from scientific literature, utilizing it to identify knowledge gaps, missing connections between concepts, and novel combinations not previously considered (Rzhetsky et al., 2015; Krenn and Zeilinger, 2020). In recent years, Graph Neural Networks (GNNs) demonstrated promising predictive capabilities for link prediction within the graph forms of semantic networks (Zhang and Chen, 2018; Li et al., 2023). A significant challenge in creating semantic networks is the provision of sufficient initial features for nodes within a graph. In many real-world graph datasets, node features are often either missing or insufficient, potentially hindering link prediction models for effective learning and prediction (Zhao et al., 2017).\nThis study aims to initialize node features using Large Language Models (LLMs). LLMs have demonstrated exceptional performance across various question-answering tasks and information retrieval systems in zero-shot conditions (Kamalloo et al., 2023; Zhu et al., 2023), significantly improving text embeddings (Wang et al., 2023). These embeddings serve as initial node representations for link prediction tasks in GNNs. The rationale behind this approach is that LLMs, trained on extensive datasets from diverse literature and online sources, can provide rich descriptions of relevant concepts. This method enhances the feature set available for GNN training and reduces the reliance on human-curated feature creation. Additionally, it has the potential to produce more reliable node representations compared to traditional connectivity-based embeddings, particularly when connectivity data is lacking. In cold-start link prediction"}, {"title": "2 Creation of Semantic Network for Quantum Computing", "content": "The construction of a concept network from scratch necessitates significant human resources and time. As an alternative, we utilize the pre-existing semantic network of quantum physics. SEMNET (Krenn and Zeilinger, 2020) was developed to forecast future research trends in quantum physics. It is based on scientific publications in the field of quantum physics and includes a list of human-compiled quantum physics concepts derived from books and Wikipedia. In SEMNET, nodes represent physical concepts, and edges indicate the co-occurrence of two concepts in an article's title or abstract.\nOur domain experts in quantum physics have scrutinized the list of concepts, selecting those that have a high correlation with quantum computing research. Certain concepts were revised, and additional ones that appeared to be pertinent to quantum computing research, such as \"quasiparticle poisoning,\" were incorporated. This process resulted in a compilation of 3,001 quantum computing concepts. Given the lack of a corpus upon which SEMNET was created, we scraped all articles under the quantum physics category on arXiv that contained at least a pair of quantum computing concepts. This resulted in a total of 136,122 papers spanning from 2007 to 2024 (as of June 15, 2024)."}, {"title": "3 LLM-powered Concept Feature Embedding", "content": "We derived quantum computing concept features from LLMs using a question-answering prompt. For this purpose, we employed a selection of state-of-the-art LLMs, specifically Gemini-1.0-pro"}, {"title": "4 Experiments", "content": "We evaluated the initial node representations generated by LLMs in a transductive experimental setup, where models predict edges between existing nodes in the graph. The models were trained on historical concept connections and tasked with predicting future, unknown connections. The evaluation was performed using various link prediction algorithms."}, {"title": "4.1 Experiment Setup", "content": "Dataset An undirected homogeneous (binary) graph, also known as a single relational graph, was constructed for the purpose of link prediction. The dataset was divided into three subsets based on specific time intervals, a common approach for time series data in link prediction tasks (chronological splitting). The training set encompassed the period from 2007 to 2021 and included 428,079 edges. The validation set corresponded to the year 2022 and contained 25,011 edges. The test set covered"}, {"title": "Implementation Details & Evaluation Setting", "content": "The experiments were conducted using 4\u00d7NVIDIA A100 80GB GPUs. Specifically, the Mixtral-8x7B-Instruct (46B) and LLaMA-3 (70B) models were executed on 4\u00d7NVIDIA A100 80GB GPUs to generate concept features. All link prediction methods were performed on a single NVIDIA A100 80GB GPU. The Google Gemini-1.0-pro and text-embedding-004 models were accessed via the Gemini APIs. The maximum number of generated tokens per query was set to 512 for all models, and the default embedding size of 768, as produced by the Google text embedding model, was employed. To maintain consistency, node embeddings of size 768 were also generated using the baseline methods. In terms of latency, the Gemini-pro model required approximately 3 hours, the Mixtral model 6 hours, and the LLaMA-3 (70B) model 8 hours to generate features related to quantum computing concepts. The text embedding process was completed in less than 2 minutes.\nWe followed the hyper-parameter ranges for the models employed in the comprehensive link prediction evaluation (Li et al., 2023). For the model evaluation, we measured the area under the receiver operating characteristic curve (AUROC) and average precision (AP), which are commonly used metrics for link prediction tasks in homogeneous graphs. These metrics provide a robust and comprehensive assessment of the model's performance"}, {"title": "4.2 Node Embedding Comparison Results", "content": "Table 1 presents the link prediction results using baseline and LLM-powered node embeddings. The majority of models initialized with LLM-generated embeddings demonstrated higher performance than their baseline counterparts. Notably, the LLM-generated features resulted in more significant improvements in MLP and message passing GNNS (GCN, GraphSAGE, GAE) than in the GNN with pair-wise information methods (NCN, BUDDY). This can be attributed to the fact that MLP and GNNs relying on message passing mechanisms are generally more impacted by the initial node embeddings compared to those models that incorporate additional link specific information. Message passing aggregates information from a node's neighborhood, and if the initial embedding already captures substantial information, it can have a stronger influence on the final embedding.\nWe further compared node feature initialization methods on isolated (zero-degree) nodes, which pose significant challenges for GNNs (Ahn and Kim, 2021; Zanardini and Serrano, 2024). We identified 30 isolated nodes in the training data and 1,382 connections to these in the test data. The evaluation results are presented in Appendix B. Although the baseline methods exhibited higher performance in certain instances, particularly with the GCN in conjunction with LINE, the representations produced by LLMs were generally more effective in identifying previously unseen connections to isolated nodes. Furthermore, they yielded a more consistent performance in comparison to the baseline methodologies. For these isolated nodes, the content of the node features is crucial for the link prediction task due to the absence of connectivity information. The baseline models typically generate node embeddings based on the connectivity information of a graph, which may result in inadequate embeddings for isolated nodes. In contrast, embeddings generated by LLMs are robust against the absence of link connectivity information and can thus produce reliable representations for isolated nodes."}, {"title": "4.3 Merging LLM embeddings", "content": "We conducted an evaluation incorporating features from various models. To merge conceptual features derived from multiple LLMs, we employed mean- and max-pooling on the embeddings, and we extracted concise conceptual features from the outputs of three LLMs using the Gemini-pro model. The prompt used was: \"Summarize this text about the features of {KEYWORD}. Text: {CONCATENATED FEATURES FROM THE LLMs}\". Furthermore, we evaluated a method to select the optimal response for each query from three distinct models, utilizing LLM-Blender (Jiang et al., 2023). This ensembling framework chooses the top"}, {"title": "4.4 Time Decayed Embedding", "content": "Additionally, we incorporated time-decay information from the dataset. In future trend predictions involving time series data, time-decayed information can be important for maintaining the relevance of recent data and highlighting recent changes. To leverage the time-decay information of concept pairs over time, we developed time-decayed node representations based on co-occurrence matrices by year. These representations can be integrated with the LLM feature embeddings to enhance the model's capabilities. The time-decayed embeddings serve as optional auxiliary data in time series analyses, as they cannot function as standalone embeddings due to the potential lack of connections between concepts, which would result in non-informative embeddings. A comprehensive explanation of the time-decayed embedding generation process is available in the Appendix C. Table 3"}, {"title": "5 Implications of Model Predictions: An Analytical Review by Domain Scientists", "content": "Our domain scientists examined the connections commonly predicted correctly by the top three models. While many of the link predictions correspond to fundamental concept connections that have existed within the field of quantum information science for many years (e.g., \u201cnonlinear oscillator\" and \u201ctransmon,\u201d \u201cHilbert space\u201d and \u201cquantum information,\u201d etc.), some of the emerging connections within the test set seem timely and point towards recent trends and scientific breakthroughs within the field. Two examples relevant to quantum engineering that were observed within the data set are listed below.\nThe models accurately predicted a breakthrough in the coherent control of magnons (Xu et al., 2023). Methods for coupling classical magnons to photon cavities have been in development for the past decade (Huebl et al., 2013; Tabuchi et al., 2015; Boventer et al., 2018, 2020), and the recent development of nonclassical coherent control of magnons built on this prior work. Therefore, it can be concluded that the models recognized the trend toward coherently controlled quantum magnonics for its prediction.\nLikewise, the models recognized the importance of phonon engineering to the performance of superconducting qubits (Kitzman et al., 2023). This concept connection, which emerged naturally with the field of superconducting technologies, has been vitally important within the context of recent studies (Wilen et al., 2021; Yelton et al., 2024) on gamma and muon ray impacts on superconducting quantum devices, wherein phonons serve as the mediating particle for qubit decoherence from such high-energy particles. Indeed, phonon engineering will likely prove to be an essential component of quantum engineering in the coming years."}, {"title": "6 Related Work", "content": "LLMs have shown impressive performance across numerous NLP tasks, particularly in node classification on graphs (Fatemi et al., 2024; Chen et al., 2024). However, they struggle to capture graph structural information (Wang et al., 2024a) and face scalability issues (Hu et al., 2020) due to higher prediction costs compared to GNNs. Despite this, LLMs provide valuable semantic knowledge, particularly for node feature initialization, enhancing GNN performance in link prediction. In this study, we employed three advanced LLMs: Google's Gemini Pro (Gemini-Team et al., 2023), a multimodal model for complex reasoning, Mixtral-8x7B (Jiang et al., 2024), which supports long sequences and efficient inference, and Meta's Llama 3 (Meta-AI, 2024), known for its optimized architecture and versatility across tasks.\nGNNs have become a powerful method for homogeneous link prediction. Architectures like GCN (Kipf and Welling, 2016a), GraphSAGE (Hamilton et al., 2017), and GAE (Kipf and Welling, 2016b) encode node features and graph topology into low-dimensional embeddings for predicting link likelihood between nodes. Variational autoencoders (VAEs) (Ahn and Kim, 2021) further enhance representation learning by encoding data into a latent space for reconstruction. GNNs excel in capturing higher-order relationships and learning expressive node representations, outperforming traditional heuristic methods, especially in large, complex networks. Recent approaches like BUDDY"}, {"title": "7 Conclusion and Future Work", "content": "Our proposed approach offers a promising avenue for enhancing the performance of link prediction models, particularly in scenarios where initial node features are sparse or inadequate. This method not only enriches the feature set available for model training but also improves the model's ability to capture and represent complex patterns within the data. We applied this method to a quantum computing semantic network constructed from relevant scientific literature, and the models with node feature initialization by LLMs outperformed baseline node embedding methods across various link prediction models. Our approach is easily extendable to other graph datasets in different domains that lack adequate node features.\nIn this study, we focused exclusively on featurizing nodes within a graph, although edge features are also crucial for training models. Unlike node features, generating edge features via LLMs may not be practical due to the significantly higher number of edges compared to nodes. More effective edge feature generation methods by LLMs, such as clustering edges based on the characteristics of the involved nodes, will be explored in future research. Additionally, our experiments were limited to static graph settings. Dynamic GNNs and time-dependent graph methods could potentially improve prediction capabilities. Future work will aim to refine this approach further and explore its applicability in other graph-based learning tasks."}, {"title": "Algorithm 1 Time-Decayed Node Embedding Generation Procedure", "content": "Require: Co-occurrence matrices for each year\nEnsure: Node embeddings\n1: Convert co-occurrence matrices to PPMI matrices\n2: for each year do\n3: Apply exponential time decay function:\n$N(t) = N_0e^{-\\lambda t}$\n4: end for\n5: Aggregate the matrices\n6: Reduce the dimension of the aggregated matrix to the same embedding size as the LLM feature embedding using SVD\n7: Concatenate the time decayed embeddings with the LLM feature embeddings"}]}