{"title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression", "authors": ["J. Pablo Mu\u00f1oz", "Jinjie Yuan", "Nilesh Jain"], "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant challenges regarding the computational resources required for fine-tuning and deployment. Recent advancements in low-rank adapters have demonstrated their efficacy in parameter-efficient fine-tuning (PEFT) of these models. This retrospective paper comprehensively discusses innovative approaches that synergize low-rank representations with Neural Architecture Search (NAS) techniques, particularly weight-sharing super-networks. Robust solutions for compressing and fine-tuning large pre-trained models are developed by integrating these methodologies. Our analysis highlights the potential of these combined strategies to democratize the use of LLMs, making them more accessible for deployment in resource-constrained environments. The resulting models exhibit reduced memory footprints and faster inference times, paving the way for more practical and scalable applications of LLMs. Models and code are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "sections": [{"title": "Introduction and Preliminaries", "content": "Structured low-rank representations (Kolda and Bader 2009) have played a significant role in the latest successes in Artificial Inteligence (AI). For example, low-rank adaptation (LoRA) (Hu et al. 2022) is a preferred method for parameter-efficient fine-tuning (PEFT) of large foundation models (Bommasani et al. 2021). LoRA expands a linear layer by attaching low-rank adapters, L\u2081 and L\u2082, as demonstrated in the following equations:\n$\\qquad Y = XW,                                                     (1)$\n$\\qquad Y = XW + \u03b4XL_1L_2,                                        (2)$\nwhere X is the input to the layer, and W are the layer's weights. s is a scaling factor. W remains frozen, and only the adapters' weights are adapted during fine-tuning, which is significantly more efficient than performing full fine-tuning. Often, the number of parameters in the adapters is a minimal fraction of the total number of parameters in the base model.\nNeural Architecture Search (NAS) techniques attempt to identify a high-performing architectural configuration from a search space of candidate architectures (White et al. 2023). NAS techniques evolved rapidly with the advent of deep learning. However, many NAS techniques have become obsolete with the increasing size of large AI models because it is too resource-demanding to evaluate many possible architectures when models have billions of parameters. A particular efficient NAS technique relevant to this paper uses weight-sharing super-networks generated by activating substructures of the original neural network (Cai et al. 2020).\nWe claim that the benefits of cross-pollination between low-rank representations and weight-sharing neural architecture search techniques are bi-directional:\n\u2022 NAS techniques enhance low-rank adapters, and,\n\u2022 NAS becomes more efficient by incorporating the guidance of low-rank representations.\nIn the following sections, we discuss several solutions that realize these benefits and suggest additional enhancements to be explored in the future."}, {"title": "Elastic LORA Adapters and Their Applications", "content": "In this section, we first introduce the Elastic LoRA Adapter, highlighting its capability to dynamically adjust adapter configurations. This adaptability, coupled with an extensive sub-adapter search space, facilitates its application across various scenarios, enhancing model compression and fine-tuning efficiency and effectiveness. Next, let us delve step by step into the story of the combination of LORA adapters and NAS techniques.\nElastic Adapter In weight-sharing NAS, an elastic layer, as opposed to a traditional static layer, has variable values for its properties. For instance, the weights, $W \\in \\mathbb{R}^{m \\times n}$, of a linear layer might be masked or sliced to activate a smaller structure, $W \\in \\mathbb{R}^{m \\times k}$ where k < n. By allowing the activation of variable configurations of a layer during the forward and backward passes, one is effectively training a super-network in which the smaller structures share their weights with their bigger counterparts. Recent advancements in NAS weight-sharing techniques have been utilized in conjunction with low-rank representations. As illustrated in Figure 1, the Elastic LoRA Adapter primarily operates in two modes:\ni) Mode A: In the LoRA adapter, matrices $L_1 \\in \\mathbb{R}^{m \\times r}$ and $L_2 \\in \\mathbb{R}^{r \\times n}$ can be rendered elastic by adopting smaller rank values. Specifically, $L_1^s \\in \\mathbb{R}^{m \\times {r_0, r_1, \\ldots, r}}$ and $L_2^s \\in \\mathbb{R}^{{r_0, r_1, \\ldots, r} \\times n}$, where $r_i \\leq r, r < m$ and $r < n$ (Mu\u00f1oz, Yuan, and Jain 2024).\nii) Mode B: Alternatively, L\u2081 and L\u2082 can achieve elasticity by allowing the activation of substructures with reduced channel widths. This is represented as $L_1^s \\in \\mathbb{R}^{{m_0, m_1, \\ldots, m} \\times {r_0, r_1, \\ldots, r}}$ and $L_2^s \\in \\mathbb{R}^{{r_0, r_1, \\ldots, r} \\times {n_0, n_1, \\ldots, n}}$, where $m_i < m$ and $n_i < n$ (Mu\u00f1oz et al. 2024).\nTo this end, we will describe several methodologies in which low-rank structures and weight-sharing supernetworks techniques can benefit each other."}, {"title": "Efficient Neural Architecture Search with the Guidance of Low-Rank Adapters", "content": "LONAS During fine-tuning, the sub-adapters activated can be used to guide the activation of substructures in the base model, as illustrated in Figure 2. This approach corresponds to Mode B in Figure 1, which is characterized by its ability to reduce the overall number of parameters in the model compared to Mode A. In this scenario, the adapters are elastic, and the frozen weights of the base model, denoted as $W \\in \\mathbb{R}^{m \\times n}$, are transformed into $W_s \\in \\mathbb{R}^{m \\times {n_0, n_1, \\ldots, n}}$ or $W \\in \\mathbb{R}^{m \\times n}$ into $W_s \\in \\mathbb{R}^{{m_0, m_1, \\ldots, m} \\times n}$.The search space of possible low-rank adapter configurations is generated by allowing several configurations in the width and rank of the adapters. This alignment, as proposed by LoNAS (Mu\u00f1oz et al. 2024), results in models with fewer parameters than the base model while maintaining a minimal drop in accuracy and achieving immediate improvements in inference speedup. LoNAS is analogous to traditional NAS, but with the key distinction that only the adapter parameters are trained. The expectation is that this training will guide and adapt the model's search and pruning processes. Due to the high cost of searching LLM with adapters, LoNAS also proposes heuristic sub-networks (i.e., middle point of the search space) to quickly evaluate the quality of the trained super-network. Users can then decide whether further search is necessary based on specific needs. This heuristic strategy has also been applied in subsequent works such as Shears (Mu\u00f1oz, Yuan, and Jain 2024) and SQFT (Mu\u00f1oz, Yuan, and Jain 2024), which are discussed later.\nEmpirical results demonstrate that this approach yields an inference speedup of up to 1.4x and can reduce the model parameters by approximately 80% compared to the original model. For more detailed information, refer to Table 1 and the subsequent section.\nLONAS enhancements have recently been proposed (Sukthanker et al. 2024) by applying elastic LoRA adapters to all the weight matrices of the transformer and allowing the removal of entire transformer blocks. Initially, LoNAS focused solely on the Self-Attention and MLP layers. While LONAS and its extensions have proven effective, they still face challenges due to the size of state-of-the-art pre-trained models, which have driven the development of more efficient solutions, which we will discuss in the following sections."}, {"title": "Restricting the Elasticity to the Adapter Rank and Exploiting Model Sparsity and Low Numerical Precision", "content": "Shears Building on LoNAS, Shears (Mu\u00f1oz, Yuan, and Jain 2024) proposes several modifications to enhance the efficiency of the fine-tuning stage. This approach constrains the application of elasticity exclusively to the low-rank structures, leaving the significantly demanding weights of the base model intact. This strategy is termed Neural Low-Rank Adapter Search (NLS). Additionally, as illustrated in Figure 3, the base model can be sparsified using an arbitrary metric, \u03a8, to determine the importance of the pre-trained weights. A popular weight importance metric is Wanda (Sun et al. 2023), where a few feature input activations, X, are used to assess the importance of the weights, i.e., $\u03a8(W) = |W| \\cdot || X ||^2$. This score, combined with a desired sparsity level, s, is used to obtain the sparse weights, WP, with a sparsity pattern $S{W^p} = {(i,j) | W_{i,j} \\neq 0, 1 \\leq i \\leq m, 1 \\leq j \\leq n}$, such that $|S{W^P}| \\leq |S{W}|$.\nOverall, Shears introduces the concept of Mode A elastic adapters, which allow the rank values to be flexible, thereby enabling the exploration of more potential sub-adapters. This approach has been demonstrated to outperform traditional LORA (with fixed rank values) and alleviates the challenge of setting the hyperparameter rank value when using LORA. Most importantly, Shears found that the NLS algorithm is particularly well-suited for sparse models. When pre-trained weights are sparsified, there is a natural and significant drop in accuracy compared to dense models. On this basis, using NLS for fine-tuning can maximally recover or adapt the model's performance to a specific downstream task.\nSQFT Shears is extended by SQFT (Mu\u00f1oz, Yuan, and Jain 2024) to manipulate sparse models on low numerical precision. SQFT is inspired by QLoRA (Dettmers et al. 2023), which was proposed to improve fine-tuning efficiency when using low-rank adapters. SQFT enables three different pipelines that account for the varying characteristics of the base models, such as whether they possess sparsity or have been quantized to low numerical precision.\nEmpirical results demonstrate that by combining elastic LORA adapters into the sparse or quantized base model, Shears, and SQFT enable effective fine-tuning of compressed models to adapt to specific downstream tasks. This approach produces compressed models that either improve or exhibit only minor drops in accuracy. The enhanced sparsity and precision can lead to significant speedups when utilizing runtimes optimized for these patterns.\nHowever, a significant challenge in Shears and SQFT when dealing with compressed models and dense adapters is the potential limitations encountered when attempting to merge the low-rank adapters with the based model after finetuning. For instance, if the model is sparse but the adapters are dense, the sparsity in the model will be lost when merging. A similar limitation arises when the based model has a different numerical precision than the low-rank adapters used for fine-tuning. In the next section, we describe how these limitations are addressed to ensure the integrity and performance of the fine-tuned models."}, {"title": "Addressing the Challenges of Merging Adapters with Low-precision Sparse Models", "content": "Within SQFT, two strategies, SparsePEFT and QA-SparsePEFT, are proposed to address the limitations described in the previous section. These limitations arise when attempting to merge the low-rank adapters with base models with differing sparsity patterns or numerical precision. The following sections provide a detailed discussion of these strategies.\nSparsePEFT This strategy ensures that the sparsity in low-rank adapters is aligned with their corresponding base model's weights during fine-tuning. SQFT achieves this by generating a binary mask M for each weight matrix W in the base model. The mask M_is $\u2200i\u2200j(W_{i,j} \\neq 0 \u21d2 M_{i,j} = 1)$. Utilizing M, SQFT sparsifies the adapters' matrix $(L_1L_2)$ to obtain $L^P$, i.e., $L^P = (L_1L_2) \\odot M$. This approach ensures sparsity awareness during fine-tuning, allowing the merging of the base model's weights and adapter's weights without losing the sparsity induced before finetuning.\nQA-SparsePEFT This strategy is employed by SQFT when the model has been quantized to lower numerical precision, and low-rank adapters with higher numerical precision are applied for fine-tuning. Quantizationaware SparsePEFT (QA-SparsePEFT) leverages the frozen zeros z and scales s resulting from the pre-fine-tuning stage in which each weight matrix, W, was asymmetrically quantized. By utilizing z, s, the numerical target range $[0, 2^{n-1}]$ for quantization (where n represents the target bit-width), and the pre-quantized sparse weights, WP, QA-SparsePEFT achieves quantization-aware fine-tuning with low-rank adapters on $W^m$, i.e., the sparse quantized (merged) weights. This process is formalized as,\n$\\qquad \\overline{W^m} = clamp(round(\\frac{(W^P + L^P)}{s} + z), 0, 2^n - 1),$        (3)\nTo obtain the dequantized weights,$\\overline{W^m}$, the inverse process is followed,\n$\\qquad \\overline{W^m} = s (\\overline{W^m} - z).$        (4)\nIn summary, the integration of Elastic LoRA Adapters with NAS techniques offers a promising approach to model compression and fine-tuning. By leveraging the flexibility of elastic adapters and the efficiency of NAS, methods like LoNAS, Shears, and SQFT demonstrate significant improvements in parameter reduction and inference speedup without sacrificing accuracy. Additionally, strategies such as SparsePEFT and QA-SparsePEFT address the challenges of merging (elastic or static) LoRA adapters with low-precision sparse models, ensuring robust performance and maintaining model integrity. These advancements highlight the potential of combining low-rank adapters with NAS to optimize large language models."}, {"title": "Performance Summary and Additional Considerations", "content": "We summarize the performance, accuracy, and compression efficiency of LoNAS (Table 1), SQFT (Table 2) and Shears (Tables 2, and 3) from their respective papers. The reader can find additional details and an exhaustive list of experiments in each solutions' source. From these tables, we can observe that LoNAS can obtain competitive results compared to vanilla LoRA. However, LoNAS application is costlier than the other two discussed solutions due to the elasticity enabled in the model's weights, in addition to the inserted elastic adapters, which makes the fine-tuning stage more expensive. Shears and SQFT, on the other hand, are more fine-tuning efficient since their manipulation is only at the adapters' level.\nThe larger research community can further improve the solutions discussed here. For instance, the additional stage to discover a high-performing adapter configuration from the search space of possible configurations presents several opportunities for improvement. As illustrated in Figure 4, evolutionary algorithms, e.g., the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) (Deb et al. 2002), might be used to discover Pareto-optimal elastic low-rank adapter configurations. In this example, a multi-objective search is performed on multiply-accumulate (MAC) operations and validation accuracy. This search can be expensive, presenting opportunities for more efficient alternatives."}, {"title": "Conclusion", "content": "This retrospective paper discusses recent work on low-rank representations and the synergy with neural architecture search (NAS). The results from the papers that propose solutions aligned with this synergy confirm the benefits in both directions: (i) Low-rank adapters are enhanced by NAS techniques, i.e., elastic low-rank adapters achieve better results than their vanilla low-rank adapter counterparts, and (ii) NAS becomes more efficient by incorporating the utilization of low-rank representations. This synergy also motivates future work to better understand the interaction between these two domains and propose more sophisticated solutions that expand on existing work."}]}