{"title": "The Impact of Inference Acceleration Strategies on Bias of LLMs", "authors": ["Elisabeth Kirsten", "Ivan Habernal", "Vedant Nanda", "Muhammad Bilal Zafar"], "abstract": "Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to deeply benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.\nThis paper contains prompts and outputs which may be deemed offensive.", "sections": [{"title": "Introduction", "content": "Modern-day LLMs like LLaMA and GPT-4 show remarkable language generation capabilities, leading to a surge in their popularity and adoption (Bubeck et al., 2023; Wei et al., 2022; Ziems et al., 2024). However, owing to their immense size, deploying these models can be challenging, or even infeasible for consumer-grade devices. A flurry of research has proposed acceleration strategies such as quantization and pruning to enable efficient inference (Park et al., 2024; Zhu et al., 2023). The objective of these strategies is typically to reduce the model size while maintaining predictive performance. Over time, these strategies have become increasingly prevalent with integration into popular libraries like HuggingFace (Hug, 2024) and libraries such as vLLM (Kwon et al., 2023).\nWhile these inference acceleration strategies aim to preserve predictive performance, they may inadvertently lead to some side-effects (Gon\u00e7alves and Strubell, 2023; Jaiswal et al., 2024). For example, compression techniques might significantly reduce model trustworthiness (Hong et al., 2024). On the other hand, smaller models have been found to mitigate privacy risks and reduce egocentric tendencies in the generated text (Hong et al., 2024; Perez et al., 2022; Sun et al., 2024).\nThis paper explores how the demographic bias in the model output changes after the implementation of inference acceleration strategies. Specifically, we aim to answer the following research questions:"}, {"title": "Related Work", "content": "Most evaluations of inference acceleration strategies focus on application-agnostic metrics like perplexity or predictive performance-driven tasks like MMLU (Dettmers et al., 2022; Hooper et al., 2024; Lin et al., 2024; Sun et al., 2024). However, recent work has shown that model compression can result in degradation of model performance in areas beyond predictive performance (Gon\u00e7alves and Strubell, 2023; Jaiswal et al., 2024).\nThe effect of model size on trust criteria. Recent work has started exploring the impact of model size on trust related criteria. For example, Perez et al. (2022) find that larger models tend to overly agree with user views. Sun et al. (2024) show that smaller models can reduce privacy risks. Huang et al. (2024) find that smaller models are more vulnerable to backdoor attacks. Mo et al. (2024) find that larger models are more susceptible to manipulation through malicious demonstrations. Jaiswal et al. (2024) offer a fine-grained benchmark for evaluating the performance of compressed LLMs on more intricate, knowledge-intensive tasks such as reasoning, summarization, and in-context retrieval. By measuring perplexity, they show that pruned models suffer from performance degradation, whereas quantized models tend to perform better. Xu and Hu (2022) find that knowledge"}, {"title": "Measuring Bias in LLM Outputs", "content": "ML bias can stem from different causes (Suresh and Guttag, 2021), can manifest in various manners (Blodgett et al., 2020; Mehrabi et al., 2022), and can cause different types of harms (Gallegos et al., 2024). While a detailed examination can be found in Gallegos et al. (2024), bias in LLMs is often categorized into the following meta-groups:\n1. Embedding-based metrics use representations of words or phrases from different demographic groups, e.g., WEAT (Caliskan et al., 2017) and SEAT (May et al., 2019).\n2. Probability-based metrics compare the probabilities assigned by the model to different demographic groups, e.g., CrowSPairs (Nangia et al., 2020).\n3. Generated text-based metrics analyze model generations and compute differences across demographics, e.g., by evaluating model responses to standardized questionnaires (Durmus et al., 2024), or using classifiers to analyze the characteristics of generations such as toxicity (Dhamala et al., 2021; Hartvigsen et al., 2022; Smith et al., 2022).\nWe leave out embedding-based metrics from our analysis since (i) the more typical use-case of modern, instruction-tuned, LLMs like LLaMA and GPT-4 is prompt-tuning or fine-tuning rather than adapting the models using embeddings and (ii) embedding bias is not guaranteed to lead to bias in the text generations. We initially considered classification-based bias metrics (e.g., Dhamala et al.), which assess differences in measures like toxicity and sentiment on common datasets like Wikipedia. Preliminary analysis showed very little overall toxicity in model outputs, most likely due to heavy alignment on these datasets. For this reason, we did not further consider these metrics.\nWith these considerations in mind, the final set of metrics we consider is as follows. We add further information, e.g., the number of inputs and license types, in Appendix A.\nCrowSPairs (Nangia et al., 2020) is a dataset of crowd-sourced sentence pairs designed to evaluate stereotypes related to race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. Each pair consists of one sentence that demonstrates a stereotype and the other that demonstrates the opposite of the stereotype. Given a pair $(S_{more}, S_{less})$ where $S_{more}$ is presumed to be more stereotypical, the metric measures $\\mathbb{I}[p(S_{more}) > p(S_{less})]$ and averages this quantity over all pairs, with $\\mathbb{I}$ as the indicator function. The score ranges from [0, 1].\nGlobalOpinionQA (Durmus et al., 2024) uses multiple-choice questions to assess the opinions stated by a model relative to aggregated population opinions from different countries. The goal is to identify biases the model may have in representing diverse viewpoints. We follow the measurement procedure of Durmus et al. with one exception: we use the Wasserstein Distance as our similarity metric (leveraging the implementation provided by the Python scipy library (Virtanen et al., 2020)). Durmus et al. use 1-Jensen-Shannon Distance as a similarity metric, which can become"}, {"title": "Experimental Setup", "content": "Models and Infrastructure. We analyze three different models: LLaMA-2 (Touvron et al., 2023), LLaMA-3.1 (Dubey et al., 2024), and Mistral-0.3 (Jiang et al., 2023). We consider the smallest size variant of each model: LLaMA-2-7B, LLaMA-3.1-8B, and Mistral-7B-v0.3 (license information in Section A). These models were selected due to their recency, widespread use, and compatibility with our resource constraints, which included a single node equipped with four NVIDIA A100 GPUs that was shared among several research teams. Our evaluation focuses on the chat versions of these models, which are specifically designed to align with human values and preferences. We used the GitHub Copilot IDE plugin to assist with coding.\nInference acceleration strategies. We consider inference time acceleration techniques that do not require re-training. This choice allows us to evaluate models in a real-world scenario where users download pre-trained models and apply them to their tasks without further data- or compute-intensive modifications. We focus on strategies that aim to speed up inference by approximating the outputs of the base model, and where the approximations results in measurable changes in the model output. This criterion excludes strategies like speculative decoding (Leviathan et al., 2023) where the output of the base and inference accelerated models are often the same. Specifically, we consider the following strategies:\nQuantization. We consider the following variants:\n1. INT4 or INT8 quantization using Bitsandbytes library (Bit, 2024) which first normalizes the model weights to store common values efficiently. Then, it quantizes the weights to 4 or 8 bits for storage. Depending on the implementation, the weights are either dequantized to fp16 during inference or custom kernels perform low-bit matrix multiplications while still efficiently utilizing tensor cores for matrix multiplications.\n2. Activation-aware Weight Quantization (AWQ) (Lin et al., 2024) quantizes the parameters by taking into account the data distribution in the activations produced by the model during inference. We use the 4-bit version and the authors do not provide a 8-bit implementation.\n3. Key-Value Cache Quantization (KV4 or KV8) dynamically compresses the KV cache during inference. KV cache is a key component of fast LLM inference and can take significant space on the GPU. Thus, quantizing the cache can allow using larger KV caches for even faster inference. We use both 4 and 8-bit quantization (Liu et al., 2023). We use the native HuggingFace implementation. This implementation does not support Mistral models.\nPruning removes a subset of model weights to reduce the high computational cost of LLMs while aiming to preserve performance. Traditional pruning methods require retraining (Cheng et al., 2024). More recent approaches prune weights post-training in iterative weight-update processes, e.g., SparseGPT (Frantar and Alistarh, 2023). We use the Wanda method by Sun et al. (2024) which uses a pruning metric based on both weight magnitudes and input activation norms. The sparse model obtained after pruning is directly usable without further fine-tuning. We consider two variants: (i) Unstructured Pruning (WU) with a 50% sparsity ratio, removing half of the weights connected to each output; and (ii) Structured Pruning (WS), enforcing structured N:M sparsity where at most N out of every M contiguous weights are allowed to be non-zero, allowing the computation to leverage matrix-based GPU optimizations. We use a 2 : 4 compression rate. Prior work has shown that pruned models can maintain comparable performance even at high compression rates (Frantar and Alistarh, 2023; Jaiswal et al., 2024; Sun et al., 2024), including the 2: 4 rate used here.\nParameters. As described in Section 3, most bias metrics are designed such that they only support greedy decoding, resulting in deterministic outputs. Only DT-Stereotyping and"}, {"title": "Results", "content": "Table 2 shows the bias of base models w.r.t. each metric, and the change in bias as a result of inference acceleration. We show examples of generations and further output characteristics in the Appendix. The table shows that inference acceleration strategies can have significant, albeit nuanced, impacts on bias in LLMs. While some strategies consistently reduce certain biases, others"}, {"title": "Conclusion & Future Work", "content": "In this study, we investigated the impact of inference acceleration strategies on bias in Large Language Models (LLMs). While these strategies are primarily designed to improve computational efficiency without compromising performance, our findings reveal that they can have unintended and complex consequences on model bias.\nKV cache quantization proved stable with minimal impact on bias scores across datasets, whereas AWQ quantization negatively affected bias. Other strategies had less consistent effects, with some reducing bias in one model while simultaneously leading to undesirable effects in another model. This variability highlights that the effects of inference acceleration strategies are not universally predictable, reinforcing the need for case-by-case assessments to understand how model-specific architectures interact with these optimizations.\nThe impact of these strategies extends beyond bias-structured Wanda pruning, for instance, appeared effective in reducing bias but led to concerns about nonsensical and incoherent texts. Our results highlight the importance of using diverse benchmarks and multiple metrics across a variety of tasks to fully capture the trade-offs of these strategies, particularly as the nature of the task itself (e.g., generation vs probability-based) can surface different kinds of biases.\nLooking ahead, it is important to consider already during model training that users may later apply inference acceleration strategies. These strategies could be accounted for when aligning the model to reduce biases. Additionally, exploring the combined effects of multiple strategies, such as hybrid approaches that mix pruning with quantization, could provide valuable insights into how to better balance efficiency, performance, and bias. Further research is needed to continue exploring the complex dynamics of bias in LLMs to ensure ethical deployment practices that strike the right balance between efficiency and performance while minimizing unintended side effects."}, {"title": "Limitations", "content": "Our study has several limitations that should be taken into account when interpreting the results. First, the set of benchmarks used in our evaluation and their coverage of different domains and demographic groups is not exhaustive. Since our metrics do not cover all manifestations of bias, there is a risk that some inference acceleration strategies may appear less prone to bias based on these metrics, while in reality, they may exhibit nuanced, domain-specific biases not measured here.\nAdditionally, we focused only on training-free acceleration strategies. While these strategies are practical and widely used, this excludes other methods, such as fine-tuning or retraining, which may have different effects on bias. Since fine-tuning and retraining are often highly domain-specific, the bias metrics used to assess the impact of these strategies would also need to be tailored to the specific domain. Furthermore, using fixed hyperparameters (e.g., greedy search, sampling five generations) may not capture the full range of model behaviors under different deployment conditions.\nThere are also potential risks associated with our findings. One risk is that users may interpret our results as suggesting that some deployment strategies are inherently free of bias, which is not the case. Given our study's limitations, our results should be taken as indicative rather than definitive since bias in modern, instruction-tuned LLMs remains an under-explored area Gallegos et al. (2024).\nFinally, the broader ethical implications of deploying LLMs with minimal bias remain a critical area of concern. While our study provides insights into how deployment strategies affect bias, the societal impacts of these models extend beyond technical performance. Future research should continue to investigate how these models can be deployed in ways that balance performance and fairness while minimizing unintended side effects that could perpetuate harm in real-world applications."}]}