{"title": "A DISEASE-SPECIFIC FOUNDATION MODEL USING OVER 100K\nFUNDUS IMAGES: RELEASE AND VALIDATION FOR\nABNORMALITY AND MULTI-DISEASE CLASSIFICATION ON\nDOWNSTREAM TASKS", "authors": ["Boa Jang", "Youngbin Ahn", "Eun Kyung Choe", "Chang Ki Yoon", "Hyuk Jin Choi", "Young-Gon Kim"], "abstract": "Artificial intelligence applied to retinal images offers significant potential for recognizing signs and\nsymptoms of retinal conditions and expediting the diagnosis of eye diseases and systemic disorders.\nHowever, developing generalized artificial intelligence models for medical data often requires a large\nnumber of labeled images representing various disease signs, and most models are typically task-\nspecific, focusing on major retinal diseases. In this study, we developed a Fundus-Specific Pretrained\nModel (Image+Fundus), a supervised artificial intelligence model trained to detect abnormalities\nin fundus images. A total of 57,803 images were used to develop this pretrained model, which\nachieved superior performance across various downstream tasks, indicating that our proposed model\noutperforms other general methods. Our Image+Fundus model offers a generalized approach to\nimprove model performance while reducing the number of labeled datasets required. Additionally,\nit provides more disease-specific insights into fundus images, with visualizations generated by our\nmodel. These disease-specific foundation models are invaluable in enhancing the performance and\nefficiency of deep learning models in the field of fundus imaging.", "sections": [{"title": "1 Introduction", "content": "Vision plays a crucial role in determining quality of life, and its significance becomes increasingly pronounced with\nadvancing age. Prevalent eye diseases like age-related macular degeneration (AMD), glaucoma, diabetic retinopathy\n(DR), retinal vein occlusion (RVO), pathologic myopia (PM), and epiretinal membrane (ERM) significantly affect many\npeople and linked to the blindness [1]. Early diagnosis and prevention of these diseases are critical, particularly in\nlight of rising prevalence rates for AMD, glaucoma, and DR, which are major eye diseases that cause blindness [2, 3].\nHowever, a recent study forecasted a sizeable shortage of ophthalmology workforce supply relative to demand by the\nyear 2035 [4].Therefore, it is required to develop efficient screening and diagnosis system which can be easily accessed\nand used by not only ophthalmologists but also general physicians and even public.\nAdvancements in fundus imaging technologies have led to the development of artificial intelligence (AI) screen-\ning systems, which are user-friendly, resource efficiency, and suitability for implementation in primary healthcare\nsettings[5].These systems hold the potential to facilitate early detection of fundus abnormalities, providing critical\ntreatment advice or referrals. Their deployment, especially in primary care areas, is anticipated to become a prevailing"}, {"title": "2 Materials and Methods", "content": "Transfer learning involves two main components: the upstream task and the downstream task. The upstream task\nfocuses on training a model using a large dataset to generate pre-trained weights. These pre-trained weights are then\nutilized in the downstream task, which employs a smaller dataset to assess the efficiency and performance of the\npre-trained weights.\nFor the upstream task in our study, fundus images were acquired retrospectively from Seoul National University Hospital\nHealthcare System Gangnam Center in South Korea. A total of 113,645 fundus images of 57,803 patients collected"}, {"title": "2.1 Dataset of Fundus Images", "content": "Transfer learning involves two main components: the upstream task and the downstream task. The upstream task\nfocuses on training a model using a large dataset to generate pre-trained weights. These pre-trained weights are then\nutilized in the downstream task, which employs a smaller dataset to assess the efficiency and performance of the\npre-trained weights.\nFor the upstream task in our study, fundus images were acquired retrospectively from Seoul National University Hospital\nHealthcare System Gangnam Center in South Korea. A total of 113,645 fundus images of 57,803 patients collected\nbetween 2003 and 2010 were used. These fundus images were labeled with normal and abnormal, and the ratio of\nnormal and abnormal data was 8:2.\nFor the downstream tasks, fundus images with confirmed abnormalities were collected independently from the same\nhospital where the upstream data was collected. The dataset consisted of 18,459 images from 9,419 patients for\nthe downstream tasks. Among them, 2,559 fundus images were labeled as abnormal and further classified into\nseven distinct classes representing different eye diseases for multi-label classification. These classes included AMD,\nglaucoma, glaucoma suspect, DR feature, PM, ERM, and RVO. Abnormal images which did not belong to above\nseven classifications were designated as \u201cother\u201d. This dataset exhibited significant class imbalance, with most samples\nbelonging to the major diseases. Furthermore, certain images were excluded from the analysis due to issues such as blur\nand defocus.\nThree public datasets were used for external validation and to generalize the tasks: RFMiD[12], JSIEC [13], and FIVES\n[14] dataset. From the RFMiD dataset, a total of 1,920 fundus images were used for validation, labeled with conditions\nsuch as normal, AMD, glaucoma, DR, and PM. The JSIEC datasets comprised 1,000 fundus images across 39 classes\nand a subset of 484 cases was used for validation, labeled with conditions including normal, glaucoma, DR, and PM.\nThe FIVES dataset, used for the vessel segmentation task, consisted of 800 fundus images each accompanied by a\nvessel segmented mask."}, {"title": "2.2 Training Visual Representation of Fundus as an Upstream Method", "content": "Two types of disease-specific foundation models were developed to enhance the final performance of predictive tasks.\nOne of the models was used a large fundus image dataset with derived labels (abnormal and normal) to establish a\ndisease-specific foundation model, referred to as 'Fundus'. The other one includes two-steps of training, which starts\nwith training on a large nonmedical dataset (ImageNet-1k) and was subsequently retrained using the large fundus image\ndataset, referred as \u2018ImageNet + Fundus'. This two-step pre-training approach is designed to capture both general and\nmedical-specific visual representations. The training employed supervised learning methods to classify the normal and\nabnormal of fundus image. To optimize the model for fundus image analysis, three different image resolutions were\ntested: 256, 512, and 1024 pixels.\nThe models were trained using 4 NVIDIA A100 GPUs with a batch size of 32. All implementations were carried out\nin PyTorch, with ImageNet pre-trained models sourced from the TorchVision library for easy usage. In this study, a\n50-layer residual network (ResNet)[15], one of most commonly used networks in deep learning was used. The Adam\noptimizer was configured with a learning rate of 1e-5, momentum of 0.9, and weight decay of le-5. Data augmentation\ntechniques included horizontal flip, grayscale, blur, and contrast limited adaptive histogram equalization (CLAHE).\nWeighted cross entropy loss was used as the loss function to specifically target fundus image abnormalities. The training\nprocess spanned 100 epochs with early stopping to prevent overfitting. The total training duration was approximately\nfive days."}, {"title": "2.3 Evaluation via Various Conditions of Downstream Task", "content": "To evaluate our disease-specific foundation model, three different downstream tasks were utilized: abnormality\nclassification (binary classification of normal and abnormal), multi-disease classification (classification of various\ndiseases from fundus images), and vessel segmentation (segmentation of vessels from fundus images). The quality of\nlearned representations within the fundus-specific transfer learning model was assessed through two methods: linear\nprobing (LP), achieved by training a linear classifier on frozen backbone weights, and full fine-tuning (FT), achieved by\nfine-tuning the entire model. Both LP and FT evaluations involved applying a nonlinear classifier to frozen embeddings\nand fine-tuning the entire model, respectively.\nThe abnormality classification task was evaluated under three scenarios: (a) the effectiveness of different pre-trained\nmodels, (b) the impact of image resolutions, and (c) a stress test under data-limited conditions. In scenario (a), the\nbinary classification task involved comparing four models: one with randomly initialized weights (referred to as\n'Scratch'), one pre-trained with ImageNet weights (referred to as 'ImageNet'), and two pre-trained with fundus-specific\nweights, namely \u2018Fundus' and 'ImageNet + Fundus'. In scenario (b), the influence of image resolution on learning\nrepresentations was examined by evaluating the models at three resolutions used in the upstream task \u2013 256 pixels (low\nresolution), 512 pixels (medium resolution), and 1024 pixels (high resolution). Scenario (c) involved stress tests, where\nmodels were trained with varying data fractions (1%, 10%, 50%, and additional conditions as detailed in Figure 2C),\ntesting the robustness of the disease-specific foundation models under data-limited conditions."}, {"title": "2.4 Measurement and Visualization of the Embeddings", "content": "To assess model performance, five-fold cross validation was used, with the mean and standard deviation values of each\nmetrics calculated for the performance comparisons. The area under the receiver operating characteristic curve (AUC)\nwas the primary metric for evaluating performance across various downstream tasks, calculated from the results of\nthe five-fold cross-validation. Differences in AUC scores between models were tested using the area test proposed by\nDeLong et al.[16], which assesses whether two classifiers have statistically different AUC scores, and a p-value of less\nthan 0.05 was considered statistically significant.\nFor visualizing how embeddings differ between general models and fundus-specific pre-trained models, t-distributed\nstochastic neighbor embedding (t-SNE)[17] was used. This method facilitates the visualization of high-dimensional data\nby projecting it into a lower-dimensional space. Addressing the opacity of how deep neural networks make predictions\na challenge often referred to as the \u201cblack-box\u201d problem \u2013 gradient-weighted class activation mapping (Grad-CAM)\nwas used[18]. Grad-CAM utilizes the gradients of any target concept flowing into the final convolutional layer to create\na localization map that highlights the region most influential for predicting the target. In this study, both Grad-CAM\nand t-SNE was provided simultaneously to enhance the interpretability of the predicted results for each fundus image,\nthus aiding clinicians in applying deep learning to clinical practice."}, {"title": "2.5 Code Availability", "content": "The source code and trained models are available from https://github.com/Research-Foundation-Retina.The code for\nimplementing the two proposed disease-specific foundation models are also available for download and use. These\nresources aim to facilitate replication of our results and further research in the field."}, {"title": "3 Results", "content": "To address the genuine data distribution and varied conditions in real clinical environments, we considered various data\nand experimental settings. The Jaccard index was employed to determine thresholds."}, {"title": "3.1 Abnormality Classification Task", "content": "To address the genuine data distribution and varied conditions in real clinical environments, we considered various data\nand experimental settings. The Jaccard index was employed to determine thresholds."}, {"title": "3.1.1 Comparison of Pre-training Weights", "content": "Table 1 summarizes the mean AUC scores obtained through different model configurations and image resolutions under\nboth LP and FT methods. The models trained from Scratch generally showed lower AUC scores, with a peak at 1024\npixels resolution (0.740 \u00b1 0.015). Models pre-trained with ImageNet weights under LP method demonstrated a decline\nin performance as resolution increased, with AUC scores starting at 0.772 \u00b1 0.002 at 256 pixels and decreasing to 0.734\n\u00b1 0.006 at 1024 pixels. Conversely, models pre-trained with Fundus weights under LP showed significant improvement,\nwith AUC scores increasing dramatically from 0.777 \u00b1 0.002 at 256 pixels to 0.913 \u00b1 0.002 at 1024 pixels.\nThe combination of ImageNet + Fundus pre-training consistently yielded the highest AUC scores across all tested\nresolutions in the LP methos, maintaining an impressive score of 0.945 at both 512 and 1024 pixels. Similarly, under\nFT, the ImageNet + Fundus models demonstrated robust performance, although the highest AUC was observed at 512\npixels (0.943 \u00b1 0.005) before slightly decreasing at 1024 pixels (0.938 \u00b1 0.003)."}, {"title": "3.1.2 Impact of Image Resolution on Model Performance", "content": "Figure 2A highlights the variation in mean AUC scores for different pre-trained models across increasing image\nresolutions within the LP method. The ImageNet pre-trained model demonstrates a decrease in performance as the\nresolution increases, starting with AUC of 0.772 at 256 pixels and decreasing to 0.734 at 1024 pixels. This trend\nsuggests that the ImageNet model, originally trained on lower resolution images (224 pixels), does not adopt well to the\nhigher resolution images typical of medical datasets. Conversely, the Fundus pre-trained model shows a significant\nimprovement with increasing resolution. The AUC starts at 0.777 for 256 pixels and increase to 0.913 for 1024 pixels.\nThis indicates that the Fundus-specific training exploits the detailed information available in higher resolution fundus\nimages more effectively, enhancing model performance substantially.\nThe ImageNet + Fundus pre-trained model consistently achieves the highest performance, maintaining an impressive\nAUC of around 0.945 for both 512 and 1024 pixels. This model benefits from a two-step pre-training approach that\nincorporates general visual features handling variations in image resolution.\nFigure 2B utilizes a t-SNE graph to compare the qualitative results of embeddings from ImageNet + Fundus pre-trained\nmodel within the LP method. This visualization provides a clear demonstration of how the model clusters and separated\ndata points based on learned feature across different image resolutions. The activation map generated using Grad-CAM\nfurther reveal that as image resolution decreases, smaller and more specific regions are identified as critical for making\nclinical decisions. These findings highlight the advanced capability of the ImageNet + Fundus model to not only\nperform well quantitatively but also to offer insightful visual explanations that are crucial for clinical interpretation."}, {"title": "3.1.3 Comparison of Stress Test", "content": "Figure 2C presents the outcomes of the stress tests using various data fractions, alongside results from using the full\ndataset. In the full dataset condition, the ImageNet + Fundus model demonstrated superior performance, achieving the\nhighest mean AUC across all tested conditions. This indicates that the two-step pre-training approach of combining\ngeneral and specialized pre-training is highly effective for maximizing model efficacy.\nParticular attention was given to the results from the 1% data fraction stress test, which is crucial for understanding\nmodel robustness when faced with extreme limitations in available training data. Under this stringent condition,\nthe models trained from Scratch, with ImageNet weights, and with Fundus-specific pre-training yielded mean AUC\nscores of 0.522 \u00b1 0.016, 0.545 \u00b1 0.010, and 0.887 \u00b1 0.003, respectively. The model combining ImageNet and Fundus\npre-training achieved an even higher score of 0.910 \u00b1 0.004. Highlighting the 1% data fraction is particularly relevant\nas it underscores the resilience of the pre-trained models, especially the ImageNet + Fundus model, in scenarios closest\nto the worst-case real-world conditions where very little annotated data is available."}, {"title": "3.2 Task Generalization", "content": "The external validation, which are crucial for assessing the generalizability and reliability of our models beyond the\ntraining environment, was conducted using two distinct datasets: JSIEC and RFMiD, and the results are summarized\nin Table 2. For the JSIEC dataset, the ImageNet + Fundus model under LP achieved the highest AUC of 0.943 \u00b1\n0.007, indicating a strong predictive performance. For the more challenging RFMiD dataset, the ImageNet + Fundus\nachieving AUCs 0.876 \u00b1 0.022. These results demonstrate the model's capability to perform effectively across different\nexternal datasets, though they also highlight the variability in performance that can arise due to differences in dataset\ncharacteristics or image quality."}, {"title": "3.2.1 External Validation for Abnormality Classification", "content": "The external validation, which are crucial for assessing the generalizability and reliability of our models beyond the\ntraining environment, was conducted using two distinct datasets: JSIEC and RFMiD, and the results are summarized\nin Table 2. For the JSIEC dataset, the ImageNet + Fundus model under LP achieved the highest AUC of 0.943 \u00b1\n0.007, indicating a strong predictive performance. For the more challenging RFMiD dataset, the ImageNet + Fundus\nachieving AUCs 0.876 \u00b1 0.022. These results demonstrate the model's capability to perform effectively across different\nexternal datasets, though they also highlight the variability in performance that can arise due to differences in dataset\ncharacteristics or image quality."}, {"title": "3.2.2 Extending Model Capabilities: Multi-Disease Classification", "content": "To enhance the versatility of our proposed model, we extended its capabilities to multi-disease classification. Utilizing\nthe disease-specific foundation model as a backbone, we adapted it for multi-label classification tasks by replacing the\noutput layer to accommodate multiple class classifications for various diseases at a resolution of 1024 image size. The\nloss function was adjusted to cross-entropy for each label. Fine-tuning the model on a multi-label annotated dataset\nallowed it to learn specific disease patterns within the fundus images, enhancing its ability to accurately identify and\ncategorize abnormal fundus images involving seven distinct disease types: AMD, Glaucoma, Glaucoma suspect, DR\nfeatures, PM, ERM, and RVO.\nGiven the imbalanced data distribution characteristic of these disease categories, we implemented different thresholds\nfor each model to optimize the final classification results. This approach ensures that the model's performance is robust\nacross various types of fundus diseases, even in the presence of significant class imbalances."}, {"title": "3.2.3 Extending Model Capabilities: Vessel Segmentation", "content": "To further demonstrate the adaptability of our\ndisease-specific foundation model, we extended\nits functionality to include vessel segmenta-\ntion. Using the same pre-trained model as its\nfoundation, we developed a dedicated vessel\nsegmentation model by incorporating a U-Net\nbased architecture and implementing the Dice\ncoefficient as the loss function. We employed\nthe FIVES dataset for both training and valida-\ntion, focusing on refining out model through FT\nmethods.\nThe performance analysis for vessel segmen-\ntation task is detailed in Figure 4, which high-\nlighted the robust and consistent efficacy of the\nImageNet-based model across various data vol-\numes. This model's success in capturing fea-\ntures relevant to vessel segmentation empha-\nsizes its applicability in tasks focused on the\nstructural delineation within fundus images, dis-\ntinguishing it as an effective tool for evaluating\nthe specialization of our models."}, {"title": "4 Discussion", "content": "In this study, we demonstrated the superior performance of the Fundus and ImageNet + Fundus models over Scratch\nand ImageNet pre-trained models across a various downstream task. This superior performance was evident in both\nabnormality binary classification and multi-disease classification tasks, addressing challenges related to data imbalances\nand deficiencies effectively.\nThe two-step pre-training method, particularly the ImageNet + Fundus model, consistently outperformed other models,\nunderscoring the benefits of disease-specific foundation models. The superior performance of this model in both the\nJSIEC and RFMiD datasets during external validation processes indicates its robustness and generalizability across\ndifferent data characteristics and qualities. The image-scale based analysis revealed that transfer learning is most\neffective when the data closely resembles the pre-trained model's original training set. However, our findings suggest\nthat models trained on ImageNet may not perform as effectively as those starting from scratch due to the mismatch\nbetween general and medical image characteristics.\nIn the multi-disease classification task, both of disease-specific foundation models are outperformed general pre-trained\nmodels, as it had already effectively learned all pertinent features. However, a different result emerged in the vessel\nsegmentation task. While the models pre-trained on Fundus data were highly effective at recognizing disease patterns,\nthey were less successful at structural tasks like vessel segmentation. This suggests that while these models are\nspecialized, their focus on disease-related features may limit their effectiveness in tasks requiring anatomical detail\nrecognition.\nA notable finding from our study is the underperformance of the commonly utilized ImageNet pre-trained model in\ncertain medical domains or tasks, particularly when confronted with insufficient data. This highlights the critical\nimportance of selecting an appropriate pre-trained model that is tailored to the specific domain and task at hand.\nFurthermore, recognizing that most medical data consist of high-quality information, it has been established that\nadopting an appropriate image size is crucial. Learning high-quality data using a pre-learning model trained on\nlow-quality images may not contribute to performance improvement. As shown in Figure 4, as the image size increases,\nthe model's heatmap can pinpoint more specific areas. While the model wasn't designed for disease classification\nper se, it was observed that the heatmap varied for each disease, suggesting the potential for classification based on\ndisease-specific diagnostic characteristics. Disease diagnosis necessitates an understanding of each condition's unique\nfeatures for accurate assessment. Our learned model demonstrated the ability to discern these distinctive characteristics\nfor each disease, suggesting its utility across various fundus studies.\nBased on the results of the stress tests, the disease-specific foundation models exhibit varying degrees of effectiveness\nin handling data shortages. The comparison across different data fractions highlights the importance of utilizing\npre-trained models tailored to the specific dataset. Notably, the ImageNet + Fundus model consistently outperformed\nothers across all conditions, even in scenarios with severely limited data (1% fraction). This suggests that leveraging\nboth general and domain-specific knowledge yields superior performance in overcoming data scarcity challenges.\nThe significant performance gap between the Scratch and pre-trained models underscores the advantage of transfer\nlearning in such contexts. Disease-specific foundation models, particularly those trained with general and Fundus\nimages, demonstrate a remarkable ability to extract relevant features despite limited training samples. These findings\nemphasize the practical benefits of employing pre-trained models in real-world research environments where data\navailability may be constrained. By leveraging existing knowledge and adapting it to the target domain, researchers\ncan effectively address data shortages and achieve robust performance in various tasks, including classification and\nprediction. Furthermore, the results underscore the need for careful consideration when selecting pre-trained models,\nas the choice can significantly impact performance outcomes. Future research could explore additional techniques to\nfurther optimize model performance under data scarcity conditions, potentially enhancing the applicability of deep\nlearning approaches in real-world scenarios.\nDespite its successes, the developed model faced with certain limitations. Primarily, its specialized focus on classifying\nfundus diseases has led to less-than-optimal performance in tasks involving the extraction of structures from fundus\nimages beyond the scope of learned normal and abnormal information. Furthermore, severe data imbalances in disease\nclassification have hindered performance, especially for diseases with limited data and unclear characteristics. Also, in\nsegmentation task, our disease-specific foundation models are not outperformed. Future improvements and fine-tuning\nare expected to enhance model performance. Our goal is not just to develop this model and conclude the process;\nrather, we aim to create a foundational model that can be comprehensively utilized across various ophthalmic research\nendeavors. Therefore, our future research direction involves developing models capable of recognizing, extracting, and\ngenerating various structures within retina, rather than solely diagnosing specific diseases. These models will serve as\nversatile tools for diverse applications in ophthalmic imaging and analysis."}]}