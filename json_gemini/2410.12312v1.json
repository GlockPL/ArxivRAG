{"title": "FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization", "authors": ["Cheng Yu", "Haoyu Xie", "Lei Shang", "Yang Liu", "Jun Dan", "Baigui Sun", "Liefeng Bo"], "abstract": "Recently, text-to-image diffusion models have gained widespread attention in the community because of their high-fidelity image generation capability. Specifically, in the field of human-centric personalized image generation, the adapter-based method obtains the ability to customize and generate portraits by text-to-image training on facial data. This allows for identity-preserved personalization without additional fine-tuning in inference. Although there are improvements in efficiency and fidelity, there is often a significant performance decrease in test following ability, controllability, and diversity of generated faces compared to the base model. In this paper, we analyze that the performance degradation is attributed to the failure to decouple identity features from other attributes during extraction, as well as the failure to decouple the portrait generation training from the overall generation task. To address these issues, we propose the Face Adapter with deCoupled Training (FACT) framework, focusing on both model architecture and training strategy. To decouple identity features from others, we leverage a transformer-based face-export encoder and harness fine-grained identity features. To decouple the portrait generation training, we propose Face Adapting Increment Regularization (FAIR), which effectively constrains the effect of face adapters on the facial region, preserving the generative ability of the base model. Additionally, we incorporate a face condition drop and shuffle mechanism, combined with curriculum learning, to enhance facial controllability and diversity. As a result, FACT solely learns identity preservation from training data, thereby minimizing the impact on the original text-to-image capabilities of the base model. Extensive experiments show that FACT has both controllability and fidelity in both text-to-image generation and inpainting solutions for portrait generation.", "sections": [{"title": "1. Introduction", "content": "As text-to-image technology, such as GLIDE, DALL-E, Imagen, and Stable Diffusion, continues to evolve, users have increasingly demanded higher capabilities from these advancements. How to integrate customized portraits into generated images has recently received widespread attention from the community. Personalized image generation technology, e.g. DreamBooth, Textual Inversion, and LORA has paved a way. Users can provide images that contain customized portraits, and the text-to-image model learns the characteristics of these portraits through fine-tuning, generating images around these portraits. However, personalized methods based on fine-tuning have inherent limitations that hinder their ability to satisfy the demands in certain scenarios. First, without domain expert level encoding of the customized faces, these methods will lose lots of detailed identity information during generation, resulting in reduced fidelity. Then, fine-tuning requires a significant amount of training costs, such as time and computing resources, which are difficult to meet user needs in many practical scenarios.\nTo address the aforementioned issues, a series of methods utilize the generalization properties of facial identity features and use customized faces as additional conditions for the Stable Diffusion model to generate personalized portraits instantly. Specifically, they treats facial embedding of input portrait images as a condition parallel with textual embeddings for T2I generation. They insert an adapter architecture into the middle of the Stable Diffusion's U-Net blocks to integrate facial embeddings, and train the adapter while freezing other modules. By text-to-image training on the large amount of portrait data, the diffusion model learns the identity feature of the faces, and then performs personalized generation using a single image with a single forward pass without additional fine-tuning.\nDespite the efficiency, there are two critical issues encountered in the process of integrating portrait generation functionality. a) Firstly, the vividness and fidelity of faces are not satisfactory, resulting in generated portraits that look like copies with reduced controllability and diversity. As shown on the right side of Fig. 1, under detailed text prompts, the quality of the generated portrait will significantly decrease and artifacts will appear. b) Secondly, compared to the basic diffusion model, there is a significant performance decline in the text-to-image ability. As shown on the left side of Fig. 1, there is a decrease in quality and complexity outside the facial region, such as monotonous backgrounds and distorted clothing. Besides, there are also issues in the generated images such as disrupted spatial continuity between faces and bodies, as well as the absence of glasses mentioned in the prompt.\nIn this paper, we rethink the identity-preserved personalization task and divide it into two processes: separating identity features from portrait images and integrating them into the generation process. From this view, the causes of the two issues mentioned above can be attributed to two aspects respectively: a) The failure to decouple identity features from others during the process of separating identity features from portrait images results in other coupled features involved hindering the generation of portraits. b) The failure to decouple the portrait generation training from the overall generation task leads to a decline in the ability of original text-to-image generation.\nTherefore, we propose Face Adapter with deCoupled Training (FACT) based on Stable Diffusion. It focuses on both model architecture and training strategy to achieve a clear decoupling of identity features and impose stronger constraints on portrait generation tasks. Specifically for decoupling the identity feature, we mask the region outside the face, effectively eliminating other distractions in the photograph. We harness fine-grained identity features from a transformer-based face recognition model. These features can better decouple information unrelated to identity and contain richer details of facial features. In addition, we insert a sequential Gated Self-Attention (GSA) module on U-Net to adapt independently to facial features, reducing the interference of textual embeddings compared to parallel cross attention. For decoupling the portrait generation training, we introduce the \"latent face adapting\" task to make the adapter focus on adapting the facial identity of the latent in denoising process. Specifically, we propose Face Adapting Increment Regularization (FAIR), which limits the effect of face adapters on the face region by restricting the increment through GSA modules in deep layers of the U-Net. Besides, to increase facial controllability and diversity, we design a curriculum learning with face condition drop and shuffle.\nWe conduct extensive experiments to verify the effectiveness of the proposed FACT. As shown in the Fig. 1, our proposed FACT preserves the text-to-image ability of the original model, which is manifested in diverse backgrounds and appropriate clothing. And FACT outperforms existing adapter-based personalization methods in terms of identity similarity, text following ability, and facial controllability and diversity. Besides, FACT can seamlessly integrate with common fine-tuning models of Stable Diffusion like LORA and ControlNet without compromising on generation performance."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Text-to-Image Diffusion Models", "content": "With the rapid progress of large-scale datasets like LAION-5B and powerful computation supports, large"}, {"title": "3. Method", "content": null}, {"title": "3.1. Overview", "content": "Given one single portrait image to be customized, the goal of identity-preserved personalization for a text-to-image diffusion model is to perform image generation that preserves the identity information of the input portrait image and retains other properties of the original model. Unlike methods such as IPAdapter and Photomaker that only consider how to integrate portraits into the generation process, our goal is to achieve better identity decoupling ability from portrait images and lossless generation ability of the diffusion model. In order for the goals above, we propose Face Adapter with deCoupled Training (FACT). From both aspects of model architecture and training strategy, we design an Identity Merging Module(IMM) to obtain better identity decoupling and merging capability, as well as a Face Adapting Increment Regularization(FAIR) to achieve face construction while preserving the original generation ability of the diffusion model. Fig. 2 shows the overall framework of FACT including model architecture, training strategy, and data preprocessing."}, {"title": "3.2. Identity Merging Module", "content": "Our Identity Merging Module (IMM) consists of a face encoder and a sequential face adapter. The former encodes the input portrait image to the identity feature, and then merges the identity feature into the Stable Diffusion.\nFace Encoding. We consider the following two aspects for an ideal identity feature for guiding portrait generation in diffusion. Firstly, the identity feature should align the feature space of the diffusion model for attention. Secondly, the identity feature should have the property of intra-indentity compactness and inter-identity separation as well as containing enough information in terms of image generation. Taking into account the above factors, we use the TransFace model for feature extraction, which is a ViT-based face recognition model to achieve better identity discrimination and compatibility with the Stable Diffusion architecture. Compared with general image encoders such as CLIP, the identity features extracted by the face expert encoder have stronger intra-identity compactness and inter-identity separation. To obtain richer facial information, we leverage the feature representations for all visual tokens from the penultimate layer of the TransFace. For alignment with Stable Diffusion architecture, we add a learnable lightweight projection module Prj (several Transformer blocks). Specifically, denote PTF as the token extraction from the penultimate ViT layer of TransFace, then the extraction of face embedding eid can be represented as:\n$e_{id} = Prj(PTF(x_{face})),$ (1)\nwhere $x_{face}$ represents the input portrait image.\nSequential Face Adapter with Gated Self-Attention. Inspired from the sequential sequential operation of face swapping methods which only changes the facial identity in the face region but keep other attributes constant, we use Sequential Face adapter with Gated Self-Attention (SFAGS) to merge the ID information contained in face embedding $e_{id}$. We insert a new gated cross-attention layer guided by the face embeddings between the self-attention layer and the cross-attention layer of the transformer block. We freeze the original two attention layers and train the gated cross attention layer for identity merging ability as is used in Flamingo and GLIGEN. The forward pass of SFAGS is formulated as:\n$x = x + \\alpha \\cdot GSA(x),$\n$GSA(x) = tanh(\\gamma) \\cdot TS(SelfAttn([x, e_{id}])),$ (2)\nwhere x represents visual tokens, TS(\u00b7) is the token selection operation that considers the visual tokens only, GSA is the gated self-attention. a denotes the scale of the face adapter, and \u03b3 is a learnable parameter initialized as 0 for stable training. As such, the face adapter only performs face adapting on the latent before cross-attention, rather than intervening in the text embedding within cross-attention. This approach minimizes the interference between face and text, thereby better preserving the guiding role of the text."}, {"title": "3.3. Identity Preserving Training", "content": "Our goal is to perform identity preservation training rather than tend to face reconstruction as some methods do. This signifies that during the portrait generation process, we need to increase the variability and controllability of the portrait while maintaining its authenticity as a prerequisite. Taking into account this, we use Face Adapting Increment Regularization (FAIR) to make the training process focus on the face region, thus improving the authenticity. In addition, we use a curriculum learning strategy with face condition drop and shuffle to expand the variability and controllability of portrait generation.\nFace Adapting Increment Regularization. We modify the training objective for the model optimization to constrain the impact of the proposed SFAGS specifically within the face region. A naive method is face-masked diffusion loss, but it is not strong enough to constrain the impact of SFAGS by only controlling the final denoising result without directly controlling the input and output of each adapter. Some adapter-based methods try to constrain the spatial distribution of attention deeply within the model, making it strictly focused on the facial region. However, we have observed that the attention map does not strictly correspond to the impact of the face adapter, and features on different intermediate layers focus on different regions of the face, which is demonstrated in Figure 3. Such a hard constraint"}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Experimental Setup", "content": "Implementation Details. We employ Stable Diffusion v1.5 architecture as the base diffusion model, and the resolution of training data is resized to 512 \u00d7 512 pixels correspondingly. The data preprocessing pipeline is similar to PhotoMaker for identity-oriented dataset. The identity feature projection module contains 4 Transformer blocks to get eid. Face adapters are inserted into each Transformer block of the diffusion model, in which the scale a is set to 1.0 and 0.5 respectively in training and inference. The CFG scale and condition drop probability for identity and text condition are set to 7.0 and 0.1 respectively. The probability of face condition shuffling increases from 0.2 to 0.6 during training. For the objective function, the probability of masked diffusion loss is set to 0.5, and the weight of FAIR A is set to 0.01. The overall framework is optimized using Adam with learning rate le-4 on 8 NVIDIA V100 GPUs for a week. The training batch size is 32.\nEvaluation Metrics. We conduct text-to-image generation as well as inpainting-based portrait generation to evaluate the effectiveness of adapter-based personalization methods. Our evaluation dataset includes 20 IDs collected by ourselves, which did not appear in the training set. For text-to-image generation, we use the same 40 prompts as PhotoMaker covering a variety of expressions, attributes, actions and backgrounds. We use CLIP-T metric to measure the text-to-image alignment. Considering the difference of base models among the methods, we also compute the drop of CLIP-T compared to the corresponding base model for each method for better evaluating the influence of the proposed adapters. Following the existing identity-preserved personalization approaches, we evaluate the CLIP-I score as well as the face similarity using embeddings extracted by FaceNet between the generated image and the reference image to mea-"}, {"title": "4.2. Personalization Results", "content": "Text-to-Image Generation. We compare our proposed FACT with the newest adapter-based identity-preserved personalization methods including IPAdapter-FaceID (simplified as IPAdapter), PhotoMaker, InstantID, and CapHuman. Table 1 shows the quantitative results. In terms of text-to-image alignment, FACT's CLIP-T score is second only to PhotoMaker. This is because they used SDXL as the base model, which has larger parameters than SD v1.5. When comparing with the corresponding base model, FACT has the lowest drop of CLIP-T, indicating that the text-to-image ability is maintained to the greatest extent by decoupling portrait from overall generation. In terms of identity preservation, FACT has the highest CLIP-I and the second highest face similarity. Despite leading the face similarity, InstantID has the largest CLIP-T drop for all comparing methods, and the facial texture is also unsatisfactory, making its CLIP-I not that high, indicating that it may overfit to information beyond identity from the reference face. In terms of style consistency and image quality, FACT leads in the CLIP Style and FID scores, validating the effectiveness of the proposed decoupled training in maintaining the overall text-to-image ability of the base model.\nWe also show the visual generation results for different reference images and text prompts in Figure 4. The image quality and text following ability of IPAdapter are satisfactory, but the face similarity of the generated images is not sufficient, which is also evident in the Face Sim. column of Table 1. PhotoMaker has similar issues with IPAdapter, and it also faces other problems, such as the centered position of the head (1st, 2nd, 3rd, and 5th row) and incorrect background (1st row) due to overfitting to the train-"}, {"title": "5. Conclusion", "content": "We propose the Face Adapter with deCoupled Training (FACT), an identity-preserved personalization text-to-image generation method. We leverage a transformer-based face-export encoder and harness fine-grained identity features to decouple identity features from others. Furthermore, we insert an adapter with a sequential Gated Self-Attention (GSA) to adapt independently to facial features, minimizing the interference from textual embeddings. To decouple the portrait generation training, we propose Face Adapting Increment Regularization (FAIR), which effectively constrains the effect of face adapters on the facial region, preserving the ability of the original model. With the help of the above decoupling strategy, FACT performs personalization solely by learning identity preservation from training data, thereby minimizing the performance impact on the original text-to-image capabilities of the base model. Extensive Experiments demonstrate that FACT exhibits both controllability and fidelity in both text-to-image generation and inpainting solutions for portrait generation."}, {"title": "Face Encoding.", "content": "We consider the following two aspects for an ideal identity feature for guiding portrait generation in diffusion. Firstly, the identity feature should align the feature space of the diffusion model for attention. Secondly, the identity feature should have the property of intra-indentity compactness and inter-identity separation as well as containing enough information in terms of image generation. Taking into account the above factors, we use the TransFace model for feature extraction, which is a ViT-based face recognition model to achieve better identity discrimination and compatibility with the Stable Diffusion architecture. Compared with general image encoders such as CLIP, the identity features extracted by the face expert encoder have stronger intra-identity compactness and inter-identity separation. To obtain richer facial information, we leverage the feature representations for all visual tokens from the penultimate layer of the TransFace. For alignment with Stable Diffusion architecture, we add a learnable lightweight projection module Prj (several Transformer blocks). Specifically, denote PTF as the token extraction from the penultimate ViT layer of TransFace, then the extraction of face embedding eid can be represented as:\n$e_{id} = Prj(PTF(x_{face})),$ (1)\nwhere $x_{face}$ represents the input portrait image."}, {"title": "Sequential Face Adapter with Gated Self-Attention.", "content": "Inspired from the sequential sequential operation of face swapping methods which only changes the facial identity in the face region but keep other attributes constant, we use Sequential Face adapter with Gated Self-Attention (SFAGS) to merge the ID information contained in face embedding $e_{id}$. We insert a new gated cross-attention layer guided by the face embeddings between the self-attention layer and the cross-attention layer of the transformer block. We freeze the original two attention layers and train the gated cross attention layer for identity merging ability as is used in Flamingo and GLIGEN. The forward pass of SFAGS is formulated as:\n$x = x + \\alpha \\cdot GSA(x),$\n$GSA(x) = tanh(\\gamma) \\cdot TS(SelfAttn([x, e_{id}])),$ (2)\nwhere x represents visual tokens, TS(\u00b7) is the token selection operation that considers the visual tokens only, GSA is the gated self-attention. a denotes the scale of the face adapter, and \u03b3 is a learnable parameter initialized as 0 for stable training. As such, the face adapter only performs face adapting on the latent before cross-attention, rather than intervening in the text embedding within cross-attention. This approach minimizes the interference between face and text, thereby better preserving the guiding role of the text."}, {"title": "Face Adapting Increment Regularization.", "content": "We modify the training objective for the model optimization to constrain the impact of the proposed SFAGS specifically within the face region. A naive method is face-masked diffusion loss, but it is not strong enough to constrain the impact of SFAGS by only controlling the final denoising result without directly controlling the input and output of each adapter. Some adapter-based methods try to constrain the spatial distribution of attention deeply within the model, making it strictly focused on the facial region. However, we have observed that the attention map does not strictly correspond to the impact of the face adapter, and features on different intermediate layers focus on different regions of the face, which is demonstrated in Figure 3. Such a hard constraint\nignores the attention difference in different parts within the face for different layers. Taking this in consideration, benefiting from the sequential structure of the face adapter, we propose a soft method to constrain the model. Essentially, the impact of the sequential face adapter is the increment of the visual token x in Eq. 2. Therefore, we propose a Face Adapting Increment Regularization (FAIR) to decrease the impact of sequential face adapter on the outside of the face region, formulated as:\n$\\mathcal{L}_{FAIR}(x) = \\frac{||GSA(x) \\cdot (1 - M_x)||}{||x \\cdot (1 - M_x)||,}$ (3)\nwhere $M_x$ is the mask of the corresponding face mask on the latent space with the same shape of x, || || is the $L_2$ norm. As such, FAIR retrains the increment by limiting the relative variation of latent caused by the face adapter in areas outside of the face and paying attention to different areas inside the face. Finally, combined with masked diffusion loss, the total training loss can be represented as:\n$\\mathcal{L}=\\mathbb{E}_{z, e_{text}, e_{id}, \\epsilon \\sim \\mathcal{N}(0, 1)} (\\lambda \\mathcal{L}_{FAIR}(x_{z_t, t, e_{text}, e_{id}})+\n||(\\epsilon_{\\theta}(z_t, t, e_{text}, e_{id}) - \\epsilon) \\cdot M_r(p)||),$ (4)\nwhere $M$ is the mask of the face, $z$ is the original input image latent, $e_{text}$ is the text embedding, $t$ is timestep, $ \\epsilon$ is the noise of the diffusion model, and $\\lambda$ is the weight of FAIR. $M_r(p)$ is the random face mask which is equal to M with a $p$ probability and is equal to 1 in other cases."}, {"title": "Face Condition Drop and Shuffle.", "content": "To increase the variability and the controllability of the portrait, we propose a curriculum learning with face condition drop and shuffle. Inspired by the Classifier Free Guidance (CFG), we train image generation under the guidance of \"paired faces\", \"unpaired identities\", and \"paired identities + unpaired faces\". Specifically, we conduct random face condition drop and shuffling from the same identity, corresponding to the \"unpaired identities\" and \"paired identities + un-\npaired faces\" respectively, before feeding to the diffusion model, as shown in Fig. 2. During the training, we adopt curriculum learning by gradually adding the shuffling probability for better convergence. As such, FACT can maintain the robust and flexible generation ability from the original Stable Diffusion and achieve facial construction through face adapter. During the inference, the denoising process is formulated as:\n$z_{t-1} = (1 + \\lambda_{CFG}) \\cdot \\mathcal{H}(z_t, t, e_{text}, e_{id}) -\n\\lambda_{CFG} \\cdot (\\mathcal{H}(z_t, t, e_{text-neg}, e_{id-drop})),$ (5)\nwhere $\\mathcal{H}$ is the forward pass for denoising U-Net, and $\\lambda_{CFG}$ is the CFG scale. We use the same CFG scale and the condition drop probability for both the identity and text conditions for simplification."}, {"title": "Effect of Face Adapting Increment Regularization.", "content": "We evaluate the effect of our proposed FAIR by comparing the text-to-image generation quality for FACT with and without (w/o) FAIR. Table 3 shows the comparison of metrics on identity preservation (CLIP-I, Face Sim.) and text-to-image\ngeneration ability (CLIP-T, CLIP Style), which is kept in Ablation Study. It can be observed that our proposed FAIR has a positive effect on all metrics. FAIR helps the face adapter to focus on the face region and perform identity preservation on it, while maintaining the attention on different areas of the face on different depths of Transformer blocks, which is demonstrated in Figure 3. As a result, the face adapter can concentrate solely on identity preservation, while the base model can focus entirely on text-to-image generation, without interfering with each other. This leads to an improvement in the metrics on both roles."}, {"title": "Effect of Face Condition Drop and Shuffle.", "content": "We perform text-to-image generation for models with different face condition drop and shuffle strategies, including model without face condition drop and shuffle (w/o DS), model with fixed drop and shuffle probability (w/o CL), and model with face condition drop and shuffle by curriculum learning (DS + CL). Table 4 shows the comparison result. Comparing the first two lines, we find that both CLIP-T and face similarity metrics benefit from face condition drop and shuffle despite a slight drop in CLIP Style. The increase of CLIP-T comes from the improvement in editability of generated faces by face condition shuffle, and the increase of face similarity metrics is due to the classifier-free guidance of face condition with the help of face condition drop. The comparison between the last two rows of data reveals that the model incorporating curriculum learning demonstrates superior identity preservation capabilities while maintaining the overall text-to-image generation quality, thereby validating the effectiveness of the proposed curriculum learning"}, {"title": "Influence of Inference Strategies.", "content": "We illustrate the curve of metrics on text-to-image generation by changing the adapter scale a and CFG scale in Figure 8. It can be observed that there is a trade-off between the identity preservation and the text following ability when changing a. In\nour experiments, we choose a = 0.5 to balance the metrics. When the CFG scale is relatively small (less than 4.0), higher CFG scale leads to higher overall performance. Then, the overall performance is stable when the CFG scale is between 4.0 and 7.0. When the CFG scale is too high, there may exist overflow in the generated faces, leading to the performance drop of the identity preservation."}]}