{"title": "MITIGATING FORGETTING IN LLM SUPERVISED FINE-TUNING AND PREFERENCE LEARNING", "authors": ["Heshan Fernando", "Han Shen", "Parikshit Ram", "Yi Zhou", "Horst Samulowitz", "Nathalie Baracaldo", "Tianyi Chen"], "abstract": "Post-training of pre-trained LLMs, which typically consists of the supervised fine-tuning (SFT) stage and the preference learning (RLHF or DPO) stage, is crucial to effective and safe LLM applications. The widely adopted approach in post-training popular open-source LLMs is to sequentially perform SFT and RLHF/DPO. However, sequential training is sub-optimal in terms of SFT and RLHF/DPO trade-off: the LLM gradually forgets about the first stage's training when undergoing the second stage's training. We theoretically prove the sub-optimality of sequential post-training. Furthermore, we propose a practical joint post-training framework with theoretical convergence guarantees and empirically outperforms sequential post-training framework, while having similar computational cost. Our code is available on github https://github.com/heshandevaka/XRIGHT.", "sections": [{"title": "INTRODUCTION", "content": "Recent years have witnessed the great capabilities of large language models (LLMs) trained on a large corpus of datasets (OpenAI, 2022; Dubey et al., 2024; Abdin et al., 2024). These models have been applied to a wide range of tasks including virtual assistant (OpenAI, 2022), code development (Roziere et al., 2023), and education/research (Achiam et al., 2023). Typically LLMs undergo the pre-training phase and the post-training phase. The post-training phase adapts the pre-trained LLM to specific tasks, thus is crucial to its successful applications.\nThe post-training phase of LLMs often has two stages (Abdin et al., 2024; Dubey et al., 2024): the Supervised Fine-Tuning (SFT) stage and the preference learning stage. Typical methods for preference learning include Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), and Direct Preference Optimization (DPO) (Rafailov et al., 2024). Given this two-stage process, a natural approach is to performing sequential training, e.g., first perform DPO then SFT or vice verse. For example, the instruct variant of popular open-source models like PHI-3 (Abdin et al., 2024) or LLAMA-3 (Dubey et al., 2024) sequentially undergo SFT and DPO training. Or in other scenarios like continual learning of an aligned model, the process can be interpreted as sequentially performing DPO/RLHF followed by SFT (Tang et al., 2020; Qi et al., 2023; Fang et al., 2024).\nHowever, sequential training of RLHF and SFT is sub-optimal in terms of the trade-off between preference learning and SFT. When the model is undergoing the second stage of training, it gradually and inevitably forgets about the first stage's training. In this case, we argue that even regularization like KL divergence used in RLHF/DPO cannot avoid forgetting due to the data distribution shift from the SFT dataset to the preference dataset. An illustration of the sub-optimality of sequential post-training is shown in Figure 1 (left), where we observe that sequential training leads to the increase of the DPO objective during SFT, resulting in a worse trade-off between the two objectives than the method to be introduced in this work. A similar issue has also been observed in, e.g., (Qi et al., 2023). To overcome this issue and achieve a better trade-off, a naive thought is to mix"}, {"title": "PRELIMINARIES", "content": "In this section, we formally introduce the notations and problem setup for RLHF and SFT.\nModel. We denote the LLM parameter to be optimized for either RLHF or SFT by \u03b8, and we use \u03c0\u03b8(y | x) to denote the LLM that generates an output y given an input x for an SFT or RLHF task.\nReinforcement learning with human feedback. We denote the dataset used for RLHF as DRLHF = {x(i), y(i)w, y(i)c}Ni1, where N1 is the number of data, x(i) are the inputs for the LLM, y(i)w and y(i)c are the chosen (preferred) and rejected (dispreferred) responses to x(i), respectively, for all i \u2208 {1, ..., N1}. We consider using DPO (Rafailov et al., 2024) to align \u03b8 with the preference dataset DDPO. The DPO objective is given by\nfDPO(\u03b8; DDPO, \u03c0ref, \u03b2) := \u2212Ex, yw, ye~DDPO log [\u03c3(\u03b2log (\\frac{\u03c0_\u03b8(y_w | x)}{\u03c0_{ref}(y_w | x)}) - \u03b2log (\\frac{\u03c0_\u03b8(y_l | x)}{\u03c0_{ref}(y_l | x)}))];\nwhere \u03c3 is the sigmoid function, \u03c0ref is a given reference model, and \u03b2 is a regularization constant that penalize the objective when \u03c0\u03b8(y | x) is diverging too much from \u03c0ref(y | x) on x ~ DDPO. In sequential training, when DPO is performed before SFT, we use the model trained on the chosen responses in DDPO as \u03c0ref. When SFT is performed before DPO, the model obtained after the SFT phase is used as the \u03c0ref. Given a data point (x, yw, yl), the gradient estimate of fDPO is given as\ngDPO(\u03b8; x, yw, yl, \u03c0ref, \u03b2) := \u2212 (1 \u2212 \u03c3(h\u03b2(\u03b8; x, yw, yl, \u03c0ref))) \u2207\u03b8h\u03b2(\u03b8;x, yw, yl, \u03c0ref),\nwhere\nh\u03b2(\u03b8; x, yw, yl, \u03c0ref) := \u03b2log (\\frac{\u03c0_\u03b8(y_\u03c9 | x)}{\u03c0_{ref}(y_\u03c9 | x)}) - \u03b2log (\\frac{\u03c0_\u03b8(y_l | x)}{\u03c0_{ref}(y_l | x)}).\nFor brevity, in the rest of the paper we will use fDPO(\u03b8) for fDPO(\u03b8; DDPO, \u03c0ref, \u03b2), gDPO(\u03b8; x, yw, ye) for gdpo(\u03b8; X, yw, ye, \u03c0ref, \u03b2), and h\u03b2(\u03b8; x, yw, yl) for h\u03b2(\u03b8; x, yw, yl, \u03c0ref). Note that Ex, yw,ye~DDPO [gDPO (\u03b8; X, yw, ye)] = \u2207 fDPO(\u03b8).\nSFT. We denote the dataset used for SFT as DSFT = {x(i), y(i)}N2i=1, where N2 is the number of data points. The SFT dataset consists of input x(i) and corresponding target outputs y(i) for all i \u2208 {1, ..., N2}. The objective used for fine-tuning \u03b8 for DSFT can be given as\nfSFT(\u03b8; DSFT) := -Ex,y~DsFr log(\u03c0\u03b8(y|x)).\nGiven a data point (x, y), the gradient estimate for the objective fSFT is given as\ngSFT(\u03b8; x, y) := \u2212\u2207\u03b8\u03c0\u03b8(y | x)/\u03c0\u03b8(y | x).\nFrom this point, we will use fSFT(\u03b8) for fSFT(\u03b8; DSFT). Note that Ex,y~DsFr [gSFT(\u03b8; x, y)] = \u2207fsFT(\u03b8).\nPerformance metric and trade-off. In this work we investigate different methods for their performance on both DPO and SFT tasks, simultaneously. Thus, to evaluate the performance of a model \u03b8 on fDPO and fSFT, we define the optimality gap of a mixture of objectives as\nGMix,\u03bb(\u03b8) := fMix,\u03bb(\u03b8) - fMix, \u03bb,\nwhere \u03bb \u2208 [0,1], fMix,\u03bb(\u03b8) := \u03bbfDPO(\u03b8) + (1 \u2212 \u03bb)fsFT(\u03b8), and fix \u03bb = mine\u2208\u0398 fMix,\u03bb(\u03b8). Here \u03bb defines a trade-off between the DPO and SFT objective: a larger \u03bb results in more emphasis on the DPO performance compared to SFT. We say a model parameter \u03b8 achieves optimal trade-off defined by \u03bb when GMix,\u03bb(\u03b8) = 0. We chose this metric because, as established in multi-objective optimization literature (Miettinen, 1999), the optimizer of GMix,\u03bb(\u03b8) for any \u03bb \u2208 [0, 1] will be \u2018Pareto optimal'. This means that no other solution can optimize both objectives simultaneously, and the solution can be viewed as one of the optimal trade-off points for the problem of optimizing fDPO and fSFT. Additionally, GMix,\u03bb(\u03b8) is differentiable when both fDPO and fSFT are differentiable, which facilitates the theoretical analysis of gradient-based methods."}, {"title": "WHY SEQUENTIAL DPO AND SFT IS SUBOPTIMAL?", "content": "This section studies the sequential DPO and SFT method commonly used in the continual training of aligned LLMs (see, e.g., Tang et al. (2020); Qi et al. (2023); Fang et al. (2024)). We give insights on why such a sequential training framework is suboptimal in terms of DPO and SFT trade-offs."}, {"title": "SEQUENTIAL TRAINING ALGORITHM AND ITS SUBOPTIMALITY", "content": "Following Rafailov et al. (2024), we first obtain a reference model \u03c0ref by performing SFT on the target outputs in the preference dataset DDPO. Given \u03c0ref, we iteratively perform the DPO update as\n\u03b8t+1 = \u03a0\u0398 (\u03b8t \u2013 \u03b11,tgDPO (\u03b8t; xt, yw, ye)), for t = 1, 2, ..., TDPO - 1\nwhere \u03b11,t is the step size, xt, yw, yl ~ DDPO, gDPO is defined in (2), and TDPO is the max iteration for DPO training. Given the aligned model parameter \u03b8TDPO, we next perform SFT updates as follows:\n\u03b82t+1 = \u03a0\u0398 (\u03b82t \u2013 \u03b12,tgSFT(\u03b82t; xt, y*)), for t = 1, 2, ..., TSFT - 1\nwhere \u03b8 is initialized as \u03b8TDPO, xt, yt ~ DSFT, gSFT is defined in (5), and TSFT is the max iteration for SFT training. The whole process is summarized in Algorithm 1. We next study why the sequential training framework is suboptimal.\nA toy illustration of suboptimality. At a given phase of Algorithm 1, the algorithm only focuses on optimizing one objective (either fDPO or fSFT) and ignores the other. This results in the model oscillating between the optimums of two objectives, without converging to a point that is 'reasonably optimal' for both objectives fDPO and fSFT. We first illustrate this on a toy example (see example details in Appendix D.1). The results are depicted in Figure 2. For the weight space trajectory (Figure 2 upper-left), although there is a region that is optimal for both DPO and SFT, the sequential DPO and SFT method fails to reach this region due to its focus on one objective at a given phase. Furthermore, from the loss space trajectory (Figure 2 lower-left), the model oscillates between extreme trade-offs for DPO and SFT, and ends up at a point far away from the ideal point."}, {"title": "THEORETICAL ANALYSIS OF SUBOPTIMALITY IN SEQUENTIAL METHOD", "content": "In this section, we provide a theoretical result on the suboptimal trade-offs between DPO and SFT in sequential training. We view the LLM as a policy \u03c0\u03b8 that is characterized by a softmax:\n\u03c0\u03b8(y|x) := \\frac{exp(\u03b8T\u03c6y,x)}{\u03a3y'\u2208Y exp(\u03b8T\u03c6y',x)},\nwhere \u03c6y,x is a feature vector corresponding to the input x and the target output y. Furthermore, reference policy \u03c0ref is similarly parameterized by a fixed parameter \u03b8ref.\nRemark 3.1. The softmax characterization is also used in previous theoretical works on RLHF; see, e.g., Zhu et al. (2023a). When the trainable parameter is the output projection weights, the LLM is fully characterized by the softmax. In other scenarios like performing LoRA Hu et al. (2021) on attention matrices or full-parameter training, we believe this characterization still provides valuable insights and our result will be verified empirically later in the experimental section.\nWe make the following mild assumption on the features.\nAssumption 3.1 (Bounded feature). For all x \u2208 X and y \u2208 Y, there exists \u03a6 >0 such that || \u03c6y,x || \u2264 \u03a6.\nWe can then have the following result for the sub-optimality of the output of Algorithm 1 to the optimum of some combination of functions fDPO and fSFT in terms of GMix,\u03bb.\nTheorem 3.1 (Lower bound for sequential method performance ). Consider Algorithm 1 with TDPO = TSFT = T under Assumption 3.1. Then there exists data DDPO and DSFT such that given any \u03bb\u2208 (0, 1), Algorithm 1 with any sufficiently large T has non-diminishing performance gap:\nE[\u03bbfDPO(\u03b8)+(1\u2212\u03bb)fsFT(\u03b8) - minf (\u03bbfDPO(\u03b8)+(1\u2212\u03bb)fsFT(\u03b8))] = \u03a9(1),\nwhere the expectation E[] is taken over the randomness of Algorithm 1.\nThe above result suggests that there exists DPO and SFT optimization problems such that given any trade-off between DPO and SFT defined by \u03bb \u2208 (0, 1), the sequential method suffers from constant suboptimality gap, even when optimized for a large number of iterations. The reason for the constant suboptimality gap is the sequential method described in Algorithm 1 suffers from forgetting, and cannot appropriately optimize both the DPO and SFT objectives. In the next section, we explore alternatives to the sequential method that can resolve this issue."}, {"title": "PROPOSED ALTERNATING TRAINING METHODS", "content": "In this section we introduce new algorithms with theoretically convergence guarantees, which also outperforms sequential DPO and SFT empirically."}, {"title": "ALRIGHT FOR JOINT DPO AND SFT", "content": "The main disadvantage of using Algorithm 1 for DPO and SFT optimization is that at a given phase of the algorithm, the model is updated with respective to only one objective. In contrast, it is computationally intensive, if not prohibitive, to optimize a linear combination of both DPO and SFT objectives. This is because, although the objectives share a single parameter, constructing two"}, {"title": "MAXRIGHT FOR JOINT DPO AND SFT", "content": "In this section, we introduce a method that can adaptively choose which objective to optimize based on the current performance of \u03b8, which can lead to faster convergence to a point that can perform well for both DPO and SFT objectives. When determining which objective to optimize at a given iteration, we first compare the current model's performance on fDPO and fSFT. Define the maximum (weighted) sub-optimality gap as\nfMax,\u03bb(\u03b8) = max (\u03bb(fDPO(\u03b8) - fDPO), (1-\u03bb)(fSFT(\u03b8) - fSFT)),\nwhere fDPO = mine\u2208\u0398 fDPO(\u03b8) (similarly for fSFT(\u03b8)), and \u03bb\u2208 [0,1]. The idea is to optimize this maximum sub-optimality to reach a balance between the two \u03bb-scaled objectives. Define the index of the objective with maximum (weighted) sub-optimality gap as it = argmaxi fi(\u03b8t), where\nf1,\u03bb(\u03b8t) := \u03bb (fDPO (\u03b8t; x1, yw, ye) \u2013 fDPO), and\nf2,\u03bb (\u03b8t) := (1 \u2212 \u03bb) (fsFT(\u03b8t; x, y) \u2013 fSFT),\nwhere x1, yw, ye ~ DDPO and x, yt ~ DSFT. Accordingly, we can update \u03b8 with respect to DPO objective using update (12) when it = 1 (or equivalently, when f1,\u03bb(\u03b8t) \u2265 f2,\u03bb(\u03b8t)), and update \u03b8 with respect to DPO objective using update (13) otherwise. This process is summarized in Algorithm 3. We can see in the toy illustration (Figure 2 Right), that MAXRIGHT can converge closer to the ideal point more directly compared to the alternating method, due to its ability to adaptively choose the worse performing objective to optimize.\nRemark 4.2. While we do not provide any theoretical convergence guarantees for Algorithm 3, it is a well-known fact in multi-objective optimization literature (Miettinen, 1999) that under some assumptions on the problem setup, the solution of problem (15) for any \u03bb \u2208 [0, 1] is guaranteed to be Pareto optimal (i.e. no other solution can further optimize both the objectives simultaneously)\nEven though MAXRIGHT allows one to compute the index needed for selecting the objective with a maximum (weighted) sub-optimality gap, in practice evaluating both objectives can be memory intensive, and only one objective is updated at a given iteration. To alleviate this issue, we propose to do simultaneous evaluations only every k steps. We call a time step that simultaneous evaluation is done as a 'max evaluation step'. At such time step t = to, we compute ito, and update the corresponding objective as in Algorithm 3. After the update, we store the computed (weighted) stale,to stale,to. Then, for every iteration before sub-optimality gap as fi = f1,\u03bb(\u03b8to) and f2 = f2,\u03bb(\u03b8to)\nthe next max evaluation step to + k, we choose the index of the objective to be optimized as\nito+k' = argmaxi ftale,to,\nwhere k' < k. Once the index is computed, we update the corresponding objective following (12) or (13), and update the stale (weighted) sub-optimality gap as\nfastale,to\ni,\u03bb = fi,\u03bb(\u03b8to+k'), if ito+k' = i,\nwhere i \u2208 {1,2}. This process is summarized in Appendix B. With this modification, we can match the evaluation and gradient computation complexity of Algorithm 2 in most iterations, at the expense of degraded accuracy in choosing it."}, {"title": "RELATED WORK", "content": "RLHF. The most fundamental form of RLHF was introduced by Christiano et al. (2017) and has been successfully used for aligning LLMs in many works such as OpenAI (2022); Ouyang et al. (2022); Bai et al. (2022a;b); Sun et al. (2024). There have been many works on RLHF for LLM alignment, including the more efficient direct preference optimization (Rafailov et al., 2024; Xu et al., 2024; Lee et al., 2024; Zhong et al., 2024), generalized RLHF (Azar et al., 2024; Munos et al., 2023), safe RLHF (Dai et al., 2023), group preference learning (Zhao et al., 2023; Chakraborty et al., 2024), and theory or understanding of RLHF (Zhu et al., 2023a; Shen et al., 2024b; Xiong et al., 2024; Wang et al., 2023; Kirk et al., 2023). In this work, we consider DPO (Rafailov et al., 2024) which has been used in training many popular open-source LLMs (Abdin et al., 2024; Dubey et al., 2024).\nSFT. Another important step before using a pre-trained LLM in downstream applications is SFT (Howard & Ruder, 2018; Devlin, 2018; Wei et al., 2021; Zhu et al., 2023b; Zhang et al., 2023b)."}, {"title": "EXPERIMENTS", "content": "In this section we compare the proposed methods with some existing baselines, in terms of their Pareto-front performance, and resource consumption such as computation time and memory usage."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we introduce models, datasets, baseline methods, and evaluation metrics used to evaluate the performance of our proposed methods. Additional experiment details are provided in Appendix D.\nModels. We employ two architectures. The first is ELEUTHERAI/PYTHIA-1B\u00b9, a widely used model balancing computational efficiency and performance. Despite not being designed for downstream tasks, it matches or exceeds models like OPT and GPT-Neo of similar size. We use it to assess optimization dynamics, performance trade-offs, and resource usage of proposed methods. The second is META-LLAMA/META-LLAMA-3-8B2, a larger model suited for fine-tuning and downstream real-world tasks. Both models are fine-tuned using Low-Rank Adaptation (LoRA) (Hu et al., 2021).\nDatasets. For the DPO dataset, we use the DAHOAS/RM-HH-RLHF dataset, which contains human feedback data designed to align models to human preference. For the SFT phase, we use the VICGALLE/ALPACA-GPT4 dataset, which consists of English instruction-following data generated by GPT-4 using Alpaca prompts, designed for fine-tuning LLMs.\nBaseline Methods. Comparing the performance of ALRIGHT and MAXRIGHT, we use the following baselines: Mix of DPO and SFT (\u2018Mix'), which simultaneously optimizes both DPO and SFT objectives by optimizing a convex combination of the objectives, and Sequential DPO and SFT ('Sequential'), where DPO and SFT objectives are optimized one after the other.\nEvaluation Metrics. To assess the performance of each method with respect to the DPO and SFT objectives, we utilize several evaluation metrics. For evaluating the DPO objective, we measure the optimality gap as fDPO(\u03b8) \u2013 fDPO, where fDPO is approximated by independently optimizing the DPO objective for the same number of iterations as used for the baselines and proposed methods. The optimality gap for the SFT objective is similarly defined using the optimal value fSFT, obtained by separately optimizing the SFT objective. To evaluate overall performance, we use the ideal distance metric, which represents the Euclidean distance between the final iterate produced by the method and the point corresponding to optimal values for both DPO and SFT objectives: \\sqrt{(fDPO(\u03b8) - f_{DPO})^2 + (fsFT(\u03b8) \u2212 fsFT)^2}. For resource efficiency, we compare the percentage increase in runtime relative to the corresponding sequential implementation, e.g., the percentage runtime increase of Alternating DPO and SFT with \u03bb = 0.01 compared to Sequential DPO and SFT with (TDPO, TSFT) = (1,5). Additionally, we compute the percentage increase in GPU utilization for each method relative to the baselines. Further details on these metrics are provided in Appendix D due to space constraints. Furthermore, for evaluating the real-world performance of proposed methods compared to baselines, we use the following benchmarks: MMLU(Hendrycks et al., 2020), a benchmark with multiple-choice questions across 57 diverse tasks; Win rate, which is calculated as the proportion of times a model's response is preferred by an evaluator over a baseline in head-to-head comparisons. For this purpose, we use ALPACAEVAL(Li et al., 2023a) framework with GPT-4-TURBO as the evaluator and DAHOAS/RM-HH-RLHF test data as the baseline."}, {"title": "EXPERIMENT RESULTS", "content": "In this section we illustrate and discuss the empirical results obtained under the experiment setup introduced in the previous section.\nALRIGHT provides better control over the trade-off compared to Sequential. As shown in the top left plot of Figure 3, the optimization trajectories for DPO followed by SFT illustrate that the set of final models produced by ALRIGHT, for various values of \u03bb, is more evenly distributed in the objective space. This distribution forms a Pareto front, indicating that no model is strictly worse than another with respect to both objectives. Moreover, the spread of these models is comparable to that of the Mix method. In contrast, Sequential tends to produce models that are biased towards the SFT objective, even when the number of DPO epochs is significantly higher than the number of SFT epochs (e.g., (TDPO, TSFT) = (5, 1)).\nMAXRIGHT achieves near-ideal performance compared to other methods. As illustrated in the top left plot of Figure 3, the optimization trajectories for DPO followed by SFT show that"}, {"title": "CONCLUSIONS AND DISCUSSION", "content": "In this paper, we have shown both theoretically and empirically that the widely adopted sequential approach to post-training LLMs with RLHF and SFT is sub-optimal, as the model gradually forgets the effects of the initial stage during the second stage of training. Our proposed ALRIGHT and MAXRIGHT methods address this issue, with ALRIGHT providing theoretical convergence guarantees and MAXRIGHT demonstrating strong empirical performance. Notably, this improvement is achieved with minimal additional computational cost, making these methods practical and efficient alternatives for enhancing both the performance and preference alignment of LLMs."}, {"title": "THEORETICAL ANALYSIS OF SUBOPTIMALITY IN SEQUENTIAL METHOD", "content": "We can then have the following result for the sub-optimality of the output of Algorithm 1 to the optimum of some combination of functions fDPO and fSFT in terms of GMix,\u03bb.\nTheorem 3.1 (Lower bound for sequential method performance ). Consider Algorithm 1 with TDPO = TSFT = T under Assumption 3.1. Then there exists data DDPO and DSFT such that given any \u03bb\u2208 (0, 1), Algorithm 1 with any sufficiently large T has non-diminishing performance gap:\nE[\u03bbfDPO(\u03b8)+(1\u2212\u03bb)fsFT(\u03b8) - minf (\u03bbfDPO(\u03b8)+(1\u2212\u03bb)fsFT(\u03b8))] = \u03a9(1),\nwhere the expectation E[] is taken over the randomness of Algorithm 1.\nThe above result suggests that there exists DPO and SFT optimization problems such that given any trade-off between DPO and SFT defined by \u03bb \u2208 (0, 1), the sequential method suffers from constant suboptimality gap, even when optimized for a large number of iterations. The reason for the constant suboptimality gap is the sequential method described in Algorithm 1 suffers from forgetting, and cannot appropriately optimize both the DPO and SFT objectives. In the next section, we explore alternatives to the sequential method that can resolve this issue."}, {"title": "ALRIGHT FOR JOINT DPO AND SFT", "content": "The main disadvantage of using Algorithm 1 for DPO and SFT optimization is that at a given phase of the algorithm, the model is updated with respective to only one objective. In contrast, it is computationally intensive, if not prohibitive, to optimize a linear combination of both DPO and SFT objectives. This is because, although the objectives share a single parameter, constructing two"}, {"title": "MAXRIGHT FOR JOINT DPO AND SFT", "content": "In this section, we introduce a method that can adaptively choose which objective to optimize based on the current performance of \u03b8, which can lead to faster convergence to a point that can perform well for both DPO and SFT objectives. When determining which objective to optimize at a given iteration, we first compare the current model's performance on fDPO and fSFT. Define the maximum (weighted) sub-optimality gap as\nfMax,\u03bb(\u03b8) = max (\u03bb(fDPO(\u03b8) - fDPO), (1-\u03bb)(fSFT(\u03b8) - fSFT)),\nwhere fDPO = mine\u2208\u0398 fDPO(\u03b8) (similarly for fSFT(\u03b8)), and \u03bb\u2208 [0,1]. The idea is to optimize this maximum sub-optimality to reach a balance between the two \u03bb-scaled objectives. Define the index of the objective with maximum (weighted) sub-optimality gap as it = argmaxi fi(\u03b8t), where\nf1,\u03bb(\u03b8t) := \u03bb (fDPO (\u03b8t; x1, yw, ye) \u2013 fDPO), and\nf2,\u03bb (\u03b8t) := (1 \u2212 \u03bb) (fsFT(\u03b8t; x, y) \u2013 fSFT),\nwhere x1, yw, ye ~ DDPO and x, yt ~ DSFT. Accordingly, we can update \u03b8 with respect to DPO objective using update (12) when it = 1 (or equivalently, when f1,\u03bb(\u03b8t) \u2265 f2,\u03bb(\u03b8t)), and update \u03b8 with respect to DPO objective using update (13) otherwise. This process is summarized in Algorithm 3. We can see in the toy illustration (Figure 2 Right), that MAXRIGHT can converge closer to the ideal point more directly compared to the alternating method, due to its ability to adaptively choose the worse performing objective to optimize.\nRemark 4.2. While we do not provide any theoretical convergence guarantees for Algorithm 3, it is a well-known fact in multi-objective optimization literature (Miettinen, 1999) that under some assumptions on the problem setup, the solution of problem (15) for any \u03bb \u2208 [0, 1] is guaranteed to be Pareto optimal (i.e. no other solution can further optimize both the objectives simultaneously)\nEven though MAXRIGHT allows one to compute the index needed for selecting the objective with a maximum (weighted) sub-optimality gap, in practice evaluating both objectives can be memory intensive, and only one objective is updated at a given iteration. To alleviate this issue, we propose to do simultaneous evaluations only every k steps. We call a time step that simultaneous evaluation is done as a 'max evaluation step'. At such time step t = to, we compute ito, and update the corresponding objective as in Algorithm 3. After the update, we store the computed (weighted) stale,to stale,to. Then, for every iteration before sub-optimality gap as fi = f1,\u03bb(\u03b8to) and f2 = f2,\u03bb(\u03b8to)\nthe next max evaluation step to + k, we choose the index of the objective to be optimized as\nito+k' = argmaxi ftale,to,\nwhere k' < k. Once the index is computed, we update the corresponding objective following (12) or (13), and update the stale (weighted) sub-optimality gap as\nfastale,to\ni,\u03bb = fi,\u03bb(\u03b8to+k'), if ito+k' = i,\nwhere i \u2208 {1,2}. This process is summarized in Appendix B. With this modification, we can match the evaluation and gradient computation complexity of Algorithm 2 in most iterations, at the expense of degraded accuracy in choosing it."}, {"title": "RELATED WORK", "content": "RLHF. The most fundamental form of RLHF was introduced by Christiano et al. (2017) and has been successfully used for aligning LLMs in many works such as OpenAI (2022); Ouyang et al. (2022); Bai et al. (2022a;b); Sun et al. (2024). There have been many works on RLHF for LLM alignment, including the more efficient direct preference optimization (Rafailov et al., 2024; Xu et al., 2024; Lee et al., 2024; Zhong et al., 2024), generalized RLHF (Azar et al., 2024; Munos et al., 2023), safe RLHF (Dai et al., 2023), group preference learning (Zhao et al., 2023; Chakraborty et al., 2024), and theory or understanding of RLHF (Zhu et al., 2023a; Shen et al., 2024b; Xiong et al., 2024; Wang et al., 2023; Kirk et al., 2023). In this work, we consider DPO (Rafailov et al., 2024) which has been used in training many popular open-source LLMs (Abdin et al., 2024; Dubey et al., 2024).\nSFT. Another important step before using a pre-trained LLM in downstream applications is SFT (Howard & Ruder, 2018; Devlin, 2018; Wei et al., 2021; Zhu et al., 2023b; Zhang et al., 2023b)."}, {"title": "EXPERIMENTS", "content": "In this section we compare the proposed methods with some existing baselines, in terms of their Pareto-front performance, and resource consumption such as computation time and memory usage."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section", "baselines": "Mix of DPO and SFT (\u2018Mix')", "Sequential": "where DPO and SFT objectives are optimized one after the other.\nEvaluation Metrics. To assess the performance of each method with respect to the DPO and SFT objectives", "objectives": "fDPO(\u03b8) - f_{DPO})^2 + (fsFT(\u03b8) \u2212 fsFT)^2. For resource efficiency, we"}]}