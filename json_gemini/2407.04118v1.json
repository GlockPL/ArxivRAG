{"title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization", "authors": ["Yuyan Chen", "Zhihao Wen", "Ge Fan", "Zhengyu Chen", "Wei Wu", "Dayiheng Liu", "Zhixu Li", "Bang Liu", "Yanghua Xiao"], "abstract": "Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Advancements in Large Language Models (LLMs) have ushered in a transformative era in natural language processing, showcasing their remarkable capabilities across a wide range of tasks (OpenAI, 2023; Bubeck et al., 2023; Lyu et al., 2019). While these models possess human-like comprehension and response abilities, their performance is heavily influenced by the quality of prompts. As can be observed in Fig. 1, answers from different LLMs vary widely when they are provided with the same task-specific prompts. Therefore, it is necessary to generate prompts that are most suitable for each LLM, thereby enhancing its performance on downstream tasks.\nA common practice towards prompt optimization is to count on human expertise (White et al., 2023; Jiang et al., 2022; Zamfirescu-Pereira et al.,"}, {"title": "2 Empirical study", "content": "In this section, we conduct empirical study on three LLMS (BLOOM-7B (Scao et al., 2022), GPT-J-6B (Wang and Komatsuzaki, 2021), and LLaMA-7B (Scao et al., 2022)) to evaluate their separate performance on question-answering (QA) (Chen et al., 2024b, 2023b), classification (Chen et al., 2023c,a), and generation (Chen et al., 2022, 2024a, 2023d) tasks with same task-specific prompts. We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers. We adopt F1 score, accuracy and ROUGE-L for QA, classification, and generation tasks, respectively. The visualization results are shown in the Fig. 2. From the violin plot, we observe significant variations in distribution among different LLMs in each task. For example, in the generation task, the results of all three models are distributed within the range of 0 to 0.5, but there are still differences in the specific distribution patterns. Moreover, the medians, means, and other statistical measures also differ greatly among three LLMs in each downstream task. Therefore, we consider that finding the optimal prompt for each specific LLM on each task is meaningful, as it can help enhance the LLMs' performance on various downstream tasks."}, {"title": "3 Methods", "content": "Based on the above empirical study, we present MAPO, a model-adaptive prompt optimization approach for LLMs. It takes the original prompt as input and generate an optimized prompt which makes an LLM give better outputs. The framework of MAPO is shown in Fig. 3.\n3.1 Warm-up Dataset Establishment\nWe first establish a warm-up dataset as training dataset for prompt optimization.\nGenerating Candidate Prompts. The original prompts are from nine above-mentioned datasets in P3 (Sanh et al., 2021). We generate 1,000 can-"}, {"title": "3.2 Prompt Optimizer Construction", "content": "The prompt optimizer seeks to refine the initial prompt (P) into an optimized prompt (Po) tailored to a particular LLM. This refinement process entails altering the structure or wording of P to pro-"}, {"title": "3.2.1 Supervised Fine-tuning", "content": "We begin by employing the warm-up dataset to conduct supervised fine-tuning (SFT) with an LLM across multiple downstream tasks. The objective of SFT is to enhance the LLM's capacity to generate responses that align with its preferences, utilizing annotated data. Prior research conducted by Ramamurthy et al. (2022) supports the notion that employing SFT prior to reinforcement learning (RL) leads to improved outcomes. Furthermore, to differentiate between specific tasks during training, we incorporate a brief instruction preceding the input, such as \u201cThis is a... (generative/question-answering/classification) task.\u201d."}, {"title": "3.2.2 Building Reward Model", "content": "Next, we construct a reward model to learn the effectiveness of prompts based on the preferences of different LLMs. This approach is motivated by the fact that discriminative annotation through sorting incurs significantly lower costs compared to generating annotations for answers. Initially, we obtain a ranking sequence for an LLM in a specific downstream task. We sort the outputs generated by candidate prompts $\\{P_1, P_2, ..., P_{k-1}, P_k\\}$ alongside the original prompt P, using the same evaluation metric as described in Sec. 3.1."}, {"title": "3.2.3 Reinforcement Learning", "content": "Subsequently, we employ Reinforcement Learning (RL) to further fine-tune LLMs. RL is used to adjust the bias in the reward model's scoring since the distribution of generated prompts might change during the SFT process. The primary objective of optimization is to maximize the scores of prompts generated by LLM after SFT (referred to as the SFT model), as evaluated by the reward model. To achieve this, we utilize a combination of Proximal Policy Optimization (PPO) (Schulman et al., 2017) and RRMF algorithms (note that RRMF is inspred by RRHF (Yuan et al., 2023)) for joint learning.\nPolicy Optimization. This step aims to optimize the RL policy to improve the performance of the RL model. We first adopt the datasets shown in Table 3, which to construct environment-action pairs. The environment refers to the original prompt, and the action represents the prompt generated by LLM without instruct-tuning. We pass the environment-action pairs to the reward model to obtain rewards. In this process, we introduce an actor model, which is LLM, and a frozen model, which is SFT model with its parameters frozen during the RL training process. The frozen model serves as a benchmark to evaluate whether the updated actor model has advantages over it. We then calculate the policy gradient loss (i.e., actor's loss) based on the importance ratio and reward $(r+\\gamma V_{next} - V_{cur})$, and calculate the value loss (i.e., critic's loss) by comparing the predicted value $V_{pred}$ with the true value $(r + \\gamma V_{next})$ as follows:\n$L_{pg} = \\frac{P_{\\pi_{\\alpha}}(t)}{P_{\\pi_{\\epsilon}}(t)}(r + \\gamma V_{next} - V_{cur}),$ (2)\n$L_v = ||V_{pred} - (r + \\gamma V_{next})||.$ (3)\nHere, $\\frac{P_{\\pi_{\\alpha}}(t)}{P_{\\pi_{\\epsilon}}(t)}$ represents the ratio of probabilities (i.e., importance ratio) of generating the same token under the actor model and the frozen model. $(r + V_{next} - V_{cur})$ represents the reward of the current step. $V_{pred}$ denotes the predicted value, and $(r + V_{next})$ denotes the true value.\nNext, we maximize the mathematical expectation of the reward model, aiming to consistently generate prompts that LLM perceives as the best in the RL-trained SFT model (referred to as RL model). We feed prompts x generated by the SFT model based on the datasets shown in Table 3 (i.e., D) into the RL model $\\pi_{RL}$ to obtain an optimized prompt y. y changes every time the RL model is updated. We then input (x, y) into the reward model $r_\theta$ and calculate a score (i.e., reward), which represents the real-time feedback from the reward model. The loss function is defined as follows:\n$L_r = E_{(x, y) \\sim D_{RL}}[r_\\theta(x, y)].$ (4)\nFinally, we combine the above loss functions to optimize the RL policy from multiple perspectives. The final loss function is defined as:\n$L_\\pi = \\alpha_1 L_{pg} + \\alpha_2 L_v + \\alpha_3 L_r,$ (5)\nwhere $\u03b1_1, \u03b1_2$, and $\u03b1_3$ represent the optimal weights of each function, which are determined through experiments (the same applies below).\nSFT Approximating. This step aims to maintain similarity between the RL model and the SFT model. When the RL model undergoes parameter updates, it leads to variations in the generated prompt y based on the given prompt x. If there is a significant discrepancy between the RL model and the SFT model, it can result in inaccurate estimation of scores by the reward model. To address"}, {"title": "3.2.4 Generalization Maintaining", "content": "Finally, we combine the above loss functions for SFT approximating as follows:\n$L_{SFT} = \\beta_1 L_{SFT} + \\beta_2 L_{ft} + \\beta_3 L_r.$ (10)\nGeneralization Maintaining. This step addresses the issue of catastrophic forgetting by ensuring that an LLM performs well not only on specific tasks but also on general NLP tasks. To achieve this, we follow a similar approach as outlined in InstructGPT (Ouyang et al., 2022). We sample 10% data from general NLP tasks in GLUE (Wang et al., 2018) and the SuperGLUE benchmark (Wang et al., 2019), which are considered representative, as indicated in Table 3, during the pre-training phase. The objective of pre-training is to generate outputs that are as good as or better than the original one based on the original prompts. The original"}, {"title": "4 Experiments", "content": "In this section, We conduct experiments with three popular LLMs as LLM, respectively, including BLOOM (7B), GPT-J (6B), and LLaMA (7B), on different downstream tasks to validate the effectiveness of MAPO.\n4.1 Experimental Setups\nThe experiments are executed on 4 Nvidia A100 GPUs with 80GB each, using PyTorch in Python. DeepSpeed is utilized in the training process. The maximum sequence length for original prompts and optimized prompts are both set to 512 tokens. The number of epochs is set to 20 in the entire training process. We provide the detailed configuration of the hyperparameters in Appendix B. The dataset and metrics we utilize are the same as those described in Sec. 2. All results are reported on the corresponding test sets or 10% dev sets if a dataset does not have a test set. Details of all used baselines and datasets are in Appendix D and E.\n4.2 Main Results\nThe main results are shown in Table 2. We observe that the performance increase evidently among all LLMs during SFT. We then utilize MAPO to make further optimization. We find the optimized prompts generated by MAPO are more adaptive in QA and generation task for BLOOM (increase by 20.5% for CloseQA and by 30.9% for Explan compared with SFT (p<0.01)) and GPT-J (increase by 21.4% for CloseQA and by 20.6% for Explan compared with SFT (p<0.01)). And the prompts are more adaptive in classification task (increase by 22.8% for News (p<0.01)) for LLaMA. These results indicate that MAPO effectively enhances the performance of various LLMs and exhibits preferences in different downstream tasks."}, {"title": "4.3 Ablation Study", "content": "The effect of RL compared with SFT. From the experiments (Table 3, Table 5 and Table 6), we can observe that the performance improvements gained solely from using SFT are less than half of those achieved by our proposed MAPO method, both on similar tasks and general NLP tasks. This clearly indicates the effectiveness of MAPO in optimizing model-adaptive prompts.\nIn order to further demonstrate RL is necessary"}, {"title": "4.4 Case Study and Error Analysis", "content": "We conduct a case study to visualize the prompts optimized by MAPO, as shown in Table 7. Additional cases are included in Appendix G. We first observe that the majority of the original prompts have significant modifications after optimization through our MAPO method. Only about 10% of the generated prompt pairs remain completely unchanged. To further quantify these changes, we calculate a normalized edit distance. Given the varying lengths of different prompt pairs, we divide the edit distance by the average length of the two strings. This yields a value between 0 and 1, where 0 indicates identical strings and 1 indicates completely different strings. The average normalized edit distance for all prompt pairs stands at 0.67, demonstrating that most prompts do experience substantial modifications."}, {"title": "4.5 Exploratory Analysis", "content": "We conduct an exploratory analysis to further investigate the patterns in optimized prompt as shown in Fig. 7, Fig. 8 and Fig. 9. We extract the three most frequent words from the original prompt and investigate their distribution in the optimized prompt for each LLM, while either retaining high-frequency words in instructions (including sentence, topics, subjects, present, statement, discussed, mentioned, included, following) or removing them."}, {"title": "5 Related Work", "content": "LLMs' prompt optimization process involves prompt retrieval, prompt generation from scratch and prompt editing. For prompt retrieval, for example, Ma et al. (2023) adopt greedy search to identify near-optimal prompts. Zhou et al. (2022) introduce APE for automatic instruction selection, etc. For prompt generation from scratch, Pang et al. (2023) introduce SharpT, which learns a shared latent space and generates soft prompts. White et al. (2023) describe a catalog of prompt engineering techniques. Zamfirescu-Pereira et al. (2023) investigate end-user prompt engineering using a prototype LLM-based chatbot design tool. Wang et al. (2022a) present Self-Instruct for improving instruction-following capabilities of PLMs. For prompt editing, Gao et al. (2020) automatically select label words and generate templates. Pryzant et al. (2023) introduce APO based on \"gradients\" to provide critical feedback on the current prompt. Deng et al. (2022) propose RLprompt based on RL. Zhang et al. (2023) propose TEMPERA, which provides interpretable prompts for different queries. Prasad et al. (2022) introduce GrIPS, a gradient-free approach for improving task instructions for LLMs. Moreover, some research focuses on incorporating additional knowledge to enhance prompt editing. For example, Li et al. (2023) propose DSP to generate \u201cdirectional stimulus\u201d of each input. Qin and Eisner (2021a) optimize a mixture of prompts using gradient descent to generate relational knowledge. Shin et al. (2020) develop Autoprompt, a gradient-guided approach to find the best tokens in the prompt. Jiang et al. (2020) propose mining-based and paraphrasing-based methods to automatically generate diverse prompts. Furthermore, some research focus on continuous prompt optimization instead of discrete prompt optimization mentioned before, such as research by Zheng et al. (2023), Hambardzumyan et al. (2021), Zhong et al. (2021), etc. However, all above-mentioned prompts optimization approaches aim to obtain task-specific prompts instead of model-specific ones. Different from theirs, we dedicate at optimizing prompts for LLMs within the NLP domain and achieve impressive performance."}, {"title": "6 Conclusions", "content": "The remarkable capabilities of LLMs have revolutionized NLP in various tasks. However, their performance heavily relies on the quality of prompts. In this work, we address the prompt optimization challenge by proposing a Model-Adaptive Prompt Optimization (MAPO) approach. Through extensive experiments, we demonstrated that MAPO can adapt different LLMs with generating model-friendly prompts to enhance their capabilities across various downstream tasks. In future work, we aim to construct more fine-grained model-adaptive prompts that can adapt to the continuously evolving data encountered in real-world production environments. Additionally, we intend to enhance its applicability across a broad spectrum of linguistic contexts."}, {"title": "Limitations", "content": "It is important to acknowledge certain limitations of our approach. Firstly, the effectiveness of prompt optimization heavily relies on the availability and quality of the warm-up dataset. In cases where the dataset is limited or does not sufficiently cover the specific task, the performance gains from prompt optimization may be constrained. Additionally, MAPO requires extensive SFT and RL, which can be computationally expensive and time-consuming. This could limit the scalability of MAPO, especially when dealing with large-scale tasks or datasets. Despite these limitations, our study provides valuable insights into model-adaptive prompt optimization for LLMs and contributes to the ongoing efforts in improving the performance of these LLMs in practical applications."}, {"title": "Acknowledgement", "content": "This work is supported by Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), the National Natural Science Foundation of China (No.62072323, U21A20488), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), Key Projects of Industrial Foresight and Key Core Technology Research and Development in Suzhou(SYC2022009)."}]}