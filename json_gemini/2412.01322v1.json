{"title": "Explainable fault and severity classification for rolling element bearings using Kolmogorov-Arnold networks", "authors": ["Spyros Rigas", "Michalis Papachristou", "Ioannis Sotiropoulos", "Georgios Alexandridis"], "abstract": "Rolling element bearings are critical components of rotating machinery, with their performance directly influencing the efficiency and reliability of industrial systems. At the same time, bearing faults are a leading cause of machinery failures, often resulting in costly downtime, reduced productivity, and, in extreme cases, catastrophic damage. This study presents a methodology that utilizes Kolmogorov-Arnold Networks to address these challenges through automatic feature selection, hyperparameter tuning and interpretable fault analysis within a unified framework. By training shallow network architectures and minimizing the number of selected features, the framework produces lightweight models that deliver explainable results through feature attribution and symbolic representations of their activation functions. Validated on two widely recognized datasets for bearing fault diagnosis, the framework achieved perfect F1-Scores for fault detection and high performance in fault and severity classification tasks, including 100% F1-Scores in most cases. Notably, it demonstrated adaptability by handling diverse fault types, such as imbalance and misalignment, within the same dataset. The symbolic representations enhanced model interpretability, while feature attribution offered insights into the optimal feature types or signals for", "sections": [{"title": "1 Introduction", "content": "Rotating machinery plays an indispensable role in modern industry, powering numerous applications across the manufacturing, energy, and transportation sectors (Song et al, 2018). Among their components, rolling element bearings (referred to simply as bearings hereafter) are vital, yet vulnerable elements, with a heavy influence on the performance and lifespan of machines. Notably, it has been reported that up to 50% of motor faults are bearing-related (Nandi et al, 2005), while several issues in rotating machinery today can be traced to the improper design or application of bearings (Cao et al, 2018). Such failures can result in severe consequences, including unexpected downtime and costly repairs (Heng et al, 2009; Jian and Ao, 2023), or even catastrophic damage or loss of life in extreme cases (Neupane and Seok, 2020). Moreover, in manufacturing environments, where continuous production is critical, disruptions caused by bearing failures can lead to substantial losses in productivity (Bagci Das and Das, 2024). Early and accurate bearing fault detection and classification are therefore essential in modern industrial and manufacturing practices.\nBefore the widespread use of machine learning (ML) and deep learning (DL) methodologies, bearing fault detection and classification relied on other widely adopted techniques to identify characteristic fault patterns. For instance, vibration analysis was commonly employed to detect frequency peaks associated with specific faults (Nandi et al, 2005), while signatures of these vibration frequencies were also identified in the current spectrum through electrical signal processing (Schoen et al, 1995). Additionally, features extracted from the Fourier spectrum of vibration signals were utilized to detect faults as peaks in the frequency-domain (Wang et al, 2016) and acoustic emission (AE) monitoring offered earlier and more fine-grained fault detection compared to vibration monitoring (Hawman and Galinaitis, 1988). However, these methods were often bound to specific fault scenarios or experimental conditions, which limited their applicability to diverse operating environments. For example, directly identifying bearing faults through raw vibration signals is challenging, as vibrations are typically dominated by imbalance and misalignment components (McInerny and Dai, 2003). Moreover, the experimental results of Schoen et al (1995) were based on cases of extensive bearing damage, raising concerns about the applicability of this approach for less severe faults (McInerny and Dai, 2003). Finally, features like spectral kurtosis have been shown to be sensitive to strong harmonic interferences when used as the only fault indicator (Hu et al, 2020) and AE monitoring has also proved to be highly susceptible to background noise (Al-Ghamd and Mba, 2006)."}, {"title": null, "content": "Despite their limitations, the aforementioned techniques laid the foundation for identifying which types of sensor data are most effective for detecting and classifying bearing faults. Modern data-driven approaches have built upon this groundwork, incorporating features extracted from sensor data to develop more robust and generalizable frameworks. Examples of features extracted directly from the time-domain signal include but are not limited to the root mean square (RMS), crest factor (CF), skewness and kurtosis (Azeez and Alex, 2014). Spectral features, such as fundamental frequencies, spectral kurtosis and spectral entropy are obtained by applying a Fast Fourier Transform (FFT) to the time-domain signals and have also been widely used in such applications (Caesarendra and Tjahjowidodo, 2017). Beyond time- and frequency-domain features, time-frequency representations, such as those derived from the Short-Time Fourier Transform or wavelet transformations, are often used to extract features for capturing transient and non-stationary behaviors (Feng et al, 2013). Drawing from these diverse feature sets, a series of ML models such as k-Nearest Neighbors (Sharma et al, 2018; Lu et al, 2021), Support Vector Machines (SVMs) (Andrijauskas and Adaskevicius, 2018; Kumar and Anand, 2024), and Random Forests (Xue et al, 2019; Roy et al, 2020; Alhams et al, 2024) have been explored for the tasks of bearing fault detection and classification. More recently, the widespread adoption of sensors in industrial settings and the advent of the big data era have driven the use of DL architectures for these tasks, including Autoencoders (Li et al, 2018; Liu et al, 2018), Recurrent Neural Networks (Liu et al, 2018; Zhang et al, 2021), Convolutional Neural Networks (Pandhare et al, 2019; Guo et al, 2022; Chung et al, 2023), and Generative Adversarial Networks (Zhang et al, 2019; Mao et al, 2019).\nAlbeit successful in achieving high performance for bearing fault detection and classification, ML and especially DL models often fall short in areas where traditional approaches excel, with explainability being a notable example. The ability to understand and interpret a model's decisions is crucial, especially in safety-critical applications or when deeper insights into the underlying physical processes are required (Decker et al, 2023). In addition to explainability, a significant challenge arises in deploying DL models for real-time condition-based monitoring on edge devices, i.e., resource-constrained computing units located close to the machinery they monitor. Many DL architectures are computationally intensive, making them unsuitable for resource-constrained environments (Hakim et al, 2023). Another important consideration is the quantity and quality of features used by these models. While leveraging a large number of features can often yield superior results, achieving comparable performance with fewer features is far more desirable (Grover and Turk, 2020); budget constraints and technical limitations in practical scenarios demand careful sensor selection, as collecting an exhaustive set of measurements is neither feasible nor economical. Furthermore, the effectiveness of features can vary significantly, depending on the dataset or system under study; features that perform well for one problem may be suboptimal for another. This variability highlights the need for models capable of adaptively selecting the most relevant features for a given problem (Maliuk et al, 2022). To address these challenges, this paper presents a unified framework centered around Kolmogorov-Arnold Networks (KANs), designed to provide explainability, efficiency, and adaptive feature selection."}, {"title": null, "content": "Inspired by the Kolmogorov-Arnold representation theorem, KANs were recently introduced by Liu et al (2024b) as an alternative to Multi-Layer Perceptrons (MLPs), serving as a new paradigm for the underlying architecture of DL models. Unlike in the case of MLPs, where activation functions are fixed, KANs contain trainable univariate functions as activations, allowing them to represent relationships in symbolic forms. This inherent explainability, along with their demonstrated performance in domains such as differential equations (Shukla et al, 2024; Howard et al, 2024; Jacob et al, 2024), high-energy physics (Erdmann et al, 2024; Abasov et al, 2024), and smart systems and devices (Wang et al, 2024; Xu et al, 2024), makes KANs a promising candidate for addressing both scientific and engineering problems (Liu et al, 2024a). In the context of bearing fault detection and classification, there is a notable lack of studies utilizing KANs, with the exception of Li et al (2024). In that study, the Case Western Reserve University (CWRU) bearing dataset (Case Western Reserve University, 2003) was employed; however, the primary focus of the paper was unrelated to bearing fault diagnosis and instead aimed at the early prediction of natural gas pipeline leaks.\nBuilding on the potential of KANs and addressing the identified gaps, the main aspects of the proposed framework - and thus the main contributions of the current work - can be summarized in the following points:\n\u2022 Explainable selection of minimum features: by training shallow KANs with sparsity-inducing regularization, the minimum number of features relevant to the problem can be automatically identified via attribution scores and dynamic thresholds.\n\u2022 Interpretable and lightweight model design: a model in symbolic form can be obtained from the trained activation functions, enabling its analysis outside of a black-box regime and ensuring efficiency for deployment on edge devices.\n\u2022 A unified approach to bearing fault diagnosis: fault detection & classification, and severity classification are addressed within the same framework.\n\u2022 Broad applicability beyond bearing faults: the framework's generalization capabilities are demonstrated by its application to non-bearing-exclusive data.\nTo the best of the authors' knowledge, this is among the first attempts to address bearing faults in a holistic and generalizable manner, incorporating detection, classification and severity estimation within a single, lightweight DL framework that also handles feature selection and provides explainable results. A recent related study by Bagci Das and Das (2024) touches upon some of these challenges by employing an SVM model for fault classification and genetic algorithms (GAs) for automated feature selection. Nevertheless, GAs are computationally demanding, and SVMs, along with GAs, lack the explainability provided by the proposed KAN-based approach.\nThe remainder of the present paper is structured as follows: Section 2 presents the proposed framework in detail, including its components, methodology and theoretical foundation. Subsequently, the two datasets utilized in this study are introduced in Section 3, along with a discussion on the rationale for their selection and a presentation of the feature libraries extracted to implement the framework. In Section 4 the experimental results obtained on both datasets are reported, focusing on selected features, model performance and symbolic representations. Finally, Section 5 provides a summary and discussion on the work's main findings."}, {"title": "2 Proposed Methodology", "content": "Prior to the discussion of the proposed methodology's technical details, an overview of KANs, their theoretical formulation and the properties that establish them as a key component of the framework are provided."}, {"title": "2.1 Kolmogorov-Arnold Networks", "content": "The theoretical foundations of KANs lie on the Kolmogorov Superposition Theorem (KST), which provides a robust theoretical framework for decomposing multivariate functions into simpler univariate functions through summation operations. Earlier attempts to apply the theorem for function approximation sought to implement it in its exact form but faced significant challenges due to the pathological behavior of the inner univariate functions (Guilhoto and Perdikaris, 2024). Recently, Liu et al (2024b) extended the KST into a \"deep\" equivalent, introducing a more flexible network architecture that contains an arbitrary number of layers with arbitrary widths, rather than adhering strictly to the original formulation of the theorem.\nOne such extended architecture with L layers is defined by an integer array $[n_0, n_1, ..., n_L]$, where $n_i$ denotes the number of input nodes of the i-th layer. Unlike in MLPs, a KAN layer corresponds to the activation functions between a set of input and output nodes, rather than the inputs or outputs themselves, which is why the array has L + 1 elements. The corresponding model can be written as\n$u (x; \\Theta) = [\\Phi^{(L)} \\circ... \\circ \\Phi^{(1)}] (x),$\nwhere $\\Theta$ represents the network's trainable parameters, $\\circ$ denotes successive application of $\\Phi^{(1)}$, and\n$\\Phi^{(2)} (x^{(t)}) =\\begin{pmatrix} \\Phi_{l,1,1}(\\cdot) & ... & \\Phi_{l,1,n_{l+1}} (\\cdot) \\\\ \\Phi_{l,1,2}(\\cdot) & ... & \\Phi_{l,n_{l},2} (\\cdot) \\\\ ... & ... & ...\\\\ \\Phi_{l,n_l, 1} (\\cdot) & ... & \\Phi_{l,n_l,n_{l+1}}(\\cdot)\\end{pmatrix} \\begin{pmatrix} x^{(1)}, \\\\ ... \\\\ x^{(n_l)}, \\end{pmatrix},$\nwith $\\Phi_{l,i,j}$ being the l-th layer's activation function, which connects the layer's i-th input node to its j-th output node. These activation functions are given by\n$\\phi(x) = \\sigma_r(x) + \\sigma_B(x),$\nwhere\n$\\sigma(x) = \\frac{1}{1 + exp(-x)},$\nis the Sigmoid Linear Unit (SiLU) function and\n$B (x) = \\sum_{i=1}^{G+k} C_i B_i (x)$\nis a spline activation composed of (G+k) B-spline basis functions of order k on a grid with G intervals. The parameters $\\sigma_r$, $\\sigma_B$ and $\\{c_i\\}_{i=1}^{G+k}$ of each activation function,"}, {"title": null, "content": "and consequently the activation function itself, are trainable, which is where the interpretability of KANs stems from: each trained activation function can be replaced with a symbolic representation that best fits it.\nBeyond this interpretability, KANs feature an attribution scoring mechanism that provides explainability by quantifying the relative importance of the model's input features (Liu et al, 2024a). This requires defining the attribution score of the i-th input node in the l-th layer, denoted by $A_{l,i}$. To compute such scores, one must also define the standard deviation of the l-th layer's activation function connecting node i to node j as\n$E_{l,i,j} = \\sqrt{ \\frac{1}{N} \\sum_{s=1}^{N} [\\Phi_{l,i,j} (x_s) - \\frac{1}{N} \\sum_{p=1}^{N} \\Phi_{l,i,j} (x_p)]^2 },$\nwhere N is the number of samples. Then, $A_{l,i}$ can be calculated recursively via\n$A_{l,i} = \\sum_{j=1}^{n_{l+1}} E_{l,i,j} \\cdot A_{l+1,j} \\cdot \\left(\\sum_{p=1}^{n_l+1} E_{l,p,j}\\right)^{-1}, l\\in \\{L,...,1\\},$\nand the initial condition $A_{L+1,i} = 1, \\forall i \\in \\{1, ..., n_1 \\}$, i.e. setting the scores of the final layer's output nodes to 1. Note that the notation refers to input nodes, which is why the subscript is L + 1, even though the model consists of L layers. The final score for the i-th input feature then simply corresponds to $A_{1,i}$."}, {"title": "2.2 Proposed Framework", "content": "Building on these theoretical foundations, the initial objective of the proposed methodology is to leverage the attribution scoring mechanism of KANs for automatic feature selection. To this end, a feature library is first constructed using existing literature and domain knowledge, automatic feature extraction methods (e.g., Yi et al (2023)), or a combination thereof, depending on the studied problem (bearing faults in the present case). Each data sample is thus represented as a K-dimensional vector, where K is equal to the total number of features in the library. After all data samples are split into distinct training, validation, and evaluation sets, the feature selection process is formulated as a grid-search multi-objective problem.\nSpecifically, multiple KAN model instances are trained on the training set by minimizing a loss function that incorporates the following regularization term:\n$\\mathcal{L}_{reg} = \\lambda \\sum_{l=1}^{L} A- \\sum_{i=1}^{\\pi_l} \\frac{A_{l,i}}{A_l} log \\left( \\frac{A_{l,i}}{A} \\right),$\nwhere\n$A_l = \\sum_{i=1}^{\\pi_l} A_{l,i}.$\nThis sparsity-inducing expression corresponds to a mixture of L1 and Entropy regularization (first and second summand of Eq. (8), respectively), with an overall weight $\\lambda$, corresponding to a hyperparameter. Each model instance is trained for a fixed number of epochs using a distinct value of $\\lambda$, selected from a discretized range $[\\, \\lambda_{max}]$. For each model instance corresponding to a particular $\\lambda$, the attribution score is computed for all K features using Eq. (7). The most important features are then selected based on the condition\n$A_{1,i} \\ge \\tau,$\nwhere $\\tau$ is a hyperparameter which determines the threshold for feature selection. To ensure an appropriate choice of $\\tau$, multiple threshold values are evaluated, drawn from a discretized range $[\\, \\tau_{max}]$, to identify the most significant features. As a result, each combination of $(\\lambda, \\tau)$ corresponds to a distinct selection of important features."}, {"title": "3 Datasets and Feature Extraction", "content": "As previously outlined, the proposed framework has been designed for applicability across a wide range of problems beyond bearing faults. To apply it for bearing fault detection and classification, two widely recognized datasets are selected: the CWRU bearing dataset (Case Western Reserve University, 2003) and the Machinery Fault Database (MaFaulDa) dataset (MaFaulDa, 2016; Marins et al, 2018). The CWRU dataset is chosen due to its characterization as a dataset where feature selection is highly nontrivial, containing data that deviate from the typical characteristics expected for certain fault types (Neupane and Seok, 2020). The MaFaulDa dataset, on the other hand, is selected for its broader scope, as it includes not only bearing faults but also additional types of machinery faults, thereby enabling the demonstration of the framework's generalizability within a single dataset. Before detailing the process of constructing a feature library from the raw time-series signals of the two datasets, a more detailed introduction to each dataset is provided Sections 3.1 and 3.2."}, {"title": "3.1 CWRU Dataset", "content": "The CWRU dataset was generated using a test rig designed to simulate bearing faults under controlled conditions. The setup consisted of a 2-horsepower motor, a torque transducer, and a dynamometer, with the test bearings supporting the motor shaft. Three types of single-point faults were induced in the bearings using electro-discharge machining: inner raceway (IR), ball (B), and outer raceway (OR) faults, with fault diameters ranging from 7 mils (1 mil is equivalent to 0.001 inches) to 40 mils. Faults were applied to both the drive-end and fan-end bearings. The dataset comprises vibration measurements collected using accelerometers attached to the motor housing at the 12 o'clock position for both the drive-end and fan-end bearings, with an additional accelerometer attached to the base plate in some experiments. The signals were recorded at sampling rates of 12 kHz and, for certain drive-end faults, 48 kHz. For OR faults, experiments were conducted at different positions relative to the load zone (3 o'clock, 6 o'clock, and 12 o'clock) to capture variations in the vibration response. Thus, the dataset contains six classes for classification, labeled as N (normal), B, IR, OR@3, OR@6, and OR@12.\nThe original dataset's files can be categorized along several axes. Based on motor speed, the files are divided into four groups: 1730, 1750, 1772, and 1797 rotations per minute (RPM). Based on fault location and sampling rate, the dataset includes normal files measured at 48 kHz, drive-end faults measured at 12 kHz, fan-end faults measured at 12 kHz, and drive-end faults measured at 48 kHz. Additionally, the files differ in the time-series data they contain: some include only drive-end measurements, most include both drive-end and fan-end measurements, and a few include drive-end, fan-end, and base measurements. Due to the inconsistencies mentioned in Rigas et al (2024b), the version of the dataset curated for the purposes of the cited work was used. Moreover, all 48 kHz drive-end measurements were excluded for two reasons: to"}, {"title": "3.2 MaFaulDa Dataset", "content": "The MaFaulDa dataset was created using a test rig designed to emulate the dynamics of motors with two shaft-supporting bearings. It comprises multivariate time-series data collected from sensors mounted on a SpectraQuest alignment/balance vibration trainer machinery fault simulator. The sensors included one triaxial accelerometer for the underhang bearing (bearing located between the motor and rotor) and three industrial accelerometers for the overhang bearing (bearing located outside the rotor, opposite the motor), oriented along the axial, radial, and tangential directions. Additionally, an analog tachometer measured the system's rotational frequency, and a microphone captured operational sound. All signals were recorded at a sampling rate of 50 kHz over a duration of 5 seconds.\nThe dataset includes scenarios representing both normal operation and various fault conditions. In the normal class (N), the system operated without faults across 49 distinct rotation frequencies, ranging from 737 to 3686 RPM in approximately 60 RPM intervals. Bearing faults, similar to those in the CWRU dataset, involved defects in the inner raceway (IR), ball (B), and outer raceway (OR). These faults were studied in both bearings, underhang and overhang, one at a time. To ensure fault detectability, additional imbalances of 6 g, 10 g, and 20 g were introduced. Bearing fault scenarios were recorded under 49 rotation frequencies for lighter imbalances, while fewer frequencies were studied for heavier ones due to increased vibrations.\nBeyond bearing faults, the dataset also includes additional machinery faults, namely imbalance (I) and axis misalignment. Imbalance faults were simulated by attaching varying load weights (6 g to 35 g) to the rotor. For weights up to 25 g, all 49 rotation frequencies were studied, whereas higher weights limited the maximum frequency to 3300 RPM due to increased vibrations. Axis misalignment was divided into horizontal misalignment (HM) and vertical misalignment (VM), induced by shifting the motor shaft by offsets of 0.5 mm to 2.0 mm for the former, and 0.51 mm to 1.9 mm for the latter. For each misalignment severity, the same 49 rotation frequencies as in the normal class were studied. In total, the dataset corresponds to 10 distinct classes and comprises 1951 data files, all of which were retained for feature extraction."}, {"title": "3.3 Feature Library", "content": "The extracted features were acquired by first augmenting and then preprocessing data from both datasets. Data augmentation was particularly critical for the CWRU dataset, which contained only four data files per fault type and severity - corresponding to the four rotational frequencies studied. In contrast, the MaFaulDa dataset included nearly 50 examples per fault case, yet augmentation was still applied to further enhance the dataset. The first step involved identifying the rotational frequency, fr, for each file. For the CWRU files, the exact RPM values were already known. However, for the"}, {"title": null, "content": "MaFaulDa dataset, where RPM values were estimated per file, the rotational frequency was calculated using the two-step algorithm proposed in de Lima et al (2013), based on the tachometer signal. This method was selected to avoid misidentification of fr, which could otherwise be obscured by spectral peaks introduced by machine faults in the signal's frequency spectrum.\nOnce the rotational frequency was determined, it was combined with the sampling rate, Fs, to split each time-series into smaller segments of N. Fs/fr data points, where N represents the number of complete motor rotation cycles. The choice of N balances a trade-off between dataset size and segment quality: a smaller value leads to more segments but at the cost of lower quality, while a larger value preserves the original time-series' quality at the expense of limited samples. For this study, N = 48 was chosen as a compromise, yielding approximately six segments per file for the CWRU dataset. The same number of cycles was chosen for the MaFaulDa dataset to maintain consistency, resulting in augmented datasets containing 603 and 6268 segments for CWRU and MaFaulDa, respectively.\nUsing the augmented datasets, a series of time-domain, frequency-domain, and time-frequency features were extracted, based on established literature in machinery fault diagnosis. For the time-domain features, the extracted metrics included the RMS, mean, variance, skewness, kurtosis, entropy, shape factor, crest factor, impulse factor, and margin factor, along with histogram upper and lower bounds as described in Caesarendra and Tjahjowidodo (2017). From the frequency domain, spectral skewness and kurtosis were calculated after applying an FFT to each signal. Additionally, the signal magnitudes at the fundamental frequency and its first two harmonics were extracted, following Marins et al (2018).\nFor time-frequency features, wavelet transformations were employed, as they are highly effective for identifying the machinery faults. The pywavelets library (Lee et al, 2019) was used to perform a multilevel decomposition of order 4 on each segment, utilizing a biorthogonal wavelet. Following Bagci Das and Das (2024), features derived from the fine-grained wavelet coefficients included the mean, median, RMS, standard deviation, variance, skewness, kurtosis, and entropy. Percentile values at the 5th, 25th, 75th, and 95th levels were also extracted, along with the number of mean and zero crossings. Using this approach, a feature library of 62 and 243 features was compiled for the augmented CWRU and MaFaulDa datasets, respectively. This corresponds to 31 features per dataset signal, with the exception of the tachometer signal in MaFaulDa, from which no spectral features were extracted (Marins et al, 2018).\nA detailed list of all extracted features for this work is provided in Appendix B. It should be noted that certain features overlap in terms of the information they encode; for instance, the impulse factor is the product of the crest and shape factors, while the standard deviation is the square root of the variance. This intentional redundancy allows the framework to identify the most relevant features automatically and discard the rest during the feature selection process. After all, if the most effective features for the studied problem were already known, the feature selection phase would be redundant."}, {"title": "4 Experimental Results", "content": "This section presents the experimental findings obtained by applying the proposed framework to the two constructed feature libraries, addressing three distinct tasks: fault detection, fault classification, and severity classification for each fault type. Exclusively shallow KANs, i.e., models with a single layer, were considered throughout the experiments to minimize the number of model parameters, thus ensuring that the models remain lightweight and their symbolic representation does not become overly complex.\nFor all tasks, the datasets were split in a stratified manner into training, validation, and evaluation sets in a 70%-15%-15% ratio, respectively, and features were standardized. Model training was performed with the Adam optimizer, using Cross Entropy as the non-regularizing loss function, as all tasks are classification problems. The primary performance metric was the F1-Score, chosen for its suitability in handling imbalanced datasets compared to accuracy. The KAN implementation and training were performed using the PyTorch (Paszke et al, 2019) and pykan (Liu et al, 2024b,a) frameworks.\nDuring the feature selection phase, KANs with k = 3, G = 5, and $\\gamma$ = 0.05 were trained non-adaptively for 80 epochs. The grid search spanned the ranges $[\\, \\lambda_{max}] \\times [\\tau_{max}] = [0.01, 0.1] \\times [0.001, 0.01]$, with each range including 20 equidistant values. If the Pareto front was not a singleton, the model achieving the highest F1-Score with up to 10 features was selected. This choice is strict, as most state-of-the-art models employ at least 15 features for these datasets.\nFor the model selection phase, higher-order KANs with k = 4 were used. Grid search was performed over G$\\in$ {8,10,12,15,20,30,40,50} and $\\gamma_e$$\\in$ {0.0, 0.05, ..., 1.0}. Each model instance was trained adaptively for 200 epochs, with the grid updated every 10 epochs until epoch 150. For the symbolic fitting's cost function of Eq. (12), a = 0.05 and $\\beta$ = 1.5 were chosen to prioritize R\u00b2 over complexity, except in cases of extremely high complexity where the exponential penalty dominates. If the resulting Pareto front contained more than one elements, the model with the highest average F1-Score between the regular and the symbolic representation was selected."}, {"title": "4.1 Fault Detection", "content": "For the fault detection task, all data samples were categorized into two classes: normal (N) and faulty (F), with the latter encompassing all fault types. Fault detection is generally simpler than fault classification, as the model only needs to distinguish between normal and anomalous data. However, this task suffers from a significant imbalance in class representation, which presents a major challenge.\nIn both the CWRU and MaFaulDa datasets, the normal class is severely under-represented. For the CWRU dataset, the normal class constitutes only 3.96% of the dataset, while the other classes range from 11.88% to 23.76%. Similarly, in the MaFaulDa dataset, the normal class accounts for just 2.51%, with the remaining classes spanning from 7.02% to 17.07%. When restructured for fault detection, the imbalance becomes even more pronounced, with the normal class constituting only 3.96% versus 96.04% for the faulty class in the CWRU dataset, and 2.51% versus 97.49% for the"}, {"title": null, "content": "faulty class in the MaFaulDa dataset. This extreme imbalance means that even a trivial classifier which predicts all entries as faulty, would achieve a high accuracy (e.g., 97.49% for MaFaulDa) while failing to provide any meaningful insights.\nTo address this imbalance, a balancing strategy is required. Among the common approaches are undersampling the dominant class or oversampling the minority class using techniques such as the Synthetic Minority Oversampling (SMOTE) (Irfan et al, 2023). For this work, undersampling was adopted; specifically, 30 samples from each fault class in the CWRU augmented dataset and 60 samples from each fault class in the MaFaulDa augmented dataset were randomly selected. This adjustment reduced the imbalance to 12.28% normal versus 87.72% faulty for CWRU and 22.86% normal versus 77.14% faulty for MaFaulDa. Although still imbalanced, these distributions are far more manageable.\nFollowing this preprocessing step, the framework's feature selection process, as detailed in Section 2, was applied to identify the most relevant features for fault detection. Regarding the CWRU dataset, the Pareto front resulting from the ($\\lambda$, $\\tau$) grid search was a singleton, yielding $\\lambda$ = 2.42\u00b710-3 and $\\tau$ = 7.16 \u00b7 10-2. These values resulted in the selection of a single feature, x24 (the 25th percentile value for the drive-end signal, see Appendix B). Proceeding to the model selection phase, the grid search using only this feature again produced a Pareto front with a single point, corresponding to G = 8 and $\\gamma_e$ = 0.0. The combination of a single feature and a small, yet fully adaptive grid, suggests that fault detection in the CWRU dataset is relatively straightforward, so approaching the task with a complex model is neither necessary nor a good practice. This is further corroborated by the final evaluation of the chosen model, for both the regular and symbolic version of the KAN, as shown in the confusion matrices of Fig. 4.\nThe selection of a single feature offers the opportunity to highlight the importance of extracting symbolic representations for the trained KAN, as they constitute the"}, {"title": null, "content": "model fully interpretable. In this case, the symbolic representations of the KAN's output edges are given by:\ny1 (x) = 42.62 \u2013 76.65\\sigma (7.69 \u2013 7.51x),\nand\ny2 (x) = 10.85 \u2013 52.3 tanh (10x \u2013 8.52),\nwhere x denotes the scaled feature and $\\sigma(x) = 0.5 [1 + tanh (x/2)]$ is the sigmoid function. It is noted that all numbers have been rounded to the second digit. Using"}, {"title": null, "content": "these analytical expressions, a sample is classified as Normal if y1 (x) > y2 (x), and as a Fault if y1 (x) < y2 (x). Eqs. (13) and (14) allow for the study of otherwise inaccessible (or hard to compute) properties of the classification problem, such as determining the decision boundary by solving y1 (x) = y2 (x). Fig. 5 illustrates the two curves alongside all CWRU data points, color-coded by class. The decision boundary is also depicted, demonstrating that all of the dataset's samples are correctly classified using these symbolic expressions.\nThe same procedure was applied to the MaFaulDa dataset. In this case, the feature selection process resulted in a Pareto front with three candidate points. The corresponding ($\\lambda$, $\\tau$) values, the associated F1-Scores for each point, and the number of features retained are presented in Table 1. Although the configuration with the fewest features also exhibited the lowest performance, it still achieved a remarkably high F1-Score of 97.07%. Notably, the four features selected in the lowest-performing case are a subset of the six features selected in the middle-performing case, which, in turn, are a subset of the nine features selected in the highest-performing case. This hierarchical relationship highlights the consistency of the framework. Following the selection rule of prioritizing the highest-performing configuration with no more than 10 features, the combination $\\lambda$ = 2.9\u00b710-3 and $\\tau$"}]}