{"title": "Automated Classification of Cybercrime Complaints using Transformer-based Language Models for Hinglish Texts", "authors": ["Nanda Rani", "Divyanshu Singh", "Bikash Saha", "Sandeep Kumar Shukla"], "abstract": "The rise in cybercrime and the complexity of multi-lingual and code-mixed complaints present significant challenges for law enforcement and cybersecurity agencies. These organizations need automated, scalable methods to identify crime types, enabling efficient processing and prioritization of large complaint volumes. Manual triaging is inefficient, and traditional machine learning methods fail to capture the semantic and contextual nuances of textual cybercrime complaints. Moreover, the lack of publicly available datasets and privacy concerns hinder the research to present robust solutions. To address these challenges, we propose a framework for automated cybercrime complaint classification. The framework leverages Hinglish-adapted transformers, such as HingBERT and HingRoBERTa, to handle code-mixed inputs effectively. We employ the real-world dataset provided by Indian Cybercrime Coordination Centre (I4C) during CyberGuard AI Hackathon 2024. We employ GenAI open source model-based data augmentation method to address class imbalance. We also employ privacy-aware preprocessing to ensure compliance with ethical standards while maintaining data integrity. Our solution achieves significant performance improvements, with HingRoBERTa attaining an accuracy of 74.41% and an F1-score of 71.49%. We also develop ready-to-use tool by integrating Django REST backend with a modern frontend. The developed tool is scalable and ready for real-world deployment in platforms like the National Cyber Crime Reporting Portal. This work bridges critical gaps in cybercrime complaint management, offering a scalable, privacy-conscious, and adaptable solution for modern cybersecurity challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "The exponential growth of the Internet usage, digital transactions, and online social interactions has led to a corresponding surge in cybercrimes worldwide. Threats ranging from financial fraud [7], [16], ransomware [15], [26], and cyberterrorism [25] to the proliferation of illicit content have become pervasive, placing immense pressure on law enforcement agencies and security analysts. Timely identification, classification, and prioritization of cybercrime complaints are now critical to effectively allocate investigative resources and prevent significant harm to individuals, organizations, and national infrastructures.\nRecent studies show a surge in cybercrime in India and globally. In 2023, India reported 129 cybercrimes per lakh population [11], and between January and April 2024, daily cybercrime complaints averaged 7,000, a 113.7% increase from 2021\u20132023 [17]. Globally, cybercrime costs are projected to rise by 15% annually, reaching $10.5 trillion by 2025 [21]. Manual triaging of cybercrime complaints remains a complex and resource-intensive process, with human analysts struggling to cope with an ever-increasing volume of reports. Automated approaches to classify and prioritize these complaints have, therefore, attracted considerable research attention.\nThe research in developing the automated model to understand the complaints has always struggled due to unavailability of public dataset of cyber crime complaints. Cybercrime complaints often contain sensitive and personal information, leading to stringent privacy concerns and limited data sharing. This scarcity of accessible datasets has hindered the development and evaluation of robust models, as researchers are unable to train and validate their approaches on diverse, real-world examples. Consequently, this gap restricts innovation and the ability to create universally effective solutions for automating and optimizing the handling of cybercrime complaints.\nAdditionally, early efforts relied on conventional machine learning (ML) techniques using Bag-of-Words or TF-IDF representations [2], [14], [18], [32]. However, these methods suffer from high dimensionality and increased complexity. They also fail to capture semantic nuances and context, especially in domains with specialized terminologies and complex scenario descriptions. For example, descriptions such as \"distribution of CSAM (Child Sexual Abuse Material)\" and \"sharing explicit images of minors\" represent the same criminal activities but use different terminologies. Bag-of-Words or TF-IDF treats these phrases as unrelated, failing to capture their semantic similarity and context.\nMoreover, in a country like India, cybercrime complaints are frequently composed in multilingual or code-mixed forms, posing an additional layer of complexity. In regions where Hindi and English coexist, Hinglish a fluid blend of Hindi and English commonly appears in user-generated content"}, {"title": "II. RELATED WORK", "content": "The increasing prevalence of cybercrime has prompted extensive research into automated crime type classification. Existing studies have explored various methods, and textual languages, but significant gaps remain, particularly in handling multilingual and code-mixed inputs. In this section, we discuss recent advancements and their limitations, setting the stage for the contributions of our work.\nSedik and Romadhony [33] proposed a system for crime information extraction from Indonesian crime news articles using a combination of Named Entity Recognition (NER) and machine learning models such as Support Vector Machines (SVM). While their work effectively categorizes crime types using TF-IDF features, it is limited by the reliance on hand-crafted features and predefined crime categories. Additionally, their approach does not address multilingual or code-mixed contexts, which are prevalent in many regions, including India.\nIslam et al. [20] focused on classifying Bangla crime news using supervised machine learning models. They use traditional feature extraction methods like CountVectorizer and TfidfVectorizer to prepare the feature vector. These methods struggle with capturing semantic and contextual nuances in textual data, particularly in scenarios involving complex linguistic structures like code-mixing. Furthermore, their reliance on Bangla-specific tools limits generalizability to other languages or mixed-language scenarios.\nRahma and Romadhony [12] employed a rule-based system for crime information extraction, combining dependency parsing and ontology-driven classification. Although this method achieves reasonable performance in extracting crime-related entities, it encounters significant challenges in adapting rules for language-specific complexities. This rigid rule-based framework is unsuitable for dynamic, code-mixed complaints and does not leverage recent advancements in deep learning for enhanced contextual understanding.\nPrabhu et al. [28] introduced a Cyber Complaint Automation System using the RAKE algorithm for keyword extraction and ERNIE for complaint classification. Their approach demonstrated the potential of integrating external knowledge for improved performance. However, it primarily focuses on predefined classes like fraud or hacking without addressing the inherent class imbalance in cybercrime datasets. Additionally, the lack of a multilingual or code-mixed capability reduces the system's applicability in diverse linguistic settings like Hinglish.\nPongpaichet et al. [10] introduced CAMELON, a methodology for classifying crime types based on online news articles annotated across seven distinct crime categories. Four deep learning models, including BiLSTM with Thai2Vec and transformer-based models like WangchanBERTa, MBERT, and XLM-ROBERTa, were fine-tuned for crime classification using binary cross-entropy loss and optimized training parameters. Out of all implemented models, XLM-ROBERTa achieves promising results. The method focus on news articles rather than complaints limits its real-world applicability to law enforcement systems. Moreover, the lack of integration with privacy-preserving mechanisms leaves concerns about ethical compliance and data security unaddressed.\nDespite these advancements, existing approaches face several limitations. Most studies are rely on word-level feature extraction methods like bag-of-words or TF-IDF, traditional machine learning models, or rule-based frameworks, which fail to capture the semantic and contextual intricacies of code-mixed complaints. Moreover, the lack of robust strategies to address class imbalance and ensure privacy compliance further restricts their real-world applicability.\nTo address these gaps, we propose a framework for automated cybercrime complaint classification. It leverages Hinglish-adapted transformers like HingBERT and HingRoBERTa to handle code-mixed inputs. A Generative AI-based data augmentation method addresses class imbalance, while privacy-aware preprocessing ensures ethical data handling. The framework achieves significant performance improvements and a tool based on this framework is designed for adoption by law enforcement agencies. The tool can be integrated with scalable tools to support platforms like the National Cyber Crime Reporting Portal. This work bridges key gaps, offering a practical and adaptable solution for modern cybersecurity challenges."}, {"title": "III. METHODOLOGY", "content": "Our overarching goal is to design a classification framework that can accurately and ethically process cybercrime complaints, many of which are written in code-mixed Hinglish and exhibit severe class imbalance. To achieve this, we integrate a pipeline of data preprocessing, language model adaptation, GenAI-based data augmentation, and fine-tuning strategies that collectively enhance the robustness, fairness, and applicability of the system. Figure 2 provides a high-level overview of our methodology."}, {"title": "A. Problem Formulation and Requirements", "content": "Given a dataset $D = \\{(x_i, Y_i)\\}_{i=1}^N$ of N cybercrime complaints, where each complaint $x_i$ is a Hinglish text snippet and $Y_i \\in \\{C_1, C_2,...,C_K\\}$ is one of K predefined classes, such as financial fraud or social media crime, we present a classification function $f : x \\rightarrow y$ that maximizes accuracy and F1-score across all categories. The presented solution also addresses practical constraints, including privacy awareness and seamless deployment."}, {"title": "B. Dataset Preparation and Splits", "content": "We obtain the real-world dataset from the Indian Cybercrime Coordination Center (I4C) during the CyberGuard AI Hackathon 2024. The dataset encompasses the total number 85,875 of crime complaints distributed in 14 crime categories, as shown in Table IV. The dataset contains two data sets: training and testing dataset. We divide 20% of the training data as validation set. The data set obtained is highly imbalanced, as shown in Table IV and contains complaints in the Hinglish. To handle imbalance data, we implement augmentation method on training dataset which we discuss in Section III-D."}, {"title": "C. Data Preprocessing", "content": "Effective data preprocessing is foundational for producing high-quality inputs that models can leverage. We begin by removing or adjusting problematic samples discovered through exploratory data analysis (EDA):\n1) Unknown and Rare Classes: Classes absent in the training set, such as Crime Against Women & Children found only in the test set, are excluded from the evaluation (step 1.a in Fig 2). Classes with fewer than 2 samples in the training set, such as Report Unlawful Content, are removed to ensure sufficient data for splitting into training and validation sets (step 1. in Fig 2).\n2) Handling Missing and Duplicate Data: Samples with critical fields, such as complaint description, containing NaN values or duplicates are removed to maintain dataset integrity and avoid skewed results (step 1.d and 1.e in Fig 2).\n3) Entity Anonymization: Cybercrime complaints often contain sensitive and personally identifiable information (PII), such as names, emails, and addresses. We replace these entities with placeholder tokens (step 1.b in Fig 2), such as <Name>, <Email>, and <Location>, to protect user privacy and ensure ethical compliance, enabling the model to learn generalizable patterns instead of memorizing unique entities. We use regex patterns to identify entities like phone numbers and emails and the Python library spacy to detect entities such as names, addresses, and organization names. The list of entities and corresponding placeholder is shown in Table II.\n4) Stopword Removal and Lemmatization: Common stopwords are removed to reduce noise, and words are lemmatized to their base forms for linguistic consistency."}, {"title": "D. Data Augmentation Method", "content": "One of the core challenges in our dataset is severe class imbalance, where a few dominant categories overshadow many minority classes. For example, 52,496 out of 85,875 samples belongs to Financial fraud. Conventional re-balancing techniques, such as naive oversampling or undersampling, often distort the data distribution or introduce artificial biases. Instead, we adopt a GenAI-based augmentation framework (step 2 in Fig 2) inspired by our recent work on contextual augmentation [6], [23], [24]. The architecture of augmentation method is shown in Fig. 3.\nThe process of data augmentation begins with preprocessing the dataset. Sensitive information, such as names, email addresses, and phone numbers, is replaced with placeholders like <Person Name>, <Phone Number> and other entities (step 2 in Fig 3) to ensure privacy and standardization. Once the data is prepared (step 3 in Fig 3), the Gen Al open-source model, LLaMA 3.1-7b, is used to generate augmented sentences (step (4) in Fig 3). The model creates paraphrased versions of the original sentences (step 6 in Fig 3) while maintaining their semantic and contextual meaning. Pre-processing steps such as removing excess spaces and new lines, segregating different examples as different augmented samples are performed (step \u2464 in Fig 3) to refine output sentences.\nAfter generating the augmented data, a similarity evaluation is performed (step \u2466 in Fig 3). Each augmented sentence is compared to the original using semantic similarity measures to ensure relevance and contextual accuracy. To perform this evaluation, we use the BERTF1-score metric, which converts both sentences into contextual embeddings using a BERT layer and calculates the cosine similarity between the embeddings. Sentences that deviate significantly from the intended meaning or introduce ambiguity are discarded based on 0 threshold (step 8) in Fig 3). For this experiment, we set a high threshold of 97% to ensure that only highly similar and relevant sentences are filtered and included in the augmented dataset. The selected augmented sentences are then added to the dataset, enhancing its diversity and robustness. This expanded dataset is validated to ensure its effectiveness in improving the performance of downstream tasks.\nBy following this augmentation process, we increase the dataset size from 85,875 to 1,09,294 data samples. The distribution of the base dataset and augmented dataset is shown in Table IV. The augmented dataset gains a more balanced class distribution, which allow the model to develop a more equitable understanding of all crime categories."}, {"title": "E. Model Selection: Transformer-based Language Models", "content": "Modern transformer-based language models are well-suited for tasks involving complex linguistic patterns and contextual dependencies. Traditional TF-IDF or Bag-of-Words representations fail to capture semantic relationships, especially in code-mixed text. We fine-tune four transformer-based models to capture semantic and contextual meaning for crime classification:\n\u2022 BERT [4]: A foundational bidirectional transformer model widely used for various NLP tasks."}, {"title": "F. Fine-tuning and Optimization", "content": "We initialize the model with pre-trained weights and fine-tune it on the preprocessed, augmented dataset. Key hyperparameters include the learning rate, tuned through grid search within the range of 1e-5 to 3e-5, and batch size as 8 to 32 and sequence length as 128 or 256 tokens to accommodate most complaints without truncation. We use the AdamW optimizer for stable optimization, along with early stopping to prevent overfitting by halting training if validation performance stagnates for five epochs. A validation set aids in hyperparameter tuning and model checkpointing. While traditional ML models rely on k-fold cross-validation for robustness, transformer models utilize a stable validation split to balance computational efficiency with performance. We also use early stopping to prevent overfitting, halting training if validation accuracy or F1-score doesn't improve for five epochs. This ensures balanced model complexity and reduces computational overhead."}, {"title": "G. Deployment Architecture", "content": "Beyond achieving good performance in a controlled experiment, we prioritize practical deployment. The final model is integrated into a Django REST-based backend that exposes RESTful APIs for classification requests. The frontend, built with modern JavaScript frameworks and Tailwind CSS, offers a user-friendly interface for operators to submit complaints and review automatically generated labels.\nThis production-ready architecture ensures scalability, monitoring, and easy integration with the National Cyber Crime Reporting Portal. The modular design allows for periodic model updates, retraining on new data, and the integration of more advanced models or additional categories as cybercrime patterns evolve."}, {"title": "H. Summary of the Methodology", "content": "In summary, our methodology strategically combines preprocessing, GenAI-based data augmentation, Hinglish-adapted transformer models, and careful optimization to build a robust and privacy-conscious classification system. By addressing code-mixing, class imbalance, and deployment concerns holistically, our approach delivers a practical, ethically responsible, and high-performing solution that can guide law enforcement efforts in navigating the complexities of modern cybercrime."}, {"title": "IV. EXPERIMENTS AND EXPERIMENTAL SETUP", "content": "In this section, we present the detailed configuration of our experiments, including the evaluation metrics, baseline setup, computational environment, and deployment configuration. Our objective is to ensure that all experiments are replicable, transparent, and sufficiently rigorous to draw meaningful conclusions about the proposed approach's capabilities."}, {"title": "A. Evaluation Metrics", "content": "Given the complexity and imbalance inherent in the data, we consider multiple metrics:\n\u2022 Accuracy: Offers a broad performance overview but may be misleading in heavily imbalanced scenarios.\n\u2022 Precision, Recall, and F1-Score: More informative measures for imbalanced classification. The F1-score, in particular, is critical because it harmonizes precision and recall, ensuring that good performance cannot be achieved by ignoring minority classes."}, {"title": "B. Baseline Models and Comparisons", "content": "Establishing robust baselines is essential for contextualizing performance gains. We compare our approach against:\n\u2022 Traditional Machine Learning Pipelines: We convert the text into TF-IDF features, apply dimensionality reduction via Singular Value Decomposition (SVD) to manage sparsity, and train models such as XGBoost, Random Forest, AdaBoost, and k-Nearest Neighbors. These pipelines serve as a reference point, showing how context-agnostic representations fare against our proposed methodologies.\n\u2022 Generic Transformer Models: BERT [4] and ROBERTa [5] provide strong baselines from the transformer family. Their performance reveals whether context-aware embeddings alone (without Hinglish adaptation) suffice for the classification task.\n\u2022 Hinglish-Adapted Transformers: HingBERT and HingRoBERTa [8] are tested to confirm whether pre-training on code-mixed data confers distinct advantages. Improved performance from these models would validate the notion that language-adaptation is critical for handling Hinglish inputs effectively.\nBy comparing baselines with specialized models, we assess the impact of code-mixed language modeling and GenAI-based augmentation on classification performance."}, {"title": "C. Hardware and Software Environment", "content": "All experiments were performed on high-performance GPU servers equipped with NVIDIA Tesla or V100 accelerators. Running transformer fine-tuning with batch sizes of up to 32 and sequence lengths up to 256 tokens was feasible on these resources. The software stack included:\n\u2022 Python 3.8 for scripting and orchestration.\n\u2022 PyTorch and the HuggingFace Transformers library for model implementation, fine-tuning, and inference.\n\u2022 spaCy and NLTK for text preprocessing, tokenization, lemmatization, and stopword removal.\n\u2022 Django REST Framework and modern JavaScript frameworks for deployment, enabling RESTful APIs and a user-friendly frontend interface."}, {"title": "D. Deployment Configuration", "content": "While much of the experimental setup focuses on model training and evaluation, equal attention is paid to deployment viability. After model selection, we integrate the best-performing model (HingRoBERTa) into a Django-based backend. RESTful endpoints allow seamless submission of complaints and retrieval of classification results. The frontend, implemented in JavaScript with Tailwind CSS, offers a responsive and intuitive platform for end-users.\nThis end-to-end integration ensures that the chosen best performing model is can immediately applicable in real-world, large-scale cybercrime reporting systems. Our architecture can be scaled or containerized like via Docker and Kubernetes to handle surges in user traffic and complaint volume, reinforcing the model's practical utility."}, {"title": "E. Summary of Experimental Setup", "content": "In summary, our experimental setup is designed to rigorously test the proposed framework under realistic conditions. By carefully preprocessing the dataset, adopting advanced augmentation strategies, tuning hyperparameters, and leveraging state-of-the-art transformer models\u2014both generic and Hinglish-specific\u2014we create a robust experimental protocol. The diversity of baselines, metrics, and analyses ensures that improvements are attributable to the innovations introduced in this work, laying a strong foundation for the subsequent presentation and interpretation of results."}, {"title": "V. RESULTS AND EVALUATION", "content": "In this section, we thoroughly examine the performance of our proposed framework, beginning with a comparative analysis of baseline methods and progressively moving to transformer-based models, including Hinglish-adapted variants. We then discuss the impact of data augmentation and reflect on the implications of findings in a broader context."}, {"title": "A. Performance of Traditional ML Models", "content": "We first evaluate the traditional ML pipelines built upon TF-IDF representations and dimensionality reduction (SVD), coupled with classifiers such as XGBoost, Random Forest, AdaBoost, and k-Nearest Neighbors (k-NN). These results, summarized in Table V, set the stage for understanding the limitations of context-agnostic representations in this domain."}, {"title": "B. Performance of Transformer-based Models", "content": "We next turn our attention to transformer-based language models. Table VI presents the performance of BERT, ROBERTa, HingBERT, and HingRoBERTa. By comparing generic models (BERT, ROBERTa) with Hinglish-specialized ones (HingBERT and HingRoBERTa), we can directly measure the value of linguistic adaptation. Several key insights arise from these results:\n1) Contextual Embeddings vs. TF-IDF: All transformer models significantly surpass traditional ML pipelines, validating the importance of contextual embeddings for understanding nuanced textual data.\n2) Hinglish Adaptation: HingBERT and HingRoBERTa outperform BERT and ROBERTa, respectively, confirming that pre-training on Hinglish corpora confers a tangible advantage. The code-mixed nature of the input is better handled by models familiar with Hinglish language patterns, vocabularies, and syntactic structures.\n3) Top Performer\u2014HingRoBERTa: Among all tested approaches, HingRoBERTa attains the highest accuracy (74.41%) and F1-score (71.49%). This improvement, albeit a few percentage points, can be meaningful in real-world operations, potentially improving the timely identification of critical yet less frequent crime types."}, {"title": "C. Impact of GenAI-based Data Augmentation", "content": "The significant improvement from baseline models to HingRoBERTa is partly attributable to the GenAI-based augmentation applied to minority classes. By synthesizing contextually consistent examples, we bolstered the model's exposure to underrepresented categories. Without augmentation, minority classes would remain overshadowed, leading to poor recall and consequently lower F1-scores.\nTo validate this, we conducted a controlled experiment by training HingRoBERTa without augmentation. The resulting model showed weaker performance, especially on minority classes, indicating that augmentation was crucial for improved class balance and model generalization."}, {"title": "D. Practical Considerations and Scalability", "content": "While our experiments focused on controlled datasets and well-defined classes, the ultimate test of these models lies in their deployment in real-world environments. The top-performing model (HingRoBERTa) is integrated into a Django REST backend with a modern JavaScript frontend, allowing law enforcement agencies to process complaints more rapidly. This practical integration ensures that the documented performance gains are not merely academic but directly transferrable to operational settings.\nMoreover, as new complaint data accumulates and cybercrime patterns evolve, the system's architecture allows for periodic retraining and augmentation. This ensures long-term adaptability and scalability key attributes for sustaining efficacy in a constantly shifting threat landscape."}, {"title": "E. Connecting to Current NLP and Cybercrime Research Trends", "content": "Our work aligns with and extends current research trajectories in several ways. First, it confirms that code-mixed and low-resource language scenarios benefit from specialized language models an area of intense interest in NLP [9], [22]. Second, it provides a concrete demonstration of how augmentation can mitigate imbalances, an ongoing challenge in many domain-specific text classification problems. Third, by systematically addressing privacy concerns, this study resonates with recent calls for responsible NLP, where ethical frameworks and technical safeguards must co-evolve to protect user data [27].\nThe success of our methodology also suggests a potential synergy between emerging LLMs and domain adaptation. As more powerful pre-trained models, such as GPT-based architectures become feasible to fine-tune, we can anticipate even greater gains in handling complex, code-mixed inputs. Furthermore, innovative approaches like instruction-tuning or multimodal integration, such as combining textual data with metadata or network graphs of cybercrime events could enhance the depth and quality of automated cybercrime classification."}, {"title": "F. Summary of the Findings and Discussion", "content": "In summary, our results reflect that:\n\u2022 Contextual embeddings from transformer-based models substantially outperform traditional ML pipelines.\n\u2022 Hinglish adaptation is critical for handling code-mixed complaints, with HingRoBERTa emerging as the clear winner.\n\u2022 GenAI-based augmentation effectively combats class imbalance, lifting recall on underrepresented categories.\n\u2022 Privacy-conscious anonymization methods enhance privacy and support generalization.\nIn essence, our discussion underlines that improved performance in cybercrime complaint classification stems not from a single \"silver bullet\" but from a combination of carefully orchestrated measures. Hinglish adaptation tackles linguistic barriers, GenAI-based augmentation rectifies imbalances, privacy safeguards uphold ethical standards, and a modular deployment strategy ensures real-world applicability.\nAs we chart the path forward, each of these components can be refined and expanded. The insights gleaned here pave the way for more nuanced, ethically responsible, and context-aware AI systems that stand ready to support law enforcement agencies, analysts, and other stakeholders in tackling the ever-evolving landscape of cybercriminal activity."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This work presented a comprehensive framework for automated classification of cybercrime complaints a task made challenging by linguistic code-mixing (Hinglish), severe class imbalance, and the presence of sensitive personal data. By leveraging Hinglish-adapted transformer-based models, particularly HingRoBERTa, and integrating GenAI-based open source LLama3.1-7b model for augmentation, we significantly outperform both traditional machine learning baselines and generic transformer architectures. Achieving an accuracy of 74.41% and an F1-score of 71.49%, the proposed approach underscores the importance of language adaptation, robust data augmentation, and privacy-aware preprocessing in real-world NLP applications.\nA key insight from this study is that ethical considerations, such as anonymizing personally identifiable information, need not come at the expense of model performance. Instead, focusing on linguistic and semantic cues enables both strong generalization and compliance with data protection standards. Furthermore, the successful deployment of our best-performing model into a production-ready environment illustrates the practical viability of the system. This architecture allows for seamless integration with large-scale platforms, providing immediate value to stakeholders like law enforcement agencies and cyber-response teams.\nDespite these advancements, there remains ample scope for further improvement. One promising direction is the integration of large language models (LLMs) with instruction-tuning or retrieval-augmented strategies to better handle ambiguous or evolving cybercrime patterns. Additionally, exploring hierarchical classification schemes could yield finer-grained category distinctions, enhancing investigative workflows. Incorporating external knowledge bases or contextual metadata such as temporal or geographic information may further enrich the model's interpretive power.\nIn essence, this study lays a robust foundation for adaptive, high-fidelity cybercrime classification. By continuously evolving our models and methodologies integrating more sophisticated language models, richer data, and nuanced classification frameworks we can better align with the shifting cyber-threat landscape. This ongoing refinement will ultimately foster more timely, informed, and ethical responses to cybercrime, empowering authorities to address digital threats with greater agility and confidence."}]}