{"title": "NVR: Vector Runahead on NPUs for Sparse Memory Access", "authors": ["Hui Wang", "Zhengpeng Zhao", "Jing Wang", "Yushu Du", "Yuan Cheng", "Bing Guo", "He Xiao", "Chenhao Ma", "Xiaomeng Han", "Dean You", "Jiapeng Guan", "Ran Wei", "Dawei Yang", "Zhe Jiang"], "abstract": "Deep Neural Networks are increasingly leveraging sparsity to reduce the scaling up of model parameter size. However, reducing wall-clock time through sparsity and pruning remains challenging due to irregular memory access patterns, leading to frequent cache misses. In this paper, we present NPU Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address cache miss problems in sparse DNN workloads. Rather than optimising memory patterns with high overhead and poor portability, NVR adapts runahead execution to the unique architecture of NPUs. NVR provides a general micro-architectural solution for sparse DNN workloads without requiring compiler or algorithmic support, operating as a decoupled, speculative, lightweight hardware sub-thread alongside the NPU, with minimal hardware over- head (under 5%). NVR achieves an average 90% reduction in cache misses compared to SOTA prefetching in general-purpose processors, delivering 4x average speedup on sparse workloads versus NPUs without prefetching. Moreover, we investigate the advantages of incorporating a small cache (16KB) into the NPU combined with NVR. Our evaluation shows that expanding this modest cache delivers 5x higher performance benefits than increasing the L2 cache size by the same amount.", "sections": [{"title": "I. INTRODUCTION", "content": "Emerging Deep Neural Networks (DNNs), particularly Large Language Models (LLMs), often scale to hundreds of billions of parameters [1], increasingly consuming more storage, memory band- width, and computational resources. Fortunately, these workloads are typically over-parameterised [2], where up to 90% of parameters in prevalent models can be pruned while maintaining comparable performance [3]. This redundancy presents an opportunity to leverage sparsity to reduce such intensive resource demands.\nTheoretically, more fine-grained sparsity patterns yield higher acceleration by skipping more zero-valued elements. However, as shown in Fig. 1 a, these patterns introduce unstructured charac- teristics when directly skipping zero values randomises vector access order, leading to irregular memory access patterns. These irregular patterns frequently result in cache misses, which not only waste main memory bandwidth but also degrade NPU performance. Taking sparse KVCache in LLM workloads [4] as an example, Fig. 1b illustrates that despite a 16x parameter reduction, frequent cache misses lead to extensive main memory interactions. This results in only a 5x actual speedup, substantially below theoretical expectations. Cache misses thus emerge as a key barrier to sparse workloads.\nExisting Work. Recent research has focused on mitigating the side effects of fine-grained sparsity. Farshchi et al. [5] propose a bitmask sparse data format in NVDLA, while Eyeriss [6] design a run-length encoding for non-zero elements. Albericio et al. [7] introduce additional mapping algorithms, regularising memory access patterns through label cache mechanisms. Meanwhile, Liu [8] et al. explored sparse domain compute units, developing a butterfly-based architecture for efficient sparse data structure processing. Whilst most of these approaches attempt to regularise memory access patterns to avoid cache misses, they often introduce substantial area and control logic overhead that outweighs their benefits; therefore, the practical applicability of these algorithm-specific are significantly restricted.\nContributions: Different from all previous works, we tackle the sparsity challenge from a different perspective. Instead of regularising irregular memory access patterns, our approach embraces the inher- ent characteristics of sparse computations. We leverage prefetching techniques directly to mitigate the impact of cache misses, rather than attempting to eliminate the irregularity itself, which incurs high overhead and lacks generality. We propose a vector runahead"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Although sparse workloads offer high theoretical memory-level parallelism (MLP), hardware systems struggle to exploit this algorith- mic advantage effectively. As shown in Fig. 1b, we systematically profile the parameters of Double Sparsity [4] in LLMs workload and its performance on NPU with a 256KB L2 cache. We find that reducing parameters does not lead to proportional decreases in off- chip memory accesses, resulting in out-of-bounds accesses for explicit buffers (like scratchpad) or cache misses for L2 cache in the chip. As these two scenarios are essentially identical, we focus our analysis on cache misses in the following discussion."}, {"title": "A. Sparse Memory Accesses: Misses Are a Fact of Life", "content": "In this section, we introduce the sparse computation and analyse the sparse irregular memory access patterns, which lead to a heavy cache miss and NPU stalls. As an example, Fig. 2 (right) illustrates a typical sparse matrix multiplication (SpMM), which is common in DNNs, computation with compressed sparse row (CSR) format. The code snippet demonstrates the sparse weight matrix (W) selectively indexes input activation (IA) vectors for multiply-accumulate operations. This workload exhibits several critical factors contributing to cache misses:\nIndirect Memory Accesses Since only non-zero data is computed, the data must be aligned, which involves numerous index dependency chains. While W [j] accesses are often sequential and can be captured"}, {"title": "B. Cache-miss Vulnerable NPUs", "content": "Facing numerous cache misses, NPUs suffer even more severe performance degradation than CPUs and GPUs. While CPUs leverage Out-of-Order (OoO) execution and reorder buffers (ROB) to tolerate this through fine-grained instruction-level overlap, and GPUs utilise thread-level scheduling for cache misses hiding, NPUs remain limited in such capabilities. Built primarily on Single Instruction Multiple Data (SIMD) architectures, NPUs possess only coarse-grained in- struction parallelism, with compute and memory movement channels decoupled. As demonstrated in Fig. 5 in the experiment, even with ideal OoO execution on NPUs, it also shows suboptimal performance in handling cache misses. Critically, the data-parallel nature of NPUs means a cache miss in any vector element stalls the entire processing pipeline, leading to performance degradation on sparse workloads.\nNPU memory access overlap is constrained by explicit preload mechanisms and structured scratchpad usage. The preloading process, which requires executing complete load instructions, involves exten- sive computations and full dependency chains, making it difficult to effectively hide memory latency, particularly in IO-bound scenarios.\nWhile Han et al. [11] proposed converting explicit memory into a cache, our experiments find this approach insufficient. In contrast, NVR's speculative execution allows flexible prefetching without the need for precise dependency calculations, reducing latency."}, {"title": "C. Challenge for NPU Prefetch", "content": "There are many conventional prefetchers designed for handling ir- regular memory access patterns mentioned in section II-A. For sparse workloads with abundant irregular memory accesses, simple pattern- based [12]\u2013[14] or history-based [15]\u2013[17] prefetchers often fail to effectively capture and predict these accesses, whereas runahead- based [9], [18], [19] prefetching has been widely adopted as the solution. Runahead employs speculative execution to hide cache miss latency by prefetching future memory accesses. VR [20] and DVR [10] explore leveraging vector units to perform runahead in parallel, which is a well-suited approach for NPUs that inherently support vector operation instructions. However, applying these techniques directly to NPUs presents several challenges.\nDespite the success of these prefetching techniques in general- purpose processors, the unique characteristics of NPU architectures and their workloads necessitate a fundamentally different approach. Diverse sparse workloads on NPUs often require customised hard- ware implementations, such as hash-table-based methods in point cloud processing. Meanwhile, sparse workloads commonly leverage specialised sparse data formats, like TACO's [21] multi-dimensional encoding and SMASH's [22] hierarchical bitmap-based decom- pression, handled by dedicated processing units. This diversity of dependency chains, coupled with the need for parallel execution, poses significant challenges for prefetchers in both pattern capture and overhead. A generalised prefetching approach decoupled from specific sparse computation patterns is necessary. Moreover, NPUS operate with coarse-grained instructions that process entire vectors or matrices. The decoding of such coarse-grained instructions implies weaker locality awareness and significant computational overhead, making traditional instruction-level optimisations less applicable."}, {"title": "III. NVR: DESIGN PHILOSOPHY", "content": "With these challenges in mind, we propose an adaptation of runahead tailored for NPU architectures, addressing three key aspects:\nDecoupled and Non-Invasive Philosophy: Throughput is the pri- mary metric for NPUs. To maximise it, prefetching mechanisms should avoid introducing additional control logic that stalls execution, ensuring maximum data parallelism [23]. Our design decouples from NPU computation logic, allowing simultaneous speculative execution to proactively generate memory requests. The system maintains non-invasive integration by passive monitoring the state and extracting load instruction information through read-only operations. This ap- proach allows early initiation of memory requests without interfering with NPU execution or custom instruction requirements.\nCoverage-Oriented Philosophy: In NPU vector operations, compu- tation can proceed only when all data in the batch are ready. Our experiments shown in Fig. 8 (a) reveal that the overall cache miss rate decreases significantly faster than per-batch cache miss rates. While optimising per-batch cache misses is more challenging, it's crucial for substantial performance improvement. Consequently, our design prioritises complete batch retrieval, accepting some prefetch"}, {"title": "Micro-Instruction-Level Vectorisation Philosophy", "content": "NPUs' SIMD architecture operates with vectorised instructions, which typically could be decomposed into multiple micro-instructions spanning sev- eral cycles. This fine-grained approach enables precise handling of cache misses while detecting stronger memory access patterns in this granularity. Leveraging NPUs' native support for vector load instructions, we can bundle memory addresses without additional execution units, improving both prediction accuracy and MLP.\nIn summary, vector runahead on NPU has several unique features. We can summarise our runahead design into three Q&A and introduce detailed micro-architecture in the next session:\nQ&A1. When to enter runahead mode: Enter runahead when a load instruction in the NPU's ROB executes, prefetching for the next load instruction in the reservation station. NVR executes in parallel with the NPU instruction stream, extracting information through snooping to generate vectorised memory requests ahead of NPU execution. By computing approximate boundaries in advance, it enables early issuance of vector load instructions.\nQ&A2. Where to execute NVR: The NVR is integrated between the CPU and NPU, prefetching speculatively. To achieve decoupled and non-invasive operation, the prefetching logic is architecturally separated from the NPU pipeline.\nQ&A3. How to prefetch: NVR leverages idle computational re- sources in NPU's sparse processing units to perform approximate dependency chain calculations in parallel. Subsequently, NVR gen- erates native NPU vectorised load instructions for the NPU pipeline, prefetching data into the L1/L2 cache hierarchy. This approach efficiently exploits NPU's vector instructions and architectural char- acteristics for effective prefetching and analysis."}, {"title": "IV. NVR MICRO-ARCHITECTURE", "content": "We use Gemmini [24] as our baseline to demonstrate NVR's appli- cability to general NPUs, as it embodies typical NPU architecture. Its coarse-grained design inherently struggles with cache miss handling, making it ideal for demonstrating NVR's design and effectiveness."}, {"title": "A. Overall Architecture", "content": "Fig. 3 illustrates NVR's micro-architectural modifications to the baseline Gemmini configuration, which comprises an in-order core and DNN accelerator sharing a unified L2 cache. The original Gem- mini does not support sparse computation, so we have incorporated a basic implementation of the sparse function to demonstrate our prefetcher's design. As illustrated in Fig. 3b, the sparse unit is primarily designed to handle alignment, skipping, and tiling of sparse data, three processing techniques discussed in Section II-A.\nTo support NVR, we augment the baseline with the following struc- tures (purple blocks): A snooper unit monitoring the status of the CPU and the NPU (Fig. 3 a). A stride detector (SD) that tracks stream memory access patterns (Fig. 3b). A loop boundary detector (LBD) that performs predictive analysis of iteration bounds for both unrolled and nested loop structures (Fig. 3). A sparse chain detector (SCD) identifies indirect memory access dependency chains and computes their corresponding memory addresses (Fig. 3 d). A vectorisation micro-instruction generator (VMIG) that bundles prefetch sequences with related access patterns into vectorised prefetch operations, opti- mising bandwidth utilisation (Fig. 3). An optional non-blocking speculative buffer (NSB), which serves as a small in-NPU cache equipped with miss status holding registers (MSHRs) (Fig. 3).\nThe steps of the NVR startup and execution process are numbered with red circles in Fig. 3. The NVR initiates runahead execution upon detecting new load instructions in execution within the NPU's ROB. The NVR's controller monitors CPU and NPU states via snoopers and triggers runahead request for speculative computations during NPU sparse unit idle periods. During execution, the stride detector captures patterns at index i to predict subsequent W[i] values. The SCD supports this by generating indirect access address generation for IA [sparse_func(W[i])] predictions. To optimise prefetch operations, the LBD analyses loop structures to prevent overruns and enables request packing. The VMIG reconstructs decomposed micro-instructions by synthesising information from these components and generates new vectorised prefetching instructions inserted into the NPU pipeline for prefetching operations. As a complementary approach, the NSB offers an optional mechanism to enhance the prefetching strategy, demonstrating a reduction in cache misses ex- perimentally. The following sections detail these components' design."}, {"title": "B. Stride Detector (SD)", "content": "The stride detector is a fundamental pattern recognition unit within the NVR design, tasked with predicting the next batch of addresses for W[i]. Its operation relies on identifying consistent striding memory access patterns to facilitate efficient stream prefetching. The stride detector employs a reference prediction mechanism to track the progression of addresses, similar to reference prediction tables in traditional stride prefetchers. It identifies memory patterns by keeping track of the previous address, stride size, and other control parameters. By leveraging these records, the stride detector predicts subsequent addresses accurately, especially when handling repetitive patterns in workloads. In neural network workloads, such as travelling W[i], the typical structure results in relatively fewer branch mispredictions, allowing for high prediction accuracy. This is further enhanced"}, {"title": "C. Snoopers and Controller", "content": "The snoopers are non-invasive probes used to precisely extract the architectural states of both the CPU and NPU. We monitor three critical signal types: (1) branch instructions from the CPU, which provide LBD with nested loop context information, (2) custom load-related instructions in the NPU, which are used to determine the optimal timing for runahead mode activation, and (3) NPU sparse unit registers, which supply the metadata essential for the NVR prediction mechanisms. The read-only, non-invasive design of the snoopers maintains architectural integrity by preventing any modifications to NPU computational logic and avoiding interference with the interactions between the CPU and NPU. Upon entering runahead mode, the controller sends speculative execution requests to the sparse unit. When runahead mode is triggered, the controller sends speculative execution requests to the sparse unit, enabling it to proceed with predictive computations. The system then monitors the unit availability, using the snooper infrastructure to retrieve the necessary sparse unit data as soon as it becomes available."}, {"title": "D. Sparse Chain Detector (SCD)", "content": "The SCD identifies and predicts patterns in sparse computations through two critical components: historical information preservation and indirect prefetching pattern learning. An Indirect Pattern Table (IPT) is maintained to record indirect prefetch patterns, including details such as the Last Prefetch Indirect (LPI) and the sparse structure's start address. In DNN workloads, computation index patterns typically exhibit locality characteristics. Due to the large volume of data processing, indirect index patterns remain relatively stable over time intervals, often appearing as shallow indirect chains, as formulated below:\n$IA_{address} = IA_{ss\\_start} + (W_{LPI} << stride)$"}, {"content": "The SCD identifies and predicts patterns in sparse computations through two critical components: historical information preservation and indirect prefetching pattern learning. An Indirect Pattern Table (IPT) is maintained to record indirect prefetch patterns, including details such as the Last Prefetch Indirect (LPI) and the sparse structure's start address. In DNN workloads, computation index patterns typically exhibit locality characteristics. Due to the large volume of data processing, indirect index patterns remain relatively stable over time intervals, often appearing as shallow indirect chains, as formulated below:\n$IA_{address} = IAssessstart + (WLPI << stride)$"}, {"content": "The SCD identifies and predicts patterns in sparse computations through two critical components: historical information preservation and indirect prefetching pattern learning. An Indirect Pattern Table (IPT) is maintained to record indirect prefetch patterns, including details such as the Last Prefetch Indirect (LPI) and the sparse structure's start address. In DNN workloads, computation index patterns typically exhibit locality characteristics. Due to the large volume of data processing, indirect index patterns remain relatively stable over time intervals, often appearing as shallow indirect chains, as formulated below:\n$IA_{address} = IAssessstart + (WLPI << stride)$"}, {"title": "E. Loop Bound Detector (LBD)", "content": "DNN workloads inherently consist of multiple nested and un- rolled loops, primarily involving matrix and vector operations, where higher-dimensional operations naturally require deeper loop nesting structures. To address the challenges of memory access patterns in these structures, the LBD employs systematic loop behaviour and boundary tracking to optimise prefetch sizing while preventing boundary-crossing invalid prefetches.\nAs illustrated in the listing of Fig. 2, nested loops typically occur during the traversal of rows and columns of a matrix, while unrolled loops are often used in parts of matrix multiplication where multiple indirect chains are executed in parallel. As depicted in Fig. 3 (b), LBD maintains historical information through a Sparse Structure Table (SST), where each parallel port has a dedicated entry to facilitate concurrent tracking of sparse chains. LBD implements a dual-mode boundary prediction, handling both sparse and normal boundaries. The mechanism distinguishes loop hierarchies from inner to outer levels through entry IDs, with each entry maintaining comprehensive loop instruction information, including PC value, boundary values, and operational mode. For standard loop boundaries (illustrated in line 1, Fig. 2 (right)), LBD captures historical boundary informa- tion by monitoring register values of jump instructions, as shown with RISC-V B-type branch instructions in Fig. 3 (b). For variable boundaries in sparse computations (shown in line 2, Fig. 2 (right)), boundary information is dynamically acquired through snoopers from sparse unit registers. Within this framework, upon detecting loop instructions, the LBD not only uses historical data to predict the boundary but also learns loop boundaries by examining input register values from comparison instructions. At the same loop level, instruc- tions are consolidated into memory access requests, while boundary values act as crucial constraints to prevent excessive prefetching."}, {"title": "F. Vectorisation Micro-Instruction Generator (VMIG)", "content": "In NPU-based sparse data processing, the efficiency of the SIMD pipeline is often compromised because traditional vector instructions fail to fully utilise memory bandwidth when dealing with skipped or zero-valued elements. Strictly following NPU runtime loading"}, {"title": "G. Non-blocking Speculative Buffer (NSB)", "content": "For discrete data structures, utilising scratchpad memory incurs substantial logic overhead for data transformation and retrieval op- erations. However, the characteristics of sparse data patterns present unique opportunities for cache-based optimisation. By strategically storing sparse discrete data in the cache while maintaining continuous data in scratchpad memory (e.g., dense vectors in one-side-sparsity), we can leverage implicit cache line reuse patterns. This cache-based approach naturally accommodates irregular reuse patterns without the costly pre-computation overhead required by scratchpad im- plementations. To exploit these characteristics, we introduce NSB, a compact non-blocking cache architecture optimised for discrete element management. Sparse workloads exhibit irregular memory access patterns with extensive index spaces at high sparsity, direct-mapped or low-associativity configurations frequently encounter con- flict misses. Thus, we implement a high-way set-associative mapping strategy. While NSB cannot mitigate L2 cache misses during prefetch operations (as data inherently resides off-chip), it significantly reduces NPU-to-L2 latency and off-chip memory accesses during actual load instruction execution. Experimental results demonstrate that NSB further reduces bandwidth requirements by 5x compared to baseline NVR prefetching. The design incorporates an MSHR file to manage concurrent memory operations, enabling tracking of outstanding load buffer requests and cache misses. This MSHR infrastructure coalesces multiple outstanding requests to the same cache line, eliminating redundant memory accesses and optimising bandwidth utilisation."}, {"title": "H. Hardware Overhead", "content": "Table II reports NVR's hardware overhead, where N determines the number of parallel entries, matching the vector processing width (default N=16). The storage overhead is merely 9.72 KiB, negligible relative to the NPU footprint. We implement it in RTL and synthesise our design using Synopsys Design Compiler on TSMC 28nm process technology at 2.0 GHz. The area overhead is 3% and 4.6% relative to the baseline Gemmini architecture, for configurations without and with NSB, respectively. These results demonstrate that NVR achieves its performance benefits with minimal hardware cost."}, {"title": "V. EVALUATION", "content": "A. Experimental Setup\nWe implement and evaluate NVR by integrating it into a Gemmini- like NPU model constructed using the ScaleSim simulator [33]. Additionally, we leverage LLMCompass [34], a specialised simulator designed to evaluate hardware optimisations in LLM inference.\nComparison We evaluate our approach against in-order Gemmini (serial execution of load and compute instructions), ideal out-of-order Gemmini (overlapping the load and computation time), and several prefetchers. We evaluate the following prefetch techniques: 1) stream prefetcher [35], the simplest prefetch mechanism based on stride; 2) IMP [36], one of the SOTA indirect memory prefetchers, prefetching indirect memory access at the L1 cache; 3) DVR [20], one of the SOTA runahead techniques, vectorising the same chain of indirect memory accesses across multiple invocations of an inner loop. 4) Our NPU Vector Runahead. For a fair comparison, we add the Gemmini custom ISA support for these prefetchers and expand them to the same number of parallels as the NVR.\nWorkload Sparsity is widely present across various workloads in DNNs. We select several key sources of sparsity, including attention mechanisms [37], [38], MoE structures [39], [40], 3D point cloud processing [41], [42], and so on. Table II presents representative workloads extracted from various models' linear layer memory access patterns based on their functionalities and domains."}, {"title": "B. Performance Breakdown", "content": "Since DNNs commonly employ low bit-width computations, we evaluate different bit-width configurations (INT8, FP16, INT32) to analyse their impact on cache behaviour and system performance, as shown in Figs. 5 (a), (b), and (c). Higher data bit-width inherently requires longer data movement time and larger cache block capacities, increasing the probability of cache misses.\nOverall, cache miss stalls represent a substantial portion of wall-clock time across most sparse workloads. The Switch Transformer"}, {"title": "C. Prefetching Effectiveness and Off-Chip Bandwidth Reduction", "content": "This experiment evaluates prefetcher performance across sparse workloads using two key metrics: accuracy and coverage. NVR demonstrates consistently high performance, maintaining both accu- racy and coverage rates above 90% across most workloads. As shown in Fig. 6 (b), coverage, which is more crucial for NPU performance, presents a greater challenge than accuracy. Benefiting from our fuzzy prefetch strategy, NVR also achieves significantly higher performance in this metric compared to other prefetchers.\nThe combined effect of accuracy and coverage directly influences prefetch bandwidth and the volume of off-chip memory accesses caused by cache misses during NPU execution. Figs. 7 (a) and (b) demonstrate the total bandwidth, including prefetch bandwidth. In both cases, compared to the InO baseline, the off-chip memory bandwidth is reduced by around 75%. Since the prefetch bandwidth is often more easily overlapped, Fig. 6 (c) shows the NPU actual load execution time with the memory accesses bandwidth removed. NVR can effectively reduce the off-chip memory accesses by 30x, and with NSB, it can be further reduced by 5x."}, {"title": "D. LLM Inference Evaluation", "content": "To validate our system-level optimisations, we evaluate the impact of NVR on the performance of LLMs. The experiments are conducted using the LLMCompass simulator. As shown in Fig. 8 (a), the NVR implementation reduces overall latency by decreasing cache miss stall time in the LLMs. Under NVR, both overall and per-batch cache miss rates decrease exponentially, with the latter showing a slower decay rate. LLMs are typically composed of two main stages: prefill and decode. It has been proven that the prefill stage is compute-bound, while the decode stage is IO-bound. Figs. 8 (b) and (c) demonstrate the benefits of applying NVR to the entire LLM network.\nIt is evident that for the compute-bound prefill stage, NVR can expedite reaching maximum throughput, particularly in low- bandwidth scenarios. For the IO-bound decode stage, the NVR architecture demonstrates an average 50% throughput improvement through reduced off-chip memory accesses. This enhancement be- comes increasingly pronounced with longer output sequences."}, {"title": "E. Sensitivity Analyses", "content": "To evaluate NSB's efficacy, we conduct comprehensive sensitivity analyses across varying NSB and L2 cache parameters. The perfor- mance metrics represent the inverse relationship between latency and area, calculated as the product of NSB and L2 Cache dimensions, ensuring equivalent area penalties for both scaling approaches. As illustrated in Fig. 9, modest NSB buffer configurations yield substan- tially higher performance improvements than equivalent L2 cache scaling. Specifically, in a configuration with 256KB L2 cache and 4KB NSB, quadrupling the NSB capacity delivers 5x the performance benefit over scaling the L2 cache to 1024KB."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present NVR, a runahead prefetching mechanism addressing the challenges of sparse DNN workloads on NPUs. This work demonstrates specialised prefetching techniques for DNN appli- cations on custom architectures beyond general-purpose processors.\nLessons we learnt. This work adopted a holistic, workload-driven approach to accelerate the DNNs' executions. By workload profiling and architectural analysis, key bottlenecks were identified, guid- ing targeted system re-architecture. Instead of simply adding more hardware resources, this work addressed fundamental inefficiencies, achieving performance gains equivalent to multiple times the benefits of hardware scaling, but with significantly reduced area and power costs. The constructed NVR provides key insights that precise, interdisciplinary optimisations rooted in a deep understanding of workload-architecture interactions. We believe that such analysis of- fers valuable insights into bridging architectural and machine learning research, fostering more effective and sustainable design strategies."}]}