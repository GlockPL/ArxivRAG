{"title": "H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving", "authors": ["Siran Chen", "Yuxiao Luo", "Yue Ma", "Yu Qiao", "Yali Wang"], "abstract": "With the prevalence of Multimodal Large Language Mod-\nels(MLLMs), autonomous driving has encountered new op-\nportunities and challenges. In particular, multi-modal video\nunderstanding is critical to interactively analyze what will\nhappen in the procedure of autonomous driving. However,\nvideos in such a dynamical scene that often contains com-\nplex spatial-temporal movements, which restricts the gener-\nalization capacity of the existing MLLMs in this field. To\nbridge the gap, we propose a novel Hierarchical Mamba\nAdaptation (H-MBA) framework to fit the complicated mo-\ntion changes in autonomous driving videos. Specifically, our\nH-MBA consists of two distinct modules, including Context\nMamba (C-Mamba) and Query Mamba (Q-Mamba). First, C-\nMamba contains various types of structure state space mod-\nels, which can effectively capture multi-granularity video\ncontext for different temporal resolutions. Second, Q-Mamba\nflexibly transforms the current frame as the learnable query,\nand attentively selects multi-granularity video context into\nquery. Consequently, it can adaptively integrate all the video\ncontexts of multi-scale temporal resolutions to enhance video\nunderstanding. Via a plug-and-play paradigm in MLLMs, our\nH-MBA shows the remarkable performance on multi-modal\nvideo tasks in autonomous driving, e.g., for risk object de-\ntection, it outperforms the previous SOTA method with 5.5%\nmIoU improvement.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of modern artificial intelligence\ntechnologies, autonomous driving (AD) has made signifi-\ncant progress. Traditional end-to-end autonomous driving\nrelies on precise environmental perception to make safe\npredictions and planning, Although it can directly generate\nplanning routes or control signals from raw sensor data,this\nprocess resembles a black box that excludes human drivers,\nmaking them difficult to understand the driving process\nand interact with the driving system. To address the chal-\nlenges, numerous studies have introduced datasets and meth-\nods (Kim et al. 2018; Deruyttere et al. 2019; Kim et al. 2019;\nAtakishiyev et al. 2021; Malla et al. 2023; Jin et al. 2023) to\nincorporate natural language interaction and interpretability.\nEquipped with language models (i.e. BERT (Devlin et al.\n2018)), driving sensor data could be translated into natu-\nral language prompts to reveal how the models understand\ndriving scenes. However, these methods can only respond to\npredefined questions due to the limitations of language mod-\nels, failing to work when faced with open-domain questions.\nMulti-modal large language models (MLLMs) with exten-\nsive general knowledge and reasoning capabilities have been\nregarded as the future direction for such problems (Zhou\net al. 2023), offering the potential as AI agent for AD tasks.\nHowever, the existing MLLM approaches (Ding et al.\n2023; Xu et al. 2023; Fu et al. 2024) are limited to model\ncomplex temporal characteristics in the first perspective\ndriving videos, e.g., background changes, motions of vehi-\ncles and pedestrians, and traffic signals. Hence, they usually\nfail in multi-modal video understanding in AD tasks, for\nexample, video MLLMs often get low performance in risk\nobject localization (Ding et al. 2023). To solve the above\nlimitation, we introduce a novel Hierarchical MamBa Adap-\ntation (H-MBA) paradigm, which can effectively and effi-\nciently adapt MLLMs with hierarchical mamba modeling\nfor video understanding in complex driving scenarios.\nSpecifically, H-MBA consists of two distinct mod-\nules, Context Mamba (C-Mamba) and Query Mamba (Q-\nMamba). C-Mamba can flexibly learn video contexts by\ndeveloping multi-granularity mamba models in multi-scale\ntemporal branches. In particular, we introduce the low and\nhigh temporal resolution branches to model appearance and\nmotion contexts in the driving videos, as shown in Fig 1(a),\nthe low temporal resolution branch captures more obvious\nmotion changes, while the high resolution branch provides\nadditional details. To master the diversified details in these\nego-like videos, With both of them, we get more compre-\nhensive understanding of the video. we further leverage\nthree mamba structures for each temporal branch, namely\nT-Mamba, DST-Mamba and JST-Mamba, to respectively\nlearn Time-only, Divided Space-Time and Joint Space-Time\ngranularity contexts. After obtaining all the contexts, Q-\nMamba generates a learnable query from the current frame,\nand attentively integrates multi-granularity contexts into\nthis query, via performing latent mamba in both temporal"}, {"title": "2 Related Work", "content": "Multimodal Large Language Models With the signif-\nicant success of Large Language Models(LLMs) (Brown\net al. 2020; Chowdhery et al. 2023; Devlin et al. 2018; Wei\net al. 2021; Lin et al. 2024), there are increasing research\ninterest to expand the ability of LLMs to deal with multi-\nmodal inputs and tasks. Flamingo (Alayrac et al. 2022) and\nPaLM-E (Driess et al. 2023) seamlessly fuse image and text\non massive image-text pairs, achieving great performance\nbreakthrough among various multi-modal tasks; BLIP-2 (Li\net al. 2023a) uses a lightweight Q-Former to bridge the\ngap of image and text modalities with little parameters and\npaves the way for LLaVA (Liu et al. 2024a), MiniGPT-\n4 (Zhu et al. 2023), InstructBLIP (Dai et al. 2024); Then\nthe exploration extends into video domain, VideoChat (Li\net al. 2023b), VideoChatGPT (Maaz et al. 2023) and Video-\nLLaMA (Zhang et al. 2023) further train their temporal\nmodule with video instruction-tuning data. These models\ntake multi-modal inputs and have conversations with users\nin multiple rounds, showing some basic logical reasoning\nability. Moreover, several works, e.g. Shikra (Chen et al.\n2023), ContextDET (Zang et al. 2023) endow MLLMs with\nperceptual capabilities, enabling them to output bounding\nboxes, but the detection is just on image level. While our H-\nMBA enhances Shikra with the proposed H-Mamba, could\nnot only understand driving video inputs with basic captions,\nbut also provide bounding boxes of the risky object.\nState Space Model Series The State Space Models\n(SSMs) have attracted widespread attention due to their\nhigh efficiency on modeling the dynamics and dependen-\ncies of long-term language sequences, S4 (Gu, Goel, and R\u00e9\n2021) designs a structured state-space sequence model es-\npecially for long-range dependencies, with only liner com-"}, {"title": "3 Method", "content": "The framework of our model is presented in Fig 2. We\nchoose Shikra (Chen et al. 2023) pre-trained with large\nlocation-aware data as the baseline model, which is able\nto give the coordinates of the output object, in addition to\nthe basic QA ability. The structure is straightforward and\nsimple, consisting of pre-trained CLIP (Radford et al. 2021)\nViT-L/14 as the frozen visual encoder, Vicuna-7/13B as the\nLLM and a fully connected layer for feature transition and\nalignment. The original Shikra can only handle image in-\nput, it's not enough for the driving scenarios where the\nunderstanding of temporal information is very important.\nWhile preserving the baseline structure, we expand the input\nphase to take multi-frame video inputs, then leverage the H-\nMBA to model the spatio-temporal representation. H-MBA\nconsists of Context Mamba (C-Mamba) to capture multi-\ngranularity video context and Query Mamba (Q-Mamba) to\nadaptively learn and fuse these features, they will be intro-\nduced in detail later. The processed features are added to the"}, {"title": "C-Mamba: Context Mamba for Multi-Granularity\nVideo Modeling", "content": "To better capture the object motion and model background\nchanges in complex driving scenarios, we use a hierarchi-\ncal C-Mamba to process multi-frame video context. The\n\"hierarchical\" has two meanings here. Firstly, considering\nthat speed serves as the \u201cbridge\" connecting spatio-temporal\nchanges, the driving video context is highly correlated to the\nspeed of the ego car, for example, the ego car may stop, drive\nquickly or turn to another lane, the video contexts vary a lot\ncorrespondingly. Thus we introduce the paradigm that in-\ncorporates both high and low temporal resolutions to fit the\ndiverse speed dynamics, the low branch observes more ob-\nvious motion changes, while the high branch involves more\ndetailed information. Secondly, for each temporal resolution\nbranch, we utilize different Mamba state space architectures\nto learn multi-granularity spatio-temporal video features.\nTo be more specific, we design three Mamba-based\nmodules, i.e., temporal only sequence Mamba, divided\nspace-time sequence Mamba and joint space-time sequence\nMamba, each block learns specific ST patterns. Note that\nwe leverage bidirectional Mamba (Bi-Mamba) to model\nvision-specific tasks here, which adapts another flattened vi-\nsual sequences through simultaneous forward and backward\nSSMs.We label the frame features from frozen visual en-\ncoder as $V_1, V_2, ..., V_t \\in \\mathbb{R}^{C \\times D}$, t is the frame number, C\nis the number of patch channel per frame, and D is the fea-\nture dimension. The three distinct structures could be seen\nin Fig 3, and introduced below.\nT Mamba. For temporal only sequence Mamba, it first\npools the patch dimension of each frame feature to get the\noverall frame representation, then the t frame features are\nsent to the bidirectional mamba block to learn the tempo-\nral relation, this module is suitable for modeling the overall\nchanges of the background:\n$f_T = Mamba(pooling(V_1, V_2, ..., V_t))$"}, {"content": "The framework of our model is presented in Fig 2. We\nchoose Shikra (Chen et al. 2023) pre-trained with large\nlocation-aware data as the baseline model, which is able\nto give the coordinates of the output object, in addition to\nthe basic QA ability. The structure is straightforward and\nsimple, consisting of pre-trained CLIP (Radford et al. 2021)\nViT-L/14 as the frozen visual encoder, Vicuna-7/13B as the\nLLM and a fully connected layer for feature transition and\nalignment. The original Shikra can only handle image in-\nput, it's not enough for the driving scenarios where the\nunderstanding of temporal information is very important.\nWhile preserving the baseline structure, we expand the input\nphase to take multi-frame video inputs, then leverage the H-\nMBA to model the spatio-temporal representation. H-MBA\nconsists of Context Mamba (C-Mamba) to capture multi-\ngranularity video context and Query Mamba (Q-Mamba) to\nadaptively learn and fuse these features, they will be intro-\nduced in detail later. The processed features are added to the"}, {"title": "DST Mamba.", "content": "For the divided space-time sequence\nMamba (DST), we first separately process the temporal se-\nquence for each patch channel v with Mamba block, so the\npatches could involve motion change information of the lo-\ncal part, labeled as vr. Then the processed patch sequences\nof the ith frame $v_{ri}$ are sent to spatial Mamba block and get\nthe divided space-time features:\n$f_{DST} = Mamba(v_T)$\n$V_T = Mamba(v_1, v_2, ..., v_t)$"}, {"title": "JST Mamba.", "content": "For the joint space-time sequence Mamba\n(JST), thanks to the O(n) computational complexity of\nMamba algorithm, we are able to jointly process all the\npatch sequences, which is computationally extravagant for\ntransformer-based method. The t frame patch feature se-\nquences are first concatenated, we add a learnable temporal\nembedding(TE) for each frame to distinguish temporal dif-\nferences. The t \u00d7 C sequences could be sent to Mamba to\njointly learn the global spatio-temporal relation.\n$f_{JST} = Mamba(Concat(V_1, V_2, ..., v_t) + TE)$"}, {"title": "Q-Mamba: Query Mamba for Multi-Scale Video\nContext Adaptation", "content": "After obtaining the multi-granularity features, another im-\nportant question is how to fuse these extra knowledge to the\npre-trained MLLM. We have three kinds of designs here.\nThe first straightforward and simple way is to directly add\nall the weighted learned features to the original visual fea-\ntures, we name it directly add (DA) adapter as the baseline\nmethod. Then, motivated by the inception (Szegedy et al.\n2015) network which processes features with different con-\nvolution kernels and concatenate them in parallel, we also\nuse such paradigm and concat all the features in the feature\ndimension D and get $feat \\in \\mathbb{R}^{C \\times (n*D)}$, n is the number\nof different granularity features. After that, we use a linear-\nlayer to transform the features form n * D to D, and this\nformat is called inception concat (IC) adapter.\n$F_{IC} = FC(Concat(f_1, ..., f_n))$\nFurthermore, we design a Query Mamba (Q-Mamba)\nadapter. Considering that the visual encoder is frozen dur-\ning fine-tuning, the original frame visual feature from the"}, {"title": "4 Experiments", "content": "Datasets and Evaluation metrics. DRAMA (Malla et al.\n2023) is a benchmark evaluating the visual reasoning\nof driving risks, meanwhile, it provides important object\nbounding boxes with captions describing their risk from\nthe ego-car perspective. The whole dataset contains 17,785\ntwo-second interactive driving scenarios, while consider-\ning spatio-temporal relationships from videos. BDD-X (Kim\net al. 2018) is a driving-domain caption dataset, consisting of\nnearly 7000 videos that are collected from BDD100K, and\nthe videos are manually captioned with vehicle behaviors,\nsuch as accelerating, and also accompanied with text justifi-\ncation for the behavior. The videos are divided into around\n29,000 clips, and the clips' length duration ranges from 1s to\n40s, so the processing of temporal information is critical. We\nperform two tasks for the driving videos:(1) language cap-\ntion, including the DRAMA and BDD-X datasets, the cap-\ntioning performance is evaluated with standard metrics, such\nas BLEU-4(B4), METEORP(M), ROGUE(R), CIDER(C)\nand SPICE(S). (2) Detection for DRAMA, the performance\nis evaluated with mean intersection over union (mIoU) for\nthe prediction of bounding box.\nImplementations details. Following previous meth-\nods (Chen et al. 2023; Li et al. 2023a,b), we use the\npre-trained Shikra (Chen et al. 2023) as the baseline, and\nfinetune the model with task specific data. We uniformly\nsample L frames for a video, the frames are resized and\ncropped to 224 \u00d7 224 resolution. For DRAMA dataset,\nL = 5, and we use the Spot-Captioning training format,\nwhich allows a bounding box along with the output caption.\nAnd for BDD-X, the videos are longer, we set L = 8 and\nuse the Instruct-tuning format. For other MLLM based\nmethods, unless specifically mentioned, we follow the\nofficial training code with the same experimental settings.\nAll the experiments are done with 4 A6000 GPUs, we train\nthe model for 5 epochs with 2e-5 learning rate in cosine\nannealing schedule (Loshchilov and Hutter 2016)."}, {"title": "SOTA Comparison", "content": "We conduct experiments on DRAMA and BDD-X datasets\nand compare the results with both MLLM and non-MLLM\nbased methods. Note that, the non-MLLM models are de-\nsigned for specific tasks, limited to singular functionalities,\nthe results are gray in the tables. In contrast, the MLLM-\nbased approaches offer greater flexibility, enabling tailored\nresponses based on user questions, all MLLMs are fine-\ntuned in same setting with official scripts, and as a unified\nframework, we only use visual inputs for all the tasks.\nThe risk localization task on DRAMA requires the out-\nput of the object with highest risk for the ego car along\nwith the bounding box, different from traditional vehicle\nobject detection, the model should not only have detection\nand localization ability, but also have a general understand-\ning of the traffic environment and vehicle motion, which is\nhelpful for risk warning and improving driving safety. As\nshown in Table 1, for the caption scores of risk objects, our\nH-MBA obtains the optimal results among MLLM models,\nmoreover, we also achieve the highest detection mIoU score,\ni.e., 66.9%, with 5.5% improvement than previous SOTA"}, {"title": "Ablations", "content": "Choice of Temporal processing module. In this part, we\nfirst ablate which temporal module is suitable for driving\ntasks with both consideration of performance and computa-\ntion cost. We compare our designed mamba-series modules\nwith corresponding mainstream attention temporal modules\nin Table 3. \"Image\" means we only use the current frame\nfeature of the video, which is also the Shikra baseline. And\n\"Average Pool\" means we mean pool the video features in\nthe frame dimension, the performance has slight improve-\nment but still not satisfying without temporal sequence mod-\neling. Then we compare our T Mamba module with Trans-\nformer (Vaswani et al. 2017) block, both of them directly\nlearn the relationships between frames. We can see that,\nMamba block has advantages in both performance and Flops\ncost, in such small scale and lightweight modeling. Next, we\nconsider spatio-temporal divided modeling, corresponding\nto the Timesformer (Bertasius, Wang, and Torresani 2021)\nand DST Mamba in the table. Though the spatial interaction\nfurther improves the results on these tasks, the extra com-\nputation cost of Timesformer is the highest. Compared to\nTimesformer, our Mamba-based methods seem to have more\nadvantages in risk object detection, which needs to integrate\ntemporal changes while retaining the current positions of\nthe objects in current frame. Finally, for our JST Mamba,\nwe fully leverage the capability of Mamba block to process\nlong sequences, with added temporal embedding to explic-\nitly denote temporal distinctions. It has better performance\nin caption tasks. Note the corresponding attention-based op-\neration requires excessive computational resources, which\nwould cause Out of Memory (OOM) in our machine. On\nthe whole, the Mamba-based modules achieve higher perfor-\nmance with lower computation cost, while different Mamba\ngranularities have advantages in different tasks, and our H-"}, {"title": "Choice of Adapter.", "content": "Next we figure out the optimal setting\nof three feature adapters. As shown in Table 4, DA aggre-\ngates all the features by learnable weights, yet the results\nindicate that it merely yields a somewhat averaged outcome.\nThe results of IC are relatively low, it may be because the\nweights are difficult to converge for too many concated fea-\ntures. Then for our Q-Mamba, the results have achieved op-\ntimal results on each metric, some even higher than single\none, which demonstrates the model could effectively learn\nthe most relevant knowledge from the multi-scale features.\nMore ablations could be seen in the Appendix."}, {"title": "Visualization.", "content": "We provide numerous visual examples and\ncompare the results obtained using different time processing\nmodules, as shown in Fig 4. In addition to normal scenes\nlike \"the car is driving\" and \"the car is stopped\", our model\ncan also accurately identify \u201cturning\u201d and \u201cmerging lane\"\nbehaviors. For some rare scenarios, such as lane changing\ncaused by a bus occupying the road ahead in the example,\nthe model can also recognize the action and provide reason-"}, {"title": "Conclusion", "content": "Recent MLLM based approaches have primarily focused on\ndata-labeling and fine-tuning for driving tasks, while we fo-\ncus on the complicated scene changes in driving videos, and\nput forward H-MBA for multiple driving tasks. H-MBA uti-\nlizes high and low temporal resolution branches, integrated\nwith three different Mamba-based processors to learn fea-\ntures of different granularity. The hierarchical features are\nfused by the Q-Mamba adapter and sent to LLMs to boost\nthe performance when handling driving videos, with little\nextra computation. We achieve SOTA results in several tasks\nsuch as risk object localization, demonstrating the effective-\nness of our approach, and showing the potential for practical\napplication and improving driving safety."}]}