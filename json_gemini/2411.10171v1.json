{"title": "Imagine-2-Drive:\nHigh-Fidelity World Modeling in CARLA for Autonomous Vehicles", "authors": ["Anant Garg", "K. Madhava Krishna"], "abstract": "In autonomous driving with image based state\nspace, accurate prediction of future events and modeling diverse\nbehavioral modes are essential for safety and effective decision-\nmaking. World model-based Reinforcement Learning (WMRL)\napproaches offers a promising solution by simulating future\nstates from current state and actions. However, utility of world\nmodels is often limited by typical RL policies being limited\nto deterministic or single gaussian distribution. By failing to\ncapture the full spectrum of possible actions, reduces their\nadaptability in complex, dynamic environments. In this work,\nwe introduce Imagine-2-Drive, a framework that consists of\ntwo components, VISTAPlan, a high-fidelity world model for\naccurate future prediction and Diffusion Policy Actor (DPA),\na diffusion based policy to model multi-modal behaviors for\ntrajectory prediction. We use VISTAPlan to simulate and eval-\nuate trajectories from DPA and use Denoising Diffusion Policy\nOptimization (DDPO) to train DPA to maximize the cumulative\nsum of rewards over the trajectories. We analyze the benefits\nof each component and the framework as a whole in CARLA\nwith standard driving metrics. As a consequence of our twin\nnovelties- VISTAPlan and DPA, we significantly outperform\nthe state of the art (SOTA) world models on standard driving\nmetrics by 15% and 20% on Route Completion and Success\nRate respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving systems must operate safely and ef-\nfectively in complex, dynamic environments, where accurate\nprediction of future events and diverse behavioral modeling\nare critical for informed decision-making. The ability to fore-\nsee potential obstacles, navigate uncertain traffic conditions,\nand make proactive adjustments to driving strategies relies\non robust future prediction capabilities.\nWorld model-based Reinforcement Learning approaches\nhave emerged as a promising solution to this chal-\nlenge by simulating future states based on current observa-\ntions and actions. These models enable autonomous vehicles\n(AVs) to internally \"imagine\" possible future scenarios, facil-\nitating more efficient exploration and reducing the risks and\ncosts associated with real-world interactions. However, the\nutility of these world models is often limited by the nature of\ntraditional RL policies. Most RL policies are constrained to\ndeterministic outputs or single gaussian distributions, which\nfail to capture the full range of possible behaviors. This\nundermines the adaptability of the world models and their\nability to handle the complexity and variability found in\ndriving environments. The significant advantages of world\nmodels also highlights the importance of learning an accurate\nworld model.\nCurrent WMRL approaches, model\nthe environment dynamics in latent space using a Recurrent\nNeural Network (RNN) based network. A common limitation\nof these approaches is their reliance on single-step transition\nmodels, where errors accumulate over multi-step planning,\ncausing planned states to drift from the on-policy distribu-\ntion. Prior works leverage video diffusion\nbased approaches to predict the future states in a single pass,\nwith VISTA being the most versatile and accurate."}, {"title": "II. RELATED WORK", "content": "For autonomous driving, world models follow two key\nparadigms: one leverages world models as neural driving\nsimulators, while the other unifies action prediction with\nfuture generation. Think-2-Drive adapts Dreamer-V3\nfor autonomous driving using BEV state-space repre-\nsentations.\nGenerative world models have been extensively explored\nfor future driving sequence prediction based on various input\nmodalities. Models like DriveGAN, DriveDreamer,\nand MagicDrive focus on generating future driving\nscenarios using actions as inputs. GAIA-I expands this\napproach by incorporating text commands alongside actions.\nVISTA, the most versatile, accepts inputs including\nactions, trajectories, text commands, and goal points, demon-\nstrating high generalizability for sequence prediction."}, {"title": "B. Diffusion Policy for Planning", "content": "Following the success of Diffuser on D4RL bench-\nmark, diffusion-based policies have been successfully applied in robotics for motion\nplanning. Most typically, these policies are trained from\nhuman demonstrations through a supervised objective, and\nenjoy both high training stability and strong performance in\nmodeling complex and multi-modal trajectory distributions.\nIn the autonomous driving domain, successful works like\nsample trajectories using diffusion models while\nformulate Constrained MDP (CMDP) to incorporate\nconstraints."}, {"title": "C. Training Diffusion Models with Reinforcement Learning", "content": "Training diffusion models using reinforcement learning\n(RL) techniques has gained traction in recent research, partic-\nularly for applications such as text-to-image generation.\nDDPO formulate the denoising process as an\nMDP and apply PPO update to this formalism. We build upon\nthese earlier works and embed the denoising MDP within the\nenvironmental MDP of the dynamics of autonomous driving.\nOur approach leverages VISTA's high-fidelity predictive\ncapabilities to build a world model (VISTAPlan) com-\nbined with a diffusion based policy (DPA) trained using\nDDPO method which effectively captures multiple behav-\nioral modes."}, {"title": "III. METHODOLOGY", "content": "Imagine-2-Drive is a World Model based RL framework\ndesigned for long-horizon trajectory planning using only\nfront-facing camera image as input. The framework aims to\ngenerate a collision-free trajectory over a prediction horizon\nH, based on a sequence of past and current observations.\nAs depicted in Fig. 2, the framework consists of three key\ncomponents:\n1) State Encoder: Encodes the sequence of P past front-\nview RGB camera observations, together with the cur-\nrent observation, to provide contextual information for\ncurrent state.\n2) VISTAPlan: A high-fidelity world model that facili-\ntates efficient planning by accurately predicting future\nobservations.\n3) DPA: A diffusion based policy actor to produce a\ntrajectory of actions which maximizes the cumulative\nsum of rewards over the prediction horizon H.\nNotation: This paper differentiates between two categories\nof timesteps: diffusion timesteps, indicated by superscripts\n$k \\in \\{1,...,T\\}$, and environment timesteps, denoted by\nsubscripts $t \\in \\{1,..., N\\}$."}, {"title": "A. Approximate POMDP with MDP", "content": "Using a single frame as the state in autonomous driving\nlacks the necessary temporal context to capture object motion\nand changes over time. This limitation makes it challenging\nto formulate the problem as a Markov Decision Process\n(MDP), which assumes that the current state captures all\nrelevant information for decision-making. To address this,\nwe approximate an MDP by incorporating a sequence of\nprevious observations, thereby adding temporal information\nto the state representation. This approach effectively trans-\nforms the problem from a POMDP to an MDP by including\na fixed-length history, which provides a richer context for\nunderstanding the dynamics of the driving environment.\nFollowing ViNT [32], we use a State Encoder which\ntokenize each observation image $\\{0\\}_p$ into an embedding\nof size dmodel = 512 with an EfficientNet-B0 [35] model\nwhich outputs a feature vector $\\phi(o_t)$. The individual tokens\nare then combined with a positional encoding and fed into\na transformer backbone $F_{sa}$. We use a decoder-only trans-\nformer with $N_t = 4$ multi-headed attention blocks, each with\n$n_H = 4$ heads and $d_{FF} = 2048$ hidden units. The output\ntokens are concatenated and flattened, then passed through\nMLP layers to give a final state embedding $s_t \\in \\mathbb{R}^{32}$\n$s_t = MLP (F_{sa} (\\{\\phi(o)\\}_{t-P:t}))$\t\t(1a)"}, {"title": "B. VISTAPlan", "content": "We leverage VISTA's high-fidelity prediction capabilities\nas the foundation for building our world model. Given the P\npast and current observations $\\{o_t\\}_p$, we predict H future\nobservations $\\{o_{t+i}\\}_{i=1}^{t+H}$ conditioned on a action trajectory $\\tau \\in\n\\mathbb{R}^{H \\times 2}$.\nFourier Trajectory Encoding: The trajectory is encoded\nusing the Fast Fourier Transform (FFT) which offers a\ncompact frequency-domain representation that captures the\ntrajectory's underlying patterns.\n$\\tau_t = (a_t, a_{t+1},..., a_{t+H})$\t\t(2a)\n$\\zeta_t = FFT(\\tau_t)$\t\t(2b)\nFuture State Prediction: To predict the future observa-\ntions $\\{o_{t+i}\\}_{i=1}^{t+H}$ corresponding to $\\tau_t$, we use the Stable Video\nDiffusion model (SVD) used in VISTA.\n$o_{t+i} = SVD(o_{t-P:t}, \\zeta_t)$ \t\t(3a)\nUsing the state encoder 1a and $\\{o_t\\}_{i=1}^{t+H}$, we can get future\nstates $\\{s_i\\}_{i=1}^{t+H}$. This capability allows the model to antic-\nipate the future state of the environment dependent on the\ntrajectory, which is crucial for safe and effective decision\nmaking in complex driving scenarios.\nAdditional Required MDP Components: To use VISTA\nas a world model, we add additional sub-modules to predict\nfuture rewards $\\hat{r}_t$ and $\\hat{\\Gamma}_t$ using $s_t$ and $a_t$\n$\\hat{r}_t \\sim P (r_t | s_t, a_t), \\hat{\\Gamma}_t \\sim P (\\Gamma_t | s_t, a_t)$\t\t(4)\nThis provides us with all the components required for\nMDP and to use VISTAPlan as a world model for planning.\nLoss Function: In addition to VISTA's loss function\n$L_{VISTA}$ defined in equation 6 of [13], we add the additional\nloss functions for learning reward to get a final loss function\n$L_{WM}$:\n$L_{WM} = L_{vista} - \\mathbb{E}_{r_{\\pi \\theta} (\\tau; \\pi)} [\\Sigma_{i=1}^{H} ln p_{\\theta} (r_t | s_t, a_t) + ln p_{\\theta}(\\Gamma_t | s_t, a_t)]$\t\t(5)"}, {"title": "C. Diffusion Policy Actor (DPA)", "content": "Given the current state $s_t$, we use a diffusion based policy\nnetwork ($\\pi_{\\theta}$) to predict the trajectory $\\tau_t \\in \\mathcal{A}$. The diffusion\nmodel captures complex, multi-modal action distributions,\nenabling effective long-horizon planning essential for au-\ntonomous driving.\nHowever, unlike imitation learning, where diffusion poli-\ncies are trained with fixed datasets of expert actions, our RL\nsetting lacks predefined targets. Instead, the policy learns\nto maximize cumulative rewards through exploration and\ninteraction with the environment.\nThe reinforcement learning (RL) objective is for the agent\nto maximize $J_{RL}(\\pi)$, the expected cumulative reward over\ntrajectories sampled from its policy.\nWithout losing generalizibility, we assume the current\nenvironment timestep (t) to be 0.\n$J_{RL}(\\pi) = E_{\\tau \\sim p_{\\theta} (\\tau; \\pi)} [\\Sigma_{t=1}^{H} \\gamma_\\theta (s_t, a_t)]$\t\t(6)\nFollowing DDPO [2], we formulate the policy denoising\nprocess as an MDP itself and use policy gradients method\nto train the DPA.\nWe begin with a randomly initialized diffusion policy\nnetwork. Sampling from the policy network begins with a\nnoisy trajectory sample drawn from a isotropic gaussian dis-\ntribution, $T_T \\sim N(0, \\sigma^2 I)$. The reverse process is defined by\na learned distribution $p_{\\theta}(r^{k-1}|r^k, s_0)$, which progressively\n\"denoises\" the action sequence to produce a sequence of\ntrajectories $\\{T_T, T_{T-1},...,T^0\\}$ ending with the sample $T^0$.\nAfter getting the final denoised trajectory $r^0$, given current\nstate $s_t$, we query our world model to get future states\n$\\{s_i\\}_{i=1}^{t+H}$ using 3a and 1a.\nDenoising as a multi-step MDP: In our formulation, the\ndenoising process itself is viewed as an MDP where:\n$S_{DDPO} = (T_k, s_0)$\t\t(7a)\n$A_{DDPO} = T_{k-1}$\t\t(7b)\n$r_{DDPO} = {\\Sigma_{t=1}^{H} \\gamma_\\theta (s_t, a_t)}$ if k = 0\t\t(7c)\notherwise\n$\\pi_{DDPO} (a_{DDPO} | S_{DDPO}) = p_{\\theta} (r^{k-1} | r^k, s_0)$\t\t(7d)\nDenoising Diffusion RL Objective: The objective for this\nMDP $J_{DDRL}(\\theta)$ is to maximize the reward signal $r_{DDPO}$.\n$J_{DDRL}(\\theta) = E_{\\tau \\sim p_{\\theta} (T_T)} [\\gamma^{DDPO} (T^0)]$\t\t(8)\nThis formulation further enables the estimation of policy\ngradients. With access to both likelihood and gradients of\nlikelihood, we follow the formulation in to make direct\nMonte Carlo estimates of $V_{\\theta} J_{DDRL}$, by sampling, and\nthen performing parameter update. We adopt the importance\nsampling estimator and substitute the per-step return\nwith the final reward $r^{DDPO}$.\n$\\nabla_{\\theta} J_{DDRL} = E[\\frac{p_{\\theta}(T^{k-1}|T^k, s_0)} {p_{old \\theta}(T^{k-1}|T^k, s_0)} \\nabla_{\\theta} \\log p_{\\theta} (T^{k-1}|T^k, s_0) r^{DDP}]\t\t(9)$\nin which the expectation is taken over the trajectories\nsampled with $p_{old \\theta}$, i.e., the previous sampler. The estimator\nbecomes less accurate if $p_{\\theta}^{old}$ deviates too much from $p_{\\theta}$.\nWe adopt the trust region for regularizing the change of\n$\\theta$ w.r.t $\\theta^{old}$, which in practice we adopt the clipping proposed\nin proximal policy optimization [30].\nThis procedure effectively trains the diffusion policy net-\nwork to generate high reward action-sequences ($\\tau$) that max-\nimize cumulative rewards, improving autonomous driving\nperformance."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We consider the task of navigation along pre-defined\nroutes in Town04 of CARLA version 0.9.11 running at\n20 Hz. We utilize the predefined routes from the CARLA\nLeaderboard. We randomly spawn scenarios at several lo-\ncations along each route. Each scenario presents unique\nenvironmental and lighting conditions, with varying number\nof lanes. Each scenario has a maximum length of upto 500\nmeters with a maximum of 20 traffic vehicles. Each dynamic\ntraffic vehicle is assigned a velocity between 1 and 5 km/hr.\nAn episode is terminated upon collision with any traffic\nvehicle or going out-of-lane bounds. We assign our ego-\nagent a constant velocity of 7 km/hr and use the default\nPID controller to follow the output waypoint trajectory. We\ntrain and evaluate our model and the baselines on a subset\nof 10 scenarios."}, {"title": "C. Implementation Details", "content": "1) Action Space: Waypoint trajectory is represented in\nthe X-Y cartesian space. To predict the trajectory effectively,\nwe simplify the action space by predicting the incremental\n$\\Delta X$ and $\\Delta Y$.\n$a_t = (\\Delta X, \\Delta Y)$ \t\t(10)"}, {"title": "D. Baselines", "content": "We focus on world model-based approaches using front\ncamera images as the sole input modality and compare with\nthe following methods:\n1) Dreamer-V3 The third generation in the Dreamer\nseries, incorporating robustness and normaliza-\ntion techniques for stable training.\n2) Iso-Dreamer which focuses on decomposing\nscenes into action controllable and non-controllable\nbranches.\n3) DQN and PPO Standard Model-free algo-\nrithms."}, {"title": "E. Metrics", "content": "Success Rate (SR %) as the percentage of runs where the\nego-agent is able to achieve at-least 90% route completion\n(RC %) with zero instances of Infractions which include the\ninstances of going out-of-lane bounds and collisions with\nother traffic actors.\nEpisodic Return is defined as the cumulative sum of the\nreward function defined in eq. 11."}, {"title": "F. Results", "content": "Simplifying $\\Delta X$: To reduce action space dimensionality and\nensure forward progress, we fix $\\Delta X = 1$, allowing the model\nto focus on lateral adjustments.\nDiscretizing $\\Delta Y$: lateral movement $\\Delta Y$ is defined within\n(0.5,0.5) meters, divided into 11 equal bins representing\nspecific lateral deviations.\nAction Encoding: Each discrete action, representing $\\Delta Y$,\nis encoded using a one-hot embedding.\n2) Reward Function: The reward function for lane-\nkeeping and collision avoidance is defined as:\n$r_t = \\frac{v_{ego} \\cdot \\hat{u}_n} {|| \\hat{u}_n ||}  \\cdot \\Delta t - \\S_1 |Collision\\_Cost| - \\S_2 |\\Delta Y| + c$\t\t(11)\nHere $v_{ego}$ is the agent's velocity projected onto the highway\ndirection $\\hat{u}_n$, normalized and scaled by $\\Delta t = 0.05$ to mea-\nsure highway progression in meters. The $Collision\\_cost =$\n10 penalizes collisions, and $\\Delta Y$ penalizes large trajectory\nchanges. The constant $c = 1$ encourages longer episodes,\nwith $\\S_1$ and $\\S_2$ set to 1.\n3) Horizon and Context Length: We set prediction hori-\nzon length H = 9 and context length P = 5."}, {"title": "V. ABLATION STUDIES", "content": "Table II evaluates the DPA within the Iso-Dreamer and\nDreamer-V3 frameworks by comparing performance with\nand without the actor (rows 1 vs. 3) and (rows 2 vs. 4).\nDPA coupled models outperform by atleast 10% and 8%\non SR and RC metrics. This analysis underscores the influ-\nence of the DPA on trajectory prediction and demonstrates\nits effectiveness across different world model architectures.\nThe results demonstrate that the diffusion policy actor en-\nhances the ability to model complex, multi-modal action\ndistributions, improving trajectory prediction accuracy across\ndifferent world model architectures. This underscores the\nactor's adaptability and potential for broader applications in\nreinforcement learning and decision-making tasks."}, {"title": "B. VISTAPlan analysis", "content": "Table II assesses prediction fidelity and environmental\ndynamics across various world models, using our diffusion\npolicy actor consistently across all models. This isolates\nthe impact of each world model and allows for a focused\nassessment of each world model's ability to capture under-\nlying environmental dynamics and its influence on trajectory\nprediction performance. Results in (rows 2, 3 and 4) high-\nlight the VISTAPlan's superior performance with a 20% and\n14.6% gain in SR and RC metrics respectively, underscoring\nthe importance of accurate future state predictions and vali-\ndating VISTA's role as the foundation for our world model\nframework."}, {"title": "C. Context Length analysis", "content": "To analyze the effect of POMDP approximation with\nMDP using the State Encoder, Fig. 6 compares the episodic\nreturns of Imagine-2-Drive using different context lengths\n(CL) for the State Encoder, with P = 5 fixed for VISTA.\nThe performance drops with CL = 1, compared to CL = 3\nand 5. This can be attributed to shorter CLs which provide\ninsufficient historical data, limiting the model's ability to\ninfer the current state in partially observable environments.\nIn contrast, longer context lengths (CL = 3,5) capture\nmore temporal information, improving state estimation and\ndecision-making, and resulting in better performance."}, {"title": "VI. CONCLUSION & FUTURE WORK", "content": "Conclusion: In this work, we introduced Imagine-2-Drive\na high-fidelity world model leveraging the VISTA frame-\nwork for long-horizon trajectory planning in autonomous\ndriving. By incorporating a diffusion-based prediction model,\nwe overcame the limitations of single-step transition models,\nenabling simultaneous trajectory generation and enhanced\ntemporal consistency. DPA, a diffusion based policy im-\nproves exploration and robustness by effectively capturing\nmultimodal action distributions. Our empirical results in\nCARLA demonstrate significant gains in trajectory predic-\ntion and planning accuracy, showcasing the superiority of our\napproach over SOTA world model and model-free methods.\nFuture Work: We intend to extend Imagine-2-Drive to\nother domains such as robotic manipulation, where accurate\nlong-horizon planning and multimodal prediction are crucial.\nExpanding the versatility of our approach across domains\nwill further validate its generalizability and adaptability."}]}