{"title": "Hidden Biases of End-to-End Driving Datasets", "authors": ["Julian Zimmerlin", "Jens Bei\u00dfwenger", "Bernhard Jaeger", "Andreas Geiger", "Kashyap Chitta"], "abstract": "End-to-end driving systems have made rapid progress, but have so far not been applied to the challenging new CARLA Leaderboard 2.0. Further, while there is a large body of literature on end-to-end architectures and training strategies, the impact of the training dataset is often overlooked. In this work, we make a first attempt at end-to-end driving for Leaderboard 2.0. Instead of investigating architectures, we systematically analyze the training dataset, leading to new insights: (1) Expert style significantly affects downstream policy performance. (2) In complex data sets, the frames should not be weighted on the basis of simplistic criteria such as class frequencies. (3) Instead, estimating whether a frame changes the target labels compared to previous frames can reduce the size of the dataset without removing important information. By incorporating these findings, our model ranks first and second respectively on the map and sensors tracks of the 2024 CARLA Challenge, and sets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover a design flaw in the current evaluation metrics and propose a modification for future challenges. Our dataset, code, and pre-trained models are publicly available at https://github.com/autonomousvision/carla_garage.", "sections": [{"title": "1. Introduction", "content": "Imitation Learning (IL) for end-to-end autonomous driving has seen great success in recent work on the CARLA simulator [9]. A key ingredient contributing to this is the scalability of IL with increased training data, which is now straightforward to collect as a result of steady progress in planning algorithms for CARLA [4, 5, 13-15, 22, 24, 26, 29, 32, 35]. However, with the introduction of the CARLA Leaderboard 2.0, driving models now face 38 new complex scenarios. These require driving at high speeds, deviating from the center of the lane, or handling unexpected obstacles. The best planning algorithm of Leaderboard 1.0 [15] does not solve these new scenarios, making it harder to collect the high-quality driving demonstrations needed for training IL models. As a result, there are no existing IL-based methods for Leaderboard 2.0.\nIn this work, we present the first attempt to tackle Leadership 2.0 with IL. To collect training data, we leverage the recently open-sourced PDM-Lite [1, 30] planner, which can solve the new Leaderboard 2.0 scenarios. We then train a simple existing IL model, TransFuser++ [15], with minimal changes to its architecture and training objective. Instead of the model, we focus on a critical but understudied aspect of IL - the training dataset. In particular, the impact of factors besides the dataset scale, such as the diversity of the training distribution, is nuanced and not yet well understood. We conduct a systematic analysis of our driving dataset, leading to multiple new insights.\nFirst, the expert's driving style, in addition to its performance, significantly influences its suitability for IL. To develop an effective expert, it is important to base the expert's behavior on signals that are easily observable and interpretable by the IL policy, rather than relying excessively on privileged inputs. This behavior also resembles how human drivers perceive and react to their environment. Second, we find the use of frequency-based class weights, a common approach to facilitate learning of classification tasks on imbalanced datasets, detrimental for target speed prediction in autonomous driving. Over-represented classes do not represent a single \"uninteresting\" mode of the data distribution- in contrast, they may contain a mixture of both uninteresting (e.g., braking while waiting at red lights) and crucial parts of the dataset (e.g., braking for obstacles). Finally, we study data filtering as an alternative means to assigning the importance of frames, by which we reduce our dataset size by ~50% while maintaining performance.\nBased on these findings, we train a model which safely handles urban driving in diverse scenarios to rank second in the 2024 CARLA challenge and first on the Bench2Drive test routes [18]. We then theoretically demonstrate how the performance metrics used by the leaderboard inadvertently encourage participants to terminate evaluation routes prematurely, and propose changes to the metrics that can solve this problem for future challenges. An extended report of all our experiments and findings is available at this link."}, {"title": "2. Preliminaries", "content": "In this section, we provide an overview of our task and baselines. The task involves urban navigation along routes with complex scenarios. Each route is a list of GNSS coordinates called target points (TPs) which can be up to 200 m apart.\nMetrics. For the following experiments, we use the official CARLA closed-loop metrics. Our main metric is the Driving Score (DS) which multiplies Route Completion (RC) with the Infraction Score (IS). RC is the percentage of the route completed. IS is a penalty factor, starting at 1.0, which gets reduced multiplicatively with each infraction.\nBenchmark. To train and evaluate agents, Leaderboard 2.0 provides 90 training routes on Town12 and 20 validation routes on Town13 which on average are 8.67 km and 12.39 km long respectively. Each route contains around 100 scenarios, distributed as shown in Figure 1. We split them into short routes, each containing only a single scenario. This allows for more accurate performance evaluation per scenario type. After splitting, we sample up to 15 routes per scenario type without replacement to create the Town13 short benchmark. There are 38 scenario types, but in some cases, fewer (or no) routes are available, which gives a total of 400 routes from 36 scenarios in this benchmark. As the calculation of MinSpeedInfractions is unsuited to short routes, we exclude them from the IS metric on Town13 short.\nTraining dataset. We reproduce TransFuser++ [15] on our benchmark using data collected with the PDM-Lite expert [1, 30]. We choose PDM-Lite as it achieves state-of-the-art DS on the official validation routes. Unlike other concurrent Leaderboard 2.0 planners [21, 34], it is also publicly available and possible to modify. We sample from the shortened training routes with replacement to obtain a set of 50 routes per scenario, on which we collect a training dataset using our expert (198k frames). The dataset contains RGB images with a resolution of 384x1024 pixels, LiDAR point clouds, and the training labels needed for TF++ (path checkpoints, expert target speed, and auxiliary labels such as BEV semantics and vehicle/pedestrian bounding box predictions). Additionally, we collect data on Towns 01-05 and 10, which contain the six scenarios from Leaderboard 1.0 (139k frames), for a total of 337k frames. For the final leaderboard submissions, we also include training data from the provided validation routes on Town13 (50 short routes per scenario), adding 194k frames (531k in total).\nPDM-Lite [1, 30] is a rule-based approach for collecting data in Leaderboard 2.0. Inspired by PDM-Closed [7], it consists of six stages (Fig. 2):\n\u2022 First, it creates a dense path of spatially equidistant points using the A* planning algorithm, given sparse TPs from the simulator. For new scenarios that require leaving this path, a short segment of the route where the scenario will be spawned is shifted laterally towards an adjacent lane.\n\u2022 It forecasts dynamic agents for 2s into the future, assuming that they maintain their previous controls.\n\u2022 It selects a leading actor and generates a target speed proposal using the Intelligent Driver Model [31].\n\u2022 The target speed proposal is converted into an actual expected sequence of ego-vehicle bounding boxes in closed-loop by using a kinematic bicycle model.\n\u2022 Having forecasted all actors, it checks for bounding box intersections between the simulated ego vehicle and other vehicles. It scores the ego vehicle's motion accordingly: if it detects an intersection, it rejects the IDM target speed proposal, and sets the target speed to zero.\n\u2022 The steering value is estimated with a lateral PID controller, which minimizes the angle to a selected point along the path ahead. For the throttle and brake predictions, it employs a linear regression model using features extracted based on the current speed and target speed.\nTransFuser++ [15] is the best-performing open-source model on Leaderboard 1.0 (Fig. 3). Given sensor inputs, it predicts a target speed and desired path which are input to a controller module to drive the vehicle. We require two changes compared to [15] for compatibility with PDM-Lite:\n\u2022 Two-hot labels. While the rule-based planner in [15] uses only 4 different target speed classes up to 8m/s (28.8km/h), PDM-Lite operates with a continuous range of target speed values up to 20m/s (72km/h). To solve target speed regression with a classification module, we employ two-hot labels [10]. This method converts a continuous value into a two-hot representation by interpolating between one-hot labels of the two nearest classes. For instance, with our 8 speed classes ([0.0, 4.0, 8.0, 10, 13.89, 16, 17.78, 20] m/s), a target speed of 3.0m/s"}, {"title": "3. Hidden Biases", "content": "In this section, we present the main findings of our study. Expert style. While expert performance is often reported in prior work, the manner in which it achieves that performance, i.e., expert style, is often overlooked. Although harder to quantify, it is an important aspect to consider for IL. For instance, consider PDM-Lite's behavior when approaching pedestrians (Fig. 4). By default, the expert slows down when a pedestrian will enter the driving path, even if the pedestrian is still obstructed by a parked vehicle and not clearly visible. We make minor adjustments to the IDM parameters, with which the expert brakes rather sharply, coming to a stop at a distance of roughly 4 meters in front of the more visible pedestrian. This leads to a ~ 4\u00d7 decrease in pedestrian collisions for models trained on the adjusted expert data. Notably, this update does not affect the expert's own pedestrian collision rate. The improvement is likely due to the adjusted behavior providing a clear braking signal for the model to learn from (a pedestrian visible directly in front of the ego vehicle). The default behavior requires the model to generalize across various situations where a pedestrian might appear further ahead.\nTarget speed weights. Training TF++ involves using class weights in a target speed classification loss, which are calculated anti-proportionally to the number of occurrences of the respective class in the dataset [15]. This means that classes that appear frequently get a lower weight than those that appear rarely. We find that removing these weights significantly improves the performance of TF++ on our task (Table 1). We believe this is due to the weight of class 0 (braking), the most common in the dataset. While some part of the data for class 0 is redundant (e.g., waiting at red lights), some frames are among the most crucial for avoiding infractions, such as coming to a stop in front of stop signs or pedestrians. With a low weight on the target speed loss for these frames, ignoring short braking phases in these situations is an easy shortcut for the model to fall into."}, {"title": "4. Additional Insights", "content": "In this section, we analyze the behavior of our models. In Table 3, we present additional experimental results with no speed weights and no filtering. Models marked with:\n\u2022 \"Big\" use the default regnety_032 [25] architecture of TF++ for the image and LiDAR perception modules instead of ResNet34 used in our \"Base\" setting.\n\u2022 \"Pre\" use two-stage training, where we first pre-train exclusively with perception losses (BEV semantics, bounding boxes, image depth, image semantics) for 15 epochs, before training for 31 epochs with all losses.\n\u2022 \"Ens\" are ensemble models, averaging predictions from 3 models trained with different random seeds.\nEnsembling (\"Ens\") and two-stage training (\"Pre\") provide small improvements. To react better to vehicles that are further away, we experiment with extending the LiDAR range in front of the ego to 64m from 32m (\"L64m\"). This also results in a small improvement, albeit at the cost of increased training time. Giving the next two target points as input instead of only one (\"2TPs\") fails to increase performance. Our final models (used in Table 2 and Section 5) combine the benefits of \"Big\", \"Pre\", and \"Ens\".\nFailures. We now discuss scenarios where the \"Base\" TF++ model fails often. We visualize the camera image input and a bird's-eye-view (BEV) image showing observed LiDAR hits, the model's path predictions, the target points used as inputs, and the auxiliary BEV semantics predictions. We use the following colors:\n\u2022 Blue: Path predictions\n\u2022 Red: Target point (used as model input)\nGrey: Road (semantics)\nYellow: Road marking (semantics)\nLight Green: Green traffic light (semantics)\nLight Orange: Vehicle (semantics)\nGreen: Ego vehicle or pedestrian (bounding box)\nOrange: Vehicle (bounding box)\nConstructionObstacleTwoWays. In this scenario, the ego vehicle must pass an obstacle by moving into an adjacent lane with oncoming traffic. Figure 5 illustrates a common failure mode, where TF++ fails to merge back to its original lane. At first, the model successfully waits for a sufficiently large gap in the oncoming traffic and switches to the adjacent lane. As long as the traffic cones marking the construction site are visible in the camera image, its path predictions correctly indicate a lane change back to the original lane. However, as shown in the final frame, once the traffic cones disappear from the camera image, the model erroneously predicts staying in the left lane. Notably, the traffic cones are still visible in the LiDAR, suggesting an over-reliance on the camera for this scenario. After staying in the wrong lane, the ego vehicle often collides with oncoming traffic.\nSignalizedJunctionLeftTurn. Figure 6 shows a common failure case where the model does not react adequately to another actor, leading to a vehicle collision. After the traffic light turns green, the vehicle accelerates and turns without reacting to the oncoming vehicle in the lane it needs to cross. After the collision, the model is able to recover, returning to its lane and completing the route.\nVehicleTurningRoutePedestrian. In this scenario, the model must make an unproteced turn through dense traffic and encounters a pedestrian on the road during or directly after the turn. Figure 7 shows two corresponding failure cases. In the first case (top), the model fails to execute the turn through very dense traffic, where the margins for selecting the right moment to accelerate are extremely narrow. After colliding with a vehicle, the model also collides with a pedestrian that walks into the ego vehicle while it is stationary. The second example (bottom) depicts an instance of this scenario at night, where TF++ does not collide with another vehicle, but fails to recognize the pedestrian hazard in time. Note that the pedestrian is barely visible until illuminated by the ego vehicle's headlights, which is only a couple of frames before the collision. This failure is likely due to covariate shift, since the expert would brake earlier in this situation, even before the pedestrian becomes visible in the RGB image. By the time the pedestrian is revealed by the headlights, the model is already outside its training distribution, where it has not learned a braking reflex.\nYieldToEmergencyVehicle. In this scenario, the ego vehicle must yield to an emergency vehicle approaching from behind on a multi-lane highway. TF++ fails here since it is impossible to distinguish emergency vehicles from regular vehicles using the LiDAR alone. Thus, solving this scenario requires the inclusion of a back camera."}, {"title": "5. Benchmarking", "content": "Reliably benchmarking autonomous driving policies is challenging. While benchmarks based on real datasets exist, they are limited to open-loop metrics [2, 8] or closed-loop evaluation of planning only, assuming privileged access to perception [3, 6, 11, 20]. Therefore, simulators such as CARLA are essential for rigorous benchmarking of end-to-end driving policies. In the setting of CARLA Leaderboard 2.0, which we explore, there are two public benchmarks, on which we present our results in this section."}, {"title": "5.1. Bench2Drive", "content": "Bench2Drive [18] consists of 220 short (~150m) routes split across all CARLA towns, with one safety critical scenario in each route. It can be considered a 'training' benchmark (reminiscent of level 4 autonomy), since methods under evaluation have seen the test towns during training.\nMetrics. Besides the standard CARLA metric DS, this benchmark tracks success rate, which is 100% for a given route if DS=100%, and 0% otherwise. Further, the 220 routes are categorized into five groups (Merging, Overtaking, Emergency Braking, Giving Way, and Traffic Signs), for which success rates are reported per category.\nResults. As seen in Table 4, TF++ significantly outperforms all prior work on this benchmark, doubling the success rate compared to the next best approach. We believe this is partly due to the higher performance of the PDM-Lite expert driver used in our dataset, compared to the closed-source Think2Drive [22] expert used to generate training data for all the baselines in this setting."}, {"title": "5.2. Official Town13 Validation Routes", "content": "This is the set of 20 long validation routes on Town13 described earlier in Section 2. As its name suggests, this is a 'validation' benchmark, so data from Town 13 may not be used during training (reminiscent of level 5 autonomy)."}, {"title": "6. Conclusion", "content": "With a systematic analysis of training dataset biases for end-to-end driving in CARLA, we reveal the impact of expert style on IL policy performance, provide insights into the challenges of assigning importance to frames through weighting or filtering, and provide a simple yet effective heuristic that estimates importance based on changes in target labels. We reproduce TransFuser++ in the Leaderboard 2.0 setting, providing the first recipe for training an end-to-end driving system that attains non-trivial performance. Finally, we propose an improvement to the existing metrics and extensively benchmark our system. We hope this can serve as a starting point for future research on this task.\nLimitations. In repeated Leaderboard submissions, we observe significant variance in DS, with identical agents yielding results that differ by more than 1 DS. Unfortunately,\nour results on the leaderboard (Table 2) also do not include all routes in the benchmark, due to technical issues during model setup in some routes. In addition, as shown in Table 5, DS is influenced significantly by early termination on long routes, which does not reflect any actual improvement in driving behavior. We believe it is necessary to consider improved metrics (such as the proposed Normalized DS) and standardize benchmarking with multiple seeds before drawing strong conclusions based on our empirical results."}]}