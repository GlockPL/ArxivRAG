{"title": "Rethinking Feature Backbone Fine-tuning for Remote Sensing Object Detection", "authors": ["Yechan Kim", "JongHyun Park", "SooYeon Kim", "Moongu Jeon"], "abstract": "Recently, numerous methods have achieved impressive performance in remote sensing object detection, relying on convolution or transformer architectures. Such detectors typically have a feature backbone to extract useful features from raw input images. For the remote sensing domain, a common practice among current detectors is to initialize the backbone with pre-training on ImageNet consisting of natural scenes. Fine-tuning the backbone is typically required to generate features suitable for remote-sensing images. However, this could hinder the extraction of basic visual features in long-term training, thus restricting performance improvement. To mitigate this issue, we propose a novel method named DBF (Dynamic Backbone Freezing) for feature backbone fine-tuning on remote sensing object detection. Our method aims to handle the dilemma of whether the backbone should extract low-level generic features or possess specific knowledge of the remote sensing domain, by introducing a module called 'Freezing Scheduler' to dynamically manage the update of backbone features during training. Extensive experiments on DOTA and DIOR-R show that our approach enables more accurate model learning while substantially reducing computational costs. Our method can be seamlessly adopted without additional effort due to its straightforward design.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the advancement of geo-spatial and deep learning technologies, remote sensing object detection has seen astonishing progress in recent years. Modern deep learning architectures, like convolutional neural networks or transformers, have greatly improved object detection in Earth observation. These models usually include a feature backbone to extract key features for downstream tasks from the raw input images. For object detection on natural scenes, it becomes a de facto standard to initialize the backbone with pretraining on ImageNet [1] over the years, as it is one of the largest databases in visual object recognition, enabling models to learn a broad spectrum of features such as edges, textures, and shapes. Interestingly, the same research trend to use ImageNet pre-trained weights for the backbone has been observed in the remote sensing community for the following reasons. (i) Aerial and satellite imagery datasets often face challenges including limited diversity and scale due to geopolitical reasons, security concerns, and cost issues. For example, Wang et al. [2] discovered the inefficiency of solely leveraging remote sensing images for detection and segmentation tasks. (ii) Furthermore, there exists an implicit consensus that though remote sensing imagery and natural scene images have different characteristics, they may still share some common visual patterns.\nFine-tuning the backbone with datasets for downstream remote sensing tasks is a common strategy to bridge the knowledge gap between natural and remote sensing scenes, yet it may impede overall low-level feature extraction due to the domain-restricted data, limiting performance enhancement. Hence, several researchers have concentrated on advanced remote sensing pre-training (e.g. [3], [4], [5], [6], [7], [8], [9]). For example, Fuller et al. [3] highlight the benefits of large unlabeled datasets for pre-training with masked autoencoding. Huang et al. [6] aim to learn robust representations for remote sensing understanding by simultaneously acquiring general knowledge from natural images with supervised learning and domain-specific knowledge from remote sensing images with self-supervised learning. Wang et al. [7] present multi-task pre-training for remote sensing foundation models. However, we argue that the effectiveness of such strategies is still restricted in long-term training because the weights of the backbone heavily shift further from the initial pre-trained values.\nTo tackle this limitation, in this work, we introduce a novel feature backbone fine-tuning method named DBF (Dynamic Backbone Freezing) for remote sensing object detection. Inspired by recent studies of frozen settings in natural language processing (e.g. [10], [11], [12], [13]), we aim to address the dilemma of whether the backbone should extract low-level generic features or specialize in domain-specific knowledge of remote sensing. Intuitively, we implement a module named 'Freezing Scheduler' to dynamically control the update of the feature backbone for each epoch during training as shown"}, {"title": "II. METHODOLOGY", "content": "In this session, we introduce a simple, but efficient feature backbone fine-tuning method namely DBF (Dynamic Backbone Fine-Tuning) for remote sensing object detection.", "sections": [{"title": "A. Background", "content": "The frozen setting in NLP (Natural Language Processing), particularly in the context of large language models, refers to a technique where pre-trained models are used without updating their parameters for other downstream tasks. This research trend has gained attention for its efficiency and effectiveness in various applications. One significant research branch is the exploration of parameter-efficient fine-tuning methods. These techniques aim to fine-tune backbone models with a minimal number of additional parameters. For example, Adapters [10], small and trainable modules, are designed to be inserted within each layer of the pre-trained model. These are fine-tuned on specific tasks while keeping the majority of the model parameters frozen. On the other hand, Prefix-tuning [11] adds task-specific prefixes to the input tokens, allowing the model to adjust its behavior for different tasks without modifying the core model parameters. Moreover, LoRA [12] decomposes weight matrices into low-rank representations, updating only the low-rank components during fine-tuning.\nIn computer vision, the usual practice for dense prediction tasks such as detection and segmentation had been to incorporate extra networks to avoid losing information in the feature backbone during fine-tuning (e.g. [16], [17]). After the success of the frozen setting in NLP, this concept has influenced research in computer vision tasks as well. For instance, Adapters [10] initially designed for NLP can be used in vision models to allow efficient fine-tuning without updating the entire model. Besides, Lin et al. [18] involves utilizing frozen pre-trained networks, with a task-specific head network to be fine-tuned for downstream tasks. Interestingly, Vasconcelos et al. [19] presents a fine-training scheme for detectors not to update the ImageNet-pretrained backbone. This simple strategy enables models to achieve competitive performance while significantly reducing resource usage for generic object detection on natural scenes. However, to our knowledge, only a few have focused on the frozen setting in the same context for remote sensing understanding. Motivated by the success of [19], we revisit the frozen setting for remote sensing object detection. Unlike [19], in our approach the backbone parameters are dynamically fixed or unfixed during training to meet two competing goals in remote sensing object detection: (i) generic feature extraction (for model generalization) vs. (ii) domain-specific feature extraction (for sophisticated prediction). Besides, it helps minimize the training cost as we often freeze the backbone, the most parameterized in detectors."}, {"title": "B. Overview of Dynamic Backbone Freezing", "content": "In this work, we propose a simple but efficient feature backbone fine-tuning method for remote sensing object detection. Our training scheme enables the model to preserve the initialized low-level generic features from natural images while harmoniously acquiring specialized knowledge in remote sensing. This is achieved by alternately freezing and unfreezing the backbone using the 'Freezing Scheduler' controller. The detail of DBF is presented in Fig. 2. Note that in this letter all the algorithms are written in PyTorch [20]-style pseudo code.\nA deep learning-based detector is typically composed of (a) a backbone network for feature extraction, (b) a neck for feature aggregation, and (c) a detection head for predicting bounding boxes and class scores. In Fig. 2, model.backbone and model.neck correspond to (a) and (b), respectively. For (c), there are two types of heads: model.roi_head and model.bbox_head. In particular, model.roi_head is a specific type of (c) typically used in two-stage detectors like Faster R-CNN to refine the Rol (Region of Interest) proposals generated by the RPN (Region Proposal Network). Since some detectors may not have model.neck and model.roi_head, conditional statements are used to handle a runtime exception in lines 5 and 7 so that the proposed method is compatible with various architectures.\nHere, forward() defines the forward process of a detector model. It is the process of passing input data through the network to generate output. In this letter, we override this function for typical detector models with freezing_scheduler as shown in lines 3-4. If the condition of line 3 is true, out from model.backbone becomes detached in line 4, which means gradients for the backbone will not be computed, stopping the gradient flow to the backbone during back-propagation."}, {"title": "C. Design choice of Freezing Scheduler", "content": "The previous subsection centered on the basic pipeline of DBF. Nevertheless, we did not consider how to implement freezing_scheduler in code. This subsection aims to discuss the design principles of 'Freezing Scheduler'.\nThis work assumes that the input is an epoch and the output is a 0 or 1 signal for the Freezing Scheduler. Here, 0 means allowing the backbone updates (i.e. unfreeze the backbone) while 1 represents vice versa (i.e. freeze the backbone). The flexibility of our framework enables researchers to design and implement various versions of the Freezing Scheduler. Among a wide range of variants, in this letter, we present one of the simplest forms named 'Step Freezing Scheduler' (Fig. 3).\nIntuitively, our Step Freezing Scheduler is similar to a step learning rate scheduler [23] that decreases the learning rate at specific intervals based on the users' demand during training. In Fig. 3, p is the only hyper-parameter to be tuned by users. Our scheduler updates the feature backbone only every p epochs. In other words, a higher p value implies that the backbone remains frozen more frequently, leading to increased efficiency in GPU memory usage and faster training time. Moreover, it helps to preserve the pre-trained low-level generic knowledge in the backbone while simultaneously allowing the detection model to acquire domain-specific knowledge for more precise prediction without bells and whistles."}]}, {"title": "III. EXPERIMENTS", "content": "In this session, we show our novel feature backbone fine-tuning method, DBF enables more accurate model learning while significantly reducing computational costs. Please refer to the supplementary material for further information.", "sections": [{"title": "A. Experimental setup", "content": "Baseline architectures. Deep learning models for object detection are conventionally split into one-stage and two-stage methods. Besides, anchor-based and anchor-free methods have been recently increasingly highlighted in the classification of detector models. In this study, we opted for Faster R-CNN [24], RetinaNet [25], and FCOS [26] as the representative models in anchor-based two-stage, anchor-based one-stage, and anchor-free one-stage object detection. For all benchmark models, we considered two exemplary neural networks as feature backbones: ResNet-50 [21] and Swin-S [22], one of the celebrated convolutional neural networks and transformer architectures, respectively. We leverage the ImageNet pre-trained weights for remote sensing object detection as they are still competitive baseline models to be readily online [8]. Moreover, FPN (Feature Pyramid Network)-1x [27] is used as a neck to enhance multi-scale invariance for our models.\nTraining details. We chose PyTorch and MMDetection / MMRotate [28] for implementing our method and baseline architectures. For model optimization, SGD (Stochastic Gradient Descent) is used with momentum of weight 0.9, weight decay 0.0001, and batch size 8. The initial learning rate is set to 0.005 for all detectors. To enhance the stability and convergence of our training process, we utilize a linear learning rate warm-up strategy over the initial 500 iterations, during which the learning rate increases linearly from zero to 33% of the maximum learning rate. The learning rate decayed by 1/4 after the 12th epoch. To prevent gradient explosion, L2 norm-based gradient clipping is used with a maximum norm of 35. We apply the following data transformations for training: 'RResize', 'RRandomFlip', 'Normalize', and 'Pad'. For testing, \u2018MultiScaleFlipAug\u2019 is used as well. For a fair comparison, we set the value of the random seed to 0.\nTo demonstrate the effectiveness of the proposed method, we compare it against (a) full training and (b) 'Frozen Backbone' [19]. In (a), the backbone is always fine-tuned every training epoch, which requires the highest computing resources. Meanwhile, in (b), the backbone is never fine-tuned (i.e. always frozen), leading to minimal resource usage. Our method balances the extremes of (a) and (b). Precisely, our approach can be seen as a generalized version of (a) and (b):\nIf p equals 1, our method is equivalent to (a);\nIf p equals \u221e, our method is equivalent to (b).\nTable I indicates the experimental configuration parameters in this work. As mentioned in Sec. II-C, we use Step Freezing Scheduler for DBF. Besides, we consider four p values, {2,5,10,\u221e} in ours: the models fully fine-tuned (i.e. p = 1) for the first 50 or 400 epochs are fine-tuned again with DBF."}, {"title": "B. Evaluation metrics", "content": "mAP (mean Average Precision) is chosen for verifying both the precision and recall of the models across different categories in multiple object detection. Precisely, 'mAP@50' [29] is used for model evaluation following [30].\nFLOPS (FLoating point Operations) is adopted to measure the efficiency of the training scheme on various hardware platforms by counting the number of unit operations in each network layer. Typically, most papers calculate FLOPs based solely on the one forward pass with a batch size of 1."}, {"title": "D. Experimental results", "content": "As seen in Table II, our method performs better in most cases for all datasets. Here, ATFLOPs denotes the difference in Tera FLOPs relative to the baseline (a). In other words, the lower \u2206TFLOPs, the better. As mentioned before, the FLOPS should be calculated as Ftotal in Eq. 1. Besides, \u2018(p = 1 \u2192) p = a' in (c) means that the backbone is fine-tuned with p = 1 for the first 50 or 400 epochs and then fine-tuned again with p = a according to Table I. Although (a) permits adjusting the backbone for every epoch, it mostly results in slightly lower mAP than ours. Besides, (b) decreases ATFLOPs the most, but it undergoes a significant drop in mAP (about 3 to 10%p). Compared to (a) and (b), the proposed approach balances low time complexity and precise inference ability: ours achieves the highest mAP for almost all cases, while effectively reducing computational costs during training.\nTo discuss the practical effectiveness of the proposed technique, we present how much training time can be saved when adopting our approach. Let the detector be FCOS with ResNet-50 as a backbone. For the training split of DOTA, using the Nvidia RTX A5000 (GPU) it takes an average of 23 minutes to fully fine-tune the model, including the backbone, in one epoch. Freezing the backbone takes 16 minutes on average. Based on these, we can approximately calculate the overall training time for each training policy as shown in Table III."}, {"title": "E. Parameter study of p", "content": "This subsection examines how users can effectively select p value. In the experiments, we consider four p values, {2, 5, 10, \u221e} for our method. One certain thing is that as the p value increases, more time and memory are saved in training. However, larger p deprives the learning opportunity of the feature backbone, which might hurt the model generalization. To find the best p, we compute the average of AmAP over all the experiments for each p. As demonstrated in Table. IV, it appears that p = \u221e is the optimal choice for better prediction."}]}, {"title": "IV. CONCLUSIONS AND FUTURE WORK", "content": "In this letter, we have proposed a simple but efficient feature backbone fine-tuning strategy named DBF for remote sensing object detection. This work showed that alternating between freezing and unfreezing the backbone aids object detection models in learning both general and domain-specific knowledge. Especially with the proposed method, a significant decrease in memory usage and training time can be achieved. In this work, our approach has shown powerful results on DOTA and DIOR-R with various detection architectures.\nIn the future, more sophisticated studies will be conducted as follows: (i) connecting the current method with the backbone initialized with other remote sensing pre-training methodologies like [6], [7] rather than ImageNet-based one; (ii) extending this work to other remote sensing understanding tasks such as segmentation and change detection; (iii) designing novel freezing schedulers for further improvement. We believe these research directions are crucial for future work."}]}