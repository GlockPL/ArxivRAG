{"title": "Iterative Batch Reinforcement Learning via Safe Diversified Model-based Policy Search", "authors": ["Amna Najib", "Stefan Depeweg", "Phillip Swazinna"], "abstract": "Batch reinforcement learning enables policy learning without direct interaction with the environment during training, relying exclusively on previously collected sets of interactions. This approach is, therefore, well-suited for high-risk and cost-intensive applications, such as industrial control. Learned policies are commonly restricted to act in a similar fashion as observed in the batch. In a real-world scenario, learned policies are deployed in the industrial system, inevitably leading to the collection of new data that can subsequently be added to the existing recording. The process of learning and deployment can thus take place multiple times throughout the lifespan of a system. In this work, we propose to exploit this iterative nature of applying offline reinforcement learning to guide learned policies towards efficient and informative data collection during deployment, leading to continuous improvement of learned policies while remaining within the support of collected data. We present an algorithmic methodology for iterative batch reinforcement learning based on ensemble-based model-based policy search, augmented with safety and, importantly, a diversity criterion.", "sections": [{"title": "1 Introduction", "content": "The objective of batch (or offline) reinforcement learning (RL) is to extract the best possible behavior out of existing data, called a batch, without any learning during deployment. This implies that learning is more successful if the initial data is diverse or collected through the deployment of an expert agent. In a real setting, the initial batch is prone to limitations (low data coverage, low reward actions, etc.), forming a challenge in learning for real-world applications. This challenge of limited information calls for safety mechanisms, such as regularization, to ensure reliable performance of the agent [1, 2].\nIn many industrial setups, the application of offline reinforcement learning is not a one-time process but iterative. After an RL agent is trained and deployed on the system, a new set of recordings becomes available. The principal contribution of our work relies on the formulation of iterative batch reinforcement learning (IBRL), a novel framework to iteratively refine the initial data batch and improve learned policies after each new batch collection, without dropping performance due to overly adventurous exploration. In every iteration, we seek to improve the data coverage by deploying a set of policies, that we previously trained to be diverse, i.e. that act in a variety of ways to explore, without compromising the rewards too much. The proposed IBRL algorithms adhere to safety constraints by restricting the state or action space of the learned policies depending on the data support. Through experiments in an illustrative 2D environment, as well as on the Industrial Benchmark [3], we demonstrate the improved exploration capability and resulting improved performance of our approach, all while maintaining safety considerations and not underperforming the behavioral. Conceptually, our work is most closely related to [4, 5, 6], however these works do not address the combination of diversity and safety, or focus solely on the iterative process without incorporating an exploration incentive."}, {"title": "2 Related Work", "content": "Offline RL: Early works in the so-called \"batch RL\" setting include [7, 8, 9, 10], as well as more recently [11, 12, 13, 14]. While these works focused on reinforcement learning in the batch setting, a key distinction to later proposed offline methods is that they mostly assume the initial batch to be collected under uniform random actions, allowing a relatively well explored environment. While the batch RL setting is certainly closer to the requirements imposed by real-world deployments of RL algorithms than commonly popular online algorithms such as [15, 16, 17], it is still not exactly what industry practitioners need most of the time. Algorithms such as [18, 19, 20, 21, 22, 23] employ various regularization techniques to keep the trained policies in regions of the environment where the models are sufficiently accurate. Most common techniques include behavior regularization [24, 19] and uncertainty-based regularization [25, 26]. In offline RL, model-based RL appears to have an edge over model-free methods, which enjoy better asymptotic performance but have generally been attributed lower data efficiency [27, 23, 28]. Methods such as [14, 1, 29, 30] have thus developed ways to extend action divergence regularizations to the model-based setting. Offline-to-Online: Some of the early batch RL works mentioned above as well as [4, 31] introduced the idea of the growing batch setting, a problem in between offline and online learning where one is constrained to only deploy a limited number of times to the real system to collect new data. While the idea is appealing, our approach differs significantly since their almost unconstrained exploration can be an issue due to safety constraints or requirements on the minimal performance. Other works that explored online finetuning of offline pretrained policies without the deployment constraint exhibit similar issues due to their exploration strategies [32, 23, 33, 34, 35]. Diversity: Intrinsically motivated agents have been introduced in [36]. Agents have since been found to perform effective exploration using intrinsic reward objectives like curiosity or diversity and have been studied e.g. in [37, 38, 39]. Previous work has also explored the effect of collecting initial diverse data that is further used for offline learning in several downstream tasks [40, 41, 42], as well recently [43, 44]."}, {"title": "3 Method", "content": "Offline reinforcement learning uses a fixed dataset D collected by one or more behavior policies $\\pi_\\rho$ to learn a policy $\\pi$. Once trained, the agent's policy is fixed and no further learning occurs during deployment. To ensure safety, the learned policy is often constrained to avoid significant deviations from the original behavior policies used to generate the data.\nOne approach for learning a policy is a model-based policy search. This is a two-step process. First, a simulation model is learned from the available data. In the second step, one or a set of policies are optimized using virtual rollouts. For policy search, we seek to minimize the loss function\n$L(\\theta) = \\frac{1}{N K H} \\sum_{k=1}^{K} \\sum_{S_{k, 1} \\sim D} \\sum_{t=1}^{H} \\gamma e(s_{k, t}, a_{k, t}),$ (1)"}, {"title": "3.1 Safety", "content": "We formulate three approaches towards realizing safety in policy search. Safety may be used (i) as an additional objective in the loss function, (ii) as a soft constraint, or finally, (iii) it may also be possible to constrain the policy directly as part of its architecture.\nSafety as an objective Safety is injected as an explicit objective in the loss function to balance between high rewards and not deviating from the behavior policy. It was used in previous research work including [45, 46, 20]. At first, a behavior policy is learned in a supervised manner from the fixed initial dataset as denoted in Eq. (3). Afterwards, for every state, the behavioral policy action and the new policy actions are predicted and used to define the deviation between the behavior policy and the learned policy. The deviation in the simplest case is a mean-squared error between both actions. If a distribution of actions is learned for both policies, the Kullback-Leibler divergence (KL divergence) could instead be used to assess the deviation. In the MSE case we have:\n$L(\\phi) = \\sum_{s,a\\sim D} || a - \\pi_{\\beta}(s; \\phi) ||^{2}.$\nThen, the loss function including the reward maximization and safety objectives is defined as\n$L(\\theta) = \\sum_{k=1}^{K} \\frac{1}{K} \\sum_{s_{k,1}\\sim D} \\sum_{t=1}^{H} [-\\gamma e(s_{k, t}, a_{k, t}) + (1 - \\lambda) p(a_{k, t}),$\n$p(a_{k, t}) = ||\\pi(s_{k, t}; \\theta_{k}) - \\pi_{\\beta}(s_{k, t}; \\phi)||^{2}.$\nWhile this is the standard approach for safe offline RL it has one key drawback: Weighing safety against performance (and possibly diversity) in the form of a trade-off (in this case via the parameter $\\lambda$) appears counter-intuitive for safety-critical applications.\nSafety as a soft constraint An alternative approach is to specify a loss term that is flat inside a safe region and then provides a large loss value outside, thereby implementing a differentiable constraint. In contrast to the aforementioned objective-based approach, here the safety term is not weighted against the objective but instead is effectively restricting the allowed solution space of the policy."}, {"title": "3.2 Diversified Policy Search", "content": "We define diversity as the ability to discover different or dissimilar state regions. This translates back to having a high entropy on the trajectory samples used for training and deployment. Given an ensemble of K policies, we draw K trajectory samples $T_{1}, ..., T_{K}$, where $T_{e}$ results from the interaction between the learned transition model $f(s, a; \\eta)$, reward model $f(s, a; \\omega)$ and policy $\\pi_{\\theta}$ under the same starting state $s_{1}$. Let $D(\\theta, \\eta, \\omega)$ denote the distances between all trajectories. The diversity-based exploration enforces the maximization of $D(\\theta, \\eta, \\omega)$, by adding the diversity loss as an intrinsic motivation for exploration.\n$L_{d}(\\eta, \\omega, \\theta) = -D(T_{1}, T_{2}, ..,T_{K}) = -D(\\theta, \\omega, \\eta)$\nDiversity measures We wish for the diversity-based objective to be differentiable w.r.t $\\theta_{k}$ [47, 48]. Different distance metrics between trajectories were used in different communities [49, 50, 51]. The simplest distance metric is the pairwise distance between trajectories, also called lock-step Euclidean"}, {"title": "3.3 Algorithm", "content": "We will show here one instantiation of a possible loss function using a soft-constrained policy and minLSD diversity.\n$L0) = \\frac{1}{N K H} \\sum_{k=1}^{K} \\sum_{S_{k, 1} \\sim D} \\sum_{t=1}^{H} e(s_{k, t}, a_{k, t})$\n$+ \\alpha_{s} \\frac{1}{K H} \\sum_{k=1}^{K} \\sum_{t=1}^{H} max(G(s_{k, t}, \\pi(s_{k, t};\\theta_{k})) + \\delta, 0)$\n$+ \\alpha_{d} \\frac{1}{H} min_{k' \\neq k, k \\epsilon K} D(T_{k}, T_{k'})$ (10)"}, {"title": "4 Experiments", "content": "We investigate the advantage of the proposed iterative batch reinforcement learning framework in improving learned policies. We seek to investigate the supplementary advantage of incorporating diversity in the process. Finally, we investigate how different forms of safety impact the diversity objective. We evaluate our methods on a 2D grid environment and the industrial benchmark [3]."}, {"title": "4.1 2D Grid Environment: Single Iteration", "content": "The 2D grid environment is a simplistic benchmark illustrating common navigation tasks. The state (x, y) represents the position of an agent in space. The agent is rewarded according to its position, following a Gaussian distribution with mean $\\mu = (3, 6)^{T}$ and diagonal covariance matrix $\\Sigma$ with standard deviation vector $(1.5, 1.5)^{T}$. Concretely, the reward of an agent at state $s_{t}$ is the likelihood of the reward Gaussian distribution: $r(s_{k, t}) = \\frac{1}{(2\\pi) \\sqrt{|\\Sigma|}} e^{-\\frac{1}{2} (S_{k, t}-\\mu)^{T} \\Sigma^{-1} (s_{k, t}-\\mu)}$. See Figure 2 for a visualization of the reward distribution. We gather the initial data by deploying a behavior policy that navigates to one of the behavior goals denoted in Figure 2 and fixed to the positions $(2.5, 2.5)^{T}$ and $(7.5, 7.5)^{T}$. The agent navigates to the goal closest to its current position. Additionally, the behavior policy is augmented with 10% uniform random actions.\nFor reference, we first train a single policy without diversity for different values of $\\lambda$ for one single iteration. Figure 3 represents the policy maps of actions taken by policies learned with increasing $\\lambda$ values in the 2D grid environment. With $\\lambda = 0.0$, the objective is reduced to mimicking the behavior policy. In the case of $\\lambda = 0.4$, the policy predominantly adheres to the behavior policy with slight adjustments in the direction of certain actions. Nevertheless, convergence towards both behavior poles is still observable. This validates our further assumption of fixing $\\lambda$ at 0.4.\nFigure 4 compares the results of a policy training without and with diversity. Here, we used the \"safety as an objective\" approach. We can see that diversity leads to a more heterogeneous set of policies. Additionally, we also observe that the safety term still has a significant impact on the policy. This result shows that diversity and safety can be combined to obtain safe, but different policies."}, {"title": "4.2 Industrial Benchmark: Iterative Batch RL", "content": "The Industrial Benchmark serves as a reinforcement learning simulator specifically designed to replicate challenges commonly encountered in industrial settings, such as high dimensionality,"}, {"title": "5 Conclusion", "content": "In this paper, we presented an algorithm for iterative batch reinforcement learning based on model-based policy search. We extended the aforementioned method via a safety mechanism using two distinct approaches and incorporated diversity to exploit the iterative nature of the problem. Our experiments demonstrate the effectiveness of using an iterative process in an offline reinforcement learning setting to enhance policy learning. Moreover, incorporating diversity provides targeted improvements to the policies with each iteration compared to setups without diversity."}, {"title": "A Dataset visualization for industrial benchmark", "content": "A.1 Random bounded dataset\nData collected by uniform sampling from the action space such that the states are restricted to lie within the safety bound [30, 70]. The samples are collected by generating five rollouts of horizon 200 each.\nA.2 Medium Policy with Randomness\nWwe use a simple policy (referred to as medium) to generate the initial batch, which tries to navigate to a fixed point in the state space, as also done in [45, 13]. The batch is collected by randomly sampling a starting state in the bound [0, 100] and subsequently following:\n$\\Pi_{\\beta \\text {medium }}(S t)=\\left\\{\\begin{array}{l}50\\\\50\\\\50-h_{t}\\end{array}\\right.$ (12)"}]}