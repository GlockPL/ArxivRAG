{"title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models", "authors": ["Andreas M\u00fcller", "Denis Lukovnikov", "Jonas Thietke", "Asja Fischer", "Erwin Quiring"], "abstract": "Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in generative AI, especially latent diffusion models (LDMs), have made it extremely challenging to distinguish between authentic and AI-generated images. Despite a multitude of beneficial applications, this also introduces a considerable range of threats. For example, so-called deepfakes-compellingly realistic but AI-generated media-are now routinely used to defraud people, distribute misinformation, and to manipulate public opinion [11, 15].\nThus, there is an urgent need to detect and to attribute Al-generated images, that is, to identify that an image is AI-generated and to determine who used the AI system to generate it. Watermarking is currently one of the main approaches for achieving these goals, and leading AI companies such as Google and Meta, as well as AI platforms such as Hugging Face and Google Gemini, are either already using it or have made commitments to do so [2, 8, 16, 19].\nRecently, a novel family of watermarking methods for LDMs has been proposed that relies on the inversion of the denoising process in the diffusion model [7, 17, 38, 42]. These semantic watermarks work by modifying the initial latent noise to contain a specific watermark pattern which can be recovered through inversion of the denoising process. Semantic watermarks offer several advantages. First, they assert greater robustness against various image transformations and attacks compared to previous watermarking methods. Moreover, the DM remains unchanged, which makes the watermarking approach easy to deploy. However, it is critical that these watermarks are also secure against forgery attacks. Fig. 1 illustrates this threat where an attacker transfers the watermark of a service provider to any image. This can lead to an erosion of trust in the watermarking system, as real images are flagged as watermarked and thus AI-generated or regular users are blamed for distributing harmful content.\nIn this work, we uncover a fundamental security weakness of semantic watermarks. We demonstrate that attackers can use unrelated models, even with a different latent space"}, {"title": "2. Background", "content": "We start by introducing our notation and revisiting diffusion models and watermarking techniques."}, {"title": "2.1. Diffusion Models and Inverse DDIM", "content": "Diffusion models (DM) [18, 33\u201336] are a class of probabilistic generative models that learn a transport map between some known distribution (e.g. a standard Gaussian) and the target data distribution as the reversal of a diffusion process. During training, DMs learn a denoising function $u(\\cdot)$ that takes a noisy image $z_t$ from an arbitrary diffusion step $t\\in \\{0,...,T\\}$ and tries to predict the noise-free version $x_{t-1}$, which is then passed to the samplers of the diffusion process to compute a slightly less noisy version of the image, $z_{t-1}$. Various samplers have been proposed, with the goal to increase sampling speed [18, 21, 23, 32, 39, 45, 46] and DDIM [18] being one of the most common and straightforward sampling methods in use.\nCurrently, most DMs are learned in a lower-resolution latent space, which enables higher computational efficiency and leads to the name LDM. Formally, given an image $x$, an LDM uses an encoder $\\mathcal{E}$ to project $x$ to the latent space, i.e., $z_0 = \\mathcal{E}(x)$, and a decoder $\\mathcal{D}$ to project the latent presentation back to pixel space, i.e., $x' = \\mathcal{D}(z_0)$. The DDIM sampler samples along a deterministic trajectory defined by the following denoising steps:\n$z_t = \\sqrt{1 - \\alpha_t} \\cdot u(z_t, t, C'),\\nonumber \\\\  z_{t-1} = \\sqrt{\\alpha_{t-1}}z_t + \\sqrt{1-\\alpha_{t-1}} u(z_t,t,C'),$ (2)\nwhere $\\alpha_t = \\prod_{s=1}^t(1 - \\beta_t)$ and $\\beta_t$ define the noise schedule. $C$ is the conditioning information (e.g. textual embeddings). During generation, $z_T$ is typically sampled from $\\mathcal{N}(0, I)$. We denote the full denoising process of the trained model mapping $z_T$ back into the latent space of the auto-encoder as $G_{T\\rightarrow0}(z_T; u)$. The full generative model is denoted as $\\Theta = (\\mathcal{E}, u, \\mathcal{D})$.\nInverse DDIM sampling [26] tries to follow the trajectory that (would have) generated a given image in reverse. It starts from a latent image $z_0$, and adds noise in a step-wise fashion, where the t-th step is described by\n$z_{t+1} = \\sqrt{\\alpha_{t+1}}z_t + \\sqrt{1 - \\alpha_{t+1}} u(z_t, t, C),\\nonumber $ (3)\nsimply inverting the denoising steps. Note that this is different from the forward diffusion process, where random noise is added to the image. Instead, this sampler tries to follow the trajectory that generated the given image in reverse, which can be done even without knowing $C$ [26]. We denote the inverse sampling process as $I\\Theta_T(z_0; u)$."}, {"title": "2.2. Watermarking", "content": "The powerful generation capabilities of diffusion models have also created an urgent need for techniques to identify and attribute images produced by these models [2, 4, 10]. In the following, we first study the use case of watermarking for generative AI and then examine watermarking approaches.\nApplication. Watermarking supports a safe and trustworthy usage of AI-generated content by enabling (1) detection and (2) attribution of generated content. Detection means determining whether a certain image was generated by the service provider (SP). The watermark allows the SP to claim copyright or to mark an image as AI-generated, which can be leveraged for deepfake detection. In attribution, the goal is to identify the user who generated a certain image using the SP. Watermarking enables the SP to identify who is violating the copyright or is distributing NSFW content.\nWatermarking methods. Various post-processing watermarking techniques for images have been proposed [1, 9, 24, 37, 43, 48], that are applied in a post-hoc manner on existing images. Recently, watermarking methods have been developed specifically for use in large-scale text-to-image"}, {"title": "3. Black-Box Attacks on Semantic Watermarks", "content": "Semantic watermarks require keeping the original generative model secret-with the assumption that this prevents an attacker from forging (and removing) the watermark. In this section, however, we propose attacks that utilize a proxy model $\\Theta_A = (\\mathcal{E}_A, u_A, \\mathcal{D}_A)$ for watermark forgery (and removal). In the following, we describe two forgery strategies and also discuss an adaption for watermark removal. Fig. 3 illustrates the attacks' main principle."}, {"title": "3.1. Watermark Imprinting Attack", "content": "Our first attack takes a clean cover image (real or generated) and slightly modifies it such that it is verified as watermarked by the SP. The procedure is as follows:\n(I) The attacker takes a watermarked target image $x^{(w)}$ that another user has generated using the SP's model $\\Theta$.\n(II) The attacker uses the encoder $\\mathcal{E}_A$ of the proxy model to map the watermarked target image to the latent space of the attacker's auto-encoder: $z_0^{(w)} = \\mathcal{E}_A(x^{(w)})$. Next, the attacker estimates the latent noise $z_T^{(w)}$ by running the inverse DDIM sampler $I\\Theta_T(z_0^{(w)}; u_A)$ of the proxy model."}, {"title": "Extension: Masking to preserve important details.", "content": "For small image sizes, certain objects such as text and human faces are susceptible to small changes in the latent variables. Simply encoding and decoding a real image with common auto-encoders already leads to noticeable distortions. In order to preserve crucial details like faces and text, we introduce a simple, yet effective extension of the procedure described above. This extension is illustrated in Fig. 4. Each gradient update on $\\delta$ uses a downsampled mask $m_z$ to keep certain parts of the latent the same, i.e., the $\\delta$ resulting from the gradient update is replaced by $\\delta * (1 \u2013 m_z)$. Additionally, after optimization and decoding, we paste the original pixels from $x^{(c)}$ corresponding to the masked region to the output of the imprinting procedure $x'^{(c)}$ to prevent distortion introduced by the decoder $\\mathcal{D}_A$.\nWatermark Removal. It is also possible to remove watermarks using the same methodology. In the removal set-up, instead of a cover image, we have the same watermarked image $x^{(w)}$ that we aim to modify so that it no longer contains its original watermark. To accomplish this, we simply change the target $z_T^{(w)}$ in Eq. (4) to its negation $\u2212z_T^{(w)}$ (Step (III)). This leads to the following loss, which encourages erasing the watermark pattern encoded in the initial latent noise of the watermarked image:\n$\\mathcal{L}_{imprint}(\\delta) = |I\\Theta_{0\\rightarrow T}(z_0^{(w)} + \\delta; u_A) + z_T^{(w)} |^2 $ (5)\nImages obtained through this procedure should no longer verify as watermarked."}, {"title": "3.2. Watermark Reprompting Attack", "content": "Our second attack strategy aims at generating new (potentially harmful) images that are verified to have the target watermark. To this end, we replace Step (III) of the previously described imprinting attack. Starting from the extracted $z_T^{(w)}$, the attacker just generates a different image $x'$ with the desired target prompt using the proxy model $\\Theta_A$.\nMultiple Prompts and Bin Resampling. To improve the attack effectiveness, we propose two simple, yet effective augmentations. First, the attacker can try multiple prompts (all of which may prompt for harmful content). Second, for Gaussian Shading, the attacker can also resample $z_T^{(w)}$ such that the values in $z_T^{(w)}$ remain in the same bin. In the default setup of Gaussian Shading, it means each value must retain its sign. This procedure preserves the encoded watermark in $z_T^{(w)}$, but provides a novel seed for the diffusion process."}, {"title": "4. Evaluation", "content": "We proceed with an empirical evaluation to demonstrate the effectiveness of the proposed attacks."}, {"title": "4.1. Experimental Setting", "content": "We use Stable Diffusion 2.1 [30] as attacker model and evaluate several common models as target model, namely Stable Diffusion XL (SDXL) [28], PixArt-\u03a3 [5] and FLUX.1. We also include our own finetune of SD2.1 trained on anime (SD2.1-Anime). All experiments use 512 \u00d7 512 images.\nWe evaluate the two primary semantic watermarking approaches: Tree-Ring [38] and Gaussian Shading [42]. To ensure consistency with their respective experimental setups, we consider the same scenarios and metrics. As Tree-Ring operates only in the detection scenario, we measure the watermark detection rate using the TPR@1%FPR. The underlying"}, {"title": "4.2. Imprinting Attack for Forgery", "content": "In our first experiment, we investigate whether an attacker can imprint a semantic watermark on existing images by using our proposed imprinting attack. For pairs of 100 watermarked images and natural cover images, we minimize Eq. (4) for 150 steps with a learning rate of 0.01, and evaluate at every tenth step. As we outline in Sec. 5, there are no suitable baselines for comparison."}, {"title": "4.3. Imprinting Attack for Removal", "content": "In our next experiment, we test the effectiveness of the proposed watermark removal technique. We optimize Eq. (5) for"}, {"title": "4.4. Reprompting Attack", "content": "In this experiment, we evaluate the effectiveness of our reprompting forgery attack. As outlined in Sec. 3.2, we have a basic variant which re-generates an image on $\\Theta_A$ with another prompt, and an augmented variant that tries $k$ prompts, and for Gaussian Shading $l$ bin resamplings in addition. For the basic variant (\"Attk\"), for each target model, we use 1000 watermarked images and re-generate an attack image using a harmful prompt. For the augmented variant (\"Attk+\"), for each target model, we use 100 target images and set $k = 3$ and $l = 3$. That is, we have 3 attack candidates for each target image with Tree-Ring, and 9 attack candidates with Gaussian Shading. If the watermark is valid in one of the candidates, we count the target image as successfully forged.\nResults. Tab. 3 depicts the attack performance. Both watermarking methods can be successfully tricked. With Gaussian Shading, the attack achieves a near-perfect detection and identification rate on all models. With Tree-Ring, three out of four models detect the forged watermark in almost all cases. Similar to the imprinting attack, FLUX.1 is harder to attack; however, 35% of the images are still detected as watermarked which is enough to undermine the method. Comparing the two attack variants, even the basic variant achieves a high success rate in different settings, which can often be increased to (almost) 100% using the augmented variant."}, {"title": "4.5. Transferability Analysis", "content": "The previous experiments show the effectiveness of our proposed attacks. In the next step, we further analyze the transferability across different models to understand the extent to which models are vulnerable. To this end, we examine our reprompting attack in the basic variant across different pairs of target and attacker model. In addition to SD2.1, SDXL and PixArt-\u03a3, we include SD1.5, Mitsua Diffusion One, Common Canvas S-C [14] and Waifu Diffusion. We consider Mitsua and Common Canvas as they share the same architecture as Stable Diffusion models but are trained from scratch on different datasets. Waifu Diffusion is an extensive community fine-tune of SD1.4. Additional information on these models are provided in Sec. C in the Supplementary Material. For each target model, we use 100 watermarked images from this model to create respective forgeries on the attacker model.\nResults. Fig. 10 illustrates the transferability in terms of bit accuracy (Gaussian Shading) and p-value (Tree-Ring). Similar models (SD1.5, SD2.1) and their fine-tunes (Waifu) show nearly-100% bit accuracy and extremely low p-values across all combinations. When considering models that have been trained independently from scratch (Mitsua, Common Canvas) on public domain data but have a similar architecture, we also observe a high transferability. Thus, even models with different training data and possible different training protocols are severely impacted by our attack.\nAnalysis. We continue by analyzing the unexpected transferability of inversion that enables our attacks. We find a correlation between the success of attacks on different target and proxy models and the functional similarity of their corresponding auto-encoders. This indicates that our attacks benefit from high similarity in the latent spaces between target and proxy models. We encourage readers to consult Sec. D in the Supplementary Material for more details."}, {"title": "4.6. No Defense by Adjusting Thresholds", "content": "Finally, we demonstrate that a defender cannot simply make the thresholds in the watermarking method tighter, that is, a lower p-value for Tree-Ring and a higher bit accuracy for Gaussian Shading. The first problem is that a watermark must be resilient to common perturbations, such as JPEG compression and Gaussian Noise, to be practical. This requirement significantly limits the feasible thresholds. The second problem is that our attack images achieve p-values and bit accuracies that already overlap with those of the original, unperturbed watermarked images.\nSetup. We adopt the attack setup as outlined in previous sections, and report results for SD2.1-Anime and PixArt-\u03a3 as target models. We consider the standard perturbations from related work that are applied to watermarked images before watermark verification: JPEG compression (82 quality), Gaussian Noise (\u03c3=0.1), Salt-and-Pepper noise (p=0.05), brightness jitter, Rotation (3 degrees), 90% crop-and-scale. We measure the p-value for Tree-Ring and the bit accuracy for Gaussian Shading on watermarked images that have undergone one of these perturbations, and on attack images from our different forgery attacks. We provide examples of the transformations in Sec. C in the Supplementary Material.\nResults. Fig. 9 depicts the measured p-values for Tree-Ring and bit accuracies for Gaussian Shading across different input types. It is not possible to tighten the detection threshold enough to distinguish attack images (orange, red and purple bars) while maintaining robustness to common perturbations (blue bars). For example, salt-and-pepper noise results in higher p-values for Tree-Ring and lower bit-accuracies for Gaussian Shading, which are comparable to those of our attack images. We can conclude that tightening thresholds is not a viable defense."}, {"title": "5. Related Work", "content": "In the following, we recap related work that examines security aspects of inversion-based semantic watermarks.\nForgery Attacks. There is little prior work on the forgery of semantic watermarks. These works focus on Tree-Ring and operate on unrealistic assumptions. Saberi et al. [31] propose a method against Tree-Ring where the attacker requests the SP to generate a white-noise image containing a watermark, and then blends this image into a clean cover image. However, obtaining white-noise images from a black-box API through prompting is challenging and easy to defend against. In their code implementation, this is realized by having the SP artificially sample white-noise pixels, applying DDIM inversion, embedding a Tree-Ring, and regenerating the image. Furthermore, the attack does not work for watermarks"}, {"title": "6. Discussion and Conclusion", "content": "The attacks described in this paper expose fundamental issues in the practical use of current inversion-based semantic watermarking approaches. In a realistic scenario, where the attacker can simply leverage their own model, we show that the attacker can forge and remove semantic watermarks with almost perfect success rate. Concerningly, even a large difference between the attacker and the target watermarked model still permits an attack, as the high attack success rates on completely unrelated and independently trained target models show. For instance, our attacks succeed even when the attacker uses an SD2.1 model against PixArt-\u2211 and FLUX.1, which are both DiT [27] models trained from scratch.\nFrom a defense perspective, we find that adjusting the watermark detection thresholds is not effective. We believe that fundamental improvements of semantic watermarking techniques are required to prevent watermarks from being easily imposed on new images. Another avenue of future work is the development of semantic watermarks that do not rely on inversion, similarly to the work of Feng et al. [12]."}, {"title": "A. Semantic Watermarking", "content": "In this section, we present more details on the examined semantic watermarking methods.\nTree-Ring. Wen et al. [38] have initially presented the concept of inversion-based semantic watermarking. Fig. 11a illustrates the proposed Tree-Ring watermark approach, which can be divided into two phases.\n\u2022 Generation. During image generation, Tree-Ring modifies a clean initial latent $z_T \\sim \\mathcal{N}(0, I)$ by adding a concentric circular pattern into its frequency representation. This step produces the watermarked latent noise $z_T^{(w)}$. This noise vector is then used as usual in the further generation process (denoising + decoding) to finally obtain the watermarked image: $x^{(w)} = \\mathcal{D}(G_{T\\rightarrow0}(z_T^{(w)}; u))$.\n\u2022 Verification. The inverse $z_0^{(w)} = I\\Theta_T(\\mathcal{E}(x^{(w)}); u)$ is computed, and then the existence of the original circular pattern in the frequency spectrum of $z_0^{(w)}$ is checked. The final detection is based on a statistical test. The test statistic aggregates the squared absolute difference between the observed and the expected frequency values from each respective ring. The null hypothesis $H_0$ is that an input image is not watermarked. In this case, its inverted initial latent noise should follow a Gaussian distribution with an unknown variance. Under $H_0$, the test statistic leads to a noncentral \u03c72 distribution. This allows computing a p-value that reflects the probability of observing the test-statistic output under the assumption that the input image is not watermarked. If the p-value is below a pre-defined threshold \u03c4, H0 is rejected and the watermark\u2019s presence is assumed.\nTree-Ring is designed as a so-called zero-bit watermarking scheme, as only watermark presence or absence can be detected. Note that the modification to the sampling procedure changes which images will be generated (it is not distribution-preserving), but appears to still have sufficient variety in generated images while retaining image quality.\nA subsequent work, RingID [7], improves Tree-Ring by making it more robust to perturbations, ensuring that the distribution of $z_T^{(w)}$ is closer to $\\mathcal{N} (0, I)$, and providing so-called multi-bit watermarking which can carry a multiple bit long message and thus allows distinguishing between different users.\nGaussian Shading. Yang et al. [42] expand the previous concept by relying on cryptographic primitives to achieve distribution-preserving generation. Fig. 11b illustrates the two phases of Gaussian Shading:\n\u2022 Generation. A message s of length k is created. This message s is first repeated (\u201cdiffused\u201d) p times to obtain $s^d$, which is then encrypted using the symmetric stream chipher ChaCha20 [3]. This stream cipher takes a secret key and a random seed as input, which are completely random for each image. The encrypted message m is then used to steer the sampling of $z_T^{(w)}$. To this end, Gaussian Shading splits a Gaussian distribution into $2^l$ bins with equal probability. In the following, we focus on the standard setting with l = 1. This means we have two bins, the negative and the positive area of the Gaussian curve. The bit in the encrypted message $m[i] \\in \\{0, 1\\}$ specifies if $z_T^{(w)}[i]$ is sampled from the negative or the positive area of a Gaussian distribution. Due to the encryption, the bits in m are uniformly distributed, ensuring a similar number of positive and negative samples from the Gaussian distribution. Hence, $z_T^{(w)}$ still follows a Gaussian distribution. After this sampling step, the usual generation process continues with $x^{(w)} = \\mathcal{D}(G_{T\\rightarrow0}(z_T^{(w)}; u)).$"}, {"title": "B. Example Images and Image Quality Trade-off", "content": "We provide image examples to give more intuition on the attacks and the resulting visual quality. Note that all image examples are of size 512 \u00d7 512, and the attacker's proxy model is SD2.1. We examine different aspects of the attacks in the following figures:\n\u2022 Fig. 12 shows the progression of the imprinting forgery attack with respect to the number of optimization steps.\n\u2022 Fig. 13 shows successful examples of the imprinting forgery attack on different target models and both watermarking approaches (Tree-Ring and Gaussian Shading).\n\u2022 Fig. 14 depicts successful examples of watermarking removal variant of the imprinting attack.\n\u2022 Fig. 15 shows successful examples of the reprompting attack.\n\u2022 Fig. 16 provides a comparison between our removal attack and the AdvEmb attack as baseline.\n\u2022 Finally, Fig. 17 also presents graphs showing the trade-off between detection rate by the target model and image quality for four combinations of settings: imprinting forgery & removal, and Tree-Ring & Gaussian Shading."}, {"title": "C. Experimental Details", "content": "Here, we provide more details on our experimental setup, including the datasets used, details on our finetune of Stable Diffusion 2.1 (SD2.1-Anime), the number of samples used in each experiment, the watermark parameters, and the runtime of our attacks."}, {"title": "C.1. Attacker and Target Models", "content": "Our default setup is to use Stable Diffusion 2.1 as the attacker model, and SDXL, PixArt-\u03a3, FLUX.1, and our SD2.1-Anime model as the target models. Our transferability analysis (Sec. 4.5) is the exception where we test multiple combinations of attacker and target models. Tab. 4 provides an overview of all the models used in our experiments, including the scheduler employed and other relevant settings."}, {"title": "C.2. Prompting and Cover Image Datasets", "content": "Except for our SD2.1-Anime model, we use the Stable-Diffusion-Prompts dataset to prompt benign watermarked images from the target models for all experiments (Secs. 4.2 to 4.6). For the cover images used in our imprinting forgery attack (Sec. 4.2), we only use the MS-COCO-2017 Dataset [20]. In experiments involving our reprompting attack (Secs. 4.4 to 4.6), we use the Inappropriate Image Prompts (I2P) dataset (sorting the \"inappropriate percentage\" column in descending order) to generate harmful images with the attacker model."}, {"title": "C.3. Finetuned Model SD2.1-Anime", "content": "Our own finetune of SD2.1 is trained on 10,000 pairs of anime images and captions from the Anime-with-caption-CC0 dataset. We use the training scripts for Stable Diffusion models from Huggingface with default parameters for full finetuning of the UNet parameters and for LoRA finetuning. We add a keyword to each caption in the training set and reuse this keyword when generating images during our experiments. For every experiment involving SD2.1-Anime as the target model, we prompt the model with prompts similar to those in the training phase, by taking prompts from a separate split of the same dataset and adding the keywords as prefix. By finetuning our own model, we achieve two goals: First, we obtain a baseline for a target model which is equal to the attacker model (SD2.1) in all aspects except for slight changes in the UNet parameters. This represents a scenario in which a service provider finetunes a publicly available model, and considers the resulting model unique enough to securely deploy semantic watermarks. Second, we are able to quantify the success of forgery attacks at different numbers of training steps for two different finetuning methods (full finetuning and LoRA) (see Sec. F). The model ultimately used in experiments is the fully finetuned variant."}, {"title": "C.4. Number of Samples in Experiments", "content": "We provide the exact numbers of prompts, generated images, and cover images for all our experiments.\n\u2022 Imprinting Attack for Forgery (Sec. 4.2). We generate 100 images with each target model (using 100 prompts) and apply our imprinting forgery attack on 100 cover images. This adds up to 400 imprinting forgery attacks (100 for 4 target models each) across a fixed set of 100 pairs of prompts and cover images in total. This procedure is performed for both Tree-Ring and Gaussian Shading, doubling the final number of attacks.\n\u2022 Imprinting Attack for Removal, Sec. 4.3. We generate 100 images with each target model (using 100 prompts) and apply our removal attack on each image. This adds up to 400 removal attacks (100 for 4 target models each) across a fixed set of 100 prompts in total. This procedure is done for both Tree-Ring and Gaussian Shading, doubling the final number of attacks.\n\u2022 Reprompting Attack, Sec. 4.4. We evaluate two different variations of our reprompting attack.\n\u2013 \u201cAttk\u201d: Here, we generate 1,000 images with each target model using 1,000 benign prompts and reprompt each one with the attacker model using 1,000 harmful prompts. This adds up to 4,000 reprompting attacks (1,000 for 4 target models each) across a fixed set of 1,000 pairs of benign and harmful prompts. This procedure is performed for both Tree-Ring and Gaussian Shading, doubling the final number of attacks.\n\u2013 \u201cAttk+\u201d: Here, we generate 100 images with each target model using 100 benign prompts. For each image, we reprompt each one 3 times with the attacker model using 300 harmful prompt, for both Tree-Ring and Gaussian Shading. For Gaussian Shading, we further resample the recovered latent $z_T^{(w)}$ for each target image 3 times before reprompting. This adds up to 1,200 reprompting attack for Tree-Ring (300 for each target model, across a fixed set of 100 benign and 300 harmful prompts), and 3,600 reprompting attacks for Gaussian Shading (900 for each target model, across a set of 100 benign and 300 harmful prompts) in total.\n\u2022 Transferability Analysis, Sec. 4.5. For each pair of attacker and target model, we generate 100 images using 100 benign prompts and reprompt each one with the attacker model using a set of 100 harmful prompts. This adds up to 4,900 reprompting attacks (100 for each of the 49 combination of target/attacker model) across a fixed set of 100 pairs of benign and harmful prompts in total. We apply this procedure for Tree-Ring and Gaussian Shading, doubling the number of attacks.\n\u2022 No Defense by Adjusting Thresholds, Sec. 4.6. We reuse the results from Secs. 4.2 and 4.4 as attack instances. Furthermore, we generate 1,000 images with each target model using 1,000 prompts. These are then transformed using common transformations and again verified by the target model. This adds up to an additional 1,000 image generations (1,000 for each of the two target models). We apply this procedure for Tree-Ring and Gaussian Shading, doubling the number."}, {"title": "C.5. Runtime of Attack Algorithms", "content": "All experiments were performed on 8 NVIDIA A40 GPUs. When executed on a single GPU for a single-batch attack example, the approximate runtimes for each attack algorithm are as follows:\n\u2022 The imprinting forgery (Sec. 4.2) and removal (Sec. 4.3) attacks require between 25 and 40 minutes to perform 150 steps. The most time-consuming part of these algorithms is the gradient-based optimization done by the attacker model (SD2.1 per default, except for Sec. 4.5). Verification by the target model is set to take place every 10 optimization steps and is comparably fast.\n\u2022 The reprompting attack (Sec. 4.4) requires between 30 seconds (smaller models including SD2.1, Mitsua Diffusion One) and 2 minutes (FLUX.1). This time includes all steps: generating a single image with a semantic watermark on the target model, inverting and regenerating on the attacker model, and verifying the presence of the watermark with the target model."}, {"title": "C.6. Image Transformations", "content": "Fig. 18 shows examples for the standard image transformations that we apply on watermarked images to examine the achievable thresholds of Tree-Ring and Gaussian Shading under these transformations (cf. Sec. 4.6 and Sec. E)."}, {"title": "C.7. Parameters for Tree-Ring and Gaussian Shading", "content": "In order to run Gaussian Shading and Tree-Ring, several parameters need to be selected.\nTree-Ring. In the original work of Wen et al. [38], the threshold for the p-value to assume the watermark's presence is computed by calculating receiver operating characteristics (ROC) curves. For their main results, they report AUROC and TPR@FPR=1%. As precise p-values are not reported, we conduct our own experiment similar to the original work. For each model evaluated, we generate 5,000 watermarked images as well as 5,000 clean images to determine the thresholds for desired"}, {"title": "D. Transferability Analysis", "content": "In this section, we present additional data for our transferability analysis from Sec. 4.5 by looking at the similarity of auto-encoders.\nWe evaluate the similarities of different VAEs, since all our models tested in Sec. 4.5 deploy VAEs with 4 latent channels and the same order of layers. We first examine the weights of the different autoencoders across the different models. SD2.1 and Common Canvas appear to share the exact same VAE, as all weights of all layers match with a precision of 10-3. Next, we examine if some of the VAEs are functionally similar by comparing their latents for a set of images. We use a random sample of 100 images from the MS-COCO-2017 dataset [20"}]}