{"title": "Are Transformers Truly Foundational for Robotics?", "authors": ["James A. R. Marshall", "Andrew B. Barron"], "abstract": "Generative Pre-Trained Transformers (GPTs) are hyped to revolutionize robotics. Here we question their utility. GPTs for autonomous robotics demand enormous and costly compute, excessive training times and (often) offboard wireless control. We contrast GPT state of the art with how tiny insect brains have achieved robust autonomy with none of these constraints. We highlight lessons that can be learned from biology to enhance the utility of GPTs in robotics.", "sections": [{"title": "Introduction", "content": "Recent years have seen major advances in generative Artificial Intelligence due to the development and deployment of a new architecture; the Generative Pre-Trained Transformer (GPT), or transformer for short 1. Through adding an attentional mechanism to deep neural networks and deploying on internet-scale training sets, transformers have led to rapid advancements in Large Language Models (LLMs) for natural language processing and generation. Following these early applications, transformers have, alongside other architectures such as diffusion models 2 been applied to the development of Visual Language Models for text to image and video (e.g. 3) as well as other multimodal applications (e.g. 4). These successes have inspired the investigation of transformer architectures for the robotics domain. The challenges of unstructured multimodal inputs sensed in complicated environments, coupled with high degrees of freedom in robot control, have constrained the development of robots that are simultaneously generally capable, and robust, in their behaviour. The promise of transformers for robotics appears to be that large-scale training can, through specialisation on further smaller-scale training sets, provide general and adaptable solutions to a wide variety of robotics tasks 5. Because they can be applied across so many application domains transformer-based approaches have been labelled Foundation Models 5 indicating their supposed fundamental status but also their incomplete nature. Applications of foundation models to robotics have recently taken off in the minds of developers and researchers.\nTransformers have their genesis in large language modelling (LLM). LLMs have also proved to be generalizable and transformative to many applications, but they are not without limitations. As we review below, there are increasingly recognised issues with LLMs in the areas of training dataset size, compute resources for training, the financial and ecological costs of both, as well as robustness of behavioural output. In this article we question whether transformer architectures are likely to be truly foundational for robotics. We ask whether transformers provide the only or best route towards Artificial General Autonomy, proposing that, unlike 'intelligence', 6 the level of autonomy of a robotics system is well-defined, measurable, and economically meaningful.\nDrawing on earlier critiques of GPTs and related approaches, we argue that transformers provide a facsimile of autonomy rather than true autonomy. We then review alternative approaches that have been proposed. The contrast between GPT solutions to autonomous robotics and biological solutions to autonomous behavioural control achieved by animal brains is stark. We explore this contrast to propose what is missing from current GPT approaches, and what could be added in to enhance robust and scalable robot autonomy."}, {"title": "Progress in Applying Transformer Architectures to Autonomy", "content": "Transformers have seen rapid application to robot autonomy. As well as high profile commercial announcements and demonstrations, end-to-end solutions to robot autonomy have been developed in the peer-reviewed literature by both academic and industrial groups, to tasks particularly focussing on robot navigation and dexterity (for a review, see 7).\nWhile the early promise of transformers for robot autonomy seems to be being realised, for a general and scalable solution it is essential to recognise that this technology still comes with significant limitations that will constrain future performance and adoption. While some of these may become less acute as the traditional efficiencies associated with the development and deployment of a novel technology are realised, we argue that there are fundamental structural issues with current transformer architectures, and that these should motivate a longer term search for alternative and complementary approaches, which we review later in this article."}, {"title": "Training Data Size and Cost Requirements are Likely to Grow", "content": "At the heart of the transformer approach to any problem is a scaling requirement. Given the lack of inductive biases these learning systems are highly flexible, however the corollary of this is that their training data requirements are vast. The usual model for deployment of a transformer-based foundation model is to train on an internet-scale corpus so that the model acquires multi-modal correspondences and domain knowledge, then further specialise on a smaller training data set for a specific set of tasks. The costs of this are very substantial. Even excluding environmental impacts, state-of-the-art LLMs cost on the order of $10s to $100s of millions per training episode 8.\nFor robotics applications, further training for particular tasks such as navigation and manipulation is usually required. The availability and cost of acquiring good training datasets is recognised as a major problem. Proposed solutions include the curation of open datasets covering multiple tasks and robot types \u00ba, although currently these can be biased to a relatively small number of tasks. There is also the extensive use of physics-based simulators to generate training data (e.g. 10). We argue that, similarly to LLMs, exponentially increasing quantities of data are likely to be required to sustain advances in performance 11. Even for text and multimodal datasets where the internet provides a very large corpus of training data' for free', the availability of training data risks becoming a limiting factor 12. For robotics datasets the costs of collecting useful training data, either physically or through simulation, will be much more acute. Furthermore, since improvement in transformers' performance is predicated on increases in scale of training data and weights this problem will only get worse."}, {"title": "Compute and Infrastructure Costs and Requirements will Persist", "content": "Once the costs of training a transformer-based architecture are paid, the inference costs at deployment can still be substantial. For example, Meta's Llama 3.1 has cloud-scale deployments (405bn double-precision parameters). There are also reduced size and precision versions suitable for deployment on local GPUs (e.g. 8bn half-precision integer parameters), which can take ~20-100GB of memory for inference. This demands a substantial GPU for even the simplest models being run on a robot 13. While binarisation, quantisation, and other approaches have been used to help design edge Al accelerators for deep and convolutional neural networks (e.g. 14), the scale of the problem for transformers is many orders of magnitude larger. For example, for one of the longest researched applications of deep nets, object detection, one state of the art algorithm has on the order of 10m-80m network weights 13, compared to the 8bn-405bn weights mentioned above for a state-of-the-art LLM. This represents a four orders of magnitude difference in scale, even before the additional requirements of training a transformer for robotics tasks are taken into account. Hence there is very active research into methods to avoid the cloud compute bottleneck, including utilisation of novel technologies such as 6G, 15. Moore's law and the advent of novel parallel compute architectures has traditionally saved Al, and computer software more generally. For foundation models, however, we argue that although available compute can be scaled exponentially, the exponential requirements for model size and throughput will be in opposition. A real-terms reduction in requirements for compute as performance improvements are sought will only occur when the exponent for the former is greater than the exponent for the latter. However increasingly we are considered to be in a post-Moore's Law world where further innovation in materials is required to make progress [e.g. 16]."}, {"title": "Hallucinations for Transformers in Robotics May Become Acute", "content": "As a consequence of their statistical training and inference, LLMs are prone to confabulation and hallucination 17,18. While such outputs can still be damaging even for a disembodied Al, when a transformer architecture is embodied the risks are magnified. As with humans, hallucinations may manifest in ways likely to cause harm to the robot or to others, and adversarial attacks on guardrails for transformers in robotics have already been demonstrated 19. This is likely to require that humans remain in the control loop as teleoperators to ensure robots are remotely supervised, or that robots are isolated from humans, or both. Any of these outcomes will of course limit the promised benefits of robotics. As other researchers have argued, these structural issues with statistical approaches to Al are unlikely to find remedy without significant architectural change 20."}, {"title": "Transformers Give a Facsimile of Intelligent Autonomy", "content": "Given the above concerns, why are transformers seeing increasing adoption for robotics? We attribute this to two factors: first, as with LLMs and VLMs, striking early advances have been made in traditionally very difficult areas, such as humanoid control, manipulation, and, of course, natural language interfaces. Second, however, we believe a tendency of human observers to anthropomorphise often leads some of them to ascribe abilities, and the potential for understanding, that the architecture does not, and cannot, technically support.\nWhile there are many types of transformer the central motif is a repeating unit composed of a self-attention block followed by a multilayer perceptron block 21 . The control flow is feedforward, while the attention mechanism learns which earlier elements of the input to attend to in predicting the next appropriate action. As with LLMs, both the power and generalizability of transformers for robotics comes from their extensive training so that, once trained, they can perform the operation of matching an input to a predicted output. In robotics transformers succeed in resolving and executing an action from an input, but this is achieved by interpolation and extrapolation of the training set, with unreliable off-training-set performance 22. There is no reasoning and no reason why a transformer selects one response over another, other than the selected option carrying the highest predictive weight following training 23. The same can be said of the language abilities of LLMs, which have been described as stochastic parrots 24.\nTraining and reference to learned experience is an important part of biological autonomous decision making too, but for humans and other animals decision making is also supported by reasoning from models of how the world works, how other involved agents should operate, and why the selected action is situation appropriate 25. Transformers lack these models 26,27. An autonomous robot's capacity will be limited by the scope of the training dataset. Since transformers responses are unreasoned products of the training data, any transformer-based application cannot justify a decision other than by statistical association. This poses serious challenges for any form of human / robot interaction. If we were to ask a well-intentioned human coworker why they made an error they would do their best to explain the reasoning behind their actions. If we ask a transformer based robot why it made an error there would be no reasoned answer per se; the answer to the query will have at best a correlation but no causal relationship to the error made."}, {"title": "Alternatives and Complements to Transformers for Autonomy", "content": "If transformers are not the full answer, what is? Here we review the main alternative proposals, with an emphasis on our preferred approach, drawing deep inspiration from how the biological brain solves the autonomy problem."}, {"title": "Natural Intelligence", "content": "The gulf between transformer approaches to robotics and how biological brains produce autonomous behaviour is stark. Most often comparisons are drawn between LLMs, GPTs and human reasoning 23,25,28, but the comparison with animal brains and animal reasoning is even more pronounced. For example, the honey bee brain is tiny (just over one cubic millimeter) and contains fewer than one million neurons 29. The number of synapses in the bee brain is not known, but if we can infer from the Drosophila connectome 30 there will be fewer than half a billion synapses in the bee brain. Demonstrably, this is all a bee needs to reliably navigate over long (several kilometre) distances, autonomously harvest pollen and nectar from the environment, communicate and coordinate their efforts with their hive mates, and perform all the many jobs needed to build and maintain their colony, including raising the next generation. They can solve complex foraging economics problems, majoring on the resources their colony needs and harvesting them from cryptic and ephemeral flowers patchily distributed in the environment 31. Bees are able to fly with no practice, and just twenty minutes of structured flight time around the hive is enough for them to be able to navigate proficiently in their environment 32. The contrast with the prolonged training needed by transformers could not be greater. The power consumption of a bee brain as it performs entirely on-board autonomous decision making is infinitesimal compared to any GPT. In contrast to transformers, animal brains have been massively 'pre-trained' on a planetary scale, to use minimal information and generate a very wide variety of behaviours .\nInsects lack the declarative reasoning of humans, but their reasoning is built around a form of elementary world model. Insects possess a unitary and coherent representation of external space within which they have a first-person perspective on objects around them 41. The valence of objects is influenced by the insect's learned experience with them, as well as innate valence and subjective physiological state 37. Differences in valence and location of objects arbitrate the insect's selection 42-44. This form of reasoning might be elementary, but it is still more comprehensible and explicit than the reasonless transformers. It is increasingly recognized that Al stands to benefit tremendously from importing concepts and algorithms from insect neuroscience 45,46"}, {"title": "Objective AI and World Models", "content": "Other researchers have proposed that indeed the autonomy abilities of animals (including those 'simpler than humans) should provide inspiration for Al researchers 47. However, this inspiration is much looser than the Natural Intelligence approach above. While the 'objective Al approach does indeed propose modular Al architectures that correspond with an understanding of the human brain developed in neuroscience, cognitive science, and psychology, the proposal is actually quite different; rather than directly seek to reverse-engineer neural circuits in specialist brain modules, instead the idea is to design trainable modules that interface with each other in order to generate more adaptive behaviour than a largely undifferentiated large neural net could be expected to. Thus, for example, rather than directly seek to understand how feature detectors in the early primate visual system function, a feature detector module would be trained. A key part of the proposal is the reintroduction of explicit and configurable worlds models, drawing inspiration from cognitive science; however these also remain trained from data 48."}, {"title": "Hybrid Approaches", "content": "Still other researchers, drawing on a long running proposal but also gaining renewed motivation from contemporary developments in Al, have proposed the \u2018neurosymbolic approach' 20. This approach argues that, while deep nets are very suitable for perceptual tasks such as object detection, they are fundamentally unsuited to the symbolic manipulation that is part of reasoning, planning, and decision making. In the context of transformers, this has recently been vindicated by observations that LLMs fail to robustly deal with and manipulate symbolic knowledge 26,27. Thus the proposal is to combine the perceptual strengths of statistical Al with the causal strengths of the older, symbolic, approach to Al. Given the neural bases of symbolic reasoning in the brain are poorly understood, this is a particularly pragmatic approach. In doing so it is hoped that the limitations of the first, symbolic, wave of Al, will be ameliorated by working around the problems they suffered in having sole responsibility for dealing with the perceptual complexity of the real world 49. We suggest that an even more powerful combination could include the use of Natural Intelligence approaches to perception and modelling of space and decision option sets within it."}, {"title": "Conclusion", "content": "Transformer architectures have brought to robotics the rapid progress that they had already brought to natural language and muli-modal Al. However, there are reasons to continue the search for solutions to the robotics autonomy problem. Transformer architectures treat the world in purely statistical terms, albeit grounded in perceptual inputs. This was arguably a deliberate choice in response to the\u02bb bitter lesson' 50, that inductive biases in Al have historically failed 49. However, this results in an autonomy solution very different to the way the only truly autonomous artefact known to humanity, the biological brain, functions. Here we have highlighted this, and conclude by arguing that the tremendous recent advances in data on, and understanding of, a variety of brains, means the time is ripe to revisit the 'bitter lesson', and see what new lessons for Al can be learned from their study."}]}