{"title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation", "authors": ["Yiyang Ma", "Xingchao Liu", "Xiaokang Chen", "Wen Liu", "Chenyue Wu", "Zhiyu Wu", "Zizheng Pan", "Zhenda Xie", "Haowei Zhang", "Xingkai Yu", "Liang Zhao", "Yisong Wang", "Jiaying Liu", "Chong Ruan"], "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [1, 7, 8, 68, 88]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [2, 15, 47, 49, 56, 57] and text-to-image generation [23, 71, 74, 77].\n\nThe field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [19, 25-27, 84]. However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [85, 93, 95, 96, 103] propose training a single LLM for both tasks, typically incorporating either diffusion models [32, 80] or vector-quantized autoregressive models [22, 83].\n\nOur approach builds upon recent breakthroughs in rectified flow models [3, 23, 55, 60, 61], which provide a simple framework for generative modeling while delivering exceptional empirical performance [23, 36, 45]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow's performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process.\n\nJanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods. Specifically, on text-to-image generation benchmarks, MJHQ FID-30k [48], GenEval [28] and DPG-Bench [34], JanusFlow achieves scores of 9.51, 0.63 and 80.09%, surpassing established text-to-image models including SDv1.5 [75] and SDXL [71]. In multimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on MMBench [62], SeedBench [46], and GQA [35], respectively, exceeding specialized models such as LLaVA-v1.5 [56] and Qwen-VL-Chat [4]. Notably, these results are achieved with a compact LLM architecture with only 1.3B parameters."}, {"title": "2. Related Work", "content": "Visual Generation with Flow-based Generative Models. Recent years have witnessed remarkable progress in visual generation through diffusion models [32, 80], leading to impressive models like [66, 71, 74-77]. Building on these advances, flow-based generative models [3, 55, 60] emerged as a simplified alternative framework. These approaches have recently enabled advanced visual generation models [23, 36] that achieve superior empirical performance with faster sampling. Our work demonstrates that rectified flow [59-61] can be effectively integrated into LLMs, creating unified models that excel in both understanding and generation tasks.\n\nUnified Models For Understanding and Generation. The development of multimodal large language models (MLLMs) has enabled effective integration of text and visual information. Building upon powerful LLMs [7, 88, 89], recent MLLMs [2, 15, 49, 56, 57, 63] have demonstrated exceptional multimodal understanding capabilities. Current research increasingly focuses on architectures that can simultaneously handle visual understanding and generation tasks. One approach extends MLLMs with pre-trained diffusion models [19, 25-27, 84, 97]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25, 84]. Another line of work [85, 93, 95, 96, 103] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22, 83] to convert images into discrete tokens, enabling unified autoregressive processing [85, 93]. While straightforward to implement, these approaches are inherently limited by their image tokenization quality.\n\nOur work focuses on developing unified models that combine autoregressive capabilities with flow/diffusion models, leveraging their proven effectiveness in visual generation. Compared to similar approaches [96, 103], JanusFlow offers three key advantages: (i) a simple yet effective generation process using rectified flow, (ii) enhanced performance through decoupled vision encoders that resolve inter-task conflicts, and (iii) improved generation quality through representation alignment regularization, enabled by our decoupled encoder design."}, {"title": "3. JanusFlow", "content": "In this section, we introduce the architecture of JanusFlow and our training strategies."}, {"title": "3.1. Background", "content": "Multimodal LLMs. Given a dataset $\\mathcal{D}$ containing discrete token sequences, each of which can be formulated as $x = (x_1,\\cdots, x_\\ell)$, large language models (LLMs) are trained to model the sequence distribution in an autoregressive manner,\n\n$\\log P_{\\Theta_{\\text{LLM}}}(x) = \\sum_{i=0}^{\\ell-1} \\log P_{\\Theta_{\\text{LLM}}}(x_{i+1}|x_1,..., x_i),$\n\nwhere $\\Theta_{\\text{LLM}}$ denotes the parameters of the LLM and $\\ell$ is the sequence length. After being trained on large-scale datasets, LLMs exhibit the ability to generalize across various tasks and follow diverse instructions [1, 8, 68]. To extend these models to handle visual inputs, LLMs are augmented with vision encoders [2, 56, 57]. For instance, LLaVA [57] integrates an LLM with a pre-trained CLIP [73] image encoder via a projection layer, transforming the extracted image features into a joint embedding space that the LLM can process as word embeddings. By leveraging large-scale multimodal datasets and increasingly powerful LLMs, this architecture has facilitated the development of advanced multimodal models capable of addressing a wide range of vision-language tasks [4, 47, 56, 63].\n\nRectified Flow. For a dataset $\\mathcal{D}$ consisting of continuous $d$-dimensional data points $x = (x_1,\\cdots, x_d)$ drawn from an unknown data distribution $\\pi_1$, rectified flow [55, 60] models the data distribution by learning an ordinary differential equation (ODE) defined over time $t \\in [0,1]$:\n\n$\\frac{dz_t}{dt} = v_{\\theta_{\\text{NN}}}(z_t, t), \\quad z_0 \\sim \\pi_0,$"}, {"title": "3.2. A Unified Framework for Multimodal Understanding and Generation", "content": "JanusFlow presents a unified framework designed to address both vision understanding and image generation tasks. Next we outline how JanusFlow handles these two tasks within a single LLM architecture.\n\nMultimodal Understanding. In multimodal understanding tasks, the LLM processes an input sequence consisting of interleaved text and image data. The text is tokenized into discrete tokens, each of which is transformed into an embedding of dimension $D_{\\text{emb}}$. For the images, an image encoder $f_{\\text{enc}}$ encodes each image $x_{\\text{im}}$ into a feature map of shape $H_{\\text{im}} \\times W_{\\text{im}} \\times D_{\\text{enc}}$. This feature map is flattened and projected through a linear transformation layer into a sequence of embeddings with shape $H_{\\text{im}}W_{\\text{im}} \\times D_{\\text{emb}}$. $H_{\\text{im}}$ and $W_{\\text{im}}$ are determined by the image encoder. The text and image embeddings are concatenated to form the input sequence to the LLM, which then autoregressively predicts the next tokens based on the input sequence of embeddings. According to common practice [85, 93, 96], we add special token |BOI| before the image and |EOI| after the image to help the model locate the image embeddings in the sequence.\n\nImage Generation. For image generation, our LLM takes a text sequence $x_{\\text{con}}$ as condition and generates a corresponding image using rectified flow. To improve computational efficiency, generation occurs in the latent space using a pre-trained SDXL-VAE [71].\n\nThe generation process begins by sampling Gaussian noise $z_0$ of shape $H_{\\text{latent}} \\times W_{\\text{latent}} \\times D_{\\text{latent}}$ in the latent space, which is then processed by a generation encoder $g_{\\text{enc}}$ into a sequence of embeddings $H_{\\text{gen}}W_{\\text{gen}} \\times D_{\\text{emb}}$. This sequence is concatenated with a time embedding representing the current time step $t$ ($t = 0$ at the beginning), resulting in a sequence of length $H_{\\text{gen}}W_{\\text{gen}} + 1$. Unlike previous approaches that employ various attention masking strategies [96, 103], we found that causal attention suffices, as our preliminary experiments showed no performance benefits from alternative masking schemes. The LLM's output corresponding to $z_0$ is transformed back into the latent space by a generation decoder $g_{\\text{dec}}$, producing a velocity vector of shape $H_{\\text{latent}} \\times W_{\\text{latent}} \\times D_{\\text{latent}}$. The state is updated by a standard Euler solver,\n\n$z_{t+dt} = z_t + v(z_t,t)dt,$\n\nwhere $dt$ is a user-defined step size. We replace $z_0$ with $z_{dt}$ on the input and iterate the process until we get $z_1$, which is then decoded into the final image by the VAE decoder. To enhance generation quality, we employ classifier-free guidance (CFG) when computing the velocity:\n\n$v(z_t,t) = wv(z_t, t | x_{\\text{con}}) + (1 -w)v(z_t, t | \\varnothing),$\n\nwhere $v(z_t, t | \\varnothing)$ denotes the velocity inferred without text conditioning and $w \\geq 1$ controls the magnitute of CFG. Empirically, increasing $w$ yields higher semantic alignment [23, 61, 71, 75]. Analogous to multimodal understanding, we prepend the special token |BOI| to indicate the start of image generation in the sequence.\n\nDecoupling Encoders for the Two Tasks. Previous approaches that unify autoregressive generation and diffusion models within a joint LLM training framework [96, 103] employ identical encoders ($f_{\\text{enc}}$ and $g_{\\text{enc}}$) for both understanding and generation tasks. For instance, Zhou et al. [103] performs both tasks in the same VAE latent space using a shared U-Net or linear encoder, while Xie et al. [96] leverages MAGVIT-v2 [98] to encode image patches into discrete tokens for both tasks.\n\nHowever, recent work on unified autoregressive models has shown this shared encoder design to be suboptimal [93], particularly in models that generate images through autoregression on vector-quantized tokens. Drawing from these insights, JanusFlow adopts a decoupled encoder design. Specifically, we employ a pre-trained SigLIP-Large-Patch/16 [102] model as $f_{\\text{enc}}$ to extract semantic continuous features for multimodal understanding, while using separate ConvNeXt blocks [92] initialized from scratch as $g_{\\text{enc}}$ and $g_{\\text{dec}}$ for generation, chosen for its effectiveness. Following established practices [5, 14, 90], we incorporate a long skip connection between $g_{\\text{enc}}$ and $g_{\\text{dec}}$. Our controlled experiments in Sec. 4.5 demonstrate that this decoupled encoder design significantly improves the performance of our unified model. The complete architecture of JanusFlow is illustrated in Fig. 2."}, {"title": "3.3. Training Schemes", "content": "As illustrated in Fig. 3, we train our model in three sequential stages, detailed below.\n\nStage 1: Adaptation of Randomly Initialized Components. In the first stage, we focus on training only the randomly initialized components: the linear layers, generation encoder, and generation decoder. This stage serves to adapt these new modules to work effectively with the pre-trained LLM and SigLIP encoder, essentially functioning as an initialization phase for the newly introduced components.\n\nStage 2: Unified Pre-Training. Following the adaptation stage, we train the entire model except for the visual encoder, consistent with previous approaches [57, 63]. The training incorporates three data types: multimodal understanding, image generation, and text-only data. We initially allocate a higher proportion of multimodal understanding data to establish the model's understanding capabilities. Subsequently, we increase the ratio of image generation data to accommodate the convergence requirements of diffusion-based models [18, 70].\n\nStage 3: Supervised Fine-Tuning (SFT). In the final stage, we fine-tune the pre-trained model using instruction tuning data, which comprises dialogues, task-specific conversations, and high-quality text-conditioned image generation examples. During this stage, we also unfreeze the SigLIP encoder parameters [63, 87, 93]. This fine-tuning process enables the model to effectively respond to user instructions for both multimodal understanding and image generation tasks."}, {"title": "3.4. Training Objective", "content": "Training JanusFlow involves two types of data, multimodal understanding data and image generation data. Both types of data contain two parts: \u201ccondition\u201d and \u201cresponse\u201d. \u201cCondition\" refers to the prompting of the tasks (e.g., text prompts in the task of generation and images in the task of understanding) while \u201cresponse\u201d refers to the corresponding responses of the two tasks. The data can be formatted as $x = (x^{\\text{con}}, x^{\\text{res}})$, where the superscript con denotes \u201ccondition\" and res denotes \u201cresponse\u201d. We denote the length of the whole sequence $x$ as $l$, the length of $x^{\\text{con}}$ as $l_{\\text{con}}$ and the length of $x^{\\text{res}}$ as $l_{\\text{res}}$. We use $\\theta$ to represent the collection of all the trainable parameters in JanusFlow, including the LLM, $f_{\\text{enc}}$, $g_{\\text{enc}}$, $g_{\\text{dec}}$ and the linear transformation layers.\n\nAutoregression Objective. For mutimodal understanding tasks, $x^{\\text{res}}$ contains only text tokens. JanusFlow is trained using the maximum likelihood principle,\n\n$\\mathcal{L}_{\\text{AR}}(\\theta) = -\\mathbb{E}_{x \\sim \\mathcal{D}_{\\text{und}}} \\left[ \\sum_{i=l_{\\text{con}}}^{\\ell-1} \\log P_{\\theta} (x_{i+1}|x_1,..., x_i) \\right],$\n\nwhere the expectation is taken over all $(x^{\\text{con}}, x^{\\text{res}})$ pairs in our multimodal understanding dataset $\\mathcal{D}_{\\text{und}}$, computing loss only over tokens in $x^{\\text{res}}$.\n\nRectified Flow Objective. For image generation tasks, $x^{\\text{con}}$ consists of text tokens and $x^{\\text{res}}$ is the corresponding image. JanusFlow is trained with the rectified flow objective,\n\n$\\mathcal{L}_{\\text{RF}}(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{D}_{\\text{gen}}, t \\sim P(t), z_0 \\sim \\mathcal{N}(0,I)} \\left[ \\|v_{\\theta} (z_t, t | x^{\\text{con}}) - (x^{\\text{res}} - z_0)\\|^2 \\right],$\n\nwhere $z_t = t x^{\\text{res}} + (1 - t)z_0$. Following Stable Diffusion 3 [23], we set the time distribution $P(t)$ to the logit-normal distribution. To enable CFG inference, we randomly drop 10% of the text prompts in training.\n\nRepresentation Alignment Regularization. Recent work [99] has shown that aligning intermediate representations between diffusion transformers and semantic vision encoders enhances diffusion model generalization. Our decoupled vision encoder design enables efficient implementation of this alignment as a regularization term. Specifically, for generation tasks, we align features from the understanding encoder $f_{\\text{enc}}$ with the LLM's intermediate features,\n\n$\\mathcal{L}_{\\text{REPA}}(\\theta, \\varphi) = -\\mathbb{E}_{x \\sim \\mathcal{D}_{\\text{gen}}} \\left[ \\text{sim} \\left( \\text{stop\\_grad}(f_{\\text{enc}}(x^{\\text{res}})), h_{\\varphi}(q_{\\theta}(z_t)) ) \\right) \\right],$\n\nwhere $q_{\\theta}(z_t)$ denotes an intermediate LLM representation given input $z_t$, and $h_{\\varphi}$ is a small trainable MLP that projects $q_{\\theta}(z_t)$ to dimension $D_{\\text{enc}}$. The function $\\text{sim}(\\cdot, \\cdot)$ computes the mean of element-wise cosine similarity between embeddings. Before computing the loss, we reshape $h_{\\varphi}(q_{\\theta}(z_t))$ to $H_{\\text{gen}} \\times W_{\\text{gen}} \\times D_{\\text{enc}}$. To simplify the implementation, we intentionally adjust the configuration of $g_{\\text{enc}}$ and $g_{\\text{dec}}$ to ensure $H_{\\text{gen}} = H_{\\text{im}}$ and $W_{\\text{gen}} = W_{\\text{im}}$. The gradient of $\\mathcal{L}_{\\text{REPA}}$ is not back-propagated through the understanding encoder. This alignment loss helps the LLM's internal feature space (given noisy input $z_t$) align with the understanding encoder's semantic feature space, thereby improving generation quality when producing images from new random noise and text conditions during inference.\n\nSummary. All three objectives are applied across all training stages. Multimodal understanding tasks use $\\mathcal{L}_{\\text{AR}}$, while image generation tasks employ the combined loss $\\mathcal{L}_{\\text{RF}} + \\mathcal{L}_{\\text{REPA}}$. Detailed experimental settings are provided in Sec. 4.1."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to evaluate the capabilities of JanusFlow in both multimodal understanding and generation tasks. First, we describe our experimental setup and implementation details. Then, we present results on standard benchmarks for multimodal understanding and image generation. Finally, we perform ablation studies to validate our key design choices."}, {"title": "4.1. Experiment Setup and Implementation Details", "content": "Our framework builds upon an enhanced version\u00b9 of DeepSeek-LLM (1.3B) [7, 63]. The LLM consists of 24 transformer blocks and supports a sequence length of 4,096. In our model, both understanding and generation exploits images of resolution 384.\n\nFor multimodal understanding, we leverage SigLIP-Large-Patch/16 [102] as $f_{\\text{enc}}$. For image generation, we utilize the pre-trained SDXL-VAE [71] for its latent space. The generation encoder $g_{\\text{enc}}$ comprises a 2 \u00d7 2 patchify layer followed by two ConvNeXt [92] blocks and a linear layer. The generation decoder $g_{\\text{dec}}$ combines two ConvNeXt blocks, a pixel-shuffle layer to upsample the feature map, and a linear layer. Our SigLIP encoder contains \u223c 300M parameters. $g_{\\text{enc}}$ and $g_{\\text{dec}}$ are light-weight modules, containing \u223c 70M parameters in total. Table 1 details the hyperparameters for each training stage. In the alignment regularization, we use the LLM features after the 6th block as $q_{\\theta}(z_t)$ and a three-layer MLP as $h_{\\varphi}$. We employ an exponential moving average (EMA) with a ratio of 0.99 to ensure training stability.\n\nFor data preprocessing, we deal with understanding and generation data differently. For understanding tasks, we maintain all image information by resizing the long side to the target size and padding the image to squares. For generation tasks, we resize the short side to the target size and apply random square cropping to avoid padding artifacts. During training, multiple sequences are packed to form a single sequence of length 4,096 for training efficiency. Our implementation is based on the HAI-LLM platform [31] using PyTorch [72]. Training was conducted on NVIDIA A100 GPUs, with each model requiring \u223c 1,600 A100 GPU days."}, {"title": "4.2. Training Data Settings", "content": "We follow Janus [93] to construct the training data. The data configuration for each training stage is listed below.\n\nData for Stage 1 and Stage 2. The first two stages of our framework uses three types of data: multimodal understanding data, image generation data and text-only data.\n\n1. Multimodal Understanding Data. This type of data contains several sub-categories: (a) Image caption data. We incorporate caption datasets from [20, 41, 50, 51, 53, 79] and generate additional captions for images from [16, 43] using open-source multimodal understanding models. The data follows template formats, e.g., \u201cGenerate the caption of this picture. \u201d. (b) Charts and tables. We directly adopt the chart and table data from the training data of DeepSeek-VL [63]. (c) Task data. ShareGPT4V [11] data is utilized to facilitate basic question-answering capabilities during pre-training,"}, {"title": "4.3. Evaluation Settings", "content": "Image Generation. We evaluate the generated images using both visual quality and semantic accuracy metrics. For visual quality assessment, we employ the Fr\u00e9chet Inception Distance [30] (FID) metric and compute FID between 30,000 generated images and their corresponding reference images from the MJHQ dataset [48]. The FID computation follows the implementation from GigaGAN [39]. To evaluate semantic accuracy, we utilize two specialized frameworks: GenEval [28] and DPG-Bench [34]. These frameworks are designed to assess whether the generated images accurately contain the objects and relationships specified in the input prompts, providing a broad evaluation of the generation capabilities.\n\nMultimodal Understanding. We evaluate JanusFlow's multimodal understanding abilities across a diverse set of vision-language benchmarks for general understanding capabilities, including POPE [52], MME [24], MMBench [62], SEEDBench [46], VQAv2 [29], GQA [35], MM-Vet [100], and MMMU [101]."}, {"title": "4.4. Quantitative Results", "content": "Image Generation Performances. We report the performances on GenEval, DPG-Bench and MJHQ FID-30k. In Tab. 2, we give comparisons on GenEval including the scores of all the sub-tasks and the overall score. JanusFlow achieves an overall score of 0.63, surpassing the previous unified framework and several generation specific models including SDXL [71] and DALL-E 2 [74]. In Tab. 3, We show results on DPG-Bench and the corresponding comparisons. It is noted that all the methods in Tab. 3 are generation-specific models except our model. The results on GenEval and DPG-Bench demonstrate the ability of instruction following of our model. We give the comparisons on MJHQ FID-30k in Tab. 4. The images which are sampled to calculate FID are generated with a CFG factor $w$ = 2 and a number of sampling steps 30. We sweep the CFG factor and the sampling steps and provide the results in the appendix. Our method achieves the best performance among all the models with 1.3B LLM. The results prove that the rectified flow is able to improve the quality of generated images over autoregressive models such as Janus [93].\n\nMultimodal Understanding Performances. We show comparisons of our method and other methods including understanding-specific models and unified understanding and generation models in Tab. 5. Our model reaches the best performances among all the models with similar number of parameters and even surpasses multiple understanding-specific methods with larger scales. Our results demonstrate that our method harmonizes autoregressive LLM and rectified flow, achieving satisfying performance in both understanding and generation."}, {"title": "4.5. Ablation Studies", "content": "We conduct comprehensive ablation studies to validate the effectiveness of our key design choices. For computational efficiency, all ablation experiments are performed on 256 \u00d7 256 resolution images\u00b2. All models are trained on our unified pre-training dataset for 50,000 iterations, except for the understanding-only and generation-only variants, which are trained for proportionally fewer iterations based on their respective data ratios in the pre-training phase. The quantitative results of these ablation studies are presented in Tab. 6.\n\nImpact of Representation Alignment. The comparison between Exp. A and F demonstrates the significant benefits of incorporating representation alignment regularization [99] during training. Specifically, models trained with representation alignment show notably lower FID scores on MJHQ dataset and higher CLIP scores, indicating simultaneous improvements in both image quality and semantic alignment. Importantly, our architecture differs from previous studies [65, 70] examined in [99] due to our incorporation of LLM and an additional skip connection between $g_{\\text{enc}}$ and $g_{\\text{dec}}$. The effectiveness of representation alignment in our modified architecture suggests its broad applicability and generalization capability across different network structures.\n\nImpact of Decoupling Visual Encoders. e efficacy of using powerful pre-trained visual encoders"}, {"title": "5. Conclusion", "content": "We present JanusFlow, a unified framework that successfully harmonizes autoregressive and rectified flow models for multimodal understanding and generation tasks. Our extensive experiments demonstrate that this unification achieves comparable performance to task-specific models. The successful integration of these fundamentally different model architectures not only addresses current challenges in multimodal learning but also opens new possibilities for future research in training unified models."}, {"title": "Appendix", "content": "A. Performance Analysis of 256 Resolution Model\nWe trained our model at two resolutions: 256 \u00d7 256 and 384 \u00d7 384. The main paper presents results from the 384 \u00d7 384 model as our primary results. Here, we provide a comprehensive evaluation of the 256 \u00d7 256 model's performance. The visual understanding performances are presented in Tab. 1. The generation capabilities are evaluated using GenEval [28], DPG-Benchmark [34], and MJHQ FID-30k [48], with results shown in Tab. 2 and 3.\n\nB. Analysis of CFG Factor and Sampling Steps\nWe investigate the impact of two key generation parameters: the Classifier-Free Guidance (CFG) factor and the number of sampling steps. While our main results use $w$ = 2 for CFG and 30 sampling steps to calculate FID, here we present a comprehensive analysis of these hyperparameters. Fig. 1(a) shows the effect of varying CFG factors while maintaining 30 sampling steps. The results reveal an optimal CFG value for FID scores, while CLIP [73] similarity continues to improve with increasing CFG values, consistent with findings from previous work [71]. Fig. 1(b) demonstrates the impact of different sampling steps while maintaining a CFG factor of 2. The number of sampling steps shows relatively minor influence on performance. Our choice of 30 steps in the main paper represents a balance between generation quality and computational efficiency.\n\nC. Additional Qualitative Results\nAdditional qualitative examples for both understanding and generation tasks are presented in Fig. 2 and Fig. 3, respectively. The understanding examples demonstrate JanusFlow's diverse capabilities, including code generation, person identification, character recognition, and visual reasoning. For image generation, our model exhibits strong performance in both visual quality and semantic alignment with input prompts."}]}