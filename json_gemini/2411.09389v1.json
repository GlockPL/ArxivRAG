{"title": "Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures", "authors": ["Shuzhi Gong", "Richard O. Sinnott", "Jianzhong Qi", "Cecile Paris"], "abstract": "The spread of fake news on social media poses significant threats to individuals and society. Text-based and graph-based models have been employed for fake news detection by analyzing news content and propagation networks, showing promising results in specific scenarios. However, these data-driven models heavily rely on pre-existing in-distribution data for training, limiting their performance when confronted with fake news from emerging or previously unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news is a challenging yet critical task. In this paper, we introduce the Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to enhance zero-shot fake news detection by extracting causal substructures from propagation graphs using in-distribution data and generalizing this approach to OOD data. The model employs a graph neural network-based mask generation process to identify dominant nodes and edges within the propagation graph, using these substructures for fake news prediction. Additionally, CSDA's performance is further improved through contrastive learning in few-shot scenarios, where a limited amount of OOD data is available for training. Extensive experiments on public social media datasets demonstrate that CSDA effectively handles OOD fake news detection, achieving a 7%~16% accuracy improvement over other state-of-the-art models.", "sections": [{"title": "Introduction", "content": "The popularity of social media has enabled rapid news dissemination, for both true and fake news. Given the potential impact of fake news, robust fake news detection methods are needed to debunk such news in a timely manner. In real-world scenarios, out-of-distribution news from unseen domains emerges over time. This brings substantial challenges to fake news detection models.\nGraph-based fake news detection methods using graph neural networks (GNN) have garnered much attention recently for modelling news propagation patterns (Gong et al. 2023a). Despite their success, existing GNN-based methods are generally built on the assumption that both training and testing data are independently sampled from an identical data distribution (i.i.d.), which often does not hold true nor reflect the real challenges of fake news detection (Li et al. 2022). Emerging and hitherto unseen fake news and their associated propagation graphs can and do appear. From an empirical perspective, these methods focus on minimising the average training error and incorporating correlations within the training data (which is considered to be in-distribution) to improve fake news detection accuracy (Liu et al. 2021). However, real-world graph-based fake news data is often mixed with biased domain-specific information in the training data. The detection model may thus learn these domain-specific biases resulting in misclassification of cross-domain news items (Li et al. 2022).\nTo detect fake news across different domains (e.g., sports and politics), some early studies (Ma, Gao, and Wong 2018; Bian et al. 2020) focus on capturing content-independent propagation patterns. However, it has been shown (Min et al. 2022) that not only the news contents but also the propagation patterns can vary across different news domains. More recent approaches (Li et al. 2023; Lin et al. 2022) collect and manually label a small dataset from emerging news domains. They utilise domain adaptation methods to adapt the trained models to the emerging domains in a few-shot manner. However, these approaches require labelled data from emerging domains which is not always available and could be expensive and time-consuming.\nTo address the limitations above, we focus on extracting causal subgraphs from news propagation graphs to eliminate potential domain biases. The patterns of such subgraphs are learnt for fake news detection in emerging domains. News from an emerging domain is considered as the out-of-distribution (OOD) data, and we generalise our model trained on in-distribution data to OOD data by capturing causal subgraphs in an unsupervised manner. From a causal analysis perspective, each propagation graph is composed of causal subgraph and biased subgraph which are initially entangled. Our intuition is that not all nodes in the propagation graph of a given news item are helpful for fake news detection. Instead, only some causal subgraphs of the propagation graph carry critical clues that can be used to identify fake news, as illustrated in Fig. 1 with an example. If we can identify and capture such causal subgraphs, we can improve fake news detection accuracy and subsequently improve the way we generalise the model to OOD data.\nBased on this intuition, a cross-domain model - the Causal Subgraph Oriented Domain Adaptive Fake News Detection (CSDA) model, is proposed. This model extracts subgraphs from propagation graphs and performs detection"}, {"title": "Related Work", "content": "Traditional Fake News Detection Methods\nTraditional fake news detection methods can be divided into content-based, social context-based and environment-based.\nContent-based methods learn content or style features from the text or multi-media content of news (Feng, Banerjee, and Choi 2012). They may also leverage external knowledge for fact checking (Samarinas, Hsu, and Lee 2021). Social context-based methods detect fake news through user features (Shu, Wang, and Liu 2019) and users\u2019 roles in news propagation. Propagation analysis is a trending topic in social-context based methods, with models being developed for sequence modelling (Ma et al. 2016; Khoo et al. 2020) and graph modelling (Bian et al. 2020). Temporal propagation features are often exploited. For example, Choi et al. (2021) encode the propagation process as graph snapshots; Song, Shu, and Wu (2021) utilise a temporal graph network (TGN) (Xu et al. 2020) to encode the propagation graph, whilst Naumzik and Feuerriegel (2022) and Gong et al. (2023b) classify fake news by different self-exciting patterns. Environment-based methods (Nguyen et al. 2020) consider associations across multiple news to extract context for fake news detection.\nCross-Domain Fake News Detection\nCross-domain fake news detection aims to train a detection model in one domain (the source domain) and apply the model to a different domain (the target domain). To achieve cross-domain detection, existing works can be largely categorised into sample-level and feature-level methods.\nSample-level methods identify domain-invariant data samples in the training set and assign larger weights to those samples (Silva et al. 2021; Yue et al. 2022). Studies in this category (Yue et al. 2022; Ran and Jia 2023) leverage clustering algorithms to augment target domain training samples and then train the models together with both source and target domain data, thereby improving the model performance on the target domain data. Feature-level methods focus on weighting or extracting domain-independent features. For example, Mosallanezhad et al. (2022) utilise reinforcement learning to select domain-invariant attributes from the news features. Inspired by the domain-adaptive neural networks (Ganin and Lempitsky 2015), studies (Min et al. 2022; Li et al. 2023) train an additional domain discriminator adversarially by attempting to generate news embeddings that cannot be recognised by the domain discriminator. In this case, the generated news embeddings are considered to be domain-invariant. In this paper, we utilise more information by extracting causal propagation substructures."}, {"title": "Preliminaries", "content": "Cross-domain fake news detection aims to transfer a model trained on a labelled (in-distribution) dataset to an OOD dataset that is unlabelled or with a few labelled samples. Given a set of news items $D_{in} = \\{(G_{in},y)\\}$ ($k \\in [1, n_{in}]$) that comes from some latent distribution $P$, we aim to train a model to detect fake news in another dataset $D_{out} = \\{(G_{out})\\}$ ($k \\in [1, n_{out}]$) that contains data from an unknown distribution $P'$ different from $P$. Here, we refer to data from $P$ as in-distribution data and those from $P'$ as OOD data. $D_{in}$ is the in-distribution data and $D_{out}$ is the out-of-distribution data, while $n_{in}$ and $n_{out}$ refer to the"}, {"title": "Proposed Model", "content": "In the section, we detail our model CSDA for the cross-domain fake news detection task. CSDA is designed to extract and capitalise on subgraphs from the news propagation graph. The architecture of CSDA is illustrated in Fig. 2.\nIn CSDA, we take a small batch of propagation graphs and apply a mask generator on them to split each propagation graph into a casual subgraph and a biased subgraph. Then, the causal subgraphs and the biased subgraphs are encoded by two individual graph encoders, which produce two separate embeddings. The training objective is to emphasise the impact of the casual subgraphs while reducing the impact of the biased subgraphs on the fake news detection output.\nFor cross-domain detection, CSDA is trained on $D_{in}$ and then tested on $D_{out}$ in an unsupervised manner. When a few labelled samples are available from $D_{out}$, they can also be incorporated into the training process to further enhance the model performance in the target domain.\nMask Generator\nOur mask generator learns a mask that splits each propagation graph $G$ (i.e., $G_k$ now we further drop the subscript 'k' as long as the context is clear) into a causal subgraph $G_c$ and a biased subgraph $G_b$. This is achieved by computing node importance scores (denoted as $\\alpha_i$ for node i) and edge importance scores (denoted as $B_{ij}$ for the edge between nodes i and j) in the propagation graph $G$ to measure the probability of a node or an edge belonging to the causal subgraph.\nThe mask generator takes graph $G$ (i.e., its features) as input and outputs the importance of its nodes and edges.\nA Graph Isomorphism Network (GIN) (Xu et al. 2018) is utilised to encode the graph and map the node features $X$ to node embeddings $H$ for its superior graph structure representation ability. After obtaining the graph features $H = \\{h_1, h_2, ..., h_N\\}$, where $N$ is the size of the node set and $h_i$ represents the embedding for the $i$-th node, the node and edge importance scores are computed using an MLP:\n$\\alpha_i = \\sigma(MLP([h_i])), \\beta_{ij} = \\sigma(MLP([h_i, h_j]))$ (1)\nHere, $\\sigma$ is the activation function.\nSince the causal and the biased subgraphs are defined as two non-overlapping substructures of $G$, the probability of a node and an edge belonging to the biased subgraph can be established by $(1 - \\alpha_i)$ and $(1 - \\beta_{ij})$, respectively.\nUsing the importance scores, we construct the causal graph mask $M_c = [\\alpha, \\beta]$ and the biased graph mask $M_b = [(1 - \\alpha), (1 - \\beta)]$. Finally, the input propagation graph $G$ is decomposed into a causal subgraph $G_c = \\{M_cG\\}$ and a biased subgraph $G_b = \\{M_bG\\}$, where $\\cdot$ is the filtering operation on the graph $G$ with the corresponding masks. The masks emphasize distinct regions of the propagation graphs, enabling subsequent GNN-based graph encoders to concentrate on different segments of the graphs.\nGraph Encoder\nTwo subgraph encoders, each of which is a two-layer Graph Convolutional Network (GCN) (Kipf and Welling 2017), are used to encode the causal subgraph and the biased subgraph, respectively.\nGiven a graph's node features $X = \\{X_1, X_2, ..., X_N\\}$ and its adjacency matrix $A$, the graph embeddings are computed through GCNs by:\n$Z^{(l+1)} = \\sigma (\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}Z^{(l)}W^{(l)})$ (2)\nwhere $l = 0$ or 1, $Z^{(0)}$ is the initial node features $X$, $\\tilde{A} = A + I$ is the adjacent matrix of the graph with self-loops, $I$ is the identity matrix, $\\tilde{D}$ is the degree matrix of $\\tilde{A}$, $W^{(l)}$ is the learnable parameter matrix, and $\\sigma$ is the activation function.\nAs shown in Figure 2, two parallel subgraph encoders are used to encode the causal subgraph $G_c$ and the biased subgraph $G_b$ into a causal embedding $z_c$ and a biased embedding $z_b$. These embeddings will subsequently be fed into news classifiers for loss calculation and label (i.e., fake news or not) prediction.\nClassification Module\nThe classification module (CM) is responsible for predicting the news veracity based on the extracted graph embeddings. It is composed of an MLP that uses the softmax function. Given the graph embedding $Z$, which is the concatenation of $z_c$ and $z_b$, the CM acquires the prediction through:\n$pred = softmax(MLP(Z))$ (3)\nSince CSDA focuses on classifying news according to causal features solely, we design a causal CM, denoted as $C_c$, and a biased CM, denoted as $C_b$, in the model. During model training, these two CMs are jointly trained to optimise CSDA to capture causal information accurately. In the"}, {"title": "Disentangling Training Objectives", "content": "In the training process, CSDA is optimised in batches. As shown in Fig. 2, a mini-batch of graphs are input into the model to calculate the loss for back propagation. Using the subgraph encoders, the casual subgraph $G_c$'s embedding $z_c$ and the biased subgraph $G_b$'s embeddings $z_b$ are computed. The subgraph embeddings $z_c$ and $z_b$ are concatenated together as $Z = z_c \\oplus z_b$, to obtain the embeddings of the current batch of training data. To disentangle the two subgraphs $G_c$ and $G_b$, the following training objectives are defined.\nNote that, for both of the causal and biased subgraph extraction and classification, we optimise the model to predict final labels correctly, even though the biased part is not utilised in the testing process. The motivation is that the biased information is easily to be captured and also strongly correlated to the labels in the in-distribution data. The training of the mask generator requires the correct predictions of the biased classification module.\nLoss for the Biased Branch Instead of traditional cross-entropy (CE) loss, the generalised cross-entropy (GCE) loss (Zhang and Sabuncu 2018) is utilised to amplify the impact of the biased information and optimise the model to extract and embed the biased subgraph correctly:\n$L_{biased} = GCE(C_b(Z), y) = \\frac{1 - C_b(Z)^q}{q}$ (4)\nwhere $C_b(Z)$ and $(Z)$ are the softmax outputs of the biased MLP classifier and the probability associated with it having the correct label $y$, respectively, and $q \\in (0, 1]$ here is a hyperparameter.\nCompared to the Cross-Entropy (CE) loss, the GCE loss can amplify the gradient of the standard CE loss for samples with a high confidence $C_b(Z)$ of predicting the correct label $y$ (Zhang and Sabuncu 2018). As a result, the GCE loss is able to amplify the impact of biased information and optimise the biased branch to extract and embed the biased subgraph correctly.\nLoss for the Causal Branch For the causal subgraph encoder, we train it with the MLP classifier $C_c$ by a weighted CE loss based on:\n$L_{causal} = CE(C_c(Z))_{weighted} = W(Z) \\cdot CE(C_c(Z))$ (5)\nwhere CE is the standard cross-entropy loss, and the weight is defined as:\n$W(Z) = \\frac{CE(C_b(Z), y)}{CE(C_b(Z), y) + CE(C_b(Z), y)}$ (6)\nThis weight is based on the fact that graphs with a high CE loss (worse prediction) from the biased MLP classifier $C_b$ can be considered as containing more causal (instead of biased) information.\nOverall, for the disentanglement of the causal and the biased substructures, the loss $L_{dis}$ is the sum of the losses $L_{biased}$ and $L_{causal}$, from the causal and biased Classification Modules, respectively.\n$L_{dis} = L_{biased} + L_{causal}$ (7)\nNext, we consider the connection between the causal and the biased subgraphs.\nBatch-wise Data Augmentation The causal and biased subgraphs of the same graph are inherently correlated. To learn and mitigate the correlations between the casual and the biased embeddings $z_c$ and $z_b$, inspired by a previous"}, {"title": "Model Fine-tuning with OOD Data", "content": "CSDA can be trained using just in-distribution data $D_{in}$. We can also use a few labelled OOD data samples for further fine-tuning CSDA on a target domain. In this subsection, we discuss model optimisation given the availability of a few OOD samples via a contrastive learning method.\nWhen limited OOD data samples are available, we can further improve the model performance via aligning the representation space of causal fake news. This is achieved by making the representations of in-distribution and OOD samples from the same veracity class closer while keeping representations from different classes further away. We achieve this via contrastive learning.\nThe in-distribution data samples are the primary resource used for model training, while the OOD data samples occur much less frequently. We therefore need to learn more separated news representations for the in-distribution data from different classes (i.e. true or fake news). To achieve this goal, we adopt a supervised contrastive learning objective to bring closer samples from the same class and separate different classes among the in-distribution samples. This is given as:\n$L_{CL}^{in} = \\frac{1}{N_{in}} \\sum_{n=1}^{N_{in}} log \\frac{\\sum_{m=1}^{N_{yin}} 1[n \\neq m]1[y_{in} = y_{in}] exp(sim(o_{in}^n, o_{in}^m)/\\tau)}{\\sum_{k=1}^{N_{in}} 1[n \\neq k] exp(sim(o_{in}^n, o_{in}^k)/\\tau)}$ (12)\nwhere $N_{in}$ is the number of in-distribution data samples in a batch, $N_{yin}$ is the number of in-distribution data samples which share the same label $y_{in}$ with sample $C_{in}^n$, 1 is the indicator function, $o_{in}^n$, $o_{in}^m$, and $o_{in}^k$ are the corresponding extracted casual representations from CSDA, sim($\\cdot$) is the cosine similarity function, and $\\tau$ is a hyperparameter that controls the temperature.\nTo fine-tune CSDA over OOD data, another supervised contrastive learning objective is proposed. Here, we aim to draw the embedding space of samples with the same label but from different distributions closer.\n$L_{CL}^{out} = \\frac{1}{N_{out}} \\sum_{n=1}^{N_{out}} log \\frac{\\sum_{m=1}^{N_{yout}} 1[y_{out} = y_{in}] exp(sim(o_{out}^n, o_{in}^m)/\\tau)}{\\sum_{k=1}^{N_{in}} exp(sim(o_{out}^n, o_{in}^k)/\\tau)}$ (13)\nwhere $N_{out}$ is the number of OOD samples in a training batch, $N_{in}$ is the number of in-distribution samples in the batch, $N_{yout}$ is the number of in-distribution samples which share the same label $y_{out}$ with sample $C_{out}^n$, and $o_{out}^n$, $o_{in}^m$, and $o_{in}^k$ are the corresponding extracted causal representations from CSDA.\nLoss $L_{CL}$ for the contrastive learning is then given as the sum of Equations (12) and (13):\n$L_{CL} = L_{CL}^{in} + L_{CL}^{out}$ (14)\nIn summary, the overall training objective $L_{en}$ for CSDA is a weighted sum of the contrastive learning loss and the original loss $L$ as shown in Equation (11).\n$L_{en} = \\gamma \\cdot L + (1 - \\gamma) \\cdot L_{CL}$ (15)\nHere, $\\gamma$ is a hyperparameter controlling the contribution of the contrastive learning loss."}, {"title": "Experiment", "content": "Experimental Settings\nDatasets. Four public datasets collected from Twitter (now called X) and Weibo (a Chinese social media platform like Twitter) are utilised in the experiment: (1) Twitter (Ma, Gao, and Wong 2017), (2) Weibo (Ma et al. 2016), (3) Twitter-COVID19 (Lin et al. 2022) and (4) Weibo-COVID19 (Lin et al. 2022). The statistics of the datasets are shown in Table 1.\nTwitter and Weibo are open-domain datasets. They cover a variety of topics except COVID-19 and are used as the main training set. Twitter-COVID19 and Weibo-COVID19 only contain news related to COVID-19,"}, {"title": "Conclusion", "content": "We proposed a model named CSDA for detecting fake news across domains by extracting and leveraging causal substructures. CSDA addresses the limitations of existing models in handling domain biases and OOD data, highlighting the importance of causal elements in news propagation graphs. Through extensive experiments, we show that CSDA outperforms not only sequence-based models but also other graph-based models, achieving higher accuracy, particularly in cross-domain scenarios. Additionally, the integration of a fine-tuning process with low-resource OOD data further enhances CSDA's robustness and adaptability.\nFor future work, it would interesting to further exploit the causal information from the textual content of the news."}]}