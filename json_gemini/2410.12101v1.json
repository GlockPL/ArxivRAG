{"title": "THE PERSIAN RUG: SOLVING \u03a4\u039f\u03a5 MODELS OF SUPER- POSITION USING LARGE-SCALE SYMMETRIES", "authors": ["Aditya Cowsik", "Kfir Dolev", "Alex Infanger"], "abstract": "We present a complete mechanistic description of the algorithm learned by a minimal non-linear sparse data autoencoder in the limit of large input dimension. The model, originally presented in Elhage et al. (2022), compresses sparse data vectors through a linear layer and decompresses using another linear layer followed by a ReLU activation. We notice that when the data is permutation symmetric (no input feature is privileged) large models reliably learn an algorithm that is sensitive to individual weights only through their large-scale statistics. For these models, the loss function becomes analytically tractable. Using this understanding, we give the explicit scalings of the loss at high sparsity, and show that the model is near-optimal among recently proposed architectures. In particular, changing or adding to the activation function any elementwise or filtering operation can at best improve the model's performance by a constant factor. Finally, we forward-engineer a model with the requisite symmetries and show that its loss precisely matches that of the trained models. Unlike the trained model weights, the low randomness in the artificial weights results in miraculous fractal structures resembling a Persian rug, to which the algorithm is oblivious. Our work contributes to neural network interpretability by introducing techniques for understanding the structure of autoencoders. Code to reproduce our results can be found on Github at https://github.com/KfirD/PersianRug.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language model capabilities and applications have recently proliferated. As these systems advance and are given more control over basic societal functions, it becomes imperative to ensure their reliability with absolute certainty. Mechanistic interpretability aims to achieve this by obtaining a concrete weight-level understanding of the algorithms learned and employed by these models. A major impediment to this program has been the difficulty of interpreting intermediate activations. This is due to the phenomena of superposition, in which a model takes advantage of sparsity in the input data to reuse the same neurons for multiple distinct features, obscuring their function. Finding a systematic method to undo superposition and extract the fundamental features encoded by the network is a large and ongoing area of research Bricken et al. (2023); Cunningham et al. (2023); Gao et al. (2024); Engels et al. (2024).\nCurrently, the most popular method of dealing with superposition is dictionary learning with sparse auto-encoders. In this method, the smaller space of neuron activations at a layer of interest is mapped to a larger feature space. The map is trained to encourage sparsity and often consists of an affine + ReLU network. This method has been applied to large language models revealing many strikingly interpretable features (e.g. corresponding to specific bugs in code, the golden gate bridge, and sycophancy), even allowing for a causal understanding of the model's reasoning in certain scenarios Marks et al. (2024).\nThe sparse decoding ability of the affine + ReLU map was recently studied in the foundational work of Elhage et al. (2022), which introduced and studied a toy model of superposition. The model consisted of a compressing linear layer modeling the superposition followed by a decompressing affine + ReLU layer, trained together to auto-encode sparse data. They showed that the network performs superposition by encoding individual feature vectors into nearly orthogonal vectors in the smaller space. The affine layer alone is unable to decode sparse input vectors sufficiently well to make use of superposition, but the addition of the ReLU makes it possible by screening out negative interference.\nGiven the extensive use of the affine + ReLU map for decoding sparse data, and the vital role of the non-linearity, it is useful to have a toy model of superposition whose learned algorithm we can understand sufficiently well to obtain an analytically tractable form of its reconstruction loss, in order to inform the design of more efficient architectures for sparse data processing.\nIn this work we obtain such an understanding by considering a particularly tractable regime of the Elhage et al. (2022) model: permutation symmetric data (no input feature is privileged in any way), and the thermodynamic limit (a large number of input features), while maintaining the full range of sparsity and compression ratio values. In this regime, the learned model weights reliably display a \u201cstatistical permutation symmetry\u201d, which sufficiently simplifies the form of the loss function to the point where it is analytically tractable. We then forwards-engineer an artificial set of weights satisfying these symmetries which minimizes the loss. We then show that large trained models achieve the same loss as this artificial model, implying that the trained models make use of the optimal statistically permutation symmetric algorithm. The artificial set of weights resembles a Persian rug fig. 1, whose structure is a relic of the minimal randomness used in the construction, illustrating that the algorithm relies entirely on large-scale statistics that are insensitive to this structure. Finally, we derive the exact power-law scaling of the loss in the high-sparsity regime.\nWe expect our work to impact the field of neural network interpretability in multiple ways. First, our work provides a basic theoretical framework that we believe can be extended to other regimes of interest, such as structured correlations in input features, which may help predict scaling laws in the loss based on the data's correlations. Second, our work rules out a large class of performance improvement proposals for sparse autoencoders. Finally, our work provides an explicit example of a learned algorithm that is insensitive to microscopic structure in weights, which may be useful for knowing when not to analyze individual weights.\nThe paper is structured as follows. In section 2 we review the model and explain our training procedure. In section 3 we show empirically that large models display a \u201cstatistical\u201d permutation symmetry. In section 4 we extract the algorithm by plugging the symmetry back into the loss, introduce the Persian rug model which optimizes the remaining parameters, show that large trained models achieve the same loss, and derive the loss behavior in the high sparsity limit. In section 5.2 we discuss related works, and conclude with section 5."}, {"title": "2 THE MODEL", "content": "We consider the following model for our non-linear autoencoder with matrix parameters $W_{in} \\in \\mathbb{R}^{n_d \\times n_s}$, $W_{out} \\in \\mathbb{R}^{n_s \\times n_d}$, $b \\in \\mathbb{R}^{n_s}$,\n\n$f_{nonlinear}(x) = ReLU(W_{out} W_{in}x + b).$ (1)\n\n$W_{in}$ is an encoding matrix which converts a sparse activation vector x to a dense vector, while $W_{out}$ perform the linear step of decoding. We also consider a simple model for the sparse data on which this autoencoder operates. Each vector x is drawn i.i.d. during training, and each component $x_i = c_i u_i$ where $c_i \\sim Bernoulli(p)$, $u_i \\sim Uniform[0, 1]$ are independent variables. This ensures that x is sparse with typically only $p n_s$ features turned on.\nWe train our toy models to minimize the expected $L_2$ reconstruction loss,\n\n$L(x; W_{out}, W_{in}, b) = E||x - f_{nonlinear}(x)||^2.$ (2)\n\nIt is known that for the linear model (eq. (1) without the ReLU), the optimal solution is closely related to principle component analysis (see, for example, Plaut (2018) and p. 563 or Bishop & Nasrabadi (2006)). In particular, the reconstruction loss scales linearly in the hidden dimension $n_d$. On the other hand, the model eq. (1) will have a much quicker reduction in loss, as will be described in section 3.1.\nFor all models present we train them with a batch size of 1024 and the Adam optimizer to comple- tion. That is training continues as long as the average loss over the past 100 batches is lower than the average loss over the 100 batches prior to that one. Our goal with training is to ensure that we have found an optimal model in the large-data limit to analyze the structure of the model itself."}, {"title": "3 EMPIRICAL OBSERVATIONS", "content": "In this section, we present empirical observations of the trained models. We start by presenting a remarkable phenomenon this model exhibits in the high-sparsity regime: a dramatic decrease in loss as a function of the compression ratio. We then turn to a mechanistic interpretation of the weights which gives empirical evidence for the phenomena needed to understand the algorithm the model learns. These are manifestations of a partially preserved permutation symmetry of the sparse degrees of freedom."}, {"title": "3.1 FAST LOSS DROP", "content": "To gauge the performance of the model, we plot the loss (eq. (2)) as a function of the compression ratio $n_d/n_s$.\nIn fig. 2b we plot the performance of the linear versus non-linear models for representative parame- ters. It is clear that the non-linear model outperforms the linear model up until near the $n_d/n_s \\approx 1$ regime due to an immediate drop in the loss starting around $n_d/n_s \\approx p$. The slope and duration of this initial fall is controlled by p. In particular, in the high-sparsity regime (p close to zero), the loss drops to zero entirely near the $n_d/n_s \\approx 0$ regime. What is going on here? To explain this behavior, we analyze the algorithm the model encodes."}, {"title": "3.2 STATISTICAL PERMUTATION SYMMETRY", "content": "Rather than looking individually at the weights, it is helpful to look at the matrix $W = W_{out} W_{in}$, shown in fig. 3. As will be made precise in section 4, we are interested in the structure of each row since it is responsible for the corresponding feature output. Though the matrix is clearly not permutation symmetric, its statistics are sufficiently so as far as the algorithm is concerned. In particular, in the $n_d/n_s \\rightarrow \\infty$ limit, the matrix is statistically permutation invariant in the following sense:\n1. the diagonal elements become the same (fig. 4),\n2. the bias elements become uniformly negative, which can be seen in the uniformity and slight blue shade in fig. 3 and is quantified in fig. 5,\n3. the statistics of the off-diagonal terms in each row are sufficiently noisy for interference to become Gaussian (fig. 7), and finally\n4. the mean and variance of each of these becomes the same across rows (fig. 6)."}, {"title": "3.3 OPTIMIZATION OF RESIDUAL PARAMETERS", "content": "The statistical permutation symmetry places constraints on the possible values of W and b. The constraint on b is straightforward: it is proportional to the all ones vector, i.e. there is a scale b such that $b_i = b$ for all i. As will be explained in section 4.1, the relevant degrees of freedom remaining in W are one number a equal to the diagonals, and $\\sigma$ characterizing the root mean square of the off diagonal rows. The precise values of the off diagonals can be thought of as irrelevant \u201cmicroscopic information\". Thus there are three relevant degrees of freedom remaining: b, a, and $\\sigma$.\nIn section 4.2.2 we give a specific set of values for $W_{in}$ and $W_{out}$ via the \u201cPersian rug\u201d matrix, which have the statistical permutation symmetry in the $n_s \\rightarrow \\infty$ limit while also optimizing $\\sigma$. The remaining parameters, a and b can be optimized numerically. In fig. 2 we compare the loss curve of this artificial model with that of a trained model, and see that they are essentially the same."}, {"title": "4 EXTRACTING THE ALGORITHM", "content": "In this section we give a precise explanation of the algorithm the model performs. We start with a qualitative description of why the statistical permutation symmetry gives a good auto-encoding algorithm when the remaining macroscopic degrees of freedom are optimized. We then find an artificial set of symmetric weights with optimized macroscopic parameters. We show that the trained models achieve the same performance as the artificial model, thus showing they are optimal even restricting to symmetric solutions. Finally, we derive an explicit form of the loss scaling and argue that ReLU performs near optimally among element-wise or \u201cselection\u201d decoders."}, {"title": "4.1 QUALITATIVE DESCRIPTION", "content": "A key simplification is to consider strategies as collections of low-rank affine maps rather than as the collection of weights directly. In other words consider the tuple (W, B) where $W = W_{out} W_{in}$ to define the strategy. We must restrict to W with rank no more than $n_d$ because it is the product of two low-rank matrices. Given any such W we may also find $W_{in}$ and $W_{out}$ of the appropriate shape (e.g. by finding the SVD), so the two representations are equivalent.\nLet us now take a close look at the reconstruction error. Let us consider output i, which is given by\n\n$(f_{nonlinear}(x))_i = ReLU(W_{ii}x_i + \\sum_{j=1,j\\neq i}^{n_s} W_{ij}x_j + b_i) = ReLU (a(x_i + v_i) + b).$ (4)\n\nwhere we used that all diagonal elements of W are equal to a and defining $v_i = a^{-1}\\sum_{j=1,j\\neq i}^{n_s} W_{ij}x_j$. At this stage we can already see that the individual off-diagonal elements aren't important, because their only purpose is to generate the random variable $v_i$. More specifi- cally, because $v_i$ becomes Gaussian only the sum, and sum of squares of the off-diagonal compo- nents matter. Because we have a bias term the mean of $v_i$ may be absorbed into it, so from now we assume that $v_i$ are all zero-mean. This simplification is why permutation symmetry emerges when $n_s$ is large, and allows us to consider only the macroscopic variables introduced when we discuss the loss.\nThe reconstruction error is (dropping the i subscript)\n\n$L = E_{x,v} [(x - ReLU (a(x + v) + b))^2].$\nWe can further decompose this by taking the expectation value over whether x is on or off, so\n\n$L = (1-p)L_{off} + pL_{on}$ (5)\n\nwhere\n\n$L_{off} = E_v [(ReLU (av + b))^2]$, and\n$L_{on} = E_{u,v}[(u - ReLU (a(u + v) + b))^2]$\n\nwith $u \\sim Uniform [0, 1]$. Let us explore the regimes of macroscopic parameters a, b, $\\sigma$ when either or both of these loss terms are low.\nThe impact of all the non-diagonal terms has been summed up in the \"noise\" v. Though it is a deterministic function of x, output i has no way to remove it because by itself it doesn't have the required information. This exemplifies a key principle \u2013 by restricting the computational capacity of our decoder, deterministic, but complicated correlations act like noise.\nThe main advantage the nonlinear auto-encoder is that the dominant contribution to the loss, $L_{off}$, is immediately screened away by making av + b small or negative, allowing the network to focus on encoding active signals. This immediate screening is always possible by choosing b + \u00b5 large and negative or a and \u03bc + b small. However, this strategy comes at a cost in the on case: the output value is distorted from u to au + b. It is thus preferable instead for v and b to be as small as possible, which occurs when $\\sigma$ is as small as possible. As we will see, $\\sigma$ is the only parameter we are not free to choose in the large $n_s$ limit, and whose value will be bounded as a function of $n_d/n_s$ and p. Since $L_{off}$ is the dominant contribution to the loss, therefore, it will thus be necessary to damp the signal by setting a small and/or b large and negative in regimes where $\\sigma$ is uncontrollably large.\nGiven that we see a statistical permutation symmetry in trained models let's consider symmetric strategies so that $W_{ii} = a$ and $B_i = b$ for all $i = 1,...,n_s$. We will show that optimizing the remaining macroscopic parameters makes $f_{nonlinear}$ act close to the identity on sparse inputs."}, {"title": "4.2 \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396ING THE MACROSCOPIC PARAMETERS", "content": "We have seen qualitatively that a statistically symmetric strategy exists in certain regimes of the macroscopic parameters. Two of these parameters, a and b are unconstrained. Furthermore the loss is monotonically increasing with \u03c3. Thus we now prove lower bounds on \u03c3 and construct an artificial set of statistically permutation symmetric weights which achieve this bound. Finally we will compare the reconstruction loss of this strategy with the learned one to justify that those ignored microscopic degrees of freedom were indeed irrelevant."}, {"title": "4.2.1 OPTIMAL \u03c3", "content": "Assuming a kind of symmetry breaking we will derive a bound on the variance of the output. Ad- ditionally for an optimal choice of b the average loss is increasing in the variance, because a larger variance corresponds to a smaller signal-to-noise ratio. Taken together these two facts will give a lower bound on the loss. We will then provide an explicit construction which achieves this lower bound which illustrates how the algorithm works.\nThe lower bound on the variance comes from the fact that W is simultaneously low-rank and also the diagonals of W are constant and large. For now let us ignore the overall scale of W, and just rescale so that the diagonals are exactly 1. The bound we are about to prove is very similar to the Welch bound Welch (1974) who phrased it instead as a bound on the correlations between sets of vectors. We produce an argument for our context, which deals with potentially non-symmetric matrices W, the details of which are located in appendix C.\nWe show that\n\n$\\sigma^2 \\geq \\frac{4p - 3p^2}{12} (\\frac{n_s}{n_d}-1)$ (6)\n\nwith equality only when W is symmetric, maximum rank, with all non-zero eigenvalues equal. This naturally leads to a candidate for the optimal choice of W, namely matrices of the form\n\n$W \\propto O P O^T \\text{ and } W_{ii} = 1$ (7)\n\nwhere O is an orthogonal matrix and P is any rank-$n_d$ projection matrix. This kind of matrix saturates the bound because it is symmetric and has all nonzero eigenvalues equal to 1."}, {"title": "4.2.2 PERSIAN RUG MODEL", "content": "We now give an explicit construction of an optimal choice for W. The construction is based on the Hadamard matrix of size $n_s = 2^m$ for some integer m, defined as\n\n$H_{ij} = parity(bin(i) \\oplus bin(j))$"}, {"title": "4.3 LossS SCALING AT HIGH SPARSITY", "content": "Having obtained a simple expression for the loss in terms of constants a, b and two simple random variables x ~ Uniform[0, 1] and \u03bd ~ N(\u03bc,\u03c3), as well has having deduced an achievable lower bound for \u03c3, we are now able to explain why the simple ReLU model performs so well at high sparsity. For ease of notation let us use $r = n_d/n_s$."}, {"title": "4.3.1 INITIAL LOSS (RATIO=0)", "content": "Let us first consider r \u2192 0 limit with all other parameters fixed. Then $\\sigma \\rightarrow \\infty$ because of the bound in eq. (6) so the fluctuations in v overwhelms the signal term, and the loss becomes\n\n$L \\rightarrow (1-p)a^2E_v [ReLU (v + b)^2] + pE_{u,v} [(u \u2013 a ReLU (v + b))^2]$\n$= a^2E_v [ReLU (v + b)^2] \u2013 2apE_{u,v} [u ReLU(v + b)] + pE_u[u^2]$\n\nOptimizing a gives\n\n$a_{opt} = p \\frac{E_{u,v} [u ReLU(v + b)]}{E_v [ReLU(v + b)^2]}$ (8)\n\nand thus the loss becomes\n\n$L\\rightarrow= pE_u[u^2] - p^2 \\frac{(E_u [u])^2 (E_v [ReLU(v + b)])^2}{E_v [ReLU(v + b)^2]} > \\frac{p}{3} - \\frac{p^2}{9}$"}, {"title": "4.3.2 LoSS UPPER BOUND", "content": "We now show that the loss drops off quickly in the sense that for $r/p \\gg 1$ we get that $L(p)/p \\rightarrow 0$, i.e. L(p) scales super-linearly with p. We will consider the regime where $r < 1$ holds so that we may take $\\sigma^2 \\sim \\ll 1$.\nTo obtain the upper bound we will make educated estimates for values of a and \u03bc that are near optimal. In particular, in appendix A we show that the optimal value of a is (after absorbing b into the mean of v, \u03bc) :\n\n$a_{opt} \\approx \\frac{E_{x,v} [x ReLU(x + v)]}{E_{x,v} [ReLU(x + v)^2]}$ (9)\n\nFrom the form of the loss, we know that \u03bc must decrease as p decreases for the loss to go down faster than O(p). Thus v has both a mean and variance approaching 0, and $a_{opt} \\rightarrow 1$. Thus we plug in a = 1 before taking these limits in the expectation of getting a good upper bound. The loss then takes the form\n\n$L = (1-p) L_{off} + pL_{on}$\n\nwith\n\n$L_{off} = (ReLU (v)^2)$, and\n$L_{on} = ((u \u2013 ReLU (u + v))^2)$"}, {"title": "4.3.3 LoSS LOWER BOUND", "content": "In appendix D we also derive a lower bound in the high-sparsity limit $L > O(\\frac{p^2}{r})$ in the high sparsity limit up to logarithmic corrections. We show this in fact holds for a more general class of activation functions. In particular, any function which acts element-wise or filters out elements will give an on-loss contribution of the form\n\n$E_{u,v}[(u \u2212 f (u + v))^2]$\n\nwhich has a lower bound due to v destroying information about u.\nThus we can conclude that\n\n$L \\approx O(\\frac{p^2}{r})$\n\nup to logarithmic factors whenever $r/p < 1$."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 SUMMARY", "content": "We have performed an exhaustive analysis of the toy model of superposition for permutation sym- metric data. We empirically showed that the network reliably learns statistically permutation sym- metric algorithms, in the sense that the information relevant to the algorithm is encoded in the summary statistics of the elements in each row of the W matrix.\nPlugging these symmetries back into the reconstruction-loss allowed us to reduce its optimization problem over all model parameters to an optimization problem over three scalar variables. Of these parameters, only the noise parameter \u03c3 was constrained by the hyper-parameters p, ns, and nd. We showed that \u03c3 is minimized for a statistical symmetric W that is proportional to a projector.\nWe then forward-engineered weights optimizing these parameters, giving the Persian rug model, and showed empirically that its loss matched the model loss closely. This shows that the trained models learn the optimal symmetric algorithm. The additional presence of small scale structure in the Persian rug model highlights the algorithm's independence on such smaller scale details.\nFinally, we considered the analytic form of the loss in the high-sparsity limit, and derived its scalings as a function of sparsity and compression ratio to be O(p) when $r/p < 1$ and $\\sim O(p^2/r)$ when $r/p < 1$, or in other words the loss drops of O(p) in a ratio window scaling faster than O(\u0440)."}, {"title": "5.2 RELATIONSHIP TO OTHER WORKS", "content": ""}, {"title": "5.2.1 MECHANISTIC INTERPRETABILITY AND SPARSE AUTOENCODERS", "content": "Mechanistic interpretability is a research agenda which aims to understand learned model algorithms through studying their weights, (see Olah et al. (2020) for an introduction). Recent results relating to language models include Meng et al. (2023), which finds a correspondence between specific facts and feature weights, along with Olsson et al. (2022), which shows that transformers learn in context learning through the mechanism of \u201cinduction heads\u201d.\nElhage et al. (2022) introduced the model we study in this paper. While that work focused on mapping empirically behaviors of the model in multiple regimes of interest such as correlated inputs, we focused on a regime with enough symmetry to solve the model analytically given observed symmetries in trained models.\nWhile the concept of dictionary learning was introduced by (Mallat & Zhang, 1993), the practical use of sparse autoencoders to understand large language models has accelerated recently due to mezzo- scale open weight models (Gao et al., 2024; Lieberum et al., 2024) and large-scale open-output models Bricken et al. (2023). These features are highly interpretable (Cunningham et al., 2023) and scale predictably. Interestingly, the scaling is quite similar for the various different architectures they consider, differing primarily by a constant, which fits with the predictions in this work.\nWe have seen that the dominant source of error is not from determining which features are present, but rather the actual values of those features. Small modifications to the activation functions, such as gating Rajamanoharan et al. (2024), k-sparse Makhzani & Frey (2013), or TRec non-linearity Taggart (2024); Konda et al. (2014), are insufficient to fix this problem as they do not solve the basic issue of noisy outputs. In this context our work implies that innovative architectures, that are suitable both for gradient-based training and also for decoding sparse features, must be developed."}, {"title": "5.2.2 COMPRESSED SENSING AND STATISTICAL PHYSICS", "content": "It is known that compressed sparse data can be exactly reconstructed by solving a convex problem (Candes & Tao, 2005; Candes et al., 2006; Donoho & Elad, 2003; Donoho, 2006) given knowledge of the compression matrix. Furthermore, using tools from statistical physics it is possible to show that this holds for typical compressed sparse data (Ganguli & Sompolinsky, 2010). Learning the compression matrix is also easy in certain circumstances(Sakata & Kabashima, 2013). For a more general review on compressed sensing and it's history consider the introduction by Davenport et al. (2012). The reconstruction procedure typically used in compressed sensing is optimizing a (convex) relaxation of finding the sparsest set of features which reproduces your data vector. This is signif- icantly different to the setting of sparse auto-encoders which try to obtain the sparse features using only one linear + activation layer.\nThe discrepancy between the ability of convex optimization techniques to achieve zero loss while a linear + ReLU model necessarily incurs an error suggests that a more complex model architecture is needed for sparse autoencoders when it is desirable to calculate the feature magnitude to high precision. This may occur, for example, if one wishes to insert a sparse auto-encoder into a model without corrupting its downstream outputs."}, {"title": "5.2.3 LINEAR MODELS", "content": "The linear version of the toy model we studied has garnered significant attention. Plaut (2018) demonstrates that the minimizers of eq. (2) can be directly related to Principal Component Analysis (PCA), a fundamental technique in dimensionality reduction and data analysis (see also Bishop & Nasrabadi (2006) for a comprehensive treatment). Kunin et al. (2019) studies the learning dynamics of linear autoencoders when regularization is added."}, {"title": "5.3 REMAINING QUESTIONS AND FUTURE DIRECTIONS", "content": ""}, {"title": "Correlated Data", "content": "We expect real world data to display significant correlations between activated features. Nevertheless, we expect there are more realistic correlation structures that remain simple enough to analyze the loss analytically. One such possibility is to use data whose correlations are described by a sparse graph."}, {"title": "Compressed Computation", "content": "We have shown that features can only be recovered using linear + element-wise or selection type activations together with a small error. How then are neural networks able to compute on information stored in superposition using just linear + ReLU layers? Noise injected into data early on in a computation tends to be compounded as the computation grows in circuit complexity. Thus we believe the neural networks must be at least one of the following:\n1. performing computation on superposed information without first localizing it,\n2. performing computation on superposed information by first localizing it using multiple layers,\n3. performing only low complexity computation on superposed information, or\n4. implementing a fault tolerance scheme (computation which actively corrects errors).\nIt would be interesting to experimentally look for these phenomena and their structure."}]}