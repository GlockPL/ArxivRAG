{"title": "Real-Time Human Action Recognition on Embedded Platforms", "authors": ["Ruiqi Wang", "Zichen Wang", "Peiqi Gao", "Mingzhen Li", "Jaehwan Jeong", "Yihang Xu", "Chenyang Lu", "Yejin Lee", "Lisa Connor"], "abstract": "With advancements in computer vision and deep learning, video-based human action recognition (HAR) has become practical. However, due to the complexity of the computation pipeline, running HAR on live video streams incurs excessive delays on embedded platforms. This work tackles the real-time performance challenges of HAR with four contributions: 1) an experimental study identifying a standard Optical Flow (OF) extraction technique as the latency bottleneck in a state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy tradeoff between the standard and deep learning approaches to OF extraction, which highlights the need for a novel, efficient motion feature extractor, 3) the design of Integrated Motion Feature Extractor (IMFE), a novel single-shot neural network architecture for motion feature extraction with drastic improvement in latency, 4) the development of RT-HARE, a real-time HAR system tailored for embedded platforms. Experimental results on an Nvidia Jetson Xavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video frame rate of 30 frames per second while delivering high levels of recognition accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "As video-based human action recognition (HAR) technology became widely available in recent years [1], [2], we have witnessed the emergence of applications requiring HAR to be performed on real-time using live video streams. For example, HAR has been employed to monitor the activities of Alzheimer's disease patients and provides alerts when anomalies are detected [3]. HAR has also been used to interpret dangerous driver behaviors [4] for just-in-time interventions to improve driving safety. Moreover, due to privacy concerns, it is often preferable to deploy HAR on local embedded platforms instead of relying on servers or the cloud.\nHowever, achieving real-time performance in HAR remains extremely challenging on embedded platforms due to the complexity of the computational pipelines and the resource constraints inherent to these platforms. Strikingly, in our benchmark experiments, a state-of-the-art HAR pipeline [5] can handle only three video frames per second (FPS) due to its excessive and fluctuating latency.\nWe first implemented and benchmarked a state-of-the-art two-stream HAR architecture [6] on a Nvidia Jetson platform to understand real-time performance issues. The HAR pipeline employs two feature streams extracted from video frames: an RGB stream encoding the spatial information from static video frames and a motion stream encoding temporal information from sequences of optical flows (OFs) across video frames. A deep learning model then recognizes actions using the two feature streams. Surprisingly, our experimental study revealed the latency of the pipeline is dominated by the OF feature extractor using a standard OpenCV implementation. Furthermore, while a deep-learning-based OF extractor effectively reduces the latency, it incurs a substantial drop in recognition accuracy. The tradeoff between the standard and deep learning approaches to OF extraction highlights the need for a novel motion feature extractor design that is both efficient and accurate.\nTo this end, we propose the Integrated Motion Feature Extractor (IMFE), a novel motion feature extractor specifically designed to meet these stringent requirements on embedded systems. In contrast to the existing motion feature extraction approach relying on computationally expensive OF extraction, IMFE introduces a lightweight single-shot motion feature extractor that directly generates motion features without the need for OF extraction.\nBased on IMFE, we have developed and evaluated RT-HARE, an end-to-end HAR framework for real-time HAR on embedded platforms. Experiments on an Nvidia Jetson Xavier NX platform demonstrated that RT-HARE can perform HAR at 30 video frames per second (10x improvement over the original HAR pipeline) while incurring only a moderate loss in recognition accuracy when compared to a server-based HAR system. While this work is motivated by the challenges in HAR, IMFE may be applied to any real-time video analytics applications using motion features, enhancing their efficiency and real-time performance on embedded platforms.\nSpecifically, our contributions are as follows:\n\u2022 Experimental study identifying a standard OF extractor as the latency bottleneck in a state-of-the-art HAR approach,\n\u2022 Exploration of the latency-accuracy tradeoff between traditional and deep learning approaches to OF extraction,"}, {"title": "II. PROBLEM FORMULATION", "content": "This section provides an overview of the problem statement of HAR and the soft real-time requirements inherent in such systems."}, {"title": "A. Objective of HAR", "content": "The objective of video-based HAR is to minimize the error between the recognized and true action labels for a set of videos. Given a video V consisting of a sequence of M frames, {X1,X2,X3,XM}, the video frames are grouped into mini clips of length K, {X1, X2,\u00b7\u00b7\u2026, XN}, where N = M/K_is the total number of clips. A feature extractor E extracts the video features f from clips, ft = E(Xt), where t \u2208 1, 2, ..., N Finally, the video is represented as a list of extracted features, V = {f1, f2, ..., fv }. The goal of HAR is to assign an action label l from a predefined set of actions L = {11,12, ...,IC}, where C is the number of possible actions classes. We aim to find a \"recognition\u201d transformation R : V \u2192 L that classifies the video features into possible action categories.\nUsually, for a live action recognition system, V refers to a buffered history\nVt = {ft-B+1, ft\u2212B+2,\u00b7\u00b7\u00b7, ft\u22121, ft},\nwhere B is the size of the buffer. The recognition algorithm classifies the action class at the current timestamp,\n\u00cet = R(Vt) \u2208 L.\nAt run time, whenever a new clip Xt+1, containing the latest frames, becomes ready, the features buffer is updated in a First-In-First-Out manner, with Vt+1 = {ft-B+2, ft-B+3,..., ft, ft+1} and initiates the new inference task, R(Vt+1)."}, {"title": "B. Soft Real-time Requirement", "content": "Live HAR can be modeled as a periodic task with soft real-time requirements. The recognition task initiated by the new clip Xt is expected to be completed before the next clip Xt+1 is available to avoid backlogs and buffer overflow. As a result, the deadline is essentially the duration for K frames to arrive,\nTD = K \u00d7 TINT,\nwhere TINT = 1/FPS is the time interval between frames, i.e., the inverse of the frame rate (FPS) of the video."}, {"title": "III. EXPERIMENTAL EXPLORATION", "content": "This study specifically focuses on the two-stream feature extractor, illustrated in Fig. 1, a foundational building block widely used in state-of-the-art HAR frameworks to extract the features in Eq. 1. A video stream periodically adds video frames into the frame buffer. The feature extraction modules utilize two feature streams to encode the spatial and temporal information separately. Then, the action recognition modules predict the action based on buffered recent features. The prediction results are finally refined with a post-processing filter to smooth out fragmented action predictions.\nMotion feature extraction involves two steps: OF extraction and flow feature extraction. For a K-frame video clip, X\u2081 = {Xi,1,Xi,2,\u00b7\u00b7\u00b7,xi,K}, the OF extractor extracts the K-1 OFs from consecutive frame pairs:\nFLOW = {flowi,j|\u22001 \u2264 j \u2264 K \u2212 1}\nwhere flowi,j = OF(Xi,j,Xi,j+1). These OFs are then fed to the Flow Feature Extractor to generate motion features.\nThe RGB feature is extracted from the central frame of the clip, Xi,K//2, using the RGB feature extractor. We use ResNet50-based 2D CNN feature extractors ERGB and EFlow for both RGB and flow features. The clip features are extracted by concatenating the outputs from ERGB and EFlow:\nfi = concat (ERGB(Xi,K//2), EFlow(FLOWi))\nThe extracted features are stored in the feature buffer Vt as mentioned in Eq. 1.\nThe action recognition module then recognizes predefined actions from extracted features. To realize live HAR and balance efficiency and performance tradeoff, we employ the Long Short-Term Transformer (LSTR) [5] for action recognition. A key characteristic of LSTR is the split of long and"}, {"title": "C. Latency Bottleneck", "content": "In our experiments, when we sequentially input video frames into the HAR pipeline and make action recognition at the designated frequency (one prediction every 6 frames), we observed a high processing time of about 21 minutes for a video with a duration of 6 minutes and 30 seconds. The lengthy processing time means the original solution is impossible for real-time video applications.\nTo better understand the computational bottlenecks in this setup, we conducted a comprehensive latency analysis and presented the breakdown of each component in Table I. Single action recognition is made every six frames, requiring five TV-L1 OF extractions between consecutive frame pairs. Surprisingly, we found that the TV-L1 OF extraction, the core in the motion feature extraction process in numerous state-of-the-art solutions, contributes to 93% of total latency on average on the embedded testbed, even though TV-L1 has already taken advantage of GPU acceleration leading to a speedup over running on CPU. The latency also shows extra variability ranging from 392 ms to 1642 ms with a standard deviation of 153 ms. In the plot, the latency and variability of other modules are trivial. The disproportionate latency and variability of the TV-L1 explains the observed long processing time."}, {"title": "D. Impact of Video Down-sampling", "content": "It sounds natural to fit the system latency into the actual playback speed of the video for live prediction via frame downsampling. A lower video frame rate reduces the number of HAR predictions and relaxes the deadline for each prediction. Although theoretically appealing, taking a 5x downsampling to 6 FPS only reduces the processing time from 21 minutes to 6 minutes and 42 seconds. The reduction in latency is not proportional to the reduction in frame rate. Further investigation shows that the latency of TV-L1 in each recognition increased, and we assume a direct correlation between latency and the intensity of motion in the video, as downsampling intensifies the motion due to an increased frame interval.\nWe verified our assumption by examining the corresponding video content with a visualization of TV-L1's latency patterns"}, {"title": "E. Extracing OF with Neural Networks", "content": "To address the large latency and instability caused by the TV-L1 OF extractor, we explore neural networks to extract OF as they can take advantage of runtime optimization with neural network inference libraries (e.g., TensorRT), resulting in more stable computations. Among state-of-the-art OF networks, e.g., RAFT [8], FlowNet 2.0 [9], FlowFormer [10], GMFlowNet [11], RPKNet [12], we turn to RAFT for OF extraction due to its efficiency and top-tier OF quality. RAFT is representative of state-of-the-art neural networks for OF extraction that adopt a similar computation procedure including frame feature extraction, cost volume construction with correlations [8] or transformer blocks [10], iterative refinement and upsampling. We select the pre-trained RAFT-Large variant due to its relatively high OF quality in comparison to RAFT-Small. On average, the latency of one RAFT-Large inference is 26.64 ms, with std = 2.02 ms. While this method fits within real-time processing constraints, it results in a notable 9.8% loss in HAR accuracy. More implementation and evaluation details will be provided in Sec. VI.\nWhile RAFT compromises recognition performance for runtime efficiency, an optimal solution should balance high-quality feature extraction with minimal latency. Our explorations into the second option reveal that redesigning the feature extractor yields exceptional results. The following sections provide the design considerations and implementation specifics of our proposed IMFE."}, {"title": "IV. INTEGRATED MOTION FEATURE EXTRACTION", "content": "While RAFT significantly accelerates OF extraction compared to TV-L1, inefficiencies persist. As illustrated in Fig.5, the improved two-stream implementation consists of RAFT and a ResNet50-based feature extractor. RAFT includes a context encoder and feature encoders for two consecutive input frames, which are then correlated and processed through a motion encoder, a recurrent neural network (RNN), and upsampling to produce OF vectors. Notably, RAFT needs to run K - 1 times to generate a stack of OF vectors (Eq.3) to form the input to the ResNet50-based feature extractor.\nWe identified the following weaknesses inherent in such a design:\n\u2022 A typical first step in neural-network-based OF extractors [8], [10], [13] is employing convolutional Feature Encoders on the input frames to encode regional information at a reduced resolution. However, this design becomes inefficient when processing consecutive video frames, as one frame will be encoded twice when extracting the OFs with frames before and after.\n\u2022 Although OF extraction takes the majority of inference latency, it is important to note that OF vectors are only used as intermediate results; they are not used directly by the action recognition model.\nTo address these problems, we propose IMFE, an integrated motion feature extractor that uses a single neural network to replace the combination of the OF model and the flow feature extractor and generates the motion features in a single pass."}, {"title": "A. Design of IMFE", "content": "As shown in Fig. 5, traditional motion feature extraction follows a two-stage process: 1) capturing the correlations between two video frames by extracting OF, and 2) condensing the correlation information into the motion feature vector using the Flow Feature Extractor. Instead of extracting OF as an intermediate input to the Flow Feature Extractor, we propose to extract motion features directly from the inter-frame correlations computed at the beginning of the OF extraction process, thereby drastically reducing the complexity and latency of the motion feature extraction process. The architecture of our novel motion feature extractor, IMFE, is shown in Fig. 6. It extracts motion features without OF. Furthermore, IMFE takes all K video frames as inputs in one batch. The \"Batched Feature Encoder\" in Fig. 6 eliminates duplicated feature extraction.\nSpecifically, for a batch of K consecutive frames represented as X \u2208 \u211dK\u00d73\u00d7H\u00d7W, a feature encoder, consists of 6 residual blocks, produce features maps of frame content down-scaled at 1/8 of their original size. The feature map is of shape K \u00d7 Dx \u00d7 \u221aH/8 \u00d7 \u221aW/8 with D being the dimension of the feature vector representing each position. Then, feature-wise correlations between consecutive frames are computed with the correlation modules (the \"Cor\" blocks in Fig. 6). It generates K - 1 correlation volumes of shape (H/8 \u00d7 W/8)\u00d7(H/8 \u00d7 W/8), which is dot-product of vector pairs between consecutive frames to correlate displaced pixel patches. In contrast to prior OF networks that generate multi-level correlation pyramids to detect motion across various scales, our design maintains only the finest resolution level at 1/8 scale to reduce complexity. These K-1 volumes are then concatenated to consolidate correlation information (the \"Concatenate\" block in Fig. 6). To ensure efficiency in motion feature extraction and address the increased data dimension after concatenation, we employ a Compression and Encoding Module, which consists of five Conv2D layers. The first two layers compress the increased data size resulting from concatenation, significantly reducing data complexity in subsequent processing. The last three layers further extract and condense the motion information into dimensions compatible with the following modules. In the final stage, the Feature Encoder encodes latent information into motion features, utilizing nine residual blocks. The residual modules, fundamental to the ResNet50 architecture, are commonly used for feature extraction in deep learning, leveraging a well-tested architecture to enhance efficiency and effectiveness. The output dimension of the encoded feature is kept unchanged to preserve structural consistency and ensure seamless integration with subsequent HAR modules in the established frameworks.\nIMFE eliminates the need for reconstructing motion vectors by directly encoding the motion and contextual information into motion features. This design, compared with the combination of RAFT and Flow Feature Extractor, bypasses the upsampling of data from the feature dimension to the original image resolution, significantly reducing the complexity of the feature extraction process. As a result, IMFE effectively enhances efficiency and reduces latency in motion feature extraction. While motivated by and evaluated in the context of real-time HAR, IMFE can also be used as the motion feature extractor for other vision applications to improve real-time"}, {"title": "B. Training Strategy", "content": "A challenge for the new integrated motion feature extractor lies in training IMFE to generate appropriate motion features. One intuitive way to achieve this is to ensure IMFE produces similar motion features as the one using the standard two-stream method. If we denote IMFE as EIMFE, the feature extracted is fIMFE,i = EIMFE (X\u2081) (The lower path in Fig. 7.), and the one from the original flow feature extractor is fFlow,i = EFlow (FLOW\u00bf) (The upper path in Fig. 7.). Our objective is to minimize the difference between fIMFE,i and EFlow (TV-L1 (X\u2081)) given any video inputs. As a result, we utilize knowledge distillation [14] which is proven efficient for lightweight models to learn from large models [15]-[18]. Such consideration not only helps the model learn from the well-established TV-L1 method but also regularizes the feature to encode motion information only. More importantly, during network training, we can take any video clip as input and automatically generate the target feature with supervision from the standard feature extraction method to make the best use of available videos. As in the rightmost part in Fig. 7, we use the Mean Square Error (MSE) as the distance metric to quantify the difference between the two extracted features and minimize the following objective,\nL = \ud835\udd3c [MSE(EIMFE (X), EFlow (TV-L1 (X)))]\nA way to achieve this is to ensure that IMFE should produce similar motion features as the one using the standard TV-L1 method. If we denote IMFE as EIMFE, the feature extracted is fIMFE,i = EIMFE(Xi) (the lower path in Fig. 7), and the one from the original flow feature extractor is fFlow,i = EFlow(FLOW\u00bf) (the upper path in Fig. 7). Our objective is to minimize the difference between fIMFE,i and EFlow (TV-L1(X\u2081)) given any video inputs.\nTo achieve this, we utilize knowledge distillation [14], which has been proven efficient for lightweight models to learn from larger models [15]-[18]. This approach not only helps the model learn from the well-established TV-L1 method but also regularizes the feature to encode motion information exclusively. More importantly, during network training, we can take any video clip as input and automatically generate the target feature with supervision from the standard feature extraction method, maximizing the utility of available videos. As illustrated in the rightmost part of Fig. 7, we use the Mean Square Error (MSE) as the distance metric to quantify the difference between the two extracted features and minimize the following objective:\nL = \ud835\udd3c [MSE(EIMFE(X), EFlow (TV-L1(X)))]\nThis loss function helps IMFE learn to generate features that closely mimic those produced by the TV-L1 method, thereby maintaining high accuracy despite the reduced network complexity."}, {"title": "V. IMPLEMENTATION", "content": "We trained IMFE and the LSTR recognition module in RT-HARE on a DL server and deployed the live HAR pipeline on an embedded system with TensorRT as the optimization technique for efficient inference. This section provides the model training and deployment details and the hardware specifications for the computing hardware we used."}, {"title": "A. Hardware", "content": "We use the same Jetson embedded platform as discussed in Sec. III-B. Model training and export are on a DL workstation with AMD Ryzen Threadripper 3960X 24-Core Processor, 128GB RAM, and dual Nvidia GeForce RTX 3090 GPUs. The server runs Ubuntu 20.04 with a kernel version of 5.15. The \"Distributed Data Parallel\" package is used to facilitate multi-GPU training. We will open source the details of the training environment and code."}, {"title": "B. RT-HARE", "content": "RT-HARE use the same two-stream architecture in Fig. 1 except with IMFE as the motion feature extractor. Consistent with prior frameworks, RT-HARE uses the same clip length K = 6, meaning that action recognition is made every 6 frames. For all feature extraction methods in this work, we keep consistent with the dimensions in the TV-L1+ResNet50 feature extraction with a short-side length of 256 pixels and an aspect ratio of 4:3. To meet the requirement of the feature encoding process in RAFT and IMFE that the input shape should be multiples of 8, we round the height and width to the nearest value, i.e., H \u00d7 W = 256 \u00d7 344. We develop and train all neural networks in PyTorch.\n1) IMFE: We set the dimension of the feature map after the batched feature encoder in IMFE, D = 256, as described in Sec. IV-A. Consistent with the original flow feature extractor, the motion feature generated by IMFE will be a vector of length 2048.\nThe training process takes a subset of 4,359 videos from the ActivityNet [19] dataset, where 4,059 videos are used for training and the rest 300 for validation. We note that each video is separated into multiple clips and forms training and validation sets of sufficiently large size. The models are trained with a batch size of 16 on each GPU using"}, {"title": "C. Embedded System Deployment", "content": "At deployment, we export each trained model to the Open Neural Network Exchange (ONNX) format for interoperability on the embedded system. Models are optimized and run on the Jetson with TensorRT, an automatic model optimization and acceleration tool for neural network inference. Specifically, we enable mixed precision execution (fp32 and fp16) for all models\u00b2 and rely on TensorRT to generate the optimized network execution plan. For model layers with parameters exceeding the range of fp16, we manually set the precision of these layers to fp32.\nIn our current design, the modules are executed sequentially on the Jetson platform. We found that the GPUs are fully utilized during execution, which suggests pipelined execution may not yield significant performance improvement. In Section VI-D, we briefly explored parallelization on heterogeneous devices and recognized the potential for integration with existing real-time optimization techniques tailored to various hardware and application scenarios. Detailed investigations are deferred to future work."}, {"title": "VI. EVALUATION", "content": "In this section, we evaluate RT-HARE on the Jetson testbed, focusing on two primary aspects. First, we assess the real-time performance of RT-HARE on the embedded platform by measuring end-to-end latency and deadline miss ratios. Next, we examine the impact of IMFE on action recognition performance, comparing it with current state-of-the-art methods."}, {"title": "A. Dataset and Frame Rate", "content": "1) Evaluation Dataset: We evaluate our HAR solutions on the 50 Salads dataset, a real-world benchmark in action recognition containing 50 top-view camera recordings at 30 FPS of various salad preparation steps, and it aligns well with the practical HAR scenarios. We keep consistent with other work using the dataset and use 5-fold cross-validation, averaging the metrics achieved at the last training epoch, to evaluate the quality of features extracted by IMFE and other baselines in differentiating actions.\n2) Various Frame Rate: In practice, HAR systems may not always operate at high frame rates for video inputs and action recognition due to varying scenarios and camera configurations. Additionally, methods with longer latency may struggle to meet real-time constraints when frame rates are high with stricter deadlines as indicated by Eq. 2. To simulate videos captured under different camera settings and expand our evaluation scope, we downsampled the training and evaluation videos in the 50 Salads dataset to 30, 15, 6, and 3 FPS, which corresponds to 100%, 50%, 20%, and 10% of the original frame rate. Each rate approximately halves the preceding one, with live prediction deadlines of 200 ms, 400 ms, 1000 ms, and 2000 ms, respectively. At each frame rate setting, we re-train all action recognition models for adaptation."}, {"title": "B. Baselines for comparison", "content": "To demonstrate the novelty and effectiveness of IMFE's design, we compare our RT-HARE with the following baseline methods. The name of the baselines indicates how the OF vectors are extracted in the original two-stream framework. For"}, {"title": "C. Latency", "content": "For real-time HAR systems, end-to-end latency reflects the system's ability to make timely predictions and handle high frame rates. To demonstrate IMFE's outstanding efficiency, we measure each module's latency, as shown in Table II, for a thorough comparison. For OF modules, we provide the measurements at 30 and 6 FPS to illustrate the neural network's stability to motion magnitude. Finally, we discuss the end-to-end latency (Table III) of each HAR pipeline on the testbed and discuss the deadline missing ratio under real-time HAR conditions.\n1) Latency of Feature Extraction Modules: Table II presents the average latency and standard deviation for each"}, {"title": "D. Inference Parallelization on A Heterogeneous System", "content": "In this work, we consider a typical AI-embedded system that uses GPU for all neural network loads. Modern embedded systems are increasingly adopting heterogeneous architectures, incorporating CPUs, GPUs, and Deep Learning Accelerators (DLAs). Task parallelism on different hardware can be utilized to accelerate AI applications. For example, the Jetson Xavier NX includes additional power-efficient DLAs with approximately 36% of the GPU's performance. The two-stream design is naturally suitable for parallel inference, where we can allocate the less complex RGB stream to the DLA and the motion feature extractor to the GPU. In this subsection, we briefly discuss the system's performance incorporating DLA.\nAt 30 FPS, RT-HARE, as expected, achieves a similar 63.59% 5-fold cross-validation accuracy after post-processing with the RGB feature extractor deployed on the DLA and RAFT achieves 59.21%. The slight but negligible difference in accuracy is due to different TensorRT optimizations and precision settings applied to the different hardware. As shown in Table III, through workload parallelization across the GPU and DLA, we decrease the end-to-end latency of RT-HARE from 68.83 ms to 62.98 ms, with a difference of approximately the RGB feature extractor's latency. The end-to-end latency of the RAFT pipeline is also reduced but with a smaller difference from 169.74 ms to 168.69 ms. Both methods exhibit increased latency variance. However, similar parallelization for the RGB stream for the TV-L1 pipeline is less beneficial as its OF extraction process dominates the end-to-end latency with extra instability.\nAs a future direction, we envision that the RT-HARE can be further accelerated by employing advanced techniques in workload distribution, memory allocation, synchronization, and scheduling. For example, integrating the emerging research in DNN inference scheduling for heterogeneous embedded computing systems [26]\u2013[28] could optimize resource utilization of RT-HARE at a finer granularity."}, {"title": "E. Impact on Activity Recognition Performance", "content": "In this section, we examine the impact of our proposed IMFE on activity recognition performance. We use a set of standard metrics to compare the performance of RT-HARE against various baselines on the targeted dataset, providing a comprehensive evaluation of activity recognition performance.\n1) Evaluation Metrics: We use the common metrics for action recognition and action segmentation on the 50 Salads datasets [7], including frame-wise accuracy, F1@k score, and edit [29]. Given a video V of length T, a set of predicted frame labels \u0302L = {\u0302l1, \u0302l2, ..., \u0302lT } and the corresponding ground truth frame labels L = {l1, l2, ..., lT }.\nAccuracy: The frame accuracy is defined as: A =  \u2211Ti=1 \u03b4(\u0302li, li)/T, where the function \u03b4(x,y) is an indicator function that outputs 1 if x = y (i.e., the prediction is correct) and 0 otherwise. The goal is to evaluate the proportion of the frames correctly classified, and the higher, the better.\nEdit and F1@k metrics additionally focus on evaluating the temporal alignment and segmentation quality of action recognition predictions, beyond mere accuracy. During evaluation, predictions \u0302L and the ground truth L are refined by merging consecutive identical predictions into action segments that include the action class, start, and end times, (l,tstart, tend). These segments become S\u0302 = {\u015d1, \u015d2, ..., \u015dm} and SL = {s1, s2, ..., sn}, where \u015di (for 1 \u2264 i \u2264 m) and sj (for 1 \u2264 j \u2264 n) represent the predicted and ground truth action segments, with m and n being the number of predicted and ground truth segments.\nEdit: The edit score is calculated as the Levenshtein distance [30], [31] between two sequences of actions, i.e., the minimum number of operations needed to transform sequence A into sequence B.\nEdit(V) = Levenshtein(S\u0302, SL)\nF1@k: The F1@k metric [32] evaluates both segment-wise accuracy and segmentation quality. A segment, \u015di, is considered a true positive (TP) if it aligns with a ground truth segment with an intersection-over-union (IoU) exceeding k/100, commonly k = 10, 25, or 50. Each ground truth segment can only be matched once. Unmatched prediction segments are false positives (FP), and unmatched ground truth segments are false negatives (FN). With precision = TP/(TP+FP) and recall = TP/(TP+FN) , the F1@k score is calculated as:\nF1@k = 2 \u00d7 precision \u00d7 recall/(precision + recall)"}, {"title": "VII. RELATED WORK", "content": "There has been significant progress in algorithms for video-based HAR. Early works [33]\u2013[35] used a stack of RGB frames as input to recognize human actions with Convolutional Neural Networks (CNNs). The two-stream structure was first proposed in [6] to recognize actions from both spatial (RGB) and temporal (motion) feature streams. The two-stream design efficiently reduced data dimensionality during feature extraction and, thus, became a popular choice in subsequent HAR algorithms [1], [2], [36]\u2013[39].\nReal-time applications require HAR to be performed on live video streams, where HAR can only utilize video inputs available up to the current timestamp. Live HAR algorithms have been developed using the two-stream architecture recently. OadTR [40] recognizes the ongoing action from historical encoded features with a transformer. Long Short-Term Transformer (LSTR) [5] splits historical information into long and short memory, sampling the long memory at a lower rate to capture longer time efficiently spans. TeSTra [41] alternatively uses a streaming attention mechanism for recognition.\nAs Vision Transformers [42] show superior performance in capturing spatio-temporal correlations with its attention mechanism, several transformer-based algorithms have been introduced to perform HAR [43]\u2013[45] using video frames directly as inputs. However, their computational complexity and memory footprint are prohibitive for embedded platforms due to the high dimensionality of video frame data.\nDespite the significant advancement in HAR algorithms, all the aforementioned algorithms were implemented and evaluated on servers. In contrast, our work focuses on real-time performance of live HAR systems on embedded platforms. Henceforth, we adopt the two-stream architecture for its relative efficiency over vision transformers and superior per-formance over approaches using a single RGB stream. While our implementation of RT-HARE and evaluation employs the transformer-based LSTR as a representative action recognition model, our IMFE motion extractor can be used with other action recognition models or video analytics using motion features with enhanced real-time performance. Furthermore, the insights on achieving real-time performance on embedded"}, {"title": "B. HAR on Embedded Platforms", "content": "HAR on embedded platforms remains largely unexplored. Earlier efforts employ primitive algorithms tailored to limited actions. For example, Kong et al. [46] implemented a fall detection task on embedded systems using histograms of oriented gradients and filtering with a support vector machine classification model. Lightweight CNNs, such as MobileNetV3 [47] and VGG [48], have been adopted to extract video features and recognize actions with simple multi-layer perceptron on embedded systems [49], [50]. However, these works use simplistic algorithms tailored to specific action labels, which cannot be generalized to more complex actions with longer time dependencies. Real-world actions are continuous, interrelated, complex, and often similar, requiring HAR systems to capture these subtleties. To address these challenges, we focus on state-of-the-art HAR algorithms that have superior recognition capabilities but introduce substantial challenges in real-time performance."}, {"title": "C. Real-Time Resource Management for Deep Learning", "content": "There has been significant research on system-level resource management and real-time scheduling of deep learning tasks. For example, LaLaRAND [51] dynamically allocates DNN layers to CPUs or GPUs to enhance schedulability. Prophet [52] optimizes real-time perception for autonomous vehicles by coordinating multiple DNNs on a CPU-GPU architecture. RED [53] employs intermediate deadline assignments to enhance throughput and timeliness in robotic systems under environmental dynamics. The Demand Layering [54] adjusts memory allocation based on current demands for effective resource utilization. RT-LM [55] uses an uncertainty-aware framework to predict output length variability, improving resource allocation for language models.\nIn this work, we focus on application-level solutions by identifying latency bottlenecks in the HAR pipeline and introducing a novel motion feature extraction approach that leads to a drastic reduction in HAR latency. Our approach is, therefore, complementary to those system-level solutions. An advantage of our application-level approach is that it can be readily deployed on standard platforms (e.g., TensorRT and Linux OS). Future work may explore system-level solutions to further improve the real-time performance of HAR systems."}, {"title": "VIII. CONCLUSION", "content": "In this work, we present an embedded system\nThis work enables embedded platforms to achieve real-time human activity recognition (HAR) on live videos using advanced deep-learning techniques. We identified that the end-to-end latency in a HAR pipeline is dominated by the optical flow (OF) extractor in the widely used OpenCV library. Although existing deep-learning-based OF extractors can reduce the latency of OF extraction, they result in a considerable decrease in recognition accuracy. To address this, we introduce IMFE, a novel single-shot neural network architecture that extracts motion features without extracting OF, significantly reducing HAR latency with only a moderate impact on recognition accuracy. We developed and evaluated RT-HARE, a real-time HAR system that can perform real-time HAR at least 10 times the video frame rates of standard HAR implementations while maintaining high recognition accuracy. Furthermore, IMFE is a highly efficient motion feature extractor that can be integrated into other HAR pipelines and video analytics systems, potentially enabling real-time performance across a wide range of computer vision applications. While RT-HARE has demonstrated substantial improvements in real-time performance, future research could further enhance this by incorporating system-level resource management techniques."}]}