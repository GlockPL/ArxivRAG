{"title": "Pathways on the Image Manifold: Image Editing via Video Generation", "authors": ["Noam Rotstein", "Gal Yona", "Daniel Silver", "Roy Velich", "David Bensa\u00efd", "Ron Kimmel"], "abstract": "Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original image's key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation.", "sections": [{"title": "1. Introduction", "content": "Image editing has witnessed remarkable advancements through deep learning and text-guided diffusion networks. These developments have set a new benchmark for image manipulations, enhancing both control and quality. However, current approaches continue to face significant limitations in real-world scenarios. These methods often struggle with two key challenges: achieving precise edits that accurately reflect the intended modifications, and preserving the essential characteristics of the original image content.\nState-of-the-art techniques predominantly rely on text-guided diffusion models, which iteratively denoise random latent representations to generate edited images. Such methods condition the generation process on both the source image\u2014using techniques such as latent inversion [30] or model fine-tuning [21]\u2014and the target edit description. However, these approaches require the model to generate a single output image that preserves source image fidelity while implementing complex edits, often compromising edit accuracy and content preservation.\nIn this paper, we propose a paradigm shift in image editing by reformulating it as a video generation task. Instead of attempting this single state transition, our approach harnesses temporal coherence: the source image serves as the initial frame of a video that progressively and naturally transforms toward the target edit. This temporal evolution allows the editing process to unfold through physically plausible intermediate states, providing a continuous path between source and target images, as illustrated in Figure 1.\nThis temporal approach leverages the sophisticated world understanding embedded in recent video generation models, which have achieved breakthrough results in temporal coherence and visual quality through training on large-scale internet data [2, 46]. From a geometrical perspective, conventional editing approaches project initial noise onto the natural image manifold, targeting a single point where the image aligns with both the source and the edit request. In contrast, our approach generates a continuous path along the manifold between the original and edited image, producing a smooth realistic transition across different image states, as thoroughly discussed in Section 4.\nWe implement the proposed approach through a structured pipeline called Frame2Frame (F2F). First, we transform the edit instruction into a Temporal Editing Caption\u2014a scenario describing how the edit should naturally evolve over time\u2014using a pretrained Vision-Language Model (VLM). Next, a state-of-the-art image-to-video model generates a temporally coherent sequence guided by the temporal caption. Finally, we identify the frame that best realizes the desired edit with the assistance of a VLM. Extensive experiments demonstrate improvements over existing image-to-image approaches. We evaluate on TedBench [21] and PosEdit, a newly curated dataset derived from UTD-MHAD [6], which focuses on human pose transformations. PosEdit pairs source images with ground-truth targets of the same subject in different poses, enabling rigorous evaluation of both edit accuracy and source fidelity. Beyond commonly defined editing tasks, our framework shows promising results in more classical computer vision problems such as de-blurring, de-noising, and relighting by recasting them as temporal progressions, suggesting broader applications for video-based image transformations.\nOur main contributions include:\n1.  Reformulating image editing as a generative video task-leveraging temporal coherence to create edit paths on the natural image manifold, enabling high-fidelity manipulations while preserving source characteristics.\n2.  Frame2Frame: an end-to-end framework that realizes the reformulation through three key components: (1) temporal editing captions, (2) generated video-based editing, and (3) automated frame selection.\n3.  Comprehensive evaluation showing state-of-the-art performance on TEdBench and PosEdit, a new dataset for evaluating human pose edits."}, {"title": "2. Related Efforts", "content": ""}, {"title": "2.1. Image Editing", "content": "Text-based image editing has advanced significantly with the success of generative diffusion models [18]. These models, in their text-to-image version, generate images through a denoising diffusion process conditioned on input text, relying on ground truth pairs of text and images for training [35, 37]. In contrast, image editing often lacks predefined ground truth data for source and target images, presenting unique challenges. This limitation has led researchers to explore a diverse range of editing methodologies [19]. For example, SDEdit [28] introduces noise into an image and then denoises it based on an editing target prompt. Imagic [21] fine-tunes a text-to-image model on a single image, subsequently interpolating between input and target text embeddings to produce edits. Other methods first invert the input image into a diffusion model's latent space [20, 30] and then generate the edited image from that latent representation using various techniques for structure preservation and manipulation [17, 32]. InstructPix2Pix [5] synthesizes an editing dataset using the approach in [17], filters it, and employs this dataset to train a diffusion model in a supervised fashion. Paint-by-Inpaint [45] further explores this supervised approach, generating a real-image dataset for object insertion."}, {"title": "2.2. Generative Video Models", "content": "Recent years have seen remarkable advancements in video generation, progressing from systems limited to specific domains [15] and brief clips to models capable of generating diverse, high-fidelity content. This progress has been notably driven by paradigm shifts and the significant scaling of both datasets [7] and architectures. Approaches have evolved from recurrent networks [16, 40] and generative adversarial networks (GANs) [9, 41, 42] to latent diffusion models (LDMs) [2, 3, 11], which leverage large U-Net or transformer-based architectures alongside vast internet-sourced datasets. Notable recent efforts include Stable Video Diffusion [2], which utilizes large curated datasets to train a latent diffusion model, and OpenAI's Sora [26], which demonstrates impressive capabilities by reportedly leveraging a large architecture and extensive public and private datasets. These models have been described as \"world simulators\" due to their emergent understanding of physical dynamics and temporal coherence. Our work builds upon CogVideoX [46], a transformer-based latent diffusion model that employs a 3D Variational Autoencoder to compress videos across both spatial and temporal dimensions, enhancing coherence. To improve text-video alignment, CogVideoX also integrates an expert transformer with adaptive LayerNorm, allowing deep fusion between visual and textual modalities.\nClose to our work, several recent efforts have utilized the world simulation capabilities of video diffusion models for various computer vision tasks. Make-A-Video3D [39] temporally extends static NeRFs using Score Distillation Sampling from video models. ViVid-1-to-3 [23] generates images along a camera trajectory around objects to enable novel view synthesis. PhysDreamer [49] models rigid object properties through 3D Gaussians and material fields, trained via distillation from pretrained video generators."}, {"title": "2.3. Image Editing and Video", "content": "The intersection of image editing and video has received limited attention. Existing methods focus on sampling pairs of random frames from videos to build image pair datasets that capture the same subject under varying conditions. For example, AnyDoor [8] uses the paired frames as an augmentation method, segmenting foreground objects in each frame and assigning one masked object as the target edited appearance of the subject. MagicFixup [1] employs these frames to build a dataset focused on refining user-made subject coarse 2D edits. Recently, drag-based editing approaches [27, 38], used frames extracted from video and computed optical flow to collect a dataset aimed at editing by spatially dragging points within the image. In contrast to these approaches, which rely on frame sampling for image data collection, we are the first to directly perform image editing using generative video models."}, {"title": "3. Frame2Frame", "content": "We present Frame2Frame, a framework that reformulates image editing as a temporal transformation process. Our approach leverages video generation models to create natural transitions between source and target images, achieving consistent and realistic edits. The proposed method has three main steps, as illustrated in Figure 3."}, {"title": "3.1. Temporal Editing Captions", "content": "Text-based image editing methods typically operate on two inputs: a source image \\(I_s\\) and a target caption \\(c\\), where \\(c\\) specifies the desired modifications to \\(I_s\\). Our approach differs fundamentally by modeling editing as a temporal process. This requires a novel type of prompt\u2014the Temporal Editing Caption, denoted by \\(\\check{c}\\)\u2014that describes the sequential transformation from source to target image. We construct \\(\\check{c}\\) by combining information from \\(I_s\\) and \\(c\\) to create a description of how the desired edit unfolds over time.\nTo automate this process, we leverage recent advances in Vision-Language Models (VLMs) [10, 14, 36, 43, 44]. The VLM, specifically ChatGPT-4o [31], is instructed to produce a concise video scenario that highlights how elements within the image change or move over time. The generated caption captures the essential transformations while maintaining a static camera perspective unless movement is necessary. To improve generation quality, we utilize in-context learning (ICL) [12], providing the VLM with nine exemplar prompt-caption pairs. The complete prompt template, ICL examples, and an ablation study comparing this approach with directly using the target captions are included in the Appendix."}, {"title": "3.2. Video Generation", "content": "We employ CogVideoX [46], a pretrained generative video latent diffusion model utilizing a transformer-based architecture. Specifically, we use its image-to-video variant, which has been fine-tuned to generate videos starting from an input image \\(I_s\\). During generation, \\(I_s\\) is encoded and concatenated with noise in latent space, where the model applies a denoising process guided by the temporal caption \\(\\check{c}\\). As elaborated in Section 4, this conditioning allows generated videos to start from \\(I_s\\) and evolve naturally along the image manifold, maintaining temporal coherence and consistency. Additionally, the model's transformer architecture enables effective fusion of visual and textual information, allowing precise control over the editing process through our temporal captions. Formally, given the video generator \\(G\\), we define the generation process as:\n\\[G(I_s, \\check{c}) = V = \\{f_1, \\dots, f_T\\}\\]\nwhere \\(V\\) denotes the generated video with \\(T\\) frames, and \\(f_t\\) represents the frame at timestep \\(t\\)."}, {"title": "3.3. Frame Selection", "content": "We observed that the optimal number of frames required for an edit can vary\u2014small changes may be completed in fewer frames, while more extensive transformations often necessitate additional ones. Additionally, later frames tend to deviate further from the source image. Thus, even though \\(V\\) serves as an editing path originating from \\(I_s\\), there is no guarantee that \\(f_T\\) is the optimal edited image in \\(V\\). For instance, in Figure 3, the edited image of the cat might be expected to capture the midpoint of its jump, but the last frame shows the jump already completed. Therefore, we aim to identify the optimal edited frame, denoted \\(f_{t^*}\\), which corresponds to the earliest timestep \\(t\\) that achieves the desired edit. The transition from the initial frame \\(f_1\\) (or \\(I_s\\)) to the final edited frame \\(f_{t^*}\\) motivates our method's name, Frame2Frame.\nTo automate the selection of \\(t^*\\) and avoid manual frame-by-frame review, we employ an automated approach. After generating the sequence \\(V\\), we sample every fourth frame, imprinting each with a unique identifier and assembling them into an image collage alongside \\(I_s\\). Inspired by [22], which introduces a novel approach to video comprehension by transforming videos into image grids, we use a VLM, specifically GPT-4o, to assist in selecting \\(t^*\\) by providing it with the collage and the editing prompt \\(c\\). The VLM is tasked with identifying the frame that best fulfills the editing intent, evaluating each frame's alignment with \\(c\\) and fidelity to \\(I_s\\). The model is instructed to select the optimal frame with the lowest index that completes the edit. An ablation study, detailed in the appendix, evaluates the effectiveness of our automated approach against the baseline of selecting the final frame of the video.\nIt is important to note that while we refer to an optimal \\(t^*\\), the definition of the required edit can be subjective and dependent on user preferences. For example, in Figure 3, different users may select frames of the cat jumping based on variations in its altitude. Thus, frame selection could serve as a flexible and customizable advantage in certain scenarios."}, {"title": "4. Editing Manifold Pathway", "content": "To illustrate the advantages of our method over conventional image-to-image approaches, we aim to visualize the editing process within the natural image manifold, where realistic images reside. Given the high dimensionality of this space, we project it into a lower-dimensional representation suitable for interpretation. To achieve this, we first generate three sets of images, each containing 200 samples, using the text-to-image generator FLUX.1-dev [24]. The sets are based on the following prompts:\n1.  AI: \"Full-body portrait of a happy person wearing a shirt with the word 'AI' on it\".\n2.  AI + Heart Hands: \u201cFull-body portrait of a happy person wearing a shirt with the word 'AI' on it and making a heart shape with their hands\".\n3.  Heart Hands: \"Full-body portrait of a happy person making a heart shape with their hands\".\nWe manually filter the outcomes to ensure alignment with the descriptions, with examples provided in the supplementary material. Then, we use a CLIP ViT-B/32 model [34] to extract a 512-dimensional feature vector for each image. Using these features, along with 25 feature vectors extracted from random noise images, we perform Principal Component Analysis (PCA) to reduce the dimensionality into a two-dimensional subspace. The resulting subspace, now suitable for visualization, is depicted in Figure 2.\nAs illustrated in the figure, the natural images form a smooth manifold, distinctly separated from the distribution of noise images. Within this manifold, there is a clear semantic progression: images of people with 'AI' shirts (green cluster) are close to images of people with 'AI' shirts making a heart shape (purple cluster), which are adjacent to images of people only making a heart shape (red cluster). Thus, transitioning smoothly along the manifold allows a person with an 'AI' shirt to perform a heart shape with their hands while preserving the shirt's text.\nConsider an original image from the AI group that we wish to edit using the target prompt: \u201cA happy person making a heart shape with their hands\". Current editing methods generate a single image, which may result in an abrupt transition to the red cluster, effectively removing the 'AI' on the shirt. This behavior is illustrated in the figure, where the edit performed by LEDITS++ is positioned near the red cluster, and the 'AI' text on the shirt disappears. In contrast, our method leverages video generation to perform the edit smoothly, moving along the manifold with incremental changes until it reaches the required edit in the purple cluster, as depicted by the black arrows in Figure 2. The temporal editing caption guiding this process is: \u201cA happy person very slowly raising their hands to form a heart shape\u201d. This gradual progression enables us to reach the purple cluster, resulting in an edited image where the person maintains the 'AI' on their shirt while making a heart shape\u2014a faithful preservation of the original image's key attributes. This experiment illustrates that the proposed paradigm enables smooth traversal across the image manifold, enabling consistent edits while preserving the essential characteristics of the original image."}, {"title": "5. Experiments", "content": "We evaluate Frame2Frame against state-of-the-art image editing methods, including LEdits++ [4], SDEdit [29], Pix2Pix-Zero [33] and Imagic [21]. Our evaluation spans two benchmarks: the established TEdBench [21] for general image editing, and our newly introduced PosEdit benchmark, specifically designed to evaluate human pose editing tasks. Finally, we conduct a human evaluation to better understand our method's performance based on real user preferences."}, {"title": "5.1. Evaluation Protocol", "content": "We conduct our experiments following a consistent evaluation protocol across all methods and benchmarks. Following common practice [4, 21], for each method and source image, we manually select the best result from fifteen random seeds based on visual quality and edit accuracy, ensuring the same seed set is used across all methods. For all methods, we use the default hyperparameters and settings as provided in their official implementations or official Hugging Face repositories 1.\nImage Preprocessing and Generation Details. We use CogVideoX [46] as our video generation backbone, which operates at a fixed resolution of 720 \u00d7 480 pixels. Both TEdBench and PosEdit benchmarks consist of images with 1:1 aspect ratio. To accommodate CogVideoX's resolution requirements while preserving image content, we first resize all source images to 480 \u00d7 480 pixels, then horizontally pad them with black pixels to reach the required 720 \u00d7 480 resolution (adding 120 pixels on each side). After generating the video sequence, we crop the central 480 \u00d7 480 region of the selected frame to remove the padding and resize it to the final evaluation resolution of 512 \u00d7 512 pixels, matching the standard resolution used in prior work. For video generation, we adopt the default hyperparameters proposed by CogVideoX: a guidance scale of 6, 49 generated frames per sequence, and 50 denoising inference steps. This configuration generates videos approximately 6 seconds in duration at a frame rate of 8 frames per second.\nMethod-Specific Requirements. Pix2Pix-Zero [33] requires additional source image descriptions along with the standard inputs. We generate these automatically using BLIP-2 [25], a state-of-the-art image captioning model, ensuring consistent source descriptions across all experiments. For our method, we transform the original editing prompts from both benchmarks into temporal editing captions, as described in Section 3.1."}, {"title": "5.2. TEdBench Evaluation Results", "content": "We quantitatively evaluate our method and baselines on the TEdBench benchmark using three complementary metrics. For each method, we compute the metrics between the source image and its corresponding best edited version. LPIPS [48] measures perceptual similarity to assess how much the edit preserves the source image's content, with lower values indicating better preservation. CLIP-I evaluates the similarity between source and edited images in CLIP's [34] feature space, where higher values indicate better preservation of semantic content. Finally, CLIP score measures alignment between the edited image and the target prompt, with higher values indicating better adherence to the editing instruction. As shown in Table 1, our method achieves strong performance across all metrics, demonstrating effective balance between preserving source content and achieving the desired edit.\nThe qualitative advantages of our method are visually evident in the comparisons presented in Figure 4. The figure demonstrates our method's superior performance across diverse editing scenarios, producing results that both faithfully execute the intended edits while maintaining strong alignment with the source image. Additional examples showcasing these capabilities are provided in the appendix."}, {"title": "5.3. PosEdit Benchmark", "content": "We introduce PosEdit, a benchmark for human pose editing derived from the UTD-MHAD dataset [6]. UTD-MHAD includes RGB videos of 8 subjects (4 females and 4 males) performing predefined actions in a controlled indoor environment. We carefully curated 58 editing tasks encompassing 8 distinct action categories, ranging from simple poses like a raised hand to complex athletic poses such as basketball shooting and lunging.\nThe proposed benchmark specifically focuses on human pose manipulation. Unlike TEdBench, each editing task in PosEdit includes a ground truth frame extracted from the same subject in the target pose. The availability of reference images allows for a more comprehensive evaluation of both editing accuracy and identity preservation. In the dataset, each editing task consists of two images: a source image showing the subject in a neutral standing pose with arms relaxed at their sides, and a ground-truth edited target image capturing the subject performing in a specific pose. The benchmark also provides a prompt for each task that describes the target pose the subject should achieve (e.g., \"A person in a basketball shooting posture.\").\nBenchmark Evaluation. The evaluation on PosEdit follows the same protocol established for TEdBench: generating multiple variants using nine different seeds and manually selecting the best result for each method. However, PosEdit's ground-truth target images enable additional evaluation metrics. Beyond measuring preservation of source content (via LPIPS and CLIP-I between source and edited images) and edit accuracy (via CLIP score with the prompt), we compute LPIPS and CLIP-I metrics between the edited image and its corresponding ground-truth target. This enhanced evaluation directly assesses both the accuracy of the transformation and preservation of identity features.\nTable 2 demonstrates that our method consistently outperforms competitive methods across all metrics, with particularly strong performance in similarity measures with the target ground-truth image. This advantage highlights our method's superior ability to preserve key features while achieving the desired edit, as illustrated by the qualitative comparisons in Figure 5. The figure shows that our method generates more natural pose transitions while maintaining crucial identity attributes such as facial features, body proportions, and clothing details.\nTwo important implementation notes: First, for Pix2Pix-Zero, which requires source image descriptions, we use the same static prompt across all tasks: \"A person standing naturally with his arms relaxed at his sides.\" Second, we exclude Imagic [21] from this comparison as their official implementation is not publicly available and their results are only reported on TEdBench."}, {"title": "5.4. Human Evaluation Survey", "content": "To further evaluate our method, we conducted a human survey. As indicated in Table 1, LEDITS++ emerges as the most competitive method compared to ours, making it a natural choice for comparison. Participants were presented with 20 random TEdBench samples, including the source image, the target edit prompt, and the corresponding edited outputs from both methods. Inspired by the methodology proposed by [47], participants evaluated each comparison based on two criteria: (1) edit accuracy relative to the prompt, and (2) edit quality-defined as the preservation of visual fidelity to the source image, seamless integration of edited elements, and the overall natural appearance of modifications. We collected responses from 59 randomly selected online evaluators. To ensure an unbiased assessment, evaluators were not informed of the study's objectives. The results were quantified using two metrics: (i) overall global preference, expressed as a percentage, and (ii) aggregated per-image preference, where a tie resulted in each method receiving 0.5 points.\nOur results, summarized in Table 3, demonstrate that F2F outperforms LEdits++ on both metrics. Specifically: For edit quality, F2F achieved a global preference score of 53%, slightly surpassing the 47% obtained by LEdits++. In terms of edit accuracy, F2F achieved a 65.6% global preference compared to LEdits++'s 34.4%. The per-image preference results mirrored this trend, indicating robustness across individual examples and no significant influence of outliers. These findings reinforce our claim that smooth temporal editing\u2014using video as a medium\u2014preserves essential scene characteristics while successfully performing the edit. Our results suggest that the gap between F2F and LEdits++ in source preservation is larger than reflected by the LPIPS scores in Table 1. The full survey details are provided in the supplementary materials."}, {"title": "5.5. Additional Vision Tasks", "content": "We demonstrate our framework's applicability beyond traditional editing by applying it to fundamental image manipulation tasks: denoising, deblurring, outpainting, and relighting. For these experiments, we utilize Runway Gen-3 [13] as our video generation backbone, as it showed superior performance on these specific tasks compared to other generative video models. Our results, shown in Figure 6, demonstrate strong performance across all tasks. We associate this success to the natural alignment between these operations and common video scenarios: deblurring maps to camera focusing, outpainting to camera motion, and relighting to time-lapse lighting changes. This alignment enables our method to leverage the video model's learned temporal dynamics, achieving high-quality results without task-specific training. Full prompts are provided in the supplementary material."}, {"title": "6. Limitations", "content": "While our approach addresses key shortcomings in current image editing methods, it also introduces unique challenges. For example, natural camera motion sometimes appears in video sequences, which, when replicated in generated content, can lead to unintended perspective shifts. As with all generative models, video models are trained on specific data domains, making it challenging to produce results that deviate significantly from the model's training data, which predominantly includes real-world transformations. Despite these challenges, F2F demonstrates success in several cases. For example, in the middle row of Figure 1, the vase is \"magically\" filled with green water, showcasing the model's ability to perform imaginative edits. Additionally, our method is computationally intensive, as transforming an image into a video sequence is resource-heavy and often slower than other image editing methods. However, video generation efficiency is advancing rapidly, as demonstrated by models like Runway Turbo\u00b2 and LTX-Video\u00b3, which can execute the process in seconds, potentially making this approach significantly less resource-intensive in the future."}, {"title": "7. Conclusions", "content": "We introduced Frame2Frame, a novel approach that reformulates image editing through video generation. By leveraging video models' inherent understanding of temporal transformations, our method achieves state-of-the-art editing results while maintaining high fidelity to source images. We demonstrated our framework's effectiveness on standard benchmarks, introduced PosEdit for human pose editing, and showed promising results on more classical vision tasks. As video generation technology advances, we expect our approach to enable increasingly sophisticated image manipulations while maintaining natural and physically plausible results.\nFuture Research. Our work opens several promising directions. First, fine-tuning video generators specifically for image editing presents an exciting opportunity, including straightforward solutions like enforcing static camera scenarios or using datasets curated for editing tasks, alongside more complex approaches yet to emerge. Secondly, a key direction could involve reducing the overhead of full video generation while preserving the benefits of gradual, temporally coherent transformations, potentially enhancing both the efficiency and speed of editing."}, {"title": "B. Temporal Editing Captions", "content": ""}, {"title": "B.1. VLM Instruction", "content": "As outlined in Section 3.1, we propose a framework for automatically generating the temporal editing caption by leveraging the original target editing prompt in conjunction with the source image. The instruction given to the VLM, along with the source image is:\n'Write a one-sentence description of a short video that begins with the provided image and smoothly transitions into a scene of a \"CAPTION\", highlighting how elements in the image undergo changes or movement over time. Keep the description simple, concise and short, focusing only on essential changes and actions without altering unnecessary details. Avoid mentioning elements that do not contribute to the main change needed, and focus the description on the main transitions. Do not add objects that are not in the original image or described in the final scene. The camera should remain static unless movement is absolutely necessary. Ensure all transitions happen within a few second duration without mentioning the length or using the word \"video\".'\nHere, \"CAPTION\" is replaced with the target caption specific to the image. Additionally, as explained, in-context learning is employed to provide the VLM with examples alongside the instruction. Before processing the desired source image and edit prompt, the instruction is presented to the VLM nine times, each paired with a distinct example consisting of a source image, target caption, and corresponding temporal editing caption. Examples of these are illustrated in Figure S9."}, {"title": "B.2. Ablation", "content": "To assess the impact of the Temporal Editing Caption, we conduct an ablation experiment comparing its use against directly using the target editing captions from the TEd-Bench benchmark. Apart from this modification, we adhere to the same protocols as described in the original experiment in Section 5.2. As shown in Table S4, this setup preserves a similar resemblance to the source image but underperforms in terms of image editing performance."}, {"title": "C. Frame Selection", "content": ""}, {"title": "C.1. VLM Instruction", "content": "As detailed in Section 3.3, our method selects the frame that best aligns with the intended edit from each generated video. To automate this process, inspired by [22], we create a collage of uniformly sampled frames from the video, along with the source image and target editing caption, and prompt a VLM to identify the optimal frame. The model is instructed to select the earliest frame (i.e., with the lowest index) that satisfies the editing intent, minimizing deviation from the original image. In both this process and the best seed selection process (applied across all methods), if none of the edited frames successfully fulfill the desired edit, the original image is retained as the final output.\nThe instruction provided to the VLM is:\n'The image displays the source photo at the top, with a collage of 12 edited versions beneath it. The target edit image caption was: \u201cCAPTION\". Your task is to choose the image from 1 to 12 that best follows this edit fully and naturally. If none of the images follows the edit, select image 0. If multiple images follow the edit equally, prioritize the one with the lowest number possible. Avoid selecting images that appear to follow the edit but are not edits of the original image. Additionally, avoid images where camera motion, zoom, or image quality differs significantly, or where the content does not appear stable relative to the original source. Respond with: \"The selected edit is:x\" where x is the number of your chosen edit.'\nHere, \"CAPTION\" refers to the target editing caption. Examples of the collages can be found in Figure $10."}, {"title": "C.2. Ablation", "content": "To validate the effectiveness of our approach, we compare it to the naive solution of using the last frame of the generated video as the edited output. The evaluation follows the same protocol described in Section 5.2. As can be seen in Table S5, this naive approach results in a lower target CLIP score for the edited outputs, highlighting the advantages of our method."}, {"title": "D. Editing Manifold Pathway", "content": "As elaborated in Section 4, to simulate the image manifold, we generated 200 images across three distinct categories using FLUX.1-dev. Examples of these generated images are shown in Figure $11."}, {"title": "E. PosEdit", "content": "In Section 5.3, we outline the construction of a human pose editing dataset. This dataset encompasses 58 editing tasks, distributed across 8 distinct action categories, featuring 8 different subjects. The source images consistently depict a neutral standing pose with arms relaxed at the sides, while the target poses vary according to the edit category. Each editing category is paired with a target caption and a temporal caption. Figure S12 and Figure S13 illustrates examples for each action category."}, {"title": "F. Human Survey", "content": "As detailed in Section 5.4, we conducted a human evaluation survey to assess our method's performance based on real user preferences. Following the framework in [47], the survey questions evaluated (1) the accuracy of the edit relative to the prompt and (2) the quality of the edit, defined as the preservation of visual fidelity to the source image. Each participant reviewed 20 edits, comparing our method with LEDITS++. Examples of the pages shown to the evaluators are provided in Figures S14 and S15."}, {"title": "G. Additional Vision Tasks Captions", "content": "As outlined in Section 5.5 and demonstrated in Figure 6, we showcase our framework's applicability for additional, more classic vision tasks that are not typically classified as image editing. For these tasks, we employ Runway Gen-3 as our video generator. Empirically, these tasks required longer and more descriptive captions. The temporal editing captions used for each task are as follows:\n1.  Relighting: 'The scene's lighting shifts gradually, changing to night. The sun is setting, and artificial lights replace it. The camera is static. Time-lapse. Cinematic.'\n2.  Outpainting: 'The image expands, adding new surroundings seamlessly beyond the original frame.'\n3.  Denoising: 'The image clears up as noise fades away, revealing smoother, cleaner details.'\n4.  Debluring: 'The camera comes into focus, revealing sharp details and enhanced clarity, as though a camera lens has adjusted perfectly. Nothing moves. Static image.'"}]}