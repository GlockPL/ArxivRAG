{"title": "Enhanced Self-supervised Learning for Multi-modality MRI Segmentation and Classification: A Novel Approach Avoiding Model Collapse", "authors": ["Linxuan Han", "Sa Xiao", "Zimeng Li", "Haidong Li", "Xiuchao Zhao", "Fumin Guo", "Yeqing Han", "Xin Zhou"], "abstract": "Multi-modality magnetic resonance imaging (MRI) can provide complementary information for computer-aided diagnosis. Traditional deep learning algorithms are suitable for identifying specific anatomical structures segmenting lesions and classifying diseases with magnetic resonance images. However, manual labels are limited due to high expense, which hinders further improvement of model accuracy. Self-supervised learning (SSL) can effectively learn feature representations from unlabeled data by pre-training and is demonstrated to be effective in natural image analysis. Most SSL methods ignore the similarity of multi-modality MRI, leading to model collapse. This limits the efficiency of pre-training, causing low accuracy in downstream segmentation and classification tasks. To solve this challenge, we establish and validate a multi-modality MRI masked autoencoder consisting of hybrid mask pattern (HMP) and pyramid barlow twin (PBT) module for SSL on multi-modality MRI analysis. The HMP concatenates three masking steps forcing the SSL to learn the semantic connections of multi-modality images by reconstructing the masking patches. We have proved that the proposed HMP can avoid model collapse. The PBT module exploits the pyramidal hierarchy of the network to construct barlow twin loss between masked and original views, aligning the semantic representations of image patches at different vision scales in latent space. Experiments on BraTS2023, PI-CAI, and lung gas MRI datasets further demonstrate the superiority of our framework over the state-of-the-art. The performance of the segmentation and classification is substantially enhanced, supporting the accurate detection of small lesion areas.", "sections": [{"title": "I. INTRODUCTION", "content": "MULTI-MODALITY magnetic resonance imaging (MRI) can provide complementary and mutually informative data about tissue composition (i.e., anatomical or functional information), enabling intuitive insight into the human body's interior. This assists radiologists and clinicians in detecting and treating diseases more efficiently. For example [1], to accurately define the shapes, sizes, and diffused locations of gliomas, the whole tumor (WT) region is highly distinguishable with fluid attenuation inversion recovery (FLAIR), and the enhancing tumor (ET) region exhibits clear structures in T1- weighted contrast-enhanced imaging (Tlce). In recent years, numerous deep learning-based computer-aided diagnosis algorithms have been proposed to fuse modality and extract the features for MRI segmentation and classification. However, these data-driven algorithms, such as U-Net [2] and its variants [3]-[8], require a large dataset with accurate annotations. Medical image annotations are still manually performed by experienced neuro-radiologists, which is extremely tedious and time-consuming. Moreover, the annotation of multi-modality MRI is more difficult to obtain than single-modality MRI.\nSelf-supervised learning (SSL), semi-supervised learning, and weakly supervised learning are viable deep learning"}, {"title": "II. RELATED WORK", "content": "So far, many deep learning based multi-modality MRI analysis methods have been proposed. For instance, Schelb et al. [25] trained a U-Net with T2-weighted and diffusion MRI to segmented prostate cancer. Based on U-Net, several networks, such as TransUNet [3],[26] and UNETR [4] employing vision"}, {"title": "B. Self-supervised Learning", "content": "Supervised learning methods mentioned above rely on fully annotated medical datasets. As a viable alternative, self- supervised learning obtains supervisory signals from given data and learns generalizable dense representations of the input. There are two main directions in self-supervised learning.\nThe first direction is contrastive learning [9],[32]-[34], which defines positive and negative sample pairs as learning tasks and treats them differently in the loss function. Marin et al. [32] pre- trained the ResNet50 backbone for object detection in chest X- ray. Zheng et al. [9] proposed a multiscale visual representation learning algorithm to perform finer-grained representation and to handle different target scales for downstream segmentation tasks. Together with the contrast loss, Zhou et al. [33] presented preservation contrastive learning for self-supervised medical representations. Moreover, Jiang et al. [34] designed a conditional anatomical feature alignment module to complement corrupted embeddings with globally matched semantics to create contrastive pairs in 3D medical analysis.\nThe other direction is MIM [21],[24],[35],[36], such as masked autoencoders (MAE), which represent the state-of-the- art (SOTA) on natural datasets. Yan et al. [35] proposed a privacy-preserving and federated self-supervised learning framework that collaboratively trained models on decentralized data using masked image modeling as the self-supervised task. Chen et al. [21] employed masked image modeling approaches to advance 3D medical image analysis using a lightweight decoder for reconstruction. Haghighi et al. [36] developed a framework that united discriminative, restorative, and adversarial learning in a unified manner. Yan et al. [24] proposed a hybrid visual representation learning framework for self-supervised pre-training on large unlabeled medical datasets using contrastive and generative modelling."}, {"title": "C. Model Collapse", "content": "Self-supervised learning that maps similar data to close spots in the representation space could lead to the collapsing problem. This means that all outputs tend to collapse to a constant feature. Some studies have introduced new self-supervised frameworks to avoid model collapse. For example, LeCun et al. [20] measured the cross-correlation matrix between the outputs of two identical networks called Barlow Twins. Zhang et al. [37] presented a new approach termed align representations with base (ARB), which aligns the learned embeddings to intermediate variables. Compared to Barlow Twins, ARB does not require pairwise decorrelation, resulting in linear complexity. Also, some researchers attempted to avoid model collapse by redesigning proxy tasks and loss function. Wang et al. [38] employed a network ended with a SoftMax operation to produce twin-class distributions of two augmentation images. Moon et al. [39] used the same random vector for the augmented embeddings of the given image. This implies that the embeddings are locally dispersed, giving a latent contrast effect between the embeddings of different images. Zhang et al. [23] established a close connection between MAE and contrastive learning and showed that MAE implicitly aligns the mask-induced positive pairs. They proposed a uniformity- enhanced MAE loss that can address model collapse."}, {"title": "III. METHODS", "content": "In this work, we investigate the reason why traditional masked strategy of SSL leads to model collapse in multi- modality MRI. For clarification, we define the following notations: D\u2081 = {(x,gi, y\u00b2)} is a labeled dataset with NL samples, where xi) is the image of the i-th sample in input multi-modality MRI, y \u2208 {0,1}denotes the class of the i-th sample, and g\u00b9 represents the corresponding ground truth segmentation.\nIt is known that, in contrastive learning, simply aligning positive pairs will result in full model collapse because the alignment loss can be minimized by the encoder [18]. MIM methods do not suffer from model collapse on natural image datasets during pre-training [23]. However, when directly applying MIM to multi-modality MRI datasets, model collapse occurs in pre-training due to the similarity of multi-modality MRI. Here, we theoretically investigate the issue of model collapse in MIM for MR images.\nIn MIM, an MR image x from D\u2081 is divided into n patches Rxs, e.g., x\u2208 R", "1}": "s drawn with probability as the mask ratio p (the ratio of removed patches), two complementary masked views of x are obtained:"}, {"title": null, "content": "x = x[m] \u2208 R*$, x\u2081 = x[1\u2212m] \u2208 R"}, {"title": null, "content": "where n\u2081 = n \u00d7 (1 \u2212 p), n2 = n \u00d7 p are integers satisfying n =\nn1 + n2. An MAE essentially learns to pair the two complementary views xm and xt via the reconstruction task, which can be modeled by a bipartite graph. Let Xm = {xm} and\nX\u2081 = {x}denote the set of all masked and target views, respectively. Each view x \u2208 X is considered as a node. Accordingly, there is an edge between two views when they are complementary views generated by masking.\nMIM produces latent connections among different input samples in the form of 2-hop connectivity [22]. Given a pair of\n2-hop input neighbors xm, xm' \u2208 Xm that share a common complementary target view x \u2208X, by enforcing xm, xm' to\nreconstruct the same output x\u00ec, MIM implicitly pulls their"}, {"title": null, "content": "LMIM (x) \u2265 -EM(x) M(x)-8+ const"}, {"title": null, "content": "LMIM \u2265 Var(X)"}, {"title": "B. Multi-modality MRI Masked Autoencoder", "content": "Here, multi-modality masked autoencoder (M\u00b2-MAE) is proposed to address the model collapse issue. As shown in Fig. 4, M2-MAE uses a masked image reconstruction model as the backbone of the self-supervised learning module for multi- modality MRI. M\u00b2-MAE is composed of hybrid mask pattern (HMP) as the masking strategy and a pyramid barlow twin (PBT) module. M\u00b2-MAE masks the multi-modality MR images and trains the encoder by reconstructing the masked patches, then the pre-trained encoder is applied to downstream segmentation and classification tasks.\nIn M2-MAE, input images X are masked by HMP to get Xm, firstly. X and Xm are then separately entered into two vision Transformer-based encoders Evir. Note that the encoders EviT are responsible for modeling latent feature representations of the masked patches, which are used to reconstruct the original image patches in the masked area. As a standard ViT, the encoders of M\u00b2-MAE embed patches through linear projection with the incorporation of positional embeddings and processes the resulting features via a series of Transformer blocks. The shared weights of encoders Evir operate on both the masked and unmasked patches of the entire dataset, which produce embedding Z and Zm for input and masked images, respectively. Thirdly, the embedding of each transformer block is fed to a PBT module to align features of the masked images and the input images at various scales.\nLastly, Zm from Evir are reconstructed by a decoder and a discriminator via adversarial learning. The input to the decoder is the full set of tokens consisting of encoded visible patches. A lightweight decoder reduces computational complexity and increases the ability of encoder to learn more generalizable representations that the decoder can quickly grasp, translate, and convey [26]. The encoder is more critical because the decoder is only used during pre-training to perform the image reconstruction task (only the encoder is used to produce image representations for recognition). Therefore, the entire decoder is replaced with a single output projection g(\u00b7) in M2-MAE."}, {"title": "1) Hybrid Mask Pattern", "content": "To alleviate model collapse in MIM, a novel masking strategy (HMP) is designed for multi-modality MRI to improve Var(X). The HMP includes three types of masks: 1) modality mask that randomly masks one modality. By modality mask, the encoder is encouraged to learn the information of the missing modality during reconstruction; 2) position mask, where positions are randomly selected, and the same positions are used to mask all the modalities; 3) patch mask complementarily selects patches of one position on different modalities. It forces the model to learn the semantic connection of each modality by reconstructing the masked patches.\nThe content information between different modalities of multi-modality MRI is highly coupled [27]. Image patches from different modalities at the same positions can be regarded as independent from each other. Therefore, the image patches can be masked or reserved independently. In this way, the content of the original image is maintained maximally while increasing the variance of the masked image, i.e., Var(X\u2081). By increasing"}, {"title": "2) Pyramid Barlow Twin Module", "content": "The representations encoded by traditional MIM methods are not suitable for voxel-level dense prediction such as segmentation task because these methods do not consider different receptive field. This leads to misclassification of neighboring voxels of the same semantics into different categories. In recognition of this issue, a PBT module is designed and combined with HMP to further ensure stability of training and make pre-training weights suitable for segmentation and classification at different vision scale. To reconstruct the masked image patches by HMP at different vision scale and perform fine representation on downstream tasks, the PBT module aligns semantic representations between masked and original views in the pre-training stage. Let z\u00b9 and zm be feature representations of the input and masked images, which are embedded by two encoders on layer l, respectively. Specifically, the embedding is resized to the same level, then"}, {"title": null, "content": "C'ij = \u03a3iz\u00b9i zmj /\u221a\u03a3(z\u00b9i) \u03a3(zm;)\u00b2"}, {"title": null, "content": "LPBT = \u2211(1-C) +\u03a3\u03a3(C)\u00b2"}, {"title": null, "content": "total = CPBT"}, {"title": "3) Optimization", "content": "In summary, the M\u00b2-MAE is based on a MIM consisting of a PBT module and a discriminator to pre-train the encoder for multi-modality MRI datasets by reconstructing masked images. Given the multi-modality MR image x, the ViT encoder Evit and the output projection g(\u00b7), the MSE loss can be written as:"}, {"title": null, "content": "LMSE = |x-g(Evit (x))|"}, {"title": "IV. EXPERIMENTS", "content": "The multi-modality MR images of glioma patients are derived from the multi-modality brain tumor segmentation (BraTS2023) challenge [1]. Part of the BraTS2023 data belongs to The Cancer Imaging Archive, which can be available from the public repository. Genomic information was available from The Cancer Genome Atlas and IDH mutation status, FLAIR, T1, Tlce, and T2 modalities were assessed. For the glioma segmentation task, the final training dataset included 5004 preoperative MR images from 1251 subjects; the test dataset included 219 unlabeled subjects. Glioma segmentation was submitted to BraTS2023 to determine the segmentation accuracy for various algorithms. In the training dataset, only 369 subjects have IDH mutation, including 76 subjects with low-grade glioma (LGG) and 293 subjects with high-grade glioma (HGG). Thus, 299 subjects with IDH mutation status were used as training data, and 70 subjects with IDH mutation status are used as test data for IDH classification tasks."}, {"title": "2) PI-CAI", "content": "The full dataset used for the PI-CAI [42] public dataset comprises a cohort of 1500 prostate MRI exams, curated from three Dutch and one Norwegian center. Imaging consisted of the following sequences: axial T2-weighted imaging (T2W), diffusion-weighted imaging (DWI), and apparent diffusion coefficient maps (ADC). Out of the 1500 cases shared in the dataset, 1075 cases have International Society of Urological Pathology (ISUP) \u22641, and 425 cases have ISUP >2. The official website also provides the segmentation labels on the transition zone (TZ) and peripheral zone (PZ). In the experiments, 1200 were used as training data and 300 subjects as test data by random data splitting."}, {"title": "3) Lung gas MRI", "content": "The LungasMRI acquisition is performed using the method previously reported by our group [31]. A total of 85 subjects were enrolled including 43 healthy volunteers and 42 patients with lung disease such as chronic obstructive pulmonary disease (COPD), asthma and so on. All subjects provided informed consents and the experiments were approved by the local Institutional Review Board (IRB). The parameters for H imaging are as follows: repetition time/echo time (TR/TE) = 2.4/0.7 ms, matrix size = 96 \u00d7 96, number of slices = 24. The parameters for 129Xe imaging are as follows: TR/TE = 4.2/1.9 ms, matrix size = 96\u00d796, number of slices = 24. Professional doctors segmented the 'H and 129Xe images to obtain thoracic cavity mask and ventilation mask using H and 129Xe MR images. In the experiments, 68 subjects were used as training data and 17 subjects as test data by randomly data splitting."}, {"title": "B. Evaluation Metrics", "content": "To quantitatively evaluate the performance of the models on segmentation tasks, the Dice similarity score (Dice) and the 95% Hausdorff distance (HD95) are employed. For the classification tasks, area under the curve (AUC) and accuracy (Acc) are used for the quantitative evaluation."}, {"title": "C. Comparison With the SOTA Methods", "content": "Extensive experiments were conducted to compare the M\u00b2-MAE with state-of-the-art (SOTA) methods [3]-[8],[12],[18]- [20],[29],[43]-[45] on the three multi-modality datasets, as shown in Table I and Table II. To enable fair comparison, all the pre-training models adopted a Vision Transformer encoder with comparable parameters and were trained with the 3D images as input for pre-training, and the trained models were transferred to the downstream segmentation and classification tasks."}, {"title": "1) Reconstruction Task on Pre-training Stage", "content": "MAE[20], SimMIM[19] and M\u00b2-MAE were used to reconstruct masked multi-modality MRI in pre-training stage. Fig. 6 shows that MAE and SimMIM yielded reconstructed images with \"averaging\u201d effect of voxel signal intensities, which is simiar to Fig. 1. However, our approach is capable of recovering high level memantic information such as the sharp contour of the brain and glioma. This suggests that M\u00b2-MAE mitigates model collapse in the pre-training stage and thus facilitates thedownstream segmentation tasks."}, {"title": "2) Segmentation Task", "content": "The segmentation results are shown in Table I. The segmentation performance of the methods without pre-training is in the top rows. The segmentation performance of pre- training model is in the bottom rows. Experimental results show that the proposed model outperforms the comparative methods in the segmentation tasks. Particularly, the proposed M2-MAE achieves a mean Dice of 89.71%, 85.02%, 81.29% for the whole tumor (WT), tumor core (TC), enhancing tumor (ET) on"}, {"title": "4) Semi-supervised Segmentation Task", "content": "Based on dataset splitting, 20%, 40%, 60%, 80% and 100% of the labeled data are taken from the training data. Semi-supervised segmentation task is designed by using unlabeled data for M2-MAE pre-training and using labeled data for supervised segmentation training (shown in Fig. 11). The dashed line represents the result of experiments using all labeled data without M\u00b2-MAE pre-training weights. This exemplifies the potential impact of our approach in resource- constrained environments and its potential utility in field of multi-modality MRI analysis."}, {"title": "D. Ablation Study", "content": "To demonstrate the effectiveness of the masking strategy, relevant ablation experiments are conducted. As shown in Table III, using the position mask and patch mask to reconstruct images during pre-training can effectively enhance the segmentation accuracy of TC and ET in BraTS2023, PZ in PI- CAI, and VD in LungasMRI. It has been indicated that, modality mask can improve the segmentation and classification accuracy of targets with different sizes, while patch mask can substantially improve the performance of small lesions."}, {"title": "2) Evaluation of Pyramid Barlow Twin Module in Different Level", "content": "The proposed PBT module facilitates the model to learn the relationship between modalities and forces the encoders to learn reconstruction features at different vision scale. To analyze the effect of PBT at different levels, the scheme is adopted at each level in the pre-trained encoders. As shown in Table IV, the performance of the downstream tasks improves with levels and reaches the maximum at level=4. Considering the accuracy and quantity of training parameters, level = 4 is chosen as the optimal parameter of the number of transformer layers."}, {"title": "3) Evaluation of Loss Function", "content": "To investigate the effects of proposed M2-MAE model, the quantitative results are presented in Table V. In comparison to the baseline in 1st row, the model with LDis and LPBT gains an improvement in dice score of 1.34% for glioma segmentation."}, {"title": "V. DISCUSSION", "content": "In this study, we deeply explore and analyze that the small lower bound of training loss induced by Var(X) is the cause of model collapse in self-supervised training for multi-modality MRI. Based on this, we propose M\u00b2-MAE for multi-modality MRI self-supervised learning. We design a masking strategy HMP as the masking strategy to increase the lower bound of the training loss, providing a way to avoid model collapse and enhance the pre-training effect. In addition, we develop a PBT module to align the semantic representation of the original and masked image patches from HMP at different vision scales. Experiments show that M2-MAE can effectively avoid model collapse and enhance the pre-training performance and the accuracy of downstream segmentation and classification tasks.\nThe HMP directly improves the variance of masked images to reduce the risk of model collapse. Current self-supervised medical image analysis methods mostly treat multi-modality MRI as an indivisible whole and ignore the relationship between paired modalities. Although images from different modalities for the same sample present their exclusive features, they inherit some global content structures. For instance, the underlying anatomical structure of the brain is shared by all modalities in the BraTS2023 dataset. At present, the modality translation task has been proved to be beneficial for the segmentation tasks. It means reconstructing missing full modality or missing modality patches as the pretext task is significant for downstream tasks. For example, small lesion areas are easily missed by Vit-based encoders while effective improvement is achieved by mask reconstruction using a modality mask and patch mask (as shown in Table III). In addition, the HMP forces the model to explicitly capture correlations\nacross\ndifferent modalities, improving the robustness of model even with a small training dataset.\nFor HMP, we introduce a PBT module in M2-MAE to regularize the model by aligning the semantic representation at different vision scales in the latent space. In the PBT module, the performance of the downstream segmentation tasks is substantially enhanced with level=4. This may be due to the high efficiency of the parameter, which corresponds to the number of layers of jump connections in UNETR. However,"}, {"title": "VI. CONCLUSION", "content": "In this work, the M\u00b2-MAE is developed to enhance SSL by preventing model collapse for multi-modality MRI segmentation and classification. The HMP masks the multi-modality MRI on three different levels and can directly improve the variance of masked images and learn the semantic connections of multi-modality images. In the pre-training stage, the PBT module aligns the semantic representation of image patches in latent space using a pyramidal hierarchy of Barlow Twin loss. Experiments showed that our proposed approach improves the performance of the downstream tasks and achieves similar performance to a fully supervised model using 60% of label. This approach provides a reliable strategy for exploiting unlabeled data to steadily enhance the performance of downstream segmentation and classification tasks, lending support to import clinical diagnosis and treatment efficacy of small lesions."}]}