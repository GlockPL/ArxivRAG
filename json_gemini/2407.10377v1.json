{"title": "Enhanced Self-supervised Learning for Multi-modality MRI Segmentation and Classification: A Novel Approach Avoiding Model Collapse", "authors": ["Linxuan Han", "Sa Xiao", "Zimeng Li", "Haidong Li", "Xiuchao Zhao", "Fumin Guo", "Yeqing Han", "Xin Zhou"], "abstract": "Multi-modality magnetic resonance imaging (MRI) can provide complementary information for computer-aided diagnosis. Traditional deep learning algorithms are suitable for identifying specific anatomical structures segmenting lesions and classifying diseases with magnetic resonance images. However, manual labels are limited due to high expense, which hinders further improvement of model accuracy. Self-supervised learning (SSL) can effectively learn feature representations from unlabeled data by pre-training and is demonstrated to be effective in natural image analysis. Most SSL methods ignore the similarity of multi-modality MRI, leading to model collapse. This limits the efficiency of pre-training, causing low accuracy in downstream segmentation and classification tasks. To solve this challenge, we establish and validate a multi-modality MRI masked autoencoder consisting of hybrid mask pattern (HMP) and pyramid barlow twin (PBT) module for SSL on multi-modality MRI analysis. The HMP concatenates three masking steps forcing the SSL to learn the semantic connections of multi-modality images by reconstructing the masking patches. We have proved that the proposed HMP can avoid model collapse. The PBT module exploits the pyramidal hierarchy of the network to construct barlow twin loss between masked and original views, aligning the semantic representations of image patches at different vision scales in latent space. Experiments on BraTS2023, PI-CAI, and lung gas MRI datasets further demonstrate the superiority of our framework over the state-of-the-art. The performance of the segmentation and classification is substantially enhanced, supporting the accurate detection of small lesion areas. The code is available at https://github.com/LinxuanHan/M2-MAE.", "sections": [{"title": "I. INTRODUCTION", "content": "MULTI-MODALITY magnetic resonance imaging (MRI) can provide complementary and mutually informative data about tissue composition (i.e., anatomical or functional information), enabling intuitive insight into the human body's interior. This assists radiologists and clinicians in detecting and treating diseases more efficiently. For example [1], to accurately define the shapes, sizes, and diffused locations of gliomas, the whole tumor (WT) region is highly distinguishable with fluid attenuation inversion recovery (FLAIR), and the enhancing tumor (ET) region exhibits clear structures in T1- weighted contrast-enhanced imaging (Tlce). In recent years, numerous deep learning-based computer-aided diagnosis algorithms have been proposed to fuse modality and extract the features for MRI segmentation and classification. However, these data-driven algorithms, such as U-Net [2] and its variants [3]-[8], require a large dataset with accurate annotations. Medical image annotations are still manually performed by experienced neuro-radiologists, which is extremely tedious and time-consuming. Moreover, the annotation of multi-modality MRI is more difficult to obtain than single-modality MRI.\nSelf-supervised learning (SSL), semi-supervised learning, and weakly supervised learning are viable deep learning"}, {"title": "II. RELATED WORK", "content": "So far, many deep learning based multi-modality MRI analysis methods have been proposed. For instance, Schelb et al. [25] trained a U-Net with T2-weighted and diffusion MRI to segmented prostate cancer. Based on U-Net, several networks, such as TransUNet [3],[26] and UNETR [4] employing vision"}, {"title": "B. Self-supervised Learning", "content": "Supervised learning methods mentioned above rely on fully annotated medical datasets. As a viable alternative, self-supervised learning obtains supervisory signals from given data and learns generalizable dense representations of the input. There are two main directions in self-supervised learning.\nThe first direction is contrastive learning [9],[32]-[34], which defines positive and negative sample pairs as learning tasks and treats them differently in the loss function. Marin et al. [32] pre-trained the ResNet50 backbone for object detection in chest X- ray. Zheng et al. [9] proposed a multiscale visual representation learning algorithm to perform finer-grained representation and to handle different target scales for downstream segmentation tasks. Together with the contrast loss, Zhou et al. [33] presented preservation contrastive learning for self-supervised medical representations. Moreover, Jiang et al. [34] designed a conditional anatomical feature alignment module to complement corrupted embeddings with globally matched semantics to create contrastive pairs in 3D medical analysis.\nThe other direction is MIM [21],[24],[35],[36], such as masked autoencoders (MAE), which represent the state-of-the-art (SOTA) on natural datasets. Yan et al. [35] proposed a privacy-preserving and federated self-supervised learning framework that collaboratively trained models on decentralized data using masked image modeling as the self-supervised task. Chen et al. [21] employed masked image modeling approaches to advance 3D medical image analysis using a lightweight decoder for reconstruction. Haghighi et al. [36] developed a framework that united discriminative, restorative, and adversarial learning in a unified manner. Yan et al. [24] proposed a hybrid visual representation learning framework for self-supervised pre-training on large unlabeled medical datasets using contrastive and generative modelling."}, {"title": "C. Model Collapse", "content": "Self-supervised learning that maps similar data to close spots in the representation space could lead to the collapsing problem. This means that all outputs tend to collapse to a constant feature. Some studies have introduced new self-supervised frameworks to avoid model collapse. For example, LeCun et al. [20] measured the cross-correlation matrix between the outputs of two identical networks called Barlow Twins. Zhang et al. [37] presented a new approach termed align representations with base (ARB), which aligns the learned embeddings to intermediate variables. Compared to Barlow Twins, ARB does not require pairwise decorrelation, resulting in linear complexity. Also, some researchers attempted to avoid model collapse by redesigning proxy tasks and loss function. Wang et al. [38] employed a network ended with a SoftMax operation to produce twin-class distributions of two augmentation images. Moon et al. [39] used the same random vector for the augmented embeddings of the given image. This implies that the embeddings are locally dispersed, giving a latent contrast effect between the embeddings of different images. Zhang et al. [23] established a close connection between MAE and contrastive learning and showed that MAE implicitly aligns the mask-induced positive pairs. They proposed a uniformity-enhanced MAE loss that can address model collapse."}, {"title": "III. METHODS", "content": "In this work, we investigate the reason why traditional masked strategy of SSL leads to model collapse in multi- modality MRI. For clarification, we define the following notations: $D_{1}=\\{(x^{(i)},g^{(i)},y^{i})\\}$ is a labeled dataset with NL samples, where $x^{(i)}$ is the image of the i-th sample in input multi-modality MRI, $y^{i} \\in \\{0,1\\}$denotes the class of the i-th sample, and $g^{i}$ represents the corresponding ground truth segmentation.\nIt is known that, in contrastive learning, simply aligning positive pairs will result in full model collapse because the alignment loss can be minimized by the encoder [18]. MIM methods do not suffer from model collapse on natural image datasets during pre-training [23]. However, when directly applying MIM to multi-modality MRI datasets, model collapse occurs in pre-training due to the similarity of multi-modality MRI. Here, we theoretically investigate the issue of model collapse in MIM for MR images.\nIn MIM, an MR image x from $D_{1}$ is divided into n patches $R \\times s$, e.g., $x \\in R^{n \\times s}$, where s denotes the patch size. Next, a random binary mask $m \\in \\{0, 1\\}^n$ is drawn with probability as the mask ratio p (the ratio of removed patches), two complementary masked views of x are obtained:\n$x_{m}=x[m] \\in R^{n_{1} \\times s}, x_{t}=x[1-m] \\in R^{n_{2} \\times s}$                                                                 (1)\nwhere $n_{1}=n \\times (1 - p)$, $n_{2}=n \\times p$ are integers satisfying n = $n_{1} + n_{2}$. An MAE essentially learns to pair the two complementary views xm and xt via the reconstruction task, which can be modeled by a bipartite graph. Let $X_{m}=\\{x_{m}\\}$ and $X_{t}=\\{x_{t}\\}$ denote the set of all masked and target views, respectively. Each view x \u2208 X is considered as a node. Accordingly, there is an edge between two views when they are complementary views generated by masking.\nMIM produces latent connections among different input samples in the form of 2-hop connectivity [22]. Given a pair of 2-hop input neighbors $x_{m}$, $x_{m'} \\in X_{m}$ that share a common complementary target view $x_{t} \\in X$, by enforcing $x_{m}$, $x_{m'}$ to reconstruct the same output $x_{t}$, MIM implicitly pulls their"}, {"title": "B. Multi-modality MRI Masked Autoencoder", "content": "Here, multi-modality masked autoencoder (M\u00b2-MAE) is proposed to address the model collapse issue. As shown in Fig. 4, M2-MAE uses a masked image reconstruction model as the backbone of the self-supervised learning module for multi-modality MRI. M\u00b2-MAE is composed of hybrid mask pattern (HMP) as the masking strategy and a pyramid barlow twin (PBT) module. M\u00b2-MAE masks the multi-modality MR images and trains the encoder by reconstructing the masked patches, then the pre-trained encoder is applied to downstream segmentation and classification tasks.\nIn M2-MAE, input images X are masked by HMP to get Xm, firstly. X and Xm are then separately entered into two vision Transformer-based encoders Evir. Note that the encoders Evir are responsible for modeling latent feature representations of the masked patches, which are used to reconstruct the original image patches in the masked area. As a standard ViT, the encoders of M\u00b2-MAE embed patches through linear projection with the incorporation of positional embeddings and processes the resulting features via a series of Transformer blocks. The shared weights of encoders Evir operate on both the masked and unmasked patches of the entire dataset, which produce embedding Z and Zm for input and masked images, respectively. Thirdly, the embedding of each transformer block is fed to a PBT module to align features of the masked images and the input images at various scales.\nLastly, Zm from Evir are reconstructed by a decoder and a discriminator via adversarial learning. The input to the decoder is the full set of tokens consisting of encoded visible patches. A lightweight decoder reduces computational complexity and increases the ability of encoder to learn more generalizable representations that the decoder can quickly grasp, translate, and convey [26]. The encoder is more critical because the decoder is only used during pre-training to perform the image reconstruction task (only the encoder is used to produce image representations for recognition). Therefore, the entire decoder is replaced with a single output projection g(\u00b7) in M2-MAE."}, {"title": "IV. EXPERIMENTS", "content": "The multi-modality MR images of glioma patients are derived from the multi-modality brain tumor segmentation (BraTS2023) challenge [1]. Part of the BraTS2023 data belongs to The Cancer Imaging Archive, which can be available from the public repository. Genomic information was available from The Cancer Genome Atlas and IDH mutation status, FLAIR, T1, Tlce, and T2 modalities were assessed. For the glioma segmentation task, the final training dataset included 5004 preoperative MR images from 1251 subjects; the test dataset included 219 unlabeled subjects. Glioma segmentation was submitted to BraTS2023 to determine the segmentation accuracy for various algorithms. In the training dataset, only 369 subjects have IDH mutation, including 76 subjects with low-grade glioma (LGG) and 293 subjects with high-grade glioma (HGG). Thus, 299 subjects with IDH mutation status were used as training data, and 70 subjects with IDH mutation status are used as test data for IDH classification tasks."}, {"title": "D. Ablation Study", "content": "To demonstrate the effectiveness of the masking strategy, relevant ablation experiments are conducted. As shown in Table III, using the position mask and patch mask to reconstruct images during pre-training can effectively enhance the segmentation accuracy of TC and ET in BraTS2023, PZ in PI- CAI, and VD in LungasMRI. It has been indicated that, modality mask can improve the segmentation and classification accuracy of targets with different sizes, while patch mask can substantially improve the performance of small lesions.\nThe proposed PBT module facilitates the model to learn the relationship between modalities and forces the encoders to learn reconstruction features at different vision scale. To analyze the effect of PBT at different levels, the scheme is adopted at each level in the pre-trained encoders. As shown in Table IV, the performance of the downstream tasks improves with levels and reaches the maximum at level=4. Considering the accuracy and quantity of training parameters, level = 4 is chosen as the optimal parameter of the number of transformer layers.\nTo investigate the effects of proposed M\u00b2-MAE model, the quantitative results are presented in Table V. In comparison to the baseline in 1st row, the model with $L_{Dis}$ and $L_{PBT}$ gains an improvement in dice score of 1.34% for glioma segmentation."}, {"title": "V. DISCUSSION", "content": "In this study, we deeply explore and analyze that the small lower bound of training loss induced by Var(X) is the cause of model collapse in self-supervised training for multi-modality MRI. Based on this, we propose M\u00b2-MAE for multi-modality MRI self-supervised learning. We design a masking strategy HMP as the masking strategy to increase the lower bound of the training loss, providing a way to avoid model collapse and enhance the pre-training effect. In addition, we develop a PBT module to align the semantic representation of the original and masked image patches from HMP at different vision scales. Experiments show that M2-MAE can effectively avoid model collapse and enhance the pre-training performance and the accuracy of downstream segmentation and classification tasks.\nThe HMP directly improves the variance of masked images to reduce the risk of model collapse. Current self-supervised medical image analysis methods mostly treat multi-modality MRI as an indivisible whole and ignore the relationship between paired modalities. Although images from different modalities for the same sample present their exclusive features, they inherit some global content structures. For instance, the underlying anatomical structure of the brain is shared by all modalities in the BraTS2023 dataset. At present, the modality translation task has been proved to be beneficial for the segmentation tasks. It means reconstructing missing full modality or missing modality patches as the pretext task is significant for downstream tasks. For example, small lesion areas are easily missed by Vit-based encoders while effective improvement is achieved by mask reconstruction using a modality mask and patch mask (as shown in Table III). In addition, the HMP forces the model to explicitly capture correlations across different modalities, improving the robustness of model even with a small training dataset.\nFor HMP, we introduce a PBT module in M2-MAE to regularize the model by aligning the semantic representation at different vision scales in the latent space. In the PBT module, the performance of the downstream segmentation tasks is substantially enhanced with level=4. This may be due to the high efficiency of the parameter, which corresponds to the number of layers of jump connections in UNETR. However, when the PBT was added to all the levels in the encoder, there was a noticeable decrease in the performance in the downstream classification/segmentation tasks, which may be attributed to the large number of training parameters that causes difficulties for the gradient to go backward.\nWe further evaluated the performance of M\u00b2-MAE by reducing the amount for labeled data. Fig. 11 demonstrates the comparison results of fine-tuning using a subset of three datasets. Using only 60% labeled dataset for supervised training, experiments with M2-MAE pre-training weights achieved similar performance compared to training from scratch on small regions like TC, ET, PZ, and VD. Our findings suggest promising future implications and potential extensions of this work for other medical imaging tasks. Compared with traditional semi-supervised learning, consistency-based semi- supervised is built on a consistent data distribution, which may not suit datasets from multiple data centers. Pseudo label-based semi-supervised learning methods may produce error amplification and increase the risk of training instability. In contrast, the self-supervised learning is used to learn universal features through a pretext task, which can reduce errors from data bias.\nWe acknowledge several study limitations. For example, regarding the generality of the method to different tasks, the Modality Mask strategy in the HMP randomly masks a whole modality for the input multi-modality MRI. However, each modality has different importance for the downstream tasks, e.g. the Tlce is more important compared to T1 for the glioma segmentation task. The adoption of the proposed mask strategy for use in specific lesions, organs, and downstream tasks warrants further refinement."}, {"title": "VI. CONCLUSION", "content": "In this work, the M\u00b2-MAE is developed to enhance SSL by preventing model collapse for multi-modality MRI segmentation and classification. The HMP masks the multi-modality MRI on three different levels and can directly improve the variance of masked images and learn the semantic connections of multi-modality images. In the pre-training stage, the PBT module aligns the semantic representation of image patches in latent space using a pyramidal hierarchy of Barlow Twin loss. Experiments showed that our proposed approach improves the performance of the downstream tasks and achieves similar performance to a fully supervised model using 60% of label. This approach provides a reliable strategy for exploiting unlabeled data to steadily enhance the performance of downstream segmentation and classification tasks, lending support to import clinical diagnosis and treatment efficacy of small lesions."}]}