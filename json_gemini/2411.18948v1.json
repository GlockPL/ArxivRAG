{"title": "Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations", "authors": ["Xue Tan", "Hao Luan", "Mingyu Luo", "Xiaoyan Sun", "Ping Chen", "Jun Dai"], "abstract": "As Large Language Models (LLMs) are progressively deployed across diverse fields and real-world applications, ensuring the security and robustness of LLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a cutting-edge approach designed to address the limitations of large language models (LLMs). By retrieving information from the relevant knowledge database, RAG enriches the input to LLMs, enabling them to produce responses that are more accurate and contextually appropriate. It is worth noting that the knowledge database, being sourced from publicly available channels such as Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves injecting malicious texts into the knowledge database, ultimately leading to the generation of the attacker's target response (also called poisoned response). However, there are currently limited methods available for detecting such poisoning attacks. We aim to bridge the gap in this work. Particularly, we introduce RevPRAG, a flexible and automated detection pipeline that leverages the activations of LLMs for poisoned response detection. Our investigation uncovers distinct patterns in LLMs' activations when generating correct responses versus poisoned responses. Our results on multiple benchmark datasets and RAG architectures show our approach could achieve 98% true positive rate, while maintaining false positive rates close to 1%. We also evaluate recent backdoor detection methods specifically designed for LLMs and applicable for identifying poisoned responses in RAG. The results demonstrate that our approach significantly surpasses them.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have remarkably enhanced performance across a diverse set of Natural Language Processing (NLP) tasks, especially in Question-Answering (QA) tasks [1], due to their stunning capabilities to understand and generate texts. Despite these advances, LLMs also face a range of issues arising from knowledge limitations. On one hand, LLMs are trained on historical data and lack up-to-date knowledge (e.g., GPT-4's data cutoff is December 2023 [2], which limits its capability to effectively process information beyond that point). On the other hand, they also exhibit knowledge gaps in specialized domains, such as medicine, finance and privacy-related contexts. Due to the absence of relevant knowledge, LLMs may produce inaccurate content and exhibit \u201challucination\u201d behaviors, where they generate information that is incorrect, misleading, or fabricated, yet appears plausible and authoritative. These limitations pose challenges to LLM applications in fields such as healthcare [3], economics [4], and scientific research [5], [6].\nRetrieval-Augmented Generation (RAG) [7] has emerged as an effective solution that leverages retrievers to incorporate external databases, enriching the knowledge of LLMs and ultimately enabling the generation of up-to-date and accurate responses. RAG comprises three components: knowledge database, retriever, and LLM. The knowledge database consists of a large amount of texts collected from sources such as latest Wikipedia entries [8], new articles [9] and financial documents [4]. The retriever is primarily responsible for using a text encoder (e.g., BERT [10]) to compute the embedding vectors of the user's question (e.g., \u201cWhat is the name of the highest mountain?\") and the texts in the knowledge database and selects the top-k retrieved texts from the knowledge database based on similarity. The LLM generates the final response (e.g., \u201cEverest\u201d) based on the user's question and the retrieved texts. Due to RAG's powerful knowledge integration capabilities, it has demonstrated impressive performance across a range of QA-like knowledge-intensive tasks [11], [12].\nRAG poisoning refers to the act of injecting malicious or misleading content into the knowledge database, contaminating the retrieved texts in RAG and ultimately leading it to produce the attacker's desired response (e.g., the target answer could be \"Fuji\" when the target question is \"What is the name of the highest mountain?\"). This attack leverages the dependency between LLMs and the knowledge database, transforming the database into a new attack surface to facilitate poisoning. PoisonedRAG [13] demonstrates the feasibility of RAG poisoning by injecting a small amount of maliciously crafted texts into the knowledge database utilized by RAG. This manipulation effectively compels the model to generate the attacker's intended response, referred to as a \"poisoned response\". Furthermore, injecting a substantial amount of adversarial texts into the knowledge database can mislead the retriever into prioritizing such content, causing it to generalize poisoned responses to queries from unseen domains [14].\""}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "The LLM community has been challenged by the generation of inaccurate and outdated responses due to their incomplete and static knowledge, and RAG [7], [20] was introduced as a promising ameliorative paradigm to provide more accurate, relevant, and up-to-date external information. RAG is most striking when it leverages the fact that the knowledge database can be seamlessly refreshed with newly added content. This feature allows the system to efficiently keep up with dynamic knowledge domains, eliminating the need for costly and time-consuming LLM fine-tuning. Because of its great flexibility, RAG arises in innumerable industrial contexts (e.g., ChatGPT Retrieval Plugin [21], WikiChat [22], FinGPT [23], and Cha- tRTX [24]) as well as in specialized domains (e.g., medical diagnostic assistants [25] and email/code completion [26]).\nRAG comprises three components: knowledge database, retriever, and LLM. The knowledge database contains data that is either newly added or updated the existing information in the LLM's training set, often collected from sources such as Wikipedia [8] and other platforms. We define the knowledge database as $C = {P_1,P_2,\u2026\u2026,P_m}$ where $p_i$ represents the i-th piece of collected information. The retriever identifies the k texts most similar to the query q from the knowledge database based on similarity comparison, providing them as external information. The LLM then generates an answer by simultaneously processing the query q and the top-k texts.\nThe workflow of RAG, having introduced its components, deserves further explanation. As illustrated in Fig. 1, it consists of two main steps: retrieval step and generation step.\nIn the retrieval step, the retriever generates the top k most relevant pieces of knowledge for the query q. First, we employ two encoders, $E_q$ and $E_p$, which can either be identical or radically different. Encoder $E_q$ is responsible for transforming the user's query q into an embedding vector $E_q(q)$, while encoder $E_p$ is designed to convert all the information $p_i$ in the knowledge database into embedding vectors $E_p(p_i)$. For each $E_p(p_i)$, the similarity with the query $E_q(q)$ is computed using $sim(E_q(q), E_p(p_i))$, where $sim(\u00b7,\u00b7)$ quantifies the similarity between two embedding vectors, such as cosine similarity or the dot product. Finally, the top k most relevant pieces are selected as the external knowledge $C_q$ for the query q.\nThe generation step is dedicated to generating a response $LLM(q, C_q)$ based on the query q and the relevant information $C_q$. First, we combine the query q and the external knowledge $C_q$ using a standard prompt (see Fig. 5 for the complete prompt). Taking advantage of such a prompt, the LLM gen- erates an answer $LLM(q, C_q) $to the query q and can quickly reference the information in the knowledge database. Therefore, RAG is a significant accomplishment, as it addresses the limitations of LLMs in acquiring up-to-date and domain- specific information."}, {"title": "B. Inner States Analysis of LLM", "content": "The interpretability of LLMs has been explored in a collection of works inspired by their inner states and latent spaces. GemmaScope [27] leverages sparse autoencoders to analyze the latent spaces of LLMs, revealing meaningful features within the model activations. These latent states are also capable of encoding safety concerns [28] and task drift [29]. BEEAR [30] detects backdoors by identifying uniform embedding drifts triggered by specific malicious training-time triggers. These methods facilitate the construction of probes to predict the behavior of LLMs, such as hallucinations [31], [32] and task drift [29], as well as to reveal latent knowledge [33].\nThe logit lens leverages the model's unembedding matrix to decode representations from intermediate layers, providing valuable insights into its decision-making process and facilitating the mitigation of harmful behaviors. The decoding outputs of intermediate layers are analyzed to identify human- understandable concepts encoded in the parameter vectors of the transformer's feed-forward layers, enabling the manipulation of model outputs to mitigate harmful content [34]. Late- layer MLPs frequently reduce the probability of the maximum- likelihood token, and this reduction can be self-corrected by removing certain attention layers [35]. Intermediate layers are decoded to reveal how harmful outputs are mimicked by language models, identifying decision-making shifts and suggesting the removal of key layers to enhance output accu- racy against misleading inputs [36]. Furthermore, LLM Facto- scope [19] employs the logit lens to decode intermediate layer outputs, capturing changes in output indices and probabilities within the LLM."}, {"title": "C. Retrieval Corruption Attack", "content": "Due to the growing attention on RAG, attacks on RAG have also been widely studied. RAG can improperly generate answers that are severely impacted or compromised once the knowledge database is contaminated [13], [37], [38]. Furthermore, the retriever can also be vulnerable to attacks, which may result in the selection of irrelevant or misleading external information, without even modifying the knowledge database [39], [40]. Specifically, an attacker can inject a small amount of malicious information onto a website, which is then retrieved by RAG [41]. PoisonedRAG [13] injects malicious text into the knowledge database and formalizes the knowledge poisoning attack as an optimization problem, thereby enabling the LLM to generate target responses selected by the attacker. GARAG [42] was introduced to provide low-level perturbations to RAG and to search for adversarial text targeting both components of RAG simultaneously. PRCAP [14] injects adversarial samples into the knowledge database, where these samples are generated by perturbing discrete tokens to enhance their similarity with a set of training queries. These methods have yielded striking attack results, and in our work, we have selected several state-of-the-art attack methods as our base attacks on RAG."}, {"title": "D. The Robustness of RAG", "content": "Efforts have been made to develop defenses in response to poisoning attacks and noise-induced disruptions, and significant accomplishments have been achieved. As a result, the robustness of RAG, defined by its capacity to resist poisoning attacks and integrate accurate external information, plays a prominent role in practical applications. Adversarial texts are applied to poison the knowledge database [13], [14], [39], necessitating rigorous scrutiny before feeding the retrieved in- formation into the LLM to prevent such adversarial attacks and out-of-distribution data [43]\u2013[46]. RobustRAG [17] mitigates the impact of poisoned texts through a voting mechanism, while INSTRUCTRAG [16] explicitly learns the denoising process to address poisoned and irrelevant information. Other approaches to enhance robustness include prompt design [47], [48], plug-in models [49], and specialized models [15], [50]. While these methods alleviate the issues of poisoning and noise, they share similar limitations\u2014specifically, that adaptive attackers can often circumvent the defenses once the underlying algorithms are made publicly available [51]-[55]. In this work, we present a detection perspective on defending against RAG attacks, which allows for the open sharing of detection algorithms and models without the risk of being easily circumvented by adaptive attacks."}, {"title": "III. PRELIMINARY", "content": "In this section, we first define our threat model, and present our design objectives. We then detail the rationale behind detecting poisoning attacks through activation analysis."}, {"title": "A. Threat Model", "content": "Assume that the attacker preselects a target question set Q, consisting of $q_1, q_2, , q_n$, and the cor- responding target answer set A, represented as $a_1, a_2,\uff65\uff65\uff65, a_n$. The attacker's goal is to compromise the RAG system by contaminating the retrieval texts, thereby manipulating the LLM to generate the target response $a_i$ for each query $q_i$. Taking the upper half of Fig. 2 as an example, the attacker's target question $q_i$ is \u201cWhat is the name of the highest moun- tain?\", with the target answer being \u201cFuji\u201d. To achieve this, the attacker injects malicious texts into the knowledge database C, effectively contaminating its content as a source of information for RAG. Consequently, the LLM generates the incorrect target answer \"Fuji\" based on the retrieved poisoned texts.\nWe assume that an attacker can in- ject m poisoned texts P for each target question $q_i$, represented as $p^1_i, p^2_i, ..., p^m_i$. The attacker does not possess knowledge of the LLM utilized by the RAG, but has white-box access to the RAG retriever. This assumption is reasonable, as many retrievers are openly accessible on platforms like Hugging- Face, such as Contriever [56], Contriever-ms (fine-tuned on MS-MARCO) [56], and ANCE [57]. The poisoned texts can be integrated into the RAG's knowledge database through two ways: the attacker publishing the maliciously crafted content on open platforms like Wikipedia, or utilizing data collection agencies to disseminate the poisoned texts.\""}, {"title": "B. Design Objectives", "content": "We aim to develop an accurate model for de- tecting poisoned responses. Accuracy, in this context, requires identifying all poisoned responses while keeping false rates to a minimum. A high false positive rate would incorrectly flag many valid responses as poisoned, disrupting the model's functionality and reducing its effectiveness. Conversely, a high false negative rate would allow poisoned responses to go undetected. This work focuses on minimizing both false positives and false negatives to ensure reliable detection.\nAnother key objective is to preserve the integrity of the RAG system by avoiding modifications to its core architecture. Our aim is to design a detection model that is both highly accurate and easy to integrate. To ensure practical- ity, we steer clear of resource-intensive changes, such as fine-tuning the large model, which can be costly. Instead, we focus on achieving accurate detection of poisoned responses within the existing RAG workflow, leveraging available information without introducing additional overhead."}, {"title": "C. Rationale", "content": "The rationale behind our research stems from the conflict between the poisoned texts in the knowledge database and the knowledge originally learned by the LLMs. This conflict arises as attackers seek to manipulate the models into generating poisoned responses that contradict accurate information. This inherent conflict drives our efforts to develop methods for detecting and exploiting it. The activations of LLMs represent input data at varying layers of abstraction, enabling the model to progressively extract high-level semantic information from low-level features. The extensive information encapsulated in these activations comprehensively reflects the entire decision- making process of the LLM. Consequently, we posit that activations can, to a certain extent, capture the aforementioned conflict. In summary, activations from all layers of the LLM can effectively serve as indicators for detecting poisoned responses in RAG systems."}, {"title": "IV. METHODOLOGY", "content": "This section begins by providing an overview of our pipeline. We then describe the methodology for crafting effective poisoned texts to execute attacks. Following the successful influence of the poisoned texts on the LLM's responses, we collect and process the corresponding activations. Finally, we present the architectural design of the RevPRAG model."}, {"title": "A. Overview of the method", "content": "We introduce RevPRAG, a pipeline designed to leverage LLM activations for detecting knowledge poisoning attacks in RAG systems, as illustrated in Fig. 2. This pipeline enables seamless expansion of dataset coverage for the poisoning attack detection model.\nTo detect poisoning attacks against RAG, we begin by collecting the texts used in the attack. These poisoned texts are injected into the RAG system's knowledge database, contaminating the previously clean retrieval texts. Using the poisoned texts as a basis, we label the activation data based on whether the LLM generates the attacker's target response. Additionally, we capture the LLM's activations for all queries.\nAfter collecting and preprocessing the data, we train a Siamese network-based model. This approach minimizes the embedding distance for data within the same class while maximizing the distance between data from different classes, ensuring robust detection of poisoning attacks."}, {"title": "B. Poisoned Data Collection", "content": "Our method seeks to extract the LLM's activations that capture the model's generation of a specific poisoned response triggered by receiving poisoned texts at a given point in time. Therefore, we first need to implement poisoning attacks on RAG that can mislead the LLM into generating target poisoned responses. There are three components in RAG: knowledge database, retriever, and LLM. In order to successfully carry out a poisoning attack on RAG and compel the LLM to generate the targeted poisoned response, the initial step is to craft a sufficient amount of poisoned texts and inject them into the knowledge database. In this paper, in order to create effective poisoned texts and with our primary focus on detect- ing poisoning attacks, we employ the following three state- of-the-art strategies (e.g., PoisonedRAG [13], GARAG [42], PAPRAG [14]) for generating poisoned texts and increasing the similarity between the poisoned texts and the queries, to heighten the likelihood that the poisoned texts would be selected by the retriever. The retrieved texts, along with the question, are then used to construct a new prompt for the LLM to generate the answer. The prompt [13], as shown in Fig. 5, mization, alleviates gradient-related issues, and enhances the is employed to achieve this objective. performance of algorithms that rely on distance metrics."}, {"title": "D. RevPRAG Model Design", "content": "After collecting and preprocessing the dataset of activations, we design a probing mechanism based on the dataset. Inspired by few-shot learning and Siamese networks, our proposed RevPRAG model is adept at distinguishing correct answers from poisoned ones and demonstrates strong generalizability. LLMs are known to generate non-factual responses due to hallucinations as well, but these differ from poisoned re- sponses resulting from poisoning attacks. To further differ- entiate whether an incorrect response from RAG is poisoned or non-factual, we also construct corresponding datasets for training and testing. Experimental results show that our pro- posed method can also clearly distinguish between poisoned and non-factual responses. This capability will further assist users in verifying responses from LLMs, enhancing model interpretability.\nIn order to efficiently capture the relationships between and within different layers of the LLM, we utilize Convolutional Neural Networks (CNNs) with the ResNet18 architecture [60]. We use triplet networks which share the same architecture and weights to learn embeddings of the tasks as shown in Fig. 2. During training, we employ the triplet margin loss [61], a commonly used approach for tasks where it is difficult to distinguish similar instances. Triplet margin loss is a loss function used in training neural networks for tasks such as face recognition or object classification. Its goal is to learn a simi- larity metric within a high-dimensional embedding space (also known as feature space), where representations of similar ob- jects (e.g., images of the same person) are close to each other, while representations of dissimilar objects are farther apart. This powerful similarity metric provided by triplet margin loss is particularly suitable for distinguishing LLM activations, enabling RevPRAG to more effectively differentiate activation differences caused by poisoning attacks. The core concept of triplet margin loss involves the use of triplets, each consisting of an anchor sample, a positive sample, and a negative sample. The anchor and positive samples represent similar instances, while the negative sample represents a dissimilar one. The algorithm learns to embed these samples such that the distance between the anchor and positive sample is smaller than the distance between the anchor and negative sample.\nGiven training samples $x_a, x_p,$ and $x_n$, they represent an- chor, positive, and negative points, respectively. The RevPRAG embedding model will output closer embedding vectors for any $Act_a$ and $Act_p$, but farther for any $Act_a$ and $Act_n$. The loss function is formally defined as: $L = max(Dist (Act_a, Act_p) - Dist (Act_a, Act_n) + margin, 0)$, where $Dist(\u00b7,\u00b7)$ denotes a distance metric (typically the Euclidean distance), and margin is a positive constant. The presence of the margin introduces an additional parameter in triplet loss that requires tuning. If the margin is too large, the model's loss will be high, and it may struggle to converge to zero. However, a larger margin generally improves the model's ability to distinguish between very similar samples, making it easier to differentiate between $Act_p$ and $Act_n$. Conversely, if the margin is too small, the loss will quickly approach zero, making the model easier to train, but it will be less effective at distinguishing between $Act_p$ and $Act_n$. At test time, given a test sample $x_t$, we compute the distance between the embedding of $Act_t$ and $Act_s$, where $Act_s$ is the embedding of the support sample $x_s \u2208 S. S$ is named support set, the dataset we provide, which contains labeled data that has not been used in the training set. Ultimately, the label of the test data $x_t$ will match the label of the support sample $x_s$ that is closest to it."}, {"title": "V. EVALUATION", "content": "RAG comprises three key components: knowl- edge database, retriever, and LLM. The setup is shown below:\nWe leverage three representative QA datasets in our evaluation: Natural Questions (NQ) [62], HotpotQA [63], MS-MARCO [64]. To evaluate the detection of poisoning attacks to the knowledge database of RAG, we selected 3,000 instances of the triple (q, t, a) from each of the three datasets. In each triple, q is a question from the datasets, t is the texts collected from Wikipedia or web documents corresponding to q, and a is the correct answer to the question q, generated using a state-of-the-art LLM. To better simulate the RAG poisoning attack scenario implemented through the knowledge database, we will employ three different methods for generating poisoning texts in the experiments, including PoisonedRAG [13], GARAG [42], and PRCAP [14]. We will evaluate the performance of our proposed detection approach across this series of different scenarios.\nIn our experiments, we evaluate four state- of-the-art dense retrieval models: Contriever [56] (pre-trained), Contriever-ms (fine-tuned on MS-MARCO) [56], DPR-mul [65] (trained on multiple datasets), and ANCE [57] (trained on MS-MARCO). Following previous works [7], [14], we default to using dot product between the embedding vectors of a question and a document in the knowledge database to calculate their similarity score.\nOur experiments are conducted on several popular LLMs, each with distinct architectures and characteristics. We employ GPT2-XL 1.5B [66], Llama2-7B [67], Llama2-13B, Mistral-7B [68], and Llama-3-8B. We use the prompt shown in Fig. 5 to guide the LLM in generating an answer to a question. In addition to the aforementioned aspects, we adopt the following default settings: a knowledge database based on HotpotQA, the Contriever retriever, GPT-2-XL 1.5B as the LLM, and a support size of 100. Moreover, we use the dot product between the embedding vectors of a question and a text to measure their similarity. The generation of poisoning texts follows the scheme outlined in PoisonedRAG [13]. Consistent with prior work [7], we retrieve the 5 most similar texts from the knowledge database to serve as context for a given question.\nTo the best of our knowledge, there are currently no dedicated detection methods or evaluations specifically for RAG poisoning attacks. Thus, we extend two current methods [69] and [70] for the RAG scenario. CoS [69] is a black-box approach that guides the LLM to generate detailed reasoning steps for the input, subsequently scrutinizing the reasoning process to ensure consistency with the final answer. MDP [70] is a white-box method that exploits the disparity in masking sensitivity between poisoned and clean samples.\nThe effectiveness of the proposed detection method is assessed using two metrics:\nThe True Positive Rate (TPR), which measures the pro- portion of effectively poisoned responses that are successfully detected. A higher TPR signifies better detection performance for poisoned responses, with a correspondingly lower rate of missed detections (i.e., lower false negative rate).\nThe False Positive Rate (FPR), which quantifies the proportion of correct responses that are misclassified as being caused by poisoning attacks. A lower FPR indicates fewer false positives for correct answers, minimizing disruption to the normal operation of RAG. Our primary goal is to detect poisoned responses as effectively as possible while minimizing the impact on RAG's normal functionality, which is why we have selected these two metrics."}, {"title": "B. Overall Results", "content": "Table I shows the TPRs and FPRs of RevPRAG on three datasets. We have the following observations from the experimental results. First, RevPRAG achieved high TPRs consistently on different datasets and LLMs when injecting five poisoned texts into the knowledge database. For instance, RevPRAG achieved 98.5% (on NQ), 97.7% (on HotpotQA), and 99.9% (on MS-MARCO) TPRs for RAG with Mistral-7B. Our experimental results show that assessing whether the output of a RAG system is accurate or poisoned based on the activations of LLMs is both highly feasible and reliable (i.e., capable of achieving exceptional accuracy). Second, RevPRAG achieves low FPRs under dif- ferent settings, e.g., close to 1% in nearly all cases. This result indicates that our approach not only maximizes the detection of poisoned responses but also maintains a low false positive rate, significantly reducing the risk of misclassifying correct answers as poisoned. This ensures the reliable operation of RAG systems, making the approach highly practical for real- world applications."}, {"title": "C. Generalization", "content": "Given the wide range of RAG application scenarios and the diverse user requirements it faces, it is impractical to ensure that our detection model has been trained on all possible scenarios and queries in real-world applications. However, the performance of neural network models largely depends on the similarity between the distributions of the training data and the test data [71]. Consequently, our model's performance may degrade when faced with training and test data that stem from differing distributions\u2014a challenge frequently observed in real-world scenarios. To address this issue, we conduct a series of generalization experiments. Specifically, we train the detec- tion model using any two datasets and test it on a third dataset that was not used during training. For example, we use NQ and HotpotQA as training datasets and MS-MARCO as the testing dataset. Although these three datasets are all QA datasets, they exhibit significant differences. For example, NQ focuses on extracting answers to factual questions from a single long document, HotpotQA involves multi-document reasoning to derive answers, and MS-MARCO retrieves and ranks relevant answers from a large-scale collection of documents. Therefore, conducting generalization experiments based on these three datasets is reasonable.\nillustrates the TPRs and FPRs of RevPRAG for RAG with four different LLMs. Overall, the experimental results demonstrate that our detection model exhibits strong generalization performance across RAG with different LLMs and various datasets. For example, when using HotpotQA and MS-MARCO as training data, the detection model achieves TPRs of 98% (with GPT2-XL 1.5B), 98.3% (with Llama2- 7B), 98.8% (with Mistral-7B), and 98% (with Llama3-8B) on the NQ dataset. Meanwhile, all FPRs remain below 8%. Furthermore, we observe that the generalization performance is best when NQ is used as the test data (for instance, 98.3% with Llama2-7B), while MS-MARCO shows the poorest per- formance (for instance, 88.6% with Llama2-7B). We attribute this to the fact that the questions and tasks in HotpotQA and MS-MARCO are more complex compared to those in NQ. Therefore, detection models trained on more complex tasks generalize well to simpler tasks, whereas the reverse is more challenging."}, {"title": "Quantity of injected poisoned texts.", "content": "illustrates the impact of varying quantities of poisoned text on the detec- tion performance of RevPRAG. Injecting different amounts of poisoned text into the knowledge database impacts the likelihood of retrieving poisoned content. The more poisoned texts are injected, the higher the likelihood of retrieving them for RAG processing. This is because one of the optimization objectives for generating poisoned text is to maximize its similarity to the target query. In our experiments, we find that when five poisoned texts are injected into the knowledge database, there is a high likelihood of retrieving all five poisoned texts when the total number of retrieved texts is also five (following practice of prior work). From the experimental results, we observe that even with varying amounts of injected poisoned text, RevPRAG consistently achieves high TPRs and low FPRs. For example, when the total number of retrieved texts is five and the injected quantity is two, RevPRAG achieves a 99.5% TPR and a 4.7% FPR for RAG with Llama2- 7B. The reason for this phenomenon is that the similarity between the retrieved poisoned texts and the query is higher than that of clean texts. Consequently, the LLM generates responses based on the content of the poisoned texts."}, {"title": "D. Ablation Study", "content": "To ensure the effectiveness of the evaluation, we employ three different methods introduced by PoisonedRAG, GARAG, and PRCAP to generate the poisoned texts. The experimental results in that RevPRAG consistently achieves high TPRs and low FPRs when confronted with poisoned texts generated by different strategies. For instance, RevPRAG achieved 97.2% (with GPT2-XL 1.5B), 98.5% (with Llama2- 7B), and 97.7% (with Mistral-7B) TPRs for poisoned texts generated with PoisonedRAG. Our method focuses on detect- ing the outputs of RAG rather than analyzing the inputs. In other words, regardless of the content of inputs into LLM or the poisoned text generation strategy, our method can reliably detect poisoned responses as long as they occur.\nDifferent methods for generating poisoned texts. To ensure the effectiveness of the evaluation, we employ three different methods introduced by PoisonedRAG, GARAG, and PRCAP to generate the poisoned texts. The experimental results in Table V show that RevPRAG consistently achieves high TPRs and low FPRs when confronted with poisoned texts generated by different strategies. For instance, RevPRAG achieved 97.2% (with GPT2-XL 1.5B), 98.5% (with Llama2- 7B), and 97.7% (with Mistral-7B) TPRs for poisoned texts generated with PoisonedRAG. Our method focuses on detect- ing the outputs of RAG rather than analyzing the inputs. In other words, regardless of the content of inputs into LLM or the poisoned text generation strategy, our method can reliably detect poisoned responses as long as they occur.\ntexts in the knowledge database may lead to varying poisoning effects and distinct LLM activations. Therefore, it is crucial to conduct ablation experiments using various similarity metrics.  shows the results on the HotpotQA dataset, indicat- ing that the choice of similarity calculation method has min- imal impact on RevPRAG's performance, which consistently achieves high TPR and low FPR. This suggests the robustness of our approach, as it reliably identifies poisoned texts even when LLM activations vary slightly under similar conditions.\nActivations from specified layers. illustrates the detection performance of RevPRAG using activations from different layers of various LLMs on HotpotQA. In our ap- proach, we utilize activations from all layers as both train- ing and testing data, yielding excellent results. Additionally, the experimental results demonstrate that utilizing activations from only the first few layers can still achieve satisfactory detection performance, providing valuable insights for future research. For example, when using activations from layers 0 to 5, RevPRAG achieved TPRs exceeding 97% while maintaining FPRs below 7% for RAG with all LLMs on HotpotQA. Furthermore, the experimental results suggest that using activations from intermediate or deeper layers can lead to performance fluctuations, including signs of degradation or slower convergence. For instance, when using activations from layers 16 to 24 with Llama3-8B as the LLM in RAG, RevPRAG achieves a TPR of 78.8% on NQ dataset and 86% on MS-MARCO dataset.\nWe experiment with various support set sizes ranging from 50 to 250 to examine their effect on the performance of RevPRAG. This evaluation was conducted on Llama2-7B with different datasets. The results in Fig. 7 indicate that varying the support size does not significantly impact the model's detection performance."}, {"title": "E. Isolating Poisoned Responses and Hallucinations", "content": "It is well-known that hallucinations are an inevitable phe- nomenon in LLMs. Even with the introduction of a knowl- edge database in RAG, LLMs may still generate non-factual responses due to hallucinations. Therefore, the incorrect re- sponses generated by RAG may also stem from hallucinations, rather than being solely caused by RAG poisoning. Fig. 8 shows the t-SNE [58] representation of mean activations for poisoned response and hallucinations across all layers for Mistral-7B and Llama2-7B on the NQ dataset. We observe that activations across all layers clearly distinguish between hallucinations and poisoned responses. This key finding has led us to extend our approach to differentiate between poisoned responses and hallucinations."}, {"title": "VI. CONCLUSION", "content": "In this work, we find that correct and poisoned responses in RAG exhibit distinct differences in LLM activations. Building on this insight, we develop RevPRAG, a detection pipeline that leverages these activation patterns to identify poisoned re- sponses in RAG caused by the injection of malicious text into the knowledge database. Our approach demonstrates robust performance across RAGs utilizing five different LLMs and four distinct retrievers on three datasets. Experimental results show that RevPRAG achieves exceptional accuracy, with true positive rates approaching 98% and false positive rates near 1%. Ablation studies further confirm its effectiveness in detect- ing poisoned responses under various poisoning attack types and intensities. Additionally, we validate our method's ability to distinguish poisoned responses from non-factual responses induced by hallucinations, ensuring precise identification of genuinely poisoned outputs. Overall, our approach reliably detects poisoned responses without disrupting the functionality of RAG systems."}]}