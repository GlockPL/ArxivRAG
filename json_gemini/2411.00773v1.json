{"title": "LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation", "authors": ["Bowen Li", "Zhaoyu Li", "Qiwei Du", "Jinqi Luo", "Wenshan Wang", "Yaqi Xie", "Simon Stepputtis", "Chen Wang", "Katia Sycara", "Pradeep Ravikumar", "Alexander Gray", "Xujie Si", "Sebastian Scherer"], "abstract": "Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks. However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions. Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities. To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents. LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios. Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning. To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors. Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data. With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI. All the code and data are open-sourced at our website.", "sections": [{"title": "1 Introduction", "content": "Unlike most existing deep neural networks [1, 2], humans are not making predictions and decisions in a relatively black-box way [3]. Instead, when we learn to drive a vehicle, play sports, or solve math problems, we naturally leverage and explore the underlying symbolic representations and structure [3-5]. Such capability enables us to swiftly and robustly reason over complex situations and to adapt to new scenarios. To emulate human-like learning and reasoning, the Neuro-Symbolic (NeSy) AI community [6] has introduced various hybrid systems [7\u201318], integrating symbolic reasoning into deep neural networks to achieve higher data efficiency, interpretability, and robustness\u00b9.\nDespite their rapid advancement, many NeSy AI systems are designed and tested only in very simplified and limited environments, such as visual sudoku [19], handwritten formula recogni-"}, {"title": "2 Related Works", "content": "NeSy AI systems aim to integrate formal logical reasoning into deep neural networks. We distinguish these systems into two categories: deductive methods and inductive methods."}, {"title": "2.1 Neuro-Symbolic AI", "content": "NeSy AI systems aim to integrate formal logical reasoning into deep neural networks. We distinguish these systems into two categories: deductive methods and inductive methods.\nDeductive Methods typically operate under the assumption that the underlying symbolic structure and the chain of deductive reasoning (rules) are either known [8, 9, 20, 39\u201341] or can be generated by an oracle [17, 18, 42, 43]. Some of these approaches constructed differentiable logical loss functions that constrain the training of deep neural networks [39, 40]. Others, such as DeepProbLog [8], have formulated differential reasoning engines, thus enabling end-to-end learning [44-46]. Recently, Large Language Models (LLMs) have been utilized to generate executable code [17, 42, 43] or planning abstractions [47], facilitating the modular integration of the grounding networks. Despite their success, deductive methods sidestep or necessitate the laborious manually engineered symbolic structure, which potentially limits their applicability in areas lacking formalized knowledge.\nInductive Methods focus on generating the symbolic structure either through supervised learning [10, 11, 35, 48-50] or by interacting with the environment [51\u201353]. One line of research explicitly searches the rule space, such as JILP [48], Difflog [54], and Popper [10]. However, as the rule space can be exponentially large for abstractions, these methods often result in prolonged search times. To address this, some strategies incorporate neural networks to accelerate the search process [11, 49, 50]. Another avenue of inductive methods involves designing logic-inspired neural networks where rules are implicitly encoded within the learned weights [7, 19, 55, 56], such as SATNet [19] and Neural Logic Machines (NLM) [7]. While these methods show promise for scalability and generalization, their applications have been predominantly limited to overly simplistic test environments."}, {"title": "2.2 Games and Simulations", "content": "Various gaming environments [22-24, 26] have been developed to advance AI agents. Atari games [22], for instance, provide diverse challenges ranging from spatial navigation in \u201cPac-Man\" to real-time strategy in \u201cBreakout\". More complex games include NetHack [26], StarCraft II [57], and MineCraft [27], where an agent is required to do strategic planning and resource management.\nLogiCity shares similarities with these games in that agent behavior is governed by rules. Especially, LogiCity can be viewed as a Rogue-like gaming environment [26], where maps and agent settings could be randomly generated in different runs. However, our simulator is uniquely tailored for the"}, {"title": "3 LogiCity Simulator", "content": "The overall framework of LogiCity simulator is shown in Figure 1. In the configuration stage, a user is expected to provide Concepts, Rules, and Agent set, which are fed into the abstraction-based simulator to create a sequence of urban grid maps. These maps are rendered into diverse RGB images via generative models, including a LLM [1] and a text-driven diffusion model [29]."}, {"title": "3.1 Configuration and Preliminaries", "content": "Concepts consist of K background predicates $P = {P_i(\u00b7)|i = 1, ..., K}$. In LogiCity, we can define both semantic and spatial predicates. For example, IsAmbulance() is an unary semantic predicate and IsClose(, ) is a binary spatial predicate. These predicates will influence the truth value of four action predicates {Slow(\u00b7), Normal(\u00b7), Fast(\u00b7), Stop()} according to certain rules.\nRules consist of M FOL clauses, $C = {C_m|m = 1,..., M}$. Following ProLog syntax [28], an FOL clause $C_m$ can be written as:\nStop(X): -IsClose(X, Y) \u2227 IsAmbulance(Y),\nwhere Stop(X) is the head, and the rest after \u201c: \u2013\u201d is the body. X, Y are variables, which will be grounded into specific entities for rule inference. Note that the clause implicitly declares that the variables in the head have a universal quantifier (\u2200) and the other variables in the body have an existential quantifier (\u2203). We assume only action predicates appear in the head, both action and background predicates could appear in the body.\nThe concepts P and rules C are abstractions, which are not tied to specific entities.\nAgents serve as the entities in the environment, which is used to ground the abstractions. We use $A = {A_n|n = 1,..., N}$ to indicate all the N agents in a city. Each agent will initially be annotated with the semantic concepts defined in P. For example, an ambulance car A\u2081 is annotated as $A_1 = {IsCar : True, IsAmbulance : True, . . ., p}$, where p\u2208 R denotes right-of-way priority.\nP, C, A make up the configuration of LogiCity simulation. A user can flexibly change any of them seamlessly without modifying the simulation and rendering process."}, {"title": "3.2 Simulation and Rendering", "content": "As the simulation initialization, a static urban map $M_s \u2208 {0, 1}^{W\u00d7H\u00d7B}$ is constructed, where W, H denotes the width and height. B indicates the number of static semantics in the city, e.g., traffic streets, walking streets, intersections, etc. The agents then randomly sample collision-free start and goal locations on the map. These locations are fed into a search-based planner [68] to obtain the global paths that the agents will follow to navigate themselves. On top of the static map, each agent will create an additional dynamic layer, indicating their latest location and planned paths. We use $M_t \u2208 {0, 1}^{W\u00d7H\u00d7(B+N)}$ to denote the full semantic map with all the N agents at time step t."}, {"title": "4 LogiCity Tasks", "content": "LogiCity introduced above can exercise different aspects of NeSy AI. For example, as shown in Figure 2, we design two different tasks. The Safe Path Following (SPF) task aims at evaluating sequential decision-making capability while the Visual Action Prediction (VAP) task focuses on reasoning with high-dimensional data. Both tasks assume no direct access to the rule clauses C."}, {"title": "4.1 Safe Path Following", "content": "SPF requires an algorithm to control an agent in LogiCity, following the global path to the goal while maximizing the trajectory return. The agent is expected to sequentially make a decision on the four actions based on its discrete, partial observations, which should minimize rule violation and action costs. In the following introduction, we assume the first agent, i.e., A\u2081 is the controlled agent.\nSpecifically, the SPF task can be formulated into a Partially Observable Markov Decision Process (POMDP), which can be defined by the tuples (S, A, \u03a9, T, Z, R, \u03b3). The state at time t is the global urban grid, together with all the agents and their conceptual attributes, $s_t = {M^t, A} \u2208 S$. The action space A is the 4-dimensional discrete action vector $a_t$. The observation at t-th step is the grounding of the agent's FOV, $o_t = g_t \u2208 \u03a9$, which can be obtained from the parsing functions $F_i$. State transition $T(s_{t+1}|s_t, a_t)$ is the simulation process introduced in Section 3.2. The observation"}, {"title": "4.2 Visual Action Prediction", "content": "Unlike SPF, which is long-horizon and assumes access to the groundings, the VAP task is step-wise and requires reasoning on high-dimensional data [13, 19]. As shown in Figure 2 (b), inputs to VAP models include the rendered image I (We omit the time superscript t here) and information for N agents $[h_1,..., h_N] \u2208 R^{N\u00d79}$, where $h_n = [x_n, y_n, w_n, h_n, d_n, p]^\u22a4$ consists of location $(x_n, y_n)$, scale $(w_n, h_n)$, one-hot direction $d_n \u2208 R^4$, and normalized priority p. During training, the models learn to reason and output the action vectors \u00e2n for all the N agents with ground-truth supervision. During test, the models are expected to predict the actions for different sets of agents.\nThis task is approached as a two-step graph reasoning problem [7, 37]. As illustrated in Figure 2 (b), a grounding module first predicts interpretable grounded predicate truth values, which are then used by a reasoning module to deduce action predicates. To be more specific, a visual encoder [2, 70] first extracts global features F from I. Agent-centric regional features are derived from ROIAlign [71], which resizes the image-space bounding boxes to match the feature scale and then crops the global feature using bilinear interpolation. The resulting regional features for each agent, denoted as $f_n$, are fed into unary prediction heads to generate unary predicate groundings. Meanwhile, binary prediction heads utilize paired agent information to predict binary predicates. Together, the groundings form a scene graph G, which a graph reasoning engine [7, 37] uses to predict actions \u00e2n.\nSimilar to the SPF task, the VAP task also features different train/test agent compositions, necessitating the model's ability to learn abstractions. Additionally, unlike reasoning on structured, symbolic knowledge graphs [7, 11, 20], the diverse visual appearances in LogiCity introduce high-level perceptual noise, adding an extra challenge for reasoning algorithms."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Safe Path Following", "content": "We first construct a ground-truth rule-based agent as Oracle and a Random agent as the worst baseline, showing their results in Table 2. Two branches of methods are considered here, behavior cloning (BC) and reinforcement learning (RL), respectively. All the experiments in SPF are conducted on a single NVIDIA RTX 3090 Ti GPU with 32 AMD Ryzen 5950X 16-core processors.\nBaselines. In the BC branch, we provide oracle trajectories as demonstration and consider the inductive logical programming (ILP) algorithms [10], including symbolic ones [10, 35] and NeSy ones [7, 11]. We also construct a multi-layer perceptron (MLP) and graph neural networks (GNN) [37] as the pure neural baselines. In the RL track, we first build neural agents using various RL algo-rithms, including on-policy [32, 33], off-policy [7, 34] model-free approaches and model-based algorithms [36, 38]. Since most of the existing NeSy RL methods [51, 52] are carefully engineered for simpler environments, we find it hard to incorporate them into our LogiCity environment. To"}, {"title": "5.1.1 Empirical Evaluation", "content": "We present the empirical results in Table 2, showing LogiCity's ability to vary reasoning complexity. In the BC track, symbolic methods [10, 35] perform well in the easy mode but struggle with more"}, {"title": "5.1.2 Continual Learning", "content": "Using LogiCity, we also examine how much data different models need to continually learn new abstract rules. We initialize models with the converged weights from easy mode and progressively provide data from medium mode rules. The results from three random runs for MLP and NLM [7] are shown in Figure 4, alongside results from models trained from scratch. NLM reaches the best result with 30% of the target domain data, demonstrating superior continual learning capabilities."}, {"title": "5.2 Visual Action Prediction", "content": "Baselines. As there exists very limited literature [72] studying FOL reasoning on RGB images, we self-construct two baselines using GNN [37] and NLM [7] as the reasoning engine, respectively. For fairness, we use the same visual encoder [2, 70] and hyperparameter configurations. We train and test all the models on a single NVIDIA H100 GPU.\nSettings. We explore four distinct training settings for the two methods. Regarding supervision signals, modular supervision offers ground truth for both scene graphs and final actions, training the two modules separately. This setting requires interpretable meanings of the scene graph elements,"}, {"title": "5.2.1 Empirical Evaluation", "content": "The empirical results for the VAP task are shown in Table 3, highlighting LogiCity's ability to adjust reasoning complexity. We observe that while GNN [37] slightly outperforms NLM [7] in the easy mode, NLM excels in the hard mode. We also find that random agent configurations improve the performance of both methods. The data imbalance poses an additional challenge, with the Stop action having 2\u00d7 ~ 6\u00d7 higher recall than the Fast action. Besides, the modular setting proves more challenging than the end-to-end (E2E) setting, as the modular system is more sensitive to perceptual noise. We further investigate this issue quantitatively in Appendix E."}, {"title": "5.2.2 Continual Learning", "content": "Similar to the SPF task, we investigate how much data the methods need to continually learn abstract rules in the VAP task. The models pre-trained in easy mode are used as the initial weights, which are continually trained with different sets of data from the hard mode. The data are sampled for 3 times and the mean and variance of the results are reported in Figure 6, where we also report the results from the models trained with 100% data from scratch as dashed lines (\"upper bound\"). We observe that the two methods could struggle to reach their \"upper bound\" if fixed training agents are used. For the random agent setting, NLM [7] could progressively learn new rules and reach its \"upper bound\" with around 50% data while GNN fails even with 100% data."}, {"title": "5.2.3 How Do LLMs and Human Perform in LogiCity?", "content": "Recent years have witnessed the increasing use of LLMs for decision making [73\u201376], concept understanding [77\u201379], and logical reasoning [80\u201383]. In this section, we investigate the capability of LLMs [1] and Human to solve the (subset of) VAP task in LogiCity through in-context demonstrations. Since we focus only on logical reasoning, true groundings are provided in natural language documents without perceptual noise. Specifically, we first convert every scene (frame) into a paragraph of natural language description (see Figure B for examples). For each entity within the frame, given the scene descriptions, we ask LLMs to decide its next action from options (\u201cA. Slow\u201d, \u201cB. Normal\u201d, \u201cC. Fast\", \"D. Stop\"). Since the entire test set of VAP is huge, we randomly selected a \"Mini\" test with 117 questions about the concept IsTiro and IsReckless. To construct demonstrations for\""}, {"title": "6 Discussions", "content": "Conclusion. This work presents LogiCity, a new simulator and benchmark for the NeSy AI community, featuring a dynamic urban environment with various abstractions. LogiCity allows for flexible configuration on the concepts and FOL rules, thus supporting the customization of logical reasoning complexity. Using the LogiCity simulator, we present sequential decision-making and visual reasoning tasks, both emphasizing abstract reasoning. The former task is designed for a long-horizon, multi-agent interaction scenario while the latter focuses on reasoning with perceptual noise. With exhaustive experiments on various baselines, we show that NeSy frameworks [7, 11] can learn abstractions better, and are thus more capable of the compositional generalization tests. Yet, LogiCity also demonstrates the challenge of learning abstractions for all current methods, especially when the reasoning becomes more complex. Specifically, we highlight the crucial difficulty of long-horizon abstract reasoning with multiple agents and that abstract reasoning with high dimensional data remains hard. On the one hand, LogiCity poses a significant challenge for existing approaches with sophisticated reasoning scenarios. On the other hand, it allows for the gradual development of the next-generation NeSy AI by providing a flexible environment.\nLimitations and Social Impact. One limitation of our simulator is the need for users to pre-define rule sets that are conflict-free and do not cause deadlocks. Future work could involve distilling real-world data into configurations for LogiCity, streamlining this definition process. Currently, LogiCity does not support temporal logic [40]; incorporating temporal constraints on agent behaviors is intriguing. The simulation in LogiCity is deterministic, introducing randomness through fuzzy logic deduction engines [8, 9] could be interesting. For the autonomous driving community [25, 62], LogiC-ity introduces more various concepts, requiring a model to plan with abstractions, thus addressing a new aspect of real-life challenges. Enhancing the map of LogiCity and expanding the action space could make our simulator a valuable test bed for them. Additionally, since the ontologies and rules in LogiCity can be easily converted into Planning Definite Domain Language (PDDL), LogiCity has potential applications in multi-agent task and motion planning [12, 85]. A potential negative social impact is that rules in LogiCity may not accurately reflect our real life, introducing sim-to-real gap."}, {"title": "D State Space Comparison", "content": "In the SPF task, we by default provide the predicate groundings as the observation of the agents, which is abstract and could be lossy [12]. Thus, we have also tried to provide exact states to the agents in this section. Specifically, we annotate each pixel of the FOV map with the agent semantics and convert the pixels into 2D semantic point clouds. Since these point clouds contain all the information needed for an optimal policy, it serves as the \"Exact State\" for the ego agent. The results comparison of using abstract (Abs.) and exact (Exa.) states is shown in Ta-"}, {"title": "E Quantitative Perceptual Noise", "content": "Compared with structured knowledge graphs [20, 91, 92], the VAP task of LogiCity introduces diverse RGB images, which require models to conduct abstract reasoning with high-level perceptual noise. We quantitatively display the perception accuracy of different concepts from the NLM model [7] in Table D. Even with supervision, the averaged recall rate for the concepts is not satisfiable (Note that the errors will actually accumulate, which will be much more worse than the 55% averaged result). Compared with binary predicates, unary predicates need operation on the RGB image, which is thus harder. We also observe that the results are highly-imbalanced across concepts. For example, pedestrains and cars are easy to recognize, but a police/tiro car is extremely hard to be distinguished from normal ones. In terms of binary predicates, CollidingClose is the hardest to learn, since it needs to consider all the locations, sizes, and directions of the two entities, while the others only involves positions or priorities. One potential solution to the perceptual noise is borrowing off-the-shelf foundation models [93, 94] for the grounding task."}, {"title": "F Visualizations", "content": "SPF: Visualizations of the SPF task episodes are displayed in Figure C. Compared with the training city shown on the left, test cities have different agent compositions. For example, training city only has 2 old man while test cities has 4 such entities, featuring compositional generalization challenge. Compared with pure neural networks, NeSy method (NLM-DQN) can better generalize to unseen compositions. For example, in Episode 92, Step 84, the ego agent sees two pedestrians InIntersection with an Ambulance AtIntersection, which is an unseen composition during training. DQN fails here, outputting Normal action while NLM-DQN succeeds with the correct Stop decision. SPF task also features realistic multi-agent interaction. As shown in Episode 93, Step 125, since the two algorithms made different decisions in previous steps, the city will be very different as the other agents are largely affected by the ego actions.\nVAP: Visualizations of the VAP task examples are shown in Figure D. Compared with GNN [37], the NeSy method NLM [7] can better understand the abstractions of LogiCity. For example, Reckless cars drives Normally when it is InIntersection, while other cars should drive Slow. We find that GNN [37] shows limitation in understanding such concept and rules, making wrong predictions."}, {"title": "A Detailed Baseline Configurations", "content": "To make our experiments reproducible, we provided detailed baseline introductions and configurations below. For more details, please refer to our code base."}, {"title": "A.1 Safe Path Following", "content": "In the BC branch, we have considered ILP methods [10, 35], including both symbolic ones [10, 35] and NeSy ones [7, 11]. For them, we convert the demonstration trajectories (step-wise truth value of all the predicates) into facts and conduct rule learning. Popper [10] is one of the most performant search-based rule induction algorithm, which uses failure samples to construct hyposithes spaces via answer set programming. It shows better scaling capability than previous template based methods [48]. Since Popper is a greedy approach, it usually costs too much time searching. Maxsynth [35] relax this greedy setting and aims at finding rules in noisy data via anytime solvers. In our experiments, we set 300 seconds (averaged training time for other methods) as the maximum search time for Popper and Maxsynth. For all the other parameters, official default settings are used for fairness. HRI [11] is a hierachical rule induction framework, which utilizes neural tensors to represent predicates and searches the explicit rules by finding paths between predicates. For different modes in LogiCity, we provided the number of background predicates as HRI initialization. All the other parameter settings are kept the same as the original implementation. When constructing the scene graph, we make sure the ratio of positive and negative samples is 1:1. For the other NLM [7] is an implicity rule induction method, which proposed a FOL-inspired network structure. The learnt rules are implicity stored in the network weights. For different modes in LogiCity, we provided the number of background predicates as NLM initialization. Across different modes, we used the same hyperparameters, i.e., the output dimension of each layer is set to 8, the maximum depth is set to 4, and the breadth is 3. For the baselines above, we used their official optimizer during training. In addition, we constructed pure neural baselines, including an MLP and a GNN [37], both having two hidden layers with ReLU activations. In the easy and medium modes, the dimensions of the hidden layers are 128 \u00d7 64 and 64 x 64. In the hard and expert modes, the dimensions of the hidden layers are 128 \u00d7 64 and 64 \u00d7 128. These self-constructed baselines are trained with Adam optimizer [86]. For more details, please refer to our open-sourced code library."}, {"title": "A.2 Visual Action Prediction", "content": "In the VAP task, we built two baseline models with similar structure, namely GNN [37] and NLM [7]. Across the two models, we used the same grounding framework. Specifically, ResNet50 [2] plus Feature Pyramid Network (FPN) [70] pre-trained on ImageNet [89] is leveraged as the feature encoder. After ROIAlign [71], the resulting regional features are in the shape of R512. The unary predicate heads are three-layer MLPs with BatchNorm1D, ReLU, and Dropout functions. Note that the unary predicates are all about the regional feature, requiring no additional information h. On the other hand, the binary predicates are all about the additional information. We first concatinate the information h for each pair of entities and used two-layer MLPs to predicate the truth values of binary predicates. For details about the structure of the MLPs, please see our code library. The truth values of unary and binary predicates form a scene graph for the reasoning networks [7, 37] to predict actions. For GNN [37], we used a hidden layer in the dimension of 128. For NLM [7], we employed official implementation, where each logic layer invents 8 new attributes, the maximum depth is set to 4 and the breadth is set to 3. In the end-to-end setting, both methods are trained using AdamW [90]. In the modular setting, the grounding module is trained using Adam [86] while the reasoning module is optimized using AdamW [90]. For all the experiments, we train the models for 30 epochs and test the best performing checkpoint in the validation set. Note that these settings are the same in the two modes of VAP task."}, {"title": "B Detailed Task Configurations", "content": "The full list of predicates and rules and their descriptions are displayed in Table A and Table B, respectively. Across different modes in the two tasks, the involved predicates and rule clauses are the subsets of these full lists. We introduce the detailed configurations below."}, {"title": "B.1 Safe Path Following", "content": "Modes and Dataset: Across all modes, we fix the (maximum) number of FOV agents into 5, i.e., N\u2081 = 5. If the number of observed agents are fewer than 5, zero-padding (closed-world assumption) is utilized, otherwise, we neglect the extra agents. The predicates involved in each mode are displayed in Table A. Easy mode includes 7 unary and 2 binary predicates, resulting in an \u03a3\u2081 = 85 dimensional grounding vector. Rules involve only spatial concepts and constrain the Stop action. Medium mode features 8 unary predicates and 4 binary predicates, creating a \u03a3\u2081 = 140 dimensional grounding vector. The medium rule sets is extended from the easy mode and incorporate both spatial and semantic concepts, constraining the Stop action. Hard mode contains 11 unary predicates and 6 binary predicates, yielding a \u03a3\u2081 = 205 dimensional grounding vector. Rules cover all spatial and semantic concepts and constrain the Stop action. The expert mode constrains all four actions with the most complex rule sets. We provide standard training/validation/test agent configurations and validation/test episodes for all the modes. The training agents cover all the necessary concepts in the rules, while validation and test agents are different and more complex, see our code library for the detailed agent configuration. For each mode, we collect 40 validation episodes and 100 test episodes using corresponding agent distribution, making sure the episodes cover"}, {"title": "B.2 Visual Action Predication", "content": "Modes: As shown in Table A, the predicates in the two modes involve the full list. As for the clauses, easy mode only constrains Stop and Slow actions, setting Normal as the default action. Hard mode constrains all the three actions with Normal set as the default actions. Note that the rule clauses in hard model is a superset of that in the easy mode.\nDatasets: In the random agent setting, we randomly generated 100 and 20 cities with different agent compositions for training and validation, respectively. For each city, we run the simulation for 100 steps and only used the data after 10 steps. In the fixed agent setting, we first pre-define different training/validation/test agent compositions. Then, we randomly initialize the cities for 100 times. For each initialization, we run the simulation for 100 steps and only used the data after 10 steps. This process results in training sets with 8.9k images (with more than 89k entity samples). The models trained with different setting are tested in the same fixed agent setting test sets. See our code library and dataset for detailed agent compositions."}, {"title": "C Full Procedure of LogiCity Simulation", "content": "We provide more details for the simulator here.\nStatic Urban Semantics: There are a total of B = 8 static semantics of the urban map, namely \u201cWalking Street\u201d, \u201cTraffic Street\u201d, \u201cCrossing\u201d, \u201cHouse\u201d, \u201cOffice", "Garage": "Store\", \"Gas Station", "House": "Office\", and \"Store"}, {"House": "Office", "Store": "while Cars are navigating between \u201cGarage", "Gas Station": "and \u201cStore\u201d to \u201cGarage"}, {"title": "D State Space Comparison", "content": "In the SPF task, we by default provide the predicate groundings as the observation of the agents, which is abstract and could be lossy [12]. Thus, we have also tried to provide exact states to the agents in this section. Specifically, we annotate each pixel of the FOV map with the agent semantics and convert the pixels into 2D semantic point clouds. Since these point clouds contain all the information needed for an optimal policy, it serves as the \"Exact State\" for the ego agent. The results comparison of using abstract (Abs.) and exact (Exa.) states is shown in Ta-"}, {"title": "E Quantitative Perceptual Noise", "content": "Compared with structured knowledge graphs [20, 91, 92], the VAP task of LogiCity introduces diverse RGB images, which require models to conduct abstract reasoning with high-level perceptual noise. We quantitatively display the perception accuracy of different concepts from the NLM model [7] in Table D. Even with supervision, the averaged recall rate for the concepts is not satisfiable (Note that the errors will actually accumulate, which will be much more worse than the 55% averaged result). Compared with binary predicates, unary predicates need operation on the RGB image, which is thus harder. We also observe that the results are highly-imbalanced across concepts. For example, pedestrains and cars are easy to recognize, but a police/tiro car is extremely hard to be distinguished from normal ones. In terms of binary predicates, CollidingClose is the hardest to learn, since it needs to consider all the locations, sizes, and directions of the two entities, while the others only involves positions or priorities. One potential solution to the perceptual noise is borrowing off-the-shelf foundation models [93, 94] for the grounding task."}, {"title": "F Visualizations", "content": "SPF: Visualizations of the SPF task episodes are displayed in Figure C. Compared with the training city shown on the left", "actions.\nVAP": "Visualizations of the VAP task examples are shown"}]}