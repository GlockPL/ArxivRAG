{"title": "Solving Sparse Finite Element Problems on Neuromorphic Hardware", "authors": ["Bradley H. Theilman", "James B. Aimone"], "abstract": "We demonstrate that scalable neuromorphic hardware can implement the finite element method, which is a critical numerical method for engineering and scientific discovery. Our approach maps the sparse interactions between neighboring finite elements to small populations of neurons that dynamically update according to the governing physics of a desired problem description. We show that for the Poisson equation, which describes many physical systems such as gravitational and electrostatic fields, this cortical-inspired neural circuit can achieve comparable levels of numerical accuracy and scaling while enabling the use of inherently parallel and energy-efficient neuromorphic hardware. We demonstrate that this approach can be used on the Intel Loihi 2 platform and illustrate how this approach can be extended to nontrivial mesh geometries and dynamics.", "sections": [{"title": "Introduction", "content": "Neuromorphic computing (NMC) seeks to emulate architectural and algorithmic features of the brain to achieve lower-power, higher-capability microelectronics, as well as computing platforms. Despite this tremendous potential, the widespread impact of neuromorphic computing has been limited by the difficulty in identifying applications that show clear performance advantages [1], particularly in the realm of scientific computing [2]. There are many approaches to brain inspiration, and these provide distinct challenges to achieving the necessary performance requirements to impact real-world computing applications [3]. While analog processing-in-memory hardware is still limited by scaling obstacles, it was recently demonstrated that memristive crossbars can reach arbitrary precision for numerical dense linear algebra tasks [4]. In contrast, modern second-generation neuromorphic systems that leverage digital complementary metal-oxide-semiconductor (CMOS) architectures, such as Loihi 2 and SpiNNaker 2, are now approaching over 1 billion neurons, a similar scale to many small mammalian and avian brains [5], suggesting that spiking neuromorphic systems could potentially reach scales necessary to impact scientific simulations that currently require thousands of conventional processors.\nWe have previously observed a neuromorphic advantage in modeling discrete-time Markov chain random walks, specifically in applications which require integration over many Monte Carlo samples [6]. While this approach is promising for application domains whose problems can be formulated as stochastic differential equations, there remains a need to identify an efficient neuromorphic algorithm for the much broader class of partial differential equations (PDEs) solved through deterministic numerical solvers.\nHere, we introduce a spiking neuromorphic algorithm that solves the sparse linear systems arising from the finite element method (FEM) for solving PDEs, and demonstrate our algorithm on Intel's Loihi 2, a modern NMC platform. Finite element methods are arguably the most predominant numerical method used in modern high-performance computing applications. We show not only that FEM can be formulated as a brain-inspired neural algorithm, but that this algorithm is scalable and highly aligned with many of the brain-inspired design choices for modern NMC systems. Importantly, unlike previous NMC algorithms, the FEM approach we show is mathematically equivalent to the standard formulations used in the numerical computing community-the only difference is how the sparse linear system is solved, a difference that is largely invisible to the user. Combined, these results demonstrate that the significant low-power advantages of NMC can be leveraged to perform energy-efficient numerical computing simulations."}, {"title": "Results", "content": null}, {"title": "Finite Elements", "content": "PDEs describe physical phenomena on a domain of definition, such as understanding electrostatic forces between molecules, turbulent water flow through a turbine, or how wireless signals propagate through a building. To solve these equations, FEMs work by first discretizing the domain with a mesh (a collection of smaller geometric elements that, when combined, fully represent the domain), then approximating the solution using a linear combination of basis functions supported on mesh elements. Mathematically, applying the weak form of the PDE to this finite-dimensional approximation yields a sparse linear system (Ax=b) for the coefficients of this linear combination. The solution of this sparse linear system yields the best linear combination of basis functions to approximate the solution of the original PDE.\nTo achieve high-fidelity solutions, the FEM mesh must discretize the domain at a high resolution with many elements. The associated linear systems are very large: modern FEM problems can contain many millions or even billions of variables. Importantly, because the physical interactions described by PDEs are local and the basis functions are usually compactly supported, elements in the mesh only directly interact with their nearest neighbors. This is the source of the sparsity in the associated linear system.\nThese large linear systems are traditionally solved with two classes of algorithms: iterative and direct solvers. Direct solvers, such as lower-upper (LU) factorization, modify the linear system in place to directly compute the solution. Iterative solvers, in contrast, compute a sequence of approximations that converge to the true solution over time. Each approach has distinct advantages, but iterative solvers tend to be the choice for truly large-scale problems [7].\nOur neuromorphic FEM solver is analogous to traditional iterative solvers in that we construct a spiking neural network with specific dynamics that converge to the solution of the linear system over time. Our neuromorphic formulation, which does not require learning or training, is intrinsically parallel and benefits from compact, spike-based communication between nodes instead of packets of data characteristic of traditional parallel algorithms wherein information is carried by the relative timing between spikes (timed all-or-nothing events) rather than numerical values passed between nodes. Memory bandwidth is the limiting computational resource in traditional large sparse linear solvers [8], and our approach colocalizes the computation and memory required to solve these problems to individual neurons and synapses."}, {"title": "Spiking Neural Network for FEM problems", "content": "Our approach begins by interpreting the solution process of the sparse FEM system (Ax = b) as a dynamical system which we embed into a spiking neural network (Figure 1B). We based this embedding on previous research in computational neuroscience that described a procedure for constructing spiking neural networks that implement a dynamical system [9]. This mapping of an FEM linear system to the spiking network is direct and therefore does not require any training during network construction. We focus initially on the steady-state Poisson equation on a disk with Dirichlet (i.e., fixed) boundary conditions (see Methods) as a prototypical example of a linear, elliptic PDE well suited to finite element methods, and often used as a benchmark problem. This problem also has an analytic solution allowing us to compare our spiking approximations to a ground truth. Our finite element formulation of the Poisson problem uses piecewise linear elements (Figure S1)\nIn our formulation (Figure 1), we associate a small population (typically between 8 and 16, but this is a free hyperparameter) of recurrently connected neurons with each mesh node. The sparse system matrix A determines the synaptic weights between neurons at different nodes (see Figure 1B and materials and methods). The right-hand side of the linear system manifests as biases applied to each neuron. Finally, each neuron receives an independent noise signal, which is important for achieving balanced, asynchronous spiking dynamics (see materials and methods).\nOur neurons do not produce real-valued activations as output. Instead, the results of local computations in each neuron are communicated through spikes. In this sense, the network leverages spikes as actions that choreograph the activity of other neurons and the ultimate network output [10] as opposed to viewing spikes as single-bit numerical quantities or symbols. The spikes thus carry information through their relative timing relationships; thus, to solve numerical problems, the spike actions must be transduced to conventional binary representations of numerical quantities.\nIn our FEM network, the spikes from neurons representing a given mesh node are effectively low-pass filtered using a readout matrix \u0393 to construct an estimate of the solution of the FEM problem at that mesh node (Figure 1C). In other words, spikes act by perturbing a first-order linear dynamical system to drive it to the correct solution value (Figure 1C, bottom).\nThe spiking dynamics of our spiking FEM network (referred to as NeuroFEM) are shown in Figure 1E, top. Over time, the network converges to a steady-state, asynchronous firing regime. Figure 1E, bottom shows the relative residual between the true solutions of the linear system (for two different forcing functions f\u2081 and f2, see materials and methods) and the instantaneous readout of the spiking dynamics in the top panel, showing that the network's steady-state spiking well-approximates FEM solutions. Figure 1F shows the solution of the spiking network rendered onto the original finite element mesh, showing close agreement with classical solutions for the different forcing functions f\u2081 and f2 (i.e., right-hand sides of the linear system).\nWe found that a direct translation of the networks prescribed by [9] does not yield a satisfactory linear solver, with such networks producing a steady-state bias in their solutions (Figure S2). We overcame this problem by realizing that [9] is mathematically equivalent to a proportional-only controller, which necessarily yields a steady-state bias. We fixed this by augmenting each neuron with an additional state variable that integrates the local residual error, turning the network into a system of distributed, spiking PI controllers (see materials and methods). This eliminates the steady-state error while still only requiring information local to each neuron.\nOnce a spiking network is constructed for a particular FEM problem (i.e., a choice of PDE, boundary conditions, mesh, and finite element space), the network may continuously solve new problem instances (right-hand sides; forcing functions) by varying the biases applied to each mesh node at run time. Figure 1E shows a switch to a new right-hand side representing a different forcing function at timestep 2000. The network immediately responds by adjusting its spiking dynamics to flow to a new steady state, representing the FEM solution of the PDE with respect to the new forcing function (Figure 1F, bottom). This reconfiguration is a consequence of the natural dynamics of the spiking network by construction and requires no training or adjustment beyond the initial construction of the FEM problem and the associated spiking network. This implies that a physical instantiation of this network could respond in real time to external data obtained from physical sensors, with a \u201cneuromorphic twin\u201d providing continuous estimates for the response of a real-world system\u2014an important application domain for neuromorphic scientific computing [11].\nImportantly, our NeuroFEM construction is scalable and adaptable to different mesh resolutions. Because the network architecture is determined by the sparsity structure of the system matrix A, the intrinsically geometric origin of this structure through the finite element mesh manifests as a geometrically structured spiking recurrent neural network with locally dense, globally sparse connectivity. Since neurons only synapse with their nearest neighbors in the mesh, the number of synapses per neuron remains essentially constant as the number of mesh nodes increases. This is critical because it allows our network to model complex geometries with large, unstructured meshes. Further, the accuracy of numerical solutions is tightly coupled to the resolution of mesh, and higher resolution meshes yield more accurate solutions (Figure 2B)."}, {"title": "Accuracy of NeuroFEM solutions", "content": "We examined the numerical accuracy of the NeuroFEM solutions for the steady-state Poisson equation on a disk, with Dirchlet boundary conditions and a constant forcing function across the domain, at several different mesh resolutions (approximately 100\u201310,000 nodes). Because the network produces a fluctuating estimate of the solution, these figures reflect the accuracy of the spiking readout averaged over 10,000 timesteps after the network reached steady state (see materials and methods). We observed that the spiking network provided comparably accurate solutions to those of a classic numerical solver (SciPy spsolve) as the number of mesh nodes increases (Figure 2, B and C). The relevant independent variable for Figure 2 is the size of the NeuroFEM circuit, implied by the number of mesh nodes. Convergence with respect to element size is a property of the specific finite element method, not the linear solver. For this example, NeuroFEM exhibits quadratic convergence (Figure S3) identical to the classical method, as expected for piecewise linear (order 1) elements (Figure S1).\nWe computed the relative residual of the linear system $||b - Ax|| / ||b|| $ and found that the relative residual per mesh node remained constant as the number of mesh nodes increased (Figure 2D), in agreement with conventional solvers. While the absolute value of this residual was worse for NeuroFEM than SciPy's spsolve we found that changing network parameters such as the number of neurons per mesh point (8 or 16) or the magnitude of the readout weights (2-6 or 2-8) directly contributed to the residual magnitude, suggesting that our performance observations are due in part to particular parameter choices and not intrinsic limitations of the algorithm. This invites more detailed investigations of the numerical properties of the NeuroFEM solutions.\nWe directly compared the NeuroFEM solutions to the solutions provided by our classical solver (SciPy spsolve) by computing the relative error between the spsolve solution (x) and the NeuroFEM solution ($ \\hat{x}$)  $||x - \\hat{x}|| / ||x|| $(Figure 2E). As the number of mesh nodes increased, this quantity appeared to reach an asymptotically constant value, indicating that the relative error between the spiking network solution and those from a conventional linear solver do not diverge as the size of the circuit increases.\nFinally, we characterized the statistical properties of the fluctuating readout of the spiking network solution. Figure 2D shows that the slope of the relative residual as a function of the number of mesh nodes is constant for a particular network. We found that as we increased the number of samples we averaged to yield a solution, the inverse of this slope grew linearly (Figure 2F). In contrast, by adding noise to the classical solver solution, this quantity scaled as |\u0393| follow more closely. This is not a convergence study as the relevant quantity is the size of the circuit (number of nodes) rather than the element size. (C) Zoomed-in view of the curves in (B). (D) The relative residual of the linear system per mesh point for NeuroFEM is constant as a function of the number of mesh nodes. Colors and line styles are the same as (A) and (B). The relative residual is improved by increasing the number of neurons per mesh node and/or decreasing the readout magnitude |\u0393|. (E) The relative error between NeuroFEM solutions and solutions generated by a conventional solver. As the mesh resolution increases, the error between NeuroFEM and the conventional solver asymptotes. (F) The relative residual improves linearly with the number of readout timesteps averaged together. Inverse relative residual slope is defined as the reciprocal of the constant relative residual per mesh node plotted in (D). Steeper curves indicate greater improvement as the number of averaging timesteps increases."}, {"title": "Implementation on Loihi 2", "content": "The NeuroFEM circuit is broadly compatible with both analog and digital neuromorphic technologies. The Loihi 2 neuromorphic chip is a digital CMOS integrated circuit specifically designed for efficient evaluation of spiking neural networks with sparse connectivity and asynchronous spiking activity [13]. We examined the suitability of our spiking FEM algorithm on the Loihi 2 platform (Figure 3B), which enables up to 1 million neurons per chip and has been built into systems with over 1 billion neurons [14]. While our initial evaluations of the spiking network in Figs. 1 and 2 used floating-point CPU simulations, our specific Loihi 2 implementation must be tailored to account for the fixed-point precision of Loihi 2 hardware.\nWe implemented the NeuroFEM neurons (see materials and methods) using custom microcode written in Intel's proprietary Loihi 2 assembly language [15]. To convert our floating-point spiking networks to fixed-point, we rescaled our floating-point networks so that the parameters occupied fixed intervals bounded by powers of two, while preserving the overall dynamics of the networks (see materials and methods). Then, we rounded these parameters to fit the available bit widths on the Loihi 2 platform, such as the 8 bits of available synaptic weight precision and 24 bits of state variable precision. Our custom microcode neurons include instructions for bit shifting different parameters to common fixed-point representations, allowing us to choose appropriate scale factors for each parameter that maximize the available precision individually (i.e., different scale factors may be used for different parameters as needed). Nevertheless, optimizing the fixed-point conversion of our network is an important open question.\nWe evaluated FEM problems with different mesh resolutions (103\u2013967 nodes) on a single Loihi 2 chip. We found that our spiking network on Loihi 2 was consistently able to approximate the solutions to the FEM linear system (Figure 3A, D and E). The relative residuals between the Loihi 2-generated solutions and the analytic solution were much larger than the floating-point or classical spsolve solutions but did not grow exceedingly with increasing mesh size (Figure 3, D and E). We found that networks with 16 neurons per mesh node and fewer than approximately 200 mesh nodes did not reliably converge, likely due to perturbations introduced by the fixed-point conversion and their outsized effect on such a small network. However, for larger meshes, all networks and runs converged to the same solutions. Thus, the fixed-point conversion necessarily introduces numerical errors in the spiking solution, but importantly, the fixed-point spiking network is stable on the hardware, at least for meshes with more than 200 nodes."}, {"title": "Energy and Time", "content": "We next evaluated the energy and time requirements for solutions on Loihi 2. Because the real \"work\" of solving the FEM problem in our spiking network happens during the transient departures from steady-state firing, which we term \u201csolution epochs\" (Figure 1E, bottom and Figure 3C), we profiled the energy and time requirements by forcing the network to flip between sign-inverted right-hand sides every 4096 timesteps (see Figure 3C and materials and methods). We used Loihi 2's built-in power measurement systems to measure the energy and runtime difference between networks that flipped every 4096 timesteps to networks that remained in the steady-state firing regime for an equivalent total number of timesteps (217 = 131,072 timesteps; 32 solution epochs). This allowed us to estimate the additional energy and time required by each solution epoch.\nWe found that the required additional energy scaled approximately linearly as the number of mesh nodes increased (Figure 3, F and G). The required additional energy per solution epoch was at most approximately 80 milliJoules for meshes up to approximately 1000 nodes. Not surprisingly, doubling the number of neurons per mesh node increased the required energy, but interestingly, did not double it. Similarly, we found that the time difference per solution epoch scaled linearly with the number of mesh nodes (Figure 3, H and I), requiring a maximum of about 30 milliseconds longer per solution epoch for the transient case compared to the steady-state network for the meshes we considered. Iterative solvers on modern CPUs are highly optimized and their profiling is mesh-dependent and beyond the scope of this study, but we do note that while this same system can be solved faster on CPU with either GMRES (~1-2x faster) and conjugate gradient (~10x-20x faster), we estimate that the energy cost for a solution is significantly less on Loihi (Figure S6). Further, we expect that this energy advantage will grow with larger and more complex systems. Expanded details on this energy comparison are provided in the Supplementary Text.\nWe finally examined the scaling of the NeuroFEM algorithm on the full 32-chip Oheo Gulch Loihi 2 platform. In these experiments, we used all 32 chips and examined an increasing number"}, {"title": "Scaling of NeuroFEM", "content": "We observed that up to a point, the NeuroFEM approach had nearly ideal strong scaling on Loihi 2 (i.e., doubling the number of cores halved the total time). Strong scaling always eventually saturates due to Amdahl's law, which dictates that there is a point at which costs that cannot be parallelized, such as communication and serial processing, dominate. When using an estimate of the serial cost (a single Loihi 2 core does not have sufficient memory to store the full model), we observe ideal strong scaling up to a factor of ~200 to ~600, depending on the model size. Per Amdahl's law, this confirms that the NeuroFEM algorithm is over 99% parallelizable."}, {"title": "Broader impact", "content": "A significant benefit of this approach is that it enables the direct use of neuromorphic hardware on a broad class of numerical applications with almost no additional work for the user. If a problem can be represented as a sparse linear system, the neural circuit can directly implement the system. To illustrate this, we used NeuroFEM (on conventional CPUs) to solve more complex FEM problems than the 2D Poisson equation on a disk. First, we generated a more topologically complex 2D domain by putting holes in the disk (Figure 5A) and a more complex, unstructured mesh with spatially inhomogeneous resolution. To illustrate more complex boundary conditions, we formulated the problem with Dirichlet boundary conditions (i.e., fixed temperature) on the outer boundary and Neumann boundary conditions (i.e., fixed heat fluxes) on the inner hole boundaries. NeuroFEM successfully solved these systems (Figure 5B) and the generated solutions were close to conventionally generated solutions (Figure 5C).\nExtending NeuroFEM even further, we solved a static linear elasticity problem in three dimensions (Figure 5D), namely, the deformation of a 3D shape under its own weight due to gravity with one face fixed. Unlike the 2D Poisson examples, this example features a more complex system of PDEs (see materials and methods) and a topologically nontrivial 3D tetrahedral mesh. Furthermore, the solution is a vector field representing the displacement of the shape (Figure 5E) rather than a scalar field. NeuroFEM again successfully solved this system with small deviation from a conventional solver (Figure 5F), a deviation which can be improved by further optimizations (Figure 2D). Importantly, this problem was generated by conventional FEM tools, Gmsh [12], and SfePy [16, 17] and translated directly to NeuroFEM. This means that"}, {"title": "Discussion", "content": "In this paper, we have demonstrated that a standard approach to numerically solving PDEs can be implemented directly on spiking neuromorphic architectures in a manner which effectively leverages the energy-efficient features of spiking hardware. Neuromorphic computing has been increasingly proposed as a solution to the growing energy crisis in computing. But, in large part because it is an entirely new paradigm for computing, there have been few specific examples in which neuromorphic computing has been shown to provide the requisite accuracy demanded by real-world applications. To date, most such successes have been shown in analog crossbars [4, 18], which can make dense linear algebra operations very efficient; however, most large-scale scientific computing tasks are sparse at scale and in this regime, data movement dominates energy costs [19]. The approach described here takes advantage of that sparsity through efficient spiking communication, reducing communication between mesh points to a minimal set of discrete spike events. While we have demonstrated this approach on fully digital CMOS spiking architectures, this approach should extend to future neuromorphic platforms that mix local analog computation with spiking communication over distance to maximize efficiency [20-22]. Since multi-chip implementations confer additional energy costs, the observed modest rise in energy costs under weak-scaling indicates that the energy benefits should be even more evident on higher density architectures, such as wafer-scale systems and hybrid analog / digital platforms.\nThis approach provides several notable benefits beyond energy efficiency. First, our approach is a direct mapping of an established numerical method to neuromorphic computing. As a result, neuromorphic computing will benefit from established numerical theory. Relatedly, unlike most neural algorithms, our approach is directly programmed and does not require training or learning. While the ability to learn from a na\u00efve model underlies much of the value of machine learning methods, the ability to implement established models has value in terms of confidence and reliability.\nIndeed, most neural approaches to scientific computing to date have focused on scientific machine learning (SciML) approaches, such as deep operator networks (DeepONets) and physics-informed neural networks (PINNs) [23-25]. While these techniques are promising as surrogate models, these methods often still require conventional simulations to provide training data, and these neural networks have largely been optimized for the use of GPUs and similar architectures. Although there are increased efforts to map these techniques onto neuromorphic systems [26], it remains an open question whether neuromorphic hardware can outperform GPUs on deep neural networks, which have largely evolved to benefit from GPU's single-instruction multiple data (SIMD) architecture [27]. The ability to take advantage of non-GPU co-processors is important for future computing systems as neuromorphic hardware in principle allows fully distributed algorithms and can tolerate heterogeneity naturally, as is evident with the complex geometries shown in Figure 5.\nFinally, an important virtue of our approach is that it should place no additional work on the application developer to effectively utilize neuromorphic hardware. To date, while there have been several demonstrations of algorithms with neuromorphic advantages [6, 28-30], they largely require computational problems to be reformulated in a manner that is different from what is conventionally used in today's scientific computing (this is similarly the case for quantum computing and many other emerging computing paradigms [31-33]). For example, in our previous demonstration of simulating Monte Carlo random walks [6], the neuromorphic advantage arises when particles are virtually represented over a state space as opposed to a more conventional representation of allocating particles to processors. In such cases, it is necessary to also account for the user cost in considering problems from a new perspective, which ultimately increases the cost of adoption. In the approach described here, without modification we can import existing finite element mathematics wholesale to the neuromorphic domain. Thus, neuromorphic hardware can now directly benefit from the decades of CAD/FEM assembly tools developed to enable numerical simulation; likewise, this allows the established numerical computing community to readily examine neuromorphic hardware. The user-unfriendliness of spiking neuromorphic hardware has long been recognized as a significant limitation to broader adoption [1] and our results directly mitigate this problem.\nThere is a final advantage of this approach that merits further exploration. The original neurobiological motivation for this class of spiking neural networks was to model cortical motor control circuits [9]. While our implementation is not a direct mapping of this original approach, the success of numerically solving PDEs in a related network suggests that cortical-like neural circuits are indeed capable of similar computations. While it is unlikely that the cortex solves PDEs directly in this way, it is not unreasonable that the underlying neural dynamics and their ability to provide online estimations of the physics of the world may be relevant for understanding higher level cortical computations."}, {"title": "Methods", "content": null}, {"title": "Finite Element Problem", "content": "We evaluated our FEM network on a specific PDE problem, the steady-state Poisson equation on a disk, with Dirchlet boundary conditions:\n$ \\nabla^2 u = f \\text{ on } \\Omega $\n$ u = 0 \\text{ on } \\partial \\Omega. $\nUnstructured finite element meshes were generated with the MeshPy library, which provides a Python interface to the Triangle package (34). We generated meshes with different resolutions by setting the maximum triangle area parameter in the mesh generation process.\nWe assembled sparse linear systems (the stiffness matrix and mass matrix) using piecewise linear elements directly from their definitions using the NumPy and SciPy libraries.\nFor our accuracy and convergence evaluations (Figs. 2 and 3), we used a constant forcing function\n$ f(x, y) = -20. $\nFor this specific f, the problem has an analytic solution:\n$ u(x, y) = 5(1 \u2013 x\u00b2 \u2013 y\u00b2). $\nFor Figure 1E, we evaluated the solution with a different forcing function defined by\n$ f(x, y) = 12 \u2013 60(x \u2013 0.25)\u00b2 \u2013 60(y + 0.13)\u00b2. $\nDiscretization of these problems on our meshes with piecewise linear elements yields a sparse linear system Ax = b where A is an ($n_{mesh}$, $n_{mesh}$) sparse matrix."}, {"title": "Spiking Neural Network", "content": "Our spiking neural network consists of neurons implementing generalized leaky integrate-and-fire dynamics. Readout neurons implement leaky integration of spikes $s_\u03b1$ into real-valued output variables $x_i$ through a readout kernel \u0393 through the differential equation:\n$ \\frac{dx_i}{dt} = -\\lambda_d x_i + \\sum_\u03b1 \u0393_{i\u03b1} s_\u03b1(t). $\n\u0393 is our readout matrix which maps individual neurons to the variables of the linear system. For each mesh point, we associate a fixed quantity of neurons (npm; neurons per mesh point); thus, \u0393 has shape $n_{mesh}$ \u00d7 ($n_{mesh}$ * npm). Nonzero elements in each row i of F correspond to neurons that project to the associated output variable $x_i$. Thus, there are npm nonzero elements per row. Half of these nonzero elements have magnitude +|\u0393|, corresponding to neurons projecting with positive weight (Figure 1C) and half have magnitude \u2013|\u0393|. Thus, \u0393 has the form\n\\begin{bmatrix}\n+|\u0413| & -|\u0413| & 0 & 0 & 0 & 0 \\\\\n0 & 0 & +|\u0413| & -|\u0413| & 0 & 0 \\\\\n0 & 0 & 0 & 0 & +|\u0413| & -|\u0413|\n\\end{bmatrix}\nwhere each nonzero block is a (1 \u00d7 npm/2) constant block with the indicated values.\nThere are two classes of synapses between neurons. \u2018Slow' synapses communicate information between mesh nodes and are called slow because they are twice integrated into the neuron's membrane potential. The slow synaptic weights are defined by\n$ \u03a9_{slow} = \u0393^T A \u0393, $\nwhere \u0393 is the readout matrix, and A is the system matrix. $\u03a9_{slow}$ is block structured and sparse. Each nonzero block of $\u03a9_{slow}$ corresponds to a nonzero element of A. Each nonzero block of $\u03a9_{slow}$ has the structure:\n\\begin{bmatrix}\n|\u0413|^2 & -|\u0413|^2 \\\\\n-|\u0413|^2 & |\u0413|^2\n\\end{bmatrix}\n The entries in the 2 \u00d7 2 matrix above represent constant submatrices with shape (npm/2) \u00d7 (npm/2). Even though $\u03a9_{slow}$ is already sparse, there is still significant redundancy in its definition and future implementations of the network may benefit from reducing this redundancy using shared synapses.\nThe second class of synapses are called \u2018fast' because they are singly integrated. They communicate entirely within mesh nodes and accomplish fast coordination between neurons within a node. The fast synaptic weight matrix is defined as\n$ \u03a9_{fast} = \u0393^T\u0393. $\nBecause of F's structure, \u03a9fast is block diagonal and each block has the form:\n\\begin{bmatrix}\n|\u0413|^2 & -|\u0413|^2 \\\\\n-|\u0413|^2 & |\u0413|^2\n\\end{bmatrix}\nSlow synaptic input is first integrated into a latent state variable within each neuron ($u_1$) with time constant $\u03bb_d$, according to the equation:\n$ \\frac{du_{1,\u03b1}}{dt} = -\u03bb_d u_{1,\u03b1} + \\sum_\u03b2 \u03a9_{slow, \u03b1\u03b2} s_\u03b2(t). $\nHere, s(t) represents the vector of Dirac delta spike trains from other neurons in the network. Fast synapses are similarly singly integrated into a separate latent variable $u_2$ with the equation\n$ \\frac{du_{2,\u03b1}}{dt} = -\u03bb_d u_{2,\u03b1} + \u03bb_d \\sum_\u03b2 \u03a9_{fast, \u03b1\u03b2} s_\u03b2(t), $\nwhere $u_1$ represents the neuron's local evaluation of the left-hand side of the linear system (Ax). A neuron's component of the difference between the left-hand side and right-hand side of the linear system (the residual) is computed locally and carried by the latent variable $u_{err}$. Here, b is the right-hand side of the linear system and acts as a bias current for each neuron:\n$ u_{err,\u03b1} = -u_{1,\u03b1} + \u0393^T b. $\nEach neuron acts as a PI controller (and integral dynamics are necessary for accurate spiking solutions), so each neuron has a state variable $u_{int}$ that tracks the integral of the error:\n$ \\frac{du_{int,\u03b1}}{dt} = u_{err,\u03b1}. $\nFinally, all these quantities are combined into a differential equation for the membrane potential v:\n$ \\frac{dv_\u03b1}{dt} = -\u03bb_v v_\u03b1 + k_p u_{err,\u03b1} + k_i u_{int,\u03b1} + u_{2,\u03b1}  - \\sum_\u03b2 \u03a9_{fast, \u03b1\u03b2} s_\u03b2(t) + \u03c3_v \u03b7. $\nHere, \u03b7 is a vector of independent samples from a Gaussian distribution. The parameter $\u03c3_\u03bd$ is the standard deviation of the noise, which we set to 0.00225 for all networks.\nNeurons emit a spike when their membrane potential v reaches a threshold value 0. The threshold is defined by\n$ \u03b8 = \\frac{1}{2}\u03b7^2. $\nUpon spiking, neuron membrane potentials are reset by subtracting the threshold value."}, {"title": "CPU Simulations", "content": "We simulated the above equations using Python. The above differential equations were integrated using the forward Euler method with a timestep dt = 2-12."}, {"title": "Spiking Network Evaluation", "content": "We evaluated our spiking network by comparing spiking solutions to both the analytic solution for our test problem, and to solutions produced by a conventional linear solver (SciPy spsolve function). We ran each network for 50,000 timesteps to ensure the network converged to steady state. We then extracted the spiking network's estimate of the solution by averaging the readout variables over the last 10,000 timesteps.\nFor all error estimates, we use the L2 norm. We computed the relative error between the analytic solution on the mesh points and either the spiking or conventional solver using the equation:\n$ RelErr = \\frac{||x - \\hat{x}||}{||x||} $\nHere, \u00ee represents either the spiking or conventional solution and x represents the analytic solution.\nWe evaluated the relative residual of the linear system per mesh node (Figure 2D) for the spiking network by computing the quantity:\n$ r= \\frac{1}{N_{mesh}} \\frac{||b - Ax||}{||b||} $"}, {"title": "Loihi 2 Implementation", "content": "We implemented the dynamical systems defining each neuron described by the equations above using custom microcode neurons on Loihi 2.\nTo map the parameters of our spiking network onto Loihi 2", "9)": "so that the numerical values of the fast and slow weight matrices were contained in intervals bounded by a power of two (arbitrarily chosen).\nNext", "relation": "n$ \\bar{x} = x * 2^{sx}. $\nFixed-point quantities denoted with overbars are rounded to the nearest integer. After defining the associated fixed-point variables for each floating-point variable, we substituted the corresponding fixed-point quantity into the above equations. This substitution brought the associated scale factors into the above differential equations. We algebraically rearranged these scale factors so that the left-hand sides of each equation were written entirely in terms of fixed-point quantities.\nThis algebraic rearrangement leads to power-of-two scale factors multiplying each quantity on the right-hand sides of the equations. These factors are the conversion factors between the scales of the different fixed-point quantities in the equations. Because these conversion factors are all powers of two by definition, we realized these conversions on Loihi 2 by introducing specific bit shift instructions into our microcode neurons.\nWe chose the exponents in the scale factors by considering the allowed precision of different variables on Loihi 2. For example, synaptic weights on Loihi 2 have 8-bit precision. Thus, we chose the scale factor so that our final weight matrix would take the values in the range [\u2013128, 127"}]}