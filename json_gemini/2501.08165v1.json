{"title": "I Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution", "authors": ["Soohyeon Choi", "Yong Kiam Tan", "Mark Huasong Meng", "Mohamed Ragab", "Soumik Mondal", "David Mohaisen", "Khin Mi Mi Aung"], "abstract": "Source code authorship attribution has received attention from the security and software engineering research communities due to its potential uses in software forensics, plagiarism detection, and protection of software patch integrity. Existing code authorship attribution techniques mainly resort to supervised machine learning techniques, which rely heavily on extensive labeled datasets yet struggle with generalization across diverse programming languages and coding styles. This paper is inspired by recent advancements in natural language authorship analysis brought about by large language models (LLMs)-LLMs have demonstrated remarkable performance and generalization in various tasks without task-specific tuning or pre-training on labeled data. Thus, we seek to similarly leverage LLMs to address the technical challenges of source code authorship tasks.\nWe design and present a comprehensive empirical study showing that state-of-the-art LLMs are capable of attributing source code authorship across different programming languages. Specifically, they offer promising performance in determining whether two source codes are written by the same individual with zero-shot prompting, achieving Matthews Correlation Coefficient (MCC) score up to 0.78; they can also attribute code authorship from a small group of reference code snippets through few-shot in-context learning, achieving MCC up to 0.77; and, they also offer a degree of adversarial robustness against state-of-the-art misattribution attacks. Despite these capabilities, we observed that na\u00efve prompting of LLMs in code authorship attribution does not scale against the number of authors due to the LLMs' inherent input token limitations. To circumvent this limitation, we propose a simple but effective tournament-style approach to leverage LLMs for code attribution over a large number of authors. We evaluate the approach on datasets written in C++ (500 authors, 26,355 code samples) and Java (686 authors, 55,267 code samples), crawled from Github as of November 2024. The results show that the proposed approach can accurately attribute code authorship even in real-world, few-shot settings achieving classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference code per author. These findings open new avenues for leveraging LLMs in code authorship attribution tasks with applications in cybersecurity and software engineering.", "sections": [{"title": "I. INTRODUCTION", "content": "Source code authorship attribution is the task of determining the author(s) of a piece of source code written in a specific programming language [1]\u2013[4]. It has received wide attention for its potential use cases in software engineering and security. For instance, it can be used to safeguard software intellectual property against copyright infringement [5] and ensure the integrity and authenticity of code modifications throughout the software life cycle [6]. From the security perspective, source code authorship attribution can be used to trace and find the programmer(s) of malicious code, thereby assisting in software forensics for cybercrime investigations and also prevention of further threats [1], [7].\nClassical methods for code authorship attribution mainly rely on machine learning (ML) and deep learning (DL) tech- niques in order to analyze linguistic and structural charac- teristics of the source code [6], [8]\u2013[10]. However, the use of supervised ML/DL methods necessitates a costly and time- consuming training process involving a vast amount of author- labeled training data to attain acceptable accuracy. The trained models' performance is also closely tied to the quality and coverage of the training data, e.g., the distribution of authors and programming languages available in the data [8], [9]. These challenges limit the generalization ability of pre-trained ML/DL models, especially on unseen authors and languages, where they cannot produce any classification result.\nRecently, large language models (LLMs) such as Gem- ini [11], ChatGPT [12], and Llama [13] have gained widespread popularity as foundation models in many appli- cations because of their user-friendly conversational inter- face and impressive human-like language capabilities [14]. These LLMs benefit from large model sizes and extensive training processes, and they have demonstrated remarkable performance in handling diverse domain-specific tasks such as natural language translation [15], creative writing [16], and source code comprehension and generation [17], [18].\nIn this work, we are inspired by the recent advancement in natural language authorship analysis brought about by LLMs [19], and we seek to leverage LLMs to address the exist- ing technical challenges of source code authorship attribution tasks. To this end, we present the first empirical study covering four mainstream LLMs families namely ChatGPT [12], Gem- ini [11], Mistral [20], and Llama [13], to explore whether they are capable of source code authorship attribution tasks given little or even no author-labeled references; these are considered to be particularly challenging settings for traditional ML/DL methods. Specifically, we investigate the various LLMs' ca- pacity for determining whether two code samples are written by the same author using a zero-shot query (RQ1) and for classifying the authorship of a code sample based on a small set of author-labeled code as reference through few-shot in-context learning (RQ2). The results of our empirical study demonstrate promising capabilities in state-of-the-art LLMs for code authorship attribution tasks.\nThroughout the empirical study, we also observe that LLM- based authorship attribution by na\u00efve prompting techniques does not scale against the number of candidate authors due to the inherent input token limitations of LLMs. To circumvent this challenge, we propose a simple but effective tournament prompting approach to perform attribution analysis across multiple rounds (RQ3). To avoid potential dataset leakage in the LLMs' training data (which may unintentionally boost our results), and to test our approach against a large num- ber of authors on real-world data, we constructed datasets by crawling public GitHub repositories dated between May and October 2024, and ran our experiments in November 2024. Our evaluation in this setting shows that the proposed tournament-style authorship approach can accurately attribute code authorship on a large scale with few-shot prompting the approach reaches a Top-1 accuracy of 65% when classifying over 500 C++ author candidates, using only a single reference code sample per author.\nAnother important application of authorship attribution is in code forensics, where an effective attribution solution must ac- count for adversarial settings. However, existing ML/DL-based approaches have been shown to be vulnerable to misattribution attacks [21], [22]. Therefore, we assess the robustness of LLMs against state-of-the-art adversarial misattribution attacks (RQ4). Here, we find that the tested LLMs offer a promising level of robustness without the need to tailor or fine-tune our prompts. This robustness can be further enhanced using adversarial-aware prompting.\nFinally, whereas ML/DL approaches need to be re-trained for each new programming language [8], [9], we carry out additional evaluations to explore whether LLMs can gen- eralize their authorship attribution capabilities to different languages (RQ5). Our results show that the tested LLMs can be readily applied to a different programming language (Java) with unchanged prompts; our proposed tournament prompting approach for large-scale problems is also language-agnostic.\nWe summarize the contributions as follows:\n\u2022 In Section III, we design and present an empirical study of code authorship attribution capabilities for state-of-the- art LLMs. Our study includes both zero-shot and few-shot prompting methods to perform various authorship attri- bution tasks. The results demonstrate LLMs' promising capabilities in code authorship tasks without reliance on extensive datasets and expensive training processes.\n\u2022 In Section IV, we propose a tournament-style approach to address the inherent input token limitations of LLMs. Our approach takes advantage of few-shot in-context learning to precisely attribute code authorship on a large scale.\n\u2022 In Section V, we examine the robustness and general- ization of LLM-based code authorship by, respectively, testing against state-of-the-art misattribution attacks and evaluating our approach on different programming lan- guages. We demonstrate robustness and generalization capabilities without the need to tailor or tune our prompts.\nBeyond answering the above-mentioned research questions, we discuss further insights from the evaluation and provide potential future directions for improvements for our LLM- based approach. These findings open new avenues for lever- aging LLMs in source code authorship attribution tasks with potential applications in broader areas of software engineering and cybersecurity."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Early research in source code authorship attribution focused on the automatic evaluation of students' programming assign- ments [23] and characterizing the authors of programs [24]. Later developments demonstrated a wider range of applica- tions such as safeguarding software integrity and security [5]\u2013 [7], malicious code attribution and forensics [7], and identifica- tion of legitimate code owners against copyright infringement and plagiarism [6], [25], [26]. Kalgutkar et al. [1] provide a comprehensive review of existing approaches for source code authorship attribution and key challenges in the field.\nSeveral studies have explored the task of identifying code authors using ML/DL techniques. For instance, Caliskan-Islam et al. [9] propose an ML-based approach to pinpoint anony- mous programmers by studying their coding style, also known as code stylometry. Their solution involves training an ML model on extracted features from the source programs' abstract syntax tree (AST) to capture stylistic patterns in C/C++ code. By employing methods like random forests, they achieve a remarkable 94% accuracy on a large Google Code Jam (GCJ) dataset featuring 1,600 programmers and 98% accuracy on a smaller dataset of 250 programmers, surpassing previous code stylometry research. Bogomolov et al. [6] explore path context in AST source code representations and propose an attribution model based on random forests and deep neural networks to characterize and identify authors. Li et al. [27] study the interpretability of authorship attribution classifiers through the lens of Siamese neural network models. Abuhamad et al. [8] leverage recurrent neural networks to analyze code structure and propose an authorship attribution model, which is effective irrespective of programming language, achieving over 90% accuracy on large datasets with thousands of programmers. Li et al. [22] study adversarial training to produce a robust attri- bution model against malicious misattribution attacks. These prior ML/DL-based approaches face two common challenges that we seek to circumvent in this work: their reliance on extensive manually labeled datasets for model training and the resulting models' lack of generalization capability against unseen authors and programming languages."}, {"title": "B. Large Language Models in Software Engineering", "content": "LLMs are a type of artificial intelligence (AI) application that can handle complex tasks like recognizing and generating text, source codes, images, and even videos. This is achieved through training on massive datasets of text and code snip- pet [28], [29], which empower the models with capabilities for various tasks without task-specific training, e.g., creating different creative text formats, writing codes, and answering questions in informative ways [14], [30].\nState-of-the-art LLMs, such as ChatGPT and Llama, have shown great potential in programming language processing (PLP) tasks. For example, LLMs have been leveraged in code comprehension and generalization [31]\u2013[33] and program testing [34], [35]. In addition to general-purpose LLMs, there are also models that are specially trained or fine-tuned with domain-specific datasets [36]\u2013[38]. For example, Codex is an adaptation of GPT-3 tailored for programming tasks [36], which was developed to aid developers by suggesting code snippets, completing code lines, and generating entire func- tions based on comments or partial code. CodeBERT [37] is a transformer-based model specializing in natural language understanding and generation according to the coding contexts, specifically for tasks like code summarization and documenta- tion. BERT4Bugs [38] is another transformer-based model that utilizes BERT architecture to automatically identify and fix bugs in programming code, leveraging large datasets of buggy and corrected code examples. In this work, we use state-of- the-art general-purpose LLMs for our empirical study, leaving code authorship task-specific fine-tuning out of scope."}, {"title": "C. Authorship Analysis with Large Language Models", "content": "A closely related task to our present study is the use of LLMs for natural language authorship analysis. Huang et al. [19] investigated the capability of LLMs in authorship analysis for English texts. Their motivation comes from the increasing demand for precise text authorship identification, which is essential for tasks such as validating content authen- ticity (including detecting plagiarism) and combating the dis- semination of misinformation. Unlike conventional techniques which rely heavily on manually engineered stylistic features, Huang et al. demonstrate that existing LLMs are capable of performing authorship analysis tasks without additional training on a domain-specific training corpus. This suggests that LLMs can analyze stylistic characteristics within text data to distinguish between different authorship styles.\nInspired by the promising results of Huang et al. [19], we investigate LLMs for authorship analysis of source code. To the best of our knowledge, we are the first to systematically evaluate LLMs for source code authorship attribution."}, {"title": "III. EMPIRICAL STUDY", "content": "Our empirical study seeks to broadly explore the capability of the mainstream LLMs for source code authorship attribu- tion. An overview of the LLM query pipeline is shown in Fig. 1. We will start by investigating RQ1 and RQ2."}, {"title": "A. Experiment Setup", "content": "Model Selection and Experiment Environment: Our em- pirical study covers four mainstream LLM families, namely OpenAI's GPT [12], Meta's Llama [13], Mistral [20], and Google's Gemini [11]. The GPT and Gemini models are closed-source, so we conduct experiments using their official APIs. For the remaining two model families (Llama and Mistral), we use the latest versions as of May 2024 and ran our experiments on a workstation with Ubuntu 22.04 LTS OS, an Intel Xeon Platinum 8368Q CPU, and an NVIDIA RTX A100 80GB GPU. Overall, we ran experiments on eight LLM models, including GPT 3.5 Turbo, GPT 40, Llama2 Chat 7B, Llama2 Chat 70B Quantized (GPTQ), Llama3 8B, Llama3 8B Instruct, Mistral2.5 7B Quantized (GPTQ), and Gemini 1.5 Pro.\u00b9 Following Huang et al. [19], we set the values of temperature to 0 and top_p to 1 for all models. All other hyperparameters are set at their default values.\nDatasets: For this study, we experimented with two datasets to answer our RQs. The first data set was taken from Google Code Jam (GCJ)\u00b2 [39] and the second was a dataset obtained by extracting GitHub repositories that met our criteria (detailed below). We conducted our small-scale experiments (cf. RQ1 and RQ2) using the GCJ 2017 dataset, which is commonly used in research on ML/DL techniques for code authorship attribution [21]. Since C++ represents the largest portion of authors in the GCJ dataset, we focused our evaluation on the C++ subset, which includes 1,632 code samples from 204 authors. Later, we also evaluate generalization to other programming languages (cf. RQ5), for which we used the GCJ Java subset containing 2,202 code samples from 74 authors. For our large-scale experiments (cf. RQ3), we additionally crawled code from public GitHub repositories. For the crawl- ing process, we restricted the collection to repositories with a single contributor, containing more than eight C++ code files, ranging from 17 to 300 lines of code, and committed between May and October 2024. This resulted in 26,355 code samples from 500 authors. Similarly, we crawled Java code from public GitHub repositories in the same period, gathering 55,267 code samples from 686 authors. An overview of both the GCJ and Github datasets is given in Table I.\nMetrics: We use the Matthews Correlation Coefficient (MCC) [40] as the main evaluation metric. Its definition takes into account both correct, i.e., true positive (TP) and true negative (TN), and incorrect, i.e., false positive (FP) and false negative (FN) predictions, as shown below:\n$MCC = \\frac{(TP \\times TN - FP \\times FN)}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FP) \\times (TN + FN)}}$"}, {"title": "B. Prompting Strategies", "content": "The narration and complexity of prompts, so-called prompt engineering [42], [43], play a vital role in determining LLMs' performance and effectiveness. We crafted three types of prompts, namely simple, detailed, and complex prompts, to explore how different levels of instructive detail in a prompt can influence outcomes in code authorship tasks.\nSimple Prompts: Simple prompts aim to provide the LLMs with instructions in the most direct and straightforward man- ner. These prompts are devoid of any additional context or guidance and simply describe the task at hand, e.g., the prompt \u201cIdentify the author of the following code snippet from among these candidates.\u201d may be used for authorship attribution. After being instructed with a simple prompt, LLMs respond by relying on their pre-trained capabilities and a general understanding of the query, code, and language patterns.\nDetailed Prompts: Moving a step up in prompt sophistica- tion, detailed prompts are designed to include specific features that could be pertinent to identifying code authorship. These prompts can offer the LLMs more context and background about what aspects of the code might be relevant. Based on prior work [8], [9], we identified three types of features that can be used for code authorship attribution purposes, namely layout features, lexical features, and syntactic features. Accordingly, LLMs are instructed to analyze these features as part of our detailed prompts.\nComplex Prompts: Complex prompts go beyond detailed prompts by incorporating an even richer set of features and context. Here, we leveraged ChatGPT to ask for as many source code characteristics and features as possible, covering a broad range of specific stylistic and structural elements, e.g., commenting style, indentation patterns, and the frequency of specific functions or libraries used. By incorporating these instructions in our prompts, we aim to push the LLMs towards utilizing a more comprehensive array of information and test their ability to integrate and analyze diverse code features to determine authorship accurately."}, {"title": "C. RQ1: Code Authorship Verification with Zero-Shot Prompts", "content": "Our first set of experiments is designed to examine LLMs' capabilities for determining whether two code samples were written by the same author, a task known as code authorship verification. Here, we adopt a zero-shot prompting approach to assess the eight LLMs; specifically, we randomly sampled a test set consisting of 100 code pairs belonging to the same author for different tasks and another 100 code pairs belonging to different authors (also for different tasks). Each LLM is given either a test \u201csame author\u201d pair or a \"different author\" pair and prompted to answer whether they were written by the same author according to three prompt templates of increasing prompt complexity (P1-P3), as shown in Fig. 2."}, {"title": "D. RQ2: Code Authorship Attribution with Few-Shot, In- Context Learning Prompts", "content": "Following the code authorship verification task, we con- ducted experiments with code authorship attribution using LLMs, focusing attention on Gemini-1.5-p and GPT-40 as they were the most promising models from RQ1.\nFor code attribution, we supplied a collection of reference code samples for each candidate author to the LLM as part of our prompts; this is followed by a query code sample for which the LLM must attribute to the most likely candidate author from its reference set (or reply that none of the authors match). The three prompt templates of increasing prompt complexity (P1-P3) are shown in Fig. 3.\nWe refer to this setting as \u201cfew-shot\u201d because we provide n samples per candidate author (n is between one to three) and \"in-context learning\" because the LLM must learn to classify the query from the provided reference samples. We experimented with the number of candidate authors k set to 3, 5, 7, or 10. For each parameter choice of n and k, we randomly sampled 100 in-distribution cases, where the queried author is in the candidate set; and 100 out-of-distribution cases, where the queried author is not present in the candidate set. In either case, the queried author's code sample is from an unseen task. We present the accuracy and MCC scores in Tab. IV. For the in-distribution cases, we regard an answer as TP if it correctly identifies the author from the candidate set; conversely, it is FN when the LLM incorrectly returns that the queried author is not among the candidates or when the returned ID is incorrect. For the out-of-distribution cases, FP and TN are defined as usual. Due to the input token limitations for LLMs, we were unable to conduct experiments with ten candidate authors in the three-shot setting. Nevertheless, based on the other results, we may infer that outcomes in this setting will be similar.\nDiscussion: We observe that Gemini-1.5-p always outper- forms GPT-40 on both accuracy and MCC for the attribution task across all variations of parameters (number of candidate authors, number of author samples, and prompt complex- ity). Across one-, two-, and three-shot settings, Gemini-1.5-p achieved strong accuracy scores of 85.5%, 88.5%, and 87.0%, respectively. While we cannot directly compare this perfor- mance with traditional approaches (see Baseline in Sec. III) due to the limited number of few-shot author samples we provided, it remains impressive for the attribution task. It may be possible that further LLM-specific prompt engineering could improve the scores for GPT-40-note that GPT-40 performed significantly better than a random guess.\nThere are also several unexpected observations from these tables. First, we expected that increasing the number of candi- date authors (e.g., from 3 to 10) would make the tasks increas- ingly harder for LLMs. For example, Gemini-1.5-p always had the best results against 3 authors. However, the remaining experimental results do not always show such a clear trend. Second, we also expected that increasing the number of code samples per author would make the tasks simpler for LLMs. This turned out to also not be the case-there were significant variations, again, with no clear trends. Lastly, whereas the best prompt for GPT-40 was always P1, the best prompt for Gemini-1.5-p ranged from P1 to P3 for different numbers of samples. Overall, these unexpected variations indicate that a larger test setup and (costly) hyperparameter search may be necessary if one would like to deploy LLMs with optimal sizing parameters for the attribution queries."}, {"title": "IV. SCALING AUTHORSHIP ATTRIBUTION WITH TOURNAMENT PROMPTING", "content": "A common issue we faced in the empirical study (Sec. III) was the inherent token limitations of current LLMs in process- ing lengthy inputs. This became particularly pronounced when we tried to perform code attribution for a large number of ref- erence author codes simultaneously. In RQ3, we propose and evaluate a tournament approach that splits the attribution task across many prompts, thus circumventing the input limitation. An overview of our approach is in Fig. 4."}, {"title": "A. Prompting Methodology", "content": "The tournament authorship attribution process involves the following steps and as shown in Alg. 1:\nInitial Author Pool Selection. Given a large pool of candidate authors $A = \\{a_1,a_2,..., a_n\\}$, and a target code snippet T, we partition the authors into smaller, evenly-distributed subsets (e.g., sample size 12).\nSubset Attribution. For each subset, we conduct an author- ship attribution query similar to Sec. III-D, where the goal is to determine the likelihood of each candidate being the author of T. The comparisons are made by formulating attribution prompts that include the target code snippet and code samples from each candidate author in the subset.\nSubset Winner Selection. From each subset, the author with the highest likelihood (HL) is selected to proceed to the next round. This can be formalized as:\n$winner = arg \\underset{\u03b1 \\epsilon A_i}{max} HL(T, a)$\nwhere $A$ is the i-th subset of authors and HL(T, a) represents the likelihood given by the LLM for author a being the writer of code snippet T.\nIterative Rounds. The winners from each subset form a new pool of candidates. This process is iterated, reducing the pool size with each round.\nFinal Attribution. In the final round, the remaining authors are compared directly, and the one with the highest HL score is attributed as the author of the target code snippet."}, {"title": "B. RQ3: Large-scale Code Authorship Attribution with Tournament Prompting", "content": "We evaluated the feasibility of large-scale code authorship using our proposed method against the Github C++ suite of 500 authors. As before, we used the best performing models, Gemini-1.5-p and GPT-40, for the tournament experiments. To facilitate this experiment, we picked a random test set containing 300 random query samples and a corresponding reference set of one-shot reference code samples for each of the 500 authors. For the tournament, we use author subsets of at most size 12, which fits well into the input token window for the LLMs. This leads to a total of 4 tournament rounds (500 \u2192 42 \u2192 4 \u2192 1). Given that simple prompts yielded the most consistent performance in previous experiments, we opted to use only the simple prompt here. The prompt template we used for this experiment is displayed in Fig. 5.\nResults are presented in Tab. V. The table shows the accuracy at each round (the second, third, and final round), i.e., whether the query sample's author was still present (not eliminated) at that round. Intuitively, later rounds should be more difficult because the candidate authors that survived to these rounds ought to have higher similarity to the query code.\nDiscussion: As before, GPT-40 had slightly weaker per- formance, except in the final round. In a sense, the initial round is an \u201ceasy\u201d authorship task, and our results confirm again that the LLMs have strong capabilities in these smaller- scale code authorship tasks. However, there is a clear drop in performance for the latter rounds. This is to be expected because the remaining candidates' perplexity likely increased because the winners of each round are increasingly similar to the query code. Nevertheless, the final round Top-1 accuracy achieved by both models shows that tournament prompting does indeed work for scaling up code authorship attribution to real-world data.\nWe further observed that, unlike in previous experiments (zero- and few-shot), Gemini-1.5-p and GPT-40 exhibited comparable performance on the real-world GitHub dataset. This may suggest that the models' performance in zero- and few-shot scenarios depends on their training data, the kinds of code sample (algorithmic tasks in GCJ vs. unconstrained code from Github), and that their performance may be affected by potential dataset leakage (for GCJ)."}, {"title": "V. ROBUSTNESS AND GENERALIZATION", "content": "In this section, we study success criteria for LLM code authorship beyond accuracy metrics, namely RQ4 their ro- bustness against adversarial code modifications; and RQ5 their generalization capability to different programming languages."}, {"title": "A. RQ4: Robustness against Adversarial Threats", "content": "To evaluate LLM's robustness against adversarial threats, we tested adversarial code authorship attacks by Quiring et al. [21] and Li et al. [22]. The former reported 77.3% and 81.3% attack success rates on baseline models [8], [9], while the latter reported 94.8% success rate against [8]. We used the GCJ 2017 dataset (C++, 204 authors) to generate modified codes intended to deceive authorship attribution models. This dataset was chosen exclusively for this experiment as the modification results had been validated in prior studies. We conducted a zero-shot experiment in two settings: \u201cSame\u201d and \u201cDifferent\u201d, similar to the previous setup (see Sec. III-C) to assess the LLMs' resilience to these attacks.\nThreat Model: In the \u201csame author\u201d settings, we provided LLMs with two code snippets: a code originally written by the author A for the task X, denoted as $T_{A,X}$, and a modified version of A's code for the task Y but styled like a different author B, denoted as $f_B(T_{A,Y})$. The transformation $f_B(\u00b7)$, aims to mislead the attribution model into misattributing the code to B, constituting an evasion attack. In the \"different author\" setting, we supplied two codes to the tested LLM: a code originally written by the author A for the task X, denoted as $T_{A,X}$, and a modified version of B's code for the task Y written in the style of the author A, denoted as $f_A(T_{B,Y})$. This transformation aims to mimic the coding style of A to mislead the attribution model, constituting an imitation attack. We used code transformations based on MCTS [21] and RoPGen [22].\nAdversarial-Aware Prompt: We explored adversarial-aware prompting, where the prompt suggests that the code samples might be altered by evasion or hiding attacks. On the basis of our proposed prompt templates (see Sec. III-B), we added a note after the task description indicating that some code samples might be modified: \"Note that some code samples might have been modified using evasion or hiding techniques to alter their stylistic features. Be mindful of these potential modifications and focus on underlying patterns and author- specific traits that remain consistent despite such alterations.\""}, {"title": "B. RQ5: Authorship Attribution with Different Languages", "content": "We selected Java as an alternative programming language to test LLM code authorship attribution capabilities; Java is the second most commonly used programming language in the GCJ dataset. We repeated our experiments using the CGJ 2017 Java dataset [39] in both the zero-shot and few-shot settings. We also ran the tournament prompting experiments with the GitHub Java dataset, following the same approach as in the C++ experiments. Our zero-shot experiments cover the four best-performing models observed from the C++ authorship verification task (RQ1, see Sec. III-C), the few- shot experiments focus on assessing Gemini-1.5-p and GPT-40 models, as before (RQ2, see Sec. III-D), and the tournament experiments utilize Gemini-1.5-p and GPT-4o models, same as (RQ3, see Sec. IV-B). We only used simple prompt P1 in this set of experiments, as the majority of best performance records were obtained through P1 in previous experiments."}, {"title": "VI. DISCUSSION", "content": "Our experiments reveal several intriguing insights into the behavior and performance of LLMs in code authorship tasks, shedding light on both their capabilities and limitations.\nFirst, we observed that providing more complex guidance in prompts or increasing the number of samples does not always lead to better results (see Sec. III-D). For instance, while Gemini-1.5-p performed well with minimal examples in the few-shot settings, its performance declined as the number of references (per author) increased. This suggests that LLMs can suffer from context overload, where the additional information does not enhance and may even hinder the model's ability to accurately attribute code authorship. A deeper study of the answering consistency (across rounds) and numerical authorship likelihood capabilities of LLMs could give further insights into such differences.\nSecond, our experiments underscore the importance of prompt engineering in the deployment of LLMs (see Sec. III, IV, and V). Crafting precise and effective prompts can sig- nificantly influence the model's performance, especially in context-sensitive tasks (e.g., code authorship). This highlights a critical area for future research and development (i.e., optimizing prompt strategies to enhance the performance of LLMs in various settings).\nThird, the varying resilience to adversarial attacks between models indicates a need for further advancements in making LLMs more robust in these tasks (see Sec. V-A). While GPT-40 showed greater resilience compared to Gemini-1.5- p, both models exhibited vulnerabilities that could potentially be mitigated through improved prompt design and adversarial prevention (e.g., adversarial fine-training).\nFourth, for LLMs such as GPT-40 and Gemini-1.5-p, we utilized their commercial APIs for authorship queries, which incurred additional expenses. To control such costs in practice, it is important to understand the unit cost of finding a single Top-1 result, i.e., given one target code by an unknown author, and one reference code for each of the candidate authors in the database, return the author attributed via tournament prompting. For our tournament experiment on the Github C++ dataset (500 authors), the unit cost was approximately USD 1.58 for GPT-40 and USD 1.60 for Gemini-1.5-p. Similarly, for the GitHub Java dataset (686 authors), the cost for each result was approximately USD 1.52 for GPT-40 and USD 1.53 for Gemini-1.5-p.\nIn summary, our findings show that LLMs can be successful in code authorship attribution tasks. However, their ultimate utility in real-world deployment will hinge on costs, sophis- ticated prompt engineering, model selection, hyperparameter choices, and an understanding of the models' sensitivity to context and adversarial conditions."}, {"title": "B. Threats to Validity", "content": "A notable limitation of this work is the potential exposure of the GCJ dataset to the LLMs used in this experiment. Since GCJ is a public dataset, it is conceivable that these models may have already been seen and trained as code samples. In that case, although the tested general-purpose LLMs may not train for specific authorship attribution purposes, these models' performance may still be artificially inflated as they can learn some patterns or authors' coding styles during the training. Thus, we carried our our tournament prompting experiments using freshly crawled Github datasets (see Table I). Based on publicly known information, the code in this dataset does not overlap with LLMs' training data. Our experiments on this latter dataset shows that LLMs have remarkable performance in large-scale C++ and Java code attribution. However, their generalization capabilities to less popular or even new pro- gramming languages remains to be seen."}, {"title": "C. Future Work", "content": "For future work, one can consider cross-language authorship attribution, e.g., attributing a query Java code against C++ references. This can provide further insights into LLMs' generalization capabilities. This line of research can also delve into creating fine-tuned models capable of understanding and linking coding styles across languages, which would be possible with open-source LLMs. Additionally, enhancing the explainability and interpretability of LLM predictions can build trust and provide deeper insights into their authorship attribution criteria. This will involve researching methods to make model decisions more transparent and understandable to users, including methods such as attention visualization [44]."}, {"title": "VII. CONCLUSION", "content": "We conducted various experiments to evaluate the capabili- ties of LLMs in code authorship attribution tasks, including zero-shot, few-shot, and tournament scenarios. Our study shows that some state-of-the-art LLMs have latent capabil- ities for code authorship attribution without the need for further task-specific fine-tuning. Moreover, these capabilities are robust against adversarial evasion and hiding attacks and generalize across languages to both C++ and Java. These insights open new avenues for future research, mining to refine prompt strategies and enhance the robustness of LLMs in diverse and complex scenarios."}]}