{"title": "CEGI: MEASURING THE TRADE-OFF BETWEEN EFFICIENCY AND\nCARBON EMISSIONS FOR SLMS AND VLMS", "authors": ["Abhas Kumar", "Kapil Pathak", "Rajesh Kavuru", "Prabhakar Srinivasan"], "abstract": "This paper analyzes the performance of Small Language Models (SLMs) and Vision Language\nModels (VLMs) and evaluates the trade-off between model performance and carbon emissions across\n4 essential tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization and\nText-to-SQL conversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of parameters, quantization\nlevel and fine-tuning parameters are evaluated. The model variants' performance and carbon emissions\nare calculated. To quantify the trade-off between model performance and carbon emissions, we\nintroduce a novel metric called CEGI (Carbon Efficient Gain Index). This metric represents the\ncarbon emission per unit percentage gain per million trainable parameters. This metric provides a\nnormalized measure to compare models' efficiency in terms of performance improvement relative to\ntheir environmental cost. The experiment's outcome demonstrates that fine-tuning SLMs and VLMs\ncan achieve performance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal gains in accuracy from\nlarger models do not justify the substantial increase in carbon emissions. Leveraging lower-bit\nquantization levels, the proposed metric further enhances energy efficiency without compromising\nperformance. This study highlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for environmentally-friendly AI development.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements of Large Language Models (LLMs) and Vision Language Models (VLMs) have significantly\nadvanced capabilities in complex tasks such as Visual Question Answering (VQA), Image Captioning, Dialogue Sum-\nmarization, and Text-to-SQL Conversion. Models like Qwen and LLaMA have demonstrated remarkable performance\nin these domains. Huge computational requirements for pre-training and fine-tuning LLMs produces significant carbon\nemissions, raising environmental concerns. This study focuses on quantifying these emissions and assessing methods to\nmitigate them, addressing the need for efficient model adaptation that balances accuracy with reduced environmental\nimpact.\nFine-tuning LLMs for domain-specific applications has become standard practice in Generative AI, allowing models\nto achieve high task-specific accuracy. However, recent studies highlight that this process consumes significant\ncomputational resources, leading to substantial carbon emissions. Strubell et al. (2019) [1] and Patterson et al. (2021)"}, {"title": "2 Related work", "content": "The effectiveness of LLMs in a variety of Natural Language Processing (NLP) tasks has firmly established their position\nas pivotal assets in modern machine learning (Brown et al., 2020)[4]. However, as these models scale up in complexity\nand are trained on increasingly larger datasets, the energy demands and environmental impact of deploying and operating\nthem have intensified (Anil et al., 2023)[5]. With wide adoption of LLMs, integrated into many mainstream applications,\ntheir significant energy consumption has led to growing concerns about sustainability (Thompson et al., 2021)[6]. For\nexample, creating a 213-million-parameter transformer model through neural architecture search can produce CO2\nemissions equivalent to those generated by five cars over their entire lifespan (Strubell et al., 2019)[1].\nWhile previous research has primarily concentrated on measuring the environmental impact of training large ML\nmodels, notably focusing on carbon emissions during this phase, it has tended to overlook the footprint associated with\ninference and fine-tuning. Research efforts, including those by Henderson et al. (2020)[7], Wu et al. (2022)[8], Dodge\net al. (2022)[9], and Strubell et al. (2019)[1], have highlighted the substantial carbon emissions resulting from the\ntraining of large-scale models.\nOur approach diverges by capturing carbon emissions across the entire lifecycle of model deployment, including\ntraining, fine-tuning, and inference. This study employs the eco2AI library (Budennyy et al., 2022)[10] to achieve\ndetailed tracking of power consumptions and associated CO2 emissions, ensuring accurate estimates across various\nhardware configurations and geographical emissions profiles. Unlike most of the other tools, eco2AI allows for nuanced\ntracking, accounting for hardware diversity and regional differences in emission factors, thus offering a precise measure\nof the environmental impact in terms of carbon emissions associated with LLM pretraining, fine-tuning, inference.\nBy evaluating carbon emissions across a range of downstream NLP tasks, like Visual-QA, Image Captioning, Text-To-\nSQL and Multiturn Dialogue Summarization, and implementing parameter-efficient strategies such as LoRA, this study\naims to inform on sustainable AI practices. The findings aim to guide the development of computationally efficient\narchitectures that maintain robust performance, advancing the broader objective of Green AI."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Eco2\u0391\u0399", "content": "Eco2AI [11] is an open-source library developed to help the data scientist measure the equivalent carbon emissions and\npower consumptions that occurred during the training or fine-tuning the deep neural networks. This library focuses on\nthe calculation of electric energy consumption, extracting energy emission coefficients, and estimating the equivalent\nCO2 emissions. Eco2AI is able to detect GPU processes with the help of GPU management and monitoring functions\nimplemented as a Python interface within the Pynvml library. The CPU utilization is monitored by os and psutil\nlibraries. The emission intensities differ according to climate change, the region, the type of fuel used in that country,\nthe economic and technological development of that region, etc. All these regional dependencies are accommodated\nby the emission intensity coefficient \u03b3, which is the weight of CO2 (in Kg) per each Mega Watt Hour (MWh) of the\ncountry. The final emission value as a carbon footprint of the ML processes is calculated by the following formula:\nCF = \u03b3 * PUE * (ECPU + EGPU + ERAM)\nwhere CF is a carbon footprint of the ML process, PUE is the power usage effectiveness of the data center if any kind\nof cloud is used in the process. ECPU, EGPU, and ERAM are the power consumption values from CPU, GPU, and\nRAM, respectively."}, {"title": "3.2 Low Rank Adaptation (LoRA)", "content": "Most of the applications in NLP rely on adapting the pre-trained model, which is trained on large-scale data and on\nspecific tasks with limited task-specific data. The resulting fine-tuned model also consists of the same number of\ntrainable parameters as the original pre-trained model. This kind of operational inefficiency can be handled by adding a\nsmall number of trainable parameters to the non-trainable weights of the original pre-trained model. In these efforts,\nEdward et al. [3] proposed a novel method Low-Rank Adaptation (LoRA) which is inspired by the hypothesis [12], [13]\nthat the trained over-parametrized models actually can be explained by the lower intrinsic dimension. They propose that\nthe weight changes in the fine-tuning process also reside in the low intrinsic space. LoRA allows us to train only a\nsmall number of dense layers by optimizing the rank-decomposition matrices of the trainable dense layers and keeps\nmost of the weights of the pre-trained model frozen.\nFor a pre-trained weight matrix Wo \u2208 Rd\u00d7k, the weight updates are represented as \u2206W, which can be decomposed\ninto low-rank matrices A \u2208 Rr\u00d7k and B \u2208 Rd\u00d7r, the rank r < min(d,k).\nWo + AW = Wo + BA\nBy this approach, we can share the large pre-trained model among different tasks and fine-tune many task-specific small\nLORA modules. LoRA makes fine-tuning more hardware efficient as we don't need to calculate the gradients and store\nthe optimizer states for all parameters of the pre-trained model. Here, both matrices Wo and AW are multiplied by the\nsame input. The corresponding output representations are summed up, and the resulting forward pass for input x can be\ngiven below.\nh = Wox + \u2206Wx = Wox + BAx"}, {"title": "3.3 Visual Question Answering", "content": "Visual Question Answering (VQA) is the task of answering open-ended questions based on the given image. In this task,\nwe give an image and a question as input to the model and get an answer in the form of text as an output. In this task, the\nmodels are expected to understand the content of the image and the semantics of the questions. The interaction between\nnatural language and the image content should enable the model to respond to the questions effectively. TY Lin et al.\n[14] presented the dataset MS-COCO for various image understanding tasks. In their work, Zhan et al. [15] proposed a\nnovel conditional reasoning method for the medical visual question-answering task. Biten et al. [16] presented a new\ndataset ST-VQA, which aims at exploiting the high-level semantic information given in the images through textual cues\nin the VQA task."}, {"title": "3.4 Image Captioning", "content": "Image Captioning is another task that involves an image as an input but expects the natural language as an output.\nIn this task, the model aims at describing the content of the image. MS COCO [14] is one of the first notable image\ncaptioning datasets in this domain. The image captioning models are based on an encoding-decoding framework where\nthe image as input is encoded into the intermediate representation by the encoder and decoded by the decoder into the"}, {"title": "3.5 Dialogue Summarization", "content": "The abstractive dialogue summarization is a text-to-text generation task where the input is a multi-turn conversation and\ngenerates its summary. There are various benchmark datasets such as SAMSum [22], DialogSum [23], MediaSum [24]\netc. The SAMSum [22] dataset consists of short conversations of leisure chitchat. DialogSum dataset [23] comprises\nthe conversations in the real-world scenarios. MediumSum [24] consists of large-scale media interviews with 463.6K\ntranscripts. Traditionally, text-to-text tasks such as dialogue summarization follow the encoder-decoder architecture,\nwhere the input text is encoded into a context vector and decoded by the decoder in the form of output text."}, {"title": "3.6 Text-to-SQL Generation", "content": "Text-to-SQL generation is a task in natural language processing whose aim is to generate SQL queries based on the\nnatural language. Yu et al. [25] presented Spider, a large-scale complex and cross-domain text-to-SQL dataset that\nbecame one of the first benchmarks for text-to-SQL tasks. Lee et al. [26] introduced KaggleDBQA, a domain evaluation\ndataset of real Web databases. This dataset is also featured with domain-specific data types, original formatting, and\nunrestricted questions."}, {"title": "4 Methodology", "content": "This section mainly outlines the experimental framework we employed to investigate the trade-offs between carbon\nemissions and model performance in fine-tuning large and small language models across four downstream NLP tasks.\nThe methodology includes details of the datasets, models, fine-tuning techniques, and carbon emission tracking tools\nused. Each component is carefully designed to ensure the reproducibility and rigor of the experiments."}, {"title": "4.1 Datasets and Tasks", "content": "Our study focuses on four tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization,\nand Text-to-SQL, selected for their diversity in modality and complexity to serve text-to-text and image-to-tasks. The\ndata sets used for each task are curated to reflect real-world domain adaptation scenarios, ensuring the robustness of the\nanalysis. Appendix section A provides additional information on all datasets used."}, {"title": "4.2 Model Configurations", "content": "We utilize diverse models for carbon tracking experiments to investigate the relationship between model size, perfor-\nmance, and emissions across tasks. For Dialogue Summarization and Text-to-SQL, we employ the Qwen2.5 instruct"}, {"title": "4.3 Fine-Tuning Technique", "content": "We have used Low-Rank Adaptation (LoRA) to fine-tune models efficiently. As known LoRA introduces low-rank\nmatrices into existing layers, enabling domain adaptation with minimal parameter updates. This method reduces the\ncomputational cost associated with full fine-tuning while maintaining task-specific accuracy.\nTo evaluate the trade-offs between performance and resource usage, we experiment with four LoRA rank configurations\n(Lr = 4, 8, 16, 32). Additionally, quantization Quis applied to compress model parameters to 4-bit and 8-bit precision,\nfurther reducing computational overhead and energy consumption. These configurations enable a detailed analysis of\nparameter-efficient fine-tuning across various scenarios."}, {"title": "4.4 Hardware Configurations and Carbon Emissions Tracking", "content": "All experiments are conducted on NVIDIA 1xH100(80 GB) GPU, selected for its high computational throughput and\nenergy efficiency. To measure the environmental impact, we utilize the Eco2AI library (Budennyy et al., 2022)[10], an\nopen-source tool for p.recise energy consumption and carbon emission tracking. Eco2AI integrates the following:\n\u2022 Hardware Metrics: Tracks GPU utilization, power consumption, and training duration.\n\u2022 Regional Emissions Factors: Incorporates electricity grid carbon intensity variations, ensuring context-aware\nemission estimates. This approach enables granular emissions measurement for fine-tuning and inference,\nproviding robust data for analyzing configuration trade-offs."}, {"title": "4.5 Evaluation Metrics", "content": "Performance is assessed using task-specific metrics to ensure relevance to each domain:\n\u2022 Image Captioning: SPICE[32] scores, which evaluate the semantic relevance of generated captions.\n\u2022 Visual Question Answering (VQA): Smoothed BLEU scores, capturing linguistic precision and fluency.\n\u2022 Dialogue Summarization: ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L), measuring n-gram overlaps\nwith reference summaries.\n\u2022 Text-to-SQL: Execution Accuracy (EA) and Valid Efficiency Score (VES), assessing the correctness and\nsemantic equivalence of SQL queries.\nSPICE evaluates semantic content by analyzing the alignment of objects, attributes, and relationships between reference\nand generated outputs, prioritizing semantic consistency over lexical similarity. Smoothed BLEU refines the traditional\nBLEU metric by addressing its limitations in handling exact n-gram matches and rare patterns. Smoothing techniques\nenhance reliability for evaluating shorter sequences and partial matches in text generation tasks. EA (Execution\nAccuracy) quantifies the correctness of generated SQL queries by comparing them to ground-truth outputs, with scores\nranging from 0 to 1, where 1 indicates complete accuracy. VES (Valid Efficiency Score) assesses the efficiency of\ngenerated SQL queries relative to reference queries, with values approaching 1 signifying equivalent or superior query\nexecution performance."}, {"title": "5 Experiments", "content": "For all tasks, fine-tuning was performed for 1 epoch to ensure uniformity across experiments. Each reported value,\nincluding performance metrics and emissions, represents the mean of 5 independent runs, ensuring stability and\nreliability in the results. Additionally, we used GPT-40 in a zero-shot setup as a baseline for comparative evaluation.\nCarbon emissions data for GPT-40 cannot be obtained since it is a proprietary model. The detailed prompts used for\nQwen2.5, Qwen-VL LLaMa3.2, and GPT-40, as well as fine-tuning configurations for each task, are provided in the\nprompt sectionB of the appendix for reproducibility. Note that the Base model(pre-trained) and fine-tuned models are\nrepresented as BM and Fr, carbon emissions CE in grams.\nT\nThe LoRA rank was varied across (Lr = 4, 8, 16, 32), allowing us to systematically study the trade-offs between the\nnumber of updated parameters and the resulting task performance with corresponding CO2 emissions. A dropout\nrate of 5 percent is applied to introduce regularization during fine-tuning stage, mitigating the potential for overfitting.\nThe target modules for LoRA focused on critical components of transformer architectures, including projection\nlayers ([Q, K, V, O])proj and gating mechanisms ([gate, up, down])proj. These layers are fundamental to the\ntransformer's ability to model complex relationships in data, and fine-tuning them ensures efficient domain adaptation\nwith causal language modeling tasks. The scaling parameter \u03b1 was set to 16, controlling the magnitude of low-rank\nupdates and ensuring effective adaptation without destabilizing the pre-trained parameters of the base model.\nLLM temperature set to 0 for fully deterministic outputs, ensuring consistent and predictable results without any\nrandomness and Maximum Token Length capped at 1024 to ensure inference efficiency without truncation of outputs."}, {"title": "5.1 Image Captioning and Visual-QA", "content": "For Image Captioning(Table 2) and Visual-QA(Table 3), we conducted experiments using Qwen-VL models (2B and\n7B) and LLaMA-3.2-11B Vision-Instruct models as the primary architectures. In both tasks, the models were fine-tuned\nusing Low-Rank Adaptation (LoRA) with ranks (L = 8, 16, 32) and 4-bit quantization (Qb = 4), enabling efficient\nparameter adaptation while minimizing computational overhead. The evaluation metrics used were SPICE (Semantic\nPropositional Image Caption Evaluation) for Image Captioning and BLEU for Visual-QA, which measure semantic\nrelevance and language-based precision, respectively. GPT-40 under a zero-shot setup was employed as a baseline for\nboth tasks to benchmark the performance of fine-tuned models. Prompts tailored to Image Captioning and Visual-QA\ntasks (B) were applied uniformly across all models to ensure consistency. Carbon emissions (CE) were recorded for\neach configuration, providing insights into the environmental cost of fine-tuning.\nGPT-40 served as a reference point, offering competitive zero-shot performance. Still, fine-tuned models surpassed\nGPT-40 with higher SPICE and BLEU scores, demonstrating that specialized models can outperform general-purpose\nLLMs. Overall, these experiments highlight the trade-off between performance improvements and environmental\nimpact, with smaller fine-tuned models like Qwen-VL-2B and Qwen-VL-7B achieving a favorable balance between\nperformance and minimal carbon emissions."}, {"title": "5.2 Summarization and Text-to-SQL", "content": "We fine-tuned Qwen2.5 models (0.5B, 3B, 7B, 14B) and LLaMA models (1B, 3B) for Dialogue Summarization and\nText-to-SQL tasks. These models were fine-tuned using Low-Rank Adaptation (LoRA) with ranks (L = 4, 8,16)\nand quantization levels (Qb = 4, 8). This systematic approach enabled efficient adaptation of the models to the specific\nrequirements of each task while minimizing computational demands. For Dialogue Summarization, ROUGE scores\n(ROUGE-1, ROUGE-2, and ROUGE-L) were employed as evaluation metrics to measure the quality and relevance of\ngenerated summaries (Table 4).\nFor the Text-to-SQL task, Execution Accuracy (EA) and Valid Efficiency Score (VES) were used as the primary\nevaluation metrics (Table 5). These metrics assessed the correctness of generated SQL queries and their execution\nvalidity across datasets. Dialogue summarization-specific and SQL-specific prompts (B) were used during the fine-\ntuning process and for GPT-40 inference is under a zero-shot setup. The prompts are tailored to ensure consistency and\noptimal performance across all model configurations. Complete experimental results, including detailed performance\nand carbon emissions comparisons for both tasks, are summarized in (C)."}, {"title": "6 Results", "content": "This section provides a comprehensive quantitative analysis of the significant performance improvements achieved\nthrough fine-tuning models across four key tasks: Image Captioning, Visual Question Answering (VQA), Dialogue\nSummarization, and Text-to-SQL conversion. These insights highlight the potential of fine-tuning in enhancing AI\nmodel performance. This analysis examines the performance improvements achieved through fine-tuning compared\nto base models and benchmarks these results against GPT-40 under a zero-shot setting for each task. Additionally,\nwe explore the trade-offs among model size, performance, and carbon emissions, highlighting their interconnection\nin promoting sustainable AI practices. Our findings reveal that fine-tuning smaller models can achieve competitive\nperformance while significantly reducing carbon emissions, offering a viable alternative to deploying larger, resource-\nintensive models. By incorporating lower-bit quantization levels (e.g., 4-bit and 8-bit), we demonstrate the feasibility\nof maintaining energy efficiency without compromising task-specific model performance. The results underscore the\nimportance of optimizing fine-tuning strategies to balance environmental responsibility and the effective development\nof high-performance AI models which are aligned with sustainable AI practices."}, {"title": "6.1 Performance Gain", "content": "We have measured the performance gain of a fine-tuned model compared to the base model and GPT-40 (under zero-shot\nsetup) relevant across tasks, the gain for a particular metric, \u03bc that is,GM,\u00b5 is calculated as the percentage improvement\nof the metric value from a reference model to a target model using\nGM,\u03bc(%) = ((\u00b5\u03c4={FT}-\u00b5R={BM,GPT-40}))/\u00b5R={BM,GPT-40}) \u00d7 100%\nwhere,\n\u2022 \u00b5\u03c4: Metric value of the Target Model e.g., fine-tuned Fr model.\n\u2022 \u00b5R: Metric value of the Reference Model, which can be either (base model) BM or GPT-40, depending on\nthe comparison.\n\u2022 GM,\u00b5 (%): Percentage gain for a specific metric \u03bc (e.g., ROUGE, BLEU, SPICE, EA, VES) from the\nreference model to the target model."}, {"title": "6.1.1 Image Captioning and visual-QA", "content": "Image Captioning: GPT-40 has a SPICE score(Table 2) of 0.16 for image captioning task on the artbench-pd-256x256\ndataset[27] matching the base Qwen-VL-2B but significantly lower than corresponding fine-tuned models."}, {"title": "6.1.2 Summarization and Test-to-SQL", "content": "Dialogue Summarization: GPT-40 has a ROUGE-1(R1) score (Table 4) of 0.37 on SAMSum dataset [22]. Fine-tuned\nmodels outperform GPT-40, with gains ranging from 13.51% to 37.84% as shown in the table 8. Larger models like\nQwen2.5-14B show higher gains(37.84 %) over GPT-40."}, {"title": "6.2 Performance Vs Emission: The Trade-off", "content": "This section examines the balance between model performance and carbon emissions across four key tasks: Dialogue\nSummarization, Image Captioning, Visual QA, and Text-to-SQL Conversion. By analyzing the mean data obtained from\nvarious configurations\u2014including different LoRA ranks and quantization levels\u2014we highlight how fine-tuning smaller\nmodels and leveraging lower-bit quantization can offer substantial performance gains with reduced environmental\nimpact. Figure 9 (appendix C) mentions emission efficiency of best performing fine-tuned model for each task.\nImage Captioning: A shown in the Plot 5 moving from Qwen-VL-2B to 3x sized Qwen-VL-7B results in a SPICE\nscore increase of only 5.56% (from 0.36 to 0.38), while carbon emissions increase by 41.30% (from 97.8 gm to 138.2\ngm). Further increasing to Llama-3.2-11B, the SPICE score decreases to 0.31, despite carbon emissions increasing2 by\n126.49% compared to Qwen-VL-2B. Qwen-VL-2B enhances performance by over 118% (SPICE score from 0.16 to\n0.35), achieving 94.6% of the larger model's performance (0.35 vs. 0.37) while emitting 56% less carbon emissions\n(97.37gm vs. 222.03gm for Llama-3.2-11B).\nVisual QA: Qwen-VL-7B achieves a BLEU score of 0.0750, which is 30.16% higher than Qwen-VL-2B's BLEU score\nof 0.0577. However, this comes with a 29.59% increase in carbon emissions(Plot 6). Surprisingly, Llama3.2-11B emits\n+252.03% more carbon (Table 3) than Qwen-VL-2B while there is a huge decline in performance by 97%. Fine-tuning\nQwen-VL-2B boosts performance by over 1,460% (BLEU score from 0.0037 to 0.0577), achieving 77% of the larger\nmodel Qwen-VL-7B's accuracy (0.0577 vs. 0.0750) with 23% less carbon emissions (60.43 gm vs. 78.33 gm)."}, {"title": "6.3 Emission per unit gain", "content": "To analyze the performance gain comprehensively, we consider various quantization levels Qu and LoRA ranks L for\na model M. The average percentage gain GM,\u00b5 for metric \u00b5 across all configurations is calculated as:\nGM,\u03bc = (\u03a3\u03a3Q_{b}L_{r} GM,\u00b5(F\u0442, \u0412\u043c))/(|Qb|\u00b7|Lr|)\nwhere:"}, {"title": "7 Conclusion", "content": "This paper tries to address the crucial balance between model performance and environmental sustainability across four\nessential tasks: Image Captioning, Visual Question Answering, Dialogue Summarization, and Text-to-SQL Conversion.\nTo quantify this trade-off, we introduced a novel index, the CEGI (Carbon Efficient Emission Index). We believe this\nindex significantly contributes to the field, allowing us to normalize and compare different models and configurations\neffectively. Our findings reveal a surprising insight: fine-tuning smaller models can lead to significant performance\nimprovements, often rivaling or surpassing larger models. This approach boosts accuracy and substantially reduces\ncarbon emissions, providing a sustainable alternative to resource-intensive larger models.\nWe also discovered that lower-bit quantization techniques can significantly reduce energy consumption without\ncompromising and sometimes even improving model performance. This finding challenges the conventional wisdom\nthat larger models are always superior. Our analysis indicates that larger models' incremental gains in accuracy may not\njustify their disproportionately higher environmental impact, especially when considering the normalized efficiency\nmetric. By fine-tuning smaller models and utilizing efficient quantization techniques, we can achieve a harmonious\nbalance between performance and sustainability. This has profound implications for developing and deploying machine\nlearning models, paving the way for a more environmentally responsible AI future."}, {"title": "Limitations and Future Directions", "content": "While our study offers valuable insights, it is essential to acknowledge its limitations. Our experiments were conducted\non specific datasets and tasks, which may not fully represent the diverse range of real-world applications. Additionally,\nour focus on Qwen and Llama variants may represent the broader landscape of model architectures. Future research\ncould explore a broader range of models and tasks to validate and extend our conclusions. Investigating the impact"}, {"title": "B Prompts", "content": "This section contains task-specific prompts designed for performing inference and fine-tuning across the base and\nfine-tuned versions of Qwen-2.5, Qwen-VL, LLaMa-3.2. The prompts are tailored to handle specific tasks and optimize\nperformance during, and GPT-40 zero-shot inference, ensuring high adaptability and relevance to diverse applications.\nFor fine-tuning, the prompts are combined with ground-truth outputs (e.g., task_prompt + ground_truth) to enhance\nmodel performance. This dual-purpose design makes them versatile for both evaluation and training phases.\nTemperature Set to 0 for fully deterministic outputs, ensuring consistent and predictable results without any randomness.\nMaximum Token Length: Capped at 1024 to ensure inference efficiency without truncation of outputs.\n# Image Caption task prompt\n= system_prompt \"You are an expert at creating clear and descriptive captions for\nhistorical artwork, focusing on main subjects and notable details.\"\n= prompt \"\"\"Generate a descriptive caption for the provided historical art image.\nDescribe the main subjects, setting, and notable details visible in the image.\nKeep it concise and focused.\nReturn only the caption text.\"\"\"\n# Visual QA prompt\nsystem_prompt = \"You are an expert at answering questions on pathology images,\nfocusing on key details relevant to the question.\"\nprompt = \"\"\"Generate a concise answer to the provided ##question## about the\npathology image.\nFocus on key details and information directly visible in the image to answer\naccurately.\n##question%22%3: question}\nReturn only the answer text.\"\"\"\n# Dialogue Summarization\nsystem_prompt = \"You are an expert summarizer. Analyze the dialogue and provide a\nclear, concise summary capturing the main points and context.\"\nprompt = \"\"\"Analyze the conversation below and provide a concise, accurate summary\nthat captures the key points and context.\n### Dialogue:\n{}\n\"\"\"\n# Text-To-SQL prompt\nsystem_prompt = \"You are an advanced SQL query generator. Analyze the user's input\nand context to generate an efficient, accurate SQL query following best\npractices.\"\nprompt = \"\"\"Below is an input question asked by the user. Context is provided to\nclarify the user's question. Generate an SQL response based on the user's\nquestion.\n### Input Question:\n{}\n### Context:\n{}\n\"\"\""}]}