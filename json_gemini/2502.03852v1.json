{"title": "PURSUING BETTER DECISION BOUNDARIES FOR\nLONG-TAILED OBJECT DETECTION VIA CATEGORY\nINFORMATION AMOUNT", "authors": ["Yanbiao Ma", "Wei Dai", "Jiayi Chen"], "abstract": "In object detection, the instance count is typically used to define whether a dataset\nexhibits a long-tail distribution, implicitly assuming that models will underper-\nform on categories with fewer instances. This assumption has led to extensive\nresearch on category bias in datasets with imbalanced instance counts. However,\nmodels still exhibit category bias even in datasets where instance counts are rel-\natively balanced, clearly indicating that instance count alone cannot explain this\nphenomenon. In this work, we first introduce the concept and measurement of cat-\negory information amount. We observe a significant negative correlation between\ncategory information amount and accuracy, suggesting that category information\namount more accurately reflects the learning difficulty of a category. Based on this\nobservation, we propose Information Amount-Guided Angular Margin (IGAM)\nLoss. The core idea of IGAM is to dynamically adjust the decision space of\neach category based on its information amount, thereby reducing category bias in\nlong-tail datasets. IGAM Loss not only performs well on long-tailed benchmark\ndatasets such as LVIS v1.0 and COCO-LT but also shows significant improvement\nfor underrepresented categories in the non-long-tailed dataset Pascal VOC. Com-\nprehensive experiments demonstrate the potential of category information amount\nas a tool and the generality of our proposed method.", "sections": [{"title": "1 INTRODUCTION", "content": "In object detection tasks, long-tailed distribution is a\ncommon phenomenon, where most instances are con-\ncentrated in a few categories, while other categories\nhave relatively few instances Jiao et al. (2019); Liu et al.\n(2020); Zou et al. (2023); Oksuz et al. (2020). A widely\naccepted perspective is that the imbalance in the num-\nber of instances causes the model to be more biased to-\nwards frequent categories during training, ignoring the\nless frequent ones, leading to significant category bias\nduring testing Cho & Kr\u00e4henb\u00fchl (2023); Alshammari\net al. (2022); Ren et al. (2020); Cui et al. (2019); Wang\net al. (2020a). However, recent research in image clas-\nsification suggests that category bias is not only caused\nby the imbalance in sample numbers but may also be\nclosely related to the complexity of intra-category fea-\ntures Ma et al. (2023a;c); Kaushik et al. (2024). This is\nevidenced in datasets with perfectly balanced samples,\nwhere models still exhibit bias. Out of curiosity, we ex-\namined the correlation between category average preci-\nsion (AP) and the number of instances on Pascal VOC, a\ntarget detection dataset with a relatively balanced num-\nber of instances (see Figure 1), and found that the corre-\nlation between the two was very low. This indicates that\nin object detection, model bias may also originate from the complexity of intra-category features."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1\nLONG-TAILED OBJECT DETECTION", "content": "In the research of long-tailed object recognition, the main approaches include data re-sampling,\nspecialized loss function design, architectural improvements, decoupled training, and data augmen-\ntation. Data re-sampling is a common method to address imbalanced datasets by increasing the\nsampling frequency of tail class samples to balance the data distribution. Common re-sampling\nstrategies include Class-aware sampling Shen et al. (2016) and Repeat factor sampling (RFS) Gupta\net al. (2019). These methods can be employed at different stages of training to achieve a multi-\nstage training process. Specialized loss function design is another technical approach to tackling\nlong-tailed challenges. For instance, EQL Tan et al. (2020) reduces suppression on tail classes by\ntruncating the negative gradients from head classes. The subsequent EQLv2 Tan et al. (2021) further\nimproves this approach through a gradient balancing mechanism. Other methods, such as Seesaw\nLoss Wang et al. (2021a), Equalized Focal Loss Li et al. (2022), ACSL Wang et al. (2021b), and\nLOCE Feng et al. (2021), reduce excessive suppression of tail classes by dynamically adjusting\nclassification logits or suppressing overconfident scores. C2AM Wang et al. (2022) observed that\nthe severe imbalance in weight norms across classes leads to pathological decision boundaries, and\ntherefore proposes learning fairer decision boundaries by adjusting the ratio of weight norms.\nCurrent research mainly focuses on these two directions. In addition, module improvement empha-\nsizes modifying the structure of detectors to address long-tailed distribution issues. For example,\nBAGS Li et al. (2020) and Forest R-CNN Wu et al. (2020) mitigate the impact of head classes on\ntail classes by grouping all classes based on valuable prior knowledge. Decoupled training Kang\net al. (2019) has found that long-tailed distributions do not significantly affect the learning of high-\nquality features, thus some methods freeze the feature extractor parameters during the classifier\nlearning phase, adjusting only the classifier Ma et al. (2023b); Wang et al. (2020a); Zhang et al.\n(2021). Data augmentation, as a means of introducing additional sample variability, has been shown\nto provide further improvements in long-tailed detection tasks. Recently proposed methods such as\nSimple Copy-Paste Ghiasi et al. (2021), FDC Ma et al. (2023b), FASA Zang et al. (2021), and FUR\nMa et al. (2024b) supplement the insufficiency of tail-class samples by performing data augmenta-\ntion in both image and feature spaces. RichSem Meng et al. (2024) and Step-wise Learning Dong\net al. (2023) introduce Transformer-based object detection architectures, with the former relying on\nexternal data and adding new network branches, while the latter incorporates multiple modules and\nmulti-stage training. The core advantage of our proposed IGAM lies in its simplicity and efficiency."}, {"title": "2.2\nMETHODS FOR MEASURING CLASS DIFFICULTY", "content": "The study of class difficulty is most relevant to our work. Most research addressing class bias has\nfocused on scenarios with sample imbalance, where rebalancing strategies based on sample size can\nbe somewhat effective. However, recent studies have reported that even when sample sizes are per-\nfectly balanced, classification models still exhibit significant performance disparities across different\nclasses. Investigating the root causes of model bias in scenarios where sample sizes are balanced is\ncrucial for improving model fairness and understanding learning mechanisms. However, research\non this issue is still limited. From a geometric perspective, DSB Ma et al. (2023a), CR Ma et al.\n(2023c), and IDR Ma et al. (2024a) conceptualize the data classification process as the disentangling\nand separating of different perceptual manifolds. These three studies respectively reveal that the ge-\nometric properties of perceptual manifolds\u2014volume, curvature, and intrinsic dimensionality-are\nsignificantly correlated with class performance. Kaushik et al. (2024) discovered that differences in\nthe spectral features of classes could be a source of class bias. Unfortunately, in the field of object\ndetection, there has been no research exploring the underlying causes of model bias. Our work is\nthe first to directly report on the widespread bias present in object detection models and to attempt\nto explore the potential mechanisms underlying this bias."}, {"title": "3 PURSUING BETTER DECISION BOUNDARIES WITH THE HELP OF\nCATEGORY INFORMATION AMOUNT", "content": "In this section, we first define and compute the category information amount (Section 3.1), then\ngradually derive how to dynamically adjust the decision space of categories based on their informa-"}, {"title": "3.1 DEFINITION AND MEASUREMENT OF CATEGORY INFORMATION AMOUNT", "content": "Recent studies have shown that the response of deep neural networks to images is similar to human\nvision, following the manifold distribution hypothesis, where the embeddings of images lie near a\nlow-dimensional perceptual manifold embedded in high-dimensional space Cohen et al. (2020); Li\net al. (2024). Continuous sampling along a dimension of this manifold corresponds to continuous\nchanges in physical features. Therefore, the volume of the perceptual manifold mapped by a deep\nneural network can effectively measure the information amount of a category. Based on this theory,\nwe define the information amount $I_i$ of category i as the volume of its perceptual manifold: $I_i =$\n$Vol(X_i)$, where $X_i = [x_1,x_2,...,x_m]$ represents the set of embeddings for instances in category\ni, m denotes the number of instances. $Vol(X_i)$ measures the volume of the perceptual manifold,\nreflecting the information amount of the category. It is important to note that the embeddings used\nto calculate the information amount should be extracted from the classification module of the object\ndetection model, not the regression module. Below is the method for calculating $Vol(X_i)$.\nGiven the embedding set $X_i = [x_1,x_2,...,x_m] \\in$\n$\\mathbb{R}^{p \\times m}$, where each instance embedding $x_j \\in \\mathbb{R}^p$, p\ndenotes the dimension of the embedding. We first\ncompute the covariance matrix of the embedding\nset $X_i$: $\\Sigma(X_i) = \\frac{1}{m} \\sum_{j=1}^{m}(x_j - \\bar{x}) (x_j - \\bar{x})^T$,\nwhere $\\bar{x}$ is the mean vector of the embedding set\n$X_i$: $\\bar{x} = \\frac{1}{m}\\sum_{j=1}^{m}x_j$.\nThe covariance matrix $\\Sigma(X_i)$ captures the distri-\nbution characteristics of the category i in the high-\ndimensional embedding space, and its determinant\ncan be used to calculate the volume of the per-\nceptual manifold. To enhance the accuracy of\nthe covariance matrix estimation, we employ the\nLedoit-P\u00e9ch\u00e9 nonlinear shrinkage method Burda\n& Jarosz (2022), which improves robustness in\nhigh-dimensional spaces through eigenvalue trans-\nformation. The covariance matrix $\\Sigma(X_i)$ is recon-\nstructed as follows:\n$\\Sigma(X_i) = Vdiag(\\Lambda_1, \\Lambda_2, ..., \\Lambda_p)V^T$,\n(1)\nwhere V is the matrix of eigenvectors, and $\\Lambda_i = \\max(\\lambda_i, \\lambda^*)$, with $\\lambda^* = (1 - \\sqrt{p/m})^2$ as\nthe nonlinearly transformed minimum eigenvalue. Finally, the information amount of category i is\nformally defined as:\n$I_i = Vol(X_i) = \\frac{1}{2} log_2 det (\\Sigma(X_i) + I)$,\n(2)\nwhere $det(\\Sigma(X_i) + I)$ is the determinant of the matrix $\\Sigma(X_i) + I$, and I is the identity matrix.\nThe determinant reflects the spread of the category's embeddings, representing the volume of the\nperceptual manifold. In this way, we can quantify the information amount of category i."}, {"title": "3.2 INFORMATION AMOUNT-GUIDED ANGULAR MARGIN (IGAM) Loss", "content": "Cross-entropy loss is widely used in deep learning, especially in classification tasks. It measures\nthe difference between the model's predicted probability distribution and the true label distribution.\nFor the classification component in object detection tasks, given a feature vector x and a label i, the\ncross-entropy loss is typically defined as:\n$L = -log(\\frac{e^{W_i^T x}}{\\sum_{j=1}^{C}e^{W_j^T x}})$\n(3)\nwhere $W_j$ is the j-th column of the final fully connected layer, corresponding to the weight vector\nfor category j. In long-tailed scenarios, it has been observed that the norm of the weight vectors\ncorresponding to each category is extremely unbalanced, making it more difficult to recognize tail"}, {"title": "3.3 Low-COST DYNAMIC UPDATE OF INFORMATION DENSITY", "content": ""}, {"title": "3.3.1 DYNAMIC UPDATE STRATEGY", "content": "The phenomenon of feature slow drift Wang et al. (2020b); Ma et al. (2023a) indicates that as\ntraining progresses, the distance between the embeddings of the same sample at different training\nstages becomes increasingly smaller, to the extent that the previous version of an embedding can\napproximate the latest version. Inspired by this, we propose the most straightforward approach:\nstore the embeddings of all instances generated during training in a queue, with the queue length\nequal to the total number of instances in the dataset. After each training epoch, all embeddings in\nthe queue can be updated once. At the end of each epoch, the embeddings in the queue are used\nto calculate and update the information amount for all categories. In the following, we refer to\nthis approach as the original strategy. Although the original strategy avoids repeatedly extracting\nembeddings for the entire dataset, it increases the demand for storage space. Considering that the\nessence of calculating the information amount lies in obtaining the covariance matrix of the instance\nembeddings, we propose a new strategy that significantly reduces storage space. The core idea of\nthis new strategy is to calculate the global covariance matrix of the samples using multiple local\nsample covariance matrices. The specific steps are as follows:"}, {"title": "3.3.2 STORAGE SPACE COMPARISON", "content": "Assume the object detection dataset contains N instances, each with an embedding dimension of p,\nand there are C categories. The queue length is set to d. The storage space required by the original\nstrategy is: $S_{original} = N \\times p$. The storage space required by the new strategy is:\n$S_{new} = d \\times p + C \\times (\\lceil{N/d}\\rceil + 1) \\times p^2$.\n(9)\nwhere $C \\times (\\lceil{N/d}\\rceil + 1) \\times p^2$ represents the space needed to store the local covariance matrices. To\nanalyze when the new strategy saves more space, we define the storage space ratio R:\n$R = \\frac{S_{new}}{S_{original}} = \\frac{d \\times p + C \\times (\\lceil{N/d}\\rceil + 1) \\times p}{N}$\n(10)\nWhen R < 1, the new strategy saves more stor-"}, {"title": "4 EXPERIMENTS", "content": "We conducted a comprehensive evaluation of the effectiveness of IGAM on long-tailed and relatively\nbalanced object detection benchmark datasets. The experiments are divided into two parts: the first\npart is carried out on the long-tailed large-vocabulary datasets LVIS v1.0 and COCO-LT, while the\nsecond part is conducted on the relatively balanced Pascal VOC dataset."}, {"title": "4.1 DATASETS AND EVALUATION METRICS", "content": "LVIS v1.0 Gupta et al. (2019) contains 1,203 categories, with the training set consisting of 100k\nimages (approximately 1.3M instances) and the validation set containing 19.8k images. Based on\nthe frequency of occurrence in the training set, all categories are divided into three groups: rare\n(1~10 images), common (11~100 images), and frequent (more than 100 images). In line with\nEFL Li et al. (2022), we report not only the widely used object detection metric $AP_b$ across IOU\nthresholds (from 0.5 to 0.95) but also the bounding box AP for frequent ($AP_f$), common ($AP_c$), and\nrare ($AP_r$) categories separately. The COCO-LT Wang et al. (2020a) dataset is a long-tailed subset\nof MS COCO Lin et al. (2014), and they share the same validation set. Consistent with previous\nwork Wang et al. (2021a), we divided the 80 classes in COCO-LT into four groups based on the\nnumber of training instances per class: fewer than 20 images, 20~400 images, 400~8000 images,\nand 8000 or more images. Pascal VOC Everingham et al. (2015) includes two versions, 2007 and\n2012, comprising a total of 20 classes. Following standard practice Tong & Wu (2023), we trained\non the train+val sets of VOC2007 and VOC2012 and tested on the test set of VOC2007. We report\nthe Average Precision (AP) for each class on Pascal VOC, and on COCO-LT, we report the accuracy\nof the four groups as $AP_0$, $AP_1$, $AP_2$, and $AP_3$. The mean average precision is reported as $mAP_b$."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "Consistent with previous studies Qi et al. (2023), we implemented the Faster R-CNN Ren et al.\n(2015) detector using the MMDetection Chen et al. (2019) toolbox and adopted ResNet-50 and\nResNet-101 He et al. (2016) with an FPN Lin et al. (2017a) structure as the backbone networks.\nDuring training, we set the batch size to 16 and the initial learning rate to 0.02, consistent with EFL\nLi et al. (2022) and C2AM Wang et al. (2022). We trained the model using an SGD optimizer with\na momentum of 0.9 and a weight decay rate of 0.0001 for 24 epochs. The learning rate was reduced\nto 0.002 and 0.0002 at the end of the 16th and 22nd epochs, respectively. In all experiments, we\napplied random horizontal flipping and multi-scale jittering for data augmentation. We did not use\nany test-time augmentations. We did not use any test-time augmentations."}, {"title": "4.3 MAIN RESULTS: EFFECTIVENESS OF IGAM LOSS", "content": "Recalling Section 3.2, we introduce the hyperparameter s in\nthe IGAM Loss, which scales the cosine value to ensure it\nfalls within an appropriate range, thereby stabilizing the op-\ntimization process. This is a standard practice in cosine-\nbased classifiers and has been widely adopted. We refer to\nWang et al. (2022) for rigorous determination of s and tested\nthe model's performance under different settings. The ex-\nperimental results, summarized in Table 2, show that when\ns = 30, the model trained with IGAM Loss achieves opti-\nmal performance. After determining the hyperparameter, we"}, {"title": "4.4 COMPARISON WITH STATE-OF-THE-ARTS", "content": "Table 4 presents the experimental results on LVIS v1.0. IGAM outperforms the current state-of-\nthe-art methods on both ResNet-50-FPN and ResNet-101-FPN backbones, achieving overall per-\nformances of 26.8% and 28.0%, respectively. Notably, for rare categories, IGAM surpasses the\nsecond-best method by 2.4% and 1.6% on the two backbones, respectively. It is worth mention-\ning that despite BACL incorporating a series of techniques, including foreground-balanced loss,\nsynthetic hallucination samples, and decoupled training, our method still exhibits strong competi-\ntiveness. For the most frequent categories, although EQLv2 and Seesaw exhibit exceptional perfor-\nmance, they significantly lack attention to rare categories. In contrast, IGAM demonstrates superior\nperformance on rare categories while maintaining competitive results on the most frequent cate-\ngories. On the ResNet-50-FPN backbone, IGAM's APf is only 0.3% and 0.2% lower than that of\nEQLv2 and Seesaw, respectively. We attribute IGAM's superior overall performance to its accurate\nmeasurement of category learning difficulty through information amount, allowing IGAM not to\nimpair the performance of frequent categories by focusing too much on rare categories."}, {"title": "4.5 EVALUATION RESULTS ON COCO-LT", "content": "The COCO-LT dataset is not yet a widely recognized benchmark in the field of long-tailed object\ndetection, and thus, there are relatively few studies validating methods on it. We trained baseline\nmodels on COCO-LT using cross-entropy loss and compared Seesaw and EQLv2 following Qi et al.\n(2023). Additionally, we independently implemented EFL and C2AM. The experimental results\nare summarized in Table 5. It can be observed that IGAM achieves the best overall performance\non both backbone networks, with 25.8% and 27.0%, respectively. Notably, IGAM surpasses the\nsecond-best method in the AP metric by 2.3% and 3.0% on the two backbones, respectively,\nhighlighting the significant advantage of our method on rare categories. While cross-entropy loss\nand EQLv2 perform well on the most frequent categories, they exhibit a clear gap in performance on\nrare categories compared to IGAM. Furthermore, IGAM does not fall behind cross-entropy loss and\nEQLv2 in the AP metric, demonstrating that our method effectively balances performance across\nboth rare and frequent categories."}, {"title": "4.6 EVALUATION RESULTS ON PASCAL VOC", "content": "We trained models using Seesaw, EFL, and C2AM losses on the relatively balanced Pascal VOC\ndataset for comparison. Table 6 presents the performance of each method across all categories as\nwell as the overall performance. It can be observed that IGAM outperforms the other methods in\nterms of overall performance on both backbone networks, achieving 77.7% and 78.6%, respectively,\nsurpassing the second-best method by 0.8% and 1.1%.\nMore importantly, our method significantly improves the performance of underperforming cate-\ngories. We selected five representative poorly performing categories for observation: aeroplane,\nboat, bottle, chair, and pottedplant. With the ResNet-50 backbone, IGAM achieved the best perfor-\nmance across all five categories, surpassing the second-best method by 2.2%, 1.3%, 3.4%, 0.7%,\nand 2.1%, respectively. Similarly, with the ResNet-101 backbone, IGAM also achieved the best\nperformance across all five categories, exceeding the second-best method by 1.6%, 2.7%, 3.0%,"}, {"title": "4.7 EFFECTIVENESS IN REDUCING MODEL BIAS", "content": "To more clearly demonstrate the effectiveness of our\nmethod in mitigating model bias, we use the variance\nof class-wise average precision (AP) as a measure of\nmodel bias. The comparison results on LVIS v1.0 are\nshown in Figure 4. It can be observed that the models\ntrained with our method exhibit lower bias compared\nto Seesaw, EFL, and C2AM, across two different back-\nbones. Notably, compared to Seesaw, our method re-\nduces model bias by approximately 50%. These results\nare attributed to the accurate reflection of learning dif-\nficulty through category information amount. We en-\ncourage other researchers to explore additional potential\nfactors influencing model bias, aiming to design more\nequitable object detection models."}, {"title": "5 CONCLUSION", "content": "This work addresses the issue that instance count fails to explain the generalized bias present in deep\nlearning models for object detection tasks. We propose using information amount to measure the\ndetection difficulty of categories, and experiments reveal a significant negative correlation between\na category's information amount and its accuracy. Based on this finding, we propose dynamically\nadjusting the decision boundaries of categories using their information amount. Comprehensive\nempirical studies demonstrate that information amount helps the model focus more on learning\nchallenging categories, both in long-tailed and non-long-tailed datasets."}]}