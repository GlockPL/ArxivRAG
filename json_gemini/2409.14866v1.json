{"title": "Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs", "authors": ["Xueluan Gong", "Chen Chen", "Mingzhe Li", "Yanjiao Chen", "Yilin Zhang", "Qian Wang", "Fengyuan Ran", "Kwok-Yan Lam"], "abstract": "Large Language Models (LLMs) have excelled in various\ntasks but are still vulnerable to jailbreaking attacks, where\nattackers create jailbreak prompts to mislead the model to\nproduce harmful or offensive content. Current jailbreak meth-\nods either rely heavily on manually crafted templates, which\npose challenges in scalability and adaptability, or struggle to\ngenerate semantically coherent prompts, making them easy to\ndetect. Additionally, most existing approaches involve lengthy\nprompts, leading to higher query costs. In this paper, to rem-\ndy these challenges, we introduce a novel jailbreaking attack\nframework, which is an automated, black-box jailbreaking\nattack framework that adapts the black-box fuzz testing ap-\nproach with a series of customized designs. Instead of relying\non manually crafted templates, our method starts with an\nempty seed pool, removing the need to search for any re-\nlated jailbreaking templates. We also develop three novel\nquestion-dependent mutation strategies using an LLM helper\nto generate prompts that maintain semantic coherence while\nsignificantly reducing their length. Additionally, we imple-\nment a two-level judge module to accurately detect genuine\nsuccessful jailbreaks.\nWe evaluated our method on 7 representative LLMs and\ncompared it with 5 state-of-the-art jailbreaking attack strate-\ngies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-\n4, and Gemini-Pro, our method achieves attack success rates\nof over 90%, 80%, and 74%, respectively, exceeding existing\nbaselines by more than 60%. Additionally, our method can\nmaintain high semantic coherence while significantly reduc-\ning the length of jailbreak prompts. When targeting GPT-4,\nour method can achieve over 78% attack success rate even\nwith 100 tokens. Moreover, our method demonstrates trans-\nferability and is robust to state-of-the-art defenses. We will\nopen-source our codes upon publication.", "sections": [{"title": "1 Introduction", "content": "Nowadays, large language models (LLMs) like ChatGPT [38]\nhave experienced widespread deployment, showcasing excep-\ntional capabilities in comprehending and generating human-\nlike text. Currently, ChatGPT has over 100 million users, and\nits website receives 1.8 billion visits per month\u00b9. However, as\nlarge language models (LLMs) become more integrated into\nsociety, concerns about their security and safety have arisen.\nA significant worry is the potential misuse for malicious pur-\nposes. In a real-life incident in May 2023, a criminal was ar-\nrested for exploiting ChatGPT to create fake news\u00b2. Moreover,\na recent Microsoft study highlighted that a notable number of\nattackers are using LLMs to craft phishing emails and develop\nransomware and malware\u00b3. Apart from misuse, LLMs are sus-\nceptible to jailbreaking attacks [6, 10, 29, 33, 43,48,49,53,56].\nAttackers aim to craft malicious prompts to mislead LLMs,\nbypass safety features, and generate responses with harmful,\ndiscriminatory, violent, or sensitive content. We present some\njailbreaking attack samples in Figure 1.\nCurrently, there is a wide array of cutting-edge jailbreaking\nattacks, and we have listed various representative methods\nin Table 1. First, since most proprietary LLMs are only ac-\ncessible as black-boxes, white-box methods [48, 63], though\nsometimes effective, are not practical. Second, various at-\ntacks [23,48,50,57] use manually crafted prompts to infiltrate\nonline chatbots powered by aligned LLMs. While manual\nattacks effectively discover stealthy jailbreak prompts, they\noften involve individual LLM users crafting prompts, leading\nto challenges in scalability and adaptability. Moreover, ap-\nproaches like those at jailbreakchat may struggle to keep up\nwith updates to LLMs. Third, while some methods [11,12,31]\ndo not rely on manual efforts to design prompts, they often"}, {"title": "2 Background", "content": "2.1 Large Language Model\nLarge language models (LLMs) refer to sophisticated artificial\nintelligence (AI) systems that are designed to understand and\ngenerate human-like text. These models are typically based on\nTransformer frameworks [46], undergo training on extensive\ntext corpora, and contain millions or billions of parameters.\nThey have the potential to revolutionize various industries by\nsignificantly enhancing user experience and efficiency.\nDuring the training phase, LLMs are optimized to mini-\nmize the difference between their generated outputs and the\nexpected outputs, typically through maximum likelihood esti-\nmation (MLE). Given a dataset of text sequences, the model\nparameters 0 are adjusted to maximize the likelihood of the\ntraining data:\nL(0) = \\sum_{(p,y) \\in D} log P(y | p; 0),\n(1)\nwhere (p, y) represents a pair of input prompt p and the corre-\nsponding target text y, and P(y | p;0) denotes the conditional\nprobability of y given p under the model parameterized by 0.\nDuring the inference phase, after receiving a prompt p,\nthe LLM produces a sequence of tokens, y = (y1, y2,..., yn),\nusing an auto-regressive mechanism [3]. Each token yi is\nderived from the conditional probability distribution P(yi |\nP, y1:i\u20131;0), where y1:i\u20131 is the sequence of previously gener-\nated tokens. The decoding strategy, D, may be either stochas-\ntic or deterministic. The deterministic strategy selects the"}, {"title": "2.2 Jailbreaking Attacks", "content": "Jailbreaking attacks [11,43,49] against LLM chatbots involve\na process where an attacker strategically crafts prompts to\ncircumvent the usage policy measures of the LLM Chatbots.\nThrough skillful manipulation of these prompts, the attacker\ncan mislead LLMs into generating harmful responses that\nviolate their usage policies. For instance, an LLM chatbot\nwill refuse to respond to a direct malicious inquiry such as\n\"how to harm a child\". However, when the same question is"}, {"title": "2.3 Jailbreaking Defenses", "content": "Existing jailbreaking defenses can be divided into three cat-\negories: input modification-based defenses, output filtering-\nbased defenses, and prompt engineering defenses.\nInput modification-based defenses. These defenses pro-\npose strategies to alter input prompts to the target LLMs, aim-\ning to disrupt the structure of potential jailbreaking templates.\nFor example, Kumar et al. [26] and Cao et al. [4] randomly"}, {"title": "2.4 Fuzz Testing", "content": "Fuzz testing [25], also known as fuzzing, is a dynamic soft-\nware testing technique used to identify potential vulnerabil-\nities, bugs, or operational issues in software systems. It in-\nvolves generating and injecting a wide array of unexpected or\nrandom data as inputs to the system to observe its behavior\nand responses. The primary goal of fuzz testing is to expose\nweaknesses in the handling of unusual, malformed, or other-\nwise unexpected input data.\nThe process of fuzz testing can be mathematically de-\nscribed by the following formula:\nIfuzz = f(Iorig, R)\n(3)\nwhere Ifuzz represents the fuzzed input, Iorig is the original in-\nput, and R denotes a random or mutated component introduced\nto create the fuzzed input. The function f modifies the orig-\ninal input by incorporating randomness or specific patterns\ndesigned to test the robustness of the system. Fuzz testing can\nbe categorized into three main types: black-box, white-box,\nand grey-box fuzzing. Black-box fuzzing [2] involves test-\ning without any knowledge of the internal information of the\nsystem, focusing solely on input-output behavior. White-box"}, {"title": "3 Threat Model", "content": "We assume that the adversary possesses the following capa-\nbilities:\n\u2022 Query the target LLM. The adversary can send queries\nto the target LLM, such as through an API, and receive\nresponses. However, there is a query budget that limits\nthe maximum number of allowable queries. The target\nLLM is assumed to be an aligned model, fine-tuned with\ninstruction tuning or reinforcement learning from human\nfeedback (RLHF), which typically refuses to respond to\nunethical or harmful questions.\n\u2022 Access to public prompts. The adversary has the capabil-\nity to gather prompts from public databases or forums.\nThis access allows for the collection of diverse input\nexamples, which may help fine-tune the model as the\njudge. Note that we do not require any existing jailbreak\nprompts as the seed pool to launch the fuzz testing.\nWe assume that the adversary operates under the following\nconstraints:\n\u2022 Black-box access to the target LLM. The adversary op-\nerates in a black-box setting, where they can only query\nthe target LLM and receive its responses. Access to the\nmodel's internals, such as parameters, logits, or losses,\nis not available."}, {"title": "4 Methodology of our method", "content": "In this section", "decision": "leveraging pre-existing, human-"}]}