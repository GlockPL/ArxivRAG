{"title": "Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos", "authors": ["Krishna Chaitanya", "Pablo F. Damasceno", "Shreyas Fadnavis", "Pooya Mobadersany", "Chaitanya Parmar", "Emily Scherer", "Natalia Zemlianskaia", "Lindsey Surace", "Louis R. Ghanem", "Oana Gabriela Cula", "Tommaso Mansi", "Kristopher Standish"], "abstract": "Accurate assessment of disease severity from endoscopy videos in ulcerative colitis (UC) is crucial for evaluating drug efficacy in clinical trials. Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS annotation is time-consuming and susceptible to inter-rater variability, factors addressable by automation. Automation attempts with frame-level labels face challenges in fully-supervised solutions due to the prevalence of video-level labels in clinical trials. CNN-based weakly-supervised models (WSL) with end-to-end (e2e) training lack generalization to new disease scores and ignore spatio-temporal information crucial for accurate scoring. To address these limitations, we propose \"Arges\", a deep learning framework that utilizes a transformer with positional encoding to incorporate spatio-temporal information from frame features to estimate disease severity scores in endoscopy video. Extracted features are derived from a foundation model (ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials (61M frames, 3927 videos). We evaluate four UC disease severity scores, including MES and three UCEIS component scores. Test set evaluation indicates significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%, 6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art methods. Prospective validation on previously unseen clinical trial data further demonstrates the model's successful generalization.", "sections": [{"title": "1 Introduction", "content": "Ulcerative colitis (UC), a chronic inflammatory bowel disease (IBD), impacts approximately 5 million individuals worldwide, causing intestinal inflammation and ulceration. In UC clinical trials, colon disease severity is often assessed through"}, {"title": "2 Data and Methods", "content": "Our framework as in Fig.1 is comprised of two components: Foundation model for feature extraction from frames (Sec 2.2), pre-trained with curated data (Sec 2.1), and a downstream classification model (Sec 2.3) to estimate severity score.\n2.1 IBD disease data curation: Inflammatory Bowel Diseases (IBD), including Ulcerative Colitis (UC) and Crohn's Disease (CD) are chronic gastrointestinal disorders characterized by inflammation, ulcers, and rectal bleeding. In our study, we use endoscopy videos from four clinical trials (two UC[16,17], two CD[15,1]) focused on drug safety and efficacy for moderate to severe UC or CD as shown in Table 1. This diverse dataset, spanning 5 continents and 30 countries, comprised 2411 patients, 4911 videos, and over 71M frames. The UC trials data included video-wise labels for MES and UCEIS subscores. For foundational encoder pre-training, we used 61M frames, a substantial 14x increase compared to previous largest SSL model for endoscopy videos[27]. For downstream tasks, unlike[27] which uses 8-frame clips for simpler tasks like polyp detection, our approach tackles a complex UC disease severity scoring task, requiring long-range modeling for accurate evaluation. Our ablation study explores the impact of clip- versus full-video pre-training. Our downstream models were rigorously evaluated on an unseen prospective dataset[11] from a multi-center, international UC clinical trial (14M frames).\n2.2 Foundation model (ArgesFM): To leverage unlabeled data from diverse IBD trials, we developed a foundation model (FM) based on SSL[9], outlined in Fig.1. ArgesFM was trained on both UC and CD data from four clinical trials to enhance generalizability to unseen data and adaptability to various downstream tasks without requiring end-to-end training. ArgesFM utilizes a vision transformer (ViT-Base [5], 86M parameters) to encode video frames, capturing intricate spatial relationships within a frame through self-attention,"}, {"title": "2.3 Downstream classifier (ArgesMES or ArgesUCEIS)", "content": "When evaluating disease severity in endoscopy videos, clinicians prefer whole videos or clips[14] over individual frames[21,19], as context from multiple frames helps distinguish true disease features from artifacts like camera blur or forceps-induced bleeding. To test if such dynamic content improves disease quantification accuracy, we employed a compact temporal model (Transformer [25], 17M parameters) to estimate disease severity from videos pre-encoded by ArgesFM. In this design, an input matrix of N \u00d7 D features is obtained by passing a video with N frames through the trained student network, D is feature vector dimension (D = 768). The extracted features undergo positional encoding before being input into a Transformer, followed by an attention-based MIL aggregator and"}, {"title": "3 Data splits, benchmarking, and implementation details", "content": "Dataset split: We partitioned data from two UC and two CD clinical trials into training (80%) and held-out test (20%) sets. ArgesFM was trained on this 80% training data, which consisted of over 61M frames. The downstream models (ArgesMES/ArgesUCEIS) require MES and UCEIS scores annotations, available only for UC trials. Hence, we trained the downstream model using the same 80% training data from two UC trials (UNIFI, JAKUC) through 4-fold cross-validation. Performance evaluation is reported on the held-out test set (20%) for UNIFI and JAKUC. Prospective validation was conducted on an unseen third UC trial (QUASAR) (100% data) using each locked model from individual training folds (Table 1).\nData Pre-processing: Videos were converted to frames at 30 fps, resized to 224x224 (native video resolution varied between 640x510 to 1280x960) and 3 RGB channels normalized using ImageNet values.\nArges models architecture: For ArgesFM, we employ ViT-Base[5] as the encoder for SSL training, utilizing DINOv2 and it acts as a feature extractor (first block in Fig. 1). Downstream models (second block in Fig. 1): ArgesMES or ArgesUCEIS incorporates a compact transformer with two encoder layers, each equipped with four multi-attention heads and dropout of 0.25. This is followed by an attention-based MIL and a dense layer classifier with dropout of 0.5 to estimate a disease severity score per video."}, {"title": "4 Experiments, Results and Discussion", "content": "4.1. Arges outperforms state-of-the-art (SOTA) models on MES classification: Table 2 compares Arges with SOTA models, including WSL and SSL, on MES classification across two UC held-out test sets (UNIFI, JAKUC) and prospective trial data (QUASAR). WSL[19] exhibits similar trends as reported previously, where Attention MIL and a modified loss outperform a basic max pooling model. EndoFM[27], a comparable foundation model, although effective in simpler tasks like polyp detection with short clips of 8 frames, shows poor generalization to unseen UC disease types and yields low F1 scores for"}, {"title": "4.2. All models significantly generalize to unseen, prospective data", "content": "Four comparison models and our two new models demonstrate non-inferior F1 scores on the unseen QUASAR dataset (refer to Table 2 and 3). This outcome is attributed to the extensive and diverse datasets used for model training, enabling effective generalization to new data."}, {"title": "4.3. Arges excels on other downstream scoring tasks without e2e training", "content": "Table 3 presents the results of employing ArgesFM features for other disease severity tasks, specifically scoring the three UCEIS components (bleeding, erosion, vascular pattern). A comparison with the WSL CNNs attention-MIL[19] model (achieves the highest F1 score in Table 2) demonstrates notable performance gains with our method when ArgesFM is used with Transformer and attention MIL, surpassing CNN-based WSL. Moreover, these models exhibit improved training and inference time on unseen data, detailed in Supplementary."}, {"title": "4.4. MIL aggregator and dynamics of Transformer improve disease scoring performance", "content": "Table 2 shows enhanced disease scoring accuracy with the Transformer architecture compared to simple average pooling. In Table 4, we detail our ablation study on the contributions of Transformer and MIL aggregator components. Initially, integrating a Transformer alone improved performance, emphasizing the importance of long-range spatio-temporal modeling that aggregates insights from consecutive frames. Subsequent addition of an attention MIL aggregator alone boosted performance, leveraging its effectiveness in weakly-supervised scenarios. Optimal results were achieved with both components, highlighting their collaborative effectiveness."}, {"title": "4.5. ArgesMES scores are interchangeable with human readers", "content": "Previous studies [4,13] has highlighted the inherent subjectivity in scoring UC severity by experts and quantify it using the weighted Cohen's kappa score (k). We had two expert readers MES evaluations with the prospective trial data. In the prospective data evaluation, the agreement between ArgesMES output and human reader assessment (k=0.66, CI=0.60-0.72) closely aligns with the two human expert raters agreement (k=0.71, CI=0.66\u20130.76). This falls within the range (k=0.61 to 0.8) considered as substantial agreement, as in prior studies[19,4,13]."}, {"title": "4.6. Interpretability", "content": "By using Attention-based MIL, attention scores are obtained for each frame in the video. Fig.2 shows the high attention regions where the model focuses, influencing the determination of severity score. In a qualitative assessment, two independent experienced gastroenterologists examined these attention regions in 15 videos. Their consensus was that the model prioritizes informative areas, aligning with clinical intuition. This design improves interpretability and may assist in making informed decisions during clinical trials."}, {"title": "5 Conclusion", "content": "The \"Arges\" framework presents key advancements in endoscopy video analysis. The incorporation of spatio-temporal modeling, utilizing a transformer with"}, {"title": "6 Supplementary", "content": ""}]}