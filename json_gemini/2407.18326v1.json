{"title": "Classification-Based Automatic HDL Code Generation Using LLMs", "authors": ["Wenhao Sun", "Bing Li", "Grace Li Zhang", "Xunzhao Yin", "Cheng Zhuo", "Ulf Schlichtmann"], "abstract": "While large language models (LLMs) have demonstrated the ability to generate hardware description language (HDL) code for digital circuits, they still suffer from the hallucination problem, which leads to the generation of incorrect HDL code or misunderstanding of specifications. In this work, we introduce a human-expert-inspired method to mitigate the hallucination of LLMs and improve the performance in HDL code generation. We first let LLMs classify the type of the circuit based on the specifications. Then, according to the type of the circuit, we split the tasks into several sub-procedures, including information extraction and human-like design flow using Electronic Design Automation (EDA) tools. Besides, we also use a search method to mitigate the variation in code generation. Experimental results show that our method can significantly improve the functional correctness of the generated Verilog and reduce the hallucination of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "As Moore's Law slowing down, there is an increasing demand for customized VLSI design. One of the key steps in the hardware design process is writing the hardware description language (HDL) code. However, HDL programming is time-consuming and labor-intensive. Therefore, the automatic HDL code generation from the specifications in natural language has attracted much attention in recent years.\nAmong the automatic HDL code generation solutions, large language models (LLMs) code generation is one of the most promising solutions. LLMs have achieved remarkable success in various fields, such as machine translation [1] and robot trajectory planning [2]. In software design, LLMs have also demonstrated the ability to generate the code for various programming languages [3]. In the field of hardware design, generative models have been employed to create designs [4]. Additionally, researchers have also recognized the potential of LLMs in generating HDL codes [5].\nDespite the advance of LLMs, there are still many challenges that hinder the application of LLMs in HDL code generation. One of the challenges is the hallucination problem. The hallucination problem refers to the generation of incorrect HDL code or misunderstanding of the specifications. The na\u00efve way as illustrated in Fig. 1(a) to apply LLMs to generate HDL codes in one iteration usually leads to many errors. The hallucination problem is caused by the lack of reasoning ability and training in the HDL domain, since the amount of datasets for HDL codes is limited. To solve this problem, a human expert can be involved as a supervisor in the workflow of LLMs, such as Chip-chat [6], as illustrated in Fig. 1(b), where human expert monitors the code generate process and provides to LLMs to correct the code. However, involving human experts increases the cost of this automatic design flow, and the effieceny of this work flow may also compromise the efficiency of the human expert.\nChipNeMo [7] and [8]-[11] aim to enhance the quality of generated HDL codes by fine-tuning the LLMs with augmented datasets. Therefore, the LLMs can learn more about the HDL design, which helps them to generate the correct HDL codes. However, this method depends on the quality of the augmented datasets. Besides, since fine-tuning an LLM needs a large amount of computation resources, the fine-tuned models are relatively small compared with commercial models such as GPT4 [12]. Accordingly, the reasoning ability of the fine-tuned LLMs may be limited.\nThe other method to enhance the quality of automatically genearted HDL codes is to leverage the in-context learning ability [13] of the LLMs. The in-context learning ability allows the LLMs to learn how to generate new patterns from the examples in the context. One of the applications is retrieval augmented generation (RAG) [14], as illustrated in Fig. 1(c), which retrieves the examples from a database to guide the code generation using LLMs. Previous work such as GPT4AIGChip [15]-[17] uses RAG either to retrieve the examples similar to the specifications to help the generation or to retrieve the examples for error correction. However, these methods need a database with a large number of examples, which may need human experts to build. AutoChip [18] uses the feedback from testbenches as a replacement for the database. Therefore, it can avoid the database building process. However, the testbenches do not always have feedbacks. In some scenarios, the final testbench can only show the pass rate of the test samples,"}, {"title": "II. MOTIVATION", "content": "To improve the performance of Large Language Models (LLMs) in HDL code generation in a training-free manner, the key is to use the in-context learning and reasoning ability of the LLMs. However, RAG-based methods heavily rely on the quality of the databases, which are also expensive to build. On the other hand, taking feedback from testbenches as a replacement for the database is not always feasible. If the coverage of testbenches is not sufficient, the feedback may drive the LLMs to generate the codes that can only pass the testbenches, which may not be the exact design consistent with the specifications. If the coverage of testbenches is sufficient, there may be too much feedback, which may then overwhelm the LLMs. Without the help of external information, our solution aims to improve the reasoning ability of the LLMs in HDL generation by reducing the design space of the HDL codes using LLM-based classification.\nRecent research [20] on the reasoning ability of LLMs reveals that LLMs suffer from multi-hop reasoning. For example, if we want the LLMs to take a value a from an array A[10] and then use a as an index to access another array B[10], the LLMs may successfully provide the correct answer. However, if we further ask the LLMs to use the value from B as an index to access another array C[10], the LLMs are likely to fail. Accordingly, the reasoning ability of state-of-the-art LLMS is usually limited to one-hop reasoning.\nAccording to the discussion above, we should shorten the reasoning hops in HDL generation using LLMs. In other words, we should extract as many conditional decisions as possible in this procedure. Accordingly, we split the task of HDL generation in conditional several sub-tasks, whose numbers of reasoning hops are smaller than the direct code generation from specifications and thus do not exceed the reasoning ability of the LLMs. This strategy is similar to Chain-of-Thought (CoT) [21] in improving the reasoning ability of the LLMS using the self-planning ability of the LLMs themselves. This method performs well in scenarios such as general-purpose QA. However, the CoT method is not suitable for the HDL code generation tasks, because the training datasets of HDL design are relatively small and usually lack comprehensive information about the low-level details of circuit design knowledge, e.g., combinational logic and sequential circuits. Consequently, LLMs struggle with self-planning during the HDL design process and cannot directly use the internal knowledge of the design methodology without explicit guidance. Referring to the design flow of human experts, we aim to design a method to guide the LLMs in generating the HDL codes in a human-like procedure, while keeping the sub-tasks within the reasoning ability of the LLMs."}, {"title": "III. PROPOSED APPROACH", "content": "In this section, we introduce the proposed method in detail. Fig. 2 shows the workflow of our approach. Similar to human experts, who leverage their experience and knowledge to determine the appropriate design types for a specific task, we start by classifying the type of the circuit based on the specification. The typical types include combinational and sequential circuits. However, for a specification, if the code generation based on the types cannot generate the correct codes, it will be considered as general type and processed by a general procedure. With this classification, the required depth of reasoning by LLMs can thus be reduced, so that the hallucination of LLMs in subsequent steps can be mitigated due to more specific available information.\nAfter circuit type classification, LLMs can thus generate HDL design according to these types. SEQU is the procedure for sequential logic, and COMB is for combinational logic. There is also a general procedure, BEHAV, to handle the tasks that are hard to process with SEQU or COMB. To take advantage of the available type information, we also transform the original design specification into a specific information list with respect to each circuit. This explicit specification is a further step in reducing the depth of reasoning and enhancing the quality of generated Verilog codes."}, {"title": "A. Circuit Classification", "content": "For a given specification, the first task is to classify the circuit type using LLMs, as Type Classifier shown in Fig. 2. This classification only needs to be performed once, and the circuit type will be shared in all iterations. A na\u00efve approach to classification is to ask LLMs to directly determine the type of the circuit based on the specifications. However, LLMS may have hallucinations during this process, because they are not specifically trained for this kind of tasks. In addition, the necessary information for classification may be obscured by the semantics or the style of the language description of the specifications. For instance, if the specification only requires a combinational logic that functions within a sequential circuit, LLMs may incorrectly classify the circuit as a sequential logic.\nTo mitigate this hallucination, we first ask LLMs to generate Verilog codes for the target circuit using the given design specification. Since LLMs may have the ability to capture the structure of the circuit, the resulting Verilog codes can be used to deduce the target circuit types, although the circuit may not be functionally correct."}, {"title": "B. Information List Extraction", "content": "After circuit type classification, the LLMs proceed to extract more specific information from the design specifications, As illustrated in Fig. 2 Information List. The purpose of the information extraction is to transform the implicit information contained within the specifications into explicit information about the relationship between the module inputs and outputs. This transformation also helps eliminate redundant information, such as descriptions of the application scenarios, which could lead to hallucinations in the later steps.\nThe extraction process varies between circuit types. For combinational logic, the information list forms the truth table as depicted in Fig. 8 in the Appendix, while for sequential logic, besides the relationship between the inputs and outputs, it is crucial to include timing descriptions in the information list, as shown in Fig. 7 in the Appendix.\nObserving the flow in Fig. 2, all the information lists are generated in the first iteration and stored in the information list cluster with their testbench pass rate p, as shown in Info. List Cluster. Apparently, all three procedures either directly rely on the newly generated information list or rely on the information list selected from the last iteration. If there is missing or incorrect information in the list, the error will be propagated to subsequent steps, such as truth table and state-transition table generation, potentially impacting the final generated codes. On the other hand, an information list of good quality benefits the subsequent steps by providing comprehensive information and emphasizing the crucial information, thereby improving the overall effectiveness of the code generation process."}, {"title": "C. Type-specific Procedures for Circuit Geneartion", "content": "After information list extraction in the first iteration, the next step is to instruct LLMs to convert the extracted information list into a standardized format, as depicted in the dashed rectangles in Fig. 2 (a). These formats naturally break down the tasks into manageable sub-tasks that fall within the reasoning capabilities of the LLMs. Additionally, the formatted data, such as the truth table, can be captured by scripts and further processed by EDA tools.\nFor the sequential logic procedure, SEQU, this format is the state-transition table, which is derived from the information list by the reasoning ability and internal knowledge of LLMs, as shown in the example in Fig. 9 in Appendix. For the combinational logic procedure, COMB, it is the JSON formatted truth table, as shown in the example in Fig. 10 in Appendix.\nIn the next step, depending on the type of the circuit, the LLMs generate the codes by following the design paradigms. For sequential logic, as shown in Fig. 2(a) SEQU, we employ the three-always-block method to complete the tasks. To ensure the sub-tasks remain within the reason capabilities of the LLMs, we instruct the LLMs to complete the always blocks sequentially. Before generating the code of an always block, we allow the LLMs to produce a description of this block, providing space for reasoning and potentially minimizing hallucination. Subsequently, the LLMs merge the generated always blocks into a complete module, as shown in Fig. 11 and Fig. 12 in Appendix.\nFor combinational logic, the standard format is the truth table formatted in JSON, which, unlike the truth table in information list, can be easily captured and processed by external scripts, as shown in Fig. 2 (a) COMB. For LLMs, performing calculations is a complex task, as they tend to provide answers that resemble the correct response rather than doing detailed calculation steps like human. These answers are usually incorrect. Therefore, LLMs have yet to obtain the ability to simplify the truth table correctly. However, since the truth table is already formatted into JSON, we utilize PyEDA to simplify it into a sum-of-products (SOP) expression. This approach is faster and more accurate than relying solely on the LLMs. Subsequently, the LLMs only need to generate the Verilog codes based on the SOP expression.\nTo mitigate the variation of LLMs, the SEQU or COMB procedure is performed N\u2081 times and generates N\u2081 code samples. These code samples are tested by the testbench, and their quality are measured. The testbench pass rate p not only indicates the code quality but also is a sign of the information list quality. Thus, the testbench pass rate works as a score and is marked on the corresponding information list. Then, it is stored in the information list cluster for the next iteration."}, {"title": "D. General Procedure for Circuit Geneartion", "content": "After the first iteration, tasks without candidates passing the testbench enter the next iteration. For tasks that are challenging to be converted into a standard format, which is indicated by the low pass rate in the first iteration, the behavior design diagram, denoted as BEHAV, is utilized, as shown in Fig. 2(b). In the BEHAV procedure, the information list generated in the first iteration is selected and reused. We select the top-Cs, where Cs is a pre-defined value, information lists from the cluster according to their pass rates, like beam search [22]. Similar to the procedure for sequential logic that emphasizes the always blocks, the behavior design diagram focuses on the components, such as a code block of a for loop to describe an adder with high bit-width, where the truth table is hard to be enumerated or it contains both combinational and sequential logic circuits. We instruct LLMs to divide the task into several components, each associated with different sub-tasks. First, these components are organized into one component list with descriptions. Then, the LLMs generate the codes for a component at a time. Finally, the LLMs integrate these components into a complete module. In each iteration, the BEHAV procedure is executed N\u2082 times and produces N\u2082 code samples. Afterward, it is tested by the testbench. Like the Type-specific procedures, the information lists should also be stored in the cluster and marked with the scores. Since in former iterations, the information lists are already marked with pass rates, to fully use this information, we use the average pass rate of the current iteration and former iterations, denoted as Ps, where ps is the pass rate in iteration s, and sc is the index of the current iteration."}, {"title": "E. Fail-safe and Short-cut Mode", "content": "To handle special situations in the first iteration, two strategies, Fail-safe and Short-cut, are incorporated, as depicted in Fig. 2(a) Fall-safe and Short-cut.\nSince the LLMs have variation in the generated answers, sometimes the script in the later step cannot capture the intermediate results from the former step. We define this error as Format Error. We allocate each task a total of E{f} chance for the format error retry, where intermediate outputs of LLMs are deprecated and the current procedure is executed again. The number of current Format Errors is denoted as e{f}. If e{f}>E{f}, we conclude that the type-specific procedures are unsuitable for the task, indicating that LLMs cannot answer or generate an incorrect answer. In this case, the iteration switches to general procedure and enters Fail-safe mode. In Fail-safe mode, we allocate all remaining test budgets to the general procedure and regenerate the information lists before each execution of the general procedure.\nConversely, if we identify a promising information list that allows the type-specific procedures to nearly pass the testbench, we should allocate all remaining test budgets to this information list and keep using the previous procedure. We denote this mode as Short-cut mode. We establish a threshold W for the testbench sample pass rate p. If the testbench sample pass rate p exceeds W, we consider the information list to be the seed player, and the corresponding procedure is sufficiently robust for the task. In this scenario, the search process switches to Short-cut mode, where all remaining test budgets are assigned to the current procedure, and the current information list is also kept."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "To demonstrate the performance of the proposed method, we conducted experiments on the VerilogEval [19] dataset, which contains 299 tasks with specifications, testbench, and ground truth Verilog codes. In this dataset, the tasks are divided into two categories: VerilogEval-human and VerilogEval-machine. The specifications of VerilogEval-human tasks are written by human experts, while the VerilogEval-machine tasks are generated by gpt-3.5-turbo [23] according to the ground truth codes.\nFor the evaluation, we adopted GPT4 [12] in the proposed framework through OpenAI APIs. Despite the newer GPT4s released, such as gpt-4-0125-preview, we chose gpt-4-0613 as our base model, because it was released before the VerilogEval dataset was published. Therefore, the training sets of this GPT4 version do not contain the VerilogEval dataset. The temperature of the LLM was set to 0.5 for all the experiments. The maximum context length was set to 4096. For simulation, we used Icarus Verilog (iverilog) [24] to run the testbench and the generated codes.\nWe used the Pass@k [25] as the metric to evaluate the functional correctness of the generated codes. In Pass@k, k code samples are tested and we evaluate the number of the samples passed the testbench. It is defined as\nPass@k:= E 1 \u2211 C/n (1)\nProblems\nwhere n is the sample number per task, and c is the number of samples that passed the testbench. Here, we consider that every execution of testbench is counted as a code sample tested in Pass@k. We set n to 10 and k to 1, 5, and 10 in the experiments, respectively. The maximum format retry E{f} was set to 10 for both the proposed method and the baseline, in which the LLM directly generates the codes from specifications. For the search parameters, we set the max iteration S{max} to 3. Thus the configuration of the number of generated code samples can be expressed as (N1,N2,N3), which is set to (7,2,1).\nThe Top-C's is set as C2=N2 and C3=1, which means in the second iteration, the two code samples are generated from top-2 information lists, and in the last iteration, all the rest ot the test budgets are given to the top-1 information list. The Short-cut threshold W is set to 0.95. To reduce the token cost, we first conducted the baseline experiments. Then, we selected the difficult tasks, for which the baseline method cannot generate valid codes to pass the test after 10 retries, to be processed by the proposed framework."}, {"title": "V. CONCLUSION", "content": "In this work, we proposed a human-expert-inspired method to improve the performance in HDL code generation using LLMs. We first let LLMs classify the type of the circuit based on the specifications. Then, according to the type of the circuit, three different design procedures are used to mitigate the hallucination of LLMs. Besides, we use a search method to distribute the test budgets efficiently. The experimental results show that our method can significantly improve the functional correctness of the generated HDL."}]}