{"title": "Classification-Based Automatic HDL Code Generation Using LLMs", "authors": ["Wenhao Sun", "Bing Li", "Grace Li Zhang", "Xunzhao Yin", "Cheng Zhuo", "Ulf Schlichtmann"], "abstract": "While large language models (LLMs) have demonstrated the ability to generate hardware description language (HDL) code for digital circuits, they still suffer from the hallucination problem, which leads to the generation of incorrect HDL code or misunderstanding of specifications. In this work, we introduce a human-expert-inspired method to mitigate the hallucination of LLMs and improve the performance in HDL code generation. We first let LLMs classify the type of the circuit based on the specifications. Then, according to the type of the circuit, we split the tasks into several sub-procedures, including information extraction and human-like design flow using Electronic Design Automation (EDA) tools. Besides, we also use a search method to mitigate the variation in code generation. Experimental results show that our method can significantly improve the functional correctness of the generated Verilog and reduce the hallucination of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "As Moore's Law slowing down, there is an increasing demand for customized VLSI design. One of the key steps in the hardware design process is writing the hardware description language (HDL) code. However, HDL programming is time-consuming and labor-intensive. Therefore, the automatic HDL code generation from the specifications in natural language has attracted much attention in recent years.\nAmong the automatic HDL code generation solutions, large language models (LLMs) code generation is one of the most promising solutions. LLMs have achieved remarkable success in various fields, such as machine translation [1] and robot trajectory planning [2]. In software design, LLMs have also demonstrated the ability to generate the code for various programming languages [3]. In the field of hardware design, generative models have been employed to create designs [4]. Additionally, researchers have also recognized the potential of LLMs in generating HDL codes [5].\nDespite the advance of LLMs, there are still many challenges that hinder the application of LLMs in HDL code generation. One of the challenges is the hallucination problem. The hallucination problem refers to the generation of incorrect HDL code or misunderstanding of the specifications. The na\u00efve way as illustrated in Fig. 1(a) to apply LLMs to generate HDL codes in one iteration usually leads to many errors. The hallucination problem is caused by the lack of reasoning ability and training in the HDL domain, since the amount of datasets for HDL codes is limited. To solve this problem, a human expert can be involved as a supervisor in the workflow of LLMs, such as Chip-chat [6], as illustrated in Fig. 1(b), where human expert monitors the code generate process and provides to LLMs to correct the code. However, involving human experts increases the cost of this automatic design flow, and the effieceny of this work flow may also compromise the efficiency of the human expert.\nChipNeMo [7] and [8]-[11] aim to enhance the quality of generated HDL codes by fine-tuning the LLMs with augmented datasets. Therefore, the LLMs can learn more about the HDL design, which helps them to generate the correct HDL codes. However, this method depends on the quality of the augmented datasets. Besides, since fine-tuning an LLM needs a large amount of computation resources, the fine-tuned models are relatively small compared with commercial models such as GPT4 [12]. Accordingly, the reasoning ability of the fine-tuned LLMs may be limited.\nThe other method to enhance the quality of automatically genearted HDL codes is to leverage the in-context learning ability [13] of the LLMs. The in-context learning ability allows the LLMs to learn how to generate new patterns from the examples in the context. One of the applications is retrieval augmented generation (RAG) [14], as illustrated in Fig. 1(c), which retrieves the examples from a database to guide the code generation using LLMs. Previous work such as GPT4AIGChip [15]-[17] uses RAG either to retrieve the examples similar to the specifications to help the generation or to retrieve the examples for error correction. However, these methods need a database with a large number of examples, which may need human experts to build. AutoChip [18] uses the feedback from testbenches as a replacement for the database. Therefore, it can avoid the database building process. However, the testbenches do not always have feedbacks. In some scenarios, the final testbench can only show the pass rate of the test samples,"}, {"title": "II. MOTIVATION", "content": "To improve the performance of Large Language Models (LLMs) in HDL code generation in a training-free manner, the key is to use the in-context learning and reasoning ability of the LLMs. However, RAG-based methods heavily rely on the quality of the databases, which are also expensive to build. On the other hand, taking feedback from testbenches as a replacement for the database is not always feasible. If the coverage of testbenches is not sufficient, the feedback may drive the LLMs to generate the codes that can only pass the testbenches, which may not be the exact design consistent with the specifications. If the coverage of testbenches is sufficient, there may be too much feedback, which may then overwhelm the LLMs. Without the help of external information, our solution aims to improve the reasoning ability of the LLMs in HDL generation by reducing the design space of the HDL codes using LLM-based classification.\nRecent research [20] on the reasoning ability of LLMs reveals that LLMs suffer from multi-hop reasoning. For example, if we want the LLMs to take a value a from an array A[10] and then use a as an index to access another array B[10], the LLMs may successfully provide the correct answer. However, if we further ask the LLMs to use the value from B as an index to access another array C[10], the LLMs are likely to fail. Accordingly, the reasoning ability of state-of-the-art LLMS is usually limited to one-hop reasoning.\nAccording to the discussion above, we should shorten the reasoning hops in HDL generation using LLMs. In other words, we should extract as many conditional decisions as possible in this procedure. Accordingly, we split the task of HDL generation in conditional several sub-tasks, whose numbers of reasoning hops are smaller than the direct code generation from specifications and thus do not exceed the reasoning ability of the LLMs. This strategy is similar to Chain-of-Thought (CoT) [21] in improving the reasoning ability of the LLMS using the self-planning ability of the LLMs themselves. This method performs well in scenarios such as general-purpose QA. However, the CoT method is not suitable for the HDL code generation tasks, because the training datasets of HDL design are relatively small and usually lack comprehensive information about the low-level details of circuit design knowledge, e.g., combinational logic and sequential circuits. Consequently, LLMs struggle with self-planning during the HDL design process and cannot directly use the internal knowledge of the design methodology without explicit guidance. Referring to the design flow of human experts, we aim to design a method to guide the LLMs in generating the HDL codes in a human-like procedure, while keeping the sub-tasks within the reasoning ability of the LLMs."}, {"title": "III. PROPOSED APPROACH", "content": "In this section, we introduce the proposed method in detail. Fig. 2 shows the workflow of our approach. Similar to human experts, who leverage their experience and knowledge to determine the appropriate design types for a specific task, we start by classifying the type of the circuit based on the specification. The typical types include combinational and sequential circuits. However, for a specification, if the code generation based on the types cannot generate the correct codes, it will be considered as general type and processed by a general procedure. With this classification, the required depth of reasoning by LLMs can thus be reduced, so that the hallucination of LLMs in subsequent steps can be mitigated due to more specific available information.\nAfter circuit type classification, LLMs can thus generate HDL design according to these types. SEQU is the procedure for sequential logic, and COMB is for combinational logic. There is also a general procedure, BEHAV, to handle the tasks that are hard to process with SEQU or COMB. To take advantage of the available type information, we also transform the original design specification into a specific information list with respect to each circuit. This explicit specification is a further step in reducing the depth of reasoning and enhancing the quality of generated Verilog codes."}, {"title": "A. Circuit Classification", "content": "For a given specification, the first task is to classify the circuit type using LLMs, as Type Classifier shown in Fig. 2. This classification only needs to be performed once, and the circuit type will be shared in all iterations. A na\u00efve approach to classification is to ask LLMs to directly determine the type of the circuit based on the specifications. However, LLMS may have hallucinations during this process, because they are not specifically trained for this kind of tasks. In addition, the necessary information for classification may be obscured by the semantics or the style of the language description of the specifications. For instance, if the specification only requires a combinational logic that functions within a sequential circuit, LLMs may incorrectly classify the circuit as a sequential logic.\nTo mitigate this hallucination, we first ask LLMs to generate Verilog codes for the target circuit using the given design specification. Since LLMs may have the ability to capture the structure of the circuit, the resulting Verilog codes can be used to deduce the target circuit types, although the circuit may not be functionally correct."}, {"title": "B. Information List Extraction", "content": "After circuit type classification, the LLMs proceed to extract more specific information from the design specifications, As illustrated in Fig. 2 Information List. The purpose of the information extraction is to transform the implicit information contained within the specifications into explicit information about the relationship between the module inputs and outputs. This transformation also helps eliminate redundant information, such as descriptions of the application scenarios, which could lead to hallucinations in the later steps.\nThe extraction process varies between circuit types. For combinational logic, the information list forms the truth table as depicted in Fig. 8 in the Appendix, while for sequential logic, besides the relationship between the inputs and outputs, it is crucial to include timing descriptions in the information list, as shown in Fig. 7 in the Appendix.\nObserving the flow in Fig. 2, all the information lists are generated in the first iteration and stored in the information list cluster with their testbench pass rate p, as shown in Info. List Cluster. Apparently, all three procedures either directly rely on the newly generated information list or rely on the information list selected from the last iteration. If there is missing or incorrect information in the list, the error will be propagated to subsequent steps, such as truth table and state-transition table generation, potentially impacting the final generated codes. On the other hand, an information list of good quality benefits the subsequent steps by providing comprehensive information and emphasizing the crucial information, thereby improving the overall effectiveness of the code generation process."}, {"title": "C. Type-specific Procedures for Circuit Geneartion", "content": "After information list extraction in the first iteration, the next step is to instruct LLMs to convert the extracted information list into a standardized format, as depicted in the dashed rectangles in Fig. 2 (a). These formats naturally break down the tasks into manageable sub-tasks that fall within the reasoning capabilities of the LLMs. Additionally, the formatted data, such as the truth table, can be captured by scripts and further processed by EDA tools.\nFor the sequential logic procedure, SEQU, this format is the state-transition table, which is derived from the information list by the reasoning ability and internal knowledge of LLMs, as shown in the example in Fig. 9 in Appendix. For the combinational logic procedure, COMB, it is the JSON formatted truth table, as shown in the example in Fig. 10 in Appendix.\nIn the next step, depending on the type of the circuit, the LLMs generate the codes by following the design paradigms. For sequential logic, as shown in Fig. 2(a) SEQU, we employ the three-always-block method to complete the tasks. To ensure the sub-tasks remain within the reason capabilities of the LLMs, we instruct the LLMs to complete the always blocks sequentially. Before generating the code of an always block, we allow the LLMs to produce a description of this block, providing space for reasoning and potentially minimizing hallucination. Subsequently, the LLMs merge the generated always blocks into a complete module, as shown in Fig. 11 and Fig. 12 in Appendix.\nFor combinational logic, the standard format is the truth table formatted in JSON, which, unlike the truth table in information list, can be easily captured and processed by external scripts, as shown in Fig. 2 (a) COMB. For LLMs, performing calculations is a complex task, as they tend to provide answers that resemble the correct response rather than doing detailed calculation steps like human. These answers are usually incorrect. Therefore, LLMs have yet to obtain the ability to simplify the truth table correctly. However, since the truth table is already formatted into JSON, we utilize PyEDA to simplify it into a sum-of-products (SOP) expression. This approach is faster and more accurate than relying solely on the LLMs. Subsequently, the LLMs only need to generate the Verilog codes based on the SOP expression.\nTo mitigate the variation of LLMs, the SEQU or COMB procedure is performed $N_i$ times and generates $N_i$ code samples. These code samples are tested by the testbench, and their quality are measured. The testbench pass rate p not only indicates the code quality but also is a sign of the information list quality. Thus, the testbench pass rate works as a score and is marked on the corresponding information list. Then, it is stored in the information list cluster for the next iteration."}, {"title": "D. General Procedure for Circuit Geneartion", "content": "After the first iteration, tasks without candidates passing the testbench enter the next iteration. For tasks that are challenging to be converted into a standard format, which is indicated by the low pass rate in the first iteration, the behavior design diagram, denoted as BEHAV, is utilized, as shown in Fig. 2(b). In the BEHAV procedure, the information list generated in the first iteration is selected and reused. We select the top-$C_s$, where $C_s$ is a pre-defined value, information lists from the cluster according to their pass rates, like beam search [22]. Similar to the procedure for sequential logic that emphasizes the always blocks, the behavior design diagram focuses on the components, such as a code block of a for loop to describe an adder with high bit-width, where the truth table is hard to be enumerated or it contains both combinational and sequential logic circuits. We instruct LLMs to divide the task into several components, each associated with different sub-tasks. First, these components are organized into one component list with descriptions. Then, the LLMs generate the codes for a component at a time. Finally, the LLMs integrate these components into a complete module.\nIn each iteration, the BEHAV procedure is executed $N_s$ times and produces $N_s$ code samples. Afterward, it is tested by the testbench. Like the Type-specific procedures, the information lists should also be stored in the cluster and marked with the scores. Since in former iterations, the information lists are already marked with pass rates, to fully use this information, we use the average pass rate of the current iteration and former iterations, denoted as $P_s$, where $p_s$ is the pass rate in iteration s, and $s_c$ is the index of the current iteration."}, {"title": "E. Fail-safe and Short-cut Mode", "content": "To handle special situations in the first iteration, two strategies, Fail-safe and Short-cut, are incorporated, as depicted in Fig. 2(a) Fall-safe and Short-cut.\nSince the LLMs have variation in the generated answers, sometimes the script in the later step cannot capture the intermediate results from the former step. We define this error as Format Error. We allocate each task a total of $E_f$ chance for the format error retry, where intermediate outputs of LLMs are deprecated and the current procedure is executed again. The number of current Format Errors is denoted as $e_f$. If $e_f>E_f$, we conclude that the type-specific procedures are unsuitable for the task, indicating that LLMs cannot answer or generate an incorrect answer. In this case, the iteration switches to general procedure and enters Fail-safe mode. In Fail-safe mode, we allocate all remaining test budgets to the general procedure and regenerate the information lists before each execution of the general procedure.\nConversely, if we identify a promising information list that allows the type-specific procedures to nearly pass the testbench, we should allocate all remaining test budgets to this information list and keep using the previous procedure. We denote this mode as Short-cut mode. We establish a threshold W for the testbench sample pass rate p. If the testbench sample pass rate p exceeds W, we consider the information list to be the seed player, and the corresponding procedure is sufficiently robust for the task. In this scenario, the search process switches to Short-cut mode, where all remaining test budgets are assigned to the current procedure, and the current information list is also kept."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "To demonstrate the performance of the proposed method, we conducted experiments on the VerilogEval [19] dataset, which contains 299 tasks with specifications, testbench, and ground truth Verilog codes. In this dataset, the tasks are divided into two categories: VerilogEval-human and VerilogEval-machine. The specifications of VerilogEval-human tasks are written by human experts, while the VerilogEval-machine tasks are generated by gpt-3.5-turbo [23] according to the ground truth codes.\nIn Table I, we compare the baseline and the proposed method. In this table, Baseline is the direct generation using GPT-4. FULL and SYNTAX are the results of the functional correctness and syntax correctness of the proposed method, respectively. The contributions of the COMB, SEQU and BEHAV procedures are individually shown as COMB, SEQU and BEHAV in the columns, respectively. For VerilogEval-human, the proposed method outperforms the baseline with 4.7%, 11.0%, and 14.7% in Pass@1, Pass@5, and Pass@10, respectively. In VerilogEval-machine, the proposed method also improves the pass rate in all Pass@k settings, where the improvements are over 5% in Pass@5 and Pass@10.\nIn Table I, it is worth noting that all three procedures, COMB, SEQU, and BEHAV, contributed to the improvements in the VerilogEval-human dataset. However, the COMB procedure had no improvement in the VerilogEval-machine dataset. This is because the specifications in VerilogEval-machine dataset are generated by summarizing the ground truth codes with LLMS, which means the logical relationship between the inputs and outputs is already shown in the specifications. Hence, the LLMs can bypass the calculation and reasoning procedure in solving combinational logic tasks, such as simplifying the truth table.\nTo demonstrate the influence of the search parameter, the results of the search configuration (5,3,2) and (7,2,1) are compared in Fig. 3. We observe that the configuration (7,2,1) is better than (5,3,2) in most cases except for Pass@1 and Pass@5 in VerilogEval-machine. Overall, the configuration (7,2,1) is better than (5,3,2) in the VerilogEval dataset. The potential reason is that configuration (7,2,1) has more information list candidates, contributing to a higher possibility of finding a high-quality information list.\nThe task error rate distributions of the baseline and the proposed method in the hard tasks of VerilogEval are illustrated in Fig. 4, where the error rate for each task is from the best code sample in Pass@10. We split the error rate distributions into five intervals with step 0.2. The proportions of the intervals are shown on the x-axis. It can be observed that the proposed method not only improves the Pass@k but also reduces the code sample error rates. However, the number of the hardest tasks, whose error rates are higher than 0.8, as shown in the rightmost of the bars, did not reduce in VerilogEval-human and is only slightly lower in VerilogEval-machine. This indicates that there remains room for improvement in the proposed method.\nIn Table II, the performance of AutoChip [18] and the proposed method are compared. For both sequential and combinational logic, we randomly chose ten tasks from the difficult tasks of VerilogEval-human dataset. Here, to evaluate the performance of the methods under the same test standard, we followed the test standard in Pass@10, where each execution of the testbench is counted as a test, regardless of whether it happened inside an iteration or after a complete attempt of the method. For AutoChip, the iteration of the feedback, n, was set to 10. The results show that the proposed method performs slightly better on sequential logic and significantly better on combinational logic. The reason is that AutoChip heavily depends on the feedback of the testbench. However, the testbench in VerilogEval only provides limited hints for sequential logic and almost no hints for combinational logic. On the contrary, the proposed method does not need the feedback of the testbench and effectively compensates for the shortcomings of LLMs in handling combinational logic.\nTable III presents a comparison of type classification accuracy between methods with and without the usage of the generated codes. In the VerilogEval-machine dataset, both methods can classify the type of the circuit correctly. However, in the VerilogEval-human dataset, the method using codes generated by LLMs has a 1.7% error rate, while the na\u00efve method has a 5.1% error rate. Analysis of the error cases reveals that the errors are due to misleading specifications, which require combinational logic parts from a sequential logic circuit. The na\u00efve method focuses more on the semantics of the specifications, while the method using codes generated by LLMs can leverage the internal knowledge of the Verilog design and avoid some errors.\nThe comparison of the information lists between two examples whose codes passed and failed in the simulation is shown in Fig. 5 and Fig. 6. In the case of the sequential logic task, as shown in Fig. 5, the information list of the failed codes contains fewer descriptions compared to the list of the codes that passed in the simulation. Because of the variation of LLMs, some information lists might be generated with information loss, leading to the generation of failed codes. While detailed descriptions might be considered redundant by human experts, they help the LLMs to concentrate on the key information. In the examples of combinational logic task, as shown in Fig. 6, the information list, which failed in the simulation, has an incorrect output description. Both the outputs, (a=0, b=0, c=1, d=0) and (a=1, b=0, c=1, d=0), are incorrect, which is caused by the hallucination of the LLMs. The correct outputs should be the information list of the codes that passed the testbench."}, {"title": "V. CONCLUSION", "content": "In this work, we proposed a human-expert-inspired method to improve the performance in HDL code generation using LLMs. We first let LLMs classify the type of the circuit based on the specifications. Then, according to the type of the circuit, three different design procedures are used to mitigate the hallucination of LLMs. Besides, we use a search method to distribute the test budgets efficiently. The experimental results show that our method can significantly improve the functional correctness of the generated HDL."}]}