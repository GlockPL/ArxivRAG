{"title": "fluke: Federated Learning Utility frameworK for Experimentation and research", "authors": ["Mirko Polato"], "abstract": "Since its inception in 2016, Federated Learning (FL) has been\ngaining tremendous popularity in the machine learning com-\nmunity. Several frameworks have been proposed to facilitate\nthe development of FL algorithms, but researchers often re-\nsort to implementing their algorithms from scratch, including\nall baselines and experiments. This is because existing frame-\nworks are not flexible enough to support their needs or the\nlearning curve to extend them is too steep. In this paper, we\npresent fluke, a Python package designed to simplify the\ndevelopment of new FL algorithms. fluke is specifically de-\nsigned for prototyping purposes and is meant for researchers\nor practitioners focusing on the learning components of a fed-\nerated system. fluke is open-source, and it can be either used\nout of the box or extended with new algorithms with minimal\noverhead.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) is a machine learning paradigm\nwhere a model is trained across multiple devices or servers\nholding local data samples, without exchanging them. This\nparadigm has been gaining popularity in recent years due to\nits privacy-preserving properties, as it allows training mod-\nels on data that cannot be shared due to privacy concerns,\nsuch as medical records, financial data, or user behavior.\nThe success of FL is evidenced by the constant increasing\nnumber of publications on the topic in top conferences.\nAs a consequence of this growing interest, several FL\nframeworks have been proposed, like Flower [6], Tensor-\nFlow Federated (TFF) [5], FATE [36], NVIDIA Flare [44],\nOpenFL [18], just to name a few. Although the cited frame-\nwork are very powerful and flexible, they are not specifically\ndesigned for research purposes, and they are not easy to ex-\ntend with new algorithms. Flower is the only exception, as\nit is designed to be extensible and user-friendly, but still its\nconfiguration and customization are not trivial tasks\u00b9. Being\nnot specifically designed for fast prototyping, these frame-\nworks may seem somewhat over engineered.\nIt is common in FL research (ant in general) the need\nfor quickly implement new algorithms to test new ideas,\nhttps://flower.ai/docs/framework/tutorial-series-customize-\nthe-client-pytorch.html\nand this is not always easy with the mentioned frame-\nworks. To support this claim, we can look at the number\nof recent papers proposing new FL algorithms with imple-\nmentations not based on any framework, where all base-\nlines and experiments have been implemented from scratch\n[32, 23, 12, 28, 29, 15, 34, 47, 2, 39]. We cannot say for\nsure why these authors decided to re-implement everything\nfrom scratch, but we can speculate that the existing frame-\nworks were not flexible enough to support their needs or the\nlearning curve to extend them was too steep.\nfluke raises from the necessity of quickly prototyping\nnew federated learning algorithms in Python without wor-\nrying too much about aspects that are not specifically re-\nlated to the algorithms themselves. fluke also aims to im-\nprove the reproducibility of FL research, making it eas-\nier for researchers to replicate and validate studies. fluke\nis meant for researchers and practitioners focusing on the\nlearning components of a federated system which is simu-\nlated on a single machine2. In its current version (release\n0.3.4), fluke assumes a centralized architecture, where the\nsimulated communication between the server and the clients\nis stable.\nWe can summarize the main features of fluke as follows:\n\u2022 Open source: fluke is an open-source Python package.\nThe code is written following the PEP8 guidelines and is\ncomprehensively documented\u00b3. The code is also \u201ceasy-\nto-read\" because it is written to mimic the description\nand mathematical notation of the algorithm definitions\nas much as possible. The code is available on GitHub4.\n\u2022 Easy to use: fluke is designed to be easy to use oof-\nthe-shelf. Running a federated learning experiment is as\nsimple as running a single command.\n\u2022 Easy to extend: fluke is designed to be easy to extend\nminimazing the overhead of adding new algorithms. Im-\nplementing a new algorithm simply requires the defini-\ntion of the client and/or the server classes.\n\u2022 Up-to-date: fluke comes with several state-of-the-art\nfederated learning algorithms and datasets and it is regu-\nOne possible future development could be to allow fluke to\nrun on more machines.\nhttps://makgyver.github.io/fluke\nhttps://github.com/makgyver/fluke"}, {"title": "fluke", "content": "fluke is available at https://pypi.org/project/fluke-fl/ and\ncan be installed using the following command:\n$ pip install fluke-fl\nconsole\nThe installation allows to use both the fluke Python API\nas well as the command line interface. Being an open-source\nproject available on GitHub at https://github.com/makgyver/\nfluke, it is also possible to start using fluke by cloning the\nrepository.\nAs mentioned in the introduction, fluke simulates a\nfederated learning environment and the communication be-\ntween the server and the clients. A very high level overview\nof how the simulation works is given in Algorithm 1. The\npseudo-code describes the main steps of a FL algorithm\nin fluke. Clearly, specific algorithms may have different\nsteps, but the general structure is the same. In fluke the"}, {"title": "Algorithm 1 Centralized Federated Learning in fluke", "content": "1: procedure SERVER.FIT\n2:  $\\theta \\leftarrow$ init()\n  $\\triangleright$ Initialize the global model\n3:  for t = 1, 2, ..., T do $\\triangleright$ T is the number of rounds\n4:  E $\\leftarrow$ select_clients(C) $\\triangleright$ C is the set of clients\n5:  broadcast_model($\\theta$, E)\n6:  for c$\\in$ E do\n7:  c.local_training()\n8:   $\\theta_c\\leftarrow$ receive_model(c)\n9:  end for\n10:   $\\theta \\leftarrow$ aggregate({$\\theta_c$}$_{c\\in E}$)\n11:  end for\n12: end procedure\n13:\n14: procedure CLIENT.LOCAL_TRAINING\n15:   $\\theta_c\\leftarrow$ receive_model(server)\n16:   $\\theta_c \\leftarrow$ fit($\\theta_c$, $D_e$) $\\triangleright$ update $\\theta_e$ using the local data\n17:  send_model($\\theta_c$, server)\n18: end procedure\nlearing process is synchronous, i.e., the server waits for all\nthe clients to finish their local training before aggregating the\nmodels. The server is responsible for selecting the clients to\nThe algorithms are usually selected according to the venue\nand/or paper's citations.\nparticipate in the training, broadcasting the global model to\nthe clients, and aggregating the models received from the\nclients. The clients are responsible for training the model on\ntheir local data and sending the updated model to the server."}, {"title": "Architecture overview", "content": "In this section, we provide an overview of the architec-\nture of the fluke package. In Figure 1, we show the main\nmodules/submosules of fluke and the interactions between\nthem. The arrows mean that the source module uses the tar-\nget module. The module utils is used by all the other mod-\nules. In the following, we give a brief description of each\nfluke's modules:\ndata contains the classes to load the datasets and to manage\nthe data distribution (and training-test splitting) across\nthe clients and the server. fluke at the moment offers\n13 datasets [27, 9, 8, 40, 13, 25, 26, 49] and five different\nnon-IID distribution functions.\nserver contains the base classes that define the server-side\nlogic of a FL algorithm. The base server class behaves as\nin the FedAvg [38] algorithm.\nclient contains the base classes that define the client-side\nlogic of a FL algorithm. The base client class behaves as\nin the FedAvg [38] algorithm.\ncomm contains the classes to be used for handling the com-\nmunication between the server and the clients. Client and\nserver must exchange data (e.g., the model) or informa-\ntion through a (simulated) channel (Channel class). Us-\ning the channel allows fluke to keep track of the com-\nmunication overhead (e.g., for logging purposes). Direct\nmethod calls between the parties are only used to trig-\nger events (e.g., the server triggers the client-side local\ntraining).\nalgorithms is the module where all the FL algorithms\navailable in fluke are defined. Each algorithm has its\nown submodule with all the classes needed to run the\nalgorithm. Besides potential support classes/functions,\neach algorithm must define a class that inherit from\nfluke.algorithms.CentralizedFL and classes\nthat inherit from fluke.client.Client and/or\nfluke.server.Server.\nnets is a support module with classes that define several\nneural networks used in the literature.\nutils is a module with a collection of utility functions and\nclasses that are used across the package.\nevaluation contains classes to evaluate the performance of\nthe algorithms."}, {"title": "fluke Command Line Interface (CLI)", "content": "The fluke CLI is the easiest and fastest way to run a fed-\nerated learning experiment with fluke. It comes with two\ncommands fluke and fluke-get.\nfluke command The fluke command is the main com-\nmand of the fluke package. This command allows to run\nthree different types of experiments (EXP_TYPE):"}, {"title": "", "content": "console\n$ fluke --config=EXP CFG EXP TYPE ALG_CFG\nwhere:\nEXP_CONFIG is the path to the experiment configuration\nfile (independent of the algorithm). In this file, the user\ncan specify the dataset, the data distribution, the number\nof clients, the number of rounds, the seed, the device, and\nother parameters related to the experiment.\nALG_CONFIG is the path to the algorithm configuration file.\nHere, the user must specify the algorithm to use with\nall its hyper-parameters (e.g., the model, the server's and\nclients' hyper-parameters).\nThe division between the experiment and the algorithm\nconfiguration allows the user to run different algorithms on\nthe same experimental setup.\nWhen the fluke command is used with the types\nclients-only and centralized, some of the parameters in\nthe configuration files are not considered or not applicable,\ne.g, with centralized, the data distribution is ignored. In-\nstead, other parameters are computed accordingly, e.g., in\nthe clients only case, the number of epochs is computed\nbased on the number of rounds, the number of clients and\nthe participation rate.\nThe output of this command is the evaluation of the al-\ngorithm on the test set(s) (according to the configuration)\nin terms of accuracy, precison, recall and F1 score (in both\nmicro and macro average) at each round7.\nfluke-get command The fluke-get command is a util-\nity to get a configuration file template for the specified algo-\nrithm or for the experiment. It directly downloads the tem-\nFor simplicity we omit the potential OPTIONS. See the docu-\nmentation for more details.\n\"The evaluation frequency can be set in the configuration file."}, {"title": "", "content": "console\n$ fluke-get config CFG_FILENAME\nwhere CFG_FILENAME is the name of the configuration file\n(without the extension .yaml) to download. To get the list of\navailable configuration files, the user can run the command\nfluke-get list."}, {"title": "Documentation and tutorials", "content": "In the spirit of making fluke easy to use, we provide a com-\nprehensive documentation that includes a quick start guide,\nand a developer guide. The documentation is available at the\nfollowing URL: https://makgyver.github.io/fluke, and it also\nincludes a series of tutorials that show how to use fluke in\npractice. The tutorials on the API are structured as Python\nnotebooks that can be run interactively on Google Colab."}, {"title": "Use cases", "content": "fluke can be employed in several scenarios related to FL re-\nsearch, however it is mainly designed for quick benchmark-\ning and quick prototyping of new ideas. In this section, we\naddress these two use cases showcasing how fluke can be\nused in practice."}, {"title": "Benchmarking", "content": "Benchmarking is a common practice in machine learning re-\nsearch, and it is particularly important in federated learning,\nwhere the number of algorithms is increasing rapidly. In this\nscenario, fluke can be used off-the-shelf to compare the\nperformance of different algorithms on the same dataset and\nunder the same conditions. To run a benchmark, one can use\nboth the Python API and the command line interface (CLI)\nof fluke. However, the fastest way to run a benchmark of\ndifferent FL algorithms is to use the fluke command with\nthe federation experiment type.\nSince the configuration of the experiment is decoupled\nfrom the algorithm, the user can run different algorithms on\nthe same experimental setup very easily: the user can define\na single experiment configuration file and then run different\nalgorithms by changing the algorithm configuration, e.g.,"}, {"title": "", "content": "console\n$ fluke --config=exp.yaml federation fedavg.yaml\n$ fluke --config-exp.yaml federation fedprox.yaml\nwhere exp.yaml is the experiment configuration file and\nfedavg.yaml and fedprox.yaml are the two hypothetical con-\nfiguration files for the FedAvg [38] and FedProx [45] algo-\nrithms, respectively. If configured to log on Weights & Bi-\nases [7], the results of the different algorithms can be easily\ncompared and analyzed. Running a batch of experiments can\nbe done by using a simple script that runs the fluke com-\nmand with different algorithm configuration files.\nThe plot in Figure 2 shows the accuracy of several al-\ngorithms run with fluke using the same setting with de-\nfault hyper-parameters of the algorithms on the MNIST [27]"}, {"title": "", "content": "Participation rate 20%, learning rate 0.1 (no scheduling), batch\nsize 32, 5 local steps."}, {"title": "Implementing a new algorithm", "content": "The second main use case is the development of a new feder-\nated learning algorithm. In this scenario, the algorithm def-\ninition must be done using the fluke Python API. When a\nnew federated learning algorithm is defined in the literature,\nits functioning is described by detailing the behavior of the\nserver and/or the clients. This is exactly how an algorithm is\nadded in fluke.\nNote: for space reasons, the following steps do not include\nall the necessary details, but they should be enough to under-\nstand the process. Please refer to the online documentation\n(https://makgyver.github.io/fluke) for more information, in\nparticular the related tutorial 9.\nDefining the Client class In fluke, the Client class rep-\nresents the client-side logic of a FL algorithm. We suggest\nto start from this class when adding a new algorithm. A new\nclient class C must inherit from fluke.client.Client10.\nThe main methods that one may need to override are:\n\u2022 fit: this method is responsible for training the client's\nmodel on its data. This is usually the method that needs\nto be overridden.\n\u2022 send_model: this method sends the model to the server.\nBy default the model is meant to be the neural network\nweights, but it can be anything. The model must be sent\nthrough the channel (i.e., a fluke.comm.Channel ob-\nject) that is an instance variable of the Client class.\n\u2022 receive_model: this method receives, through the\nchannel, the model from the server.\n\u2022 finalize: this method is called at the end of the learn-\ning (after the last round). It is useful to perform any final\noperations, e.g., fine-tuning.\nDefinining the Server class The server-side logic of\na FL algorithm is represented by the Server class\nin fluke. A new server class s must inherit from\nhttps://makgyver.github.io/fluke/examples/tutorials/\nfluke_custom_alg.html\n10 Or PFLClient in case of personalized FL.\nfluke.server.Server. The main methods that one may\nneed to override are:\n\u2022 fit: this method is where all the federated training hap-\npens. Calling the server's fit method means starting the\nfederation simulation. As long as the protocol follows the\nusual one in centralized FL, this method should not need\nto be overridden.\n\u2022 aggregate: this method is responsible for aggregating\nthe models received from the clients. By default, it per-\nforms a (weighted) average of the client models.\n\u2022 broadcast_model: broadcasts the global model to the\nclients. By default, it is called at the beginning of each\nround.\n\u2022 finalize: this method is called at the end of the learn-\ning (after the last round). Ideally, this method should be\nused to perform any final operation, e.g., to get the final\nevaluation of the model. In this method the server can\ntrigger the finalization client-side.\nDefining the algorithm The final class to define is the one\nrepresenting the whole algorithm, which must inherit from\nthe fluke.algorithms.CentralizedFL11 class. In this\nclass there is no logic involved, it is only the entry point of\nthe algorithm and its sole purpose is to initialize the server\nand the clients. The main methods that one should need to\noverride are:\n\u2022 get_client_class: that returns the name of the class\nrepresenting the client, if defined.\n\u2022 get_server_class: that returns the name of the class\nrepresenting the server, if defined.\nAfter overriding these methods, the algorithm is ready to\nbe run.\nRunning the algorithm using the fluke CLI Once the al-\ngorithm is ready, it can be run using the fluke command.\nLet's assume to have defined our algorithm, with\nall the necessary classes, in a python file named\nmy_algorithm.py and that the class representing the al-\ngorithm is named MyAlgorithm. The configuration file\nof the algorithm (e.g., alg_cfg.yaml) must contain in the\nkey name the fully qualified name of the class, e.g.,\nname: my_algorithm.MyAlgorithm 12. Then, the algorithm can\nbe run using the following command:\nconsole\n$ fluke --config=exp.yaml federation alg_cfg.yaml\nwhere exp.yaml is the experiment configuration. The com-\nmand must be run in the directory where my_algorithm.py is\nlocated."}, {"title": "What's next?", "content": "fluke is a relatively young project, and there are several\ndirections in which it could evolve without losing its orig-\n\"Or fluke.algorithms.PersonalizedFL in case of a\npersonalized FL algorithm.\n12 Clearly, all potential hyper-parameters must be properly set in\nthe configuration file.\ninal focus. Our plan is to continue to integrate state-of\nthe-art algorithms, and to make fluke more efficient. Cur-\nrently, fluke is designed to run (mostly) single-threaded\non a single machine, but in the future, we plan to support\nmulti-threading and multi-device, for example leveraging\nthe Ray 13 library."}]}