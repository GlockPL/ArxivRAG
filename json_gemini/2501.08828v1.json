{"title": "Benchmarking Multi-Modal Retrieval for Long Documents", "authors": ["Kuicai Dong", "Yujing Chang", "Xin Deik Goh", "Dexun Li", "Ruiming Tang", "Yong Liu"], "abstract": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval. MMDocIR is available at https://huggingface.co/MMDocIR.", "sections": [{"title": "1 Introduction", "content": "Multi-modal document retrieval [15, 16, 24, 28] aims to encode and retrieve various forms of content from visually rich documents based on user queries. Unlike traditional document retrieval [8, 14, 36, 50] which primarily deals with textual data, multi-modal document retrieval must handle documents that seamlessly integrate text with multi-modal elements such as images, tables, charts, and layout designs. These multi-modal content often carry significant information that plain text cannot convey [11, 37, 52], thereby enriching the depth and relevance of retrieved content. For example, our analysis of MMLongBench-Doc benchmark [29] reveals that: text occupies only 52.7% of content area, while images and tables account for 29.2% and 12.8% respectively, as shown in Figure 2. This significant presence of multiple data modalities requires retriever to leverage all data modalities present in a document.\nHowever, current benchmarks (shown in Table 1) for evaluating multi-modal document retrievers are insufficient, lacking in certain aspects that are critical for a comprehensive assessment. The major shortcomings include: 1. Question Quality: The design and curation of questions in most benchmarks do not align with the specific needs of multi-modal document retrieval. The questions, often reused from Visual Question Answering (VQA) tasks, lack the specificity required for this purpose and require expert filtering to be truly effective for retrieval tasks. 2. Document Quality: A significant limitation of existing benchmarks is the missing of complete document pages, thus hindering accurate evaluation of retrieval capabilities within actual document contexts. Additionally, the range of document domains covered is often too narrow, limiting the benchmarks' applicability across different fields. 3. Retrieval Granularity: Most benchmarks, except for SciMMIR [45], only allow retrieval at a page level which is insufficient, as user queries may pertain to specific parts of a page like a particular figure or table rather than the entire page.\nRecognizing these gaps, we introduce MMDocIR Multi-Modal Document Information Retrieval benchmark. MMDocIR is innovatively structured around two critical tasks: page-level and layout-level retrieval. (1) The page-level retrieval task is designed to identify the most relevant pages within a document in response to a user query. (2) The layout-level retrieval aims to retrieve most relevant layouts. The layouts are defined as the fine-grained elements such as paragraphs, equations, figures, tables, and charts. This task allows for a more nuanced content retrieval, honing in on specific information that directly answers user queries. To support these tasks, we first develop MMDocIR evaluation set comprising 313"}, {"title": "2 Dual-Task Retrieval Definition", "content": "Let D be a document corpora consisting of a set of document pages: $P = {P_1, P_2, ..., p_n}$, and layouts: $L = {l_1, l_2, . . ., l_m}$ extracted via layout detection\u00b3. The objective is to perform document page-level and layout-level retrieval. Specifically, the task involves retrieving a subset of these pages and layouts that are most relevant to a query Q, identified by the top k entries where k is significantly smaller than n or m. The relevance of pages (p) and layouts (l) to Q is quantified by similarity scores, Sim(Q, p) and Sim(Q, l) respectively. The retrieval system can be decomposed into (1) an offline indexing phase in which each page and layout from P and L is transformed into a vector representation, and (2) an online querying phase in which a query Q is converted into a vector form, and the vectorized query is then compared against the indexed vectors using the similarity scores Sim(Q, p) for pages and Sim(Q, l)."}, {"title": "3 MMDocIR: Evaluation Set", "content": "To facilitate the development of MMDocIR, we leverage resources from existing Document Visual Question Answering (DocVQA) benchmarks [22, 25, 29, 31, 38, 40, 51, 53]. Although DocVQA benchmarks are not primarily designed for IR, they offer valuable document corpora and questions that can be adapted for our purposes. To select useful DocVQA datasets, we adhere to following criteria:\nDocument Source: The dataset must include accessible original documents or sources for these documents. We need to access and enrich them to support more complex retrieval tasks.\nDiverse Domain/Modality: The document collections must (1) encompass diverse domains e.g., academia, finance, publishing, and research, and (2) embrace multiple modalities, such as text, figures, tables, charts, and layouts.\nLong Document: We seek documents of sufficient length as longer texts pose more significant challenges. This criterion can evaluate models in handling complex and lengthy documents.\nQA Diversity and Comprehensiveness: The questions included in the dataset should be diverse and challenging. For example, some questions should require reasoning across both text and visual tables/figures, while some are multi-hop questions that require synthesizing information from multiple pages.\nAfter considering aforementioned criteria, we choose to leverage on the document corpora and questions from MMLongBench-Doc [29]"}, {"title": "3.2 Annotation Process", "content": "Question Filtering and Revision. To ensure that the questions in MMDOcIR are optimally suited for document retrieval tasks. To this end, we identify four specific types of questions that do not align well with the objectives of IR. By filtering out or modifying these questions, we ensure the integrity and relevance of MMDocIR.\nSummarization Questions: These questions, such as \u201cWhat does this book mainly illustrate?", "Questions": "Questions that demand extensive data computation or collation, such as \"How many words are there in total in the paper?\" also fall outside our scope. These require a level of statistical analysis that goes beyond retrieval.\nOnline Search Questions: For example, queries like \u201cWhat is the Google Scholar citation count of the main author of this article?"}, {"Questions": "These are designed to test if models generate answers based on non-existent information (model hallucinations). Since they do not facilitate the retrieval of factual document-based information, these questions are excluded.\nPage-level annotation process. To address the challenge of accurately locating relevant information within extensive documents, we annotate page-level labels that pinpoint the exact pages containing evidence needed to answer specific questions. Considering that"}, {"title": "3.3 Quality Control", "content": "To ensure annotation quality and reliability in MMDocIR, we have adopted a rigorous quality control process using a cross-validation"}, {"title": "3.4 Statistics and Analysis", "content": "Document Analysis. As shown in Table 2, MMDocIR evaluation set includes 313 long documents averaging 65.1 pages, categorized into ten main domains. Different domains feature distinct distributions of multi-modal information. For instance, research reports, tutorials, workshops, and brochures predominantly contain images, whereas financial and industry documents are table-rich. In contrast, government and legal documents primarily comprise text. Overall, the modality distribution is: Text (60.4%), Image (18.8%), Table (16.7%), and other modalities (4.1%)."}, {"title": "4 MMDocIR: Training Set", "content": "The training set corpora are mainly collected from DocVQA benchmarks, adhering to similar selection criteria (see Section 3.1).\nMP-DocVQA [40] contains 47,952 images collected from Industry Documents Library (IDL) 5. We group the 47,952 document images into separate document files, and obtain 875 long documents (46.8 pages on average) with 15,266 QA pairs.\nSlideVQA [38] contains 2,619 slide documents collected from slideshare and covering 39 topics. Note that SlideVQA contains only the first 20 pages of each slide decks. In our research, we manually collect the complete slide decks, and obtain 2,011 long documents (averaging 49.3 pages) with 11,066 QA pairs.\nTAT-DQA [51] consists of 3,067 document pages from financial reports 7, dated between 2018 and 2020. Note that neither original documents nor links is provided. We use OCR to extract text in the pages, and use search engine to find relevant documents. After careful tracing and recognition, we identify 163 original documents (averaging 147.3 pages) with 15,814 QA pairs.\nArXivQA [25] comprises 32k figures cropped from academic pages 8. We use the arXiv DOIs provided to collect the academic papers. After careful tracing, recognition, and document length filtering, we identify 1,579 documents averaging 18.4 pages.\nSciQAG [41] consists of 22,728 papers in 24 scientific disciplines, collected from Web of Science (WoS) Core Collection database. We sample 50 documents from each discipline, and manually collect 1,197 papers using the DOIs provided."}, {"title": "4.2 Label Construction", "content": "The page labels can be converted for MP-DocVQA, SlideVQA, and DUDE datasets. Among them, only DUDE provides layout labels. SciQAG provides only question and answer in texts. We utilize them to reversely obtain the page-level and layout-level labels. Specifically, we first use MinerU to obtain layout-level passage chunks. For each QA pair, we deploy E5 and BGE retrievers to obtain question-passage and answer-passage similarity scores against all extracted passage chunks. If both scores rank within top 3 for a certain passage chunk, we confirm this layout to be the layout-level labels for the given QA pair.\nSimilarly, ArXivQA provides only cropped images, without document page/layout labels. We first use MinerU to obtain layout-level image. For each cropped image, we calculate the image similarity between all extracted images, and keep only the highest scoring one. Subsequently, we manually examine if the selected image matches the cropped image. In this way, we filter around 20% unmatched images and their QA pairs, resulting 1,579 QA pairs with page-level and layout-level labels.\nFor TAT-DQA, the layout-level labels are provided for sampled single page. We need to localize the page index of the sampled page. We utilize PDF mapping technique to retrieve the best matched page for every sampled page in this document. Then, we manual examine if the retrieved page is identical to the given page, and correct the labels if there were any errors.\nThe overall statistics (e.g., document information, modality distribution, domain, etc) of MMDocIR training set are in Table 4."}, {"title": "5 Model Training: DPR-Phi3&Col-Phi3", "content": "To evaluate the effectiveness of the MMDocIR training set, we train two visual retrievers based on Phi3-Vision [1]. Phi3-Vision"}, {"title": "Query-Doc Similarity", "content": "The similarity between the query and the document is computed as follows:\n$Sim(q, d)_{dpr} = \\frac{E_{dpr}^q \\cdot E_{dpr}^d}{||E_{dpr}^q|| ||E_{dpr}^d||}$"}, {"content": "where $Sim(q, d)_{dpr}$ is computed as the cosine similarity between their embeddings. and (..) is the dot product.\n$Sim(q, d)_{col} = \\sum_{i \\in [1, N_q]} max_{j \\in [1, N_d]} (E_{col}^q(i) \\cdot E_{col}^d(j))$"}, {"content": "where $Sim(q, d)_{col}$ is the sum over all query vectors $E_{col}^q(i)$, of its maximum dot product (1) with each of the $N_d$ document embedding vectors $E_{col}^d(j)$."}, {"title": "Contrastive Loss", "content": "Given the query q, we have the positive document $d^+$ and a set of negative documents $d^-$ including hard negatives and in-batch negatives. The hard negatives are negative pages within the document with highest $Sim(q, d^-)$ scored by ColPali [15] retriever. We calculate the loss as:\n$L_{dpr}(q,d^+,d^-) = -log\\frac{exp(Sim_{dpr}(q,d^+)/\\u03c4)}{\\sum_{d \\in {d^+ \\cup d^-}} exp(Sim_{dpr}(q,d)/\\u03c4)}$"}, {"content": "where DPR-Phi 3 is trained on the InfoNCE loss, and the temperature parameter $\\tau = 0.02$ in our experiments.\n$L_{col}(q,d^+,d^-) = log\\left(1+exp\\left(\\underset{d^{-} \\in d}{\\text{max }} (Sim_{col}(q,d^{-})) - Sim_{col}(q,d^{+})\\right)\\right)$"}, {"content": "where Col-Phi 3 is trained via the softplus loss based on the positive scores w.r.t. to the maximal negative scores."}, {"title": "6 Experiment", "content": "The retriever scores each page or layout in the document based on its relevance to the question, and returns the top k candidates with the highest scores. Recall@k is defined as the proportion of the ground truth page/layout evidence that is successfully retrieved. Note for layout matching, we calculate recall based on the overlaps of bounding boxes between retrieved layouts and gold layouts."}, {"title": "6.2 Baseline Models and Setting", "content": "We evaluate 6 state-of-the-art text retrievers: namely DPR [19], Col-BERT [20], BGE [46], E5 [43], Contriever [18], and GTE [26] (refer to Section 7.1). Meanwhile, we evaluate 5 VLM-based retrievers: 3 off-the-shelf models named DSEwiki-ss [28], DSEdocmatix [28], and ColPali [15] (refer to Section 7.2), and 2 our trained models (see section 5). All retrievers are adapted into dual-task setting:\nPage Retrieval: For textual retrievers, we use MinerU [42] to extract the layout of all document pages. Non-textual layouts are converted into 'OCR-text' and 'VLM-text' by using Tesseract OCR 12 and GPT-4o respectively. The document page is represented by concatenating the text of all layouts within it. For visual retrievers, we directly utilize document page screenshots."}, {"title": "6.3 Main Results for Page-level Retrieval", "content": "For page-level retrieval (Table 5), our key findings are as follows:\nSuperiority of Visual Retrievers: Visual retrievers consistently outperform text retrievers across various domains and retrieval metrics (e.g., Top k = 1, 3, 5). This highlights the significance of leveraging document screenshots to capture multi-modal information, which is often lost when relying solely on OCR-text.\nEffectiveness of MMDocIR: Among the visual retrievers, DPR-Phi3ours and Col-Phi3ours trained on theMMDocIR train set demonstrate superior performance. It suggests dataset quality to effectively enhances the retrieval ability.\nEffectiveness of VLM-Text: Although VLM-text approaches underperform visual retrievers, they achieve much better performance than the OCR-text methods. This indicates benefits of using GPT-4o to preserve visual cues in text.\nNecessity of Token-level Embeddings: Token-level retrievers (e.g., ColBERT, ColPali, Col-Phi3ours) compared with their document/page-level counterparts (e.g., BGE, DSE, DPR-Phi3ours), achieve more advantageous results in Recall@1 and marginal performance increase in Recall@5/10. Yet, the storage overhead of token-level embeddings can be 10 times more than page-level embeddings.\nTop 5 Coverage: Retrieving top 5 pages provides substantial coverage, ensuring that relevant information is captured."}, {"title": "6.4 Main Results for Layout-level Retrieval", "content": "For layout-level retrieval (Table 6), our key findings are as follows:\nSuperiority of Visual Retrievers: Visual retrievers exhibit substantial performance advantages over text retrievers utilizing OCR-text. This highlights the limitations of OCR in capturing the multi-modal nature of documents, where visual context can significantly enhance retrieval accuracy.\nEffectiveness of VLM-Text: Interestingly, VLM-text approaches achieve comparable performance as visual retrievers. This indicates strong image description capabilities of state-of-the-art VLM, which can offer significant benefits to textual retrievers in understanding multi-modal information.\nComparison of Hybrid vs. Pure Image Sequences: Visual retrievers relying on hybrid image-text sequences generally perform less effectively than those utilizing pure image sequences. This suggests that current VLMs may have stronger capabilities in modeling images than text within the multi-modal framework.\nNecessity of Token-level Embeddings: Token-level retrievers (e.g., ColBERT, ColPali, Col-Phi3ours) compared with their document/page-level counterparts (e.g., BGE, DSE, DPR-Phi3ours), do not achieve advantageous results in layout retrieval.\nTop 10 Coverage: Retrieving top 10 layouts does not provide comprehensive coverage of the ground truth layouts for document retrieval tasks. It suggests the challenge of layout retrieval."}, {"title": "6.5 Analysis of OCR and VLM Text", "content": "Interestingly, text retrievers leveraging VLM-text can significantly outperform OCR-text, in both page and layout retrieval. OCR-text which is the the raw text extracted via OCR tools, and as experimental results suggest, it is not suitable for multi-modal retrieval. In comparison, VLM-text can facilitate multi-modal retrieval, suggesting that VLMs can largely preserve rich multi-modal information.\nThe averaged word length and distribution of OCR and VLM text obtained from the tables and figures in MMDocIR in shown in"}, {"title": "7 Related Work", "content": "Document indexing. The process of indexing a multi-modal document involves multiple steps, including Document Parsing [6, 42], Optical Character Recognition (OCR) [5, 7, 32], Layout Detection [37, 47, 48], Chunking [9, 14, 33], and Image Captioning [2, 49]. These steps are time-consuming and can introduce errors that impact the overall retrieval performance.\nText retrieval. Current text retrieval are primarily classified into sparse and dense retrieval. For two widely-used sparse retrievers: TF-IDF [35] calculates the relevance via word frequency with the inverse document frequency, and BM25 [34] introduces nonlinear word frequency saturation and length normalization. Dense retrievers encode content into vector representations. DPR [19] is the pioneering work of dense vector representations for QA tasks. Similarly, ColBERT [20] introduces an efficient question-document interaction model with late fine-grained term matching. Contriever [18] leverages contrastive learning to improve content dense encoding. E5 [43] and BGE [46] propose novel training and data preparation techniques to enhance retrieval performance. Moreover, GTE [26] integrates graph-based techniques to enhance dense embedding. Most systems focus on text-based representations, neglecting the valuable visual information present in documents."}, {"title": "7.2 Vision-Driven Document Retrieval", "content": "Vision Language Models (VLMs) [1, 3, 4, 10] have gained popularity for their ability to understand and generate text based on combined text and visual inputs. This advancement has led to the development of cutting-edge visual-driven retrievers, such as ColPali [15] and DSE [28]. These models specifically leverage PaliGemma [4] and Phi3-V [1] to directly encode document page screenshots for multi-modal document retrieval. ColPali adopts similar question-document interaction as ColBERT, and represent each document page in token-level embeddings. By contrast, DSE is similar to DPR to encode each page with a single dense embedding. Visual Retrievers can directly model useful visual information, allowing for the direct utilization of multi-modal content without first converting it into text. Despite these advancements, visual retrievers face challenges, particularly in dealing with text details when document page resolutions are high. The high resolution of document pages substantially increases the computational cost and complexity of the embedding process, which may hinder the model's performance."}, {"title": "7.3 Multi-modal Document Retrieval Datasets", "content": "As described in Section 1 and Table 1, there is a notable lack of a robust benchmark for multi-modal document retrieval. Doc-CVQA [39] is the first multi-modal document retrieval-answering task, which extracts information from a document image collection and then provides the answer. However, DocCVQA provides only 20 questions, and the corpora is limited to finance domain specifically in political disclosure. PDF-MVQA [13] is tailored for multi-modal retrieval in research articles. Note that PDF-MVQA is annotated by Chat-GPT (GPT-3.5-turbo) but not experts, and its corpora contains only biomedical articles. SciMMIR [45] proposes multi-modal retrieval in scientific research papers. However, it provides only the image-caption pairs, not the natural user query with the targeting document pages. Ma et al. [28] introduce two relevant datasets, namely Wiki-SS and DocMatix-IR. Wiki-SS is constructed based on Natural Questions [21] where the evidence passage is the screenshot wikipedia webpage. Natural Questions are mainly designed for text rather than multi-modal retrieval. We also notice that the screenshot may not include the ground-truth evidence as only the front-page is screenshotted. DocMatix-IR is constructed from the largest document understanding dataset DocMatix [23] via filtering and hard negative mining. However, the VQA questions in DocMatix is constructed by Phi-3-small [1] rather than experts, and these VQA questions are not de-contextualized for retrieval.\nViDoRe [15] is the most relevant benchmark to MMDocIR. It integrates multiple DocVQA datasets [25, 30, 31, 51], and provides new documents in scientific, medical, administrative, and environment domains. Upon thorough examination of the 1,180 questions within the ViDoRe dataset, we find that more than 80% of these questions present drawbacks in terms of their complexity and context-specific nature. Typical examples include \u201cWhat's the title of the picture\u201d and \"what's the author's name\". They tend to be either overly simplistic or excessively dependent on a specific context which does not necessarily reflect the broader, more complex needs of effective IR. Additionally, a significant limitation of the ViDoRe dataset is its reliance on sampled document pages or cropped images rather than providing the complete document corpora."}, {"title": "8 Conclusion", "content": "In conclusion, multi-modal document retrieval presents a complex challenge that necessitates the integration of diverse data modalities beyond plain text. Our contributions address this challenge by introducing the MMDocIR benchmark, which includes innovative dual-task retrieval functionalities targeting both page-level and layout-level specifics of documents. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation purposes. Our comprehensive evaluation reveals that visual-driven retrievers significantly outperform text-driven ones, underscoring the critical importance of incorporating visual content in enhancing the effectiveness of document retrieval systems. Future work can expand on optimizing these retrieval algorithms to improve the accuracy and efficiency of multi-modal document retrieval systems."}]}