{"title": "A hybrid solution for 2-UAV RAN slicing", "authors": ["Nathan Boyer"], "abstract": "It's possible to distribute the Internet to users via drones. However it is then necessary to place the drones according to the positions of the users. Moreover, the 5th Generation (5G) New Radio (NR) technology is designed to accommodate a wide range of applications and industries. The NGNM 5G White Paper [9] groups these vertical use cases into three categories:\n1. enhanced Mobile Broadband (eMBB)\n2. massive Machine Type Communication (mMTC)\n3. Ultra-Reliable Low-latency Communication (URLLC).\nPartitioning the physical network into multiple virtual networks appears to be the best way to provide a customised service for each application and limit operational costs. This design is well known as network slicing. Each drone must thus slice its bandwidth between each of the 3 user classes. This whole problem (placement + bandwidth) can be defined as an optimization problem, but since it is very hard to solve efficiently, it is almost always addressed by AI in the litterature. In my internship, I wanted to prove that viewing the problem as an optimization problem can still be useful, by building an hybrid solution involving on one hand AI and on the other optimization. I use it to achieve better results than approaches that use only AI, although at the cost of slightly larger (but still reasonable) computation times.", "sections": [{"title": "Probleme presentation and literature analysis", "content": "According to [8], the concept of network slicing has been explored for some time. With the aim of providing multiple core networks over the same network infrastructure, DECOR and eDECOR [1] have already been used in legacy LTE networks. However, Radio Access Network (RAN) slicing has not been considered in these works. Since then, the concept of 5G network slicing has evolved to provide better modularisation and flexibility of network functions. RAN slicing is now of great interest in the literature, as it can support resource isolation between different slices, providing a way to allocate radio resources to the connected User Equipments (UEs) based on the slices they belong to. Several RAN slicing approaches have been proposed in recent years. Traditional strategies ([5], [2]) mainly deal with orthogonal resource allocation in time and/or frequency domain, such that the isolation between different services is guaranteed. Their aim in mainly oriented at allocating virtual Resource Blocks (vRBs) to UEs for intra-slice scheduling and to map the allocated vRBs to Physical Resource Blocks (PRBs) at a second stage.\nMachine Learning (ML) techniques ([10], [8]), mainly deep reinforcement learning, have also been extensively considered, given their capability to find patterns within the huge amount of data that is exchanged in cellular networks.\nIn Deep Reinforcement Learning for Combined Coverage and Resource Allocation in UAV-Aided RAN-Slicing [8], the authors develop a deep reinforcement learning agent that decides the best direction to go for the UAVs over a few 100 steps, and that is the most common approach. Indeed, in UAV-Assisted Fair Communication for Mobile Networks: A Multi-Agent Deep Reinforcement Learning Approach [10], the authors do pratically the same thing but add the dimension of fairness between the users. Moreover, in QoS-Aware UAV-BS Deployment Optimization Based on Reinforcement Learning [6], the authors show that using deep-Q-network, it is possible to achieve optimal performance when placing only 1 base station. Finally, in Reinforcement Learning Aided UAV Base Station Location Optimization for Rate Maximization [4], the authors use again deep reinforcement learning to resolve the placement of 3 UAVs and find that it achieves around 70% of the optimal performance, and that it is better than a k-means algorithm when the measure of imperfection is higher.\nMy internship aims at developping the following points on a 2-UAVs-aided RAN slicing:\n1. Improving the optimization algorithm to make it able to generate big amount of data in reasonable time\n2. Developping an AI agent that learns from the optimal optimization algorithm with supervised learning\n3. Building an hybrid agent from this AI agent that combines AI and a classical optimization algorithm to achieve better performance\n4. Realizing an honest comparison between the different methods both in terms of performance achieved and in terms of computation time"}, {"title": "Problem definition and solution via mathematical optimization", "content": "In [8] it is showed how to model the problem considered with a mathematical model.\nFirst, we need to define the equation that calculates how well a user will receive a connection from a base station, depending on where it is.\nWe have the following equations for the SINRs of the drones, which is a factor that represents how much data is actually received by the user in relation to the data sent to the user by the base station:\n$SINR_{i,j} = \\frac{p_c p(y_j,u) ((\\lVert y_j \\rVert)^2 + (h)^2)^{-\\alpha/2}}{\\sum_{k \\in U\\backslash i} p_c p(y_j,u) ((\\lVert y_j \\rVert)^2 + (h)^2)^{-\\alpha/2} + \\sigma^2}$\nTo evaluate a configuration, we will therefore proceed as follows:\n\u2022 First, each user must be associated with the nearest base station."}, {"title": "Optimization problem", "content": "This problem can be resolved as an optimization problem, I will show you how to resolve it for 2 base stations.\nLet's first define u a matrix of dimensions N * N, with N being the number of positions possible for a base station. Ui,j is a binary variable that is equal to one if and only if the first base station is in position i and the second is position j.\nbw is a 3 * 2 matrix that represents the bandwidth each base station allocates to each user class.\nd is a vector that represents for every whether it is satisfied or not.\nThe function to optimize is then: $max_{u,bw} \\sum_{g\\in G} \\delta_g$\nUnder the following constraints:\n$\\sum_{b\\in U} \\sum_{i,j\\in U} U_{i,j} \\times bw_{slice(g), b} \\times bps_{g} G_{conn}(slice(g), b) \\frac{100}{\\sigma} \\geq th_g \\times \\delta_g \\quad \\forall g \\in G$\n$bw_{em,b} + bw_{ur,b} + bw_{mm,b} = 1 \\quad \\forall b \\in U$ (1a)\n$bw_{em,b} \\geq 0; bw_{ur,b} \\geq 0; bw_{mm,b} \\geq 0 \\quad \\forall b \\in U$ (1b)\n$\\sum_{i,j\\in U} U_{i,j} = 1$ (1c)\n$\\delta_g \\in \\{0,1\\} \\quad \\forall g \\in G$ (1d)\n$U_{i,j} \\in \\{0,1\\} \\quad \\forall i, j \\in \\{0, 1, ..., |U - 1|\\}$ (1e)\nThe resolution of this optimization problem can be implemented in Python with the help of a module such as pulp."}, {"title": "Instances generation", "content": "This internship is mainly based on this article: Deep Reinforcement Learning for Combined Coverage and Resource Allocation in UAV-Aided RAN-Slicing [8]. As such, instances of the problem are generated in a very similar way as they are in this article, and are as follows:"}, {"title": "Improvement of the optimization algorithm", "content": "In this section we present an effective preprecessing of the data that can be used to obtain a good tradeoff between quality of the solution and computing time.\nFirst, only positions in the convex envelope of the users can actually be candidates for optimal base station placements. This allows us to significantly reduce the size of the matrix u.\nIt is also possible to go further, but not without losing precision. In fact, we can divide the users into k clusters and take the union of this convex hull of these clusters.\nThe transformation carried out can be visualised as follows:"}, {"title": "Machine learning to solve the problem", "content": "However, the model presented in the previous sectioncan still require a signficant amount of time to besolved. This can be problematic, especially if the user is moving and the optimal position needs to be recalculated frequently.\nA machine learning solution is therefore considered. In fact, solving an instance of the problem would only require a forward pass of the neural network, which is negligible in time compared to solving an instance with the optimal agent."}, {"title": "Reinforcement learning", "content": "A reinforcement learning solution is designed and implemented, in article [8]\nThe authors tried to solve the problem I just presented and, in short, managed to get an average reward of 72-73% of users satisfied.\nHowever, there are several differences between their approach and mine:\n1. First, they move the base stations step by step and computes the mean reward over 100 movements. However, since the base stations spend most of their times idling (because they have already reached their best possible position), I took the bias of putting the base stations directly at the computed best position, and then computing the reward only once (which also allows to test the different agents over more instances in the same amount of time).\n2. Secondly, what is shown in their article is the average reward during the reinforcement process. In other words, the neural network is only tested on the instances that it has just learnt from. What I have done instead is to have a separate test set that I use to evaluate the different agents."}, {"title": "Supervised learning from the optimal solution", "content": "In this section we present how to use supervised learning to train an agentthat leanes from the optimal solutionsprovided by the mathematical model presented in section 2. By using an optimal agent, the neural network can learn to replicate the optimal solutions. Developing such an AI was the first topic of my internship.\nThe neural network is therefore divided into 2 neural networks:\n1. A neural network that takes the positions of the users as input and learns to output the best positions for the 2 base stations.\n2. A neural network that takes as input the positions of the users and the positions of the base stations and learns to output the best allocation of bandwidth between the 3 classes of users."}, {"title": "Learn only what's necessary", "content": "From figure 5 it is possible to see that the test error on the bandwidths stops decreasing rapidly. That's why I decided to show the performances when the positions are optimally decided (i.e. decided by the optimization algorithm), but the bandwidths are decided by the AI."}, {"title": "Generalizing to a variable number of users", "content": "In the previous sections, I have only solved instances with a fixed number of users (50).\nThe first solution we came up with was to use graph neural networks. Graph Neural Networks (GNNs) are neural networks that consist of a variable (i.e. not fixed) number of nodes and edges and perform some aggregation functions (such as mean, min, or max) on them to get a final answer.\nThe graph given to the neural network was constructed using a k-neighbourg algorithm. I made a graph showing the final test error on positions as a function of the k in the k-neighbourg algorithm:\nUnfortunately, regarless of the choice for k, the the error obtained is always significantly higher than the one obtained with a model for a fixed number of users\nSo I decided on a simpler but less scalable approach: I simply add a one-hot to each user, indicating whether the user exists or not. Thus, the neural network can be used to resolve instances with a variable number of users, as long as the number in question is less than a certain maximum.\nThe accuracy with a variable number of users from 25 to 100 is very similar to the accuracy with a fixed number of users, as you can see in the following graphs:"}, {"title": "Comparison of the different methods", "content": "In this subsection I will compare the 3 agents we have seen in this report: the optimal agent, the full AI agent, and the hybrid agent. The 2 criteria will be first average reward and second time computation. Also, so far I have always generated users in 2 clusters, which is the simplest case (and the one used in the article my internship is based on [8]). However, I will now evaluate the different agents on a number of clusters ranging from 0 (meaning random) to 4.\nWe first evaluate the average reward, i.e. the average percentage of users satisfied.\nWhile in some cases, the results are a bit lacking (namely in the random and 3-clusters cases), the results are generally pretty close to the optimal solution. On the other hand, the hybrid option is always better than the full AI.\nMoreover, in the case that is the most similar to the one the situation in the article I tried to improve upon [8], while they had a normalized user coverage of around 87%, we have one of 96% (= 83.62). And even if the situations are not exactly the same (details), the changements should normally make it harder for my agent, since evaluating on the optimal positions makes it harder to be optimal rather than just having to move towards it, and having a separate test set punished overfitting harder.\nTo obtain a fair comparison of the methodologies proposed we need to consider also the computiong time needed\nThe time computations for the optimal agent depend of number of clusters because of the optimizations made.\nHowever, it is stable for the other 2 agents, with an average computation time of about 8-9 ms for the full-AI agent and an average computation time of about 25-30 ms for the hybrid agent, and an average computation time of about 25-30 ms for the hybrid agent.\nThus, even if the hybrid agent can be seen as a middle ground between the short computation times but low rewards of the full-AI agent and the high rewards but long computation times of the full-AI agent, the hybrid agent still performs well, and the high rewards but long computation times of the optimal agent, I think the balance is in its favor, with only slightly longer computation times than the full-AI agent and slightly worse rewards than the optimal agent."}, {"title": "Areas for improvement", "content": "First, the AI agents were only trained with a data set of 9000 instances, which could be increased if needed, and the rewards would be better.\nSecond, a relatively new idea in solving optimization problems with AI is to use the distance to the optimal solution. (but in terms of reward, not in terms of answer as we did) as the reward for the neural network, which could definitely be implemented for this problem, and might give higher performance.\nThirdly, as seen in this article [8], the reward the optimal agent has is stongly tied to the precision of the grid used. I used a grid of 10 \u00d7 10 in my internship but by increasing the precision, the rewards would be better.\nFinally, if you wanted to extend the method presented in this article to a number of drones of 3 or more, you would have to rewrite the optimal agent completely, as it scales very poorly. For example, it currently requires 37.1 GB of ram to declare the u matrix for 4 drones."}, {"title": "Conclusion", "content": "In my internship, I developed 2 RL agents for a 2-UAV assisted 5GNR network slicing scenario, with the goal of optimizing the SLA satisfaction of the deployed users.\nThe goal was to learn from the optimal agent, with the idea that it is not really the movements of the UAVs that are important, but their final destinations.\nI demonstrated how a hybrid approach can outperform a full-AI approach, which is commonly used in the literature.\nFinally, this method showed great adaptability, performing well with many different distributions of users.\nThe promising results obtained during my internship can encourage us to investigate towards improving the performance, and try to scale this method to situations with more than 2 UAVs."}, {"title": "Appendix detailling how the neural networks are built", "content": "For the position agent, each user is represented as:\n1. Its x and y axis\n2. A one-hot for each of the possible type\n3. A one-hot that indicates whether the user actually exists (if the user does not exist, the positions are set to random and the one-hots above are set to 0).\nThe number of users is easily adjustable (I set it to 100 in the tests). The neural network consists of 2 convolutional layers followed by 2 linear layers. The hyperparameters are the following: learning rate of 5e-5 and weight decay of le-5.\nFor the bandwidth agent, each user is represented as before, but an entry is added for the position of each of the UAVs. The neural network then consists of a convolutional layer for the drones, a convolutional layer for the users, then the 2 results are concatenated and passed to 2 linear layers. The hyperparameters are the following: learning rate of 2e-4 and weight decay of le-6."}]}