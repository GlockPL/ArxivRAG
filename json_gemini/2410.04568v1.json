{"title": "Ranking Policy Learning via Marketplace Expected Value Estimation From Observational Data", "authors": ["Ehsan Ebrahimzadeh", "Nikhil Monga", "Hang Gao", "Alex Cozzi", "Abraham Bagherjeiran"], "abstract": "We develop a decision making framework to cast the problem of learning a ranking policy for search or recommendation engines in a two-sided e-commerce marketplace as an expected reward optimization problem using observational data. As a value allocation mechanism, the ranking policy allocates retrieved items to the designated slots so as to maximize the user utility from the slotted items, at any given stage of the shopping journey. The objective of this allocation can in turn be defined with respect to the underlying probabilistic user browsing model as the expected number of interaction events on presented items matching the user intent, given the ranking context. Through recognizing the effect of ranking as an intervention action to inform users' interactions with slotted items and the corresponding economic value of the interaction events for the marketplace, we formulate the expected reward of the marketplace as the collective value from all presented ranking actions. The key element in this formulation is a notion of context value distribution, which signifies not only the attribution of value to ranking interventions within a session but also the distribution of marketplace reward across user sessions. We build empirical estimates for the expected reward of the marketplace from observational data that account for the heterogeneity of economic value across session contexts as well as the distribution shifts in learning from observational user activity data. The ranking policy can then be trained by optimizing the empirical expected reward estimates via standard Bayesian inference techniques. We discuss the connections and distinctions between our proposed perspective and the standard supervised approach to learning to rank via empirical risk minimization with respect to standard information retrieval metrics. The specific focus of this paper is to highlight the significance of the empirical context value distribution in shaping the properties of the corresponding ranking policies by contrasting various empirical importance sampling distributions. We report empirical results from online randomized controlled experiments on a product search ranking task in a major e-commerce platform demonstrating the fundamental trade-offs governed by ranking polices trained on empirical reward estimates with respect to extreme choices of the context value distribution.", "sections": [{"title": "1 Introduction", "content": "Two sided e-commerce marketplaces are intermediary economic platforms that connect buyers and sellers, usually providing a wide selection of products for the buyers from a diverse array of sellers. The primary buyer focused objective of the marketplace is to guide buyers through their browse and discovery journeys to identify and purchase the items that fulfill their shopping intent. Users' browsing and purchase journeys in the marketplace are impacted by an ecosystem of decision making systems, most notably via the ranking policies in various stages of their shopping journeys from discovery pages to the Search Engine Result Pages(SERP). An effective ranking policy aims to showcase a set of results matching the user intent at any given ranking context along the shopping journey with rewards realized as interaction events on the slotted items on the page. Collectively, user journeys are not equally likely to produce value for the marketplace and the goal is to expand the set of successful user sessions, optimizing a suitable notion of long term value for the users and the marketplace. It is therefore essential for the ranking policy to account for the utility of all stakeholders in this economic setting. In standard formulations of learning to rank in the information retrieval literature, however, there is usually no clear connection between the training objective for the ranking policy, the long term value for the collective of the users and the key performance metrics of the marketplace. In this paper, focusing primarily on the search ranking policy invoked in response to users' search queries, we formulate the ranking policy learning as an optimization problem based on a (counterfactual) estimate of the marketplace reward from observational data."}, {"title": "1.1 Motivation", "content": ""}, {"title": "1.2 Contributions and Related Work", "content": "Contribution 1. We propose a decision making framework estab-lishing explicit connections between learning a ranking policy for a search/recommendation engines and building effective empirical estimates for a suitable notion of marketplace expected reward.\nThe problem of developing merit scores for ranking items, post a selection stage from a large pool of candidates, is widely studies in the context of recommendation systems[18], display advertising[3], sponsored search[35], and search ranking[34], where the sequential and hierarchical nature of the user interaction events and sparsity of success events[15, 18, 21, 30] in user journeys are taken into account. Value-aware policies in the context of advertising [35, 35], and economic recommender systems[6, 7, 18] account for business objectives, primarily through manipulations of the merit scores based on conversion likelihood estimates and the price of the can-didate items to develop a point-wise notion of expected value for a given candidate item. In contrast, our approach is user focused in that the goal of the ranking policy is to optimize for the user utility in the sense of maximizing the expected number of engagements on desirable items at every stage of the search journey. An alter-native formulation is to frame the search ranking policy learning as a multi-stakeholder multi-objective optimization problem[2, 19], with potentially conflicting objectives[28] that account either for business constraints [27] or group exposure constraints [20]. The notion of value for the marketplace is introduced in the ranking policy objective via an importance weighting distribution that sig-nifies both the economic value and the likelihood of realizing some reward from an interaction event with an item that satisfies the user intent.\nContribution 2. We characterize the key elements in building effec-tive (policy-dependent) expected reward estimates from observational data, controlling for (1) the heterogeneity of the session value distri-bution, (2) the contribution of interventions within a user journey via the reward attribution scheme, and (3) the distribution shifts incurred by selection biases in observational data.\nReinforcement learning(RL) is a powerful framework to account for sequential interventions within the session by formulating the problem of recommending new items[26] or search ranking[12] as a Markov Decision Process (MDP). By expanding the planning horizon and adopting intermediary reward shaping techniques, RL-based approaches account for delayed rewards in the session, via suitable representations of the dynamic session context(state) in session trajectories. Recognizing the selection biases in the ob-served user behavior data, offline reinforcement learning tech-niques, including inverse propensity weighting[4], and actor-critic methods[5], are adopted to account for distribution shifts in learn-ing from logged data. Similar counterfactual training techniques based on propensity weighting and potential outcome modeling are developed in the context of counterfactual learning to rank for search ranking problems [11, 16, 22]. There is, however, no clear account of the heterogeneity of marketplace reward across session trajectories, neither in the standard counterfactual supervised learn-ing perspective nor in offline reinforcement learning approaches.\nContribution 3. We highlight the significance of the empirical session-context value distribution in building effective marketplace ex-pected reward estimates by demonstrating fundamental performance trade-offs governed by the search ranking policies trained on extreme choices of the context value distribution via rigorous counterfactual evaluations as well as online randomized controlled experiments in a major e-commerce platform.\nThe definition of success events and the associated reward to the user events is flexible in our framework and is informed by the strategic choices of the marketplace. Specifically, an early-stage marketplace may focus on maximizing the collective number of engagements, while an acquisition-oriented marketplace targets the collective number of purchases, while a revenue-driven marketplace chooses to maximize the long-term gross merchandise value."}, {"title": "1.3 Notation", "content": "Here is a list of notation adopted throughout the paper. Sets and ordered sets(lists) are represented with upper-case calligraphic symbols; such as X. Random quantities are shown in bold such as x with realization x. The expected value of random variable x is denoted by E[x] and the conditional expectation of a random variable z = f(x, y) given y is denoted by E[z|y] or Ex~P(x) [z]. For a function f: X \u2192 R, the |X| dimensional array [f(x)]x\u2208x is denoted by f(X)."}, {"title": "2 Problem Setup", "content": "The marketplace is interested in maximizing the average total re-ward across all user session trajectories over a long time horizon"}, {"title": "2.1 Decision Making Framework", "content": "$\\sum_{t<T} U_s^t$\nwhere us is the economic value from a successfully served search ses-sion s. Our framework is flexible in the choice of the reward function and we discuss the fundamental trade-offs between multiple strate-gic marketplace long term reward choices, namely revenue-based, value per engagement and value per acquisition marketplaces. The reward from a session trajectory is assumed to be non-negative. Although our framework, can be extended to account for negative rewards, we ignore it in our formalization. We assume that the re-ward over search journeys is a stationary ergodic stochastic process. By invoking Birkhoff's ergodic theorem[9], with probability 1, the long term temporal average is same as the expected reward, i.e.\n$\\mathbb{E}_{s,v} [V_s]$,\nwhere the expectation is taken with respect to the randomness in the session context and reward distribution.\nWhile our formalization can naturally be extended to search journeys with complex goals, we focus on a typical e-commerce purchase decision making scenario of session trajectory a with a sin-gle product intent, ignoring sessions with multi-product purchase intent, as well as informational and navigational search sessions. We recognize that users' decisions are impacted by multiple inde-pendently optimized decision making systems, but we are oblivious to potential interactions of the ranking policy with these systems, specifically to the closely related query understanding and candi-date retrieval policies. We only focus on policy learning for search result pages with a single layer presentation semantic where the action of the ranking policy is the permutation/ranking of a largely homogeneous set of comparable items for a flat single-layered pre-sentation of the results, ignoring the multiplicity of user's search intent and diversity considerations for the result set.\nWe cast the problem into a Bayesian decision making framework with a user-focused perspective on the definition of success upon a ranking action. The ranking policy aims to increase the expected number of engagements on items that meet the user intent, and the reward is proportional to the likelihood of a success event(non-zero reward) from the user interactions on the search results page produced by the ranking policy. A crucial aspect of this framework is to account for distribution shifts in observational data, i.e. the distinction between the distribution of the logged search activity data that the policy is trained and the inference time distribution of user events."}, {"title": "2.2 Success From a Ranking Intervention", "content": "Given a search query q within a session context s, the ranking policy \u03c0 : Dq \u2192 {1,\u2026\u2026, N} maps a candidate item d from the re-trieved set Dq to a slot \u03c0(d). The notion of success with respect to a slotting \u03c0(Dq) of the items on the SERP q is defined based on the effectiveness of the policy in driving user interaction events(Click). Specifically, the objective of the ranking policy on a given ranked SERP is to increase the expected number of engagements on desir-able items(suitably defined) cr(Dq) given the session context upon issuing the query s"}, {"title": "2.3 Success From a User Session", "content": "The notion of success with respect to a user session is defined based on interaction events on desirable items across all interventions by the marketplace within a user journey. Given the per query ranking objective E[Cr(Dq)|s"}, {"title": "2.4 Marketplace Expected Reward", "content": "The expected reward of the marketplace from the presented ranking is then shaped by the distribution of the value across search contexts, which signifies the economic value of the user-interaction events in the sessions for the marketplace. The random variable vs captures the strategic notion of the value of the session for the marketplace, which corresponds to the value of the interactions events on the item(s) that satisfy the user's intent. The expected reward of the marketplace can then be written as\n$\\mathbb{E}_{s,v} [V_s \\mathbb{E}_{q \\sim P(q|s<q)}[\\mathbb{E}[C_r(D_q)|s<q]]]$,\nWe can also consider an alternative formulation where we assume that session value distribution Po(s) subsumes both the likelihood of the user session to lead to some reward for the search engine as well as the reward value attributed to the user session s:\n$\\mathbb{E}_{s \\sim P_0(s)}[\\mathbb{E}_{q \\sim P(q|s<q)}[\\mathbb{E}[C_r(D_q)|s<q]]]$.\nFor a value-aware search engine with marketplace revenue objec-tive, economic value is realized only in the event of a transaction as the success event from a search session and the reward is pro-portional to the price of the sold item(s). For a search engine that aims to optimize for the volume of transactions, it is more suitable to adopt a value per acquisition notion of reward oblivious to the price of the sold items. For a search engine with strategic goal of maximizing user engagements for increased user retention and minimizing abandonment, it is more suitable to adopt a value per click notion of reward oblivious to the post click transaction events.\nIn the next section, we discuss empirical modeling techniques to build effective empirical reward estimates from observational data, which effectively frame the problem as a standard counter-factual empirical risk minimization with a value-aware context distribution."}, {"title": "3 Expected Reward Estimation from Observational Data", "content": ""}, {"title": "3.1 Estimating the Per Query Success", "content": "We are interested in maximizing E[cr(Dq)|s"}, {"title": "3.2 Estimating the Session Expected Reward", "content": ""}, {"title": "3.2.1 In-session success attribution.", "content": "Several techniques can be adopted to estimate the contribution of a ranked SERP q and the correspond-ing observed or potential interactions on that page to the overall success of the user search session s. A simple yet popular solution in the context of online advertising is to adopt an attribution dis-tribution that assigns all the probability mass to the immediate query context preceding the post-click conversion event"}, {}, {"title": "3.2.2 Session Value estimation.", "content": "In order to highlight the impor-tance of the session value distribution,\nP(s)\nlet us focus on a search engine with a value per acquisition ob-jective. A straightforward empirical session value distribution is adopt a uniform distribution on sessions that lead to a transaction event. Such session value selection distribution leads to survivor-ship bias in training context selection in that traffic segments where transaction events are rare, e.g. user sessions with luxury intent, will be under-represented in training. A simple approach is to ex-pand the definition of success events and estimate the likelihood of session success with a content-oblivious estimate based on the aggregate conversion likelihood of the richest engagement event attributed to the element(s) engaged. This perspective on building mixture distributions based on the richest post-click engagement event was shown to be effective in capturing potential conversions from browse-heavy user journeys[25].\nFor a revenue-focused marketplace, as discussed in Section 2.2, the value of a search session is proportional to the price of the item that matches the user intent. In the presence of an observed success event in the logged data, the purchase price of the item to which the success event is attributed is the realization of the session value; otherwise, in the absence of an interaction event, the value of the session has to be estimated from the content of the asked intent in user queries, or a Canonical set of actual or synthesized items that match the user intent."}, {"title": "3.3 Selection Bias Correction", "content": "One of the main challenges in learning from observational data is the distribution shift between the training data collected from the logging policies and the inference data distribution. We there-fore have to introduce another set of techniques, e.g. importance weighting distributions, to account for this mismatch between the (population) expected reward in (6) and the estimated expected reward from the estimated quantities in the previous sections; that is,\n$\\mathbb{E}_{s\\sim P(s)}[\\mathbb{E}_{q\\sim P(q|s<q)}[\\hat{\\mathbb{E}}[C_r(D_q)|s<q]]]$.\nAn important source of distribution shift in observational search activity data is the the selection bias due to presentation of the items on the page and the sequential browsing of the users, implying that we only observe relevancy of the items to the user only in the event of an explicit user engagement and it is more likely to observe engagements on SERPs from higher ranking slots.\nA key technique to account for this effect is to define a suitable notion of propensity, which is developed in the context of study-ing the effect of a treatment(an intervention) on a population by taking into account attributes of the treatment unit in the way the treatment is assigned. In the context of ranking, the treatment is defined in correspondence to the examination of a slotted item by the user, but the key difference with the standard applications of this concept is that the examination variable is not fully observable. An alternative approach based on potential outcome modeling, sim-ilar to actor-critic networks in the context of offline reinforcement learning, is proposed in [11], where distilled knowledge from a"}, {"title": "3.4 Variance Reduction and Generalization", "content": "Having discussed an array of importance weighting schemes to build empirical expected reward estimates, it is essential to develop variance reduction techniques to control the generalization behav-ior of expected reward estimators. For brevity of presentation, we briefly discuss the various reduction techniques adopted and ig-nore developing generalization bounds on the bias and variance of the estimation error of the proposed empirical reward estimation techniques."}, {"title": "3.4.1 Truncation and Bucketing.", "content": "Clipping and truncated impor-tance sampling techniques[1, 13] are popular techniques to control the variance and generalization behavior of inverse propensity weighting estimators when there is high variance in the estimated propensities. Since we combine multiple importance sampling tech-niques to account for selection bias, success likelihood, and context value distribution across highly heterogeneous user trajectories, we adopt this simple variance reduction technique off the shelf.\nIn building empirical session value distributions for a revenue focused marketplace reward, relying on the purchase price of the success items leads to a very high variance estimator, particularly in the presence of high heterogeneity in price intent across user trajectories. Instead, we can use a stratification technique by buck-eting user sessions based on value buckets defined according to the empirical revenue distribution. Specifically, we can build a ses-sion value distribution based on the empirical revenue share of the bucket corresponding to the price of the purchased item."}, {"title": "3.4.2 Potential Outcome Modeling.", "content": "One of the the primary chal-lenges of counterfactual learning to rank from logged search ac-tivity data is that the relevancy of the items is observed only in the event of explicit user engagements. A popular idea in the con-text of contextual bandits and recommendation systems to circum-vent the challenges in this partial information setting is to use predictive models for reward estimates as potential outcome mod-els in conjunction with inverse propensity weighting[8, 24, 33]. There are a number of recent works in the context of unbiased response prediction that leverage and analyze the doubly robust technique [22, 23, 32, 36]. In [11], a generalized form of potential outcome modeling is proposed where the distilled knowledge from a relevance teacher is used in the form of soft predicted relevance labels to help the student with more effective list-wise comparisons, variance reduction, and improved generalization behavior. This is similar to the idea of actor-critic networks in the context of offline reinforcement learning[5], and augmentation policy in the context of contextual bandits [29]. Using knowledge distillation helps build training contexts from logged search contexts without user interac-tion events leveraging complex models. To simplify the discussions, we ignore discussing any details about the teacher models used in our experimental setup."}, {"title": "3.4.3 Stratification and Normalization.", "content": "Effective stratification is a key technique in the context of importance weighting estimators, e.g. the context value binning idea discussed in sub-section 3.4.1"}, {"title": "3.5 Optimization Objective for the Ranking Policy", "content": "We consider deterministic policies parameterized by a scoring func-tion f"}, {}, {"title": "4 Evaluations and Discussions", "content": "In Section 3, we discussed essential elements of building empirical expected reward estimates for training effective search ranking policies. Since conducting thorough ablation studies for charac-terizing the effect of each element in building empirical expected reward estimates is not possible given the space constraints, we fo-cus primarily on the rather under-explored element in the literature, which is the effect of context value distribution discussed in section 3.2 in shaping the properties and the generalization performance of the ranking policy.\nWe focus on a product search ranking scenario in a major E-commerce platform and evaluate candidate policies via online ran-domized control experiments, as well as rigorous counterfactual evaluations on user session data collected from the online traffic. Since all experiment are performed on proprietary data, we only report lifts compared to a simple clearly-specified baseline, with a focus on the relevant choices for controlling the estimation er-ror with respect to the research question of interest, oblivious to the optimization framework, the feature representations, and the hypothesis class. Specifically, we only discuss the choice of the ranking objective and the relevant importance sampling and attri-bution techniques for building our estimators of interest, without discussing the details of the models."}, {"title": "4.1 Online Evaluation Framework", "content": "Since the main goal of the proposed decision making framework is to build search ranking policies that generalize with respect to a given notion of marketplace expected reward, we primarily evalu-ate the performance of the candidate policies in online randomized controlled experiments. Specifically, we adopt an experiment design and primary success metric defined with respect to lifts in cumula-tive reward in treated user sessions. This cumulative reward driven design is in contrast to the standard experiment design practices for incremental ranking changes, where the primary success metric is set to be the standard (immediate) ranking efficiency metrics that measure concentration of success events in Top slots, through simple attribution and aggregation schemes across search result pages. In fact, top slot engagement concentration metrics, e.g. per query DCG with respect to SERP interactions aggregated uniformly across all queries, which are usually tightly correlated with the marketplace reward, should only be treated as secondary metrics in the presence of a measurement of cumulative reward in online experiments. We do recognize, however, that DCG-type metrics are particularly crucial for counterfactual off-policy evaluations, as approximations to the per query expected reward using logged data, because all we can do is to measure concentration of logged success events in top slots upon the shuffling action of the new target policy."}, {"title": "4.2 Training Objectives and Offline Evaluation Metrics", "content": "We adopt the standard supervised counterfactual training and eval-uation framework based on logged search activity data collected from the online traffic of a major E-commerce platform. We are oblivious to the logging policy and collect datasets with importance sampling and reward attribution semantics based on the corre-sponding notions of expected reward of interest. Specifically, given a target notion of expected reward, the context value distribution remains the same for training and evaluation datasets. For candi-date item selection per SERP, however, we sample three negative samples at random from impressed unengaged items within each training context, but keep all the candidate items to be re-ranked by the candidate ranker for the evaluation datasets.\nFor all empirical expected reward metrics, we use the same, suitably debiased and normalized, DCG approximation for the per query expected reward according to (11). Unless explicitly stated otherwise, we use the following vanilla empirical context value distribution for building expected reward estimates as training objectives and the counterfactual metrics.\nExpected number of engagements \u00ca[C]: The session value distribution PC (s) is a uniform distribution across logged sessions with at least one click event. We consider a simple last touch at-tribution scheme PC (q|s"}, {"title": "4.3 Research Questions", "content": "4.3.1 Marketplace Reward Trade offs. The primary insight that we would like to highlight in our evaluations is the heterogeneity of"}]}