{"title": "DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in Real-Time", "authors": ["Jin Hu", "Xianglong Liu", "Jiakai Wang", "Junkai Zhang", "Xianqi Yang", "Haotong Qin", "Yuqing Ma", "Ke Xu"], "abstract": "Physical adversarial examples (PAEs) are regarded as \"whistle-blowers\" of real-world risks in deep-learning applications, thus worth further investigation. However, current PAE generation studies show limited adaptive attacking ability to diverse and varying scenes, revealing the emergency requirement of dynamic PAEs that are generated in real-time and conditioned on the observation from the attacker. The key challenges in generating dynamic PAEs are exploring their patterns under noisy gradient feedback and adapting the attack to agnostic scenario natures. To address the problems, we present DynamicPAE, the first generative framework that enables scene-aware real-time physical attacks beyond static attacks. Specifically, to train the dynamic PAE generator under noisy gradient feedback, we introduce the residual-driven sample trajectory guidance technique, which is mainly achieved by redefining the training task to break the limited feedback information restriction that leads to the degeneracy problem. Intuitively, it allows the gradient feedback to be passed to the generator through a low-noise auxiliary task, thereby guiding the optimization away from degenerate solutions and facilitating a more comprehensive and stable exploration of feasible PAEs. To adapt the generator to agnostic scenario natures, we introduce the context-aligned scene expectation simulation process, consisting of the conditional-uncertainty-aligned data module and the skewness-aligned objective re-weighting module. The former enhances robustness in the context of incomplete observation by employing a conditional probabilistic model for domain randomization, while the latter facilitates consistent stealth control across different attack targets by automatically reweighting losses based on the skewness indicator. Extensive digital and physical evaluations demonstrate the superior attack performance of DynamicPAE, attaining a 1.95x boost (65.55% average AP drop under attack) on representative object detectors (e.g., Yolo-v8) over state-of-the-art static PAE generating methods. Overall, our work opens the door to end-to-end modeling of dynamic PAEs by overcoming the uncertainty inherent in their generation.", "sections": [{"title": "1 INTRODUCTION", "content": "NUMEROUS intelligent applications in real-world scenarios have been landed in recent years, such as autonomous driving [1], healthcare [2] and intelligent assistant [3]. However, adversarial examples (AEs), which are specially designed for misleading machine learning models [4], have long been a challenge for deep learning applications [5]. Among them, the physical-world adversarial examples (PAEs) are attracting broader attention due to their feasibility in the real world and de facto threats to business AI systems [6]\u2013[9]. Besides revealing the risks, the research on AEs (PAEs) also deepens the understanding of deep neural networks and reveals their defects in industrial and science applications [10], [11]. Therefore, modeling AEs, especially PAEs, is worth further investigating to alleviate the trustworthy issues (e.g., interpretability, security, and robustness) of deep learning models and related applications.\nThe existing adversarial attacks can be categorized as digital attacks and physical attacks, carried out in digital and real-world scenarios, respectively. On this basis, a line of studies aims to further propose adversarial attacks and promote the explainability of deep learning models [12]\u2013[15]. Another line of research focuses on realizing PAEs in real-world settings [16], [17], as digital AEs may not be robust when fabricated in the real world [18]. In terms of threat, PAEs pose greater social risks, therefore highly attracting broader focuses. However, mainstream PAE methods address the real-world PAE generation problem by treating it as a static issue, either attempting to generalize the PAE across all simulated physical scenes [19] or requiring retraining each time to adapt to new settings [20]. This approach results in inadequate adaptability and generation efficiency. On the contrary, we define scene-aware and realtime generated PAEs as dynamic PAEs, which is a problem that has yet to be addressed in the field. Related attempts are limited to determining patch locations only [21], simulated control of a few states [22], optimization for each clustered scene in the lab [23], and resisting dynamic fluctuations [24] rather than enabling dynamic responses.\nThis work focuses on overcoming the core challenge, i.e., the key inherent uncertainty of PAEs, of dynamic PAE generation. It is a new generative task that takes the observation from the attacker as the input and the corresponding PAE as the output. However, compared to conventional generative tasks that learn from existing data, this task requires creating unknown adversarial data that is effective in the uncertain world, and the following challenges stem from modeling the inherent uncertainty of the PAE generation task need to be tackled: \u25cf Training the dynamic PAE generator is difficult, presented as optimization degeneracy and infinite gradients. The difficulty arises from the noisy gradient feedback [25] in attack training that hinders the exploration of the potentially sparse distributions [26] of PAEs, and the noise originates from the necessary randomness injected into PAE training. The real-world attack scenario contains agnostic properties, including incomplete observations that limit the perception of the generator and different attack targets that influence the balance between objectives. Therefore, further modeling of scene-aware attacks is required to eliminate the physical-digital gap and balance the objectives for consistent generative behavior.\nTo address the dynamic PAE generation problem, we present the DynamicPAE framework that effectively addresses the challenges. As shown in Figure 1, the framework trains the scene-aware generator in the end-to-end paradigm and significantly improves the attack performance under varying conditions. Specifically, to address the training problem that arises from noisy gradients, residualdriven sample trajectory guidance is proposed by breaking the limited feedback information restriction that leads to the degeneracy problem. Inspired by deep residual learning [27], we present the construction of the residual task that redefines the optimization problem, enriches the guidance information by relaxing the training with a low-noise auxiliary task, and leads the optimization toward exploring more aggressive results in the sparse space of PAEs. A regularized latent encoding is further proposed to stabilize the exploration. To adapt the agnostic attack scenarios, context-aligned scene expectation simulation is proposed. Specifically, the conditional-uncertainty-aligned data module is proposed to inject the randomness that is aligned with the real-world uncertainty, including the uncertainty of incomplete observation, and the skewness-based weight controller is proposed to automatically align the objectives when directly adapting the framework to agnostic target models. By solving the key training and task modeling issues, the framework bridges the gap between generative neural networks and dynamic PAE generation.\nWe construct a comprehensive digital benchmark on patch attacks, an example of simulated adversarial testing in the autonomous driving scenario, and a physical-world attack prototype system for object detection to evaluate the proposed framework and demonstrate its effectiveness. Our contribution is summarized as follows:"}, {"title": "2 PRELIMINARIES", "content": "2.1 Backgrounds\nAdversarial Examples (AEs) are a type of specially designed data for fooling deep-learning models [28]. Formally, given a visual recognition model $F$, a benign input $x$, and its corresponding ground truth $y$, the AE $\\delta$ is designed to satisfy:\n$y \\neq F(x\\oplus \\delta), \\quad ||\\delta||_p \\leq \\epsilon,$\nwhere $|| \\cdot ||_p$ indicates the distance metric under the p-norm and could be replaced by other metrics, $\\epsilon$ is a small constant that controls the magnitude of the adversarial attack.\nFor the AEs to reveal real vulnerabilities, a key issue is whether AEs retain validity in the physical world. In the presence of diverse observations of the physical world by"}, {"title": "3 METHODOLOGY", "content": "The overview of the DynamicPAE framework is shown in Figure 2. We aim to establish an end-to-end framework for dynamic PAE generation based on the unified representation learning paradigm, which formulates the architecture of the neural PAE generator as follows:\n$G := Dec \\circ Enc, \\quad Enc : P \\rightarrow Z \\subset \\mathbb{R}^d,$\nto enable effective training of the PAE generator G, the framework synergizes two key components: residual-driven sample trajectory guidance to tackle the training difficulties arise from noisy gradient feedback; and context-aligned scene expectation simulation to adapt to agnostic attack scenarios, including incomplete observations and agnostic attack targets, thereby achieving the dynamic adversarial attack in the real world.\n3.1 Residual-Driven Sample Trajectory Guidance\nTraining the dynamic PAE generator faces the challenge of noisy gradient feedback, which leads to the degeneracy and instability problem of the generative training (shown in the experiments in section 4.3.1). To analyze the training problem, we bridge the noisy gradient model and the sample trajectory analysis with the limited feedback information restriction and recognize that the necessary randomness injected in the PAE optimization task leads to the training degeneracy of the dynamic PAE generator. Inspired by deep residual learning, we tackle the problem by constructing a residual task that relaxes the original attack training and breaks the limited feedback information restriction. We further propose the additional regularization of the latent encoding to tackle the latent evasion problem related to the unconventional gradient property and stabilize the training. Afterward, the training escapes from degenerate solutions and allows a more stable and comprehensive exploration of feasible PAEs.\n\u2022 Analysis of the Degeneracy: Previous works have demonstrated that obfuscated gradient [25] is a mechanism of adversarial defense techniques and adaptive attacks shall overcome it [44]. Section 4.3.1 provides compelling evidence that the simulated transformation has a similar impact on PGD optimization, confirming the existence of the noisy gradient problem. Intuitively, a small distortion in the environment (X, 0) may cause the gradient feedback of the attack task $\\nabla_{\\delta} \\mathcal{L}_{Atk}$ changes significantly. We formulate the limited feedback information restriction to model the problem based on the mutual information and entropy metrics on $\\nabla_{\\delta} \\mathcal{L}_{Atk}$ and the (bottleneck) latent variable Z:\n$I(\\mathcal{L}_{Atk}; Z) / H(\\nabla_{\\delta}\\mathcal{L}_{Atk}) < \\epsilon,$\nindicating that the uncertainty of $\\nabla_{\\delta} \\mathcal{L}_{Atk}$ is significantly higher than the uncertainty in $\\nabla_{\\delta}\\mathcal{L}_{Atk}$ reduced by Z.\nWe then analyze the sample trajectory with this model to understand the optimization degeneration problem. Specifically, considering the gradient descent process of deep learning, the trajectory of the generated PAEs $\\delta$ in training step t can be modeled by:\n$\\delta^{(t+1)} = \\delta^{(t)} + \\eta \\nabla_{\\delta} \\mathcal{L}_{Atk}^{(t)}$\n$\\nabla_{\\delta} \\mathcal{L}_{Atk} = f^{(t)}(Z, \\delta) + g^{(t)}(\\delta),$\nwhere $\\eta$ is the learning rate, $g^{(t)}$ is the noise introduced by random simulated transformation, and the term $f^{(t)}$ is the components correlated with Z, representing the learnable information. By rate-distortion theory, the signal power $E[| f^{(t)} |^2]$ is significantly lower than the noise power $E[| g^{(t)} |^2]$ if $I(\\nabla_{\\delta} \\mathcal{L}_{Atk}; Z)$ is relatively small according to Eq. 4. Based on the empirical fact of the existence of universal AE [45], the AE optimization contains isotropic components, and $E[g^{(t)}]$ is large. Thus the optimization of PAE may converge to the local minimum determined by $g^{(t)}$ and $E[g^{(t)}]$ even with sufficient large steps t, which is coherent with the experiment results in section 4.3.1.\nThis result indicates the hardness of SGD optimization in learning PAEs and is independent of whether the optimizer includes techniques like momentum to escape from local optima. Generative models, especially single-step models such as GAN, have long faced the similar experimental challenge of mode collapse in the manifold sampling tasks, in particular on disconnected manifolds [46]\u2013[48]. Rather than relying on generative model construction techniques, we propose a task-driven approach and formulate solutions that are well-suited to the problem itself.\n\u2022 Optimization Guidance with Residual Task: Inspired by deep residual learning [27], we bypass this difficulty by redefining the optimization task. Specifically, we simply relax the adversarial attack task with the additional task R with the conditional parameter $\\lambda$, and modify the loss from $\\mathcal{L}_{atk} (\\delta(\\cdot))$ to $\\mathcal{L}_{X} (\\mathcal{L}_{atk}, \\mathcal{L}_R)$, defined as:\n$\\mathcal{L}_{X}(\\mathcal{L}_{atk}, \\mathcal{L}_R) := \\lambda \\mathcal{L}_{atk}(\\delta(\\cdot,1)) + (1 - \\lambda )\\mathcal{L}_{R}(\\delta(\\cdot,1)),$\nWe denote $\\mathcal{L}$ as the residual fusion operator. The modification aims to break the noisy gradient model in Eq. 4 by constructing $\\mathcal{L}_R$ to be more dependent on the learnable latent representation:\n$\\exists s.t. I(\\nabla_{\\delta}\\mathcal{L}_{X} (\\mathcal{L}_{atk}, \\mathcal{L}_R); Z(\\cdot,1)) \\gg I(\\nabla_{\\delta}\\mathcal{L}_{atk}; Z).$\nAfterward, the imbalanced noise issue in SGD optimization is solved, which mitigates the occurrence of degeneration. Further, the optimal PAE solution remains identical when $\\lambda = 0$. Since the gradient takes a noiseless path, or a residual path, to the parameters of G, we identify task R as the auxiliary residual task.\nMore intuitively, the goal of the residual task in generating PAEs is to encourage the exploration of the global space of PAEs. Inspired by the construction of the denoising task in diffusion models to learn the entire gradient field [49], we designate local area reconstruction as the auxiliary residual task R $(\\mathcal{L}_R := \\mathcal{L}_{Inv})$ since it is directly related to the"}, {"title": "3.2 Context-Aligned Scene Expectation Simulation", "content": "The practical PAE methods should be compatible with the variable and agnostic scenario. For the dynamic attack scenarios, the critical issues are incomplete observations from the attacker, which limit the perception of the generator, and the agnostic nature of target models and tasks, which affect the balance between objectives. To bridge the gap between the training environment and potential deployment scenarios, firstly, we propose the conditional-uncertainty-aligned data module. This module allows the generator to accommodate incomplete observations by employing a conditional probabilistic model for domain randomization. Secondly, we propose the skewness-aligned objective re-weighting module, which uses the skewness indicator to balance multiple optimization objectives, including stealth and aggressive, under agnostic attack targets. Thus, consistency in the sampling and control of stealth and attack objectives is maintained, and hyper-parameter tuning is eliminated.\n\u2022 Conditional-Uncertainty-Aligned Data Model: We align the training and real-world scenes by creating a conditional probabilistic model to generate the incomplete observation $P_X$, as illustrated in Figure 3, based on the dynamic attack problem formulation in Equation 2. In this model, the parameters $\\theta$ of the attack injection process are generated by the current state of the world X and a random factor s, which, along with X and another random factor $s'$, jointly generate the physical context data $P_X$ that the attacker acquires. The training is performed by minimizing the expected losses under the data model, where $\\Omega$ and $\\Theta$ are the probability distribution that generates $P_X$ and $\\theta$, respectively:\n$\\min E_{Px\\sim \\Omega(\\theta,X),\\theta\\sim\\Theta(X),X\\sim \\mathcal{X}} [\\mathcal{L}(F(X + \\Theta (G(Px), \\theta)))],$ (12)\nwhere $\\mathcal{L}$ is the total loss function, which is defined as $\\mathcal{L}_{Total}$ in the following context. For patch attacks, the attack injection $\\Theta$ is formulated as:\n$X + (\\delta, \\theta) :=X\\otimes (1 - m_{\\theta})+$\n$\\text{AffineTransform} (\\delta, A_{\\theta}) \\otimes m_{\\theta},$\nwhere the binary mask $m_{\\theta} \\in {0,1}^{H_1\\times W_1}$ representing the configurable patch location corresponding to $\\theta$. Considering the input of the attack device, the local feature corresponding to the stealthiness and the global context corresponding to the aggressiveness are included in $P_X$, as the attack objective is correlated with the global attention field of the model [53]. Specifically, the local feature is introduced by the local image content $X_{local} \\in \\mathbb{R}^{H_2\\times W_2\\times 3}$ around the patch, while the global context corresponds to a global"}, {"title": "4 EXPERIMENTS", "content": "We evaluate our framework in both digital and physical environments on the object detection task since it is classical for both physical and digital attacks. The experiments are conducted in the following aspects: The performance of our model in the benchmark. The analysis corresponds with the methodologies that tackle the challenge of noisy gradient feedback in PAE generator training and the agnostic nature of attack scenarios. \u2192 The ablation of the key components."}, {"title": "4.1 Experiment Settings", "content": "In this subsection, we describe the key settings of the experiments. We use the COCO [57] and the Inria [58] as the dataset of the person detection task, while data collected from CARLA is used for the simulation experiment. Detailed attack transformation, method implementation, dataset description, and victim model description are shown in the supplementary material.\nMetrics: The average precision (AP), formulated as follows by the precision $Pre(c)$ and recall $Rec(c)$ of confidence threshold c is applied as the metric of adversary performance.\n$AP = \\int_{c=0}^{1} \\max_{r} Pre(c) [Rec(c) \\leq r] dc \\in [0, 1].$\nWe adopt two confidence thresholds: the first is $C_{min}$ = 50%, with the corresponding metric denoted as AP50, which serves as the evaluation metric identical to the practice of previous object detection attack evaluation. The second threshold is $C_{min}$ = 1%, with the metric denoted as AP01, as a more stringent evaluation metric. All iou thresholds are set as 0.5. The low-confidence and unattacked bounding boxes are filtered out during post-processing.\nTo ensure fairness, we use consistent patch placement parameters in the evaluation to control the overall stealthiness of the attack. Some specific methods also have other factors affecting stealthiness, such as the naturalness constraint of GAN-NAP, so we additionally assessed relevant metrics in the experiment. We use the Structural Similarity Index (SSIM) and the Learned Perceptual Image Patch Similarity (LPIPS) as metrics to measure the stealthiness of the patches. Both SSIM and LPIPS have a range of [0, 1]. SSIM is not included as a loss during the training of our model."}, {"title": "4.2 Performances Evaluation", "content": "We evaluate performance based on settings similar to classical PAE studies [39] to focus on comparing PAE optimization or learning methods instead of the simulation. Furthermore, the stealthiness metrics are included to enhance the fairness for biased optimization.\nWe trained our model on the COCO dataset only and others on data identical to the evaluation datasets. The trade-off between the attack performance and the distortion of the original image is shown in Figure 5, demonstrating the significantly superior attack-distortion curve. Since our model can perform generation conditioned on A and automatically balance the attack and invisibility by the SkewnessAligned Objective Re-weighting method, the curve in each sub-graph is evaluated in just one training session with the test-time settings of A = {0.0, 0.1, 0.2, ..., 1.0}. However, the static PAE methods are only able to produce a single result, and thus our model achieves better practicality in PAE-driven model evaluation. The numerical results are shown in Table 3, presenting the setting of A = 0.8 of DynamicPAE, which is generally more stealthy according to the attack-distortion curve. It is distinct that the dynamic PAE generated by our model achieves better performance, i.e., lower AP50 and AP01 values. Specifically, DynamicPAEx=.8 boost the average drop of AP50 to \u221265.55%(1.95\u00d7 on average) while preserving comparable stealthiness measured by SSIM, and only under one configuration (Yolov5-m, Inria) can T-sea achieve comparable aggressiveness. Furthermore, our model achieves 1.3\u00d7 of AP50 average drop compared to performing PGD on each image. In particular, for models with larger parameter sizes (Yolov3-m and_Faster-RCNN), our approach exhibits more significant attack performance superiority, suggesting that our model can be scaled up for evaluating the adversarial robustness of larger-scale models. Furthermore, our method has not been trained on the Inria dataset, so the successful attacks on the Inria dataset indicate the zero-shot generalization capability in applying to proprietary data. We evaluate more transformation settings in the supplementary material and have the same result, except PGD performs better in zero spatial transformation.\nWe acknowledge that with a sufficiently large number of iterations, attacks based on traditional optimization, represented by PGD, under the same parameter distribution of and the same image X may be able to achieve better attack results, but at an unacceptable time overhead. We present a comparison of time consumption in Table 4. All of the training and validation except DynamicPAEInf. is done on one Nvidia A40 with bf16 enabled. DynamicPAEval., DynamicPAE Inf. and PGD are timed through torch.cuda.Event after CUDA warmup, and they are tested on the Inria dataset with multiple runs. DynamicPAEval. achieves a speedup of more than 2000 times compared to generating the PAE for each image with PGD and maintains higher attack performance. DynamicPAEInf. shows that our model can provide real-time patch generation even on personal graphics cards. Moreover, our training time is acceptable (about 40 hours"}, {"title": "4.3 On Overcoming the Noisy Gradient in PAE Training", "content": "Existing works have shown the special characteristics of AEs and the adversarial attack problems, including its physical robustness [18], distributional properties [26], [60], [61], and gradient properties of the optimization process [25]. For the dynamic PAE training, we identify the key challenge as the noisy gradient problem, causing the problem of degenerated solution and latent evasion in the generative training, which can be regarded as originating from the necessary randomness injected into PAE training. To illustrate the challenge and evaluate the effectiveness of the proposed model and solution, we analyze each problem and our corresponding solution in terms of gradients and sample/latent space.\n4.3.1 Analysis and Solving the Noisy Gradient Problem\nObfuscated Gradient Perspective: Obfuscated Gradient [25] is recognized as a typical defense mechanism in digital adversarial attacks since the defenders could significantly increase the difficulty of performing gradient descent optimization by adding random transformation. The solution for the attack is applying multiple-step attacks, or EoT, which is also a key algorithm for PAE [29] since the fluctuations in the physical world can be regarded as naturally introduced randomness.\nFirst, we show that the simulated physical adversarial patch attacks, as a typical physical attack, can be regarded as facing the same obfuscated gradient, or noisy gradient problem indicated by our experimental results, as the spatial transformations significantly increase the difficulty of PGD attacks. Specifically, as shown in Table 5 and Table 6, an increase in the total number of steps, whether it's the number of EoT iterations or the total iteration steps of the optimizer, consistently improves the PGD performance under spatial transformations. Additionally, we use 2048 steps in the main experiment based on these results.\nThe noisy gradient problem also brings severe challenges for training the dynamic generator, and our methodology focused on it. We present the performance of DynamicPAE framework after solving the problem. To ensure a fair comparison, both our model and PGD are evaluated by resampling the patch locations. Note that even with this high step count, the performance of PGD is still less aggressive than our result. In the main performance evaluation, we found that the Zero transformation settings facilitate the PGD method in achieving better results. We further analyze it on the Inria dataset. As shown in Table 7, PGD surpasses our method in attack capability at 32 steps, but our method still retains an advantage in inference time. From another perspective, the superiority of our method mainly lies in overcoming physical-world transformations.\n4.3.2 Analysis and Solving the Latent Evasion Problem\nGradient Explosion Perspective: Another difference from training generative model on benign data is that the learning"}, {"title": "4.4 On the Practicality in Agnostic Attack Scenarios", "content": "To evaluate the real-world practicality of DynamicPAE when facing agnostic scenarios, we conduct experiments on different environments and parameters to show the effectiveness of the proposed simulation model. The experiments demonstrate the ability of DynamicPAE to (1) reveal potential security issues and (2) generate adversarial evaluation data that is aligned with the real world in applications.\nResults in Physical Environments: We implement our model and deploy it with edge-computing devices (2080Ti) and the local area network (LAN), and evaluate the attack performance of the framework within the physical environment. As shown in Figure 8, our method generally achieved higher attack performance. Although our attacks are experimental, real attacks can be realized with dynamic materials (e.g., color-changing costumes [63]) and spread through transformation simulation. Furthermore, we find that presenting the static patch in varying conditions could also the attack effectiveness, indicating that the adversarial examples generated by our method achieve a balance between attack adaptability and robustness, and the visualization, including more cases showing the adaptability and the robustness, is provided in the supplementary material.\nConfirmation of Physical Robustness: Since accurate location as model input may be difficult to obtain in applications, we perform tests with inaccurate inputs. As shown in Table 8, the attack performance did not drop much as we resample the patch locations, which is attributed to the implicit regularization of DNN and augmentation pipeline. Moreover, the attack performance is still significantly higher than the baselines in the main performance evaluation. This indicates that the model can leverage precise location information when available, while still maintaining relatively high performance even with inaccurate location inputs.\nResults in Simulated Environment: To evaluate the applicability of the proposed model in the simulated testing, we obtain the simulation results from different perspectives in the CARLA autonomous driving simulation platform [64], divide them into training and testing datasets, and apply the adversarial example as a perturbation of the vehicle's texture. Supplement to the digital environment benchmark, we compare the performance with the GAN-based perturbation generation framework AdvGAN, since both our model and AdvGAN are learning-based and real-time.\nAs shown in Table 9, our model also achieves strong perturbation attack performance and significantly enhances the attack aggressiveness compared to AdvGAN. To measure the stealthiness of the attack more accurately, we measure the SSIM similarity in addition to the $l_\\infty$ constraint. It can be seen that the attack performance of our method significantly outperforms the baseline method for all different magnitude settings. Interestingly, our method performs better in the"}, {"title": "5 RELATED WORKS", "content": "Digital Adversarial Example Generation: Classical type of AE research concentrated on generating $l_p$-constrained examples and developed techniques based on model gradient [28], projected gradient descent [59], SGD optimizer [65] and neural network [66]. Further study focused on constructing more complex attack scenarios, such as the black-box attack [67], data-manifold constrained AEs [68], and the transfer attacks among models and scenarios.\nPhysical Adversarial Attacks: Based on the paradigm of optimizing static PAEs, research focuses on simulating the physical world [69] and the target AI system [20], and modeling PAEs under naturalness constrains [42] or new attack mediums [70]. Recent works began to focus on the challenge of generating dynamic PAEs. [24] proposes the Dynamic Adversarial Patch on the dynamically changing clothes, while the PAE data itself is still static. [23] proposes the adaptive PAE by manually clustering the attack scenario and optimizing the static PAE for each cluster, but it is not suitable for the open world. [22] proposes a physical adversarial attack by controlling a dynamic laser beam in the simulation using reinforcement learning. Nevertheless, it is only capable of modeling a limited number of laser states but not the general space of PAE.\nGenerative Models and Generative Adversarial Attacks: Motivated by learning representations [71], classical generative neural networks adapt generative learning methods to train a model that maps the space of real data X to a latent space Z, which has better mapping modeling and sampling efficiency. With refined learning tasks and neural network construction [49], [72], generative models is able to generate complex data, e.g., model the relation of texts and natural images [73], molecular dynamics [74], protein structure [75] and images and corresponding digital perturbation AEs [37]. In terms of adversarial attacks, NN-based generators have been applied in improving perturbation optimization [37], generating naturalistic AEs by modeling the data manifold [21], [42], [68], [76] and probabilistic modeling perturbations for black-box attacks [77]. However, only unconditional & static PAEs have been proposed under the generative framework [42], [76], [78]. Except for some specific adversarial attack mediums, e.g., audio [79] and face subspace [70], the physical-digital gap [18] is small, the randomized transformation may not be necessary, and therefore the digital AE generation network may be effective. As an important step forward, we bridge the general gap between scene-aware dynamic PAE and generative NNs.\nMultidisciplinary Optimization Techniques: Similar optimization problems also occurred in the application of neural networks [80] that learns complex tasks in the field of AI4Science, e.g., approximating complex fields [81] and training on noisy measurements [82]. We believe the relevant research will be beneficial for scene-aware PAE generation, and visa versa. In the field of reinforcement learning, research has been conducted on the problem of exploring the action space and utilizing inaccurate feedback [83]. However, the general scene-aware PAE generation problem is defined in the complex state space, such as a patch with 10K+ pixels, which cannot is intractable for reinforcement models. Recently, language-driven models have been adapted for optimization [84], but there is still a gap between language and low-level AE data [85], which should be filled by the generative embedding model."}, {"title": "6 CONCLUSIONS", "content": "This study focuses on the dynamic physical adversarial examples (PAEs), a fundamental and largely unaddressed vulnerability of deep learning models in applications. A highly effective and versatile method, DynamicPAE, is proposed for generating real-time scene-aware PAE, offering a significant advancement. Extensive experimental results demonstrate that the proposed method exhibits superior dynamic attack performance in open and complex scenarios.\nFuture research directions can focus on technical improvements to the patch representation and training mechanism, exploring the application of the proposed framework, and facilitating the developed techniques in broader domains. In terms of application, although we have presented the preliminary prototype in our experiments, work on generating adversarial test and training data and exploring real-world red team attacks for specific tasks is still worthy of further investigation. We believe that our study has the broader potential to benefit the optimization of defense-insensitive adversarial attacks and noisy open-world tasks, and the model for the hardness of PAE optimization can bring insights to the defense of PAEs. In addition, the proposed scene-aware PAE generation task is representative and beneficial for foundation model research."}, {"title": "SPPLEMENTARY MATERIALS", "content": "1 EXPERIMENT SETTINGS\n1.1 Benchmark Settings\nDatasets: We use the COCO [57] and the Inria [58] as the dataset of the person detection task, while data collected from CARLA is used for the simulation experiment. For COCO, bounding boxes marked as crowds or smaller than 1/36 of the image, resulting in about 60,000 training boxes and 2,500 testing boxes are filtered out in both training and evaluation. Inria is a smaller and cleaner dataset for pedestrian detection, containing about 1,200 training boxes and 300 testing boxes. We did not filter Inria bounding boxes. We attacked and evaluated individual objects directly from the datasets, allowing precise assessment of attack performance on different objects within a single image.\nAttack Simulation: We mainly evaluate our model under simulated physical adversarial attack to enable more accurate numerical analysis. We define a patch placement model for evaluation based on affine transformation A, which can be decomposed into rotation matrices, scaling matrices, and translation matrices:\n$A = A_{rotate} A_{scale} A_{trans}.$\nThus, we define separate parameter distribution models for rotation, scaling, and translation in the transformation. Specifically, we sample the relative position and size of the patch for the candidate bounding box, as well as the parameters for rotation in the 2D space of the image and rotation in the 3D space (the rotation axis is parallel to the camera's imaging plane), from a predefined uniform distribution shown in Table 13. In comparison to previous evaluations, a setting of smaller patch size was implemented across the evaluated attacks, which increases the attack difficulty. For color distortion, we apply brightness, saturation, and contrast transformation with uniform sampled parameters, and add Gaussian noise with standard variance sampled from the uniform distribution, as shown in Table 14. For each image and each selected bounding box in the test set, 10 random transformation parameters are sampled to expand the evaluation scale. We provide a visualized example of evaluation in Figure 10."}, {"title": "1.2 Method Implementations", "content": "Neural Network Architecture of DynamicPAE: To efficiently and robustly model the complex correlation", "G": "Deco Enc", "Enc": "P \u2192 Z \u2286 Rd", "m'": "and A", "as": "n$Enc_p := MLP_1(LN(MLP_2(ResNet_1(X_{local})))$ +$\nLN(MLP_3(ResNet_2(Concat[X_{global}, m'"}]}