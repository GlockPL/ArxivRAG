{"title": "PAD: PERSONALIZED ALIGNMENT AT DECODING-TIME", "authors": ["Ruizhe Chen", "Xiaotian Zhang", "Meng Luo", "Wenhao Chai", "Zuozhu Liu"], "abstract": "Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model's predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements have demonstrated success in aligning language models with human preferences and values (Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022; Achiam et al., 2023). Representative methods such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and DPO (Rafailov et al., 2024b) typically optimize a policy model with training signals from an explicit or implicit reward model. The reward model captures the 'general' human preferences or values from human feedback. However, in this pluralistic world, users' preferences can diverge significantly based on their different cultures, educational backgrounds, religions, and political stands (Gordon et al., 2022; Sorensen et al., 2024b; Jang et al., 2023; Cheng et al., 2023). Furthermore, even for the same person, the preference of a particular LLM response can vary when the application scenario changes. Hence, there always exists a proportion of human preferences that cannot be unified by the general preference, also known as personalized preferences, which current alignment frameworks struggle to align with due to the need for high-quality datasets and substantial computational costs in policy optimization.\nHow can we align with personalized preferences without the need for additional data collection and policy training? In this paper, we introduce Personalized Alignment at Decoding-time (PAD), which aims to align LLM outputs with diverse personalized preferences during the inference phase without requiring additional training. To achieve this, we first propose a personalized reward modeling strategy, which decouples the text generation process (modeled as a Markov Decision Process) from personalized preferences, thereby enabling the acquisition of generalizable token-level personalized rewards. Based on this, we then formulate a personalized reward model (PRM). During decoding, the PRM scores the base model's top-K predictions at each token generation step based on the current generation and personalized preferences. Finally, this score is combined with standard decoding likelihoods to adjust the base model's predictions. The advantages of PAD are as follows: (1) It requires only a single policy model (i.e., the base model) aligned with general preferences (General"}, {"title": "RELATED WORKS", "content": "Large language model alignment. Large language model alignment aims to align LLMs with human preferences. The mainstream methods are training-based. A common approach involves using an RLHF (Reinforcement Learning with Human Feedback) framework (Christiano et al., 2017; Bai et al., 2022) where a reward model is trained based on human feedback, and Proximal Policy Optimization (PPO) (Schulman et al., 2017) is employed to derive the aligned policy model. Recent efforts explore alternatives to enhance stability and reduce resources. Notably, DPO (Rafailov et al., 2024b) leverages the Bradley-Terry assumption (Bradley & Terry, 1952) for direct optimization of the preference-based objective. Decoding-time alignment offers an alignment paradigm that does not require expensive RL training (Han et al., 2024; Liu et al., 2024). Controlled Decoding (CD) (Mudgal et al., 2023) utilizes a prefix scorer module trained to assess value functions for rewards, allowing controlled generation from a frozen base model. ARGS (Khanov et al., 2024) proposed using a reward signal to adjust probabilistic predictions, thereby generating semantically aligned texts. DeAL (Huang et al., 2024) focusing on heuristic-guided searches to better meet diverse alignment objectives.\nPersonalized alignment. As humans exhibit diverse preferences and values for a single task, it is essential to align large language models (LLMs) to users' personalized preferences (Kirk et al., 2023; Sorensen et al., 2023; 2024c; Yao et al., 2023; Kirk et al., 2024; Zhong et al., 2024; Han et al., 2024). One line of work achieves joint optimization for different personalized preferences by defining a reward function with multiple dimensions and performing policy optimization (Zhou et al., 2023; Wang et al., 2024a;b; Guo et al., 2024; Yang et al., 2024b; Chakraborty et al., 2024; Sun et al., 2024; Li et al., 2024). Additionally, some approaches involve merging model parameters or predictions trained for each dimension to accommodate the diverse combinations expressed by those dimensions (Jang et al., 2023; Rame et al., 2024; Park et al., 2024; Shi et al., 2024). Lastly, prompt-based methods align personalized user preferences by designing diverse prompts or post-processing techniques (Yang"}, {"title": "METHOD", "content": "In this section, we first define the per-token Markov Decision Process (MDP) for large language models (LLMs) and describe its relationship to classic Reinforcement Learning from Human Feedback (RLHF) approaches. Then, we describe the characteristics and challenges of personalized alignment.\nText generation as token-level markov decision process. The standard text generation process of large language models (LLMs) with prompt x and response y can be defined as a token-level Markov Decision Process (MDP). MDP is denoted as a tuple $M = (S,A,P, R,T)$, where the state space S consists of the prompt and all tokens generated so far (i.e., $s_t = (x, y_{1:t-1})$). The action space A is the tokens from the vocabulary (i.e., $a_t = y_t$). P is the transition kernel, which is deterministic that given state $s_t = (x, y_{1:t-1})$ and action $a_t = y_t$, the next state is $s_{t+1} = (S_t, a_t) = (x, y_{1:t})$. $R : S \\times A \\rightarrow \\mathbb{R}$ represents the reward at each step. The maximum token count, T, sets the length limit for LLM outputs, which conclude with an end-of-sentence (EoS) token $y_T =$ EoS that ends the generation. Given an MDP, the objective is to maximize the expected return $R(x, y) = \\sum_{t=1}^TR(s_t, a_t)$. To achieve this, the agent computes a (Markov) policy $\\pi : S \\rightarrow \\Delta(A)$ that maps from state to a distribution over actions.\nThe RLHF Pipeline. Classic RLHF approaches (Ziegler et al., 2019) first learn a reward function from human feedback on prompt and response pairs (x, yw, y'). The reward function is modeled under a contextual bandit setting using the Bradley-Terry preference model (Bradley & Terry, 1952):\n$p^* (y^w \\succeq y^l) = \\frac{exp \\mathbb{R}(x, y^w)}{exp \\mathbb{R}(x, y^w) + exp \\mathbb{R}(x, y^l)},$ (1)\nwhere yw and y\u00b9 denote the preferred and not-preferred completions for the prompt x. $p^*(y^w > y^l)$ denotes the probability that y\u2122 is preferred to y'. The reward model can be learned through Maximum Likelihood Estimation (MLE) on this dataset D:\n$L(\\mathbb{R}, D) = E_{(x,y^w,y^l)\\sim D} [log \\sigma(\\mathbb{R}(x, y^w) - \\mathbb{R}(x, y^l))].$ (2)\nSubsequently, we use the learned reward model to provide feedback. The policy model (i.e., the language model) \u03c0\u03b8 is optimized with a gradient-based method such as PPO (Schulman et al., 2017) with an entropy-bonus using the following KL-constrained RL objective:\n$J_E = \\max_{\\pi_{\\theta}} \\mathbb{E}_{y \\sim \\pi_{\\theta}(y/x)} (\\sum_{t=0}^{T}(R(x,y) - \\beta D_{KL} (\\pi_{ref} (y|x), \\pi_{\\theta}(y|x)))$ , (3)\nwhere ref represents a reference policy, typically the language model resulting from supervised fine-tuning, from which the learned policy should not significantly deviate. \u03b2 is a parameter used to control this deviation. In practice, the language model policy \u03c0\u03b8 is initially set to ref. Additionally, it is important to note that we exclude the supervised fine-tuning (SFT) stage in the RLHF pipeline. This is because the SFT stage is not directly relevant to the focus of this paper.\nPersonalized alignment within MDP Unlike the unidirectional reward in traditional MDPs, we posit that personalized preferences may vary across different dimensions; for example, some users may prefer concise and understandable responses, while others might favor comprehensive and expert answers. In this way, the reward function of personalized alignment can be defined as $\\mathbb{R}: S \\times A \\rightarrow \\mathbb{R}^n$, which describes a vector of n rewards, one for each dimension of personalized"}, {"title": "PERSONALIZED ALIGNMENT AT DECODING-TIME", "content": "In this section, we propose Personalized Alignment at Decoding-Time (PAD), a novel approach that does not require training a policy and is transferable to diverse personalized preferences. Inspired by the concept of successor features (Dayan, 1993; Barreto et al., 2017), we first define the personalized reward function, which is composed of the features of the current state and personalized preferences. By linking this reward function to the value function, we can decouple personalized preferences from the dynamics of the MDP. Consequently, we only need to learn the generic features under the current policy, and by merely altering the personalized preferences, we can achieve personalized alignment. Finally, we introduce a guided decoding algorithm, which aligns with personalized preferences by utilizing the value function for weighting during the inference phase.\nDefinition 1: A personalized reward function R can be represented by:\n$\\mathbb{R}(p, s, a) = w_p^T \\phi(s, a),$ (4)\nwhere $\\phi(s, a) \\in \\mathbb{R}^d$ represents the features of current state and action (s, a), and $w_p \\in \\mathbb{R}^d$ are weights derived from personalized preference p.\nIntuitively, $\\phi(s, a)$ can be understood as the salient features (e.g., expert, informative) of the current state, and the vector wp models personalized preferences p as the degree of desirability for each feature. wp is independent of (s, a). Consequently, when the user's personalized preferences change, only wp is altered, which in turn modifies the reward function R(p, s, a). We derive the value function (Watkins & Dayan, 1992) of personalized reward function, inspired by that the optimal policy \u03c0* obtained by Equation 3 can be formulated as (Ziebart, 2010; Rafailov et al., 2024a):\n$\\pi^*(a_t| S_t, p) = e^{(Q^*(p,s_t,a_t)-V^*(p,s_t))/\\beta},$ (5)\nwhere Q, the action-value function (i.e., Q function) based on the token-level reward $\\mathbb{R}^p (p, s, a)$, models the total future reward from (s, a) under policy \u03c0. V is the corresponding state-value function at current state, where $V^*(p, s_t) = \\beta log (\\int_{a\\in A} e^{Q^*(p,s_t,a)/\\beta} da)$. Q* and V* denote the optimal value functions under optimal policy \u03c0*. Q function can be expressed as:\n$Q^{\\pi} (p, s_t, a_t) = E[\\sum_{i=t}^T R(p, s_i, a_i)|a_i \\sim \\pi(\\cdot|s_i)],$ (6)\n$= E[\\sum_{i=t}^T w_p^T \\phi (Si, ai)|ai \\sim \\pi(\\cdot|si)],$ (7)\n$= w_p^T E [\\sum_{i=t}^T \\phi (Si, ai ai \\sim \\pi(\\cdot|Si)]$ ,\nwhere E is the expectation over the randomness due to the sampling from the base language model \u03c0. Eq. 7 is derived by substituting Eq. 4 into Eq. 6. \u03c6\" gives the expected sum of \u03c6(s, a) when following policy \u03c0 starting from (s, a), which is known as successor features (SFs) that also satisfy a Bellman equation of \u03c6(s, a) (Bellman, 1966; Barreto et al., 2017). Therefore, it can be noticed that the vector wp representing personalized preferences is decoupled from the MDP dynamics.\nTo obtain the Q* function, we begin by rewriting Eq. 5 as in Rafailov et al. (2024b):\n$R(p, x, y) = \\sum_{t=1}^T R(p, s_t, a_t) = \\sum_{t=1}^T \\beta log \\frac{\\pi_{\\theta} (a_t| S_t, p)}{\\pi_{ref} (a_t| S_t, p)} + V^*(S_1).$ (8)\nDenote log$\\frac{\\pi_{\\theta} (a_t| S_t, p)}{\\pi_{ref} (a_t| S_t, p)}$ as the features of (s, a), which satisfies $logn(a|s, p) = w_p^Tlog(a|s)$. By substi- tuting this relationship and Eq. 8 into Eq. 2, we can derive the loss function for personalized reward\""}, {"title": "GENERALIZED PERSONALIZED ALIGNMENT", "content": "In this section, we discuss the ability of PAD to transfer to unseen personalized preferences. Suppose now that we have computed the optimal value functions for n personalized prefer- ences $w_1,w_2,...,w_n \\in w$, denoted as {$Q_1^*, Q_2^*,\u2026\u2026\u2026,Q_n^*$} Now, if the reward changes to $R(p_{n+1},s,a) = w_{n+1}^T\\phi(s, a)$, as long as we have wn+1 we can compute the new value func- tion of \u3160 by simply making $Q_{n+1}(s, a) = w_{n+1}^T\\phi^*(s, a)$. Once the functions $Q_{n+1}^*$ have been computed, we can apply generalized policy improvement (Bellman, 1966; Barreto et al., 2017) to estimate its performance on wWn+1.\nTheorem 1. Let $w_1 \\in W^\\phi$ and let Q be the action-value function of an optimal policy of $w_i$. for all $s \\in S, a \\in A$, and $j \\in \\{1,2,..., n\\}$, let \u03c0(s) \u2208 arg maxa maxi $Q_j^* (s, a)$. Finally, let $\\phi_{max} = \\max_{s,a} ||\\phi(s, a)||$, where || . || is the norm induced by the inner product adopted. Then,\n$Q_{n+1}^\\pi(s,a) - Q_{n+1}^\\pi(s,a) \\leq |\\mathbb{H}| \\phi_{max} min || w_{n+1} - will$ (12)\nThis term is a multiple of the distance between wi and the closest wj for which we have already computed a policy. The formula formalizes the intuition that if the agent has previously solved a similar task, it should perform well on task wi. The proofs of Theorem 1 are in the Appendix C.2."}, {"title": "PRACTICAL IMPLEMENTATIONS", "content": "In this section, we introduce the practical implementation of our Personalized Alignment at Decoding- time (PAD), which includes the optimization of the personalized reward model (PRM) and the inference-time guided decoding with token-level personalized reward.\nOptimization As previously mentioned, we can decouple personalized preferences from the MDP process during the personalized alignment procedure. Thus, we consider utilizing an LLM-based model as a personalized reward model (PRM) \u03c0\u03b8 to independently predict the features \u03c6(s, a) and preferences wp. For simplicity, we employ a single backbone to predict the embeddings for both \u03c6(s, a) and p, and then utilize an additional value head to predict wp. Optimization occurs in two stages with Equation 9. In the first stage, we fix wp as a unit vector and optimize the backbone to learn general features. In the second stage, we freeze the backbone and only optimize the value head for wp to learn different user preferences. The optimized PRM is denoted as \u03c0\u03bf.\nGuided Decoding The inference phase of PAD is shown in Figure 1. Given the personalized preference p and the current context s = (x, y0. Note that \u03b2 in Eq. 13 is also treated as a weight hyperparameter for simplicity, which slightly differs from its previous definition."}, {"title": "EXPERIMENT", "content": "PAD SETUP\nDuring the development of our personalized reward model, we utilized datasets from multiple sources including Ultrafeedback (Cui et al., 2023), HelpSteer2 (Wang et al., 2024c), Rewards-in- Context (Yang et al., 2024b), and SafeRLHF (Dai et al., 2023). To simulate personalized preferences, synthetic user prompts are prepended to instructions, following Jang et al. (2023). We employ the Llama-3-8B model (AI@Meta, 2024) as our backbone, and append a linear layer directly following the embeddings, featuring an output dimension of 4096. Comprehensive details on the datasets, the pre-processing process and the implementations are documented in Appendix A.1. During the decoding phase, we utilize greedy decoding with top-k candidates. We restrict the maximum lengths of the initial prompt and subsequent generations to 2,048 and 128 tokens, respectively. The hyperparameters, specifically \u03b2 = 1.0 and k = 10, are optimized to maximize the average reward performance observed in our validation datasets. An exhaustive analysis of the decoding strategies and hyperparameter settings is provided in Section 4.3.\nEVALUATION SETUP\nDatasets and base models. We utilize two evaluation datasets. The P-Soups (Jang et al., 2023) evaluation dataset has been filtered and modified based on the Koala evaluation by Jang et al. (2023). The HelpSteer2 (Wang et al., 2024c) (validation split) dataset is a multi-aspect alignment dataset"}, {"title": "MAIN RESULTS", "content": "Alignment on Pre-defined Preferences We initiate our evaluation by focusing on three pre-defined dimensions seen in the training phase: \u201charmless\u201d, \u201chelpful\u201d, and \u201chumor\u201d. As depicted in Figure 2, each point represents the average rewards corresponding to specific user preferences, encompassing either single or dual dimensions, evaluated across two distinct datasets. We dynamically adjust the weights of these dimensions for the baseline models. The results demonstrate that PAD can effectively align with various preferences, outperforming the baselines in terms of achieving a superior frontier. Subsequently, we compare the performance of PAD with baseline methods in aligning to the three dimensions simultaneously. For baselines, we set uniform preference weights for three dimensions. The performance of PAD across two datasets is presented in Table 2. The findings reveal that PAD has achieved substantial improvements for all three objectives. Within the P-Soups dataset, PAD achieves an average win rate of 84%, elevating the average score of the reward model from 0.32 to 1.06. Across all eight rewards or win rates, PAD surpasses all baselines in six metrics. On the HelpSteer2 evaluation dataset, PAD achieves the best in seven out of eight metrics and significantly enhances overall personalized preference alignment performance. These results demonstrate the superiority of PAD in personalized alignment.\nAlignment on Customized Preferences In previous experiments, we were limited to aligning with pre-defined preferences. As previously noted, most existing methods focus on aligning with preferences or dimensions defined during the training phase. However, in real-world personalization scenarios, there still exist diverse unseen personalized preferences, which current methods struggle to address. Considering this aspect, we evaluate the ability of alignment on customized preferences in this part. We additionally define three dimensions \u201cexpert\u201d, \u201cinformative\u201d and \u201ccreative\" that were unseen during the training phase, which result in 8 personalized preferences. We compare the alignment performance of PAD with preference prompting and MetaAligner on the P-soups dataset. The GPT-4 judgment results between the generations of different methods and Llama-3-Base are provided in Figure 3. As can be seen, PAD consistently improves win rate across all eight types of"}, {"title": "DECODING STRATEGY", "content": "Sensitivity Analysis on Decoding Hyperparameters In our initial analysis, we investigate the impact of adjusting the hyperparameters \u03b2 and k on the performance of reward models within the P-Soups dataset. Additionally, we incorporate a diversity score as a metric. A higher diversity score indicates an enhanced capability to generate texts with a broader variety of vocabulary, as detailed in Appendix A.3. The results are illustrated in Figure 5. It is evident that increasing the weighting parameter \u03b2 leads to a rise in reward scores. However, beyond a certain threshold, the scores for helpfulness and harmlessness begin to diminish, suggesting that excessively elevated \u03b2 values might lead the generation process to deviate substantially from the base model's core knowledge, thereby compromising its intrinsic qualities. Concurrently, as \u03b2 increases, there is a slight increase in diversity, indicating that higher \u03b2 values encourage the generation of a more varied vocabulary. Regarding the number of candidates k, the performance depicted in Figure 6 suggests that a larger number of candidates may slightly encourage the generation of more diverse responses. However, it has minimal impact on producing more personalized-aligned responses.\nEffect of Decoding strategies. We compare the performance with three decoding strategies, which can be integrated with our PAD. (1) Greedy: select the token with maximum score. (2) Stochastic: sample a token from the prob-"}, {"title": "MODEL-AGNOSTIC", "content": "We further apply PAD to a broader range of base models to verify its scalability and model-agnostic properties across different base models. As illustrated in Table 3, PAD significantly enhances the personalized alignment performance of models across a diverse spectrum of models, demonstrating its model-agnostic nature. In comparison with other methods that require retraining of the policy model (i.e., the language model), our approach requires only the training of a reward model to achieve model-agnostic personalized alignment. This highlights the superiority of our PAD. Additionally, we report the \u03b2 values across different base models when achieving optimal alignment. Compared to Llama-3, the \u03b2 values for other models are considerably lower, and the performance improvements are not as pronounced. This may be attributed to variations MDP dynamics between the base model and the reward model, leading to imprecise estimates of personalized rewards."}, {"title": "CONCLUSION", "content": "In this paper, we introduce a novel personalized alignment strategy, Personalized Alignment at Decoding-time (PAD), which decouples the MDP dynamics from personalized preference in the reward modeling, facilitating flexible adaptation to diverse user preferences. By employing guided"}, {"title": "ETHICS STATEMENT", "content": "Our personalized alignment approach can effectively adapt pre-trained LLMs to meet the diverse personalized preference of a broader range of users, ensuring that even underrepresented users can be aligned fairly. Moreover, our method does not require extensive training processes, allowing those with limited computational resources to benefit from state-of-the-art LLMs without incurring significant costs. This research on personalized alignment utilizes publicly available datasets, ensuring that all data complies with privacy regulations and has been anonymized where necessary. Our aim is to promote the responsible and fair use of LLMs to enhance accessibility and automation, while advocating for ethical AI development. Our study does not involve human subjects or violate legal compliance."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have made several efforts to ensure the reproducibility of our work. All the key implementation details, including the architecture of our model, the training procedures, and hyperparameter settings, are described in the Appendix A. Detailed information about the datasets used, including pre- processing steps and data template, can be found in Appendix A. We have also outlined any hardware and software configurations used for our experiments to further support reproducibility. All code and models will be made available for reproducibility and further research."}]}