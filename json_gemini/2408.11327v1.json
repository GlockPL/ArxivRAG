{"title": "Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking Across Diverse Vocabularies", "authors": ["Sai Koneru", "Matthias Huck", "Miriam Exel", "Jan Niehues"], "abstract": "Recent advancements in NLP have resulted in models with specialized strengths, such as processing multimodal inputs or excelling in specific domains. However, real-world tasks, like multimodal translation, often require a combination of these strengths, such as handling both translation and image processing. While individual translation and vision models are powerful, they typically lack the ability to perform both tasks in a single system. Combining these models poses challenges, particularly due to differences in their vocabularies, which limit the effectiveness of traditional ensemble methods to post-generation techniques like N-best list re-ranking. In this work, we propose a novel zero-shot ensembling strategy that allows for the integration of different models during the decoding phase without the need for additional training. Our approach re-ranks beams during decoding by combining scores at the word level, using heuristics to predict when a word is completed. We demonstrate the effectiveness of this method in machine translation scenarios, showing that it enables the generation of translations that are both speech- and image-aware while also improving overall translation quality.", "sections": [{"title": "1 Introduction", "content": "A broad spectrum of Large Language Models (LLMs) are being developed at an increasing pace, with efforts focused alone or together on adapting them to specific domains (Roziere et al., 2023; Bolton et al., 2024; Colombo et al., 2024), enhancing their ability to process multiple modalities (Liu et al., 2023; Tang et al., 2023; Li et al., 2024; Beyer et al., 2024), or training general-purpose LLMs using high-quality data, advanced architectures, and larger numbers of parameters (Touvron et al., 2023; Dubey et al., 2024; Jiang et al., 2023a; Mesnard et al., 2024). As a result, numerous models are now publicly available, each with its own unique strengths and weaknesses.\nMany use cases, such as image-aware translation in movie subtitling, require combining these strengths because visual cues can be essential for disambiguating the text and ensuring accurate translations.. Currently, LLMs, such as Tower (Alves et al., 2024), Alma-R (Xu et al., 2024a), and Madlad-400 (Kudugunta et al., 2024), excel at translation tasks (Kocmi et al., 2024), while models like PaliGemma (Beyer et al., 2024) and LLava (Li et al., 2024) are leading in vision-related tasks. To effectively address image-aware translation, it is essential to harness the strengths of both translation and vision models.\nOne way to address such a task is to train a multimodal LLM to enhance its translation capabilities without compromising its vision abilities or vice versa. However, this approach requires additional training and task-specific data. Another approach is to leverage ensembling the two models via shallow fusion (Gulcehre et al., 2015) or re-ranking the N-best list (Hasan et al., 2007). The disadvantage of shallow fusion is that it assumes both models share the same vocabulary, which is often not the case with current open-source models.\nAdditionally, re-ranking the N-best list is insufficient because it doesn't allow models to influence each other during decoding. Therefore, developing a plug-and-play approach that seamlessly combines different models without requiring additional training or task-specific data is highly advantageous.\nThis work aims to enable the ranker model to influence the decoding process (online) without any constraints compared to conventional offline N-best list re-ranking. We address this by ensuring that the ranker model only influences the scores for completed words and not for the last word if it is unfinished. Additionally, we propose using the ranker model to determine whether the last word is finished rather than relying on look-ahead approaches to maintain efficiency.\nOur main contributions are summarized below:\n1. Online Re-Ranking Algorithm: We introduce a novel re-ranking algorithm that operates at the word level during decoding at sub-word level, allowing for more accurate tokenization and better integration of information from different models\n2. Plug-and-Play Approach: Our method does not require additional training or task-specific data, making it a flexible and practical solution for integrating multiple models with different strengths.\n3. Context-aware Translations: We demonstrate through experiments including targeted multimodal test sets, which require information from both modalities, that our approach effectively combines the strengths of different models and improves translation quality (Illustrated in Figure 1)."}, {"title": "2 Methodology", "content": "Given that many models are trained on different tasks, architectures, modalities, and data types, combining these models to leverage inputs from multiple modalities and facilitate knowledge sharing is highly beneficial. Moreover, it is ideal if the ensembling approaches satisfy the following constraints: 1) It should not rely on shared vocabularies for flexibility in choosing models and maximizing potential combinations. 2) Effective knowledge sharing should occur during decoding to better navigate the search space exploiting this knowledge at each step. 3) Avoid requiring additional training, parameters, or major dependence on task-specific data for maximum applicability and not cause deviations from the pre-trained model.\nThis section presents our algorithm for ensembling models with different vocabularies that satisfy the aforementioned constraints. First, we explain why re-ranking partial hypotheses can lead to incorrect probability estimates if the word is incomplete. Next, we introduce and justify a heuristic-based approach that predicts whether a hypothesis is at the end of a word, allowing for accurate re-ranking of completed words in partial hypotheses. Finally, we formally describe the complete algorithm, detailing how we merge probabilities from different models and how this process can be integrated with decoding strategies."}, {"title": "2.1 Challenges of Re-Ranking Partial Hypotheses", "content": "Current Neural Machine Translation (NMT) and LLM-based models can utilize various tokenization methods, such as byte-pair encoding (BPE) (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018). These methods often result in distinct vocabularies due to variations in the data and tokenizer training processes. Despite these differences, techniques like re-ranking can still enable estimating the probability of sentences generated from another model. This is achieved by detokenizing the hypothesis from generator model and re-tokenizing it using the ranker model's vocabulary. This process enables the ranker model to produce accurate probability estimates based on its own tokenization scheme.\nNow, consider the case of re-ranking while the hypotheses are still being decoded. Assume we have model A (the generator) and model B (the ranker), each using different tokenizers assign all the tokens in the sentence \"Decoding is awesome\" with a probability of p for a particular input. However, model A tokenize the sentence with subword tokens as \"Dec od ing_is_awe some,\" while model B would tokenize it as \"Dec od ing _is_awes ome.\" If we attempt to re-rank during the decoding process, the ranker model B will provide correct probability estimates up until \"_is\" is generated. However, when the generator predicts \"_awe,\" model B would incorrectly estimate the probability because it expects \"_awes\" instead. Even though both models aim to generate the same sentence, this tokenization mismatch leads to incorrect probability estimates during the decoding process, making online re-ranking challenging."}, {"title": "2.2 End-of-Word Prediction in Decoding for Accurate Re-Ranking", "content": "While the partially generated hypothesis cannot be accurately ranked at every time step, consider the cases when each word is finished. At that time, we can re-rank the complete hypothesis as the last word is fully generated and the ranker model can tokenize the completed word as it would have done naturally, thereby providing accurate probability estimates. If we know that the last word is incomplete, we can use this information to wait and only rank the previously completed words. Knowing the end of the word enables more precise re-ranking during decoding, even with models that use different tokenization schemes.\nNonetheless, a significant challenge remains: how do we determine when the last word is completed? If the tokenizer places spaces at the right of characters, we could check the predicted token to see if it includes a space, signaling the end of a word. However, this approach is not universal, as many tokenizers do not follow this pattern, and we aim to develop a tokenizer-agnostic solution.\nOne alternative is to perform a look-ahead step to check if the word has been completed, but this method is also sub-optimal, as it would require decoding twice for each step in the generation process, significantly increasing computational complexity and reducing efficiency. We need a more efficient and generalizable method to determine when a word has been completed during decoding.\nTo address these challenges, we propose using the ranker model to predict the next token and determine if the word has been completed. This approach offers two key advantages.\nFirstly, if the ranker model predicts a space as the next top character, it indicates that the current last word has been completed. The hypothesis will be tokenized correctly, given that it is the prediction from the ranker model itself. Secondly, this prediction can be done together with the re-ranking process by simply also predicting the next token given the previous tokens of the current hypothesis to the ranker model.\nThis method is more efficient than the look-ahead approach, requiring only one pass of the generator and the ranker model. In contrast, the look-ahead method would require two passes of the generator and one pass of the ranker model. Using the ranker model in this way, we can ensure proper tokenization and accurate probability estimates during the decoding process (online) without additional computational overhead."}, {"title": "2.3 Integrating Online Re-Ranking with Search", "content": "This section formalizes achieving online re-ranking at a word level using beam search as an example of a decoding strategy. Note that the approach can also be applied to other strategies, with slight modifications when necessary.\nA set of candidate sequences is typically maintained during the search, with the number of candidates equal to the configured beam size b. At each time step, for each of the b candidate sequences, the model computes likelihood scores for all possible token extensions based on the vocabulary size V. This results in a total of b \u00d7 V possible extensions. From these b \u00d7 V extensions, the top b sequences with the highest scores are selected to form the new set of candidate sequences. This process is repeated iteratively, updating the candidate sequences at each step until enough beams are generated that include end-of-sentence tokens or until a predefined length limit is reached.\nTo enable re-ranking during the decoding process, we need to adjust the scores of the possible extensions using the ranker model. Directly calculating the likelihood of all extensions would be computationally impractical. Therefore, we introduce a new parameter topk, which selects the top topk extensions for each beam during re-ranking.\nHence, at each time step, the generator model calculates the likelihood scores for all V possible extensions for each of the b candidate sequences, resulting in b\u00d7 V extensions. Instead of re-ranking all b \u00d7 V extensions, the top topk extensions with the highest likelihood scores are selected for each beam. Thus, only b \u00d7 topk extensions are considered during re-ranking. For the selected b \u00d7 topk extensions, the ranker model estimates their scores and combines them with the original generator scores. For the remaining b \u00d7 (V \u2013 topk) extensions, the scores are set to -\u221e (logically equivalent to discarding them) since they would not be selected in the top beams.\nThis method significantly reduces computational complexity while allowing effective re-ranking of the most promising candidate extensions, improving the decoding process.\nAt every decoding step, the problem can be reformulated as determining the merged score of the top candidates according to both models.\nWhen calculating the merged score during decoding, it's essential to exclude the ranker model's probability if the last word in the current beam is incomplete. This prevents incomplete words from skewing the final score. For beams with incomplete final words, we combine the joint scores of the preceding words with the generator's score for the last word, ensuring proper normalization to address scale differences between finished and unfinished beams.\nAfter computing the merged scores, we select the top extensions and repeat the process until all beams reach the end-of-sentence token. This method ensures that the final translation is based on fully formed words, optimizing the ranker model's effectiveness and maintaining consistent scoring across all candidates."}, {"title": "2.3.1 Unified Scoring with Generator and Ranker", "content": "The algorithm to compute the merged score is formally defined in Algorithm 1 and explained below.\nLet us consider two models: the Generator MG and the Ranker MR. Let C denote the current candidate for re-ranking and inputs IG and IR for MG and MR respectively.\nLet the full candidate C consist of tokens g1, g2, g3,..., gn and r1, r2, r3, ..., rm according to MG and MR, respectively. Note that n and m denote the length of the sequence, and they may differ due to different tokenization.\nThe key idea is to rank and merge scores for completed words. We use the ranker model to predict the next token and determine if the last word is finished (Line 4).\nIf the last word is finished: We can calculate the probability of the full sequence in this case, similar to the case of N-best list re-ranking. First, we calculate the likelihood of the candidate by averaging the log probabilities for both the generator and the ranker (Line 6-7). Then, we merge the scores from both models to determine the final score for the candidate sequence using a hyper-parameter \u03b1 for weighting (Line 8). This combined score considers the estimates from both models, allowing for contributions from both models.\nIf the last word is incomplete: We cannot rank the last word due to potential incorrect tokenization. However, we can still estimate the tokens preceding the last word using the ranker model and merge their probabilities. First, we split the candidate into previous and last words based on the ranker and generator (Lines 10-11). We compute the merged score for the previous words using the weighting parameter \u03b1 (Lines 12-14). For the last word, we rely solely on the generator's scores. To address length normalization issues when combining scores from both models, we re-normalize the merged score for the previous words by multiplying it by the length of the previous word tokens j from the generator, adding the last word's score, and normalizing by the total length n (Lines 15-16).\nThis integration process ensures that the re-rankers are utilized at the appropriate decoding stages, thereby enhancing the overall quality of the generated sequences by combining the strengths of both models."}, {"title": "3 Test Suites", "content": "The major advantage of combining models with different vocabularies zero-shot is that it leverages the strengths of available pre-trained models to generate more accurate and robust output. This is particularly relevant in multimodal scenarios, where unimodal systems excel in their respective modalities but are weaker or incapable of processing other modalities. Furthermore, it can also enhance quality compared to N-best list re-ranking when used as an ensembling technique as it waits until the complete sequence is generated. Hence, to validate our approach, we consider three MT scenarios as a test bed where quality can be improved by combining different sources and evaluating with targeted test sets that require information from both models. An overview of the test suites is provided in Table 1."}, {"title": "3.1 Unimodal MT", "content": "We evaluate the use case of ensembling different LLM models to enhance translation quality. This is particularly relevant given the rapid development of various translation LLMs, where combining different systems can improve quality and robustness. We use the WMT 2022 English \u2192 German test set (Kocmi et al., 2022) to validate our approach and focus solely on assessing translation quality."}, {"title": "3.2 Multimodal MT", "content": "Translating from English to gender-marked languages is challenging when the source text lacks clear gender cues. To evaluate bias in current NMT systems, Bentivogli et al. (2020) developed the MuST-SHE test suite, which includes examples with varying forms of gender bias. This suite features cases where gender information is conveyed through audio cues, such as the speaker's voice.\nWhile End-to-End ST systems can handle such cases, they often fall short compared to advanced translation LLMs (Agarwal et al., 2023). Therefore, we use MuST-SHE for English \u2192 French to investigate if combining ST and translation LLMs can improve translation quality and address gender ambiguity.\nSimilarly, images can assist in disambiguating text and enhancing translation quality. However, translation LLMs typically do not process images, and vision LLMs alone are inadequate for translation tasks. By combining these models, we aim to leverage their strengths for better image-aware translations.\nExisting vision translation test sets often lack ambiguity, making image inputs unnecessary (Vijayan et al., 2024). To address this, Futeral et al. (2023) introduced CoMMuTE, which features ambiguous source sentences with two images and their translations. We use CoMMuTE for English \u2192 German translation in a generative framework to evaluate if images can enhance translations without compromising overall quality."}, {"title": "4 Results", "content": "This section presents the experiments conducted using our ensembling approach across various test suites. Since each test suite has a distinct experimental setup, we will address them individually. First, we will specify the models and evaluation metrics applied in each scenario. Then, we will present the results and highlight our main findings."}, {"title": "4.1 Ensembling for Improving Translations", "content": "Models: We aim to combine two models that excel in translation but possess different strengths. For this purpose, we chose the Madlad-10B, an encoder-decoder architecture trained on extensive parallel data, and ALMA-13B-R, a decoder model trained using contrastive preference optimization and selecting high quality data (Xu et al., 2024b).\nMetrics: As the models that we would like to ensemble are high quality, we report with several neural metrics to reliably validate the improvements. For reference-based we report with COMET (Rei et al., 2022a) and BLUERT (Sellam et al., 2020; Pu et al., 2021) whereas for reference-free we report with COMET-KIWI (Rei et al., 2022b), COMET-KIWI-XXL (Rei et al., 2023) and XCOMET-XXL (Guerreiro et al., 2023) metrics.\nHyper-parameters: We set the re-ranking weight \u03b1 to 0.5 given that both models have high quality and should be weighted equally. Furthermore, we set the topk to 5 and the number of beams for the generator as 5.\nTo validate our combined model and online re-ranking approach, we compare it against several baselines. First, we check if the ensemble outperforms each individual model. Next, we evaluate if our method surpasses offline re-ranking techniques, indicating a more effective ranker influence and improved search space exploration during decoding.\nWe evaluate our approach using N-best list re-ranking, with Madlad as the generator and Alma as the ranker. We generate an N-best list of 25 hypotheses with \u03b1 set to 0.5 to facilitate a fair comparison between offline and online re-ranking methods. Additionally, we test a scenario where the N-best lists from both models are concatenated and jointly re-ranked on 50 hypotheses. We reports the results for the baselines and our approach in Table 2.\nEnsembling enables to reach state-of-the-art quality: Both Madlad and Alma produce high-quality translations, though they still lag behind GPT-4 across all metrics. However, after applying offline re-ranking, their performance improves consistently, becoming competitive with GPT-4. When using our online re-ranking approach, the ensemble outperforms GPT-4 across all metrics and shows our proposed approach can improve the translation quality by a substantial margin.\nOnline re-ranking outperforms offline joint re-ranking: When Madlad serves as the generator and Alma as the ranker in our approach, the results are superior to those achieved with joint re-ranking, where both models are used simultaneously. Our approach enhances knowledge sharing and collaboration during the decoding process, leading to better translation quality."}, {"title": "4.1.1 Quality of N-best list", "content": "The primary motivation behind our approach was to influence the decoding process in real-time, rather than waiting until the end. If this is effective, we expect the N-best list to improve with online re-ranking. Additionally, using quality estimation should enhance the selection of the best hypothesis from the N-best list. To validate this, we utilize COMET-KIWI-XXL for selecting the best candidate from the top 5 beams of Madlad, comparing scenarios with and without online re-ranking and report the scores in Table 3.\nWe observe that integrating quality estimation significantly enhances Madlad's performance across all metrics. Using COMET-KIWI-XXL to select the best candidate from the top 5 beams improves score from 82.65 \u2192 86.45. This improvement is also evident in the BLUERT score, increasing from 76.79 \u2192 77.78. Additionally, comparing the top 5 beams with our approach, we find that the quality is superior, demonstrating that the early influence of ALMA in decoding. Furthermore, this allows to integrate multiple NMT models to generate N-best list together and later combined with quality estimation for maximum performance."}, {"title": "4.2 Speech-Aware Translations", "content": "Models: To tackle gender ambiguity in text translation using speaker voice information, we combine a robust text translation model with a speech-based model that excels at disambiguating gender, even if it is not as strong in translation. We use the Madlad model (Kudugunta et al., 2024) for high-quality text translation and the Seamless model for speech translation. Our approach employs Madlad as the generator and Seamless as the ranker, allowing us to leverage the speech model's ability to correct gendered forms in the translation.\nHowever, we observed that the Seamless model exhibited a bias toward the masculine gender and struggled to effectively resolve gender ambiguities using speech. To mitigate this, we conducted additional fine-tuning using LoRA (Hu et al., 2021) on a balanced speaker dataset derived from MuST-C (TED talks) with gender annotations (Di Gangi et al., 2019; Gaido et al., 2020) (Training details in Appendix A.1). We remove talks that are present in MuST-SHE for no overlap. This \"debiasing\" process improved the model's ability to disambiguate gender based on speech. Consequently, we use the Madlad and adapted Seamless models to generate high-quality, speech-aware translations.\nMetrics: To evaluate the effectiveness of our approach in disambiguating gender and improving translation quality, we use several key metrics. For gender disambiguation, we follow the methodology of Bentivogli et al. (2020) and report two metrics: accuracy (correct gender form is present) and coverage (either gender form is present).\nFor overall translation quality, we report BLEU (Papineni et al., 2002), ChrF2 (Popovi\u0107, 2016) calculated using SacreBLEU (Post, 2018), and COMET (Rei et al., 2022a).\nAdditionally, we report Sensitivity, which measures the difference between the scores of correctly and incorrectly gendered references, as suggested by Bentivogli et al. (2020).\nHyper-parameters: For decoding with Madlad, we use beam search with 5 beams. Our proposed algorithm involves two key parameters: \u03b1 and topk. We set topk to 5, resulting in a total of 25 candidates being ranked by Seamless at each step.\nWe optimized \u03b1 through grid search on the MuST-C development set (Appendix A.3) via offline re-ranking and setting it to 0.8 based on these results.\nMadlad and Seamless complement each other: Madlad excels in overall translation quality (83.5) compared to Seamless (79.31). While Seamless initially favors masculine terms, fine-tuning on balanced data improves overall quality to 80.48, significantly reducing masculine bias (90.44 to 65.89) and increasing feminine representation (25.92 to 50.18). Thus, the adapted Seamless demonstrates improved gender disambiguation, though Madlad remains superior in overall translation. Hence, combining the models can be highly beneficial.\nOnline re-ranking improves overall translation quality: After re-ranking with N-best list, we see that the translation quality is improved when Madlad as a generator and Seamless Bal as a ranker model (83.50 \u2192 83.66). In the opposite scenario where Seamless Bal uses Madlad as a ranker model, the quality also improves (80.48 \u2192 81.31) but is lower than Madlad alone. However, during online re-ranking, we see that we achieved the best performance of 83.78. This suggests that our approach facilitates knowledge sharing between the models during decoding, leading to significant quality enhancements.\nBalance between translation quality and gender disambiguation through online re-ranking\nWe observe that the highest accuracies for feminine terms (1F) are achieved when Seamless Bal is employed as a generator. Nevertheless, the overall translation quality in these instances is considerably lower compared to scenarios where Madlad is the generator. By using Madlad as a generator, we attain a higher average 1F score of 60.32 compared to offline re-ranking without compromising overall translation quality and better distribution across gender. Moreover, we achieved the highest sensitivity score of 1.1 across all configurations. This shows that our approach can consistently perform better than traditional N-best list re-ranking.\nWhile the scores for the disambiguation are not high, we would like to highlight that we focused on combining the strengths of the models. However, one can use targeted systems such as Gaido et al."}, {"title": "4.3 Image-Aware Translations", "content": "Models: To integrate image information for disambiguating source text, a robust multimodal machine translation (MT) system is essential. Initially, we experimented with the off-the-shelf instruction-tuned Llava model (Li et al., 2024). While Llava provided reasonable results, its performance was sub-par for our needs. Consequently, we chose to fine-tune the PaliGemma model (Beyer et al., 2024), which was originally trained to generate captions in multiple languages. We fine-tuned PaliGemma using the Multi30k image captions dataset (Elliott et al., 2016), adapting it with Q-LORA (Appendix A.2) for enhanced image-aware translations (PaliGemma-3B MT).\nPaliGemma is highly sensitive to image context: We observe that the sensitivity \u0394 of our fine-tuned PaliGemma model for MT is notably high across all metrics (e.g., 5.7 BLEU), demonstrating that the model is effectively using the image information to influence its translations. This suggests that PaliGemma does not disregard the visual context during translation. However, despite this sensitivity, PaliGemma's overall translation quality significantly lags behind that of Madlad, as indicated by the lower COMET score (difference of 3.32). This disparity highlights the potential benefit of combining the strengths of both models to achieve more accurate and image-aware translations.\nNo clear winner between offline and online re-ranking: Comparing offline and online re-ranking, we find that re-ranking with PaliGemma enhances translations, evidenced by a sensitivity \u25b3 increase of up to 1.28 COMET. There's also a slight improvement in overall translation quality after re-ranking. However, the difference between the two approaches is modest, especially given the small test set size of 300 examples\nWe attribute the results to two main factors based on our analysis. First, Madlad assigns very low probabilities to translations of ambiguous words other than the one it is biased toward, while PaliGemma also avoids assigning extremely high probabilities. Consequently, even after merging probabilities, the model tends to favor the incorrect translation with the highest overall probability. Second, the test sentences are relatively short, averaging 4 to 5 words, so the N-best list often includes diverse variations, making offline re-ranking similar to the online approach.\nHowever, we believe that our online re-ranking method could be more advantageous for longer sentences and more advanced vision translation models, potentially leading to better image-aware translations."}, {"title": "5 Related Work", "content": "Fusion for MT: Integrating additional language models into MT systems via shallow or deep fusion, or through re-ranking, to improve translation quality is a well-studied area (Chen et al., 2006; Hasan et al., 2007; Gulcehre et al., 2015; Li and Jurafsky, 2016; Gulcehre et al., 2017; Herold et al., 2023). Stahlberg et al. (2018) explored advanced fusion method where an NMT model is trained from scratch while keeping a pre-trained language model fixed, allowing the model to learn only what is missing. There has also been growing interest in combining NMT with document-level language models (Stahlberg et al., 2019; Petrick et al., 2023; Hoang et al., 2024). Unlike previous works that utilize static weights for merging probabilities, Jean and Cho (2020) propose dynamic coefficients, which are crucial for effectively combining models with different strengths.\nEnsembling: System combination, which involves merging multiple hypotheses to generate a better version, is one approach to leveraging the strengths of different models (Bangalore et al., 2001; Matusov et al., 2006; Heafield and Lavie, 2010; Freitag et al., 2014). Another approach is to merge model parameters (Junczys-Dowmunt et al., 2016) or distill knowledge from the models (Freitag et al., 2017). With the increasing diversity of LLMs, recent research has explored methods to combine them through vocabulary merging (Xu et al., 2024b), generating new outputs based on hypotheses (Jiang et al., 2023b), or dynamically selecting different models at each step (Shen et al., 2024).\nOur work differs from these approaches as it neither relies on vocabulary matching nor requires additional training data."}, {"title": "6 Conclusion", "content": "We proposed a novel ensembling strategy that operates at the word level during the decoding process to enhance knowledge sharing. Our approach demonstrated significant benefits across multiple scenarios. It proved effective for ensembling translation systems, and even when combined with quality estimation models, it achieved state-of-the-art translation quality. Additionally, experiments on targeted multimodal test sets revealed that our method facilitates better knowledge sharing compared to traditional re-ranking techniques.\nFor future work, we propose to explore unsupervised dynamic selection, enabling models to generate outputs only when they are better equipped for the task. We believe this approach could address the current limitations and lead to more significant improvements in image-aware translation."}, {"title": "7 Limitations", "content": "The major limitation of this work is that we operate at word-level which is not compatible for several languages that are character based. Hence, it is not trivial to merge models for generating such languages. Further analysis is necessary on character-level tokenization to accurately re-rank during the decoding steps.\nAnother drawback is that, although re-ranking enhances translation quality, it incurs a latency cost. Unlike offline re-ranking, our approach employs the ranker model at each time step, resulting in significantly slower performance.\nFinally, we focused mainly on ensembling the two models using static weights. However, since the models have different strengths, it is crucial to determine when to rely on one model versus when to ensemble both. This dynamic ensembling approach would better exploit each model's strengths while avoiding the integration of their weaknesses."}, {"title": "A Appendix", "content": "A.1 Adapting Seamless\nWe use the gender annotations from Gaido et al. (2020) to select talks with feminine speaker pronouns and an equal amount of randomly sampled masculine talks that are in the training set. We use the huggingface transformer's library (Wolf et al., 2019) for fine-tuning Seamless. We use LORA (Hu et al., 2021) to fine-tune Seamless on this data. We set the rank to 16, lora_alpha to 64 and lora_dropout to 0.1. We apply adapters on the following modules: q_proj, v_proj, linear_q, linear_v. We set batch_size to 16, gradient_accumulation_steps to 8 and train with fp16 for 20 epochs validating at every 200 steps. The learning_rate is set to 1e\u00af5. The other parameters are set to default in the transformers library.\nA.2 Adapting PaliGemma\nWe also fine-tune the PaliGemma model with the huggingface transformer's library (Wolf et al., 2019) but use Q-LORA (Dettmers et al., 2023) with 4-bit quantization as the vision models require more VRAM. We set the rank to 8, lora_alpha and lora_dropout to default. We apply adapters on the following modules: q_proj, k_proj, v_proj, gate_proj, up_proj, down_proj. We set batch_size to 2, gradient_accumulation_steps to 6 and train with bf16 for 5 epochs validating at every 200 steps. The learning_rate is set to 2e\u00af5 with AdamW optimizer. The other parameters are set to default in the transformers library.\nA.3 Hyper-parameter Tuning for Speech-Aware Translations\nTo find the re-ranking weight \u03b1, we generate the 25-best list of Madlad and Seamless on the MuST-C development set. Then, we calculate the scores of the models on these hypothesis and perform a grid search to find the optimal weight. Here, \u03b1 = 1 means that the score is only from Madlad and \u03b1 = 0.5 means equal contribution. The grid search is plotted in Figure 2.\nWe see that \u03b1 as 0.8 is always achieving higher scores. Furthermore, we see that using Seamless as generator (Figure 2b) leads to poor translation quality and \u03b1 as 1. However, in the case of Madlad as a generator (Figure 2a), we see that \u03b1 as 1 is not optimal showing that re-ranking with Seamless is indeed beneficial. Finally in the case of both models as generator (Figure 2c), we again see that \u03b1 as 1 achieves highest quality showing that Seamless is not beneficial. Using our approach of online re-ranking, we can integrate Seamless without losing quality and enable better knowledge collaboration."}]}