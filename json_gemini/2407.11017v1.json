{"title": "Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation", "authors": ["Jihyun Janice Ahn", "Rui Zhang", "Ryo Kamoi", "Lu Cheng", "Wenpeng Yin"], "abstract": "Mainstream LLM research has primarily focused on enhancing their generative capabilities. However, even the most advanced LLMs experience uncertainty in their outputs, often producing varied results on different runs or when faced with minor changes in input, despite no substantial change in content. Given multiple responses from the same LLM to the same input, we advocate leveraging the LLMs' discriminative capability to reduce this generative uncertainty, aiding in identifying the correct answers. Specifically, we propose and analyze three discriminative prompts: Direct Prompt, Inverse Prompt, and Combination, to explore the potential of both closed-source and open-source LLMs in self-improving their generative performance on two benchmark datasets. Our insights reveal which discriminative prompt is most promising and when to use it. To our knowledge, this is the first work to systematically analyze LLMs' discriminative capacity to address generative uncertainty.", "sections": [{"title": "1 Introduction", "content": "Generative AI is revolutionizing various fields by utilizing large language models (LLMs) trained to generate human-like responses based on given instructions. Despite the increasing strength of existing LLMs in terms of generation capability, a widely recognized issue is their uncertainty in responses to inputs\u2014the same model may produce significantly different responses on different runs or to equivalently varied inputs.\nPrevious studies have explored LLMs' self- improving capability that either relied on external human/tool supervision (Wang et al., 2023a; Paul et al., 2024; Gou et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Gao et al., 2023) or have not successfully explored the inner capabilities of LLMs, such as their own discriminative capability, to reduce uncertainty (Jiang et al., 2024). We argue that LLMs should focus on both their generative"}, {"title": "3 Direct-Inverse Discriminative Prompting", "content": "Given multiple answer options by LLMs' generative process (here uses five for example), this section introduces our discriminative approach Direct-Inverse Discriminative Prompting, that asks LLMs with Direct Prompt, Inverse Prompt, and finally combines their lens to find the most certain answer in self-improving generation.\nDirect Prompt. Here, we directly ask LLMs which options are correct with the following prompt:\nThis problem [problem description] has the following reasoning paths you generated: \n\u201cA: [path1]\u201d, \u201cB: [path2]\u201d, \u201cC: [path3]\u201d, \u201cD: [path4]", "E": ["path5]\". Please output the correct ones.\nInverse Prompt. Here, we ask LLMs which options are incorrect with the following prompt:\nThis problem [problem description] has the following reasoning paths you generated: \n\u201cA: [path\u2081]\u201d, \u201cB: [path2]\u201d, \u201cC: [path3]\u201d, \u201cD: [path4]", "E: [path5]\". Please output the incorrect ones.\nCombination. As humans, when asked using both Direct Prompt and Inverse Prompt prompts, their answers should be consistent. However, this is not the case with LLMs, as our analysis in Section 5.2 shows. For instance, using Direct Prompt, an LLM may believe \"A and B\" are correct, but when asked using Inverse Prompt, it might believe \u201cB and C\" are incorrect, implying that \u201cA, D, and E\" are correct.\nDirect Prompt and Inverse Prompt reflect LLMs' discriminative analysis of the problem from different perspectives, and we combine their results to improve accuracy. Specifically, we run Direct Prompt and Inverse Prompt separately multiple times and select the final answer by identifying the most consensus among the responses."]}, {"title": "4 Experiments", "content": "Datasets. Two datasets. An example of each dataset is given in appendix A."}, {"title": "5 Results", "content": "5.1 Main Results\nTable 1 presents the main results comparing different discriminative prompts (Direct Prompt, Inverse Prompt, and Combination) of LLMs on the MATH and MathQA datasets. Here are some key observations:"}, {"title": "5.2 Analysis", "content": "Q1: How frequently do LLMs experience uncertainty in their decisions, indicated by conflicts"}, {"title": "6 Conclusion", "content": "This study analyzed the development of LLM's discriminative capability to enhance self-improving generation performance. Specifically, we introduce Direct-Inverse Discriminative Prompting, a multi-faceted complementary approach to evaluating LLMs' discriminative potential. Our findings indicate that both Direct Prompt and Inverse Prompt are effective for closed-source LLMs, while for open-source LLMs, using Direct Prompt is highly and solely recommended."}, {"title": "Limitations", "content": "Our study is limited by the fact that experiments were conducted using only two datasets. In addition, if budget permits, exploring more closed-source LLMs is preferred."}, {"title": "Ethics Statement", "content": "This study uses publicly and automatically accessed datasets, and no ethical issues are present."}, {"title": "A Example Appendix", "content": "A.1 \u039c\u0391\u03a4\u0397\nQ: What is the 100th term of the arithmetic sequence 6, 10, 14, 18, ...?\nR: The common difference is $10-6 = 4$, so the 100th term is $6 + 99 \\cdot 4 = \\boxed{402}$.\nwhere \"Q\" denotes questions and \u201cR\u201d for rationale. \"R\" includes the answer in a specific format which is boxed{A}, where A is the answer for the problem.\nA.2 MathQA\nQ: what will be the difference between simple and compound interest at 14 % per annum on a sum of rs 1000 after 4 years ?\nR:s.i = ( 1000 * 14 * 4 ) / 100 = rs 560 c. i = [ 1000 * (1 + 14 /\n100) 4 - 1000] = rs  689 difference = ( 689 - 560 ) = rs 129 answer: a\nO: a) 129 , b) 130 , c) 124 , d) 133 , e) 145\nA: a\nwhere \"Q\" denotes questions, \"R\" for rationale, \"O\" for options, and \"A\" for answers."}, {"title": "2 Related Work", "content": "LLM self-improves generation. Various methods are being devised to increase the certainty of LLM-generated answers. Chain-of-Thought (Wei et al., 2023) tries to add a detailed reasoning path from the input to the output answer so that the answer is more explainable and certain. Self-Consistency (Wang et al., 2023b) has the LLM solve the same problem multiple times to obtain several results. A majority vote is then conducted to choose the most consistent result as the final answer. This approach guarantees a higher success rate than Chain-of-Thought. Based on this, diverse variants of Self-Consistency exist; for example, Universal Self-Consistency (Chen et al., 2023a), which includes reasoning to select the most consistent value as the final answer, or Early Stop Self-Consistency (Li et al., 2024), which reduces the number of answer sets used in the majority vote to save cost and time. It is worth mentioning that the above approaches are fully unsupervised, namely no human or external signals are needed.\nExploring LLM discriminative capability to enhance generation. To assess the generative and discriminative capabilities of LLMs, Liu et al. (2023) and Arora and Kambhampati (2023) carried out experiments on summarization and planning problem, respectively. The most related work, (Jiang et al., 2024), concluded that LLMs struggle to enhance their generation performance through discriminative capability because their discriminative capability is not stronger than their generative capability. Our work differs from this study in two key ways: i) Jiang et al. (2024) only considered a simplified discriminative prompt similar to our Direct Prompt. They provided the discriminative prompt with all the generated final answers without the reasoning paths. In contrast, our Direct Prompt includes reasoning-path equipped answers, which we believe can help LLMs better determine the correct answer. ii) We further analyze another complementary discriminative capability expressed by Inverse Prompt. While Inverse Prompt should theoretically yield the same conclusions if applied to humans, the inconsistency between Direct Prompt and Inverse Prompt in LLMs allows us to better understand their discriminative potential in reducing generative uncertainty. iii) Our findings suggest a different conclusion: LLMs' discriminative capabilities can indeed enhance their generation if used skillfully."}]}