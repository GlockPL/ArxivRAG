{"title": "Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation", "authors": ["Jihyun Janice Ahn", "Rui Zhang", "Ryo Kamoi", "Lu Cheng", "Wenpeng Yin"], "abstract": "Mainstream LLM research has primarily focused on enhancing their generative capabilities. However, even the most advanced LLMs experience uncertainty in their outputs, often producing varied results on different runs or when faced with minor changes in input, despite no substantial change in content. Given multiple responses from the same LLM to the same input, we advocate leveraging the LLMs' discriminative capability to reduce this generative uncertainty, aiding in identifying the correct answers. Specifically, we propose and analyze three discriminative prompts: Direct Prompt, Inverse Prompt, and Combination, to explore the potential of both closed-source and open-source LLMs in self-improving their generative performance on two benchmark datasets. Our insights reveal which discriminative prompt is most promising and when to use it. To our knowledge, this is the first work to systematically analyze LLMs' discriminative capacity to address generative uncertainty.", "sections": [{"title": "1 Introduction", "content": "Generative AI is revolutionizing various fields by utilizing large language models (LLMs) trained to generate human-like responses based on given instructions. Despite the increasing strength of existing LLMs in terms of generation capability, a widely recognized issue is their uncertainty in responses to inputs\u2014the same model may produce significantly different responses on different runs or to equivalently varied inputs.\nPrevious studies have explored LLMs' self- improving capability that either relied on external human/tool supervision (Wang et al., 2023a; Paul et al., 2024; Gou et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Gao et al., 2023) or have not successfully explored the inner capabilities of LLMs, such as their own discriminative capability, to reduce uncertainty (Jiang et al., 2024). We argue that LLMs should focus on both their generative"}, {"title": "3 Direct-Inverse Discriminative Prompting", "content": "Given multiple answer options by LLMs' generative process (here uses five for example), this section introduces our discriminative approach Direct-Inverse Discriminative Prompting, that asks LLMs with Direct Prompt, Inverse Prompt, and finally combines their lens to find the most certain answer in self-improving generation.\nDirect Prompt. Here, we directly ask LLMs which options are correct with the following prompt:\n\u201cThis problem [problem description] has the following reasoning paths you generated: \u201c A: [path1]", "B": ["path2]\u201d, \u201cC: [path3]\u201d, \u201cD: [path4]", "E: [path5]", ".", "Please output the cor- rect ones.\"\nInverse Prompt. Here, we ask LLMs which op- tions are incorrect with the following prompt:\n\u201cThis problem [problem description] has the following reasoning paths you generated: \u201c A: [path\u2081]\u201d, \u201cB: [path2]\u201d, \u201cC: [path3]\u201d, \u201cD: [path4]", "E: [path5]", ".", "Please output the in- correct ones.\"\nCombination. As humans, when asked using both Direct Prompt and Inverse Prompt prompts, their answers should be consis- tent. However, this is not the case with LLMs, as our analysis in Section 5.2 shows. For instance, using Direct Prompt, an LLM may believe \"A and B\" are correct, but when asked using Inverse Prompt, it might believe \u201cB and C\" are incorrect, implying that \u201cA, D, and E\" are correct.\nDirect Prompt and Inverse Prompt reflect LLMs' discriminative analysis of the problem from different perspectives, and we combine their results to improve accuracy. Specifically, we run Direct Prompt and Inverse Prompt separately multiple times and select the final answer by identifying the most consensus among the responses."]}, {"title": "4 Experiments", "content": "Datasets. Two datasets. An example of each dataset is given in appendix A."}, {"title": "5 Results", "content": "5.1 Main Results\nTable 1 presents the main results comparing dif- ferent discriminative prompts (Direct Prompt, Inverse Prompt, and Combination) of LLMs on the MATH and MathQA datasets. Here are some key observations:\n\u2022 Discriminative prompts (Direct Prompt, Inverse Prompt, and Combination) do not work for MetaMath. This is because Meta- Math was specifically optimized for solving math problems rather than following instruc- tions. In our experiments, MetaMath re- sponded to our discriminative prompts with noise and unstructured outputs, making an- swer parsing impossible.\n\u2022 Excluding MetaMath, Inverse Prompt out- performs Direct Prompt in 2 out of 6 cases, performs equally in one case (GPT-40 on MathQA), and underperforms in the remain- ing three cases. This is expected because nega- tion is often more challenging for AI models to understand.\n\u2022 In most cases (except for MetaMath), both Direct Prompt and Combination outper- form Universal Self-Consistency (and even Inverse Prompt generally surpasses it on closed-source LLMs), indicating the effective- ness of using LLMs' discriminative capabili- ties to find the most certain answer."}, {"title": "5.2 Analysis", "content": "Q1: How frequently do LLMs experience uncer- tainty in their decisions, indicated by conflicts between Direct Promptand Inverse Prompt? When Inverse Prompt outputs, for instance, \u201cB, C\" as incorrect answers, we consider the remain- ing options, i.e., \u201cA, D, E\u201d as the correct answer inferred by Inverse Prompt. Conflicts arise when Direct Prompt and Inverse Prompt reach differ- ent conclusions. The conflict degree is calculated as the number of conflicts divided by the total num- ber of problems for each dataset.\nTable 2 provides a summary of the severity of self-conflict within each LLM. GPT-4 demon- strates the highest consistency and self-confidence, with the lowest conflict percentages across both datasets. GPT-40 shows moderate consistency, per- forming better on the MathQA dataset than on MATH. Llama-3 exhibits the weakest performance in terms of consistency on the MathQA dataset, with the second-highest conflict rates in the MATH dataset, indicating its unreliability in this analy- sis. Lastly, MetaMath shows the highest conflict rates in both datasets having 100% of conflict rates. These results underscore the enhanced reliability of advanced models like GPT-4. They also emphasize the interestingness of our work, which leverages the inconsistency in discriminative capability to enhance the certainty in generative\nQ2: How are LLMs performing when their choice is agreed or disagreed by Direct Prompt and Inverse Prompt? To an- swer this question, we check the fine-grained Combination performance for the agreed and disagreed subsets between Direct Prompt and Inverse Prompt.\nTable 3 presents the performance of LLMs when they are certain (both Direct Prompt and Inverse Prompt agree) or uncertain (they con- flict). It is clear that when Direct Prompt and Inverse Prompt agree, the answers are more likely to be correct, demonstrating significantly higher performance than both their disagreed sub- set and the overall dataset in Table 1. This fur- ther suggests that combining Direct Prompt and Inverse Prompt is an effective method for reduc- ing uncertainty. If Direct Prompt and Inverse Prompt disagree, a comparison between Table 1 and Table 3 indicates that Direct Prompt is the preferred approach. These conclusions generally apply to most LLMs, except for MetaMath, which is non-functional due to its pretraining limitations.\nQ3: When to suggest using Direct Prompt and Inverse Prompt to self-improve generation? Based on Table 1, we can summarize two crite- ria: i) For top-performing closed-source LLMs like GPT-4 and GPT-40, using either Direct Prompt or Inverse Prompt, or their combina- tion Combination, shows promise. These top LLMs perform similarly when Direct Prompt and Inverse Prompt are used separately. Com- bining them can result in robust performance, but the additional time and budget required for Combination may not be appealing. Therefore, the concise conclusion for the top-performing closed- source models is that either Direct Prompt or Inverse Prompt is sufficient. ii) For open-source LLMs, the decision to try discriminative prompts depends on two factors: a) If the LLMs are not op- timized to follow instructions, such as MetaMath, neither Direct Prompt nor Inverse Prompt is recommended. b) Even if the model is instruction- tuned, open-source LLMs are more likely to strug- gle with understanding negation, so only Direct Prompt is strongly and exclusively recommended."}, {"title": "6 Conclusion", "content": "This study analyzed the development of LLM's dis- criminative capability to enhance self-improving generation performance. Specifically, we introduce Direct-Inverse Discriminative Prompting, a multi-faceted complementary approach to evalu- ating LLMs' discriminative potential. Our find- ings indicate that both Direct Prompt and Inverse Prompt are effective for closed-source LLMs, while for open-source LLMs, using Direct Prompt is highly and solely recommended."}, {"title": "Limitations", "content": "Our study is limited by the fact that experiments were conducted using only two datasets. In ad- dition, if budget permits, exploring more closed- source LLMs is preferred."}, {"title": "Ethics Statement", "content": "This study uses publicly and automatically ac- cessed datasets, and no ethical issues are present."}, {"title": "A Example Appendix", "content": "A.1 \u039c\u0391\u03a4\u0397\nQ: What is the 100th term of the arithmetic sequence 6, 10, 14, 18, ...?\nR: The common difference is $10-6 = 4$, so the 100th term is $6 +99 \\cdot 4 = \\boxed{402}$.\nwhere \"Q\" denotes questions and \u201cR\u201d for ratio- nale. \"R\" includes the answer in a specific format which is boxed{A}, where A is the answer for the problem.\nA.2 MathQA\nQ: what will be the difference between simple and compound interest at 14 % per annum on a sum of rs 1000 after 4 years ?\nR: s.i = ( 1000 * 14 * 4 ) / 100 = rs 560 c. i = [ 1000 * (1 + 14 / 100) 4 - 1000] = rs 689 difference = ( 689 - 560 ) = rs 129 answer : a\nO: a) 129 , b) 130 , c) 124 , d) 133 , e) 145\nA: a\nwhere \"Q\" denotes questions, \"R\" for rationale, \"O\" for options, and \"A\" for answers."}]}