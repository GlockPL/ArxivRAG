{"title": "TREEBON: ENHANCING INFERENCE-TIME ALIGNMENT WITH\nSPECULATIVE TREE-SEARCH AND BEST-OF-N SAMPLING", "authors": ["Jiahao Qiu", "Yifu Lu", "Yifan Zeng", "Jiacheng Guo", "Jiayi Geng", "Huazheng Wang", "Kaixuan Huang", "Yue Wu", "Mengdi Wang"], "abstract": "Inference-time alignment enhances the performance of large language models without requiring\nadditional training or fine-tuning but presents challenges due to balancing computational efficiency\nwith high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates\nmultiple responses and selects the best one, achieving improved performance but with a high computa-\ntional cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy\ninto Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and\npruning low-quality responses, thereby reducing computational overhead while maintaining high\noutput quality. Our approach also leverages token-level rewards from Direct Preference Optimization\n(DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm,\nUltraFeedback, GSM8K, HH-RLHF, and TutorEval datasets, demonstrating consistent improvements.\nSpecifically, TreeBoN achieves a 65% win rate at maximum lengths of 192 and 384 tokens, outper-\nforming standard BoN with the same computational cost. Furthermore, TreeBoN achieves around a\n60% win rate across longer responses, showcasing its scalability and alignment efficacy.", "sections": [{"title": "1 Introduction", "content": "Aligning large language models (LLMs) with human values is essential for ensuring their outputs reflect human\nintentions and ethical standards. When data on human preferences is available, a pretrained LLM can be fine-tuned\nto align with these preferences. One popular approach for fine-tuning is Reinforcement Learning from Human\nFeedback (RLHF), where a reward model is trained on a human-labeled preference dataset, followed by reinforcement\nlearning to fine-tune the LLM as a policy model [Ouyang et al., 2022]. Alternative methods such as Direct Preference\nOptimization [Rafailov et al., 2024a] and its variants [Azar et al., 2024, Ethayarajh et al., 2024, Meng et al., 2024]\nenable direct alignment via fine-tuning using a contrastive loss, eliminating the need for a separate reward model.\nThis paper focuses on optimizing inference-time alignment of large language models (LLMs). By leveraging inference-\ntime search, the capability of LLMs is enhanced during the generation process, improving real-time decision-making.\nVarious techniques, such as Monte Carlo Tree Search (MCTS), have been effectively applied to reasoning, planning,\nand accelerated decoding tasks [Zhao et al., 2024, Hao et al., 2023, Brandfonbrener et al., 2024, Choi et al., 2023],\ndemonstrating the potential for better decoding outcomes [Liu et al., 2023a]. In this work, we aim to explore tree search\nstrategies to further capitalize on decoding-time alignment. Our goal is to enhance the quality of alignment while\nsimultaneously reducing the computational cost of inference, providing a more efficient and aligned LLM experience.\nA most simple, yet powerful inference-time alignment method is the Best-of-N (BoN) method. We start our discussion\nwith BoN to motivate our development of more efficient solutions. BoN generates multiple sample responses and"}, {"title": "chooses the best one based on a reward function r(y|x) which characterizes how well-aligned a generated response y is\nwith respect to the given prompt x. More formally, BoN aims to approximate the solution to the following optimization\nproblem:", "content": "$$\\underset{y}{max} r(y|x),$$"}, {"title": "where the only access to y is through auto-regressively sampling the next token $y_t$ from the base policy $\\pi_{base}(\u00b7|x, y_{1:t-1})$,\nconditioned on the previous tokens.", "content": "While Best-of-N sampling has proven effective, it has a significant drawback: efficiency. Naively implementing BoN\nrequires generating N separate responses and the total inference FLOPs scales linearly with N. This not only demands\nN times more computation but also potentially leads to N times longer latency. The computational overhead can be\nprohibitively expensive for LLMs with billions of parameters, particularly when real-time or low-latency responses are\nneeded.\nSome potential solutions involve more intelligent sampling strategies such as pruning to improve efficiency. Speculative\nBest-of-N (SBON) [Zhang et al., 2024], which aims to speed up the process while only sacrificing minimal performance\non reward compared to the Best-of-N, alleviates the problem by continuing the generation of high-quality responses and\nrejecting the low-quality responses at an early stage of the generation. Cascade Reward Sampling(CARDS) [Li et al.,\n2024] use rejection sampling to iteratively generate small semantic segments to form such prefixes, based on rewards\ncomputed using incomplete sentences.\nThese accelerated methods are based on the hypothesis that utterances receiving high/low rewards early on in the\ngeneration process are likely to yield high/low rewards in the final complete response. However, this hypothesis is too\ngood to be true. In fact, off-the-shelf reward models are typically trained on complete responses, and therefore the score\nof partial completions by the reward model is usually chaotic and doesn't accurately predict the final output's quality,\nespecially for long responses. Our analysis confirmed that rewards of partial completions are not necessarily positively\ncorrelated with the final reward (see our experiment results in Appendix C).\nTo enable faster, efficient inference-time alignment, we propose to incorporate a tree search strategy into BoN sampling,\nin order to improve the alignment quality as well as reduce the overall inference cost. Our TreeBoN method maintains\nan active set of nodes, and actively grows a tree via branching and pruning. In other words, TreeBoN would sample\nmore frequently from good parent nodes but prunes nodes with low predicted rewards. This tree search strategy makes\nit possible to efficiently explore the search space.\nAnother design feature of TreeBoN is the use of implicit reward from DPO-aligned models for guidance of the tree\nresearch. DPO [Rafailov et al., 2024a] states that the DPO policy model can provide an implicit reward. Rafailov et al.\n[2024b] further points out that DPO training implicitly learns a token-level reward function. Thus, we design TreeBoN\nto be able to leverage any off-the-shelf DPO model for inference-time decoding of the target model. Our extensive\nexperiments show that a weighted combination of implicit DPO rewards would lead to superior, robust performance.\nOur observation is consistent with the fact that one can detect safety levels of the full response using the first few tokens\nQi et al. [2024].\nOur experiments show that under the same computing budget, TreeBoN achieves better performance than BoN\nextensively and stably, with the highest win-rates of 65% on UltraFeedBack [Cui et al., 2024] with length 192 and\n384, and above 60% across other datasets. By choosing a smaller N, TreeBoN could achieve better performance and\nimprove efficiency at the same time. With only 6.3% of the compute, TreeBoN still maintains a 55% win-rate against\nBoN. On the other hand, SBON can be viewed as a special example of our method with a two-layer tree whose children\nnumber is equal to one and BoN can be viewed as a two-layer tree with the children number equal to N. TreeBoN has\nthe potential to further improve efficiency than expected by taking advantage of the key-value cache which is especially\nbeneficial to the tree structure since the keys and values of parent tokens can be cached and shared by children.\nThe main contributions of this paper are as follows:\n1. We incorporate the Speculative Tree-search framework into Best-of-N Sampling to enhance efficiency and\nalignment performance simultaneously."}, {"title": "2 Related Works", "content": "Best-of-N (BON) sampling is a commonly used strategy for aligning large language models with human preferences by\nselecting the best sample out of N candidates. At training time, Amini et al. [2024] fine-tunes models by minimizing the\nKL divergence to approximate the BoN distribution, improving value alignment using variational BoN, which reduces\nthe computational cost during inference. Sessa et al. [2024], Gui et al. [2024] further enhance alignment by distilling\nthe BoN sampling behavior directly into the model during training, aiming to replicate the BoN distribution with a\nsingle sample at inference time. At inference time, Zhang et al. [2024] speeds up BoN by stopping the generation of\nunlikely candidates, and Khaki et al. [2024] combines rejection sampling with preference optimization to improve\nefficiency without sacrificing alignment performance. From a theoretical perspective, an initial estimate for the KL\ndivergence between the BoN output policy and the base model was provided for small values of N Coste et al. [2023],\nGao et al. [2023], Go et al. [2023], and this estimate was later improved to cover all values of N Beirami et al. [2024]. It\nhas also been shown that BoN and KL-regularized reinforcement learning methods achieve similar asymptotic expected\nrewards, with minimal KL deviation between them Yang et al. [2024a]. Compared with the works mentioned above, our\nwork utilizes a tree-structured search scheme / segment-wise beam search to accelerate best-of-N sampling by pruning\nthe low-reward branches early. To terminate low-reward branches early, we utilize the implicit value function from a\nDPO policy.\nMCTS has been employed in large language model tasks recently [Kocsis and Szepesv\u00e1ri, 2006]. Zhao et al. [2024]\nand Hao et al. [2023] integrates MCTS into planning and logical reasoning tasks. VerMCTS [Brandfonbrener et al.,"}, {"title": "2.1 Best-of-N Sampling for Alignment", "content": null}, {"title": "2.2 Tree-Search/MCTS For Language Model", "content": null}, {"title": "2.3 Reward Modeling", "content": "Full-sequence reward modeling. RLHF uses the Bradley-terry model to learn a reward function for full-\nsequence [Christiano et al., 2017, Stiennon et al., 2020]. DPO [Rafailov et al., 2024a] implicitly solves the KL-\nregularized RLHF problem by representing the reward with a language model.SimPO [Meng et al., 2024] considers\na different BT model based on the average (length-normalized) reward rather than the sum of rewards. It is worth\nnoting that alignment can go beyond a reward model due to the inconsistency in human preference. To this end [Azar\net al., 2023, Rosset et al., 2024, Wu et al., 2024], also optimize LLM's log-ratio according to different criteria, and the\nlog-ratio can serve as sequence-level reward indicator.\nPartial/Token-level reward modeling. Not every token contributes to human preference equally. A token-level reward\nsignal is thus desirable so that we can do credit assignments to each token. Reward grounding [Yang et al., 2024b]\nattempts to learn a token-level reward via Maximum Likelihood Estimation (MLE). They define a specific aggregation\nfunction so that token rewards can be transformed into sequence rewards, which can then be learned via MLE under the\nBT model. Reward reshaping can also be used to obtain token-level rewards. For instance, Chan et al. [2024] uses\nattention weights to redistribute the sequence reward to each token. Mudgal et al. [2023] and Han et al. [2024] propose\nlearning a value function to guide token-level sampling in controlled decoding tasks.\nInverse Q preference learning: DPO reward is a token-level reward model More recent works go beyond reward\nmodeling by treating the problem as inverse Q-learning. Rafailov et al. [2024b] shows that the DPO loss can be\ninterpreted as implicitly learning a token-level $Q^*$ function, represented by the LLM's logits. Similarly, Contrastive\nPreference Learning (CPL) [Hejna et al., 2023] assumes that human preferences follow a Bradley-Terry model based on\nthe sum of Q values rather than the sum of rewards, and proposes to learn the Q function directly. Zeng et al. [2024]\nsimilarly expand on this idea, presenting token-level direct preference optimization based on the Q value function.\nIn this work, we examine the effectiveness of these reward modeling approaches by incorporating these signals with our\ntree-search BoN framework. Additionally, we propose a new design: the weighted sum of implicit DPO rewards that\nturns out highly effective."}, {"title": "2.4 Decoding-Time Alignment", "content": "DeAL views decoding as a heuristic-guided search process and integrates alignment to decoding using a wide range\nof alignment objectives [Huang et al., 2024]. RAD [Deng and Raffel, 2023] uses a unidirectional reward model and\nARGS designs a weighted scoring function involving the reward model [Khanov et al., 2024] to do the reward-guided\nsearch for decoding-time alignment. URIAL [Lin et al., 2023] and RAIN [Li et al., 2023a] use in-context learning by\nprompting the LLMs to do the self-alignment without SFT or RLHF. Controlled decoding [Mudgal et al., 2023] trains\na value function from the reward model for better token-level scoring. RLMEC [Chen et al., 2024b] trains a generative\ntoken-level reward model for alignment. Cascade Reward Sampling(CARDS) [Li et al., 2024] uses a reward model\non semantically complete segments to accelerate the decoding. Shi et al. [2024] extends decoding-time alignment to\nmultiple objectives by generating the next token from a linear combination of predictions of all base models."}, {"title": "3 Preliminaries", "content": "To approximate the optimization problem of maximizing the reward function r(y|x) which measures how well a\ngenerated response y sampled from the base policy $\\pi_{base}(\u00b7|x)$ aligns with respect to the given prompt x, Best-of-N\nSampling (BoN) selects the response with the highest reward score from N independent and identically distributed\n(i.i.d.) responses generated by the language model $\\pi_{base}$:\nwhere the only access to y is through auto-regressively sampling the next token yt from the base policy $\\pi_{base}(\u00b7|x, y_{1:t-1})$,\nconditioned on the previous tokens."}, {"title": "3.1 Best-of-N sampling (BON)", "content": "$$y^* = \\underset{y\\in\\{y_k\\sim\\pi_{base}(x)\\}_{k=1}^{N}}{argmax} r(y|x),$$"}, {"title": "Algorithm 1 Best-of-N Sampling (BON)", "content": "1: Input: Prompt x, base policy $\\pi_{base}$, reward model r, number of samples N, max length $I_{max}$\n2: Output: Response y* with the highest reward using BoN\n3: Initialization: Generate N responses $\\{y^1, y^2, ..., y^N\\}$, each with maximum length $I_{max}$\n4: Query the reward model to compute the reward scores r(y|x) for each generated response $y \\in \\{y^1, y^2, ..., y^N\\}$\n5: Find the response y* with the highest reward:\n$$y = \\underset{y\\in\\{y^1,y^2,...,y^N\\}}{argmax} r(y|x)$$\n6: Return the response y*"}, {"title": "3.2 Token-Level Markov Decision Process and Soft Q-Learning", "content": "Rafailov et al. [2024a] demonstrated that under the Max-Entropy reinforcement learning (RL) formulation, the token-\nlevel log-ratio can be interpreted as an implicit token-level reward or advantage function, which remains invariant under\nreward shaping. Below, we briefly restate the key setting and results.\nThe token-level Markov Decision Process (MDP) defines the state $s_t = (x_1,x_2,..., x_m, y_1, y_2, . . ., y_t)$ as the tokens\ngenerated so far, and the action $a_t = y_{t+1}$ as the next token to be predicted. The auto-regressive language model is\nthus a policy $\\pi(a_t | s_t)$. The transition dynamics are deterministic: $S_{t+1} = s_t|a_t$, simply appending the next token to the\ncurrent generated tokens to form a new sequence.\nThe RLHF formulation is expressed as a reverse-KL regularized reward maximization problem:\nwhich can be rewritten as a Max-Entropy RL problem:\nOr equivalently at the token level:\nwith the token level reward function $r'$ defined as:"}, {"title": null, "content": "$$\\underset{\\theta}{max} E_{x\\sim X,y\\sim\\pi_{\\theta}(\\cdot|x)} [r(y|x)] - \\beta E_{x\\sim x} [KL(\\pi_{\\theta}(\\cdot|x)||\\pi_{ref}(\\cdot|x))],$$"}, {"title": null, "content": "$$\\underset{\\pi}{E_{x\\sim X,y\\sim\\pi_{\\theta}(:\\x)}} [r(y|x) + \\beta log \\pi_{ref}(y|x)] + \\beta E_{x\\sim x} [H(\\pi_{\\theta}(\\cdot|x))].$$"}, {"title": null, "content": "$$\\underset{\\pi_{\\theta}}{Eso\\sim X, a_t\\sim\\pi_{\\theta} (S_t)} \\sum_{t=1}^{T} r' (S_t, a_t) + \\beta E_{so\\sim x} [H(\\pi_{\\theta}(\\cdot|So))],$$"}, {"title": null, "content": "$$r' (st, at) := \\begin{cases}\n\\beta log \\frac{\\pi_{ref} (at |st)}{\\pi_{\\theta}(at |st)}, \\text{ if } st+1 \\text{ is not terminal,} \\\\\n(r(y|x) + \\beta log \\frac{\\pi_{ref}(at|st)}{\\pi_{\\theta}(at|st)}, \\text{ if } st+1 = (x, y) \\text{ is terminal.}\n\\end{cases}$$"}, {"title": "For simplicity, let us assume that the horizon is fixed at T. The derivation of the Max-Entropy RL formulation [Ziebart,\n2010, Rafailov et al., 2024b] utilizes the (soft) optimal value function $V^*$ and the (soft) optimal Q-function $Q^*$, as\nfollows:", "content": "$$V^*(ST+1) = 0, \\text{ (terminal state)},$$\n$$Q^* (st, at) = r' (st, at) + V^* (St+1),$$\n$$V^*(st) = log \\sum_{a} exp(Q^* (st, a)), \\text{ when } t \\leq T.$$"}, {"title": "The optimal policy $\\pi^*$ satisfies the following equation:", "content": "$$\\beta log \\pi^* (at st) = Q^* (st, at) \u2013 V^*(st),$$"}, {"title": "which can be further rewritten when t < T:", "content": "$$\\beta log \\frac{\\pi^*(at St)}{\\pi_{ref}(at St)} = V^*(st+1) \u2013 V^*(st).$$"}, {"title": "This suggests that we can use the partial sum of the implicit reward from a DPO policy to characterize the potential\nfinal reward given a prefix sequence of length K:", "content": "$$\\sum_{t=0}^{K-1} \\beta log \\frac{\\pi^*(ak Sk)}{\\pi_{ref}(ak Sk)} = V^*(SK) \u2013 V^*(So).$$"}, {"title": "Since $s_0 = (x_1, x_2,..., x_m) = x, V^*(s_0)$ is the same for all responses.", "content": null}, {"title": "4 Method", "content": "In this section, we introduce TreeBoN, a novel inference-time algorithm that enhances alignment quality and efficiency\nby incorporating a speculative tree-search structure into the Best-of-N (BoN) sampling framework. TreeBoN iteratively\nexpands high-reward partial responses, pruning low-quality candidates at early stages. The algorithm leverages a\nweighted implicit reward from a Direct Preference Optimization (DPO) policy model to improve the quality of partial\nresponse evaluation. Below, we describe the key steps involved in TreeBoN."}, {"title": "4.1 Overview of TreeBoN Algorithm", "content": "TreeBoN operates by generating candidate responses layer-by-layer in a tree structure. The algorithm begins with a\nset of initial root responses, and at each subsequent layer, only high-reward responses are selected and expanded into\nmultiple children. This speculative search through the tree space improves both the efficiency and the final response\nquality. The overall structure of TreeBoN is illustrated in Algorithm 2 and Figure 1.\nThe algorithm takes as input the prompt x, a base policy $\\pi_{base}$ for generating candidate responses, a partial-reward\nfunction r, and key hyper-parameters including the number of root samples N, maximum response length $I_{max}$,\nbranching factor(number of children per node) $N_{children}$, and the number of tree layers $N_{layer}$.\nFurthermore, $C_i$ denotes the candidate set containing all partial responses generated in the i-th layer. $P_i$ denotes the i-th\nlayer active set containing all promising partial responses for expansion in the next layer. $l_i$ is the max new token length\nfor generation in each layer, where"}, {"title": null, "content": "$$l_i = \\frac{I_{max}}{N_{layer}}.$$"}, {"title": "Algorithm 2 TreeBoN Algorithm", "content": "1: Input: Prompt x, base policy $\\pi_{base}$, partial-reward function r, number of root samples N, max length $I_{max}$,\nbranching factor $N_{children}$, number of tree layers $N_{layer}$\n2: Output: Response y* with the highest reward using TreeBoN\n3: Initialization: Split the total max length $I_{max}$ into segments $l_1, l_2,..., l_{N_{layer}}$ where $l_i = \\frac{I_{max}}{N_{layer}}$\n4: Generate N initial candidate responses for the first-layer candidate set $C_1 = \\{y^1, y^2, ..., y^N\\}$, each with a length\nof $l_1$.\n5: for i = 1 to $N_{layer}$ do\n6: Query the reward model or partial reward function r(y|x) to compute the reward scores for each candidate\nresponse $y \\in C_i$\n7: Select the top $N_{children}$ candidates from $C_i$ based on reward scores to form the i-th layer active set $P_i$\n8: for each parent response $y \\in P_i$ do\n9: For each parent y, continue generation by sampling $N_{children}$ child responses from the base policy $\\pi_{base}$, each\nwith a max new token length $l_{i+1}$, to form the next set of candidates $C_{i+1}$\n10: end for\n11: end for\n12: After all layers are generated, query the reward model for the final set of responses $C_{N_{layer}}$\n13: Find the response y* with the highest reward:\n$$y^* = \\underset{Y\\in C_{N_{layer}}}{argmax}r(y|x)$$\n14: Return the response y*"}, {"title": "4.2 TreeBoN Generation Process", "content": "1. Initial Candidate Generation: TreeBoN begins by generating N candidate responses $C_1 =\n$\\{y^1,y^2,...,y^N\\}$ with a length of $l_1$ using the base policy $\\pi_{base}$. The total maximum response length\n$I_{max}$ is split into segments $l_1, l_2, ...,l_{N_{layer}}$ evenly where $l_i = \\frac{I_{max}}{N_{layer}}$.\n2. Partial Reward Scoring: At each layer i, the reward model or partial-reward function r(y|x) is used to\ncompute the reward score for each candidate response $y \\in C_i$. This is performed after generating partial\nresponses of length $l_i$.\n3. Pruning and Selection: Based on the reward scores, the top $N_{children}$ candidates from the current layer are\nselected to form the active set $P_i$. These high-reward parent responses are used to generate child responses at\nthe next layer.\n4. Response Expansion: For each parent response $y \\in P_i$, TreeBoN generates $N_{children}$ child responses by\nsampling from the base policy $\\pi_{base}$ with a maximum new token length $l_{i+1}$. This process generates the\nnext-layer candidate set $C_{i+1}$. It is worth noting that the set size of the candidate set is always N and the\nset size of $P_i$ is always $N_{children}$ to ensure an equal number of total generated tokens without requiring extra\ncomputing budget.\n5. Final Selection: After generating candidates for all layers, the reward model computes the final rewards for\nthe candidate responses in the last layer $C_{N_{layer}}$. The response y* with the highest reward is selected as the\nfinal output:\n$$y^* = \\underset{y\\in C_{N_{layer}}}{argmax}r(y|x)$$"}, {"title": "4.3 Weighted Implicit Reward As Guidance", "content": "One of the key contributions of TreeBoN is the use of a weighted implicit reward function, inspired by Rafailov et al.\n[2024a,b], Qi et al. [2024], to evaluate partial responses. This approach allows TreeBoN to replace the traditional\nreward model with a DPO policy model, which provides more accurate rewards for incomplete responses. The partial\nreward for a sequence $y_{:K}$ is computed as:"}, {"title": null, "content": "$$r_{partial} (y_{:K}|x) = \\sum_{k=0}^{K-1} w_k log \\frac{\\pi^*(y_k|x, y_{:k})}{\\pi(y_k|x, y_{:k})},$$"}, {"title": null, "content": "where $w_k = \\frac{1}{K}$ acts as a weighting factor to adjust the contribution of each token-level log-likelihood ratio. This\nweighted reward helps prune low-quality responses early and encourages the continuation of higher-quality candidates\nthroughout the tree expansion process."}, {"title": "4.4 Comparison to Baseline Methods", "content": "TreeBoN builds upon and extends earlier sampling strategies, such as Accelerating Best-of-N via Speculative Rejection\n(SBON) [Zhang et al., 2024], by integrating a speculative tree-search framework and partial reward function. SBON\nrelies on the assumption that partial-reward scores are positively correlated with full-response rewards. However, this\nassumption often leads to suboptimal performance in alignment tasks due to the inaccurate scoring of partial responses\nby reward models which are typically trained on complete responses. TreeBoN addresses this limitation by utilizing\na more precise implicit reward signal derived from the Direct Preference Optimization (DPO) policy model, which\nsignificantly enhances the reliability of partial-reward approximation.\nMoreover, TreeBoN leverages a hierarchical tree structure to explore the response space more comprehensively,\nbalancing both alignment quality and computational efficiency. This tree-based approach allows for more flexible and\neffective pruning of low-quality responses while expanding promising candidates over multiple layers. As a result,\nTreeBoN can be seen as a generalization of SBoN, where setting $N_{children} = 1$ and $N_{layer} = 2$ reduces TreeBoN to the\ntwo-layered structure of SBON.\nCompared to traditional Best-of-N (BoN) sampling, which explores candidate responses without any hierarchical\nstructure, TreeBoN employs a more structured exploration strategy. By generating and refining responses layer by\nlayer, TreeBoN achieves a more efficient search of the response space using fewer overall samples. This leads to\nimprovements in both speed and performance, as the tree-based generation effectively balances the trade-off between\nexploration and exploitation.\nTreeBoN can be further accelerated while maintaining high alignment quality by taking advantage of key-value caching\nmechanisms, particularly beneficial in the tree structure, where the keys and values of parent tokens can be reused by\ntheir children."}, {"title": "5 Experiments", "content": "We use a set of Llama models: LLaMA3-iterative-DPO-final [Xiong et al., 2024, Dong et al., 2024] as the DPO policy\nmodel, with its SFT (supervised fine-tuning) checkpoint trained from Llama 3 8B [AI@Meta, 2024] and reward model\nFsfairX-LLaMA3-RM-v0.1 [Dong et al., 2023, Xiong et al., 2024] from Llama 3 8B Instruct [AI@Meta, 2024]. The\nSFT model was trained on a set of high-quality instruction datasets for 1 epoch; the reward model was formulated\nas a Bradley-Terry model optimizing the negative log-likelihood loss function on a mixture of filtered datasets; and\nnotably, the DPO policy model was initialized from the SFT model and updated on the online preference signals\nproduced by the aforementioned reward model (as a proxy of human feedback). We refer readers to Xiong et al.\n[2024] for the details of iterative online RLHF and the training of these models. We also use an additional DPO model\nLlama-3-8B-SFR-Iterative-DPO-R2, abbreviated as SFR in this section. The baseline is the Best-of-N sampling with\nN equal to 128 and the max token length of responses varies from 192 to 768. For Tree-based BoN with Weighted\nImplicit Reward, unless otherwise specified, we set the number of tree layers as 4, the number of children per node\n4. Considering the cost of the evaluation, we take 100 randomly selected samples from each dataset, following the\nsame setting as SBON [Zhang et al., 2024]. We evaluate the baseline and our methods and take the average of 3 runs of\ndifferent seeds on AlpacaFarm [Dubois et al., 2024], UltraFeedback [Cui et al., 2024], GSM8K [Cobbe et al., 2021],\nand HH-RLHF [Bai et al., 2022]. For TutorEval [Chevalier et al., 2024], we choose 100 closed-book questions."}, {"title": "5.1 Experiment Setting", "content": null}, {"title": "5.1.1 Metrics", "content": "For all datasets except for GSM8k, we conduct the standard GPT4 win-rate evaluations of our proposed\nmethod against the baseline. More specifically, given the same prompt, a response from the baseline and a response\nfrom the compared method are fed to an automatic evaluator of AlpacaEval [Li et al., 2023b] with randomized positions,\nwhich then formats them into a prompt, and asks GPT4 [Achiam et al., 2023] to rank both responses.\nFor GSM8k, we report the zero-shot pass@1 solve rate [Cobbe et al., 2021]. Pass@k measures the\nrate of successfully passing the test (answering the math question correctly) from the k responses that the algorithm\ngenerates. Thus, pass@1 means that the algorithm only outputs one response per question. We first split the response\nby space into words and numbers, and then count it to be correctly solved if the answer is in any of the numbers. We\nextract the number after \"answer is \" as the final answer.\nWe consider FLOPs as a cost metric, and the Llama models we are using are all based on Llama 3 8B, which has\n8 billion parameters, 32 layers, a context length of 8192, and token dimensions of 4096. The cost of LLMs mainly arises\nfrom the number of generated tokens and the matrix multiplications for dense transformers like Llama 3, considering\nthe practical implementations of KV Cache that enable keys and values of parent tokens to be reusable (for the reward\nmodel and DPO model as well), we can approximate inference FLOPs with the same formula as in Brown et al. [2024]:"}, {"title": "GPT4 Win-rate", "content": null}, {"title": "Pass@1 Solve Rate", "content": null}, {"title": "FLOPS", "content": null}, {"title": null, "content": "FLOPS per token \u2248 2 * (num parameters +"}]}