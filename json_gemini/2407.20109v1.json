{"title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning", "authors": ["Liyuan Mao", "Weinan Zhang", "Xianyuan Zhan", "Haoran Xu", "Amy Zhang"], "abstract": "One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those local-optimal modes. We thus generate a few candidate actions and carefully select from them to approach global-optimum. Different from all other diffusion-based offline RL methods, the guide-then-select paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoids that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.", "sections": [{"title": "1 Introduction", "content": "We study the problem of offline reinforcement learning (RL), where the goal is to learn effective policies solely from offline data, without any additional online interactions. Offline RL is quite useful for scenarios where arbitrary exploration with untrained policies is costly or dangerous, but sufficient prior data is available, such as robotics [18] or industrial control [56]. Most previous model-free offline RL methods add a pessimism term to off-policy RL algorithms [50, 9, 2], this pessimism term acts as behavior regularization to avoid extrapolation errors caused by querying the Q-function about values of potential out-of-distribution (OOD) actions produced by the policy [25]. However, explicitly adding the regularization requires careful tuning of the regularization weight because otherwise, the policy will still output actions that are not seen in the dataset. DIstribution Correction Estimation (DICE) methods [35, 41, 34] provide an implicit way of doing so. By applying convex duality, DICE-based methods solve for the optimal stationary distribution ratio between the"}, {"title": "2 Preliminaries", "content": "We consider the RL problem presented as a Markov decision process [45], which is specified by a tuple M(S, A, P, d_0, r, \\gamma). Here S and A are state and action space, P(s'|s, a) and d_0 denote transition dynamics and initial state distribution, r(s, a) and \\gamma represent reward function and discount factor, respectively. The goal of RL is to find a policy \\pi(a|s) which maximizes expected return \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]. Another equivalent LP form of expected return is \\mathbb{E}_{(s,a)\\sim d^{\\pi}} [r(s, a)], where d^{\\pi}(s, a) := (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t Pr(s_t = s, a_t = a) is the normalized discounted stationary distribution of \\pi [36]. For simplicity, we refer to d^{\\pi}(s, a) as the stationary distribution of \\pi. Offline RL considers the setting where interaction with the environment is prohibited, and one need to learn the optimal \\pi from a static dataset \\mathcal{D} = \\{s_i, a_i, r_i, s'_i \\}_{i=1}^N. We denote the empirical behavior policy of \\mathcal{D} as \\pi_{\\mathcal{D}}."}, {"title": "2.1 Distribution Correction Estimation", "content": "DICE methods [41] incorporate the LP form of expected return \\mathbb{I}(\\pi) = \\mathbb{E}_{(s,a)\\sim d^{\\pi}} [r(s, a)] with a regularizer D_f(d^{\\pi}||d_{\\mathcal{D}}) = \\mathbb{E}_{(s,a)\\sim d^{\\pi}} [f(\\frac{d^{\\pi}}{d_{\\mathcal{D}}})], \\frac{d^{\\pi}}{d_{\\mathcal{D}}} = \\mathbb{E}_{(s,a)\\sim d_{\\mathcal{D}}} [f'(\\frac{d^{\\pi}}{d_{\\mathcal{D}}})], where D_f is the f-divergence induced by a convex function f [3]. More specifically, DICE methods try to find an optimal policy \\pi^* that satisfies:\n\n\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{(s,a)\\sim d^{\\pi}} [r(s, a)] - \\alpha D_f(d^{\\pi}||d_{\\mathcal{D}}). \\tag{1}\n\nThis objective is generally intractable due to the dependency on d^{\\pi}(s, a), especially under the offline setting. However, by imposing the Bellman-flow constraint [33] \\sum_{a \\in A} d(s, a) = (1 - \\gamma)d_0(s) +"}, {"title": "2.2 Diffusion Models in Offline Reinforcement Learning", "content": "Diffusion models [42, 15, 43] are generative models based on a Markovian noising and denoising process. Given a random variable x_0 and its corresponding probability distribution q_0 (x_0 ), the diffusion model defines a forward process that gradually adds Gaussian noise to the samples from x_0 to x_T (T > 0). Kingma et al. [20] shows there exists a stochastic process that has the same transition distribution q_{t0} (x_t|x_0) and Song et al. [43] shows that under some conditions, this process has an equivalent reverse process from T to 0. The forward process and the equivalent reverse process can be characterized as follows, where w_t is a standard Wiener process in the reverse time.\n\nq_{t0}(x_t|x_0) = \\mathcal{N}(x_t | a_t x_0, \\sigma_t^2 \\mathbb{I}) dx_t = [f(t)x_t - g^2(t) \\nabla_{x_t} \\log q_t(x_t)]dt + g(t)dw_t, x_T \\sim q_T(x_T). \\tag{5}\n\nHere we slightly abuse the subscript t to denote the diffusion timestep. a_t, \\sigma_t are the noise schedule and f(t), g(t) can be derived from a_t, \\sigma_t [31]. For simplicity, we denote q_{t0} (x_t|x_0) and p_{0t}(x_0|x_t) as q(x_t|x_0) and p(x_0|x_t), respectively. To sample from q_0(x_0) by following the reverse stochastic differential equation (SDE), the score function \\nabla_{x_t} \\log q_t (x_t) is required. Typically, diffusion models use denoising score matching to train a neural network \\epsilon_{\\theta}(x_t, t) that estimates the score function [46, 15, 43], by minimizing \\mathbb{E}_{t \\sim U(0,T), x_0 \\sim q_0(x_0), \\epsilon \\sim \\mathcal{N}(0,1)} [||\\epsilon_{\\theta}(x_t, t) - \\epsilon||^2], where x_t = a_t x_0 + \\sigma_t\\epsilon. As we mainly focus on diffusion policy in RL, this objective is usually impractical because q_0(x_0) is expected to be the optimal policy \\pi^*(a|s). A more detailed discussion is given in Appendix A."}, {"title": "3 Diffusion-DICE", "content": "In this section, we introduce our method, Diffusion-DICE. We start from an extension of DICE-based method that considers DICE as a transformation from the behavior distribution to the optimal policy distribution, which motivates us to use diffusion models to perform such transformation. We then propose a guide-then-select procedure to achieve the best action, i.e., we propose in-sample guidance learning for accurate policy transformation and use the critic to do optimal action selection to boost the performance. We also propose a piecewise f-divergence to stabilize the gradient during learning. We give an illustration of the guide-then-select paradigm and use a toycase to showcase the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that."}, {"title": "3.1 An Optimal Policy Transformation View of DICE", "content": "As mentioned before, DIstribution Correction Estimation (DICE) methods, an important line of work in offline RL and IL provides us with an elegant way to estimate the optimal stationary distribution ratio between d^*(s, a)) and d_P (s, a) [28, 41, 34]. We show that this ratio also directly indicates a proportional relationship between the optimal in-support policy \\pi^*(a|s) and the behavior policy \\pi_\\mathcal{D}(a|s). This proportional relationship enables us to transform \\pi^D into \\pi^*.\n\nOur key observation is that the definition of d^{\\pi} (s, a) inherently reveals a bijection between \\pi(a|s) and d^{\\pi}(s, a). Given a relationship between d^* (s, a) and d_\\mathcal{D} (s, a), we can use this bijection to derive a relationship between \\pi^*(a|s) and \\pi_{\\mathcal{D}}(a|s). We formalize the bijection and derived relationship as:\n\n\\pi(a|s) = \\frac{d^{\\pi}(s, a)}{\\int_{a \\in A} d^{\\pi}(s, a)da},  \\pi^*(a|s) = \\frac{\\frac{d^{*}(s,a)}{d_{\\mathcal{D}} (s,a)} \\pi_{\\mathcal{D}}(a|s)}{\\int_{a \\in A} \\frac{d^{*}(s,a)}{d_{\\mathcal{D}} (s,a)} \\pi_{\\mathcal{D}}(a|s)da}.\n\nThe proof is given in Appendix B. Since the denominator in the second equation involves an integral over a \\in A and is unrelated to the chosen action, the relationship indicates \\pi^*(a|s) \\propto \\frac{d^*(s,a)}{d_{\\mathcal{D}} (s,a)} \\pi_{\\mathcal{D}}(a|s). This means that the transformation between d_\\mathcal{D} (s, a) and d^* (s, a) can be extended to the transformation between \\pi_{\\mathcal{D}}(a|s) and \\pi^*(a|s). This transformation motivates the use of deep generative models, e.g. diffusion models, to first fit the behavior distribution using their strong expressiveness and then directly perform this transformation during sampling. More specifically, the score function of the optimal policy and the behavior policy satisfy the following relationship:\n\n\\nabla_{a_t} \\log \\pi_t^*(a_t|s) = \\nabla_{a_t} \\log \\pi_t^{\\mathcal{D}}(a_t|s) + \\nabla_{a_t} \\log \\mathbb{E}_{a_0 \\sim \\pi_{\\mathcal{D}}(a_0|a_t,s)} [\\frac{d^{*}(s, a_0)}{d_{\\mathcal{D}} (s, a_0)}]. \\tag{6}\n\nThe proof is given in Appendix B. Intuitively, \\nabla_{a_t} \\log \\pi_t^{\\mathcal{D}}(a_t|s) tells us how to generate actions from \\pi_{\\mathcal{D}} and \\nabla_{a_t} \\log \\mathbb{E}_{\\pi_{\\mathcal{D}}(a_0|a_t,s)} [\\frac{d^{*}(s, a_0)}{d_{\\mathcal{D}} (s, a_0)}] tells us how to transform from \\pi_\\mathcal{D} to in-support \\pi^* during the reverse diffusion process. As mentioned before, when representing \\pi^* with a diffusion model, all we need is its score function \\nabla_{a_t} \\log \\pi_t^*(a_t|s). Equivalently, we focus on the right-hand side of Eq.(6). The first term is just the score function of \\pi^D, which is fairly easy to obtain from the offline dataset [14, 4]. To perform the transformation, we still require the second term. Fortunately, DICE allows us to acquire the inner ratio term in an in-sample manner directly from the offline dataset, and we will show in the next section how to exactly compute the whole second term using offline dataset."}, {"title": "3.2 In-sample Guidance Learning for Accurate Policy Transformation", "content": "Although DICE provides us with the optimal stationary distribution ratio, which is the cornerstone of the transformation, the second term on the right-hand side of Eq.(6) is still intractable due to the conditional expectation. Our key observation is that for arbitrary non-negative function f(x), the optimizer of the following convex problem is unique and takes the desired form of log-expectation.\n\nLemma 1. Given a random variable X and its corresponding distribution P(X), for any non-negative function f(x), the following problem is convex and its optimizer is given by y^* = \\log \\mathbb{E}_{X \\sim P(X)} [f(x)],\n\n\\min \\mathbb{E}_{X \\sim P(X)} [f(x) \\cdot e^{-y} + y].\n\nIt is evident that the optimizer can be used to derive the second term in Eq.(6). In fact, we show the second term can be obtained directly from the offline dataset, if we optimize the following objective. Here g_{\\theta} is a guidance network parameterized by \\theta and we denote d^{*}(s,a) (a) as w^*(s, a) for shorthand.\n\nTheorem 1. \\nabla_{a_t} \\log \\mathbb{E}_{\\pi_{\\mathcal{D}}(a_0|a_t,s)} [w^*(s, a_0)] can be obtained by solving the following optimization problem:\n\n\\min \\mathbb{E}_{t \\sim U(0,T)} \\mathbb{E}_{a \\sim \\pi_\\mathcal{D}^0 (a|s)} \\mathbb{E}_{a_t \\sim p(a_t|a_0)} [w^*(s, a)e^{-g_{\\theta}(s,a_t,t)} + g_{\\theta}(s,a_t,t)], \\tag{7}\n\nas the optimal solution \\theta^* satisfies \\nabla_{a_t} g_{\\theta^*}(s, a_t,t) = \\nabla_{a_t} \\log \\mathbb{E}_{\\pi_{\\mathcal{D}}(a_0|a_t,s)} [w(s, a_0)].\n\nThis objective has two appealing properties. Firstly, compared to guide-based and select-based methods, we don't need to use multiple actions generated by the behavior diffusion model, we only need one sample from \\pi_{\\mathcal{D}}(a|s) to estimate the second expectation, enabling an unbiased estimator of this objective using only offline dataset possible. Secondly, because w^*(s, a) must be non-negative to ensure a valid transformation, this objective is convex with respect to g_{\\theta}(s, a_t, t). This indicates a guarantee of convergence to g_{\\theta^*} (s, a_t, t) under mild assumptions (see proof in Appendix B).\n\nDirectly inducing \\nabla_{a_t} \\log \\mathbb{E}_{\\pi_{\\mathcal{D}}(a_0|a_t,s)}[w^*(s, a_0)] from the dataset is crucial in the offline setting. Firstly, it avoids the evaluation of OOD actions and solely depends on reliable value predictions of in-sample actions. Consequently, the gradient exploits minimal error in the critic, enabling it to more accurately transform \\pi^\\mathcal{D} into in-support \\pi^*. Moreover, because all values of w^*(s, a) used in our method are based on in-sample actions, applying this gradient in the reverse process will guide the samples towards in-sample actions with high w^*(s, a) value.\n\nWe refer to this method as In-sample Guidance Learning (IGL). Note that previous methods either have a biased estimate of the gradient [17, 6], or have to rely on value predictions for diffusion generated actions, which could be OOD [32]. We refer to Appendix C for an in-depth discussion of IGL against other guidance methods."}, {"title": "Stablizing gradient using piecewise f-divergence.", "content": "In practice, there exists one issue when computing the guidance term. Note that in Eq.(4), w^*(s, a) could become zero, depends on the choice of f. Given a specific a_t, if w^*(s, a_0) = 0 for actions under \\pi_{\\mathcal{D}}(a_0|a_t, s), the second gradient term will not be well-defined due to taking the logarithm of 0. In practice, this can result in an unstable gradient. To avoid such issue, (f')^{-1}(x) should be positive for any given x. Also, to facilitate the optimization process, f^*(x) should possess a closed-form solution and good numerical properties. Unfortunately, none of the commonly used f-divergence can accommodate both. However, considering the following two f-divergences:\n\nIt is obvious that Reverse KL divergence possesses the positive property but exhibits numerical instability given a large x due to the exponential function in f^*(x), while Pearson \\chi^2 divergence avoids the instability in the exponential function, its (f')^{-1}(x) value is negative when x < -2.\n\nTo take advantage of both divergences while avoiding their drawbacks, we propose the following piecewise f-divergence that has properties similar to Pearson \\chi^2 when x is large while has properties"}, {"title": "3.3 Boost Performance with Optimal Action Selection", "content": "After transforming \\pi^\\mathcal{D} into in-support \\pi^*, we now introduce the select-step to boost the performance during evaluation. One issue is the multi-modality in the optimal policy, this partially arises from the policy constraint used to regularize its output [9, 25, 55, 34]. Under policy constraint, the regularized optimal policy is unlikely to be deterministic and may be multi-modal.\n\nMore specifically, in our method, as we strictly follow the score function \\nabla_{a_t} \\log \\pi_t^*(a_t|s) of in-support optimal policy \\pi^* to generate high-value actions during the reverse diffusion process, any mode that has non-zero probability under \\pi^*(a|s) could be generated, which lead to a sub-optimal choice. Similar issues have been discussed in previous works [14, 13]. In Diffusion-DICE, it's natural to leverage the optimal critic Q^* derived from DICE in Eq.(3) to identify the best action. Given a specific state s, we first follow \\nabla_{a_t} \\log \\pi_t^*(a_t|s) in the reverse diffusion process to generate a few actions from \\pi^*(a|s). Then we evaluate these actions with Q^* and select the action with the highest Q^*(s, a) value as the policy's output.\n\nWe term this guide-then-select method Diffusion-DICE and present its pseudo-code in Algorithm 1. During the training stage, Diffusion-DICE estimates the optimal stationary distribution ratio using DICE with our piecewise f-divergence and calculates the guidance with IGL, both in an in-sample manner. During the testing stage, Diffusion-DICE selects the action from \\pi^*(a|s) with the highest value. By selecting from a small number of action candidates from \\pi^*(a|s), minimal error of the action evaluation model will be exploited."}, {"title": "4 Experiments", "content": "In this section, we present empirical evaluations of Diffusion-DICE. To validate Diffusion-DICE's ability to transform the behavior policy into the optimal policy while maintaining minimal error exploitation, we conduct experiments on two fronts. On one hand, we evaluate Diffusion-DICE on"}, {"title": "4.1 D4RL Benchmark Datasets:", "content": "We first evaluate Diffusion-DICE on the D4RL benchmark [8] and compare it with several related algorithms. For the evaluation tasks, we select MuJoCo locomotion tasks and AntMaze navigation tasks. While MuJoCo locomotion tasks are popular in offline RL, AntMaze navigation tasks are more challenging due to their stronger need for trajectory stitching. For baseline algorithms, we selected state-of-the-art methods not only from traditional methods that use Gaussian-policy (including DICE-based methods) but also from diffusion-based methods. Gaussian-policy-based baseline includes CQL [26], in-sample based methods IQL [24], SQL [55] and DICE-based method O-DICE [34]. Notably, O-DICE is a recently proposed DICE-based algorithm that stands out among various DICE-based offline RL methods. Diffusion-policy-based baseline includes Diffusion-QL [49] SfBC [4], QGPO [32] and IDQL [14]. We also compare Diffusion-DICE with its Gaussian-policy counterpart to show the benefit of using Diffusion-policy in Appendix D. While SfBC and IDQL simply sample from behavior policy candidates and select according to the action evaluation model, Diffusion-QL and QGPO will guide generated actions towards high-value ones. It is worth noting that Diffusion-QL will also resample from generated actions after the guidance.\n\nThe results show that Diffusion-DICE outperforms all other baseline algorithms, including previous SOTA diffusion-based methods, especially on MuJoCo medium, medium-replay datasets, and AntMaze datasets. The consistently better performance compared with Diffusion-QL and QGPO demonstrates the essentiality of in-sample guidance learning. Compared with SfBC and IDQL, Diffusion-DICE also shows superior performance, even with fewer action candidates. This is because the guide stage provides the in-support optimal policy for the select stage to sample from, which underscores the necessity of sampling carefully from an in-support optimal action distribution, rather than from the behavior distribution. We refer to the comparison of candidate numbers under different environments in Appendix D. Furthermore, the substantial performance gap between Diffusion-DICE and O-DICE reflects the multi-modality of DICE's optimal policy and firmly positions Diffusion-DICE as a superior policy extraction method for other DICE-based algorithms."}, {"title": "4.2 Further Experiments on Error Exploitation", "content": "We then continue to verify that the guide-then-select paradigm used in Diffusion-DICE indeed exploits minimal error. This is obvious for the guide stage because we only leverage in-sample actions for both critic training and guidance learning. For the select stage, additional evidence is"}, {"title": "5 Related Work", "content": "Offline RL To tackle the distributional shift problem, most model-free offline RL methods augment existing off-policy RL methods with a behavior regularization term. Behavior regularization can appear explicitly as divergence penalties [50, 25, 51, 9], implicitly through weighted behavior cloning [48, 38, 52], or more directly through careful parameterization of the policy [11, 58]. Another way to apply action-level regularization is via modification of value learning objective to incorporate some form of regularization, to encourage staying near the behavioral distribution and being pessimistic about OOD state-action pairs [26, 24, 55, 47]. There are also several works incorporating action-level regularization through the use of uncertainty [2] or distance function [29].\n\nAll these methods are based on the actor-critic framework and use unimodal Gaussian policy. However, several works indicate their limited ability to model multi-modal policy distribution [49, 14, 4], thereby leading to suboptimal performance. To remedy this, it's natural to employ powerful generative models to represent the policy. DT [5] uses transformer as the policy, Diffusion-QL [49], SfBC [4], QGPO [32] and IDQL [14] leverage diffusion models to represent policy. Other generative models like CVAE and consistency model have also been used as policy in offline RL [58, 54, 7]. Another line of methods utilizes diffusion models for trajectory-level planning by generating high-return trajectories and taking the corresponding action of the current state [17, 1]. While generative models are widely used in offline RL, few methods consider the existing errors in these models and whether the generation process exploits these errors."}, {"title": "DICE-based methods", "content": "The core of DICE-based methods revolves around the ratio of the stationary distribution between two policies. This ratio can serve as the estimation target in off-policy evaluation [36, 57] or as part of a state-action level constraint term in RL [37, 41, 34]. In offline IL [53], it can connect the optimal stationary distribution of a regularized MDP with the dataset distribution [22, 12, 19]. In constrained RL, this ratio can induce the discounted sum of cost without an additional function approximator [28]. Under the imperfect rewards setting, this ratio can reflect the gap between the given rewards and the underlying perfect rewards [30]. All of these DICE methods consider this ratio as a transformation from one stationary distribution to another. In this paper, however, we extend this ratio as a transformation from the behavior policy to the optimal policy and propose a novel way of doing so by using diffusion models, which also serve as a replacement for the Gaussian-based policy extraction in DICE."}, {"title": "6 Conclusion", "content": "In this work, we propose a new diffusion-based offline RL algorithm, Diffusion-DICE. Diffusion-DICE uses a guide-then-select paradigm to select the best in-support actions while achieving minimal error exploitation in the value function. Diffusion-DICE also serves as a replacement for the Gaussian-based policy extraction part in current DICE methods, successfully unleashing the power"}, {"title": "A A More Detailed Discussion of DICE and Diffusion Model in RL", "content": "Derivation of learning objectives in DICE DICE algorithms consider the following regularized RL problem as a convex programming problems with Bellman-flow constraints and apply Fenchel-Rockfeller duality or Lagrangian duality to solve it. The regularization term aims at imposing state-action level constraints.[35, 27, 34].\n\n\\max_{\\pi} \\mathbb{E}_{(s,a)\\sim d^{\\pi}} [r(s, a)] - \\alpha D_f(d^{\\pi}(s, a)||d_{\\mathcal{D}}(s,a)) \\tag{10}\n\nHere D_f(d^{\\pi}(s, a)||d_{\\mathcal{D}}(s,a)) is the f-divergence which is defined with D_f(P||Q) = \\mathbb{E}_{w \\sim Q} [f(\\frac{P}{Q})]. Directly solving \\pi^* is impossible because it's intractable to calculate d^{\\pi}(s, a). However, one can change the optimization variable from \\pi to d^{\\pi} because of the bijection existing between them. Then with the assistance of Bellman-flow constraints, we can obtain an optimization problem with respect to d:\n\n\\max_{d \\geq 0} \\mathbb{E}_{(s,a)\\sim d}[r(s, a)] - \\alpha D_f(d(s, a)||d^{\\mathcal{D}} (s, a))\n\ns.t. \\sum_{a \\in A} d(s, a) = (1 - \\gamma)d_0(s) + \\gamma \\sum_{(s',a')} d(s', a')p(s|s', a'),\\forall s \\in S \\tag{11}\n\nNote that the feasible region has to be \\{d : \\forall s \\in S,a \\in A,d(s, a) \\geq 0\\} because d should be non-negative to ensure a valid corresponding policy. After applying Lagrangian duality, we can get the following optimization target following [27]:\n\n\\min_{\\mathcal{V}(s)} \\max_{d \\geq 0} \\mathbb{E}_{(s,a)\\sim d}[r(s,a)] - \\alpha D_f(d(s, a)||d^{\\mathcal{D}}(s, a))\n\n+ \\mathcal{V}(s) \\bigg( (1 - \\gamma)d_0(s) + \\sum_{(s',a')} d(s', a')p(s|s', a') - \\sum_{a}d(s,a) \\bigg)\n\n\\min_{\\mathcal{V}(s)} \\max_{w \\geq 0} (1 - \\gamma) \\mathbb{E}_{d_0 (s)}[\\mathcal{V}(s)]\n\n+ \\mathbb{E}_{s,a \\sim d} w(s, a) \\bigg( r(s,a) + \\gamma \\sum_{s'} p(s'|s,a)\\mathcal{V}(s') - \\mathcal{V}(s) \\bigg) - \\alpha \\mathbb{E}_{s,a \\sim d^{\\mathcal{D}}} [f^*(w(s, a))] \\tag{12}\n\nHere we denote w(s, a) as \\frac{d(s,a)}{d^{\\mathcal{D}}(s,a)} for simplicity. By incorporating the non-negative constraint of d and again solving the constraint problem with Lagrangian duality, we can derive the optimal solution w^*(s, a) for the inner problem and thus reduce the bilevel optimization problem to the following optimization problem:\n\n\\min_{\\mathcal{V}(s)} (1 - \\gamma) \\mathbb{E}_{d_0(s)} [\\mathcal{V}(s)] + \\mathbb{E}_{s,a \\sim d^{\\mathcal{D}}} \\bigg[ \\alpha f^* \\bigg( \\frac{Q(s, a) + \\gamma \\sum_{s'} p(s'|s, a)\\mathcal{V}(s') - \\mathcal{V}(s)}{\\alpha} \\bigg) \\bigg] \\tag{13}\n\nHere f^* is a variant of f's convex conjugate. Note that in the offline RL setting, the inner summation \\sum_{s'} p(s'|s, a)\\mathcal{V}(s') is usually intractable because of limited data samples. To handle this issue and increase training stability, DICE methods usually use additional network Q(s, a) to fit r(s, a) + \\gamma \\sum_{s'} p(s'|s, a)\\mathcal{V}(s'), by optimizing the following MSE objective:\n\n\\min_{\\mathcal{Q}} \\mathbb{E}_{(s,a,s') \\sim d^{\\mathcal{D}}} [(r(s, a) + \\mathcal{V}(s') - Q(s,a))^2] \\tag{14}\n\nAnd because of doing so, the optimization objective in 13 can be replaced with:\n\n\\min_{\\mathcal{V}} \\mathbb{E}_{s \\sim d^{\\mathcal{D}}} [(1 - \\gamma)\\mathcal{V}(s)] + \\mathbb{E}_{(s,a) \\sim d^{\\mathcal{D}}} \\bigg[ \\alpha f^* \\bigg( \\frac{[Q(s, a) - \\mathcal{V}(s)]}{\\alpha} \\bigg) \\bigg] \\tag{15}\n\nAlso note that to increase the diversity of samples, one often extends the distribution of initial state d_0 to d^{\\mathcal{D}} by treating every state in a trajectory as initial state [22]."}, {"title": "B Additional Proofs", "content": "Optimal policy transformation with stationary distribution ratio: Given that \\pi^\\mathcal{D"}, "is the behavior policy, \\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} and \\pi^* are the optimal stationary distribution correction and its corresponding optimal policy. \\pi^* and \\pi^\\mathcal{D} satisfy the following proposition:\n\n\\pi^*(a|s) = \\frac{\\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\pi_{\\mathcal{D}}(a|s)}{\\int_{a \\in A} \\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\pi_{\\mathcal{D}}(a|s)da} \\tag{16}\n\nProof. Using the definition between d^{\\pi}(s) and d^{\\pi}(s, a), we have for any \\pi, d^{\\pi}(s, a) = d^{\\pi}(s) \\cdot \\pi(a|s). Then we have:\n\n\\frac{\\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\pi_{\\mathcal{D}}(a|s)}{\\int_{a \\in A} \\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\pi_{\\mathcal{D}}(a|s)da} = \\frac{\\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\frac{d^{\\mathcal{D}}(s,a)}{d^{\\mathcal{D}}(s)}}{\\int_{a \\in A} \\frac{d^{*}(s,a)}{d_{\\mathcal{D}}(s,a)} \\frac{d^{\\mathcal{D}}(s,a)}{d^{\\mathcal{D}}(s)}da} = \\frac{\\frac{d^{*}(s,a)}{d^{\\mathcal{D}}(s)} \\pi_{\\mathcal{D}}(a|s)}{\\int_{a \\in A} \\frac{d^{*}(s,a)}{d^{\\mathcal{D}}(s)}da} = \\frac{\\frac{d^{*}(s,a)}{d^{\\mathcal{D}}(s)} \\pi_{\\mathcal{D}}(a|s)}{\\frac{d^{*}(s)}{d^{\\mathcal{D}}(s)}} = \\pi^*(a|s) \\tag{17}\n\nVerification of Eq.(6): The score function of the optimal policy \\pi^*(a|s) in DICE and the behavior policy \\pi^\\mathcal{D}(a|s) satisfy: \\nabla_{a_t} \\log \\pi_t^*(a_t|s) = \\nabla_{a_t} \\log \\pi_t^{\\mathcal{D}}(a_t|s) + \\nabla_{a_t} \\log \\mathbb{E}_{a_0 \\sim \\pi_{\\mathcal{D}}(a_0|a_t,s)} [\\frac{d^{*}(s,a_"]}