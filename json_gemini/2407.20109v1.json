{"title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning", "authors": ["Liyuan Mao", "Weinan Zhang", "Xianyuan Zhan", "Haoran Xu", "Amy Zhang"], "abstract": "One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those local-optimal modes. We thus generate a few candidate actions and carefully select from them to approach global-optimum. Different from all other diffusion-based offline RL methods, the guide-then-select paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoids that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.", "sections": [{"title": "1 Introduction", "content": "We study the problem of offline reinforcement learning (RL), where the goal is to learn effective policies solely from offline data, without any additional online interactions. Offline RL is quite useful for scenarios where arbitrary exploration with untrained policies is costly or dangerous, but sufficient prior data is available, such as robotics [18] or industrial control [56]. Most previous model-free offline RL methods add a pessimism term to off-policy RL algorithms [50, 9, 2], this pessimism term acts as behavior regularization to avoid extrapolation errors caused by querying the Q-function about values of potential out-of-distribution (OOD) actions produced by the policy [25]. However, explicitly adding the regularization requires careful tuning of the regularization weight because otherwise, the policy will still output actions that are not seen in the dataset. DIstribution Correction Estimation (DICE) methods [35, 41, 34] provide an implicit way of doing so. By applying convex duality, DICE-based methods solve for the optimal stationary distribution ratio between the optimized and data collection policy in an in-sample manner, without ever querying the values of any unseen actions [55].\nIn this paper, we extend the analysis of DICE-based methods: we show that the optimal distribution ratio in DICE-based methods can be extended to a transformation from the behavior distribution to the optimal policy distribution. This different view motivates the use of deep generative models, e.g. diffusion models [42, 15, 43], to first fit the behavior distribution using their strong expressivity of fitting multi-modal distributions and then directly perform this transformation during sampling. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio learned by DICE. The first term can be easily obtained from the diffusion model trained on the offline dataset. While it is usually intractable to find a closed-form solution of the second term, we make a subtle mathematical transformation and show its equivalence to solving a convex optimization problem. In this manner, both of these terms can be learned by only dataset samples, providing accurate guidance towards in-distribution while high-value data points. Due to the multi-modality contained in the optimal policy distribution, the transformation may guide towards those local-optimal modes due to stochasticity. We thus generate a few candidate actions and use the value function to select the max from them to go towards the global optimum.\nWe term our method Diffusion-DICE, the guide-then-select procedure in Diffusion-DICE differs from all previous diffusion-based offline RL methods [49, 4, 14, 32], which are either only guide-based or only select-based. Guide-based methods [32] use predicted values of actions generated by the diffusion behavior policy to guide toward high-return actions. Select-based methods [4, 14] bypass the need for guidance but require sampling a large number of actions from the diffusion behavior policy and using the value function to select the optimal one. All these methods need to query the value function of actions sampled from the diffusion behavior policy, which may produce potential OOD actions and bring overestimation errors in the guiding or selecting process. Our method, however, brings minimal error in the guide-step by using accurate in-sample guidance to generate in-distribution actions with high values, and by doing so, in the select step only a few candidate actions are needed to find the optimal one, which further reduces error exploitation. We use an illustrative toy case to demonstrate the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that. We also verify the effectiveness of Diffusion-DICE in benchmark D4RL offline datasets [8]. Note that Diffusion-DICE also provides a replacement for the Gaussian policy extraction part used in current DICE methods, successfully unleashing the power of DICE-based methods. Diffusion-DICE surpasses both diffusion-based and DICE-based strong baselines, reaching SOTA performance in D4RL benchmarks [8]. We also conduct ablation experiments and validate the superiority of the guide-then-select learning procedure."}, {"title": "2 Preliminaries", "content": "We consider the RL problem presented as a Markov decision process [45], which is specified by a tuple $\\mathcal{M}(\\mathcal{S}, \\mathcal{A}, P, d_0, r, \\gamma)$. Here $\\mathcal{S}$ and $\\mathcal{A}$ are state and action space, $P(s'|s, a)$ and $d_0$ denote transition dynamics and initial state distribution, $r(s, a)$ and $\\gamma$ represent reward function and discount factor, respectively. The goal of RL is to find a policy $\\pi(a|s)$ which maximizes expected return $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)\\right]$. Another equivalent LP form of expected return is $\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s, a)]$, where $d^{\\pi}(s, a) := (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{P}r(s_t = s, a_t = a)$ is the normalized discounted stationary distribution of $\\pi$ [36]. For simplicity, we refer to $d^{\\pi}(s, a)$ as the stationary distribution of $\\pi$. Offline RL considers the setting where interaction with the environment is prohibited, and one need to learn the optimal $\\pi$ from a static dataset $\\mathcal{D} = \\{s_i, a_i, r_i, s'_i\\}_{i=1}^N$. We denote the empirical behavior policy of $\\mathcal{D}$ as $\\pi_\\mathcal{D}$."}, {"title": "2.1 Distribution Correction Estimation", "content": "DICE methods [41] incorporate the LP form of expected return $J(\\pi) = \\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s, a)]$ with a regularizer $D_f(d^{\\pi}||d^\\mathcal{D}) = \\mathbb{E}_{(s,a)\\sim d^{\\pi}}[f(\\frac{d^{\\pi}}{d^\\mathcal{D}}(s,a))] = \\mathbb{E}_{(s,a)\\sim d^{\\mathcal{D}}}[f(\\frac{d^{\\pi}}{d^\\mathcal{D}}(s,a))]$, where $D_f$ is the f-divergence induced by a convex function $f$ [3]. More specifically, DICE methods try to find an optimal policy $\\pi^*$ that satisfies:\n$\\pi^* = \\arg \\max_\\pi \\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s, a)] - \\alpha D_f(d^{\\pi}||d^\\mathcal{D}).$ (1)\nThis objective is generally intractable due to the dependency on $d^{\\pi}(s, a)$, especially under the offline setting. However, by imposing the Bellman-flow constraint [33] $\\sum_{a\\in \\mathcal{A}} d(s, a) = (1 - \\gamma)d_0(s) + \\sum_{(s', a')} d(s', a')p(s|s', a')$ on states and applying Lagrangian duality and convex conjugate, its dual problem has the following tractable form:\n$\\min_V (1 - \\gamma) \\mathbb{E}_{s\\sim d_0}[V(s)] + \\alpha \\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[f^*([(TV(s, a) - V(s))] /\\alpha)].$ (2)\nHere $f^*$ is a variant of $f$'s convex conjugate and $TV(s, a) = r(s, a)+\\gamma\\mathbb{E}_{s'\\sim p(\\cdot|s,a)} [V(s')]$ represents the Bellman operator on $V$. In practice, one often uses a prevalent semi-gradient technique in RL that estimates $TV(s, a)$ with $Q(s, a)$ and replaces the initial state distribution $d_0$ with dataset distribution $d_\\mathcal{D}$ to stabilize learning [41, 34]. In addition, because $\\mathcal{D}$ usually cannot cover all possible $s'$ for a specific $(s, a)$, DICE methods only use a single sample of the next state $s'$. The update of $Q(s, a)$ and $V(s)$ in DICE methods are as follows and we refer to a detailed derivation in Appendix A:\n$\\min_V \\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[(1 - \\gamma)V(s) + \\alpha f^* ([(Q(s, a) - V(s))] /\\alpha)]$\n$\\min_Q \\mathbb{E}_{(s,a,s')\\sim d^\\mathcal{D}} [(r(s, a) + V(s') - Q(s,a))^2].$ (3)\nNote that learning objectives of DICE-methods can be calculated solely with a $(s, a, s')$ sample from $\\mathcal{D}$, which is totally in-sample. One important property of DICE methods is that $Q^*$ and $V^*$ have a relationship with the optimal stationary distribution ratio $w^*(s, a)$ as\n$w^*(s, a) := \\frac{d^*(s, a)}{d_\\mathcal{D}(s, a)} = \\max (0, (f')^{-1}(Q^*(s, a) - V^*(s)))$, (4)\nwhere $d^*$ is the stationary distribution of $\\pi^*$. To get $\\pi^*$ from $w^*$, previous policy extraction methods in DICE methods include weighted behavior cloning [34], information projection [27] or policy gradient [37]. All these methods parametrize an unimodal Gaussian policy in order to compute $\\log \\pi(a|s)$ [13], which greatly limits its expressivity."}, {"title": "2.2 Diffusion Models in Offline Reinforcement Learning", "content": "Diffusion models [42, 15, 43] are generative models based on a Markovian noising and denoising process. Given a random variable $x_0$ and its corresponding probability distribution $q_0(x_0)$, the diffusion model defines a forward process that gradually adds Gaussian noise to the samples from $x_0$ to $x_T (T > 0)$. Kingma et al. [20] shows there exists a stochastic process that has the same transition distribution $q_{t0}(x_t|x_0)$ and Song et al. [43] shows that under some conditions, this process has an equivalent reverse process from $T$ to 0. The forward process and the equivalent reverse process can be characterized as follows, where $w_t$ is a standard Wiener process in the reverse time.\n$q_{t0}(x_t|x_0) = \\mathcal{N}(x_t | a_t x_0, \\sigma_t^2 I) dx_t = [f(t)x_t - g^2(t)\\nabla_{x_t} \\log q_t(x_t)]dt + g(t)dw_t, x_T \\sim q_T(x_T)$. (5)\nHere we slightly abuse the subscript $t$ to denote the diffusion timestep. $a_t, \\sigma_t$ are the noise schedule and $f(t), g(t)$ can be derived from $a_t, \\sigma_t$ [31]. For simplicity, we denote $q_{t0}(x_t|x_0)$ and $p_{0t}(x_0|x_t)$ as $q(x_t|x_0)$ and $p(x_0|x_t)$, respectively. To sample from $q_0(x_0)$ by following the reverse stochastic differential equation (SDE), the score function $\\nabla_{x_t} \\log q_t(x_t)$ is required. Typically, diffusion models use denoising score matching to train a neural network $\\epsilon_\\theta(x_t, t)$ that estimates the score function [46, 15, 43], by minimizing $\\mathbb{E}_{t\\sim U(0,T),x_0\\sim q_0(x_0), \\epsilon \\sim \\mathcal{N}(0,I)} [||\\epsilon_\\theta(x_t, t) - \\epsilon||^2]$, where $x_t = a_t x_0 + \\sigma_t \\epsilon$. As we mainly focus on diffusion policy in RL, this objective is usually impractical because $q_0(x_0)$ is expected to be the optimal policy $\\pi^*(a|s)$. A more detailed discussion is given in Appendix A.\nTo make diffusion models compatible with RL, there are generally two approaches: guide-based and select-based. Guide-based methods [32, 17] incorporate the behavior policy's score function with an additional guidance term. Specifically, they learn a time-dependent guidance term $I_t$ and use it to drift the generated actions towards high-value regions. The learning objective of $I_t$ can be generally formalized with $\\mathcal{L}(I_t(a_t), w(s, \\{a_t\\}_{t=1}^K))$, where $w(s, \\{a_t\\}_{t=1}^K)$ are critic-computed values on $K$ diffusion behavior samples. $\\mathcal{L}$ can be a contrastive objective [32] or mean-square-error objective [17]. After training, the augmented score function $\\nabla_{a_t} \\log \\pi_t(a_t|s) = \\sqrt{a_t} \\log \\pi^{\\mathcal{D}}_t(a_t|s) + \\sqrt{a_t} I_t(a_t)$ is used to characterize the learned policy.\nSelect-based methods [4, 14] utilize the observation that for some RL algorithms, the actor induced through critic learning manifests as a reweighted behavior policy. To sample from the optimized policy, these methods first sample multiple candidates $\\{a^k\\}_{k=1}^K$ from the diffusion behavior policy and then resample from them using critic-computed values $w(s, \\{a^k\\}_{k=1}^K)$. More precisely, the sampling procedure follows the categorical distribution $\\mathbb{P}r[a = a^k|s] = \\frac{w(s,a^k)}{\\sum_{k=1}^N w(s,a^k)}$.\nError exploitation As we can see, both guide-based and select-based methods need the information of $w(s, \\{a^k\\}_{k=1}^K)$ to get the optimal action. However, this term may bring two sources of errors. One is the diffusion model's approximation error in modeling complicated policy distribution, and the other is the critic's error in evaluating unseen actions. Although trained on offline data, the diffusion model may still generate OOD actions (especially with frequent sampling) and the learned critic can make erroneous predictions on these OOD actions, causing the evaluated value on these actions to be over-estimated due to the learning nature of value functions [11, 25]. As a result, the generation of high-quality actions in existing methods is greatly affected due to this error exploitation, which we will also show empirically in the next section."}, {"title": "3 Diffusion-DICE", "content": "In this section, we introduce our method, Diffusion-DICE. We start from an extension of DICE-based method that considers DICE as a transformation from the behavior distribution to the optimal policy distribution, which motivates us to use diffusion models to perform such transformation. We then propose a guide-then-select procedure to achieve the best action, i.e., we propose in-sample guidance learning for accurate policy transformation and use the critic to do optimal action selection to boost the performance. We also propose a piecewise f-divergence to stabilize the gradient during learning. We give an illustration of the guide-then-select paradigm and use a toycase to showcase the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that."}, {"title": "3.1 An Optimal Policy Transformation View of DICE", "content": "As mentioned before, DIstribution Correction Estimation (DICE) methods, an important line of work in offline RL and IL provides us with an elegant way to estimate the optimal stationary distribution ratio between $d^*(s, a))$ and $d^{\\mathcal{D}}(s, a)$ [28, 41, 34]. We show that this ratio also directly indicates a proportional relationship between the optimal in-support policy $\\pi^*(a|s)$ and the behavior policy $\\pi^{\\mathcal{D}}(a|s)$. This proportional relationship enables us to transform $\\pi^\\mathcal{D}$ into $\\pi^*$.\nOur key observation is that the definition of $d^{\\pi}(s, a)$ inherently reveals a bijection between $\\pi(a|s)$ and $d^{\\pi}(s, a)$. Given a relationship between $d^*(s, a)$ and $d^{\\mathcal{D}}(s, a)$, we can use this bijection to derive a relationship between $\\pi^*(a|s)$ and $\\pi^{\\mathcal{D}}(a|s)$. We formalize the bijection and derived relationship as:\n$\\pi(a|s) = \\frac{d^{\\pi}(s, a)}{\\int_{a \\in \\mathcal{A}} d^{\\pi}(s, a)da},\\qquad \\pi^*(a|s) = \\frac{\\frac{d^*(s,a)}{d^{\\mathcal{D}}(s,a)} \\pi^{\\mathcal{D}}(a|s)}{\\int_{a \\in \\mathcal{A}} \\frac{d^*(s,a)}{d^{\\mathcal{D}}(s,a)} \\pi^{\\mathcal{D}}(a|s)da}.$ (4)\nThe proof is given in Appendix B. Since the denominator in the second equation involves an integral over $a \\in \\mathcal{A}$ and is unrelated to the chosen action, the relationship indicates $\\pi^*(a|s) \\propto \\frac{d^*(s,a)}{d^{\\mathcal{D}}(a)} \\pi^{\\mathcal{D}}(a|s)$. This means that the transformation between $d^{\\mathcal{D}}(s, a)$ and $d^*(s, a)$ can be extended to the transformation between $\\pi^{\\mathcal{D}}(a|s)$ and $\\pi^*(a|s)$. This transformation motivates the use of deep generative models, e.g. diffusion models, to first fit the behavior distribution using their strong expressiveness and then directly perform this transformation during sampling. More specifically, the score function of the optimal policy and the behavior policy satisfy the following relationship:\n$\\nabla_{a_t} \\log \\pi^*_t (a_t|s) = \\nabla_{a_t} \\log \\pi^{\\mathcal{D}}_t (a_t|s) + \\nabla_{a_t} \\log \\mathbb{E}_{a_0 \\sim \\pi^{\\mathcal{D}}(a_0|a_t,s)} [\\frac{d^*(s, a_0)}{d^{\\mathcal{D}}(s, a_0)}]$ (6)\nThe proof is given in Appendix B. Intuitively, $\\nabla_{a_t} \\log \\pi^{\\mathcal{D}}(a_t|s)$ tells us how to generate actions from $\\pi^\\mathcal{D}$ and $\\nabla_{a_t} \\log \\mathbb{E}_{\\pi^{\\mathcal{D}}(a_0|a_t,s)} [\\frac{d^*(s, a_0)}{d^{\\mathcal{D}}(s, a_0)}]$ tells us how to transform from $\\pi^\\mathcal{D}$ to in-support $\\pi^*$ during the reverse diffusion process. As mentioned before, when representing $\\pi^*$ with a diffusion model, all we need is its score function $\\nabla_{a_t} \\log \\pi^*_t (a_t|s)$. Equivalently, we focus on the right-hand side of Eq.(6). The first term is just the score function of $\\pi^{\\mathcal{D}}$, which is fairly easy to obtain from the offline dataset [14, 4]. To perform the transformation, we still require the second term. Fortunately, DICE allows us to acquire the inner ratio term in an in-sample manner directly from the offline dataset, and we will show in the next section how to exactly compute the whole second term using offline dataset."}, {"title": "3.2 In-sample Guidance Learning for Accurate Policy Transformation", "content": "Although DICE provides us with the optimal stationary distribution ratio, which is the cornerstone of the transformation, the second term on the right-hand side of Eq.(6) is still intractable due to the conditional expectation. Our key observation is that for arbitrary non-negative function $f(x)$, the optimizer of the following convex problem is unique and takes the desired form of log-expectation.\nLemma 1. Given a random variable $X$ and its corresponding distribution $P(X)$, for any non- negative function $f(x)$, the following problem is convex and its optimizer is given by $y^* = \\log \\mathbb{E}_{X\\sim P(X)} [f(x)]$,\n$\\min_y \\mathbb{E}_{X\\sim P(X)} [f(x)\\cdot e^{-y} + y].$\nIt is evident that the optimizer can be used to derive the second term in Eq.(6). In fact, we show the second term can be obtained directly from the offline dataset, if we optimize the following objective. Here $g_\\theta$ is a guidance network parameterized by $\\theta$ and we denote $w^*(s, a_0)$ as $w(s, a_0)$ for shorthand.\nTheorem 1. $\\nabla_{a_t} \\log \\mathbb{E}_{\\pi^\\mathcal{D}(a_0|a_t,s)} [w^*(s, a_0)]$ can be obtained by solving the following optimization problem:\n$\\min_\\theta \\mathbb{E}_{t\\sim U(0,T)} \\mathbb{E}_{a \\sim \\pi^{\\mathcal{D}}(a|s)} \\mathbb{E}_{a_t \\sim p(a_t|a_0)} [w(s, a_0)e^{-g_\\theta(s,a_t,t)} + g_\\theta(s,a_t,t)],$ (7)\nas the optimal solution $\\theta^*$ satisfies $\\nabla_{a_t} g_{\\theta^*}(s, a_t,t) = \\nabla_{a_t} \\log \\mathbb{E}_{\\pi^\\mathcal{D}(a_0|a_t,s)} [w(s, a_0)]$.\nThis objective has two appealing properties. Firstly, compared to guide-based and select-based methods, we don't need to use multiple actions generated by the behavior diffusion model, we only need one sample from $\\pi^\\mathcal{D}(a|s)$ to estimate the second expectation, enabling an unbiased estimator of this objective using only offline dataset possible. Secondly, because $w(s, a_0)$ must be non-negative to ensure a valid transformation, this objective is convex with respect to $g_\\theta(s, a_t, t)$. This indicates a guarantee of convergence to $g_{\\theta^*}(s, a_t, t)$ under mild assumptions (see proof in Appendix B).\nDirectly inducing $\\nabla_{a_t} \\log \\mathbb{E}_{\\pi^\\mathcal{D}(a_0|a_t,s)}[w^*(s, a_0)]$ from the dataset is crucial in the offline setting. Firstly, it avoids the evaluation of OOD actions and solely depends on reliable value predictions of in-sample actions. Consequently, the gradient exploits minimal error in the critic, enabling it to more accurately transform $\\pi^\\mathcal{D}$ into in-support $\\pi^*$. Moreover, because all values of $w^*(s, a)$ used in our method are based on in-sample actions, applying this gradient in the reverse process will guide the samples towards in-sample actions with high $w^*(s, a)$ value.\nWe refer to this method as In-sample Guidance Learning (IGL). Note that previous methods either have a biased estimate of the gradient [17, 6], or have to rely on value predictions for diffusion generated actions, which could be OOD [32]. We refer to Appendix C for an in-depth discussion of IGL against other guidance methods.\nStablizing gradient using piecewise f-divergence. In practice, there exists one issue when computing the guidance term. Note that in Eq.(4), $w^*(s, a)$ could become zero, depends on the choice of $f$. Given a specific $a_t$, if $w^*(s, a_0) = 0$ for actions under $\\pi^\\mathcal{D}(a_0|a_t, s)$, the second gradient term will not be well-defined due to taking the logarithm of 0. In practice, this can result in an unstable gradient. To avoid such issue, $(f')^{-1}(x)$ should be positive for any given $x$. Also, to facilitate the optimization process, $f^*(x)$ should possess a closed-form solution and good numerical properties. Unfortunately, none of the commonly used f-divergence can accommodate both. However, considering the following two f-divergences:\n$\n\\begin{array}{cccc}\\text { Divergence } & f(x) & f^*(x) & (f')^{-1}(x) \\\\ \\text { Reverse KL } & x \\log x & e^{x-1} & e^{x-1} \\\\ \\text { Pearson } x^{2} & (x-1)^{2} & \\max (-1, \\frac{x}{4}) & \\max (0, \\frac{x}{4}+1)\\end{array}\n$\nIt is obvious that Reverse KL divergence possesses the positive property but exhibits numerical instability given a large $x$ due to the exponential function in $f^*(x)$, while Pearson $x^2$ divergence avoids the instability in the exponential function, its $(f')^{-1}(x)$ value is negative when $x < -2$.\nTo take advantage of both divergences while avoiding their drawbacks, we propose the following piecewise f-divergence that has properties similar to Pearson $x^2$ when $x$ is large while has properties similar to Reverse KL when $x$ is small:\n$f(x) = \\begin{cases}(x - 1)^2 & x \\geq 1, \\\\ x \\log x - x + 1 & 0 < x < 1.\\end{cases}$ (8)\nIn Appendix B, we prove that $f(x)$ is a well-defined f-divergence. We also prove that $f(x)$ has the following proposition:\nProposition 1. Given the piecewise f-divergence in Eq.(8), $(f')^{-1}(x)$ and $f^*(x)$ has the following formulation:\n$(f')^{-1}(x) = \\begin{cases}\\frac{x}{4}+1 & x \\geq 0 \\\\ e^{x} & x < 0\\end{cases} \\qquad f^*(x) = \\begin{cases}\\frac{x^2}{4}+x & x \\geq 0 \\\\ e^{x-1} & x < 0\\end{cases}$ (9)\nWe can see that given any $x$, the exponential function in $(f')^{-1}(x)$ ensures a strictly positive value. Meanwhile, $f^*(x)$ is at most a quadratic polynomial, which ensures numerical stability in practice. Note that $f(x)$ enables a stable policy transformation and can also be applied to other DICE-based methods when the optimal stationary distribution ratio needs to be positive."}, {"title": "3.3 Boost Performance with Optimal Action Selection", "content": "After transforming $\\pi^{\\mathcal{D}}$ into in-support $\\pi^*$, we now introduce the select-step to boost the performance during evaluation. One issue is the multi-modality in the optimal policy, this partially arises from the policy constraint used to regularize its output [9, 25, 55, 34]. Under policy constraint, the regularized optimal policy is unlikely to be deterministic and may be multi-modal.\nMore specifically, in our method, as we strictly follow the score function $\\nabla_{a_t} \\log \\pi^*_t (a_t|s)$ of in-support optimal policy $\\pi^*$ to generate high-value actions during the reverse diffusion process, any mode that has non-zero probability under $\\pi^*(a|s)$ could be generated, which lead to a sub-optimal choice. Similar issues have been discussed in previous works [14, 13]. In Diffusion-DICE, it's natural to leverage the optimal critic $Q^*$ derived from DICE in Eq.(3) to identify the best action. Given a specific state $s$, we first follow $\\nabla_{a_t} \\log \\pi^*_t (a_t|s)$ in the reverse diffusion process to generate a few actions from $\\pi^*(a|s)$. Then we evaluate these actions with $Q^*$ and select the action with the highest $Q^*(s, a)$ value as the policy's output.\nDifferent from previous guide-only or select-only methods, we base the select stage on the guide stage. As mentioned before, using IGL accurately guides the generated candidate actions towards in-support actions with high value. With the assistance of the guide stage, we can sample candidates from $\\pi^*(a|s)$, which has a high prob- ability of being high-quality. This means only a small number of candidates are required to be sampled, which reduces the probability of sam- pling OOD actions. This leads to minimal error exploitation while still attaining high returns. We give an illustration of the guide-then-select paradigm in Figure 1.\nWe term this guide-then-select method Diffusion-DICE and present its pseudo-code in Algorithm 1. During the training stage, Diffusion-DICE estimates the optimal stationary distribution ratio using DICE with our piecewise f-divergence and calculates the guidance with IGL, both in an in-sample manner. During the testing stage, Diffusion-DICE selects the action from $\\pi^*(a|s)$ with the highest value. By selecting from a small number of action candidates from $\\pi^*(a|s)$, minimal error of the action evaluation model will be exploited."}, {"title": "4 Experiments", "content": "In this section, we present empirical evaluations of Diffusion-DICE. To validate Diffusion-DICE's ability to transform the behavior policy into the optimal policy while maintaining minimal error exploitation, we conduct experiments on two fronts. On one hand, we evaluate Diffusion-DICE on the D4RL offline RL benchmark and compare it with other strong diffusion-based and DICE-based methods. On the other hand, we use alternative criteria to demonstrate that the guide-then-select paradigm results in minimal error exploitation. Experimental details are shown in Appendix D."}, {"title": "5 Related Work", "content": "Offline RL To tackle the distributional shift problem, most model-free offline RL methods augment existing off-policy RL methods with a behavior regularization term. Behavior regularization can appear explicitly as divergence penalties [50, 25, 51, 9], implicitly through weighted behavior cloning [48, 38, 52], or more directly through careful parameterization of the policy [11, 58]. Another way to apply action-level regularization is via modification of value learning objective to incorporate some form of regularization, to encourage staying near the behavioral distribution and being pessimistic about OOD state-action pairs [26, 24, 55, 47]. There are also several works incorporating action-level regularization through the use of uncertainty [2] or distance function [29].\nAll these methods are based on the actor-critic framework and use unimodal Gaussian policy. However, several works indicate their limited ability to model multi-modal policy distribution [49, 14, 4], thereby leading to suboptimal performance. To remedy this, it's natural to employ powerful generative models to represent the policy. DT [5] uses transformer as the policy, Diffusion-QL [49], SfBC [4], QGPO [32] and IDQL [14] leverage diffusion models to represent policy. Other generative models like CVAE and consistency model have also been used as policy in offline RL [58, 54, 7]. Another line of methods utilizes diffusion models for trajectory-level planning by generating high-return trajectories and taking the corresponding action of the current state [17, 1]. While generative models are widely used in offline RL, few methods consider the existing errors in these models and whether the generation process exploits these errors.\nDICE-based methods The core of DICE-based methods revolves around the ratio of the stationary distribution between two policies. This ratio can serve as the estimation target in off-policy evaluation [36, 57] or as part of a state-action level constraint term in RL [37, 41, 34]. In offline IL [53], it can connect the optimal stationary distribution of a regularized MDP with the dataset distribution [22, 12, 19]. In constrained RL, this ratio can induce the discounted sum of cost without an additional function approximator [28]. Under the imperfect rewards setting, this ratio can reflect the gap between the given rewards and the underlying perfect rewards [30]. All of these DICE methods consider this ratio as a transformation from one stationary distribution to another. In this paper, however, we extend this ratio as a transformation from the behavior policy to the optimal policy and propose a novel way of doing so by using diffusion models, which also serve as a replacement for the Gaussian-based policy extraction in DICE."}, {"title": "6 Conclusion", "content": "In this work, we propose a new diffusion-based offline RL algorithm, Diffusion-DICE. Diffusion-DICE uses a guide-then-select paradigm to select the best in-support actions while achieving minimal error exploitation in the value function. Diffusion-DICE also serves as a replacement for the Gaussian-based policy extraction part in current DICE methods, successfully unleashing the power of DICE-based methods. Through toycase illustration and extensive experiments, we show that Diffusion-DICE outperforms prior SOTA methods on a variety of datasets, especially those with multi-modal complex behavior distribution. One limitation of Diffusion-DICE is the sampling process of diffusion models is costly and slow"}]}