{"title": "Scenario-based Thermal Management Parametrization Through Deep Reinforcement Learning", "authors": ["Thomas Rudolf", "Philip Muhl", "S\u00f6ren Hohmann", "Lutz Eckstein"], "abstract": "The thermal system of battery electric vehicles demands advanced control. Its thermal management needs to effectively control active components across varying operating conditions. While robust control function parametrization is required, current methodologies show significant drawbacks. They consume considerable time, human effort, and extensive real-world testing. Consequently, there is a need for innovative and intelligent solutions that are capable of autonomously parametrizing embedded controllers. Addressing this issue, our paper introduces a learning-based tuning approach. We propose a methodology that benefits from automated scenario generation for increased robustness across vehicle usage scenarios. Our deep reinforcement learning agent processes the tuning task context and incorporates an image-based interpretation of embedded parameter sets. We demonstrate its applicability to a valve controller parametrization task and verify it in real-world vehicle testing. The results highlight the competitive performance to baseline methods. This novel approach contributes to the shift towards virtual development of thermal management functions, with promising potential of large-scale parameter tuning in the automotive industry.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern battery electric vehicles (BEVs) comprise sophisticated thermal systems (TSs). The TS plays a pivotal role in balancing the BEV design challenge with its opposing objectives of energy efficiency, tractive performance, and thermal comfort. The thermal management (TM) addresses the derived operational requirements and oversees the energetically coupled refrigerant and cooling circuits. Their operation strategy and control pose a large-scale parameter optimization problem in order to provide sufficient thermal power for each component's condition [1].\nTuning the feedback controllers involved in the TS demands considerable efforts. Current virtual parametrization procedures in the automotive industry include gradient-based or global optimization methods that are limited to few parameters and particular test cases [2]. To verify vehicle's hardware and software interactions, further improvements and tests are conducted at the real system. It is validated in iterations and in various climate domains around the world, see Fig. 1. However, this is not scalable and falls short in representing the final customer usage patterns. Thus, virtual development becomes imperative for sustainable verification and validation loops [1,3].\nA survey of expert TS calibration engineers, see appendix A, highlights two pressing requirements: a) enhanced automation in the parametrization task, and b) adaption to TM complication and changing TS configurations. We identify the need for an intelligent methodology to increase real-world scenario coverage during TM optimization. With the importance around recurring and adaptive creation of initial parameter sets for process efficiency, we investigate learning methods and thermal system scenario (TSS) generation.\nPrevious work outlines a scenario development method for highly automated driving [4]. The method has been transferred to TS verification challenges [3]. To minimize manual efforts, the relevance of data-driven approaches within automotive research and development has continuously increased [2, 5, 6]. However, due to cost-effectiveness and traceability in safety-critical applications, explicit embedded logic will likely endure [6, 7]. Therefore, we limit this paper's scope to intelligent parameter optimization methods for embedded functions instead of learning direct control.\nSolutions with fixed-structure control systems and its complications have been widely applied. Gain-scheduling PID control is used for nonlinear problems, and its control parameter dependencies account for exogenous influences on the system dynamics [6,8]. Further effort is required whenever subsystems are changed, or vehicle variants are introduced. Today's methods for control function optimization still struggle to handle the high dimensionality of the parameter space. The industry lacks sufficient approaches to deal with extensive search spaces while satisfying the flexibility and speed, demanded by the development cycles [2, 8]. Our paper focuses on solving these fixed-structure feedback-control parametrization tasks autonomously.\nIn [7]-[9], autonomously learning agents have been investigated to address aforementioned challenges. In these works, data-driven parameter tuning processes are formulated as sequential decision-making tasks that enables deep reinforcement learning (DRL) as suitable approach. Though, the training data generation mainly focused on existing measurements from historic system operation. Furthermore, only a small parameter subset per decision or operating region could be regarded due architectural inflexibility.\nFurther industrial applications have recently been explored, such as vehicle velocity tracking control [5], parameter tracking under nonlinear system dynamics [6, 10], or chip design [11,12]. In [11] and [12], the authors approach matrix-like circuit and chip design using efficient neural networks, often used in image processing. Similarly, embedded implementations involve matrix-like parameter look-ups.\nIn this work, we introduce a methodology that integrates the scenario method as technique for TS excitation and integrate it into a learning parametrization process for feedback-control. Our key contributions are as follows:\n1) we propose a novel image-based electronic control unit (ECU) parameter representation for efficient processing,\n2) we combine the TS scenario generation [3] with a DRL agent for control function parametrization, and\n3) we demonstrate the applicability to a TM valve controller and evaluate it in the real-world BEV.\nWe verify the resulting parameter sets on control performance metrics in three scenarios against baselines. Finally, we conclude with a discussion of our findings and potential future advancements."}, {"title": "II. BACKGROUND", "content": "Intelligent use of available heat has a positive impact on the primary energy demand of BEVs. Advanced TSs use valves to couple the different sub-circuits in relevant operation scenarios and leverage efficiency gains through optimal use of available heat [7]. The topological schematic of the TS in this study is depicted in Fig. 2. The continuous actuation of the mixing valve $u_{vlv}$ is proportional to its rotary piston angle $\\alpha$, and controls the downstream temperature $T_D$ by mixing two enthalpy streams with the upstream temperatures $T_{U_1}$ and $T_{U_2}$ toward a set-point temperature. The downstream temperature $T_D$ depends on both upstream temperatures, the valve angle $\\alpha$, and coolant flow rate in the system. An electric pump imposes a flow rate $V$ on the hydraulic system proportional to the hydraulic plant characteristics $k_{hyd}$ and the pump actuation $u_{pmp}$. Further, the electric fan speed $u_{fan}$ and radiator shutter angle $u_{shu}$ control the air volume flow $V_{air}$ over the heat exchanger proportional to the vehicles speed $u_{veh}$. The heat flux $Q_{ED}$ of electric drive tractive components is proportional to the power $P_{ED}$, vehicle speed $v_{veh}$, and the thermal and electrical operational states {$\\xi_{th}$, $\\xi_{el}$}. Since the system heat rejection $Q_{HE}$ is nonlinear w.r.t. the air volume flow $V_{air}$, the partial coolant flow rate $V$, the temperature difference $\\Delta T_{amb}$ between $T_{U_1}$ and the ambient temperature $T_{amb}$. The nonlinear TS dynamics can be described as a function of relevant actuator controls {$u_{pmp}, u_{vlv}, u_{fan}, u_{shu}$} and the potential variables, result in:\n\n$T_D = f(\\alpha, V, T_{U_1}, T_{U_2}, T_D),$ (1)\n\n$T_{U_1} = f(Q_{ED}, V, T_{U_1})$\n\n$= f(P_{ED}, u_{veh}, \\xi_{th}, \\xi_{el}, k_{hyd}, u_{pmp}, T_{U_1}),$ (2)\n\n$T_{U_2} = f(\\alpha, Q_{HE}, V, T_{U_2})$\n\n$= f(u_{vlv}, u_{fan}, u_{shu}, u_{veh}, \\Delta T, k_{hyd}, u_{pmp}, T_{U_2}).$ (3)\n\nBased on the operational conditions, $T_D$ is mixed via the ports configuration sets {1,2,3} or {1,3,4} and the temperature mixing function is either implemented in an upstream or downstream flow control. Similar dependencies for the level of heat rejection can be defined between the system free cut (blue) in Fig. 2 induced by connected subcircuits.\nThe parametrization of the described temperature controller and TM represents a difficult resources and time-intensive process [1]. State of the industry methods predominantly use physical prototypes for testing in all relevant ambient climatic zones [3], see Fig. 1. The described system is representative to various enthalpy controllers in BEVs with similar configurations and system dynamics, yet simply understandable in the discussed form."}, {"title": "B. Controller Parametrization Problem Formulation", "content": "The TS is a nonlinear parameter-varying dynamic system:\n\n$\\dot{x}(t) = f (x(t), u(t), \\theta(x, u, w)),$ (4)\n\nwith system states \u00e6, inputs u, and unknown variant parameters $\\theta$ depending on exogenous inputs w. The exogenous inputs w influence the dynamic states \u00e6 not directly but describe other vehicle's subsystem states. We regard the nonlinear system dynamics f as unknown.\nThe regarded TS features a nonlinear valve geometry and hose routing design that influence the dynamic system behavior. In addition to its nonlinearity, dead time from TS transport delay or actuators complicates the procedure of finding an optimal control law. Furthermore, changes to the TS configuration through different valve positions couple subcircuit dynamics and create coupling dynamics.\nBy understanding the significance of these valve positions with respect to the thermal flow management within the system, the criticality becomes evident. Each adjustment of the position has a direct influence on the overall system efficiency and responsiveness. The continuous positioning of the valve, see Fig. 2, results in a) a lower downstream temperature when opening the 1-2 connection with flow via the radiator while increasing the dead time due to an increased transport length. Vice versa, b) opening the 2-3 connection, thus simultaneously closing 1-2, creates a shortcut via $T_{U_1}$ and reduces the transportation via the radiator, inhibiting a shorter dead time, and cycles heat from the thermal load. Simultaneously, the time constants depend on the temperature difference between the two upstream fluid temperatures. Wax thermostats, the previous industry-standard solution, are not able to fulfil today's dynamic requirements as fixed-gain controllers once the vehicle is at operation temperature and do not allow for efficient heat rejection control.\nFixed-structured PID controllers often apply the gain-scheduling technique to handle nonlinear parameter-varying systems [6] in industrial embedded implementations. Therefore, we assume the gain-scheduling control law [8]:\n\n$u(t) = g(x(t), y(t), w(t), \\phi(x, y, w))$ (5)\n\nwith g as function of the system states x, the outputs y, and mapping functions \u03c6 as adaptive gains based on the operating conditions (x, y, w). We investigate a PI structure:\n\n$u(t) = \\varphi_p(e_T(t), \\Delta T_{amb}(t)) \\cdot e_T(t) + \\varphi_i(e_T(t), \\Delta T_{amb}(t)) \\int_0^t e_T(\\tau)d\\tau,$ (6)\n\nwith proportional $ \\varphi_p$ and integral $ \\varphi_i$ control gains as a function of the control error $e_T(t) = T_D(t) - T_{D,ref}(t)$ and the difference to ambient temperature $\\Delta T_{amb} = T_{U_1} - T_{amb}$ which is proportional to the potential heat dissipation via the heat exchanger. In embedded industrial controllers, the parameter mappings \u03c6 are often implemented as multidimensional lookup tables [7, 8, 10].\nThe challenging parametrization task can be formulated as an optimization problem to find the optimal parameter mapping \u03c6 that minimizes the control objective $J \\in \\mathbb{R}^+$:\n\n$\\phi^* = \\underset{\\Phi}{\\text{argmin}} J.$ (7)\n\nThough, finding a parameter set that is optimal for all individual usages is infeasible. Instead, the expectation is to find a compromise over a-priori unknown usage that is robust against fatal performance in edge cases. Referring to the actual parametrization process, we regard the controller parametrization task as a sequential procedure [7, 8]. The parameters are iteratively adapted after each assessment of the current parameter set's closed-loop performance J."}, {"title": "C. Scenario-based Virtual Development", "content": "The methodology of advanced scenario-based development, testing and verification was first developed for the complex challenges in assisted and autonomous driving systems (ADAS/AD) in the PEGASUS research project [13]. It provides a standardized and regulatory approved virtual development, verification & validation method to grasp possible driving situations to address the non-feasible billions of necessary testing mileage [14]. The core method projects any driving situation onto six standardized layers. Each layer represents a distinguished set of information relevant to describe a driving situation to its full extent. Assuming complete layer information, the recombination of individual layer's data could create any possible driving situation [4]. This approach is useful for virtual development as well as verification & validation purposes through standardized scenario description and simulation, e.g., via OpenDRIVE and OpenSCENARIO formats [4]. The methodology is recently adapted for TS development by enriching the layers with additional domain specific information, as a comprehensive approach to formulate TSSs for novel tools in the TM development and verification [3]."}, {"title": "D. Reinforcement Learning for Parametrization", "content": "Intelligent control through closed-loop feedback interaction increasingly grows in relevance. Nonlinear system dynamics, high degree of freedom, or black-box problems are favorable domains for reinforcement learning (RL) applications. Autonomously learning agents have also been applied to white-box nonlinear system identification [10] and controller tuning [6]-[8].\nThe formulated TM controller parametrization task can be regarded as a sequential decision-making problem, see [7,8,15]. As depicted in Fig. 3, static configurations, stationary information and measurable states are observed (0) from the TS in its application domain. The RL environment E comprises the closed-loop controlled TS which is in interaction with the RL agent via control performance for different ECU parameter sets \u03c6. Subsequently, an action a to the control function parameters \u03c6 is inferred to tune the ECU control function. Based on the performance J, e.g., linear-quadratic regulator (LQR) objective, the rewarding feedback r is calculated [6]. Dense rewards facilitate a steady improvement of the policy \u03c0, optimizing the expected sum of discounted rewards with discounting factor \u03b3\u2208 [0, 1] [15]:\n\n$\\underset{\\pi}{\\text{argmax}}E { \\sum_{n=1}^{\\infty} \\gamma^n r_n }.$ (8)"}, {"title": "III. OUR SCENARIO-DRIVEN RL APPROACH", "content": "In order to create versatile training data for the RL-based parametrization procedure, we use the adapted scenario methodology introduced by [3] to create responsive TSSs, depicted in Fig.4 (left). We collect driving data from real world test drives and customer fleet data as statistical distributions to create a representative vehicle usage scenario database. Additional expert engineer knowledge and public databases enrich each layer with TS information, i.e., charging infrastructure, solar irradiation, and humidity. We then perform the data allocation to the layer scheme introduced by the adapted scenario methodology. Through recombination of available information from the scenario database layers in combination with randomized driving routes, we create multiple individual scenarios representing customer-like usage. A longitudinal dynamics vehicle model calculates the vehicle response trajectory to each scenario, which is used to parameterize and evaluate controller performance. Furthermore, we explicitly use real world development drive data for the different climatic zones, e.g., measurement drives from Dubai (hot), Sweden (cold), or Spain (moderate), see Fig. 1. The driving data were independently collected over the period of six months with different development vehicles."}, {"title": "A. Methodology for Scenario Generation", "content": "In the following, we present the procedure of the TS-adapted scenario methodology to tackle the challenge of understanding and modeling customer vehicle usage for the exemplary valve controller parametrization problem.\nThe set M describes vehicle usage data and comprises available customer fleet data and selected development drive data sets. We derive layer-focused statistics from M to describe the usage behavior with respect to TSS influencing factors, i.e., mileage per ambient temperature. From this we create a subset S with customer-focused metrics, as detailed in [3]. A Gaussian-normal distribution fit $N (\\mu_\\varsigma,\\sigma_\\varsigma)$ for each scenario layer random variable of S yields thermal system operation distributions to describe vehicle and TS behavior. To limit the solution space of possible scenarios, we disregard values below the 10% and above the 90% behavioral distribution quantiles. We ensure a robust and safe operation by adding TS specialized edge cases, e.g., high loads at extreme temperatures from sub-polar and desert conditions, on top of the broad distributions. By optimizing the system response for the resulting distributions, development resources are well allocated to improve customer value according to the Pareto-principle.\nFor the studied valve controller, layers 1, 2, and 5 from [3] are of particular interest since those hold the primary excitation influences for the control loop. Layer 1 information respects the road profile {[x, y, z], road friction} from map data about the latitude/longitude/altitude position and influences the velocity trajectory. With respect to vehicular limitations, driver skill and regulatory speed restrictions (see Layer 2) the achievable velocity trajectory primarily effects the heat losses from the tractive system. Further, layer 5 holds the temperature and humidity ambient conditions, thus affecting heat rejection significantly.\nEach draw from the scenario database builds a concrete drive reference with TS-related enriched description, denoted as TSS for subsequent simulation. The TSSs act as system excitations for the overall vehicle with TS response based on the parametrizated TM in particular. We use a longitudinal dynamics vehicle model with integrated human driver behavior and thermal system model to provide a simulation result as reference for a virtual test of the TM in closed-loop, see Fig. 4 (left). We then simulate batches of TSSs that result in load, disturbance, and reference TS trajectories for different subsystems. Significant for the simulation of the virtual TS is the modeling of thermodynamic and hydraulic phenomena. For that we use existing development system models and the embedded ECU software function implementations with the valve controller as the respected tunable part. In simulated operation, the TM changes the cooling circuit configurations for highest efficiency based on the TS states \u03be. This significantly affects the TS dynamics and switches between control parameter sets, as described in section II. The TS trajectories are inputs to the RL agent and used to calculate the control performance objective. We limit the scope by disregarding the feedback from the TS to the scenario level. For a given scenario and control parameter set \u03c6 we simulate a TS dynamic behavior response, see Fig. 4 (center), interfacing the subsequent RL agent in the parameter adaption loop."}, {"title": "B. Contextual DRL for Embedded Controller Tuning", "content": "The following describes our novel contextual RL-based controller parametrization agent with its architecture depicted on the right half of Fig. 4. We describe the TSS-driven training algorithm to solve for a contextual parametrization problem based on the obtained TS information.\nFor each decision step about adaptions to the parameters, the agent gathers information about relevant ECU signals and the current parametrization. The TS carries stationary information that describe a context in which the parametrization task needs to be solved, i.e., high-level operational TM states \u03be based on the current TSS correlate with significant diverting system dynamics and subsystem control references. Accordingly, we regard a context vector c = [$\\xi_{th}$, $Y_{ref}$] with \u03beth as thermal operational state describing the desired TS sub-circuit configuration and therefore possible valve control piston positions and $Y_{ref} = T_{D,ref}$ as the downstream temperature reference, desired by the high-level TM. Additionally, we observe measured signals s(n) as time-discrete sequences within a windowed interval, with samples n \u2208 [0, N]. For the valve controller, we regard s(n) = [$T_D(n)$, $e_T(n)$, $\\Delta T_{amb}(n)$, $u_{vlv}(n)$]. Last, we need to observe the current tunable parameters \u03c6 in (6).\nWe reinterpret the embedded implementation of varying control gains in the form of look-up tables as $M_i \\times M_j$-sized image-like matrices. Given the operating states {\u03beth, \u03beel}, one of two controllers with a PI parameter set is dynamically active and regarded, for a total of 100 parameter values. The matrices \u03c6 are dependently evaluated on the current ECU signals, namely $s_i = e_T$ and $s_j = T_{amb}$ over their i, j-axes with each matrix of size M = $M_i = N_i = 5$. Consequently, we retrieve C = 2 channels of stacked parameter values, comparable to colour-based channels of an images. For generalized applicability of the DRL architecture, we upsample and interpolate the matrices to \u03c6 of common quadratic size M = 8. We derive a similar approach for look-ups curves (1D) or cubes (3D) used in the industry. Finally, we consider the joint DRL agent observation o = [c, \u03c6, s(n)].\nBased on the observation o, we formulate an LQR-inspired objective as a dense reward function:\n\n$r_j = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(b_1 \\sqrt{\\left|e_T(n)\\right|} - e_T^2(n) \\right) + b_2 \\cdot u_{vlv}(n),$ (9)\n\nwith design choices $b_1$ = 25, and $b_2$ = 0.1 in this work. We quadratically penalize large errors and the required cooling power through wide open valve positions but also stimulate the sensitivity at small control errors with the root term. Unstable controller parameters result in a extreme negative reward that may lead to divergence during training. Therefore, we clip the negative rewards to a lower limit and clip the positive controller parameters to an upper limit.\nThe DRL architecture in Fig. 4 (right) processes the observation o and derives the action a \u2208 A to tune the parameters \u03c6 in expectation of subsequent rewards. Neural networks, hence DRL, are predominantly applied to sophisticated RL problems to approximate a decision strategy [15, 16]. In parallel, a) the task context is inferred to a latent context vector $z_c$ via a multilayer perceptron (MLP) with nonlinear activations and dropout layers, b) the signals are processed via recurrent long short-term memory (LSTM) layers with the final output as latent signal vector $z_s$ [7,8]. Previous work did not regard the state-spatial dependency of neighboring operating points for parameter-varying systems and adaptive controllers in the observation [6]. In contrast, we use a convolutional neural network (CNN) feature extractor to encode the channeled parameter matrices into $z_\\varphi$. Subsequently, we derive a latent vector \u03da = [$z_c, z_s, z_\\varphi$] by concatenation of the individual latent representations. Based on \u03da, the CNN decoder infers the latent vector. Alternating convolution blocks and upscaling layers generate the target matrix structure \u03c6. With \u03da, the parallel critic MLP estimates the state-action value Q for the RL training algorithm [16].\nHowever, applying the full tuning matrix \u0394\u03c6 to the parameter tables would create an infeasible sequence of decisions. Depending on the TS state trajectories of an individual scenario, only subsets of the parameters actively contribute to the closed-loop performance, depending on the parameter table axes $s_{i/j}$. Action masking contributes to a faster convergence during training, enabling real-world applications of otherwise sample inefficient RL algorithms [17]. Therefore, we introduce binary parameter action masks m \u2208 {0,1}$^{M_i \\times M_j}$. During dynamic mask construction, a 1 is registered for at least one time sample in which the TS operates in the vicinal table area. In contrast to the observation, we downsample and interpolate from M to the original parameter matrices' shapes. We derive the new controller parameter set for step k via the function \u03c6, applying a Hadamard product with the masks to the downsampled (\u2193) set of parameter changes:\n\n$\\Phi_k = \\varphi (\\Phi_{k-1}, a, m) = \\Phi_{k-1}+ \\downarrow_{M_i,M_j} (\\Delta \\phi_k)\\odot m.$ (10)"}, {"title": "C. Training of the DRL Parametrization Agent", "content": "In this work, we deploy the soft actor-critic algorithm derivative dropout Q-function (DroQ) for increased sample-efficiency through multiple Q-value updates per step on parallel Q-networks [15, 16]. The encoder-decoder actor $\u03c0_\u03c8$ and critic $Q_\u03c9$ networks are parametrized by \u03c8 and \u03c9, and trained through backpropagation of the policy and Q-objective gradients based on the reward r [16]. Algorithm 1 describes the training routine for our scenario-based DRL controller parametrization task. It requires a TSS subset S and a TS system dataset $D_\u0398$ for initialization and simulation (lines 2,4). For each episode of length K, we reset the TS model and a new scenario $x_\\varsigma$ is sampled via the TSS method. We chose conservative stable parameter sets to initialize each episode. In each episode step, we sample a new scenario $x_\\varsigma$ via the TSS method (lines 5,11) and simulate it with the virtual closed-loop controlled TS (EvalScenario) which derives the observation and mask (lines 6,12). The DRL actor policy derives the parameter adaption (lines 9, 10) and a reward is calculated (CalcReward) w.r.t. (9), and the gathered experience stored in the buffer R (line 14).\nAfter the successful episodic training from off-policy updates of our DRL architecture (lines 15, 16), see Fig. 5, the converged policy $\u03c0_\u03c8$ can be independently inferred with in-distribution observations from real-world systems."}, {"title": "IV. REAL-WORLD EXPERIMENT AND RESULTS", "content": "To verify our methodology, we apply our DRL architecture to a TS learning environment, implemented in the Gymnasium RL standard. Referring to section II, the simulation includes our previously discussed cooling circuit. It comprises the PI valve controller and its adjustable parameter tables from section III. We use 32 parallel environments which interface the agent with a TS simulation model, using C++ code generated by MATLAB\u00ae SIMULINK\u00ae.\nWe build our agent on top of the PyTorch-based framework stable-baselines3 [18], using the proposed agent from Fig. 4.We extend a DroQ agent to automatically construct its neural network architecture and to compute the parameter action mask during runtime. On the masked policy outputs, the soft action distribution parameters are set to \u03bc\u03b1 = 0 and highly negative log \u03c3\u03b1. The hyperparameters are given in TABLE II, applied beyond the baseline's default values [18].\nIn Fig. 5, we show the training average reward curve and compare it to the highest reward obtained for a single TSS. We aim to generate a robust parameter set across the scenario distribution, rather than overfitting a parametrization to a single scenario that produces the maximum reward. After about 60000 training steps at K = 125 steps per episode, the episode mean rewards stabilize. For the real-world verification, we consider the parameter set from the episode with the highest accumulated reward after convergence and transfer it to the ECU.\nWe conduct the verification of our proposed methodology using a real BEV sports car. Our real-world tests take place at the Nard\u00f2 Technical Center, Italy. A location not present in the training dataset but comparable to the climatic conditions represented in a sub-dataset from Spain. To ensure reproducibility and comparability, each test is executed with the same vehicle and driver. The initial temperatures are in steady-state and reflect the quasi-constant ambient environment. We compare four different parameter sets:\nbaseline: initial parametrization by supplier optimization with functional experience from earlier vehicles,\nexpert: manual tuning by expert calibration engineer from previous testing under realistic time constraints,\nours: virtual parametrization derived by our approach,\nours & expert: our parameter set, manually fine-tuned by a calibration engineer on-site.\nThe experiments are real car scenarios:\n1) oscillating acc-/deceleration between 80 kmh-1\nand 180 kmh-1 into constant velocity of 70 kmh-1,\n2) see scenario 1 but into constant velocity 110kmh-1,\n3) sportive Nard\u00f2 GP track drive, see Fig. 6.\nThe initially oscillating scenarios 1 and 2 evaluate the stabilizing control behavior after intensive thermal excitation into a steady-state heat exchange phase. The oscillating accelerations up to 180 kmh-1 build up a thermal load. The subsequent constant lower velocity stresses the TS and TM with significantly reduced heat dissipation via the front heat exchanger. The delayed fluid transport and decaying heat influx from the BEV electric drive challenges the valve controller response. In contrast, track driving evaluates the dynamic control response to changing TS demands. We prepend an initial lap to condition the thermal system and familiarize the driver with the track conditions, followed by a flying start into the evaluated lap.\nWe evaluate the recorded ECU measurements on the metrics: mean absolute error (MAE) and root mean squared error (RMSE) of the control error eT, the mean square valve actuator energy based on the valve rotation \u00fc, and the mean total variation (MTV) of the controlled downstream temperature y = TD to evaluate the stabilizing behavior. All metrics are shown in TABLE I with the lowest (best) metric score per test scenario in bold. When our scenario-based DRL agent is involved (ours, ours & expert), the resulting parameter sets yield superior control error metrics (MAE, RMSE) in all scenarios against the baseline. Compared to the expert's parameter set, our approach provides better results in the steady-state scenarios. We consider the wide vehicle usage distributions in our scenario-driven agent as a reason for the slower control response on the track in scenario 3. However, due to the time advantage via automation with our approach, an a-posteriori fine tuning is possible and sufficient. In collaboration, ours & expert achieves the best overall result on the GP track (scenario 3, MAE and RMSE).\nFig. 6 shows the controller signals during the demanding high-performance scenario 3. The velocity profile v (green) and resulting coolant temperature rise from the tractive system heat flux $\u0394T_{TS} = T_{U_1} - T_D$ are shown in Fig. 6. The temperature difference $\u0394T_{TS}$ changes rapidly between maximum power/recuperation phases while showing a phase shift against the velocity profile. The control error $e_T$ shows a slow return to the setpoint temperature (dashed) for all parametrizations, except for the collaborative parameter set (ours & expert, lilac). This indicates a too slow parameter set for the rapid disturbances to control. A more aggressive parameter set is necessary to react quickly by opening the valve $u_{vlv}$. However, an overshoot is noticed at 160s. Compared to the expert parameter set (red), the ours & expert combination (lilac) sufficiently reacts to disturbances by dynamically exploiting the full control value range.\nIn summary, our approach enables a trade-off between performance and robustness across test scenario dynamics. The results demonstrate the applicability of our approach for real-world parametrization tasks for nonlinear embedded TM. The scenario-based DRL approach creates parameter sets that can outperform expert parametrizations. Nevertheless, the further incorporation of expert knowledge in our agent's parameter sets enhances the control behavior beyond today's levels without our approach. Real-world testing will remain for validation purposes and presents such opportunity for fine-tuning. Conclusively, we achieve a virtual and automated process that can reduce development time."}, {"title": "V. CONCLUSION", "content": "This work presents a novel approach to the parametrization of embedded TM valve controllers. Leveraging scenario-driven virtual development and image-based parameter representations, our proposed DRL agent obtains competitive parameter sets in a fully automated manner. We demonstrate the validity of learning virtual development processes for the industry applications. The results show effectiveness to current industry baselines in real-world testing scenarios. Due to the similarity of enthalpy controllers and system dynamic requirements in BEVs, the discussed principles can be easily transferred to different controllers within the same vehicle and to different vehicles as well.\nOur approach enables streamlined allocation of costly physical prototype resources. Future work could investigate methodological feedback to the scenario generation within learning curricula. In addition, the cooperation between autonomous agents and human expert engineers holds great potential for future innovation through distillation of their respective strengths and experience. Conclusively, the paper highlights the potential for transfers to new application domains and parametrization tasks beyond controller tuning."}]}