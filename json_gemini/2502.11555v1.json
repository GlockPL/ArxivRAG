{"title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models", "authors": ["Yingshui Tan", "Yilei Jiang", "Yanshi Li", "Jiaheng Liu", "Xingyuan Bu", "Wenbo Su", "Xiangyu Yue", "Xiaoyong Zhu", "Bo Zheng"], "abstract": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an \"overly safe\" state rather than a \"truly safe\" state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness. Our code, prompt, dataset will be made public available at https://anonymous. 4open.science/r/E-RLHF-DB6D/.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning large language models (LLMs) based on human preferences, commonly referred to as model alignment, has significantly improved their text generation capabilities (Ouyang et al., 2022; Askell et al., 2021; OpenAI, 2023). These models, when provided with well-structured instructions, can generate valuable responses for a variety of tasks, including answering scientific questions (Singhal et al., 2023), creative writing (Yuan et al., 2022), coding (Chen et al., 2021b; Guo et al., 2024), and planning (Wang et al., 2023a; Valmeekam et al., 2023). Despite their enhanced instruction-following capabilities, these models can also produce harmful content when prompted, such as sexist or racist remarks, guidance on criminal activities, or sensitive medical advice (Bender et al., 2021; Weidinger et al., 2021; Zou et al., 2023). Ensuring that LLMs remain both helpful and safe is therefore an essential objective (Ouyang et al., 2022; Askell et al., 2021; Bai et al., 2022).\nBalancing the safety and helpfulness of LLMs presents a significant challenge due to the inherent tension between these objectives (Bai et al., 2022; Touvron et al., 2023; Qi et al., 2023; Jiang et al.,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Safety Alignment of LLMs", "content": "Ensuring the safety and ethical alignment of large language models (LLMs) requires a balance between human-guided methodologies and innovative automated approaches. Traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) rely heavily on human involvement, utilizing curated datasets, red teaming, and reward modeling to align model behavior with ethical standards (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; Dai et al., 2024; Ji et al., 2024b). While effective, these approaches face limitations in scalability and efficiency due to their dependence on extensive human annotation and oversight. To address these challenges, recent advancements have explored automated techniques, such as rule-based rewards (Mu et al., 2024) and generation-aware alignment (Huang et al., 2024), which reduce reliance on human intervention. Building on these efforts, a fully automated framework offers a promising alternative by eliminating the need for human-curated safety data (Mu et al., 2024; Huang et al., 2024), enabling adaptive alignment across diverse generative scenarios. This automation not only complements existing methodologies but also pushes the boundaries of scalable and context-aware safety alignment, paving the way for LLMs that are both robust and ethically reliable across a wide range of applications (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; Dai et al., 2024; Ji et al., 2024b)."}, {"title": "2.2 Balance Between Helpfulness and Harmlessness of LLMs", "content": "Several studies focus on finding the right balance between LLM helpfulness and safety. Some work (Ji et al., 2024a) suggests improving responses during reasoning by using an extra model called a residual aligner. Safe-RLHF (Dai et al., 2023) achieves preference alignment under safety constraints, and subsequent works (Zhang et al., 2024) have been proposed to further improve the reward objective. Other research has explored improving safety through psychological techniques (Heston, 2023; Wu et al., 2024) and red teaming methods (Ge et al., 2023; Perez et al., 2022; Ganguli et al., 2022). However, all of the previous works overlook the role of safety data in safety alignment. Our work aims to bridges this gap and balance the safety-helpfulness trade-off by analyzing and curating the safety dataset."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Notation and Terminology", "content": "Let x denotes an input prompt and y its corresponding response. For any two responses y and y' generated from a prompt x, a binary preference label I(y > y'|x) is assigned by a human annotator, indicating whether y is preferred over y'. The preferred response is referred to as the \"win response,\" denoted as yw, while the other is termed the \"lose response,\" denoted as y\u00b9. A dataset D = {(x, y, y', I(y > y'|x))}, comprising prompts, multiple responses, and the human preferences over these responses, is called a preference dataset."}, {"title": "3.2 Reinforcement Learning with Human Feedback (RLHF)", "content": "RLHF typically involves two phases (Stiennon et al., 2020; Zheng et al., 2023): supervised reward learning and policy optimization via reinforcement learning (RL). The reward model $r_\\phi$, parameterized by \u03c6, is trained using the Bradley-Terry (BT) model (Bradley and Terry, 1952), which employs logistic loss to maximize the difference in reward scores between the win and lose responses:\n$L_r(\\phi) = -E_{(x,y^w,y^l)\\sim D} [log \\sigma(r_\\phi(x, y^w) \u2013 r_\\phi(x,y^l))]$ (1)\nwhere \u03c3 is the sigmoid function, and D is the preference dataset.\nThe trained reward model $r_\\phi$ provides reward scores for the RL phase. In this phase, the language model $\u03c0_\\theta$ (the policy) is optimized to maximize the KL-regularized reward (Schulman et al., 2017a):\n$\\max_\\theta E_{x\\sim D,y\\sim \u03c0_\\theta(y|x)} [r_\\phi(x, y) \u2013 \u03c4KL [\u03c0_\\theta(y|x)||\u03c0_{ref}(y|x)]]$ (2)\nwhere \u03c4 is a penalty coefficient for the KL divergence term, which constrains the policy $\u03c0_\\theta$ from deviating significantly from a reference policy $\u03c0_{ref}$\u00b7 In practice, reward learning and policy training are often performed iteratively, with $\u03c0_{ref}$ initialized as the starting model in each round of RL."}, {"title": "3.3 Direct Preference Optimization (DPO)", "content": "DPO (Rafailov et al., 2023) introduces an approach to re-parameterize the reward r in terms of the policy \u03c0, allowing the policy to be optimized directly via supervised learning:\n$\\min_\\theta -E_{(x,y^w,y^l)\\sim D} [log \\sigma (\\beta(\\pi_\\theta(y^w|x) - \\pi_{ref}(y^w|x) - \\pi_\\theta(y^l|x) + \\pi_{ref}(y^l|x)))]$\n(3)\nNotably, the data points (x, yw, yl) in this objective do not need to be generated by $\u03c0_\\theta$ during updates; instead, they can be sampled from a public preference dataset D."}, {"title": "4 Methodology", "content": "In this section, we investigate the connection between harmlessness and helpfulness during LLM alignment. And propose an Equilibrate RLHF framework including two algorithms to achieve a win-win situation."}, {"title": "4.1 Fine-grained Data-centric Approach", "content": "The primary objective of alignment is to equip models with a deep understanding of safety principles, enabling them to generate responses that adhere to these requirements. Ideally, this alignment should be both precise and generalizable. In this paper, we define this ideal state as the \"truly safe\" state. To achieve it, extensive efforts have been made to construct and curate large-scale, high-quality safety datasets. However, many existing models instead fall into an \"over-safe\" or \"over-aligned\" state, where they excessively refuse to respond even to queries that pose no inherent harm.\nAs illustrated in Figure 4, extensive experimental studies reveal that simply increasing the quantity of high-quality, diverse safety data does not consistently enhance a model's safety performance. Instead, it can introduce fluctuations in the model's ability to mitigate risks. Moreover, as the volume of safety data increases, the model's general capabilities tend to degrade.\nTo better understand these challenges, we conducted an in-depth analysis of LLM safety. As shown in Figure 2, two primary factors contribute to the generation of unsafe responses: (1) an insufficient reserve and understanding of safety knowledge, and (2) an inability to produce safe responses to harmful prompts. In real-world applications, risks can arise from either or both of these factors. While safety alignment is often expected to fully resolve these issues, its primary role is to guide the model in generating appropriate responses to harmful prompts, rather than expanding its underlying safety knowledge. For a more comprehensive analysis, we categorize LLM prompts into three distinct groups, with specific examples provided in the supplementary materials.\n\u2022 Explicit Harmful Data (EHD), or factual risk data, contains explicit harmful information without malicious intent, such as racial slurs; child exploitation; prohibited politically sensitive words. We propose that a model's performance on such risk data is significantly influenced by its inherent knowledge base, making it challenging to achieve optimal safety outcomes solely through alignment.\n\u2022 Implicit Harmful Data (IHD), or intentional risk data, does not contain explicit risk-related content but conveys malicious intent, such"}, {"title": "4.2 Adaptive Message-wise RL Alignment", "content": "Although RL-based approaches exhibit strong safety alignment, they possess notable limitations. Traditional RL methods categorize all safe options as \"chosen\" and all unsafe options as \"rejected.\" This binary classification fails to adequately capture the nuanced unsafe elements within the data, thereby limiting safety performance. Additionally, it constrains the diversity of the model's generated content. Inspired by the dense RL works (Zeng et al., 2024), we propose an Adaptive Message-wise Alignment (AMA) method based on Open-\n$M(x,y) = \\begin{cases}\n1 & \\text{if } (y \\in Y^w \\text{ and } r(x, y) > b) \\\\\n& \\text{or } (y \\in Y^l \\text{ and } r(x, y) \\le b) \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (4)\nwhere Yw and Yl are the chosen-rejected sample sets, respectively. b is the baseline value that determines whether a token is considered good or bad within a given context. Ideally, assuming a perfect reward model, the baseline will be set 0, however, during the real training process, assuming the existing of bias, we normally choose the average reward of the whole batch as the baseline value. We propose an adaptive message-wise RLHF, which can be formulated as follows:\nAdaptive Proximal Policy Optimization (APPO)\n$L_{mask-PPO} = E_{(s,a) \\sim \\pi_{\\theta^{old}}}\\left[ \\min \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta^{old}}(a|s)} A(s, a), clip \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta^{old}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) A(s,a) \\right) M(s, a) \\right]$ (5)\nAdaptive Direct Preference Optimization (ADPO)\n$L_{ADPO} = -E_{(x,y_w,y_l)\\sim D} \\left[ log \\left( \\sigma \\left( \\beta \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right) \\cdot M(x, y_w, y_l) \\right]$ (6)"}, {"title": "Adaptive Rejected Sampling (ARS)", "content": "$L_{RS} = L_{SFT} + \u03b2 \u00b7 D_{KL}(\u03c0_\u03b8||\u03c0_{ref})$, (7)\nWhere M(s, a), M(x, yw, yl), and M(x, y) represent the masks applied to PPO, DPO, and Rejected Sampling, respectively; for APPO and ARS, the reward of yw and yl is labelled by the offline reward model and for ADPO, the reward is labelled by the human annotators.\nSchmitt trigger (Filanovsky and Baltes, 1994; Depenbrock, 1988; Lazar and Toth, 2004) approach exploits the hysteresis characteristic of the Schmitt trigger by introducing the offset value \u03b4 to create a \"neutral zone,\" which helps reduce frequent classification changes due to small variations in rewards, thus making the classification more stable and reliable.\nG = {t | rt > b + \u03b4},\nB = {t | rt < b \u2212 \u0431},\nN = {t | b \u2212 d \u2264 rt \u2264 b + d}. (8)\nrt is the reward for the t-th token, b be the baseline value, and \u03b4 be the offset value.\n$M(t) = \\begin{cases}1, & \\text{if } r_t > b + \\delta \\\\\n0, & \\text{if } b - \\delta \\le r_t \\le b + \\delta \\\\\n-1, & \\text{if } r_t<b - \\delta\\end{cases}$ (9)"}, {"title": "5 Experiment", "content": "To validate our proposed training approach, we conducted experimental investigations in this section. We selected two different post-sft models as our base models: Qwen2-7B-instruct and Llama3-8B-instruct. For training data construction, we did merged training by mixing varying amounts of safety-related data with approximately 260k general-domain data points. For testing, we used the Beavertail-30k-test dataset (Ji et al., 2024b) and a random selection of 3k examples from the Wildchat dataset (Zhao et al., 2024) as our test set. Additionally, we also selected 10k real-world hard examples collected from our search APP, named Bal-Safe. The detailed dataset distributions can be found in Appendix A.2. The inference hyperparameters were set as follows: temperature = 0.8, top_P = 0.8, top_K = 50. For general performance evaluation, we reported objective scores on 11 diverse open-source datasets and calculated a subjective win-tie rate based on 1k meticulously annotated samples from Helpsteer (Wang et al.,"}, {"title": "5.1 Main Results", "content": "Firstly, we evaluated our proposed FDC+ADPO method on LLAMA3-8B-instruct and Qwen2-7B-instruct models, focusing on both safety and general performance. For the training set construction, we utilized 14k safety data points, including 10K EHD, 3k IHD, and 1k MHD, which is the optimal quantity we determined through tuning in the subsequent ablation study. As summarized in Table 1, our approach achieves superior safety alignment with only 14k safety data, outperforming DPO methods that utilize substantially more safety data (20k and 60k), particularly evident in the improved scores on Natural Harmful Data. For instance, achieving a safety score of 0.9020 on LLAMA3-8B-instruct compared to DPO's 0.7750 with 60k safety data. Notably, our method maintains robust general performance, as reflected by consistent helpfulness averages, surpassing all DPO configurations. This reflects that our method demonstrates an effective balance between safety enhancement and general performance retention, reaching the \"truly safe\" state."}, {"title": "5.2 Ablation study on Fine-grained Data-centric Approach", "content": "Facts and intent reinforce mutually in safety alignment. In Section 4, we observe that the model's anti-risk-intent and anti-risk-fact capabilities grow simultaneously as the quantity of safety data increases. This suggests that risk facts and risk intents mutually reinforce each other in safety alignment. We conducted an DPO experiment where the number of IHD in the training set was fixed at 1k, 3k, and 10k while incrementally increasing the number of EHD from 0 to 100k. The models' safety scores, which evaluate their safety performance, are reported in Figure 5. Initially, the safety scores for both EHD and IHD rapidly increase as the EHD training data grows, indicating mutual enhancement in safety alignment. However, as the EHD data continue to expand, the safety score for EHD keeps rising, while the safety score for IHD experiences no significant improvement. This suggests that, at this point, the model has developed robust safety values and is proficient in responding"}, {"title": "5.3 Ablation study on Adaptive Message-wise Alignment", "content": "To validate the effectiveness of the proposed adaptive message-wise approach, we designed a ablation study across different alignment approaches. We used Qwen2-7B-instruct as our baseline model and chose different RL-based methods for align-"}, {"title": "5.4 Further Experiments on EHD Data", "content": "It is important to note that in all our experiments, the safety score for EHD did not reach a satisfactory level (above 0.9), regardless of the alignment approach used. Based on our prior theoretical analysis, we attribute this to the model's limited safety knowledge. The number of harmful entities in the test set exceeds the model's capacity, resulting in hallucinations and a failure to produce appropriate safe responses. To address this, we further optimize the model using an RAG (Lewis et al., 2020) strategy and a self-reflection strategy (see Appendix B for a detailed prompt). With these methods, we found that safety scores across various datasets can indeed surpass 0.9. For instance, in the ADPO dataset, the EHD score improved from 0.7290 to 0.9010, MHD increased from 0.8835 to 0.9130, and the score for \"natural\" rose from 0.9020 to 0.9215. Meanwhile, the average helpfulness score remained around 0.80. These results indicate that the model has achieved a state of being \"truly safe and helpful.\""}, {"title": "6 Conclusion", "content": "In conclusion, this paper investigates the underlying causes of risks associated with large language models and proposes a novel alignment system to achieve a balance between safety and helpfulness. Our approach encompasses three critical dimensions: data management, training architecture. The experimental results demonstrate that our method significantly outperforms existing solutions. Future work will extend our findings from the textual domain to the field of Multimodal Large Language"}, {"title": "7 Limitations", "content": "Although our Equilibrate RLHF shows great performance in balancing safety and helpfulness of LLM alignment. It still have some shortcomings. Firstly, in this paper, we mainly talk about LLM alignment and benchmark evaluation, without more sophisticated red-team attack methods. Moreover, we propose that the security effectiveness of EHD data is mainly dependent on the volume of knowledge of the base model. However, in this paper, we have not proposed a perfect solution. The RAG+reflection approach is a promising method, but it also introduces risks associated with inaccurate external knowledge and data poisoning. Finally, in this paper, we openly release all processed public datasets, data generation and evaluation prompts, as well as the reinforcement learning framework. However, we are unable to disclose our self-developed evaluation dataset at this time, as it contains numerous prohibited harmful entities, such as internationally disputed political events, discriminatory statements, and negative information about leaders. We will process the data as soon as possible and gradually make this portion of the data publicly available in the future."}, {"title": "8 Potential Risks", "content": "In this paper, we introduce a novel approach to aligning large language models with safety and ethical guidelines, contributing significantly to the ongoing discourse in the field of AI safety alignment. Although we aim to advance the understanding and practical application of AI alignment, it is imperative to address the potential risks associated with our work, ensuring a balanced perspective on its implications.\nOne primary concern involves the inclusion of specific data examples and the generation of prompts that, although essential for illustrating our methods, may inadvertently harbor ethical and moral risks. The nature of these data, used to test the limits and assumptions of our approach, could be misinterpreted or misappropriated outside the context of academic research. It is crucial to emphasize that the inclusion of such data is strictly for demonstration purposes, serving to highlight potential vulnerabilities within existing models, and showcasing the robustness of our proposed solution.\nMoreover, the release of prompts used to generate this data poses a dual-use dilemma. While they can significantly aid researchers in replicating our experiments and conducting further investigations, there exists a risk that these tools could be exploited to intentionally produce harmful or biased content. We acknowledge this potential misuse and have taken substantial steps to mitigate these risks, such as implementing detailed guidelines and usage restrictions for accessing and utilizing the prompts.\nOur commitment remains firmly rooted in the responsible advancement of AI technologies. By openly discussing these potential risks, we advocate for increased awareness and discourse around the ethical implications of AI research, encouraging the development of comprehensive safeguards that accompany technological progress. We also encourage fellow researchers and practitioners to collaborate in refining these safety measures, fostering an environment where innovation proceeds hand-in-hand with ethical responsibility.\nWe believe that the proactive management of these risks will not only protect against adverse outcomes but will also enhance the credibility and societal acceptance of AI as a beneficial tool. Our stance is clear: the pursuit of knowledge and technological prowess must never overshadow the imperatives of ethical responsibility and societal good. As such, we remain vigilant and committed to contributing positively to the field of AI safety alignment."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Dataset construction", "content": "Our methodology for constructing safety data begins with the identification of risk-related keywords. We gather an extensive collection of security-related keywords, key phrases, news articles, and events from the Internet. This data is then refined, categorized, rewritten, and expanded through deep synthesis and manual annotation. Keywords recognized as posing risks are retained as risk entities (risk facts) for the creation of Explicit Harmful Data (EHD). Conversely, data deemed non-risky are transformed into risk-intent data through prompt engineering techniques and incorporated as Implicit Harmful Data (IHD) into our dataset. Through this systematic approach, we successfully amassed over 1,000k EHD entries and more than 300k IHD entries. By combining IHD and EHD data, we also generated over 200k Mixed Harmful Data (MHD) entries, which are critical for training and testing the safety performance of our models. During the partitioning of training and testing datasets, we implemented measures to prevent data leakage by isolating both risk entities and risk intentions from the synthesized data sources. This rigorous process enhances the quality of our dataset and significantly contributes to the reliability of our model evaluation. In this paper, we consider risk entities (harmful facts) across various domains, including ideological risks, legal and regulatory violations, abusive and hate speech, biases and discrimination, mental and physical health concerns, and ethical and moral issues. Each domain encapsulates specific manifestations that can pose significant threats to individuals, communities, and society at large. Defining the factual risks within each domain is essential for developing effective mitigation strategies. The following table outlines the definitions of the factual risks associated with these six domains."}, {"title": "A.2 Detailed description of datasets", "content": "The following content describes the safeguards that large language models (LLMs) implement when addressing different types of risk issues. We outline two distinct problems that LLMs may encounter in adhering to safety regulations: a) Insufficient Knowledge: The model may generate responses that do not comply with safety guidelines due to a lack of comprehensive knowledge or information. b) inadequate alignment: The model may produce responses that fail to meet safety standards because it has not been perfectly aligned with the desired values and norms. Figure 10 demonstrates some examples of EHD, IHD, and MHD in different domains.\nHere we also provide the detailed data distributions used in this paper in Figure 11."}, {"title": "A.3 Detailed methodology", "content": "Figure 12 to Figure 14 are the diagrams our proposed adaptive message-wise approach. From the diagram, it is clearly that the adaptive mask tends to choose the high-score tokens in positive (chosen) data and low-score tokens in negative (rejected) data and mask the rest, which highlights the significant segments and helps our model to learn the underlying reason why a data is chosen or rejected. Therefore, RLHF training is able to leverage the information within the data more efficiently, achieving better safety alignment even with limited data. Consequently, it improves the model's safety while maintaining its general capabilities."}, {"title": "B Prompts for data generation and GPT evaluation", "content": "In this section, we will introduce different prompts used in this paper, including: Figure 15 illustrates how to distinguish EHD, IHD and MHD data; Figure 15 illustrates how to generate harmful data from seed risky entities 16; Figure 17 illustrates how to generate MHD data from IHD and EHD data; Figure 18 illustrates how to do safety judgment using GPT-40; Figure 19 illustrates how to do subjective judgment (win-tie rate) using GPT-4."}]}