{"title": "Making Universal Policies Universal", "authors": ["Niklas Hoepner", "David Kuric", "Herke van Hoof"], "abstract": "The development of a generalist agent capable of solving a wide range of sequential decision-making tasks remains a significant challenge. We address this problem in a cross-agent setup where agents share the same observation space but differ in their action spaces. Our approach builds on the universal policy framework, which decouples policy learning into two stages: a diffusion-based planner that generates observation sequences and an inverse dynamics model that assigns actions to these plans. We propose a method for training the planner on a joint dataset composed of trajectories from all agents. This method offers the benefit of positive transfer by pooling data from different agents, while the primary challenge lies in adapting shared plans to each agent's unique constraints. We evaluate our approach on the BabyAI environment, covering tasks of varying complexity, and demonstrate positive transfer across agents. Additionally, we examine the planner's generalisation ability to unseen agents and compare our method to traditional imitation learning approaches. By training on a pooled dataset from multiple agents, our universal policy achieves an improvement of up to 42.20% in task completion accuracy compared to a policy trained on a dataset from a single agent\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "The challenge of developing a generalist agent capable of address-ing a diverse range of sequential decision-making tasks remains an open problem [37]. Successfully solving this challenge holds the potential to eliminate the need for task-specific engineering and retraining, while also enhancing performance through positive transfer between tasks [45, 50]. Given the vast differences in obser-vation and action spaces across tasks, most general agents are still developed for specific domains such as robotic manipulation [4, 45], web agents [10], computer control [51], or embodied navigation [55]. However, image-based observations offer a common ground for many sequential decision-making tasks, even when the corre-sponding action spaces vary significantly. Image observations are prevalent in contexts such as gameplay [17, 29], robotic control [33], and web or computer interfaces [10, 46], among others. Training a policy that can solve tasks that have environment observations in a unified format, such as images, is an important step towards creating a generalist agent. In this work, we move closer to this goal by developing a policy capable of controlling agents that share a common observation space but have different action spaces.\nRecently, universal policies [14] have emerged as a promising framework for learning multi-task policies through text guided video generation. This method is composed of two stages: first, a conditional diffusion model is trained to translate task descrip-tions into image-observation sequences; then, an inverse dynamics model is applied to map these sequences to the corresponding ac-tions. A key advantage of this approach is the ability to pretrain the video generation model on vast datasets of instruction-video pairs [15, 31, 41] or directly leveraging text-to-video foundation models [8, 18] as the planner. Additionally, the visual nature of the generated plans enhances interpretability by allowing inspection of the proposed solutions. Another potential advantage, which has not been investigated yet, lies in creating a shared planner for multiple agents, which can be paired with agent-specific inverse dynamics models to produce a policy capable of controlling all agents.\nWe explore this problem in a cross-agent setting where a group of agents shares the same observation space but operates with different action spaces. For each agent, there exists a small dataset of instruction-trajectory pairs. However, a single agent-specific dataset alone does not contain enough demonstrations such that training an agent-specific instruction-following policy through imitation learning on top of it would yield a policy capable of adequately solving the tasks for this agent. Our objective is to develop a policy that can successfully solve tasks for all agent types by pooling data from individual agents. To achieve this, we extend the universal policy framework to learn a policy on the combined dataset capable of controlling all the different agents.\nThe primary challenge lies in ensuring that the diffusion-based planner accounts for the varying capabilities of each agent (see Figure 1); otherwise, it may generate observation sequences that cannot be labelled via the agent-specific inverse dynamics model, leading to unpredictable behavior. On the other hand, this approach offers the potential for positive transfer, as the planner is exposed to a larger number of examples during training, which could lead to improved policies for each agent type. We investigate different methods for conditioning the planner on agent type information and assess their ability to generalise to unseen agents. In summary, our contributions are as follows:\n(1) Extending the universal policy framework to train a policy on a dataset consisting of trajectories from a diverse set of agents, capable of controlling all of them.\n(2) Comparing different approaches to condition the shared planner on agent type information and studying their ability to generalise to unseen agents."}, {"title": "2 RELATED WORK", "content": "We begin with an overview of approaches to training a general-ist agent-a policy capable of solving a wide range of sequential decision-making tasks. Next, we discuss algorithms developed for"}, {"title": "2.1 Building generalist agents", "content": "The GATO agent [37] represents the policy trained across the most diverse set of sequential decision-making tasks. It accomplishes this by mapping various action and observation spaces into to-kens, which are then processed and predicted by a transformer architecture [47]. The effectiveness of this approach relies on the presence of generalizable patterns within the tokenized trajectory data, enabling positive transfer. A bottleneck in training general policies is the need to train policies from scratch which is com-putationally intensive. To address this, researchers have begun integrating multi-modal foundation models [5, 13, 34] into their policies [23, 53, 57]. An example is the RT-2 model [57], which translates robotic actions into tokens and co-finetunes PaLI-X [5] and Palm-E [13] vision-language models using robotic trajectory data. Similarly, our adaptation of the universal policy approach can be combined with text-to-video foundation models [8, 18]."}, {"title": "2.2 Cross Embodiment Policy Learning", "content": "In robotics where data collection is costly, the community created increasingly larger datasets of task-trajectory pairs [9, 28, 30, 33]. One possibility to scale the amount of available data quickly is to pool data over robotic agents with different embodiments. For example the Open X-Embodiment (OXE) dataset pools several ex-isting robotic manipulation datasets [33]. Learning over different embodiments comes with the challenge of learning a policy that needs to be able to handle different action and observation spaces.\nWhile training agent-agnostic policies is not a new goal [21], the embodiment gap has widened significantly, such that robot behaviour is even learned from human demonstrations [35, 52]. \u03a4\u03bf address this challenge, two main approaches have emerged. The first maps different observation and action spaces of various em-bodiments into token sequences with a uniform structure, allowing them to be processed by a shared transformer [12, 45]. The second focuses on learning a joint latent space that aligns different embodi-ments within a common embedding space, enabling policy learning over this latent representation [48, 52].\nOur own data setup mirrors the OXE dataset as it pools agent-specific datasets from different agents. However, in the case of the OXE dataset, agents have varying observation spaces in addition to their differing action spaces. To learn a policy that can control a diverse set of agents we follow the universal policy approach [14] and frame policy learning as learning a generative model for ob-servation sequences in combination with agent-specific inverse dynamics models. We further show that training a shared diffusion planner achieves greater positive transfer than imitation learning approaches with separate policy heads for each agent [12, 45]."}, {"title": "2.3 Policy Learning with Diffusion", "content": "The recent success of diffusion models in learning generative mod-els for images [38, 39], audio [54], and videos [18] has extended to policy learning [7, 14, 16, 24, 27]. These models can learn policies by either mapping states to action distributions [7, 27] or mod-elling probability distributions over agent trajectories as sequences of state-action pairs [24]. In the latter approach, a trajectory is sampled based on a given task description and initial observation and then executed by the agent. Another approach proposed by Du et al. [14], known as universal policies, learns a generative model over observation sequences conditioned on natural language instructions. Instead of generating action sequences directly, it gen-erates observation sequences, which are then labelled with appro-priate actions using an inverse dynamics model. This method not only outperforms imitation learning baselines [3] and trajectory-based diffusion models [24] but also shows potential for integration with text-to-video foundation models. Additionally, it enhances interpretability by allowing inspection of the generated observa-tion sequences. If agents share a common observation space, the diffusion-based planner for universal policies can be trained us-ing trajectories from agents with varying capabilities. However, at test time, the generated plans must conform to the specific agent's capability constraints. To the best of our knowledge, we are the first to explore diffusion-based generative modeling of observation sequences on datasets containing trajectories from agents with heterogeneous action spaces."}, {"title": "3 METHODOLOGY", "content": "We begin by extending the theoretical framework underlying uni-versal policies called Unified Predictive Decision Process (UDPD) [14] to be compatible with the cross agent setting. Then we in-troduce the data setup assumed for our approach, followed by a description how we implement the shared diffusion planner that can be conditioned on different types of agent information."}, {"title": "3.1 CA-UDPD", "content": "A cross agent UDPD (CA-UDPD) is a tuple G = (N, X, C, H, p), where N is a set of agents, X represents the observations space for all agents, C is a space of task descriptions, H the time horizon and p(x0, c, k) : X \u00d7 C \u00d7 N \u2192 \u2206(XH) a conditional probability distribution over H-step observation sequences that depends on the current observation x0, the task c and the agent k executing the task."}, {"title": "3.2 Problem Setup", "content": "For each agent n \u2208 N, we have a dataset Dn consisting of Mn\ninstruction-trajectory pairs, denoted as Dn = {(ci, X1:t;, ai:ti)} Mn,\nwhere ci \u2208 C represents the instruction for the i-th sample and\nai:t; represents the action sequence the agent chose to complete the\ntask, generating the observation sequence x1:t; \u2208 Xti. Regarding\nthe relation of the observation space of the different agents we\nconsider two extreme cases. In the first case, all agents share the\nsame observation space, i.e., Xn = Xm = X \u00a5n, m \u2208 N. In the\nsecond case, the observation spaces are entirely disjoint, such that\nXnXm = 0 Vn, m\u2208 N. In the latter scenario, the agent identifier\nis inherently embedded in the observation xt, allowing the planner\np to identify the agent from the observation xo it is conditioned on\nand plan accordingly. Both scenarios are plausible, i.e. for example\nin the case of robotic manipulation [33] the image from the end-effector cameras would not let the planner identify which agent\nto plan for, while the static cameras will often show the agent the\nplanner controls. We pool the individual datasets Dn to obtain a\nlarger mixture dataset D = {(ci, X1:ti, ai:ti, ni)}=1, where ni \u2208 N\nis the agent id of the i-th sample and M = 2-1 Mn is the total\nnumber of trajectories aggregated over all datasets."}, {"title": "3.3 Universal Cross Agent Policy", "content": "The goal is to train the conditional observation sequence gener-ator p(xo, c, k) on the mixture dataset D, leading to a UniversalCross Agent Policy (UCAP). Instead of generating the completeobservation sequence x1:t; we sample random windows of size 4and take the first timestep to be the starting observation xo, i.e.p(xo, c, k) plans the next three timesteps for agent k followinginstruction c starting in x0. In contrast to previous work we do notapply temporal super resolution, i.e first generating a coarse videoplan that is refined in a second step [14]. We estimate p using aconditional diffusion model. Diffusion models perturb data samplesby gradually adding noise at different scales, then learn to reversethis process to recover the original data. New samples are generatedby starting with noise and iteratively applying the learned reverseprocess to approximate the data distribution [19, 43]."}, {"title": "4 EXPERIMENTS", "content": "We begin with outlining the experimental environment that facili-tates the creation of agents with varying action spaces. Next, we investigate whether training on a pooled dataset, composed of tra-jectories from agents of different types, results in positive transfer. Additionally, we examine the impact of conditioning the planner on the different types of agent information presented in Section 3.3. In the sections that follow, we compare the performance of UCAP against imitation learning baselines tailored to our data setup and conclude with ablations that investigate the performance of UCAP in the case of disjoint observation spaces, the effect of planning granularity on performance and the effect of number of agent types on generalisation performance."}, {"title": "4.1 Environment", "content": "We choose BabyAI [6] as the evaluation environment since it is easily modifiable and offers a range of tasks with varying com-plexity. In the BabyAI environment an agent needs to navigate a gridworld to complete different tasks ranging from navigating to objects to opening doors with keys. The environment state can either be represented as a gridworld or an RGB image. Here we choose the gridworld representation as it is computationally more efficient. The environment state is fully observable. An agent's performance is evaluated by the percentage of tasks completed cor-rectly over 512 episodes. The standard action space consists of six actions (turn left, turn right, move forward, pickup, drop, toggle). We extend the action space to a total of 18 actions (e.g. turn 180 degrees, move diagonal, etc). To create different action spaces for different agents we mask out actions that are not available for the specific agent. In total we create eight agent types. An overview of the different agent types and their capabilities can be found in Table 1. A summary of all the possible actions can be found in Appendix A. The mixture dataset D is created by pooling the demonstrations of agents 0-5 (see Table 1). The other two agents are left out to test generalisation of models to unseen agents. We refer to agents whose datasets are included in the pooled dataset as In-Distribution (ID) agents, while agents whose datasets are not included are called Out-of-Distribution (OOD) agents. We test the universal policy approach on the following three environment instances 2:\nGoToObj: The agent needs to navigate to the only object in the environment. The object can be a key, box, or ball, and comes in one of six different colours. This task does not require any natural language task instructions as the task is always the same. The environment observations have size 8 \u00d7 8 \u00d7 3 and the trajectory length is at most 25 timesteps.\nGoToDistractor: The same environment as the GoTo environment but with 3 distractor objects added. Now the natural language instruction is necessary to understand to which object to navigate.\nGoToDistractorLarge: The goal of the environment is the same as in the GoToDistractor environment. However the agent is now in one of nine rooms that are connected via doors and the environment contains 7 distractor objects. The environment observation is of size 22 \u00d7 22 \u00d7 3 and the maximum trajectory length is at most 100 timesteps. An image representation of the different environment observations can be seen in Figure 3."}, {"title": "4.2 Positive Transfer", "content": "Positive transfer occurs if UCAP outperforms on average the uni-versal policies trained for a specific agent type on the agent specific dataset. In order to investigate whether positive transfer occurs we perform the following steps:\n(1) Train a universal policy for a single agent on a small agent-specific dataset for each agent type.\n(2) Train a universal policy on the mixture of small datasets(UCAP). Here we train a policy for each type of agent infor-mation from Section 3.3 (Mixture-Example, Mixture-AgentID,Mixture-ActionSpace) and one UCAP that is not conditionedon any agent information (Mixture).\n(3) Train a universal policy for a each agent type on an agent-specific dataset equal in size to the mixture dataset.\nThe performances of the universal policies trained on the small and larger dataset serves as anchor to evaluate the amount of posi-tive/negative transfer that occurs. The policy generated by training on the large dataset is an upper bound to the amount of transfer we can expect. Any performance improvement above the policy trained on the smaller dataset is a sign of positive transfer, and any performance decrease is a sign of negative transfer. For the two smaller environments we perform the analysis for all agent types. For the GoToDistractorLarge environment, we only do the analysis for the standard action space, i.e. no single agent policies are trained for the other agent types to save computational resources.\nIn Figure 4 we can see the performance of the different universal policies in the three environments for the standard agent. To save compute we only train UCAP conditioned on the action space en-coding for the GoToDistractorLarge environment since it worked best in the smaller environments. For all environments UCAP con-ditioned on any type of the agent representation shows positive transfer, where conditioning on a representation of the action space works best and conditioning on examples has the worst mean task completion rate. One possible reason for the weaker performance"}, {"title": "4.3 Baselines", "content": "We compare UCAP to imitation learning baselines adapted to learn-ing from multiple smaller datasets from different agents. The policy architecture for all imitation learning baselines consists of a con-volutional stack followed by an MLP [22]. Policies are trained to predict the agent's next action given a state sampled from the expert demonstrations. More details on the architecture and hyperparam-eters can be found in Appendix B.3. We implement the following approaches:\nImitation Learning (IL): We train a single agent policy on the small and large dataset. Again the performance on the large dataset serves as an upper bound of imitation learning, while the performance on the smaller dataset serves as a threshold to whether we observe positive or negative transfer.\nIL - Union of Action Spaces: A common way of handling dif-ferent actions spaces is to form the union over all agents action spaces [56]. The model receives a one-hot encoded vector repre-senting the agent ID, which is concatenated with the output of"}, {"title": "4.4 Ablations", "content": "4.  1 Disjoint Observation Spaces. As mentioned in Section 3.2 the observation space of different agents does not necessarily need to overlap, i.e. the observation itself contains information on the agent type. The planner could potentially leverage this information to adjust the plan of the agent accordingly. To emulate this setting in the BabyAI environment, we give each agent type a different colour and train the universal policy on a newly created mixed dataset where agents are identifiable via their colour. In Figure 5 we compare the results to the best UCAP model with uniformly coloured agents in the GoToObj and GoToObjDistractor environ-ment. The planner successfully leverages the colour information and reaches a task completion rate comparable or higher than the UCAP conditioned on an action space encoding. As expected the planner does not generalise to OOD agents with an OOD colour.\n5.  2 Planning Granularity. An alternative strategy to handle dif-fering action spaces is to plan at a coarser timestep, where the plan-ner suggests the next goal state, and each agent's goal-conditioned policy chooses the action to move from the current state to the pro-posed goal. This approach has the benefit that the generated plans do not need to conform with the agents capabilities at a timestep level, but transfers the responsibility from the planner to the agent-specific goal-conditioned policies. Compared to learning an inverse dynamics model, developing a local goal-conditioned policy typ-ically requires more data due to the increased complexity of the mapping. Here, local refers to the fact that the goal is reachable within a few timesteps.\nTo test this, we trained the diffusion planner on subsampled trajectories in the GoToDistractor environment, selecting the start and end states along with every n-th timestep, and trained a goal-conditioned policy through imitation learning by sampling start and goal states from the agent-specific datasets. First, we eval-uated only the planner's performance across different planning granularity levels using an oracle policy, which moved the agent to each planner-suggested goal state. In Figure 6 we can see that the best planning performance is reached when planning for two timesteps. Evaluating the 2-step planner with the agent-specific goal-conditioned policies shows also an improved task completions rate compared to combining a 1-step planner with agent-specific IVD models (see Table 4). Notably, the performance on the OOD agents is on par with the performance for the ID agents as the 2-step planner cannot propose impossible actions for agents anymore.\n6.  3 Increased Agent Diversity. One possible reason why condi-tioning on either an encoding of the action space or trajectory examples fails to generalize to OOD agents is the limited diversity of agents in the training set. With only six different agents, it may"}, {"title": "5 LIMITATIONS", "content": "While we observed positive transfer from training on a pooled dataset of different agents, it is unclear whether this transfer will hold when environment observations vary more significantly, such as in the OXE dataset [33], which includes observations from di-verse robotic manipulators. Future work should attempt to scale the approach to larger datasets with heterogenous A drawback of using the diffusion planner is the increased training and inference time. Convergence requires approximately 40 times more updates compared to imitation learning, and the sampler needs 128 neural function evaluations versus just one for imitation learning. Tech-niques like progressive distillation [40] and consistency models [42] improve sampling speed with minimal loss in generative ability."}, {"title": "6 CONCLUSION", "content": "We showed that it is possible to leverage the universal policy ap-proach [14] to train a diffusion based planner that generates obser-vations sequences for agents with different capabilities. Training the planner on a pooled dataset from all agent types leads to an improved universal policy compared to training on the smaller agent specific datasets alone. The performance of the planner can be improved by conditioning on agent specific information such as the encoding of the action space or example observation sequences. Independent of the agent information the planner is conditioned on generalisation to OOD agents without finetuning remains a chal-lenging task, where potential solution strategies include increased agent diversity and planning at a higher granularity level. Future work should extend the approach to agents with heterogenous observation spaces [12, 45] and scale it to more complex environ-ments [33]."}]}