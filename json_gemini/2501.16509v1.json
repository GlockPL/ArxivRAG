{"title": "Reinforcement Learning for Quantum Circuit Design: Using Matrix Representations", "authors": ["Zhiyuan Wang", "Chunlin Feng", "Christopher Poon", "Lijian Huang", "Xingjian Zhao", "Yao Ma", "Tianfan Fu", "Xiao-Yang Liu"], "abstract": "Quantum computing promises advantages over classical computing. The manufacturing of quantum hardware is in the infancy stage, called the Noisy Intermediate-Scale Quantum (NISQ) era. A major challenge is automated quantum circuit design that map a quantum circuit to gates in a universal gate set. In this paper, we present a generic MDP modeling and employ Q-learning and DQN algorithms for quantum circuit design. By leveraging the power of deep reinforcement learning, we aim to provide an automatic and scalable approach over traditional hand-crafted heuristic methods.", "sections": [{"title": "Introduction", "content": "Quantum computing has the potential to revolutionize computing beyond the reach of classical computers (Gill et al. 2021). A major hurdle is the quantum circuit design that maps a quantum circuit to gates in a universal gate set. Traditional hand-crafted heuristic methods are often inefficient and not scalable.\n\nThe automated design of quantum circuits remains a major challenge. (Ali et al. 2015) and (Bhat, Khanday, and Shah 2022) used a method that utilizes the Toffoli gate decomposition technique, reducing cost and enhancing efficiency. Machine learning, in particular reinforcement learning, has recently been applied. (Sogabe et al. 2022) explored a model-free deep recurrent Q-network (DRQN) method for an entangled Bell-GHZ circuit. Recently, (Liu and Zhang 2023) and (Meirom et al. 2022) utilized tensor network representations of Google's Sycamore circuit (Arute et al. 2019) and studied the Tensor Network Contraction Ordering (TNCO) problem.\nIn this paper, we explore reinforcement learning methods to automate the task of quantum circuit search. Our contributions can be summarized as follows:\n\u2022 We present three generic Markov Decision Process (MDP) modelings for the quantum circuit design task.\n\u2022 We study 10 quantum circuit design tasks: 4 Bell states, SWAP gate, iSWAP gate, CZ gate, GHZ gate, Z gate and Toffoli gate, respectively, given a universal gate set {H, T, CNOT}."}, {"title": "Problem Formulation", "content": "Taking Bell state |\u03a6+\u27e9 as an example, we formulate the task of quantum circuit design as two versions of Markov Decision Process (MDP). In particular, we specify the state space, action set, reward function, and Q-table, respectively."}, {"title": "Task: Quantum Circuit Design", "content": "Given two qubits with initial state |q\u2081q\u2080\u27e9 = |00\u27e9 and a universal gate set G = {H,T,CNOT}, the goal is to find a quantum circuit that generates the Bell state |\u03a6\u207a\u27e9:\n\n|\u03a6\u207a\u27e9 = 1/\u221a2 (|00\u27e9 + |11\u27e9). (1)\n\nThe target quantum circuit to generate |\u03a6\u207a\u27e9 is shown in Fig. 1, whose matrix representation is:\n\nU = CNOT\u2080\u2081\u00b7 (H \u2297 I)\n= \u239b\u239c\u239c\u239d 1 0 0 0\n0 1 0 0\n0 0 0 1\n0 0 1 0\u239e\u239f\u239f\u23a0 \u00b7 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0\n= 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0. (2)\n\nNote that |\u03a6\u207a\u27e9 = U |00\u27e9."}, {"title": "Modeling as Markov Decision Process (MDP)", "content": "We provide three types of MDP modelings.\nThere are four Bell states, physically the two qubits are maximally entangled."}, {"title": "Matrix Representation", "content": "\u2022 Actions A = {H\u2080, H\u2081, T\u2080, T\u2081, CNOT\u2080\u2081}, since H and T can be executed on either q\u2080 or q\u2081. An action a \u2208 A is represented as a matrix A \u2208 C\u2074\u00d7\u2074.\n\u2022 State space S: The initial state is U\u2080 = I\u2084 and the terminal state is U given in (2). Let S be the current state (a node in Fig. 2), A \u2208 A be the action, then the resulting state at a child node S' is given by\n\nS' = A \u00b7 S. (3)\n\nThe state space S is a tree in Fig. 2. The connecting lines 1, 2, 3, 4, and 5 correspond to the five actions in A. At the initial state S\u2080 = I\u2084, taking an action a \u2208 A will generate 5 states {S\u2081, S\u2082, S\u2083, S\u2084, S\u2085}. Then, taking a second action a \u2208 A at a state S \u2208 {S\u2081, S\u2082, S\u2083, S\u2084, S\u2085} will generate 25 states S\u2086, S\u2087,..., S\u2083\u2080. Thus, S has a total of 31 states.\n\u2022 Reward function R: At state S\u2081, taking action a = CNOT\u2080\u2081, the reward is R(s = S\u2081, a = CNOT\u2080\u2081) = 100; otherwise, R(s, a) = 0.\n\nExample for Fig. 1: Given initial state S\u2080 = I\u2084, let us consider the optimal trajectory S\u2080 \u2192 S\u2081 \u2192 S\u2081\u2080.\nState after taking the first action a = H\u2080,\n\nS\u2081 = (H\u2080 \u2297 I)S\u2080 = 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0. (4)\n\nFinal state after taking the second action a = CNOT\u2080\u2081,\n\nS\u2081\u2080 = CNOT\u2080\u2081 \u00b7 S\u2081\n= \u239b\u239c\u239c\u239d 1 0 0 0\n0 1 0 0\n0 0 0 1\n0 0 1 0\u239e\u239f\u239f\u23a0 \u00b7 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0\n= 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0 = U, (5)\n\nwhich corresponds to the target circuit in (2).\nAdvantage: Different sequences of quantum gates may result in the same matrix state, thus this matrix representation would reduce the state space.\nDisadvantage: RL agent needs to be trained for each target matrix, even though different circuits may share similar or identical intermediate states. This approach makes the training process repetitive."}, {"title": "Reverse Matrix Representation", "content": "\u2022 Actions A\u207b\u00b9 = {H\u2080\u207b\u00b9, H\u2081\u207b\u00b9, T\u2080\u207b\u00b9, T\u2081\u207b\u00b9, CNOT\u2080\u2081\u207b\u00b9}, since H\u207b\u00b9 and T\u207b\u00b9 can be executed on either q\u2080 or q\u2081. An action a \u2208 A\u207b\u00b9 is represented as a matrix A\u207b\u00b9 \u2208 C\u2074\u00d7\u2074.\n\u2022 State space S: The initial state is S\u2080\u207b\u00b9 = U given in (2) and the terminal state is I\u2084. Let S\u207b\u00b9 be the current state (a node in Fig. 3), A\u207b\u00b9 \u2208 A\u207b\u00b9 be the action, then the resulting state at a child node S'\u207b\u00b9 is given by\n\nS'\u207b\u00b9 = A\u207b\u00b9 \u00b7 S\u207b\u00b9. (6)\n\nThe state space S\u207b\u00b9 is a tree in Fig. 3. The connecting lines 1, 2, 3, 4, and 5 correspond to the five actions in A\u207b\u00b9. At initial state S\u2080\u207b\u00b9 = U, taking an action a \u2208 A\u207b\u00b9 will generate 5 states {S\u2081\u207b\u00b9, S\u2082\u207b\u00b9, S\u2083\u207b\u00b9, S\u2084\u207b\u00b9, S\u2085\u207b\u00b9}. Then, taking a second action a \u2208 A\u207b\u00b9 at a state S\u207b\u00b9 \u2208 {S\u2081\u207b\u00b9, S\u2082\u207b\u00b9, S\u2083\u207b\u00b9, S\u2084\u207b\u00b9, S\u2085\u207b\u00b9} will generate 25 states S\u2086\u207b\u00b9, S\u2087\u207b\u00b9,..., S\u2083\u2080\u207b\u00b9}. Thus, S\u207b\u00b9 has a total of 31 states.\n\u2022 Reward function R: At state S\u2085\u207b\u00b9, taking action a = H\u2081\u207b\u00b9, the reward R(s = S\u2085\u207b\u00b9, a = H\u2081\u207b\u00b9) = 100; otherwise, R(s, a) = 0.\n\nExample for Fig. 1: Given initial state S\u2080\u207b\u00b9 = U in (2), we consider the optimal trajectory S\u2080\u207b\u00b9 \u2192 S\u2085\u207b\u00b9 \u2192 S\u2082\u2086\u207b\u00b9.\nState after taking the first action a = CNOT\u2080\u2081\u207b\u00b9,\n\nS\u2085\u207b\u00b9 = CNOT\u2080\u2081\u207b\u00b9 \u00b7 S\u2080\u207b\u00b9 = \u239b\u239c\u239c\u239d 1 0 0 0\n0 1 0 0\n0 0 0 1\n0 0 1 0\u239e\u239f\u239f\u23a0 \u00b7 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0\n= 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0. (7)\n\nFinal state after taking the second action a = H\u2080\u207b\u00b9,\n\nS\u2082\u2086\u207b\u00b9 = (H\u207b\u00b9 \u2297 I)S\u2085\u207b\u00b9 = 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0 \u00b7 1/\u221a2 \u239b\u239c\u239c\u239d 1 0 1 0\n0 1 0 1\n1 0 \u22121 0\n0 1 0 \u22121\u239e\u239f\u239f\u23a0 = \u239b\u239c\u239c\u239d 1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\u239e\u239f\u239f\u23a0 = I\u2084. (8)\n\nTo construct the target circuit, one can reverse the ordering of actions and take the inverse of each action. In this example, gate CNOT\u2080\u2081\u207b\u00b9 is followed by gate H\u2080\u207b\u00b9. Therefore, the result is H\u2080 followed by CNOT\u2080\u2081, which corresponds to the target circuit in Fig. 1."}, {"title": "Tensor Network Representation", "content": "The Tensor Network (TN) is a powerful representation for quantum circuits. A tensor network is a collection of interconnected tensors. A single-qubit gate can be represented as a 2-order tensor, while a double-qubit gate can be represented as a 4-order tensor. For example, we convert the circuit in Fig. 1 to Fig. 4.\nConsider Fig. 1 and a universal gate set G = {H,T, CNOT\u2080\u2081}. The gate list is L = {H\u2080, H\u2081, T\u2080, T\u2081, CNOT}. We allow up to two gates for demonstration purpose.\n\u2022 Actions A = {H\u2080, H\u2081, T\u2080, T\u2081, CNOT\u2080\u2081,\n(H\u2080, H\u2081), (H\u2080, T\u2081), (H\u2081, T\u2080), (T\u2080, T\u2081),\n(T\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2080), (T\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2081),\n(H\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2080), (H\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2081)}.\nThere are 17 different actions in total. Taking action (H\u2080, CNOT\u2080\u2081) results in the TN representation in Fig. 4.\n\u2022 State space S: The initial state is S\u2080 = |00\u27e9, and the terminal state is |\u03a6\u207a\u27e9 given in (1). Let S be the current state (a node in Fig. 5), A \u2208 A be an action, then the resulting state at a child node S' is given by:\n\nS' = A \u00b7 S. (9)\n\nThe state space S is represented as a tree in Fig. 5. The connecting lines 1, 2, 3, . . ., 17 correspond to the 17 actions in A. At the initial state S\u2080 = |00\u27e9, taking an action A \u2208 A will generate 17 states {S\u2081, S\u2082, S\u2083, ..., S\u2081\u2087}. Thus, S contains a total of 18 states."}, {"title": "Q-Learning and DQN Algorithms", "content": "Q-Learning Algorithm\nThe Q-learning algorithm updates a Q-table (Watkins and Dayan 1992) in each step as follows\n\nQ\u207f\u1d49\u02b7(S\u209c, A\u209c) = (1 \u2212 \u03b1) Q(S\u209c, A\u209c) + \u03b1 [R\u209c\u208a\u2081 + \u03b3 max\u2090 Q(S\u209c\u208a\u2081, a)] (11)"}, {"title": "Experiment Results", "content": "We verify the above three MDP modelings for 10 well-known quantum circuits, namely, circuits to generate 4 Bell states, SWAP gate, iSWAP gate, CZ gate, GHZ Gate, Z gate, and Toffoli gate. For Matrix and Reverse Matrix Representations, we apply both Q-learning and DQN algorithms, while for TN Representation, we applied only Q-learning. Our codes can be found at this link\u00b2.\nToffoli Gate: we used an action set with gates in Fig. 6,\n\nA = {CNOT\u2082\u2081, H\u2080, CP\u2081\u2080, CP\u2081\u2080\u207b\u00b9, CP\u2082\u2080},\n\nwhere the CP gate refers to controlled-phase gate with a phase shift of \u03c0, and CP\u207b\u00b9 with a phase shift of -\u03c0.\nAn expert trajectory were stored in the replay buffer to improve learning efficiency. For Matrix Representation, the expert trajectory is:\n\n{H\u2080 \u2192 CP\u2081\u2080 \u2192 CNOT\u2082\u2081 \u2192 CP\u2081\u2080\u207b\u00b9 \u2192\nCNOT\u2082\u2081 \u2192 CP\u2082\u2080 \u2192 H\u2080}.\n\nFor Reverse Matrix Representation,\n\nA\u207b\u00b9 = {CNOT\u2082\u2081\u207b\u00b9, H\u2080\u207b\u00b9, CP\u2081\u2080\u207b\u00b9 , CP\u2081\u2080, CP\u2082\u2080\u207b\u00b9},\n\nand the expert trajectory becomes to:\n\n{H\u2080\u207b\u00b9 \u2192 CP\u2082\u2080\u207b\u00b9 \u2192 CNOT\u2082\u2081\u207b\u00b9 \u2192 CP\u2081\u2080 \u2192\nCNOT\u2082\u2081 \u2192 CP\u2081\u2080\u207b\u00b9 \u2192 H\u2080\u207b\u00b9}.\n\nEach state expands in a branching factor (size of actions) c across b + 1 levels (length of the tasks+1), as in Fig. 2, the size of the state space is given by a geometric series:\n\nSize of state space = c\u2070 + c\u00b9 + \u00b7\u00b7\u00b7 + c\u1d47 = (c\u1d47\u207a\u00b9 \u2212 1) / (c - 1) (13)\n\nThe complexity of the task is measured by the size of the states space, given in Table 4. To evaluate the effectiveness of Q-learning and DQN, we conduct 100 rounds. In each round, the agent is trained for 100 episodes, and we measure the success ratio (in percentage) of correct testing results over the 100 rounds. The results are summarized in Table 6.\nFrom Table 6, we observe that both Q-learning and DQN perform well on simpler tasks, such as generating the Bell state |\u03a6\u207a\u27e9. However, as task complexity increases, for example, the iSWAP gate task with a state space size of 56, the performance of both algorithms significantly degrades, indicating the challenges of learning in large state spaces."}, {"title": "Conclusion and Future Work", "content": "In this paper, we applied Q-learning and Deep Q-Network (DQN) algorithms to three MDP modelings of the quantum circuit design task. We demonstrated that RL algorithms successfully discovered the expected quantum circuits for 4 Bell states, SWAP gate, iSWAP gate, CZ gate, GHZ gate, Z gate, and Toffoli gate. We noticed that Reverse Matrix Representation and TN Representation have greater potential in this problem. For more difficult tasks, both Q-learning and DQN struggle to converge due to insufficient sampling quality and efficiency.\nIn future work, we will improve sample quality and implement algorithms like Monte Carlo Tree Search (MCTS) to increase efficiency and address the convergence challenge. Finally, we will investigate the robustness of RL algorithms by testing more complex quantum circuits."}, {"title": "Appendix: Task Description", "content": "Two-Qubits Action Set (TN)\nFor tasks below in TN representation:\n\u2022 Bell state |\u03a6\u207a\u27e9,\n\u2022 Bell state |\u03a6\u207b\u27e9,\n\u2022 Bell state |\u03a8\u207a\u27e9,\n\u2022 Bell state |\u03a8\u207b\u27e9,\nThey share the same action set:\n{H\u2080, H\u2081, T\u2080, T\u2081, X\u2080, X\u2081, CNOT\u2080\u2081,\n(H\u2080, H\u2081), (H\u2080, T\u2081), (H\u2081, T\u2080), (T\u2080, T\u2081), (Z\u2080, Z\u2081),\n(T\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2080), (T\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2081),\n(H\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2080), (H\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2081)}\nFor tasks below in TN representation:\n\u2022 SWAP gate,\n\u2022 iSWAP gate,\n\u2022 CZ gate,\nThey share the same action set:\n{H\u2080, H\u2081, T\u2080, T\u2081, CNOT\u2080\u2081, CNOT\u2081\u2080, (H\u2080, H\u2081), (H\u2080, T\u2081),\n(H\u2081, T\u2080), (T\u2080, T\u2081), (CNOT\u2080\u2081, CNOT\u2081\u2080), (CNOT\u2081\u2080, CNOT\u2080\u2081),\n(T\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2080), (T\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, T\u2081),\n(H\u2080, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2080), (H\u2081, CNOT\u2080\u2081), (CNOT\u2080\u2081, H\u2081)}\nThree-Qubit Action Set (TN)\nFor tasks below in TN representation:\n\u2022 GHZ gate,\n\u2022 Z gate,\nThey share the same action set:\n{H\u2080, H\u2081, H\u2082, T\u2080, S\u2080, S\u2081, S\u2082, T\u2081, T\u2082, CNOT\u2080\u2081, CNOT\u2081\u2082, CNOT\u2080\u2082,\n(H\u2080, H\u2081), (H\u2080, T\u2081), (T\u2080, H\u2081), (T\u2080, T\u2081),\n(H\u2080, H\u2082), (H\u2080, T\u2082), (T\u2080, H\u2082), (T\u2080, T\u2082),\n(H\u2081, H\u2082), (H\u2081, T\u2082), (T\u2081, H\u2082), (T\u2081, T\u2082),\n(H\u2080, CNOT\u2080\u2081), (T\u2080, CNOT\u2080\u2081), (H\u2081, CNOT\u2080\u2081),\n(T\u2081, CNOT\u2080\u2081), (H\u2082, CNOT\u2080\u2081), (T\u2082, CNOT\u2080\u2081),\n(H\u2080, CNOT\u2080\u2082), (T\u2080, CNOT\u2080\u2082), (H\u2081, CNOT\u2080\u2082),\n(T\u2081, CNOT\u2080\u2082), (H\u2082, CNOT\u2080\u2082), (T\u2082, CNOT\u2080\u2082),\n(H\u2080, CNOT\u2081\u2082), (T\u2080, CNOT\u2081\u2082), (H\u2081, CNOT\u2081\u2082),\n(T\u2081, CNOT\u2081\u2082), (H\u2082, CNOT\u2081\u2082), (T\u2082, CNOT\u2081\u2082),\n(CNOT\u2080\u2081, CNOT\u2080\u2082), (CNOT\u2080\u2081, CNOT\u2081\u2082), (CNOT\u2080\u2082, CNOT\u2081\u2082)}"}, {"title": "Appendix: Test Result", "content": "Appendix: Reward Calculation\nThe reward for all tasks in Q-Learning, Q-Learning (Reverse), DQN, DQN (Reverse); and four tasks in TN Representation (SWAP gate, iSWAP gate, CZ gate, Z gate) are calculated as follows:\n\n1. The quantum circuit is executed using a Qiskit simulator.\n2. The unitary operator of the current circuit is compared with the target unitary operator.\n3. If the comparison results in a value greater than 0.99, a reward of 100 is given."}, {"title": "Appendix: Examples of Quantum Circuits", "content": "The reward calculation can be expressed as:\n\nReward = [100, if Tr(S'\u2020U) / 2\u207f\u1d58\u1d50_\ud835\udcc6\u1d58\u1d47\u2071\u1d57\u02e2 > 0.99\n0, otherwise\n\nwhere S' is the current unitary operator and U is the target unitary operator.\nThe reward for the five tasks in TN representation (four Bell states, GHZ gate) are calculated as follows:\n\n1. The quantum state of the circuit is obtained using the function get_quantum_state.\n2. The current state is compared with the target state.\n3. If the comparison results in a value greater than 0.99, a reward of 100 is given.\nThe reward calculation can be expressed as:\n\nReward = [100, if |\u27e8S'|U\u27e9|\u00b2 > 0.99\n0, otherwise\n\nwhere S' is the current state vector and U is the target state vector, and \u27e8S'|U\u27e9 represents the inner product between the current state and the target state."}, {"title": "Appendix: Q-learning and DQN Environment", "content": "Environments follow the training loop according to the example code snippet (Listing 1).\nQ-Learning Environment\n\u2022 State Space: The environment consists of 100 discrete states, each representing a unique configuration of the system.\n\u2022 Q-Table: A 100 \u00d7 6 table is used to store the Q-values for each state-action pair.\n\u2022 Training Parameters:\n\u2013 Learning Rate (\u03b1): 0.1\n\u2013 Discount Factor (\u03b3): 0.95\n\u2013 Exploration Rate (\u03b5): Initial value of 1.0, decays at a rate of 0.99, with a minimum value of 0.05.\nDQN Environment\n\u2022 State Space: The state is represented as a feature vector and passed to a neural network. The environment supports continuous state spaces.\n\u2022 Neural Network: The Q-values are approximated using a 3-layer fully connected neural network:\n\u2013 Input Layer: Accepts the state vector as input.\n\u2013 Two Hidden Layers: Each with 128 neurons and ReLU activation.\n\u2013 Output Layer: Produces Q-values for 6 actions.\n\u2022 Training Parameters:\n\u2013 Learning Rate (\u03b1): 0.1\n\u2013 Discount Factor (\u03b3): 0.95\n\u2013 Exploration Rate (\u03b5): Initial value of 0.9, decays at a rate of 0.995, with a minimum value of 0.05.\n\u2013 Batch Size: 64\n\u2013 Replay Buffer Size: 10,000\n\u2013 Target Network Update: Every 100 episodes.\nNote: We designed our own custom environment QuantumEnv built with gym\u2074. The environment implementation can be found in this link.\u2075"}, {"title": "Appendix: Expert Trajectories for Toffoli Gate in Q-Learning and DQN", "content": "Overview\nBoth Q-Learning and DQN use an expert action sequence to embed optimal behavior for constructing a Toffoli gate. This sequence (Fig. 6) guides the agent's learning process by providing predefined state-action pairs that achieve the desired result.\nSimilarities\n\u2022 The expert trajectory is applied over multiple iterations, starting with an environment reset.\n\u2022 Selected actions are executed sequentially, returning the next state, reward, and completion flag.\nDifferences\n\u2022 Q-Learning:\n\u2013 Applied over 10 iterations of the expert trajectory.\n\u2013 The Q-Table is updated at the end of each iteration using the transitions observed during the trajectory.\n\u2022 DQN:\n\u2013 Applied over 150 iterations of the expert trajectory.\n\u2013 After every action in the trajectory, the transition is stored in memory, and a small replay step is performed.\n\u2013 At the end of each iteration, the target network is updated using update_target_net() for stable training."}]}