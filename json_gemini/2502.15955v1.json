{"title": "Compression Barriers in Autoregressive Transformers", "authors": ["Themistoklis Haris", "Krzysztof Onak"], "abstract": "A key limitation of autoregressive Transformers is the large memory needed at inference-time to cache all previous key-value (KV) embeddings. Prior works address this by compressing the KV cache but often assume specific structural properties of the embeddings. This raises the following natural question: Can truly sub- linear space utilization be achieved without such assumptions? In this work, we answer this question in the negative. Any algorithm for attention-based token generation must use $\\mathcal{O}(nd)$ space, where $n$ is the number of tokens generated so far and $d > \\Omega(\\log n)$ is the dimension of the KV embeddings. Our proof involves a re- duction from a classic communication complexity problem and uses a randomized construction that leverages properties of projections in the spirit of the Johnson-Linderstrauss lemma. For the low-dimensional regime $d = o(\\log n)$, we show that any algorithm requires $\\Omega(de^d)$ space and prove, using tight bounds on cover- ing numbers, that SUBGEN, proposed by Zandieh et al. [2024b], matches this bound. Further, we investigate how sparsity assumptions enable token generation in truly sublinear space, presenting impossibility results and proposing a new KV cache compression algorithm for sliding window attention when the value cache outside the window is unmasked. Finally, we analyze token generation's time complexity, using an indistinguishabil- ity argument to prove that no non-adaptive algorithm can compute attention online in sublinear time for all tokens.", "sections": [{"title": "Introduction", "content": "Transformers have revolutionized the field of Deep Learning by enabling the creation of models that achieve state- of-the-art performance on a wide range of tasks [Dosovitskiy, 2020, Vaswani, 2017]. Central to their success lies the attention mechanism, which allows the model to \u201cfocus\u201d on different parts of the input sequence when generating the output. However, the attention mechanism comes with a significant drawback: the large memory footprint required at token generation for caching numerous embedding vectors for each previous token. The cached to- ken embeddings are known as the key-value (KV) cache. Indeed, due to the KV cache, the memory required by Transformer LLMs for long-context token generation grows linearly with the number of tokens, which can be prohibitive for large prompts.\nNumerous past works [Jin et al., 2024, Liu et al., 2024c, Zandieh et al., 2024a, Zhang et al., 2023, Liu et al., 2023b, Zandieh et al., 2024b, Chen et al., 2021, Liu et al., 2024a] have attempted to address this issue by compressing the KV cache to achieve sublinear space utilization. These algorithms typically rely on specific structural assump- tions about the key-value token embeddings. For example, the sliding window attention mechanism assumes that each token attends only to the $W$ tokens before it, masking away both the key and value embeddings outside of a short sliding window. This assumption allows for sublinear space utilization by storing only the key and value vectors in the sliding window.\nThis leads to the following natural question:\nIs there a sublinear space algorithm for attention-based autoregressive token generation\non inputs that lack significant structural assumptions?\nIn this paper, we answer the above question in the negative by showing that any algorithm for attention-based token generation must use linear space in the number of tokens to store the KV cache, even if sparsity is present. This result suggests that KV cache compression should probably best be viewed from a data-driven perspective, with the most successful algorithms being the ones that quickly and effectively exploit, learn and adapt to the structure of the data. Beyond this insight, our work also draws on techniques from the extensive literatures on streaming, property testing, sublinear algorithms, communication complexity, and dimensionality reduction, demonstrating their effectiveness and utility in complexity theory, learning, and optimization."}, {"title": "Related Work", "content": "KV Cache Compression A large body of work has recently focused on KV cache compression for Transformer LLMs. These algorithms exploit either structural or learned sparsity patterns in the input embeddings. Zhang et al. [2023], for instance, proposed an eviction scheme for the KV cache that leverages techniques from dynamic sub- modular maximization. Xiao et al. [2024] observed empirically that the initial tokens in the input sequence\u2014which they called attention sinks\u2014often retain high attention weights throughout the entire generation process. This sug- gests a natural way to compress the KV cache by storing these sinks, in addition to a sliding window of recent token embeddings. In a similar fashion, Liu et al. [2023a] observe that tokens with high attention weights tend to remain highly influential to the attention output, a phenomenon they call persistence of importance. The work of Liu et al. [2023b] has also been influential in this direction, proposing the importance of contextual sparsity in Transformer models.\nBeyond the assumptions of sparsity, the work of Zandieh et al. [2024b] leverages clusterability properties of the key embeddings to achieve provably sublinear space in certain parameter regimes. More hardware-oriented methods have also been proposed, such as the works of Duanmu et al. [2024] and Zandieh et al. [2024a] which leverage quantization, and the work of Jin et al. [2024] which examines the I/O complexity of the problem.\nImpossibility Results for Transformers As research towards efficient Transformers progresses, complexity- theoretic techniques have helped shed light on their limitations. The computation of the full attention matrix has been shown to require $\\Omega(n^2)$ time via fine-grained complexity reductions [Keles et al., 2022, Alman and Song, 2024a]. Similar impossibility results have also been shown for the attention gradients [Alman and Song, 2024b]. Furthermore, important insights have been obtained by studying the limitations of Transformers as a computa- tional model through the lens of communication complexity and circuit complexity [Alman and Yu, 2024, Chen et al., 2024]. Finally, the representational and generalization power of Transformers has been theoretically studied as well [Sanford et al., 2024a,b, Li et al., 2023]."}, {"title": "Our Contributions and Results", "content": "We provide a theoretical analysis of the space and time complexity of token generation in Transformers. During inference, an autoregressive Transformer uses the existing sequence of tokens it generated to produce the next to- ken. Formally, given a stream of d-dimensional token embeddings $\\{(q_i, k_i, v_i)\\}_{i=1}^{n}$, the algorithm calculates at each timestep the attention function $\\text{Attn}(q_i, K_i, V_i)$, where $K_i$ and $V_i$ contain all previously seen embedding vectors. The attention function (see Section 2) considers a normalized inner-product similarity metric between $q_i$ and all the vectors in $K_i$, weighting the vectors $v_i$ based on that metric:\n$Z_i := \\text{Attn}(q_i, K_i, V_i) := \\sum_{l=1}^{i} w(q_i, k_l) v_l$, where $w(q_i, k_l) \\propto e^{q_i^T k_l}$\nOur main theorem is a space lower bound for algorithms that approximate the attention function during token generation.\nTheorem 1. Let $\\eta \\in (0, 1)$ be an arbitrary constant and $d > 2$. Let $Z_n := \\text{Attn}(q_n, K_n, V_n) \\in \\mathbb{R}^d$. Any algorithm that can, with probability at least 9/10, for all timesteps $i < n$, produce a $(1 + \\eta)$-approximation O\u017c of $Z_n$ must use at least $(\\text{d} \\cdot \\min\\{n, e^d\\})$ bits of memory.\nWe establish this result by constructing a reduction from a well-known communication complexity problem, utilizing Johnson-Linderstrauss (JL) random projections and the probabilistic method to create a communication protocol. Our proof relies on the malleability of the softmax distribution, as we use it to approximate a \u201cDirac\u201d-like spike on a discretized support set. A similar proof technique was used by Sanford et al. [2024b], where they used the restricted isometry property from the theory of compressed sensing to analyze the representational capabilities of Transformer models.\nTheorem 1 is tight for the low-dimensional regime as well: we show that the SUBGEN algorithm of Zandieh et al. [2024b] achieves optimal space utilization for $d = o(\\log n)$ (Theorem 15 and Corollary 9). Our proof relies on a well-known bound for the covering number of the $l_2$-ball to give a tight characterization of the clusterability of low-dimensional embedding spaces. Our results imply that space-efficient token generation is only possible if the embedding dimension d scales sub-logarithmically with the number of tokens n, a regime in which Transformers unfortunately start to lose their expressivity [Sanford et al., 2024b].\nWe also investigate the impact of sparsity on the space complexity of KV cache compression. First, we formally demonstrate that unstructured sparsity cannot achieve sublinear space utilization (Corollary 16), a surprising ob- servation given the widespread success of sparsity-based algorithms in the literature. Next, we examine the sliding window attention mechanism, a very popular fixed sparsity pattern, in a more general setting, where value vectors outside the sliding window are not assumed to be masked. This scenario extends beyond the typical assumptions in the literature, and no sublinear space algorithm has been proposed for it so far. In this setting, we present a novel algorithm (Algorithm 1) for KV cache compression, achieving sublinear space utilization under mild assumptions. We establish that our algorithm is nearly space-optimal by proving a matching lower bound (Theorem 17).\nFinally, we study the time complexity of non-adaptive streaming algorithms, which are algorithms that pre- decide their data access pattern before seeing the stream. Such algorithms are common in the literature, as they are more straightforward to implement (e.g. fixed sparsity patterns).\nTheorem 2. Let A be a streaming algorithm for attention-based token generation that decides before seeing any data which entries of $Q K^T$ to access. Suppose that A, with probability at least 9/10, outputs for each $i \\in [n]$ an $(1 \\pm \\eta)$-approximation O\u00bf \u2208 Rd of $Z_i := \\text{Attn}(q_i, K_i, V_i)$. Then A must take $\\Omega(nd)$ time to process the last token $(q_n, k_n, v_n)$ in the worst case.\nWe show this result by using an indistinguishability argument, with which we show that sampling from a soft- max distribution requires $\\Omega(n)$ queries to the distribution in the worst case. Such techniques are very widespread in the literature of sublinear algorithms and testing."}, {"title": "Preliminaries", "content": "The Attention Mechanism and KV Caching\nWe can formalize the KV Cache Compression problem as follows: The input is a stream of triples $(q_i, k_i, V_i) \\in (\\mathbb{R}^d)^3$, where $d$ is the embedding dimension. After each stream entry, define the Attention function as:\n$\\text{Attn}(q_i, K_i, V_i) := \\sigma_i(K_i \\cdot q_i)^T \\cdot V_i \\in \\mathbb{R}^d$\nwhere:\n$K_i = \\begin{bmatrix}k_1^T \\\\ k_2^T \\\\ ... \\\\ k_i^T\\end{bmatrix}$\nand\n$V_i = \\begin{bmatrix}v_1^T \\\\ v_2^T \\\\ ... \\\\ v_i^T\\end{bmatrix}$\nare two $i \\times d$ matrices, and $\\sigma_i : \\mathbb{R}^i \\rightarrow \\mathbb{R}^i$ is the softmax function with support $[i]$. The $K_i$ and $V_i$ matrices are called the key-value (KV) cache. The attention function can be viewed as a collection of expected values under suitable softmax distributions [Kratsios, 2021, Singh and Buckley, 2023, Haris, 2024]. Let $D_i$ be the softmax distribution over $[n]$ corresponding to the values $q_i^T k_1, ..., q_i^T k_n$. Then it holds that:\n$\\text{Attn}(q_i, K_i, V_i) = \\mathbb{E}_{l \\sim D_i}[V_l]$\nIf the stream has length n, then it is easy to compute this function exactly for every stream update in $\\mathcal{O}(nd)$ space by storing the $K_i$ and $V_i$ matrices explicitly.\nJohnson-Linderstrauss (JL) Random Projections\nOur analysis makes extensive use of the distributional Johnson-Linderstrauss (JL) lemma (see Appendix B), which is a fundamental result in the theory of dimensionality reduction. Specifically, we utilize the ability of the standard JL transform to approximately preserve inner products between vectors. The following lemma encapsulates this property:\nLemma 3. Suppose we have n points $P_1,..., P_n \\in \\mathbb{R}^n$ such that $||P_i||_2 \\leq 1$ for all $i \\in [n]$. Let $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ be a random mapping defined as $f(u) = \\frac{Au}{\\sqrt{d}}$ where $A \\in \\mathbb{R}^{d \\times n}$ is a random matrix with its entries drawn independently from a standard normal distribution. Then setting:\nd\u2265 \\frac{12 \\ln n}{\\epsilon^2 - \\epsilon^3}\nallows us to guarantee that with probability at least $1 - \\frac{1}{n}$ it holds for all $(i, j) \\in [n]^2$ that:\n|$p_i^T p_j - f(p_i)^T f(p_j)| \\leq \\epsilon$\nCommunication Complexity Fundamentals\nIn the most basic communication model [Rao and Yehudayoff, 2020], Alice and Bob possess two n-bit strings, x and y respectively. They wish to compute a function $f (x, y)$ by exchanging information in a certain number of rounds. They both have access to unlimited computing resources, and their goal is to minimize the total amount"}, {"title": "Sublinear Space for KV Cache Compression is Impossible", "content": "In this section, we prove our main lower bound result. We cover the case of exact computation ($\\eta = 0$) on the high-dimensional ($d = \\Omega(\\log n)$) regime to illustrate the key-ideas of the proof. We later extend this proof with slightly more complicated arguments to the general case.\nTheorem 6. There exists some universal constant $C_u > 1$ such that for $d > C_u \\log n$, any algorithm that can, with probability at least 9/10 produce an output $O_n \\in \\mathbb{R}^d$ such that:\n$O_n = Attn(q_n, K_n, V_n)$\nmust use at least $\\Omega(nd)$ bits of memory.\nProof. We construct a reduction from INDEX, in which Alice holds a bit string $x \\in \\{0, 1\\}^{n \\times d}$ and Bob holds an index $(i, j) \\in [n] \\times [d]$. Alice sends a single message to Bob, whose goal is to output $x_{ij}$ with probability at least 2/3. We know by Theorem 5 that the one-way, randomized communication complexity of this problem is $\\Omega(nd)$. Our goal is to design a protocol for INDEX by using a supposed algorithm A for calculating the attention function that uses S bits of memory. Having that reduction in place, Alice simply communicates these S bits of the algorithm's memory tape to Bob, allowing us to show that $S = \\Omega(nd)$, and thus proving the theorem.\nWe now describe the protocol for INDEX using A and explain the memory lower bound in more detail.\nAlice Alice begins by inserting the following n triples $\\{(q_i, k_i, v_i)\\}_{i=1}^{n}$ of vectors in $\\mathbb{R}^d$ to the streaming algo- rithm A:\n\u2022 $q_1, ..., q_n$ are all the zero vector in $\\mathbb{R}^d$, and they do not affect the final output.\n\u2022 $k_1, ..., k_n \\in \\mathbb{R}^d$ are calculated before the protocol starts (and agreed upon by both Alice and Bob) as d- dimensional projections of the orthonormal basis $e_1 = (1, 0, ..., 0), ..., e_n = (0, 0, ..., 1)$ of $\\mathbb{R}^n$ in a way that approximately preserves orthonormality\u00b2. Specifically, we can invoke Lemma 3 to produce $k_1, ..., k_n \\in \\mathbb{R}^d$ such that with probability at least $1 - \\frac{1}{n}$ it holds for all $i \\neq j$ that:\n$|k_i^T k_j| \\leq \\epsilon$\nand for all $i \\in [n]$ that:\n$|k_i^T k_i - 1| \\leq \\epsilon$\nWe do this by letting $f(x) = \\frac{Ax}{\\sqrt{d}}$ where $A \\in \\mathbb{R}^{n \\times d}$ is a JL random matrix and defining $k_i = f(e_i)$. Crucially, orthonormality is preserved because $d = \\Omega(\\log n)^3$. We resolve the correct value for $\\epsilon$ later in the proof.\n\u2022 $V_1,..., V_n \\in \\mathbb{R}^d$ contain the rows of $x \\in \\{0, 1\\}^{n \\times d}$. In other words, Alice uses $V_n$ to store her input $x$ through A:\n$V_n := x$\nAfter inserting these n triples into A, Alice observes A's execution and sends its memory state, consisting of S bits, to Bob. This allows Bob to continue the execution of A exactly where Alice left off, without, of course, having any additional knowledge of x.\nBob Recall that Bob's input is an index (i, j) into x. In our protocol, Bob only enters a single triple (q, 0, 0) into A. He defines:\n$q := C \\cdot k_i = C \\cdot f (e_i) \\in \\mathbb{R}^d$\nwhere C is a positive value to be determined later. Now, we claim that Bob can recover the value of $x_{ij}$ from the output $o_{n+1}$ that A produces, which is equal to $\\text{Attn}(q, K_{n+1}, V_{n+1}) \\in \\mathbb{R}^d$. We have that:\n$\\text{Attn}(q, K_{n+1}, V_{n+1}) = \\sigma_{n+1}(K_{n+1} \\cdot q)^T \\cdot V_{n+1}$\nStarting with $s := K_{n+1} \\cdot q \\in \\mathbb{R}^{n+1}$, we know with probability at least $1 - \\frac{1}{n}$ that this is a vector in $\\mathbb{R}^{n+1}$ with the property that $s_l$ is close to 0 for $l \\neq i$ and $s_i$ is close to C. Specifically, it holds with probability at least $1 - \\frac{1}{n}$ that:\n$s_l < C\\epsilon$ for $l \\neq i$ and $s_i \\geq C(1 - \\epsilon)$\nThis is also true vacuously for $l = n + 1$ as $s_{n+1} = 0$ by construction. Now, let $\\xi := \\sigma_{n+1}(s) \\in \\mathbb{R}^{n+1}$. We can see that the softmax vector $\\xi$ \u201cspikes\u201d at index i. We can use this spike to read off the value of $x_{i,j}$ via the V matrix. Let us calculate the product $\\xi^T \\cdot V_{n+1}$. This is a vector in $\\mathbb{R}^d$, whose j-th entry is:\n$(\\xi^T \\cdot V_{n+1})_j = \\sum_{l=1}^{n+1} \\xi_l \\cdot x_{l,j} = \\xi_i x_{ij} + \\sum_{l \\neq i} \\xi_l x_{lj}$\nas the last row of $V_{n+1}$ is the zero vector. We can now examine two separate cases:\n\u2022 Case 1: $x_{ij} = 0$. Then we have that:\n$(\\xi^T \\cdot V_{n+1})_j = \\sum_{l \\neq i} \\xi_l x_{lj} = \\frac{\\sum_{l \\neq i} x_{lj} e^{s_l}}{e^{s_i} + \\sum_{l \\neq i} e^{s_l}} < \\frac{\\sum_{l \\neq i} e^{s_l}}{e^{s_i} + \\sum_{l \\neq i} e^{s_l}}$\nThe function $\\frac{x}{x+y}$ is maximized when x is maximized and y is minimized, which allows us to bound:\n$(\\xi^T \\cdot V_{n+1})_j \\leq \\frac{n e^{C \\epsilon}}{e^{C(1 - \\epsilon)} + n e^{C \\epsilon}} := \\delta$\n\u2022 Case 2: $x_{ij} = 1$. Then we have that:\n$(\\xi^T \\cdot V_{n+1})_j = \\frac{\\xi_i + \\sum_{l \\neq i} x_{lj} e^{s_l}}{e^{s_i} + \\sum_{l \\neq i} e^{s_l}} = \\frac{e^{s_i}}{e^{s_i} + \\sum_{l \\neq i} e^{s_l}}$\nSimilarly, the function $\\frac{x}{x+y}$ is minimized when x is minimized and y is maximized, so we have:\n$(\\xi^T \\cdot V_{n+1})_j \\geq \\frac{e^{C(1 - \\epsilon)}}{e^{C(1 - \\epsilon)} + n e^{C \\epsilon}} := \\Delta$\nFor Bob to always be able to distinguish between the two cases, we want to ensure that $\\delta < \\Delta$.\n$\\frac{n e^{C \\epsilon}}{e^{C(1 - \\epsilon)} + n e^{C \\epsilon}} < \\frac{e^{C(1 - \\epsilon)}}{e^{C(1 - \\epsilon)} + n e^{C \\epsilon}} \\Leftrightarrow C > \\frac{\\ln n}{1 - 2 \\epsilon}$\nThus, we can set $C = \\frac{2 \\ln n}{1 - 2 \\epsilon}$ and $\\epsilon = 0.1$ to allow Bob to distinguish between $x_{ij} = 1$ and $x_{ij} = 0$ with probability at least $9/10 - 1/n > 2/3$. Any choice of $\\epsilon \\in (0, 1/2)$ would work. Overall, we conclude that the number of bits communicated, which equals the memory of A, is bounded below by a factor of nd, which concludes our proof."}, {"title": "Lower bound on approximation algorithms", "content": "Now we extend the above result to the approximate computation of the attention function.\nTheorem 8. Let $Z_n := \\text{Attn}(q_n, K_n, V_n)$ and $d = \\Omega(\\log n)$. Any algorithm that can, with probability at least 9/10 produce an output O \u2208 Rd that is a $(1 + \\eta)$-approximation of $Z_n$ for $\\eta \\in (0, 1)$ must use at least $\\Omega(nd)$ bits of memory.\nProof. Our reduction is the same as before, except Bob now relies on the value of O to distinguish between the two cases $x_{ij} = 0$ and $x_{ij} = 1$. When $x_{ij} = 0$ then $O_j \\leq (1 + \\eta) \\delta$, but if $x_{ij} = 1$, then $O_j \\geq (1 - \\eta) \\Delta$, where $\\delta$ and $\\Delta$ are defined as in the proof of Theorem 6. Then we have to guarantee that $(1 + \\eta) \\delta < (1 - \\eta) \\Delta$ or $\\delta < \\frac{1 - \\eta}{1 + \\eta} \\Delta$, which resolves to $C \\approx \\Omega(\\ln n)$, compensating for $\\epsilon = \\frac{0.1}{1 + \\eta}$.\nThe result also similarly extends to additive error approximation. The proof is similar to the one above, and we omit it for brevity."}, {"title": "The low-dimensional regime", "content": "If $d = o(\\log n)$, our preceeding proof breaks down because the JL projection is not be able to preserve the inner products of the all pairs of key vectors with high probability. Our technique, however, still yields the following lower bound:\nCorollary 9. Let $Z_n := \\text{Attn}(q_n, K_n, V_n)$ and $d > 2$ with $d = o(\\log n)$. Any algorithm that can, with proba- bility at least 9/10 produce a $(1 + \\eta)$-approximation O \u2208 Rd of $Z_n$ for $\\eta \\in (0, 1)$ must use at least $(e^d \\cdot d)$ bits of memory.\nSurprisingly, this bound is tight for the low-dimensional regime. We show that KV cache compression algo- rithms that use space sublinear in the number of tokens n do exist in this case. In fact, such an algorithm has already been proposed by the work of Zandieh et al. [2024b]. Their algorithm works by assuming a clusterability structure in the key embeddings, defined as follows:\nDefinition 10 (Clusterability). For a positive integer m and real-valued $\\delta > 0$, a dataset of points $x_1, ..., x_n \\in \\mathbb{R}^d$ is said to be $(m, \\delta)$-clusterable if there exists a partition of the dataset into m clusters $C_1, ..., C_m$ such that for all $i \\in [m]$ and $x, y \\in C_i$ it holds that $||x \u2212 y||_2 < \\delta$.\nThe SUBGEN algorithm of [Zandieh et al., 2024b] uses this clusterability structure to achieve sublinear space complexity. Their main result is the following:\nTheorem 11 (SUBGEN algorithm of [Zandieh et al., 2024b]). Suppose the input is a sequence of tokens $\\{(q_i, k_i, v_i)\\}_{i=1}^{n}$ in $\\mathbb{R}^d$ such that:\n\u2022 The key embeddings $k_i$ are $(m, \\delta)$-clusterable.\n\u2022 $||q_i||_2 \\leq r$ for all $i \\in [n]$.\nThen there exists an algorithm that uses $O(d\\varepsilon^{-2} \\cdot (d + m\\varepsilon^{2}dr \\log n))$ bits of space and outputs an estimate $\\hat{O}_n$ such that with probability at least 0.99 it holds that:\n$||\\hat{O}_n - \\text{Attn}(q_n, K_n, V_n)||_2 \\leq \\varepsilon \\cdot || \\text{softmax}(K_n q_n) ||_2. ||V_n||_2$"}, {"title": "How can structural assumptions help? The case of unstructured sparsity", "content": "Achieving sublinear space for KV cache compression, as shown in Theorem 8, requires structural assumptions about KV embeddings. A common assumption is the sparsity of the attention matrix $Q^T K$, supported by em- pirical observations [Liu et al., 2024c, 2023b, Xiao et al., 2024, Zhang et al., 2023]. However, it remains unclear if sparsity alone suffices or if additional structural assumptions are needed. In the proof of Theorem 6, $Q^T K$ is in- deed extremely sparse, with only one element in its last row having significant weight. This means that the sparsity of $Q^T K$ by itself is not a strong enough assumption for sublinear space. In other words, we have the following corollary:\nCorollary 16. Suppose that an algorithm operates under the promise that a 0.99-fraction of the entries of $Q K^T$ are close to zero. Note that the locations of the non-zero elements are not disclosed. Then, any algorithm that can produce a good approximation of the attention function in this modified streaming setting must use $\\Omega(nd)$ bits of memory.\nThus, we have to complement sparsity with yet another structural assumption, which is the route taken by many recent works in the literature. Specifically, they assume knowledge of the sparsity pattern (the locations of the non-zero elements) of $Q^T K$ to reduce the memory requirements.\nSliding window attention revisited The most common such sparsity pattern is the sliding window attention mechanism. In sliding window attention, we do not compare $q_n$ with every prior key vector $k_1, ..., k_n$ but only with a window of W prior key vectors: $k_{n\u2212W+1}, k_{n\u2212W+2}, ..., k_n$. Let $S_n := K_n \\cdot q_n \\in \\mathbb{R}^n$ and $h(S_n) := S_n \u25e6 M$ where \u25e6 is element-wise multiplication, $M \\in \\mathbb{R}^n$ is a masking vector such that $M_i = 1\\{i > n - W\\}$ and $W$ is the sliding window width. Then we define sliding window attention as:\n$Attn_W(q_n, K_n, V_n) := \\sigma(h(S_n)) \\cdot V$\nIntuitively, a benefit of this definition is that even though we only attend to W key vectors, we still have access to all value vectors, meaning that every token embedding is considered in the output.\nHowever, with this redefinition of sliding window attention, it is no longer clear whether we can calculate it in sublinear space. Since our softmax distribution is supported on every index in [n], every row vector of V has a non- zero probability of influencing the output, implying that we might be required to keep all of V in memory. It turns out we can indeed achieve a space complexity of $\\mathcal{O}(dW)$ even with this definition of sliding window attention.\nThe Algorithm Our algorithm is shown as Algorithm 1. For the first W token embeddings, we calculate atten- tion directly, while also storing the key and value vectors. For the remaining embeddings, we cache the last W key and value vectors, while also maintaining a uniformly sampled value-vector $v_s$ from outside the sliding window. We can achieve this using reservoir sampling (Section 2). We sample from the sliding window softmax distribu- tion by examining two cases. In the first case, we sample an index inside the sliding window using our explicitly built cache. In the second case we sample an index outside the sliding window by using our reservoir sample. As a result we obtain an unbiased estimator of the sliding window attention function, which we can boost using the median-of-means technique. The analysis can be found in Appendix F."}, {"title": "A Tight Lower Bound for Sliding Window Attention", "content": "We can show that the space complexity of $\\mathcal{O}(dW)$ is tight for sliding window attention, even with our updated definition. We include the proof in Appendix G.\nTheorem 17. For $d = \\Omega(\\log n)$, any algorithm that can, with probability at least 9/10 approximate sliding window attention must use at least $\\Omega(dW)$ bits of memory."}, {"title": "Non-Adaptive Lower Bounds for the Time Complexity of Token Generation", "content": "Definition 18. We call an algorithm non-adaptive if the data accesses / queries it performs are pre-decided (either deterministically or through a randomized procedure) before the data arrives.\nTheorem 19. Let $Z_n := \\text{Attn}(q_n, K_n, V_n)$ and suppose A is a non-adaptive streaming algorithm for token gener- ation that calculates the attention function in an online manner and outputs a $(1 + \\eta)$-approximation O \u2208 Rd of $Z_n$ with probability at least 9/10 for $\\eta \\in (0, 1)$. Then A must take $\\Omega(nd)$ time to process the last token $(q_n, k_n, V_n)$.\nProof. The full proof can be found in Appendix H, but we provide an outline here. We start from the expectation- based reformulation of self-attention (Equation 3):\n$\\text{Attn}(q_n, K_n, V_n) = \\mathbb{E}_{l \\sim D_n}[V_l]$\nWe can treat A as an algorithm that makes queries to $D_n$ to estimate this expectation. Each query takes time $\\Theta(d)$ just to calculate the corresponding score $q_n^T k_l$, so it suffices to show that $\\Omega(n)$ queries are required to obtain a good approximation with high constant probability.\nWe use the technique of indistinguishability. We construct a family of stream instances $\\mathcal{H} = {H_1, ..., H_n}$ and an instance \u03c3 such that the distribution $D_n$ in any $H_i$ differs from the distribution $D_n$ in \u03c3 in at most one position. Thus, if A makes o(n) non-adaptive queries to $D_n$, then with probability at least 1/poly(n) it will not be able to distinguish between \u03c3 and a randomly chosen $H_i$. However, our construction ensures that the attention function differs significantly between \u03c3 and $H_i$, contradicting the assumption that A approximates the attention function well."}, {"title": "Conclusion", "content": "In this paper, we performed a theoretical study of the space and time complexity of approximating the attention function in Transformer LLMs during inference. We demonstrated that linear memory is essentially required, im- plying that efficient KV cache compression must make structural assumptions about the attention function inputs. Several future directions arise in the aftermath of our investigation, such as, but not limited to, the following:\n1. Does sparsity or structure in general arise naturally when learning Transformer models? This seems to hap- pen in practice, but a theoretical study of such phenomena is still missing.\n2. Can sublinear space be achieved in the average case, under distributional assumptions on the input embed- dings?\nOverall, our work contributes to the growing theoretical analysis of deep learning models, which, despite being relatively poorly understood, are widely used in modern applications."}, {"title": "A Sublinear Space Algorithm for the One Dimensional Case", "content": "In this section, we provide an algorithm that uses sublinear space to calculate the self attention function for to- ken generation"}]}