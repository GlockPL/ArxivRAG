{"title": "Why the Agent Made that Decision: Explaining Deep Reinforcement Learning with Vision Masks", "authors": ["Rui Zuo", "Zifan Wang", "Simon Khan", "Garrett Ethan Katz", "Qinru Qiu"], "abstract": "Due to the inherent lack of transparency in deep neural networks, it is challenging for deep reinforcement learning (DRL) agents to gain trust and acceptance from users, especially in safety-critical applications such as medical diagnosis and military operations. Existing methods for explaining an agent's decision either require to retrain the agent using models that support explanation generation or rely on perturbation-based techniques to reveal the significance of different input features in the decision making process. However, retraining the agent may compromise its integrity and performance, while perturbation-based methods have limited performance and lack knowledge accumulation or learning capabilities. Moreover, since each perturbation is performed independently, the joint state of the perturbed inputs may not be physically meaningful. To address these challenges, we introduce VisionMask, a standalone explanation model trained end-to-end to identify the most critical regions in the agent's visual input that can explain its actions. VisionMask is trained in a self-supervised manner without relying on human-generated labels. Importantly, its training does not alter the agent model, hence preserving the agent's performance and integrity. We evaluate VisionMask on Super Mario Bros (SMB) and three Atari games. Compared to existing methods, VisionMask achieves a 14.9% higher insertion accuracy and a 30.08% higher F1-Score in reproducing original actions from the selected visual explanations. We also present examples illustrating how VisionMask can be used for counterfactual analysis.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement Learning (DRL) is a powerful technology in machine intelligence, widely used for many applications, including ChatGPT (Brown et al. 2020; Achiam et al. 2023) and Alpha Go Zero (Silver et al. 2017). It enables effective end-to-end training of decision models that map an agent's observations to actions, aiming to maximizing the long term expected rewards. However, understanding a DRL agent's decision-making process is challenging, due to the inherent lack of explainability in the high-dimensional, non-linear structure of its underlying Deep Neural Network (DNN). The lack of transparency undermines human users' trust and acceptance, particularly in safety-critical scenarios, such as medical diagnosis and military operations. To ensure wide acceptance and efficient use of this technology, it is crucial to provide accurate and human-interpretable explanations. These considerations motivated the recent interests in Explainable Reinforcement Learning (XRL).\nSome existing works integrate explanation generation into the agent model and train them together. We refer to these as model-based approaches. For example, Interpretable DQN (i-DQN) (Annasamy and Sycara 2019) implements a deep Q-network with an attention mechanism, using the attention weights between state and action embeddings to represent their importance. Saliency-guided Q-networks (SGQNs) (Bertoin et al. 2022) calculate the importance of inputs as a derivative of the Q-function and train the critic model to both minimize the value prediction error and maximize the fidelity of the explanation. Some other works moved away from DNN and adopt special policy architectures that are inherently more interpretable (Liu et al. 2018; Verma et al. 2018; Bastani, Pu, and Solar-Lezama 2018). However, these models usually experience performance loss compared to the DNN-based policy model due to the limitations imposed by the special structure. Those model-based approaches cannot provide explanations for a trained DRL agent, unless they retrain the agent using their specific models, which is prohibitively expensive. And the retrain process may compromise the agent's performance and integrity as it strives to maintain a balance between the optimality and explainability.\nOne of the most widely used techniques to provide explanations for machine learning models is the attribution-based method, which assigns credits to input features based on their contribution to AI agent's decisions. This technique was originally developed to explain image classification models in computer vision. It generates saliency maps by perturbing the input image to reveal their contributions to the classification process without modifying the classifier itself (Dabkowski and Gal 2017; Petsiuk, Das, and Saenko 2018; Ribeiro, Singh, and Guestrin 2016). Inspired by this, several works have explored to use attribution-based technique to explain DRL models while treating the agent as a black-box. Greydanus et al. (2018) perturb the input features using Gaussian blur at fixed intervals and measure how the perturbation changes the action probability. The difference in action probability before and after perturbation are used to generate a saliency-map for the importance of each input feature. Similarly, Iyer et al. (2018) and Puri et al. (2020)"}, {"title": "VisionMask", "content": "address the aforementioned challenges, we propose VisionMask, a model trained end-to-end to generate attribution-based explanations for vision-based DRL without perturbation or altering the agent. VisionMask ouputs a saliency map that highlights the critical regions in the agent's visual input responsible for triggering the agent's action. The saliency map reveals the importance of each pixel in the decision making process, and will also be referred to as importance map or importance mask in the rest of the paper. VisionMask is trained using self-supervised contrastive learning. No human generated label is required. The training of VisionMask does not alter the agent model, thereby preserving the agent's performance and integrity. The (fixed) policy model of the DRL agent is integrated into the training pipeline to generate the loss, and involved in the backpropagation to pass the gradient to the mask generation model. Therefore, each VisionMask model is trained to explain a specific DRL agent. The training process is agent agnostic, meaning that it works with any agent with a policy model that maps visual input to actions. VisionMasks's learnable capability allows it to accumulate knowledge from previous experiences to improve future explanations. During testing time, VisionMask does not need to access agent's internal state to generate the importance mask. The mask is generated by a forward pass of the VisionMask model.\nAs shown in Table 1, VisionMask accurately identifies the critical regions in the visual input that explains the agent's action. Beyond providing precise explanations directly from the visual input, VisionMask also facilitates counterfactual analysis. By selectively activating or deactivating different critical regions, meaningful counterfactual scenarios can be generated and assessed. This is similar to the perturbation-based approach, however, at a much higher level.\nThe contribution of our work can be summarized as the following:\n\u2022 We present VisionMask, a novel attribution method for explainable visual reinforcement learning that generates action-specific importance masks. Generated using a single forward pass, the mask does not only highlight the regions in the visual input that explain the agent's behavior but also facilitates counterfactual analysis.\n\u2022 VisionMask is model-agnostic in that it works with any vision-based DRL agent without modifying the agent model.\n\u2022 VisionMask is trained using self-supervised contrastive learning. No human intervention is needed for data labeling.\n\u2022 The fidelity of the explanation is measured by the agreement between the agent's actions generated from the original visual input versus from the masked visual input. We quantify this agreement using accuracy, precision, recall and F1-score. In average, VisionMask outperforms other attribution-based explanation models by 30.08% on F1-score and 14.9% on insertion accuracy.."}, {"title": "Background and related work", "content": "The nested structure and non-linear operation of DNN make it challenging for humans to understand how the outputs are derived from inputs. Some existing works address this challenge by explaining model outputs through input attributions (Dabkowski and Gal 2017; Petsiuk, Das, and Saenko 2018; Ribeiro, Singh, and Guestrin 2016; Fong and Vedaldi 2017). For example, Randomized Input Sampling for Explanation of Black-box Models (RISE) (Petsiuk, Das, and Saenko 2018) is a perturbation-based approach that explains neural image classifiers by applying randomly generated masks to the input image and assessing their impact. Local Interpretable Model-agnostic Explanations (LIME)(Ribeiro, Singh, and Guestrin 2016) tries to explain local instances by approximating them within a nearby vicinity using a linear decision model, where the explainable elements are super-pixels (i.e., small image regions of similar pixels).\nBuilt on top of the DNNs, the lack of transparency of the DRL agents (Samek, Wiegand, and M\u00fcller 2017; Ras et al. 2022; Angelov and Soares 2020) undermines humans trust and hinders their adoption. Several studies have addressed this challenging problems. According to (Qing et al. 2022), existing efforts can be categorized into four main approaches: model explaining, reward explaining, task explaining, and state explaining.\nModel explaining relies on inherently interpretable model architectures or auxiliary reasoning mechanisms to generate explanations (Topin et al. 2021; Jiang and Luo 2019; Landajuela et al. 2021). However, these self-explanatory or symbolic models often suffer from decreased performance compared to state-of-the-art neural network based RL policies and may lack the representational power needed to learn more complex policies. Reward explanation typically involves using explicitly designed explainable reward functions to generate explanations (Juozapaitis et al. 2019; Ashwood, Jha, and Pillow 2022). Task explanation considers a policy as a composition of sub-task and action sequences, explaining the behavior in terms of relationships among sub-tasks (Shu, Xiong, and Socher 2017; Sodhani, Zhang, and Pineau 2021). These approaches often assume the existence of a reward decomposition scheme or a predefined sub-task partition mechanism, which may not hold true in all RL environments.\nState explanations are based on agent observations from the environment. This approach determines the significance of explainable elements within a state observation in relation to action selection or reward determination. An explainable element in the state could be a small region in the visual input or semantic features in the environment state. The proposed VisionMask falls into this category.\nState explaining methods can further be divided into three categories: attention-based, Shapley value-based and perturbation-based mechanisms, which are detailed below. Attention is a common approach used for explainable RL in several existing works (Peng, Riedl, and Ammanabrolu 2022; Xu et al. 2020; Annasamy and Sycara 2019). However, similar to i-DQN (Annasamy and Sycara 2019), these methods cannot explain a given DRL model because their attention model must be trained alongside the agent model. Shapley value (Shapley et al. 1953), a concept from game theory, has also been introduced into XRL. SVERL (Beechey, Smith, and \u015eim\u015fek 2023) provides a theoretical analysis on how Shapley values can be applied to explain value and policy network in DRL. However, this approach is still in its early stages and assumes prior knowledge of the transition model of the environment, which is not realistic for realistic applications.\nPerturbation-based methods (Greydanus et al. 2018; Iyer et al. 2018; Puri et al. 2020) compute saliency maps of the input features by comparing the action probability or value function before and after perturbation. Greydanus et al. (2018) perturbed the state with Gaussian blur at fixed intervals, while Iyer et al. (2018) identified objects and perturbed them with a constant gray value. Puri et al. (2020) improved on Greydanus et al. (2018) by removing the effect of perturbations from other irrelevant actions. As discussed in section 1, the performance of these techniques is constrained by predefined perturbation rules and the lack of knowledge accumulation. Furthermore, there is no guarantee that the perturbed input remains physically meaningful."}, {"title": "VisionMask", "content": "This section presents VisionMask, an innovative agent-agnostic framework for XRL. The primary goal is to produce action-wise importance-mask that attribute explainable elements in the current state to each possible action. The definition of explainable elements may vary across different applications. However, in this paper, we focus on identifying explanatory pixel regions in the agent's visual input."}, {"title": "Problem Formulation", "content": "We approach the task of VisionMask as an imitation learning problem based on Behavioral Cloning (BC) (Bain and Sammut 1995; Ross, Gordon, and Bagnell 2011). Formally, we define the environment as a Markov Decision Process (MDP) {S, A, P, R, \u03b3}, where S represents the state space; A denotes the action space; the state transition function P: S \u00d7 A \u2192 A(S) depicts the transition between states based on actions, where \u2206(S) represents the set of probability distributions over states; the reward functionr: S \u00d7 A \u2192 R provides the immediate reward for state-action pairs; and \u03b3\u2208 [0,1] and \u03c0 : S \u2192 \u2206(A) represent the discount factor and policy. Return G is defined as G = \u2211k=0YkRk+1, and the expected cumulative reward of a policy \u03c0is \u0395\u03c0 [G] = \u0395\u03c0[\u03a3\u03ba=0Rk+1], where the expectation is taken with respect to the initial state distribution, transition probabilities, and action probabilities determined by \u03c0.\nVisionMask operates on a given pre-trained expert policy \u03a0\u0395 such that\n\u03a0\u0395 \u2248 \u03c0* arg max E [G],\n\u03c0\nwhere \u03c0* is the optimal policy. We can obtain a dataset of expert demonstrations DE = {(Si,\u03c0\u03b5(si))}=1 consisting of N state-action pairs, from trajectories sampled while executing TE in the environment. Our goal is to learn an explainer fe that minimizes the loss\n0*\n= arg min \u2211 L(a,s,0),\n\u03b8 (s,a) EDE\nwhere L is the training loss function to be discussed in section 3.3. The explainer function fo : S \u2192 [0,1]K\u00d7ds (with ds representing the state dimension and K the number of candidate actions) provides the attributions of each action to each feature in the state s. In the case of visual DRL, s is the visual input of the agent and d, is the number of pixels in s, i.e., ds = W \u00d7 H, with W and H representing the width and height of the visual input. The value of each element in the attribution is bounded within the range [0, 1], with the (i, j)th element indicates the jth pixel's attribution to the ith action. This attribution will then be overlaid onto the visual input, masking unimportant pixels while retaining important ones. The overlaid visual input will be analyzed by the policy model again. We will update parameters 0 of explainer f so that the overlaid visual input always produces the same action as the original one. The expert policy TE remains frozen during the training process of fe, meaning its parameters are fixed."}, {"title": "Architecture", "content": "As shown in Fig. 1, the expert dataset DE is collected using the expert policy \u03a0\u0395. From this dataset DE, state-action pairs (si, ai) are sampled. Subsequently, the explainer fo takes the state si as input and produces a set of action-wise masks M = {mo, m\u2081,....,MK-1}, where K denotes the number of candidate actions and mi \u2208 [0,1]W\u00d7H. A mask value mi[x, y] near 1 indicates that the pixel (x, y) has significant contributions to the corresponding action i, whereas a value near 0 denotes irrelavance. This process shares similarities with image segmentation, where each pixel is attributed to an object or background. Hence, we structure the explainer fe akin to the widely used image segmentation model, DeepLabV3 (Chen et al. 2017), but retrain it with the expert demonstrations \u03a0\u0395. \u03a4\u03bf make sure that the output masks are bounded to the range [0, 1], a sigmoid function is applied at the output of fe .\nFollowing (Stalder et al. 2022), we partition the masks in M into two sets: the active set and the inactive set. Unlike (Stalder et al. 2022), our active set has only one mask, ma where a is the action selected by expert policy \u03a0\u0395 All the other masks, mi, where i \u2260 a, belong to the inactive set. Each mask mi contains the attributions for the current agent state that explain the corresponding action i. The explainer will be trained to produce masks for pixels that contribute the most to the policy's action selection.\nWe aggregate the masks in the inactive set into a non-target mask n by taking the maximum value pixel-wise. We also calculate a complement mask ma = 1 ma representing the less important regions for the selected action a. Then the masks ma and ma are overlaid onto the original state s to generate two masked states sm and sm using the following overlay function:\nSm = s\u2299 ma + r \u2299 (ma), sm = s \u2299 ma + r \u2299 (ma)\nwhere is Hadamard Product and r is a reference value. The final step is to feed these masked states sm and sm through the expert policy \u03a0\u0395 to generates action distributions p and p, respectively:\np = TE (Sm), p = TE(Sm).\nThe model fe is optimized so that p matches the action distribution chosen for the unmasked state s, while p does not. It is worth mentioning that the agent may be sensitive to the reference value in the overlay function. Numerous options exist for the reference value, such as setting the pixel to"}, {"title": "Training Loss", "content": "To train the explainer to capture the most important regions in the state, we carefully designed the training loss function Las follows:\nL(s, a, 0, n) = Lbc(s, a) + deLe(s, a) + davgLavg(s, a, n)\n+ AsmoothLsmooth (s, a, n) + AL2||0||2,\nwhere Lbc(s, a) is Behavior Cloning loss, Le(s, a) is entropy loss, Lavg(s, a, n) is the mask average loss, Lsmooth (s, a, n) is the smoothness loss, ||0||2 is L2 regularization and de, Aavg, Asmooth, AL2 are regularization hyperparameters.\nBehavioral Cloning loss Lbe. Our goal is to train the explainer to select regions in the visual input that that are sufficient and necessary for the agent to choose action a. Hence, we design the behavior cloning loss as cross entropy loss between the action distribution p predicted for the overlaid state sm and the target a chosen for the original state s:\nLbc(s, a) =\n1\nK\nK\nk=1\n[k = a] log(p[k]).\nHere [k = a] denotes the indicator function which returns 1 if k is the same as label a, and 0 otherwise.\nNegative Entropy loss Le. To ensure that the visual input regions selected by ma is sufficient and necessary for the agent to make decisions, we also need to make sure that the unselected region, i.e., sma, does not provide useful information for action selection, hence the action distribution p should follow a uniform distribution. Motivated by this rationale, we define negative entropy loss regarding m as the following:\nLe(s, a) =\n1\nK\nK\nk=1\np[k] log p[k].\nMask average loss Lavg. A low effort way to minimize Lbe and Le is to include all pixels in the importance mask, ma, and no pixel in the complement mask, ma, which obviously is not a valid solution. We need to ensure that each importance mask only consists a small number of crucial pixels. Thus, we define Lavg as follows:\n1\n1\nLavg (s, a, n) =\nWH\u03a3[i, j] +\nWH\u03a3[i, j],\ni,j\ni,j\nwhere, n represents the non-target mask aggregated from the inactive set. By minimizing the mask average loss, we limit the number of pixels selected by the masks.\nSmoothness loss Lsmooth. Finally, we minimize the Variation between adjacent pixels to generate smoother masks:\nTV(x)\n=\n1\nW\u00d7H\u2211(x[i, j]-x[i + 1, j]|\ni,j\n+ [x[i, j] \u2013 x[i, j + 1]])\nLsmooth (s, a, n) = TV(m) + TV(n)\nThe overall architecture of VisionMask is given in Fig. 1. The loss will backpropagate through the frozen expert model TE to train the explainer model fo."}, {"title": "Experiments", "content": "We evaluated VisionMask across four different environments and compared its performance against four baseline methods.\nEnvironment Selection. Four agent-centered RL environments were selected to showcase the effectiveness of VisionMask: Super Mario Bros (SMB)(Kauten 2018), and the Atari games Seaquest, Enduro, and MsPacman. We select these environments following existing works (Puri et al. 2020; Greydanus et al. 2018).\nBaseline Selection. We compared VisionMask's performance against four perturbation-based explainer, LIME (Ribeiro, Singh, and Guestrin 2016) RISE (Petsiuk, Das, and Saenko 2018), Greydanus et al. (2018) and SARFA(Puri et al. 2020). For LIME, we used the original public implementation with the default configuration available from (Ribeiro 2017). For RISE, we adopt the public implementation from torchray (Fong, Patrick, and Vedaldi 2019). For Greydanus et al. (2018) and SARFA, we used the respective public implementation provided by the authors. To ensure a fair comparison, we did not include any explainers that require retraining the agent model.\nExpert Policy. For reproducibility, we did not train the expert policies ourselves; instead, we used publicly available implementations. In the SMB environment, We apply the trained PPO (Schulman et al. 2017) policy from (Nguyen 2020). In the Atari environments, we used three trained PPO policies from Stable-Baselines3 (Raffin et al. 2021) ."}, {"title": "Assessment of Explanation Fidelity", "content": "We quantify fidelity using four metrics: accuracy, precision, recall, and F1-score. Fidelity measures the agreement between the agent's action selected for the original visual input and the action selected for the masked visual input. In this context, we define the accuracy as the following:\nAccuracy =\n1\nDtest\nDrest \u03a3 [\u03a0\u0395(i) = \u03a0\u0395(8)],\ni=1\nwhere s = simi+r\u2299mi, [\u00b7] again denotes the indicator function, mi is the target mask for action a\u017c under state si, mi = fo(si)[ai].\nPrecision and recall are averaged over actions as the following: Precision =\n1K\n\u0397 \u03a3a=1\nTPa\nTPa+FPa, Recall =\n1K\n\u03a3a=1\nTPa\nTPa+FNa, and F1-score = 2 \u00d7 Precision \u00d7 Recall\nPrecision+Recall."}, {"title": "Assessment of Effectiveness", "content": "An effective explainer should precisely identify the exact input regions that contribute to the expert decision, without including irrelevant areas. To quantify the effectiveness of explanations, We adopt the insertion and deletion metrics from RISE (Petsiuk, Das, and Saenko 2018). The rationale behind these metrics is as follows: If certain pixels in the visual state are crucial to the action (i.e., the true cause of the action), deleting them from the state or inserting them into the background should result in a corresponding decrease or increase in the probability of the specific action. By plotting the relationship between the probability of the target action and the number of pixels added or removed, we obtain an insertion curve and a deletion curve. The Area Under the Curve (AUC) of those plots represents the effectiveness of the explanation. A higher AUC for the insertion curve and a lower AUC for the deletion curve stand for more effective explanation. The order of insertion and deletion affects the result. We sort the pixels based on their values in the importance map and insert or delete them in descending order of their importance. Figure 2 shows an example of insertion and deletion curves with corresponding AUC values displayed at the top right corner. See Appendix C for the pseudocode of each metric.\nIt is worth noting that the effectiveness varies as different number of pixels are inserted or deleted from the original input. Instead of showing the actual curve for each explainer under different games, we report the AUC under 25%, 50%, and 100% level of insertion and deletion. The results are given in Table 3. VisionMask outperforms all other methods across all environments, with an average insertion score that is 14.9% higher and a deletion score that is 8.8% lower compared to the other methods."}, {"title": "Counterfactual Explanations", "content": "By removing the most important regions identified by VisionMask from the visual input, we can create scenarios for counterfacutal analysis. Table 3 provides three examples of this approach. In the first example, VisionMask correctly identified the Piranha Plants as the reason the agent chose to move left. After removing the Piranha Plant by replacing the highlighted pixels with reference value and run the modified image by the agent again, we observed that the agent's action changed to jumping right, as the threat from the Piranha Plant no longer existed. In the second example, VisionMask correctly identified the edge of the road and a car in front as the reason why the agent chose to move left (with current speed). After we removed the car in front the agent chose to move left with acceleration. In the third example, VisionMask identified the shark in the bottom left as the cause of the downward and fire action. When the corresponding shark was removed, the agentno longer chose to fire. Appendix H, I and J contain more examples. We believe such explanations can further help users to understand the RL agent's behavior."}, {"title": "Conclusion", "content": "We presented VisionMask, an agent-agnostic DRL explanation model trained in self-supervised contrastive learning. VisionMask generates explanations with higher fidelity and better effectiveness compared to existing attribute-based methods. It is our future plan to extend this approach to multi-modality input and couple the visual explanation generated by the VisionMake with other information such as agent's long-term goals and future rewards."}, {"title": "Technical Appendix", "content": "Meanings of actions across four environments\nIn Table 4, we present the list of actions and their corresponding meanings for the four environments evaluated in the experiment section.\nReference values used across different environments\nIn our experiment, we found that the PPO agents we adopted from Stable Baselines3 (Raffin et al. 2021) and Mario-PPO (Nguyen 2020) are sensitive to reference values. We tried using a constant value (zero) and blur as reference values but did not obtain clear and reasonable importance masks. However, using reference values that are visually similar to the environment background gave us better results. In Figure 4, we list the reference values we used in each environment. Specifically, for the SMB and Atari Enduro environments, we use the background color of the content in each state as the reference value. Note that the reference value for different states may vary, as the background content of each state can be different. For the Atari Seaquest environment, since all the states share the same background, we use a single reference value for each state, which combines the empty sea background and the oxygen bar background. For the Ms. Pacman environment, we simply used a constant value of the background color blue as the reference value.\nAlgorithms for insertion and deletion metrics\nAlgorithms 1-2 present the pseudocode for evaluating the insertion and deletion metrics of (Petsiuk, Das, and Saenko 2018).\nAlgorithm 1: Evaluate insertion metric\n1: Input: target mask m, state s, action a, expert policy\nTE, fraction of pixels inserted a, Number of pixels N\n2: Initialize m' \u2190 zeros(m), p \u2190 \u00d8\n3: I \u2190 indices(sort(flatten(m)))\n4: for k\u2190 0 to [aN] \u2013 1 do\n5:\n(x, y) I[k], m' [x, y] \u2190 1.0\n6:\np \u2190 p\u222a \u03c0E(s\u2299 m' + r \u2299 (1 \u2212 m'))\n7: end for\n8: return AUC(p)\nAlgorithm 2: Evaluate deletion metric\n1: Input: target mask m, state s, action a, expert policy\n\u03a0\u0395, fraction of pixels deleted a, Number of pixels N\n2: Initialize m' \u2190 ones(m), p \u2190 \u00d8\n3: I \u2190 indices(sort(flatten(m)))\n4: for k\u2190 0 to [N] - 1 do\n5:\n(x, y) \u2190 I[k], m'[x, y] \u2190 0.0\n6:\np \u2190 p\u222a\u03c0\u03b5(s\u2299 m' + r \u2299 (1 \u2212 m'))\n7: end for\n8: return AUC(p)\nObject importance percentage calculation\nAttention percentages paid to objects are calculated by summarizing the attention map on target objects. Existing object detection models, trained on real-world images, perform unsatisfactorily in gaming environments. Furthermore, most object detection models use bounding boxes to detect objects, which are not suitable for fine-grained attention percentage calculations. To address these challenges, we use a segmentation model as an alternative approach. Specifically, we employ Segment Anything (SAM) (Kirillov et al. 2023), which automatically generates masks indicating potential target objects, denoted as mSAM \u2208 {0,1}NSAM \u00d7W\u00d7H, where NSAM is the number of potential target objects. We then multiply the vision mask mi \u2208 {0,1}W\u00d7H of the ground truth action i element-wise with each target object mask MSAM \u2208 {0,1}W\u00d7H to obtain attention percentages p\u2208 R for all potential target objects. By re-ranking p and filtering out values below a predefined threshold, we derive the attention percentages on target objects. Finally, we manually label the target objects. Note that we preprocess the vision mask mi for attention percentage calculation by setting values below the average to zero. This preprocessing step filters out objects that received less attention but still show high attention percentages due to their large size, such as the background sky. The hyper-parameters used in SAM automatic mask generator: points_per_side = 64, pred_iou_thresh = 0.7, stability_score_thresh = 0.8. The threshold we set on attention percentage calculation is 0.05.\nTime efficiency\nTo assess the time efficiency of mask generation, we compare the total time taken for mask generation using VisionMask against that of two baseline methods on testing datasets from four environments. Note that the time we measure does not include the training time for VisionMask. We can see that LIME and RISE are slower than VisionMask. All the experiments are conducted on A40 GPU.\nExperimental details\nDataset details\nRecall from Section 3.1 that we collected the expert dataset TE with the agent being explained as expert in four envrionments. In the SMB environment(Kauten 2018), \u03c0Mario comprises a total of 9,692 state-action pairs. In Atari Enduro, Seaquest and MsPacman (Brockman et al. 2016) environment, \u03c0Atari includes 10,000 total state-action pairs. We"}]}