{"title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation", "authors": ["Yiyi Chen", "Qiongkai Xu", "Johannes Bjerva"], "abstract": "With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that ALGEN attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against ALGEN, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as OpenAI's GPT series (Radford, 2018; Radford et al., 2019; Brown et al., 2020; OpenAI et al., 2024) and Claude from Anthropic, have become essential across a wide range of applications, extending far beyond natural language processing (NLP). These models are deeply integrated into people's daily lives and business operations, e.g., powering search engines, virtual assistants, and content generation. A critical component enabling the efficiency of these applications is vector databases (DB), which allow for fast and scalable retrieval and processing of high-dimensional vector representations. Companies such as Pinecone and Weaviate, provide vector DB services and build AI services on top of them. In a recent Google whitepaper on generative AI agents (Wiesinger et al., 2025), vector DBs are considered one of the essential components enabling such agents through external sources. Retrieval-augmented generation (RAG) is another common use case in leveraging vector DBs to generate more diverse and factually grounded responses (Lewis et al., 2020).\nWhile applications such as these benefit from vector DBs, the potential security and privacy risks permeate the process. Fig. 1 illustrates three separate threat scenarios where a vector DB can be exploited to expose private and sensitive information: (I) a malicious user exploits the model API to extract embeddings to train an attack model; (II) when an Al agent interacts with the DB, a malicious attacker can compromise the communication channel to intercept sensitive data; (III) a misconfigured vector DB may expose private data, either through access vulnerabilities or insider threats.\nThe attacker can train an attack model (e.g. an embedding-to-text generator) to reconstruct text from intercepted embeddings, which might contain sensitive, private, or proprietary information. This so-called embedding inversion attack poses significant risks and potential harm.\nPrevious work has demonstrated the feasibility and detrimental effects of inversion attacks (Song and Raghunathan, 2020; Li et al., 2023). However, either a massive amount of intercepted (victim) embeddings and their texts are required for training an attack model (Morris et al., 2023), or the attack is conducted under white-box settings, where the model parameters and architecture are known to the attacker (Song and Raghunathan, 2020). Moreover, inversion attacks have been demonstrated to threaten multiple languages, especially lower-resource ones (Chen et al., 2024a,b).\nWe propose a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), to first align victim embeddings to the attack embedding space, and then reconstruct text from the aligned embeddings using the generative attack model. In contrast to previous work, we investigate inversion attacks using a small handful of samples - e.g., a Rouge-L score of 10 can be reached by leveraging a single leaked data point. Our work makes the following main contributions:\n\u2022 We propose and verify the effectiveness of a novel few-shot inversion attack, which drastically reduces the cost and complexity of such attacks, making them plausible real-world threats.\n\u2022 We demonstrate the transferability of the inversion attack across various languages, models and domains.\n\u2022 We examine several established defense mechanisms, none of which are successful mitigation strategies for this attack, highlighting the new security and privacy vulnerabilities of embeddings in vector databases."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Textual Embedding Inversion Attacks", "content": "Textual embedding inversion attack aims to learn the inversion function that reconstructs the original textual inputs given their embeddings. Song and Raghunathan (2020) demonstrates that it is possible to recover over half of the input words from a text embedding without preserving their order. Li et al. (2023) starts to treat the inversion attacks as a generation task, generating coherent and contextually similar sentences compared to the original text. Morris et al. (2023) adopts an iterative approach to train the attack model by parameterizing attack and hypothesis embeddings based on decoded text from the previous step, which results in exact matches between original and reconstructed text in certain settings. Huang et al. (2024) implements adversarial training to align victim embeddings to attack embeddings, making them not differentiable. Chen et al. (2024a,b) expand inversion attacks beyond English embeddings to multilingual spaces, leveraging linguistic typology to investigate inversion attack performance, finding that certain languages are particularly vulnerable.\nHowever, all existing works in embedding inversion attacks require an enormous amount of data leakage to train the generative attack models, such as 100k samples for Li et al. (2023), 1-5 million for Morris et al. (2023); Chen et al. (2024a,b) and 8k for Huang et al. (2024). In comparison, our proposed approach ALGEN does not require training an attack model on leaked/private embeddings, and the inversion attack succeeds with few leaked data, we additionally experiment on multiple languages."}, {"title": "2.2 Embedding Alignment", "content": "Embedding alignment has continuously progressed in NLP with the development of embedding representations and LLMs. In the early stages, a common approach involved independently training monolingual word vectors (Mikolov et al., 2013b) and then learning a mapping between source and target language embeddings using a bilingual dictionary (Mikolov et al., 2013a; Smith et al., 2017; Artetxe et al., 2017). When this mapping is restricted to an orthogonal linear transformation, the optimal word pair alignment can be computed in closed form (Artetxe et al., 2016; Sch\u00f6nemann,"}, {"title": "2.3 Mitigating Embedding Inversion Attacks", "content": "Most research on textual embedding inversion focuses on attacks (Li et al., 2023; Huang et al., 2024; Chen et al., 2024a). While Song and Raghunathan (2020) adopt an adversarial training approach to mitigate the risks of inversion attacks, this method is ineffective for defending textual embeddings in black-box settings. To defend against inversion attacks while maintaining embedding utility in downstream tasks, Morris et al. (2023) propose inserting Gaussian noise as a defense mechanism. Expanding inversion attacks into multilingual space using the same method, Chen et al. (2024b) find that Gaussian noise effectively protects monolingual embeddings but is less effective for multilingual ones.\nDifferential privacy (DP) limits the impact of individual element (Dwork et al., 2014), and has been shown to preserve the privacy of the extracted representation from text, when applied during model training (Lyu et al., 2020). To ensure sequence-level metric-based local DP, which can be employed during inference, a sentence embedding sanitization pipeline has been developed, maintaining non-private task accuracy and effectively thwarting privacy threats of membership inference attacks (Du et al., 2023). Pertinent to vector DBs, Watermarking EaaS with Linear Transformation (WET) introduces a method that applies"}, {"title": "3 Methodology", "content": "We explore a situation in which a malicious attacker gains access to a limited set of embeddings, and attempts to reconstruct private and sensitive text data. We propose ALGEN to circumvent the disadvantage of scarce data and leverage a pretrained encoder-decoder to align the victim embeddings to the attack space and reconstruct the texts. We note the victim and attack embedding spaces as V and A, respectively. As illustrated in Fig. 2, the framework consists of three steps: (1) we train an embedding-to-sequence generation model by fine-tuning a pre-trained decoder $dec_A(\\cdot)$; (2) we align the embeddings from a black-box victim encoder $e_V$ to attack embeddings in attacker's model space $e_A$; (3) the attacker leverages the capability of the generation model $dec_A(\\cdot)$ with the embedding alignment model W to reconstruct the original text from $e_V$ to $e_{V \\rightarrow A}$ and finally to text."}, {"title": "3.1 Local Embedding-to-Text Generator", "content": "To train the local embedding-to-text generator, we use a publicly available text corpus, noted as $D_L$. Given a sentence $x \\in D_L$, and attack encoder $enc_A(\\cdot)$, the token embeddings are obtained $H = enc_A(x) \\in \\mathbb{R}^{s \\times n}$ where s is the sequence length and n the embedding dimension. The sentence embeddings are computed"}, {"title": "3.2 Embedding Space Alignment", "content": "Suppose there is a leaked data pair $(X, E_V)$, given that $X \\subset D_V$ is the victim dataset with $b \\in \\mathbb{N}$ samples, and embedding matrix $E_V = enc_V(X)$, where $enc_V$ is the black-box viticm encoder. To align the victim embeddings to the attack space A, we obtain the embedding matrix $E_A = enc_A(X)$ given the leaked text dataset, and seek a solution to solve the system\n$E_V W \\approx E_A$,\nand the best possible $W \\in \\mathbb{R}^{m \\times n}$, given $E_V \\in \\mathbb{R}^{b \\times m}$ and $E_A \\in \\mathbb{R}^{b \\times n}$, where n and m are the regarding embedding dimensions of victim and attacker embeddings. While there is no exact solution to the system, our approach is to minimize their deviation, $e = E_A - E_VW$. Taking the square of the error by each sample, the objective is to minimize the following:\n$\\min_{W} \\sum_{i=1}^{b} ||e_i - e_v W ||^2$.\nThe solution to this least squares loss is:\n$W = (E_V^T E_V)^{-1} E_V^T E_A$,\nwhere $(E_V^T E_V)^{-1} E_V^T$ is the Moore-Penrose Inverse of $E_V$ (see the detailed derivation in Appendix A).\nThe aligned embedding from V to A is thus:\n$E_{V \\rightarrow A} = E_V W$,\nwhere $E_{V \\rightarrow A} \\in \\mathbb{R}^{b \\times n}$. Implementing this alignment does not require any training; it is a one-step linear scaling. Moreover, aligning using $D_V$ with a batch size b varying from 1 to 1,000, we observe that even with as few as 30 samples the Rouge-L score exceeds 20, and a reasonably successful attack can be initiated with only a single data point. The density distribution of the alignment transformation matrix W's weights of encoders remains consistent across different datasets (see Fig 5)."}, {"title": "3.3 Textual Embedding Inversion Attack", "content": "Given the attack model, i.e. the local embedding-to-text generator $dec_A(\\cdot)$ and the $e_V$ to $e_A$ alignment model, and a body of eavesdropped embeddings $E_V$, we launch the inversion attack:\n$\\hat{X} = dec_A(E_V W)$."}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 LLMs", "content": "We use pretrained FLANT5 as the backbone to launch our attack modules, encoder $enc_A(\\cdot)$ and decoder $dec_A(\\cdot)$. For victim models, a variety of encoders are experimented on, including T5, GTR, MT5, MBERT and OpenAI text embedders TEXT-EMBEDDING-ADA-002 (ADA-2) and TEXT-EMBEDDING-3-LARGE (3-LARGE) (see the details of LLMs in Tabel 8)."}, {"title": "4.2 Attack Experimental Setup", "content": "Datasets and Attack Model We train the embedding-to-text generator $dec_A(.)$ by fine-tuning FLANT5-decoder, using the MultiHPLT English dataset (de Gibert et al., 2024) to explore few-shot inversion attacks. For multilingual inversion attacks, we utilize English, German, French, and Spanish datasets from mMarco (Bonifacio et al., 2021). In dataset, we split 150k samples ($D_L$) to train $dec_A(.)$; and up to 1k samples ($D_V$) to derive the alignment metrix W by aligning $e_V$ to $e_A$, as alignment samples; and 200 for evaluation."}, {"title": "5 Few-shot Inversion Attacks", "content": "Each subsection aims to answer one Research Question (RQ)."}, {"title": "5.1 How few Leaked Data do Attackers Need?", "content": "With only a single leaked data sample, our attack model manages to invert the victim embeddings, achieving a Rouge-L score of 10 across the encoders, as shown in Fig. 3. We use randomly generated embeddings as a baseline to verify that the aligned embeddings from ALGEN capture meaningful information. As shown in Table 1, all victim embeddings substantially outperform the RANDOM across metrics, validating our approach. Furthermore, when the number of leaked data samples increases until 1k, the inversion performance increases sharply, reaching 45.75 in Rouge-L and 0.9464 for cosine similarity for T5 embeddings. Notably, while GTR and T5 share the same tokenizer with the attack model, their inversion performances are not superior to others. Moreover, the inversion performance on proprietary OPENAI embeddings also reaches comparable performance, more than 41 in Rouge-L and 0.9 in cosine similarities for both ADA-2 and 3-LARGE, highlighting the risks posed by inversion attacks. As shown in the qualitative analysis in Table 9, some sentences can be inverted with almost an exact match. Furthermore, we conduct an ablation study with the size of alignment data samples and find that 1k alignment samples strike a balance between data size and performances (see Fig. 6 in Appendix. C).\nWe compare our method with Vec2Text (Morris et al., 2023), which trains two-step models (i.e., Base and Corrector) with iterative access to the victim encoder, and it requires training on embeddings from each encoder to invert the regarding embeddings. In comparison, our method only requires training one local attack model, and training does not involve specific victim encoders. Up to 1k samples, Vec2Text performance is much inferior compared to our method both in Rouge-L and cosine similarities, lower than 20 and 0.72, respectively, as detailed in Fig. 3 and Table 1.\nWe generally observe small alignment errors, with the cosine similarities between victim embeddings and attack embeddings consistently higher than 0.8 and near 1.0 when the number of alignment samples increases. The bottleneck of the performance of ALGEN is likely to lie in the decoding, as the trained attack decoder can only reach 54.16 in Rouge-L to invert the attack embeddings, as shown in Table 1, which is considered to be the upper bound of inversion attack performance."}, {"title": "5.2 Are Other Languages (More) Vulnerable?", "content": "Building on previous work on multilingual embedding inversion, we also investigate the impact of ALGEN on languages other than English. To achieve this, we trained local attack models in English, French, German, and Spanish. We further"}, {"title": "5.3 Is Risk Transferable across Domains?", "content": "To evaluate the cross-domain transferability of ALGEN, we attack the embeddings on the mMarco English dataset using an attack model trained on MultiHPLT English data. Although the cross-domain inversion performance in Rouge-L is about 25% lower than that of in-domain attacks on mMarco (see Table 3), the results remain alarmingly high - with Rouge-L near 20 and BLEU1 near 31 across victim encoders."}, {"title": "5.4 Does Inversion Recover Key Information?", "content": "To examine whether ALGEN attacks real key information, we apply Named Entity Recognition5 on input and reconstructed test data from MultiHPLT English dataset to calculate the current ratio of Named Entities in the reconstructed texts. Table 5 shows the results in F1 scores for overall and individual entities. In the qualitative analysis, as shown in Table 9, the input and reconstructed texts in the inversion attacks on OPENAI (ADA-2) embeddings are compared, with named entities highlighted. These attacks reveal sensitive details, such as organization, country, and numbers, highlighting the risks of privacy disclosure by embeddings."}, {"title": "6 Defending Textual Embeddings", "content": "To explore defenses against ALGEN, we evaluate defense mechanisms designed to protect textual embeddings from adversarial attacks."}, {"title": "6.1 Defense Methods", "content": "WET We implement WET on textual embeddings, to examine whether it makes embeddings robust against inversion attacks, since it is effective in"}, {"title": "6.2 Results", "content": "As shown in Table 6, 10 and 11, WET and Shuffling have minimal impact on both inversion attack performance and the utility performance across victim models and datasets. The randomly generated embeddings are also used as a baseline. With Gaussian noise insertion, the bigger the noise \u03bb, both performance in inversion and utility decrease. Using local DP, while the utility performance is preserved almost as the non-private embeddings with \u03b5 = 12, as shown in Fig. 4, Tabel 12 and 13. However, the inversion performance still maintains"}, {"title": "7 Discussion and Conclusion", "content": "In this work, we introduce and validate the effectiveness of a novel few-shot inversion attack, ALGEN, which drastically lowers the cost and complexity of such attacks on widely used vector databases. Our results show that the attack transfers effectively across domains and languages while revealing critical information. Moreover, its ability to align embeddings from different LLMs with minimal loss highlights its broad NLP applications, especially in cross-lingual embedding alignment. Finally, our evaluation of existing defense mechanisms reveals that none can adequately protect textual embeddings from inversion attacks while maintaining utility, highlighting significant security and privacy vulnerabilities."}, {"title": "Limitations", "content": "Our work does not propose a sufficient defense mechanism for ALGEN. Although we evaluated a number of existing defense mechanisms for textual embeddings, we found them to be ineffective against the proposed embedding inversion attack. The primary focus of this work is to expose the security vulnerabilities in embedding services and to inspire the development of future defense paradigms."}, {"title": "Computational Resources", "content": "We conduct experiments and train each text-to-embedding generator model on a single Nvidia A40 GPU, with the training process completing in three hours. Beyond this, ALGEN requires minimal GPU resources, making it a genuinely few-shot experimental setting."}, {"title": "Ethics Statement", "content": "We comply with the ACL Ethics Policy. The inversion attacks implemented in this paper can be misused and potentially harmful to proprietary embeddings. We discuss and experiment with potential mitigation and defense mechanisms, and we encourage further research in developing effective defenses in this attack space."}, {"title": "A Derivation of Normal Equation", "content": "To determine the optimal transformation matrix W, we aim to minimize a cost function J that quantifies the discrepancy between the attack embedding matrix $E_A$ and the victim embeddings $E_V$:\n$J(W) = \\frac{1}{2}(E_A - E_VW)^T (E_A \u2013 E_VW)$\n$= \\frac{1}{2}(E_A^T E_A - E_A^T E_VW - (E_VW)^T E_A$\n$+ (E_VW)^T E_VW)$\n$= \\frac{1}{2}(E_A^T E_A - E_A^T E_VW \u2013 W^T E_V^T E_A$\n$+ W^T E_V^T E_VW)$\nTo compute the derivatives of J(W):\n$\\nabla_W J(W) = \\nabla_W(\\frac{1}{2}(E_A^T E_A - E_A^T E_VW$\n$- W^T E_V^T E_A + W^T E_V^T E_VW))$\n$= 2 E_V^T E_VW \u2013 2 E_V^T E_A$.\nTo minimize J, setting its derivatives to 0, we obtain the normal equation :\n$E_V^T E_VW = E_V^T E_A$\nThe matrix W that minimizes J(W) is\n$W = (E_V^T E_V)^{-1} E_V^T E_A$"}, {"title": "B Defense Mechanisms", "content": ""}, {"title": "B.1 WET", "content": "T is constructed by adopting circulant matrices (Gray et al., 2006) to ensure that the transformation matrix is both full-rank and well-conditioned to allow for accurate pseudoinverse computation for recovering the original embeddings from watermarked embeddings (Shetty et al., 2024), refer to Shetty et al. (2024) for the complete algorithm for generating T.\nIn detail, WET as a defense is applied to aligned embeddings with the equation 11, where W is the optimal solution for alignment, and T is invertible.\n$Norm(T E_{V \\rightarrow A}) = Norm(T(E_V W))$\n$= Norm((T E_V) W)$"}, {"title": "B.2 (Local) Differential Privacy (DP)", "content": "As illustrated in Du et al. (2023), DP ensures that a randomized mechanism M behaves similarly on any two neighboring datasets X ~ X' differing in only one individual's contribution (e.g., a sequence). It is formally defined as follows:\nDefinition B.1. Let $\u03b5 \u2265 0, 0 \u2264 \u03b4 \u2264 1$ be two privacy parameters. M fulfills (\u03b5, \u03b4)-DP, if \u2200X ~ X' and any output set O \u2286 Range(M), $Pr[M(X) \u2208 O] \u2264 e^\u03b5 \u00b7 Pr[M(X') \u2208 O] + \u03b4$.\nIf \u03b4 = 0, then we say that M is \u03b5-DP or pure DP.\nThere are two popular DP settings, central and local. In central DP, a trusted curator can access the raw data of all individuals, apply a Mechanism M with random noise to ensure DP, and then release the perturbed outputs. Local DP (LDP) is ensured without the curator by letting individuals perturb their data locally before being shared. The local DP (Kasiviswanathan et al., 2011) is defined as follows:\nDefinition B.2. Let \u03b5 \u2265 0 be a privacy parameter. M is \u03b5-LDP, if for any two private inputs X, X' and any output set O \u2286 Range(M), $Pr[M(X) \u2208 O] < e^\u03b5 \u00b7 Pr[M(X') \u2208 O]$.\nHowever, \u03b5 - LDP offers homogenous protection for all input pairs, which can be too stringent in certain scenarios. When \u03b5 is too small, the noisy outputs are useless for utility tasks.\nThus, Du et al. (2023) customizes heterogeneous privacy guarantees for different pairs of inputs, so called metric-based LDP, formally defined as follows:\nDefinition B.3. Let \u03b5 \u2265 0 be the privacy parameter, and d be a suitable distance metric for the input space. M satisfies \u03b5d-LDP, if for any two inputs X, X' and any output set O \u2286 Range(M), $Pr[M(X) \u2208 O] \u2264 e^{\u03b5d(X,X')}$ \u00b7 Pr[M(X') \u2208 O].\nPurkayastha Mechanism with Purkayastha distribution and Planar Lapalace Mechanism with Euclidean metric, are thus proposed to ensure \u03b5d-LDP on embeddings. Refer to Du et al. (2023) for details in transforming embeddings with these mechanisms."}, {"title": "C Ablation Study of Leakage Data Size", "content": "The more data for alignment, the better the performance for ALGEN. However, after a certain"}]}