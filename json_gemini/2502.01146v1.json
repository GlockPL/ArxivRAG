{"title": "Quantum Machine Learning A Hands-on Tutorial for Machine Learning Practitioners and Researchers", "authors": ["Yuxuan Du", "Xinbiao Wang", "Naixu Guo", "Zhan Yu", "Yang Qian", "Kaining Zhang", "Min-Hsiu Hsieh", "Patrick Rebentrost", "Dacheng Tao"], "abstract": "This tutorial intends to introduce readers with a background in AI to quantum machine learning (QML)-a rapidly evolving field that seeks to leverage the power of quantum computers to reshape the landscape of machine learning. For self-consistency, this tutorial covers foundational principles, representative QML algorithms, their potential applications, and critical aspects such as trainability, generalization, and computational complexity. In addition, practical code demonstrations are provided in https://qml-tutorial.github.io/ to illustrate real-world implementations and facilitate hands-on learning. Together, these elements offer readers a comprehensive overview of the latest advancements in QML. By bridging the gap between classical machine learning and quantum computing, this tutorial serves as a valuable resource for those looking to engage with QML and explore the forefront of AI in the quantum era.", "sections": [{"title": "Introduction", "content": "The advancement of computational power has always been a driving force behind every major industrial revolution. The invention of the modern computer, followed by the central processing unit (CPU), led to the \u201cdigital revolution\", transforming industries through process automation and the rise of information technology. More recently, the development of graphical processing units (GPUs) has powered the era of artificial intelligence (AI) and big data, enabling breakthroughs in areas such as intelligent transportation systems, autonomous driving, scientific simulations, and complex data analysis. However, as we approach the physical and practical limits of Moore's law\u2014the principle that the number of transistors on a chip doubles approximately every two years\u2014traditional computing devices like CPUs and GPUs are nearing the end of their scaling potential. The ever-growing demand for computational power, driven by the exponential increase in data and the complexity of modern applications, necessitates new computing paradigms.\nAmong the leading candidates to meet these challenges are quantum computers (Feynman, 2017), which leverage the unique principles of quantum mechanics such as superposition and entanglement to process information in ways that classical systems cannot, with the potential to revolutionize diverse aspects of daily life.\nOne of the most concrete and direct ways to understand the potential of quantum computers is through the framework of complexity theory (Watrous, 2008). Theoretical computer scientists have demonstrated that quantum computers can efficiently solve problems within the BQP (Bounded-Error Quantum Polynomial Time) complexity class, meaning these problems can be solved in polynomial time by a quantum computer. In contrast, classical computers are limited to efficiently solving problems within the P"}, {"title": "A First Glimpse of Quantum Machine Learning", "content": "So, what exactly is quantum machine learning (QML)? In its simplest terms, the focus of this tutorial on QML can be summarized as follows:\nQML explores learning algorithms that can be executed on quantum computers to accomplish specified tasks with potential advantages over classical implementations.\nThe three key elements in the above interpretation are: quantum processors, specified tasks, and advantages. In what follows, let us elucidate the specific meaning of each of these terms, providing the necessary foundation for a deeper understanding of the mechanisms and potential of QML."}, {"title": "Quantum computers", "content": "The origins of quantum computing can be traced back to 1980 when Paul Benioff introduced the quantum Turing machine (Benioff, 1982), a quantum analog of the classical Turing machine designed to describe computational models through the lens of quantum theory. Since then, several models of quantum computation have emerged, including circuit-based quantum computation (Nielsen and Chuang, 2011), one-way quantum computation (Raussendorf and Briegel, 2001), adiabatic quantum computation (Albash and Lidar, 2018), and topological quantum computation (Kitaev, 2003). All of them have been shown to be computationally equivalent to the quantum Turing machine, meaning that a perfect implementation of any one of these models can simulate the others with no more than polynomial overhead. Given the prominence of circuit-based quantum computers in both the research and industrial communities and their rapid advancement, this tutorial will focus primarily on this model of quantum computing.\nQuantum computing gained further momentum in the early 1980s when physicists faced an exponential increase in computational overhead while simulating quantum dynamics, particularly as the number of particles in a system grew. This \u201ccurse of dimensionality\u201d prompted Yuri Manin and Richard Feynman to independently propose leveraging quantum phenomena to build quantum computers, arguing that such devices would be far more"}, {"title": "Different measures of quantum advantages", "content": "What do we mean when we refer to quantum advantage? Broadly, quantum advantage is demonstrated when quantum computers can solve a problem more efficiently than classical computers. However, the notion of \u201cefficiency\u201d in this context is not uniquely defined.\nThe most common measure of efficiency is runtime complexity. By harnessing quantum effects, certain computations can be accelerated significantly\u2014sometimes even exponentially\u2014enabling tasks that are otherwise infeasible for classical computers. A prominent example is Shor\u2019s algorithm, which achieves an exponential speedup in large-number factorization relative to the best classical algorithms. In terms of runtime complexity, the"}, {"title": "Explored tasks in quantum machine learning", "content": "What are the main areas of focus in QML? QML research is extensive and can be broadly divided into four primary sectors, each defined by the nature of the computing resources (whether the computing device is quantum (Q) or classical (C)) and the type of data involved (whether generated by a quantum (Q) or classical (C) system). The explanations of these four sectors are as follows.\nCC Sector. The CC sector refers to classical data processed on classical systems, representing traditional machine learning. Here, classical ML algorithms run on classical processors (e.g., CPUs and GPUs) and"}, {"title": "Progress of Quantum Machine Learning", "content": "Huge efforts have been made to the QC and QQ sectors to determine which tasks and conditions allow QML to offer computational advantages over classical machine learning. In this regard, to provide a clearer understanding of QML's progress, it is essential to first review recent advancements in quantum computers, the foundational substrate for quantum algorithms."}, {"title": "Progress of quantum computers", "content": "The novelty and inherent challenges of utilizing quantum physics for computation have driven the development of various computational architectures, giving rise to the formalized concept of circuit-based quantum computers, as discussed in Chapter 1.1.1. In pursuit of this goal, numerous companies and organizations are striving to establish their architecture as the leading approach and to be the first to demonstrate practical utility or quantum advantage on a large-scale quantum device.\nCommon architectures currently include superconducting qubits (employed by IBM and Google), ion-trap systems (pioneered by IonQ), and Rydberg atom systems (developed by QuEra), each offering distinct advantages (Cheng et al., 2023). Specifically, superconducting qubits excel in scalability and fast gate operations (Huang et al., 2020a), while ion-trap systems are"}, {"title": "Progress of quantum machine learning under FTQC", "content": "A key milestone in FTQC-based QML algorithms is the quantum linear equations solver introduced by Harrow et al. (2009). Many machine learning models rely on solving linear equations, a computationally intensive task that often dominates the overall runtime due to the polynomial scaling of complexity with matrix size. The HHL algorithm provides a breakthrough by reducing runtime complexity to poly-logarithmic scaling with matrix size, given that the matrix is well-conditioned and sparse. This advancement is highly significant for AI, where datasets frequently reach sizes in the millions or even billions.\nThe exponential runtime speedup achieved by the HHL algorithm has garnered significant attention from the research community, highlighting the"}, {"title": "Progress of quantum machine learning under NISQ", "content": "The work conducted by Havl\u00ed\u010dek et al. (2019) marked a pivotal moment for QML in the NISQ era. This study demonstrated the implementation of quantum kernel methods and quantum neural networks (QNNs) on a 5-qubit superconducting quantum computer, highlighting potential quantum advantages from the perspective of complexity theory. Unlike the aforementioned FTQC algorithms, quantum kernel methods and QNNs are flexible and can be effectively adapted to the limited quantum resources available in the NISQ era. These demonstrations, along with advancements in quantum hardware, sparked significant interest in exploring QML applications using NISQ quantum devices. We will delve into quantum kernel methods and QNNs in Chapter 3 and Chapter 4, respectively.\nA quantum neural network (QNN) is a hybrid model that leverages quantum computers to implement trainable models similar to classical neural networks, while using classical optimizers to complete the training process.\nAs shown in Figure 1.4, the mechanisms of QNNs and deep neural networks (DNNs) are almost the same, whereas the only difference is the way of implementing the trainable model. This difference gives the potential of quantum learning models to solve complex problems beyond the reach of classical neural networks, opening new frontiers in many fields. Roughly speaking, research in QNNs and quantum kernel methods has primarily focused on three key areas: (I) quantum learning models and applications, (II) the adaptation of advanced AI topics to QML, and (III) theoretical foundations of quantum learning models. A brief overview of each category is provided below."}, {"title": "A brief review of quantum machine learning", "content": "Unlike quantum hardware, where the number of qubits has rapidly scaled from zero to thousands, the development of QML algorithms\u2014and quantum algorithms more broadly\u2014has taken an inverse trajectory, transitioning from FTQC to NISQ devices. This shift reflects the move from idealized theoretical frameworks to practical implementations. The convergence of quantum hardware and QML algorithms, where the quantum resources required by these algorithms become attainable on real quantum computers, enables researchers to experimentally evaluate the power and limitations of various quantum algorithms.\nBased on the minimum quantum resources required to complete learning tasks, we distinguish between FTQC algorithms, discussed in Chapter 1.2.2, and NISQ algorithms, including QNNs and quantum kernel methods, in Chapter 1.2.3. FTQC-based QML algorithms necessitate error-corrected quantum computers with tens of billions of qubits\u2014an achievement that remains far from realization. In contrast, QNNs and quantum kernels are"}, {"title": "Organization of This Tutorial", "content": "To encourage and enable computer scientists to engage with the rapidly growing field of quantum AI, we provide this hands-on tutorial that revisits QML algorithms from a computer science perspective. With this aim, the tutorial is designed to balance theory, practical implementations, and applications, making it suitable for both researchers and practitioners with a background in classical machine learning. The tutorial is divided into the following chapters:\nChapter 2: BASICS OF QUANTUM COMPUTING. Before delving into QML, this chapter lays the groundwork by introducing the fundamental concepts of quantum computing. It covers the transition from classical bits to quantum bits, explains quantum circuit models, illustrates how quantum systems interface with classical systems through quantum read-in and read-out mechanisms, and presents some fundamental concepts of quantum linear algebra. By the end of this chapter, you will understand that a solid grasp of linear algebra is all you need to comprehend the basics of quantum computing.\nChapters 3, 4 and 5: CLASSICAL ML MODELS EXTENDED TO QUANTUM FRAMEWORKS. Each of these chapters follows a consistent structure, starting with a review of the classical model and progressing to its quantum extension\u2014Quantum kernel methods in Chapter 3, Quantum neural networks in Chapter 4, and Quantum Transformers in Chapter 5. This unified structure enables readers to clearly understand how classical machine learning models can be translated into quantum implementations and how quantum computers may offer computational advantages.\nAppendix. The Appendix serves as a supplementary resource, providing a summary of notations and essential mathematical tools that are omitted from the main text for brevity. In particular, it includes basic introduction of concentration inequalities, the Haar measure, and other foundational concepts relevant to the tutorial."}, {"title": "Basics of Quantum Computing", "content": "In this chapter, we introduce the fundamental concepts of quantum computation, such as quantum states, quantum circuits, and quantum measurements, along with key topics in quantum machine learning, including quantum read-in, quantum read-out, and quantum linear algebra. These foundational elements are essential for understanding quantum machine learning algorithms and will be repeatedly referenced throughout the subsequent chapters.\nThis chapter is organized as follows: Chapter 2.1 introduces quantum bits and their mathematical representations; Chapter 2.2 covers quantum circuits, including quantum gates, quantum channels, and quantum measurements; Chapter 2.3 discusses how to encode classical data into quantum systems and extract classical information from quantum states; Chapter 2.4 delves into quantum linear algebra; Chapter 2.5 provides practical coding exercises to reinforce these concepts; and finally, Chapter 2.6 presents recent advancements in efficient quantum read-in and read-out techniques for further exploration."}, {"title": "From Classical Bits to Quantum Bits", "content": "In this section, we define quantum bits (qubits) and present the mathematical tools used to describe quantum states. We begin by discussing classical bits and then transition to their quantum counterparts. We recommend interested readers consult the textbook (Nielsen and Chuang, 2011) for the detailed explanations."}, {"title": "Classical bits", "content": "In classical computing, a bit is the basic unit of information, which can exist in one of two distinct states: 0 or 1. Each bit holds a definite value at any given time. When multiple classical bits are used together, they can represent more complex information. For instance, a set of three bits can represent $2^3 = 8$ distinct states, ranging from 000 to 111."}, {"title": "Quantum bits (Qubits)", "content": "Analogous to the role of 'bit' in classical computation, the basic element in quantum computation is the quantum bit (qubit). We start by introducing the representation of single-qubit states and then extend this to two-qubit and multi-qubit states.\nSingle-qubit state. A single-qubit state can be represented by a two-dimensional vector with unit length. Mathematically, a qubit state can be written as\n$\\alpha =  \\begin{bmatrix} a_1 \\ a_2 \\end{bmatrix} \\in \\mathbb{C}^2, $"}, {"title": "Density matrix", "content": "Another description of quantum states is through density matrix or density operators. The reason for establishing density operators instead of Dirac notations arises from the imperfection of physical systems. Specifically, Dirac notations introduced in Chapter 2.1.2 are used to describe \u2018ideal' quantum states (i.e., pure states), where the operated qubits are isolated from the environment. Alternatively, when the operated qubits interact with the environment unavoidably, the density operators are employed to describe the behavior of quantum states living in this open system. As such, density operators describe more general quantum states.\nMathematically, an N-qubit density operator, denoted by $\\rho \\in \\mathbb{C}^{2^N \\times 2^N}$, presents a mixture of m quantum pure states $|\\Psi_i\\rangle \\in \\mathbb{C}^{2^N}$ with probability $p_i \\in [0, 1]$ and $\\sum_{i=1}^{m} p_i = 1$, i.e.,\n$\\rho = \\sum_{i=1}^{m} p_i |\\Psi_i\\rangle\\langle\\Psi_i|,$"}, {"title": "From Digital Logical Circuit to Quantum Circuit Model", "content": "To process quantum states, we need to introduce quantum computation, a fundamental model of which is the quantum circuit model. In this section, we will begin with classical computation in Chapter 2.2.1 and transit to details about the quantum circuit model in Chapter 2.2.2, including quantum gates, quantum channel, and quantum measurements."}, {"title": "Classical digital logical circuit", "content": "Digital logic circuits are the foundational building blocks of classical computing systems. They process classical bits by performing logical operations through logic gates. In this subsection, we introduce the essential components of digital logic circuits and their functionality, followed by a discussion of how these classical circuits relate to quantum circuits."}, {"title": "Quantum circuit", "content": "Classical digital logical circuits provide the essential framework for understanding computation. While classical circuits operate on bits and perform deterministic operations, quantum circuits manipulate qubits and involve probabilistic behavior. The concepts of logic gates, circuit design, and universality lay the groundwork for transitioning to quantum circuits introduced in this subsection."}, {"title": "Quantum Read-in and Read-out protocols", "content": "The terms quantum read-in and read-out refer to the processes of transferring information between classical systems and quantum systems. These are"}, {"title": "Quantum read-in protocols", "content": "Quantum read-in refers to the process of encoding classical information into quantum systems that can be manipulated by a quantum computer, which can be regarded as the classical-to-quantum mapping. It acts as a bridge to utilize quantum algorithms to solve classical problems in quantum computing. Here, we will introduce several typical encoding methods, including basis encoding, amplitude encoding, angle encoding, and quantum random access memory. Some easy-to-use demonstrations are provided in Chapter 2.5."}, {"title": "Quantum read-out protocols", "content": "Quantum read-out refers to the process of translating the quantum state resulting from a quantum computation into classical data, enabling further processing, interpretation, or optimization in classical systems. This process can be considered the inverse operation of quantum read-in, representing a quantum-to-classical mapping.\nBased on the completeness of the information obtained during the read-out process, quantum read-out protocols can be broadly categorized into two types, i.e., full information and partial information read-out protocols. These protocols enable tailored read-out processes that match the requirements of different quantum applications, ranging from tomography to optimization and machine learning tasks."}, {"title": "Quantum Linear Algebra", "content": "We next introduce quantum linear algebra, a potent toolbox for designing various FTQC-based algorithms introduced in Chapter 1.2.2. For clarity, we start with the definition of block encoding in Chapter 2.4.1, which is about how to implement a matrix on the quantum computer. Based on this, we introduce some basic arithmetic rules for block encodings in Chapter 2.4.2, like the multiplication, linear combination, and the Hadamard product. Finally, in Chapter 2.4.3, we introduce the quantum singular value transformation"}, {"title": "Code Demonstration", "content": "This section provides code implementations for key techniques introduced earlier, including quantum read-in strategies and block encoding, to give readers the opportunity to practice and deepen their understanding."}, {"title": "Read-in implementations", "content": "This subsection demonstrates toy examples of implementing data encoding methods in quantum computing, as discussed in earlier sections. Specifically, we cover basis encoding, amplitude encoding, and angle encoding from Chapter 2.3.1. These examples aim to provide readers with hands-on experience in applying quantum data encoding techniques."}, {"title": "Block encoding", "content": "Here, we provide an example of how we may construct a block encoding. We construct the block encoding via the linear combination, as fact 2.21. We use PennyLane to keep consistency, yet there are many other platforms that are available as well. Please note that it is time-consuming to do the Pauli decomposition (for an N-qubit matrix, it takes time $\\mathcal{O}(N4^N)$), so we suggest not trying a large matrix with this method."}, {"title": "Bibliographic Remarks", "content": "We end this chapter by discussing the recent advancements in efficiently implementing fundamental components of quantum computing. For clarity, we begin with a brief discussion of advanced quantum read-in and read-out protocols, which are crucial for efficiently loading and extracting classical data in the pipeline of quantum machine learning. Next, we review the latest progress in quantum linear algebra."}, {"title": "Advanced quantum read-in protocols", "content": "Although conventional read-in protocols offer feasible solutions for encoding classical data into quantum computers, they typically face two key challenges that limit their broad applicability for solving practical learning problems. To address these limitations, initial efforts have been made to develop more advanced quantum read-in protocols.\nChallenge I: high demand for quantum resources. Encoding methods like amplitude encoding and basis encoding presented in Chapter 2.3.2 generally suffer from high quantum resource requirements. While amplitude encoding is highly compact in terms of qubit requirements, the trade-off is the requirement of an exponential number of quantum gates with the data size to prepare an exact amplitude-encoded state. In contrast, while basis encoding can be implemented with a small number of quantum gates, it requires a large number of qubits proportional to the input size. The high demand for either quantum gates or qubit counts makes these basic encoding strategies infeasible for practical use."}, {"title": "Quantum Kernel Methods", "content": "The fundamental goal of machine learning (ML) algorithms is to learn the underlying feature representations embedded in the training data, allowing data points to be effectively modeled using simple models like linear classifiers. Kernel methods are a powerful approach to achieving this by enabling non-linear patterns to be captured in a computationally efficient manner. In kernel methods, a kernel function is defined as the inner product between the high-dimensional feature representations of data points. These feature representations are generated by a hidden feature map that transforms the original data into a higher-dimensional space where complex patterns become easier to identify and model. The kernel function, therefore, serves as a measure of similarity between data points in this transformed space.\nThe effectiveness of kernel methods heavily depends on the hidden feature map's ability to capture relevant patterns in the data. The more effectively this feature map can reveal the underlying structure, the better the kernel method's performance in learning and generalizing from the data. However, classical kernel methods are inherently limited by the types of patterns they can recognize, as these are constrained by classical computational frameworks. Essentially, classical models excel at detecting patterns they are specifically designed to recognize but may struggle with patterns that deviate from this framework.\nIn contrast, quantum mechanics is known for generating complex, non-intuitive patterns that are often beyond the reach of classical algorithms. Quantum systems can produce statistical correlations that are computationally challenging\u2014or even impossible\u2014for classical computers to replicate. This suggests that employing quantum circuits as hidden feature maps"}, {"title": "Classical Kernel Machines", "content": "To effectively introduce quantum kernel machines, it is essential to recog-nize the limitations of classical kernel machines. As discussed in Chapter 3.1,classical kernel machines rely on manually tailored feature mappings, suchas polynomials or radial basis functions. However, these mappings mayfail to capture the complex patterns behind the dataset. Quantum kernelmachines emerge as a promising alternative, as they perform feature map-ping using quantum circuits, enabling them to explore exponentially largerfeature spaces that are otherwise infeasible for classical computation."}, {"title": "Motivation of kernel methods", "content": "Before delving into kernel machines, it is essential to first understand the motivation behind kernel methods. In many machine learning tasks, particularly in classification, the goal is to find a decision boundary that best"}, {"title": "Dual representation", "content": "To understand why many machine learning algorithms rely primarily on the inner products between data points, we need to introduce the concept of the dual representation. In essence, many linear parametric models used for regression or classification can be re-cast into an equivalent dual form, where the kernel function evaluated on the training data emerges naturally.\nLet's start with a linear regression model with the training dataset $\\{(x^{(i)},y^{(i)})\\}_{i=1}^n$, where the parameters are determined by minimizing a regularized sum-of-squares error function\n$\\mathcal{L}(w) = \\frac{1}{2}\\sum_{i=1}^{n} (w^{\\top}\\varphi(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2} w^{\\top} w,$"}, {"title": "Kernel construction", "content": "To utilize the kernel trick in machine learning algorithms, it is essential to construct valid kernel functions. One approach is to start with a feature mapping $\\varphi(x)$ and then derive the corresponding kernel. For a one-dimensional input space, the kernel function is defined as\n$k(x, x') = \\varphi(x)^\\top \\varphi(x') = \\sum_{i=1}^{D} (\\varphi_i(x), \\varphi_i(x')),$"}, {"title": "Quantum Kernel Machines", "content": "To effectively introduce quantum kernel machines, it is essential to recognize the limitations of classical kernel machines. As discussed in Chapter 3.1, classical kernel machines rely on manually tailored feature mappings, such as polynomials or radial basis functions. However, these mappings may fail to capture the complex patterns behind the dataset. Quantum kernel machines emerge as a promising alternative, as they perform feature mapping using quantum circuits, enabling them to explore exponentially larger feature spaces that are otherwise infeasible for classical computation."}, {"title": "Motivations for quantum kernel machines", "content": "To effectively introduce quantum kernel machines, it is essential to recognize the limitations of classical kernel machines. As discussed in Chapter 3.1, classical kernel machines rely on manually tailored feature mappings, such as polynomials or radial basis functions. However, these mappings may fail to capture the complex patterns behind the dataset. Quantum kernel machines emerge as a promising alternative, as they perform feature mapping using quantum circuits, enabling them to explore exponentially larger feature spaces that are otherwise infeasible for classical computation."}, {"title": "Quantum feature maps and quantum kernel machines", "content": "The key difference between quantum kernel machines and classical kernel machines lies in how the feature mapping is performed. In the quantum context, a feature map refers to the injective encoding of classical data $x \\in \\mathbb{R}^{d}$ into a quantum state $|\\phi(x)\\rangle = U(x)|\\psi\\rangle$ on an N-qubit quantum register, where $U(x)$ refers to the physical operation or quantum circuit that depends on the data $x$. This feature map is implemented on a quantum computer and produces quantum states, which are referred to as quantum feature maps.\nGiven an N-qubit quantum system initialized in state $|\\psi\\rangle$, let $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ be classical data. The quantum feature map is defined as the mapping\n$\\phi : \\mathcal{X} \\rightarrow \\mathcal{F},$$\\phi(x) = |\\phi(x)\\rangle \\langle\\phi(x)| = \\rho(x),"}, {"title": "Classical Neural Networks", "content": "Classical neural networks (LeCun et al., 2015) are the foundation of modern artificial intelligence technologies and have achieved widespread success in fields such as computer vision (Voulodimos et al., 2018) and natural language processing (Otter et al., 2020). However, despite these achievements, classical neural networks face significant challenges, including excessively large model sizes and the corresponding high computational costs (Hoffmann et al., 2022), especially in terms of energy consumption (de Vries, 2023). These limitations result from their dependence on classical computational resources, which are becoming increasingly unsustainable as models grow in complexity.\nQuantum neural networks (QNNs) (Jeswal and Chakraverty, 2019) offer a promising solution by enhancing neural networks with the computational potential of quantum circuits (Liu et al., 2024a). In QNNs, classical input data is encoded into quantum states, and quantum gates with trainable parameters process these states in ways that classical systems cannot easily replicate. This computational regime leverages quantum mechanics to explore new forms of pattern recognition and problem-solving that go beyond classical methods. Thus, QNNs have the potential to outperform classical neural networks in specific learning tasks (Huang et al., 2022), where the advantages in processing and learning can be explored."}, {"title": "Perceptron", "content": "The perceptron model, first introduced by (McCulloch and Pitts, 1943), is widely regarded as a foundational structure in artificial neural networks, in-"}, {"title": "Multilayer perceptron", "content": "The multilayer perceptron (MLP) is a fully connected neural network architecture consisting of three components: the input layer, hidden layers, and output layer, as illustrated in Figure 4.2. Similar to the single-layer perceptron introduced in Chapter 4.1.1, the neurons in the MLP are connected through weighted sums, followed by non-linear activation functions.\nThe mathematical expression of MLP is as follows. Let $x^{(a,1)}$ be the $a$-th input data and $l = 1$ denote the input layer. Define $L$ as the number of total layers. The forward propagation at the $(l + 1)$-th layer $\\forall l \\in \\{1, 2, ..., L - 2\\}$ yields\n$z^{(a,l+1)} = W^{(l)}x^{(a,l)} + b^{(l)},$$\\forall x \\in \\varphi,\\quad \\psi^{(t+1)} = \\exp^{-H^{(u)}t} \\psi^{(t)},$"}, {"title": "Fault-tolerant Quantum Perceptron", "content": "The primary aim of advancing quantum machine learning is to harness the computational advantages of quantum mechanics to enhance performance across various learning tasks. As outlined in Chapter 1.1.2, these advantages manifest in several ways, including reduced runtime, lower query complexity, and improved sample efficiency compared to classical models. A notable example of this is the quantum perceptron model (Kapoor et al., 2016).\nAs a FTQC-based QML algorithm grounded in the Grover search, quantum perceptron offers a quadratic improvement in the query complexity during the training over its classical counterpart. For comprehensiveness, we first introduce the Grover search algorithm, followed by a detailed explanation of the quantum perceptron model."}, {"title": "Grover search", "content": "Grover search (Grover, 1996) provides runtime speedups for unstructured search problems, which have broad applications in cryptography, quantum machine learning, and constraint satisfaction problems. Unlike classical search methods that require $\\mathcal{O}(d)$ queries for a dataset with $d$ entries, Grover's algorithm can identify the target element with high probability using only $\\mathcal{O}(\\sqrt{d})$ queries to a quantum oracle. Consequently, quantum algorithms incorporating Grover search have the potential to achieve a quadratic speedup over classical approaches.\nIn general, a search task can be abstracted as a function $f(x)$ such that $f(x) = 1$ if $x$ belongs to the solution set of the search problem, and $f(x) = 0$ otherwise. We consider a dataset consisting of $d = 2^N$ elements, where each element is represented by the quantum state $|x\\rangle$ with $x = 0,1,\\dots, d - 1$.\nIn this process, two key quantum oracles are introduced. The first oracle, $U_o = 2(|0\\rangle \\langle 0|)^{\\otimes N} - \\mathbb{I}_d$, applies a phase shift of $e^{i\\pi} = -1$ to all quantum states except $|0\\rangle$, which remains unchanged. The second oracle, $U_f$, operates in a similar manner: it applies a phase shift of $-1$ to quantum states that belong to the solution set while leaving all other states unaffected. The procedure for Grover search is described in Algorithm 2."}, {"title": "Online quantum perceptron with quadratic speedups", "content": "As stated in Theorem 4.1, for a linearly separable dataset with a margin \u03b3, a perceptron model can achieve perfect classification after making $\\mathcal{O}(1/\\gamma^2)$ mistakes during training. In classical approaches, identifying a sample that is misclassified by the current model may require up to $\\mathcal{O}(d)$ queries, where $d$ denotes the size of the training dataset. In contrast, the quantum perceptron model (Kapoor et al., 2016) can identify misclassified samples more efficiently by employing the Grover search algorithm, achieving a quadratic speed-up in the query complexity.\nTo begin, we introduce the setup of the input data. We consider the classification of a dataset $\\{z^{(i)}\\}_{i=1}^d = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^d$, where the label $y^{(i)} \\in \\{-1, 1\\}$. For convenience, we assume that the number of samples is a power of 2, i.e., $d = 2^N$. Each data vector $x^{(i)}$ is assumed to be represented by using $B$ bits. The information of each sample $z^{(i)}$ is stored in the quantum state $|z^{(i)}\\rangle$ by using $B + 1$ qubits."}, {"title": "Near-term Quantum Neural Networks", "content": "Following recent experimental breakthroughs in superconducting quantum hardware architectures (Arute et al., 2019; Acharya et al., 2024; AbuGhanem, 2024; Gao et al., 2024), researchers have devoted considerable effort to developing and implementing quantum machine learning algorithms optimized for current and near-term quantum devices (Wang and Liu, 2024). Compared to fault-tolerant quantum computers, these devices face three primary limitations: quantum noise, limited coherence time, and circuit connectivity constraints. Regarding quantum noise, state-of-the-art devices have single-qubit gate error rates of $10^{-4} \\sim 10^{-3}$ and two-qubit gate error rates of approximately $10^{-3} \\sim 10^{-2}$ (AbuGhanem, 2024; Gao et al., 2024). The coherence time is around $10^{2}\\mu s$ (Acharya et al., 2024; AbuGhanem, 2024; Gao et al., 2024), primarily limited by decoherence in noisy quantum channels. Regarding circuit connectivity, most superconducting quantum processors employ architectures that exhibit two-dimensional connectivity patterns and their variants (Acharya et al., 2024; AbuGhanem, 2024; Gao et al., 2024). Gate operations between non-adjacent qubits must be executed through intermediate relay operations, leading to additional error accumulation. To address these inherent limitations, the quantum neural network (QNN) framework has been proposed. Specifically, these QNNs are designed to perform meaningful computations on near-term quantum devices."}, {"title": "General framework", "content": "In this section, we introduce the basic architecture of QNNs. As illustrated in Figure 4.4, a basic QNN consists of three components: the input, the"}, {"title": "Quantum Transformer", "content": "Transformers, introduced by Vaswani (2017), have become one of the most important and widely adopted deep learning architectures in modern AI. Transformers were first developed to improve previous architectures for natural language processing on the ability to handle long-range dependencies and capture intricate relationships in data. Unlike previous sequential models, such as recurrent neural networks, which process information in a step-by-step manner, transformers use a mechanism called self-attention to capture correlations among all elements in a sequence simultaneously. This parallel processing capability significantly reduces training time and improves learning performance.\nDespite its many advantages, the transformer architecture has several drawbacks, particularly the required computational resources. As discussed in previous chapters, quantum computing provides unique advantages over classical computing in certain applications by leveraging quantum phenomena such as superposition, entanglement, and interference. These capabilities have inspired researchers to explore whether integrating quantum computing with Transformers could lead to superior performance compared to their classical"}]}