{"title": "Go-SLAM: Grounded Object Segmentation and Localization with Gaussian Splatting SLAM", "authors": ["Phu Pham", "Dipam Patel", "Damon Conover", "Aniket Bera"], "abstract": "We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting SLAM to reconstruct dynamic environments while embedding object-level information within the scene representations. This framework employs advanced object segmentation techniques, assigning a unique identifier to each Gaussian splat that corresponds to the object it represents. Consequently, our system facilitates open-vocabulary querying, allowing users to locate objects using natural language descriptions. Furthermore, the framework features an optimal path generation module that calculates efficient navigation paths for robots toward queried objects, considering obstacles and environmental uncertainties. Comprehensive evaluations in various scene settings demonstrate the effectiveness of our approach in delivering high-fidelity scene reconstructions, precise object segmentation, flexible object querying, and efficient robot path planning. This work represents an additional step forward in bridging the gap between 3D scene reconstruction, semantic object understanding, and real-time environment interactions.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous robots are becoming increasingly vital in various fields, including search and rescue, manufacturing, and military operations [1], [2], [3], [4]. To effectively navigate and interact with their environment, these robots need the ability to accurately reconstruct the surroundings, segment objects of interest, and plan paths in real-time. One of the key challenges in building such systems lies in achieving high-fidelity scene reconstruction while also inte- grating semantic understanding of objects within the scene. Additionally, enabling robots to query objects in an open- vocabulary manner and generate optimal paths to interact with these objects enhances their flexibility and adaptability in challenging environments.\nTraditional SLAM or Simultaneous Localization and Map- ping techniques [5], [6], [7] have proven effective in re- constructing environments but often fail to provide detailed, object-level segmentation and interaction capabilities. In contrast, methods like point cloud or voxel-based recon- structions, while offering spatial accuracy, tend to struggle with incorporating object semantics in a robust and scalable manner. Recent advances in 3D Gaussian Splatting [8] offer a promising alternative for scene representation and rendering by using 3D Gaussian primitives to model the geometry and appearance of a scene.\nWhile accurate 3D reconstruction is essential, true scene understanding requires the ability to identify and label ob- jects within the environment. To address this, we incorpo- rate advanced computer vision models that provide robust object detection and precise segmentation capabilities. By leveraging these techniques with 3D Gaussian Splatting, we generate a semantically rich environmental representation, where each Gaussian splat is associated with an object label. This enables robotic systems to understand both the spatial structure of the environment and the semantic relationships between objects, allowing for accurate object identification, tracking, and interaction across multiple camera frames.\nAnother novelty of our approach is the support for open- vocabulary queries. By incorporating natural language pro- cessing techniques, our system allows users or higher-level planning algorithms to locate objects using flexible, human- like descriptions. This capability significantly enhances the adaptability of robotic systems, enabling them to understand and act upon a wide range of commands without being limited to a predefined set of object categories. Our entire approach is outlined in Figure 1.\nFinally, we demonstrate the practical utility of our frame- work by implementing an optimal path-planning algorithm that leverages the semantically annotated 3D model. This allows a robot to efficiently navigate from its current position to a queried object, taking into account the spatial layout and potential obstacles in the environment.\nThe main contributions of this paper can be summarized as follows:\n\u2022 A novel implementation of 3D Gaussian Splatting SLAM with state-of the-art object segmentation and labeling techniques."}, {"title": "II. RELATED WORK", "content": "A. 3D Gaussian Splatting\n3D Gaussian Splatting (3DGS) has emerged as a pow- erful technique for representing and rendering 3D scenes. Originally introduced by Kerbl et al. [8], 3DGS uses a collection of 3D Gaussian primitives to model geometry and appearance. This approach offers several advantages over traditional 3D reconstruction methods. Unlike mesh- based [9], [10], which can be computationally expensive and complex to manipulate, Gaussian Splatting provides a more flexible representation by modeling surfaces with continuous, parameterized Gaussians. This allows for smoother surface approximations, especially for objects with irregular or com- plex geometries. Additionally, Gaussian Splatting can handle partial or noisy data more robustly, making it well-suited for real-time applications such as SLAM, where incomplete or uncertain information is common. The method also facilitates efficient integration of multimodal data, such as combining RGB and depth information, which enhances the accuracy of scene reconstructions.\nB. SLAM systems\nSimultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics and computer vision, aim- ing to construct a map of an unknown environment while simultaneously tracking an agent's location within it. Tra- ditional SLAM approaches have relied on sparse feature- based methods [11] or dense volumetric representations [12]. More recently, neural implicit representations like Neural Radiance Fields (NeRF) [13] have been adapted for tasks like SLAM [14] and navigation [15], offering high-quality scene reconstruction, but often at the cost of computational efficiency [16], [17], [14], [18].\nC. Gaussian Splatting SLAM\nThe integration of 3D Gaussian Splatting into SLAM systems is a recent development that aims to leverage the advantages of 3DGS for real-time mapping and localization. Matsuki et al. [19] introduced the first Gaussian Splatting SLAM system, demonstrating its effectiveness in monocular settings. GS-SLAM by Yan et al. [20] proposed a dense vi- sual SLAM system using 3DGS, achieving competitive per- formance in both reconstruction and localization with lower time consumption compared to other methods. SplaTAM by Keetha et al. [21] introduced a system for dense RGBD SLAM using 3DGS, demonstrating real-time performance and high-quality reconstruction.\nThese Gaussian Splatting SLAM approaches have shown promising results in terms of reconstruction quality, lo- calization accuracy, and computational efficiency. However, challenges remain in areas such as large-scale mapping, loop closure, and handling dynamic environments.\nD. Object detection and segmentation\nObject detection and segmentation are essential for en- abling robots to understand and interact with their environ- ment. Traditional approaches, such as Faster R-CNN [22] and Mask R-CNN [23] have been instrumental in detecting objects and generating instance-specific segmentation masks, enabling robots to recognize and differentiate objects within complex scenes. In addition, methods such as YOLO (You Only Look Once) [24] introduced single-shot detectors that achieve real-time object detection by predicting bounding boxes and class probabilities directly from full images, significantly improving the efficiency of object recognition tasks.\nMore recent models, such as Grounding DINO [25] and Segment Anything Model (SAM) [26], have pushed the boundaries of object segmentation. Grounding DINO uses transformer-based architectures to detect and localize objects by understanding semantic context, while SAM excels at universal segmentation by generating masks for any object given minimal input, without needing retraining. Grounded SAM, which integrates Grounding DINO with SAM, enables detection and segmentation of any objects specified by an input text prompt. These models empower robotic systems to recognize and segment objects flexibly, even in environments with undefined or unseen objects.\nE. Language embedded for 3D reconstruction\nRecent developments in 3D scene reconstruction have increasingly incorporated language understanding, enabling more intuitive interactions with 3D environments. A notable work in this field is LERF (Language Embedded Radiance Fields) [27], which embeds CLIP-based language representa- tions into Neural Radiance Fields (NeRF). LERF constructs a dense, multi-scale language field by rendering CLIP embed- dings along training rays, facilitating zero-shot, pixel-aligned queries without the need for region proposals or masks. This approach supports real-time generation of 3D relevancy maps for diverse language prompts, offering potential applications in robotics, vision-language model analysis, and interactive scene exploration.\nBuilding upon LERF, LangSplat [28] offers a more ef- ficient approach for creating 3D language fields using 3D Gaussian Splatting rather than NeRF. By encoding CLIP- based language features [29] into 3D Gaussians, LangSplat significantly reduces computational cost through a tile-based splatting method. Additionally, it incorporates a language autoencoder to lower memory usage and uses SAM for hierarchical semantics learning, improving object boundary"}, {"title": "III. METHOD", "content": "In this section, we outline the comprehensive methodol- ogy of our framework, namely Go-SLAM, which employs cutting-edge techniques to achieve high-precision and effi- cient 3D reconstruction of environments captured through RGBD cameras. To the best of our knowledge, this is the pioneering SLAM system to integrate language features, enabling open-vocabulary object detection and localization. Figure. 2 illustrates the overview of our framework. In the following sections, we will elaborate on each component in the pipeline.\nA. 3D Gaussian Splatting SLAM framework\nWe employ 3D Gaussian Splatting to represent the recon- structed environment. Our proposed method builds upon the SplaTAM [21] approach to implement a robust and efficient 3DGS SLAM system. We leverage the strengths of explicit volumetric representations using 3D Gaussians to enable high-fidelity reconstruction from a single RGBD camera.\n1) Differential 3D Gaussian Splatting representation: The core concept behind 3D Gaussian splatting is to represent the scene as a collection of Gaussians, where each Gaussian splat encodes key attributes like position, scale, orientation, color, opacity, and object association. These Gaussians act as probabilistic volumetric representations of the scene, al- lowing us to efficiently approximate the underlying geometry and appearance from multiple viewpoints.\nEach splat is modeled as a 3D Gaussian distribution with parameters:\n$G(x) = c. exp(-\\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x-\\mu))$\nwhere $x \\in R^3$ is a 3D point in space, $\\mu \\in R^3$ is the Gaussian center, representing the position of the splat, $\\Sigma\\in R^{3\\times3}$ is the covariance matrix, defining the scale and orientation of the splat in 3D space, and $c\\in R^3$ is the color vector (RGB) associated with the splat,\nThe exponential term models the spatial influence of the splat, decaying with distance from the center $\\mu$. Our framework uses this probabilistic approach to model both the geometry and appearance of the environment, with splats ef- ficiently representing 3D surfaces across multiple viewpoints.\nA key advantage of this 3DGS representation is that it allows for differentiable rendering, enabling us to optimize the parameters of each Gaussian (such as $\\mu$, $\\Sigma$, and $c$) through backpropagation. By rendering the scene from mul- tiple viewpoints and comparing the rendered images with ground truth, we can compute a loss function, such as L1 or L2 loss, and backpropagate the error to adjust the parameters. This optimization process refines the Gaussian representa- tions to better approximate the geometry and appearance of the scene, leveraging the differentiability of the rendering process to iteratively improve the 3D reconstruction.\n2) Tracking and Gaussian densification: For each cap- tured RGBD image, the framework back-projects each pixel (u, v) with depth d into 3D space using the camera's intrinsic matrix K, and converts it into a 3D Gaussian splat. The back- projection is computed as:\n$Xc = d. K^{-1} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}, \\qquad Xw = RcXc + tc$\nwhere $X_c$ and $X_w$ are the point coordinates in camera and world frames, respectively, $R_c$ and $t_c$ are the rotation and translation matrices representing the camera's pose.\nEach back-projected point is then converted into a Gaus- sian splat with its center $\\mu$ set to $X_w$, and its color $c$ taken from the corresponding pixel in the RGB image. We employ the camera tracking method introduced by [21] to estimate the camera pose for the current RGBD image. Camera parameters are optimized using gradient descent based on the L1 losses of the rendered colors and depths. Additionally, a silhouette mask is rendered to capture the density of the scene, which facilitates in quickly identify- ing previously mapped areas. This facilitates more efficient Gaussian densification for incoming RGBD images.\nB. Grounded object segmentation\nIn our framework, object segmentation plays a critical role in embedding semantic information into the reconstructed 3D environment. We use Grounding DINO [25] for object detec- tion and SAM [26] for instance segmentation, as inspired by"}, {"title": "C. Open-vocabulary object queries", "content": "Our framework supports open-vocabulary object querying, enabling users to search for objects in the 3D reconstructed environment using textual descriptions. The process involves matching the input query with detected object classes and refining the search to ensure precise identification of the queried object. The overall pipeline is shown in Figure. 4.\n1) Query matching: Given an input text query Q, we first compute the similarity between the CLIP embedding [29] of the input text and the CLIP embeddings of the detected object classes from the reconstructed environment. This allows us to determine the object class that most closely matches the input query based on semantic similarity.\nFor each detected object class $C_j$, we compute the cosine similarity between the CLIP embedding of the input query $E_Q$ and the embedding of the object class $E_{c_i}$:\n$s = cos(E_Q, E_{c_i})$\nThe object class with the highest matching score $s$ is selected as the best match for the query.\n2) Pruning the search space: Once the most similar object class is identified, we prune the search space to include only the keyframes that contain instances of this detected object class. This reduces the search complexity by narrowing the candidate frames where the queried object may be located. However, the detected object classes are typically in their most general form (e.g., \"table\" instead of \"dining table\" or \"chair\" instead of \"swivel chair\"). To ensure precise object identification, we rerun the grounded segmentation on these keyframes, refining the object boundaries to better match the specific object characteristics from the query. If the queried object is not found in the selected keyframes, the system expands the search to the remaining keyframes that were initially excluded. This ensures that objects potentially missed in the initial pruning phase are still considered during the search process.\n3) Object Localization: Once the object matching the query is identified, we backproject all its segmented pixels into 3D space and compute a 3D bounding box that encom- passes the object. This bounding box provides an approxi- mate spatial boundary of the object within the reconstructed environment.\nNext, we select all the 3D Gaussian splats that lie within this bounding box. Each Gaussian splat has an associated object ID, and the set of object IDs corresponding to the Gaussians inside the bounding box is denoted as $C_{in}$. Since the 3D bounding box may not fully capture the entire object, we expand the selection by including all Gaussians whose object IDs belong to $C_{in}$, ensuring that the complete object is included in the final set of Gaussians corresponding to the query Q:\n$G_Q = {G_k | ID(G_k) \\in C_{in}}$\nFor system evaluation and deployment, we utilized Gazebo (version 11.0) [34] in conjunction with ROS2 (Robot Oper- ating System 2) [35] to simulate complex 3D environments. Our navigation strategy employs the PRM (Probabilistic Road Map) [36] path planning algorithm. This algorithm operates on a point cloud generated from the Gaussian centers derived from the reconstructed environment. The center of the object's bounding box, as determined by our object detection algorithm, serves as the goal point for path planning. This approach enables efficient obstacle avoidance and precise navigation within the reconstructed 3D space. By integrating these components, our system demonstrates robust performance in localizing queried objects and gener- ating optimal paths for robot navigation, effectively bridging the gap between 3D scene reconstruction, semantic under- standing, and real-time robotic interaction."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "This section details the experimental setup, evaluation metrics, and results obtained from testing our Go-SLAM framework on different scene settings. The experiments are structured to evaluate the robustness and accuracy of our system in varied environments.\nA. Experimental Setup\nThe experimental setup for assessing the effectiveness of our grounded object segmentation method included a variety of environments. For controlled testing, we utilized a subset of 18 scenes from the Replica dataset [37].\nTo establish a benchmark for our system, we conducted comparative evaluations against two baseline models. The first baseline utilized the Faster R-CNN segmentation model while maintaining the rest of our proposed pipeline. The second baseline incorporated our advanced grounded object segmentation coupled with 3D Gaussian Splatting SLAM (3DGS SLAM); however, it diverged from our full method- ology by directly comparing the query against detected object labels, thereby omitting our carefully designed object matching algorithm. This approach allowed us to isolate the impact of our matching algorithm on the system's overall performance.\nB. Evaluation Metrics\nThe evaluation of model performance utilized precision and recall metrics, which assess the accuracy of identifying relevant objects and the capability to detect all human-labeled ground truth 3D bounding boxes, respectively. Additionally, the Intersection over Union (IoU) was measured, calculat- ing the overlap between the predicted bounding boxes and these human-labeled ground truth bounding boxes, thereby providing a quantitative measure of localization accuracy."}, {"title": "V. CONCLUSION", "content": "In conclusion, Go-SLAM introduces a novel approach to 3D scene reconstruction, combining Gaussian Splatting SLAM with state-of-the-art object segmentation and open-vocabulary querying. Our framework successfully integrates 3D reconstruction, object detection, and natural language understanding to enable real-time environmental interac- tions. Through comprehensive experiments, we demonstrated that Go-SLAM achieves higher precision, recall, and IoU compared to the baseline methods, particularly in handling complex, unknown environments. The system's ability to seamlessly embed object-level information into the 3D scene allows for flexible object localization and querying. Overall, Go-SLAM represents a significant step forward in SLAM technology, bridging the gap between scene reconstruction and semantic object understanding."}]}