{"title": "Effective Off-Policy Evaluation and Learning in Contextual Combinatorial Bandits", "authors": ["Tatsuhiro Shimizu", "Koichi Tanaka", "Ren Kishimoto", "Haruka Kiyohara", "Masahiro Nomura", "Yuta Saito"], "abstract": "We explore off-policy evaluation and learning (OPE/L) in contextual combinatorial bandits (CCB), where a policy selects a subset in the action space. For example, it might choose a set of furniture pieces (a bed and a drawer) from available items (bed, drawer, chair, etc.) for interior design sales. This setting is widespread in fields such as recommender systems and healthcare, yet OPE/L of CCB remains unexplored in the relevant literature. Typical OPE/L methods such as regression and importance sampling can be applied to the CCB problem, however, they face significant challenges due to high bias or variance, exacerbated by the exponential growth in the number of available subsets. To address these challenges, we introduce a concept of factored action space, which allows us to decompose each subset into binary indicators. This formulation allows us to distinguish between the \"main effect\" derived from the main actions, and the \"residual effect\", originating from the supplemental actions, facilitating more effective OPE. Specifically, our estimator, called OPCB, leverages an importance sampling-based approach to unbiasedly estimate the main effect, while employing regression-based approach to deal with the residual effect with low variance. OPCB achieves substantial variance reduction compared to conventional importance sampling methods and bias reduction relative to regression methods under certain conditions, as illustrated in our theoretical analysis. Experiments demonstrate OPCB's superior performance over typical methods in both OPE and OPL.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalizing decision-making using past interaction logs is a primary interest in many intelligent systems. In particular, off-policy evaluation and learning (OPE/L), which aim to evaluate or learn a new decision-making policy based only on logged data with no exploration, is considered crucial for mitigating potential risks and ethical concerns encountered in online learning and A/B testing. However, while many methods have been extensively explored in the contextual bandit setting with a single action, more complicated settings involving multiple actions remain sparse in the literature. In particular, one of the most underexplored involves OPE/L for contextual combinatorial bandits (CCB) where a policy chooses a subset in the action space to maximize the reward. CCB encompasses the following practical examples.\nExample 1.1 (Total Outfit Coordination). When the candidate items are earrings, a pendant, bracelet A, and bracelet B, a policy can recommend, e.g., (1) only earrings, (2) earrings and pendant, (3) both bracelets A and B, or (4) all items, depending on the user's profile to maximize the revenue or profit as the reward.\nExample 1.2 (Precision Medicine). Given medical data for each patient, medical institutions prescribe a combination of medications, e.g., (1) a fever reducer and cough suppressant, (2) cough suppressant, sinus medicine, and flu medicine, or (3) no medicine, to facilitate fast recovery from the disease.\nOPE/L using logged data in these CCB problems presents significant challenges due to the complexities of managing the action subset space, which is likely to be vast. Two typical OPE approaches, the Direct Method (DM) and Inverse Propensity Scoring (IPS), aim to estimate the policy value by imputing counterfactual rewards via regression or applying importance sampling in the action subset space, respectively. However, DM often encounters challenges with inaccurate regression due to model misspecification and the sparsity of the rewards. In contrast, IPS faces a critical variance issue, with importance weights growing exponentially as the"}, {"title": "2 PROBLEM FORMULATION", "content": "This section formulates the problem of OPE in the contextual combinatorial bandits (CCB). Let x \u2208 X \u2286 Rdx be a context vector such as user demographics, sampled from an unknown distribution p(x). We use A to denote some given set of actions, and s \u2208 S to denote a subset in A. For example, when A = {a1, a2}, the corresponding subset space is S = {{0}, {a1}, {az}, {a1, a2 }}. Given a context x, the decision-maker, such as a recommendation interface, chooses a subset in the action space, s, following a stochastic policy \u03c0, where \u03c0(s|x) is the probability of choosing a subset s in A given context x. Let r \u2208 [rmin, rmax] be a reward, drawn from an unknown conditional distribution p(r | x, s). We typically measure the effectiveness of policy n by the following policy value:\nV(\u03c0) := Ep(x)\u03c0(s|x)p(r|x,s) [r] = Ep(x)\u03c0(s|x) [q(x, s)],\nwhere q(x, s) := E [r | x, s] is the expected reward given x and s.\nThe goal of OPE/L is to accurately estimate V (\u03c0) of target policy \u03c0 or to maximize V (\u03c0) using only offline logged data. The logged data D = {(xi, Si, ri)}=1 ~ \u041f\u00b2=1 P(xi)\u03c0\u03bf(si|xi)p(ri|xi, si) contains n i.i.d. observations collected under a logging policy \u03c0\u03bf, which is often different from \u03c0. Particularly in OPE, we aim at developing an estimator V (\u03c0; D) that minimizes the MSE as an accuracy measure:\nMSE (V(\u03c0; D)) := \u03922 [(V(\u03c0; D) - V(x)\u00b2] = Bias [V(\u03c0; D)]\u00b2 + Var [V(\u03c0; D)],"}, {"title": "2.1 Applications of Typical Ideas", "content": "Although there is no existing literature that formally studies OPE for CCB, we discuss how to apply the conventional approaches, DM [3], IPS [15], and DR [9] to this setup, and their limitations.\nDM employs a regression model q(x, s) (\u2248 q(x, s)) to estimate the policy value by imputing the missing rewards as follows:\nVDM(\u03c0; D, \u011d) :=  \u03a3\u03c0(s|xi) [q(xi, 5)] =  \u03a3\u03a3\u03c0(s|xi)q(xi, s).\nwhere \u011d is optimized to minimize the estimation error against the rewards ri observed in the logged data. DM has low variance compared to other typical approaches, and is accurate when \u011d is accurate for all possible (x, s) \u2208 X \u00d7 S. However, DM incurs considerable bias because it becomes extremely difficult to accurately estimate the expected reward of subset s' that is not observed in the logged data, i.e., s' \u2260 si. This issue becomes particularly problematic as the number of subsets grows exponentially [43, 45].\nIn contrast, IPS re-weighs the rewards observed in the logged data, ri, using the ratio of the probabilities of observing subset si under the logging policy \u03c0\u03bf and target policy \u03c0:\nVIPs (\u03c0; D) :=  \u03a3\u03c0(Si Xi)ri =  \u03a3 w(xi, Si)ri,\nwhere w(x, s) = \u03c0(s|x)/\u03c0\u03bf(s|x) is called the vanilla importance weight. IPS is unbiased under the common support condition: \u03c0(s|x) > 0 = \u03c0\u03bf(\u03c2|x) > 0, \u2200(x, s) \u2208 X \u00d7 S. However, in order to satisfy this condition in the entire subset space S, \u03c0\u03bf has to allocate extremely small values to some of the subsets, resulting in substantial weight values and variance issue [40, 42].\nDR is the third baseline that is considered better than DM and IPS, and is defined as follows.\nVDR (\u03c0; D, 9) :=  {w(xi, Si) (ri \u2013 \u011d(xi, Si)) + \u0395\u03c0(s|x\u2081) [\u011d(xi, s)]} .\nDR is unbiased under the same support condition as IPS. Moreover, by employing the reward model \u011d, DR often reduces variance compared to IPS under a mild condition regarding the regression accuracy. However, the variance of DR remains extremely high when the action space is large [40, 42] as in our problem of CCB."}, {"title": "3 OUR APPROACH", "content": "In the previous section, we have seen the critical challenges of OPE in the problem of CCB. To tackle them, we first introduce a formulation of CCB based on the factored action space and binary indicators that indicate if each action in A is included in the selected subset s. Specifically, we factorize the action subset space S as\nS = {0, a1} \u00d7 {0, a2} \u00d7\u06f0\u06f0\u06f0 \u00d7 {0, a\u2081} =  M\u2081 := M\nwhere m\u2081 \u2208 M\u2081 = {0, a1} is a binary indicator; m\u2081 = 0 means that the action at is not chosen in the subset (a\u2081 & s), while m\u2081 = a means the opposite (al \u2208 s). The advantage of this factorized formulation is that it becomes possible to focus more on the indicators of specific key actions and to be able to treat them differently from other supplemental actions when constructing an estimator."}, {"title": "3.1 The OPCB Estimator", "content": "The primary idea of our OPCB estimator is to extract the main elements from the action set and distinguish them from other supplemental elements. For instance, a patient's recovery from a disease can mostly depend on some important medicines, but is also affected by some adjunctive medicines. To distinguish these effects, we consider the following decomposition of the expected reward.\nq(x,m) = g(x, \u03c6(m)) + h(x,m)\nwhere g(x, (m)) is the main effect accounted by the set of main actions (m), and h(x, m) is the residual effect that is not captured only by the main actions, as illustrated in Figure 1. Note that we do not impose any restrictions of the functional form of g and h; Eq. (1) is not an assumption. Note also that the function to extract the main actions, (m), is currently assumed somewhat given, but we will discuss how to optimize it from the logged data after analyzing the proposed estimator.\nBased on this reward function decomposition, OPCB estimates the main and residual effects via importance sampling and regression, respectively, as follows.\n\u0474\u041e\u0420\u0421\u0412 (\u03c0; \u0414, \u0444) := w(xi, (mi)) (ri mi+Ex(m\\x\u2081) [f(x,m)]}},\nwhere f(x, m) is some regression model whose optimization is discussed based on our theoretical analysis given in the following"}, {"title": "3.2 Theoretical Analysis", "content": "We first show OPCB's unbiasedness under the following conditions.\nCondition 3.1 (Common Support w.r.t the Main Actions). The logging policy \u03c0\u03bf is said to satisfy common support w.r.t the main actions for policy \u03c0 if \u03c0(\u03c6(m) |x) > 0 \u21d2 \u03c0\u03bf(\u03c6(m)|x) > 0 for all me Mand x \u2208 X.\nCondition 3.2 (Conditional Pairwise Correctness). A regression model f : X \u00d7 M \u2192 R is said to have conditional pairwise correctness if the following holds for all x \u2208 X and m, m' \u2208 M:\n$(m) = $(m') \u21d2 Aq(x, m, m') = f(x, m, m'),\nwhere Aq(x, m, m') := q(x, m) \u2013 q(x, m') is the relative difference of the expected rewards between m and m', and \u2206\u00ee(x, m, m') := f(x,m) \u2212 f(x, m') is its estimate.\nTheorem 3.3 (Unbiasedness of the OPCB estimator). Under Conditions 3.1 and 3.2, the OPCB estimator ensures unbiasedness. i.e., \u0415\u0414 [VOPCB (\u03c0; D, \u00a2)] = V(\u03c0).\nThe common support w.r.t the main actions (Condition 3.1) ensures unbiased estimation of the main effect g(x, (m)). Since this requires common support regarding only the main actions, this is milder than the common support condition of IPS. Conditional pairwise correctness (Condition 3.2) ensures unbiased estimation of the residual effect h(x, m). This condition is milder than the condition for DM to become unbiased because DM requires \u011d(x, s) = q(x, s) for all (x, s). Moreover, even when conditional pairwise correctness is not satisfied, we can characterize the bias of OPCB as follows.\nTheorem 3.4 (Bias of OPCB). Under Condition 3.1, the OPCB estimator has the following bias.\nBias (VOPCB (\u03c0; D, \u03c6)) = Ep(x)\u03c0(\u03c6(m)|x)\u03c0(m'|\u0445, \u0444(m))\u03c0(m|\u0445, \u0444(m)) (Aq(x, m, m') \u2013 Af(x, m, m'))\nwhere \u03c0(m|x, \u03c6(m)) = \u03c0(m|x){$(m) = (m)}/\u03c0(\u03c6(m)|x) and m, m' \u2208 M.\nTheorem 3.4 implies that three factors contribute to the bias of OPCB. The first factor (i) is the stochasticity of the logging policy conditional on the main actions ($(m)), which implies that the bias of OPCB becomes minimal when the conditional policy, \u03c0\u03bf(m|\u0445, \u0444(m)), is near deterministic. The second factor (ii) is the difference in the marginal importance weights of two action subsets that share the same main actions. This indicates that the bias becomes small when the distribution shift between policies regarding the supplemental actions is small. The final factor (iii) is estimation accuracy of the regression model f(x, m) against the relative difference in expected rewards Aq(x, m, m') only for $(m) = $(m'), suggesting that a more accurate estimation of the residual effects by f(x, m) results in a smaller bias of OPCB.\nIn contrast, the variance of OPCB is characterized by the scale of the marginalized importance weight and another aspect of f(x,m).\nTheorem 3.5 (Variance of OPCB). Under Conditions 3.1 and 3.2, the variance of the OPCB estimator is as follows.\nnVD [VOPCB(\u03c0; D, \u00a2)] = Ep(x)\u03c0\u03bf(m|x) [w(x, \u03c6(m))\u00b2\u03c3\u00b2 (x,m)]+Ep(x) [Vo(m|x) [w(x, (m)\u2206q,f(x,m)]]+Vp(x) [\u0395\u03c0(m|x) [q(x,m)]],\nwhere f(x,m) := q(x,m) \u2013 f(x,m) is the estimation error of f(x, m) against the expected reward function q(x, m) and o\u00b2 (x, m) := Vp(r|x,a) [r].\nTheorem 3.5 demonstrates that the variance of OPCB depends only on the importance weights of the main actions, i.e., w(x, (\u0442)). Since OPCB does not consider the importance weight regarding supplemental actions, we can expect a significant variance reduction compared to IPS and DR, which rely on the importance weight regarding the entire action subsets. Moreover, Eq. (5) shows that the variance of OPCB depends on the accuracy of the regression model f(x, m) against the expected reward q(x, m), not against the relative reward difference as in the bias expression. This implies that we can minimize the second term of the variance by minimizing A f(x,m) when deriving the regression model f (x, m)."}, {"title": "3.3 Optimizing Regression and Decomposition", "content": "Based on the theoretical analysis, this section discusses how to optimize a regression model f(x, m) to minimize the bias and variance of OPCB. We then discuss how to identify the appropriate main actions (m) to decompose the expected reward based only on the logged data to minimize the MSE of OPCB.\n3.3.1 Optimizing the Regression Model. The theoretical analysis suggests that optimizing the accuracy of the regression model f(x, m) against the relative reward difference Aq(x, m, m') and expected reward q(x, m) results in the reduction of the bias and variance of OPCB, respectively. We first deal with the bias of OPCB and"}, {"title": "3.4 Extension to Off-Policy Learning in CCB", "content": "We have thus far focused on evaluating the policy value of a new CCB policy using only the logged data by OPCB. In this section, we extend it to solve the OPL problem to learn a new CCB policy, aimed at optimizing the policy value. In particular, we use a policy gradient approach to learn the parameter of a parametrized CCB policy \u03c0\u03b6(m|x) to maximize the policy value, i.e.,\n\u03b6* = arg max V(\u03c0\u03b6).\nThe policy gradient approach updates the policy parameter via iterative gradient ascent as \u0160t+1 \u2190 \u03b6\u03b9 + \u03b7\u2207\u03b6V(\u03c0\u03b6\u2081), where \u03b7 is the learning rate. Since we have no access to the true policy gradient \u2207\u03b6V(\u03c0\u03b6) = Ep(x)\u03c0\u03b5(m|x) [q(x, m)\u2207r log \u03c0\u03b6(m|x)], we need to estimate it using D. We achieve this by extending OPCB as\n\u2207 VOPCB (\u03c0\u03b6; D, \u03c6) :=  \nWe can derive the bias and variance of this policy-gradient estimator similarly to the analysis of OPCB, which can be found in the appendix. Note that we call our policy gradient (PG) method to solve OPL based on Eq. (8) as OPCB-PG."}, {"title": "4 SYNTHETIC EXPERIMENTS", "content": "This section evaluates OPCB using synthetic data and identifies the situations where OPCB becomes particularly more effective in OPE/L compared to the baseline methods. Note that in our experiments, we focused on settings with a relatively small number of \"unique\" actions (|A| \u2264 10). This might seem small compared to real-world applications, but existing methods cannot even handle this due to their variance. We believe OPE/L for CCB with large unique action spaces, potentially leveraging structure in A as studied by, would be an interesting future topic."}, {"title": "4.1 Synthetic Data Generation", "content": "To generate synthetic data, we first define 200 synthetic users characterized by 5-dimensional context vectors (x) sampled from the standard normal distribution. Then, for each action subset m = (m1,\u2026, m\u2081) \u2208 M (or equivalently s \u2208 S), we simulate its expected reward via synthesizing the main and residual effects:\nq(x,m) = \u03bb \u00b7 g(x, true (m)) + (1 \u2212 \u03bb) \u00b7 h(x,m),\nwhere g(x, true (m)) and h(x, m) in Eq. (9) are defined rigorously in the appendix. A is an experiment parameter to control the contribution of the main effect, where the number of main actions in the true reward function is true (m)| = 3. Based on the above synthetic reward function, we sample reward r from a normal distribution whose mean is q(x, m) and standard deviation o is 3.0."}, {"title": "5 REAL-WORLD EXPERIMENT", "content": "This section conducts an OPE experiment on the real-world recommendation dataset called KuaiRec [14], which consists of recommendation logs of the video-sharing app, Kuaishou. Each record in the dataset includes a user ID, recommended video ID as action a, and the watch ratio of the recommended video as reward r\u2208 [0,\u221e), which represents the play duration divided by the video duration. Each user and video is associated with user and video features, which we consider as context x and features of action a, respectively. The key property of KuaiRec is that the user-item interactions are almost fully observed with nearly 100% density for the subset of its users and items (including 1,411 users and 3,327 items), meaning that the reward function is fully accessible. By leveraging this unique property, we can perform an OPE experiment on this dataset with minimal synthetic component [14].\nTo perform an OPE experiment on this dataset, we define the expected reward for each action subset as follows.\nq(x,m) = (\u03a3=1I{m=a1})-1\nwhere q(x, m\u2081) = q(x, a\u2081) when m\u2081 = a\u2081 and q(x, m\u2081) = 1 when m\u2081 = 0. We then sample the reward r from a normal distribution whose mean is q(x, m) and standard deviation \u03c3 is 3.0.\nWe define the logging and target policies following Eq. (10) and Eq. (11) in the synthetic experiments, but we replace the true expected reward q(x, m) with that estimated by ridge regression and we use \u1e9e = -0.3 and e = 0.1 in the real-world experiment. Finally, to simulate a realistic situation where the true MSE is inaccessible regarding the OPCB's data-driven optimization procedure in Eq. (7), we vary the estimation errors of the bias term (\u03c3 = 1.0, 3.0, 5.0) to sample the noise df in Eq. (12). A larger value of o indicates a lower accuracy in estimating the MSE to perform Eq. (7).\nResults. Figure 6 compares the MSE, Squared Bias, and Variance of the estimators with varying logged data sizes on KuaiRec. The results demonstrate a similar trend as observed in the synthetic experiment - OPCB performs consistently better than the baseline methods across various logged data sizes by effectively reducing the bias and variance. It would also be remarkable to see that OPCB brings in a significant reduction in MSE even with the largest noise on its bias estimation when performing the data-driven optimization procedure in Eq. (7). These results support the advantage of OPCB in the real CCB problems even with a noisy bias estimation."}, {"title": "6 CONCLUSION", "content": "This paper studied OPE/L for CCB for the first time. We start by identifying the bias and variance issues of the standard approaches, which arise due to the exponential growth of the action subset space. To tackle these issues, we proposed the OPCB estimator based on the formulation of CCB via the factored action space and the corresponding reward function decomposition. Our theoretical analysis highlights the conditions under which OPCB's bias and variance become particularly small. We also discussed a data-driven procedure to optimize the main elements in the action space to identify the appropriate reward decomposition to perform OPCB and extension of OPCB to an OPL method. Experiments on synthetic and real-world datasets demonstrated that OPCB enables far more accurate estimation and policy learning than the baseline methods in a variety of CCB problems."}, {"title": "A EXTENDED RELATED WORK", "content": "This section summarizes important related work in detail."}, {"title": "A.1 Combinatorial Bandits", "content": "The Combinatorial Multi-Armed Bandits (CMAB) problem, first introduced by, is the generalized version of the Multi-Armed Bandits (MAB) where an agent can choose a subset of actions from the action space in each round. In MAB, one aims to find a policy that minimizes regret, a gap between the accumulated expected reward achieved by the optimal policy and the given policy of interest. There are mainly two types of CMAB based on the difference in the reward feedback. The first one is based on semi-bandit feedback, in which an agent observes sub-rewards corresponding to each action in the chosen subset of the actions. The second one is full-bandit feedback, in which an agent cannot observe the sub-rewards for each action within an action subset but observes a single reward to"}, {"title": "A.2 Off-Policy Evaluation and Learning", "content": "Off-Policy Evaluation and Learning (OPE/L) is known as a safe and ethical alternative to online A/B tests and online learning, as OPE/L aims to evaluate or learn a new policy solely from offline logged data.\nIn OPE, there are mainly three approaches: model-based, model-free, and hybrid. A typical model-based approach, referred to as Direct Method (DM) [3], estimates the policy value using the estimated expected reward given context and action (\u011d(x, a)), learned by an off-the-shell supervised machine learning method. DM has low variance but may incur considerable bias when the estimated reward is inaccurate, which is often the case under partial reward, and covariate shift arises from logged data. In contrast, the model-free approach called Inverse Propensity Scoring (IPS) re-weighs the observed reward by the density ratio of actions between the logging and target policies. IPS achieves unbiasedness and consistency under common support and unconfoundedness. However, IPS is susceptible to a significant variance caused by a large action space. Although there are some techniques to reduce the scale of importance weight, such as clipping and normalizing, these transformation incurs nonnegligible bias instead. Doubly Robust (DR) [10] is a hybrid approach that uses the estimated reward as a control variable and applies importance sampling only on the residual. DR is unbiased either when IPS or DM is unbiased, and also reduces the variance of IPS due to the use of a control variate. However, the variance reduction is limited since DR still uses the large importance weight. Thus, DR suffers from significant variance when encountering large importance weights due to the exponential number of subsets to be considered in the CCB setting.\nTo deal with large action spaces in the single action (a \u2208 A) setting, uses auxiliary information, such as action embeddings, in OPE. In particular, Marginalized Inverse Propensity Scoring (MIPS) applies importance sampling on the action embedding space (8):\nVMIPS (\u03c0; D) :=  \u03a3p(eixi, \u03c0)ri =  w(xi, ei)ri,\nwhere e \u2208 & \u2286 Rde is an action embedding, sampled from a conditional distribution p(e|x, a). p(e|x, \u03c0) := \u03a3\u03b1\u03b5\u03c0\u03c0(a|x)p(e|x, a) is the probability of choosing action embedding e given context x induced by policy \u03c0, and w(x, e) = p(e|x, \u03c0)/p(e|x, \u03c0\u03bf) is the marginalized importance weight. By using the marginalized importance weight, we can expect a significant variance reduction as |8| becomes small compared to the original action space of |A|. However, the unbiasedness of MIPS depends on the satisfaction of the no direct effect assumption, which requires that action embedding e mediates all the effect of action a on reward r (i.e., a \u315b r|x, e). Thus, when we naively apply MIPS to the CCB settings, the action subset's embedding can be the concatenating of action embeddings, which can be high-dimensional, leading to a high variance issue as the original IPS has. To tackle the issue associated with high-dimensional embeddings, Off-policy evaluation estimator based on the Conjunct Effect Model (OffCEM) defines the estimator leveraging the clustered action space:\nVoffCEM (\u03c0; D) :=  {w(xi, p(xi, ai)) (ri \u2013 f(xi, ai)) + En(a|xi) [f(xi, a)]},\nwhere : X \u00d7 A \u2192 C is a clustering function, w(x, c) := \u03c0(c|x)/\u03c0\u03bf(c|x) is the importance weight in the cluster space C, \u03c0(c|x) := \u03a3\u03b1\u03b5\u03c0\u03c0(a|x)[{$(x, a) = c}, and f : X \u00d7 A \u2192 R is the estimator of the expected reward function. By applying cluster-wise importance weight, OffCEM reduces the variance of naive IPS. Moreover, OffCEM mitigates the bias of MIPS caused by the no-direct assumption, by introducing the regression estimation about the residual effect. In particular, OffCEM is unbiased under the local correctness ($(x, a) = \u00a2(x,b) \u2192 \u2206q(x,a,b) = \u2206 f(x, a, b) \u2200x \u2208 X, a, b \u2208 A) holds. In this paper, we consider a way to apply OffCEM in the CCB setting, where clustering of action subsets is often challenging. Specifically, we cluster action subsets by identifying important (main) actions, leveraging the newly introduced factored action space in CCB. Introducing the main actions in action subsets enables us to interpret the clusters of action subsets readily. Moreover, we provided a detailed data-driven procedure to obtain the main actions in a given action subset so that we can minimize the MSE of the proposed estimator in Section 3.3.2 with empirical guarantees on the performance of the optimization procedure in Section 4.1. Furthermore, we extended the OPCB estimator to the OPL in CCB to learn a better policy where only considered OPE in a single action space.\nIn OPL, there are two main approaches: regression-based approaches and policy gradient approaches. Regression-based approaches estimate the expected reward and then use it to define a new policy, such as the epsilon-greedy or softmax policies. Like DM in OPE, regression-based approaches often suffer from bias when the regression is inaccurate. In contrast, policy gradient approaches estimate the gradient of the parameterized policy value and use it to update the parameter of the policy via the iterative gradient ascent. IPS is the typical choice for the estimator of the policy gradient but it suffers from high variance. Our approach effectively reduces the variance of the policy gradient estimation by leveraging the OPCB estimator in CCB."}, {"title": "A.3 Off-Policy Evaluation for Ranking and Slate Policies", "content": "Ranking and slate settings are similar settings to CCB. In OPE for ranking policies, we consider the ordered list of the candidate items as the action, where the number of elements in the ordered list is predefined, and we can observe the position-wise rewards corresponding to each sub-action. Most of the estimators for ranking OPE are based on assumptions about how the users behave towards the ranked items, such as independently or from top to bottom. However, when we consider a combination of the candidate actions, the order of the chosen actions in an action subset does not matter, and we cannot observe the corresponding sub-reward to each unique action in an action subset. Thus, the existing estimators in ranking OPE are not applicable to evaluating action subsets.\nIn OPE for slate bandit policies, we aim to evaluate the slate policies where the slate consists of L slot actions, whose action set A\u2081 may differ across different slots (e.g., in online ads, A\u2081 can be the candidate set of slogans, A2 can be that of key visuals). Similar to the CCB settings, we cannot observe the slot-level rewards. To deal with the lack of observations of sub-rewards, PseudoInverse (PI) estimators assume that the total reward is linearly decomposable to sub-rewards corresponding to each slot (called linearity assumption). This reduces the importance weight from the product to the sum of slot-wise importance weight. However, PI incurs considerable bias when the linearity assumption often does not hold, which is often the case in practice due to the interaction effects among the elements in a slate. Another estimator called Latent IPS (LIPS) , which employs importance weight defined in a latent slate abstract space to mitigate the estimator's variance. However, it completely disregards the effect of the slate that the latent abstraction cannot capture, while our estimator is able to take the residual effect into account via regression."}, {"title": "B CONNECTION AND COMPARISON TO SLATE OPE ESTIMATORS", "content": "In this section, we further explore the connection and comparison of CCB settings to slate contextual bandit including formal definitions of the estimators in slate contextual bandits settings. Recall that, in CCB settings, we aim to evaluate a target policy \u03c0 : X \u2192 \u0394(S) that selects a subset s \u2208 S = 2A of the action space A. This problem can be reformulated by introducing a binary indicator, m\u2081 \u2208 M\u2081 = {0, a1}, that represents if each candidate action aj \u2208 A (e.g., bed, drawer, mirror) is in the selected subset s (i.e., aj \u2208 s or aj & s). This reformulation allows us to consider OPE with factored action space M := \u041f\u0406\u2208[L] M\u2081 = \u041fl\u2208[L]{0, a1}, which is instrumental in focusing on the specific primary items in the chosen set s, instead of an action subset space S. In contrast, in slate OPE, we aim to evaluate a target policy \u03c0 : X \u2192 \u0394(\u03a0\u0399\u2208[L] Al) that chooses a list of actions a = (a1,\u2026\u2026, a\u2081) \u2208 \u03a0\u03b9\u2208[L] A[ where we always choose one action aj \u2208 A for each slot I from the corresponding action set Aq := {al,1, a1,2,\u00b7\u00b7\u00b7, al,|A\u2081| }.\nTo bridge the gap between these two distinct setups, we can indeed come up with a more general formulation that includes them as special cases by introducing a more general action space M := \u03a01\u2208[L] M[ where, in each action space M\u012b, the agent can choose m\u2081 \u2208 M\u2081 = {0} U A\u2081, which is either an empty set 0 or the element aj \u2208 A\u012e. This allows us to consider the two-step decision process to decide what to include in a recommendation, e.g., in online ads, we first decide (1) which component (e.g., title, discount rate, slogan, etc.) to include in the ads, and then decide (2) which title to use (e.g., title 1, title 2, etc.) if the title is included in the combination. Under this general formulation of CCB and slate contextual bandits, the OPCB estimator in Eq. (2) can readily be applied to this generalized setting. The following paragraphs cover the comparison and connection of OPCB and two primary estimators for OPE in slate contextual bandits under the newly introduced general formulation of CCB and slate settings.\nPseudoInverse (PI) estimator. The PI estimator considers applying the element-wise importance weight, i.e., w(x, m\u2081) = \u03c0(m1|x)/\u03c0\u03bf(m1|x) to estimate the policy value as follows.\n\u0474\u03c1\u03b9 (\u03c0; D) :=  w(xi, mi,l) - L + 1 ri,\nPI reduces the variance of IPS, as the importance weight is reduced from to . PI is also unbiased when the expected reward is linearly decomposable as q(x, m) = \u03a3\u013a=1 \u03a6\u03b9(x, m\u2081), where {1}}=1 is some (intrinsic) element-wise reward function. However, this linearity assumption is often unrealistic in the combinatorial-slate setting because the reward always increases if the chosen elements increase. For example, in interior design, having too much furniture in a bedroom can negatively affect the reward. However, this linearity assumption implies that using all the elements is always the best. The proposed OPCB, in contrast, avoids such an unrealistic linearity assumption by taking the interactions among candidate actions into account.\nTo further show an interesting connection between OPCB and PI, let us introduce the following 1-main-element OPCB estimator:\nVOPCB (\u03c0; \u0414) :=  { w(xi, m\u2081,1) (ri - fi(xi, mi)) + \u0395\u03c0(m|xi) [f(x,m)] },\nwhere the main effect g(x, \u00a2(m)) = g(x, m\u2081) corresponds to the element-wise expected reward of candidate action m\u2081 and f\u2081(\u00b7, \u00b7) considers the residual effect h(x, m), including the interaction with other actions. Therefore, by taking the average of Vl VOPCB Over l\u2208 le [L], we can reproduce the linearity structure of PI while also taking the interaction as q(x, m) = \u2211[-\u2081 \u03c6(x, m\u2081) + H(x, m), where H(x, m) is the interaction effect. This estimator is strictly more general than PI and overcomes the limitation of introducing linearity assumption. We call this estimator \u00dbOPCB-PI(\u03c0; D) := \u03a3\u0399-1 VOPCB (\u03c0; D)/L as the OPCB-PI estimator."}, {"title": "C THEORETICAL ANALYSIS OF THE POLICY GRADIENT ESTIMATED BY THE OPCB ESTIMATOR", "content": "In this section, we provide a detailed theoretical analysis of the gradient of the OPCB estimator used for OPL in CCB proposed in Section 3.4. Overall, the bias and variance of the gradient of the OPCB estimator are similar to those of the"}]}