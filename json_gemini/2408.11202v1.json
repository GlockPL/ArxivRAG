[{"title": "Effective Off-Policy Evaluation and Learning in Contextual Combinatorial Bandits", "authors": ["Tatsuhiro Shimizu", "Koichi Tanaka", "Ren Kishimoto", "Haruka Kiyohara", "Masahiro Nomura", "Yuta Saito"], "abstract": "We explore off-policy evaluation and learning (OPE/L) in contextual combinatorial bandits (CCB), where a policy selects a subset in the action space. For example, it might choose a set of furniture pieces (a bed and a drawer) from available items (bed, drawer, chair, etc.) for interior design sales. This setting is widespread in fields such as recommender systems and healthcare, yet OPE/L of CCB remains unexplored in the relevant literature. Typical OPE/L methods such as regression and importance sampling can be applied to the CCB problem, however, they face significant challenges due to high bias or variance, exacerbated by the exponential growth in the number of available subsets. To address these challenges, we introduce a concept of factored action space, which allows us to decompose each subset into binary indicators. This formulation allows us to distinguish between the \"main effect\" derived from the main actions, and the \"residual effect\", originating from the supplemental actions, facilitating more effective OPE. Specifically, our estimator, called OPCB, leverages an importance sampling-based approach to unbiasedly estimate the main effect, while employing regression-based approach to deal with the residual effect with low variance. OPCB achieves substantial variance reduction compared to conventional importance sampling methods and bias reduction relative to regression methods under certain conditions, as illustrated in our theoretical analysis. Experiments demonstrate OPCB's superior performance over typical methods in both OPE and OPL.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalizing decision-making using past interaction logs is a primary interest in many intelligent systems. In particular, off-policy evaluation and learning (OPE/L), which aim to evaluate or learn a new decision-making policy based only on logged data with no exploration, is considered crucial for mitigating potential risks and ethical concerns encountered in online learning and A/B testing. However, while many methods have been extensively explored in the contextual bandit setting with a single action [9, 11, 17, 30, 37, 39, 40, 42, 46-48, 50, 57], more complicated settings involving multiple actions remain sparse in the literature [18-20, 28, 52]. In particular, one of the most underexplored involves OPE/L for contextual combinatorial bandits (CCB) where a policy chooses a subset in the action space to maximize the reward [5, 33]. CCB encompasses the following practical examples.\nExample 1.1 (Total Outfit Coordination). When the candidate items are earrings, a pendant, bracelet A, and bracelet B, a policy can recommend, e.g., (1) only earrings, (2) earrings and pendant, (3) both bracelets A and B, or (4) all items, depending on the user's profile to maximize the revenue or profit as the reward.\nExample 1.2 (Precision Medicine). Given medical data for each patient, medical institutions prescribe a combination of medications, e.g., (1) a fever reducer and cough suppressant, (2) cough suppressant, sinus medicine, and flu medicine, or (3) no medicine, to facilitate fast recovery from the disease.\nOPE/L using logged data in these CCB problems presents significant challenges due to the complexities of managing the action subset space, which is likely to be vast. Two typical OPE approaches, the Direct Method (DM) [3] and Inverse Propensity Scoring (IPS) [15], aim to estimate the policy value by imputing counterfactual rewards via regression or applying importance sampling in the action subset space, respectively. However, DM often encounters chal-lenges with inaccurate regression due to model misspecification and the sparsity of the rewards. In contrast, IPS faces a critical variance issue, with importance weights growing exponentially as the"}, {"title": "Contributions", "content": "To address the aforementioned challenges in handling the action subset space, we first formulate the problem of CCB via factorizing a subset of actions into the product of binary indicators that indicate if each action is included in the subset. This formulation enables us to focus on some of the influential actions referred to as the main actions, such as cold medicine and fever reducer for flu patients, among other supplemental medicines. We then introduce the concept of decomposing the expected reward of a subset of actions into the main and residual effects, which are the outcomes of the main actions and that of supplemental actions. Leveraging this reward function decomposition, we develop a novel estimator for OPE of CCB called Off-Policy estimator for Combinatorial Bandits (OPCB), which applies importance sampling only to the main actions to reduce the exponential variance of IPS, while dealing with the residual effect via regression to reduce the bias. OPCB ensures unbiasedness under a condition called conditional pairwise correctness, which requires that the regression model accurately estimate the relative reward difference of the subset of actions when the main actions are identical. In addition to the development of OPCB, considering a realistic situation where there is no prior knowledge about the main actions, we discuss a data-driven procedure to identify an appropriate set of main actions to minimize the mean squared error (MSE) of OPCB. Finally, empirical results on synthetic and real-world data reflecting the CCB problem demonstrate that our method evaluates a new CCB policy more precisely and learns a new CCB policy more efficiently than existing methods for a range of experiment configurations."}, {"title": "Related work", "content": "Combinatorial bandits to choose a subset in the action space to maximize the reward has been studied much in the online learning literature [5, 33], but there is no existing work discussing OPE/L in this setup thus far. Ranking OPE [19, 20, 24, 28] and slate OPE [18, 49, 55] study similar types of OPE/L problems. However, as shown in Table 1, the problem of CCB differs from both regarding the form of the action space. First, the ranking setup considers evaluating a ranking policy that aims to optimize the ranking of a given action space A, and in the logged data, the rewards corresponding to each position in the ranking are observable, which is unavailable in CCB. Thus, estimators developed for the ranking setup [19, 20, 24, 28] cannot be applied to solve OPE/L in"}, {"title": "2 PROBLEM FORMULATION", "content": "This section formulates the problem of OPE in the contextual combinatorial bandits (CCB). Let \\(x \\in \\mathcal{X} \\subseteq \\mathbb{R}^{d_x}\\) be a context vector such as user demographics, sampled from an unknown distribution \\(p(x)\\). We use \\(\\mathcal{A}\\) to denote some given set of actions, and \\(s \\in \\mathcal{S}\\) to denote a subset in \\(\\mathcal{A}\\). For example, when \\(\\mathcal{A} = \\{a_1, a_2\\}\\), the corresponding subset space is \\(\\mathcal{S} = \\{\\emptyset, \\{a_1\\}, \\{a_z\\}, \\{a_1, a_2 \\}\\} \\). Given a context \\(x\\), the decision-maker, such as a recommendation interface, chooses a subset in the action space, \\(s\\), following a stochastic policy \\(\\pi\\), where \\(\\pi(s|x)\\) is the probability of choosing a subset \\(s\\) in \\(\\mathcal{A}\\) given context \\(x\\). Let \\(r \\in [r_{\\min}, r_{\\max}]\\) be a reward, drawn from an unknown conditional distribution \\(p(r | x, s)\\). We typically measure the effectiveness of policy \\(\\pi\\) by the following policy value:\n\\[V(\\pi) := \\mathbb{E}_{p(x)\\pi(s|x)p(r|x,s)} [r] = \\mathbb{E}_{p(x)}[\\mathbb{E}_{\\pi(s|x)} [q(x, s)]],\\]\nwhere \\(q(x, s) := \\mathbb{E} [r | x, s]\\) is the expected reward given \\(x\\) and \\(s\\). The goal of OPE/L is to accurately estimate \\(V(\\pi)\\) of target policy \\(\\pi\\) or to maximize \\(V(\\pi)\\) using only offline logged data. The logged data \\(\\mathcal{D} = \\{(x_i, s_i, r_i)\\}_{i=1}^n \\sim \\Pi_{i=1}^n p(x_i)\\pi_0(s_i|x_i)p(r_i|x_i, s_i)\\) contains \\(n\\) i.i.d. observations collected under a logging policy \\(\\pi_0\\), which is often different from \\(\\pi\\). Particularly in OPE, we aim at developing an estimator \\(\\hat{V}(\\pi; \\mathcal{D})\\) that minimizes the MSE as an accuracy measure:\n\\[\\text{MSE}(\\hat{V}(\\pi; \\mathcal{D})) := \\mathbb{E}_{\\mathcal{D}}[ (\\hat{V}(\\pi; \\mathcal{D}) - V(x))^2 ]\\]\n\\[= \\text{Bias} [\\hat{V}(\\pi; \\mathcal{D})]^2 + \\text{Var} [\\hat{V}(\\pi; \\mathcal{D})],\\]"}, {"title": "2.1 Applications of Typical Ideas", "content": "Although there is no existing literature that formally studies OPE for CCB, we discuss how to apply the conventional approaches, DM [3], IPS [15], and DR [9] to this setup, and their limitations.\nDM employs a regression model \\(\\hat{q}(x, s) (\\approx q(x, s))\\) to estimate the policy value by imputing the missing rewards as follows:\n\\[\\hat{V}_{DM}(\\pi; \\mathcal{D}, \\hat{q}) := \\frac{1}{n} \\sum_{i=1}^n \\pi(s|x_i) [\\hat{q}(x_i, s)] = \\frac{1}{n} \\sum_{i=1}^n \\sum_{s \\in \\mathcal{S}} \\pi(s|x_i)\\hat{q}(x_i, s),\\]\nwhere \\(\\hat{q}\\) is optimized to minimize the estimation error against the rewards \\(r_i\\) observed in the logged data. DM has low variance compared to other typical approaches, and is accurate when \\(\\hat{q}\\) is accurate for all possible \\((x, s) \\in \\mathcal{X} \\times \\mathcal{S}\\). However, DM incurs considerable bias because it becomes extremely difficult to accurately estimate the expected reward of subset \\(s'\\) that is not observed in the logged data, i.e., \\(s' \\neq s_i\\). This issue becomes particularly problematic as the number of subsets grows exponentially [43, 45].\nIn contrast, IPS re-weighs the rewards observed in the logged data, \\(r_i\\), using the ratio of the probabilities of observing subset \\(s_i\\) under the logging policy \\(\\pi_0\\) and target policy \\(\\pi\\):\n\\[\\hat{V}_{IPS} (\\pi; \\mathcal{D}) := \\frac{1}{n} \\sum_{i=1}^n \\frac{\\pi(s_i |x_i)}{\\pi_0 (s_i|x_i)} r_i = \\frac{1}{n} \\sum_{i=1}^n w(x_i, s_i)r_i,\\]\nwhere \\(w(x, s) = \\frac{\\pi(s|x)}{\\pi_0(s|x)}\\) is called the vanilla importance weight. IPS is unbiased under the common support condition: \\(\\pi(s|x) > 0 \\implies \\pi_0(s|x) > 0, \\forall (x, s) \\in \\mathcal{X} \\times \\mathcal{S}\\). However, in order to satisfy this condition in the entire subset space \\(\\mathcal{S}\\), \\(\\pi_0\\) has to allocate extremely small values to some of the subsets, resulting in substantial weight values and variance issue [40, 42].\nDR is the third baseline that is considered better than DM and IPS, and is defined as follows.\n\\[\\hat{V}_{DR} (\\pi; \\mathcal{D}, \\hat{q}) := \\frac{1}{n} \\sum_{i=1}^n \\{w(x_i, s_i) (r_i - \\hat{q}(x_i, s_i)) + \\mathbb{E}_{\\pi(s|x_i)} [\\hat{q}(x_i, s)]\\} .\\]\nDR is unbiased under the same support condition as IPS. Moreover, by employing the reward model \\(\\hat{q}\\), DR often reduces variance compared to IPS under a mild condition regarding the regression accuracy. However, the variance of DR remains extremely high when the action space is large [40, 42] as in our problem of CCB."}, {"title": "3 OUR APPROACH", "content": "In the previous section, we have seen the critical challenges of OPE in the problem of CCB. To tackle them, we first introduce a formulation of CCB based on the factored action space and binary indicators that indicate if each action in \\(\\mathcal{A}\\) is included in the selected subset \\(s\\). Specifically, we factorize the action subset space \\(\\mathcal{S}\\) as"}, {"title": "3.1 The OPCB Estimator", "content": "The primary idea of our OPCB estimator is to extract the main elements from the action set and distinguish them from other sup-plemental elements. For instance, a patient's recovery from a disease can mostly depend on some important medicines, but is also af-fected by some adjunctive medicines. To distinguish these effects, we consider the following decomposition of the expected reward.\n\\[q(x,m) = g(x, \\phi(m)) + h(x,m)\\]\nwhere \\(g(x, \\phi(m))\\) is the main effect accounted by the set of main actions \\(\\phi(m)\\), and \\(h(x, m)\\) is the residual effect that is not captured only by the main actions, as illustrated in Figure 1. Note that we do not impose any restrictions of the functional form of \\(g\\) and \\(h\\); Eq. (1) is not an assumption. Note also that the function to extract the main actions, \\(\\phi(m)\\), is currently assumed somewhat given, but we will discuss how to optimize it from the logged data after analyzing the proposed estimator.\nBased on this reward function decomposition, OPCB estimates the main and residual effects via importance sampling and regression, respectively, as follows.\n\\[\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi) := \\frac{1}{n} \\sum_{i=1}^n \\frac{\\pi(\\phi(m_i)|x_i)}{\\pi_0(\\phi(m_i)|x_i)} \\{r_i - \\hat{h}(x_i, m_i) + \\mathbb{E}_{\\pi(m|x_i)}[\\hat{f}(x,m)]\\},\\]\nwhere \\(\\hat{f}(x, m)\\) is some regression model whose optimization is discussed based on our theoretical analysis given in the following"}, {"title": "3.2 Theoretical Analysis", "content": "We first show OPCB's unbiasedness under the following conditions.\nCondition 3.1 (Common Support w.r.t the Main Actions). The logging policy \\(\\pi_0\\) is said to satisfy common support w.r.t the main actions for policy \\(\\pi\\) if \\(\\pi(\\phi(m) |x) > 0 \\implies \\pi_0(\\phi(m)|x) > 0\\) for all \\(m \\in \\mathcal{M}\\) and \\(x \\in \\mathcal{X}\\).\nCondition 3.2 (Conditional Pairwise Correctness). A regression model \\(\\hat{f} : \\mathcal{X} \\times \\mathcal{M} \\rightarrow \\mathbb{R}\\) is said to have conditional pairwise correctness if the following holds for all \\(x \\in \\mathcal{X}\\) and \\(m, m' \\in \\mathcal{M}\\):\n\\[\\phi(m) = \\phi(m') \\implies \\Delta q(x, m, m') = \\Delta \\hat{f}(x, m, m'),\\]\nwhere \\(\\Delta q(x, m, m') := q(x, m) - q(x, m')\\) is the relative difference of the expected rewards between \\(m\\) and \\(m'\\), and \\(\\Delta \\hat{f}(x, m, m') := \\hat{f}(x,m) - \\hat{f}(x, m')\\) is its estimate.\nTheorem 3.3 (Unbiasedness of the OPCB estimator). Under Conditions 3.1 and 3.2, the OPCB estimator ensures unbiasedness. i.e., \\(\\mathbb{E}_{\\mathcal{D}} [\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi)] = V(\\pi)\\). See the appendix for the proof.\nThe common support w.r.t the main actions (Condition 3.1) ensures unbiased estimation of the main effect \\(g(x, \\phi(m))\\). Since this requires common support regarding only the main actions, this is milder than the common support condition of IPS. Conditional pairwise correctness (Condition 3.2) ensures unbiased estimation of the residual effect \\(h(x, m)\\). This condition is milder than the condition for DM to become unbiased because DM requires \\(\\hat{q}(x, s) = q(x, s)\\) for all \\((x, s)\\). Moreover, even when conditional pairwise correctness is not satisfied, we can characterize the bias of OPCB as follows.\nTheorem 3.4 (Bias of OPCB). Under Condition 3.1, the OPCB estimator has the following bias.\n\\[\\text{Bias} (\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi)) = \\mathbb{E}_{p(x)}\\pi(\\phi(m)|x)\\]\n\\[\\times \\sum_{\\substack{m<m': \\ \\phi(m)=\\phi(m')=\\phi(m)}}\\]\n\\[\\left(\\frac{\\pi(m|x, \\phi(m))}{\\pi_0(m|x, \\phi(m))} - \\frac{\\pi(m'|x, \\phi(m))}{\\pi_0(m' |x, \\phi(m))}\\right) \\left(\\Delta q(x, m, m') - \\Delta \\hat{f}(x, m, m')\\right),\\]\nwhere \\(\\pi(m|x, \\phi(m)) = \\frac{\\pi(m|x)I\\{\\phi(m) = \\phi(m)\\}}{\\pi(\\phi(m)|x)}\\) and \\(m, m' \\in \\mathcal{M}\\). See the appendix for the proof.\nTheorem 3.4 implies that three factors contribute to the bias of OPCB. The first factor (i) is the stochasticity of the logging policy conditional on the main actions (\\(\\phi(m)\\)), which implies that the bias of OPCB becomes minimal when the conditional policy, \\(\\pi_0(m|x, \\phi(m))\\), is near deterministic. The second factor (ii) is the difference in the marginal importance weights of two action subsets that share the same main actions. This indicates that the bias be-comes small when the distribution shift between policies regarding the supplemental actions is small. The final factor (iii) is estimation accuracy of the regression model \\(\\hat{f}(x, m)\\) against the relative dif-ference in expected rewards \\(\\Delta q(x, m, m')\\) only for \\(\\phi(m) = \\phi(m')\\), suggesting that a more accurate estimation of the residual effects by \\(\\hat{f}(x, m)\\) results in a smaller bias of OPCB.\nIn contrast, the variance of OPCB is characterized by the scale of the marginalized importance weight and another aspect of \\(\\hat{f}(x,m)\\).\nTheorem 3.5 (Variance of OPCB). Under Conditions 3.1 and 3.2, the variance of the OPCB estimator is as follows.\n\\[\\mathbb{V}_{\\mathcal{D}} [\\hat{V}_{OPCB}(\\pi; \\mathcal{D}, \\phi)] = \\mathbb{E}_{p(x)\\pi_0(m|x)} [w(x, \\phi(m))^2\\sigma^2 (x,m)]\\]\n\\[+\\mathbb{E}_{p(x)} [\\mathbb{V}_{\\pi_0(m|x)} [w(x, \\phi(m))\\Delta_{q,f}(x,m)]]\\]\n\\[+\\mathbb{V}_{p(x)} [\\mathbb{E}_{\\pi(m|x)} [q(x,m)]],\\]\nwhere \\(\\Delta \\hat{f}(x,m) := q(x,m) - \\hat{f}(x,m)\\) is the estimation error of \\(\\hat{f}(x, m)\\) against the expected reward function \\(q(x, m)\\) and \\(\\sigma^2 (x, m) := \\mathbb{V}_{p(r|x,a)} [r]\\). See the appendix for the proof.\nTheorem 3.5 demonstrates that the variance of OPCB depends only on the importance weights of the main actions, i.e., \\(w(x, \\phi(m))\\). Since OPCB does not consider the importance weight regarding supplemental actions, we can expect a significant variance reduction compared to IPS and DR, which rely on the importance weight regarding the entire action subsets. Moreover, Eq. (5) shows that the variance of OPCB depends on the accuracy of the regression model \\(\\hat{f}(x, m)\\) against the expected reward \\(q(x, m)\\), not against the rela-tive reward difference as in the bias expression. This implies that we can minimize the second term of the variance by minimizing \\(\\Delta_{q,\\hat{f}}\\hat{f}(x,m)\\) when deriving the regression model \\(\\hat{f} (x, m)\\)."}, {"title": "3.3 Optimizing Regression and Decomposition", "content": "Based on the theoretical analysis, this section discusses how to optimize a regression model \\(\\hat{f}(x, m)\\) to minimize the bias and variance of OPCB. We then discuss how to identify the appropriate main actions \\(\\phi(m)\\) to decompose the expected reward based only on the logged data to minimize the MSE of OPCB.\n3.3.1 Optimizing the Regression Model. The theoretical analysis suggests that optimizing the accuracy of the regression model \\(\\hat{f}(x, m)\\) against the relative reward difference \\(\\Delta q(x, m, m')\\) and expected reward \\(q(x, m)\\) results in the reduction of the bias and variance of OPCB, respectively. We first deal with the bias of OPCB and"}, {"title": "3.3.2 Optimizing the Set of Main Actions", "content": "The previous sections defined OPCB assuming that the set of main actions \\(\\phi(m)\\) is ap-propriately given. However, in practical applications, identifying the appropriate main actions to perform OPCB is non-trivial, even though this identification is crucial as we will see in the empirical section. When performing such an optimization, we should ide-ally optimize \\(\\phi\\) to minimize the MSE of OPCB, recalling that our technical goal is to build an estimator that minimizes the MSE:\n\\[\\phi^* = \\arg \\min_{\\Phi \\in \\mathcal{I}} \\text{Bias} (\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi))^2 + \\text{Var} (\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi)) .\\]\nHowever, the true MSE of any estimator is unknown as it depends on \\(V(\\pi)\\), so we propose to optimize the set of main actions \\(\\phi\\) via the following empirical formula.\n\\[\\phi^* = \\arg \\min_{\\Phi \\in \\mathcal{I}} \\text{Bias} (\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi))^2 + \\widehat{\\text{Var}} (\\hat{V}_{OPCB}(\\pi; \\mathcal{D}, \\phi)),\\]\nwhere \\(\\mathcal{I}\\) is a (finite) set of candidate functions \\(\\phi\\) and \\(\\widehat{\\text{Var}}(\\cdot)\\) is the sample variance of OPCB, which is defined as [57]:\n\\[\\widehat{\\text{Var}} (\\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi)) := \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{V}_{OPCB} (\\pi; \\mathcal{D}, \\phi)^2,\\]\nwhere \\(Y_i = w(x_i, \\phi(m_i))(r_i - \\hat{f} (x_i, m_i)) + \\mathbb{E}_{\\pi(m|x_i)}[\\hat{f}(x_i, m)]\\). In contrast, \\(\\widehat{\\text{Bias}}(\\cdot)\\) is an estimate of the bias of OPCB. There exist several methods to estimate the bias of an estimator using only the logged data [6, 12, 47, 54]. In the experiments, we empirically demonstrate that OPCB substantially outperforms the existing methods in a variety of environments even with a noisy estimate of the bias when performing Eq. (7)."}, {"title": "3.4 Extension to Off-Policy Learning in CCB", "content": "We have thus far focused on evaluating the policy value of a new CCB policy using only the logged data by OPCB. In this section, we extend it to solve the OPL problem to learn a new CCB policy, aimed at optimizing the policy value. In particular, we use a policy gradient approach to learn the parameter \\(\\zeta\\) of a parametrized CCB policy \\(\\pi_\\zeta(m|x)\\) to maximize the policy value, i.e.,\n\\[\\zeta^* = \\arg \\max_{\\zeta} V(\\pi_\\zeta).\\]\nThe policy gradient approach updates the policy parameter via iterative gradient ascent as \\(\\zeta_{t+1} \\leftarrow \\zeta_t + \\eta \\nabla_{\\zeta}V(\\pi_{\\zeta_t})\\), where \\(\\eta\\) is the learning rate. Since we have no access to the true policy gradient \\(\\nabla_{\\zeta}V(\\pi_\\zeta) = \\mathbb{E}_{p(x)\\pi_\\zeta(m|x)} [q(x, m)\\nabla_{\\zeta} \\log \\pi_\\zeta(m|x)]\\), we need to estimate it using \\(\\mathcal{D}\\). We achieve this by extending OPCB as\n\\[\\nabla_{\\zeta} \\hat{V}_{OPCB} (\\pi_{\\zeta}; \\mathcal{D}, \\phi)\\]\n\\[:= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\pi_{\\zeta} (\\phi(m_i)|x_i)}{\\pi_0(\\phi(m_i)|x_i)} \\{(r_i - \\hat{f} (x_i, m_i)\\nabla_{\\zeta} \\log \\pi_{\\zeta} (\\phi(m_i)|x_i)\\]\n\\[+ \\mathbb{E}_{\\pi_{\\zeta}(m|x_i)} [\\hat{f}(x_i, m)\\nabla_{\\zeta} \\log \\pi_{\\zeta}(m|x_i)]\\}.\\]\nWe can derive the bias and variance of this policy-gradient estima-tor similarly to the analysis of OPCB, which can be found in the appendix. Note that we call our policy gradient (PG) method to solve OPL based on Eq. (8) as OPCB-PG."}, {"title": "4 SYNTHETIC EXPERIMENTS", "content": "This section evaluates OPCB using synthetic data and identifies the situations where OPCB becomes particularly more effective in OPE/L compared to the baseline methods. Note that in our experiments, we focused on settings with a relatively small number of \"unique\" actions (\\(|A| \\leq 10\\)). This might seem small compared to real-world applications, but existing methods cannot even handle this due to their variance. We believe OPE/L for CCB with large unique action spaces, potentially leveraging structure in \\(\\mathcal{A}\\) as studied by [18, 36, 40, 42, 44], would be an interesting future topic."}, {"title": "4.1 Synthetic Data Generation", "content": "To generate synthetic data, we first define 200 synthetic users characterized by 5-dimensional context vectors (\\(x\\)) sampled from the standard normal distribution. Then, for each action subset \\(m = (m_1, \\dots, m_L) \\in \\mathcal{M}\\) (or equivalently \\(s \\in \\mathcal{S}\\)), we simulate its expected reward via synthesizing the main and residual effects:\n\\[q(x,m) = \\lambda \\cdot g(x, \\phi_{\\text{true}} (m)) + (1 - \\lambda) \\cdot h(x,m),\\]\nwhere \\(g(x, \\phi_{\\text{true}} (m))\\) and \\(h(x, m)\\) in Eq. (9) are defined rigorously in the appendix. \\(\\lambda\\) is an experiment parameter to control the con-tribution of the main effect, where the number of main actions in the true reward function is \\(|\\phi_{\\text{true}} (m)| = 3\\). Based on the above synthetic reward function, we sample reward \\(r\\) from a normal dis-tribution whose mean is \\(q(x, m)\\) and standard deviation \\(\\sigma\\) is 3.0."}, {"title": "6 CONCLUSION", "content": "This paper studied OPE/L for CCB for the first time. We start by identifying the bias and variance issues of the standard approaches, which arise due to the exponential growth of the action subset space. To tackle these issues, we proposed the OPCB estimator based on the formulation of CCB via the factored action space and the corresponding reward function decomposition. Our theoretical analysis highlights the conditions under which OPCB's bias and variance become particularly small. We also discussed a data-driven procedure to optimize the main elements in the action space to identify the appropriate reward decomposition to perform OPCB and extension of OPCB to an OPL method. Experiments on synthetic and real-world datasets demonstrated that OPCB enables far more accurate estimation and policy learning than the baseline methods in a variety of CCB problems."}, {"title": "A EXTENDED RELATED WORK", "content": "This section summarizes important related work in detail.\nA.1 Combinatorial Bandits\nThe Combinatorial Multi-Armed Bandits (CMAB) problem, first introduced by [13], is the generalized version of the Multi-Armed Bandits (MAB) where an agent can choose a subset of actions from the action space in each round. In MAB, one aims to find a policy that minimizes regret, a gap between the accumulated expected reward achieved by the optimal policy and the given policy of interest. There are mainly two types of CMAB based on the difference in the reward feedback. The first one is based on semi-bandit feedback [5, 7, 13, 22, 23, 27, 29, 32, 56, 58], in which an agent observes sub-rewards corresponding to each action in the chosen subset of the actions. The second one is full-bandit feedback [1, 8, 21, 34], in which an agent cannot observe the sub-rewards for each action within an action subset but observes a single reward to"}, {"title": "A.2 Off-Policy Evaluation and Learning", "content": "Off-Policy Evaluation and Learning (OPE/L) [9", "57": "is known as a safe and ethical alternative to online A/B tests and online learning", "approaches": "model-based", "3": "estimates the policy value using the estimated expected reward given context and action (\\(\\hat{q"}, "x, a)\\)), learned by an off-the-shell supervised machine learning method. DM has low variance but may incur considerable bias when the estimated reward is inaccurate, which is often the case under partial reward, and covariate shift arises from logged data. In contrast, the model-free approach called Inverse Propensity Scoring (IPS) [15"], "mathcal{E}\\))": "n\\[\\hat{V"}, {"mathcal{D})": "frac{1}{n} \\sum_{i=1}^n \\frac{p(e_i|x_i, \\pi)}{p(e_i|x_i, \\pi_0)} r_i = \\frac{1}{n} \\sum_{i=1}^n w(x_i, e_i)r_i,\\]\nwhere \\(e \\in \\mathcal{E} \\subseteq \\mathbb{R}^{d_e}\\) is an action embedding, sampled from a conditional distribution \\(p(e|x, a)\\). \\(p("}]