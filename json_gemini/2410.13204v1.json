{"title": "MEASURING FREE-FORM DECISION-MAKING INCONSISTENCY OF LANGUAGE MODELS IN MILITARY CRISIS SIMULATIONS", "authors": ["Aryan Shrivastava", "Max Lamparth", "Jessica Hullman"], "abstract": "There is an increasing interest in using language models (LMs) for automated decision-making, with multiple countries actively testing LMs to aid in military crisis decision-making. To scrutinize relying on LM decision-making in high-stakes settings, we examine the inconsistency of responses in a crisis simulation (\"wargame\"), similar to reported tests conducted by the US military. Prior work illustrated escalatory tendencies and varying levels of aggression among LMs but were constrained to simulations with pre-defined actions. This was due to the challenges associated with quantitatively measuring semantic differences and evaluating natural language decision-making without relying on pre-defined actions. In this work, we query LMs for free form responses and use a metric based on BERTScore to measure response inconsistency quantitatively. Leveraging the benefits of BERTScore, we show that the inconsistency metric is robust to linguistic variations that preserve semantic meaning in a question-answering setting across text lengths. We show that all five tested LMs exhibit levels of inconsistency that indicate semantic differences, even when adjusting the wargame setting, anonymizing involved conflict countries, or adjusting the sampling temperature parameter T. Further qualitative evaluation shows that models recommend courses of action that share few to no similarities. We also study the impact of different prompt sensitivity variations on inconsistency at temperature T = 0. We find that inconsistency due to semantically equivalent prompt variations can exceed response inconsistency from temperature sampling for most studied models across different levels of ablations. Given the high-stakes nature of military deployment, we recommend further consideration be taken before using LMs to inform military decisions or other cases of high-stakes decision-making.", "sections": [{"title": "INTRODUCTION", "content": "Language models (LMs) are capable of generating human-like text and recommendations from user-provided prompts and information. Sparking the curiosity of individuals, businesses, and governments alike, LMs have been adopted for decision-making across various industries such as healthcare (Berger et al., 2024; Eastwood, 2024) and finance (Maple et al., 2024). Conversations surrounding the adoption of artificial intelligence (AI) and language models (LMs) into militaries have also increased in recent years. For example, multiple news reports have surfaced in the past few years about the United States military testing LMs across their operations (Manson, 2023; Dou et al., 2024; Sentinent Digital, 2024). As a result of Task Force Lima (U.S. Department of Defense, 2023), the US Marine Corps and the US Army have adopted LMs to enhance battle planning and assist military commanders (Jensen & Tadross, 2023; Bello, 2024) and the US Air Force launched a GPT framework to advance wargaming techniques (Caballero & Jenkins, 2024). Industry actors are also getting involved, with Palantir developing a LLM-based chatbot targeted for military use"}, {"title": "RELATED WORK", "content": ""}, {"title": "COMPUTERS IN STRATEGIC DECISION-MAKING", "content": "Wargames are typically defined as strategy games that simulate an armed conflict (Dunnigan, 1992). Previous work has explored behavior of LMs in environments that require strategic reasoning (FAIR"}, {"title": "CONSISTENCY OF LANGUAGE MODELS", "content": "Previous work has explicitly studied the consistency of LMs in varying environments. For example, LMs exhibit poor levels of consistency for general knowledge questions (Saxena et al., 2024) and for ambiguous moral scenarios (Scherrer et al., 2024). Another study showed that LMs respond inconsistently to semantically equivalent prompts (Ye et al., 2023). On the other hand, LMs are relatively consistent across paraphrases and within topics, but some inconsistencies remain, particularly on controversial topics (Moore et al., 2024). Recently, Manakul et al. (2023a) and Farquhar et al. (2024) showed that inconsistency can be indicative of LM hallucinations with the underlying idea that higher levels of inconsistency indicate lower levels of confidence in the given response. LMs were tested in the high-stakes setting of automated mental health care and it was found that models exhibit inconsistency in the safety of their user responses (Grabb et al., 2024)."}, {"title": "BERTSCORE-BASED INCONSISTENCY METRIC", "content": "A core aspect of our analysis depends on choosing a sufficient metric to quantitatively measure inconsistency of free-form responses. Evaluating dissimilarity of natural language is a difficult task. In particular, one can say semantically similar things in many different ways. For example, the phrase people like foreign cars is very semantically similar to the phrase consumers prefer imported cars. Some metrics that rely on n-gram matching do not capture semantic similarities in structurally different texts, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee & Lavie, 2005). On the other hand, BERTScore better captures semantic similarities between texts by computing token similarity using contextual embeddings (Zhang* et al., 2020).\nSpecifically, a tokenized reference text $x = (x_1,...,x_n)$ and a comparison text $y = (y_1,\u2026\u2026,y_m)$ are mapped to a sequence of vectors $(x_1,...,x_n)$ and $(y_1,..., y_m)$ by an embedding model. The underlying embedding model is BERT Devlin et al. (2018), which creates token embeddings conditioned on both the left and right context of the surrounding text in all layers. Then the cosine"}, {"title": "VALIDATING INCONSISTENCY SCORE FOR QUESTION-ANSWERING", "content": "To validate that the inconsistency score can also be used to capture inconsistency in free-form text responses in a question-answering setting, we further scrutinize its ability to capture semantic differences while ignoring structural ones."}, {"title": "METHODOLOGY", "content": "To perform this analysis, we generated a text corpus containing a diverse array of topics by prompting an LM (GPT-40 mini)\u00b3 to answer all questions from the TruthfulQA dataset (Lin et al., 2022) four separate times - each time, we ask the LM to respond with different output lengths. To elicit the robustness and sensitivity of the performance of the inconsistency score across different types of linguistic variations, we define four types of textual ablations: lexical substitution, syntactic restructuring, addition of irrelevance, and semantic shift. Lexical substitution refers to replacing words from the reference text by synonyms that do not change the overall syntactic structure or semantic meaning of the reference text. Syntactic restructuring refers to changing word order or even full sentence orders while preserving the semantic meaning of the reference text. Addition of irrelevance refers to appending one sentence of irrelevant information to the end of the reference text. Semantic shift refers to changing the entire semantic meaning of the sentence, but attempting to preserve the lexical and syntactic form of the reference as much as possible. We employ an LM (GPT-40 mini) to apply each ablation to a particular output length, so we compare texts with similar output lengths. We verified that this is akin to what we do in our main analysis. See Appendix C for full prompt details. We also note that we tested a metric based on bi-directional entailment clustering (Kuhn et al., 2023). A discussion of our tests can be found in Appendix E."}, {"title": "RESULTS", "content": "In Figure 1, we plot the effects that different text ablations had on our inconsistency score. Encouragingly, we find that lexical substitution and syntactic restructuring generate the least inconsistency. Thus, the inconsistency score is able to emphasize semantic meaning in texts, even if the lexical or syntactic form of the sentence is changed. There is no relationship between inconsistency score and text length, showing that the metric remains reliable across texts of similar length. The decaying relationship observed for addition of irrelevance is expected because as output length increases, the one sentence of irrelevance makes up a smaller portion of the whole text. Shifting the semantics of the reference text while maintaining as much lexical and syntactic form as possible generated the highest inconsistency score. This shows that our score, and more generally BERTScore, is robust to structural differences that do not affect semantic meaning and is able to capture semantic differences despite minimal changes to lexical or syntactic form in a question-answering setting."}, {"title": "How TO INTERPRET THE INCONSISTENCY SCORE?", "content": "Because BERTScore originally assigns scores of 1 to identical texts, our inconsistency score will generate a score of 0 when comparing two identical texts. A score of 1 typically implies that the two texts are not related semantically or lexically. For example, comparing the texts i like apples and she dislikes driving would yield an inconsistency score of 1. The mean inconsistency scores produced by textual ablations (without addition of irrelevance) are as follows: Lexical substitution 0.08, syntactic restructuring: 0.17, and semantic shift: 0.37. The average text lengths for our later experiments vary between models but are in the range of 50 to 120 tokens. Thus, we conservatively take scores greater than or equal to 0.25 to imply at least some semantic variation between texts."}, {"title": "MEASURING INCONSISTENCY IN HIGH-STAKES DECISION-MAKING", "content": "To measure the inconsistency of LMs in a high-stakes military setting, we have LMs play a wargame. The wargame is the same used in Lamparth et al. (2024), with slight modifications to fit the focus of this work. It is originally based on a fictitious, but plausible (Cancian et al., 2023; Pettyjohn et al., 2022), crisis between the United States (\"player nation\") and the People's Republic of China (\"counterpart\") in the Taiwan Strait (\"disputed territory\"). This setting is motivated by reported real-world government tests (Manson, 2023; Dou et al., 2024).\nHere, we outline two experiments, which we call the Initial Setting experiment and the Continuations experiment. A schematic of both experimental setups can be seen in Figure 2. In the Initial Setting experiment, we provide the LM with the general scenario brief and an initial description of the ongoing crisis. This context outlines the initial heightening military tensions between a player nation and counterpart over a disputed territory. Then, we prompt the LM to provide a recommended course of action to the president of the player nation in the wake of this developing crisis. We provide the full prompt in Appendix A.1.\nThe Continuations experiment builds off of the context provided in the Initial Setting experiment to explore how different degrees of escalation influence response inconsistency. We give the LM one of two continuations to respond to: status quo and revisionist. Both continuations involve one of the"}, {"title": "INITIAL SETTING EXPERIMENT", "content": "In the left plot of Figure 3, we plot the results of this experiment. We find that each of the five studied models exhibits inconsistency far beyond what one would expect from mere lexical substitutions or syntactic restructurings. That is, we may reasonably infer that each model tends to generate responses that are semantically dissimilar. As a qualitative illustration, we provide an example response pair in Figure 4. Further example response pairs are provided in Appendix B.1, which also include pairs from the Continuations experiment. We also observe significant differences in response inconsistency between models. We show that Claude 3.5 Sonnet and GPT-40 mini exhibit the least response inconsistency, with GPT-4 exhibiting the highest response inconsistency. In a more fine-grained analysis of our results, we find that no individual pair of responses is semantically consistent for all settings and models."}, {"title": "CONTINUATIONS EXPERIMENT", "content": "In the right plots of Figure 3, we plot the results of this experiment. For each model, we show that response inconsistency decreases with both wargame continuations relative to inconsistencies observed in the Initial Setting experiment. In Claude 3.5 Sonnet, GPT-40, and GPT-40 mini, the"}, {"title": "EFFECT OF ANONYMIZATION OF COUNTRY NAMES", "content": "We also study the effect of anonymizing country names on inconsistency. We change all mentions of explicit country information in the original prompts with color names. This is common practice in historically influential wargames (e.g., National Defense University, 1983; United States Joint Forces Command, 2002). We do this to see whether any underlying bias related to countries affects inconsistency.\nWe find that anonymizing country information does not significantly change response inconsistency across most studied models across both experiments. Thus, decision-making inconsistency within the wargame is not affected by any underlying bias pertaining to countries held by the studied LMs. Inconsistency was only significantly different between explicit and anonymous country names in both continuations for Claude 3.5 Sonnet, and for just the status quo continuation for GPT-40 mini. Interestingly, in Claude 3.5 Sonnet, we see an inverse relationship between the status quo and revisionist continuations when anonymizing country information than we did for explicit: significantly higher inconsistency in the revisionist continuation than in the status quo one. No other model across both the explicit and the anonymized wargame exhibited this behavior. See Figure 5 for full results."}, {"title": "TEMPERATURE VARIATIONS", "content": "Because the temperature parameter is inherently tied to the randomness of a model's outputs, we ablate the temperature parameter to determine how inconsistency is affected. To do this, we ran"}, {"title": "INCONSISTENCY DUE TO PROMPT SENSITIVITY VERSUS TEMPERATURE", "content": "We examine to what extent LMs provide consistent answers when faced with slight prompt ablations and compare the results to the inconsistency observed in the previous experiments, which were a result of the inherent stochasticity of temperature sampling. Considering that LMs in military contexts are likely to be deployed with human oversight, it should be expected that the queries themselves will be differently phrased even when wargaming identical scenarios. Previous work has demonstrated that LMs can produce poorly consistent outputs to semantically similar queries (Ye et al., 2023). We conduct two main experiments that are designed to demonstrate to what extent LMs exhibit inconsistency when faced with non-identical prompts that call for equivalent decision-making while playing a military crisis simulation. Additionally, we compare these results to observed inconsistency due to temperature sampling.\nWe induce Level One ablations that entirely preserve the semantics of the prompt (e.g. by using synonyms and changing sentence structure). We induce Level Two ablations that change more"}, {"title": "INCONSISTENCY DUE TO LEVEL ONE PROMPT ABLATIONS", "content": "The center plot of Figure 7 depicts the inconsistency scores of the studied LMs under Level One prompt ablations. We find that, compared to the results shown in the Initial Setting experiment, all studied models exhibit significantly lower levels of inconsistency. We find that GPT-40 mini continues to display the lowest level of inconsistency while GPT-3.5 Turbo now display the highest level of inconsistency. However, we still find that inconsistency scores still remain above what one would expect between semantically similar responses.\nCompared to inconsistencies observed when conducting the Initial Setting experiment outlined in Section 5 (which employed temperature T = 1.0), we observe that each of the studied models exhibit significantly less inconsistency. When comparing these results with the results discussed in Section 5.4, we find that inconsistency as a result of prompt ablations that completely preserve semantic similarity is comparable to inconsistency due to temperature T = 0.2 for all models except GPT-4. GPT-4 is the only studied model whose inconsistency due to semantic preserving prompt ablations is less than inconsistency due to sampling with temperature T = 0.2. Because we observe"}, {"title": "INCONSISTENCY DUE TO LEVEL TWO PROMPT ABLATIONS", "content": "The right-most plot of Figure 7 depicts the inconsistency scores of the studied LMs under Level Two prompt ablations. We find that LMs respond with higher levels of inconsistency. We additionally show that the observed inconsistency levels are approximately comparable to those observed in the original Initial Setting experiment for GPT-40 mini only. In fact, GPT-4 is significantly more inconsistent when responding to identical prompts with T = 1.0 while GPT-3.5 Turbo is significantly less inconsistent when responding to identical prompts with T = 1.0 . Again comparing these results with those discussed in Section 5.4, we find that inconsistencies due to these more meaningful ablations leads to inconsistency scores comparable to inconsistency due to temperatures closer to T = 0.6 or T = 0.8 for all the studied models with the exception of GPT-3.5 Turbo. GPT-3.5 Turbo's inconsistency in this setting was comparable to inconsistency with T = 1.2.\nIn real-world applications both semantically similar and semantically different prompt variations would be present. Thus, it is reasonable to assume that one can expect inconsistencies greater than or equal to the inconsistencies observed in the present experiments, even with T = 0.0."}, {"title": "CONCLUSION", "content": "In this work, we had five off-the-shelf LMs play a wargame and demonstrated their tendency to give inconsistent responses despite being given the same prompts. We find that inconsistency persists, although to varying degrees, with different levels of escalation. We also show that this inconsistency persists due to slight prompt ablations that preserve semantic meaning, even when the temperature is set to 0.0. Additionally, masking bias by anonymizing country names did not significantly impact LM inconsistency. Finally, we observed that even with low temperature levels, LMs still behave inconsistently. To measure inconsistency, we used a BERTScore-based metric, which we validated was able to ignore textual ablations, emphasizing semantic differences. Future work concerned with free-form semantic consistency evaluations may then use BERTScore for analysis. Given that we find a tendency for inconsistency across various wargame settings, we recommend policymakers and military officials to deeply examine LM behavior in high-stakes military settings. Inconsistent responses may lead to high volatility in decision-making, resulting in unpredictability.\nLimitations: The wargame may not fully capture the complexities of real world military crises as we only use one type of conflict concerning just three countries. Examining LM behavior across different types of crises and involved countries would strengthen future studies. Also, in Section 6, we assume that the level 2 ablations call for the same decision-making. However, latent knowledge about different, although similar, settings might affect decision-making. Additionally, we only test"}]}