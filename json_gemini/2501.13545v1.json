{"title": "LLMS CAN PLAN ONLY IF WE TELL THEM", "authors": ["Bilgehan Sel", "Ruoxi Jia", "Ming Jin"], "abstract": "Large language models (LLMs) have demonstrated significant capabilities in nat-\nural language processing and reasoning, yet their effectiveness in autonomous\nplanning has been under debate. While existing studies have utilized LLMs with\nexternal feedback mechanisms or in controlled environments for planning, these\napproaches often involve substantial computational and development resources\ndue to the requirement for careful design and iterative backprompting. Moreover,\neven the most advanced LLMs like GPT-4 struggle to match human performance\non standard planning benchmarks, such as the Blocksworld, without additional\nsupport. This paper investigates whether LLMs can independently generate long-\nhorizon plans that rival human baselines. Our novel enhancements to Algorithm-\nof-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in\nplanning benchmarks out-competing prior methods and human baselines all au-\ntonomously.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) based on the transformer architecture (Vaswani, 2017) have emerged\nas a transformative force in artificial intelligence, revolutionizing natural language processing and\ndemonstrating remarkable capabilities across diverse domains. These models, trained on vast cor-\npora of text data, have shown prowess not only in language-related tasks but also in problem-solving\n(Huang & Chang, 2022), reasoning (Brown, 2020; Chowdhery et al., 2022), and even coding (Chen\net al., 2021; Thoppilan et al., 2022). The rapid advancements in AI technology have sparked in-\ntense interest in exploring their potential for more complex cognitive tasks, reinforcement learning\n(Khattar & Jin, 2023; Khattar et al., 2024; Gu et al., 2024; 2025; Meimand et al., 2023), control\n(Sel et al., 2021a; Gunes et al., 2023; Coskun et al., 2022; Sel et al., 2021b; ul Abdeen et al., 2024;\n2022), optimization (Jin et al., 2023; 2024; Al-Tawaha et al., 2023; Khattar & Jin, 2024; Al-Tawaha\n& Jin, 2024; Yang et al., 2023), federated-learning (Khan et al., 2023), cyber-security (Manzoor\net al., 2024; Roy et al., 2024a; Cody et al., 2022; Huang et al., 2022; Wang et al., 2024), including\nsequential decision-making and planning. These efforts have yielded promising results, showcasing\nthe models' ability to generate solutions for a wide array of challenges (Huang & Chang, 2022;\nSuzgun et al., 2022). However, as the complexity of tasks increases, particularly in domains requir-\ning long-horizon planning and precise execution, the limitations of current LLM-based approaches\nbecome apparent (Yao et al., 2022; Long, 2023; Valmeekam et al., 2023; Sel et al., 2024a).\nOne of the primary challenges in utilizing LLMs for planning tasks is their inherent difficulty in self-\nverifying outputs (Stechly et al., 2024; Liu et al., 2024b; Roy et al., 2024b). This limitation manifests\nin various ways, from suggesting potentially illegal actions to failing to recognize whether a goal"}, {"title": "RELATED WORK", "content": "Sequential Decision-Making with LLMs. Having been trained on a large corpus of world-wide\ntext, LLMs excel at understanding a wide range of topics that helps them coming up with possible\ncontinuations. The earliest works have observed improvements over standard prompting (Brown,\n2020) for general problem solving, where we directly expect the model to generate the steps one after\nthe other, by step-by-step reasoning by transforming the original problem to a sequential decision-\nmaking one, e.g., CoT (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2022)."}, {"title": "PROMPTING METHODOLOGIES FOR PLANNING PROBLEMS", "content": "To understand the challenges faced by Large Language Models (LLMs) in planning tasks, it is\ncrucial to distinguish between myopic and planning problems (Keeney, 1993; Bertsekas, 1995):\nMyopic Problems. A myopic problem is a task that can be solved through simple reasoning and\nmemorization, typically requiring a straightforward, step-by-step approach without the need for\nlong-term strategy or consideration of future consequences.\nPlanning Problems. A planning problem is a task that requires the ability to formulate a sequence\nof actions to achieve a specific goal, often involving multiple steps, consideration of future states,\nand the ability to backtrack or revise the plan based on intermediate outcomes.\nThe key distinction lies in the cognitive processes required for each type of problem. Myopic prob-\nlems can often be solved using a predetermined set of steps, making them amenable to simple\nprompting techniques. Planning problems, however, demand a more sophisticated approach that\nincorporates: Self-doubt and verification (the ability to question and verify each step's validity and\nits contribution to the overall goal); Heuristic reasoning (the use of intuition or learned strategies to\nguide the exploration of more promising solution paths); Backtracking (the capability to recognize\ndead-ends and return to previous states to explore alternative paths); and State-tracking (maintaining\nan accurate representation of the current problem state throughout the solution process). These re-\nquirements pose significant challenges for LLMs, which are primarily trained on static text corpora\nand may lack explicit training in dynamic problem-solving scenarios. This discrepancy manifests in\ncurious phenomena: LLMs can often generate code/plans to solve planning problems but struggle to\nexecute the same logic in natural language reasoning tasks. We posit that this disconnect stems from\nthe nature of the training data and the inherent limitations of current prompting methodologies."}, {"title": "THE INCOMPATIBILITY OF CHAIN-OF-THOUGHT IN PLANNING PROBLEMS", "content": "Chain-of-Thought (CoT) prompting has emerged as a popular technique for enhancing LLMs' rea-\nsoning capabilities. However, current literature reveals fundamental incompatibilities between CoT\nand the requirements of planning problems:\n1. Linear thinking: CoT encourages a linear progression of thoughts, which is often insuffi-\ncient for problems requiring exploration of multiple paths or backtracking (Stechly et al.,\n2023; Sel et al., 2024a).\n2. Lack of self-correction: The step-by-step nature of CoT does not inherently support the\nrecognition and correction of mistakes made early in the reasoning process (Yao et al.,\n2022).\n3. Overreliance on example structure: LLMs tend to mimic the structure of provided ex-\namples, leading to rigid thinking patterns that may not generalize well to novel problem\ninstances Sel et al. (2024b).\nTo illustrate these limitations, we conducted experiments using the Game of 24, a simple yet illus-\ntrative planning problem with a depth of 3 and a maximum breadth of 48. Figure 2 demonstrates\nhow major LLMs, when presented with CoT examples, tend to produce responses that stylistically\nmatch the examples but often fail to arrive at correct solutions.\nThis observation underscores a critical insight: the effectiveness of prompting techniques can be\nheavily influenced by the distribution of problem-solving approaches in the training data. The preva-\nlence of step-by-step solutions in educational contexts may inadvertently bias frontier LLMs towards\nCoT-like reasoning, limiting their ability to adapt to problems requiring more flexible thinking.\nWhile traditional planning algorithms like A* or MCTS can cleanly separate planning from exe-\ncution, this separation becomes less clear when considering LLM-based planning. In real-world\napplications where we rely on LLMs, the action space is often vast or infinite, and the execution\nitself may require complex natural language generation (e.g., creative writing) or reasoning (e.g.,\ncrossword puzzles) that cannot be easily reduced to simple programmatic execution. Even in seem-\ningly straightforward domains like Blocksworld, our experiments reveal that LLMs struggle with\nmaintaining accurate state representations during plan execution, as evidenced by the increasing\nerror rates in both CoT and vanilla AoT approaches (see Appendix A.1)."}, {"title": "ALGORITHM-OF-THOUGHTS PROMPTING FOR PLANNING", "content": "The Algorithm-of-Thoughts (AoT) prompting technique represents a significant advancement in\naddressing the limitations of CoT for planning problems. Key features of AoT include:\n\u2022 Explicit search process: AoT incorporates a more verbose description of the problem-\nsolving steps, including exploration of multiple paths.\n\u2022 Backtracking examples: In-context examples demonstrate the process of backtracking\nwhen reaching dead-ends, teaching LLMs that direct paths to solutions are not always\navailable.\n\u2022 Heuristic guidance: AoT prompts include human-like intuitions to guide the search pro-\ncess, mimicking expert problem-solving strategies.\nAoT shows marked improvements over CoT in various planning domains, including the Game of\n24, crossword puzzles, and creative writing tasks. However, AoT is not without its drawbacks:\n1. Complexity of prompt creation: The requirement for human-like intuitions in the search\nprocess makes crafting effective AoT prompts time-consuming and challenging.\n2. Potential for bias: The inclusion of human heuristics may inadvertently introduce biases\nor limit the LLM's ability to discover novel solution strategies.\n3. State hallucination: While AoT reduces false positives (invalid solutions), it still struggles\nwith accurately maintaining the problem state throughout the reasoning process.\nThe issue of state hallucination is particularly intriguing. Our analysis reveals that these hallucina-\ntions occur not just at the conclusion of the reasoning process but throughout the solution attempt.\nThis suggests that while AoT improves the overall planning capabilities of LLMs, it does not fully\naddress the fundamental challenge of maintaining an accurate internal representation of the problem\nstate.\nThese findings motivate our research into more advanced prompting techniques that can better lever-\nage the latent capabilities of LLMs while addressing the specific challenges of planning problems. In\nthe following sections, we introduce our novel AoT+ methodology, which builds upon the strengths\nof AoT while incorporating mechanisms to mitigate its weaknesses, particularly in the areas of state\ntracking and heuristic discovery."}, {"title": "AOT+ PROMPTING", "content": "Motivated by our new understanding of the failure modes in AoT prompting and the challenges in\ndeveloping prompts that include human-like intuition in the search process, we propose enhance-\nments that drastically improve the performance of LLMs in benchmarks where they were previously\nshown to be inadequate."}, {"title": "USE OF RANDOM SOLUTION TRACES DOES NOT DEGRADE PERFORMANCE", "content": "While including in-context examples showing the search process improves performance, the re-\nquirement for these examples to incorporate human intuition makes development more involved and\npotentially arbitrary. To support the notion that LLMs can plan autonomously, we tested completely\nrandom trajectories, only interwoven with the correct solution path that reaches the goal at the end.\nWe utilize a novel approach to generate search trajectories by combining successful and unsuccess-\nful solution attempts. Starting with one successful trajectory that reaches the goal state and four\nunsuccessful ones, we first select a random number of steps from the initial solving process. We\nthen intersperse these with random jumps between states drawn from the unsuccessful trajectories,\nagain selecting a random number of steps at each transition. Crucially, we ensure that our in-context\nexamples always terminate with the final successful steps that reach the goal state from the suc-\ncessful trajectory. This approach introduces controlled randomness while maintaining goal-directed\nbehavior - the random portions allow exploration of the search space, while consistently ending with\nsuccessful goal achievement creates an implicit bias that helps guide the model toward finding valid\nsolutions. Despite the predominantly random nature of these trajectories, our empirical results show\nthat this method effectively engages the model in active search behavior. The randomness appears\nto help prevent the model from fixating on specific solution patterns while still maintaining enough\nstructure through the guaranteed successful conclusion to guide it toward valid solutions."}, {"title": "MEMOIZATION AVOIDS HALLUCINATIONS", "content": "Our analysis revealed frequent hallucinations in state representation during the AoT process. We\nhypothesize that these hallucinations stem from the LLM's need to continuously recompute and\ntrack the current state after each action, potentially overwhelming its computational capacity as the\nsolution trace grows longer.\nTo address this issue, we draw an analogy to the concept of memoization in dynamic programming.\nIn computer science, memoization is an optimization technique that stores the results of expensive\nfunction calls and returns the cached result when the same inputs occur again. We adapt this prin-\nciple to our prompting strategy, periodically restating and caching the current problem state with\nidentifiers such as \"x.y.z.\" where x is the LLM's x-th candidate for the first decision step, and y\nrepresents the y-th candidate for the second operation after x-th candidate for the first one. through-\nout the solution process as shown in Figure 3.\nThis approach offers several advantages over external state tracking methods used in techniques like\nToT:\n1. It eliminates the need for external models to interpret actions and compute states, which\ncan be complex and error-prone.\n2. It avoids the computational overhead of reprocessing the entire context when new informa-\ntion is added, leveraging the caching mechanisms inherent in transformer architectures.\n3. It significantly reduces API costs and latency in real-world applications, as it doesn't re-\nquire stopping and restarting the generation process to inject external state information."}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we show that our simplified and enhanced prompting version is able to get state-\nof-the-art results in planning benchmarks, Blocksworld and Logistics, and in inductive reasoning\nbenchmarks, List Functions and ACRE, which are all known to be quite challenging for LLMs\n(Valmeekam et al., 2023; Stechly et al., 2024; Qiu et al., 2023). We further investigate whether our\nsetups work in a wide range of LLMs."}, {"title": "PROBLEM SETUPS", "content": "In this section, we present descriptions of the benchmarks we use, along with prompt generation\nmethodologies for the methods tested. Our problem setups closely follow those in Valmeekam et al.\n(2023) for Blocksworld and Logistics, and Qiu et al. (2023) for ACRE and List Functions. For pure\nplanning problems such as Blocksworld and Logistics, we utilize PDDL to formalize the instances\nand to check the validity of the outputs. For detailed descriptions of these problem setups, we refer\nreaders to the aforementioned papers."}, {"title": "MAIN RESULTS", "content": "Our experiments demonstrate the effectiveness of the AoT+ methodology across a range of challeng-\ning planning and reasoning tasks. Table 3 presents a comprehensive comparison of our approach\nagainst other methods, including Chain-of-Thought (CoT), LLM-Modulo, and with various LLM\narchitectures. Across all benchmarks\u2014Blocksworld, Logistics, List Functions, and ACRE-AoT+\nconsistently outperforms or matches the best existing methods, including those using external veri-\nfication tools like LLM-Modulo. This performance is particularly noteworthy in complex planning\ndomains such as Logistics, where AoT+ shows substantial improvements over both CoT and LLM-\nModulo approaches. It also surpasses human performance of 78% (Valmeekam et al., 2023) in the\nBlocksworld domain when GPT-4 or Claude is used.\nThe benefits of AoT+ are evident across different LLM architectures, from GPT-4 to smaller models\nlike LLaMA and Gemini variants. This consistency suggests that our method successfully leverages\nthe inherent capabilities of LLMs, enabling more effective planning and reasoning within a single\nprompt framework. It is particularly noteworthy that AoT+ consistently outperforms or matches\nLLM-Modulo across all tasks, despite not relying on external verification tools. This suggests that\nour method successfully leverages the inherent capabilities of LLMs, enabling them to plan and\nreason more effectively within a single prompt framework.\nThe gains of AoT+ are more substantial with larger models, revealing an emergent ability for plan-\nning as the scale of the models increases. Notably, the open-source LLaMA 3.1 405B model demon-\nstrates remarkably competitive results with GPT-4 when used with AoT+, a level of performance it\nfails to achieve within LLM-Modulo frameworks. This observation underscores the effectiveness\nof AoT+ in unlocking the latent planning capabilities of large language models. The strong perfor-\nmance on both planning (Blocksworld, Logistics) and inductive reasoning (List Functions, ACRE)\ntasks highlights the versatility of AoT+. By addressing the core challenges of state tracking and\nexploration in LLM reasoning, our method appears to unlock latent capabilities that are applicable\nacross a wide range of cognitive tasks."}, {"title": "CONCLUSION", "content": "This paper introduces AoT+, an enhanced prompting technique that significantly improves the plan-\nning and reasoning capabilities of large language models (LLMs). The key innovations of AoT+\naddress fundamental limitations in how LLMs process long sequences of information in planning\ntasks. Through comprehensive experiments across challenging benchmarks, our results consistently\nshow that AoT+ matches or outperforms existing SOTA methods, including those using external ver-\nification, across various LLM architectures. By demonstrating that LLMs can autonomously plan\nand reason at high levels of performance, AoT+ opens new avenues for research and applications."}, {"title": "ADDITIONAL EXPERIMENTS", "content": "In order to provide further evidence to the use of memoization for reducing state errors and hal-\nlucinations, we conducted an experiment to analyze the error rates for states for AoT and AoT+\nin the Logistics benchmark using LLaMA 3.1 70B model. This model is chosen since it is a rel-\natively cheap to do inference on computationally while already having a good performance in the\nbenchmark with AoT+.\nWe chose 200 games from the Logistics benchmark where both AoT+ and AoT was providing\nsolutions after reaching a solution depth of 20 actions, whether it be correct or not. Then we sampled\n20 states in each depth and checked whether the state assumed by the LLM would be reached if we\nwere to follow the actions it proposed starting from the initial state. If there was a discrepancy, we\nmarked it as an error. In Figure 4, we see that AoT+ dramatically reduces state hallucinations and\nerrors compared to AoT, which in return helps it achieve a superior performance as shown in Table\n3 across various benchmarks and LLMs."}, {"title": "IMPACT OF EACH INNOVATION OF AOT+", "content": "We also provide a more complete main results together with ablation studies on the impact of each\ninnovation of AoT+. We denoted AoT with random solution traces instead of human intuitions as\nAoT+R and AoT with memoization as AoT+M. As we can see in Table 5, AoT+R do have very\nclose performance to AoT, whereas AoT+M, or we can think of it AoT+ with human intuitions,\ngetting similar performance to AoT+."}, {"title": "ADDITIONAL BASELINE \u2013 SELF-REFINEMENT", "content": "To demonstrate simple iterative methods for improving planning performance through LLM self-\nfeedback, we evaluated Self-Refine (Madaan et al., 2024). We adhered to their original hyper-\nparameters (Temperature = 0.7) but extended the maximum iterations from 4 to 10 to ensure fair"}, {"title": "ADDITIONAL BASELINE - TREE-PLANNER", "content": "We evaluated Tree-Planner (Hu et al., 2023) on the Blocksworld benchmark, incorporating sampling,\nmerging, and backtracking steps for comparative analysis. It is crucial to note that Tree-Planner re-\nquires knowledge of action reversibility, which may be challenging or infeasible in non-ergodic envi-\nronments. Fortunately, Blocksworld allows straightforward action reversal (e.g., \u201cPick-up\u201d reverses\n\u201cPut-down\u201d, \u201cUnstack\u201d reverses \u201cStack\u201d). We maintained the structural hyperparameters from the\noriginal implementation, with N = 25 initial samples and a maximum of 10 error corrections.\nFor detailed information about these hyperparameters and the method, readers should consult the\noriginal paper. As demonstrated in Table 5, Tree-Planner consistently outperforms Self-Refine and\noccasionally matches AoT's performance. While it appears to surpass LLM-Modulo, we attribute\nthis partially to LLM-Modulo's optimization for GPT-4, their primary LLM. For generation prompts,\nwe utilized the Self-Refine Initial Generation prompt detailed in Appendix B.1. The supplementary\nmaterial includes implementation code for Tree-Planner in the Blocksworld environment."}, {"title": "ALL USED PROMPTS", "content": null}, {"title": "SELF-REFINE BLOCKSWORLD - INITIAL GENERATION", "content": null}, {"title": "SELF-REFINE BLOCKSWORLD - FEEDBACK", "content": null}, {"title": "AOT+ BLOCKSWORLD", "content": null}, {"title": "AOT+ LOGISTICS", "content": null}, {"title": "AOT+ ACRE", "content": null}, {"title": "AOT+ LIST FUNCTIONS", "content": null}]}