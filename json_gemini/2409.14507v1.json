{"title": "A IS FOR ABSORPTION: STUDYING FEATURE SPLITTING AND ABSORPTION IN SPARSE AUTOENCODERS", "authors": ["David Chanin", "James Wilken-Smith", "Tom\u00e1\u0161 Dulka", "Hardik Bhatnagar", "Joseph Bloom"], "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call \"feature absorption\" where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution. We release a feature absorption explorer at https://feature-absorption.streamlit.app.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have achieved remarkable performance across a wide range of tasks, yet our understanding of their internal mechanisms lags behind their capabilities. This gap between performance and interpretability raises concerns about the \"black box\" nature of these models (Rudin, 2019). The field of mechanistic interpretability aims to address this issue by reverse-engineering the internal algorithms of neural networks and performing causal analysis on them (Olah et al., 2020).\nOne recent promising approach in this field is the use of Sparse Autoencoders (SAEs), which have shown potential in decomposing the dense, polysemantic activations of LLMs into more \"interpretable\" latent features (Huben et al., 2024; Bricken et al., 2023) using sparse dictionary learning (Olshausen & Field, 1997).\nSAE neurons (hereafter called \u201clatents\") are said to be interpretable if they appear to detect some property of the input (which we refer to as a \"feature\") and they classify that feature with high precision / recall (Bricken et al., 2023)."}, {"title": "2 BACKGROUND", "content": "Linear probing: A linear probe is a simple linear classifier trained on the hidden activations of a neural network, typically using logistic regression (LR) (Alain & Bengio, 2017).\nK-sparse probing: A k-sparse probe (Gurnee et al., 2023) is a linear probe trained on a sparse subset of k neurons or SAE latents. Training a k-sparse probe first requires selecting the k best neurons or SAE latents that in-aggregate act as a good classifier, and then training a standard linear probe on just those k neurons or latents.\nGurnee et al. (2023) proposed several methods of estimating the best k neurons or features to pick, one of which involves first training a LR probe with a L1 loss term, and selecting the k largest elements by probe weight. When we refer to k-sparse probing in this work, we use this method of selecting k features.\nSparse autoencoders: An SAE consists of an encoder, $W_{enc}$, a decoder, $W_{dec}$, and corresponding biases $b_{enc}$ and $b_{dec}$. The SAE has a nonlinearity, typically a ReLU (or variants such as JumpReLU (Rajamanoharan et al., 2024)). Given input activation, $a$, the SAE computes a hidden representation, $f$, and reconstruction, $\u00e2$:\n$f = ReLU(W_{enca} + b_{enc})$\n$a=W_{decf}+b_{dec}$\nSAEs attempt to reconstruct input activations by projecting into an overcomplete basis using a sparsity-inducing loss term (typically L1 loss), or a certain number of non-zero features (L0) on the hidden activations. SAEs learn feature decompositions in an unsupervised manner, and while"}, {"title": "SAE feature ablation", "content": "We often want to understand how an SAE latent causally influences a downstream output. In an ablation study, the latent in question is removed from the computation graph of the model to see the effect this has on a downstream metric. A negative ablation effect means removing the SAE latent would lower the metric.\nWe follow the work of Marks et al. (2024) and provide the procedure in Algorithm 1 below. We also make use of the integrated-gradients (IG) approximation (Sundararajan et al., 2017) to improve the speed of running multiple ablation experiments."}, {"title": "3 EXPERIMENTAL SETUP", "content": "Our experiments focused on predicting the first-letter of a single token containing characters from the English alphabet (a-z, A-Z) and an optional leading space. We use in-context learning (ICL) prompts to elicit knowledge from the model, using templates of the form:\n{token} has the first letter: {capitalized_first_letter}\nAn example of an ICL prompt consisting of 2 in-context examples is shown below. The model should output the _D token:\ntartan has the first letter: T\nmirth has the first letter: M\ndog has the first letter:\nIn the above prompt, we extract residual stream activations at the dog token index. These activations are used both for LR probe training and for applying SAEs. We use a train/test split of 80% / 20%, and evaluate only on the test set of the probes, including when running experiments on SAEs.\nWhen applying SAEs, we always include the SAE error term to avoid changing model output.\nTo determine how well multiple features perform as a classifier when used together, we use k-sparse probing, increasing the value of k from 1 to 15. We train a LR probe using a L1 loss term with coefficient 0.01, and select the top k features by magnitude.\nTo determine the causal effect of SAE latents on the first-letter identification task we conduct ablation studies. We use a metric consisting of the logit of the correct letter token minus the mean logit of all incorrect letters. This measures the propensity of the model to choose the correct starting letter as opposed to other letters. Formally, our metric m is defined below, where g refers to the final token model logits, L is the set of uppercase letters, and y is the uppercase letter that is the correct starting letter:\n$m = g[y] - \\frac{1}{|L|-1}\\sum_{l \\in {L \\setminus y}} g[l]$"}, {"title": "4 RESULTS", "content": "Our results are divided into three sections. First, we compare the performance of linear probes with SAE latents on recovering first-character information from model activations, showing that despite appearing to track first letter features, a wide variety of precision / recall is achieved. Second, we motivate our definition of feature absorption with a case-study, emphasizing how an absorbing feature can unexpectedly causally mediate first letter information whilst the first-letter latent (unexpectedly) fails to fire. Finally, we attempt to quantify feature splitting and feature absorption, showing that tuning of hyper-parameters may partially assist but not fully alleviate feature absorption."}, {"title": "4.1 DO SAES LEARN LATENTS THAT TRACK FIRST LETTER INFORMATION?", "content": "We compare the performance of LR probes with the performance of the SAE latent whose encoder direction has highest cosine similarity with the probe, resulting in 26 \"first-letter\" latents. We observed that for each probe, there was clearly one or at most a couple of outlier SAE latents with high probe cosine similarity. Full plots of cosine similarity vs letter are shown in Appendix A.4.\nWe also experimented with using k=1 sparse probing to identify SAE latents (Gao et al., 2024), and find this gives similar results. Further comparison of k=1 sparse probing and encoder cosine similarity is explored in Appendix A.3.\nWe observe wide variance in the performance of Gemma Scope SAEs at the first-letter identification task, but no SAE matches LR probe performance. We show the mean F1 score by layer as well as the F1 score of the LR probe in Figure 2a. We further investigate the F1 score of these SAE encoder latents as a function of LO and SAE width in Figures 2b and 2c.\nWhether or not an SAE learns a clear \u201cfirst-letter\" latent for each letter is highly dependent on L0, with low LO SAEs tending to learn high-precision low-recall latents, and high LO SAEs learning"}, {"title": "4.2 WHY DO SAE LATENTS UNDERPERFORM?", "content": "The Gemma Scope layer 3, 16k width, 59 LO SAE has a latent, 6510, which appears to act as a classifier for \"starts with S\", achieving an F1 of 0.81. However, this latent fails to activate on some tokens the probe can classify, and which the model can spell, such as the token short.\nFigure 4a shows a sample prompt containing a series of tokens that start with \"S\", and the activations of top SAE latents by ablation score for these tokens. The main \u201cstarts with S\" latent, 6510, activates on all these tokens except short. This SAE also has a token-aligned latent, 1085, which activates on variants of the word \"short\" (\" short\u201d, \u201cSHORT\", etc...). The Neuronpedia dashboard (Lin & Bloom, 2023) for feature 1085 is shown in Appendix A.7. For the token short, the main \u201cstarts with S\" latent does not activate but the \"short\" latent activates instead.\nLatent 1085 has a cosine similarity with the \u201cstarts with S\" probe of 0.12, indicating it contains a component of the \"starts with S\u201d direction, although much smaller than the main \u201cstarts with S\" latent. Cosine similarity of the SAE decoder with the \"starts with S\" LR probe is shown in Figure 4b. Interestingly, despite latent 1085 having only about 1/5 the cosine similarity with the probe as the main latent 6510, we see it activates with about 5 times the magnitude of latent 6510 on the short token, thus contributing a similar amount of the \"starts with S\" probe direction to the residual stream.\nWe conduct an ablation experiment on the short token, shown in Figure 5a, and see that latent 1085 has a dramatically larger ablation effect compared with all other SAE features. This suggests latent 1085 is causally responsible for the model knowing that short starts with S.\nIs it possible that the probe projection is not the causally important component of feature 1085? We conduct another ablation experiment, except now we remove the probe direction from feature 1085 via projection before ablation. The results of this ablation experiment are shown in Figure 5b. After removing the probe component from feature 1085, it no longer has a significant ablation effect. Thus we know the probe projection of feature 1085 is responsible for model behavior.\nThese experiments show the \u201cstarts with S\u201d feature has been \"absorbed\" by the token-aligned latent 1085, likely along with other semantic concepts related to the word \"short\". After observing that the main \"starts with S\" latent 6510 activates on most tokens that begin with \"S\", it may be tempting to conclude this latent tracks the interpretable feature of beginning with the letter \u201cS\u201d. However, this latent quietly fails to activate on the short token, leading us to a false sense of understanding.\nWe call this phenomenon feature absorption. In feature absorption a seemingly interpretable SAE latent fails to activate on arbitrary positive examples, and instead the feature is \"absorbed\" into approximately token-aligned latents.\""}, {"title": "4.3 MEASURING FEATURE SPLITTING AND FEATURE ABSORPTION", "content": "Feature splitting A key phenomenon identified from previous studies of SAEs is feature-splitting (Bricken et al., 2023), where a feature represented in a single latent in a smaller SAE can split into two or more latents in a larger SAE. During our experiments, we found strong evidence of feature-splitting in the Gemma Scope SAEs.\nFor instance, in the layer 0, 16k width, 105 LO SAE, we find two encoder latents (id:7112 and id:7657) which align with the \u201cL\u201d starting letter probe. Inspecting max activating examples, we see latent 7112 activates on tokens starting with lowercase \"l\", while 7657 activates on tokens starting with uppercase \u201cL\u201d. Some activating examples for these latents are shown in Table 1.\nFeature splitting like this is not necessarily problematic for interpretability efforts since the split features are still easily identifiable, and depending on the context it may be more useful to have either a single \u201cstarts with L\u201d latent or a pair of \"starts with uppercase / lowercase L\u201d latents.\nWe measure feature splitting using k-sparse probing (Gurnee et al., 2023) on SAE activations. If increasing the k-sparse probe from k to k + 1 causes a significant increase in probe F1 score, then the additional SAE latent provides a meaningful signal, and the combination of these k + 1 latents is likely a feature split. In the example of the uppercase \u201cL\u201d and lowercase \u201cl\u201d split, a k-sparse probe with k = 2 trained on both these these features should predict \"starts with letter L\" much better than either feature on its own. Figure 6a shows F1 vs K for letters \u201cL\u201d and \u201cN\u201d. The \u201cL\u201d k-sparse probe shows a significant jump in F1 score moving from k=1 to k=2 corresponding to feature splitting, while the F1 score for the \"N\" k-sparse probe is relatively constant.\nWe detect feature splitting by measuring whether increasing k by one causes a jump in F1 score by more than threshold \u0442. We set r = 0.03 after manually inspecting latents with various thresholds. Figure 6b shows feature splitting vs LO for all 16k and 65k width Gemma Scope SAEs."}, {"title": "5 DISCUSSION", "content": "Interpretability of SAE latents In this work, we use a simple first-letter identification task to investigate whether SAEs extract monosemantic and intrepretable features, and how this is affected by varying hyperparameters like SAE size, sparsity, or layer. We find that the SAE latents we investigated were not interpretable and that varying the sparsity or the size of the SAE did not meaningfully change this.\nThe significance of our results hinges in part on whether we believe that using SAE latent classification performance on a simple first-letter identification task is a valid measure of their interpretability. One may argue it is unreasonable to judge an SAE latent against a linear probe directly optimized to perform the same classification task and that it has been established that sometimes unsupervised methods can surprise us (Nanda, 2023). However, we argue that for a latent to be considered interpretable, its behavior should match what one would reasonably expect the latent to be doing after inspecting its activation patterns. In our experiments, we use SAE latents that do appear to perform first letter classification, and then evaluate how well they perform this task. We validate as well that these latents causally mediated model performance on the first-letter task in Appendix A.2. We have been convinced that these latents should reasonably be considered \u201cfirst-letter\" latents and thus that their performance on the first-letter identification task is a valid measure of their interpretability.\nFeature absorption In trying to understand why SAE latents fail to match the performance of LR probes, we identified a form of feature splitting we call \u201cfeature absorption\u201d. Feature absorption may be particularly problematic for SAEs because it creates an interpretability illusion where we believe we have found an interpretable latent, but absorption induces lower recall by creating clear false negatives / exceptions to the mainline interpretation of the latent. This lower recall poses problems for methods which rely on using SAEs to find sparse circuits (Marks et al., 2024), as the number of latents needed to characterize model behavior may be much larger than expected. For example, we may believe we have found a SAE latent which tracks deceptive behavior in the model, but due to feature absorption, there may be many cases where that latent fails to fire with an absorbing feature firing instead. We find that feature absorption happens even in high-recall latents, so this is not only a problem for low LO SAEs and appears to be a more fundamental issue.\nFeature absorption is not predicted by toy-models where features do not have high co-occurrence with each other (Elhage et al., 2022). While more work is required to understand why feature absorption occurs, we suspect it is a consequence of co-occurrence between sparse and dense features. If a dense feature like \"starts with letter D\" always co-occurs with a more sparse feature like \u201cdogs\u201d, the SAE can increase sparsity by absorbing the \u201cstarts with D\u201d feature into a \u201cdogs\u201d latent.\nIt remains to be seen if we can predict or identify excepted instances where a feature \"should have activated\" but does not activate due to absorption. One promising direction is meta-SAEs, a novel method for decomposing SAE latents and may decompose absorbed features (Bussmann et al., 2024). One way to interpret our results is that competition may exist between \u201clatents\" and \"meta-latents\" for activation on particular examples and that re-allocation of examples between SAE latents enables SAEs to interpolate between different possible decompositions.\nFuture Work A primary goal of future work should be to secure further external validity of our findings. This could include finding examples of feature absorption in SAEs trained on other models, with other architectures, or finding examples of feature absorption unrelated to character identification. We expect it should be possible to demonstrate feature absorption in a toy model by mixing dense features with sparse features that always co-occur with these dense features.\nWe hope as well this investigation may lead to research into solutions, particularly those involving Meta-SAEs (Bussmann et al., 2024), to solve or mitigate feature absorption. One solution may be attribution dictionary learning (Olah et al., 2024).\nLimitations The results presented in our paper are based on a single model (Gemma-2-2B) using one type of SAE architecture (JumpReLU). Our feature absorption metric requires having ground-truth knowledge of true labels to first train a LR probe, whereas many features of interest in a LLM lack such clear-cut ground-truth labels. Our metric uses ablation effect to ensure absorbed features causally mediate model behavior, but therefore cannot be easily used in final model layers."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 HOW GOOD IS GEMMA-2 ON CHARACTER IDENTIFICATION TASKS?", "content": ""}, {"title": "A.2 INTERVENING ON THE FIRST LETTER", "content": "If the model is using the identified SAE latents for predicting the first letter we should also be able to change what first letter it predicts just by changing the activations. For this experiment we use the SAE latents most cosine similar with the LR probe for the true first letter and for a new randomly selected letter. We take the intermediate activations of Gemma-2-2B in the residual stream and encode them using the SAE. Then we zero out the activation of the SAE latent associated with the original letter and change the activation of the SAE latent associated with the new letter into the average activation it has on tokens starting with this new letter.\nEditing works better with latents from the narrower 16k SAE compared to the 65k, with the best LOs in the 75-150 range. This corresponds to the observed pattern of these SAE latents having higher F1 scores for classification. We report the results in Figure 9. The best SAEs on the layers 7-9 can achieve a substantial replacement, but note that the averages hide variance across individual tokens, where some get edited completely and others get unaffected. The edit success also varies based on the true first letter and the random new letter; for illustration we show a breakdown by letter for two specific SAEs in layer 7 in Figure 10."}, {"title": "A.3 PROBE COSINE SIMILARITY VS K=1 SPARSE PROBING", "content": "The first step when searching for a SAE feature that acts as a first-letter classifier involves searching for SAE feature which best acts as a classifier. In Figure 2, we achieve this by first training a LR probe on the first-letter task and using cosine similarity between that probe and the SAE encoder to find the best feature for the first-letter task. We also investigated using k-sparse probing with k=1 to select the best SAE feature instead. This involves training a linear probe with L1 loss and selecting the feature with the highest positive weight from the probe.\nWe find that both k=1 sparse probing yield nearly identical results, as seen in Figures 11 and 12. Additionally Figure 13 shows the cosine similarity of the LR probe with each SAE feature by letter for the canonical Gemma Scope layer 0 16k width SAE. In most cases there is an obvious probe-aligned feature. Likely any reasonable method of feature selection will find the same feature for"}, {"title": "A.4 PRECISION, RECALL, AND F1 SCORE FOR THE FIRST-LETTER TASK", "content": "We evaluated precision, recall, and F1 score for the first-letter classification task, and found that the precision and recall vary depending on the LO of the SAE. Low L0 SAEs learn high precision, low recall features, while high LO SAEs learn low precision, high recall features. These results are shown in Figure 14. We thus chose to use F1 score as our core metric in this paper to balance precision and recall as many if the SAEs we tested have extreme values in either precision or recall.\nWhile it may appear that there is an optimal LO from looking at aggregate statistics across letter, we find that breaking down the F1 vs LO plot by letter reveals that the optimal LO appears different for different letters, with low frequency letters like z actually having the best F1 score at the lowest LO,"}, {"title": "A.5 CAUSAL INTERVENTIONS AND ABSORPTION", "content": "In this work, we rely on causal interventions like ablation experiments to verify that SAE latents have a causal impact on model behavior. In these experiments for spelling tasks, we set up an ICL prompt to elicit spelling information from the model, for instance the ICL prompt below:\ntartan has the first letter: T\nmirth has the first letter: M\ndog has the first letter:"}, {"title": "A.6 ADDITIONAL PLOTS", "content": "In this section, we include additional plots that are too large to fit in the main body of the paper."}, {"title": "A.7 FEATURE DASHBOARDS", "content": "We include feature dashboard screenshots from Neuronpedia for some prominent latents mentioned in this work. Figure 18 shows a dashboard for Gemmascope layer 3, latent 1085, which is a token-aligned latent firing on variations of the word short and we find absorbs the \"starts with S\" direction. Figure 19 shows latent 6510 from the same layer which should be the main \"starts with S\" latent."}]}