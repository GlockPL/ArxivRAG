{"title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding", "authors": ["Zhanpeng Chen", "Mingxiao Li", "Ziyang Chen", "Nan Du", "Xiaolong Li", "Yuexian Zou"], "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models' comprehensive perception performance across different levels of granularity. In this work, we propose Pyramid-descent Visual Position Encoding (PyPE), a novel approach designed to enhance the perception of visual tokens within VLMs. By assigning visual position indexes from the periphery to the center and expanding the central receptive field incrementally, PyPE addresses the limitations of traditional raster-scan methods and mitigates the long-term decay effects induced by Rotary Position Embedding (RoPE). Our method reduces the relative distance between interrelated visual elements and instruction tokens, promoting a more rational allocation of attention weights and allowing for a multi-granularity perception of visual elements and countering the over-reliance on anchor tokens. Extensive experimental evaluations demonstrate that PyPE consistently improves the general capabilities of VLMs across various sizes.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate significant universal capabilities that contribute to the pursuit of general artificial intelligence. However, language constitutes only one aspect of communication. Visual information plays a crucial role in augmenting and enhancing our understanding of the world. Consequently, there is a growing interest in the development of Vision-language Models (VLMs) that can simultaneously process and integrate various input modalities. To effectively leverage the powerful contextual understanding capabilities of LLMs, VLMs project visual information to the same dimensionality as textual embeddings through specific projection layers, which are then directly embedded into the text sequence to form the input for the foundation LLMs, enabling end-to-end cross-modal alignment and instruction-following learning using next-token prediction. Therefore, the image tokens are position-embedded together with the entire sequence.\nDespite their commendable progress, the typical processing of visual information does not align with the distribution patterns of visual elements. Since visual information is composed of fixed-sized patches obtained through raster scanning, patches located closer to the bottom right corner of the image are positioned nearer to the instruction tokens within the sequence. Due to the long-term decay from Rotary Position Embedding (RoPE), visual tokens closer to the instruction tokens will be more likely to receive higher attention weights, and vice versa. This is counter-intuitive, as the importance of visual information is not defined by the order of raster-scanning. Xing et al. observe a similar phenomenon by visualizing the attention information flow from instruction tokens to visual tokens in the first layer of the Decoder. Consequently, they propose Concentric Causal Attention (CCA), which starts assigning the position indexes of images from the peripheral and ends in the center, to alleviate the limitations of long-term decay in RoPE and improve causal attention following 2D spatial locality of images. Although CCA is both intuitive and effective, its applicability is constrained by the assumption that all significant elements related to the instructions are situated at the center of the image. This assumption inherently results in a loss of detail, limiting its effectiveness in capturing comprehensive information.\nTo further investigate the impact of raster-scan and concentric PE on the fine-grained modeling of visual information, we extend the visualization to all layers of the Decoder. As illustrated in Figure 1, CCA demonstrates exceptional performance in the first layer, alleviating the long-term decay caused by RoPE in the raster-scan approach, thereby directing the model's attention to more significant areas. However, in the subsequent layers, both methods largely maintain the same attention patterns as observed in their respective third layers, with changes only occurring in the final layer. A similar phenomenon, namely \"aggregation pattern\", is observed in OPERA, where both LLMs and VLMs tend to generate new tokens by concentrating on a limited number of summary tokens (also referred to as anchor tokens) rather than considering all preceding tokens. This tendency towards partial overtrust leads to the neglect of fine-grained image tokens, resulting in the generation that may be hallucinatory and do not accurately reflect the image content. Moreover, it has been demonstrated in OPERA that more hallucinations are generated when more summary tokens appear in the context.\nTo this end, we present Pyramid-descent Visual Position Encoding (PyPE), a novel position assignment approach for visual tokens, to alleviate the long-term decay induced by RoPE, avoid the \"aggregation pattern\" in the LLM, and ensure a comprehensive understanding of visual contents. PyPE reorganizes the flattened visual tokens into the 2D shape and assigns visual position indexes from the periphery to the center. This reduces the relative distance between interrelated visual elements, as well as the distance between significant visual elements and instruction tokens, thereby ensuring a more rational allocation of attention weights. Furthermore, to mitigate the impact of anchor tokens on the model's fine-grained perception of visual elements, we draw inspiration from Pyramid Vision Transformer (PVT): consistently combining global and local receptive fields. PyPE gradually expands the central receptive field, i.e., the central region of the position index matrix, at predetermined intervals of layers. Specifically, we expand the central region of the position index matrix by a circle every certain number of layers. Such expansion weakens the anchor tokens and enhances the model's ability to perceive visual elements at varying levels of granularity (more cases can be found in Section 5.4).\nWith extensive experiments on visual question answering and general multimodal benchmarks, PyPE consistently improves general perception capabilities across VLMs of different sizes. In a nutshell, the main contributions of this work are as follows: (I) We make an in-depth analysis of how position encoding affects visual perception in VLMs. (II) Our proposed PyPE effectively mitigates long-term decay and the \"aggregation pattern\", which helps better perceive visual elements at different granularities. (III) Extensive evaluations demonstrate the superior performance of PyPE, a simple yet effective method that applies to any VLMs."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Vision-language Model", "content": "Recent advancements in Vision-language Models (VLMs) have demonstrated impressive performance in processing multi-format information. VLMs are typically built upon existing Large Language Models (LLMs) and incorporate visual in-"}, {"title": "2.2 Position Encoding for Transformers", "content": "Since transformer-based models contain no recurrence and convolution structure, additional information about the relative or absolute position of the tokens in the input sequence is required. Therefore, the community has witnessed the development of various position encoding methods, e.g. sinusoidal, learnable, relative, and conditional position encoding. Among these studies, RoPE are introduced to encode both absolute and relative positional information, showing superiority in pre-training LLMs. The success of LLMs has led to the continued adoption of the effective ROPE scheme in VLMs for the unified encoding of positional information across sequences that incorporate multimodal features. However, it is important to note that visual information does not conform to the same one-dimensional sampling paradigm as language. Simple raster scanning is insufficient for modeling the spatial correlations among different patches. Consequently, numerous recent studies have sought to explore improved solutions that extend ROPE to visual tasks. In this paper, we investigate a novel multi-granularity position assignment strategy aimed at enhancing the VLM's comprehension of visual information and improving the alignment between modalities."}, {"title": "3 Approach", "content": null}, {"title": "3.1 Preliminaries", "content": "ROPE (Rotary Position Embedding) ROPE unifies both absolute and relative positional encodings, demonstrating a certain degree of extrapolation capability in LLMs and VLMs. Given the m-th query and n-th key vectors with a dimension D, denoted as $q_m, k_n \\in \\mathbb{R}^{|D|}$, ROPE multiplies a bias to the key or query vector in the complex vector space as follows:\n$f_q(q_m, m) = e^{im\\Theta}q_m, f_k(k_n, n) = e^{in\\Theta}k_n$ (1)\nwhere $\\Theta = Diag(\\theta_1,\\dots,\\theta_{|D|/2})$ is the rotary frequency matrix, where $\\theta_d = b^{-2d/|D|}$ and the rotary base $b = 10000$. In real space, for $l = |D|/2$, the rotary matrix $e^{im\\Theta}$ can be expressed as:\n$\\begin{pmatrix}\n\\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & \\cdots & 0 & 0\\\\\n\\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & \\cos m\\theta_l & -\\sin m\\theta_l\\\\\n0 & 0 & \\cdots & \\sin m\\theta_l & \\cos m\\theta_l\\\\\n\\end{pmatrix}$ (2)\nThe attention score using RoPE is calculated as follows:\n$A_n = Re(f_q(q_m, m), f_k(k_n, n)) = Re(q_m e^{i(m-n)}k_n)$ (3)\nwhere $Re(\\cdot)$ is the real part of a complex number and $e^{i(m-n)} = (e^{in\\Theta})^T e^{in\\Theta}$. As the relative"}, {"title": "All-one Position Encoding", "content": "To further explore the impact of visual position encoding on the model's perception of visual elements, we propose All-one Position Encoding: directly setting the relative distance between all image tokens and instruction tokens to 1. By doing so, the relative distances from all image tokens to the instruction token become equal, thereby excluding the influence of relative position decay introduced by RoPE. As a result, all patches are treated equally.\nAs indicated in Table 1, All-one PE performs weaker than the baselines and Concentric PE in perception but keeps competitive in coarse-grained perception tasks on different sizes of models. This suggests that, on the one hand, even when assigning the same positional weight to all image tokens, the VLM still possesses certain perception capabilities and performs better than raster-scan and concentric in coarse-grained perception. This is more pronounced on LLaVA-1.5-13B because larger models have stronger sequence modeling and feature capturing capabilities, which correspondingly bridge the gap in fine-grained abilities between All-one PE and other methods. On the other hand, the results from the baselines indicate that the long-term decay property of RoPE endows the image tokens with distinct weights, allowing the model to distinguish fine-grained tokens better and capture visual details."}, {"title": "3.2 Pyramid-descent Visual Position Encoding", "content": "Though presenting competitive coarse-grained perception capabilities, All-one PE still falls short in fine-grained perception. Using identical position weights hampers the model's ability to differentiate the significance of image tokens, while the positional priors introduced by raster scanning conflict with general cognitive principles.\nSimilar challenges were also present in the early development of Vision Transformer (ViT). Due to the columnar structure of ViT, which uses coarse image patches as input, it is difficult to apply it directly to pixel-level dense predictions such as object detection and segmentation. This difficulty arises because its output feature map is single-scale and low-resolution. To address these issues, Wang et al. proposed the Pyramid Vision Transformer (PVT). They utilize fine-grained image patches as input to learning high-resolution representations and introduce a progressive shrinking pyramid to reduce the sequence length of the Transformer as the network deepens, significantly lowering the computational cost. Moreover, compared to CNNs, PVT consistently produces a global receptive field, ensuring a holistic perception of visual elements and benefiting its performance in detection and segmentation tasks.\nIn light of this, we propose the Pyramid-descent Visual Position Encoding (PyPE), a simple yet effective position assignment strategy for visual tokens in VLMs. As shown in Figure 3, we first reorganize the visual tokens from their vanilla flattened 1D sequence form into the 2D format. Subsequently, we adopt a decay pattern for the corresponding position indexes of the image tokens that spread outward from the center following concentric PE . Given the maximum assignable position index $P_{max}$, the position assignment matrix P is calculated as follows,\n$P(i, j) = p, \\forall p \\in [1, P_{max}], s.t. \\{(i, j) | i \\in [p, H - p), j \\in [p, W \u2013 p)\\}$, (4)\nwhere H and W represent the height and width of the input image, respectively. $P_{max}$ is initialized to $H//2$. This design maintains spatial continuity in the row and column dimensions. It reduces the average distance between significant image tokens and instruction tokens, facilitating cross-attention among the image tokens and cross-attention between the image tokens and instruction tokens.\nSubsequently, we propose gradually expanding the central receptive field to weaken the anchor tokens and enhance the model's ability to perceive visual elements at varying levels of granularity. Specifically, we reduce $P_{max}$ every t layer, thereby controlling the perception granularity in terms of position encoding. When $P_{max}$ is reduced to 1, the corresponding position encoding degenerates into All-one PE. In addition, we modify the causal mask M accordingly to ensure causal attention among tokens.\nBy introducing hierarchical position indexes, the Pyramid-descent Visual Position Encoding allows for a multi-granularity perception of visual elements. This method not only aligns better with human cognition but also improves the model's"}, {"title": "4 Experiment Setup", "content": null}, {"title": "4.1 Benchmarks", "content": "We evaluate PyPE on visual question answering and general multimodal benchmarks, including VQAv2, OK-VQA, GQA, VizWizQA, TextVQA, RealWorldQA, ScienceQA, MME, MMBench, SEED-Bench, POPE, AI2D, MM-Vet, MMMU, MMT-Bench, and MMStar. Refer to Appendix A for more details."}, {"title": "4.2 Implementation Details", "content": "To demonstrate the generalizability of our proposed method across models with different parameter sizes, we conduct experiments using three model architectures with 3B, 7B, and 13B parameters. For 3B models, we follow TinyLLaVA to use SigLIP as the visual encoder and Phi-2 as the base LLM. For 7B and 13B models, we adopt pre-trained CLIP ViT-L/14 (3362) as visual encoder and Vicuna v1.5 as the base LLM. Following Liu et al., we first pretrain the models on CC-558K dataset for 1 epoch with a learning rate of 1e-3, and finetune on the mix-665K dataset for 1 epoch, with a learning rate of 2e-5. All experiments are conducted on 8 NVIDIA A100 and 8 NVIDIA H20 GPUs."}, {"title": "5 Empirical Results and Analysis", "content": "We evaluate the visual capabilities of the models trained with the PyPE through various visual question answering and general multimodal benchmarks. This novel position encoding demonstrates highly competitive performance at different scales. Our proposed method consistently delivers top-tier performance across most evaluation metrics, frequently surpassing other baselines."}, {"title": "5.1 Results of Visual Question Answering Benchmarks", "content": "To rigorously evaluate the capabilities of our models in general visual question answering tasks, we conduct comprehensive assessments across a diverse array of state-of-the-art benchmarks. The results presented in Tables 1 and 2 indicate that the PyPE series demonstrates exceptional performance across all benchmarks, with the three variants consistently achieving or surpassing baseline performance. In the MME benchmark, PyPE exhibits a superior understanding of visual content at various levels of granularity. It retains a coarse-grained perception capability comparable to that of All-one"}, {"title": "5.2 Results of General Multimodal Benchmarks", "content": "As illustrated in Table 3, the PyPE series demonstrates exceptional performance on mainstream general multimodal benchmarks. In the MMStar benchmark, which is designed to assess genuine multimodal capabilities using visually indispensable samples, PyPE outperforms all baseline models. On MM-Vet, which evaluates the integration of core vision-language capabilities across 16 complex multimodal tasks, the 3B model of PyPE achieves an impressive score of 35.00, significantly surpassing the scores of 33.00 and 33.40 obtained by Raster-scan and Concentric PE, respectively. In the MMT-Bench evaluation, which assesses advanced reasoning and instruction-following across 32 core meta-tasks and 162 subtasks in multimodal understanding, PyPE markedly exceeds baseline performance, demonstrating its ability to apply expert knowledge and execute deliberate visual recognition, localization, reasoning, and planning. On MMBench, which evaluates fine-grained abilities across 20 dimensions, PyPE exhibits strong performance, matching or leading the state-of-the-art. Additionally, we test the methods on AI2D, a frame. Consequently, as shown in Table 2, All-one PE demonstrates competitive performance on this dataset, while our proposed PyPE exhibits superior zero-shot performance on both VizWizQA and ScienceQA. This improvement can be attributed to the flexible receptive field enabled by PyPE."}, {"title": "5.3 Ablation Analysis", "content": "Effect of the Descent Interval As shown in Table 4, we evaluate the performance of different models using PyPE with varying descent intervals on VQA and general multimodal benchmarks. Across all models, a moderate descent interval PyPE 2x generally provides the best or near-best performance, which strikes a balance between the model's ability to handle perception (MME), external knowledge integration (OK-VQA), text comprehension (TextVQA), and vision-critical tasks (MMStar). While the 2x interval is generally optimal, there are exceptions, such as the LLaVA-1.5-13B model performing best on OK-VQA with a 4x interval. This indicates that larger models might benefit from longer intervals for specific tasks."}, {"title": "5.4 Qualitative Results on LLaVA-Bench", "content": "Figure 4 demonstrates a case study on how, given identical prompts and images, other baselines misperceive or inadequately process visual information, resulting in the generation of hallucinatory content. For instance, in the displayed example, the baseline methods exhibit object hallucinations, identifying nonexistent items such as \"dining table\", \"hat\", \"scarf\", and \"boat\". In contrast, the implementation of PyPE notably mitigates these hallucination issues while simultaneously maintaining the coherence and informativeness of the output text. This can be attributed to the multi-scale visual modeling capability afforded by the dynamic local receptive fields of PyPE, in conjunction with the stable global receptive fields. Furthermore, the visualization results of layer-wise attention indicate that our proposed method effectively alleviates the phenomenon of \"aggregation pattern\", thereby creating a synergistic effect with the former."}, {"title": "6 Conclusion", "content": "In this paper, we conduct an in-depth analysis of how visual position encoding affects visual perception in VLMs, particularly from the aspect of long-term decay and the \"aggregation pattern\". Our findings indicate that conventional visual position encoding methods are constrained by the \"aggregation pattern\" derived from Large Language Models (LLMs) and lacks multi-scale perceptual capabilities. To address these limitations, we introduce Pyramid-descent Visual Position Encoding (PyPE) a novel approach designed to enhance the perception of visual tokens within VLMs. Our extensive experiments across multiple benchmarks and VLM families demonstrate the efficacy of PyPE in addressing these challenges and ensuring a thorough understanding of visual content."}, {"title": "Limitations", "content": "Although PyPE demonstrates exceptional performance in enhancing the overall capabilities of Vision-language Models (VLMs), it is currently limited to single-frame images and has not yet been extended to video and other modalities. Future research will focus on how to effectively integrate the temporal dimension for unified position encoding and extending PyPE to a broader range of VLMs."}]}