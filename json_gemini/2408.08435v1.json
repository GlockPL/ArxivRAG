{"title": "Automated Design of Agentic Systems", "authors": ["Shengran Hu", "Cong Lu", "Jeff Clune"], "abstract": "Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We formulate a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.", "sections": [{"title": "1. Introduction", "content": "Foundation Models (FMs) such as GPT (OpenAI, 2022, 2024) and Claude (Anthropic, 2024b) are quickly being adopted as powerful general-purpose agents for agentic tasks that need flexible reasoning and planning (Wang et al., 2024). Despite recent advancements in FMs, solving problems reliably often requires an agent to be a compound agentic system with multiple components instead of a monolithic model query (Rockt\u00e4schel, 2024; Zaharia et al., 2024). Additionally, to enable agents to solve complex real-world tasks, they often need access to external tools such as search engines, code execution, and database queries. As a result, many effective building blocks of agentic systems have been proposed, such as chain-of-thought planning and reasoning (Hu & Clune, 2024; Wei et al., 2022; Yao et al., 2023), memory structures (Lewis et al., 2020; Zhang et al., 2024c), tool use (Qu et al., 2024; Schick et al., 2023), and self-reflection (Madaan et al., 2024; Shinn et al., 2023). Although these agents have already seen significant success across various applications (Wang et al., 2024), developing these building blocks and combining them into complex agentic systems often requires domain-specific manual tuning and substantial effort from both researchers and engineers.\nHowever, the history of machine learning reveals a recurring theme: manually created artifacts become replaced by learned, more efficient solutions over time as we get more compute and data (Clune, 2019). An early example is from computer vision, where hand-designed features like HOG (Dalal & Triggs, 2005) were eventually replaced by learned features from Convolutional Neural"}, {"title": "Automated Design of Agentic Systems", "content": "may not only potentially save human effort in developing powerful agents but also could be a faster path to more effective solutions than manual design.\nAlthough a few existing works can be considered as ADAS methods, most of them focus only on designing prompts (Fernando et al., 2024; Yang et al., 2024), greatly limiting their ability to invent flexible design patterns in agents (Section 5). In this paper, we show that there is an unexplored yet promising approach to ADAS where we can define the entire agentic system in code and new agents can be automatically discovered by a \u201cmeta\u201d agent programming even better ones in code. Given that most programming languages, such as Python, which we use in this paper, are Turing Complete (Boyer & Moore, 1983; Ladha, 2024), searching within a code space theoretically enables a ADAS algorithm to discover any possible agentic systems, including all components such as prompts, tool use, control flows, and more. Furthermore, with recent FMs being increasingly proficient in coding, we can use FMs as a meta agent to create new agents in code for ADAS, enabling novel agents to be programmed in an automated manner.\nFollowing the aforementioned ideas, we present Meta Agent Search in this paper as one of the first algorithms in ADAS that enables complete design in code space (Figure 1). The core concept of Meta Agent Search is to instruct a meta agent to iteratively create interestingly new agents, evaluate them, add them to an archive that stores discovered agents, and use this archive to help the meta agent in subsequent iterations create yet more interestingly new agents. Similar to existing open-endedness algorithms that leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a), we encourage the meta agent to explore interesting (e.g., novel or worthwhile) agents. To validate the proposed approach, we evaluate the proposed Meta Agent Search on: (1) the challenging ARC logic puzzle task (Chollet, 2019) that aims to test the general intelligence of an AI system, (2) four popular benchmarks on reading comprehension, math, science questions, and multi-task problem solving, and (3) the transferability of discovered agents to held-out domains and models (Section 4).\nOur experiments show that the discovered agents substantially outperform state-of-the-art hand-designed baselines. For instance, our agents improve F1 scores on reading comprehension tasks in DROP (Dua et al., 2019) by 13.6/100 and accuracy rates on math tasks in MGSM (Shi et al., 2023) by 14.4%. Additionally, they improve accuracy over baselines by 25.9% and 13.2% on GSM8K (Cobbe et al., 2021) and GSM-Hard (Gao et al., 2023) math tasks, respectively, after transferring across domains. The promising performance of our algorithm over hand-designed solutions illustrates the potential of ADAS in automating the design of agentic systems. Furthermore, the experiments demonstrate that the discovered agents not only perform well when transferring across similar domains but also exhibit strong performance when transferring across dissimilar domains, such as from mathematics to reading comprehension. This highlights the robustness and transferability of the agentic systems discovered by Meta Agent Search. In conclusion, our work opens up many exciting research directions and encourages further studies (Section 6)."}, {"title": "2. New Research Area: Automated Design of Agentic Systems (ADAS)", "content": "At the time of writing, the community has not reached a consensus on the definitions or terminologies of agents. Here, by agents we refer to agentic systems that involve Foundation Models (FMs) as modules in the control flow to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing (Chase, 2024; Ng, 2024).\nIn this paper, we propose a new research area Automated Design of Agentic Systems (ADAS). Similar to research areas in AI-GAs (Clune, 2019) and AutoML (Hutter et al., 2019), such as Neural Architecture Search (Elsken et al., 2019), we formulate ADAS as an optimization process and identify three key components of ADAS algorithms (Figure 2)."}, {"title": "Formulation", "content": "Automated Design of Agentic Systems (ADAS) involves using a search algorithm to discover agentic systems across a search space that optimize an evaluation function.\n\u2022 Search Space: The search space defines which agentic systems can be represented and thus discovered in ADAS. For example, works like PromptBreeder (Fernando et al., 2024) mutate only the text prompts of an agent, but their other components, such as control flow, remain the same. Thus, in these search spaces, agents that have a different control flow than the predefined one can not be represented. Existing works also explore search spaces such as graph structures (Zhuge et al., 2024) and feed-forward networks (Liu et al., 2023).\n\u2022 Search Algorithm: The search algorithm defines how ADAS algorithms explore the search space. Since the search space is often very large or even unbounded, the exploration-exploitation trade-off (Sutton & Barto, 2018) should be considered. Ideally, the algorithm can both quickly discover high-performance agentic systems and avoid remaining stuck in a local optimum. Existing approaches include using Reinforcement Learning (Zhuge et al., 2024) or an FM iteratively generating new solutions (Fernando et al., 2024) as search algorithms.\n\u2022 Evaluation Function: Depending on the application of the ADAS algorithm, we may consider different objectives to optimize, such as performance, cost, latency, or safety of agents. An evaluation function defines how to evaluate a candidate agent on those objectives. For example, to assess the agent's performance on unseen future data, a simple method is to calculate the accuracy rate on the validation data for a task, which is commonly adopted in existing works (Fernando et al., 2024; Zhuge et al., 2024).\nAlthough many search space designs are possible and some have already been explored (Section 5), there is an unexplored yet promising approach where we can define the entire agentic system in code and new agents can be automatically discovered by a meta agent programming even better ones in code. Searching within a code space theoretically enables the ADAS algorithm to discover any possible building blocks (e.g., prompts, tool use, control flow) and agentic systems that combine any of these building blocks in any way. This approach also offers better interpretability for agent design patterns since the program code is often readable, making debugging easier and enhancing AI safety. Additionally, compared to search spaces using networks (Liu et al., 2023) or graphs (Zhuge et al., 2024), searching in a code space allows us to more easily build on existing human efforts. For example, it is possible to search within open-source agent frameworks like LangChain (LangChainAI, 2022) and build upon all existing building blocks (e.g., RAG, search engine tools). Finally, since FMs"}, {"title": "Automated Design of Agentic Systems", "content": "are proficient in coding, utilizing a code search space allows us to leverage existing expertise from FMs during the search process. In contrast, search algorithms in custom search spaces, such as graphs, may be much less efficient due to the absence of these priors. Therefore, we argue that the approach of using programming languages as the search space should be studied more in ADAS."}, {"title": "3. Our Algorithm: Meta Agent Search", "content": "In this section, we present Meta Agent Search, a simple yet effective algorithm to demonstrate the approach of defining and searching for agents in code. The core idea of Meta Agent Search is to adopt FMs as meta agents to iteratively program interestingly new agents based on an ever-growing archive of previous discoveries. Although any possible building blocks and agentic systems can theoretically be programmed by the meta agent from scratch, it is inefficient in practice to avoid providing the meta agent any basic functions such as FM query APIs or existing tools. Therefore, in this paper, we define a simple framework (within 100 lines of code) for the meta agent, providing it with a basic set of essential functions like querying FMs or formatting prompts. As a result, the meta agent only needs to program a \u201cforward\u201d function to define a new agentic system, similar to the practice in FunSearch (Romera-Paredes et al., 2024). This function takes in the information of the task and outputs the agent's response to the task. Details of the framework codes and examples of the agents defined with this framework can be found in Appendix B.\nAs shown in Figure 1, the core idea of Meta Agent Search is to have a meta agent iteratively program new agents in code. We show the main prompt for the meta agent to program new agents below, where variables in the prompts are highlighted. Similar to existing open-endedness algorithms that leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a), we encourage the meta agent to explore interestingly new (e.g., novel or worthwhile) agents based on an ever-growing archive of previous discoveries. We also adopt self-reflection (Madaan et al., 2024; Shinn et al., 2023) iterations in our meta agent, where it performs two iterations of refinement on the novelty and correctness of the proposal and performs up to three refinements when errors occur while running the code. Full details of the prompt are presented in Appendix A.\nAfter a new agent is generated, we evaluate it using the validation data from the target domain. Here, we calculate the performance (e.g., success rate or F1 score) and 95% bootstrap confidence interval as the metrics for the meta agent to maximize. The generated agent is then added to the archive with the evaluation metrics, and the iteration continues with the updated archive until the maximum number of iterations is reached."}, {"title": "Main prompt for the meta agent.", "content": "You are an expert machine learning researcher testing different agentic systems.\n[Brief Description of the Domain]\n[Framework Code]\n[Output Instructions and Examples]\n[Discovered Agent Archive] (initialized with baselines, updated at every iteration)\n# Your task\nYou are deeply familiar with prompting techniques and the agent works from the literature. Your goal is to maximize the performance by proposing interestingly new agents\nUse the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design."}, {"title": "4. Experiments", "content": "We conduct extensive experiments on: (1) the challenging ARC logic puzzle task (Chollet, 2019) (Section 4.1), (2) four popular benchmarks assessing the agent's abilities on reading comprehension, math, science questions, and multi-task problem solving (Section 4.2), (3) the transferability of the discovered agents on ARC to three held-out models, and (4) the transferability of discovered agents on Math to four held-out math tasks and three tasks that are beyond math (Section 4.3). Across all experiments, we find that the discovered agents substantially outperform baseline state-of-the-art hand-designed agents. Notably, our discovered agents improve over baselines on reading comprehension tasks in DROP (Dua et al., 2019) by 13.6/100 (F1 score) and on math tasks in MGSM (Shi et al., 2023) by 14.4% (accuracy rate). Additionally, our discovered agents improve over the baseline on ARC tasks by 14% (accuracy rate) after transferring from GPT-3.5 to GPT-4, and by 25.9% and 13.2% (accuracy rate) after transferring from MGSM math tasks to held-out math tasks in GSM8K (Cobbe et al., 2021) and GSM-Hard (Gao et al., 2023) respectively. All code, prompts, and experiment results are available at https://github.com/ShengranHu/ADAS."}, {"title": "4.1. Case Study: ARC Challenge", "content": "We first demonstrate how Meta Agent Search discovers novel agentic systems and outperforms existing state-of-the-art hand-designed agents in the Abstraction and Reasoning Corpus (ARC) challenge (Chollet, 2019). This challenge aims to evaluate the general intelligence of AI systems through their ability to efficiently acquire new skills. Questions in ARC include (1) showing multiple examples of visual input-output grid patterns, (2) the AI system learning the transformation rule of grid patterns from examples, and (3) predicting the output grid pattern given a test input grid pattern. Since each question in ARC has a unique transformation rule, it requires the AI system to learn efficiently with"}, {"title": "Automated Design of Agentic Systems", "content": "few-shot examples, leveraging capabilities in number counting, geometry, and topology.\nSetup. Following common practice (Greenblatt, 2024), we require the agent to write code for the transformation rule instead of answering directly. We provide tool functions in the framework that evaluate the generated transformation code. Given the significant challenge that ARC poses to current Al systems, we sample our data from questions with grid dimensions \u2264 5 \u00d7 5 in the \u201cPublic Training Set (Easy)\". We sample a validation set and a test set with 20 and 60 questions, respectively, for searching and testing. We calculate the validation and test accuracy of an agent by assessing it over the validation and test sets five times to reduce the variance from the stochastic sampling of FMs. We evaluate all discovered agents on the held-out test set and report the test accuracy in Figure 3. Meta Agent Search runs for 25 iterations and the meta agent uses GPT-4 (OpenAI, 2024), while discovered agents and baselines are evaluated using GPT-3.5 (OpenAI, 2022) to reduce compute cost. More algorithmic details and examples of ARC questions can be found in Appendix C.\nBaselines. We compared against five state-of-the-art hand-designed agents: (1) Chain-of-Thought (COT) (Wei et al., 2022), which instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps; (2) Self-Consistency with Chain-of-Thought (COT-SC) (Wang et al., 2023b), which ensembles multiple parallel answers from COT to produce a more accurate answer; (3) Self-Refine (Madaan et al., 2024; Shinn et al., 2023), which allows iterative self-reflection to correct mistakes made in previous attempts; (4) LLM-Debate (Du et al., 2023), which enables different LLMs to debate with each other, leveraging diverse perspectives to find better answers; (5) Quality-Diversity, a simplified version of Intelligent Go-Explore (Lu et al., 2024c), which produces and ensembles diverse answers to better explore potential solutions. We also use all baselines as initial seeds in the archive for Meta Agent Search. More details about baselines can be found in Appendix E.\nResults and Analysis. As shown in Figure 3a, Meta Agent Search effectively and progressively discovers agents that perform better than state-of-the-art hand-designed baselines. Important breakthroughs are highlighted in the text boxes. As is critical in prior works on open-endedness and AI-GAs (Faldor et al., 2024; Lehman & Stanley, 2011; Wang et al., 2019, 2020; Zhang et al., 2024a), Meta Agent Search innovates based on a growing archive of previous stepping stones. For example, an important design pattern emerged in iteration 3 where it uses multiple COTs to generate possible answers, refines them, and finally ensembles the best answers. This became a crucial stepping stone that subsequent designs tended to utilize. Additionally, the best-discovered agent is shown in Figure 3b, where a complex feedback mechanism is adopted to refine answers more effectively. Careful observation of the search progress reveals that this sophisticated feedback mechanism did not appear suddenly. Instead, the ideas of incorporating diverse feedback, evaluating for various specific traits (via experts) such as efficiency and simplicity, and simulating human-like feedback emerged in iterations 5, 11, and 12, respectively. The final mechanism is an innovation based on these three stepping stones. This illustrates that even though these stepping stones did not achieve high performance immediately upon emergence, later discoveries benefited from these innovations by combining different stepping stones, resembling crossover in evolution via LLMs (Meyerson et al., 2023). Overall, the results showcase the potential of ADAS and the effectiveness of Meta Agent Search to progressively discover agents that outperform state-of-the-art hand-designed baselines and invent novel design patterns through the innovation and combination of various stepping stones.\""}, {"title": "4.2. Reasoning and Problem-Solving Domains", "content": "Setup. Next, we investigate the potential of our algorithm to improve the capabilities of agents across math, reading, and reasoning domains. We test Meta Agent Search on four popular benchmarks: (1) DROP (Dua et al., 2019) for evaluating Reading Comprehension; (2) MGSM (Shi et al., 2023) for"}, {"title": "Automated Design of Agentic Systems", "content": "evaluating Math capability under a multi-lingual setting; (3) MMLU (Hendrycks et al., 2021) for evaluating Multi-task Problem Solving; and (4) GPQA (Rein et al., 2023) for evaluating the capability of solving hard (graduate-level) questions in Science. The search is conducted independently within each domain. Meta Agent Search runs for 30 iterations. The meta agent uses GPT-4 (OpenAI, 2024), while the discovered agents and baselines are evaluated using GPT-3.5 (OpenAI, 2022). More details about datasets and experiment settings can be found in Appendix D.\nBaselines. We adopt all baselines introduced in Section 4.1. Additionally, since the above domains require strong reasoning skills, we include two additional baselines that specifically focus on enhancing the reasoning capabilities of agents for a more thorough comparison: (1) Step-back Abstraction (Zheng et al., 2023), which instructs agents to first consider the principles involved in solving the task for better reasoning; (2) Role Assignment, which assigns different roles to FMs similar to Xu et al. (2023) to obtain better answers. More details about the baselines can be found in Appendix E.\nResults and Analysis. The results across multiple domains demonstrate that Meta Agent Search can discover agents that outperform state-of-the-art hand-designed agents (Table 1). We want to highlight the substantial gap between the learned agents and hand-designed agents in the Reading Comprehension and Math domains, with improvements in F1 scores by 13.6/100 and accuracy rates by 14.4%, respectively. While Meta Agent Search also outperforms baselines in the Multi-task and Science domains, the gap is smaller. We hypothesize that for challenging questions in the Science and Multi-task domains, the knowledge in FMs is not sufficient to solve the questions, limiting the improvement through optimizing agentic systems, which is a problem that will diminish as FMs improve. In contrast, in the Reading Comprehension and Math domains, FMs possess adequate knowledge to solve the questions, and errors could mainly be hallucinations or calculation mistakes, which can be mitigated through well-designed agentic systems, like the ones discovered by Meta Agent Search. Overall, the results across various domains showcase the effectiveness of Meta Agent Search in searching for agents tailored to specific domains. This could be increasingly useful for saving human efforts and developing better task-specific agents as we continue to create agents for a diverse set of applications (Wang et al., 2024)."}, {"title": "4.3. Generalization and transferability", "content": "In the previous sections, we illustrated that Meta Agent Search can find effective agents for individual tasks. In this section, we further demonstrate the transferability and generalizability of the discovered agents. To show that the invented building blocks and design patterns are generalizable, we conduct experiments on the transferability of the discovered agents.\nTransferability Across Foundation Models. We first transfer discovered agents from GPT-3.5 (OpenAI, 2022) to other FMs on ARC to test whether agents found when performing Meta Agent Search with one FM generalize to others. We test the top 3 agents with the best test accuracy evaluated with GPT-3.5 on ARC and then transfer them to three popular models: Claude-Haiku (Anthropic, 2024a), GPT-4 (OpenAI, 2024), and Claude-Sonnet (Anthropic, 2024b). We adopt the same baselines as those used in ARC (Section 4.1) and MGSM (Section 4.2). As shown in Table 2, we observe that the searched agents consistently outperform the hand-designed agents with a substantial gap. Notably, we found that Claude-Sonnet, the most powerful model from Anthropic, performs the best among all tested models, enabling our best agent to achieve nearly 50% accuracy on ARC.\nTransferability Across Domains. Next, we transfer the discovered agent from the MGSM (Math) domain to other math domains to test whether the invented agents can generalize across different domains. Similarly, we test the top 3 agents from MGSM and transfer them to (1) four popular math domains: GSM8K (Cobbe et al., 2021), GSM-Hard (Gao et al., 2023), SVAMP (Patel et al., 2021), and ASDiv (Miao et al., 2020) and (2) three domains beyond math adopted in Section 4.2. As shown in Table 3, we observe a similar superiority in the performance of Meta Agent Search compared to baselines. Notably, our agents improve accuracy by 25.9% and 13.2% on GSM8K (Cobbe et al., 2021) and GSM-Hard (Gao et al., 2023), respectively, compared to the baselines. More surprisingly, we observe that agents discovered in the math domain can be transferred to non-math domains (Table 4). While the performance of agents originally searched in the math domain does not fully match that of agents specifically designed for the target domains, they still outperform (in Reading Comprehension and Multi-task) or match (in Science) the state-of-the-art hand-designed agent baselines. These results illustrate that Meta Agent Search can discover generalizable design patterns and agentic systems."}, {"title": "5. Related Work", "content": "Agentic Systems. Researchers develop various building blocks and design patterns for different applications. Important building blocks for agentic systems includes: prompting techniques (Chen et al., 2023a; Schulhoff et al., 2024), chain-of-thought-based planning and reasoning methods (Hu & Clune, 2024; Wei et al., 2022; Yao et al., 2023), reflection (Madaan et al., 2024; Shinn et al., 2023), developing new skills for embodied agents in code (Vemprala et al., 2023; Wang et al., 2023a), external memory and RAG (Lewis et al., 2020; Zhang et al., 2024c), tool use (Nakano et al., 2021; Qu et al., 2024; Schick et al., 2023), assigning FM modules in the agentic system with different roles and enabling them to collaborate (Hong et al., 2023; Qian et al., 2023, 2024; Wu et al., 2023; Xu et al., 2023), and enabling the agent to instruct itself for the next action (Richards, 2023), etc. While the community has invested substantial effort in developing all the above important techniques, this is only a partial list of the discovered building blocks, and many more remain to be uncovered. Therefore, in this paper, we propose a new research area, ADAS, which aims to invent novel building blocks and design powerful agentic systems in an automated manner.\nAI-Generating Algorithms and AutoML. Following the lessons learned from the history of machine learning, research in AI-Generating Algorithms (AI-GAs) (Clune, 2019) and AutoML (Hutter et al., 2019) continually strives to learn more components in AI systems to replace handcrafted ones. There are mainly three pillars in this field: (1) meta-learning architectures, (2) meta-learning the learning algorithms, and (3) generating effective learning environments and training data (Clune, 2019). For example, Neural Architecture Search (Elsken et al., 2019; Hu et al., 2021; Lu et al., 2019) aims to automate the design of neural network architectures like convolution, which falls under the first pillar. The second pillar includes works like MAML (Finn et al., 2017) and Meta-RL (Duan et al., 2017; Norman & Clune, 2023; Wang et al., 2016; Zintgraf et al., 2021a,b), which allow \u201clearning to learn\" for better sample efficiency, generalizability, and continuous learning of multiple tasks. Additionally, works like POET (Dharna et al., 2020; Wang et al., 2019, 2020) and OMNI-EPIC (Faldor et al., 2024) under the third pillar aim to generate learning environments in an open-ended manner. We believe\""}, {"title": "Automated Design of Agentic Systems", "content": "that the proposed Automated Design of Agentic Systems belongs to both the first and second pillars: Pillar one because ADAS meta-learns the architecture of agentic systems, but also Pillar two because agents are proficient in in-context learning, thus ADAS can also be considered as learning to learn, as demonstrated in the ARC challenge (Section 4.1).\nAdditionally, recent AI-GA and AutoML works have incorporated Foundation Models (FMs) to write code. For example, in FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024), FMs write code to discover better optimization algorithms. In DiscoPOP (Lu et al., 2024a), FMs program the loss function for preference learning in FM alignment training (Rafailov et al., 2024). Additionally, Eureka (Ma et al., 2023) and language-to-reward (Yu et al., 2023) enable FMs to write reward functions for reinforcement learning in robotics. Finally, OMNI-EPIC (Faldor et al., 2024) enables FMs to create robotics learning environments by programming in code. Here, we adopt a similar idea that enables FMs to program new agents in code.\nExisting Attempts to ADAS. There are two categories of works that can be considered attempts at ADAS in the literature: those that learn better prompts only, and those that learn more components in agents than just prompts. Most works fall into the first category: learning prompts only. Works like OPRO (Yang et al., 2024), PromptBreeder (Fernando et al., 2024), and Self-Discover (Zhou et al., 2024a) adopt FMs to automate prompt engineering for agents, primarily focusing on the phrasing of instructions in the prompt to enhance the reasoning capability of agents. Thus, the learned prompts are domain-specific and difficult to generalize. Beyond instructions, works like EvoAgent (Yuan et al., 2024) and AgentVerse (Chen et al., 2023b) optimize role definition in the prompt, as assigning personas or roles to agents has been shown to be beneficial (Xu et al., 2023). Although tuning prompts effectively improves performance, other important components in agentic systems remain fixed and hand-designed, vastly limiting the space of agents that can be discovered.\nThere are far fewer attempts in the second category, which involves learning more components than just prompts in agentic systems. Most represent agents as networks or graphs in the search space. In these formulations, the FM with a certain prompt is considered a transformation function for text on nodes, and the information flow of the text is considered as edges. DyLAN (Liu et al.,"}, {"title": "Automated Design of Agentic Systems", "content": "2023) starts with a fully connected feed-forward network and uses FMs to score the response quality of nodes in each layer to prune the connections. DSPy (Khattab et al., 2024) first generates a set of possible nodes and then optimizes across the Cartesian product of these nodes while optimizing the few-shot examples for nodes. GPT-Swarm (Zhuge et al., 2024) represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the possible connections between nodes while optimizing the prompt for each node in a separate stage. Although these works allow the learning of control flow (optimizing edges in networks or graphs), many other components, such as whether and which tools to learn or even how many nodes to have, are still not learned, greatly limiting the space of agents that can be discovered. Besides learning prompts and control flow, AgentOptimizer (Zhang et al., 2024b) learns the tools used in agents, and Agent Symbolic Learning (Zhou et al., 2024b) learns prompts, tools, and control flow together. While Agent Symbolic Learning shares similar motivations to learn more components in agents, it manually designs the search space for each component separately, which may make it a harder search space for search algorithms. In addition, it mainly improves agents based on an existing complex agent, without showing the emergence of new design patterns or building blocks. In contrast, our work represents all possible components in code, allowing the search to be easier by leveraging human efforts in the existing codebase of agents and FMs' expertise in coding. We also demonstrate how novel and diverse building blocks and design patterns emerge from a set of basic agent designs, illustrating the potential creativity that can emerge from ADAS."}, {"title": "6. Discussion and Conclusion", "content": "Safety Considerations. We strongly advise researchers to be aware of the safety concerns when executing untrusted model-generated code in Meta Agent Search and other research involving code generation. While it is highly unlikely that model-generated code will perform overtly malicious actions in our current settings and with the Foundation Models (FMs) we use, such code may still act destructively due to limitations in model capability or alignment (Chen et al., 2021; Rokon et al., 2020). Ideally, sandbox environments can be used to safely run untrusted model-generated code (Chen et al., 2021; Yee et al., 2010).\nMore broadly, research on more powerful Al systems raises the question of whether we should be conducting research to advance AI capabilities at all. That topic clearly includes the proposed Automated Design of Agentic Systems (ADAS) as a new area in AI-GA research, which could potentially contribute to an even faster way to create Artificial General Intelligence (AGI) than the current manual approach (Clune, 2019). The question of whether and why we should pursue AGI and AI-GA has been discussed in many papers (Bengio et al., 2024; Bostrom, 2002; Clune, 2019; Ecoffet et al., 2020; Yudkowsky et al., 2008), and is beyond the scope of this paper. Specifically as regards ADAS, we believe it is net beneficial to publish this work. First, this work demonstrates that with the available API access to powerful FMs, it is easy to program powerful ADAS algorithms, and do so without any expensive hardware like GPUs. We feel it is beneficial to let the community know such algorithms are powerful and easy to create, so they can be informed and account for them. Moreover, by sharing this information, we hope to motivate follow-up work into safe-ADAS, such as algorithms that conduct ADAS safely during both search itself (e.g. not risking running any harmful code) and that refuse to create dishonest, unhelpful, and/or harmful agents. Such an open-source research approach to create safe-ADAS could be a better way to create safer AI systems (Caldwell"}]}