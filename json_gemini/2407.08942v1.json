{"title": "A Neural Matrix Decomposition Recommender System Model based on the\nMultimodal Large Language Model", "authors": ["Ao Xiang", "Bingjie Huang*", "Xinyu Guo", "Haowei Yang", "Tianyao Zheng"], "abstract": "Recommendation systems have become an important solution to information search problems. This article proposes a\nneural matrix factorization recommendation system model based on the multimodal large language model called BoNMF.\nThis model combines BOBERTa's powerful capabilities in natural language processing, ViT in computer in vision, and\nneural matrix decomposition technology. By capturing the potential characteristics of users and items, and after interacting\nwith a low-dimensional matrix composed of user and item IDs, the neural network outputs the results. recommend. Cold\nstart and ablation experimental results show that the BoNMF model exhibits excellent performance on large public data sets\nand significantly improves the accuracy of recommendations.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of the Internet, users are confronted with vast amounts of data and information.\nThe challenge of finding content that aligns with users' interests within this abundance has become\nincreasingly important. Recommender systems play a crucial role in addressing this issue, as they have the\npotential to provide precise recommendations that enhance user experience and save time in commercial\napplications [1]. These systems predict user ratings for specific items by employing data mining techniques\nand related predictive algorithms to make highly relevant predictions. By analyzing user historical behavior,\npreferences, and item characteristics, recommender systems effectively solve the information filtering problem\nby automatically matching items that may be of interest to users. Traditional recommender systems primarily\nconsist of collaborative filtering [2], content-based recommendations [3], and hybrid recommendation methods,\namong which collaborative filtering is one of the earliest and most widely used techniques for recommending\nproducts or items based on past purchasing history.\nHowever, in the era of big data, the scale of multi-modal data has become enormous, posing a greater\nchallenge in providing accurate recommendations to users [4]. Collaborative filtering also presents limitations\nsuch as the cold start problem, sparsity, and scalability issues, which result in poor predictive performance.\nWith the advancement of deep learning technology, more researchers are turning their focus towards neural\nnetwork models to enhance the performance of recommender systems. Neural network technology has been\nintegrated into matrix factorization, giving rise to the Neural Matrix Factorization (NMF) model [5], which has\ndemonstrated outstanding results in recommendation prediction. However, NMF models still encounter\ndrawbacks similar to traditional collaborative filtering methods when dealing with scenarios involving cold\nstarts or small data volumes. As large models become more prevalent, they appear to offer a solution to the\ncold start problem in recommender systems. BoBERTa, a variant of the BERT large model, has exhibited\nstrong performance across various natural language processing tasks. Therefore, this paper introduces\nBoBERTa into recommender systems by combining it with neural matrix factorization to propose the BoNMF\nmodel. This model harnesses the vectorization capabilities of large models effectively alleviating the cold start\nproblem in recommender systems while enhancing recommendation accuracy and personalization."}, {"title": "2 RELATED WORK", "content": "In the field of recommender systems, matrix factorization is a classic and effective method for achieving\npersonalized recommendations. It works by decomposing the user-item interaction matrix into low-\ndimensional latent feature vectors. Collaborative filtering can be divided into two main categories: user-based\ncollaborative filtering (User-based CF), which recommends items liked by users similar to the target user, and\nitem-based collaborative filtering, which recommends items similar to those the user likes. Matrix factorization\nis an implementation of collaborative filtering, with common methods including Singular Value Decomposition\n(SVD) and Non-Negative Matrix Factorization (NMF) [6]. The principle behind matrix factorization is to\ndecompose the user-item rating matrix into two low-dimensional matrices to capture deep features of users\nand items. Collaborative filtering has always been a classic method used [7], however, traditional matrix\nfactorization methods have been found to be difficult to deal with situations such as sparse data [8] and may\nnot be able to effectively capture the deeper nonlinear characteristics of users and items. [9].\nAs neural networks have been widely used in many fields including medicine [10], business [11], intelligent\ndriving [12], smart Contract [13], etc., recommendation systems have gradually introduced this mature\ntechnology. Neural collaborative filtering combines matrix factorization with neural networks, utilizing deep\nlearning models to better capture complex interaction relationships between users and items. Traditional\nmatrix factorization predicts ratings through the inner product of latent feature vectors of users and items,\nwhile neural collaborative filtering models achieve this process by constructing interaction vectors between\nusers and items and feeding them into neural networks such as multilayer perceptron's [14]. Traditional\ncollaborative filtering faces the cold start problem. The cold start problem refers to the situation where a\nrecommender system cannot provide accurate recommendations for new users or new items [15].\nCollaborative filtering methods rely on historical interaction data between users and items. When there is\ninsufficient interaction data for new users or new items, collaborative filtering algorithms may fail to generate\neffective recommendations, leading to lower evaluation metrics for the recommender system. To address this\nissue, some researchers use additional information such as content and tag information to solve the cold start\nproblem [16]. A newer approach might involve introducing large models to obtain prior knowledge [17].\nResearchers have preliminarily demonstrated that large language models can effectively enhance the training\nsignal for cold start items, significantly improving the recommendation of cold start items in various\nrecommendation models. This study will further integrate large models with neural matrix factorization\nalgorithms to improve the accuracy of recommender systems.ACM's new manuscript submission template\naims to provide consistent styles for use across ACM publications and incorporates accessibility and\nmetadata-extraction functionality necessary for future Digital Library endeavors. Numerous ACM and SIG-\nspecific templates have been examined, and their unique features incorporated into this single new template.\nIf you are new to publishing with ACM, this document is a valuable guide to the process of preparing your\nwork for publication. If you have published with ACM before, this document provides insight and instruction\ninto the current process for preparing` your manuscript."}, {"title": "3 METHODOLOGY", "content": "To verify the effectiveness of the BoNMF model, we use an augmented dataset based on the public\nMovieLens dataset, which contains 1,000,209 ratings of about 3,900 movies, generated by a total of 6,040\nMovieLens users. The rating information includes the movie name and category as shown in Figure 1, and the\nuser information includes ID, gender, occupation, etc. At the same time, we added images of all movie\nposters and movie introductions from Wikipedia as additional supplementary information.\nBERT is a pre-trained language representation model. Unlike traditional unidirectional language models or\nshallow concatenation of two unidirectional language models and fixed vector encoding methods like TF-IDF,\nBERT employs a novel masked language model (MLM), allowing it to generate deep bidirectional language\nrepresentations. The structure is shown as Figure 2. The input vector of BERT consists of token embedding,\nposition embedding, and segment embedding. In the composition of token embedding, the first element is\nCLS, and the corresponding vector of the last layer of the BERT model serves as the semantic representation\nof the entire sentence. It can more fairly integrate the semantic information of each word in the text, better\nrepresent the emotional semantics of the entire sentence, and help the prediction of the recommendation\nsystem. ROBERTa [18] made several adjustments based on the pre-trained BERT model. These adjustments\ninclude longer training times, more training data, the removal of next predict loss, and a dynamically adjusted\nmasking mechanism."}, {"title": "3.3 Evaluation Metrics", "content": "In the task, three methods commonly used evaluation metrics, mean square error, average accuracy, and\nprecision@K, are applied in this paper.\nMean squared error (MSE), which shown in Formula 1, a metric used to evaluate the performance of a\nregression model and measures the average squared difference between predicted and actual values. It can\nreflect the overall prediction accuracy of the model and is the most widespread evaluation index. n is the total\nnumber of samples, $r_i$ is the predicted result of the i-th observation predicted by the model, and $r_i$ is the actual\nvalue of the i-th observation.\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{r_i} - r_i)^2$ #(1)\nPrecision@K is an indicator used to evaluate the performance of recommendation systems, measuring the\nproportion of the number of relevant items in the top K recommendation results to the total number of\nrecommendations. U represents the set of users, $R_u$ is the interaction between the user and the items,\nand $R_u \\cap R_{u,K}$ is the results of recommended items that the user is interested in $R_u$ is the interaction matrix\nbetween the user and the item, and $R_u \\cap R_{u,K}$ is the final recommended project results with the highest ranking.\nPrecision@K = $\\frac{1}{|U|} \\sum_{u \\epsilon U} \\frac{|R_u \\cap R_{u,K}|}{K}$ #(2)\nNormalized Discounted Cumulative Gain (NDCG) is also popular metric for evaluating the quality of\nrecommendation systems that takes into account the relevance and position of the results. DCG@K\n(Discounted Cumulative Gain) is used to calculate the cumulative gain of the top k recommended items for a\ngiven user. IDCG@K (Ideal Discounted Cumulative Gain) represents the maximum possible value of DCG@K\nunder ideal conditions. By assigning weights to positions, the highest-ranked correlations receive greater\nimportance, reflecting the significance of more relevant rankings. Among them, DCG calculates the sum of\ncorrelations in the recommended results, regardless of the location of the results. DCG introduces the total\ncorrelation in the recommendation results of the location loss factor.\nNDCG@K = $\\frac{1}{|U|} \\sum_{UEU} \\frac{DCG@K}{IDCG@K}$ #(3)"}, {"title": "4 EXPERIMENT", "content": "The user's merged embedding vector and the movie's BoBERTa embedding vector are both 769-dimensional,\nand the user's movie id is reorganized into a 50-dimensional embedding vector through MLP. The two are\ndirectly spliced and then output through a 128-dimensional MLP. II MLPs were configured with a two-layer\nstructure: the first layer consisting of 128 neurons and the second layer consisting of 64 neurons. The third\nlayer served as the output layer. We split the dataset into training and testing sets in a 7:3 ratio and employed\ncross-validation for training. and the number of recommended items, was set to 5, all neural networks were\ntrained for 10 epochs. At the same time, the study removed part of the movie's modal information for\ncomparison, removing images, text, and structural data respectively. At the same time, to evaluate the\neffectiveness of the proposed model, we conducted multiple ablation experiments to test the importance of\ndifferent modalities on model performance and the impact of adding LLM on improving semantic vector\nrepresentation.\nFirst, we directly used traditional matrix decomposition as the baseline model. At the same time, we also\nadded a simple neural matrix decomposition model as another comparison. Simple neural matrix\ndecomposition the vector representation of large model decomposition will not be used but will be directly\nconverted into low-dimensional vectors through MLP and then spliced. We also added three models that\nremoved images, text, and structured data. Finally, we compared some recently popular methods for splicing\ndifferent large models, such as Tensor product-based model [20], Text Early Fusion Model [21] and DNN\nbased model [22]. The Tensor product-based model uses a large language model to combine multiple\nmodalities in the form of vector products. The Text Early Fusion Model uses an early fusion approach to\nconcatenate text and low-dimensional vectors for training, while the DNN-based model directly maps text into\nvectors and directly enters the deep network for training after performing dot products. The final model\ntraining results are shown in the table 1."}, {"title": "5 ANALYSIS", "content": "The BoNMF model exhibits the MSE of 0.6917, while the NMD model shows an MSE of 0.8075. The SVD\nmodel has the highest MSE at 5.011, significantly surpassing the other two models. This indicates that the\nBONMF model achieves the smallest average error between its predicted ratings and the actual ratings,\ndemonstrating superior accuracy in capturing user rating behaviors. The NMD model, which employs a neural\nnetwork structure, substantially alleviates the shortcomings associated with the absence of high-dimensional\nvector representations. In contrast, the traditional SVD model fails to capture deeper semantic meanings\neffectively.\nAdditionally, the BoNMF model attains the highest NDCG@K of 0.6911, indicating that its recommendation\nresults are the most relevant and well-ranked. The NMD model achieves an NDCG@K of 0.6367, slightly\nlower than that of the BoNMF model, while the SVD model's NDCG@K is 0.6184, placing it between the\nBoNMF and NMD models, indicating relatively good relevance and ranking of its recommendation results.\nLastly, the Precision@K for the BONMF, NMD, and SVD models is uniformly 0.8865, indicating that all three\nmodels have the same accuracy in the top K recommendations. However, this does not imply that the\neffectiveness of the three models is identical. This uniformity could be attributed to the dataset's lack of long\nsentence semantic information or the evaluation function's leniency, which does not account for the weight of\ndifferent factors. Therefore, future experiments should aim to improve the completeness of the experimental\ndataset by including more subjective and abstract semantic information to enhance the recommendation\nsystem's accuracy.\nWhen the BoNMF model does not include image information, the MSE increases to 0.7112, and\nPrecision@K and NDCG decrease to 0.8677 and 0.6331, respectively. Although the precision and NDCG\ndecrease, the difference is not particularly large compared to the BoNMF model that includes image\ninformation. When the BoNMF model does not include text information, the MSE increases to 0.7258, and\nPrecision@K and NDCG decrease to 0.8512 and 0.6224, respectively. Compared with the full BoNMF model,\nthe performance decreases significantly, which indicates that the description is more accurate than the\ninformation presented in the image. Secondly, the addition of the image does not significantly improve the\naccuracy and recommendation level, so it is reasonable to guess that most of the elements that appear in the\nimage may already be included in the description."}, {"title": "6 DISCUSSION", "content": "We can see that compared with some popular fusion methods currently in use, our model still has certain\nadvantages. Although not shown in the table, we also compared the impact of different large models such as\nBERT and Xlnet on high-dimensional vector decomposition with some mainstream large models, but\naccording to the results, the difference is within 0.5%, which may still be caused by defects such as data sets;\nIt must be admitted that for different modes, some models such as Swin-T model will make the progress\nhigher and higher, and the impact of large models with different structures on high-dimensional vector\ndecomposition needs further discussion."}, {"title": "7 CONCLUSION", "content": "In this study, we introduce the BoNMF model, a neural matrix factorization recommendation system based on\nlarge-scale pre-trained models BoBERTa and ViT. By integrating the high-dimensional text feature vectors\nfrom BoBERTa and the high-dimensional image feature vectors decomposed by ViT and combining them with\nthe low-dimensional embeddings of user and item IDs, this model effectively leverages the capabilities of\nlarge models in understanding text and images and captures deep feature representations of users and items\nthrough the advantages of neural matrix factorization networks. Our experimental results on the MovieLens\ndataset demonstrate that the BoNMF model outperforms the NMD and SVD models across various evaluation\nmetrics, achieving an impressive MSE of 0.69. Our findings indicate that the BoNMF model is a promising\napproach for improving recommendation systems, especially in addressing the cold-start problem and\ncapturing deep semantic information. Future work will focus on further exploring the utilization of multimodal\ndata to provide deeper insights for optimizing recommendation systems"}]}