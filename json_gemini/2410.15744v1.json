{"title": "UNLEASHING THE POTENTIAL OF VISION-LANGUAGE PRE-TRAINING FOR 3D ZERO-SHOT LESIon Segme\u039d\u03a4\u0391-TION VIA MASK-ATTRIBUTE ALIGNMENT", "authors": ["Yankai Jiang", "Wenhui Lei", "Xiaofan Zhang", "Shaoting Zhang"], "abstract": "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. Codes will be publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "3D medical image segmentation has witnessed rapid advancements in recent years (Isensee et al., 2021; Tang et al., 2022; Ye et al., 2023; Liu et al., 2023; Chen et al., 2024; Zhang et al., 2024). However, most tate-of-the-art (SOTA) methods are restricted to a closed-set setting, where they can only predict categories present in the training dataset and typically fail to generalize to unseen disease categories. Given the diversity and prevalence of new anomalies in clinical scenarios, along with the challenges of medical data collection, there is an increasing demand for zero-shot models capable of handling unseen diseases in an open-set setting.\nThe advent of vision-language pre-training methods, particularly CLIP (Radford et al., 2021), has illuminated a new paradigm for remarkable zero-shot object recognition. This breakthrough also paves the way for significant advancements in zero-shot disease detection and diagnosis. Numerous recent methods (Huang et al., 2021; Tiu et al., 2022; Wu et al., 2023; Phan et al., 2024; Hamamci et al., 2024) align visual and textual features of paired medical image-report data, enabling transferable cross-modal representations. However, leveraging the zero-shot capability of vision-language pre-training for 3D lesion/tumor segmentation remains a scarcely explored area. This extension is nontrivial and faces two obvious challenges: (i) The substantial gap between the upstream contrastive pre-training task and the downstream per-pixel dense prediction task. The former focuses on aligning image-level global representations with text embeddings, while the latter requires fine-grained lesion-level visual understanding. This inherent gap necessitates the development of more advanced fine-grained vision-language alignment techniques that can facilitate the perception of nuanced, patient-specific pathological visual clues based on the text descriptions. (ii) Lesions can exhibit significant variations in shape and size, and may present with blurred boundaries. Models often struggle when encountering unseen lesion types due to their out-of-distribution visual characteristics. Simply using text inputs, such as raw reports (Boecking et al., 2022; Tiu et al., 2022; Hamamci et al., 2024), or relying on common knowledge of disease definitions (Wu et al., 2023; Jiang et al., 2024), is insufficient for learning generalized representations needed to segment novel lesions not mentioned in the training dataset.\nMotivated by the aforementioned limitations, we introduce Malenia, a novel multi-scale lesion-level mask-attribute alignment framework for superior zero-shot lesion segmentation. Malenia first leverages multi-scale mask representations with inherent boundary information to capture diverse lesion regions, then matches fine-grained visual features of lesions with text embeddings, effectively bridging the gap between the contrastive pre-training task and the per-pixel dense prediction task. To learn extensible representations that are robust to the out-of-distribution visual characteristics of unseen lesions, we incorporate domain knowledge from human experts to structure textual reports into descriptions of various elemental disease visual attributes, such as shape, location, density, and specific patterns related to disease manifestations. Despite the significant variability among lesions, these fundamental attributes are shared across different diseases and are often represented similarly in images. By aligning mask representations of lesions with their corresponding visual attributes, the model mimics the decision-making process of human experts, explicitly linking the visual features of unseen diseases to the intrinsic attributes learned from seen lesions. This alignment significantly enhances the model's ability to recognize and segment various unseen lesions. Furthermore, we propose a novel Cross-Modal Knowledge Injection (CMKI) module in Malenia, inspired by the observation that visual and textual embeddings, after feature alignment, are complementary and can mutually reinforce each other. Unlike existing language-driven segmentation methods (Liu et al., 2023; Jiang et al., 2024), which keep text embeddings fixed, the CMKI module updates both mask and attribute embeddings to facilitate fine-grained multi-modal feature fusion. We leverage both enhanced mask and attribute embeddings to generate predictions by matching query features with image features, and then ensemble these predictions to produce the final segmentation results, demonstrating improved performance for both seen and unseen lesions. To thoroughly validate the effectiveness of Malenia, we evaluate its segmentation performance on both seen and unseen lesions using the MSD (Antonelli et al., 2022), KiTS23 (Heller et al., 2023), and a curated real-world in-house dataset. Malenia consistently outperforms state-of-the-art methods across 12 lesion categories. Our contributions can be summarized as follows:\n\u2022 We present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework that captures extensible multi-modal representations for significantly improved zero-shot lesion segmentation by effectively matching the fine-grained visual appearances of new diseases with the textual representations of various fundamental pathological visual attributes.\n\u2022 Malenia introduces a novel Cross-Modal Knowledge Injection (CMKI) module that enriches both mask and text embeddings with mutually beneficial information through feature fusion, leveraging their complementary strengths to further enhance lesion segmentation performance.\n\u2022 State-of-the-art lesion segmentation performance in the zero-shot setting across three datasets. Malenia significantly outperforms previous methods, and key ablation experiments demonstrate the effectiveness of our strategies in handling lesions with varying characteristics."}, {"title": "2 RELATED WORKS", "content": "Medical Vision-Language Pre-training. Numerous methods (Huang et al., 2021; Tiu et al., 2022; Wu et al., 2023; Phan et al., 2024; Hamamci et al., 2024; Lai et al., 2024) build upon CLIP and align medical images with their corresponding reports or disease definitions to enable zero-shot disease classification. However, these works focus on single body parts (e.g., chest), which limits their applicability in broader medical contexts. Additionally, these methods primarily adhere to a paradigm that matches text embeddings with global image-level or patch-level semantics, originally designed for classification tasks. When adapting their zero-shot capability to fine-grained segmentation tasks, mismatches can occur due to the model's inability to align text embeddings with detailed pixel features. A recent work, CT-GLIP (Lin et al., 2024) uses organ-level vision-language alignment for zero-shot organ classification and abnormality detection, but due to architectural limitations, it requires fine-tuning with a segmentation head and lacks zero-shot segmentation capabilities. In contrast, we extend the zero-shot capability of vision-language pre-training from image-level to pixel-level by aligning lesion features with fundamental multi-aspect disease visual attributes.\nZero-Shot 3D Medical Image Segmentation. Motivated by the impressive zero-shot performance of SAM (Kirillov et al., 2023) and SAM 2 (Ravi et al., 2024) in natural images, numerous studies have evaluated their application in medical image segmentation (Wald et al., 2023; Zhang et al., 2023; Huang et al., 2024; Yamagishi et al., 2024). However, due to the significant domain gap, directly applying SAM or SAM 2 to medical images typically results in unsatisfactory performance. Consequently, recent works (Ma et al., 2024; Guo et al., 2024; Shen et al., 2024; Zhu et al., 2024; Shaharabany & Wolf, 2024) explore effective adaptations of prompted SAM or SAM 2 on medical datasets. Nevertheless, these SAM adaptations require prompts (points, bounding boxes, or masks) sampled from ground truth during testing, which demands significant expertise and is often impractical in real-world clinical scenarios. As a result, prompt-free SAM adaptation methods (Hu et al., 2023; Zhang & Liu, 2023; Cheng et al., 2024; Aleem et al., 2024) have also been proposed. To the best of our knowledge, although the current SAM-based 3D medical image segmentation methods demonstrate promising zero-shot performance in segmenting certain organs in CT scans, they have not been evaluated or proven effective when confronted with unseen lesions that have less defined structures. Apart from SAM-based models, a self-prompted method, ZePT (Jiang et al., 2024), achieves competitive zero-shot tumor segmentation performance by matching class-agnostic mask proposals with text descriptions of general medical knowledge. However, this method does not leverage patient-specific information from reports and overlooks the multi-aspect visual attributes shared across different diseases, leading to compromised zero-shot performance. Our approach goes a step further, achieving significantly superior zero-shot lesion segmentation performance without the need for complex visual prompts."}, {"title": "3 \u041c\u0415\u0422\u041dOD", "content": "Fig. 1 illustrates the pipeline of Malenia. It is built upon the recent mask-based segmentation method Mask2Former (Cheng et al., 2022), which has achieved promising performance in the field of medical image segmentation (Yuan et al., 2023; Chen et al., 2023; Jiang et al., 2024). Below, we introduce (i) our novel multi-scale mask-attribute alignment strategy (Sec.3.1) that effectively aligns lesion representations with text embeddings of fundamental disease attributes; (ii) the Cross-Modal Knowledge Injection module (Sec.3.2) for multi-modal feature representation enhancement; and (iii) the overall training objectives and inference details (Sec. 3.3). We provide the preliminaries, discussing Mask2Former and the problem formulation of zero-shot lesion segmentation, as prior information required for our work in Appendix A."}, {"title": "3.1 MULTI-SCALE MASK-TEXT CONTRASTIVE LEARNING", "content": "Given a set of B image-report pairs, D = {(I\u2081, R\u2081), ..., (I\u0299, R\u0299)}, we aim to establish lesion-level alignment between the mask and text representations within the multi-modal feature space. We first adopt a 3D image encoder to extract high-level features f from the input 3D images I. Then a 3D image decoder gradually upsamples f to generate multi-scale high-resolution per-pixel image features F\u2071 \u2208 \u211d\u1d34\u2071\u00d7\u1d42\u2071\u00d7\u1d30\u2071\u00d7\u1d9c\u2071. Here, H\u2071, W\u2071, D\u2071 and C\u2071 denote the height, width, depth and channel dimension of F\u2071, respectively, which vary depending on the resolution level. Subsequently, we feed N learnable mask tokens (queries) M = {m\u2081,\u2026, m\u0274}, along with successive feature maps F\u2071 from the initial three layers of the 3D image decoder into a Transformer decoder with 3 blocks in a round-robin fashion to process the mask tokens. At each block i \u2208 [1,3], the mask tokens M undergoes a series of layer-wise attention refinements, including cross attention and self-attention as follows:\nM\u207d\u2071\u207ecross = CrossAttn(q\u207d\u2071\u207e, k\u207d\u2071\u207e, v\u207d\u2071\u207e), M\u207d\u2071\u207a\u00b9\u207e = SelfAttn(M\u207d\u2071\u207ecross) (1)\nHere, q\u207d\u2071\u207e, k\u207d\u2071\u207e, and v\u207d\u2071\u207e denote the query matrices of mask tokens and the key and value matrices of image features from the 3D image decoder at the i-th resolution level, respectively. During the cross-attention phase, mask tokens interact with image features, focusing on the specific regional context within the image. In the self-attention phase, mask tokens interact with each other to enhance the understanding of relationships between different anatomical areas. Building on the segment-level embeddings of mask tokens, we establish fine-grained alignment between mask and text representations through a contrastive learning approach. This vision-language alignment in Malenia has three novel key components:\n(1) Utilization of Multi-Scale Features. Existing methods (Jiang et al., 2024; Lin et al., 2024) overlook the advantage of leveraging multi-scale visual features during cross-modal alignment. In contrast, we match the hierarchical mask token embeddings from different Transformer decoder blocks with text features. This approach enables the model to accurately capture and segment across a range of mask sizes. Specifically, the mask tokens interact with image features whose dimensions are set as (h\u2071, w\u2071, D\u2071) = (H/32, W/32, D/32), (H/16, W/16, D/16), (H/8, W/8, D/8) for blcoks i = 1, 2, 3, respectively. This variation in feature resolution across blocks ensures mask-text alignment at different scales, which is crucial for segmenting classes with large size variations, such as tumors.\n(2) Dissecting Reports into Descriptions of Fundamental Disease Attributes. Human experts make diagnoses by carefully analyzing key image features that describe distinctive lesion attributes (e.g., shape and density) across different disease classes (Ganeshan et al., 2018; Nobel et al., 2022). Drawing from this inspiration, we consult medical experts and decompose reports into structured descriptions of 8 shared visual attributes of disease imaging characteristics, including location, shape, density, density variations, surface characteristics, enhancement status, relationship with surrounding organs and specific features. Specifically, we adopt a semi-automatic pipeline to transform patients' reports into structured descriptions. First, we prompt the Large Language Model (LLM) GPT-4 (Achiam et al., 2023) to extract descriptions related to the lesions from the findings section of each report. Then, two experienced radiologists collaborate to review, correct, supplement, and expand the GPT-generated visual descriptions based on the CT images and corresponding original reports. Leveraging disease attribute descriptions offers two key advantages. First, disease attributes provide fine-grained prior knowledge about the visual manifestations of pathologies, inherently improving alignment with target diseases. Second, new diseases can be described using the elemental visual attributes of previously seen diseases, thereby enhancing the model's zero-shot capability.\n(3) Multi-Positive Contrastive Loss. Given the multi-scale lesion-level mask embeddings M and visual attribute descriptions, we construct multiple positive and negative pairs, which are then used to learn to optimize a distance metric that brings the positive pairs closer while pushing the negative pairs away. At the i-th resolution scale, we obtain N binary mask proposals B\u1d39\u207d\u2071\u207e \u2208 [0,1]\u1d34\u2071\u00d7\u1d42\u2071\u00d7\u1d30\u2071\u00d7\u1d3a by a multiplication operation between the mask embeddings and image features followed by a Sigmoid. We then apply bipartite matching between the upsampled binary mask proposals and the ground truth masks, following (Cheng et al., 2022), to select S foreground mask tokens that correspond to the S lesions in the input 3D CT image in a one-to-one manner. Next, we feed the ground truth descriptions of all visual attributes into the text encoder followed by a MLP layer to acquire the text embeddings T = {t\u2081,\u2026,t\u1d63}, where R denotes the number of different attribute descriptions. In our pipeline, we also require a background category for the N \u2013 S background mask tokens that do not have a matched ground truth segment. Therefore, we add an extra learnable embedding, t\u2080 \u2208 \u211d\u1d9c, representing \u201cno lesion found\u201d. Finally, each foreground mask token is paired with its corresponding eight distinct text features, forming multiple positive sample pairs. While each background mask token is paired with t\u2080. For the j-th mask token, let P\u2c7c = {k|(m\u2c7c, t\u2096) is a positive pair} represent the set of all its positive text embedding indices, and let N\u2c7c = {k|(m\u2c7c, t\u2096) is a negative pair} represent the set of all its negative text embedding indices. We calculate the pairwise similarity score S(m\u2c7c, t\u2096) between the j-th mask token m\u2c7c and the k-th text embedding t\u2096 as a dot product, normalized by a temperature parameter \u03c4, given by S(m\u2c7c,t\u2096) =  m\u2c7c\u22c5t\u2096 / \u03c4 . The lesion-level mask-attribute alignment at the i-th resolution level is refined using a contrastive loss function  \u2112\u207d\u2071\u207esim designed to maximize the similarity scores of positive pairs while minimizing those of negative pairs. Since each lesion has eight positive text embeddings of visual attributes, our framework includes multiple positive pairs for each foreground mask token. The commonly used N-pair loss (Sohn, 2016) and InfoNCE loss (Oord et al., 2018), which handle a single positive pair and multiple negative pairs, are not suitable. Therefore, we adopt the Multi-Positive NCE (MP-NCE) loss (Lee et al., 2022) to define the \u2112\u207d\u2071\u207esim as:\n\u2112\u207d\u2071\u207esim = - 1 / N \u03a3\u2c7c\u208c\u2081\u1d3a log [ exp ( S(m\u2c7c, t\u2096)) / (\u03a3\u2096\u2208P\u2c7c exp ( S(m\u2c7c, t\u2096)) + \u03a3\u209c\u2099\u2208N\u2c7c exp (S(m\u2c7c, t\u2099)))]  (2)\nIn this way, we explicitly brings lesion-level mask embeddings closer to their corresponding attribute features while distancing them from unrelated ones. This enables the textual features of each attribute to act as a bridge between the visual features of different diseases, effectively improving the model's zero-shot performance by linking the attributes of unseen lesions with base visual knowledge. Moreover, we apply \u2112\u207d\u2071\u207esim to output mask tokens from each Transformer decoder block, utilizing multi-scale feature maps. The multi-scale lesion-level mask-attribute alignment loss is formulated as \u2112sim = \u03a3\u2071\u208c\u2081\u1d38  \u2112\u207d\u2071\u207esim, where L = 3 denotes the number of Transformer Decoder blocks."}, {"title": "3.2 CROSS-MODAL KNOWLEDGE INJECTION MODULE", "content": "In Malenia, we introduce a novel Cross-Modal Knowledge Injection (CMKI) module to generate the final segmentation predictions. This module enriches the features of one modality by incorporating information from another, enabling deeper understanding and improved feature representations. Specifically, we fuse the output mask tokens m\u1d62, i \u2208 [1, N] from the last Transformer decoder block with their corresponding positive attribute embeddings. As shown in Fig. 1 (a), for each mask token, we first concatenate all its corresponding attribute embeddings t\u2c7c, j \u2208 P\u1d62, and transform them into a single text embedding t\u1d62 using an MLP layer to obtain the textual features for a comprehensive description of the mask token m\u1d62. Then we accomplish deep fusion between two modalities through a series of cross-attention and self-attention layers:\nm\u1d62 = SelfAttn(CrossAttn(q\u2098, k\u209c, v\u209c) + m\u1d62), t\u1d62 = SelfAttn(CrossAttn(q\u209c, k\u2098, v\u2098) + t\u1d62) (3)\nHere, q\u2098, k\u2098, and v\u2098 represent the query, key, and value matrices of the mask tokens, while q\u209c, k\u209c, and v\u209c represent those of the attribute embeddings. The deep fusion of vision and language offers two key benefits: 1) Mask representations are enriched with textual information from language models, resulting in more context-aware segmentation. 2) Text embeddings enhance their descriptive capabilities by attending to visual features, enabling segmentation conditioned on specific text prompts.\nWe leverage both enhanced mask tokens and text embeddings to generate predictions. By capitalizing on fine-grained vision-language alignment, we formulate semantic segmentation as a matching problem between representative multi-modal query embeddings and pixel-level image features. Given the N enhanced mask tokens m\u2208 \u211d\u1d3a\u00d7\u1d9c and text embeddings t \u2208 \u211d\u1d3a\u00d7\u1d9c, as well as the output pixel-level image features F\u2074 \u2208 \u211d\u1d34\u00d7\u1d42\u00d7\u1d30\u00d7\u1d9c from the last 3D image decoder layer, we apply linear projections \u03a6 to generate Q(query) and K(key) embeddings as:\nQ\u2098 = \u03a6\u2098(m) \u2208 \u211d\u1d3a\u00d7\u1d9c, Q\u209c = \u03a6\u209c(t) \u2208 \u211d\u1d3a\u00d7\u1d9c, K = \u03a6\u2096(F\u2074) \u2208 \u211d\u1d34\u00d7\u1d42\u00d7\u1d30\u00d7\u1d9c. (4)\nThen, the mask predictions could be calculated by the scaled dot product attention:\nMask\u2098 = Q\u2098K\u1d40 / \u221aC\u2096 \u2208 \u211d\u1d34\u00d7\u1d42\u00d7\u1d30\u00d7\u1d3a Mask\u209c = Q\u209cK\u1d40 / \u221aC\u2096 \u2208 \u211d\u1d34\u00d7\u1d42\u00d7\u1d30\u00d7\u1d3a (5)\nHere, Mask\u2098 and Mask\u209c refer to the two-branch output mask predictions derived from mask tokens and text embeddings, respectively. \u221aC\u2096 is the dimension of the keys as a scaling factor. We ensemble these mask predictions as Mask = MLP(\u03b2\u2081Mask\u2098 + \u03b2\u2082Mask\u209c). Here \u03b2\u2081 and \u03b2\u2082 are weighting factors for the vision and language branches, respectively, set to 0.5 by default. The final segmentation results are obtained by applying the Argmax operation along the channel dimension of the Mask."}, {"title": "3.3 TRAINING OBJECTIVES AND INFERENCE", "content": "Overall Loss Function: We adopt a composite loss function that balances mask-attribute alignment and mask segmentation. Specifically, we use the dice loss \u2112dice to supervise the segmentation predictions of mask tokens matched with ground truth at each feature level i, achieving deep supervision: \u2112deep = \u03a3\u1d62\u208c\u2081\u1d38 \u2112\u1d62dice Additionally, we have the similarity loss \u2112sim defined by Eq. (2) for aligning mask tokens with attribute embeddings. For the final segmentation results generated by both visual and textual features, we utilize the binary cross-entropy loss and dice loss: \u2112seg = \u03b1\u2081\u2112ce + \u03b1\u2082\u2112dice. We set \u03b1\u2081 = 2.0 and \u03b1\u2082 = 2.0. The overall loss function is a weighted sum of these components:\n\u2112 = \u03bb\u2081\u2112deep + \u03bb\u2082\u2112sim + \u03bb\u2083\u2112seg. (6)\nwhere \u03bb\u2081, \u03bb\u2082, and \u03bb\u2083 are weighting factors balancing the contribution of each loss component to the overall training objective. We set \u03bb\u2081 = \u03bb\u2082 = \u03bb\u2083 = 1.0 as default."}, {"title": "4 EXPERIMENTS", "content": "Datasets. We utilize annotated datasets encompassing lesions from 12 diseases and 6 organs, sourced from both public and private datasets. Specifically, we consider the MSD dataset (Antonelli et al., 2022) and KiTS23 (Heller et al., 2023) dataset. We also collect a private dataset that includes four lesion types: liver cyst, gallbladder tumor, gallstone, and kidney stone. For training, we use the following datasets: 1) colon tumor, lung tumor, liver tumor, and pancreas tumor from the MSD dataset; 2) kidney cyst from the KiTS23 dataset; 3) gallstone from our private dataset. For zero-shot segmentation testing, we adopt: 1) hepatic vessel tumor and pancreas cyst from the MSD dataset; 2) kidney tumor from the KiTS23 dataset; and 3) liver cyst, gallbladder tumor and kidney stone from our private dataset. Details of all datasets, the annotation process for disease attribute descriptions, and preprocessing are provided in Appendix B.\nEvaluation Metrics. We adopt standard segmentation metrics, including the Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD). Additionally, we report the the computational efficiency evaluation, which are detailed in Appendix E.\nImplementation Details. We use nnUNet (Isensee et al., 2021) as the backbone for 3D image encoder and 3D image decoder. We choose Clinical-Bert (Alsentzer et al., 2019) as the text encoder. We employ AdamW optimizer (Loshchilov & Hutter, 2017) with a warm-up cosine scheduler of 40 epochs. The batch size is set to 2 per GPU. Each input volume is cropped into patches with a size of 96 \u00d7 96 \u00d7 96. The training process uses an initial learning rate of 1e-4, momentum of 0.9, and weight decay of 1e-5, running on 8 NVIDIA A100 GPUs with DDP for 4000 epochs. The number of the mask tokens N is set as 16."}, {"title": "4.2 MAIN RESULTS", "content": "Zero-shot segmentation performance on unseen lesions. Table 1 presents the zero-shot lesion segmentation results on unseen datasets from various institutions, encompassing a wide range of lesion types. All available CT volumes in these tasks are directly used for testing. We compare Malenia with five state-of-the-art zero-shot medical image segmentation methods: the self-prompted ZePT (Jiang et al., 2024), the prompt-free SAM-based method H-SAM (Cheng et al., 2024), the fine-tuning-free method combining CLIP and SAM, SaLIP (Aleem et al., 2024), and two methods (Shaharabany & Wolf, 2024; Yamagishi et al., 2024) that respectively adopt SAM and SAM 2. Malenia achieves superior performance across all the datasets, substantially outperforming the other five methods. The weak performance of SAM (Shaharabany & Wolf, 2024) and SAM 2 (Yamagishi et al., 2024) can be attributed to the inherent variability of segmentation tasks in different clinical scenarios. Without target data for fine-tuning and accurate manual prompts, the SAM and SAM 2 struggle with lesion segmentation in the zero-shot setting. Additionally, creating accurate prompts requires domain knowledge from medical experts, which is often limited and time-consuming. SaLIP leverages the capabilities of CLIP to refine automatically generated prompts for SAM. However, its performance is still constrained by CLIP, which focuses on aligning image-level global features while neglecting fine-grained local lesion semantics, resulting in suboptimal performance for lesion segmentation. H-SAM achieves better results by incorporating enhanced mask attention mechanisms in a two-stage decoding process. However, it does not explore cross-modal feature representations. Without the rich textual knowledge from language models, its zero-shot segmentation performance is significantly hindered. ZePT is trained using both organ and lesion labels to generate anomaly score maps as prompts, and it aligns mask features with common medical knowledge to enhance zero-shot tumor segmentation. However, it overlooks the patient-specific multi-aspect elemental attributes shared across different diseases, which limits the scalability and generalization of its learned representations. In contrast, Malenia learns fine-grained lesion-level mask-attribute alignment to link unseen lesions with base knowledge and leverages both visual and textual context for advanced cross-modal understanding, resulting in at least a 6.40% improvement in DSC on MSD, a 8.14% improvement in DSC on KiTS23, and a 9.08% improvement in DSC on the in-house dataset.\nSegmentation performance on seen lesions. We also evaluate the segmentation performance of Malenia on seen lesions in the fully-supervised setting. We compare Malenia with SOTA medical image segmentation methods, including TransUNet (Chen et al., 2021), nnUNet (Isensee et al., 2021), Swin UNETR (Tang et al., 2022), and the Universal Model (Liu et al., 2023). As shown in Table 2, Malenia outperforms competing baselines on seen lesions, achieving an average improvement of 1.92% in DSC for tumors on the MSD dataset (Antonelli et al., 2022) and 1.35% in DSC for kidney cysts on KiTS23 (Heller et al., 2023). Most segmentation baselines (e.g., TransUNet, nnUNet, and Swin UNETR) focus solely on visual features and overlook the semantic relationships between different lesions. Although the Universal Model incorporates text embeddings of category names to help learn correlations between anatomical regions, it still lacks fine-grained information from both the vision and language domains. In contrast, Malenia outperforms these methods by aligning fine-grained lesion-level semantics with comprehensive, patient-specific textual features and enhancing mask representations through cross-modal knowledge injection. This improvement demonstrates that our novel strategies also enhance the segmentation of seen lesion categories.\nQualitative Comparison. As shown in Fig. 2, Malenia accurately segments various types of lesions across diverse organs, having substantially better performance in segmenting both seen and unseen lesions than the other methods. Most competing methods suffer from incomplete segmentation and false positives. In contrast, Malenia produces results that are more consistent with the ground truth. This further demonstrates the superior lesion segmentation capability of Malenia on datasets with highly diverse lesion semantics."}, {"title": "4.3 ABLATION STUDIES", "content": "Multi-scale mask-attribute alignment strategy ablation. We validate the effectiveness of different components of the multi-scale mask-attribute alignment strategy on both seen and unseen lesion datasets, as detailed in Table 3. 'Baseline' refers to the naive single-scale mask-report alignment performed at the last Transformer decoder block. We gradually enhance the baseline by (S\u2081) enriching raw reports with structured eight visual attribute descriptions of the lesions; (S\u2082) utilizing multi-scale features for cross-modal alignment; (S\u2083) formulating mask-attribute alignment as multi-positive contrastive learning using multiple mask-attribute positive pairs, rather than a single mask-report positive pair. Each component contributes to the remarkable segmentation performance of Malenia. Enriching raw reports with visual attributes of lesions (S\u2081) helps the model leverage pre-established knowledge of the diseases' visual manifestations to enhance the alignment of fine-grained image features with the representations of target diseases, thereby improving segmentation performance. Multi-scale cross-modal alignment (S\u2082) leverages multi-level features to accurately capture and segment both seen and unseen lesions across various sizes, which is essential for handling the shape and size variations of lesions. Furthermore, instead of combining the eight visual attribute descriptions of each lesion into a single paragraph and then extracting the text features (S\u2081), we extract the text features for each visual attribute description separately (S\u2083). As a result, the mask embeddings for each lesion is paired with eight distinct text features, forming multiple positive sample pairs. Simply treating a lesion's attribute descriptions as a single paragraph yields only one positive sample pair for each foreground mask token. Consequently, reports of other lesions that share some of the same attribute descriptions are treated as negative samples, leading to compromised feature representations. In contrast, our formulation of the multi-positive contrastive learning (S\u2083) focuses on establishing comprehensive and extensible correlations between pathological features and fundamental disease attributes. This enables the model to associate the visual cues of unseen lesions with foundational visual knowledge, allowing for effective segmentation of new diseases by translating their complex visual features into elemental attributes shared with seen diseases. This significantly enhances zero-shot lesion segmentation performance.\nAblation of the Cross-Modal Knowledge Injection module. We examine key components in the proposed Cross-Modal Knowledge Injection module in detail, including (1) the significance of deep fusion (DF), which is designed to enhance the representation of mask tokens and text embeddings; (2) the effectiveness of leveraging both text embeddings (TE) and mask tokens (MT) to generate predictions. The results are shown in Table 4. Comparing the results of the first three rows with the last three rows (highlighted in light red), it is evident that deep fusion significantly improves performance, whether using only text embeddings, only mask tokens, or both for segmentation result prediction. This observation shows the importance of enabling cross-modal information interaction for optimal performance. Furthermore, whether or not deep fusion is applied, using both text embeddings and mask tokens for segmentation prediction, combined with ensembling the results, consistently outperforms using only unimodal token embeddings for segmentation prediction. This confirms the superiority of leveraging the complementary strengths of both visual and textual embeddings to further enhance segmentation performance. Fig. 3 provides detailed examples illustrating how combining text tokens and mask tokens improves performance. While using mask tokens for segmentation prediction is more intuitive and generally yields better performance than using text embeddings alone (as shown in Table 4), we observed that when lesion boundaries are particularly blurry (as seen in the Hepatic Vessel Tumor cases on the left side of Fig. 3), the visual features from the mask tokens fail to accurately capture the lesion's area. In such instances, text descriptions of attributes such as the lesion's shape and surface characteristics-provide additional information. The segmentation generated by the corresponding text embeddings refines the results produced by mask tokens alone, leading to improved performance. Moreover, we observed that, owing to the clear semantic information and specific contexts carried by the textual features of attribute descriptions, the segmentation results generated using attribute embeddings exhibit significantly fewer false positives. This further refines the segmentation produced by mask tokens by reducing false positives, as demonstrated in the colon tumor cases on the right side of Fig. 3."}, {"title": "5 CONCLUSION", "content": "In this work, we propose Malenia, a novel vision-language pre-training method designed for 3D zero-shot lesion segmentation, which incorporates an innovative multi-scale lesion-level vision-language alignment strategy. Inspired by the image interpretation process of human experts, we transform patient-specific reports into structured descriptions of disease visual attributes and then match them with mask representations. Additionally, we introduce a novel Cross-Modal Knowledge Injection module, which enhances cross-modal representations and leverages the complementary strengths of both visual and textual features to further improve segmentation performance. Extensive experiments demonstrate that Malenia consistently outperforms previous state-of-the-art approaches across diverse datasets for 3D zero-shot lesion segmentation. We hope this work inspires further innovation in this challenging yet promising research area."}, {"title": "F LIMITATIONS AND FUTURE WORK", "content": "Malenia currently achieves superior zero-shot lesion segmentation performance, approaching that of fully-supervised, task-specific models. However, zero-shot lesion segmentation across anatomical regions and imaging modalities remains highly challenging. For instance, training on lesion data from abdominal CT and directly testing on brain cancer MRI is extremely challenging due to the significant differences in anatomical structures and feature distribution between the training and testing images. We believe that achieving cross-domain and cross-modality zero-shot generalization is a challenging yet meaningful research direction in both natural and medical imaging domains. This will be the focus of our future work."}]}