{"title": "EzSQL: An SQL intermediate representation for improving\nSQL-to-text Generation", "authors": ["Meher Bhardwaj", "Hrishikesh Ethari", "Dennis Singh Moirangthem"], "abstract": "The SQL-to-text generation task traditionally uses template base, Seq2Seq, tree-to-\nsequence, and graph-to-sequence models. Recent models take advantage of pre-trained\ngenerative language models for this task in the Seq2Seq framework. However, treating\nSQL as a sequence of inputs to the pre-trained models is not optimal. In this work,\nwe put forward a new SQL intermediate representation called EzSQL to align SQL\nwith the natural language text sequence. EzSQL simplifies the SQL queries and brings\nthem closer to natural language text by modifying operators and keywords, which can\nusually be described in natural language. EzSQL also removes the need for set operators.\nOur proposed SQL-to-text generation model uses EzSQL as the input to a pre-trained\ngenerative language model for generating the text descriptions. We demonstrate that\nour model is an effective state-of-the-art method to generate text narrations from SQL\nqueries on the WikiSQL and Spider datasets. We also show that by generating pretraining\ndata using our SQL-to-text generation model, we can enhance the performance of Text-\nto-SQL parsers.", "sections": [{"title": "1. Introduction", "content": "An SQL-to-text model automatically generates human-like text descriptions by\ninterpreting the meaning signified by the structured query language (SQL) query. This\ntask has been crucial for the natural language to interact with a database as it helps users\nwho are unfamiliar with SQL to understand the queries. A newly found application\nof this task is training data generation for enhancing the performance of Text-to-SQL\nparsers Shi et al. (2021); Wang et al. (2021). Earlier attempts were made for the SQL-\nto-text task in the form of rule-based and template-based Koutrika et al. (2010); Ngonga\nNgomo et al. (2013) models. Further, newer approaches use sequence-to-sequence\n(Seq2Seq) networks to model the SQL queries and natural language in an interconnected\nmanner Iyer et al. (2016). Furthermore, several models were also developed based on\ngraph encoding techniques Kipf & Welling (2016); Song et al. (2018), since SQL is\ndesigned to express the query intent in a graph-structured manner.\nRecently, self-supervised deep contextual language models Devlin et al. (2019);\nLiu et al. (2019); Lewis et al. (2020) have shown their effective modeling ability\nfor text, demonstrating state-of-the-art results in a series of NLP tasks. These tasks\ninclude sequence understanding and language generation. Previously, Shi et al. (2021);\nWang et al. (2021) have explored the use of BART Lewis et al. (2020) in SQL-to-Text\ngeneration. To generate the text utterances, the authors train their SQL-to-Text model\nwhere the input is the original SQL. The input SQL is directly taken as a sequence by\nthe BART tokenizer since the pre-trained model expects a sequence of text as input.\nHowever, directly feeding SQL as a sequence is not optimal.\nIn this paper, we present EzSQL, a new intermediate representation of queries\nfocusing on the SQL-to-text task. EzSQL bridges the gap between SQL and natural\nlanguage (NL). In the\nexample, EzSQL uses the OR conjunctive operator instead of the UNION operator,\nwhich also removes the multiple SELECT clauses, and aligns better with the natural\nlanguage. These simplification steps in EzSQL make it more suitable as an input to a\npre-trained language model for the generation of the text descriptions. We propose an\nSQL-to-Text generator that leverages the capability of a pre-trained language model"}, {"title": "3. Proposed Approach", "content": "We describe our proposed approach in this section."}, {"title": "3.1. Overview", "content": "Intermediate representation (IR) is widely employed by the Text-to-SQL models to\nsynthesize SQL queries with more complex structures Wang et al. (2020); Guo et al.\n(2019a); Yu et al. (2018a); Shi et al. (2021); Gan et al. (2021). However, such IRs are\neither too complicated or have limited coverage of SQL structures due to the restriction\nthat IRs should be converted back to executable SQLs.\nOur EzSQL IR focuses on SQL-to-text and hence has no limitations on the simplifi-\ncation process of SQL. The main goal of EzSQL is to simplify the structure of SQL\nand bring it as close as possible to natural language while maintaining the context and\nsemantic meaning. Considering the example in Figure 2, the set operator \u2018INTERSECT',\nused to combine SELECT statements, is never mentioned in the question. INTERSECT\nis a keyword in SQL that allows the user to seek a combination of the results of multiple\nfunctions. Such implementation details are not needed for the BART model to generate"}, {"title": "3.2. EzSQL simplification procedure", "content": "The simplification process in EzSQL can comprise of several steps depending on\nthe type and complexity of the query. EzSQL takes several inspirations from existing IR\nused for Text-to-SQL. The IR in Yu et al. (2018a) represents SQL statements without\nFROM and JOIN ON clauses. Guo et al. (2019a) removes the FROM, JOIN ON and\nGROUP BY clauses, and combines the WHERE and HAVING conditions. The recent IR\nin Gan et al. (2021) dispenses nested subqueries and set operators. By taking advantage\nof the existing IRs and combining the generation processes, we implement the EzSQL IR\nfor SQL-to-text with the single goal of representing the SQL statement closest to natural\nlanguage text. Since our representation does not have any limitations and restrictions\nfor simplification, the implementation is much simpler in the one-way process. The\nsimplification process is represented in figure 5.\nHandling set operators. EzSQL simply concatenates the conditions in a set operator\nwhere the conditions can be connected, as shown in Figure 1. However, if the two"}, {"title": "3.3. SQL-to-text Generation Model", "content": "Our SQL-to-text generation approach takes an SQL query as the input and outputs a\nnatural language question. Our approach is based on pre-trained sequence-to-sequence\nBART model Lewis et al. (2020). The model consists of an SQL simplifier module to\ngenerate EzSQL and a BART encoder-decoder module. The SQL simplifier takes SQL\nas an input and produces the EzSQL IR. The EzSQL IR is then taken by the BART\nmodel to generate text descriptions.\nAs an IR, the EzSQL need not be a complete text narration and can still contain SQL\nterms. This flexibility makes the system more robust. We also rely on the text-infilling"}, {"title": "4. Experiments", "content": "We first evaluate the performance of our proposed SQL-to-text model on the avail-\nable benchmarks. We also show the effectiveness of our proposed approach in training\ndata generation for the Text-to-SQL task."}, {"title": "4.1. SQL-to-text", "content": "We evaluate our model on two datasets, WikiSQL Zhong et al. (2017) and Spider Yu\net al. (2018c). WikiSQL consists of a corpus of 87,726 hand-annotated SQL queries\nand natural language question pairs. These SQL queries are further split into training\n(61,297 examples), development (9,145 examples), and test sets (17,284 examples).\nSpider consists of 7000, 1034, and 2147 samples for training, development, and testing\nrespectively, where 206 databases are split into 146 for training, 20 for development, and\n40 for testing. We use the BART-large variant in our model. The BLEU-4 score Papineni\net al. (2002) is used as the automatic evaluation metric for this task."}, {"title": "4.2. Text-to-SQL", "content": "We further experiment on the effectiveness of our model in pretraining data genera-\ntion for Text-to-SQL parsers. Following Wang et al. (2021), we use the database-specific\nprobabilistic context-free grammar (PCFG) model to sample new SQL queries from the\nSpider dataset. In this method, ASDL Wang et al. (1997) grammar of SQL is represented"}, {"title": "5. Conclusion and Future Work", "content": "We proposed a new SQL intermediate representation aimed to improve the SQL-\nto-text task that is simple yet effective for enhancing translation performance using\npre-trained language models. The proposed SQL-to-text model was able to outperform\nthe baselines on both WikiSQL and Spider datasets. Moreover, we successfully show\nthat the proposed model can be used to synthesize pre-training data to further enhance\nthe performance of Text-to-SQL semantic parsers. In the future, we also intend on\nadding natural keywords to EzSQL, which relay the semantic sense more clearly, that\ncurrent LLMs tend to struggle with. For example, when given the SQL -"}]}