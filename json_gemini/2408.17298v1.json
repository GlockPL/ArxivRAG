{"title": "Accelerating the discovery of steady-states of planetary interior dynamics with machine learning", "authors": ["Siddhant Agarwal", "Nicola Tosi", "Christian H\u00fcttig", "David S. Greenberg", "Ali Can Bekar"], "abstract": "Simulating mantle convection often requires reaching a computationally expensive steady-state, crucial for deriving scaling laws for thermal and dynamical flow properties and benchmarking numerical solutions. The strong temperature dependence of the rhe- ology of mantle rocks causes viscosity variations of several orders of magnitude, leading to a slow-evolving \"stagnant lid\" where heat conduction dominates, overlying a rapidly- evolving and strongly convecting region. Time-stepping methods, while effective for flu- ids with constant viscosity, are hindered by the Courant criterion, which restricts the time step based on the system's maximum velocity and grid size. Consequently, achieving steady- state requires a large number of time steps due to the disparate time scales governing the stagnant and convecting regions.\nWe present a concept for accelerating mantle convection simulations using machine learning. We generate a dataset of 128 two-dimensional simulations with mixed basal and internal heating, and pressure- and temperature-dependent viscosity. We train a feed- forward neural network on 97 simulations to predict steady-state temperature profiles. These can then be used to initialize numerical time stepping methods for different sim- ulation parameters. Compared to typical initializations, the number of time steps required to reach steady-state is reduced by a median factor of 3.75. The benefit of this method lies in requiring very few simulations to train on, providing a solution with no predic- tion error as we initialize a numerical method, and posing minimal computational over- head at inference time. We demonstrate the effectiveness of our approach and discuss the potential implications for accelerated simulations for advancing mantle convection research.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Challenges in achieving steady-state in mantle convection simula- tions", "content": "Simulating natural convection phenomena, particularly in mantle dynamics, often necessitates achieving a steady-state, a condition where parameters of interest either re- main constant or oscillate around a mean value. This equilibrium phase is essential for deriving scaling laws for convective heat transfer (e.g., Christensen, 1984; Solomatov, 1995; Grasset & Parmentier, 1998; Korenaga, 2010; Vilella & Deschamps, 2018; Ferrick & Ko- renaga, 2023, to name just a few studies spread over three decades of research), validat- ing numerical codes (e.g., Blankenbach et al., 1989; King et al., 2010; Tosi et al., 2015), studying chaotic mixing processes (e.g., Farnetani & Samuel, 2003; Coltice & Schmalzl, 2006; Samuel & Tosi, 2012; van Keken et al., 2014), and determining characteristic flow wavelengths (e.g., Grign\u00e9 et al., 2005; H\u00f6ink & Lenardic, 2010; Phillips & Coltice, 2010)."}, {"title": "1.2 Machine learning in fluid dynamics", "content": "The high computational cost of running fluid dynamics simulations has made them a prime area of research in scientific machine learning (e.g., Brunton et al., 2020; Lino et al., 2023). This is not only true for surrogate modeling strategies where trained ma- chine learning models serve to approximate the forward problem, but also for hybrid strate- gies, where machine learning is used to accelerate partial differential equations solvers without compromising their numerical accuracy (e.g., Tompson et al., 2017; Kochkov et al., 2021; Illarramendi et al., 2020).\nLikewise, machine learning remains a promising avenue for accelerating discovery in geo- and planetary physics . Recent successes include modeling of seismic wave prop- agation (Xiong et al., 2022), seismic fault segmentation (Wu et al., 2019), wave inver- sion (Rasht-Behesht et al., 2022), predictions of pairwise planetary collisions from Smooth Particle Hydrodynamics simulations (Winter et al., 2022), trace gases prediction (Azmi et al., 2023), weather prediction (e.g., Lam et al., 2023; Kurth et al., 2023), air pollu- tion forecasting based on a foundation model (Bodnar et al., 2024), probabilistic char- acterization of the interior structure of planets (Baumeister & Tosi, 2023), prediction of surface heat flow from seismic structure for modeling ice sheet dynamics (Zhang & Ritz- woller, 2024), pressure and temperature predictions for mineral combinations in the Earth's lithosphere based on experiments (Qin et al., 2024), and gravitational mass and moments modeling of Jupiter (Ziv et al., 2024). The recent publication of a few review articles on data-driven methods in geophysics and geodynamics (e.g., Bergen et al., 2019; Morra et al., 2020; Yu & Ma, 2021) underscores the field's increasing significance.\nCloser yet to the topic of convection simulations, Shahnas & Pysklywec (2020) used a feedforward neural network (NN) to predict the surface heat flux and mean temper- ature of steady-state simulations as a function of parameters such as the Rayleigh num- ber (Ra), the core-to-planet radius ratio, and the presence of melting. Agarwal et al. (2021) demonstrated that two-dimensional surrogate models of thermal evolution of planetary interiors could be modeled using autoencoders for dimensionality reduction and then by using Long Short-Term Memory (LSTM) networks for advancing the compressed rep- resentations of this flow in time. This was a follow-up study to Agarwal et al. (2020), where an NN was used to predict the horizontally-averaged temperature profiles from thermal evolution simulations. Akbari et al. (2023) leveraged Proper Orthogonal Decom- position and LSTM networks to build reduced-order models for data assimilation in Rayleigh-B\u00e9nard convection. Ali Boroumand et al. (2024) used deep convolutional networks to estimate the Prandtl number and Ra from two-dimensional snapshots of convection simulations."}, {"title": "1.3 Using neural networks to predict statistical steady-state", "content": "In this paper, we exploit the findings of Agarwal et al. (2020) that an NN can pre- dict horizontally-averaged temperature profiles as a function of simulation parameters. Here, we focus on predicting temperature profiles of statistically steady-state mantle con- vection simulations instead of thermal evolution simulations. We demonstrate that these predicted profiles serve as excellent initial conditions for a traditional numerical solver. By using the profiles predicted by our NN, we can determine the statistical steady-state of a given system in a fraction of the time than would be needed when starting from a typical initial condition such as an arbitrary high or low temperature, or a conductive profile.\nWhile the predictions of the temperature profile from the NN are expected to be relatively accurate, feeding these to numerical solvers allows obtaining other valuable in- formation on the system, namely, (1) two-dimensional temperature, velocity, and pres- sure fields, and thereby the characteristic flow pattern and wavelength, as well as (2) numerically- accurate heat fluxes at the top and bottom of the domain, which can then be used to derive suitable scaling laws for convective heat transfer (see Sec. 3.3). Hence, we pro- pose a hybrid modeling approach, where NNs serve as an accelerator for our specific prob- lem of predicting the statistical steady-state of thermal convection simulations. To the best our knowledge, this is the first time such an approach has been proposed in the con- text of mantle convection. The data used in this study as well as a trained NN are made available on HuggingFace. The trained model can be run to generate and download tem- perature profiles: https://huggingface.co/spaces/agsiddhant/steadystate-mantle. No installation is needed.\nThe outline of this paper is as follows. We first explain the methods used, includ- ing the mantle convection equations solved and the numerical setup of the simulations (Sec. 2.1). We provide details on the simulation dataset (Sec. 2.2), on the neural net- work used (Sec. 2.3) as well as on the other regression methods used as baselines (Sec. 2.4) and on the calculation of the stagnant lid thickness and of some basic scaling laws (Sec. 2.5). In Sec. 3.1, we examine the accuracy of the NN predictions and compare it to the other regression baselines. In Sec. 3.2 we compare the number of iterations needed to reach a statistical steady-state for three qualitatively different simulations that are started from different initial conditions. In Sec. 3.3, we derive some basic scaling laws using simulations accelerated by NN predictions as an example application. We then con- clude the paper discussing the significance of this approach and possible improvements (Sec. 4) and summarizing our main findings (Sec. 5)."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Convection model", "content": "We solve the conservation equations of mass, linear momentum and thermal en- ergy for an incompressible fluid with variable-viscosity and negligible inertia heated from"}, {"title": "2.2 Parameters, dataset, and output quantities", "content": "We generate a dataset of 128 simulations of steady or statistically-steady convec- tion depending on the three parameters \u03b3, \u03b2 and Q. We vary \u03b3 between 10^6 and 10^{10}, \u03b2 between 1 and 100, and Q between 1 and 10. This choice results in effective Rayleigh numbers ranging from 10^4 (for \u03b3 = 10^6 and \u03b2 = 100) to 10^{10} (for \u03b3 = 10^{10} and \u03b2 = 1).\nDue to choice of rectangular geometry, the combination of high internal heating, depth dependence of the viscosity and fixed-temperature bottom boundary conditions can lead to internal temperatures exceeding the bottom temperature, which is set to the non-dimensional value of 1 (Fig. 1, last column). Indeed, plane-layer convection mod- els may need to introduce artificial internal cooling in order to reproduce temperatures obtained in a spherical shell geometry (O'Farrell & Lowman, 2010; O'Farrell et al., 2013). Also, in a more realistic mantle convection setting, the bottom temperature (i.e. the tem- perature of the core) would quickly adapt according to the local heat flux, at least as long as the core is liquid and can be approximated as a convecting and homogeneous body of given density and heat capacity (e.g. Stevenson et al., 1983). In this situation, the core would tend to heat up and the heat flux out of it to vanish as in a purely internally-heated system. Yet, for the purpose of this work, these cases provide useful tests to probe the ability of our ML predictions to reproduce non-trivial temperature profiles."}, {"title": "2.3 Neural network model", "content": "We use a typical feedforward neural network, such as the one used in Agarwal et al. (2020), but modify it for our purposes:\n\u2022 Unlike the NN in Agarwal et al. (2020), the network output is a scalar prediction of temperature at a given height (T(y)). Such coordinate-based networks are heav- ily used for more memory-efficient learning on unstructured representations such as point clouds and meshes (e.g., Park et al., 2019; Hines Chaves et al., 2023). This also helps avoid wiggles in the prediction, which we see when the profile is high- dimensional (more than \u2248 60 points, as empirically observed in Agarwal et al. (2020)). This also allows the end-user to directly train and evaluate on arbitrary depth profiles that are no longer tied to the underlying discretization.\n\u2022 We train on mini-batches of only 32 points. Therefore, we find it beneficial to sam- ple comparatively more points in the upper (100 times between from 15th point from top) and lower (50 times from the 10th point from bottom) thermal bound- ary layers of the temperature profile as these would otherwise be underrepresented in a batch as most of the mantle tends to be isothermal due to convective heat transfer.\n\u2022 Skip connections are introduced from each hidden layer to every following hidden layer by addition before applying the activation function SELU().\n\u2022 We inject the input to the NN into the last hidden layer of the network by con- catenation, as it has shown to be useful for improving accuracy (e.g., Park et al., 2019).\n\u2022 Finally, we apply a correction at the boundaries of the domain. As NNs are ap- proximators optimized by an iterative process, their predictions naturally tend to be inexact and have a hard time satisfying boundary conditions. In other words, the boundary conditions tend to be no more or less accurate than any other point in the domain. In the future, it could be worth exploring some non-trivial approaches for exact enforcing of the boundary conditions (e.g., Wang et al., 2023; Sukumar & Srivastava, 2022), but here we simply overwrite the NN predictions at the top with 0 and at the bottom with 1. However, this also means that the gradient of the boundary layer can become inconsistent, especially if the grid resolution is changed. To ensure that at least the heat fluxes are consistent with respect to the discretiza- tion, we overwrite the points within a certain distance of the top (0.04) and bot- tom (0.985) with the same gradient between the first and last point.\nWe train the NNs in PyTorch (Ansel et al., 2024) using an Adam optimizer (Kingma & Ba, 2014). To keep over-fitting in check, especially for such a small dataset, we use a batch size of 32, a weight decay value of 0.0005 with the Adam optimizer and save the weights of the NN after an epoch only if the loss on the cv set has decreased. We train for 140 epochs and decrease the initial learning rate of 0.001 by a factor of 0.5 every 20 epochs. On a Tesla V100 GPU, training takes up to a couple of minutes. In contrast to Agarwal et al. (2020), the combination of small mini-batches, SELU() activation func- tion (Klambauer et al., 2017) instead of tanh(), pointwise formulation (using y as an in- put) and a small number of simulation profiles (128) compared to 10,000 simulations \u00d7100 time-steps makes for a significantly faster training time. We tested NN architectures by"}, {"title": "2.4 Regression baselines", "content": "For a fair comparison, we include the following baseline algorithms for predicting the temperature profile using scikit-learn (Pedregosa et al., 2011):\n\u2022 Linear regression: we test linear regression to predict the temperature profiles as a function of the normalized simulation parameters.\n\u2022 Kernel ridge regression: it is a standard workhorse for regression problems and has the advantage of being able to approximate non-linear functions while being ro- bust to outliers due to regularization. We use a radial basis function kernel with \u03b1 = 0.1, which controls the regularization strength, and leave the default value of the multiplier of the radial basis function.\n\u2022 Nearest neighbor: as a simple baseline, we checked how far off the temperature profile would be for a set of parameters if we simply picked the profile at the near- est neighbor in terms of the simulation parameters in our dataset. The nearest neighbor was determined using KDTree, which uses the Minkowski distance in an n-dimensional parameter space.\n\u2022 Nearest neighbor interpolation: as a slightly more advanced version of nearest neigh- bor, we also baseline by taking (1) the temperature profiles of the three nearest neighbors in terms of simulation parameters and (2) by using inverse distance weight- ing to sum the three profiles resulting in an interpolated result. If the set of pa- rameters being evaluated matches a set of parameters in the \"training\" dataset, we do not perform the averaging and simply take the exact output from the dataset."}, {"title": "2.5 Derivation of basic scaling laws for convection", "content": "As an example of downstream applications of the kind of simulations we aim to ac- celerate in this study, we derive some basic scaling laws for mixed heating with temperature- and pressure-dependent viscosity. There are three main steps for deriving the scaling laws.\nFirst, we determine the thickness of the stagnant lid using the \"tangent method\" (e.g., Reese et al., 1999) as shown in Fig. 3. The point at which the tangent from the point of maximum gradient of the horizontally-averaged root mean squared (RMS) ve- locity profile intersects the y-axis, i.e. where RMS velocity equals zero, is taken as the height of the lid.\nSecond, the difference between the temperature at the base of this lid and the max- imum temperature is used to calculate an effective Rayleigh number (Ra_{eff}):\nRa_{eff}=Ra\\frac{\\gamma_{lid} (T_{max} \u2013 T_{lid})}{\\eta_{eff}},\nwhere, Ra is the reference Rayleigh number defined in eq. (4), \u03b3lid is the height of the lid, Tmax is the maximum temperature of the profile, Tiid is the temperature at the base of the lid, and \u03b7eff is calculated as the harmonic mean of the viscosity below the lid.\nThird, the simulations are used to derive a Nusselt-Rayleigh scaling. As this is not the primary goal of this study, we avoid a comprehensive survey of and comparison to the vast literature on scaling laws and instead use a basic Nusselt-Rayleigh scaling of the form proposed by Moore (2008) that considers an additive term for internal heat pro- duction:\nNu = a Ra_{eff}^b + c Q,"}, {"title": "3 Results and Discussion", "content": ""}, {"title": "3.1 Neural network predictions vs. regression baselines", "content": "In this section, we present the results of predictions from an NN (see Sec. 2.3 for details) with 5 hidden layers containing 128 units each and compare these to the pre- dictions of the regression baselines outlined in Sec. 2.4. Table 1 shows that the NN achieves the lowest mean absolute error (MAE) of all the algorithms on the cv and test sets. Of course, in the case of nearest neighbor and nearest neighbors interpolation, the predic- tions on the training set are redundant and the MAE is exactly zero. The NN seems to generalize not only for parameters within the bounds of the training dataset (cv), but also generalizes better than the other algorithms when extrapolating slightly (test)."}, {"title": "3.2 Accelerating convergence of numerical simulations", "content": "We demonstrate the impact of different initial conditions on the time it takes to reach a statistical steady-state. For this, we consider three qualitatively different sim- ulations by varying the three parameters Q, \u03b2 and \u03b3 as follows:\n\u2022 Case 1 (weak convection): Q = 5, \u03b3 = 10^8, \u03b2 = 50\n\u2022 Case 2 (medium convection): Q = 7.5, \u03b3 = 10^9, \u03b2 = 25"}, {"title": "3.3 Scaling laws for convection with variable viscosity and internal heat- ing", "content": "As an example of an application of simulations accelerated by NN predictions, we derive some basic scaling laws for three cases, where we vary the maximum viscosity con- trast due to temperature (\u03b3), and thereby the effective Rayleigh number:\n1. no internal heating and no pressure dependence of viscosity: Q = 0 and \u03b2 = 1\n2. internal heating and no pressure dependence of viscosity: Q = 2 and \u03b2 = 1"}, {"title": "4 Discussion", "content": "While sophisticated machine learning methods have demonstrated remarkable ca- pabilities in approximating highly non-linear forward maps, they often tend to be data- intensive. This is also true for mantle convection, where surrogate models must be a func- tion of simulation parameters and thereby learn features on vastly different spatio-temporal scales. For example, Agarwal et al. (2021) were able to learn the two-dimensional evo- lution of temperature fields, but they needed more than 10,000 simulations and did not predict other system variables such as flow velocities. The predictions also had errors with respect to the numerical simulations and the two-step learning procedure (dimen- sionality reduction with autoencoders and time-stepping with LSTMs) took almost a week to train on a single GPU.\nThis study provides an alternative approach where only 97 simulations were used to learn the profiles with a training time of approximately 2 minutes. No High Perfor-"}, {"title": "5 Conclusions", "content": "We presented a method to accelerate the computationally-intensive determination of the statistical steady state of mantle convection simulations. Using a neural network (NN), one can learn the final horizontally-averaged temperature profile of the system. For unseen simulations, this prediction serves as an efficient initial condition for the solver which attains a statistical steady-state significantly faster than with other typically used initial conditions. While the NN prediction is already quite accurate in most cases, run- ning the solver enables one to obtain higher order statistics of the system such as the"}, {"title": "6 Author Contributions", "content": "We list the author contributions following the CRediT taxonomy.\nConceptualization: CH, SA, NT, DG, AB;\nMethodology: SA, CH, NT, DG, AB ;\nSoftware: SA, CH;\nValidation: SA, CH, NT;\nFormal analysis: SA, CH;\nInvestigation: SA, CH, NT ;\nResources: NT, CH;\nData curation: NT, CH, SA;\nWriting-Original Draft: SA, NT, CH, AB ;\nWriting-Review & Editing: DG, AB, CH, NT, SA ;\nVisualization: SA, NT;\nSupervision: NT, DG;\nProject administration: NT, DG, AB;\nFunding acquisition: NT, DG, SA;"}, {"title": "Open Research Section", "content": "The trained model can be run to generate and download temperature profiles: https:// huggingface.co/spaces/agsiddhant/steadystate-mantle. No installation is needed. The repository also contains the raw dataset used to train, validate and test the machine"}]}