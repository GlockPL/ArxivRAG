{"title": "OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews", "authors": ["Maximilian Idahl", "Zahra Ahmadi"], "abstract": "We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and Al conference papers. At its core is Llama-OpenReviewer-8B\u00b9, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top ML conferences. Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines. Our evaluation on 400 test papers shows that OpenReviewer produces significantly more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings. The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review. OpenReviewer is available as an online demo\u00b2 and open-source tool.", "sections": [{"title": "Introduction", "content": "The peer review process is fundamental to maintaining scientific rigor in academic research, particularly in fast-moving fields like machine learning (ML) and artificial intelligence (AI). As submission volumes to major conferences continue to surge with top venues receiving over 10,000 submissions annually \u2013 the traditional peer review system faces challenges. The task load for reviewers consistently increases, while authors lack access to preliminary expert feedback with a quick turnaround that could help improve their work before submission.\nLarge language models (LLMs) have recently demonstrated remarkable capabilities in under-"}, {"title": "Motivation", "content": "The motivation behind OpenReviewer is not to replace human peer reviews. Instead, we want to assist authors who face challenges in the pre-submission phase. Without access to expert feedback before submission, they may overlook significant weaknesses in their manuscripts or fail to address potential reviewer concerns. This can result in unnecessary desk rejections or negative reviews that could have been avoided with earlier feedback.\nWhile recent advances in large language models (LLMs) have shown promise in various tasks related to academic research, including paper summarization, understanding, and analysis, existing models often struggle to generate reviews that match the depth, specificity, critical perspective, and structure expected in academic peer review. General purpose LLMs may miss field-specific conventions, fail to properly evaluate technical contributions, or provide feedback that doesn't align with established reviewing practices. OpenReviewer addresses this gap, aiming to provide authors with valuable pre-submission feedback that closely mirrors the standards and expectations of human peer reviewers."}, {"title": "OpenReviewer", "content": ""}, {"title": "Demo Interface", "content": "We host a demo of OpenReviewer on HuggingFace Spaces\u00b3. The interface, depicted in Figure 1, is built with the Gradio (Abid et al., 2019) library. It starts with a short description and some guideline text to help users navigate the application. The user first faces a file-upload dialogue, where they can upload a PDF file. Uploading a file automatically triggers a PDF to markdown conversion process. This process takes some time, as OpenReviewer uses transformer-based PDF processing models that run on a GPU. Once the markdown conversion finishes, the markdown paper text will be displayed in a corresponding text area. The user can edit the text in case he wants to fix any conversion errors. Alternatively, if the user already has a markdown"}, {"title": "Llama-OpenReviewer-8B", "content": "The core component powering OpenReviewer is Llama-OpenReviewer-8B, a large language model finetuned on a large dataset of paper-review pairs. This section describes the data collection, prompt design, and training details."}, {"title": "Training Data", "content": "From OpenReview, we collected a dataset of 36K submitted papers and 141K reviews from the International Conference on Learning Representations (ICLR) and the Conference on Neural Information Processing Systems (NeurIPS), considering editions from 2022 onwards. We obtain each paper in PDF format by downloading the earliest revision possible. Later revisions are typically camera-ready versions that already incorporate feedback from the reviews, rendering at least some parts of the reviews invalid for final revisions. Unfortunately, the original double-blind submissions are no longer available for some venues; we obtain the non-anonymized version. We convert all papers from PDF to markdown using Marker\u2076. This open-source PDF processing pipeline combines heuristics with multiple transformer models to extract text, apply optical character recognition (OCR) when necessary, detect page layouts, determine the"}, {"title": "Prompt Design", "content": "OpenReviewer uses a system prompt that conditions the LLM on its reviewer role and defines a fixed set of reviewer guidelines inspired by the ICLR 2024 reviewer guide\u2077. The system prompt specifies that the review must be written in markdown format and follow a specific review template, which is part of the input and differs across venues. The user prompt is minimalistic and only contains"}, {"title": "Training", "content": "We full finetune Llama-3.1-8B-Instruct for three epochs with an effective batch size of 64 and a learning rate of 2 \u00d7 10\u207b\u2075, using bfloat16 precision. The maximum sequence length during fine-tuning is 128k tokens to accommodate long paper texts. Model training used the axolotl library using Deepspeed ZeRO-3 (Rajbhandari et al., 2020) for parallelization and Flash Attention V2 (Dao, 2024) and LIGER Kernel (Hsu et al., 2024) to reduce memory usage and increase throughput. The training process took approximately 34 hours using 64 NVIDIA A100 80GB GPUs. Refer to Section A.3 for all training hyperparameters."}, {"title": "Evaluation", "content": "Evaluating automatically generated paper reviews is challenging. There is no single objective measure of review quality since even human experts often disagree in their assessments. Additionally, evaluat-"}, {"title": "Matching Recommendations", "content": "While the sections in a review vary based on the corresponding venue review template, the recommendation is a numerical rating that consistently exists for all reviews. Additionally, the recommendation can be expected to reflect the overall sentiment and arguments expressed throughout the review. To measure how well the recommendation of a generated review matches the recommendations of the human reviewers, we check whether it exactly matches one of the human reviewers' recommendations. Additionally, we measure the average absolute distance between the generated review's recommendation and the human reviewers'"}, {"title": "Review Arena", "content": "We run an arena-style preference evaluation with an LLM-as-a-judge setup to measure whether OpenReviewer produces better reviews than the other LLMs. This is similar to MT bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023b), which use an LLM-as-a-judge to evaluate the quality"}, {"title": "Discussion", "content": "Our results demonstrate that OpenReviewer generates reviews that align significantly better with human expert judgments than general-purpose LLMs. The key findings warrant several important discussions:\nReview Quality and Criticism Level: A striking observation is that general-purpose LLMs consistently produce overly positive reviews, with average recommendations between 6.9 and 8.1 on a 10-point scale. In contrast, OpenReviewer's average rating of 5.4 matches the human reviewer distribution exactly. This suggests that specialized training on peer review data helps overcome the tendency of LLMs to be overly favorable a critical feature for providing constructive feedback that can help improve papers.\nUse Cases and Limitations: While OpenReviewer shows promise as a pre-submission feedback tool, it is important to emphasize that it is not intended to replace human peer review. The system can help authors identify potential weaknesses and prepare stronger submissions, but should be used alongside other forms of feedback. Additionally, the model's training data comes primarily from ML/AI venues, potentially limiting its effectiveness for other fields.\nEthical Considerations: The development of automated review systems raises important questions about maintaining review quality and preventing potential misuse. There is a risk that authors might try to \"game\" such systems or that conferences might be tempted to use them to replace human reviewers. Clear guidelines about appropriate use cases and limitations are essential.\nTechnical Tradeoffs: Our choice of an 8B parameter model balanced performance with accessibility and computational requirements. While larger models might achieve better results, our evaluation suggests that specialized training on peer review data could be more important than the model scale. However, we believe scaling up the LLM powering OpenReviewer will further improvement the generated reviews."}, {"title": "Related Work", "content": ""}, {"title": "AI-Assisted Peer Review", "content": "Prior work has explored various ways NLP can support the peer review process. Early approaches focused on reviewer-paper matching using text similarity and topic modeling (Charlin and Zemel, 2013; Wieting et al., 2019). More recent work has investigated automated analysis of review quality (Yuan et al., 2022), detection of biases in peer review (Manzoor and Shah, 2021), gen-"}, {"title": "Review Generation with Large Language Models", "content": "With the emergence of powerful LLMs, several recent studies have explored their potential for automated review generation. Liu and Shah (2023) conducted experiments using OpenAI's GPT-4 for reviewing papers but found limitations in technical depth and consistency. Liang et al. (2023) show that while LLMs can provide useful feedback, they tend to focus more on writing and presentation issues rather than technical contributions. The AI Scientist (Lu et al., 2024) is a recent framework for fully automated scientific discovery based on GPT40, including research idea generation, coding, experiment execution, full scientific paper writing, and simulation of the review process. Faizullah et al. (2024) investigate approaches to harness open LLMs for producing suggestive limitations of research papers. Unlike these approaches that use general-purpose LLMs, OpenReviewer employs a model specifically fine-tuned on peer review data to better align with reviewing standards and expectations."}, {"title": "Datasets and Resources", "content": "Several datasets have been developed to study peer review, including PeerRead (Kang et al., 2018), which contains reviews from machine learning conferences, and NLPeer (Dycke et al., 2023), which focuses on computational linguistics venues. MOPRD (Lin et al., 2023) is a multi-disciplinary dataset for peer review. Our training data incorporates a larger and more recent collection of human reviews from top ML conferences."}, {"title": "Evaluation of Review Quality", "content": "Measuring review quality is an ongoing challenge in the field. Prior work has proposed various met-"}, {"title": "Ethics and Bias in Peer Review", "content": "Research has highlighted various forms of bias in peer review, including institutional bias (Tomkins et al., 2017), gender bias (Hofstra et al., 2020), and strategic manipulation (Jecmen et al., 2024). While automation through NLP offers potential solutions, it also raises new ethical concerns about fairness, transparency, and accountability (Rogers and Augenstein, 2020)."}, {"title": "Conclusion", "content": "We presented OpenReviewer, an open-source system for generating high-quality peer reviews of ML/AI papers. Through careful fine-tuning on expert reviews and evaluation against multiple baselines, we demonstrated that OpenReviewer produces significantly more realistic and critical reviews than general-purpose LLMs. Our key contribution is showing that specialized training on peer review data can overcome the tendency of LLMs to generate overly positive assessments.\nLooking ahead, several promising directions emerge:\n\u2022 Expanding training data to cover more venues and domains.\n\u2022 Incorporating related literature search and citation graph information to improve the assessment of novelty claims.\n\u2022 Developing better automatic evaluation metrics for review quality.\n\u2022 Creating interfaces for collaborative human-AI reviewing.\nWhile OpenReviewer shows promise as a tool for generating preliminary feedback, it should complement rather than replace human peer review. We hope this work spurs further research into AI-assisted academic reviewing while maintaining high standards of scholarly assessment."}, {"title": "Limitations", "content": "Our study faces several key limitations across data, technical, evaluation, and practical dimensions. Regarding data, our dataset is restricted to papers from 2022 onwards from only ICLR and NeurIPS conferences within the machine learning and AI domain. However, given rapid field advances, this temporal bound helps ensure contemporary relevance. The partial use of non-anonymized papers may also introduce information leakage concerns. Technical limitations include OpenReviewer's dependence on PDF-to-markdown conversion accuracy and its relatively modest 8B parameter size compared to larger models with potentially better document understanding capabilities. Our evaluation scope is constrained by a test set of only 400 papers due to commercial LLM usage costs, focuses primarily on automated metrics rather than detailed human analysis, and compares against a limited set of baseline models due to resource constraints. Practical limitations include performance constraints from running on limited hardware and the risk of giving authors false confidence through automated feedback alone. Additional challenges involve the bias toward certain conferences and review templates, the limited domain focus on machine learning and AI, the handling of figures and images, and the incorporation of relevant background knowledge from references areas we continue to work on improving."}, {"title": "Ethics and Broader Impact Statement", "content": "OpenReviewer raises several important ethical considerations that warrant careful discussion. While our demo aims to assist authors with pre-submission feedback, it could potentially be misused to automate the peer review process entirely, compromising scientific rigor. We strongly emphasize that OpenReviewer is designed to complement, not replace, human peer review.\nThere are also concerns about fairness and bias. Our training data comes primarily from top ML/AI conferences, which may encode existing biases in the field regarding what constitutes \"good research.\" This could disadvantage work from underrepresented perspectives or methodological approaches. Additionally, researchers with access to better computational resources might gain an unfair advantage in preparing submissions.\nOn the positive side, OpenReviewer could democratize access to high-quality feedback, par-"}, {"title": "Appendix", "content": ""}, {"title": "Prompts", "content": "The prompts used by OpenReviewer are shown in Figures 3 and 4. The prompts used for GPT-4o as a judge are shown in Figures 5 and 6."}, {"title": "Example Outputs", "content": "Figure 7 shows an example output of GPT-40 as a judge, comparing two reviews."}, {"title": "Training Hyperparameters", "content": "Figure 8 shows all training hyperparameters used to train Llama-OpenReviewer-8B with the axolotl library."}]}