{"title": "Real-time Fake News from Adversarial Feedback", "authors": ["Sanxing Chen", "Yukun Huang", "Bhuwan Dhingra"], "abstract": "We show that existing evaluations for fake news detection based on conventional sources, such as claims on fact-checking websites, result in an increasing accuracy over time for LLM-based detectors-even after their knowledge cutoffs. This suggests that recent popular political claims, which form the majority of fake news on such sources, are easily classified using surface-level shallow patterns. Instead, we argue that a proper fake news detection dataset should test a model's ability to reason factually about the current world by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive fake news that challenges LLMs. Our iterative rewrite decreases the binary classification AUC by an absolute 17.5 percent for a strong RAG GPT-40 detector. Our experiments reveal the important role of RAG in both detecting and generating fake news, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG detection helps discover more deceitful patterns in fake news.", "sections": [{"title": "Introduction", "content": "Disinformation and fake news have emerged as major societal concerns. The spread of fake news can cause serious consequences, such as influencing elections, inciting violence, and misleading critical decision-making, especially in health. In the NLP community, researchers have long focused on developing methods and evaluation for automatic fake news detection (Zellers et al., 2019). The rise of LLMs has significantly changed the landscape of this problem (Goldstein et al., 2023; Chen and Shu, 2023).\nLLMs have demonstrated impressive knowledge and reasoning ability. They can generate fake news that is more deceitful than human-written news, and they can detect fake news with high accuracy (Chen and Shu, 2024; Pelrine et al., 2023). The pre-training and prompting paradigm of LLMs reduces the risk of creating dataset-specific models that are prone to in-distribution shortcut learning (Pagnoni et al., 2022). However, due to the opaque nature of LLM training, their evaluation is often influenced by potential contamination issues, which can result in misleading outcomes and a need for out-of-distribution evaluation (Zhou et al., 2023; Vu et al., 2024; Huang et al., 2024). Existing fake news detection datasets are commonly sourced from past claims from fact-checking websites (Wang, 2017; Shu et al., 2018). These websites, such as PolitiFact and Snopes, often feature popular claims from politicians that are widely circulated on the internet, leading to potential contamination during the large-scale pre-training of LLMs.\nCollecting new data from fact-checking websites does not address this issue. Recent research has reported that LLMs (e.g., GPT-4) remain effective in detecting fake news that falls outside their knowledge cutoffs (Pelrine et al., 2023). We conduct the first comprehensive analysis of retrieval-free LLM detectors on PolitiFact data over the years, revealing a surprising upward trend in detection performance (Figure 1). The trend continues to rise even after the knowledge cutoff dates of models, suggesting a gain from non-factual salient patterns of recent political claims rather than from increased model knowledge or reasoning ability. In practice, this affects the continued applicability of datasets sourced from political fact-checking for evaluating LLM-based detectors on factual reasoning. As the field evolves toward evidence-based detection, LLMs should rely on retrieval results to examine claims with up-to-date information (Yang et al., 2022; Hu et al., 2023b; Liao et al., 2023). The fact that LLMs can detect most fake news without retrieval augmentation under this evaluation setup motivates us to explore new ways to create fake news datasets that can better evaluate retrieval-augmented (RAG) detectors.\nInspired by LLMs' ability of continual improvement based on feedback (Madaan et al., 2023), we propose an adversarial iterative approach to generate fake news that can deceive strong LLM-based detectors. The generator leverages feedback from an adversary detector in the form of a rationale discussing the factuality of the machine-generated fake news, mimicking the real-world scenario in which fact-checkers provide explanations for their verdicts. Given this verdict and the fake news generated in the previous round, the generator then adds another round of modifications to the fake news such that it would fool the prior verdict. This process is repeated a few times, allowing the generator to learn from the detector's perspective. Empirically, highly performant LLM-based detectors on conventional political fact-checking struggle with the fake news generated by our approach.\nOur approach emphasizes the importance of retrieval-augmentation in both the generation and detection of fake news. First, we source real-time news articles from various domains, which are beyond the knowledge of LLMs and necessitate retrieval for fact-checking. Second, we show that RAG-based detection with up-to-date external knowledge is a strong defense against machine-generated fake news. Lastly, we demonstrate that the iterative adversarial process creates fake news with increasing deception that generalizes well across different setups varying LLM backbones and retrieval contexts, while being more effective when utilizing feedback from a RAG-based detector."}, {"title": "Background", "content": ""}, {"title": "Problem Formulation", "content": "We formulate the fake news detection task as a binary classification problem, where the primary objective is to determine whether a given news article is genuine or fabricated. Let D = {(xi, Yi)}=1 represent the dataset, where each xi is a news article and yi \u2208 {0,1} is a binary label indicating the authenticity of the article. A detector f : X \u2192 [0, 1] maps each news article xi to a probabilistic score \u0177\u2081 = f(xi), reflecting the likelihood of the article being true news. The effectiveness of a detector is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), which quantifies the model's capability to classify across various threshold settings."}, {"title": "Analysis of Political Fake News Detection", "content": "Popular fake news detection datasets, such as LIAR (Wang, 2017) and FakeNewsNet (Shu et al., 2018), obtain news content and human labels from fact-checking websites like PolitiFact. Depending on the time of data collection, the datasets consist of claims ranging from 2007-2018, which are within the knowledge cutoff of most LLMs available today. In order to evaluate the performance of LLM-based detectors on recent year data, we collect all 15171 PolitiFact claims from June, 2015 to August, 2024. We filter out anonymous social media posts that PolitiFact has widely fact-checked"}, {"title": "Methodology", "content": "The wide accessibility of LLMs has not only enabled strong detection models but also facilitated the mass generation of more credible and persuasive fake news (Kreps et al., 2022; Goldstein et al., 2023). Therefore, in combating these emerging threats, the evaluation of detection models should also evolve to incorporate challenging machine-generated misinformation.\nOne key challenge in creating fake news datasets is obtaining automatic labels for the data. Open-ended generation, although effective in creating diverse fake news, simultaneously introduces false positives that are hard to verify due to the professional skills required for fact-checking. Our proposed approach leverages the strengths of LLMs in controlled misinformation generation (Zellers et al., 2019; Chen and Shu, 2024) to manipulate true news articles and introduce factual errors, while maintaining the overall context and style of the original news. We implement filtering protocols as an additional safeguard to reduce invalid generation.\nFigure 2 illustrates our pipeline. Formally, we start with a true news corpus T, which contains real-time news articles from various domains. Each news article consists of a headline, a body text, and a publication date. We independently rewrite each news article Ti \u2208 T to generate multiple fake news candidates Fi. To ensure that Fi contradict the original true news Ti, we employ a LLM-based contradiction detector. We put an additional upper limit on the amount of edits allowed in the rewriting process using a threshold on the Levenshtein distance \u03b4(Ti, fij), where fij \u2208 Fi, between the original true news T\u1d62 and the rewritten news fij.\nAfter filtering out the candidates that do not meet these criteria, we rank the remaining candidates based on the plausibility score provided by a fake news detector g(fij|c), where c is the optional external context retrieved by a retrieval model R(fij). If none of the candidates meet the criteria, the generator resamples a new batch of candidates. From this ranked list, we select the top-ranked candidate as the most plausible fake news article.\nfi = arg max g(fij|c)\nfij EFi\nfi and the detector's rationale are then serve as additional information to inform the next round of generation on Ti, creating an iterative process that gradually deceives the detector. In the end of the process, we obtain the most deceptive fake news fi across all iterations.\nA critical component of our approach is the retrieval-augmented detector. Using a retrieval-free detector as an adversary, the generator can only improve the factual consistency within the news content or exploit the weaknesses of a LLM with outdated internal knowledge, which can generally be seen as a case of self-evaluation (Kadavath et al., 2022). However, with the information from the retrieved external context, the generator can learn to deceive cross-verification and improve factual"}, {"title": "Experimental Setup", "content": ""}, {"title": "Data Collection", "content": "For the true news corpus, we obtain 431 actual news stories from NBC News\u00b3 using the news-please crawler (Hamborg et al., 2017). These news stories are from March 1 to March 13, 2024, covering domains such as politics, business, sports, U.S., and world news. We filter out advertisements and other non-news content. The date range is chosen to be beyond the knowledge cutoff of the latest LLMs, thus ensuring no contamination from the training data. This set of news stories serve as seed true news for our generation pipeline and as positive examples for evaluating the detectors. We manually examine the final rewrite and filter out 29 fake news generation that turn out to be true. Comparing 100 examples from the first and last round of rewrites, the noisy ratio remains similar.\nTo build an in-house RAG pipeline, we gather 811,000 news articles from various news sources"}, {"title": "Generator Setup", "content": "We adopt GPT-40 as the main LLM for the generation pipeline. The pipeline is iterated for 6 rounds with an extra preparation round 0 of a direct rewrite (no rationale and ranking) on the seed true news. We instruct the LLM to \"introduce some believable factual errors\" without mentioning any concrete strategies or constraints, leaving the candidate generation process open-ended. The generator produces 8 candidates in a zero-shot chain-of-thought fashion for each news story in each round, which are then filtered and ranked based on the plausibility score from the detector. The contradiction detector produces 10 binary scores for each candidate, only if more than eight of them are positive, the candidate is considered to contradict the original true news. String edit distance is used to limit the amount of change in each iteration, with a threshold of more than 60% overlapped tokens."}, {"title": "Detector Setup", "content": "For implementing the LLM detectors, we simply instruct the models to produce plausibility scores (1-10) for the given news story in a zero-shot setting. We use the directive \u201cToday is March 26, 2024. You predict the plausibility of a news you haven't seen\" to inform the model about the current date and the unseen nature of the news story. The detector generates multiple plausibility scores for each news story using temperature t = 1, which we average to obtain the final score. Our GPT-40 de"}, {"title": "Experimental Results", "content": "Evaluation results are presented in Table 2 and Figure 4. These results show that our adversarial iterative generation pipeline can produce fake news that can deceive strong LLM-based detectors. We find the Llama 3.1 model to be the best at detecting fake news generated by our pipeline using GPT-4o, especially for later iterations. As the earliest released model in our selection, GPT-3.5 is the most vulnerable, consistent with results of well-established public benchmarks (Chiang et al., 2024).\nThrough each iteration, the pipeline progressively enhances the deceptive quality of the generated fake news. Relying on the feedback from the GPT-40 RAG-based detector with the in-house retrieval corpus, the generator proves most effective at deceiving this particular detection setting, achieving a reduction of 17.5 AUC-ROC points. Nevertheless, these enhancements are shown to consistently generalize across different LLM backbones and retrieval contexts."}, {"title": "Analysis", "content": ""}, {"title": "Qualitative Examples", "content": "Since we leave it to the LLM to decide how to introduce factual errors, the generated fake news exhibits various types of misinformation. We pro"}, {"title": "Related Work", "content": "Fake News Generation and Detection LLMS have been widely studied for fake news detection and generation (Goldstein et al., 2023; Su et al., 2023a; Hu et al., 2024; Chen and Shu, 2024; Liu et al., 2024). External evidence in the form of search results or knowledge graphs has been demonstrated to improve the detection of fake news (Fung et al., 2021; Xu et al., 2022; Liao et al., 2023; Pelrine et al., 2023).\nFrom the generation perspective, recent research has also shown that external evidence can improve the style, domain and factual consistency of generated fake news (Shu et al., 2021; Mosallanezhad et al., 2022; Huang et al., 2023; Wang et al., 2024). However, they investigate one-round generation that is less effective in deceiving today's state-of-the-art LLM-based detectors. Our approach introduce the iterative process and the detector perspective which helps to digest the external evidence in examining the flaws of generation.\nAdversarial setups have been used to improve the robustness of LLMs in tasks such as AI-generated text detection and math problem-solving (Hu et al., 2023a; Zhu et al., 2023; Xie et al., 2024). Most of these works focus on modifications that do not change the semantics of the text, which is different from our approach that aims to introduce factual errors in the text. Tailoring to the task of fake news detection, we design a feedback loop using the detector's rationale to guide the generation process, which is a rather realistic threat model in the real-world fact-checking scenario.\nTemporal Reasoning on Future Events Predicting the plausibility of future events requires temporal knowledge and reasoning (Dhingra et al., 2022). LLMs have the knowledge of the past, but not the future, which they rely on retrieval to access (Kasai et al., 2023; Vu et al., 2024). A similar ability has also been studied in the forecasting task with retrieval augmentation (Zou et al., 2022; Halawi et al., 2024)."}, {"title": "Conclusion", "content": "In this paper, we evaluate large language models on fake news detection of events that happen beyond their knowledge cutoff date. We find that the conventional use of political claims from fact-checking websites is unsuitable for such tests because of emergent shortcuts in data. We thus propose an adversarial iterative pipeline to generate fake news that can gradually evade strong RAG-based detectors. Our experimental results shed light on the behaviors of LLMs in detecting and generating fake news about the current world. We hope the evaluation pipeline and dataset encourage research efforts toward robust factual reasoning models under temporal distribution shift."}, {"title": "Limitations", "content": "Our work focuses on evaluating prompting-based LLM detectors. LLM-generated fake news does exhibit patterns that are not present in human-written fake news, which may limit the generalizability of our data for training detectors (Zellers et al., 2019; Huang et al., 2023). Future work may employ debiasing techniques (e.g., paraphrasing using the same LLM) to mitigate this issue (Su et al., 2023b).\nOur experiments are primarily based on English data and U.S. news, which might limit the conclusion to generalize to other languages and news of countries. The findings from the background study focus specifically on the set of U.S. political news that the fact-checking website selects, which we do not intend to apply to other environments."}, {"title": "Ethics Considerations", "content": "Our work aims to improve the robustness of fake news detection models by generating challenging fake news that can evade detection. We acknowledge the potential misuse of our method to create more deceptive misinformation. Simultaneously we have shown that RAG-based detectors with high-quality retrieval can effectively counter such misinformation. Our approach focuses on the factual reasoning aspect of fake news detection. The generated fake news does not usually contain propaganda, hate speech, or other harmful content. The release of generators is critical to prepare detectors against adversarial attacks (Zellers et al., 2019). We will responsively release our code and data to facilitate further research on fake news detection."}, {"title": "Appendix", "content": ""}, {"title": "PolitiFact Experiment Details", "content": "PolitiFact adopts six fine-grained labels for the truthfulness ratings: pants-fire, false, barely-true, half-true, mostly-true, and true. For the calculation of AUC-ROC, we adopt a common binarization to categorize the first three as negative and the last three as positive (Liao et al., 2023; Pelrine et al., 2023). Digging into the increasing differential trend of the fake news classes, we find that it results from a combination of reasons: 1) there is an increasing proportion of the less truthful class in the data (i.e., 'false' grows over 'barely true' in Figure 6); 2) both the 'false' and 'barely true' classes become less plausible (Figure 7).\nAblating the attributes we use to classify a news story (Figure 9), we find that none of the attributes affect the increasing trend. Interestingly, we find that GPT-40 makes more accurate prediction when we ignore the date of publication in the prompt. Removing the originator of the claim is harmful, but not decisive\u2014the classifier can judge the validity of a claim regardless of the speaker's credibility."}, {"title": "Detector and Generator Prompts", "content": "We adopt simple prompts that implement corresponding functionality but do not introduce domain or dataset-specific heuristics to maximize generalizability."}]}