{"title": "On Synthetic Texture Datasets: Challenges, Creation, and Curation", "authors": ["Blaine Hoak", "Patrick McDaniel"], "abstract": "The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 362,880 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse.", "sections": [{"title": "Introduction", "content": "Large, high-quality data sources have long been a defining factor in the success of AI research, and have enabled progress in a plethora of fields including (but not limited to): object classification (Russakovsky et al. 2015), visual emotion recognition (You et al. 2016), medical image interpretation (Irvin et al. 2019), scene recognition (Zhou et al. 2018), and more. Texture image data and corresponding analysis has played a vital role in uncovering the high-level features image classifiers learn and their implications. Works have shown how models exhibit texture bias (Geirhos et al. 2019), how to use texture data to construct texture-object associations (Hoak and McDaniel 2024), and even how patterned images can create adversarial examples (Zhang et al. 2022).\nHowever, the findings from prior texture-based works are limited due to the lack of a large and diverse texture dataset. For example, the most widely used texture dataset, the Describable Textures Dataset (DTD) (Cimpoi et al. 2014), contains only 5640 images across 47 texture categories. This limitation, coupled with the need for diverse texture data, has led to the creation of one-off texture datasets that are only applicable for specific use cases, such as the shape-cue conflict dataset in (Geirhos et al. 2019) and patterned images from (Zhang et al. 2022). Furthermore, current approaches for collating texture images rely on manual search from public sources like Flikr (2014). The result of manual methods is omnipresent in the literature; works often rely on fewer than 100 texture images for their findings. As a result, larger scale analysis on the relationship between textures and models has simply not been feasible.\nThe advent of generative AI models has significantly reduced the manual effort required to create new datasets. However, generative models are not explicitly designed for texture image generation, presenting several challenges including: designing prompts to achieve specific texture, modifying Stable Diffusion models to better handle texture data, and ensuring that the generated textures are diverse, representative, and of high quality. These challenges necessitate novel adaptations to the entire pipeline to enable the generation of texture data.\nIn this work, we introduce an extensible methodology for generating high-quality, representative, diverse texture images capable of supporting a broad range of texture-based tasks. Our approach: (1) constructs prompts from a range of descriptors to serve as input to text-to-image models, (2) adopts and adapts Stable Diffusion pipelines, which generates and filters the corresponding images created from our prompts, and (3) further filters the generated textures down to the highest-quality samples through CLIP scores. With this, we create the Prompted Textures Dataset (PTD): a dataset of 362,880 texture images (examples in Figure 1b) across 56 different texture classes.\nIn creating the PTD, we conduct a series of experiments at each stage of our pipeline. At prompt generation, we investigate the effect of different descriptor combinations on the quality and representativity of generated images. At image generation, we observe that Stable Diffusion pipelines and their built-in filtering mechanisms cause many of our images"}, {"title": "Background", "content": "Texture images. The Describable Textures Dataset (DTD) (2014) is perhaps the most popular texture dataset to date. It contains 5640 images sourced from Flickr in 47 texture categories such as polka-dotted, scaly, and striped. This texture dataset has had a variety of uses in computer vision and machine learning. Most recently, (Hoak and McDaniel 2024) used the DTD to construct texture-object associations to quantify the extent to which specific textures are learned by object classification models. Aside from the DTD, other works operating on texture datasets have created their own sets of textures to suit their specific use cases. In (Geirhos et al. 2019), the authors construct a shape-cue conflict dataset, which contains images with the texture of one object and the shape of another for the purposes of studying if CNNs were more biased towards texture or shape. Finally, in (Zhang et al. 2022) patterned images were created and overlayed onto existing object images; the authors found that this method produced an effective attack against machine learning models, wherein these patterns caused the model to misclassify the images and could be constructed even without any access to the model weights or training data.\nAside from benchmark texture datasets, there have also been works focused on synthesizing textures. (Portilla and Simoncelli 2000) introduced the first parametric model for texture synthesis. This model worked by taking reference images of a texture and using the introduced statistical model to expand the textures provided. More recent works have introduced a similar approach using CNNs rather than the previous statistical model (2015; 2016), which has proved to be extremely useful in style transfer (2016). Additionally, some texture synthesis methods opt for a \"quilting\" approach that tiles together multiple textures to make a new texture (Efros and Freeman 2001). While these approaches are performant at extending existing textures, one of the main drawbacks is the reliance on starting texture images.\nText-to-image models and data metrics. Text-to-image models are generative models that take textual descriptors as input and aim to generate representative images based on the provided input. Stable Diffusion (SD) models have become the state-of-the-art in image generation, demonstrating great promise in producing high-quality images that closely align with the given prompts (Rombach et al. 2022). These models are trained by iteratively adding noise to latent representations of images and then learning a denoising process to recover the original images. During evaluation, the models utilize the learned denoising process to transform random noise into coherent images, guided by textual descriptions.\nThe quality of generated data can be assessed using a variety of metrics, depending on the desired properties of the generated data. Here, we focus on quality, diversity, and representativeness of images. When generating image data using text-to-image models such as Stable Diffusion, it is important to ensure not only that the images themselves are good, but that they also well represent the text descriptions that they are generated from. CLIP scores (Hessel et al. 2022) were originally introduced as a means to measure the quality of image captions but have become a popular metric for measuring the representativeness of generated images with respect to any portion of text and also play an important role"}, {"title": "Creating Prompts", "content": "To create texture data using text-to-image models, we first construct prompts that describe the textures we want to generate. Later on, these prompts are input to Stable Diffusion (2022) to produce the corresponding images.\nSelecting Descriptors. In creating our texture images, the goal is to produce high quality, diverse, and descriptive prompts that will result in a wide variety of high-quality texture images. To achieve this, we want to not only capture a wide variety of textures and patterns, but also want each texture/pattern to be varied itself, and have examples of many different styles, colors, size, shape, and other elements that effect how each texture or pattern may be represented. In other words, we want to produce more than just \"a striped image\". To achieve this, we introduce these elements as additional descriptive words in our prompts. By providing specific descriptions of the textures we want to generate, we can ensure that the images we ultimately generate diverse in a controlled manner, rather than solely relying on diversity coming from randomness introduced by the generative model.\nWhen producing the Describable Textures Dataset, Cimpoi et al. (2014) cultivated a list of 47 different texture classes to serve as the building blocks for their dataset. We start from this list and then source other lists of textures (Barnett 2023) to identify new candidate textures for us to generate. We consolidate down to only the textures that are meaningfully different from our starting 47 classes. From this, we add 9 additional texture classes. In total, we have 56 texture classes to serve as the foundation for our prompts.\nWe then provide more specificity to our prompting by introducing other descriptive categories, which we choose by drawing inspiration from the 7 basic elements of art: line, shape, form, texture, space, color, and value (Ocvirk et al. 2001). When selecting these descriptive categories, we focus on those that will allow us to capture multiple elements in a single category, and particularly on the elements that are not already captured by the texture classes we introduce (e.g., texture, shape, line) in order to keep our prompt space tractable. For example, we choose to include color enhancers as a category to not only enhance the colors presented in the color category, but also as a way to introduce value and form into the prompts. We aim to select a few words for each category that are sufficiently different from each other to capture as many unique textures as possible.\nAn overview of all the descriptor categories and the specific words used in each category are shown in Table 1. We produce prompts combining one word from each category, in standard English adjective ordering:\n{Artistic} {Spatial} {Color Enhancer} {Color} {Texture}\nWe enumerate across all possible combinations of words, and produce prompts such as \u201cimpressionistic randomized vivid red striped texture\u201d. This results in a total of 96,768 prompts to use when subsequently producing our texture images.\nIt is important to note that this methodology for creating prompts is not limited to texture prompts alone. The methodology we present here can be applied to any task that generates images from prompts. For example, by swapping out the texture descriptors for shape descriptors, one could generate prompts such as \u201cimpressionistic vivid red circle\u201d or \"photo-realistic green square\" to generate shape images. This way, the diversity we introduce through prompt descriptors can be applied to a wide variety of image generation tasks. In addition to being applicable to non-texture data, this methodology can also be applied to generate other texture"}, {"title": "Generating Images", "content": "Next, we discuss how we use and adapt Stable Diffusion pipelines to generate and filter images from our prompts, and investigate specific challenges we uncover when using this pipeline on texture data.\nTo generate the images for our dataset, we use our created texture prompts as input to text-to-image generative models. Specifically, we use the Stable Diffusion v1.5 model (2022) from HuggingFace (2020). The Stable Diffusion pipeline also contains a content safety filter to check the generated images for content that is deemed inappropriate, explicit, or NSFW (Not Safe For Work). These safety checks measure the CLIP score of the images with secret NSFW content words (though there have been efforts to reverse engineer the words (Rando et al. 2022)). If this score is above a certain threshold, the image will be flagged and replaced by an all black image. These NSFW filters have been added to many of the large HuggingFace generative models as a way to safeguard against producing potentially harmful or inappropriate images.\nInterestingly, one of the main challenges that we face during the generation of our images is to actually get images not\nflagged as \"NSFW\u201d, even despite the fact that our prompts are void of anything that would suggest the generative model to produce NSFW content. To ensure we were producing the same number of images per prompt, we regenerate images in the case that they are flagged by the safety filter. For further analysis in the next section, we further adapt the Stable Diffusion Pipeline by disabling the safety filter to record when an image is flagged as NSFW but still get back the original (non-corrupted) image. For ethical reasons, we do not detail how we disable the safety filter, or release the images flagged as NSFW. However, it is important to note that we find no images that actually represent explicit content.\nWith these adaptations made to the Stable Diffusion pipeline, we then generate 5 images for each of our 96,768 prompts. This results in the creation of 483,840 images before any additional filtering, not including the images flagged by the safety filter. A randomly selected subset of the images we generate can be seen in Figure 1b.\nInvestigating Safety Filtering. With as many as 60% of our total images being flagged as NSFW, this was initially a major barrier in producing our dataset and a perplexing issue. Our prompts are relatively simple and void of any suggestions that the generative model should produce NSFW content. This leads us to two questions: (1) Do the flagged images actually represent explicit content? and (2) What is causing the NSFW filter to flag our images?\nWith our modified Stable Diffusion pipeline and the safety filter disabled, we investigate the flagged images to see if they may actually contain explicit content or if this phenomenon is demonstrating a larger issue with safety filters. In the process of generating images that pass the safety filter until we have 483,840 images (5 images per prompt), we save all the images that are flagged as NSFW. This results in a total of 133,857 flagged images. From all the images we observe, we find no examples of images that actually represent explicit content. Randomly chosen examples from the entire set of images that are flagged as NSFW are shown in Figure 3.\nWhen observing differences between images that passed the safety filter and the images that did not (e.g., images from Figure 1b vs. the images in Figure 3), we find that the flagged images are typically smoother, more dull in color, and appear \"noisier\u201d. Besides these observations, we do not find any clear and obvious differences between the image sets or any indication as to why these images are flagged.\nTo better understand what causes the NSFW filter to flag"}, {"title": "Refinement", "content": "At this point, there are 483,840 images that have been generated from our prompts via a Stable Diffusion pipeline. Besides just filtering for safety content that is intrinsic to the generative model, we also want to perform additional refinement on our dataset to filter such that we select the best images and ensure that the final dataset we produce is as high quality as possible. Unlike other refinement processes such as prompt engineering, we are refining our dataset rather than the prompt itself; at this point, we have generated all our data and are picking which of the generated images we are including in our final dataset.\nTo perform our refinement, we first calculate the CLIP score of each image with respect to its prompt. Using these CLIP scores, we then filter out any images that fall below a certain threshold. In practice, in order to keep our dataset balanced, for each of the 56 texture classes, we remove images that fall below the 25th percentile of the CLIP scores of that texture class. We chose the 25th percentile because we observed a separation between the CLIP scores above and below the cutoff at this point. This also allows us to keep the majority of our data, while also ensuring that the images we keep are of higher quality. This parameter can be easily chosen to be more or less strict depending on the desired balance between data quality and quantity. After refinement, we are left with 362,880 images in the Prompted Textures Dataset, spread evenly across the 56 texture classes.\nPrompt Quality Trends. Given the volume of prompts we generate, we naturally want to investigate if there are trends in prompts producing images that are not filtered out by our refinement process. In other words, How do different descriptors affect the quality of the images produced?"}, {"title": "Quality evaluation", "content": "Now that we have our final Prompted Textures Dataset (PTD), totaling 362,880 images, we evaluate the quality, diversity, and representativeness of the dataset in two parts: (1) using standard metrics such as Inception Scores and FID scores, and (2) through a human evaluation study.\nStandard metrics\nHistorically, works have evaluated generated image datasets using Inception and FID scores. Inception scores measure the quality and diversity of the images in a dataset, while FID scores measure the dissimilarity of the generated images to a set of real images. We use these metrics to evaluate the quality of our dataset. However, the underlying assumption in these metrics is that the optimal generated image contains objects, since both scores build their metric on output distributions of the images when run through an object classification model. Given that our dataset consists of textures, we expect that these metrics will not be particularly useful at evaluating the quality of our dataset.\nGiven the wide variety of interesting challenges we have uncovered when working with texture images, we are naturally curious to see how standard metrics respond to texture images. For this, and to provide a complete quality evaluation of our dataset, we measure both the Inception scores and FID scores of our dataset. In Figure 5, we report both the Inception score (higher is better) and FID score (lower is better), separated by texture class, on both the initial dataset generated from Stable Diffusion, and our dataset after we perform our refinement process. For comparison to real data,\nwe measure the FID score with respect to the DTD and also report the Inception Scores of the DTD.\nFrom this, the Inception scores are higher than we would anticipate, and are much higher than the Inception scores on real data (DTD). Given that our Inception scores reach up to 9.02, despite not being directly related to ImageNet object classes, demonstrates that perhaps object classification models are able to classify based on texture alone. This theory is also supported by prior work on texture bias (2019) and texture learning (2024). While this could also imply that the images being generated actually do contain objects, we see later in our human evaluation that this is not the case.\nIn both Inception and FID scores we see that the scores get worse (Inception gets lower and FID gets higher) when evaluating on our data after our refinement step. Given that our refinement step filters out images with lower CLIP scores, this shows a dichotomy between CLIP scores and FID/Inception scores. This would suggest one of two things, either (1) our refinement step does not actually help improve the quality, diversity, and realism of our dataset, suggesting that CLIP is not well suited for use in texture data or (2) Inception scores and FID scores are an imperfect indicator of quality in this domain, a plausible outcome given the hypothesized texture bias (Geirhos et al. 2019) of the underlying models. While this may initially appear to be a negative result, this illustrates an unexpected disagreement between the two classes of metrics, and opens a new avenue for investigating the suitability of these metrics for texture data, which we explore in the next section.\nGiven the potential weaknesses of automated evaluation metrics, obtaining valid metrics for image quality necessitates an approach that avoids the pitfalls of texture-biased models. Towards this, we next conduct a human-evaluation of both our generated samples and-indirectly-the suitability of automated metrics for texture sample evaluation.\nHuman Evaluation\nFor our human evaluation, we recruited 9 participants to evaluate the images. Each participant was shown 100 images\nin random order and asked two questions for each image: (1) How would you rate the overall quality of the image? and (2) How well does the image represent the provided descriptor? Participants were asked to supply a rating on a scale of 1 to 5, with 1 being the worst and 5 being the best, for each of these questions for every image.\nThe images for the image sets provided to the participants were selected randomly from the dataset, but we ensured there were no duplicate images between or within the sets, meaning that we evaluated 900 unique images from our dataset. These images were selected before the refinement stage in our pipeline, such that some of the evaluated images were removed as part of our refinement process. This was done to compare the human evaluation scores before and after refinement to see if our refinement process does indeed help to improve the overall quality of our dataset.\nThe results of the human evaluation are shown in Figure 6. This histogram shows the distribution of scores given by the participants, normalized by the number of images. Here, we show both the quality and representative scores, and for each, we show the scores before and after the refinement process.\nFrom this, we find that our pipeline does help to produce good images. Comparing the scores on the images before and after the CLIP refinement, we can see that low quality and representative scores make up a larger majority of the unrefined dataset compared to the post-refined dataset and the post-refined dataset has a larger majority of images that were scored higher in our human eval. Additionally, the average quality score increases from 3.87 to 3.94, and the average representative score increases from 3.56 to 3.66. This indicates that the refinement process helps to improve the quality of the images in our dataset. Finally, the majority of the images receive scores of 3 or higher, indicating that the images are generally of good quality.\nIn addition to the raw scores, the participants also had the option of commenting on any trends they may have observed when evaluating the images. Some of the comments are: the descriptors containing the word \u201cmuted\" often had \"destroyed\" images in terms of quality (P1), describing colors"}, {"title": "Suitability of CLIP Scores", "content": "Given the previously discussed disagreement between FID/Inception and CLIP scores, we want to ensure that the CLIP scores we are using to filter our images are still a suitable metric for determining image representativeness, even on texture images. To investigate this, we compare the CLIP scores against our human evaluation scores of the same images. In Figure 7 we show the average representative score (provided in the human evaluation) across various CLIP score quantiles.\nFrom these results, we can see that as we increase our CLIP score quantile cutoff, and thus filter out more images, the average representative score provided by our human evaluation increases. This means that CLIP scores are indeed a good metric to use for image filtering, even on texture images, as they roughly track human scores."}, {"title": "Conclusions", "content": "In this work we provide an extensible methodology for generating high-quality texture images by leveraging generative AI. Following this methodology, we create a new texture image dataset, the Prompted Textures Dataset (PTD). In creating and evaluating PTD, we find (a) that our dataset is diverse, representative, and high-quality and (b) uncover a wide variety of previously undiscovered challenges when working with texture data. We find, broadly speaking, that existing metrics and safety filters are not well-suited for texture images. We make our data and code publicly available, and encourage future work on texture based tasks to use our pipeline or dataset as a resource to continue exploring texture bias and texture-based tasks."}]}