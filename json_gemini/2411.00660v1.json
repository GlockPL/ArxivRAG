{"title": "Physics in Next-token Prediction", "authors": ["Hongjun An", "Yiliang Song", "Xuelong Li"], "abstract": "We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.", "sections": [{"title": "1. Introduction", "content": "Currently, the state-of-the-art (SOTA) artificial intelligence models predominantly employ an auto-regressive architecture. Utilizing the Next-token Prediction (NTP) approach, these models seamlessly integrate various modalities, including text, images, audio, and video. This remarkable versatility and intelligence are reshaping human production and lifestyle in profound ways.\nHowever, beneath this promising landscape looms a substantial \"scientific dark cloud.\u201d Guided by the Scaling Law (Kaplan et al., 2020), researchers are relentlessly constructing ever-larger training datasets and expending increasing amounts of energy to train larger auto-regressive models in pursuit of more \u201cintelligent\" systems. Why do big data and immense computational power lead to the emergence of \u201cintelligent\"? What is the essence behind this phenomenon? Where does the Scaling Law ultimately lead? What awaits us on the horizon?\nIn this paper, we delve deeply into these questions and uncover the underlying physics in NTP. We identify the law of information conservation (Hawking, 2014) within NTP and derive the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. Additionally, we introduce Landauer's Principle (Landauer, 1961) into NTP, formulating the Second Law of Information Capacity (IC-2), which reveals the relationship between the training process of auto-regressive models and the principles of energy in the real world. Based on our findings, we derive several corollaries that can effectively guide our daily practices and production activities. Finally, we validate the compatibility and complementarity of our insights with existing theories, attesting to the rationality of our theoretical framework."}, {"title": "The 1st Law of Information Capability: the Conservation of Information in NTP", "content": ""}, {"title": "2.1. Preliminaries", "content": "In 2023, Rae conceptualized the process of NTP as a mechanism for compressing information of dataset (Rae, 2023), elucidating the relationship between the compression process and the emergence of intelligence: given a token vocabulary V and a dataset D = {x\u2081, x\u2082, ..., x|D|}(x\u1d62 \u2208 V), our goal is to transmit D from A to B token-by-token as accurately as possible. During transmission, both parties can share an encoding and decoding function f. In this section, we will demonstrate that the intelligence level of function f is related to its ability to compress D."}, {"title": "2.1.1. BASELINE: NON-INTELLIGENT TRANSMISSION", "content": "Assuming we have transmitted D\u209c = {x\u2081:\u209c}(t < |D|) and are about to transmit x\u209c\u208a\u2081, for a non-intelligent f\u2080, according to information theory (Shannon, 1948), the length of the code z\u209c\u208a\u2081\u1da0\u2070 = f\u2080(x\u209c\u208a\u2081|x\u2081:\u209c) is at least \u2013 log P(x\u209c\u208a\u2081|x\u2081:\u209c) (Eq. 1), which is its self-information.\n|z\u209c\u208a\u2081\u1da0\u2070| = I(x\u209c\u208a\u2081|x\u2081:\u209c) = \u2212 log P(x\u209c\u208a\u2081|x\u2081:\u209c), (1)\nwhere the initial condition is P(x\u2081|x\u2080) = P(x\u2081).\nThus, the total cost required by this method is as indicated by I(D) (Eq. 2)."}, {"title": "2.1.2. INTELLIGENT TRANSMISSION BASED ON COMPRESSION", "content": "When auto-regressive models, such as large language models (LLMs), are applied to predict the next token, at each iteration, they input x\u2081:\u209c and predict P(x\u209c\u208a\u2081|x\u2081:\u209c, f\u2090). So the total cost required by this method to transmit D is at least I(D|f\u2090) (Eq. 3)."}, {"title": "2.2. A Derivation of the First Law of Information Capability", "content": "If we conduct a meticulous examination of Eq. 2 and Eq. 4, we may uncover an even more intriguing insight. That is, when f\u2090 is sufficiently powerful, I(D|f\u2090) is absolutely likely to be smaller than I(D). Does this imply that some information disappears into thin air (Eq. 5)?\nIn fact, from the perspective of physics, information is conserved (Hawking, 2014), these pieces of information do not vanish; rather, they are transferred into the model f\u2090 (Eq. 6)."}, {"title": "2.3. A Dynamic Perspective to the Process of Intelligence Emergence", "content": "Currently, Eq. 8 remains static. In particular, within our derivation, D represents the total number of tokens in the dataset, H denotes the overall entropy of the dataset, and L is the overall average loss after training has concluded. From the perspective of the law of conservation of information, it is imperative that conservation be observed not only at the terminal state, but throughout the entirety of the dynamic training process. Therefore, we will restate the meanings of the variables in Eq. 8:\n\u2022 H: the overall entropy of the dataset, is a constant.\n\u2022 N: parameter size of the model, measured in bits. Once the model architecture is established, it will become a fixed constant.\n\u2022 D: the token numbers that has been trained. This is a variable that monotonically increases as training progresses.\n\u2022 \u03b7, L: \u03b7 is the information capacity of the model, and L is the dynamic average cross-entropy loss. Both change dynamically as D increases.\nIt is therefore possible to describe the entirety of the model training process from a dynamic perspective based on IC-1:\nInitial State. Training has not yet begun, with no information transfer, thus \u03b7 = 0.\nTraining State. As training progresses, D gradually increases, leading to a decrease in L. To satisfy the equation, \u03b7 must inevitably increase. During this dynamic process, the information gradually transfers to the f\u2090, prompting the model to learn.\nTerminal State. When \u03b7 = \u03b7\u2098\u2090\u2093 (determined by the model architecture), the information that the model parameters can store reaches saturation. At this point, continuing the training process does not enable the model to learn more tokens; L converges, and training concludes."}, {"title": "3. The Second Law of Information Capacity: the Energy Relationship in NTP", "content": "In Sec. 2.2, the IC-1 indicates that the learning process in auto-regressive models is fundamentally an information transfer process. The driving force behind this transfer comes from the back-propagation (Rumelhart et al., 1986) algorithm, while the energy is sourced from the power of the physical world. One might wonder, what is the minimum amount of energy required to complete this information transfer process?\nIn 1961, Landauer proposed that the energy required to erase a single bit is at least kBT ln 2, known as the Landauer's Principle (Landauer, 1961). Therefore, according to Eq. 8, when we transfer information I(f\u2090\u209c), at least energy E\u2080 = I(f\u2090\u209c)kBT ln 2 must be consumed. Thus, the training process of auto-regressive models establishes an energy relationship with the physical world. We can derive the Second Law of Information Capacity (IC-2) (Eq. 9)."}, {"title": "4. Corollaries to Guide Practice", "content": ""}, {"title": "4.1. Estimating the Entropy of the Dataset", "content": "From Sec. 2.3, it can be inferred that when the model training process is in its initial state, \u03b7 \u2248 0. From this, we can derive the following corollary: the Data Entropy Estimation Theorem (Corollary 4.1).\nCorollary 4.1. The entropy of the dataset is approximately equal to the initial loss of the model training."}, {"title": "4.2. Estimating the Quality of the Dataset", "content": "When the number of tokens in the dataset is fixed, a higher entropy of the dataset indicates that it can provide more information for the model to learn. Therefore, we can consider entropy H as a metric for evaluating dataset quality, with the assessment method outlined in Corollary 4.1."}, {"title": "4.3. Matching Suitable Model Size with Dataset Size", "content": "In the practice of large model production, determining the appropriate model size, the amount of training data required, and the duration of training is a critical issue. In previous work, the Knowledge Capacity Scaling Laws indicated that current large language models (LLMs) generally store only 2 bits of information per parameter (Allen-Zhu & Li, 2024). For models using the FP16/BF16 or INT8 formats to store parameters, the information capacity \u03b7 is approximately 0.125 ~ 0.25. Additionally, according to the technical reports of well-known large models (Kaplan et al., 2020), it has been inferred that the entropy of datasets typically ranges from 10 to 15 approximately. Thus, if N and L are given, the required dataset size D can be estimated; if D and L are given, the required model size can be estimated.\nSec. 5.1 indicates that the IC-1 is compatible with the Scaling Law proposed by the OpenAI team in 2020 (Kaplan et al., 2020). Their advantage lies in providing power-law relationships between L and N, L and D, as well as L and C (computation cost) respectively through extensive experiments. However, these power-law relationships are validated only for large models based on the Transformer architecture. From the IC-1, we cannot derive the relationships between L and N or D respectively; we can only calculate their corresponding values from a macro perspective. Nevertheless, the IC-1 is applicable to all auto-regressive models, regardless of architecture, because its underlying principle is based on the physical principle of information conservation. Therefore, we are compatible and complementary with the OpenAI's Scaling Law."}, {"title": "4.4. Identify the Energy Limits Required for Training Auto-regressive Models", "content": "As chip technology advances and autore-gressive model learning algorithms are optimized, the energy required to train models achieving similar intelligence levels is expected to decline. Additionally, future developments may lead to a shift from GPUs to quantum computers, which could further reduce energy consumption. Nevertheless, regardless of technological advancement, there is an inherent limitation to the amount of energy that can be consumed for the transmission of information, known as Landauer's Limit, which is represented by IC-2 (Eq. 9)."}, {"title": "5. Consistency with Existing Theories", "content": ""}, {"title": "5.1. Consistency with the Scaling Law of Neural Language Models", "content": "In this section, we will substitute the experimental data from (Kaplan et al., 2020) into the IC-1 to verify the consistency between the IC-1 and the Scaling Law of Neural Language Models. According to Sec. 2.3, we can know that the H is a constant.\nWhen L is fixed, it indicates that model training has stopped. At this point, for a given set of N and D-that is, for a specific model trained on a defined number of tokens-\u03b7 becomes a constant. Consequently, (H \u2013 L)/\u03b7 is also a constant, resulting in N being proportional to D.\nWhen D is fixed, it implies that the same number of tokens is being trained. If the model structure is altered to increase N, the change in the information capacity \u03b7 cannot be directly determined. Consequently, the relationship between L and N cannot be directly established (Eq. 12).\nWhen N is fixed, it implies that the model is deterministic. If D is increased, meaning that the model is trained on a larger number of tokens, \u03b7 will also increase correspondingly. However, the relationship between the change rate of the two cannot be directly determined, and thus the relationship between L and D also cannot be established directly (Eq. 12).\nIn summary, to verify whether the IC-1 aligns with the empirical formula measured by the OpenAI team in 2020, we can approach the analysis from the perspective of fixed L. Here is the empirical formula (Eq. 13 and Eq. 14):"}, {"title": "5.2. Consistency with the Knowledge Capacity Scaling Laws", "content": "As noted in Sec. 4.3, the information capacity of LLMs in the Knowledge Capacity Scaling Laws should generally be less than 0.125 ~ 0.25 (Allen-Zhu & Li, 2024), which is consistent with the value of [0.115, 0.268] in Sec. 5.1."}, {"title": "6. Conclusion", "content": "This paper has revealed the fundamental physical principles that underpin NTP by establishing the IC-1 and the IC-2. These laws elucidate not only the conservation of information and the energy requirements in auto-regressive model training, but also offer practical corollaries that can guide the development and training of intelligent systems. By aligning our findings with existing theories, we have demonstrated the broad applicability and significance of our theoretical framework, thereby paving the way for more efficient and sustainable advancements in artificial intelligence."}], "equations": ["|z_{t+1}^{f_0}| = I(x_{t+1}|x_{1:t}) = - \\log P(x_{t+1}|x_{1:t}), (1)", "I(D) = \\sum_{t=1}^{|D|-1} = \\sum_{t=0}^{|D|-1} - \\log P(x_{t+1}|x_{1:t}) = |D|H(D). (2)", "I(D|f_a) = \\sum_{t=0}^{|D|-1} - \\log P(x_{t+1}|x_{1:t}, f_a). (3)", "l(f_a) = \\frac{1}{|D|} \\sum_{t=0}^{|D|-1} - \\log P(x_{t+1}|x_{1:t}, f_a) = \\frac{1}{|D|}I(D|f_a). (4)", "I(D) - I(D|f_a) = ? (5)", "I(f_a^t) = I(D) - I(D|f_a), (6)", "\\eta = \\frac{I(f_a^t)}{N}. (7)", "\\eta N = D(H - L). (8)", "E_0 = \\eta N(k_B T \\ln 2), (9)", "\\eta N = D(H - L), D > 0, \\eta \\approx 0, \\Rightarrow H \\approx L. (10)", "N = kD, k = \\frac{H-L}{\\eta}; N \\propto D. (11)", "L = H - \\eta \\frac{N}{D}. (12)", "L = (5.4 x 10^{13}) (\\frac{N}{16})^{-0.095}. (13)", "L = (8.8 x 10^{13}) (\\frac{N}{16})^{-0.076}, (14)", "(\\frac{5.4 x 10^{13}}{D})^{0.095} = (\\frac{8.8 x 10^{13}}{N})^{0.076}", "(\\frac{5.4 x 10^{13}}{D})^{1} = (\\frac{8.8 x 10^{13}}{N})^{\\frac{0.076}{0.095}}", "(\\frac{5.4 x 10^{13}}{D})^{1} = (\\frac{8.8 x 10^{13}}{N})^{\\frac{4}{5}}", "N \\approx 26.08 D.", "\\eta = \\frac{H - L}{N} = k \\frac{[10 - 7]}{26.08} - \\frac{[10 - 3]}{26.08} = [0.115, 0.268]. (11)"]}