{"title": "QUERY-EFFICIENT PLANNING WITH LANGUAGE MODELS", "authors": ["Gonzalo Gonzalez-Pumariega", "Wayne Chen", "Kushal Kedia", "Sanjiban Choudhury"], "abstract": "Planning in complex environments requires an agent to efficiently query a world model to find a feasible sequence of actions from start to goal. Recent work has shown that Large Language Models (LLMs), with their rich prior knowledge and reasoning capabilities, can potentially help with planning by searching over promising states and adapting to feedback from the world. In this paper, we propose and study two fundamentally competing frameworks that leverage LLMs for query-efficient planning. The first uses LLMs as a heuristic within a search-based planner to select promising nodes to expand and propose promising actions. The second uses LLMs as a generative planner to propose an entire sequence of actions from start to goal, query a world model, and adapt based on feedback. We show that while both approaches improve upon comparable baselines, using an LLM as a generative planner results in significantly fewer interactions. Our key finding is that the LLM as a planner can more rapidly adapt its planning strategies based on immediate feedback than LLM as a heuristic. We present evaluations and ablations on Robotouille and PDDL planning benchmarks and discuss connections to existing theory on query-efficient planning algorithms. Code is available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Planning is the process of determining a sequence of feasible or optimal actions that guide an agent from an initial state to a desired goal state (LaValle, 2006). Planning assumes access to a world model, enabling the agent to simulate and evaluate potential actions without relying on trial-and-error in the real environment. However, in many domains, such as robot task and motion planning, querying the world model is the most computationally expensive step (Kaelbling & Lozano-P\u00e9rez, 2013; Garrett et al., 2021). For instance, each query involves running physics or geometric computations or even running a local optimizer. Consequently, planning algorithms must judiciously query the world model, relying on learning-based approaches to efficiently infer the most promising paths with minimal queries (Choudhury et al., 2018; Ichter et al., 2017; Khodeir et al., 2023).\nLarge language models (LLMs), trained on Internet-scale data, offer multiple opportunities to enable query-efficient planning. Notably, LLMs come with key capabilities such as (1) powerful priors to identify promising states that make progress toward the goal (Ahn et al., 2022), (2) tractable posteriors by easily conditioning on feedback to adaptively choose actions (Lee et al., 2023), and (3) generating complex sequences of actions to plan to the goal (Janner et al., 2021). Recent works leverage one or more such capabilities to design LLM-based agents that solve various decision-making tasks (Yao et al., 2022; Shinn et al., 2023b; Huang et al., 2022b; Zhao et al., 2023). However, we show that naively extending such LLM agents to the planning setting becomes quickly intractable. It must not only select among all possible state-action queries but condition on the history of all queries and observations.\nInstead, one tractable way is to use a LLM as a heuristic within an existing planner. Heuristics guide a search tree from start to goal by selecting promising nodes to expand (Bonet, 2001; Pearl, 1984). The planner provides the LLM with a restrictive set of nodes to choose from, making the problem more tractable for the LLM. This is the defacto approach that several recent works adopt to design"}, {"title": "2 PROBLEM FORMULATION", "content": "We are interested in planning problems where querying the environment world model is computationally expensive or resource intensive. This is a common assumption in many applications, especially in robotics. Planning robot motion in high-dimensional configuration spaces requires queries to computationally expensive collision checks (Hauser, 2015; Dellin & Srinivasa, 2016b; Mandalika et al., 2019; Hou et al., 2020). In the task and motion planning (TAMP) domain (Kaelbling &\nLozano-P\u00e9rez, 2013; Ding et al., 2023; Lozano-P\u00e9rez & Kaelbling, 2014; Srivastava et al., 2014;\nToussaint et al., 2018), each query is a high-level action proposed by a task planner, and the world model invokes an expensive motion planning subroutine to generate the next state. Traditional TAMP planners can take up to minutes to solve complex TAMP problems (Lin et al., 2023). Hence, real-time planning involves strategically selecting queries that minimize the total number of queries to the world model to find a feasible plan.\nQuery-Efficient Planning as Sequential Decision Making. Consider an agent operating within a known state space, $\\mathcal{S}$, and action space, $\\mathcal{A}$. We assume the existence of a deterministic world model, $M: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}'$, which maps a state-action pair to the subsequent state in the world. The goal of the planner is to find a sequence of actions that joins the initial state $s_0$ and the goal state $s_g$, i.e.,\n$\\left\\{s_{0}, a_{0}, s_{1}, a_{1}, \\ldots, s_{g}\\right\\}$, where each transition $s_{i+1}=M\\left(s_{i}, a_{i}\\right)$ has been verified to be feasible by the world model. We can formulate the problem of query-efficient planning in this setting as one of sequential decision-making. At each decision-making step, the planner queries the world with a state-action pair, $q=\\{s, a\\}$. The world model responds with $r=\\{s', e\\}$ containing the next state $s'$ and an optional error message $e$ if the action is invalid.\nLeveraging LLMs for Query-Efficient Planning. LLM agents have shown promising results in various sequential decision-making problems (Yao et al., 2022; Shinn et al., 2023b; Huang et al., 2022b). LLMs encode a vast array of commonsense priors that can be used to generate plausible plans from the outset. Good queries reveal useful information about the planning problem, which can be incorporated into the LLM agent's context to update its posterior for future decision-making. Formally, we represent the LLM agent's $k^{t h}$ interaction with the world model as a policy $\\pi\\left(q_{k} | \\Phi, H_{k}\\right)$, where $\\Phi$ is the world context that describes the problem domain in natural language, and $H_{k}=\\left\\{q_{1}, r_{1}, q_{2}, r_{2}, \\ldots, q_{k-1}, r_{k-1}\\right\\}$, is the history of the past queries and responses. This formulation allows the LLM to leverage its pre-trained knowledge and update its priors with every interaction with the world model.\nReAct (Yao et al., 2022)-style prompting provides a simple recipe to use LLMs for query-efficient planning. The policy is represented as $\\pi(a_k|\\Phi, H_k, s_k)$, where the last state $s_k$ is the result of the agent's last query to the world model. At every query step, the agent considers its history of interactions to generate reasoning for its decision before taking an action from its current state. However, ReAct policies are susceptible to getting trapped in local optima or cul-de-sacs. In such cases, the agent tries to backtrack from its current state instead of querying from more promising states in its history."}, {"title": "3 APPROACH", "content": "We tackle the problem of query-efficient planning with LLMs by exploring two fundamentally different approaches for interactively querying a world model. The first approach integrates LLMs into a heuristic search framework (Bonet, 2001; Pearl, 1984). Rather than using the entire interaction history to determine the next query, the LLM serves as a heuristic within a higher-level search algorithm, ranking the most promising states in the search tree and guiding the selection of optimal actions. The second approach employs the framework of lazy search (Dellin & Srinivasa, 2016a), utilizing the LLM as a generative planner. In this method, the LLM generates an entire action sequence from start to goal based on its current understanding of the environment's world model. After each planning iteration, the LLM updates its internal world model using feedback from the environment. An illustration of both approaches is shown in Fig. 1."}, {"title": "3.1 LLM AS A HEURISTIC: TREE OF INTERACTION (TOI)", "content": "Tree of Interaction (TOI) utilizes LLMs as a heuristic within an external planner. The planner maintains a search tree and invokes the LLM to choose which states to expand and what actions to propose. By judiciously choosing which states to expand, the LLM minimizes unnecessary queries to the world model to guide the search tree towards the goal. We build on prior work Tree of Thought (Yao et al., 2024), by incorporating queries to an external world model during the expansions phase. At a high level, the LLM is used to define two modules: Action Proposal and State Evaluation.\nAction Proposal $\\pi_{\\theta}(\\tilde{\\mathcal{A}} | s, \\phi, k)$. This module proposes diverse actions to expand promising new states. An LLM is prompted to generate an action set, $\\tilde{\\mathcal{A}}$, with k actions from state s."}, {"title": "3.2 LLM AS A GENERATIVE PLANNER: BOOMERANG", "content": "We previously discussed that the gold-standard algorithm, ReAct-Select (Sec. 2), is intractable as it requires conditioning on a large context containing the entire history of prior world model interactions while choosing queries from an enormous decision space. We propose a more tractable generative planner, Boomerang, which uses an LLM to propose an entire sequence of actions from the start to the goal state and receives feedback on the entire sequence at once. Alg. 2 provides an overview of Boomerang. Key components are:\nPlanning with an Internal World Model. Equipped with context of the problem description and history of interactions with the world model, LLM agents can be prompted with Chain-of-Thought (Wei et al., 2022) techniques to build an internal world model of the problem domain. The agent uses this internal world model to reason and generate a plan $\\Pi$, a sequence of actions that attempt to reach the goal from the start state. In every iteration, the generated plan receives feedback from the true world model, which is used by the LLM agent to propose new plans.\nUpdating Internal World Model with Feedback. At every iteration, the generated plan is validated by the true world model by rolling out actions from the start state using its generated plan. We re-use any queries made to the world model in previous iterations during this verification. If the true world model verifies that the plan reaches the goal state, the algorithm terminates and returns the verified trajectory. Otherwise, the true world model responds with a partial trajectory $\\xi$ to the goal with an error message $e$ at some action in the plan. The partial trajectory and error message are appended into the LLM's prompt context, updating its internal world model for future iterations of planning.\nWe also note the connection of Boomerang with the framework of lazy search (Dellin et al., 2016) in motion planning in Appendix A.2. We derive a Bayesian regret bound that is sub-linear with the planning iterations needed by Boomerang before it returns a feasible solution. The core principle of laziness is to query edges that belong to promising paths to the goal state, thus minimizing queries to the world model to find either the shortest path (Dellin et al., 2016), a feasible path (Choudhury et al., 2017) or anytime path (Hou et al., 2020). Concretely, we can view the LLM as the policy $\\pi(\\xi | \\phi_{k})$, sampling plans $\\xi$ from the posterior $P(\\Phi_{k} | \\Phi_{prior}, H_{k})$ using feedback from the world model. Posterior sampling is a provable way (Hou et al., 2020) to tradeoff exploration and exploitation. While these classical works rely on discretization approaches to construct the posterior, we leverage the flexibility of the LLM in approximating posteriors. We conjecture that the empirical success of Boomerang is explained by this close connection."}, {"title": "4 EXPERIMENTS", "content": "We evaluate both classical and LLM planners across a variety of fundamental planning problems. First, we assess all methods on the Blocksworld benchmark from PlanBench (Valmeekam et al., 2024), which consists of 600 block-rearrangement problems described in PDDL. Blocksworld has long been a classic AI planning benchmark and is now the de facto standard"}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "for evaluating LLMs' commonsense reasoning abilities in planning tasks. In addition, we create 100 planning problems in the Logistics and Grippers PDDL environments. We use PDDLGym (Silver & Chitnis, 2020) to interact with these environments, serving as the world model oracles. Beyond these classic PDDL environments, we introduce 100 planning problems in the realistic robot cooking simulator, Robotouille (Wang et al., 2023), which poses unique challenges due to complex task dependencies, including time delays and task multi-threading. Since Robotouille is not described in PDDL, we use the simulator itself as the world model oracle. We note that while these planning problems do not include computationally expensive world models, query efficiency directly correlates with time spent in such environments. Therefore, query-efficient planners would maintain similar trends in reducing computation time.\nMetrics. We evaluate the efficiency of the planners in solving planning problems under a fixed budget of World Model Queries (WMQs). A success is defined as a planner finding a feasible path to the goal without exceeding the query budget. We also measure the average number of queries each method makes to the world model across all problems. Additionally, we introduce an optimality metric, which indicates whether a planner finds an optimal path within the WMQ budget. Beyond these planning metrics, we report the number of LLM API calls and input tokens used by the LLM-based methods to provide insights into the cost and runtime of the experiments\nBaselines. We test a range of LLM planner approaches in our experiments. In the simplest case, we evaluate two non-interactive direct input-output methods that do not involve back-and-forth communication with the world model. I/O (Huang et al., 2022a) takes a problem description and an in-context demonstration as input, then generates a sequence of actions. I/O + COT (Wei et al., 2022) builds on this by incorporating a chain of thought component. Interactive LLM planners are divided into two categories. The first is heuristic planners, including TOI-DFS and TOI-BFS. These planners embed the LLM as a heuristic within a higher-level classical search algorithm. We also evaluate generative planners that generate action sequences while interacting with the world model. ReAct (Yao et al., 2022) generates an action sequence by repeatedly outputting a single action and adapting its strategy based on immediate feedback. Boomerang generates entire action sequences toward the goal before receiving feedback from the world model. For all PDDL environments, we also run state-of-the-art classical PDDL planners using the FastDownward system (Helmert, 2006). We conduct a hyperparameter sweep across multiple classical planner configurations and report the best-performing results as Classical (more details in A.10)."}, {"title": "4.2 RESULTS AND ANALYSIS", "content": "Boomerang achieves 78% efficient success on 600 Blocksworld problems from PlanBench compared to Classical with 63% and ReAct with 52%. (Figure 2 and Sec 4.2.2).\n\u2022 Boomerang achieves 82%, 89% and 57% efficient success on Logistics, Grippers, and Robotouille respectively compared to TOI-DFS with 4%, 31%, and 17% respectively and Classical with 5% and 13% on Logistics and Grippers. (Table 5 and Sec 4.2.2).\n\u2022 Boomerang can overcome cul-de-sacs while ReAct and ReAct-Select struggle. See\nSec 4.2.3.\n\u2022 I/O + Pand I/O + CoT + P surpass ReAct by 12.3% and 14.8% respectively after simple\nprompt changes. See Sec 4.2.4."}, {"title": "4.2.2 COMPARISON OF QUERY-EFFICIENCY, SOLUTION QUALITY, AND TOKEN USAGE", "content": "Question 1. How query-efficient are the various approaches on the PlanBench dataset?\nFig. 2 shows the overall success rates of all algorithms on 600 PlanBench problems with world model queries capped at 20. Fig. 3 shows a histogram of queries for all interactive planning approaches.\nAmong the LLM-based approaches, Boomerang has the highest success rate (0.78) and the lowest mean queries 12.15. ReAct, has a success rate of 0.52 with 13.06 mean queries; we scarcely observe successes for higher world model queries because ReAct tends to fail on longer horizon problems as it gets suck in cul-de-sacs and cycles between states due to misunderstanding the environment. We supplemented ReAct with a \"reset\" mechanism akin to (Shinn et al., 2023a) to alleviate this,"}, {"title": "4.2.3 ANALYSIS OF FAILURE MODES OF APPROACHES", "content": "Question 5. What are failure modes for ReAct and ReAct-Select?\nWe look at a qualitative example where all interactive approaches fail except Boomerang. This instance has an optimal plan length of 12 and involves rearranging a stack of blocks into another stack. ReAct makes quick progress towards the goal (red block at the bottom) in Fig. 6; however, it ends up stuck in a cul-de-sac \u2013 it picks up the red block thinking it can be put underneath other blocks and puts it back down, creating an endless cycle. This is counter-intuitive since ReAct keeps everything in history but in practice we observe that ReAct selectively pays attention to the most recent history and tends to ignore past important signals in favor of newer ones. Similarly, in Fig. 7, ReAct-Select begins its search by selecting the next state, and as the history fills with this reasoning the only state"}, {"title": "4.2.4 ABLATIONS FOR BETTER LLM PLAN GENERATION", "content": "Question 6. How far can we improve non-interactive methods for planning?\nWe break down the success and failure modes\nof I/O and I/O + COT in Fig. 9. We\nmake a simple prompt change to the action\nspace of I/O resulting in I/O + P and ad-\nditionally output the goal repeatedly in I/O\n+ COT + P resulting in I/O + CoT +\nMost notably, I/O + P and I/O + CoT +\nPachieve higher success than ReAct (by\n12.3% and 14.8%). The key reason for this is\nbecause the I/O + P variants have the most\nuseful information immediately available while\nReAct has useful information scattered through\nits exploration history. Similarly, Boomerang\nbenefits from its shorter history since it can im-\nmediately act upon useful information in its next\ndecision; however, some improvements can still\nbe made. Future work investigating how to ex-\ntract the most useful information and keep that\nin context can immensely reduce history length and improve decision-making."}, {"title": "5 RELATED WORKS", "content": "To motivate our approach and differentiate from other works, we present related work that uses LLMs as heuristics, incorporate feedback into LLMs, and uses LLMs for planning in PDDL environments."}, {"title": "6 DISCUSSION", "content": "In this paper, we look at query-efficient planning with language models. We propose two approaches, Tree of Interaction (TOI) where an LLM is used as a heuristic and Boomerang where an LLM is used as a generative planner. We evaluate our approach on PDDL domains and show that Boomerang is more query-efficient than both classical and LLM planning baselines. Key to this is Boomerang adapting the entire plan based on feedback from the world model, which has close ties to known results on posterior sampling for query-efficient planning. Two interesting future directions:\nScaling to longer horizons. As the horizon increases, Boomerang may fail to generate good plans. A path to scaling would be to make it plan with its internal world models using any number of approaches (Yao et al., 2023; Besta et al., 2023; Zhao et al., 2023), query the world model to validate the plan, and iterate with feedback.\nScaling to complex planning problems requiring geometric reasoning. Our end goal is to solve complex task and motion planning problems. While LLMs can reason about semantics but struggle to reason about grids (Lehnert et al., 2024) or geometry (Trinh et al., 2024). An exciting direction of future work is to look at using LLM to plan at a higher semantic level, pass this to a low-level geometric planner to produce actions, and crucially adapt the high-level planner based on failures of the geometric planner to guide it better."}, {"title": "7 LIMITATIONS", "content": "While Boomerang is promising, it is hampered during prolonged searches. We frequently observed the method reproposing failed action sequences during long horizon examples, which suggests a tendency to forget about feedback early on in its context similar to ReAct and ReAct-Select.\nThis issue is amplified in environments with many objects or large action spaces because this increases context length.\nThe TOI-BFS and TOI-DFS methods are also overly dependent on the LLM providing quality rankings of states; erroneous rankings cause states to be unnecessarily explored in TOI-BFS or can send a search on the path to a dead end in TOI-DFS. Additionally, the LLM heuristic itself ranks states independently from one another which tend to make the heuristic inadmissible (failing to guarantee optimal paths) and inconsistent (making it possible to select visited nodes)."}, {"title": "A APPENDIX / SUPPLEMENTAL MATERIAL", "content": ""}, {"title": "A.1 BROADER IMPACT", "content": "Tree of Interaction (TOI) and Boomerang are planning approaches that incorporate external world feedback to boost LLMs' planning abilities. As these approaches improve, they can be applied in unstructured environments like software applications that search the Web or robots that assist us in our homes to improve the quality of our lives. On the other hand, giving full autonomous control to these LLM planners can lead to dangerous actions or advice; it is important to apply additional safety checks on generated plans and for future work to better align LLMs toward safe plans."}, {"title": "A.2 ANALYZING THE PERFORMANCE OF BOOMERANG", "content": "Why is laziness essential for query efficiency? Assume we have a perfect heuristic h*(s, sg). Let's say the heuristic is used by A* search. A* will expand only the vertices corresponding to the optimal path \u00a7*. However, at every vertex expansion step, A* will evaluate every outgoing edge from the vertex, i.e. degree k. Hence, the total number of queries will be k|\u00a7*|, where |\u00a7*| is the number of edges in the optimal path. Contrast this to a lazy shortest path (Dellin & Srinivasa, 2016b), which initializes an internal model where every edge is assumed to be feasible, finds the shortest path in its internal model, and then queries the path to update its model. Initializing the model with the true model would yield the true shortest path in the first iteration \u00a7*, resulting in |\u00a7* | queries. Hence, even with access to perfect information, A* heuristic search is k times more query expensive than lazy shortest path. We refer the reader to (Mandalika et al., 2018) for a more rigorous proof, that shows"}, {"title": "A.3 MODELS AND HYPERPARAMETERS", "content": "All approaches use gpt-4-turbo. The interactive LLM approaches are allowed to make 20 decisions for exploring the environment. TOI-DFS and TOI-BFS both use k = 2 and TOI-BFS uses b = 2. We experimented with alternative values for b, but found little change in success with a fixed query budget: on 10 randomly sampled Blocksworld problems, there were 3, 2, and 2 successes for b values 2, 3, and 5, respectively. For a fixed length action sequence, increasing either parameter entails increasing the number of queries. So while the \u201cbranching factor\" of the search is wider, the search can not traverse enough successive actions to reach the goal. Although we only experimented with varying b values, we believe modifying k entails a similar effect. Finally, all approaches use a temperature of 0.7 and use no in-context examples unless otherwise specified. We empirically observed no difference in performance on Boomerang when additionally supplied with in-context examples as shown in Table 2 below"}, {"title": "A.4 PROMPTS", "content": ""}, {"title": "A.4.1 STATE TRANSLATION PROMPT", "content": "Below is a sample prompt and gpt-4-turbo output for state translation. We find that gpt-4-turbo is able to effectively translate state in the form of PDDL predicates into a natural language form. Our prompt can be flexible across domains: the parts in \"Below is a description of the environment:\" and \"The actions are formatted as follows:\" can easily be swapped out depending on the domain"}, {"title": "A.4.2 GOAL TRANSLATION PROMPT", "content": "Since goals in PDDL are also given in terms of predicates, we also use GPT4 to translate goals into a natural language form. Below is an example from Blocksworld."}, {"title": "A.4.3 BoOMERANG ACTION PROPOSAL PROMPT", "content": "Below is a sample prompt and GPT4 output for the action proposal component of Boomerang. Here, the LLM is tasked with generating a trajectory from the start state to the goal. This builds off the start state and goal translation examples from before (they are provided as input in the action proposal prompt)."}, {"title": "A.4.4 TOI-BFS AND TOI-DFS ACTION PROPOSAL PROMPT", "content": "We again use the state and goal translation examples from before. Below is the action proposal prompt for TOI-BFS and TOI-DFS. The prompt tasks the LLM to pick several actions from the set of available actions (so they may be expanded later)."}, {"title": "A.4.5 TOI-BFS AND TOI-DFS STATE EVALUATION PROMPT", "content": "We follow the some initial state and goal translation examples from before. Below is the state evaluation prompt for TOI-BFS and TOI-DFS. The evaluation is necessary in deciding which states to explore later."}, {"title": "A.5 CONTEXT", "content": "Our prompts are designed to allow for quickly injecting brief context information to align an LLM's priors for the current environment. We emphasize that the context is brief because the interactive approaches can fill in the gaps through interactions with the environment."}, {"title": "A.5.1 BLOCKSWORLD", "content": "The 4-operator version of the classic Blocksworld. This domain \u2192consists of a set of blocks, a table and a robot hand. The \u2192 blocks can be on top of other blocks or on the table; a block that has nothing on it is clear; and the robot hand can hold \u2192one block or be empty. The goal is to find a plan to move from \u2192one configuration of blocks to another.\nThe actions are formatted as follows:\nput-down(x:default) where x is the block to put down\npick-up(x:default) where x is the block to pick up\nstack(x:default,y:default) where x is stacked on top of y\nunstack(x:default,y:default) where x is unstacked from the top of y"}, {"title": "A.5.2 GRIPPERS", "content": "Given a robot with one or more gripper hands, transport a number of balls from their starting rooms to their destination rooms\nExamples of how some actions might be formatted are as follows:\nmove(robot1,room1, room2) to move robot robot1 from room room1 \u2192to room room2\npick(robot1,ball2,room3,gripper3) to have robot robot1 pick up ball ball2 using gripper gripper3 in room room3\ndrop(robotl,ball2,room3,gripper3) to have robot robot1 drop \u2192ball ball2 using gripper gripper3 in room room3"}, {"title": "A.5.3 LOGISTICS", "content": "Transport packages within cities via trucks, and between cities via airplanes. Locations within a city are directly connected (trucks can move between any two such locations), and so are the cities. In each city there is exactly one truck, each city has one location that serves as an airport.\nThe actions are formatted as follows:\ndrive-truck(t0,11-2,13-2,c2) where t0 is a truck driving from \u2192location 11-2 to location 13-2 in city c2\nfly-airplane(a0,11-2,13-4) where a0 is the airplane flying \u2192 from the location 11-2 in city 2 to location 13-4 in city 4\nload-airplane(p0,a1,12-3) where p0 is the package loaded onto \u2192 airplane al at location 12-3 in city 3\nload-truck(p0,t1,12-3) where p0 is the package loaded onto \u2192 truck tl at location 12-3 in city 3\nunload-airplane(p0,a1,12-3) where p0 is the package unloaded \u2192 from airplane al at location 12-3 in city 3\nunload-truck(p0,t1,12-3) where p0 is the package unloaded from truck tl at location 12-3 in city 3"}, {"title": "A.6 TOI-DFS ALGORITHM", "content": "We show TOI-DFS in Alg. 3. The algorithm builds a search tree greedily with a depth-first search and uses the heuristic as a termination condition. Starting from some initial state so, the search attempts to expand states for T iterations until the goal state sg has been expanded. At each iteration, we pop a state off of the DFS queue SQ. Then, the action proposer module is called to generate action-set A of size k for that state. Then, the world model is queried to produce the set of next states St using the popped state and its proposed action set. Finally, the state evaluation module evaluates each state to add to the queue and ignores the states that are below the value threshold Umin."}, {"title": "A.7 TREE OF INTERACTION (TOI) QUALITATIVE FAILURES", "content": "wards the goal because despite having queried vast information, the lack of history in Tree of Interaction (TOI) makes information useful only for expanding the next states."}, {"title": "A.8 PROMPTING ABLATION DETAILS", "content": "To explore the generative power of LLMs, we run ablations on the I/O and I/O + COT prompts to boost their performance. In I/O we found that the LLM misuses the 'pickup' action, intended for block-on-table interaction, in Blocksworld rather than the 'unstack' action, intended for block-on-block interaction, when picking up blocks from other blocks. We addressed this by enhancing 'pickup' and 'putdown' to allow for block-on-block interactions in the approach I/O + P where P stands for prompt engineering. For I/O + COT we additionally found that the LLM benefits from tracking the environment state throughout its action sequence. Though not always accurate, this extra grounding reduced invalid action errors. Finally, we observed cases where the LLM hallucinated a goal midway through its generation (i.e. stating \u201cI have reached the goal since the final state has: the orange block is on top of the blue block\" when the goal was actually to have the orange block on top of the red one. We remedied some of these cases in I/O + CoT + P by instructing the goal condition to be repeated following every action, next state pair.\nWe break down the success and failure modes in Fig. 9. The 'Invalid Actions' failure mode refers to an outputted action sequence that contains an invalid action while 'Search Failure' refers to a valid outputted action sequence that does not reach the goal. Adding in prompt changes makes a significant difference - I/O + Pachieves a 87% performance improvement over I/O and I/O + COT + P achieves an 38% performance improvement over I/O + COT. We note that I/O + P makes 53% fewer invalid action failures than I/O. I/O + COT + P achieves 3.9% better performance than I/O + P but makes 8.8% more invalid action failures. This is attributed to small state prediction errors getting worse throughout a plan."}, {"title": "A.9 DATASET STATISTICS", "content": ""}, {"title": "A.10 COMPARISONS OF CLASSICAL PLANNERS AND BOOMERANG", "content": ""}, {"title": "A.11 TOTAL COSTS OF LLM-BASED APPROACHES ON 600 BLOCKSWORLD EXAMPLES", "content": ""}, {"title": "A.12 REACT, REFLEXION, AND BOOMERANG", "content": "Reflexion (Shinn et al., 2023a) produces one action at a time similar to ReAct; however, similar to Boomerang, it can restart to the start whenever a mistake is made. We use this mechanism to address ReAct cycling states by prompting for feedback when such a cycle is detected. Specifically, we reset to the starting state and provide feedback to the LLM that cycling occurred for it to reflect on its mistake.\nReflexion performed better than ReAct as expected, but underperformed Boomerang. Interestingly, Reflexion tends to cycle after resetting from the start, similar to the failure modes discussed"}]}