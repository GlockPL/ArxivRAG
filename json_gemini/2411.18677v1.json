{"title": "MatchDiffusion: Training-free Generation of Match-Cuts", "authors": ["Alejandro Pardo", "Fabio Pizzati", "Tong Zhang", "Alexander Pondaven", "Philip Torr", "Juan Camilo Perez", "Bernard Ghanem"], "abstract": "Match-cuts are powerful cinematic tools that create seamless transitions between scenes, delivering strong visual and metaphorical connections. However, crafting match-cuts is a challenging, resource-intensive process requiring deliberate artistic planning. In MatchDiffusion, we present the first training-free method for match-cut generation using text-to-video diffusion models. MatchDiffusion leverages a key property of diffusion models: early denoising steps define the scene's broad structure, while later steps add details. Guided by this insight, MatchDiffusion employs \u201cJoint Diffusion\" to initialize generation for two prompts from shared noise, aligning structure and motion. It then applies \"Disjoint Diffusion,\" allowing the videos to diverge and introduce unique details. This approach produces visually coherent videos suited for match-cuts. User studies and metrics demonstrate MatchDiffusion's effectiveness and potential to democratize match-cut creation. Visit our website for video results. Our code is open source.", "sections": [{"title": "1. Introduction", "content": "\"The art challenges the technology, and the technology inspires the art.\" John Lasseter\nCinematic transitions are powerful storytelling tools that evoke emotions, suggest the passage of time, or visually connect themes [29]. Among transitions, match-cuts are particularly effective in seamlessly bridging two scenes with strikingly different content but similar composition, creating a strong sense of connection. This technique, famously employed in Kubrick's \"2001: A Space Odyssey\",\n1 leaps from a bone thrown by an ape to a satellite orbiting Earth-conveying Humanity's evolutionary leap, from primitive tools to space technology, without a single word.\nDespite their visual elegance and narrative power, match-cuts are notoriously difficult to create. They require careful planning and precise visual alignment, often shaping the entire production process to ensure a seamless transition [1, 31, 35, 43, 44]. This complexity limits match-cuts to experienced filmmakers with substantial resources, making them rare cinematic gems. Our aim is to democratize this powerful tool by providing a simple method, that allows creators of various skill levels to experiment with match-cuts, helping both amateurs and experienced filmmakers to quickly iterate and refine ideas before full-scale production.\nMatch-cuts require scenes with disconnected semantic content to share broad structural and motion characteristics. We use this property of match-cuts to model their generation as the synthesis of a pair of videos that share structural coherence but differ in semantics.\nTo generate such a pair of videos, we harness an empirical property observed in text-to-video diffusion models. In particular, previous works [6, 23, 36] observed that these models synthesize scenes by establishing broad structural features in the early denoising steps, with finer details emerging in later steps. Motivated by this property, we propose MatchDiffusion, a training-free method for synthesizing match-cuts from two prompts. Our method first performs \"Joint Diffusion\", by initializing the synthesis for both prompts from a single noise sample and then guiding both along a common denoising path for the first denoising steps. This process translates into a cohesive layout and structure being shared between the two videos. After this stage, we then perform Disjoint Diffusion, where we allow the videos' diffusion paths to diverge, as guided by their corresponding prompts. With these processes, MatchDiffusion generates videos that independently exhibit unique content while jointly displaying visual coherence established in the early stages-resulting in distinct yet harmonized scenes suitable for a match-cut. Please refer to Fig. 1 for an overview of our approach.\nTo thoroughly evaluate our diffusion-based approach to synthesizing match-cuts, we implement intuitive baselines using existing methods (e.g. [28, 47, 51]). We selected each of these methods for its potential to effectively"}, {"title": "2. Related works", "content": "Conditional video synthesis. With large-scale training, introducing conditional control over video diffusion models has become fundamental. Most approaches include textual control [3, 4, 18, 27, 45], also allowing often for single image control, or targeting animation of existing elements [9, 25, 30, 38, 52].\nVideo-based control, though, is arguably the closest to our task. The video-to-video translation approaches [8, 20, 28, 49] edit a video's semantics while preserving rigid structures. Differently, motion transfer approaches [13, 47, 51] allow for disentanglement of motion only, irregardless of structure. Other works finetune the model to isolate motion [54]. None of these approaches allow for balancing structural preservation and semantic flexibility, which is essential for match-cuts.\nMatch-cut synthesis. Cutting in video editing has been widely explored. Some focus on detecting cut points in untrimmed videos using audio-visual cues [32], audio-beat alignment [34], or transitions for dialogue scenes [19], without differentiating types of transitions. Shen et al. [41] propose smooth transitions such as fades, and panes, excluding straight cuts. Pardo et al. [33] offer a dataset for straight-cut classification, with match-cuts as one category, though underrepresented. Recently, retrieval-based approaches addressed match-cut creation: one curating candidates via audio-visual features [7], the other focusing on audio-based match-cuts [10]. These studies tackle match-cut synthesis through retrieval, whereas we propose a generative approach to synthesize video pairs that form a match-cut.\nMuti-scene video generation. Recent works have explored multi-shot video generation. VideoDrafter [26] and VideoDirectorGPT [22] generate multi-scene layouts from scripts derived by large language models (LLMs), while StreamingT2V [14] and DreamFactory [48] focus on ensuring temporal coherence and reducing hallucinations between frames. TALC [2] improves temporal alignment with time-aligned captions, and Contrastive Sequential-Diffusion Learning [37] enhances visual coherence in"}, {"title": "3. MatchDiffusion", "content": "Given two prompts (p', p\") describing different scenes, our goal is to generate a pair of videos (x', x\") that align with their respective prompts while remaining visually cohesive for match-cut transitions. Each video is generated independently, making it possible to combine them seamlessly in a match-cut, for instance, by joining the first half of x' with the second half of x\". We rely on a key property of diffusion models to achieve these transitions: as highlighted in prior works [6, 23, 36] and illustrated in Figure 2, diffusion models establish broad structural and color patterns in the early denoising stages, while finer details and prompt-specific textures emerge later. By leveraging this progression, we design MatchDiffusion, a two-stage training-free pipeline tailored for match-cut generation. MatchDiffusion comprises: (1) Joint Diffusion (Section 3.2), where we set up a shared visual structure based on both prompts, followed by (2) Disjoint Diffusion (Section 3.3), where each video independently develops the semantics corresponding to its prompt. In the following sections, we introduce preliminaries, then go into detail into each stage of MatchDiffusion, elaborating on how the joint and disjoint diffusion stages provide the balance needed for match-cut generation."}, {"title": "3.1. Preliminaries", "content": "We first introduce the working mechanism of diffusion models for text-to-video (T2V) synthesis. T2V models operate by iteratively denoising Gaussian noise, with the goal of producing a fully denoised video that aligns with a conditioning textual prompt. Recent methods [50] execute this process in a latent space established by a pretrained autoencoder, mitigating computational costs [40]. The autoencoder comprises an encoder & and a decoder D. The latent space of this autoencoder is then iteratively denoised by a noise estimation network $e_\\theta$ over T steps, starting from sampled Gaussian noise $z_T \\sim \\mathcal{N}(0, I)$. We denote the latent video representation at the t-th iteration as $z_t$, where $t \\in \\{0, ..., T\\}$. That is, the network $e_\\theta$ predicts the noise $E_t$ for $z_t$. The network's prediction is conditioned on both the input textual prompt p and the timestep t:\n$E_t = e_\\theta(z_t, p, t)$.\nThis noise prediction is then used to update the noisy latent representation, following scheduling strategies such as DDPM [16] or DDIM [42]. Namely, at step t, the noisy representation $z_t$ is denoised into $z_0^{(t)}$ by combining the estimated noise with the latent representation:\n$z_0^{(t)} = z_t - \\gamma_tE_t$,\nwhere $\\gamma_t$ is a scaling factor function of t. Then, another Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, I)$ sample is used to noise $z_0^{(t)}$ again, following a noise schedule whose intensity decreases over timesteps. Formally:\n$z_{t-1}^{(t)} = \\eta_t z_0^{(t)} + \\sigma_t\\epsilon$,\nwhere $\\eta_t$ and $\\sigma_t$ regulate the noise intensity, and decrease with increasing t [16, 42]. After T timesteps, $z_0$ is decoded via $x = \\mathcal{D}(z_0)$ into the output video x.\nFor the purpose of creating a match-cut, we propose to generate two videos simultaneously by breaking the diffusion process into two stages: a joint stage where the latent representation of the videos is shared, and a disjoint stage where the representations are allowed to diverge. Next, we elaborate on the specifics of each stage."}, {"title": "3.2. Joint Diffusion", "content": "The first stage of MatchDiffusion is Joint Diffusion. During this stage, we simultaneously generate both videos by forcing the synthesis to incorporate both input prompts for the first K denoising iterations, where $K \\in \\{0, ..., T\\}$. After these K iterations, the result is a single latent displaying an abstract structure that broadly satisfies both prompts. Our intuition behind this design builds on previous work on hybrid images [5, 11, 12], showing that the diffusion process can be manipulated to produce images displaying different scenes depending on viewing conditions. However, our scenario is unique, since we require each output, x' and x\", to clearly and independently comply with its own prompt, sharing only selected appearance-related traits. As illustrated in Figure 2, the intermediate denoising outputs $z_0^{(t)}$ reveal motion patterns and the scene layout-the essential elements for match-cuts\u2014-emerge in early stages, while later refinement steps focus on details related to semantic content. As shown in Figure 3 (left), for the first K iterations, we combine noise predictions from each prompt using a function f, ensuring shared foundational characteristics early in synthesis. The joint diffusion process is defined by modifying Equation (1) to:\n$E_t = f(e_\\theta(z_t, p', t), e_\\theta(z_t, p\", t))$,\nwhile maintaining the computation of $z_{t-1}$ as before, i.e. following Eqs. (2) and (3). Although this formulation supports different expressions for f, we choose it to simply be the averaging function, i.e. $f(a, b) = (a+b)/2$."}, {"title": "3.3. Disjoint Diffusion", "content": "After K iterations of Joint Diffusion, we obtain a noisy latent $z_{T-K}$ encoding characteristics that are desirable to preserve in both x' and x\". This second stage of Disjoint Diffusion allows the remaining T \u2013 K steps of the diffusion process to start from this latent but depart from the shared path to introduce the characteristics that are specific to the individual prompts. In particular, Disjoint Diffusion starts from $z_{T-K}$ and finishes denoising via T \u2013 K evaluations of $e_\\theta$, conditioned on one prompt at a time. As such, Disjoint Diffusion produces separate noise predictions $\\epsilon_t'$ and $\\epsilon_t''$, as shown in Figure 3 (right). This procedure ensures that the emergence of semantics and details specific to each prompt occurs while maintaining the structure encoded in the initial K steps. For $t \\in \\{0, ..., T \u2013 K\\}$, this becomes:\n$\\epsilon_t' = e_\\theta(z_t', p', t), \\epsilon_t'' = e_\\theta(z_t'', p'', t)$.\nWhen t = T \u2013 K, both $z_t'$ and $z_t''$ are set to $z_{T-K}$. After the remaining Disjoint Diffusion iterations, we obtain two videos, $x' = \\mathcal{D}(z_0')$ and $x'' = \\mathcal{D}(z_0'')$, which can be combined into a match-cut.\nOne might assume that results of MatchDiffusion resemble those of video-to-video translation based on SDEdit [28], which perform prompt-based editing by injecting noise into an existing video $x_{init}$ from step K onward. However, our approach is fundamentally different, as we jointly synthesize the two scenes, rather than modifying an initial video. That is, MatchDiffusion generates outputs that satisfy both prompts from scratch, effectively narrowing the range of possible appearances to those that align with the shared structure and characteristics specified by both prompts. This process enables the synthesis of match-cuts for semantically uncorrelated scenes, as shown in Fig. 6, where the video-to-video translation approach fails."}, {"title": "User intervention.", "content": "To allow for iterative user editing, we propose a human-in-the-loop strategy for a finer customization of the generated videos. Namely, a user may wish to depart from the strict color adherence of the match-cut to better align with the tone of a preceding sequence, or to modify the background. While this could be achieved with post-processing, we propose a more natural mechanism that integrates user interventions directly into the diffusion process.\nWe define as $\\tau$ a generic user-driven modification, which may be automatic (e.g., a color look-up table) or manual (e.g., adding scene elements). We incorporate T in the denoised video at the start of a disjoint diffusion path, e.g. $x_0^{(K)} = \\mathcal{D}(z_0^{(K)})$, as shown in Fig. 4. By doing so, we obtain an updated video i.e. $x_0^{(K)} = \\tau(x_0^{(K)})$. We then"}, {"title": "4. Experiments", "content": "We introduce our experimental setup in Section 4.1, then provide results of the match-cuts generated by MatchDiffusion in Section 4.2, and afterwards compare against baselines, using qualitative and quantitative evaluations as well as user studies, in Section 4.3. We further report results with potential user interventions in Section 4.4, and we conclude with an ablation analysis of the sensitivity of MatchDiffusion to K in Section 4.5. See video results on our website."}, {"title": "4.1. Setup", "content": "MatchDiffusion settings. For the backbone of MatchDiffusion, we choose the open-source text-to-video (T2V) diffusion model CogVideoX-5B [50], as well as its corresponding encoder & and decoder D models. For sampling, we use a DDIM scheduler [42] with T = 50 steps. For all baselines and our method, we generate videos with 40 frames, and form a match-cut by concatenating the first 20 frames of x' with the last 20 of x\". We tune K for each pair of prompts. Generating one match-cut with MatchDiffusion requires around 7 minutes on an NVIDIA A100.\nBaselines. To the best of our knowledge, we are the first to synthesize match-cuts from scratch. Hence, the definition of suitable baselines is challenging. We define here three strong baselines in our best efforts to define different strategies for training-free match-cut synthesis:\nVideo-to-video. We define a video-to-video (V2V) translation baseline, and note that these approaches are designed for structural consistency. Here, we first use p' to generate a video x' with the T2V version of CogVideoX-5B. Then, we use the V2V version of the same model (based on SDEdit [28]) to inject noise at step K in x', and denoise using p\", obtaining x\".\nMotion Transfer. Recent literature has highlighted the possibility of conditioning the generation of new videos with the motion of an existing video. These motion transfer approaches allow for disentangling the motion from the reference scene content. Compared to V2V, this approach increases the flexibility in the outputs, allowing to significantly depart from the appearance of the reference video. We use a T2V model to generate x' from p', then we use either SMM [51] or MOFT [47] to synthesize a new video with p\" as input, and x' as guidance. For a fair comparison, we reimplemented SMM and MOFT on top of CogVideoX-5B. Hence, all our baselines use the same backbone.\nMetrics. The evaluation of a match-cut is a highly subjective task. However, we propose different metrics to quantify the different aspects of a match-cut. First, we exploit a frame-wise CLIPScore [15] to assess prompt adherence of the generated video. Namely, we average the CLIPScore of x' and p', and x\" and p\" for each frame. This procedure ensures that each video respects its prompt. To evaluate motion agreement between x' and x\", we use the Motion Consistency metric proposed in SMM [51]. In particular, we evaluate the motion consistency of tracklets extracted by a pre-trained tracking model [17]. Finally, we use LPIPS [53] to quantify frame-wise perceptual similarity across x' and x\". Intuitively, a low LPIPS should indicate structurally-consistent outputs, i.e. suitable for match-cuts."}, {"title": "4.2. Results", "content": "We report outputs of MatchDiffusion in Figs. 1, 5, and 6. In Fig. 5, we show a variety of match-cuts generated by our method, highlighting its ability to connect diverse concepts across different scenes. In the first two rows, MatchDiffusion demonstrates capacity to bridge unrelated scenes through background elements. For example, in the lighthouse scene, the beam of light seamlessly transitions into the fog of the adjacent scene, creating a cohesive visual connection. The third row illustrates a color-based match: transitioning from a spice market to a painter's palette by aligning the colors in each scene. The last two rows highlight structural alignment across scenes. In the fourth row, the shape of a bottle transitions into a wooden cabin, exploiting how the liquid's color mirrors the hues of the cabin. The final row connects a highway with an ice-skating scene, aligning the circular highway shape with the ice ring's structure."}, {"title": "4.3. Comparison with baselines", "content": "Qualitative comparison. Fig. 6 displays frames before and after the transition for three different prompts, comparing MatchDiffusion with our proposed baselines. This figure illustrates how each approach handles various cases of match-cuts. As seen in the first column, V2V tends to produce similar-looking scenes across prompts. This result is expected, as these methods are primarily designed to translate features within scenes that already share visual similarities (e.g., changing the season from summer to winter). When faced with highly dissimilar prompts, V2V typically alters minor aspects of the scene, which fall short of achieving the strong semantic shifts needed for a high-quality match-cut. For example, in the first row, the burning parchment merely becomes more rounded in the subsequent frame. Instead, motion transfer methods, such as SMM and MOFT, yield results aligned with the prompts, preserving movement across frames. However, in the same example, we observe that SMM and MOFT depart significantly from the appearance of the original image, preventing the structural alignment present in match-cuts. Finally, MatchDiffusion achieves smoother and cohesive transitions by aligning both structure and motion across scenes. In the first row, the burning flame seamlessly becomes the sunrise reflection, creating a visually appealing transition that aligns well with the match-cut effect.\nMetrics evaluation. We now compare with baselines quantitatively. We include an additional lower-bound baseline,"}, {"title": "4.4. Evaluating user interventions", "content": "We now evaluate our optional user intervention strategy (Section 3.3). We want to test if MatchDiffusion can relax strict color/structure adherence while still generating match-cuts. We evaluate three 7 functions applied to x(K): (1) color jittering, (2) histogram matching with random images from COCO [24], and (3) gamma correction. Ideally, T should adapt to the Disjoint Diffusion, preserving the final realism and scene structure. We report results in Fig. 8a. We first display samples generated by MatchDiffusion and post-processing results, i.e. applying each with random parameters on x. This procedure yields exaggerated and thus unrealistic colors. We apply 1 to $x_0^{(10)}$ (the \"Ours\u201d column), and successfully apply naive transformations while maintaining realism. This is exemplified, for instance, by the blue shift in the ice ring (first row), background and leaf color changes (second row), and darker tone (third row). Despite minor structure shifts (e.g., leaf shape), the scene composition remains intact, suitable for match-cuts.\nWe also quantify the impact of modifications on realism. We randomize T's parameters five times and apply it to 36"}, {"title": "4.5. Impact of K", "content": "We investigate the impact of the number of Joint Diffusion steps (K) on MatchDiffusion. Fig. 9 visualizes the impact of K on metrics. While most results presented in Figure 5 have K between 10 and 15, we notice that although CLIPScore decreases, Motion Fidelity and LPIPS monotonically improve. This fact deserves ad hoc considerations. The case of K = 0 is equivalent to the lower bound (i.e. no shared structure), while K = 50 means that x' and x\" share all the diffusion process (similar to Factorized Diffusion [11]), and hence x' = x\". In this case, MatchDiffusion produces a hybrid video (as shown in supplementary). This property is not useful for match-cuts but might enable other applications. Ultimately, we find that, for the purpose of match-cut generation, the user's needs play a central role, with K serving as a tunable parameter to adjust the results according to artistic preferences."}, {"title": "5. Conclusions and Limitations", "content": "In this paper, we presented MatchDiffusion, the first automatic method for the synthesis of match-cuts. We formalized the match-cut generation problem as a synthesis of two videos, and consequently proposed a methodology that exploits emerging characteristics of diffusion models to perform the match-cut generation. MatchDiffusion has limitations that suggest future research directions. Effective prompting requires substantial creativity and intuition, and automated prompt engineering could make the method more accessible to a broader audience. Additionally, refining conditioning mechanisms to give users control over specific aspects of the match-cut generation could further simplify interaction and reduce reliance on precise prompts. Given the limited data available for training on match-cuts, fine-tuning diffusion models specifically for this task-perhaps through transfer learning-could enhance performance and broaden the method's applicability."}, {"title": "A. Additional Analysis.", "content": "Effect of classifier-free guidance. We analyze here the effect of the CFG (classifier-free guidance) parameter when making match-cuts with MatchDiffusion. Here we fix the K and analyze the different metrics when varying CFG. In Figure A10, we observe that larger CFGs tend to drop the CLIPScore but also make the entanglement (Motion) of motion and structure (LPIPS) to be stronger. Similar to the K, parameter there is a sweet spot in which Motion and Structure and shared across the two videos, while still following the prompt. We found that a CFG between 5 to 7 works well for the majority of the cases. In rare occasions, we found CFG = 10 also performing well for specific prompts.\nDifferent combination function f In Section 3.2 we defined f as the average of the estimates from the two paths. However, one could try a different strategy to combine the two path estimates. In Figure A11 we show the results but this time combining the two paths by linearly decaying the weight of one another until making them independent. This would change the previous approach of the combination of the two paths from a step function to a simple linear decay. The results, show that variations of K (diffusion step in which the decay starts) yield more motion-entangled results, quantified by the higher values in motion fidelity (middle plot). We advocate anyways that having more flexibility in the motion (hence with lower motion fidelity) allows to generate more variable videos, assuming outputs respecting the definition of a match-cut. Hence, we still selected averaging as our f of choice, to allow users to tune better the amount of motion in common between x' and x\"."}, {"title": "Sampling", "content": "We show results of different match-cuts produced for the same prompt and the same parameters, by just sampling with different seeds. We observe that sampling from the method can help at creating different interpretations of the same matching concepts. We show sampling from our method in Figures A12, A13, A14, and A15."}, {"title": "B. Limitations", "content": "A key limitation of our method lies in its reliance on prompt quality and creative input. While the system can generate visually appealing match-cuts, achieving truly compelling results often depends on carefully crafted prompts and sampling. We found that prompts inspired by existing match-cuts-such as those from iconic film scenes or curated blog posts-significantly improve the system's success rate, whereas randomly devised prompts frequently fail. This underscores that the creative process heavily relies on human ingenuity to guide the system. Currently, the system autonomously determines key aspects of the match cut, including structure, color, layout, and motion. Future work could focus on providing users with finer control over these elements, enabling a more deliberate and customized match cut generation process."}, {"title": "C. Application on images", "content": "Although our paper focuses on match-cuts, we also found that by using an Image-Diffusion model like Stable Diffusion 1.5 [39], we can create couples of images that also"}]}