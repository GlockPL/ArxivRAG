{"title": "Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning", "authors": ["Rashmeet Kaur Nayyar", "Siddharth Srivastava"], "abstract": "Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action representations to enable transfer and generalization remains a challenging open problem. This paper presents a novel approach for inventing, representing, and utilizing options, which represent temporally extended behaviors, in continual RL settings. Our approach addresses streams of stochastic problems characterized by long horizons, sparse rewards, and unknown transition and reward functions.\nOur approach continually learns and maintains an interpretable state abstraction, and uses it to invent high-level options with abstract symbolic representations. These options meet three key desiderata: (1) composability for solving tasks effectively with lookahead planning, (2) reusability across problem instances for minimizing the need for relearning, and (3) mutual independence for reducing interference among options. Our main contributions are approaches for continually learning transferable, generalizable options with symbolic representations, and for integrating search techniques with RL to efficiently plan over these learned options to solve new problems. Empirical results demonstrate that the resulting approach effectively learns and transfers abstract knowledge across problem instances, achieving superior sample efficiency compared to state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) for enabling autonomous decision-making has been constrained by two fundamental challenges: sample inefficiency and poor scalability, particularly in environments with long horizons and sparse rewards. To address these limitations, researchers have focused on reducing the problem complexity through: (1) state abstraction, which creates compact state representations (Jong and Stone 2005; Dadvar, Nayyar, and Srivastava 2023), and (2) temporal abstraction, which captures hierarchical task structures through temporally extended behaviors (Barto and Mahadevan 2003; Pateria et al. 2021), such as options (Sutton, Precup, and Singh 1999). Abstraction-based methodologies offer principled approaches for knowledge transfer across tasks (Abel et al. 2018), especially in the challenging setting of continual learning (Liu, Xiao, and Stone 2021; Khetarpal et al. 2022), where agents must interact with and solve tasks indefinitely. However, most existing research on option discovery in RL focuses either on continuous control tasks with short horizons and dense rewards (Bagaria, Senthil, and Konidaris 2021; Klissarov and Precup 2021), on single-task settings (Bagaria and Konidaris 2020; Riemer, Liu, and Tesauro 2018), or lacks support for lookahead planning over options to guide low-level policy learning (Machado et al. 2017; Khetarpal et al. 2020). A critical challenge remains open: the autonomous discovery of generalizable, reusable options for long-horizon, sparse-reward tasks in continual RL settings. This is particularly relevant to real-world scenarios such as warehouse management, disaster recovery operations, and assembly tasks, where agents must adapt to shifts in context and required behaviors without dense reward feedback and closed-form analytical models.\nWe present a novel approach that coherently addresses option discovery and transfer with a unified symbolic abstraction framework for factored domains in the continual RL settings. We focus on long-horizon, goal-based Markov decision processes (MDPs) in RL settings with unknown transition functions and sparse rewards. In particular, we consider three conceptual desiderata for options: 1) composability: support chaining options for enabling hierarchical planning, 2) reusability: support transfer of options to new problem instances, minimizing the need for relearning, and 3) mutual independence: reduce interference among options, allowing options to be learned and executed independently with minimal side effects while ensuring composability at well-defined endpoints. Most prior works meet some of these criteria but not all.\nOur approach, Continual Hierarchical Reinforcement Learning and Planning (CHiRP) takes as input a set of state variables and a stochastic simulator, and invents options satisfying desiderata 1-3: the invented options have symbolic abstract descriptions that directly support composability and reusability through high-level planning; these options have stronger effects on different sets of variables and/or values, supporting mutual independence. The core idea is to capture notions of context-specific abstractions that depend on and change with the current state by identifying salient variable values responsible for greatest variation in the Q-function, and to use changes in these abstractions as a cue for defining option endpoints. With every new task in a continual stream"}, {"title": "2 Formal Framework", "content": "Problem Definition. We assume RL settings where an agent interacts with a goal-oriented Markov decision process (MDP) M defined by the combination of an environment (S, A, T, \u03b3,h) and a task (si, Sg, R). Here, S is a set of states defined using a factored representation, where V = {v1,..., vn} is a set of continuous real-valued or discrete variables, and each vi \u2208 V has an ordered domain Dvi = [Dmin, Dmax). We denote the value of variable vi in state s as s(vi). Each state s \u2208 S is defined by assigning each vi \u2208 V a value from its domain, i.e., s(vi) \u2208 Dvi. A is a set of finite actions. T : S \u00d7 A \u2192 \u00b5S is a stochastic transition function where \u00b5S is a probability measure on S. \u03b3\u2208 (0, 1) is a discount factor and h is a horizon. Lastly, si \u2208 S is an initial state, Sg \u2286 S is a set of goal states, and R:S\u00d7A\u2192 R is a reward function underlying the task.\nRunning Example. Consider a hybrid state space (defined by both continuous and finite variables) adaptation of the classic taxi domain (Dietterich 1999), where a taxi starts at a random location and is tasked with picking up a passenger and transporting them to their destination. The pickup and dropoff locations are chosen randomly among n specific locations. States are represented by variables V: x \u2208 R (taxi's x-coordinate), y \u2208 R (taxi's y-coordinate), l \u2208 {0,1,..., n} (passenger's location, where integers indicate specific locations and 0 indicates elsewhere), and p \u2208 {0,1} (passenger's presence in the taxi). For clarity, consider variables with small domains: Dx = [0.0, 5.0), Dy = [0.0, 5.0),\nD\u2081 = {0,1,2,3,4}, and Dp = {0,1}. A state assigns a\nvalue to each variable from its domain, e.g., s = (s(x) ="}, {"title": "Conditional Abstraction Trees (CATs)", "content": "State abstraction\non a variable's values (such as taxi's location) is conditioned\non the values of the other variables (such as passenger's\npresence in the taxi). Such rich conditional abstractions for a\ntask can be captured in the form of a Conditional Abstraction\nTree (CAT) (Dadvar, Nayyar, and Srivastava 2023) where\nthe root denotes the coarsest abstract state, while lower-\nlevel nodes represent abstract states with greater refinement\non variables requiring higher resolution in variable values.\nFig. 2 illustrates a CAT for a small problem of taxi domain.\nCATs are defined formally as follows.\nDefinition 2.3 (Conditional Abstraction Trees (CATs)). A\nCATA is a tuple (N,E), where N is a set of nodes rep-\nresenting possible abstract states and E is a set of directed\nedges connecting these nodes. The root represents the coars-\nest abstract state Sinit. An edge e \u2208 E from a parent abstract\nstate sp \u2208 N to a child abstract state sc \u2208 N exists iff\nSe can be obtained by splitting atleast one of the variable\nintervals in Sp at most once. The leaf nodes represent the\nactive abstract state space S\u25b3. A defines a state abstraction\n\u25b3 : S \u2192 S\u25b3 mapping each state s \u2208 S to the abstract state\n\u00a7\u2208 S\u25b3 represented by the unique leaf in A containing s.\nIn this work, we use novel notions based on CAT-based\nstate abstractions to coherently address option invention and\ntransfer with a unified abstraction framework. We compute"}, {"title": "Abstract Options", "content": "We use the standard notion of options\n(Sutton, Precup, and Singh 1999). An option o is a triple\n(\u0399\u03bf, \u03b2\u03bf, \u03c0\u03bf), where ICS is the initiation set where o\ncan initiate, B. CS is the termination set where o termi-\nnates, and \u03c0\u03bf: S \u2192 A is the option policy prescribed by\no that maps states to actions. Our approach autonomously\nlearns all components of options, defined over an abstract\nstate space SA, as follows. We define an abstract option o\nas a tuple (\u0394\u03bf, \u0399\u03bf, \u03b2\u03bf, \u03c0\u03bf), where \u2206\u3002 is the CAT-based state\nabstraction \u222e\u25b3\u3002: S \u2192 S\u25b3\uff61, I\uff61 CS\u25b3\u300f is the abstract initi-\nation set, Bo CS\u25b3, is the abstract termination set, and \u03c0\u03bf:\nS\u2206. \u2192 A is the abstract partial policy. I and \u1e9e\u3002denote\noption endpoints. The declarative description of an option is\ntermed as option signature\u3008Io, \u03b2\uff61). Additionally, two op-\ntions or and oj are composable iff Bo Zoj."}, {"title": "3 Continual Hierarchical RL and Planning", "content": "The core contribution of this paper is a novel approach for\nautonomously inventing a forward model of abstract options\nusing auto-generated CAT-based state abstractions and ef-\nficiently utilizing them for solving continual RL problems.\nOur approach CHiRP (Fig. 1) takes as input a continual\nstream of tasks M and a stochastic simulator, and computes\na policy for each task. The key insight for option invention\nis that CATs, auto-generated using CAT+RL, for each task\ninherently capture abstractions that remain stable within a\nsubtask, but change significantly across subtasks within the\ntask. We use CATs to capture notions of context-specific\nabstractions that depend on the current state and then use\nchanges in these salient abstractions as a cue for defining\noption endpoints. For instance, in the taxi domain (Fig. 2),\nwhen the passenger has not been picked up, the abstraction\nneeds greater refinement on the value of the taxi's location\ncloser to the passenger's location. However, when the con-\ntext changes to a situation where the passenger is in the taxi\nand has not been dropped off, the abstraction needs greater\nrefinement on the value of the taxi's location near the des-\ntination. In this scenario, the pickup option (option 02 in\nFig. 2) can be seen as an option that achieves a significant\nchange in context-specific abstractions.\nCHIRP maintains a universal CAT created from the cur-\nrent and all previous problems in the stream, and exploits its\nstructure to identify context-variables, such as the passen-\nger's presence in the taxi. These variables are used to define\na context-specific distance between states in a manner such\nthat higher distances correspond to greater changes in salient\nvariables and values. CHIRP operationalizes this notion of\nchanges in saliency to invent abstract options. Note that the\ndescriptions of the invented options are symbolic, hence di-\nrectly support efficient composition and reuse. When a new\ntask is encountered, CHiRP uses foresight to plan ahead by\nconnecting endpoints of learned options, while also invent-\ning additional option signatures to bridge gaps if needed.\nEach option maintains its own encapsulated CAT-based state\nabstraction, allowing options to be used and updated inde-"}, {"title": "3.1 Algorithm Overview", "content": "Given a continual RL problem, Alg. 1 begins with an empty\nmodel of abstract options O and a CAT A with the coarsest\nstate abstraction (lines 1-2). For each new task in the stream,\nthe CAT's abstraction is used to compute the initial and goal\nabstract states (line 4). Once a solution policy is found or a\nbudget of H timesteps is reached, the agent moves to solv-\ning the next task (line 5). To solve the current task, the agent\ninterleaves: (1) a planner to plan with the current model of\nabstract options, (2) CAT+RL to refine the current CAT'S\nstate abstraction during learning, and (3) an option inven-\ntor to invent novel abstract options using the updated CAT\n(lines 6-16). The updated model of options O and CAT A\nare transferred to solve subsequent tasks (line 3).\nGiven a new task, Alg. 1 uses an offline search process\nwith the current model of abstract options to compute a plan\nfrom the current abstract state to a goal abstract state, de-\nnoted by II = (00,..., On), Oi \u2208 O. The learned option\nrepresentations are used to compose this plan, as detailed\nin Sec. 3.3 (line 6 computeOptionPlan()). The method ad-\nditionally creates new option signatures to allow connecting\nendpoints of learned options with gaps between them. If no\nplan is found with the current model, we initialize the plan\nwith a new option signature from the current abstract state\nto the goal abstract states (line 8). The CAT and policies for\nthese options are learned later during RL.\nFor each newly created option signature or previously\nlearned option in the plan, \u03bf \u2208 II, we generate an MDP with\na sparse intrinsic reward for reaching the option's termina-\ntion (line 10). The option's CAT A, and policy \u03c0\u03bf are then\nlearned or fine-tuned using CAT+RL (line 11). We use these\noption-specific CATs and policies to invent new abstract op-\ntions with updated representations, as detailed in Sec. 3.2\n(line 13 inventOptions()). This process converts option sig-\nnatures into abstract options with learned CATs and policies.\nWe also update the universal CAT \u2206 with each invented op-\ntion's CAT A., adjusting the current abstract state (line 14).\nNote that option executions can be stochastic, and the agent\nmay fail to successfully reach the termination set of an op-\ntion. In such cases, we use active replanning (Kaelbling and\nLozano-P\u00e9rez 2011) from the current abstract state to a goal\nabstract state (line 16), and continue learning option-specific\nCATs and policies. This process repeats until the computed\nplan of abstract options successfully solves the problem. Fi-\nnally, Alg. 1 transfers the updated model O with the new\noptions and CAT A to solve new tasks."}, {"title": "3.2 Inventing Generalizable Options", "content": "We now discuss our approach for inventing options using a\nlearned CAT and an abstract policy (Alg. 1 line 13) (Sec. 3.1\nexplains our approach for obtaining these inputs). The key"}, {"title": "Identifying Option Endpoints", "content": "We identify transitions\nthat lead to significant changes in context-specific abstrac-\ntions to define endpoints of new options. For instance, con-\nsider C-CATs in Fig. 2 before and after the passenger is\npicked up. These C-CATs are significantly different from\neach other, indicating a significant change in abstraction.\nTo measure difference between abstraction functions repre-\nsented by two C-CATs generated from the same CAT, we\nuse a context-specific distance function. Intuitively, this dis-\ntance is computed by traversing from the root node and sum-\nming the structural differences between corresponding sub-\ntrees of C-CATs. Given a C-CAT \u2206, for state s, let A de-\nnote the subtree rooted at node n, and depth max(\u25b3) denote\nthe maximum depth of that subtree. We drop n from \u0394\nwhen n = Sinit. Let ni denote the ith child of node n in the\nCAT \u0394. We formally define this distance as follows.\nDefinition 3.2 (Context-specific distance between C-CATs).\nGiven two C-CATs obtained from A rooted at node n, \u0394 S1\nand A, the distance between them is defined as\n\u03b4(\u03941, \u0394) = {depthmax(\u03941),if n not in \u0394S2;depthmax(\u03942),if n not in \u0394S1;\u03a3i\u03b4(\u0394n1i, \u0394n2i),otherwise."}, {"title": "Defining Distance", "content": "To identify option endpoints using trajectory and the\ncontext-specific distance, we first identify context-variables\nfrom CAT \u2206, and generate C-CATs \u25b3\u2081 = (\u220650,..., Asn).\nLet thre be a distance threshold. Then, for each transition\n(Si, Si+1) CT, we use abstract states \u25b3(si) and \u25b3(si+1) to\ndefine option endpoints if \u03b4(\u2206s\u2081, si+1) > thre. The initial\nand goal abstract states are also included.\nAdditionally, our approach uses a context-independent\ndistance, also derived from the CAT, to allow decomposing\nan option into multiple options. For example, navigating to\nthe pickup location can be decomposed into first reaching\nthe larger bottom-left quadrant and then the exact pickup lo-\ncation (Fig. 2). Intuitively, this distance is greater between\nstates that belong to highly distinct (having higher low-\nest common ancestor (LCA)) and highly refined CAT sub-\ntrees. Let depthmax be the maximum depth of the CAT, and\ndepth(n1, n2) denote the number of edges between nodes n1\nand n2. We formally define this distance as follows.\nDefinition 3.3 (Context-independent distance between ab-\nstract states). Given a CAT A and LCA of two abstract\nstates 31 and 32, the distance between them 0\u25b3(\u00a71, \u00a72) is de-\nfined as the weighted sum of (depth,max - depth(root, LCA)+\n1) and (depth(LCA, 51) + depth(LCA, 32))/2.\nWe decompose options by extending the previously com-\nputed sequence of option endpoints as follows. We com-\npute trajectory segments Tseg = (Sk,...,Sm)\u2286 Ts.t.\n\u03a6\u25b3(sk) and (sm) are consecutive option endpoints. We\nalso compute corresponding abstract trajectory segments\nTseg using the CAT. Let Othre \u2264 1.0 be a distance thresh-\nold and max be the maximum distance between abstract\nstates in any transition in Tseg. Then, for each transition\n(Si-1, Si)\nTseg, Si is additionally identified as an option\nendpoint if (Si-1, Si) > thre \u00d7 max."}, {"title": "3.3 Planning over Auto-Invented Options", "content": "We now describe a novel planning process to compute a\nplan II for a new task using the learned model of abstract\noptions and the learned CAT overlayed with abstract tran-\nsitions, termed as a Plannable-CAT (Alg. 1 line 6). This\nplannable-CAT is used to guide the search process over the\noption endpoints, while creating new option signatures to\nconnect them when needed. We apply single-outcome de-\nterminization (Yoon, Fern, and Givan 2007) for planning by\nconsidering only the most likely effects (here, the termina-\ntion sets) of options. To compute a plan of options, we first\naugment the Plannable-CAT with transitions between option\nendpoints, including lifted transitions between higher levels\nof abstract states. We then use A* search over this Plannable-\nCAT with a cost function that prioritizes lower-level transi-\ntions and a heuristic defined by the context-independent dis-\ntance (Def. 3.3). The idea is to compose abstract transitions\nat different levels of abstractions. The resulting plan is re-\nfined by replacing consecutive higher-level transitions with\na new option signature. This helps to bridge gaps between\noption endpoints. Finally, CHiRP interleaves the execution\nof the computed plan with learning of option policies for\nany newly created option signatures to solve the new task."}, {"title": "4 Empirical Evaluation", "content": "We evaluated CHiRP\u00b9 on a diverse suite of challenging do-\nmainss in continual RL setting. Full details about the used\ndomains and hyperparameters are provided in the extended\nversion of our paper (Nayyar and Srivastava 2024)."}, {"title": "4.1 Experimental Setup", "content": "Domains. For our evaluation, we compiled a suite of test\ndomains for continual RL that are amenable to hierarchichal\ndecomposition and challenging for SOTA methods. We then\ncreated versions of these problems that are significantly\nlarger than prior investigations to evaluate whether the pre-\nsented approaches are able to push the limits of scope and\nscalability of continual RL. Our investigation focused on\nstochastic versions of the following domains with contin-\nuous or hybrid states: (1) Maze World (Ramesh, Tomar,\nand Ravindran 2019): An agent needs to navigate through\nrandomly placed wall obstacles to reach the goal; (2) Four\nRooms World (Sutton, Precup, and Singh 1999): An agent\nmust move within and between rooms via hallways to reach\nthe goal; (3) Office World (Icarte et al. 2018): An agent\nneeds to collect coffee and mail from different rooms and\ndeliver them to an office; (4) Taxi World (Dietterich 2000):\nA taxi needs to pick up a passenger from its pickup location\nand drop them off at their destination; (5) Minecraft (James,\nRosman, and Konidaris 2022): An agent must find and mine\nrelevant resources, build intermediate tools, and use them to\ncraft an iron or stone axe.\nBaselines. We selected the best-performing contemporary\nmethods that do not require any hand-engineered abstrac-\ntions or action hierarchies as baselines to match the absence\nof such requirements in our approach: (1) Option-Critic"}, {"title": "Hyperparameters", "content": "A key strength of CHiRP over base-\nlines is that it requires only five additional hyperparameters\nbeyond standard RL parameters (e.g., decay, learning rate),\nunlike SOTA DRL methods that need extensive tuning and\nsignificant effort in network architecture design. Throughout\nour experiments, we intuitively set thre = 0 and thre ~ 1\nto minimize hyperparameter tuning. These values are robust\nacross domains, preventing options from being too small or\nnumerous. We use a limited set of values for kcap, Sfactor, and\nemax parameters across different domains to adaptively con-\ntrol the training of an option's policy and CAT. All parame-\nters are set to the same values across a continual stream of\ntasks. Details on the used hyperparameters for CHiRP and\nthe baselines are provided in the extended version."}, {"title": "Evaluation setting and metrics", "content": "We evaluate in a contin-\nual RL setting where an agent needs to adapt to changes\nin initial states, goal states, transition and reward functions.\nFor each domain, 20 tasks are randomly sampled sequen-\ntially from a distribution. Each approach is provided a fixed\nbudget of H timesteps per task before moving on to the\nnext task. Due to stochasticity and lack of transition mod-\nels, a task is considered solved if the agent achieves the\ngoal \u2265 90% of the time among 100 independent evalua-\ntion runs of the learned policy. We report the fraction of\ntasks solved within the total allocated timesteps for each ap-\nproach. The reported timesteps include all the interactions\nwith the environment used for learning state abstractions,\noption endpoints, and option policies. Results are averaged,\nand standard deviations are computed from 10 independent\ntrials across the entire problem stream."}, {"title": "4.2 Results", "content": "We evaluate the presented work across a few key dimen-\nsions: sample-efficiency in continual RL setting, and sat-\nisfaction of key conceptual desiderata for task decomposi-\ntion-composability, reusability, and mutual independence.\nQ1. Does CHiRP help improve sample-efficiency over\nSOTA RL methods in continual RL setting?\nFig. 4 shows that CHiRP consistently outperforms all\nbaselines. Our results confirm that, while in principle base-\nline approaches can solve problems without hand-designed\nabstractions and hierarchies, they require orders of magni-\ntude more data and struggle to solve streams of distinct long-\nhorizon tasks with sparse rewards. We found that CAT+RL\ndelivered the second-best performance, while Option-Critic\nand PPO consistently underperformed across all domains,\nfailing to solve tasks within the allotted budget. While\nOption-Critic has the advantage of reusing options, it strug-\ngled to learn useful and diverse options. This is at least partly"}, {"title": "Q2. Does CHiRP invent mutually independent options?", "content": "Can the options be composed and reused effectively?\nOptions invented by CHiRP have a key advantage: their\ninterpretable symbolic representation, where each option's\ninitiation and termination conditions are defined in terms of\nspecific value ranges of variables. Our analysis revealed that\nthe invented options express distinct, complementary behav-\niors, with each option primarily affecting different state vari-\nables and value ranges. E.g., in the taxi domain, CHiRP in-\nvented four options that operate independently: two navi-\ngation options that affect different values of taxi location\nvariable and specialize in moving to pickup/drop-off loca-\ntions, and two passenger interaction options that affect dif-\nferent values of passenger variables and focus on picking\nup/dropping off the passenger. These options demonstrate\nmutual independence through minimal overlap in their core\naffected variables and value ranges in terminations of the op-\ntions. Their clear symbolic endpoints enable direct chaining\nof options, making them both composable and reusable."}, {"title": "5 Related Work", "content": "Abstraction has been a topic of significant research interest\n(Karia, Nayyar, and Srivastava 2022; Shah and Srivastava\n2024; Shah et al. 2024; Karia et al. 2024). Early research\nin RL largely focused on hand-designed abstractions (An-\ndre and Russell 2002; Dietterich 2000), with more recent\nframeworks also using high-level planning models or action\nhierarchies (Illanes et al. 2020; Kokel et al. 2021). Typically,\nresearch on learning abstractions has focused on either state\nabstraction or action abstraction in isolation (Jonsson and\nBarto 2000; Wang et al. 2024). A variety of methods have\nbeen developed for automatic discovery of subgoals or op-\ntions, such as identifying bottleneck states through graph-\npartitioning (Menache, Mannor, and Shimkin 2002; \u015eim\u015fek\nand Barto 2007; Machado, Bellemare, and Bowling 2017),\nclustering (Mannor et al. 2004), and frequency-based (Mc-\nGovern and Barto 2001; Stolle and Precup 2002) techniques.\nA large body of work learns hierarchies in which a high-\nlevel policy sets subgoals for a lower-level policy to achieve\n(Vezhnevets et al. 2017; Nachum et al. 2018).\nMost prior research in option discovery focuses on con-\ntrol tasks with short horizons, often using dense rewards\nor distance metrics due to computational intractability (Ba-\ncon, Harb, and Precup 2017; Bagaria, Senthil, and Konidaris\n2021). Much of this research is limited to single-task set-\ntings (Bagaria and Konidaris 2020; Riemer, Liu, and Tesauro\n2018). Many recent methods learn a fixed, prespecified num-\nber of options and depend on learning a policy over options\nto use them (Bacon, Harb, and Precup 2017; Machado et al.\n2017; Khetarpal et al. 2020; Klissarov and Precup 2021). In\ncontrast, our work tackles a stream of long-horizon, sparse-\nreward tasks by continually learning generalizable options\nwith abstract representations and planning over them."}, {"title": "6 Conclusion and Future Work", "content": "This paper presents a novel approach to continual RL based\non autonomously learning and utilizing symbolic abstract\noptions. This work assumes full observability and discrete\nactions. An interesting future research direction is to extend\nour approach to settings with continuous parameterized ac-\ntions. Optimality is another good direction for future work."}]}