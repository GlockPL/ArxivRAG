{"title": "Agents for self-driving laboratories applied to quantum computing", "authors": ["Shuxiang Cao", "Zijian Zhang", "Mohammed Alghadeer", "Simone D Fasciati", "Michele Piscitelli", "Mustafa Bakr", "Peter Leek", "Al\u00e1n Aspuru-Guzik"], "abstract": "Fully automated self-driving laboratories are promising to enable high-throughput and large-scale scientific discovery by reducing repetitive labour. However, effective automation requires deep integration of laboratory knowledge, which is often unstructured, multimodal, and difficult to incorporate into current AI systems. This paper introduces the k-agents framework, designed to support experimentalists in organizing laboratory knowledge and automating experiments with agents. Our framework employs large language model-based agents to encapsulate laboratory knowledge including available laboratory operations and methods for analyzing experiment results. To automate experiments, we introduce execution agents that break multi-step experimental procedures into state machines, interact with other agents to execute each step and analyze the experiment results. The analyzed results are then utilized to drive state transitions, enabling closed-loop feedback control. To demonstrate its capabilities, we applied the agents to calibrate and operate a superconducting quantum processor, where they autonomously planned and executed experiments for hours, suc-cessfully producing and characterizing entangled quantum states at the level achieved by human scientists. Our knowledge-based agent system opens up new possibilities for managing laboratory knowledge and accelerating scientific discovery.", "sections": [{"title": "I. MAIN", "content": "Automating laboratory experiments has the potential to accelerate scientific discovery by closing the loop between experimental execution, artificial intelligence (AI) and human-in-the-loop decision-making [1]. Although it is increasingly common in laboratories that experiments can be implemented through programming interfaces [2-4], automating experiments still requires detailed laboratory knowledge to set the parameters of each experiment, interpret the results of the experiment, and execute complicated experiments workflows. Traditional methods of automating experiments require human experts to translate their knowledge into code for machine execution [5-9]. However, the knowledge required for laboratory work has increased significantly over the past decades due to the growing complexity of the experimental apparatus, which now involves more intricate technical details for operation. This makes translating the required knowledge and maintaining consistency an increasingly significant human effort, which compromises the efficiency brought about by automation. In addition, the multimodal and complex nature of laboratory knowledge also makes it challenging to develop automated programs that can perform these tasks at a level comparable to that of humans.\nThe advent of large language models (LLMs) and multimodal LLMs has sparked new hope for more efficient experiment automation due to their universal ability to process text and image information [10-16]. Although limited by the length of their context windows, LLMs offer the hope of efficiently understanding laboratory documents, performing analyses, generating code, and interpreting experimental images. The development of multi-agent systems has further enhanced LLM's capabilities by integrating multiple LLM-based agents to work together on a complex objective, improving both accuracy and scalability [17-23]. The merits of LLM-based agents make them well suited for assisting laboratory automation without the need for extensive human intervention. However, several challenges remain. First, much of the media, such as code and documents, that carry laboratory knowledge is proprietary and inaccessible online, meaning that it is unlikely to be included in the training data of publically available LLMs. In addition, the amount of text that includes such specialized laboratory knowledge is often insufficient to effectively fine-tune an LLM [24]. Furthermore, laboratory knowledge is usually dynamic and frequently updated, making fine-tuning models impractical, as it is often hard to rewrite the knowledge in LLMs [25-27]. As a result, incorporating laboratory-specific knowledge into LLMs remains difficult. Recent work on retrieval-augmented generation (RAG) offers a foundation to equip LLM-based agents with larger-scale knowledge without fine-tuning [28-30]. However, laboratory knowledge is usually heterogeneous and multimodal, making it challenging to apply standard RAG methods directly [5, 9, 31].\nDespite challenges, pioneering efforts to develop automated LLM-based agents to carry out experiments are already underway [32-35]. For example, in [32] and [33], researchers equipped LLM-based agents with tools that allow them to acquire professional knowledge from the expert-designed toolkit and even the Internet. Then, these agents are capable of performing chemistry experiments on the basis of this knowledge. However, in general, previous results lack a scalable memory system and rely on keeping the conversation history when calling LLMs. This inherently restricts the agents' ability to automate long-duration tasks involving many sequential steps, as most current LLMs exhibit diminishing performance as the input length increases [36-38]. This construction also prohibits further extensions of the system, such as supporting the handling of scientific plots.\nIn this work, we introduce k-agents (see Figure 1), a knowledge-based multi-agent system designed to automate experiments, particularly those requiring large-scale, multimodal laboratory knowledge and complex workflow. We developed tools for users to transfer their knowledge, creating LLM-based knowledge agents to manage laboratory knowledge without fine-tuning. These knowledge agents can help operate laboratories by holding knowledge ranging from single experiments to complex procedures and how to inspect the results of each experiment. These agents are designed to be activated selectively, allowing the system to scale efficiently as the number of agents increases. In order"}, {"title": "A. Knowledge agents", "content": "We define knowledge agents as AI agents whose performance is measured by their ability to receive and transfer knowledge. For instance, an agent qualifies as a strong knowledge agent if it accepts knowledge from natural language inputs and responds correctly to related queries using natural language. Developing more advanced knowledge agents offers significant advantages. First, a more general ability to accept knowledge reduces the effort needed for humans to translate knowledge into computer-friendly formats. Furthermore, the ability to transfer knowledge facilitates more rigorous testing of agents [43] and avoids treating the system as a black box, improving the general trustworthiness of AI systems. Building on the above ideas, we introduce the k-agents framework, which provides tools to implement the aforementioned bidirectional knowledge transferability. In k-agents, we implement user-friendly interfaces that enable users to encapsulate knowledge within different LLM-based agents. These agents can then transfer their knowledge to the users and other agents in the format of natural language with LLMs.\n(Code translation agent) This first kind of knowledge agent in k-agents is the code translation agent, which is designed to accept and apply the knowledge of the available experiment interfaces that can be called by code. Here, the term \"experiment\" stands for not only a direct experiment on an instrument, but any general action that is available in the laboratory. It can also be a call to a tool or even another agent. Given instructions in natural language, these agents are responsible for translating natural language instructions into the corresponding code. To help the users of k-agents construct code translation agents, we provide an abstract Python class that allows users to define the set of experiments available in the laboratory explicitly. The users can then define all possible experiment interfaces, ensuring that the AI system can access and utilize them. As illustrated in Figure 2(a), human experimentalists are expected to document background knowledge, required parameters, and implementation code for each experiment in the run method of the subclass representing the experiment. After indexing all available experiment classes, k-agents will construct a code translation agent for each of the classes.\n(Procedure translation agent) Experiment procedures in laboratories can involve complex workflows. These workflows may require experimentalists to determine a sequence of experiments based on the results of the experiments executed. Traditionally, this knowledge has been maintained either through unstructured documentation or by relying on the memory of the experimentalists themselves. To address this, as shown in Figure 2(a), k-agents introduces a standardized format to store examples of how to implement instructions using multistep experiment procedures. These examples are then used to produce another kind of knowledge agent that we call the procedure translation agent, which is capable of translating an instruction into its corresponding procedure based on stored examples. Similarly to code translation agents, procedure translation agents also output code. The code will call an execution agent to execute the procedure, which we will introduce later.\n(Inspection agent) A major burden for experimentalists has been the need to wait for experimental results and decide on the next step based on them. In many scenarios, experimentalists must analyze figures to assess the success of an experiment and determine the next steps. In the k-agents framework, we introduce inspection agents that have the knowledge needed to evaluate the results of each experiment. These agents will be called after the execution of each experiment to analyze its outcomes. We provide an interface to inject knowledge for each function that produces experiment figures. This feature is implemented as a Python decorator. In the decorator, the users can add instructions for how to analyze the figure. Besides, as illustrated in Figure 3(a), our interface allows users to add example figures to help others understand the new figures. Based on the knowledge from the interfaces, we construct inspection agents equipped with multimodal LLMs to inspect and analyze new experiment figures and output text-based reports based on their knowledge."}, {"title": "B. Execution agent", "content": "We introduce the execution agent to coordinate knowledge agents and manage complex experiment procedures. This agent takes experiment procedures written in natural language as input. Since the selection of the next experiment in a procedure could depend on the result of previous experiments, we design the execution agent to first decompose the procedure into a state machine, in which each state represents a distinct experiment stage of the procedure (See Figure 1(c)). Each stage of the experiment is assigned a single instruction that contains only one experiment or sub-procedure. As a state machine, there is also a transition rule at each stage in natural language that determines how to select the next stage based on the result of the experiment.\nBy constructing the state machine, the execution agent transforms the task of executing the procedure into executing the state machine. The execution is illustrated in Figure 1(a). Starting from the initial stage, at each stage, the execution first translates the instruction at the stage into executable code with the assistance of translation agents. To increase the accuracy and efficiency of the translation, the execution agent will activate only translation agents related to the context. The relevance score will be calculated based on the embedding similarity between the context and the description of the translation agents. The execution will select a reasonable response from the translation agents and execute the translated code. After executing the code, the agent gathers a natural language report from inspection agents and, based on the report and the transition rules, determines the next stage to transition to. If the reports suggest updating the parameters in the new stage, the execution agent will also pass the update to it. This state-machine architecture enables the execution agent to focus on executing each stage of the procedure efficiently without needing to monitor the entire experimental history when deciding on the next experiment."}, {"title": "III. DISCUSION", "content": "In this study, we introduce the k-agents framework for creating laboratory-related knowledge agents and demonstrate its capability to automate experiments. We applied the system in a real-world setting, successfully demonstrating the ability of k-agents by performing single-qubit and two-qubit gate calibrations, as well as generating and characterizing a GHZ state on a quantum processor based on human instructions. The k-agents succeeded in orchestrating the experiments, interpreting the multimodal experimental results, and guiding the execution of the experiment with closed-loop feedback. These achievements suggest that our system is a valuable tool for research groups working with superconducting quantum processors, with the potential for broader applications in research automation across other fields. Additionally, we also applied two benchmark experiments on k-agents. We found our agent-based translation of natural language instructions outperforms standard RAG approaches in terms of accuracy. By our benchmark of the inspection agents, we showed that current multimodal LLMs have the ability to analyze experiment figures based on human instructions, especially when provided with example figures.\nHowever, the k-agents framework has some potential weaknesses that make full automation challenging. First, our method relies on clean and well-formatted codebases and procedure documents. However, such structured knowledge may not always exist, and transforming existing knowledge could take considerable human effort. Future research on structuralizing unstructured documents and codebases is still needed to completely automate knowledge translation. A possible direction of research could involve enabling the framework to generate structured code and procedures based on unstructured project files and human inputs. Secondly, using LLM, the execution of k-agents exhibits higher latency than traditional automation methods that do not require LLM-based planning and translation of knowledge at runtime. We note that this problem can be well mitigated by caching the response of LLMs and using the same response whenever the input is the same.\nWe monitor the cost of the k-agents system. During the parameter search for the two-qubit gate, we observed that the LLM used 1,373,207 input tokens (including image tokens) and 168,039 output tokens in 3 hours, costing less than 5 US dollars. We believe that there is potential for further optimization to reduce these costs. Regarding time efficiency, despite network delays, LLMs currently take longer than humans to perform inspections. However, LLMS tend to be more efficient than humans in generating code. Therefore, overall efficiency is comparable.\nThe use of superconducting quantum processors simplifies some challenges, such as safety control and error tolerance. In our system, the risk of the agent executing code that causes significant damage or unacceptable costs is very low. If it fails, we can simply allow it to try again. However, this may not be the case in other applications. Prior research [1, 67, 68] has focused heavily on safety control, and this is an area we plan to explore further in the future."}, {"title": "IV. METHOD", "content": "A. Details on the knowledge agents developed\n1. Definition of knowledge agents\nThe concept of knowledge agents is defined based on the work proposed by Zhang et al. [16], in which an agent's learning ability is defined as its ability to internalize knowledge given related inputs. A knowledge agent can then be interpreted as an agent with the ability to learn (i.e., acquire knowledge) and to apply the learned knowledge to respond to other agents (i.e. transfer knowledge). A bare LLM can be regarded as a knowledge agent as it is able to absorb knowledge from its training data. However, it is difficult for LLMs to internalize laboratory documents and codes as their knowledge because of the current pitfalls in model editing [25]. Therefore, we regard bare LLMs as weak knowledge agents compared to the knowledge agents we proposed, which are good at accepting knowledge from laboratory documentation.\n2. Translation Agents\nIn k-agents, we included two types of translation agents: code translation agent, which translates instructions into code, and procedure translation agent, which translates instructions into experiment procedures that may contain"}, {"title": "Prompt for generating possible instructions", "content": "You are trying to produce imperative sentences that would invoke the execution of action '{exp_name}' based on its documentation.\n{doc_string}\nHere are a few of examples of imperative sentences:\nRun the calibration experiment with duts='duts' and start=0.0\nCarry out a calibration on 'duts'\nPlease execute the Ramsey experiment\nDo the Drag experiment.\nYou should output a JSON dict. The keys should be string of indices of the sentences and the values should be the sentences.\nEach sentence should be complete and independent. Name of the experiment should be transformed to natural language and be mentioned.\nThe sentences should be imperative and should be based on the documentation.\nYou should output 4 sentences."}, {"title": "Prompt for generating candidate code", "content": "You are trying to call an experiment to fill the code_to_complete in Python. The description of the task is written in the slot.\n{self.get_exp_description()}\n\n# [slot: {instruction}]\n\n{available_variables}\nYou should output a JSON dict. The keys should be\n\"experiment_name_in_slot\" (string): The name of the experiment extracted from the slot.\n\"analysis\" : The brief analysis of the relation between the experiment and the code_to_complete. You should notice that the code_to_complete might be irrelevant to the experiment. You should be careful not to assume additional information. The experiment should considered irrelevant if it contains extra keywords or irrelevant information.\n\"applicable\": A boolean whether the experiment you hold is suitable for implementing the task.\n\"code\": A code snippet that is helpful for filling the slot. The last line of the snippet must be in the format: 'experiment_<name> = { self.exp_cls.__name__} (argument1, argument2, ...). No import statements are needed.\n\"explanation\": A detailed explanation of what the code snippet does based solely on the documentation.\n\"suitable\": A boolean whether the code snippet matches the task based on the documentation."}, {"title": "2. Procedure translation agent", "content": "When initializing procedure translation agents, similar to code translation agents, a few variants of the procedure's title will be generated for their activation.\nWhen activated, a procedure translation agent first checks whether the given instruction to translate matches the procedure title it holds. Then, the agent tries to rewrite the given instruction into a format that is closer to the procedure's title it knows. In the meantime, as many experiment procedures require inputs, the agent also generates a map that maps the available variables in the context to the required inputs of the procedure."}, {"title": "Prompt for rewriting instructions based on knowledge of an experiment procedure", "content": "You are trying to rewrite the following instruction based on your knowledge to suit more general parameters"}, {"title": "Prompt for stage extraction", "content": "{description}\nYour objective is to decompose the experiment description into standalone instruction.\nEach instruction should include an experiment.\nThe instructions should make a minimal modification to the original description.\nYou should not make any inferences or interpret the description.\nYou are encouraged to copy the description as is.\nYou should output as few instructions as possible. You must not expand the instructions.\nThe instructions must not contain any information about what to do next after the instruction, such as a change of parameter and go to fail.\nFor example, if a piece of description is:\n\"Run experiment A with frequency=10. If failed, retry 3 times.\"\nYou should change it into:\n\"Run experiment A with frequency=10.\"\nYou are required to output a JSON dict with a single key \"instructions\", which contains a list of instructions. Each instruction should be represented as a string."}, {"title": "4. Execution agent (Stage transition)", "content": "After executing the experiment in a stage, the execution agent decides the next stage to transition to based on the summary of the reports from the inspection agents. The prompt for deciding the next stage includes the summary, the information of the current stage, including how many times it has been executed, and transition rules."}, {"title": "Prompt for stage transition", "content": "You are analyzing experiment result from current stage and use the rule of transition to determine the next stage of the experiment.\n{current_stage.label}:{current_stage.description}\nThe current stage has been executed {current_stage.n_executed} times.\nThere are { current_stage.n_failed} failed attempts and {current_stage.n_success} successful attempts.\nHere are the results from the experiments. Note that results must be consistent to indicate the validity. Otherwise they are both invalid.\n{result_prompt}\n{rules}\nReturn your decision in JSON format With the following keys:\n\"analysis\" (string): an analysis of the results and the rule of transition to determine the next stage.\n\"next\" (string): the name of the next stage."}, {"title": "Appendix C: Benchmarking our methods", "content": "1. Benchmarking translation\nTo test the performance of translating instructions into code, we manually make a test set that consists of 80 pairs of experiments and instructions. For example, for the Normalised Rabi experiment, which calibrates the driving frequency of a single qubit, we set the following instructions in the test set.\nExamples of the instructions for NormalisedRabi\nRun rabi experiment to calibrate single qubit gate driving amplitudes\nMeasure Rabi oscillations to determine single qubit gate driving amplitudes\nImplement Rabi experiment to find pi pulse duration\nCalibrate the driving amplitudes for single qubit gate by Rabi\nDetermine single qubit gate parameter using Rabi experiment\nRun Rabi experiment with default parameters on the single qubit 'dut'\nSingle qubit gate amplitudes estimation using Rabi experiments\nDo Rabi experiment to measure single qubit drive amplitudes\nRun Rabi experiment on single qubit 'dut with amp=0.3\nCalibrate single qubit drive amplitudes using Rabi experiment'\nIn k-agents, the translation is done by calling the related translation agents, and the execution agent picks one of the code candidates from the translation agents. To implement the standard RAG implementation of this process, after retrieving the related translation agents, we extract the experiment classes from them and use the signature and documentation of these classes to generate a translation of the instruction.\nFor a fair comparison, we only included code translation agents in the benchmark as the information as the prompt for code generation with procedures is different from that with experiment classes (See Section B2). Finally, we include 17 experiment classes in the benchmark. When triggering the translation agents, we select the 2 agents with the highest score. If there is no available code candidate, we increase the number of agents to trigger by 2. Accordingly, we set 2 as the number of experiment classes retrieved in the RAG method being compared.\nWe additionally note that we use the text-embedding-3-large model from OpenAI as the embedding model throughout all the experiments, even when the LLM we use is not from OpenAI. We use the same embedding model to control the variables and compare differences just in the performance of LLMs."}, {"title": "2. Benchmarking visual inspection", "content": "Here, we describe the method for benchmarking the performance of inspection agents to assess their ability to determine the success of an experiment. The agent has three methods for evaluating success: (1) by reading only the fitting report generated by human-written code, (2) by analyzing only the figures generated from the experimental data, or (3) by first analyzing the figures and generating a report, then combining this report with the fitting results to assess success. These methods are referred to as \"Fitting\", \"Visual\", and \"Combined\", respectively.\nIt is challenging to precisely define success criteria using natural language for the inspection of plots. To address this, we employ a few-shot prompting approach. This involves providing the agent with example images of a successful experiment and a failed experiment, followed by a new image. The agent is asked to determine whether the new image more closely resembles the successful or failed example, with some guidance from natural language descriptions.\nThe benchmarking dataset consists of four typical experimental results representing common types of plots in quantum computing experiments. To evaluate the agents, we specifically crafted a synthetic dataset where it is difficult to assess success based solely on fitting results-highlighting the need for language model assistance. Each experiment produces 100 successful and 100 failed examples, and the agent is tasked with determining whether each experiment is successful or not. Detailed prompts and reference images are provided below.\na. Rabi experiment The rabi experiment represents a set of examples where we expect to observe a set of oscillations. To observe these oscillations, we do a Fourier transform of the input data and plot its spectrum for the LLMs to determine its correctness. Benchmark one sample of this experiment costs on average 6,056 input tokens and 1,220 output tokens."}, {"title": "Examples of the instructions for NormalisedRabi", "content": "Analyze this quantum mechanics Rabi oscillation experiment plot in the Fourier frequency domain. A successful experiment should have a significant peak in the figure.\nFor example, the following Image is a successful Rabi oscillation experiment plot:"}, {"title": "b. Resonator spectroscopy", "content": "The resonator spectroscopy experiment involves looking for a resonator feature in a 1D space. The failure example observes noise and cannot find a clear resonator. Benchmark one sample of this experiment costs on average 6,886 input tokens and 1,365 output tokens."}, {"title": "Examples of the instructions for ResonatorSpectroscopy", "content": "Analyze the resonator spectroscopy magnitude plot to determine if it exhibits characteristics indicative of a resonator.\nSpecifically, look for a sharp dip or peak in the plot, which would signal the presence of a resonator.\nThe rest of the plot should remain relatively stable, without significant fluctuations.\nProvide a detailed analysis of the magnitude and frequency data. Identifying a resonator indicates a successful experiment.\nFor example, the following Image is a successful experiment plot:"}, {"title": "C. Qubit Readout with Gaussian Mixture model", "content": "The experiment visualizes the integrated readout signal on the IQ plane. Typically, the effectiveness of a Gaussian mixture model can be assessed using a spherical Gaussian mixture model (where the covariance matrix is diagonal) by examining the signal-to-noise ratio (SNR), which is the distance between the Gaussian centers divided by the average standard deviation of the two distributions. However, this approach becomes less effective when there are more than two clusters in the response signal, which can occur if the resonator is overdriven or the driving frequency excites higher energy states. In such cases, using the SNR as a metric for model accuracy is not straightforward. In this synthetic dataset, we introduce scenarios where higher excited states may exhibit small leakage or significant population. Describing such complex criteria in natural language also poses challenges. Benchmark one sample of this experiment costs on average 7,840 input tokens and 1,857 output tokens."}, {"title": "Examples of the instructions for Gaussian Mixture Model readout", "content": "Analyze a plot of collected signal data to determine experiment success:\n1. Identify clusters: The signal represents hidden system states, with each state generating a 2D Gaussian distribution (spherical blobs).\n2. Count and evaluate distributions:\nTreat partially overlapped clusters with two visible density centers as separate distributions.\nConsider elliptical distributions with only one visible density center as a single distribution.\nCompare densities of observed distributions.\nIf three or more distributions are present, but only two have major density, consider only the two high-density distributions and ignore the low-density ones.\n3. Experiment outcome:\nSuccess: Exactly two major distributions observed (after accounting for density).\nFailure: Any other outcome (e.g., one distribution, or more than two major distributions).\nFor example, the following Image is a successful experiment plot:"}, {"title": "d. DRAG experiment", "content": "The goal of the DRAG experiment calibration is to identify the intersection point of two lines. We synthesize a dataset where the fitting process will always predict a crossing point between the blue and red lines, though the data may be too noisy to be reliable. The language model is designed to assess whether the input data from the experiment is valid and whether the fitting result can be trusted. Benchmark one sample of this experiment costs on average 7,899 input tokens and 1,980 output tokens."}, {"title": "Examples of the instructions for DRAG", "content": "Analyze the scatter plot with blue and red data points and trend lines:\n1. Compare the slopes of the trend lines.\n2. Assess how well data points fit their trend lines, noting outliers or patterns.\n3. Evaluate data point distribution along the DRAG coefficient axis.\n4. Determine if trend lines accurately represent their datasets.\n5. Compare trends between the two datasets.\n6. Estimate the fitting residuals.\nSuccess criteria:\n1. Distinct trends for each color\n2. Appropriate line fitting, with the blue and red lines has significant difference in distribution.\n3. Lines intersect near the plot's center region, small shifts away from the center is acceptable.\n4. Residuals are within acceptable range.\nIf criteria aren't met, mark the experiment as failed and suggest a new range for the sweep.\nFor example, the following Image is a successful experiment plot:"}, {"title": "e. Randomized benchmarking experiment", "content": "The randomized benchmarking experiment evaluates the error rates of quantum gates by applying a sequence of random gate operations followed by their inverse, measuring how well the system returns to its initial state [74-76]."}, {"title": "Randomized benchmarking example", "content": "Execution agent (input): Measure the single qubit gate fidelity using randomized benchmarking."}, {"title": "Appendix D: Built-in experiments", "content": "Here we detail the implementation of the built-in experiments for single qubit calibration and provide details on how the language model gets prompted. These experiments are standard for superconducting quantum processor calibration, please refer to [46] for more details. For the two-qubit gate related built-in experiments, please refer to appendix E.\na. Ramsey experiment The Ramsey experiment measures the qubit frequency and coherence time of a qubit by applying two \u03c0/2 pulses separated by a free evolution period, during which the qubit's phase oscillation reflects the offset between the driving frequency and the qubit frequency."}, {"title": "Ramsey experiment example", "content": "Execution agent (input): Conduct the Ramsey experiment with parameters \"frequency offset=0.1 MHz\", \"stop time=35 microseconds\", \"step size=0.5 microseconds\"."}, {"title": "Fitting (input):", "content": "The Ramsey experiment for qubit Q1 has been analyzed. The expected offset was set to 0.100 MHz, and the measured oscillation is 0.102+/-0.000 MHz. Oscillation amplitude is 0.599+/-0.019. The number of oscillations is 3.562+/-0.011."}, {"title": "Visual inspection agent (output):", "content": "The plot shows data from a quantum mechanics experiment involving Ramsey oscillations. The data points exhibit a clear, regular oscillatory pattern, indicating successful oscillations. The fit line closely follows the data points throughout the plot, suggesting a good fit quality. The data points are relatively tightly clustered around the fit line, with some minor dispersion. The amplitude of the oscillations is around 0.8, which is close to the ideal value of 1, and the frequency is approximately 0.10176 MHz, which is within the expected range. Overall, the plot demonstrates the typical characteristics of a successful Ramsey oscillation experiment, with clear oscillations, a good fit, and consistent amplitude and frequency."}, {"title": "Final analysis (output):", "content": "The Ramsey experiment for qubit Q1 was successful. The plot shows a clear, regular oscillatory pattern with a good fit. The amplitude of the oscillations is 0.599, which is slightly lower than the ideal value of 1 but still within an acceptable range. The frequency of the oscillations is 0.102 MHz, which is very close to the expected offset of 0.100 MHz. The number of oscillations observed is 3.562, which is within the acceptable range (greater than 3 and less than 10)."}, {"title": "Execution Agent (output):", "content": "The results from the experiment indicate that the Ramsey experiment was successful. The plot shows a clear oscillatory pattern with a good fit, and the final analysis confirms that the frequency and amplitude are within acceptable ranges. The experiment success flag is also set to True, meaning the experiment was successful. Therefore, based on the rules, the next state should be 'Complete'."}, {"title": "b. Rabi experiment", "content": "The Rabi experiment involves driving a qubit with an external electromagnetic pulse on resonance to the qubit frequency, to induce transitions between its quantum states. The oscillation speed of the experiment can be used to infer the rough optimal single-qubit gate pulse amplitude."}, {"title": "Rabi experiment example", "content": "Execution agent (input): Conduct the Rabi experiment to determine the Rabi rate for rough amplitude calibration."}, {"title": "Fitting (input):", "content": "The fitting result of the Rabi oscillation suggests the amplitude of 0.5893195663534404, the frequency of 17.602131636706986, the phase of 1.5439919053018119. The offset of 0.2087721909927606. The suggested new driving amplitude is 0.19815270673482663.From the fitting results, the plot should exhibit 5.104618174645026 oscillations."}, {"title": "Visual inspection agent (output):", "content": "The plot shows data points that exhibit a clear, regular oscillatory pattern, indicative of Rabi oscillations. The amplitude of the oscillations appears consistent, with peaks and troughs occurring at regular intervals. The frequency of the oscillations also seems stable throughout the time range. Overall, the plot demonstrates the typical characteristics of successful Rabi oscillation experiments, with well-defined oscillations and consistent amplitude and frequency."}, {"title": "Final analysis (output):", "content": "The experiment was successful. The plot shows a clear, regular oscillatory pattern with consistent amplitude and frequency, which is characteristic of a successful Rabi oscillation experiment. The fitting results suggest an amplitude of 0.5893, a frequency of 17.6021, and a phase of 1.5440. The plot should exhibit approximately 5.1 oscillations, which is within the acceptable range of 5 oscillations."}, {"title": "Execution Agent (output):", "content": "Transitioning to the next stage Stage2 with the following description: Conduct the Pingpong experiment. The experiment was successful based on the results provided. The plot shows a clear, regular oscillatory pattern with consistent amplitude and frequency, which is characteristic of a successful Rabi oscillation experiment. The fitting results are consistent with the plot, and the final analysis confirms the success of the experiment. Since the experiment was successful, the next stage should be Stage2 (Pingpong Experiment) as per the rules."}, {"title": "C. Pingpong experiment", "content": "The ping-pong experiment repeatedly applies sequences of quantum gates to estimate the precise amplitude of the single-qubit drive pulse."}, {"title": "Pingpong experiment example", "content": "Execution agent (input): Conduct the Pingpong experiment."}, {"title": "Visual inspection agent (output):", "content": "The amplitude plot shows a decreasing trend initially, followed by a stabilization around the 5th iteration. This indicates that the amplitude converges through the iterations, as the values become relatively constant after the initial decrease."}, {"title": "Final analysis (output):", "content": "The amplitude plot shows a decreasing trend initially, followed by stabilization around the 5th iteration, indicating that the amplitude converges through the iterations. This suggests that the experiment was successful in achieving fine calibration of the single qubit pulses."}, {"title": "Execution Agent (output):", "content": "The results from both the plot and Final analysis indicate that the amplitude converges and stabilizes around the 5th iteration, which suggests that the experiment was successful in achieving fine calibration of the single qubit pulses. Additionally, the 'Experiment success' result is marked as True, confirming that the experiment was successful. Therefore, based on the rules, the next stage should be 'Complete' as the experiment has been successful."}, {"title": "d. DRAG calibration experiment", "content": "The DRAG (Derivative Removal by Adiabatic Gate) calibration experiment we used in this experiment reduces phase errors during gate operations by sweeping the DRAG parameter of the pulse."}, {"title": "DRAG experiment example", "content": "Execution agent (input): Conduct the DRAG calibration on the single qubit gate."}, {"title": "Fitting (input):", "content": "Sweep start: -0.009333333333333334 Sweep stop: 0.0026666666666666666 The fitting results are as follows: The estimated optimal DRAG coefficient is -0.0061392677674084704 The estimated optimal DRAG coefficient falls within the central half of the sweep. Residual average for Xp: 0.002345"}]}