{"title": "Screw Geometry Meets Bandits: Incremental Acquisition of Demonstrations to Generate Manipulation Plans", "authors": ["Dibyendu Das", "Aditya Patankar", "Nilanjan Chakraborty", "C. R. Ramakrishnan", "IV Ramakrishnan"], "abstract": "In this paper, we study the problem of methodically obtaining a sufficient set of kinesthetic demonstrations, one at a time, such that a robot can be confident of its ability to perform a complex manipulation task in a given region of its workspace. Although Learning from Demonstrations has been an active area of research, the problems of checking whether a set of demonstrations is sufficient, and systematically seeking additional demonstrations have remained open. We present a novel approach to address these open problems using (i) a screw geometric representation to generate manipulation plans from demonstrations, which makes the sufficiency of a set of demonstrations measurable; (ii) a sampling strategy based on PAC-learning from multi-armed bandit optimization to evaluate the robot's ability to generate manipulation plans in a subregion of its task space; and (iii) a heuristic to seek additional demonstration from areas of weakness. Thus, we present an approach for the robot to incrementally and actively ask for new demonstration examples until the robot can assess with high confidence that it can perform the task successfully. We present experimental results on two example manipulation tasks, namely, pouring and scooping, to illustrate our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to perform manipulation tasks is key to the use of robots in many application areas, including flexible manufacturing, service robotics, and assistive robotics. Complex manipulation tasks, such as opening drawers, pouring, scooping, etc., require the motion of the end effector to be constrained during the execution of the task. It is often hard to manually specify the complete set of constraints for each task. One popular approach is to generate the manipulation plans (a sequence of configurations of the robot's manipulators such as its arms and grippers) directly based on a set of human-provided demonstrations. Such demonstrations can be obtained from video, teleoperation, kinesthetic manipulation of the robot's end effector, or simulations in virtual or augmented reality, e.g., see [1]\u2013[13]. Despite this rich history, the problem of evaluating the sufficiency of a given set of demonstrations and systematically seeking additional human-provided demonstrations has remained open.\nOur Setting: We focus on manipulation tasks with rigid objects in a tabletop environment where all task-relevant objects are located within a work area (see Figure 1)."}, {"title": "II. RELATED WORK", "content": "Manipulation tasks, such as pouring from a container or opening a hinged door, involve constraints on motion. Such constraints are hard and often impossible to specify generically with soundness or completeness guarantees. Learning from Demonstrations (LfD) [2], [3], [18] avoids this problem by using a set of demonstrations provided a priori to generate a constrained motion. This naturally raises two questions: 1) whether plans for motion plans generated from demonstrations indeed satisfy the implicit constraints of the given task, and 2) whether a set of demonstrations are sufficient to generate robust motion plans for tasks in a specified work area.\nA. Motion planning from demonstrations\nThere are a wide variety of techniques for generating motion plans from demonstrations, including probabilistic models based on hidden Markov models (HMM) [19], Gaussian mixture models (GMM) [20], and bio-inspired techniques based on dynamical systems such as dynamical movement primitives (DMP) [21]\u2013[24]. However, these works attempt to mimic the demonstrations regardless of aspects that are crucial and meaningful to the implicit task constraints. Hence, there is no formal way to establish that a generated plan is correct for the given task.\nAn important class of task constraints are restrictions on the path of the end effector or manipulated object. Holding an open container upright when moving it, pulling a door in a circle around its hinge, etc., are examples of path-based task constraints. Our earlier work has developed methods based on screw linear interpolation, ScLERP [14], [15] to generate motion plans in a subspace of the task space that preserves path-based task constraints implicit in the demonstrations. By focusing on the parts of the trajectory close to the objects of interest, these works pay particular attention to the constraints most relevant to the task. Two additional points on ScLERP-based motion plan generation are noteworthy. First, planning in the task space implies that the same demonstrations can be used to generate plans for various robot configurations (varied degrees of freedom, joint lengths, etc.). Second, planning is done online, ensuring that the generated plan is executable by a given robot arm (i.e. satisfies joint limits and constraints).\nB. Demonstration sufficiency using self-evaluation\nThe high-level objective of our work is to develop a framework in which the robot can self-evaluate its ability to generate low-level motion plans using a few demonstrations for complex manipulation tasks such as pouring, scooping, etc. This enables the robot to methodically seek additional demonstrations at targeted locations until it achieves a threshold of confidence in plan generation.\nPrevious works on self-evaluation of the satisfiability of task-related constraints [25], [26], have focused mainly on sensor information available from a perception system [27], [28] or high-level task specification [29]. However, none of the existing work on self-evaluation can be used to evaluate the robot's confidence in manipulation plan generation using kinesthetic demonstrations.\nA recent work [30] in the context of navigation tasks looks at the problem of demonstration sufficiency by doing autonomous assessment. However, in that work, a demonstration is phrased as expert-provided directions required to successfully navigate a 2D grid environment, which does not apply in the setting of manipulation problems because in manipulation, task constraints are embedded in the entire trajectory of the demonstration."}, {"title": "III. SCREW-GEOMETRY BASED MOTION PLANNING", "content": "We first describe the pertinent mathematical background along with motion planning based on screw geometry from demonstrations, which will allow us to formally define our problem in \u00a7IV."}, {"title": "A. Task Instance", "content": "The reference frame fixed to the link of the robot arm beyond the last joint or on any object that is rigidly held by the robot is called the end effector frame. The pose of the end effector refers to the position and orientation of the end effector frame. We consider manipulation planning tasks where the task specification includes the initial pose of the end effector and the poses of other task-relevant objects that determine the end effector's goal pose. Let $X \\subseteq SE(3)^n$ be a compact set, where $n$ is a positive integer representing the total number of task-relevant objects, including the object held by the robot. For a given manipulation task, an instance of the task is defined by $x \\in X$. Thus, the set $X$ defines the set of all task instances for a given task. We assume that all task-relevant objects are within the set of all reachable poses of the robot's end effector."}, {"title": "B. Kinesthetic Demonstration and its Representation", "content": "For any manipulation task, there may be constraints on the end-effector's path that characterize the essential characteristics of the manipulation task. A kinesthetic demonstration of a task, given by holding the hand of the robot in a zero-gravity mode, traces an end-effector path that satisfies the task constraints. Let $J \\subset \\mathbb{R}^d$ be the joint space, i.e., the set of all possible joint configurations of the robot, where $d$ is the number of joints of the robot's manipulator arm. For a manipulation task instance, $y \\in X$, a kinesthetic demonstration, $\\Theta$, is recorded as a discrete sequence of points from a joint space path, i.e., a sequence of joint angle configurations $\\Theta = (\\theta^{(1)},..., \\theta^{(m)})$, where $\\theta^{(j)} \\in J, j = 1,2,...,m$, is a vector of dimension $d$, whose $i$-th component, $\\theta_i^{(j)}$, represents the $i$-th joint angle of the robot arm. Every demonstration instance, $\\Theta$, is associated with a task instance $y$. We keep this association implicit to avoid notational clutter.\nUsing a single demonstration to represent the task space motion constraints in the joint space is statistically ill-posed since the (position inverse kinematics) mapping from the task space to the joint space is multi-valued (especially for redundant manipulators). Typically, any joint space-based technique uses multiple demonstrations, and it is usually"}, {"title": "C. Motion Planning from a Single Demonstration and Evaluation of Motion Plan", "content": "We will use the two-step motion planner discussed in [15] to generate motion plans for new task instances that differ from the demonstration instance. In particular, we will denote such a motion plan generator by $\\text{hasMotionPlan}(x, \\Gamma)$ which takes as input any task instance $x$, a demonstration $\\Gamma$ and decides whether or not a motion plan can be generated for $x$ using $\\Gamma$. The process of plan generation involves (a) transferring the guiding poses, $\\Gamma$, to a new set of guiding poses, $\\Gamma'$, to account for the new poses of the task-relevant objects and (b) using screw linear interpolation (SCLERP) to generate the path between two consecutive guiding poses. As proven in [16], ScLERP ensures that the constant screw constraint encoded by two consecutive guiding poses is always satisfied without explicitly enforcing it. Therefore, the planner used in this paper to generate a manipulation plan always ensures that the manipulation constraints are satisfied. However, the motion planner may fail to generate a plan for a given task instance, e.g. due to violation of joint limits.\nThus, we assume that given a demonstration $\\Gamma$ we have a mapping $f_{\\Gamma}: X \\rightarrow \\{0,1\\}$, such that, for any new task instance $x \\in X, f_{\\Gamma}(x) = 1$, if and only if the robot can successfully generate a manipulation plan for the task instance $x$ using the demonstration $\\Gamma$. If we consider a set of demonstrations $D = \\{\\Gamma_i\\}_{i=1}^k$ such that for any $x \\in X, \\max_{\\Gamma \\in D} f_{\\Gamma}(x) = 1$, then we have a set of demonstration examples such that the robot is guaranteed to generate a manipulation plan for any task instance of a given task."}, {"title": "IV. SELF-EVALUATION", "content": "In this section, we formally characterize the sufficiency of a set of demonstrations $D$ to cover a given set of task instances $X$. If $D$ is considered insufficient by this metric, we describe an approach to identify a region (subset) of $X$ that contains a good candidate for a new demonstration. This forms the basis for incremental acquisition of demonstrations further developed in \u00a7V.\nA. Sufficiency of Demonstrations\nFor a demonstration, $\\Gamma_i$, let $B(\\Gamma_i, X) \\subseteq X$, be the set of task instances where i can be used to generate manipulation plans successfully. Thus,\n$B (\\Gamma_i, X) = \\{x \\in X \\mid f_{\\Gamma_i}(x) = 1\\}$ (1)\nLet $\\text{Vol}(A)$, with $A \\subseteq X$ be a volume measure [31] defined on $X$.\nDefinition 1 (Coverage): The coverage of a demonstra-tion $\\Gamma_i$ with respect to a set of task instances $X$, denoted by $P_X(\\Gamma_i)$, is defined as:\n$P_X(\\Gamma_i) = \\frac{\\text{Vol}(B(\\Gamma_i, X))}{\\text{Vol}(X)}$\nLet $D = \\{\\Gamma_i\\}_{i=1}^k$ be a set of demonstrations. We can lift B from individual demonstrations to sets of demonstrations. Let $B(D, X) = \\bigcup_{i=1}^k B(\\Gamma_i, X)$. Then,\n$B(D, X) = \\{x \\in X \\mid \\max_{\\Gamma \\in D} f_{\\Gamma} (x) = 1 \\}$ (2)\nThus, $B(D, X)$ is the set of all task instances in $X$ such that there is at least one demonstration D that can be used to generate a successful manipulation plan.\nAnalogously, we can lift the notion of coverage to sets of demonstrations:\n$P_X(D) = \\frac{\\text{Vol}(B(D, X))}{\\text{Vol}(X)}$ (3)\nDefinition 2 (Sufficiency): Given a probability threshold $0 \\le \\beta \\le 1$, a set of demonstrations $D$ is said to be sufficient for a set of task instances $X$ with respect to $\\beta$ if\n$P_X(D) \\ge \\beta$\nB. Identification of New Demonstration Candidates\nIf a set of demonstrations $D_0$ is insufficient for task instances $X$, we would like to incrementally seek additional demonstrations, one at a time, thereby constructing a sequence of demonstration sets $D_0, D_1,...$ such that some element of this sequence, say $D_n$ is sufficient for $X$.\nWe identify a small subset of task instances to seek the next demonstration by partitioning $X$ into $K$ disjoint compact sets $X_j: 1 \\le j \\le K$. Note that\n$P_X(D, X) = \\frac{\\text{Vol}(B(D, X))}{\\text{Vol}(X)} = \\frac{\\Sigma_j \\text{Vol}(B(D, X_j))}{\\Sigma_j \\text{Vol}(X_j)}$\nThen, if $P_X(D_i) < \\beta$, then there is a partition $X_j$ such that $P_{X_j}(D_i) < \\beta$, that is, $D_i$ is not sufficient for $X_j$. Such a subset $X_j$ contains candidate task instances for the next demonstration.\nC. Formulation as Multi-Arm Bandit Optimization\nWe pose the problem of identifying the partition $X_j$ that has the least coverage, $P_{X_j}(D_i)$, in terms of the K-arm bandit optimization problem [32]. In the special case of Bernoulli K-arm bandit, each pull of the $j$-th arm yields"}, {"title": "V. INCREMENTAL DEMONSTRATION ACQUISITION USING SELF-EVALUATION", "content": "Algorithm 1 codifies our approach to acquire demonstrations one at a time such that the set is sufficient for a given set of task instances $X$. The algorithm takes the K partitions of task instances, $X_{1...K}$, the threshold $\\beta$ to determine sufficiency, and two parameters $\\epsilon$ and $\\delta$, both $\\in (0,1)$, and a non-empty set of initial demonstrations, $D_0$. Each element of the set $D_0$ is a sequence of guiding poses, where two consecutive guiding poses constitute a constant screw motion. The algorithm proceeds by adding"}, {"title": "A. Finding an $\\epsilon$-Optimal Arm", "content": "Algorithm 2 uses a na\u00efve $(\\epsilon, \\delta)$-PAC learning algorithm to identify an $\\epsilon$-optimal arm of our bandit formulation. In this algorithm, $T_j$ (line 4) is the set of all task instances for which we cannot find a motion plan and hence have reward 1. This algorithm ensures with confidence $1-\\delta$ that the empirical estimate of the expected reward for each arm, $\\hat{\\mu}_j$, is $\\epsilon$-close to the true expected reward $\\mu_j$. That is,\n$\\mathbb{P}(\\max_j |\\hat{\\mu}_j - \\mu_j| < \\epsilon) \\ge 1 - \\delta$.\nThe samples for each arm are drawn from uniformly distributed random task instances in each partition. Since each partition is a subset of SE(3), we can use the techniques described in [33] used for sampling from SE(3). The reward for each sample $x$ is determined by checking if an executable motion plan can be derived for $x$ based on the currently available set of demonstrations. We use the SCLERP based motion planner [15] to generate plans for each sample task instance. The ScLERP motion planner generates a set of guiding poses for each given task instance, but may fail to produce a sequence of feasible joint configurations needed to execute the plan even when such feasible configurations exist. This is a fundamental problem due to the inherent difficulty in always finding feasible paths in the joint space via inverse kinematics. Hence, determining rewards via SCLERP motion planner gives us an upper bound of rewards. Nevertheless, this only means that the set of demonstrations we find will be conservative sufficient to cover the task instances - but may not be minimal. The number of samples drawn from each arm, $N$, needed to ensure the confidence bound, is $\\frac{\\ln(2/\\delta)}{\\epsilon^2}$, as shown by a short proof based on the Hoeffding-Chernoff inequality given in the appendix A.\nNote that we do not need to estimate each $\\mu_j$ accurately to identify $j^*$; only the expected rewards of arms that are close to optimal need to be evaluated accurately. Several PAC-learning algorithms reduce the number of samples needed to identify an arm that is $\\epsilon$-close to optimal with confidence $(1-\\delta)$ [34]. Other techniques such as UCB [35] may also be used to identify $j^*$. We are currently evaluating the"}, {"title": "B. Suggestion for the Next Demonstration", "content": "Having determined the partition with least coverage, we seek the next demonstration using the following heuristic. Since we use ScLERP-based motion planner to check for feasible motion plans, we know that for each failed task instance (i.e. those in $T_j$ with reward 1), there will be a screw segment $(g^{(i)}, g^{(i+1)}) \\in G$ such that $g^{(i)}, g^{(i+1)} \\in G$ for which we could not find a feasible joint-space path (due to joint limit violations). We choose the failed task instance $y^*$ for which the joint-limit violation occurred in the earliest screw segment. Intuitively, a task failing early may have fewer viable alternative executions."}, {"title": "C. Obtaining a New Demonstration", "content": "A kinesthetic demonstration is collected, as described in \u00a7 III-B, by recording a discrete sequence of joint angles$\\,\\Theta$ and extracting from them a sequence of guiding poses $\\Gamma$. Note that even though we were seeking a demonstration for task instance $y^*$, the task instance for the provided demonstration may be different object poses may differ due to errors in manual placement."}, {"title": "D. Stopping Condition", "content": "This incremental process continues until the robot is confident enough that the overall success probability in the entire work area meets or exceeds a chosen threshold $\\beta$ i.\u0435., $P_X(D) \\ge \\beta$. In our setup $\\mu_{j^*}$ is the (true) probability of failure to generate a successful motion plan in the region $X_{j^*}$ corresponding to the optimal bandit arm $j^*$. When $\\mu_{j^*} < 1-\\epsilon-\\beta$, we can then claim with confidence $1-\\delta$ that for each partition $X_j, P_{X_j}(D_i) \\ge \\beta$. Note that we only know the empirical estimate $\\hat{\\mu}_j$ with the constraint $|\\mu_j - \\hat{\\mu}_j| < \\epsilon$. The testable condition used as the stopping condition (line 4 of Alg. 1) is $\\hat{\\mu}_{j^*} < 1-\\epsilon-\\beta$ since it implies $\\mu_{j^*} < 1-\\beta$. When this condition is satisfied, we can claim with confidence $(1-\\delta)$ that the current set of demonstrations $D$ is sufficient for all task instances in $X$.\nNote that although $\\hat{\\mu}_{j^*}$ is monotonically non-increasing, we cannot bound the number of demonstrations needed to satisfy the stopping condition a priori. Alternatively, if we use a budget of a maximum number of demonstrations as our stopping criterion or stop early, we can compute the probability $\\beta' = 1 - \\epsilon - \\hat{\\mu}_{j^*}$ such that for each partition the robot believes it will be successful with probability $\\beta'$. Here, $\\hat{\\mu}_{j}$ is the largest empirically estimated failure probability among all the regions in the last iteration."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "We present experimental results for two example manipulation tasks, namely pouring and scooping. Both tasks are characterized by end-effector motion constraints that must be satisfied for successful manipulation. Fig. 3 shows screenshots of the example demonstrations.\nWe present three types of experimental results.\nFirst, we illustrate the process of self-evaluation and interactive demonstration acquisition for both pouring and"}, {"title": "A. Incremental Acquisition of Demonstrations", "content": "To illustrate the application of Alg. 1 to incrementally acquire a sufficient set of demonstrations, we choose $\\epsilon = 0.02, \\delta = 0.05, \\beta = 0.95$, and $K = 16$ for the experiments.\nFig. 4 shows the results of the experiment for pouring and scooping. In particular, Fig. 4(a) shows the two probability heat maps after incrementally acquiring 2 demonstrations for the pouring task. After acquiring the 1st demonstration (Fig. 4(a) left heat map), Alg. 1 picks the region with the highest failure probability (ties broken arbitrarily). In this case, the chosen region happens to be the one in the top right corner, from where we acquire the 2nd demonstration, and after collecting that, the failure probability heatmap evolves into the one shown to the right of Fig. 4(a). Similarly, in Fig. 4(b), we show the four failure-probability heat maps for the scooping task, after acquiring 4 demonstrations incrementally. The results indicate that for pouring and scooping, respectively, 2 and 4 demonstrations are sufficient for the robot to have a belief that with probability $0.95 (1 - \\delta)$, it can perform the task successfully for 95%($\\beta$) of the task instances in the given region $X$."}, {"title": "B. Effect of the Choice of K", "content": "The number of demonstrations obtained above may change with different executions and also depends on the choice of K. To get an idea of the distribution of the number of sufficient demonstrations, we performed 1000 tests using self-evaluation, each for a different value of the number"}, {"title": "C. Experimental Validation of Manipulation Capability Over the Prescribed Work Area", "content": "In the results discussed thus far, sufficient sets of kinesthetic demonstrations were obtained using a robot, but plans for random task instances were evaluated via simulation. Next, we describe experiments to validate the simulation results by executing the generated plans on a robot.\nFor a given set of sufficient demonstrations, we generated random task instances over the workspace and a plan was generated for each task instance, demonstration pair. The first feasible plan (which did not violate the joint limits) was used to execute the task. We selected 32 random task instances (2 from each of the 16 subregions) for which plan generation was successful in simulation and executed the generated plans. We did not observe execution failure in terms of joint limit violation in any of these executions."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This paper presents a novel approach to systematically obtain a sufficient set of kinesthetic demonstrations for complex manipulation tasks, one example at a time, such that the robot can develop a probabilistic confidence bound in its ability to generate feasible manipulation plans. Inspired by multi-arm bandit strategies, we propose an algorithm to partition the work area into disjoint subregions, ensuring successful plan generation in each region with a given probability that ultimately ensures overall success with high confidence.\nThe number of regions of interest for demonstrations (and the task space) increases exponentially with the number of task-relevant objects. Consequently, for the algorithm getBestArm (Alg. 2) to scale well, we will need to use a combination of non-naive sampling methods such as adaptive sampling [35], and hierarchical methods to focus on smaller sets of promising candidates.\nFinally, we are studying whether demonstrations given in one context can be reused in a new environment, i.e., a new work area or robot. In this setting, it is possible to collect additional demonstrations in the new environment using the performance of the reused demonstrations as prior knowledge. Techniques for effectively reusing demonstrations in different environments remain an area to be further explored."}, {"title": "APPENDIX", "content": "A. Identification of an $\\epsilon$-Optimal Arm in K-Arm Bandit\nGiven K arms with unknown binary reward distributions $R_j$, $j \\in [1, K]$, where $\\mu_j = \\mathbb{E}[R_j]$, the objective is to identify an arm 'a' which is $\\epsilon$-close to the optimal arm i.e. $\\max\\{\\mu_j\\} - \\mu_a \\le \\epsilon$.\nAlg. 3 gives a natural way to detect the arm with the highest expected reward by sampling each arm an equal number of times, say, for a given number of samples T, and output the arm with the best empirical mean."}, {"title": "Theorem 1.1:", "content": "Algorithm 3 is correct if $|\\mu_j - \\hat{\\mu}_j| < \\epsilon/2 \\forall j \\in [1, K]$.\nProof: We assume '*' is the unidentified optimal arm, that is, $\\forall j, \\mu_* \\ge \\mu_j$ and 'a' is the arm found by Algorithm 3, that is $\\forall j, \\hat{\\mu}_a \\ge \\hat{\\mu}_j$. We need to show that $\\mu_* - \\mu_a < \\epsilon$.\nOnly $\\hat{\\mu}_a \\in [\\mu_*, \\mu_* + \\epsilon]$ (the hatched interval below) is consistent with our previous assumption as observed below.\nThus, from the above diagram it is obvious that $\\hat{\\mu}_a - \\frac{\\epsilon}{2} \\le \\mu_a \\le \\hat{\\mu}_*$. Therefore, the difference between $\\mu_*$ and $\\mu_a$ is the maximum when $\\mu_a = \\hat{\\mu}_a - \\frac{\\epsilon}{2}$ and $\\mu_* = \\hat{\\mu}_* + \\frac{\\epsilon}{2}$ and the maximum difference is $\\epsilon$.\nHence, $\\mu_* - \\mu_a \\le \\epsilon$ that is, if $|\\mu_j - \\hat{\\mu}_j| \\le \\epsilon/2 \\forall j \\in [1, K]$, Algorithm 3 always returns an $\\epsilon$-optimal arm.\nTheorem 1.2: Algorithm 3 returns an $\\epsilon$-optimal arm with a probability of at least $1 - 2K e^{-\\frac{T \\epsilon^2}{2K}}$.\nProof: Let $\\mathcal{E}_i$ be the event $|\\mu_i - \\hat{\\mu}_i| \\le \\frac{\\epsilon}{2}$. Thus, Alg. 3 outputs the correct arm, i.e. an $\\epsilon$-optimal arm 'a' with probability\n$\\mathbb{P}(\\mathcal{E}) = 1-\\mathbb{P} \\left(\\bigcup_{i=1}^K \\overline{\\mathcal{E}_i}\\right) \\qquad[\\overline{\\mathcal{E}_i} \\text{ is complement of }\\mathcal{E}_i]\\qquad(4)$\n$\\qquad = 1-\\mathbb{P} \\left(\\bigcup_{i=1}^K A_i\\right) \\le 1-\\sum_{i=1}^K \\mathbb{P}(A_i) \\le \\sum_{i=1}^K \\mathbb{P}(\\overline{\\mathcal{E}_i})$\nUsing the Hoeffding-Chernoff inequality for Bernoulli random variable $R_j$ with true-mean $\\mu_j$ and estimated-mean $\\hat{\\mu}_j$ determined using $t$ samples, we get\n$\\mathbb{P}(|\\hat{\\mu}_j - \\mu_j| \\ge \\frac{\\epsilon}{2}) \\le 2e^{-\\frac{t \\epsilon^2}{2t}} \\qquad \\forall j \\in [1, K]$\\n$\\mathbb{P}(\\overline{\\mathcal{E}_i}) = 1 - 2e^{-\\frac{T \\epsilon^2}{4K}}$\n$\\mathbb{P}(\\mathcal{E}) = 2e^{-\\frac{T \\epsilon^2}{4K}} \\qquad (5)$\nSubstituting (5) in (4), we get $\\mathbb{P}(\\mathcal{E}) = 1 - 2Ke^{-\\frac{T \\epsilon^2}{2K}}$\nHence, Algorithm 3 outputs an $\\epsilon$-optimal arm with a probability of at least $1 - 2Ke^{-\\frac{T \\epsilon^2}{2K}}$.\nCorollary 2.1: For Alg. 3 to succeed with a probability of at least $1 - \\delta$, the number of samples $T$ should be at least $\\frac{4K}{\\epsilon^2}\\ln(\\frac{2K}{\\delta})$.\nSubstituting $2Ke^{-\\frac{T \\epsilon^2}{2K}}$ by $\\delta$ in Theorem 1.2 gives us an $(\\epsilon, \\delta)$-PAC bound, in which using at least $\\frac{4K}{\\epsilon^2}\\ln(\\frac{2K}{\\delta})$ samples we can identify an $\\epsilon$-optimal arm with a confidence of at least $1 - \\delta$."}]}