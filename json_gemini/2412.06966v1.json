{"title": "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice", "authors": ["A. Feder Cooper", "Christopher A. Choquette-Choo", "Miranda Bogen", "Matthew Jagielski", "Katja Filippova", "Ken Ziyu Liu", "Alexandra Chouldechova", "Jamie Hayes", "Yangsibo Huang", "Niloofar Mireshghallah", "Ilia Shumailov", "Eleni Triantafillou", "Peter Kairouz", "Nicole Mitchell", "Percy Liang", "Daniel E. Ho", "Yejin Choi", "Sanmi Koyejo", "Fernando Delgado", "James Grimmelmann", "Vitaly Shmatikov", "Christopher De Sa", "Solon Barocas", "Amy Cyphert", "Mark Lemley", "danah boyd", "Jennifer Wortman Vaughan", "Miles Brundage", "David Bau", "Seth Neel", "Abigail Z. Jacobs", "Andreas Terzis", "Hanna Wallach", "Nicolas Papernot", "Katherine Lee"], "abstract": "We articulate fundamental mismatches between technical methods for machine unlearning in Generative AI, and documented aspirations for broader impact that these methods could have for law and policy. These aspirations are both numerous and varied, motivated by issues that pertain to privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of targeted information from a generative-AI model's parameters, e.g., a particular individual's personal data or in-copyright expression of Spiderman that was included in the model's training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual's data or reflect the concept of \u201cSpiderman.\u201d Both of these goals\u2014the targeted removal of information from a model and the targeted suppression of information from a model's outputs\u2014present various technical and substantive challenges. We provide a framework for thinking rigorously about these challenges, which enables us to be clear about why unlearning is not a general-purpose solution for circumscribing generative-Al model behavior in service of broader positive impact. We aim for conceptual clarity and to encourage more thoughtful communication among machine learning (ML), law, and policy experts who seek to develop and apply technical methods for compliance with policy objectives.", "sections": [{"title": "Introduction", "content": "\u201cMachine unlearning\" has recently captured public attention as a potential general-purpose approach for purging unwanted information from machine-learning (ML) models. It raises problems of technical interest, but perhaps more significantly, machine unlearning also finds broader appeal outside of technical circles for its perceived ability to support law and policy aims (Section 2). Since around 2016, technical experts and policymakers have invoked unlearning as a way to operationalize compliance with an individual's \u201cright to be forgotten,\u201d with respect to removing personal data from deployed models, as granted in the E.U.'s General Data Protection Regulation (GDPR) [102].\u00b9 Now, with the emergence of Generative AI, machine unlearning's presumptive mandate has expanded significantly. More and more, research papers, policy briefs, and media reports suggest machine unlearning as an approach for meeting a broad range of objectives for both open and closed models and systems,\u00b2 spanning privacy, copyright, safety, and more [e.g., 51, 63, 71, 75, 96, 118, 134, 140].\nUnfortunately, the fit between unlearning and policy is not so straightforward in practice. Machine unlearning is a set of technical methods and here, as always, there are critical gaps-gaps that are too often overlooked\u2014between what technical methods do and what policy aims to achieve [31]. Our goal is to provide conceptual clarity that elicits these gaps, and to encourage more thoughtful communication among ML, law, and policy experts who seek to develop and apply technical methods for compliance with law and policy objectives (Sections 6 & 7). In summary:\nDeleting information from an ML model is not well-defined. First, information cannot be deleted from an ML model in the same way that it can from a database. During training, data are transformed into patterns that get encoded in the model's parameters\u2014patterns that are not directly or easily interpretable (Section 2). There is no way to cleanly identify, target, and delete specific, contained pieces of information from these parameters. Instead, it is possible, even if computationally expensive, to train a new model on a dataset that does not contain problematic data (Section 4)\u2014for example, a specific scientific paper on designing novel flu viruses or a specific in-copyright image of Spiderman. This is typically what it means to \u201cremove\u201d data from a generative-AI model in machine unlearning, which deviates from intuitive understandings of the term. Removal applies to discrete pieces of data in the training dataset before training occurs; it cannot target the latent patterns that a trained model has learned across different data examples (Section 3). For example, there is no clear way to remove the more general concepts of \u201chow to synthesize a toxic molecule\" or \"Spiderman\u201d from a model; there is no single obvious or appropriate way to go about translating such open-ended aims to concrete tasks that can be implemented by an algorithm (Section 5).\nRemoving information from a model does not provide guarantees about model outputs. Second, removing information from a model's parameters does not guarantee that this model could never produce related information at generation time. Even if one removed all in-copyright images of Spiderman from a model's training data, this does not mean it would be impossible for the model to generate outputs that resemble Spiderman when put to use. Generative-AI models are impressive in part because they are able to generate novel outputs that transcend the information that is exactly contained in their training data. It is therefore a mistake to think that making a limited set of targeted changes to a model's parameters is sufficient to make promises about what types of outputs that model could or could not possibly produce (Section 5). This point is further complicated by the fact that users can introduce information at generation time through prompts. In the context of machine unlearning, user prompts can even reintroduce information whose effects were previously removed from the model's parameters. Combining such a prompt with a model's reasoning abilities, it may be possible to produce outputs that are effectively the same as those that would have been produced if an unlearning method had never been used in the first place (Section 5).\nIn general, removal on its own is often neither necessary nor sufficient to constrain model outputs in a controlled manner. Instead, suppressing certain model outputs from being surfaced to users may be a more appropriate area of focus for technical methods [e.g., 39]. While it is now common to"}, {"title": "Background and Motivations for Machine Unlearning", "content": "The natural starting place for our discussion is to address first what machine learning attempts to accomplish. This will let us provide an intuition, grounded in an interpretation of the E.U.'s General Data Protection Regulation (GDPR) that is prevalent in ML research, for why one might want to do unlearning to revert or change the results of this process (Section 2.1). From this intuition, we provide a loose definition for machine unlearning that originates from traditional AI settings (Section 2.2). We then discuss evolving motivations for machine unlearning in response to the ascendance of Generative AI (Section 2.3). These new motivations have encouraged an expanded definition for machine unlearning (Section 2.4), which we will rely on throughout the paper."}, {"title": "ML research and its interpretation of the GDPR", "content": "In brief, machine learning is an area of computer science and engineering that uses techniques from probability and statistics to develop algorithms that produce models that encode patterns learned from data. Model architectures range from simple linear models or decision trees to complicated, large-scale neural networks. In a bit more detail, we rely on the GenLaw glossary [31]:\u2074\nMachine-learning neural-network models all contain parameters. ... During an algorithmic process called training, these parameters are repeatedly updated based on the training data within the training dataset that the model has seen. Each update is designed to increase the chance that when a model is provided some input, it outputs a value close to the target value we would like it to output. By presenting the model with all of the examples in a dataset and updating the parameters after each presentation, the model can become quite good at doing the task we want it to do.\nEach training-data example in the training dataset \u201cis a self-contained piece of data, such as an image, a piece of text (e.g., content of a web page), a sound snippet, a video, or some combination of these\" [31]. These examples can also include personal information-home addresses, sensitive demographic attributes, health information, personal photos, and more.\nIn some jurisdictions, individuals have rights associated with the control of their personal data. Notably, since its adoption in 2018, Article 17 of the E.U.'s GDPR provides the \"Right to erasure\" (more commonly called the \"right to be forgotten\") [45, 102], which gives individuals rights (with exceptions) to demand that companies delete their personal data.\u2075 ML researchers often interpret Article 17 to apply to both to the individual's data examples that have been used as training data and to the resulting trained models themselves [e.g., 10, 24, 73, 74, 84, 90, 106] (Appendix A).\u2076 This presents a problem because, in almost all cases, the model would not just be trained on a specific, right-exercising individual's data. It would also be trained on data associated with thousands of others, if not many more. Wholesale erasure of a trained model, in response to one individual's deletion request, would therefore likely be an extreme, over-broad interpretation of Article 17.\u2077\nThis problem raises a natural question for ML research: rather than deleting a trained model altogether, is it possible to develop algorithms that can achieve more targeted removal of training data from the model? In the specific context of ML research's common interpretation of the GDPR: is it possible to remove the influence of the right-exercising individual's data from the model, without imposing on the model controller the undue burden of the cost of retraining a new model from scratch without that individual's data examples (Section 4.1)? Machine unlearning is the area of ML research that attempts to address this question."}, {"title": "A loose definition for machine unlearning", "content": "What is referred to as \u201cmachine unlearning\" in the technical literature actually corresponds to a wide variety of different methods and techniques, which are loosely grouped together. For this reason, we will rely on a loose, intuitive (rather than rigorous) definition of machine unlearning that we construct in relation to this common underlying technical motivation. In relation to early research, machine unlearning is a subarea of machine learning that develops methods for the targeted removal of the effect of training data from the trained model. We will soon refine this definition (Section 2.4), in response to changing motivations in the field for when to use unlearning (Section 2.3) and how to implement it (Section 4).\nThis definition is deliberately broad, rather than prescriptive. We intentionally do not include specific requirements for how certain information is \u201ctargeted\u201d or \u201cremoved.\u201d\u2078 For now, we also are not prescriptive about what the exact \u201ceffects\u201d are of \u201clearned information\u201d on the trained model's behavior. We will address this in more detail in Section 3, where we discuss different types of learned information that could reasonably be targeted for unlearning.\nThis definition covers a variety of methods in the technical literature. It encompasses prior work from the last 10 years that has studied unlearning in clustering [53], classification and regression [11, 42, 100, 122], federated learning [67, 85], and more [17, 146]. It also applies to the classic paper by Cauwenberghs and Poggio [19], which studies the problem of unlearning in support vector machines (SVMs) under the name \"decremental learning\" over two decades ago."}, {"title": "Generative AI and evolving motivations for machine unlearning", "content": "Given the particularly high cost of (re)training large-scale generative-AI models, there is a developing interest to apply efficient unlearning methods in this area. However, translating unlearning methods to generative-Al models exhibits some important technical challenges, since these models differ from the more traditional ML models to which much prior work in unlearning has applied [e.g., 11, 53]. Traditional AI settings tend to involve models that produce concise outputs from a bounded and typically fixed set, for example, classification labels like dog or cat. After using an unlearning method on such a model, its outputs for a given input may change (e.g., its classification may flip from cat to dog), but the set of possible outputs (e.g., cat and dog) generally remains the same. In contrast, generative-AI models produce \u201cinformation-rich\" [26] outputs of the same modality as their training data. The set of possible outputs is significantly more expansive. For example, text-to-text models like Llama 3 [87] and those embedded in systems like Claude [4], ChatGPT [103], and Gemini [127] produce long-form text outputs.\nWith this key difference, the desired goals for what machine unlearning could achieve have also shifted. They have begun to expand beyond the scope of our loose definition for unlearning\u2014beyond removal of the influence of training-data inputs on the trained model's parameters\u2014to also encompass desired effects on the model's possible generated outputs when the model is put to use."}, {"title": "An expanded, loose definition for machine unlearning", "content": "In the context of more recent research on Generative AI, the loose definition for unlearning in Section 2.2 has widened in scope. In relation to these developments, we offer an expanded loose definition: machine unlearning is now a subarea of machine learning that both develops methods for (1) the targeted removal of the effect of training data from the trained model and (2) the targeted suppression of content in a generative-AI model's outputs. Later, we will organize our discussion of concrete unlearning methods in relation to this split (Section 4).\nIn other words, the scope for unlearning no longer just concerns what we, following Cooper and Grimmelmann [26], will refer to as back-end considerations: \"characteristics and capabilities of the model itself that directly result from its training.\" Unlearning also concerns front-end considerations: \u201chow the model behaves in generating outputs in response to ... specific prompt[s]\"(emphasis\""}, {"title": "Targets for Machine Unlearning", "content": "Our loose definition for unlearning is abstract (Section 2.4); in fact, it is so abstract that it allows for an enormous number of reasonable interpretations and possible techniques that satisfy it. In this section, our aim is to provide some language that can help us be more precise. Building on our loose definition, we now pin down useful ways to think about what a \u201cpiece of information\" could mean-what types of information one might want to target with unlearning. In the sections that follow, we show that the targets we define are places where concrete unlearning methods could potentially apply in practice (Section 4). We will also note that some articulated goals for unlearning escape these target definitions altogether, highlighting instances where unlearning methods could not be applied rigorously or reliably for certain desired ends (Section 5)."}, {"title": "Observed information", "content": "Data that are explicitly presented to the model during training. These data serve as inputs to computations that update the model's parameters.\nObserved information includes training-data examples: the contiguous pieces of data that are the base-level unit of input to model training (Section 2.1). For example, consider that the text \"Susan's phone number is 555-123-4567\" is included as an example in an LLM's training data. Since this text is used directly to train the LLM, it is observed information. Observed information also captures sets of training examples, such as all examples in the overall training dataset that mention Susan. It also includes data contained within examples, such as just the phone number \"555-123-4567\" in \"Susan's number is 555-123-4567.\"\nEffective trained models generalize: the learning process instills models with complex patterns that are derived from the observed information in the training data\u2014patterns that models can apply to previously unseen information when they are put to use for inference or generation (Section 2.1). This learned information is latent in the training data."}, {"title": "Latent information", "content": "Data that are not explicitly presented to the model during training, but that can be derived or otherwise elicited from a trained model based on the patterns that the model has learned during training.\nUnlike observed information (Definition 1), latent information is not literally observed in the training data. However, there are ML-based methods that claim to identify latent information and make it observable in the trained model's parameters\u00b9\u00b2 or indirectly through a model's outputs when the model is put to use.\u00b9\u00b3 Latent information can include simple deductions [68, 111]. For example, given the observed information \u201cCarlos is going to Susan's house for a birthday party this Thursday\u201d and \u201cSusan lives in Philadelphia,\u201d a possible piece of latent information is that Carlos is going to be in Philadelphia on Thursday.\u00b9\u2074 This information is not literally contained in the training data; it is derived from relationships learned from observed information. Of course, latent information can also be significantly more complex than such simple deductions. The power of large-scale models trained on enormous datasets [76] comes from their flexibility to capture all sorts of latent information\u2014across observed information, across latent information, or across some combination of the two. Indeed, information can interact to produce sophisticated, higher-order information that ML research often refers to as \u201cknowledge\u201d or \u201ccapabilities.\u201d"}, {"title": "Higher-order concepts", "content": "Combinations of latent and observed information that manifest in the model as complex and coherent abstractions, knowledge, capabilities, or skills.\nBefore giving some examples of higher-order concepts, some disclaimers are in order. Definition 3 is not intended to suggest something particularly deep about how models organize information or exhibit complex behaviors. (This is not, after all, a paper about ontology or metaphysics.) Instead, we give a definition of higher-order concepts for convenience: to align with how the ML technical literature tends to refer to conceptual learned representations. But it is nevertheless reasonable to think of higher-order concepts as complex combinations of latent information-that there is, loosely speaking, a spectrum of complexity for latent information (Definition 2), with simple deductions drawn directly from observed information on one end, and significantly more complex patterns (often called capabilities or emergent abilities [112, 123, 126, 138]) at the other.\nThis spectrum reveals that Definition 3 is somewhat arbitrary, since it is not clear how to distinguish when a piece of latent information is sufficiently complex to be considered a higher-order concept. We do not attempt to draw these lines. Nevertheless, we still find it useful in the discussion that follows to have a target definition that lets us to refer to the unlearning of higher-order concepts, since this is a type of information that could reasonably be\u2014and, in some cases, is claimed to be a target for machine unlearning."}, {"title": "Unlearning Methods and Evaluating Evidence for Their Success", "content": "With an understanding of the different types of information one may want to target with machine unlearning (Section 3), we next discuss concrete unlearning methods that aim to address them. By focusing on targets, we show how unlearning in generative-AI contexts attempts to have targeted effects in two overarching ways. First, there are methods that, in line with the original loose definition of unlearning (Section 2.2), address the targeted removal of observed information (Section 4.1) on the back-end. Second, in response to the shifting motivations for unlearning in generative-AI contexts (Sections 2.3 & 2.4), there are methods for output suppression of targeted information in a model's outputs on front-end.\nOur treatment of specific unlearning methods for removal and suppression is fairly brief.15 This is because our purpose is to provide sufficient framing that will enable us to elicit important conceptual gaps and limitations\u2014fundamental mismatches between unlearning motivations, targets, and methods (Section 5). It is these mismatches that are the heart of our paper, and are relevant for understanding misalignment with law and policy aims (Section 6)."}, {"title": "Methods for removal (of observed information)", "content": "As discussed above, one of the articulated goals for machine unlearning is to purge unwanted information from models (Sections 1 & 2.2). This is a fundamentally challenging technical problem because an ML model is not like a database. For a database, it is typically the case that specific pieces of information can be identified, targeted, and deleted; but there is no direct analogue for deleting targeted information from a generative-AI model. While each of a model's training examples \"is a self-contained piece of data\" [31], this is not the case for how information learned from these examples is arranged in a trained model's parameters. The training process encodes patterns learned from training data in the model's parameters in ways that are not directly or easily interpretable (Sections 2.1 & 3).\nAs a result, \"removal\" of information from a generative-Al model deviates from intuitive understandings of the term \u201cremoval.\u201d Instead, the most straightforward way one might \u201cremove\u201d16 information is to replace the original model with a new model that is trained on a dataset that does not contain problematic examples\u2014for example, a specific scientific paper on designing novel flu viruses or a specific in-copyright image of Spiderman. This process removes specific observed information (Definition 1) from a model's training dataset, instead of literally removing it from a"}, {"title": "Methods for output suppression", "content": "The majority of unlearning methods in Generative AI focus on output suppression (Sections 2.3 & 2.4). In this setting, potentially problematic training data is observed during the training process, and there is no attempt to guarantee (with certainty or probabilistically) that this is not the case. Instead, output-suppression methods aim to prevent undesirable content from appearing in generations on the front-end, rather than attempting to remove the effects of targeted observed information on the back-end. These methods tend to be more computationally feasible than retraining from scratch (Section 4.1). Since they focus on model outputs, they are not limited to observed information; they also apply (to varying degrees of success) to latent information (Definition 2) and higher-order concepts (Definition 3).\nWe organize our discussion around two overarching approaches to output suppression: (1) methods that make modifications to the trained generative-AI model, and (2) methods that leave the model unchanged, but implement guardrails in the system in which model is embedded, in order to constrain the outputs that are presented to end users. Both of these approaches include a wide range of techniques that operate very differently from the removal methods discussed above. While it is now common to include output suppression under the umbrella of \"machine unlearning,\" arguably, these methods have nothing to do with \u201cunlearning\u201d some information from a model; they bear more resemblance to alignment techniques."}, {"title": "Mismatches between Unlearning Motivations, Targets, and Methods", "content": "Four important problems emerge directly from our discussion of removal of observed information (Section 4.1) and output suppression (Section 4.2) above. Output suppression is not a replacement for removal of observed information (Mismatch 1). Conversely, removal of observed information does not guarantee meaningful output suppression (Mismatch 2). More generally, models are not equivalent to their outputs (Mismatch 3) or, relatedly, to how their outputs are put to use (Mismatch 4). We address each of these points in turn."}, {"title": "Machine Unlearning in Policy and Practice", "content": "The mismatches (Section 5) between unlearning targets (Section 3), methods (Section 4), and goals (Section 2) present clear technical and substantive challenges. We next consider how these mismatches manifest in specific ways and introduce complications for three law and policy areas where researchers and organizations have suggested that unlearning could help achieve certain desired ends for broader impact: privacy (Section 6.1), copyright (Section 6.2), and safety (Section 6.3).\nA common theme for these areas is the underlying assumption that using unlearning methods to constrain model outputs could potentially act in the service of more general ends for content moderation-to prevent users from generating potentially private, copyright-infringing, or unsafe outputs. For each, bringing in domain-specific details amplifies the mismatches that we describe in Section 5, revealing an even deeper disconnect between the use of unlearning methods in practice, actual policy considerations, and regulatory compliance. To address this disconnect, judges and policymakers will need to set reasonable expectations concerning the imperfect outcomes of best-effort implementations of unlearning methods to support specific policy goals (Section 7.2)."}, {"title": "Privacy", "content": "Given the breadth of data generative-AI models ingest for training, many experts worry about models revealing private information that they were trained on through their generations [e.g., 9, 12, 31, 94, 99, 120]. These concerns relate to privacy rights in different jurisdictions and associated remedies to preserve those rights. As discussed above (Section 2.1), in a number of jurisdictions, individuals have the right to request that organizations delete their personal data, also referred to as the \"right to be forgotten,\u201d following Article 17 of the GDPR [102]. Regulators may seek remedies that require the removal of a set of data examples used for model training that they assess to have been unlawfully or improperly collected. In other cases, remedies may be more far-reaching; regulators may seek to delete a trained model in its entirety, which is often referred to as algorithmic disgorgement."}, {"title": "U.S. Copyright", "content": "At first glance, as part of a response to claims in the U.S. that allege copyright infringement in connection with generative-AI models and systems, it may seem appealing to attempt to use machine-unlearning methods to target higher-order concepts that relate to creative expression, as perhaps a way to operationalize notice-and-takedown requests. However, U.S. copyright is not a straightforward problem, and unlearning is not a straightforward solution.\nWe begin with some brief background on U.S. copyright law. Copyright law protects \u201coriginal works of authorship fixed in any tangible medium of expression\u201d [33]. This means that copyright protection extends to a particular image or a particular paragraph of writing, but not to any ideas or facts contained in it. Because copyright law gives creators the exclusive right to prepare reproductions (copies) and derivative works, courts examine whether potential copies are \"substantially similar\" to the original work, and whether those copies thus infringe on the rights of the copyright holder. Substantial similarity is a challenging concept with a varied and complicated history in copyright caselaw."}, {"title": "Safety", "content": "Last, we address concerns about AI safety, which span a wide range of issues and communities [e.g., 2, 3, 9, 13, 15, 27, 29, 44, 61, 105, 124, 130, 132, 139, 142, 147]. Among this variety, there is one recurring theme that is especially important to address in relation to machine unlearning: the concern that \"dual-use,\u201d large-scale generative-AI models exhibit\nhigh levels of performance at tasks that pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters, such as by ... substantially lowering the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological, radiological, or nuclear (CBRN) weapons [130].\nWe draw the quote above from the U.S. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence; however, similar concerns and language can be found in a variety of legislative and policy documents, including the E.U. AI Act , the International Scientific Report on the Safety of Advanced AI produced by the AI Seoul Summit [9, Chapter 4], and OpenAI's Preparedness Framework [105]. Generative-AI models and systems \u201care sometimes called 'dual-use' because of their potential for both benefit and harm\" [133].\nSome researchers and policymakers claim that, to limit potential harmful uses, machine-unlearning methods could be used to remove \u201cunsafe,\u201d \u201chazardous,\u201d or otherwise \u201cundesirable behaviors\" from generative-AI models [e.g., 9, 81, 83, 86, 89, 149]. For one notable example, in the cross-stakeholder AI Seoul Summit report, Bengio et al. claim that \u201c'Machine unlearning' can help to remove certain undesirable capabilities,\u201d e.g., those \u201cthat could aid malicious users in making explosives, bioweapons, chemical weapons, and cyberattacks\" [9, p. 75]."}, {"title": "Discussion and Conclusion", "content": "We have covered a lot of ground, but our main takeaway is fairly simple: there are significant mismatches between what technical methods for machine unlearning can achieve and aspirations for how these methods could make generative-AI models and systems operationalize law and policy aims in practice. To close, we briefly summarize key points about these mismatches and then we offer some concrete takeaways for ML research and AI policy."}, {"title": "Recap: Machine unlearning doesn't do what you think", "content": "Now, having arrived at the end, we are able to revisit our main arguments through a set of relatively simple questions. With an understanding of the different goals, targets, methods, and potential application domains for machine unlearning, we can ask whether (1) removing certain observed information or (2) suppressing certain outputs is (a) necessary or (b) sufficient to meaningfully comply with policy aims."}, {"title": "Takeaways for ML research and AI policy", "content": "Following from above, we offer five takeaways for ML researchers and AI policymakers.\nUnlearning is just one approach in the ML and policy toolkit. There are clear gaps for what machine unlearning can do to achieve policy aims, both with respect to methods for removal of observed information and output suppression. Different methods may be useful to certain extents in specific contexts, but it is important to view unlearning as just one approach among may others that could sometimes help achieve specific policy aims. Nevertheless, ML researchers should not claim\u2014and policymakers should not misunderstand\u2014that machine unlearning is generally on its own effective for making generative-Al models and their outputs compliant with any desired policy goals.\nEvaluation of an unlearning method for a specific domain is a specific task. Further, such general claims about the broader impacts of unlearning are likely to be wrong from first principles because each legal and policy regime has its own specific expectations, which can be subtle and nuanced. To make rigorous claims about the broader usefulness of particular unlearning methods, as much as possible, ML experts need to evaluate specific unlearning techniques against specific regimes. This requires an understanding of these specific regimes, not just generalized ideas of how they might work-generalized ideas that may be so oversimplified that they are misleading or incorrect. To make claims about how an unlearning method might or might not be useful for operationalizing compliance with Article 17 of the GDPR, a layperson's reading of the text is not enough. It is important to be familiar with the complexity of different interpretations, rulings, and exceptions. To make claims about the relevance of an unlearning method for U.S. copyright compliance, it is important to make specific claims about specific areas of copyright law, rather than to treat copyright law as a monolith . At a minimum, this requires understanding those specific areas of copyright."}]}