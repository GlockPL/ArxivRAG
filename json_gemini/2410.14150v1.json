{"title": "Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis", "authors": ["Xiaoyong Huang", "Heli Sun", "Qunshu Gao", "Wenjie Huang", "Ruichen Cao"], "abstract": "With the rapid development of the internet, the richness of User-Generated Contentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis (MABSA) a research hotspot. Existing studies have achieved certain results in MABSA, but they have not effectively addressed the analytical challenges in scenarios where multiple entities and sentiments coexist. This paper innovatively introduces Large Language Models (LLMs) for event decomposition and proposes a reinforcement learning framework for Multimodal Aspect-based Sentiment Analysis (MABSA-RL) framework. This framework decomposes the original text into a set of events using LLMs, reducing the complexity of analysis, introducing reinforcement learning to optimize model parameters. Experimental results show that MABSA-RL outperforms existing advanced methods on two benchmark datasets. This paper provides a new research perspective and method for multimodal aspect-level sentiment analysis. The related code will be open-sourced for further research.", "sections": [{"title": "Introduction", "content": "With the rapid development of the Internet, user-generated content has become increasingly rich. How to accurately mine users' emotional information from massive multimodal data has become a research hotspot in the field of multimodality. Sentiment analysis, as an important branch of data mining, aims to identify and analyze subjective emotional tendencies within texts. Among these, Multimodal Aspect-Based Sentiment Analysis (MABSA) focuses on analyzing users' emotional expressions towards a particular aspect or object with the assistance of image data, which holds high practical application value (Yang et al. 2024b).\nThere is already a lot of excellent work being done in the area of aspect-based sentiment analysis (Cao et al. 2022)proposed an undirected differential emotion framework that eliminates affective biases to obtain stronger representations for sentiment classification (Zhang, Zhou, and Wang 2022) improved the accuracy of aspect-level sentiment analysis by learning semantic associations related to aspects and the global semantics of sentences through syntactic dependency trees. Considering multimodal input (Zhou et al. 2023),addressed the reduction of visual and textual noise brought about by complex image-text interactions.\nHowever, we argue that the aforementioned work does not take into account the following challenge: the complexity of multimodal aspect-based sentiment analysis mainly stems from the fact that texts often contain multiple aspect terms, and each term may carry different sentiment polarities. As shown in Figure 1, the multimodal data contains three aspect terms, and the sentiments corresponding to these terms are all distinct. Traditional sentiment analysis models often struggle to achieve precise aspect term identification and sentiment prediction when confronted with scenarios featuring multiple aspect terms and coexisting sentiments.\nTo address this issue, our paper innovatively employs Large Language Models (LLMs) for event decomposition, refining the original text into sub-events that contain single or a few entities. LLMs such as ChatGPT and Qwen (Yang et al. 2024a) have demonstrated remarkable capabilities across various natural language processing tasks, capable of extracting information from text via specific instructions, aiding in the construction of knowledge graphs among other tasks (Xu et al. 2023). Following this line of thinking, we utilize LLMs to decompose the original text into a set of events, where each sub-event contains only one or a few aspect terms, as illustrated by the Sequence Event Set in Figure 1. The advantage of this approach lies in the fact that each sub-event involves only one or two points of evaluation, significantly reducing the complexity of the sentiment analysis task. Moreover, since each sub-event in the event set can be considered in chronological order, the event set can be regarded as a Sequence Event Set. Given the superior performance of reinforcement learning in sequential tasks, we can incorporate it into the multimodal aspect-based sentiment analysis task to enhance the accuracy of aspect term prediction and sentiment analysis.\nSpecifically, we propose a reinforcement learning for Multimodal Aspect-based Sentiment Analysis (MABSA-RL). This framework initially breaks down the original text into a Sequence Event Set using a text decomposition module, extracting sub-events to reduce the complexity of aspect term prediction and sentiment analysis. Subsequently, we design a simple multimodal aspect prediction and sentiment analysis agent. We set up a specialized reinforcement learning environment based on the Sequence Event Set, pre-training the agent with supervised imitation learning and optimizing it with REINFORCE (Williams 1992) reinforcement learning policy to improve model performance.\nOur contributions are as follows:\n\u2022 We innovatively propose an event decomposition strategy based on LLMs, which refines the original text into a Sequence Event Set through specific instructions. Each sub-event in the Sequence Event Set contains only a single or a few aspect terms. This method effectively reduces the complexity of multimodal sentiment analysis, as each sub-event involves only one or two evaluation points, thereby simplifying the sentiment analysis process.\n\u2022 Targeting the characteristics of the Sequence Event Set, we design a specialized reinforcement learning environment for the MABSA task. By pre-training with imitation learning and optimizing with the REINFORCE algorithm, we improve the strategies for aspect identification and sentiment prediction, enhancing the model's performance. To our knowledge, this is the first work that applies reinforcement learning to MABSA tasks.\n\u2022 We develop a framework called MABSA-RL, which provides a new perspective on applying reinforcement learning to non-sequential decision-making tasks.\n\u2022 Experiments on two benchmark datasets demonstrate that the MABSA-RL framework outperforms state-of-the-art methods overall. This validates the effectiveness of our approach in MABSA tasks. Furthermore, our code will be open-sourced to facilitate further exploration and validation of our method by other researchers."}, {"title": "Related works", "content": "Previous work in multimodal aspect-based sentiment analysis has primarily focused on modal alignment. For instance, JML (Ju et al. 2021) developed an auxiliary text-image relationship detection module within a hierarchical framework to achieve multimodal integration. UMAEC (Ru et al. 2023) established a shared feature module to capture semantic relationships between tasks. DTCA (Yu et al. 2022) enhanced inter-modal attention by introducing additional auxiliary tasks. VLP-MABSA (Ling, Yu, and Xia 2022) transformed the analysis task into a text generation problem, reinforcing the model's understanding of aspects, opinions, and their coherence through specific pretraining tasks. Recent trends have concentrated on strengthening sentiments and aspects. CMMT (Yang, Na, and Yu 2022) learned intramodal representations of sentiments and aspects via auxiliary tasks and introduced a text-guided cross-modal interaction module to modulate the contribution of visual information. GMP (Yang et al. 2023) predicted the number of aspects in instances through multimodal prompts. AESAL (Zhu et al. 2024a) constructed aspect-enhanced pretraining tasks and adopted a syntax-adaptive learning mechanism to discern differences in word importance within text. Atlantis (Xiao et al. 2024) augmented multimodal data by incorporating visual aesthetic attributes. FITE (Yang, Zhao, and Qin 2022) concentrated on capturing visual emotional cues through facial expressions, selectively matching and fusing them with textual modalities pertaining to target aspects.\nDespite these successes, they overlooked the fundamental issue that the complexity of multimodal aspect-based sentiment analysis stems from the presence of multiple aspect terms in the text, each potentially bearing different sentiment polarities. To address this, we leverage LLMs to decompose texts into Sequence Event Set, where each sub-event contains only one to two aspect terms, thereby reducing the task's complexity."}, {"title": "Reinforcement Learning", "content": "Deep Reinforcement Learning (DRL), combining the powerful representation capabilities of deep learning with the decision optimization abilities of reinforcement learning, has achieved remarkable results across various domains such as games (Ye et al. 2020), robotics control (Tang et al. 2024), autonomous driving (Kiran et al. 2021), and medical decision-making (Hao et al. 2022). Since the mathematical foundation and modeling tools of reinforcement learning are rooted in Markov Decision Processes, it has been predominantly applied to sequential decision-making tasks (Ladosz et al. 2022).\nOur proposed MABSA-RL framework utilizes LLMs to transform non-sequential decision tasks into sequential ones, enabling the application of reinforcement learning techniques to non-sequential decision problems, thus offering a novel approach for future research in handling such tasks."}, {"title": "Methodology", "content": "In this section, we first introduce the task formulation, followed by a detailed description of the proposed MABSA-RL framework. Figure 2 illustrates the overall architecture of MABSA-RL, which consists of a Text Decomposition Module, a Multimodal Aspect Prediction and Sentiment Analysis Agent, and a Sequential Decision Enhancement Module. Specifically, we first employ LLMs to decompose the text into a Sequence Event Set. Subsequently, an agent is designed to predict the probability distributions of aspect terms and sentiments using both textual and visual information. Finally, based on the Sequence Event Set, the non-sequential decision-making task is transformed into a sequential decision-making task. We utilize supervised cloning learning and the reinforcement learning algorithm REINFORCE to update the agent's parameters, thereby enhancing the quality of aspect term prediction and sentiment analysis."}, {"title": "Task Formulation", "content": "Formally, we assume that the dataset D = {(Ti, Vi, Ai, Si)}{1} consists of K samples. For each sample x \u2208 D it includes a text T = {t1, t2,..., tn} composed of n words, an associated image V\u2208 \u211d^{3\u00d7H\u00d7W}, and aspects A = {a1,a2,..., am} consisting of m words along with their corresponding sentiments S = {81, 82, ..., Sm}, where 3,H,W denote the number of channels, height, and width of the image, respectively. a\u017c denotes the i-th aspect item, and si \u2208 {POS, NEU, NEG} denotes the sentiment corresponding to the i-th aspect item, with POS, NEU, NEG representing positive, neutral, and negative sentiments, respectively. Our objective is to learn a model F(T, V) \u2192 (A, S), that is, given T and V, predict A and S."}, {"title": "Text Decomposition Module", "content": "As we introduced earlier, in MABSA tasks, texts often accompany multiple aspect items, each potentially bearing different sentiment polarities. To reduce the complexity of the MABSA task and improve model performance, as shown in Equation (1), we employ LLMs to decompose the original text into a set of events, where l represents the number of events in the set, and ej is a sub-event containing a single or a few aspect items.\nE = LLMs(T) (1)\nWe use the prompts listed in Table 1 to ensure that the narrative of each ej is complete. Since each sub-event in the event set E is decomposed according to the narrative order from front to back in T, each sub-event in E exhibits a certain temporal sequence. Based on this, E can be considered as a sequence, so we refer to E as a Sequence Event Set."}, {"title": "Multimodal Aspect Prediction and Sentiment Analysis Agent", "content": "We designed a straightforward multimodal aspect prediction and sentiment analysis agent. Specifically, we input a state St = {Tt, V}, where Tt represents the textual information at time t, and V denotes the image. We append two special tokens [CLS] and [SEP] at the beginning and end of the text as sentence start and end markers, and use [CLS] as the marker for the start of the image. Then, we utilize ROBERTa (Liu et al. 2019) to extract text embeddings and employ ViT (Dosovitskiy et al. 2020) to extract visual embeddings from the image.\nHT = ROBERTa(Tt) (2)\nHV = MLP (ViT(V)) (3)\nAmong these, we used an MLP (Multi-Layer Perceptron) to adjust the shape of the extracted visual embedding Hv to match that of the text. HT, HV \u2208 \u211d^{n*\u00d7d},where nt indicates the number of words, and d represents the dimension of the hidden state.\nSubsequently, as illustrated by Equation (4), we apply a cross-attention mechanism to fuse HT and HV, obtaining the fusion embedding Hf \u2208 \u211d^{n*\u00d7d}:\nHf = Softmax( \\frac{HTWQ \u00d7 (HVWK)^T}{\\sqrt{d}} ). (HVWV) (4)\nwhere WQ,WK,Wv are learnable parameters.\nFollowing this, we obtain the probability distributions of the text's aspects A and sentiments S according to Equations (5) and (6):\nP(A) = softmax(WAHf + ba) (5)\nP(S) = softmax(WsHf + bs) (6)\nwhere WA and Ws are the weight matrices of the aspect and sentiment prediction layers, respectively, and ba and bs are the corresponding bias vectors."}, {"title": "Sequential Decision Enhancement Module", "content": "Our approach is to incrementally incorporate sub-events from the Sequence Event Set E into the original text T, and then calculate the F1 score for aspect term extraction and sentiment prediction with respect to T. This F1 score serves as a reward to optimize the parameters of the entire agent.\nEnvironment Setup:Following reinforcement learning terminology, we introduce states, actions, and rewards.\nState St:The state at time step t consists of the current text Tt and the associated image V, denoted as St = {Tt, V}, Tt = Tt-1+ < /event > +et.Specifically, So = {To, V}, To = T.The < /event > serves as an identifier.\nAction At: The action space contains all possible distributions of sentiments and aspects. The Agent outputs the predicted distributions of aspects Pt(A|St) and sentiments Pt(S|St) based on the state St.\nReward Rt: Defined as the F1 score predicted for T at time step t. Specifically, we first convert the probability distribution into predicted labels, then calculate the confusion matrix, and subsequently compute the F1 score. If we denote this process using a function f\u2081 (\u00b7), then our reward function can be expressed as:\nRt = (f1(Pt(A St)) + f1(Pt(S|St)))/2 (7)\nPolicy Network Setup: We utilize the Multimodal Aspect Prediction and Sentiment Analysis Agent as the policy network \u03c0\u03b8 (At St), denoted with parameters @ representing the policy network.\nPre-training with Clone Learning: To enhance subsequent training efficiency and avoid excessive random exploration during the reinforcement learning phase, we extract clone learning pre-training from any state St = {Tt, V} across all training data. Using cross-entropy loss as the objective function, for the prediction of aspects and sentiments, we can define the loss functions as per Equations (8) and (9), with the overall loss function defined by Equation (10).\nLA = -\\frac{1}{N} \\sum_{i=1}^{N} YA,i log(PA,i) (8)\nLs = -\\frac{1}{N} \\sum_{i=1}^{N} ys,i log(ps,i) (9)\nL = 0.5\u00d7 LA + 0.5 \u00d7 Ls (10)\nWhere YA,i and ys,i are the true labels for their respective categories, and PA,i and ps,i are the probabilities predicted by the model.\nReinforcement Learning: We update the policy parameters 0 of the Agent using the REINFORCE algorithm to maximize the long-term return Gt = \u2211k=tyk-tRk where \u03b3 is the discount factor. The update rule for each data instance is given by Equation (11).\n\u03b8 = \u03b8 + \u03b1 \u00b7 Velog \u03c0\u03bf(At St). Gt (11)\nwhere \u03b1 is the learning rate. The entire algorithmic process is detailed in Table 2."}, {"title": "Experiments", "content": "In this section, we will verify the performance of MABSA-RL through experiments. Experimental setups, comparative models, experimental results, ablation studies, and case analyses will all be introduced."}, {"title": "Experimental Setup", "content": "Datasets: We conduct experiments on two multimodal benchmark datasets, including Twitter-2015 and Twitter-2017(Hu et al. 2019). Table 3 provides statistics on the datasets. These two Twitter datasets separately collected user posts published on Twitter during the periods of 2014-2015 and 2016-2017.\nHyperparameter Settings: Our experiments were implemented under the PyTorch framework utilizing NVIDIA 3090 GPUs. The learning rate was set to 2e-5 during the supervised clone learning phase and adjusted to 1e-5 for the reinforcement learning stage. The dimension of the hidden layer was 768, with dropout set to 0.1.For the LLM, we utilize Qwen-Max-0428.\nEvaluation Metrics: To assess the performance of the algorithms, in line with previous work, we utilize Micro-F1 (F1), Precision (P), and Recall (R) to evaluate our model. Higher metrics indicate superior model performance."}, {"title": "Comparative Models", "content": "We compare the proposed MABSA-RL against three textual Aspect-Based Sentiment Analysis(ABSA) methods and eight MABSA methods.\nMethods for ABSA:\n1) SPAN (Hu et al. 2019) directly extracts multiple opinion targets and identifies sentiment polarities from sentences under supervision that spans boundaries.2) D-GCN (Chen, Tian, and Song 2020) models syntactic dependencies using GCN (Kipf and Welling 2016). 3) BART (Yan et al. 2021) is a pre-trained sequence-to-sequence model that addresses all ABSA subtasks within an end-to-end framework.\nMethods for MABSA:\n1) UMT-collapse (Yu et al. 2020), OSCGA-collapse (Wu et al. 2020), and rbert-collapse (Sun et al. 2021) use the same visual input to fold individual tokens. 2) UMT+TomBERT, OSCGA+TomBERT are two pipelined approaches combining UMT, OSCGA with TomBERT respectively.3) JML (Ju et al. 2021) is a multimodal joint method capable of handling aspect term extraction and sentiment classification simultaneously.4) VLP-MABSA (Ling, Yu, and Xia 2022) is a unified multimodal encoder-decoder architecture for all pre-training and downstream tasks.5) CMMT (Yang, Na, and Yu 2022) is a multitask learning framework for extracting aspect-sentiment pairs from pairs of sentences and images.6) AOM (Zhou et al. 2023) is an aspect-oriented network designed to alleviate the noise in vision and text produced by complex image-text interactions.7) Atlantis (Xiao et al. 2024) augments multimodal data by introducing visual aesthetic attributes.8) AESAL(Zhu et al. 2024a) designs a syntactic adaptive learning mechanism to capture the difference in the importance of different words in the text."}, {"title": "Experimental Results", "content": "Table 4 demonstrates the results of various models on the MABSA task. Firstly, our proposed MABSA-RL significantly outperforms all text-based models, indicating the effectiveness of multimodal information in ABSA tasks. Secondly, compared to the state-of-the-art AESAL model, MABSA-RL boosts the P, R, and F1 values by 3%, 1.3%, and 1.9% respectively on the Twitter-2015 dataset. On the Twitter-2017 dataset, the P value increases by 3.8%, and the F1 value improves by 1.1%. The slightly lower R value might be due to the imbalanced distribution of sentiments in the training data, particularly in the Twitter-2017 training set, where the number of negative instances is far less than those of the other two sentiments, causing the model to be biased towards predicting the majority sentiment, thus resulting in a lower recall.\nOverall, MABSA-RL also outperforms other multimodal models. This is because previous research has primarily focused on the utilization of image and text information but neglected the complexity of multimodal aspect-level sentiment analysis, which mainly stems from the presence of multiple aspect terms in the text, each possibly carrying different sentiment polarities. Our proposed MABSA-RL tackles this issue by decomposing the textual information into a Sequence Event Set via LLMs, where each sub-event contains only a small number of evaluation points, reducing the difficulty for aspect term prediction and sentiment analysis. Additionally, by employing clone learning and reinforcement learning, we optimize the entire model, enhancing its predictive and decision-making capabilities."}, {"title": "Ablation Studies", "content": "In this section, we investigate the impact of each module on the final performance. The results of the ablation experiments are shown in Table 5. We use the multimodal aspect prediction and sentiment analysis agent as a baseline, training solely on the raw text and image without the enhancement provided by the Sequence Event Set. This allows us to understand the contributions of each component, such as the text decomposition module and the reinforcement learning strategy.\nIt can be observed that after incorporating the Sequence Event Set E for supervised training, there are improvements across all metrics on both benchmark datasets. On the Twitter-2015 dataset, the P, R, and F1 values increase by 2.3%, 1.8%, and 1.3% respectively. Meanwhile, on the Twitter-2017 dataset, the P, R, and F1 values rise by 3.6%, 3.2%, and 3.3% respectively. This directly validates the efficacy of the event decomposition module. Moreover, it indicates that the Sequence Event Set facilitates the simplification of aspect term prediction and sentiment analysis, thereby enhancing model performance.\nUpon the introduction of reinforcement learning, on the Twitter-2015 dataset, the P, R, and F1 values further increase by 0.5%, 1.2%, and 0.9% respectively. Similarly, on the Twitter-2017 dataset, the P, R, and F1 values increment by 0.6%, 0.2%, and 0.4% respectively. Clearly, the sequential decision-making training approach aids in boosting model performance. However, the performance gain attributed to reinforcement learning is relatively modest. We hypothesize that this is because the Sequence Event Set E can only be approximated as a sequence and does not perfectly align with the sequential nature required for reinforcement learning. Furthermore, inherent drawbacks of reinforcement learning, such as instability, have impacted the extent of performance improvement. These issues will be a focus in our future research endeavors."}, {"title": "Case Study", "content": "To further substantiate the effectiveness of MABSA-RL, we present a case study as follows. Figure 3 illustrates two examples of predictions made using UMT+TomBERT, VLP-MABSA, and our MABSA-RL. In Example (a), UMT+TomBERT failed to identify David Bowie and its corresponding sentiment. VLP-MABSA, on the other hand, did not fully recognize Brian Eno and incorrectly analyzed its sentiment. In Example (b), UMT+TomBERT misjudged the sentiment associated with Bill Clinton, while VLP-MABSA failed to completely recognize Bill Clinton. This may be due to the models' difficulties in analyzing the complex context under multiple aspect terms and varied sentiments, leading to incorrect aspect term predictions and sentiment judgments. In contrast, our proposed MABSA-RL correctly identified all aspect terms and provided accurate sentiment predictions in both cases. This is attributable to our use of LLMs to decompose textual information into a Sequence Event Set, where each sub-event contains only a small number of evaluation points, thereby reducing the complexity of aspect term prediction and sentiment analysis. Additionally, by employing reinforcement learning, we optimized the entire model, further enhancing its performance."}, {"title": "Conclusion", "content": "This paper proposes a reinforcement learning framework for multimodal aspect-level sentiment analysis called MABSA-RL. The framework encompasses a Text Decomposition Module, a Multimodal Aspect Prediction and Sentiment Analysis Agent, and a Sequential Decision Enhancement Module. Firstly, the Text Decomposition Module leverages LLMs to decompose text into a Sequence Event Set, with each sub-event containing only a limited number of appraisal points, thereby reducing the complexity for the model in predicting aspect terms and analyzing sentiments. Secondly, by constructing a Multimodal Aspect Prediction and Sentiment Analysis Agent, probability distributions for aspect term prediction and sentiment analysis are obtained. Lastly, within the Sequential Decision Enhancement Module, a specialized reinforcement learning environment is built for the Sequence Event Set, and the agent's parameters are optimized using behavior cloning and REINFORCE to enhance its performance. Experiments on two authoritative datasets demonstrate that MABSA-RL outperforms existing baseline methods in general, showcasing its superior performance. Furthermore, MABSA-RL offers new insights into applying reinforcement learning to non-sequential decision-making tasks."}]}