{"title": "TRACKING THE COPYRIGHT OF LARGE VISION-LANGUAGE MODELS THROUGH ParameTER LEARN-ING ADVERSARIAL IMAGES", "authors": ["Yubo Wang", "Jianting Tang", "Chaohu Liu", "Linli Xu"], "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable image un-derstanding and dialogue capabilities, allowing them to handle a variety of visualquestion answering tasks. However, their widespread availability raises concernsabout unauthorized usage and copyright infringement, where users or individualscan develop their own LVLMs by fine-tuning published models. In this paper,we propose a novel method called Parameter Learning Attack (PLA) for trackingthe copyright of LVLMs without modifying the original model. Specifically, weconstruct adversarial images through targeted attacks against the original model,enabling it to generate specific outputs. To ensure these attacks remain effectiveon potential fine-tuned models to trigger copyright tracking, we allow the orig-inal model to learn the trigger images by updating parameters in the oppositedirection during the adversarial attack process. Notably, the proposed method canbe applied after the release of the original model, thus not affecting the model'sperformance and behavior. To simulate real-world applications, we fine-tune theoriginal model using various strategies across diverse datasets, creating a rangeof models for copyright verification. Extensive experiments demonstrate that ourmethod can more effectively identify the original copyright of fine-tuned modelscompared to baseline methods. Therefore, this work provides a powerful tool fortracking copyrights and detecting unlicensed usage of LVLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large vision-language models (LVLMs) have emerged with remarkable prowess in various imageunderstanding tasks, especially those involving detailed image descriptions or complex visual reasoning. Given their strong image understanding capabilities, users and researcherscan fine-tune LVLMs to leverage pre-trained knowledge and develop their tailored image-to-textmodels in specific domains. Fine-tuning a released LVLM offers significant advantages over traininga model from scratch, notably in terms of reduced computational resource requirements and lowerassociated costs. Consequently, it has become a widely adopted technique for domain adaptationamong researchers and developers.\nThe release of LVLMs to the public by certain companies and research teams, along with the per-mission for open fine-tuning, has catalyzed significant advancements in the artificial intelligencecommunity. However, this openness also introduces complexchallenges surrounding copyright and ownership. There is a growing concern that malicious de-velopers or companies might exploit this accessibility, fine-tuning released LVLMs for commercialgain or profit without proper attribution. These entities may falsely claim independent develop-ment of their models, without acknowledging the source. Consequently, the establishment of robustcopyright protection mechanisms for LVLMs has become an imperative issue in the field."}, {"title": "2 RELATED WORK", "content": "Large vision-language models. Research on LVLMs has been advancing rapidly, driven by inno-vative model architectures and specific training strategies. Prominent baseline LVLMssuch as LLaVA and MiniGPT-4 are generally capable of han-dling most visual question-answering tasks. Latest models like InternVL 1.5support higher-resolution image inputs and utilize larger-scale image encoders, enabling them tohandle more complex or specialized image dialogue tasks. The release of increasingly powerful LVLMs has led to a growing trend of researchers and developersfine-tuning these models for specific applications, which underscores the urgent need for researchon copyright tracking of LVLMs.\nCopyright tracking of LLMs. With the increasing demand for fine-tuning (large) language mod-els, efforts to protect the copyright of these models have begun to emerge. The common approach involves using backdoor attacks tomake the model memorize specific patterns or \u201cfingerprints\" that persist even after fine-tuning. Forinstance, inserts trigger text near instructions or questions to create specific finger-prints. utilize rare texts to create fingerprint pairs and train the model with limiteddata to reduce model damage. While copyright protection for LLMs has been widely studied, thereare currently no similar studies that have shifted their focus to LVLMs. Our work aims to bridge thisgap by addressing the unique challenges posed by the multimodal nature of LVLMs.\nAdversarial attacks against LVLMs. Extensive studies have been conducted on adversarial attacksagainst LVLMs. These studies have shown that even large-scale modelslack adversarial robustness, which presents both a challenge and an opportunity to use adversarialattacks for copyright tracking. Instead of compromising LVLMs, our objective is to leverage adver-sarial attacks as a tool to safeguard them. In this paper, we utilize targeted attacks against LVLMsto construct triggers for tracking model copyright."}, {"title": "3 METHOD", "content": "3.1 PROBLEM FORMULATION\nDenote the large vision-language model released by the publisher as \\(F_\\{\\Theta\\}(\\mathbf{x}, q)\\), where \\(\\mathbf{x}\\) is the inputimage and \\(q\\) is the textual question input to the LVLM. Suppose there are two models, one fine-tunedfrom the original model \\(F_\\{\\Theta\\}\\) denoted as \\(F_{\\Theta'}\\), and the other unrelated to the original model denoted as\\(G_{\\Psi}\\). To achieve copyright tracking, the publisher can use trigger input \\(((\\mathbf{x}, \\hat{q})\\) to query \\(F_{\\Theta'}\\) and \\(G_{\\Psi}\\).The trigger should satisfy the following criteria: the original model \\(F_{\\Theta}\\) and its derivative model \\(F_{\\Theta'}\\)should both generate the predetermined target text, while an unrelated model \\(G_{\\Psi}\\) should produce a\ndistinct output. Formally, this can be expressed as:\n\\[F_{\\Theta}(\\mathbf{\\hat{x}}, \\hat{q}) = F_{\\Theta'}(\\mathbf{\\hat{x}}, \\hat{q}) = \\hat{a}, \\quad G_{\\Psi}(\\mathbf{\\hat{x}}, \\hat{q}) \\neq \\hat{a}.\\]\n3.2 THREAT MODEL\nStealer. The stealer's objective is to fine-tune published LVLMs for personal use or profit whiledenying their copyright, which costs much less than training from scratch. The stealer has full"}, {"title": "3.3 PARAMETER LEARNING ATTACK", "content": "3.3.1 QUESTION-ANSWER PAIR DESIGN\nTo facilitate copyright tracking, we propose designing rare question \\(\\hat{q}\\) and answer \\(\\hat{a}\\) pairs. We usegeneric images to initialize adversarial images, which are typically unrelated to the rare questionand answer. We need to ensure that when queried with clean images, neither the original model northe fine-tuned models will generate \\(\\hat{a}\\) in response to \\(\\hat{q}\\). Examples of the question-answer pairs we\ndesign are shown in Figure 2(b). Based on these QA pairs, we perform targeted adversarial attacks\non the original LVLM, and obtain an adversarial image \\(\\mathbf{\\hat{x}}\\), which can elicit the predefined answer \\(\\hat{a}\\)from the model. The trigger should satisfy the following conditions:\n\\[F_{\\Theta}(\\mathbf{x}, \\hat{q}) \\neq \\hat{a}, \\quad F_{\\Theta}(\\mathbf{\\hat{x}}, \\hat{q}) = \\hat{a}.\\]\nHere we refer to \\(\\mathbf{\\hat{x}}\\) as the trigger image, \\(\\hat{q}\\) as the trigger text, and \\(\\hat{a}\\) as the trigger target. Designingtriggers with rare QA pairs ensures that fine-tuned LVLMs will not inadvertently learn the triggerpatterns, as such combinations are typically absent from conventional datasets.\n3.3.2 TRIGGER CONSTRUCTION\nSince the adversarial optimization is solely based on the original model's parameters, the adver-sarial image tends to \u201coverfit\" to the original model and lack generality.For copyright tracking, this may result in the trigger image failing on fine-tuned models, therebycompromising its ability to track these derivative versions. Formally, this can be expressed as:"}, {"title": null, "content": "\\[F_{\\Theta}(\\mathbf{\\hat{x}}, \\hat{q}) = \\hat{a}, \\quad F_{\\Theta'}(\\mathbf{\\hat{x}}, \\hat{q}) \\neq \\hat{a}.\\]\nTo mitigate the overfitting issue, it is necessary to enhance the trigger's generalization to modelparameters or reduce its sensitivity to parameter variations. Based on our observations, LVLMstypically achieve convergence with fewer fine-tuning steps, with relatively small parameter shifts.Thus, an intuitive baseline approach is to add slight random Gaussian noise to the model parametersat each iteration of the adversarial attack, formulated as\n\\[\\Theta' = \\Theta + \\lambda \\cdot \\mathcal{N}(0, \\sigma^2),\\]\nwhere \\(\\lambda\\) represents the noise magnitude. We refer to this method as random noise attack (RNA),as shown in Figure 3(b). The noise-augmented model can be considered a simulated, randomlyfine-tuned variant. Triggers constructed on such models have the potential to mark authentic fine-tuned models. However, this approach has several inherent limitations. First, the noise magnitude\\(\\lambda\\) is difficult to determine. In practice, the degree of parameter shift induced by fine-tuning variesconsiderably across different tasks. Consequently, we lack a universally applicable standard forsetting \\(\\lambda\\). Second, the parameter modifications caused by model fine-tuning are gradient-based, incontrast to the simplistic Gaussian noise.\nTo design triggers with enhanced tracking performance, we propose a novel methodology termedParameter Learning Attack (PLA), which augments the generality of triggers and reduces their sen-sitivity to parameter shifts from an innovative perspective. Generally, fine-tuned models tend toresist generating the target \\(\\hat{a}\\) when queried with vanilla adversarial images and trigger text \\(\\mathbf{\\hat{x}}\\). To\novercome this limitation and enhance the trigger's ability to track fine-tuned models, we introduce an\nadversarial learning dynamic to compel the model to emulate the behavior of the fine-tuned variants,\nspecifically their tendency to resist generating the predetermined target. The optimization problemcan be formulated as:\n\\[\\min_{\\mathbf{x}'} \\max_{\\Theta'} \\mathcal{L} (F_{\\Theta'}(\\mathbf{x}', \\hat{q}), \\hat{a}).\\]\nIn this framework, the objective of the adversarial attack is to minimize the cross-entropy loss be-tween the model output and the trigger target. Conversely, we set the model's learning objective tomaximize this loss, as shown in Figure 2(a). During each iteration, we update not only the pixels ofthe trigger image but also the model parameters:\n\\[\\begin{aligned}\n\\Theta' &= \\Theta' + \\beta \\cdot \\text{clip} (\\nabla_{\\Theta'} \\mathcal{L} (F_{\\Theta'} (\\mathbf{x}', \\hat{q}), \\hat{a})), \\\\\n\\mathbf{x}' &= \\mathbf{x}' - \\alpha \\cdot \\text{sign} (\\nabla_{\\mathbf{x}'} \\mathcal{L} (F_{\\Theta'} (\\mathbf{x}', \\hat{q}), \\hat{a})),\n\\end{aligned}\\]\nwhere \\(\\beta\\) and \\(\\alpha\\) represent the learning rates for the model and the trigger image, respectively. We\nregulate the learning rate of the model parameter updates and apply gradient clipping to ensure\nsuccessful convergence of the trigger image. Through the competitive process between adversarial\nattack and model learning, the trigger image that converges by overcoming the model's inherent\nresistance is hypothesized to possess enhanced efficacy in inducing potential fine-tuned models to"}, {"title": "3.3.3 COPYRIGHT VERIFICATION", "content": "Consider a scenario where \\(n\\) users fine-tune the original model \\(F_{\\Theta}\\), resulting in \\(n\\) derivative modelsdenoted as \\(F_{\\Theta_1}, F_{\\Theta_2}, ..., F_{\\Theta_n}\\). While their architectures are consistent with the original model,\ntheir parameters are different from \\(\\Theta\\). Employing our proposed attack method, we generate a series\nof triggers \\(\\mathcal{X} = \\{(\\mathbf{x}\\_1, \\hat{q}\\_1), (\\mathbf{x}\\_2, \\hat{q}\\_2),..., (\\mathbf{x}\\_m, \\hat{q}\\_m)\\}\\). During the copyright verification phase, we\nuse these triggers to access each fine-tuned model, as illustrated in Figure 2(c). Then we compute\nthe target match rate (TMR) on each model \\(F_{\\Theta_i}\\) to quantify the tracking performance:\n\\[\\text{TMR} = \\frac{|\\{(\\mathbf{x}\\_j, \\hat{q}\\_j) \\in \\mathcal{X} \\mid F_{\\Theta_i} (\\mathbf{x}\\_j, \\hat{q}\\_j) = \\hat{a}\\}|}{\\quad m \\quad},\\]\nA match is considered successful if the output text contains the exact trigger target or conveyssemantically equivalent content. In general, a higher TMR indicates better tracking performance ofthe triggers. We calculate the TMR for multiple fine-tuned models across various tasks to ensurea reliable estimate of the performance in tracking the copyright of suspicious models in real-worldscenarios."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nTrigger dataset. We initialize the trigger images using regular images randomly sampled from theImageNet 2012 validation set. To validate the effectiveness of the pro-posed method, we sample 200 images and design 5 different trigger question-answer pairs, yieldinga total of 1000 trigger queries (200 images \u00d7 5 QA pairs).\nFine-tuning. We use LLaVA-1.5 as the original LVLM, given its widespreadadoption as a baseline for vision-language tasks and its popularity among developers for fine-tuning. We consider two commonly used training strategies: full fine-tuning and LoRA fine-tuning. To simulate various types of fine-tuned models, we utilize various VQA datasetsfrom multiple domains, all previously unseen by LLaVA. The datasets include the grounded VQA"}, {"title": "4.2 MAIN RESULTS", "content": "We report the TMRs of our proposed PLA and the baseline methods for copyright tracking on sixfine-tuned models in Table 1. The result in each cell represents the average TMR using differentQA pairs. A higher response rate indicates better copyright tracking performance and demonstratesgreater generality across various fine-tuned models. The method Ordinary refers to constructingtrigger images using vanilla adversarial attacks. For the baseline method IP, we follow its originalconfiguration by setting the number of fingerprints to 10 (as an increase in fingerprints would impairmodel performance) and employ SFT to inject fingerprints to keep the black-box style during thecopyright verification phase. To mitigate variance, we perform five rounds ofexperiments for IP.\nThe experimental results in Table 1 show that our method PLA achieves the best tracking perfor-mance on all six fine-tuned models (both LoRA fine-tuning and full fine-tuning). In contrast, themethod Ordinary exhibits poor performance, indicating that using standard adversarial attacks toconstruct trigger images leads to overfitting on the original model. The IP method based on back-door attacks achieves good tracking performance on certain fine-tuned models, but the results onother models indicate that their fingerprints are completely erased, suggesting a lack of robustnesswhen applied to LVLMs. This may be due to the differences in architecture and task modalitybetween LVLMs and LLMs. The tracking performance of RNA is inferior to that of PLA, which"}, {"title": "4.3 ROBUSTNESS ANALYSIS", "content": "In real-world scenarios, after unauthorized fine-tuning of the publisher's LVLM to create their ownmodels, stealers may prevent the publisher from tracking the copyright via input transformationsand model pruning (or perturbation). Through input transformations, a stealer can disrupt the subtleperturbations in trigger images, leading to tracking failures. Similarly, model pruning and pertur-bation directly modify the model parameters, potentially erasing the model's memory of the triggerQA pairs. While these actions will compromise the model's performance, stealers may deem thisdegradation an acceptable trade-off in their attempts to circumvent copyright tracking. We con-duct corresponding experiments to assess the robustness of our method against these strategies andprovide the following analysis."}, {"title": "5 CONCLUSION", "content": "In this paper, we focus on a critical yet relatively unexplored issue: copyright tracking for LVLMs.We propose an innovative method that leverages adversarial attacks to generate trigger images forcopyright tracking, circumventing the need for direct model parameter alterations. To address thelimitations of conventional adversarial attacks, which often result in overfitting to the original model,we introduce Parameter Learning Attack (PLA). This method allows the model to update its pa-rameters to hinder the convergence of trigger images, making them capable of tracking potentialfine-tuned models. Extensive experiments demonstrate that our method outperforms other baselinemethods in terms of tracking performance, showing the potential to serve as a crucial tool in thedetection and prevention of copyright infringement in LVLMs."}, {"title": "A ADDITIONAL IMPLEMENTATION DETAILS", "content": "A.1 DETAILS OF THE ORIGINAL LVLM\nWe use LLaVA 1.5-7b as the original model. The architecture consists of a pre-trained vision encoder CLIP ViT-14L , a projector with two linear layers, and\na large language model decoder LLaMA-2. It supports an input image resolution of 336x336. The\nlanguage model has a total of 32 layers, and the hidden size is 4096.\nA.2 DETAILS OF FINE-TUNING\nTo simulate downstream fine-tuned models for copyright tracking, we consider two fine-tuning\nstrategies: full fine-tuning and LoRA fine-tuning. The training configuration details are shown in\nTable 5."}, {"title": "B DETAILS OF DOWNSTREAM DATASETS", "content": "In this section, we provide a detailed description of the datasets used for fine-tuning, including\noverviews of all datasets and sample examples.\nV7W. A large-scale visual question answering (VQA) dataset with object-level annotations and\nmultimodal responses. The dataset comprises 47,300 images and includes a total of 327,929\nquestion-answer pairs, together with 1,311,756 human-generated multiple-choices and 561,459 ob-\nject groundings from 36,579 categories. QA examples are shown in Figure 6.\nST-VQA. A visual question answering dataset where the questions and answers are attained in a\nway that questions can only be answered based on the text present in the image. The ST-VQA\ndataset comprises 23,038 images with 31,791 questions/answers pair separated into 19,027 images\nand 26,308 questions for training. Examples are shown in Figure 7.\nTextVQA. A dataset to benchmark visual reasoning based on text in images. TextVQA requires\nmodels to read and reason about text in images to answer questions about them. The dataset com-\nprises 28,408 images from and 45,336 questions. Examples are shown in Figure 7.\nPaintingForm. An artwork understanding dataset with about 19k painting images and 220k ques-\ntions. Examples are shown in Figure 9.\nMathV360k. A multimodal mathematical reasoning dataset with 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new pairs. Examples are\nshown in Figure 10.\nChEBI-20. A molecular image QA dataset with 33,010 molecule-description pairs. Examples are\nshown in Figure 11."}, {"title": "CLOSS DECLINE IN ADVERSARIAL ATTACKS", "content": "To investigate the iterative process of our method compared to ordinary adversarial attacks and\nrandom noise attacks, we check the loss reduction, as shown in Figure 12. It is evident that, with\nincreasing iterations, the losses of ordinary adversarial attacks and random noise attacks fall below\nthose of our proposed method, indicating a tendency toward overfitting. In contrast, the loss of the\nPLA fluctuates during convergence, suggesting an ongoing competition with model updates, which\nenhances the generality of the trigger images."}, {"title": "D ADDITIONAL EXPERIMENTAL RESULTS", "content": "D.1 TRACKING RESULTS OF ADDITIONAL DOWNSTREAM TASKS\nTo validate the generalizability of the proposed method, we use additional datasets to construct fine-\ntuned models, including the visual grounding dataset RefCOCO and the\nmultimodal classification dataset Hateful Memes. The experimental results are\nshown in Table 6. The results indicate that our method remains effective in these tasks, achieving\nbetter performance compared to baseline method IP.\nD.2 TRACKING PERFORMANCE ON UNRELATED LVLMS\nWe perform copyright tracking on LVLMs unrelated to the original model, including MiniGPT-\n4, QWEN2-VL, InternVL2, LLaVA-\nNEXT, and InstructBLIP. The results are shown in Table 7."}, {"title": "D.3 COMPARISON WITH TRANSFERABLE ATTACKS", "content": "We compare PLA with several transferable attack methods, such as MIM, DIM , and CroPA. The experimental results are shown in Ta-\nble 8. The results show that our PLA outperforms these transferable attack methods. We believe\nthis is because PLA is specifically designed to trigger fine-tuned models to produce predetermined\noutputs, which can be understood as \u201cfine-tuning transferability.\" In contrast, these methods focus\non cross-model (cross-architecture or cross-prompt) transferability."}, {"title": "D.4 TRACKING RESULTS WITH ADDITIONAL ORIGINAL MODELS", "content": "We also conduct experiments using QWEN2-VL-7B and InternVL2-2B\nas the original models. The results are shown in Table 9. The experimental results\ndemonstrate that our method is effective in protecting the copyright of QWEN2VL and InternVL2,\nfurther showing the generalizability of PLA to other LVLMs."}, {"title": "E ADDITIONAL ABLATION STUDIES", "content": "E.1 ABLATION OF TRAINABLE MODULES IN FINE-TUNING\nIn the fine-tuning experiments, we set the trainable components to the MLP projector and the LLM\nby default, while keeping the vision encoder frozen, which is consistent with the instruction-tuning\nphase of LLaVA. We conduct ablation experiments on the trainable modules using the ChEBI-20\ndataset, and the results are shown in Table 10. It can be observed that our proposed PLA achieves\nstrong copyright tracking performance across various common fine-tuning configurations.\nE.2 ABLATION OF THE PERTURBATION BUDGET\nThe ablation results of the perturbation budget in trigger construction are shown in Table 11. Exper-\nimental results show that the tracking performance does not significantly improve when the pertur-\nbation budget exceeds 16/255. Therefore, considering the concealment of the triggers, we chose a\nperturbation budget of 16/255.\nE.3 ABLATION OF ATTACK STEPS\nThe ablation results of attack steps in trigger construction are shown in Table 12. Experimental\nresults show that performance is poor when the number of attack steps is small; as the attack steps\napproach 1000, performance improves and begins to stabilize."}]}