{"title": "Dynamics-Aware Gaussian Splatting Streaming\nTowards Fast On-the-Fly Training for 4D Reconstruction", "authors": ["Zhening Liu", "Yingdong Hu", "Xinjie Zhang", "Jiawei Shao", "Zehong Lin", "Jun Zhang"], "abstract": "The recent development of 3D Gaussian Splatting (3DGS)\nhas led to great interest in 4D dynamic spatial recon-\nstruction from multi-view visual inputs. While existing ap-\nproaches mainly rely on processing full-length multi-view\nvideos for 4D reconstruction, there has been limited explo-\nration of iterative online reconstruction methods that en-\nable on-the-fly training and per-frame streaming. Current\n3DGS-based streaming methods treat the Gaussian prim-\nitives uniformly and constantly renew the densified Gaus-\nsians, thereby overlooking the difference between dynamic\nand static features and also neglecting the temporal conti-\nnuity in the scene. To address these limitations, we propose\na novel three-stage pipeline for iterative streamable 4D dy-\nnamic spatial reconstruction. Our pipeline comprises a se-\nlective inheritance stage to preserve temporal continuity, a\ndynamics-aware shift stage for distinguishing dynamic and\nstatic primitives and optimizing their movements, and an\nerror-guided densification stage to accommodate emerging\nobjects. Our method achieves state-of-the-art performance\nin online 4D reconstruction, demonstrating a 20% improve-\nment in on-the-fly training speed, superior representation\nquality, and real-time rendering capability. Project page:\nhttps://www.liuzhening.top/DASS", "sections": [{"title": "1. Introduction", "content": "The rapid advancements of stereoscopic cameras and ren-\ndering techniques have expanded human visual perception\nfrom 2D planes to spatial 3D representations. This evo-\nlution has paved the way for 4D dynamic free-viewpoint\nvideo (FVV) reconstruction by integrating the temporal di-\nmension, which unlocks substantial potential for a wide\nrange of applications, including augmented/virtual reality\n(AR/VR) [1] and holographic communications [41]. Nev-\nertheless, constructing 4D dynamic FVVs from multi-view\n2D inputs remains a significant challenge.\nIn recent years, Neural Radiance Field (NeRF) [33] has\nemerged as a promising approach for spatial representation\nand reconstruction. NeRFs optimize neural networks to es-\ntimate color and density based on spatial position and view-\npoint for 3D reconstruction using captured multi-view in-\nputs. The extensions of NeRFs to dynamic scene recon-\nstruction have demonstrated significant effectiveness [14,\n15, 26, 35, 37], yielding photo-realistic novel view synthe-\nsis results. However, the efficiency of NeRF-based meth-\nods is severely hindered by their low rendering speed due\nto the dense queries of neural networks. To address this"}, {"title": "2. Related Works", "content": "issue, 3D Gaussian Splatting (3DGS) [21] has been pro-\nposed as a solution to provide high-quality reconstruction\nand real-time rendering capabilities, leveraging its flexible\npoint-based primitive design and tile-based differentiable\nrasterization. Subsequent research efforts [3, 19, 27, 47, 52]\nhave been dedicated to applying Gaussian Splatting for 4D\ndynamic reconstruction, with representative works integrat-\ning the time dimension into each Gaussian primitive [52] or\nlearning spatio-temporal deformations [27, 47].\nDespite these advancements, most NeRF-based and\n3DGS-based methods for dynamic spatial reconstruction\nrely on full-length multi-view videos, i.e., non-causal in-\nputs. This reliance overlooks applications such as live\nstreaming, where only per-frame causal inputs are avail-\nable and on-the-fly training is required. This scenario is\nformalized as iteratively reconstructing 3D space at the cur-\nrent frame based on previous reconstruction caches and cur-\nrent multi-view inputs. The key challenges in this context\nare two-fold: (i) how to model temporal variations between\nframes in 3D space and (2) how to facilitate the optimiza-\ntion convergence from the previous frame to the current one.\nCritical metrics in this scenario include both the quality of\nnovel view synthesis and streaming time efficiency.\nOne intuitive solution is to directly optimize a new set\nof 3DGS primitives for each frame. However, tuning and\nstoring all 3DGS parameters for each frame results in sig-\nnificant time costs and storage overhead. A representative\nbaseline, 3DGStream [38], efficiently optimizes the trans-\nformation of Gaussian positions and rotation quaternions,\nand adaptively densifies a small number of new Gaussians.\nAlthough this method achieves fast and high-quality results,\nit overlooks the difference between inherent dynamics and\nstatics in the scene, instead treating the whole scene uni-\nformly. When modeling movements in the space, dynamic\nand static components showcase different deformation char-\nacteristics. For instance, moving objects, such as humans or\nanimals, may display substantial dynamics, with the Gaus-\nsian properties like position experiencing significant offsets.\nIn contrast, static background and stationary objects show\nminimal movement, where Gaussians remain unchanged or\nundergo slight jitters. Besides, in most natural scenes, only\na small subset of Gaussian primitives corresponds to dy-\nnamic areas. Consequently, uniformly modeling the trans-\nformation of all Gaussians results in a sub-optimal solution.\nMoreover, renewing the added Gaussian primitives for each\nframe fails to fully exploit the temporal continuity.\nBased on these insights, we propose a dynamics-aware\n3DGS streaming paradigm for on-the-fly 4D reconstruc-\ntion, termed DASS, where the optimization of each frame\ncomprises three stages: inheritance, shift, and densification.\nSpecifically, considering the temporal continuity, the newly\nadded Gaussians in the previous frame are likely to persist\nin subsequent frames. Therefore, instead of renewing these"}, {"title": "2.1. Neural Static Scene Representation", "content": "added Gaussians for each frame and optimizing them from\nscratch, we propose a selective inheritance mechanism to\nadaptively include a portion of the added Gaussians from\nthe previous frame using a learnable selection mask. Then,\nin the shift stage, we employ 2D dynamics-related prior op-\ntical flow [45, 48, 49] and Gaussian segmentation [36, 53]\nto calculate a per-Gaussian dynamics mask. Subsequently,\nwe assign grid-based layers to learn the offsets of dynamic\nand static Gaussians with different representation complex-\nities. In the densification stage, apart from the Gaussian off-\nsets that present the deformations of existing objects, new\nGaussian primitives are introduced to accommodate newly\nemerging objects. In this stage, both positional gradients\nand error maps from the shift stage serve as criteria for\nidentifying regions that require densification. The inheri-\ntance stage in the subsequent frame will process these added\nGaussians, thereby mitigating errors in the shift stage and\nreducing the optimization burden in the densification stage.\nThis three-stage pipeline effectively captures dynamic spa-\ntial components and exploits the temporal correlation, pro-\nviding fast on-the-fly training and high-fidelity streaming.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel three-stage pipeline for 4D dynamic\nspatial reconstruction that supports on-the-fly training and\nper-frame streaming. Our method builds on causal inputs\nand eliminates the need for full-length multi-view videos,\nthereby enhancing the practicability.\n\u2022 Our approach seamlessly integrates the three stages to\noptimize the reconstruction quality. By selectively in-\nheriting newly introduced Gaussians from the preceding\nframe, effectively distinguishing dynamic primitives to\nallocate optimization emphasis, and enhancing areas with\nweak reconstruction using gradient information and op-\ntimization errors, our method ensures high-fidelity dy-\nnamic spatial reconstruction.\n\u2022 Extensive experiments demonstrate the superiority of our\nmethod in multiple aspects, including a 20% improve-\nment in online training speed, superior reconstruction\nquality, and real-time rendering capability.\nIn recent years, reconstructing 3D representations from 2D\nplane visual inputs has experienced significant advance-\nments, driven by the development of NeRF [33]. NeRF-\nbased methods represent spatial scenes by optimizing multi-\nlayer perceptrons (MLPs) and generate novel views through\nvolume rendering [18]. Subsequent research has enhanced\nboth the training and rendering efficiency through grid-\nbased designs [13, 14, 34, 50]. Nonetheless, NeRF-based\napproaches typically require dense ray tracing and strug-\ngle to fulfill high-speed rendering. Recently, 3DGS [21]"}, {"title": "3. Methodology", "content": "has emerged to address these limitations by utilizing ex-\nplicit unstructured scene representation while preserving\npoint-based differentiable splatting rendering [57]. This\napproach achieves real-time rendering speed and photo-\nrealistic quality. Based on these advancements, subse-\nquent studies have further enhanced the representation ef-\nficiency [8, 11, 24, 30, 31, 55], developed feed-forward re-\nconstruction models [6, 17, 29, 56], and expanded applica-\ntions in understanding and editing [7, 36, 44, 53].\nExtending static scene representation to dynamic FVV re-\nconstruction remains a significant challenge, primarily due\nto the difficulties in modeling temporal correlations and\nvariations in 3D space. To address this issue, several studies\nhave extended NeRF to incorporate spatio-temporal struc-\ntures [5, 14, 25, 37, 42], facilitating dynamic space recon-\nstruction. Similarly, dynamic scene reconstruction meth-\nods based on Gaussian Splatting have been proposed, which\ncan be categorized into deformation-based methods, 4D\nprimitive-based methods, and iterative streaming methods.\nDeformation-based methods [3, 32, 47, 51] maintain 3D\nGaussian representations and optimize a neural deformable\nfield to capture temporal variations. 4D primitive-based\nmethods [27, 52, 54] augment Gaussian primitives by inte-\ngrating the temporal axis as an intrinsic property, thereby\ndirectly learning the spatio-temporal distributions. Note\nthat both categories of methods rely on full-length multi-\nview videos for training, which limits their ability to sup-\nport on-the-fly training and per-frame streaming. In con-\ntrast, iterative streaming methods build upon previously\nconverged representations and perform per-frame optimiza-\ntion for each frame's multi-view inputs, thereby addressing\nthe above limitations. However, this area remains largely\nunderexplored. Our work aims to develop an iterative\nstreaming pipeline that accelerates per-frame optimization\nconvergence and enhances the reconstruction quality.\nThe work most closely related to ours is\n3DGStream [38]. It proposes a two-stage optimiza-\ntion process for each frame, where the first stage optimizes\na grid-based MLP [34] for Gaussian property transforma-\ntion and the second stage wisely adds new Gaussians based\non positional gradients. However, 3DGStream does not\nconsider the intrinsic dynamic and static features in the\nscene and erases the added primitives in each frame, which\nneglects the temporal consistency. We have also noticed\nseveral concurrent studies on iterative 4D reconstruction,\nalthough their focuses differ from ours. For instance,\nS4D [16] directly manipulates 3D control points to guide\nthe movements, but it requires full optimization that\nincurs significant time costs on key frames. V\u00b3 [43] aims\nto facilitate streaming free-viewpoint videos on mobile\ndevices, with a particular emphasis on the reduction of"}, {"title": "3.1. Overview", "content": "the per-frame streaming storage overhead. Conversely,\nour work identifies the per-frame convergence speed as\nthe critical bottleneck. Moreover, while V\u00b3 decomposes\nmotion and appearance attributes for efficient reconstruc-\ntion, it primarily applies to scenarios with moving objects\nagainst vacant background and does not accommodate\nemerging objects. In addition, SwinGS [28] concentrates\non maintaining a consistently stable stream data volume\nover time and reducing storage overhead through the use\nof Markov Chain Monte Carlo [22] and a window-based\ndesign. Another work [39] considers the bit allocation in a\nsystem-level implementation. In contrast, our work focuses\non achieving fast convergence in per-frame optimization\nwhile maintaining high reconstruction quality, which is\northogonal to the contributions of aforementioned studies.\nOur method, referred to as DASS, achieves 4D dynamic spa-\ntial reconstruction in an iterative manner, enabling on-the-\nfly training and per-frame streaming. At each timestep t,\nthe optimization begins with the representation of the pre-\nvious timestep, denoted by Gt-1, to derive the current rep-\nresentation Gt, based on the current multi-view inputs from\nC viewpoints. For each scene, the pipeline starts from an\ninitial set of 3D Gaussians at frame 0. This initial represen-\ntation Go consistently serves as a base representation gbase\nwith a constant number of primitives N across subsequent\nframes. Moreover, each timestep incorporates a varying yet\nlimited number of densified Gaussians, denoted by Gdensi, to\nmodel the unique per-frame objects. A subset of these den-\nsified Gaussians is inherited to the next timestep as Ginher.\nFig. 2 depicts an overview of our DASS framework,\nwhere the optimization for each frame comprises three\nstages: inheritance, shift, and densification. Unlike pre-\nvious methods that regenerate densified Gaussians at each\ntimestep without considering temporal continuity, our ap-\nproach introduces a selective inheritance stage (Sec. 3.2)\nat the beginning of optimization for each timestep. Dur-\ning this stage, the densified Gaussians from the preced-\ning timestep, Gdensit-1, are selectively inherited using a per-\nGaussian learnable mask. This mask effectively elimi-\nnates redundant Gaussians while preserving essential ones,\ntransforming the set from {Gbase, Gdensi} to {Gbase, ginher}.\nThen, the shift stage (Sec. 3.3) processes the base Gaus-\nsians Gas and the inherited densified Gaussians Ginher.\nIn this stage, a Gaussian-level dynamics mask is esti-\nmated to categorize the Gaussians into dynamic and static\ngroups. These categories are then processed by two hash-\nencoding MLPs that operate at different complexities. This\nshift stage efficiently manages the movements and rota-\ntions of Gaussian primitives, accommodating changes from"}, {"title": "3.2. Selective Inheritance", "content": "the previous timestep and updating the representation as\n{Gbase, Ginher} \u2192 {Gbase, Ginher}. In the densification stage\n(Sec. 3.4), emerging objects, such as coffee being poured\nfrom a cup and flames from an oven, are identified by an-\nalyzing positional gradients and distortions resulted from\nthe shift stage. Then, this stage densifies additional Gaus-\nsian primitives to accurately represent these emerging ob-\njects and yields {Gbase, Ginher} \u2192 {Gbase, densi}. Our pro-\nposed design facilitates fast per-frame convergence and pro-\nduces high-fidelity view synthesis results. The details of\neach stage are elaborated in the following subsections.\nIn our pipeline, the shift and densification stages focus on\nthe deformation of existing objects and the densification for\nemerging objects, respectively. Due to the strong temporal\nconsistency inherent in the scene, the densified Gaussians\nfrom the previous frame, Gt-1,\nCdensi are likely to persist and re-\nmain valid in the current frame, since they have been specif-\nically optimized for the scene. Leveraging these frame-wise\npriors and dependencies for subsequent frames can signif-\nicantly accelerate on-the-fly training convergence. How-\never, previous methods, such as 3DGStream [38], typically\navoid reusing these Gaussians to prevent the accumulation\nof an excessive number of Gaussians, which can lead to pro-\nhibitive training and streaming overhead. To address this\nlimitation, we propose a selective inheritance mechanism\nfor the densified Gaussian primitives Gdensit-1. This approach\nadaptively selects a subset of the Gaussians that are benefi-\ncial for the reconstruction while controlling the total num-\nber of Gaussians to avoid excessive accumulation.\nSpecifically, for each Gaussian primitive in Gdensit-1, we as-\nsign a learnable parameter. The parameter vector for these\nGaussians is denoted by m. We then apply the sigmoid\nfunction to m, yielding sigmoid(m), followed by quantiza-\ntion to generate a binary mask. This mask indicates the ex-\nistence of inherited Gaussians, which is element-wise mul-\ntiplied with the Gaussian opacities and scales as follows:\nor = Quant(sigmoid(m)) \u0966\u0966, \nsr = Quant(sigmoid(m)) os, (1)\nwhere or and s\u2081 are the opacity and scale values used during\nrendering, o and s are the original opacity and scale values,\nand o represents the element-wise multiplication. When the\nmask is quantized to zero, the corresponding Gaussian has\nzero opacity and scale, thus contributing nothing to the ren-\ndering process. Therefore, this mask determines whether\neach Gaussian is retained in the rendering. By optimizing\nm, the method selectively inherits the most relevant densi-\nfied Gaussians from Gdensit-1. The loss function for optimizing\nm is expressed as follows:\nL = (1 \u2212 1)L1 + ALD-SSIM + \u03bbinher \u2211 sigmoid(m). (2)\nThe first two terms represent the fidelity loss in the vanilla\n3DGS [21], where L\u2081 and LD-SSIM are pixel-level L1\nloss and D-SSIM [46] loss, respectively. The final term,\nweighted by inher, is referred to as the mask loss and serves\nas a regularizer that encourages the parameters in m to\napproach zero. This regularization effectively reduces the\nnumber of inherited Gaussians and controls the numerical\naccumulation, while the fidelity loss aims to keep the Gaus-\nsians that are beneficial for reconstruction. Consequently,"}, {"title": "3.3. Dynamics-Aware Shift", "content": "this trade-off optimizes the learnable m and selectively in-\nherits important Gaussians from Gdensit-1. After 81 optimiza-\ntion steps, the final optimized mask Quant(sigmoid(m))\nis employed to remove redundant densified Gaussians and\npreserve the important ones as Ginher, which is subsequently\nfed into the following shift and densification stages.\nThis selective inheritance enhances the optimization\npipeline in two key aspects. First, when training at timestep\nt with the previous timestep's base Gaussians Gbasete-1, dis-\ntortions may occur due to temporal transformations and\nemerging objects. Feeding the base Gaussians directly into\nthe shift stage can lead to inaccurate adjustments in Gaus-\nsian properties. Selectively inheriting the previous densified\nGaussians Ginher before the shift stage mitigates these opti-\nmization errors. Second, after the selective inheritance and\nshift stages, we obtain a refined set of optimized Gaussians\nGinher, which reduces the number of densified Gaussians re-\nt-1\nquiring optimization, thus improving training efficiency.\nIn this stage, we focus on modeling the movements and ro-\ntations of Gaussian primitives from the preceding frame to\nthe current one. A common approach to achieve this is\ndirectly learning deformation fields to accommodate tem-\nporal transformations, as in previous deformation-based\nmethod [47] and iterative streaming method [38]. However,\nthese approaches often overlook the significant diversity in\nthe movements of Gaussian primitives present in natural\nscenes, which can result in slower convergence. Specifi-\ncally, Gaussians representing background or stationary ob-\nject typically show high similarity with minimal variations\nor remain unchanged, while those in the foreground, such as\nhumans, animals, and other moving objects, display signifi-\ncant dynamics. This disparity is illustrated in Fig. 3, which\npresents a histogram of Gaussian positional offsets between\nframes in the flame steak scene of N3DV [26]. We observe"}, {"title": "3.4. Error-Guided Densification", "content": "that most offsets fall within the range of (0, 0.01], which in-\ndicates that the majority of Gaussians undergo only minor\nshifts. Therefore, it is inappropriate to optimize the diverse\nvariations in the scene using a single deformation field.\nTo address this, we propose estimating a per-Gaussian\ndynamics mask for Gt-1 = {Gbase, Gipher} before deforma-\ntion. This mask is used to categorize all primitives into dy-\nnamic and static groups. Then, we assign two deformation\nlayers to learn the respective Gaussian transformations: A\ncomplex hash-encoding MLP [34] to capture the intricate\nshifts and rotations in the dynamic group, and a simpler\nhash-encoding MLP to model the regionally similar minor\nvariations in the static group.\nTo construct the per-Gaussian dynamics mask, we lever-\nage established techniques from optical flow [45, 48, 49]\nand segmentation [20, 23, 36, 53]. Specifically, we utilize\nGaussian Grouping [53] to initialize the scene Go, which\nassigns an object property, akin to color property, to each\nGaussian primitive and optimizes this property with 2D seg-\nmentation results serving as ground truth. These object\nproperties provide per-Gaussian segmentation results that\nconnect 2D images with 3D Gaussians, which facilitates the\nrendering of segmentation results on any viewpoint. No-\ntably, these object properties remain fixed in subsequent\nframes Gt(t > 0), thus posing no additional burden on\nthe time efficiency of on-the-fly training. For subsequent\nframes, we infer an optical flow estimation network [49]\nfrom consecutive frames captured from the same viewpoint\nand identify the areas where the optical flow exceeds a\nthreshold Yop, which we classify as dynamic areas. We then\nquery the rendered segmentation results to find out the dy-\nnamic object IDs. The dynamic Gaussians are consistently\ndetected using these IDs, as shown in Fig. 4. This process\nutilizes off-the-shelf methods that do not require network\ntraining, allowing us to obtain the per-Gaussian dynamics\nmask within milliseconds (about 300 ms). An introduction\nof these methods is provided in the supplementary material.\nAfter obtaining the dynamics mask, we employ the\nmulti-resolution hash-encoding layer I-NGP [34] to learn\nthe deformations. For each Gaussian, the deformation in-\ncludes a position offset \u03bc\u03b7 \u2208 R\u00b3, which is added to the\nGaussian position pn, and a rotation offset on \u2208 R4, which\nis applied to the Gaussian rotation quaternion qn by using\nnorm(qn) \u00d7 norm(\u03c3\u03b7). Here, norm(\u00b7) denotes the nor-\nmalization operation. The expressive ability of I-NGP is\naffected by the network complexity parameters, including\nthe hash table size THash and the number of feature dimen-\nsions per entry FHash. For the dynamic group, we utilize a\nlarge-sized hash-encoding layer Hdyn with THash = Tlarge\nand FHash = Flarge to accommodate the complex varia-\ntions of dynamic Gaussians. In contrast, for the static\ngroup, we employ a simpler hash-encoding layer Hst with\nTHash = Tlarge/4 and FHash = Flarge/2, conserving computa-\ntional resources while effectively managing minor jitters in\nbackground and stationary objects. These lightweight net-\nworks are optimized and provide a more efficient alternative\nto directly tuning the parameters of all Gaussians. To train\nHdyn and Hst, we use the fidelity loss function as follows:\nL = (1 \u2212 1)L1 + ALD-SSIM. (3)\nFig. 3 also depicts the optimized deformation distributions\nof Hdyn and Hst in the flame steak scene. The different\ndeformation patterns observed in the dynamic and static\ngroups validate the effectiveness of our proposed strategy.\nThe above two stages inherit temporally continuous Gaus-\nsians and manage deformations, without introducing new\nGaussians. However, similar to the vanilla 3DGS [21] and\n4DGS [52], densification remains essential to address areas\nwith insufficient reconstruction quality and new objects.\nTo achieve fast and high-quality densification, it is cru-\ncial to identify the subset of Gaussians in need of den-\nsification. Previous methods [21, 38] detect these under-\nreconstructed Gaussians based on historical view-space po-\nsitional gradients, denoted by \u2207p \u2261 {\u2207pn, \u2200n} \u2208 RN\u00d71\nThis approach tracks gradients from previous training steps\nand densifies Gaussians whose positional gradients exceed a\nspecific threshold Tpos. While this criterion effectively iden-\ntifies general regions with inadequate reconstruction, it does\nnot provide sufficient emphasis on regions with emerging\nobjects or significant reconstruction errors. This limitation\ncan hinder the efficient optimization of emerging objects,\npotentially slowing the convergence of on-the-fly training.\nTo address this, we propose an error-adaptive densification\nstrategy that incorporates an additional indicator to identify\nGaussians that require densification. This approach projects\nhigh-distortion areas from the image space to the 3D space\nand adaptively enhances densification in critical areas.\nSpecifically, we collect 2D error maps by comparing the\nground truth with the rendered results in the shift stage for\neach training viewpoint, denoted by EC, c = 1,2,\u2026, C."}, {"title": "3.5. Summary", "content": "Then, we filter out the pixel positions with severe distor-\ntion above Yerr, yielding an H \u00d7 W binary matrix Derr =\n{1[e > Verr], Ve\u2208 E, i\n1,..., H \u00d7 W} in the\n2D pixel plane, where 1[\u00b7] is the binary indicator function.\nNext, we project the positions of the 3D Gaussians gbase\nonto the 2D image plane as (X, Y) = Proj(gbase), where\nX \u2261 {xn,\u2200n}, Y = {yn,\u2200n}, with (xn, yn) represent-\ning the 2D pixel coordinate of the n-th Gaussian primi-\ntive along the H and W axes. The projection Proj(\u00b7) uti-\nlizes the known intrinsic and extrinsic camera matrices,\nwith further details provided in the supplementary mate-\nrial. We then identify the subset of Gaussians that fall\nwithin the erroneous areas for each viewpoint, represented\nas Serr = {gbase Der(xn, yn) = 1,\u2200n}.\nFor these highlighted Gaussian primitives, we apply a\nrelatively lower gradient threshold Terr < Tpos, thereby plac-\ning greater emphasis on high-distortion areas and facilitat-\ning more effective error rectification compared to strategies\nthat rely only on a single indicator. This leads to an error-\nguided adaptive densification scheme, yielding the subset of\nGaussians for densification S as follows:\nS = {gbase|\u2207pn > Tpos, \u2200n} \u222a {Serr \u2229 {\u04abbase|\u2207pn > Terr, \u2200n}},\n(4)\nwhere Serr\n{UC1Serr} is the combination of high-\ndistortion subsets over all viewpoints. This error-guided\nadaptive subset S prioritizes the defects in historical op-\ntimization steps while accommodating under-reconstructed\nareas. We then perform spawn densification to compensate\nfor both weakly reconstructed regions and emerging ob-\njects, following the vanilla 3DGS [21] and 3DGStream [38].\nThe newly added Gaussians, along with the transformed in-\nherited Gaussians Ginher, are optimized using the fideltiy\nloss function as in Eq. (3) to yield densi. During this\noptimization, the pruning strategy is employed to elimi-\nnate excessive Gaussian candidates with very low opacity.\nNotably, these densified Gaussians Gdensi constitute only a\nsmall fraction of the total number of primitives in the scene,\nsignificantly saving computational and time costs compared\nto full-scene optimization.\nOur proposed pipeline, DASS, effectively achieves iterative\nstreamable 4D reconstruction through the seamless integra-\ntion of the above three stages. The inheritance stage se-\nlectively preserves the optimized densified Gaussians from\nthe previous frame. By exploiting the temporal consistency,\nthis stage compensates for distortions caused by emerg-\ning objects before the shift stage and alleviates the opti-\nmization burden in the subsequent densification stage. The\ndynamics-aware shift stage incorporates separate deforma-\ntion fields for dynamic and static features. This design is tai-\nlored to accommodate complex movements in the dynamic\nelements while conversing optimization resources for static"}, {"title": "4. Experiments", "content": "components. After solving the temporal deformations, the\nerror-guided densification stage reconstructs emerging ob-\njects in the current frame. It employs both positional gra-\ndients and rendering distortions as references to effectively\nidentify the areas with weak reconstruction and emerging\nelements. Collectively, these three stages enable DASS to\nachieve high-fidelity reconstruction while significantly ac-\ncelerating on-the-fly optimization.\nDatasets. To evaluate the effectiveness of our pipeline, we\nemploy two real-world benchmark datasets that are repre-\nsentative of 4D scene reconstruction and streaming tasks.\n(1) Neural 3D Video (N3DV) dataset [26] comprises six\nmulti-view videos captured from 18 to 21 viewpoints, each\nat a resolution of 2704 \u00d7 2028. Following previous meth-\nods [25, 26, 38, 52, 54], our experiments are conducted at a\nhalf resolution of 1352 \u00d7 1014, using one view for evalua-\ntion and the remaining views for training. (2) Meet Room\ndataset, provided by the previous work on the streaming\ntask [25], includes videos at a resolution of 1280 \u00d7 720 and\na frame rate of 30 FPS. This dataset is captured from 13\nviewpoint cameras, which is relatively sparser than N3DV.\nConsistent with the baselines [25, 38], one view is reserved\nfor testing and the other twelve views are used for training.\nImplementation. We initiate our training at timestep 0 us-\ning Gaussian Grouping [53] and optimize the object prop-\nerties for segmentation concurrently during this initializa-\ntion, after which these properties remain fixed for subse-\nquent frames. The optical flow is directly inferred using\nUnimatch [49]. These off-the-shelf methods obtain the seg-\nmentation and optical flow results without requiring extra\npre-training, thereby imposing minimal computational or\ntemporal overhead during on-the-fly training and streaming.\nFor subsequent timesteps, the optimization processes for the\ninheritance, shift, and densification stages are executed with\n$1 = 20, $2 = 100, and s3 = 60 steps, respectively.\nBaselines. Our primary comparisons are against iterative\n4D dynamic spatial reconstruction methods that support\non-the-fly training and per-timestep streaming, specifically\nStreamRF [25] and 3DGStream [38], which are categorized\nas online methods. To evaluate reconstruction fidelity, we\nalso include other 4D dynamic spatial reconstruction meth-\nods that require full-length multi-view videos as input and\ndo not support on-the-fly training or per-timestep stream-\ning. This offline category includes NeRF-based methods\nsuch as DyNeRF [26], NeRFPlayer [37], HexPlane [5], K-\nPlanes [14], HyperReel [2], and Mix Voxels [42], as well as\nrecent 3DGS-based methods like 4DGS-Wu [47], 4DGS-\nYang [52], E-D3DGS [3], and STG [27]. Note that NeRF-\nPlayer [37] allows streaming after optimization but does not"}, {"title": "4.1. Experimental Setup", "content": "support on-the-fly training. In addition, we consider static\nmethods, including Plenoxels [12] and I-NGP [34], which\nfully train a static scene representation for each frame.\nMetrics. We focus on achieving fast on-the-fly training\nfor streamable dynamic spatial reconstruction, emphasizing\nboth time efficiency and reconstruction fidelity. To evaluate\ntime efficiency, we calculate the average per-frame train-\ning time over the 300 frames in the video and measure the\nframes per second (FPS). For offline methods that do not\nsupport on-the-fly training, we report their training time av-\neraged over all frames. To assess reconstruction fidelity, we\nuse metrics including peak signal-to-noise ratio (PSNR) and\ndissimilarity structural similarity index measure (DSSIM).\nThe quantitative evaluation results of our method on the\nN3DV dataset [26] are detailed in Tab. 1, and a comprehen-\nsive comparison of multiple metrics is shown in Fig. 1. Our\nmethod achieves the fastest on-the-fly training speed, which\nconverges from previous frame to the current one within 10\nseconds, yielding a 20% improvement in efficiency. In con-\ntrast, the average training time of baseline methods ranges\nfrom tens of seconds to several minutes. This time effi-\nciency advantage originates from several aspects: First, our\ninheritance stage effectively captures the temporal continu-\nity and selectively preserves the optimized results from the"}, {"title": "4.2. Results and Comparisons", "content": "previous frame. This significantly alleviates the optimiza-\ntion burden in the following stages, especially for the densi-\nfication stage, which requires 60 steps compared to the 100\nsteps in 3DGStream. Moreover, the selective inheritance\nstage is lightweight, as it optimizes only one learnable pa-\nrameter for each Gaussian primitive, in contrast to the tens\nof parameters in a pipeline fully dependent on densifica-\ntion. In addition, our dynamics-aware shift stage assigns\ndifferent deformation layers for dynamic and static compo-\nnents, which focus on significant and subtle variations, re-\nspectively. This strategy enhances the convergence achiev-\ning high-quality deformations with fewer optimization steps\n(100 steps instead of 150 steps in 3DGStream). Although\nour pipeline requires optical flow and segmentation, these\nresults are either directly inferred or rendered within mil-\nliseconds and are executed only once every few frames.\nBeyond the significant time advantage, the reconstruc-\ntion fidelity of our method also outperforms previous online\nstreamable baselines [25, 38"}]}