{"title": "CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device", "authors": ["Yicheng Fu", "Raviteja Anantha", "Jianpeng Cheng"], "abstract": "While server-side Large Language Models (LLMs) demonstrate proficiency in function calling and complex reasoning, deploying Small Language Models (SLMs) directly on devices brings opportunities to improve latency and privacy but also introduces unique challenges for accuracy and memory. We introduce CAMPHOR, an innovative on-device SLM multi-agent framework designed to handle multiple user inputs and reason over personal context locally, ensuring privacy is maintained. CAMPHOR employs a hierarchical architecture where a high-order reasoning agent decomposes complex tasks and coordinates expert agents responsible for personal context retrieval, tool interaction, and dynamic plan generation. By implementing parameter sharing across agents and leveraging prompt compression, we significantly reduce model size, latency, and memory usage. To validate our approach, we present a novel dataset capturing multi-agent task trajectories centered on personalized mobile assistant use-cases. Our experiments reveal that fine-tuned SLM agents not only surpass closed-source LLMs in task completion F1 by 35% but also eliminate the need for server-device communication, all while enhancing privacy.", "sections": [{"title": "Introduction", "content": "Server-side Large Language Models (LLMs) are powerful semantic parsers that interpret user intent and map queries to executable function calls. To ground a query within a personal environment such as an open toolbox, retrieval-augmented generation (RAG) (Borgeaud et al., 2022) can be adopted to pre-fill the LLM prompt with external knowledge relevant to the user query, such as top-K most relevant tools. An orthogonal strategy is long-context language modeling (Beltagy et al., 2020; Zaheer et al., 2020), which pre-loads the prompt with all available external knowledge, taking advantage of a larger context window up to 128K tokens (Dubey et al., 2024). However, a server-side LLM is not optimal for a mobile assistant due to privacy and latency concerns.\nPrivacy. User queries to mobile assistants are often ambiguous, making it crucial to ground them in personal information, such as contacts, installed tools, and past activities. While the assistant needs access to personal data to improve understanding, it must also prioritize user privacy by keeping sensitive information on the device. Even private cloud solutions are not ideal for this, as they commonly avoid storing user-specific data, which prevents KV caching in multi-turn dialogues (Li et al., 2024).\nLatency. In addition to the limited flexibility of KV caches due to privacy concerns, server-side LLMs introduce extra latency between understanding (which occurs on the server) and execution (which happens on the user's device). This latency can degrade the user experience, particularly for solutions requiring multiple server-device round trips. For instance, the ReAct framework (Yao et al., 2023), which breaks down the understanding task into multiple steps and reasons over intermediate execution results, is hindered by this added latency.\nA more effective solution to address latency and privacy concerns is to deploy a small language model (SLM) on-device, allowing it to process personal data and interpret user queries locally. However, SLMs introduce new challenges related to accuracy due to tighter budgets on prompts and KV caches. While a RAG approach is scalable across personal databases, the retriever is a separate model which gates the performance of the language model since it is impossible to achieve a perfect recall given the fixed prompt budget (Fan et al., 2024). The problem becomes worse for compositional queries which are naturally harder for retrieval. Consider the following example:"}, {"title": "Related Work", "content": "Small Language Models and On-device Agents demonstrate benefits of faster inference, lower latency, and enhanced privacy protection. Studies show that SLMs like Mistral (Jiang et al., 2023), Phi (Abdin et al., 2024), TinyLlama (Zhang et al., 2024), MobileLLM (Liu et al.), MiniCPM (Hu et al., 2024), and Gemma (Team et al., 2024), when fine-tuned for specific tasks, can outperform prompting Large Language Models (LLMs). In particular for the function calling task, the Octopus series (Chen et al., 2024b) has achieved remarkable accuracy, exceeding 97% for function calling on device. Most related to our work is the on-device Octo-planner (Chen et al., 2024a), which breaks down a query into multiple subqueries for function call generation. However, we argue that query decomposition in natural language space is an unconstrained optimization problem, as the granularity of decomposition depends on the available toolset.\nMulti-agent Planning is the process where multiple agents, each with unique capabilities, knowledge, and objectives, work together towards shared or interrelated goals. The rise of large language models (LLMs) has significantly advanced the development of multi-agent planning, as tasks for each agent can potentially be solved through prompts. Agentic frameworks like ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2024), LATS (Zhou et al.), SwiftSage (Lin et al., 2024), and AUTOACT (Qiao et al.) continuously prompt LLMs to reflect on and critique their past actions, sometimes incorporating additional external information, such as environmental observations and feedback. In this work we focus on multi-agent that solves user queries while understanding user data on a device. Examples of such data includes past user actions, personal entities and installed toolsets (Wu et al., 2024).\nRetrieval Augmented Generation and Long-context Language Models are two orthogonal approaches to ground a fine-tuned LM with external data sources, which in this work include the dynamic set of personal entities and tools (Borgeaud et al., 2022). A standard workflow of RAG includes possibly a query generation step (or a query decomposition step for compositional utterances) (Ma et al., 2023; Rackauckas, 2024) followed by sparse and/or dense retrieval. The retrieval model is commonly a separate set of parameters which can be trained either separately or jointly with the LLM. A major limitation of RAG is that a sub-optimal retrieval model will gate the performance of the LLM which has access to more contextual information. In contrast, long-context LLMs allow for the direct incorporation of more external data into the prompt (Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2019; Ding et al., 2023). However, this comes at the cost of increased size of prompt and KV caches, making it impractical for small language models (SLMs) and on-device deployment.\nPrompt Compression is an optimization to reduce the number of prompt tokens at least at the inference time. We adopt the technique to enable SLMs to retrieve directly from a dynamic toolbox. Related to this work are the work of Gist tokens (Mu et al., 2024), Parallel Context Encoder (Yen et al., 2024), and Squid which compresses a piece of long text into a single embedding (Chen et al., 2024c). These approaches differ in terms of how the compressed embedding is learned and incorporated with the base LLM, as either prompt tokens or late fusions in the attention layer."}, {"title": "Methodology", "content": "CAMPHOR is a collaborative agent framework that performs grounded query parsing on a user device. It consists of the following agents, including an orchestrator:\n\u2022 High-order reasoning agent plans the process of understanding a user query, solving the query by determining the order in which other expert agents are invoked, effectively using the expert skills to complete sub-tasks. and various experts:\n\u2022 Personal context agent generates function calls to search relevant personal context that would be helpful in resolving entity ambiguities and under-specified queries. The set of function calls that can be invoked by the agent is unique for each user device, as the databases of personal entities are linked to the apps installed on a user device.\n\u2022 Device information agent generates generic function calls to retrieve device information including current location, time and screen entities.\n\u2022 User perception agent represents a single function call to fetch the recent user activities on device.\n\u2022 External knowledge agent generates generic function calls to seek information from external sources including web search, Wikipedia and calculator.\n\u2022 Task completion agent generates function calls to represent the user intent towards task completion. The set of function calls that can be invoked by the agent is unique for each user device, as the capabilities are determined by the apps installed on a user device.\nTake the following query as an example\nCan you show me the cheapest flight options to Barcelona next month and add it to my calendar? Also, let my travel buddy know about our trip plan.\nThe high-order agent proactively gathers personal information to understand the user intent. This includes Device information agent to obtain the current location and Personal context agent to look up the entity travel buddy. The Task completion agent is finally invoked to generate the task completion function calls. \nWe model all agents in CAMPHOR with the same underlying SLM. A general formulation of all agents is that they take as input an agent-specific prompt and produces a function call which can be executed. The execution result of each expert agent is sent back to instruct the higher-order agent and the next expert agent. An agent prompt \\(p_a\\) is generated by a template formatting function \\(f(i_a, h_a, t_a)\\), where \\(i_a\\) is an agent-specific task instruction and \\(h_a\\) stands for the message history (i.e., the past agent actions and observations) that the agent has access to.\n\\(t_a\\) is an optional parameter representing function definitions that go into the prompt. Note that most agents actually use a static set of functions that are shared across user devices. The static set of function calls and parameters can be directly memorized by the model without definitions revealed in the prompt. However, two agents\u2014the personal context agent and the task completion agent\u2014interact with a dynamic set of device-specific functions. This is because the entity databases and capable tools are dependent on the apps installed on each user\u2019s device. For these two agents, we need to present a dynamic set of function definitions in the prompt."}, {"title": "Prompt Compression", "content": "As discussed earlier and demonstrated through experiments in Section 5.5, a RAG-based approach is sub-optimal because the retriever gates the performance of an SLM. Additionally, it is impractical to include the entire set of function definitions in the prompt, as this would quickly exceed the prompt token limit. To address this, we compress each function definition into a single token, which is then appended to the beginning of the prompt. This prompt compression approach is reminiscent of the cross-modality token used in multi-modal language modeling. By doing so, the agent can still access and reason over the full set of function definitions, while significantly reducing the number of input tokens\u2014by a factor corresponding to the average length of function definitions in the prompt.\nWe opt for the SLM itself as a text encoder to obtain the single-token embedding for each function definition, by taking the output embedding of the last token therein, as illustrated in Figure 3. The choice is motivated by the fact that the language model is already pre-trained to encode text, offering meta-learning generalization. During fine-tuning, gradients will not be back-propagated through the function tokens. Comparing to gist tokens (Mu et al., 2024) which also leverage a pre-trained language model to encode texts as KV caches, our approach significantly reduces the cache size since only a single embedding is needed for each function definition, whose KV caches are computed on the fly of language model inference.\nPositional Embeddings. We set custom position indices for the computation of the Rotary Positional Embeddings (Su et al., 2024). Every function token in the prompt shares the same position index 0 while the first token in the formal prompt starts with with position index 1. Function tokens are restricted from attending to each other, but each prompt token can attend to all function tokens, reasoning over the toolbox jointly."}, {"title": "CAMPHOR Dataset", "content": "A central focus of CAMPHOR is personalized planning and query understanding on device. However, existing function calling datasets (Patil et al., 2023; Qin et al.) only provide task completion annotations for user queries but not incorporating personal knowledge for understanding. On the other hand, there exist a few datasets on agent planning but they largely focus on mathematics (Cobbe et al., 2021; Mishra et al., 2022; Lu et al.) and common sense reasoning (Talmor et al., 2019; Geva et al., 2021) instead of query parsing.\nTo this end, we created the CAMPHOR dataset by annotating each query with a trajectory of function calls that demonstrate how a multi-agent system proactively fetches personal information to solve a user query by breaking down the understanding task into smaller actions. The dataset is developed by assigning a personal device state to each query, which includes a randomly sampled history of user activities, as well as the personal entities and tools available on the device. Each query in the dataset is generated by GPT-4o based on a device state and a set of global function definitions. The GPT-4o is also instructed to annotate the query in a multi-step fashion. The execution results are fetched from the device state for each function calling, which are then used to guide the next step of annotation. The final solution path is reviewed and verified with human oversight. Overall, the CAMPHOR dataset contains 3,410 queries, which are split into 2,728 for training and 682 for test. The dataset is flattened, resulting in 35,444 prompt-completion pairs for SLM fine-tuning, with an average of 10.39 pairs per query."}, {"title": "Experiments", "content": "We consider two SLM candidates for fine-tuning the CAMPHOR agents: Phi-3.5 and Gemma-2. The sequence of prompt and completion pairs associated with each query is obtained by unrolling the ground truth trajectories in the dataset."}, {"title": "Evaluation Metrics", "content": "We consider three end-to-end evaluation metrics on task completion:\n\u2022 Tool F1 measures the accuracy of the function names used in task completion function calls. F1 is selected as the metric because it not only accounts for true-positive predictions within the ground truth set, but also penalizes false-positive predictions outside of it.\n\u2022 Delexicalized Plan F1 measures the accuracy of both function names and parameters in task completion function calls. A true-positive prediction must not include any parameter hallucinations. The prediction is measured at the abstract syntax tree level, disregarding the order of parameters.\n\u2022 Plan F1 measures the accuracy of both function names, parameters and their values in task completion function calls. A subset of the parameters does support a open set of values, instead of closed-set enums. We adopt a lenient match rule to evaluate open-ended values: a match is incurred if the Sentence-BERT (Reimers, 2019) embedding similarity between target and predicted values is higher than threshold 0.7."}, {"title": "LLM Baseline Experiments", "content": "Before presenting results for the fine-tuned SLM agents, we first evaluated the performance of state-of-the-art LLMs on the CAMPHOR test set as baselines. We choose Claude-3.5 as the LLM for evaluation to avoid any potential label leakage as the CAMPHOR dataset is generated with GPT-4 in the loop.\nA key difference between instruction-based inference and fine-tuning is that the former relies fully on the prompt instructions which must be clear and often framed with specific structure to guide the pre-trained model. In comparison (as we will show in Section 5.3), the prompts used in fine-tuning can be more concise and tailored according to prompt budgets, as the model is tuned to act for certain pattern of inputs. Given the requirement of instruction-based inference, we evaluated a wide range of prompting strategies and aim to pick the best for the comparison with SLM fine-tuning. The prompting strategies include:\n\u2022 Static employs a consistent prompt template which contains all available function definitions for all CAMPHOR agents. The LLM agent is tasked to generate a sequence of function calls for each CAMPHOR query. The prediction history is also appended to the prompt of each turn.\n\u2022 ReAct is similar to the Static baseline, but additionally has the option to perform an explicit reasoning step before generating a function call.\n\u2022 Reflexion is similar to ReAct, but additionally incorporates a reflection step to examine the generated function calls and provide feedback. Reflexion inherently requires multiple trials, the number of which is set to 3.\n\u2022 AUTOACT employs three district prompt templates that respectively handle function call generation, parameter filling and reflection of the results. Similar to other baselines, each CAMPHOR query is parsed as a sequence of function calls with parameter values. The prediction history is also appended to the prompt of each turn.\n\u2022 CAMPHOR Agents employ distinct prompt templates for each CAMPHOR agent. The prompt of each agent contains agent-specific task descriptions, function calls of that agent and in-context examples. The prediction history is also appended to the prompt of each turn. This setting is closer to the dynamic prompt construction adopted in fine-tuning."}, {"title": "SLM Fine-tuning Experiments", "content": "Remember that we consider two base SLMs, Phi-3.5 and Gemma-2, for fine tuning. A key question we aim to answer is how to formulate the prompt such that the SLM maintains high accuracy while satisfying the prompt budget of on-device deployments.\nWe start with the dynamic prompt formatting function described in Section 3.2. Each agent prompt contains an agent-specific task description, the prediction history and optionally function definitions. Compared to the agent-specific prompts in LLM experiments, there are two differences in the SLM fine-tuning. First we only append definitions for the dynamic set of functions for the personal context agent and the task completion agent, since the static functions (and their parameters) can be memorized via fine-tuning. Second, we removed in-context examples for each agent from the prompt, considering the prompt budget and also because the model can be trained to react to input patterns without in-context learning.\nTable 2 shows the results comparing the fine-tuned SLMs with the best LLM prompting strategy. The fine-tuned SLMs, including both Phi-3.5 and Gemma-2, outperform the LLM result in task completion metrics. Meanwhile, the Phi-3.5 model without fine-tuning does poorly in task completion. The results highlight the effectiveness of fine-tuning an SLM for specialized agent tasks, showing it to be more powerful than simply prompting a pre-trained LLM with task instructions. Moreover, the performance of fine-tuning is not compromised by prompt simplification since the model is trained to learn fixed input-output mappings patterns.\nTo further optimize the prompt, we remove system instructions from each agent prompt and only reveal the prediction history, based upon which the SLM is fine-tuned to predict the next function call in the trajectory. Surprisingly we found that the prompt simplification leads to only marginal degradation of the task completion, with a plan F1 38.3% compared to 38.7% in the original setting. The result demonstrates that fully non-instruction tuning is also a promising direction to further improve on-device efficiency without sacrificing much accuracy."}, {"title": "Prompt Compression", "content": "Even though we only append definitions for the dynamic function set in the prompt, they still consume a significant amount of prompt space for large toolboxes. We further experiment with the prompt compression technique described in Section 3.3 where each function definition is represented as a single token in the prompt.\nAs shown in Table 3, applying the prompt compression technique only leads to marginal changes in the task completion F1, from 39.89 % to 38.45%. But it should be noted that the prompt compression technique reduces the number of static prompt tokens (without message history which dynamically grows) further by 96.00% for the personal context agent and 95.02% for the task completion agent."}, {"title": "Comparison with RAG", "content": "One could argue that an alternative approach to generalize to a dynamic toolbox is retrieval-augmented generation (RAG). However, we showcase here that RAG creates a performance bottleneck for the SLM when handling CAMPHOR queries due to sub-optimal retrieval recall."}, {"title": "Why is RAG not working well?", "content": "Queries in CAMPHOR are compositional with multiple task completion function calls. The average number of task completion function calls for each query is 3. However, given a tight prompt budget of K=5, it is rather difficult to make sure the retriever is able to fetch all function calls into the K=5 bucket. As a direct consequence, the language model will not see the correct function definitions in the prompt, conditioned on which it is trained to generate the completion."}, {"title": "Conclusion", "content": "This work introduces CAMPHOR, a collaborative, SLM-based agent framework designed for personalized query parsing on user devices. CAMPHOR proactively retrieves on-device information and decomposes the understanding tasks into multiple steps of function calls. Our results show that a fine-tuned SLM outperforms instruction-based LLMs in this task. By employing advanced prompt compression techniques, CAMPHOR strikes an optimal balance between accuracy and efficiency, while safeguarding user data directly on the device."}, {"title": "Limitations", "content": "The personalized user query parsing task studied in this work is restricted to single interactions. While many user queries can indeed be resolved in one interaction, this approach oversimplifies the problem space. In practice, many real-world tasks\u2014especially those requiring user disambiguation or confirmation\u2014still depend on multi-turn interactions between the user and the assistant. In such cases, system policies play a critical role in guiding the conversation and triggering the next agent. Future work should focus on extending CAMPHOR to handle multi-turn conversations, incorporating system policies and user follow-ups.\nThe simulated device environment in this work also primarily focuses on the \"happy path\" of personal information retrieval. It does not account for more complex runtime feedback and error-handling logic, such as disambiguation requests for multiple search results, which would need to be communicated back to the user before task continuation. In future, we aim to scale our data simulation approach to handle more complex runtime feedback and in multi-turn conversational settings, as discussed in the first paragraph."}]}