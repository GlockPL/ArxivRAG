{"title": "TIMRL: A Novel Meta-Reinforcement Learning Framework\nfor Non-Stationary and Multi-Task Environments", "authors": ["Chenyang Qi", "Huiping Li", "Panfeng Huang"], "abstract": "In recent years, meta-reinforcement learning (meta-\nRL) algorithm has been proposed to improve sample efficiency\nin the field of decision-making and control, enabling agents to\nlearn new knowledge from a small number of samples. However,\nmost research uses the Gaussian distribution to extract task\nrepresentation, which is poorly adapted to tasks that change\nin non-stationary environment. To address this problem, we\npropose a novel meta-reinforcement learning method by lever-\naging Gaussian mixture model and the transformer network to\nconstruct task inference model. The Gaussian mixture model is\nutilized to extend the task representation and conduct explicit\nencoding of tasks. Specifically, the classification of tasks is\nencoded through transformer network to determine the Gaussian\ncomponent corresponding to the task. By leveraging task labels,\nthe transformer network is trained using supervised learning.\nWe validate our method on MuJoCo benchmarks with non-\nstationary and multi-task environments. Experimental results\ndemonstrate that the proposed method dramatically improves\nsample efficiency and accurately recognizes the classification of\nthe tasks, while performing excellently in the environment.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has achieved remark-\nable achievements in solving complex and challenging\ntasks [1], [2]. However, training an excellent agent using RL\nalgorithms typically requires a significant amount of inter-\naction with the environment. Agents, unlike humans, cannot\nlearn from previous tasks and acquire experience that allows\nthem to quickly master new tasks with minimal attempts. The\ntopic of how to enable agents to leverage prior task experience\nto rapidly adapt to new tasks is both valuable and urgent.\nMeta-reinforcement learning (meta-RL) aims to improve the\nefficiency and generalization of RL in the face of new tasks\nthrough a meta-learning perspective. The existing research on\nmeta-RL can generally be summarized into three categories.\nThe first one is a gradient-based method that generates policies\nfor new tasks by optimizing the meta-policy and adjusts the\nmeta-parameters online during the meta-learning [3], [4]. The\nsecond one is recurrence-based method, aiming to leverage the\nadvantage of recurrent neural networks (RNNs) in handling\nsequential data to assist in constructing the meta-RL frame-\nwork [5], [6]. All the above methods use the on-policy method"}, {"title": "II. RELATED WORK", "content": "The research on meta-RL can be mainly divided into three\ncategories: the gradient-based method, the recurrence-based\nmethod and the context-based method.\nThe representative achievement of gradient-based method is\nthe Model-Agnostic Meta-Learning (MAML) [4], which learns\nto acquire initial \u201chighly adaptive\u201d parameters and employs the\npolicy gradient to quickly adjust initial parameters in order to\nadapt to new tasks. The advantage of the MAML algorithm\nlies in its model-agnostic, which means it can be applied to\nany model that can be trained using gradient descent methods.\nBased on MAML, subsequent studies had conducted further\nexplorations in terms of model-agnostic and rapid adaptability\n[3], [10], [11]. In addition, Al-Shedivat et al. [12] proposed\na gradient-based meta-RL algorithm to improve the efficiency\nof adaptation in non-stationary and competitive environments.\nThe main feature of recurrence-based method is that the\npolicy network of the algorithm consists of the recurrent\nnetworks [13]. The recurrence-based method uses prior experi-\nence to train recurrent networks to equip the network with task\nrepresentations, thereby enabling agent to quickly adapt to new\ntasks [14], [15]. RL2 [5] utilized RNNs to receive and process\ntime-series data and encoded information using the network\nweights to represent task-related information. Mishra et al.\n[6] proposed a lightweight meta-learner model, simple neural\nattentive learner (SNAIL), that utilized temporal convolutions\nto process information from prior experiences.\nThe purpose of the context-based method is to train a\nsuperior task-conditioned policy. This method uses context to\nderive task representation, which is provided as auxiliary input\nto the policy network. PEARL [7], as an influential context-\nbased algorithm based on SAC [8], adopted off-policy method\nthat decoupled task inference from policy learning, greatly\nenhancing sample efficiency. Wu et al. [16] decomposed\nthe task representation and encoded different aspects of the\ntask. Xu et al. [17]proposed Posterior Sampling Bayesian\nLifelong In-Context Reinforcement Learning (PSBL), which\nperforms well when there are significant differences between\nthe distributions of training and testing tasks."}, {"title": "B. Task Inference and Task Embedding for Meta-RL", "content": "The task inference model utilizes context to infer task\nfeatures or attributes as task embeddings, which serves to\nguide the learning process for new tasks, playing a significant\nrole in meta-RL [18]. We focus on those methods that based\non task inference and task embedding.\nBing et al. proposed Continuous Environment Meta-\nReinforcement Learning (CEMRL) in [9] for non-stationary\nenvironments, where the training strategy and inference model\ncan achieve excellent representation and task embedding. Jiang\net al. [19] designed two independent exploration goals in the\naction and task embedding spaces, reconstructing optimization\nobjectives for the inference and policy networks to enhance\nexploration efficiency. A conditional variant autoencoder is\ndesigned by Chien et al. [20] to learn task embeddings,\nwhich enhances the generalization of the strategy. Ada et al.\n[21] proposed Unsupervised Meta-Testing with Conditional\nNeural Processes (UMCNP), which combines parameterized\npolicy gradient and task inference-based method. To solve\nthe problem of sparse rewards, Jiang et al. [22] proposed a\nDoubly Robust augmented Transfer (DRaT) method. Lee et\nal. [23] proposed Subtask Decomposition and Virtual Training\n(SDVT) to improve the performance of the algorithm in non-\nparametric tasks. Lan et al. [24] optimized the task encoder\nto generate specific task embeddings, while learning a shared\npolicy conditioned on the task embeddings. These works [25],\n[26], [27] also make an excellent contribution to improving\nthe sample efficiency and adaptability of the algorithm."}, {"title": "III. PRELIMINARIES", "content": "In the framework of context-based meta-RL algorithms,\nagent obtains task-relevant information from context and uses\nthe inference model to derive an embedding code for task rep-\nresentation, ensuring that the agent completes the task quickly.\nSpecifically, agent integrates context into the latent variable\nspace by task inference model, enabling the conditioned policy\nto quickly adapt to new tasks.\nAs a classical context-based meta-RL algorithm, PEARL [7]\nhas performed excellently in various benchmarks. It introduces\nthe meta-learning approach based on the SAC algorithm, in\nwhich variational auto-encoder (VAE) is employed to train task\ninference method [28]. The encoder in the VAE encodes the\ntransition $(s, a, r, s')$ to obtain independent Gaussian factors\n$\\psi(z|c_{1:N})$. The product of these Gaussian factors is used to\nrepresent the prediction of the posterior $q_{\\phi}(z|C_{1:N})$, which is\nthe permutation-invariant function"}, {"title": "A. Context-based Meta-RL", "content": "$\\qquad q_{\\phi}(z|C_{1:N}) \\propto \\prod_{n=1}^{N}\\psi(z|c_n)$.\nThe objective of the optimization is:\n$\\qquad E_{T} [E_{z \\sim q_{\\phi}(z|c_t)} [R(T, z)] + \\beta KL (q_{\\phi}(z|c_T)||p(z))]$.\nwhere R(T, z) represents the objective functions, p(z) is a\nunit Gaussian prior over tasks."}, {"title": "B. Gaussian Mixture Model", "content": "The standard GMM is modeled based on multiple Gaussian\ndistributions. In the initial stage of the task, the parameters\nof each Gaussian distribution (i.e. mean and variance) are\nunknown. Furthermore, the proportion of each Gaussian distri-\nbution within the overall GMM model is randomly assigned.\n$\\qquad M_{GMM} = \\lambda_1N(\\mu_1, \\sigma_1) + \\lambda_2N(\\mu_2, \\sigma_2) + ...\\qquad\\qquad \\\\= \\sum_k \\lambda_k N(\\mu_k, \\sigma_k)$.\nwhere $N(\\mu_k,\\sigma_k)$ is the Gaussian distribution, which has the\nparameters of mean $\\mu_k$ and variance $\\sigma_k$, $\\lambda_k$ is the proportion\nof each Gaussian distribution.\nGiven N samples and K Gaussian components, we aim to\ndetermine Gaussian component each sample belongs to and\nestimate the parameters of each component. The Expectation-\nMaximization algorithm is used to calculate the parameters of\nthe GMM. First, let $\\gamma(i,k)$ denote the probability that the i-th\nsample belongs to the k-th Gaussian distribution:\n$\\qquad\\gamma(i,k) = \\frac{\\lambda_k N(x_i | \\mu_k, \\sigma_k)}{\\sum_{j=1}^{K} \\lambda_j N(x_i | \\mu_j, \\sigma_j)}$.\nwhere $x_i$ is the i-th sample. At the beginning each parameter\nin (4) is obtained by initializing and $\\gamma(i,k)$ is calculated based\non the parameters. This means that the current estimates of\nthe model parameters are used to calculate the probabilities.\nThen, the probabilities calculated above are used to update the\nestimates of the model parameters. Specifically, the parameters\nare updated in each step:\n$\\qquad N_k = \\sum_{i=1}^{N} \\gamma(i,k), \\qquad \\lambda_k = \\frac{N_k}{N}$ $\\qquad, \\qquad \\mu_k = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma(i,k) x_i$\n$\\qquad\\sigma_k^2 = \\frac{1}{N_k}\\sum_{i=1}^{N} \\gamma(i,k) (x_i - \\mu_k)^2$\nwhere $N_k$ is the number of samples in k-th component."}, {"title": "C. Transformer Network", "content": "Transformer network was first proposed by Vaswani et\nal. [29] as an architecture to efficiently model sequences,\nwhich is constructed based on the self-attention mechanism.\nIn RL, each transition is treated as a sequence unit and the\ncontext is recognized using the efficient sequence processing\ncapability of transformer [30]. The encoder in the transformer\nis mainly responsible for transforming the sequence of input\ninto a fixed-length vector of representation, while the decoder\ndecodes this vector into the sequence of output. The encoder\nblock is a stack of multiple encoder layers, and each encoder\nlayer contains the Multi-Head attention and the Feed-Forward\nnetwork. The Multi-Head attention is one of core mechanisms\nin transformer, enhancing the focus on different features. It\nrepresents each position in the input sequence as query(Q),\nkey (K), and value (V), and computes the attention distri-\nbution between each position and other positions to obtain a\nweighted sum, which captures the relevant information of each\nposition. The equation of attention as in (6):\n$\\qquad Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere $d_k$ is the dimension of K. The equation of Multi-Head\nattention as in (7):\n$\\qquad MultiHead(Q, K, V) = Concat(h_1, h_2, ..., h_h)W^O$\n$\\qquad\\qquad h_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\nwhere Concat represents to concatenate the results of heads,\n$W^O$, $W_i^Q$, $W_i^K$, $W_i^V$ are parameter matrices. The Feed-\nForward network performs further nonlinear transformations\non the output of the Multi-Head attention to enhance the\nrepresentation of the model. The decoder block is also a\nstack of multiple decoder layers, but the decoder layer adds\na Masked Multi-Head attention to prevent access to future\ninformation."}, {"title": "IV. TASK INFERENCE IN META-RL", "content": "In this section, we introduce in detail the architectural\nframework of the TIMRL, which utilizes the task infer-\nence model with GMM and transformer to generate task\nembeddings that aid in the training of policy networks. In\norder to adapt to non-stationary and multi-task environments,\nwe use GMM to extend the task representation and design\nthe transformer-based recognition network to classify tasks.\nTIMRL decouples task inference and global policy learn-\ning and reduces the complexity of training. In addition, the\nrecognition is individually trained using supervised learning\nin the task inference model to improve the accuracy of task\nrecognition."}, {"title": "A. Task Inference Model", "content": "In traditional context-based meta-RL, the task representation\ncapabilities of inferred networks based on a single Gaussian\ndistribution are inadequate in some scenarios. In contrast, our\napproach achieves the task inference model leveraging the\nframework of GMM, meaning the model needs to use multiple\nGaussian distributions, as shown in Fig. 1. The latent variable\nz sampled by the corresponding Gaussian component, which\nis selected by the transformer-based recognition network.\nWe construct recognition network based on transformer,\nwhich is aimed at learning how to accurately recognize tasks in\nthe environment. First, the pre-processing program is designed\nfor pre-processing the input data e of the recognition network,\nwhich is the context during meta-training. The data cleansing\nis utilized to repair errors and dimensional inconsistencies in\nthe data. In addition, data pre-processing improves perfor-\nmance and reduces overfitting of the recognition network."}, {"title": "B. Training Model", "content": "In the task inference model, VAE achieves generative mod-\neling by learning the distribution of latent space. Based on\ntraditional VAE, there are two objectives to be minimized in\nthe loss function, the reconstruction term and the regularization\nterm. Considering the framework of GMM, we denote the loss\nfunction as:\n$\\qquad L_{vae} = \\frac{1}{K}\\sum_{k_i=1}^{K} (E_{z\\sim q_{\\phi_{k_i}}(z|x_i)} [log P_{\\theta} (x_i|z)]$\\n$\\qquad\\qquad\\qquad -D_{KL}[q_{k_i}(z|x_i)||P_{k_i}(z)])$.\nwhere $q_{k_i}(z|x_i)$, $p_{\\theta}(x_i|z)$ represent the encoder and decoder,\nand $p_{k_i}(z)$ represents the prior of z. To ensure proper propa-\ngation of the gradient during the sampling process, the repa-\nrameterization method is employed, z = g(x, \\epsilon), $\\epsilon$ ~ N(0, I).\nWe expect the output of decoder to attain the utmost\npossible value, thereby enhancing the likelihood of generating\n$X_i$. This means that reconstruction term is employed to ensure\nthat the output generated by the decoder closely resembles\nthe input data as much as feasible. The reconstruction loss is\nexpressed using the mean-square error:\n$\\qquad \\mathscr{L}_{recons} = \\frac{1}{m}\\sum_{i=1}^{m} ||X_i - \\hat{X_i}||^2$", "content_id": "3"}, {"title": "C. TIMRL", "content": "The framework of TIMRL is shown in Fig. 2, and the\ntraining process is summarised as pseudo-code in Algorithm 1.\nIn meta-RL, when training tasks are given, we aim to leveraga\nthe task inference model for accurately inferring the features\nof the tasks and training the policy to quickly solve new tasks.\nWe focus on the task inference process and propose the task\ninference model based on the framework of GMM. Utilizing\nthe recognition network to classify various tasks and assigning\neach classification to a distinct Gaussian component enables\nagent to learn optimal policy for each task more efficiently.\nIn TIMRL, we first design the MLP network for pre-\nprocessing context. In the task inference phase, the inference\nmodel is utilized to obtain latent variable z to guide the\ntraining of policy. We can describe the probability of obtaining\nz as the posterior p(z|c), which is approximated by the encoder\nq(z c). The encoder is constructed using the GMM architec-\nture with K components. We utilize different components to\nfit the posterior corresponding to each classification of tasks.\nThe recognition network, as represented by (8), is responsible\nfor recognizing the classification of each task and updating\nit based on the current context and corresponding labels.\nThe contribution of the encoder to the objective function of\nVAE corresponds to the regularization term. Inspired by [9],\nwe use the method of reconstructing MDP for decoding the\nlatent variable. Specifically, the decoder with two specialized\nnetworks for reconstructing the next state and the reward. Input\nthe tuple (s, a, z) to the decoder network and the encoder\nnetwork estimates the next state and reward. This process can\nbe described by the transition function $p(s_{t+1}|s_t, a_t, z_t)$ and"}, {"title": "C. TIMRL", "content": "where $\\hat{x_i}$ is the output decoder data, and m is the number of\ndata.\nUnlike the reconstruction term, the regularization term\navoids the disappearance of randomness and improves the\ndiversity of outputs. the regularization term is expressed using\nthe KL divergence with the expectation that the distribution of\nlatent space approximates the priori Gaussian distribution. For\nthe inference of various tasks in meta-RL, it is necessary to\nfurther refine the task inference model to clarify the boundaries\nbetween tasks. We define different tasks corresponding to dif-\nferent prior distributions. The regularization loss is expressed\nby the KL divergence between the Gaussian mixture and\nthe priori distribution. The ki-th component corresponds to\nthe parameters of the Gaussian distribution are $\\mu_{k_i}$ and $\\sigma^2_{k_i}$,\nmeanwhile, the parameters of the priori Gaussian distribution\nare $\\mu_{k^'}$ and $\\sigma^2_{k'}$. The regularization loss is defined as:\n$\\qquad \\mathscr{L}_{regula} = \\sum_{k_i=1}^{K} D_{KL} [N(\\mu_{k_i}, \\sigma^2_{k_i}) || N(\\mu_{k'}, \\sigma^2_{k'})]$\n$\\qquad\\qquad = \\sum_{k_i=1}^{K} \\frac{1}{2}\\sum_{k_i=1}^{K} (1 - log\\frac{\\sigma^2_{k_i}}{\\sigma^2_{k'}} + \\frac{\\sigma^2_{k'}}{\\sigma^2_{k_i}} + \\frac{(\\mu_{k'} - \\mu_{k_i})^2}{\\sigma^2_{k_i}})$", "content_id": "4"}, {"title": "C. TIMRL", "content": "p(rt St, at, zt). So, the reconstruction term in the loss function\nof VAE can be rewritten as:\n$\\qquad \\mathscr{L}_{recons} = \\mathscr{L}_{state} + \\mathscr{L}_{reward}$$\n$\\qquad\\qquad = ||S_{t+1} - \\hat{S}_{t+1}||^2 + ||r_{t} - \\hat{r}_{t}||^2$\\nfollowing meta-training objective:\n$\\qquad \\mathscr{L} = \\mathscr{L}_{recons} + \\alpha \\mathscr{L}_{regula}$\nwhere $\\alpha$ is the hyperparameter.\nTo accurately recognize the classification of the task, we\ndecouple the GMM from the recognition network during train-\ning. In the practical implementation, we train the recognition\nnetwork using (9) after updating VAE in each epoch. The\ninput of the recognition network is sampled from the replay\nbuffer that is shared concurrently with both the VAE and the\npolicy learning. This method not only improves efficiency but\nreduces complexity when training task inference model.\nWe use SAC for policy learning to realize off-policy training\nand the latent variable z will be used as the auxiliary input to\nthe SAC. The objective function of SAC is:\n$\\qquad \\mathscr{L}_{SAC} = E_{s_t \\sim D} [E_{a_t \\sim \\pi} [log (\\pi_{\\upsilon}(a_t|s_t)) - Q_{\\omega}(s_t, a_t)]]$\n(16)", "content_id": "5"}, {"title": "V. EXPERIMENTS", "content": "In this section, we introduce the non-stationary and multi-\ntask environment and evaluate the performance of TIMRL\ncompared to state-of-the-art meta-RL algorithms. In order to\nverify that our proposed recognition network and task infer-\nence model are effective, we performed ablation experiments\non the relevant modules of our algorithm."}, {"title": "A. Experiments Setup", "content": "We evaluate TIMRL by setting up non-stationary and multi-\ntask environments with the MuJoCo benchmark. Specifically,\nwe extend the traditional stationary setup to build non-\nstationary environments, where the goal may change at any\ntimestep within an episode, that is to say, the MDP will change\nat any time. Based on the above setup, we design the multi-\ntask environment, i.e., completing multiple tasks in a single\ntraining process, to evaluate the adaptability of the proposed\nalgorithm in the multi-task environment. We provide a brief\ndescription of the setup of these environments as follows.\nNon-stationary environment:\n1) Cheetah-Nonstat-Dir: Cheetah is required to randomly\nchange direction of movement, forward or backward, in\nthe episode, namely direction-following.\n2) Cheetah-Nonstat-Vel: The speed of movement that\nCheetah is required to achieve will change at any\nttimestep in the episode, namely velocity-following.\n3) Cheetah-Nonstat-Flipping: Cheetah is required to ran-\ndomly change direction of flip, flipping forward or\nbackward, in the episode, namely flip-following.\n4) Ant-Nonstat-Dir: Ant is required to randomly change\ndirection of movement, forward or backward, in the\nepisode.\nMulti-task environment:\n1) Cheetah-Dir/Goal: Cheetah is required to complete\ntwo classifications of tasks in the episode, includ-\ning direction-following and goal-following. The goal-\nfollowing means the Cheetah is required to randomly\nchange its target location in the episode.\n2) Cheetah-Flipping/Jumping: Cheetah is required to com-\nplete two classifications of tasks in the episode, in-\ncluding flipping-following task and jumping-following.\nThe jumping-following means the Cheetah is required\nto jump randomly in the episode.\n3) Cheetah-Vel/Goal/Dir: Cheetah is required to complete\nthree classifications of tasks in the episode, includ-\ning velocity-following, direction-following and goal-\nfollowing.\nWe set up a different number of training and testing tasks\nfor each environment, and randomly select from these tasks\nwhen training the algorithm. During the training process, we\ndivide the samples in such a way that the percentage of training\nsamples is 80% and that of validation samples is 20%."}, {"title": "B. Performance", "content": "We compare TIMRL with the classical context-based meta-\nRL algorithms PEARL [7] and CEMRL [9], and complete\ntraining based on the setup of their proposed algorithms to\nobtain experimental results.\n1) Non-stationary environment\nWe analyze the ability of algorithm to learn the behaviour\nof agent in new tasks when using TIMRL for training in\nnon-stationary environment. Since the tasks change randomly\nduring an episode and rewards vary from one task to another,\nit is necessary to accurately identify the task classification in\norder to complete the tasks. Fig. 4 shows the rewards obtained\nby the learned policies of algorithms in the test tasks.\nThe result on the four task environments we set up shows\nthat TIMRL is able to quickly adapt and learn in non-\nstationary environment, and its performance is superior to that\nof CEMRL in some of environments. Reward curves indicate\nthat our proposed task inference model can be adapted to\nthe meta-RL learning process. In contrast, PEARL does not\nperform well and is unable to learn the policy to complete the\ntask in non-stationary environment, as described above, we\nspeculate that it is because the task inference model used by"}, {"title": "3) Ablation Studies", "content": "We explore the role of the proposed core task inference\nmodule, especially that of the task recognition network within\nit, through ablation studies. In TIMRL, the recognition net-\nwork helps the policy network to train faster and better by\nclassifying and encoding the task. In each epoch, the recogni-\ntion network we have set up needs to be trained 10 times so\nthat it can accurately recognize the tasks. In order to verify the\neffectiveness of the recognition network, we make the results\nof the recognition network randomly selected. This will result\nin the algorithm cannot accurately determine the classification"}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed a novel meta-RL algorithm to\nadapt to non-stationary and multi-task environments. A GMM-\nbased task inference model was designed to encode tasks and\nassist in the training of the policy network. To improve the\nperformance of the task inference model, a transformer-based\ntask recognition network was designed to recognize tasks, and\nthe recognition network was decoupled from the GMM to\nimprove recognition accuracy and reduce training difficulty.\nThe TIMRL can effectively utilize contextual information\nto provide accurate task encoding for the training of policy\nnetworks and achieve SOTA performance."}]}