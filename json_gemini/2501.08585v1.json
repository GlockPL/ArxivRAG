{"title": "A Systematic Review of Machine Learning Methods for Multimodal EEG Data in Clinical Application", "authors": ["Siqi Zhao", "Wangyang Li", "Xiru Wang", "Stevie Foglia", "Hongzhao Tan", "Bohan Zhang", "Ameer Hamoodi", "Aimee Nelson", "Zhen Gao"], "abstract": "Machine learning (ML) and deep learning (DL) techniques have been widely applied to analyze electroencephalography (EEG) signals for disease diagnosis and brain-computer interfaces (BCI). The integration of multimodal data has been shown to enhance the accuracy of ML and DL models. Combining EEG with other modalities can improve clinical decision-making by addressing complex tasks in clinical populations. This systematic literature review explores the use of multimodal EEG data in ML and DL models for clinical applications. A comprehensive search was conducted across PubMed, Web of Science, and Google Scholar, yielding 16 relevant studies after three rounds of filtering. These studies demonstrate the application of multimodal EEG data in addressing clinical challenges, including neuropsychiatric disorders, neurological conditions (e.g., seizure detection), neurodevelopmental disorders (e.g., autism spectrum disorder), and sleep stage classification. Data fusion occurred at three levels: signal, feature, and decision levels. The most commonly used ML models were support vector machines (SVM) and decision trees. Notably, 11 out of the 16 studies reported improvements in model accuracy with multimodal EEG data. This review highlights the potential of multimodal EEG-based ML models in enhancing clinical diagnostics and problem-solving.", "sections": [{"title": "Introduction", "content": "patterns (1) The process involves amplifying these electrical signals and displaying them as waveforms, which can be analysed to understand different states of brain activity, such as wakefulness, sleep, and various levels of consciousness. EEG signals are divided into different frequency bands, including delta, theta, alpha, beta, and gamma waves, each associated with specific cognitive and physiological states. This capability to monitor and interpret brain waves in real"}, {"title": "1.1 EEG background", "content": "Electroencephalography (EEG) is a non-invasive method used to capture and record the brain's electrical activity through electrodes placed on the scalp. These electrodes measure voltage fluctuations caused by ionic currents within neurons, enabling the real-time recording of brain wave"}, {"title": "1.2 Machine Learning Techniques for EEG Signals", "content": "Building on the foundational understanding of EEG data and its significance, it is essential to explore the advanced methodologies that have been developed to analyse and address the challenges associated with EEG signals. Machine Learning (ML) and Deep Learning (DL) methods have become increasingly prominent in analyzing and resolving EEG data and related problems due to their ability to automatically extract complex patterns from large datasets. ML is a broad field of study that focuses on developing algorithms that can learn from and make predictions on data. DL has revolutionized the field of ML by enabling the automatic extraction of intricate structures in large datasets (4). DL methods significantly outperform traditional approaches in various domains, such as speech recognition, image recognition, and natural language processing, by utilizing multiple layers of representation to learn from raw data (5). Various EEG problems that ML or DL methods aim to address include epileptic seizure detection, sleep stage classification, mental state and cognitive workload assessment, emotion recognition, and brain-computer interface (BCI) applications (6,7). Several DL techniques have been employed to tackle these challenges. Convolutional Neural Networks (CNNs) are widely used for their ability to capture spatial features in EEG signals, making them effective in tasks like seizure detection and sleep stage classification (8,9). Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) (10) networks are utilized to analyse the temporal dependencies in EEG data, which is crucial for understanding time-series data and predicting mental states (11). Autoencoders and Variational Autoencoders (VAEs) have been applied for feature extraction and noise reduction, enhancing the quality of EEG signal analysis (12,13). Furthermore, hybrid models combining CNNs and RNNs are increasing popular for leveraging both spatial and temporal information, providing robust solutions for complex EEG-based applications. These DL methods collectively contribute to significant advancements in the interpretation and utilization of EEG data in both clinical and non-clinical settings."}, {"title": "1.3 EEG Multimodal data", "content": "Multimodal data integration involves the simultaneous collection and analysis of data from multiple sources, providing a more holistic view of brain function and its interactions with physiological and behavioral states. This approach leverages the strengths of different neurophysiological tools to overcome the limitations of individual modalities, resulting in richer and more comprehensive datasets (14).\nNeurophysiological data, including EEG, functional magnetic resonance imaging (fMRI), eye tracking, skin conductance, and heart rate monitoring, have enabled significant advancements in both academic and commercial neuromarketing research (15). EEG is frequently integrated with other modalities, such as fMRI, eye tracking, and various bio-signals (e.g., heart rate and skin conductance), to achieve a more comprehensive understanding of brain function and its interplay with physiological and behavioral states.\nIn recent studies, several modalities are commonly combined with EEG to enrich data analysis and interpretation, including functional near-infrared spectroscopy (fNIRS), eye tracking, electromyography (EMG), electrocardiography (ECG), electrooculography (EOG), and magnetic resonance imaging (MRI), providing a multidimensional perspective on brain function and its correlation with physiological and behavioral responses (16-20)."}, {"title": "1.4 Multimodal EEG Data & Disease", "content": "Multimodal EEG data refers to the integration of EEG with other physiological and neuroimaging modalities such as fNIRS, MRI, positron emission tomography (PET), and various biosignals, including ECG and respiratory signals. The primary advantage of using multimodal data lies in its enhanced ability to capture a more comprehensive picture of brain activity, thus enabling more accurate diagnosis and tailored treatment approaches for neurological disorders.\nThe integration of EEG with various physiological and neuroimaging modalities has been extensively explored in the study of multiple neurological and psychiatric disorders, including epilepsy, autism spectrum disorder (ASD), depression, anxiety disorders, and cognitive impairments. Each modality adds unique value, providing structural, functional, and behavioral insights that lead to more accurate assessments and understanding of these conditions.\nMultimodal EEG data has proven highly effective in advancing the detection and management of various neurological and psychiatric conditions. For epilepsy, combining EEG with fNIRS enhances seizure detection by capturing both electrical and hemodynamic responses during seizure events, thereby increasing diagnostic accuracy in clinical monitoring settings (21,22). Integrating EEG with EMG and accelerometer data further improves the detection of tonic-clonic seizures, enabling robust, long-term"}, {"title": "1.5 Terminology", "content": "This review incorporates various terminologies and acronyms spanning machine learning models, human health data, and disease references. For machine learning and deep learning models, terms like SVM, KNN, CNNs, and LSTM are used to represent specific algorithms. Acronyms such as ECG, EEG, EMG, and EOG stand for Electrocardiography, Electroencephalography, Electromyography, and Electrooculography, respectively. Additionally, disease-related terms are included, such as AD for Alzheimer's disease and ASD for Autism Spectrum Disorder. A comprehensive list of these and other terms can be found in Appendix A."}, {"title": "1.6 Objective", "content": "This systematic review explores the state-of-the-art methods in ML and DL for processing EEG data in combination with other modalities. It examines many recent studies, offering a comprehensive overview for researchers experienced in traditional EEG data fusion or multimodal processing interested in leveraging ML and DL techniques. Additionally, the review aims to guide clinical researchers in applying ML and DL to multidimensional EEG data for specific clinical applications. The review provides detailed methodological insights into the various components of an ML-EEG pipeline, assisting readers in implementing these techniques in their work. Beyond highlighting trends and notable approaches, the review concludes with several recommendations to promote reproducible and efficient research in the field."}, {"title": "1.7 Organization", "content": "The review is structured as follows: Section 1 offers a brief introduction to essential concepts in EEG, ML, DL, and multimodal data, along with the objectives of the review. Section 2 explains the methodology of the systematic review, including the selection, assessment, and analysis of studies. Section 3 highlights the key analysis result of the selected studies, identifying key approaches in applying ML/DL in multimodal EEG data. Section 4 addresses critical issues and challenges in DL-EEG, providing recommendations for future research. Section 5 concludes with suggestions for future ML/DL multimodal EEG research directions."}, {"title": "Paper Review Method", "content": "In our systematic review, we employed a structured approach to identify and analyse relevant literature on machine learning methods for multimodal EEG data in clinical applications. Our search strategy followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines to ensure a comprehensive and unbiased review process (6).\nWe conducted our search across three major databases: PubMed, Web of Science, and Google Scholar. These databases were chosen due to their extensive scientific and clinical research papers repository.\nFor PubMed, we used a comprehensive search query incorporating key terms related to EEG, BCI, and machine learning models:\n(EEG OR Electroencephalogra* OR Brain-Computer Interface OR Brain Computer Interface OR BCI) AND (Resting State OR Resting-State OR Eye* OR Motor Imagery OR Neck Movement) AND (Deep Learning OR Machine Learning OR Transformer OR Multi* Kernel Learning OR Ensemble Learning OR CNN OR RNN OR ANN OR Neural Network OR SVM OR Clustering OR Transfer Learning) AND (Multidimensional Data OR multimodal data OR multimodal data fusion OR multimodal learning OR data integration OR heterogeneous data integration OR heterogeneous data fusion)\nFilters: Full text, from 2012 - 2024.\nA similar structure was used for the Web of Science database, with the addition of \"TS=\" before each search term group to accommodate its search syntax."}, {"title": "2.2 Filter methods", "content": "Our filtering process was designed to systematically narrow down the vast number of search results to a manageable and relevant subset of papers. This multi-layered filtration process involved three main stages.\nInitially, each review group member individually screened the retrieved papers' titles using our predefined search criteria. Papers whose titles clearly indicated relevance to our core topics, research areas, models, and data management techniques were included in this initial selection. This step ensured a diverse yet focused pool of initial results. The individually selected titles were then consolidated, and common selections were identified for further review.\nThe second stage involved a detailed review of the abstracts of the selected titles. This allowed us to evaluate the depth of content and its alignment with our research objectives. Abstracts providing insights into the study's methodology, objectives, results, and conclusions were considered for further review. Group discussions followed individual assessments to reach a consensus on the papers advancing to the final review stage.\nThe final stage involved an in-depth examination of the full texts of the shortlisted papers. This comprehensive review ensured that the methodologies, findings, and discussions were directly relevant to our research aims and provided valuable insights or models applicable to our study. Each paper was reviewed individually and then discussed in group sessions to finalize the selection of papers included in our systematic review. After this thorough paper review and filtering process, we decided to keep 16 papers covering clinical populations as our study's focus. Additionally, we retained 47 papers that only included healthy control EEG data for reference. In the later sections of this paper, we will analyse these 16 clinically related papers, creating summary tables that detail the preprocessing methods, data fusion methods, and machine learning applications used. This structured approach ensures that our review is both comprehensive and focused, enabling us to draw meaningful insights from the existing literature."}, {"title": "Result", "content": "bl,G, and answers to an internet gaming disorder questionairre (30). Lastly, the detction of mild depression was improved by combining synchronized EEG and eye movement recordings during free viewing of neutral and negative faces through a multimodal deep learning model with fusion at the feature level (31)."}, {"title": "3.1 Which EEG multidimensional diseases/tasks have been explored with machine learning", "content": ""}, {"title": "3.1.1 Neuropsychiatric Disorders", "content": "Multimodal learning has been applied to different neuropyshciatric disorders. For instance, a virtual reality biofeedback exposure therapy system for public speaking anxiety, recognition of mild depression, and prediciton of internet gaming disorder have all benefited from mutlimodal learning. Heart rate variability and EEG data recorded during virtual reality simulated public speaking enhanced the accuracy of detecting high arousal states (18). Predictions of internet gaming disorder were improved by training a multi-kernel SVM that combines, positron emission topography,"}, {"title": "3.1.2 Neurological Disorders", "content": "Mild cognitive impairement (MCI), Alzheimer's disease, epilepsy, disorders of conciousness, and amyotrophic lateral sclerosis (ALS) have also benefited from mutlimodal learning. Early detection of MCI was enhanced by incoorperating multimodal features including EEG, handwriting (32), speech and parameters derived during a virtual reality-based scene description task (25). Interestingly in Alzheimer's disease detection, combining EEG with cognitive questionairres does not improve the classificaiton accuracy of a multimodal model"}, {"title": "3.1.3 Neurodevelopment Disorders", "content": "Multimodal machine learning models have been trained to accurately diagnose children with Autism Spectrum Disorder (ASD). Liao et al., developed a high performing model for diagnosing ASD while also identifying robust ASD markers for healthcare professionals. They used features extracted from EEG, eye-tracking, and facial expressions recorded during presentation of social and non-social videos (17). Other work has fused EEG and eye tracking data to achieve an overall accuracy of 87.5% for early detectyion of ASD (24)."}, {"title": "3.1.4 Sleep stage classification", "content": "CNNs have been used for multimodal classification of sleepstages. Abdollahapour et al., combined EEG and EOG along with transfer learning across two different datasets (Sleep-EDF and Sleep-EDFx) to classify different sleep stages. Sleep casetes and sleep telemetry data was used from both datasets (28). Ellis et al., combined EEG, EOG, and chin EMG data to classify different sleep stages through sleep telemetry data from approximately 9 hours of sleep. The authors used a novel \u201cablation\" approach to identify which modality was most important to classification accuracy. Interestingly, chin EMG had the least impact on model performance, whereas EEG was important for most stages and EOG was important for rapid eye movement and non-rapid eye movement stages (27)."}, {"title": "3.2 Data fusion methods for multimodal EEG data", "content": "Multimodal Data fusion methods are commonly used to combine information from different modalities or signals so that a uniform input format can be built for the machine learning models. The fusion methods can also be used to obtain the more useful information while excluding the less important information for the model performance. Reducing information redundancy can potentially prevent the machine learning model from overfitting. In general, the data fusion methods overcome the limitations of individual modalities by incorporating complementary information from other modalities (29). There are three main levels of data fusion methods, including signal level, feature level, and decision level."}, {"title": "3.2.1 Signal Level", "content": "The signal-level data fusion methods combine the data from different signal sources, which are the modalities of the studies after preprocessing the multi-modal data. The fusion could optionally include the application of a modality selection method to choose among data of multiple modalities based on their importance on the machine learning model's performance. The fused data will be used for further feature extraction or directly as inputs to the machine learning model. For instance, in the study of Ellis et al. (27), preprocessed EEG, EOG, and EMG data have been fused together as input to a deep learning model with 1-D CNN architecture. An ablation approach has been applied in the study, which replaces the signal data of each of the 3 modalities that have been mentioned with a sinusoidal and Gaussian noise that can be commonly found in electrophysiology data to determine the importance of each modality on the performance of the CNN model.\nSignal-level data fusion has only been employed in 2 (out of 15) clinical studies that have been full-text reviewed. It is because most of the clinical tasks needed to use the feature vectors, which are acquired through feature extraction and feature-level fusion methods, as inputs to the machine learning models, and the effects of modality selection methods on the signal level can be achieved by excluding all the features that are extracted from signal data of the corresponding modality during feature-level data fusion."}, {"title": "3.2.2 Feature Level", "content": "The feature-level data fusion refers to the process of combining the features extracted from different modalities, such as EEG signals and EOG signals. It is performed to create a more informative feature set. The combined features will be the input to the machine learning models.\nThere are various feature-level data fusion methods depending on the types of features and the machine learning models which the features' data will be fed into. Most conventional machine learning models (e.g., Regression, K- Nearest Neighbour, SVM, etc.) and deep learning models, such as the regular fully connected artificial neural network,"}, {"title": "3.2.3 Decision Level", "content": "The decision-level data fusion takes place when the training data is split into different sample and/or feature subsets, where each subset will be used as a training set of a separate machine learning model. Each trained model will generate a separate prediction result, then the decision-level data fusion method will be employed to merge the models' results (e.g., by a weight voting strategy) to generate the final result. Because decision-level data fusion is closely related to the concept of ensemble learning in the field of machine/deep learning, specific examples will be discussed in the next section (30,32). Decision-level data fusion has been applied in 5 (out of 15) clinical studies that have been full-text reviewed."}, {"title": "3.3 Machine Learning Models Application", "content": "This section provides an overview of the trends and advancements in machine learning models applied to multimodal EEG data in clinical studies. By examining the methods and outcomes from various research papers, we aim to highlight the most prevalent models, their applications, the evolution of techniques, and the performance metrics that underline the efficacy of these models in clinical contexts."}, {"title": "3.3.1 Commonly used machine learning models", "content": "The clinical application of machine learning models to multimodal EEG data has seen a variety of approaches, reflecting the diversity of tasks and datasets involved. Among the most popular models are SVM, Decision Trees (DT), K- Nearest Neighbors (KNN), CNNs, and RNNs, particularly LSTM networks.\nSVMs are widely utilized across various clinical fields, including study of ASD detection, disorders of consciousness (DoC), mild depression, anxiety, and sleep stages classification. They have proven effective in integrating multimodal inputs such as EEG, fNIRS, PET scans, and speech data. Advanced variants like Radial basis function kernel and Multiple-Kernel SVMs further enhance their applicability in complex tasks such as mild cognitive impairment (MCI) screening and Internet Gaming Disorder (IGD) prediction (17,18,23\u201326,28\u201330,32,35).\nDT are widely employed in clinical studies due to their simplicity, interpretability, and effectiveness. They are used in tasks such as anxiety detection, MCI screening, and sleep stage classification by integrating multimodal inputs like EEG, heart rate variability, and speech data. Random Forests (RF), an ensemble method based on Decision Trees, extend these capabilities by improving robustness and accuracy. RF has been applied in ASD detection, mild depression recognition, and predicting functional outcomes in disorders of consciousness, effectively handling complex multimodal datasets such as fMRI, eye movement, and handwriting dynamics (17,18,25,26,28,32,35).\nCNNs are extensively utilized in clinical studies for their ability to extract hierarchical spatial features from complex data. They are particularly effective in tasks such as epileptic seizure detection, where EEG is combined with respiratory signals and ECG to enhance predictive accuracy (21). CNNs are also applied in explainable sleep stage classification, leveraging multimodal data such as EEG, EOG, and EMG to provide interpretable insights into classification outcomes (27). Additionally, CNNs are used in multimodal epilepsy analysis, integrating EEG with resting-state fMRI to capture both temporal and spatial features, showcasing their versatility in handling diverse multimodal datasets (33).\nKNN is a simple yet effective machine learning model applied in various clinical tasks involving multimodal EEG data. In Autism Spectrum Disorder (ASD) detection, KNN is utilized to classify children as either ASD or typically developing based on EEG and features like facial expressions, eye fixation, and eye-tracking data (17,24). For anxiety detection, KNN integrates EEG with heart rate variability (HRV) to analyze physiological arousal states (18). Additionally, in sleep stage classification, KNN processes EEG and horizontal EOG data, demonstrating its adaptability"}, {"title": "3.3.2 Diseases/Task specific application", "content": "The application of machine learning models to multimodal EEG data in clinical settings exhibits distinct trends based on the specific clinical tasks addressed. These trends highlight the versatility and adaptability of machine learning techniques in tackling a wide range of neurological and psychiatric conditions."}, {"title": "3.3.3 Machine Learning Model Architecture Design", "content": "The design of machine learning models for multimodal EEG data in clinical applications involves various architectures and data fusion techniques to optimize performance and accuracy. This section reviews the model architectures and the decision-level data fusion methods employed in the reviewed studies.\nThe architectural design of machine learning models applied to multimodal EEG data in clinical studies showcases a wide range of approaches tailored to specific tasks and data characteristics. CNNs are widely used in the analysis of multimodal EEG data due to their robust feature extraction capabilities. In epileptic seizure detection, CNNs demonstrate significant adaptability by leveraging both Conv1D and Conv2D architectures, tailored to the nature of the input data. Conv1D processes sequential one-dimensional signals, while Conv2D handles two-dimensional representations, such as combined EEG, ECG, and respiratory signals, capturing spatial relationships among them. These two approaches enable the detection of intricate patterns and improves classification outcomes by fusing diverse biosignals (21). Additionally, CNNs have been employed in automated sleep staging, where one-dimensional convolutional layers, max pooling, and dropout layers are combined to refine feature extraction and enhance classification accuracy. These architectures are often supported by optimization techniques like transfer learning, enabling the application of pre-trained models to multimodal datasets, which further enhances their performance (27,28). The versatility and hierarchical learning capabilities of CNNs make them particularly effective for tasks involving spatial and temporal data, solidifying their role in multimodal EEG analyses.\nRNNs, particularly LSTM networks, are widely utilized for their ability to model sequential and time-series data, making them well-suited for analyzing multimodal EEG data in epilepsy-related studies. In seizure detection and epilepsy monitoring tasks, LSTM networks have been employed to integrate EEG data with functional near-infrared spectroscopy (fNIRS), leveraging the temporal dependencies between these modalities to enhance prediction accuracy. These architectures incorporate hyperbolic tangent activation for LSTM units, logistic sigmoid for gates, and categorical cross- entropy loss functions optimized with Adam, effectively capturing both long-term and short-term dependencies in the data (22). In multimodal integration tasks, LSTM models are utilized to combine rs-fMRI and EEG data, enabling the identification of epileptogenic networks and seizure foci. This approach benefits from the distinct temporal and spatial features of each modality, with LSTM layers processing sequential data to predict seizure intervals. The combination of feature extraction from convolutional layers and multimodal integration through LSTM networks demonstrates the effectiveness of these architectures in managing complex multimodal datasets (33). Advanced hybrid models like ResNet-LSTM and ChronoNet extend the capabilities of LSTM by combining residual networks or other advanced features with temporal modeling. These architectures, benchmarked against cyclic transformers and multimodal input cyclic transformers in recent studies, exhibit strong performance in tonic-clonic seizure detection and other epilepsy-related applications. Their ability to balance feature extraction, temporal modeling, and multimodal integration makes RNN-LSTM models a powerful choice for handling EEG data in clinical contexts(23,36,37).\nSupport Vector Machines (SVMs) are extensively utilized across a wide range of clinical tasks involving EEG data, demonstrating their versatility and robustness. In autism spectrum disorder (ASD) detection, SVMs are applied to multimodal data such as facial expression and eye fixation features, effectively distinguishing children with ASD from typically developing individuals (17,24). For biofeedback- driven self-guided virtual reality exposure therapy, SVMs process EEG data alongside heart rate variability (HRV) to classify arousal states, showcasing their adaptability in combining physiological and behavioral features (18). SVMs also play a pivotal role in predicting outcomes and classifying neurological disorders. In mild cognitive impairment (MCI) studies, Radial Basis Function kernel SVMs (RBF-SVMs) are employed to integrate speech data and cognitive parameters, offering enhanced accuracy in detecting this precursor to Alzheimer's disease (25,32). Similarly, in the classification of patients with pathological conditions versus healthy controls, non-linear polynomial kernel SVMs leverage hybrid EEG and fNIRS data, utilizing nested cross-validation to optimize feature selection and performance (29). Linear SVMs are employed in studies predicting long-term functional outcomes in patients with disorders of consciousness, as well as in depression recognition tasks involving EEG and eye movement data (26,31). Advanced variants of SVMs, such as Multiple-Kernel Support Vector Machines (MK-SVMs),"}, {"title": "3.3.4 Decision-Level Data Fusion", "content": "Decision-level data fusion is a pivotal strategy in enhancing the performance of machine learning models by integrating the outputs of multiple classifiers or models to reach a final, consolidated prediction. This approach leverages the strengths of various models, thereby improving the overall accuracy and robustness of clinical applications.\nOne exemplary application of decision-level data fusion is observed in the detection of ASD, where a hybrid fusion framework is employed. This framework combines feature- level and decision-level fusion, utilizing a weighted Naive Bayes algorithm to integrate sub-decisions from different feature sets. Specifically, behavioral features such as facial expressions and eye fixation data are fused at the feature level, while physiological features from EEG are processed separately. The sub-decisions derived from these features are then combined at the decision level using weighted coefficients that reflect the relative importance of each attribute, significantly enhancing classification accuracy (17)\nIn the context of detecting MCI, decision-level data fusion is implemented through a weighted voting strategy. This study integrates features from EEG, speech data, and digitized cognitive parameters. By employing a weighted voting strategy, the contributions of different modalities are balanced, improving the detection accuracy of MCI. This approach ensures that the final decision benefits from the complementary strengths of each modality, thereby providing a more holistic assessment of cognitive impairment (25). Similarly, a dual fusion strategy is utilized in the diagnosis of MCI based on handwriting dynamics and qEEG. Here, features extracted from both handwriting and EEG data are fused at the decision level. The final classification results are obtained by combining predictions from multiple base classifiers through a voting mechanism, which aggregates individual classifier decisions to enhance overall accuracy (32).\nAnother notable example is in predicting IGD, where the MK-SVM method is employed. This technique integrates multimodal data by modifying the kernel function to combine individual kernel matrices from each modality into a single kernel matrix. The MK-SVM model then uses this combined kernel matrix to perform classification, effectively handling the complexities of multimodal data and improving prediction accuracy (30).\nFurthermore, in the integration of rs-fMRI and EEG data for seizure prediction, LSTM networks are used to capture temporal dependencies, and their outputs are combined with SVM classification results. This combination leverages the temporal analysis capabilities of LSTMs with the robust classification power of SVMs, resulting in enhanced prediction accuracy for seizure occurrences (33)."}, {"title": "3.3.5 Evolution of Techniques", "content": "The evolution of machine learning techniques for multimodal EEG data in clinical applications is evident from the progression of methodologies and models over the years. By analyzing the publication dates of the reviewed papers, we can trace how the field has developed and adapted to new challenges and technological advancements.\nIn the late 2010s, machine learning techniques for multimodal EEG data analysis began to integrate traditional classifiers and deep learning approaches. RNN, particularly LSTM networks, were employed to combine EEG with functional near-infrared spectroscopy (fNIRS), effectively capturing temporal dependencies for seizure detection. Meanwhile, traditional models like linear SVMs, gradient boosting decision trees, and batch-normalized neural networks demonstrated their effectiveness in tasks such as depression recognition by leveraging multimodal inputs like EEG and eye movement data. These studies laid the groundwork for integrating multimodal features with diverse machine learning frameworks (22,35).\nEntering the 2020s, advancements in neural network architectures, particularly CNNs, gained prominence for tasks like seizure detection and sleep stage classification. These models effectively captured spatial patterns within multimodal data, often integrating EEG with structural MRI, cognitive scores, and other physiological signals. Hybrid approaches also emerged, combining CNNs for feature extraction and LSTMs for temporal modeling, enabling accurate epilepsy predictions through multimodal inputs like rs-fMRI and EEG. Additionally, ensemble methods such as random forests and elastic net were utilized for integrating multimodal features in diagnosing conditions like Alzheimer's disease and amnestic mild cognitive impairment (aMCI), showcasing increasing sophistication in feature selection and model design (20,21,33).\nBy the early 2020s, the focus shifted toward explainability and robustness in machine learning applications. For example, explainable sleep staging models incorporated CNNs with dropout layers and class imbalance handling mechanisms to improve performance. SVMs with advanced kernels, such as non-linear polynomial kernels, were employed in ALS classification, leveraging EEG and fNIRS data through nested cross-validation and hybrid frameworks to enhance model accuracy and interpretability. These approaches emphasized the need for both precision and transparency in clinical applications (27,29).\nAs the mid-2020s approached, ensemble learning and kernel-based techniques became increasingly prevalent. Random forests, XGBoost, and multiple-kernel SVMs were utilized for diagnosing conditions like ASD, Internet Gaming Disorder (IGD), and mild cognitive impairment (MCI), effectively handling multimodal inputs such as EEG, PET scans, and eye-tracking data. These models demonstrated significant improvements in handling diverse data types and integrating information from multiple modalities, further advancing diagnostic capabilities (17,30).\nMore recently, advanced transformer architectures like the Cyclic Transformer (CT) and Multimodal Input Cyclic Transformer (MICT) have set new benchmarks in handling complex multimodal datasets. These transformers, capable of circular modality translation and cross-attention mechanisms, efficiently processed EEG, accelerometer, and electromyography data while addressing challenges like missing modalities. Compared to hybrid models like ResNet-LSTM and ChronoNet, transformers exhibited superior performance in capturing both temporal and spatial relationships within multimodal data, exemplifying the progression toward more flexible and powerful architectures (23,36,37).\nThe evolution of techniques over the years underscores a clear trajectory towards more sophisticated and integrated approaches. From basic linear models to advanced deep learning architectures, the progression highlights the continuous effort to improve machine learning models' accuracy, reliability, and interpretability for multimodal EEG data in clinical settings. This evolution demonstrates technological advancements and reflects a deeper understanding of the complexities involved in neurological and psychiatric disorders."}, {"title": "3.3.6 Performance Metrics", "content": "Performance metrics are crucial in evaluating the effectiveness of machine learning models in clinical applications. The highest accuracy reported in each reviewed paper indicates the best-performing model for specific clinical tasks. Additionally, the consistent observation across studies is that multimodal data integration invariably enhances model performance compared to unimodal data.\nIn the context of ASD detection, the integration of EEG with eye-tracking data and computational data achieved the highest accuracy. The SVM model, utilizing multimodal data, reached an impressive accuracy of 92%, significantly outperforming models that relied on single modalities (17,24). For anxiety detection in biofeedback-driven therapy, the RF model again demonstrated superior performance. Combining EEG with GSR, HR, and EMG data, the model achieved an accuracy of over 85%, highlighting the value of incorporating multiple physiological signals (18).\nIn epileptic seizure detection, the use of CNN integrating EEG, respiratory signals, and ECG data showed substantial improvements. The Conv2D model attained an accuracy of 64.96%, illustrating the enhanced detection capabilities provided by multimodal data (21). Similarly, in studies involving EEG and fNIRS data, LSTM achieved an accuracy of 98.3%, further validating the effectiveness of multimodal approaches (22).\nFor tonic-clonic seizure detection, CT combined with EEG, accelerometer (ACC), and EMG data reached a sensitivity of 100% and an F1 score of 60.9%, showcasing the potential of advanced models in handling complex multimodal data(23).\nIn sleep stage classification, transfer learning with CNNs yielded the highest accuracy of 94.34%, significantly outperforming models trained without transfer learning and those using unimodal data (28). This underscores the advantage of leveraging pre-trained models and multimodal integration in improving classification accuracy.\nThe detection of MCI also benefited from multimodal data integration. SVM combined with handwriting dynamics and qEEG data achieved the highest accuracy of 96.73%, demonstrating the significant boost in performance when multiple data sources are utilized (32).\nIn the context of predicting IGD outcomes, an MK-SVM incorporating PET, EEG, and clinical features achieved an accuracy of 84.6%. This result highlights the importance of integrating diverse data types to enhance model robustness and predictive power (30).\nFinally, explainable AI techniques applied to sleep stage classification using multimodal electrophysiology time-series data achieved notable precision and recall rates, further emphasizing the importance of multimodal data in improving model interpretability and performance (27).\nOverall, the highest accuracies reported in these studies underscore the superior performance of machine learning models that utilize multimodal data compared to those relying on unimodal data. The consistent trend across various clinical tasks demonstrates that integrating multiple physiological and behavioral signals provides a more comprehensive understanding of the underlying conditions, leading to more accurate and reliable clinical predictions."}, {"title": "Discussion", "content": "Choosing the appropriate machine learning model for EEG data analysis depends on the specific task, data characteristics, and desired outcomes. SVMs, particularly linear and RBF kernels, are ideal for classification tasks in high-dimensional data with small to medium-sized datasets due to their robustness against overfitting and efficiency in handling complex patterns. Decision Trees and their ensemble variants, such as Random Forests and XGBoost, are preferred when interpretability and handling of large datasets are crucial, offering insights into feature importance and enhancing model robustness through ensemble methods. CNNs are best suited for tasks that involve complex spatial hierarchies, such as detecting intricate patterns in EEG signals, making them highly effective for image and time-series data analysis. RNNs), including LSTMs, excel in analyzing sequential and time-series data due to their ability to capture long-term dependencies, making them suitable for tasks that involve temporal dynamics in EEG signals. Simpler models like KNN and MLP are effective for smaller datasets and adaptable to a variety of tasks, while advanced models like Transformers and specialized hybrid models are recommended for handling multimodal data and capturing complex relationships, providing tailored solutions for specific EEG analysis challenges."}, {"title": "Conclusion", "content": "Multimodal EEG data has been effectively applied to ML and DL techniques to a variety of clinical tasks. Applications include the diagnosis of neuropsychiatric disorders, neurological conditions (e.g., seizure detection), neurodevelopmental disorders (e.g., ASD), and sleep stage classification. Multimodal EEG data has demonstrated its ability to enhance the accuracy of ML and DL models across these tasks. Different types of data are typically fused with EEG data depending on the clinical task. For instance, eye- tracking data is often combined with EEG to diagnose neurodevelopmental disorders such as ASD, while fNIRS and ECG are commonly integrated with EEG for detecting neurological conditions like seizures. Various ML and DL models have been applied to these tasks, with models such as SVM and Decision Trees frequently used in clinical applications. In contrast, DL models, including CNNs, RNNs, and LSTM networks, have been employed for more complex tasks. However, the use of Transformers for multimodal EEG data in clinical datasets remains limited, with only one study (out of 16 reviewed) exploring their potential. We recommend future research focus on leveraging advanced DL techniques, such as Transformers, to further improve performance in clinical tasks. Additionally, incorporating more diverse data types, when feasible, may enhance model accuracy and expand the scope of clinical applications. This approach has the potential to address a broader range of clinical challenges and contribute to the advancement of multimodal EEG-based research."}]}