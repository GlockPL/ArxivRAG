{"title": "InTec: integrated things-edge computing\nA framework for distributing machine learning pipelines in edge\nAl systems", "authors": ["Habib Larian", "Faramarz Safi-Esfahani"], "abstract": "With the rapid expansion of the Internet of Things (IoT), sensors, smartphones, and\nwearables have become integral to daily life, powering smart applications in home automa-\ntion, healthcare, and intelligent transportation. However, these advancements face signifi-\ncant challenges due to latency and bandwidth constraints imposed by traditional cloud-based\nmachine learning (ML) frameworks. The need for innovative solutions is evident as cloud\ncomputing struggles with increased latency and network congestion. Previous attempts to\noffload parts of the ML pipeline to edge and cloud layers have yet to fully resolve these\nissues, often worsening system response times and network congestion due to the compu-\ntational limitations of edge devices. In response to these challenges, this study introduces\nthe InTec (Integrated Things-Edge Computing) framework, a groundbreaking innovation in\nIoT architecture. Unlike existing methods, InTec fully leverages the potential of a three-\ntier architecture by strategically distributing ML tasks across the Things, Edge, and Cloud\nlayers. This comprehensive approach enables real-time data processing at the point of data\ngeneration, significantly reducing latency, optimizing network traffic, and enhancing sys-\ntem reliability. InTec's effectiveness is validated through empirical evaluation using the\nMHEALTH dataset for human motion detection in smart homes, demonstrating notable\nimprovements in key metrics: an 81.56% reduction in response time, a 10.92% decrease in\nnetwork traffic, a 9.82% improvement in throughput, a 21.86% reduction in edge energy\nconsumption, and a 25.83% reduction in cloud energy consumption. These advancements\nestablish InTec as a new benchmark for scalable, responsive, and energy-efficient IoT appli-\ncations, demonstrating its potential to revolutionize how the ML pipeline is integrated into\nEdge-AI (EI) systems.", "sections": [{"title": "1 Introduction", "content": "In the digital transformation era, IoT drives innovation in home automation, healthcare, and\ntransportation, promising enhanced efficiency and safety. The value of IoT applications lies\nin their ability to perform reliably and instantly, especially in critical areas such as healthcare\nand Industry applications, where timely and accurate decisions are crucial. In healthcare, real-\ntime data processing enables prompt, life-saving interventions, while in industrial settings, edge\ncomputing supports autonomous decision-making and predictive maintenance [31]. However,\nsignificant challenges in data processing and system reliability arise due to the vast amounts"}, {"title": "2 Background", "content": "2.1 Deep learning and edge-AI integration in IoT systems\nVarious algorithms, including rule-based and data-driven approaches, have been developed to\naddress the complexity and heterogeneity of data generated by IoT devices. New IoT devices,\nlike smart cameras, generate feature-rich data that traditional algorithms struggle to handle\ndue to their complexity and volume, necessitating advanced analytical methods like deep learn-\ning [35]. With its multilayer abstraction, deep learning significantly impacts fields like speech\nand image recognition and object detection, making it useful for IoT environments where data\ncomplexity exceeds traditional processing capabilities [19]. The advent of EI leverages computa-\ntional power at the network's edge, facilitating local processing to reduce latency and conserve\nbandwidth [13, 7, 35, 27]. This local processing capability is further enhanced within the broader\nedge-cloud continuum by frameworks like [33], where computational tasks can be dynamically\ndistributed across the edge and cloud environments. This combination of EI and the edge-cloud\narchitecture transforms IoT devices into intelligent systems capable of immediate, informed ac-\ntions, representing a significant shift in data processing and utilization. Moving computational\ntasks closer to data generation and simultaneously enabling cloudbased analytics when neces-"}, {"title": "2.2 Machine Learning Pipeline", "content": "Integrating ML into operational environments, particularly within the IoT ecosystem, necessi-\ntates a structured approach to managing the lifecycle of ML models, leading to the development\nof ML pipelines. These pipelines automate and streamline training, deployment, and utilization\nprocesses, ensuring efficient execution in a sequential and automated workflow [14]. However,\nas depicted in Fig. 1, this workflow is iterative, ensuring stages are repeated to refine and\nemploy the most accurate trained model. Utilizing ML pipelines in IoT settings offers benefits\nsuch as reusability, containerization, and parallel processing. These features enhance scalability\nand efficiency: reusability allows pipelines to be applied across different projects with minimal\nadjustments, containerization enables deployment on various platforms, and parallel process-\ning distributes tasks across multiple resources, reducing analysis time. ML pipelines provide a\nstructured framework for managing AI models, streamlining development and deployment, and\nensuring efficient updates and maintenance, fostering continuous improvement and innovation\nin IoT systems [10]. Below is an elucidation of these stages:\n1. Data Validation: The foundation of any robust ML pipeline begins with data validation.\nThis critical first step ensures the integrity and quality of data before it enters the training phase.\nBy examining the statistical properties of the dataset, such as range, count, and distribution\nof classes, data validation aims to preemptively identify and rectify discrepancies that could\ncompromise the learning model's performance.\n2. Data Preprocessing: Following validation, data undergoes preprocessing to transform it\ninto a format suitable for model training. This stage addresses the heterogeneity and complexity\nof IoT data, involving tasks such as normalization, feature engineering, and dimensionality\nreduction. Given IoT data's diverse and often unstructured nature, preprocessing is vital for\nteasing out relevant features that can significantly impact the model's accuracy and efficiency.\n3. Model Training: With the data prepared, the pipeline progresses to model training,\nwhere algorithms learn from the data. This iterative process adjusts the model's parameters\nto minimize error and enhance its ability to make accurate predictions. In the context of IoT,\nwhere models often need to operate under resource constraints, the training phase also involves\noptimizing the model for performance and efficiency.\n4. Model Analysis: After training, the model undergoes a thorough evaluation to assess its\nperformance. Metrics such as accuracy, precision, recall, and AUC provide insights into the\nmodel's effectiveness in addressing the problem. This stage is crucial for identifying areas for\nimprovement and ensuring that the model meets the requisite standards for deployment.\n5. Model Deployment: The culmination of the ML pipeline is deploying the model for\ninference. In IoT systems, this often means integrating the model into edge devices or cloud\nplatforms, which can process real-time data and provide actionable insights. This stage requires\ncareful consideration of the deployment environment to optimize for latency, energy consump-\ntion, and scalability."}, {"title": "3 Related Works", "content": "3.1 Evolution and trends in edge-AI research\nThe domain of EI has experienced rapid expansion, which has been distinguished by a series of\nmethodological and applied breakthroughs from 2018 to the present. A chronological overview,\nillustrated in Fig. 2, serves to articulate this progression:\n2018-Foundational Integrations of Deep Learning and IoT: The year marked the in-\nception of integrating deep learning with IoT, as seminal works by Li et al. [21] and Zhao et\nal. [36] established the initial frameworks for embedding ML models within IoT infrastructures,\nlaying the groundwork for subsequent EI innovations.\n2019: The Advent of Deep Learning in Edge Computing: In this phase, the focus\ntransitioned to deploying deep learning models directly onto the Edge layer, with studies by\nManogaran et al. [24] and Azar et al. [3] exploring architectural strategies to enhance compu-\ntational offloading.\n2020: Data Inference Convergence with the Edge: This year witnessed an emphasis on\ndata analytics at the edge, highlighted by contributions from Hu et al. [15], and Li et al. [20],\nadvanced the practical implementation of ML models for real-time data inference within edge\nenvironments.\n2021: Empowering Things with EI: Researchers such as Kristiani et al. [17], Ghosh et\nal. [12], and Raj et al. [26] shifted focus towards empowering IoT devices ('Things') with edge\nintelligence, proposing architectures that leveraged the computational proximity of Edge layers.\n2022: Setting up ML Pipelines on EI: The setup of ML pipelines on EI became a focal\npoint, with studies like Arunachalam et al. [2] exploring the benefits of distributed ML processes\nto optimize the entire data lifecycle on edge devices.\n2023: Utilizing ML Pipelines for EI Challenges: Investigations by Achar et al. [1] and\nWazwaz et al. [34] probed deeper into the application of ML pipelines to solve complex EI\nproblems, underscoring the transition from theory to application-centric solutions.\n2024: Empowering Edge-AI by Inference at Things: As of 2024, our research takes a\nstep further by advancing inference at the 'Things' level, optimizing latency and reliability for\nEI applications, and pushing the evolution of EI research beyond prior works for real-world\nimplementation."}, {"title": "3.2 Overview of edge-AI architectural methodologies", "content": "Various methods have been proposed for deploying ML models on the Edge layer, each tailored\nto specific contexts. In Fig. 3, each of these methods is generally categorized into sub-methods.\nThis figure also represents the research domain addressed explicitly in this study. The research\nbegins by searching in the field of EI studies. This field consists of two broad sub-areas known\nas training and inference, and our research focuses on designing hybrid architectures for EI\nsystems by deploying an ML pipeline across the Things, Edge, and Cloud layers, optimizing\nlatency and network traffic by leveraging the processing capabilities of these layers. Under-\nstanding EI architectural methodologies is crucial for appreciating the context and effectiveness\nof ML pipeline deployment in El systems. Tables 1 and 2 provide a concise overview of critical\nstudies in the El domain from 2018 to 2024, highlighting architectural choices, ML models, use\ncases, datasets, methodologies, techniques, and performance metrics, including details on the\nInTec model. These comparisons cover cloud-edge to device-edge integrations using public and\nprivate datasets, primarily in healthcare and industry. Techniques such as model partition-\ning, compression, and optimization are detailed alongside parameters like accuracy, efficiency,\nlatency, and energy consumption, showcasing each approach's operational effectiveness and re-\nsource impact. The purpose of Table 2 is to present various performance metrics reported by\neach work, even if they differ across studies. For instance, some works, like [14], reported Recall"}, {"title": "4 Proposed Framework", "content": "4.1 Pipeline Design for Proposed Framework\nThe pipeline for our proposed framework is meticulously structured to cover all essential phases:\ndata validation, preprocessing, model training, analysis, and deployment. As visualized in Fig.\n4, we have assigned modules to each stage, carefully distributing these across the Cloud, Edge,\nand Things layers to leverage their unique computational capabilities.\nAt the foundation, the Things layer deploys the ML model, allowing the Inference module to\nmake data-driven decisions close to the data source, thus enhancing responsiveness and reducing\nlatency. The edge layer initially processes data using the Outlier Detection and Data Reduction\nmodules, which validate and preprocess the data. This step significantly reduces the volume\nof data transmitted to the Cloud, conserving bandwidth and accelerating the workflow. In the\nCloud layer, the Model Trainer and Model Validator modules leverage extensive computational\nresources for training and analyzing the ML model, ensuring rigorous evaluation of performance\nand accuracy.\nThis hierarchical, layered approach to pipeline design ensures that each phase of the ML process\nis optimally placed within the architecture, from the immediate handling of data at its point\nof generation to the sophisticated analysis and model refinement processes in the cloud. This\nstrategic distribution not only maximizes the efficiency and efficacy of the ML pipeline but also"}, {"title": "4.2 Inference at Things, in a Things-Edge-Cloud Architecture (InTec) Frame-\nwork", "content": "This research introduces an advanced InTec framework designed to optimize the ML pipeline\nwithin a comprehensive cloud-edge-things ecosystem, as illustrated in Fig. 5. The architecture\naims to streamline data processing, improve system response times, and minimize the amount\nof data transmitted to the cloud by strategically distributing tasks across three principal layers:\ncloud, edge, and Things. Each layer's role and operations are detailed below.\nCloud Layer: The cloud layer serves as the central hub for the training and analysis phases of the\nML pipeline. It receives data from edge servers for periodic model training based on predefined\npolicies, followed by result evaluation. The trained models are compressed to facilitate updates\nback to the edge layer.\nEdge Layer: As the intermediary, the edge layer focuses on refining data collected from IoT\ndevices. It employs data preprocessing and validation to structure data for cloud-based training.\nIt includes (1) Data Dimensionality Reduction: Streamlines data, preserving user privacy and\nreducing cloud server load by minimizing data size. (2) Outlier Detection: Filters anomalies to\nmaintain model integrity, ensuring only valid data influences model training. Additionally, this\nlayer is responsible for disseminating the updated ML models to the sensors.\nThings Layer: At the forefront of this framework, the Things layer employs IoT devices\nto execute data inference, utilizing pre-trained ML models for immediate analysis. This setup\npresumes IoT devices have adequate processing capabilities to run these optimized models. Data\nanalysis is conducted directly on the devices, which then relay pertinent information to the edge\nserver for subsequent actions. The operational blueprint for each IoT device encompasses raw\ndata acquisition, indevice analysis, and communication with the edge server, demonstrating a\nself-sufficient approach to initial data processing. Through this layered architecture, the InTec\nframework achieves a balanced distribution of ML operations, from initial data capture to in-\ndepth analysis, harnessing the unique strengths of cloud, edge, and Things layers to enhance\noverall system efficiency and responsiveness."}, {"title": "4.3 \u0391\u0399 Model Trainer Module in the Cloud", "content": "The AI Model Trainer Module, as outlined in Fig. 6, is the core of the ML operations within the\ncloud layer of our InTec framework, performing critical functions, including data preparation,\nmodel training, validation, and compression. The workflow starts by retrieving data from the"}, {"title": "4.4 Updating ML Models on IoT Devices", "content": "In the InTec framework, a tiered module system manages updating ML models on IoT devices.\nAt the cloud layer, the Firmware Updater module creates firmware updates using the latest\nmodels by comparing device version data to determine if updates are needed. It then produces"}, {"title": "4.5 Analysis Core in Edge", "content": "The edge Layer's Analysis Core in the InTec framework, illustrated in Fig. 7, is pivotal for\nrefining raw data from IoT sensors. It starts with Outlier Detection, using an Outlier Model\nto filter out anomalies and ensure data integrity. The Data Reduction module then processes\nthe validated data, which uses a Reduction Model to condense the dataset, retaining essential\nfeatures. The Database (DB Model) is central to these operations, which stores and organizes\ndata, facilitating its structured progression through the pipeline. Environment Variables dy-\nnamically adjust the detection and reduction modules in real-time, enhancing data fidelity and\ncompactness for efficient El analytics. We will further explain the Outlier Detection and Data\nReduction modules.\n1. Outlier Detection Module: In the InTec framework's Edge layer, Outlier Detection is critical\nfor ensuring data quality. As shown in Fig. 8 and detailed in Pseudocode 3, raw data batches\nare converted to a structured data frame and assessed using a pre-trained Outlier Model to filter\nanomalies. This process employs a sliding window to evaluate data validity against a drop-rate\nthreshold. Valid data is calculated by dividing the count of valid data points by the sliding"}, {"title": "4.6 Service Core in Edge", "content": "The Service Core is a pivotal component of the InTec framework, designed to facilitate user inter-\naction through a REST-based architecture. It handles user requests and delivers services while\nadhering to RESTful API standards for standardized communication. The Web Framework is\nthe operational backbone, efficiently managing requests and responses. The DB Engine Module\norchestrates interactions between the Web Framework and the database, ensuring scalable and\nresponsive service by managing multiple concurrent connections. The Services Module houses"}, {"title": "4.7 Inference Module in Things", "content": "The Inference module, within the Things layer of the InTec architecture, is crucial for real-time\ndata analysis, as depicted in Pseudocode 5. It executes the ML model on sensor-acquired data,\nextracting actionable insights. The workflow includes three sub-modules:\n1. Preprocess Sub-module: This submodule initiates the workflow by preparing the raw sensor\ndata for analysis. It converts the data into a standardized format, normalizes it to ensure\nconsistency, and categorizes it for better identification and subsequent processing.\n2. Feed Sub-module: Serves as the analytical engine, where the prepared data is inputted into\nthe ML model. It processes the data and outputs the inference results, and the interpreted data\npoints are ready for use in decision-making or further action.\n3. Data Tagging Sub-module: This submodule functions post-analysis to attach relevant meta-\ndata or labels to the processed data, preparing it for communication. It also formats the\nanalyzed data, ensuring it is ready for efficient transmission to other system components or"}, {"title": "4.8 General Dynamics of User-System Interaction within the InTec Frame-\nwork", "content": "Figure 10 detail a generalized sequence of interactions between a user and the InTec framework,\ndelineating the flow and processing of data. The sequence involves the following steps:\n1. Data Capture: The process begins with the sensors collecting data from the environment.\nThis data could encompass a wide array of information depending on the sensor type and the\nmonitoring needs.\n2. Data Processing Request: After data collection, the Edge Server receives a request to process\nthis data. The request may come directly from the user or be part of a predefined workflow\nwithin the system.\n3. Edge Processing: Upon receiving the data, the Edge Server undertakes the initial processing\nsteps. These may include validation, preliminary analysis, and preparation for any subsequent\ndeep processing that may be required.\n4. User Query: In parallel, the user may issue a request for specific information or actions to\nbe performed by the system. This request is communicated to the Edge Server.\n5. Edge Analysis and Response: The Edge Server processes the user's request, potentially\nutilizing the pre-processed data. It then formulates a response based on the request's parameters\nand the results of any analysis conducted.\n6. Information Delivery: Finally, the Edge Server sends the response back to the user. This"}, {"title": "4.9 Optimizing Data Integrity in Sensor-Edge-Cloud Interaction", "content": "As depicted in the sequence diagram Fig. 11, this process ensures a seamless flow of data from\nthe collection at the sensor level to utilization in the cloud while maintaining data integrity and\nprivacy. The edge layer serves as a crucial intermediary, filtering and condensing data before it\nreaches the cloud, optimizing network resources, and safeguarding sensitive information. Here's\na description of this interaction:\n1. Data Collection: Sensors actively gather data and initiate the interaction by publishing\ntheir findings to a designated topic. This information is typically in the form of a message that\nencapsulates the raw sensor data.\n2. Initial Processing at Edge: Upon receiving the data, the Edge Server subscribes to the same\ntopic to capture the sensor message. It employs an outlier detection algorithm on the received\nsensor data to determine its validity. If the data is flagged as an outlier and exceeds a pre-set\ndrop rate, it is discarded to maintain data quality."}, {"title": "4.10 Case Study", "content": "The previous sections provided a detailed explanation of the proposed framework. Now, in this\nsection, a practical example is presented to demonstrate the performance of this framework in\nsmart edge environments.\nA pre-trained and compressed model is deployed in the Objects layer and on each sensor. This\nmodel combines CNN and LSTM, trained on the cloud layer, and converted into the TFLight\nformat. The data preprocessing stages, outlier detection, and dimensionality reduction are also\ndeployed on the edge server.\nThis use case scenario consists of an edge server and ten IoT sensors, simulated on Raspberry\nPi devices. These ten sensors are implemented as separate containers on Docker. Each of these\nsensors analyzes its data with a window size of 25 and then sends it to the edge server at 50 Hz\n(matching the dataset's sampling rate) with a time interval of 0.5 s (0.02 \u00d725)."}, {"title": "5 Evaluation Methodology", "content": "To comprehensively evaluate the InTec framework, we tested its performance and applicability\nin a semi-real world environment through a meticulously crafted emulation scenario. This ap-\nproach closely replicates real-world conditions, providing a robust platform to understand the\nframework's operation beyond theoretical settings. Through this emulation, we conducted a\nseries of experiments to test the framework's capabilities and compare its results with bench-\nmarks from related studies [11, 1, 34], highlighting its impact on essential research variables\nand offering insights into its efficacy and areas for refinement. The source code for the InTec\nframework implementation is publicly available at [18]."}, {"title": "5.1 Emulation Scenario: HAR Problem", "content": "To faithfully replicate the baseline study conditions cited in [11], this research adopts human\nmotion detection as the focal emulation scenario. In alignment with the baseline study's meth-\nods, this emulation is executed on virtual machines using the MHEALTH dataset [4], the same\ndataset employed by the baseline study for both training and validating the efficacy of the\nlearning model. The MHEALTH dataset serves as the foundation for HAR research, compris-\ning motion data from 10 distinct subjects captured by seven sensors per individual at a sampling\nrate of 50 Hz. Each recorded data instance encompasses 21 distinct features-derived from the\n3-axes measurements of the seven sensors and is classified into one of 12 movement classes,\nsuch as walking, standing, and running, offering a comprehensive range of human activities for\nanalysis [28]. Table 3 illustrates the dataset's structure, while Table 4 delineates the various\nmovement categories recognized within it."}, {"title": "5.2 Emulation Setup and Implementation", "content": "The emulation of the InTec framework involved a detailed setup across IoT devices, edge, and\ncloud infrastructures using specialized tools and technologies. Raspberry Pi devices emulated\nsensors and used TensorFlow Lite Runtime and Docker containers for scalability. Virtual ma-\nchines running Ubuntu OS were employed in both edge and cloud infrastructures. TensorFlow\nmanaged ML tasks in the cloud, while Scikit-Learn handled preprocessing functions at the edge."}, {"title": "5.3 Analyzing Training Results of the CNN-LSTM Model for InTec Frame-\nwork", "content": "Our research employs a combined CNN and LSTM model tailored for time-series data analysis\nwithin the InTec framework. The architecture depicted in Fig. 14 addresses the nuanced"}, {"title": "5.4 Evaluated Frameworks", "content": "In our study, the InTec framework was rigorously evaluated against existing frameworks high-\nlighted in references [11, 1, 34]. This comparative analysis focused on how each framework imple-"}, {"title": "5.5 Parameters and variables", "content": "5.5.1 Independent Variables\n(1) Sensor Quantity: This reflects the system's total number of active sensors. A larger quantity\nimplies more frequent data communication and higher network utilization. (2) User Count:\nDenotes the volume of concurrent users interacting with the system. Increasing user numbers\ncan intensify the demand for system resources due to increased incoming queries. (3) Data\nBatching Window: The window size determines the dataset segment a sensor transmits in a\nsingle operation. A larger window reduces the frequency of transmissions but increases the data\nvolume per batch. 4) Feature Reduction Ratio: This specifies the degree to which original sensor\ndata dimensions are compressed before transmission. While a higher reduction can decrease data\nsize and ease network load, it may inversely affect the precision of the data analysis.\n5.5.2 Dependent Variables\n1. Network Traffic: Quantifies the volume of data traffic flowing through the network, typically\npresented in megabytes, highlighting system efficiency in data handling.\nLatency: Measured in milliseconds, represents the average delay between a userinitiated request\nand the system's response. This metric is crucial for evaluating the system's responsiveness,\nindicating how quickly it can process and provide feedback or results to the user's queries.\nNetwork Throughput: Defines the capacity of the network to handle data over a specific period,\nwith typical measurements in megabits per second, indicating the robustness of data transmis-\nsion capabilities.\nPower Consumption: Monitors the power draw of the deployed applications across devices, a\ncritical factor for resource optimization and sustainability, measured in milliwatts."}, {"title": "6 Experimental Design", "content": "The experimental framework aims to systematically address five critical questions, as detailed\nin Table 8, each corresponding to distinct aspects of the InTec framework's performance under\nvarying conditions. These queries delve into the influence of data reduction rates, window sizes,\nthe number of sensors, user requests, and the overall framework performance under high-load\nscenarios.\nEach experimental scenario is crafted precisely, focusing on specific variables such as network\ntraffic, latency, throughput, and power consumption to comprehensively evaluate the frame-\nwork's robustness and efficiency. The experimental design utilizes two-dimensional reduction\ntechniques, PCA (Principal Component Analysis) and AE (AutoEncoder), across all scenarios\nto elucidate their effects on the performance metrics.\nSelect variables were held constant to guarantee consistency and equitable comparison in all\nexperimental evaluations. The architecture employs a CNNLSTM model for ML tasks [26] and\nutilizes the Isolation Forest algorithm for outlier detection. Data for these experiments were\nsourced from the MHEALTH dataset [11]. Sensors operated at a uniform sampling rate of 50\nHz [4], and the threshold for outlier removal was below 80%. Additionally, each experiment was\nconducted three times to ensure reliability, with outcomes averaged to mitigate anomalies and\nprovide a robust performance analysis under controlled variables.\nWe've incorporated a strategic approach in our experiments to address the latency inherent in\ncloud-based user response frameworks and maximize cloud processor load. Each experiment\nutilized the entire MHEALTH dataset to train the ML model, ensuring that the cloud pro-\ncessor was under maximum operational stress. Additionally, to account for typical network\ndelays experienced in real-world cloud interactions, a constant latency value representing the"}, {"title": "6.1 Experimental Scenarios", "content": "Experimental scenarios aim to cover a comprehensive range of operational conditions, provid-\ning insights into the scalability and reliability of the InTec framework. These scenarios are\nmentioned below.\nExperiment 1: Explores the optimal data reduction rate to enhance network efficiency and\nminimize latency.\nExperiment 2: Investigates the impact of varying data window sizes on network traffic and\nresponse times.\nExperiment 3: Examines the effects of the number of active sensors on network traffic and\nlatency.\nExperiment 4: Assesses how the frequency of user requests influences network performance\nand service delivery.\nExperiment 5: Assesses the performance of various frameworks, including InTec, under heavy\nnetwork traffic and user demand."}, {"title": "6.2 Experiment 1: Evaluating the Effects of Data Reduction Rates on Net-\nwork Efficiency and Latency Across Frameworks", "content": "This experiment focuses on the influence of varying data reduction rates, employing PCA and\nAE algorithms, on key network performance indicators. Specifically, it aims to uncover how data\nfeature reduction by 24% and 66% impacts metrics such as latency, network traffic, throughput,\nand power consumption across the different layers of InTec architecture.\nData reduction rates are expected to impact dependent variables because they directly influ-\nence the volume and complexity of data that needs to be transmitted, stored, and processed.\nReduction in data dimensionality can lead to less network congestion (reduced network traffic),\nimproved speed in data processing (lower latency), and potentially increased throughput as\nless data needs to be handled at any given time. Furthermore, fewer data features to analyze\ncan result in lower power consumption since fewer computational resources are required. The\nexperiment aims to quantify these effects and establish the most beneficial reduction rates for\nthe InTec framework's efficiency and effectiveness.\nWith fixed variables like the data window size, sensor count, and user interactions, the experi-\nment sought to observe the effects of two distinct data reduction rates. The setup mirrored the\nstandardized environment, with 30 sensors transmitting data at predetermined intervals. The\nedge and cloud servers were emulated on virtual platforms, with all devices interconnected via\na TCP/IP network to facilitate seamless data exchanges."}, {"title": "6.3 Experiment 2: Assessing the Influence of Sensor Data Window Size on\nNetwork Traffic and Response Times in Various Frameworks", "content": "Experiment 2 aimed to explore how varying data transmission window sizes (25, 50, and 100)\nalign with methodologies from prior research [11] and impact network traffic, response times,\nthroughput, and power consumption across different frameworks. This study provides insights\ninto optimizing data transmission strategies for enhanced network performance and efficiency.\nAdjusting the data transmission window size influences vital performance indicators due to its\nimpact on how data is managed over the network. A larger window size could lead to higher\nnetwork traffic volumes and potentially longer response times as more data is transmitted less\nfrequently. Conversely, smaller window sizes may increase the frequency of transmissions, which\ncan reduce individual data packet sizes, potentially enhancing response times and adding com-\nmunication overhead. These variations directly affect throughput efficiency and power con-\nsumption at all network layers, necessitating a balance to optimize overall system performance.\nExperiment 2 investigates the optimal window size that harmonizes these factors for improved\nnetwork efficiency.\nThe experiment was structured to maintain constant variables such as data reduction rate\n(66%), number of sensors (30), and number of users (30) to isolate the effects of changing\nwindow sizes. The experiment used AE and PCA algorithms to reduce data dimensions and\nassess their effectiveness across different window sizes. In addition, the results can be discussed\nin terms of several parameters as follows:\nLatency improvements: As window sizes increased, latency improvements slightly decreased,\nwith smaller window sizes (particularly a window size of 25) providing the most significant re-\nduction in response times up to 93.56% with PCA. This trend likely results from the smaller\nwindows allowing more frequent data updates, which aligns well with InTec's distributed frame-\nwork to handle real-time processing demands.\nNetwork efficiency: The improvements in network traffic and throughput were most pronounced\nwith smaller window sizes, achieving an average of 14.98% and 13.53%, respectively, with PCA.\nSmaller windows reduce the data payload per transmission, leading to smoother data flow and\nminimizing bottlenecks within the network."}, {"title": "6.4 Experiment 3: Examining the Role of Active Sensor Quantity on Net-\nwork Performance Metrics Across Frameworks", "content": "The third experiment aims to evaluate the performance of different frameworks in processing\nsensor data with varying sensor counts. It specifically investigates how the number of active sen-\nsors-10, 20, 30, and 40\u2014affects critical performance indicators such as response time, network\ntraffic, throughput, and power consumption across various framework layers.\nThe count of active sensors within a network is a critical factor affecting performance metrics\ndue to the direct correlation between the number of sensors and the volume of data generated.\nAs the sensor count increases, there's an expected uptick in network traffic, potentially leading\nto longer response times due to the higher volume of data needing processing. Conversely, a\nmore significant number of sensors can lead to improved data accuracy and a richer dataset for\nanalysis, which could enhance the overall throughput. This experiment assesses the impact of"}, {"title": "6.5 Experiment 4: Analyzing User Request Volume's Impact on Network\nTraffic and Latency in Different Frameworks", "content": "The primary aim of Experiment 4 is to scrutinize how the volume of user requests impacts\nthe operational efficiency and resource consumption of the four frameworks under study. This\nanalysis focuses on understanding the frameworks' scalability and responsiveness to varying\nlevels of user engagement.\nThe volume of user requests is a significant variable that can affect network efficiency and the\nperformance of ML models within a framework. Increased user requests can lead to higher\nnetwork traffic", "levels": 10, "follows": "nLatency improvements: The InTec framework demonstrates significant latency reductions across\nall user request levels", "efficiency": "As the user count increases, network traffic and throughput demands also\nrise. However, the InTec framework maintains efficient network traffic and throughput, with\nimprovements of 15.32% and 15.89% for AE and PCA, respectively, at the highest user count\n(40 users). The results suggest that InTec's layered architecture efficiently handles increased\nuser-generated data, preventing congestion and optimizing data flow. Notably, PCA's superior\nperformance in managing traffic and throughput aligns with its ability to reduce data dimensions\neffectively, even when faced with high user volumes, making it ideal for systems that require\nconsistent network performance under variable demand.\\"}]}