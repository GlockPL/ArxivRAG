{"title": "MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents", "authors": ["Arya Bulusu", "Brandon Man", "Ashish Jagmohan", "Aditya Vempaty", "Jennifer Mari-Wyka", "Deepak Akkil"], "abstract": "There has been significant recent interest in harnessing LLMs to control software systems through multi-step reasoning, planning and tool-usage. While some promising results have been obtained, application to specific domains raises several general issues including the control of specialized domain tools, the lack of existing datasets for training and evaluation, and the non-triviality of automated system evaluation and improvement. In this paper, we present a case-study where we examine these issues in the context of a specific domain. Specifically, we present an automated math visualizer and solver system for mathematical pedagogy. The system orchestrates mathematical solvers and math graphing tools to produce accurate visualizations from simple natural language commands. We describe the creation of specialized data-sets, and also develop an auto-evaluator to easily evaluate the outputs of our system by comparing them to ground-truth expressions. We have open sourced the data-sets and code for the proposed system.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have had extraor-dinary recent success in tasks involving natural language generation and code generation. Spurred by this, there has been significant interest in harnessing LLMs to control software systems and embodied agents through multi-step reasoning, planning and leveraging tools and APIs Karpas et al. (2022); Hosseini et al. (2021); Hao et al. (2023); Schick et al. (2023); Patil et al. (2023). Promising results have been obtained in many tasks, from device and web-control to game-playing and roboticsWen et al. (2024); Lutz et al. (2024); Wang et al.(2023a;b); Ahn et al. (2022).\nThe creation of AI-driven automated systems for specialized domains holds great economic promise; estimated by a recent study at more than a trillion dollars Company (2024). While there exist several general multi-agent frameworks that can be built on, such as Autogen Wu et al. (2023b); Park et al. (2023), the development of LLM-driven agentic workflows in specialized domains requires overcoming several additional challenges:\n\u2022 Firstly, such domains come with specialized tools and problems, different from the general-purpose problems that past work has often focused on.\n\u2022 Secondly, there is often a paucity of datasets for training or benchmarking. For example, common math benchmarks like Cobbe et al. (2021), while important, are of limited value for settings like math pedagogy.\n\u2022 Thirdly, automated evaluation of such systems is hard, and human evaluation does not scale. This makes it hard to create continuous improvement loops, which are essential for robustness."}, {"title": "Related Work", "content": "Our work in this paper is related to a large body of recent literature in using LLMs for tool-usage, multi-step reasoning, and plan execution by agents. There is also related work"}, {"title": "Methodology", "content": null}, {"title": "Dataset Construction", "content": "The Common Core standards (CCSSO & NGA-Center, 2010) are a set of national educational standards describing what students are expected to know at each grade level, and they have been widely adopted in the United States. Based on the math Common Core standards, we identify a set of learning objectives that teachers use visualization tools to teach in the classroom. We use these categories as the basis of our dataset, creating approximately 10 questions per category to evaluate our system on.\nThe categories included in our datasets and the style of utterances were refined through teacher feedback, to reflect language that is commonly used by teachers in math pedagogy (e.g. \"Graph a unit circle\" rather than \"Draw a circle of radius 1\u201d). This feedback also informed the type of problems we chose and the learning objectives we targeted in our datasets. From the identified categories, we create three datasets; the first (referred to as the"}, {"title": "System", "content": "The MathViz-E system consists of four main components: the creation of the solver query, the generation of a written explanation based on the solver's output, the generation of the visual calculator graphing expressions based on the solver's output, and the validation and correction of the graphing expressions based on LLM self-critique. We incorporate multi-turn functionality into the system by including a calculator state in our prompts which describes the current expressions graphed on the calculator. As a result, the system is able to understand queries within the context of the expressions that have already been graphed. In this paper, we use Desmos as visual calculator, Wolfram Alpha as the solver and GPT-4 as the LLM, but the main principles of the system can be broadly applied to other tools and LLMs. The system is voice-driven; the presented version uses the Web Speech API MDN Web Docs (2023) for speech recognition, but other ASR pipelines can also be used.\nFor a given problem, we create the solver query by prompting the LLM with instructions and a series of examples demonstrating how to write queries for certain math problems. These examples were chosen by identifying problems the LLM consistently misunderstood. The LLM is also provided with the spoken-utterance version of the problem and the calculator"}, {"title": "Experiments", "content": null}, {"title": "Autoevaluation", "content": "Traditional evaluation metrics for text similarity fail for comparing mathematical statements due to the precise nature of math statements. Consider the statement 5=2+3. Lexical"}, {"title": "Results", "content": "For the results reported in this section, we comprehensively evaluate the LLM+Solver system by manually validating all of the outputs generated for the three datasets (utterance-focused, textbook-focused, and multi-turn). We compare the performance of the LLM+Solver system to the results of the LLM-only system. The LLM-only system consists of directly prompting an LLM with instructions to write Desmos expressions and examples, while also providing the natural language utterance problem and the calculator state. No solver solution is provided. Although the framework of the system can be applied to LLMs and solvers broadly, in this paper we evaluate using GPT-4 as the LLM and Wolfram Alpha as the solver. Tables 4, 5 and 6 compare the results for the LLM-only system and the LLM+Solver systems."}, {"title": "Discussion", "content": "The results presented in this paper highlight the potential of domain-specific automation created via LLM orchestration of specialized tools. The presented case-study highlights some of the main challenges that need to be overcome in such systems. The paucity of preexisting datasets requires careful creation of new datasets for benchmarking; there is a need to identify and control specialized tools; and there is a need for auto-evaluation to validate and improve system performance.\nFor the case of mathematical pedagogy, we showed that through the design and develop-ment of an LLM-orchestrated system incorporating multiple specialized tools, through the creation of new benchmark datasets based on Common Core, and through the creation of an auto-evaluation pipeline, we created an effective automated system and the means to easily evaluate future iterations. The system has strong performance, and for many categories of problems it can consistently produce accurate outputs.\nFor the domain under consideration, there are categories of problems for which the system is not yet fully reliable. To improve system performance in these categories, there are several directions that we can take in the future. An especially promising approach is to add retrieval-based techniques to make adjustments to the system for the individual problem categories. This individualized approach could improve accuracy for some categories as compared to our current, one-size-fits-all system. Another key direction is to reduce latency; we plan to evaluate our system using smaller open-source LLMs, trained via supervised fine-tuning, instead of a large general model like GPT-4.\nMore broadly, for domain-specific agentic solutions, the approach presented in this paper offers a set of patterns that can be generalized. We expect that the generalization and application of these patterns to other domains and problems will continue to remain fertile ground for future research."}]}