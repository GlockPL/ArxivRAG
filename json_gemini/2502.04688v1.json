{"title": "M-IFEval: Multilingual Instruction-Following Evaluation", "authors": ["Antoine Dussolle", "Andrea Carde\u00f1a D\u00edaz", "Shota Sato", "Peter Devine"], "abstract": "Instruction following is a core capability of modern Large language models (LLMs), making evaluating this capability essential to understanding these models. The Instruction Following Evaluation (IFEval) benchmark from the literature does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement. However, it only includes English instructions, limiting its ability to assess LLMs in other languages.\nWe propose the Multilingual Instruction Following Evaluation (M-IFEval) benchmark, expanding the evaluation to French, Japanese, and Spanish, with both general and language-specific instructions. Applying this benchmark to 8 state-of-the-art LLMs, we find that benchmark performance across languages and instruction types can vary widely, underscoring the importance of a multilingual benchmark for evaluating LLMs in a diverse cultural context.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated amazing accuracy in many fields including medicine (Tian et al., 2024; Frisoni et al., 2024), law (Jiang et al., 2024), and education (Luo et al., 2024). However, their accuracy has also shown to be low for some tasks such as reasoning (Tong et al., 2024) and cultural understanding (Wang et al., 2024).\nOne type of task of particular importance for LLMs is that of instruction following, where an LLM must carry out the instructions of the user in a \"zero-shot\" setting (i.e. without necessarily being trained specifically to perform that instruction) (Zhong et al., 2021; Mishra et al., 2022; Wei et al.; Sanh et al., 2022).\nBenchmarks such as Instruction-Following Evaluation (IFEval) (Zhou et al., 2023) have proposed ways of evaluating LLMs on instruction following without the need for using an external LLM-as-a-judge (Zheng et al., 2023), which may exhibit self-enhancement bias (Xu et al., 2024).\nHowever, this benchmark is a purely English-based benchmark, raising questions as to the applicability of its results to other languages. While some efforts have been made to make a multilingual version of this benchmark, at present this only extends as far as translating the original prompts into other languages (Yang et al., 2024). This approach fails to evaluate aspects of instruction following that are specific to different languages. Specifically developed code is required to understand whether an LLM can, for example, uses the correct punctuation or script for a given language when prompted.\nWe present Multilingual Instruction Following Evaluation (M-IFEval), a benchmark for evaluating LLM instruction following beyond English. Our benchmark consists of three popular natural languages, French, Japanese, and Spanish, and contains both instructions previously assessed in English as well as novel instructions that are specific to our evaluation languages. We assess 8 state-of-the-art LLMs using M-IFEval and compare their evaluation results to the original English IFEval scores.\nOur evaluation results show that, among the models tested on the English instruction-following benchmark, widely-used LLMs like GPT4o achieve the highest relative performance. However, for benchmarks in other languages, models such as ol and Sonnet perform better in instruction following. We also highlight that state-of-the-art LLMs achieve surprisingly low scores on some language-specific instructions such as using or not using special characters or scripts.\nOur work demonstrates the value of a multilingual benchmark when selecting LLMs for a non-English based task and identifies key areas for improvement, such as character- and script-level in-"}, {"title": "2 Related work", "content": "Benchmarks like GLUE (Wang et al., 2018), ARC (Clark et al., 2018), SuperGLUE (Wang et al., 2019), Winogrande (Sakaguchi et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and others, that contain objective tasks such as natural language inference and semantic similarity have been widely used within the literature to evaluate LLMs (Anil et al., 2023; Le Scao et al., 2023; Dettmers et al., 2023). However, these tasks do not fully represent the realistic usage of LLMs in practical scenarios, for example as conversational agents or decision-making systems.\nOther benchmarks such as MT-Bench (Zheng et al., 2023), AlpacaEval (Dubois et al., 2024), Arena-Hard (Li et al., 2024) and InFoBench (Qin et al., 2024) provide a framework for evaluating LLMs in a more practical conversational or instruction-based setting. However, their reliance on subjective AI scoring raises self-enhancement bias concerns (Xu et al., 2024), making them less suitable for model evaluation.\nChatbot Arena (Zheng et al., 2023) involves evaluating LLMs using human users, thus removing the potential for self-enhancement bias. However, Chatbot Arena requires large-scale deployment, making it difficult to replicate this evaluation for local models.\nIFEval (Zhou et al., 2023) was designed to evaluate the ability of LLMs to follow instructions in more practical scenarios, making it an effective measure of models intended for real-world usage. Unlike MT-Bench, IFEval uses objective criteria for evaluation, removing the subjectivity inherent in AI-judged systems. However, its primary limitation is that it is currently available only in English, which restricts its applicability for evaluating multilingual models.\nThe model evaluation of the instruction-tuned Qwen 2.5 (Yang et al., 2024) extended IFEval to support multilingual settings by translating 100 examples per language and removing instructions that were not applicable to a given language. While this approach allows for deterministic evaluation of LLMs on multilingual data, it may neglect language-specific aspects of instruction-following."}, {"title": "3 Method", "content": "This section details how we constructed the M-IFEval benchmark and how we used it to evaluate various state-of-the-art LLMs.\nWe first chose the languages that would be included in our benchmark. We chose French, Japanese, and Spanish as our research team included one native speaker in each language, with each acting as language lead for their respective language.\nOur team consulted the list of instructions included in the original English language IFEval benchmark and considered any that were not applicable to their language. Following this, we removed the word number length constraint instruction and the case change instructions (all uppercase, all lowercase, and frequency of all capital words) for Japanese, as its writing system does not account for letter case.\nNext, each language lead created a list of instructions that could be evaluated using objective criteria, including those specific to their language. The design of these verifiable instructions was guided by two key considerations. First, the instructions needed to evaluate language-specific linguistic and textual control, addressing elements such as diacritics, script-level constraints, and cultural nuances. Secondly, they were designed to maintain a reasonable difficulty level, ensuring that the tasks remained easily achievable for native speakers, thus promoting fairness across the three evaluation languages. These instructions encompassed grammatical, stylistic, and script-based elements tailored to each language.\nThe Spanish-specific instructions consisted of three special character-based instructions (\u00f1 frequency, accent frequency, and \u00fc frequency) and two punctuation-based instructions (using grammatically correct question marks and exclamation marks).\nThe French-specific instructions consisted of three special character-based instructions (forbidding use of \u0153/\u00e7, forbidding accents, and adding the correct accents to a given text), and two content based instructions (not using Arabic numerals in"}, {"title": "4 Results", "content": "Table 1 shows the average evaluation score for each model evaluated in each language in the M-IFEval benchmark, along with with the English scores in the original IFEval benchmark.\nWe observe that while GPT40 and Sonnet are the top two models for English M-IFEval, o1 and Opus have the highest score on average for the three languages in our benchmark. We also observe a greater spread in scores between the best and worse performing models in our evaluation compared to the English IFEval, with the best and worst scores on the original English benchmark having a difference of 11.3 percentage points, while we observe differences of 13.9, 13.0, and 15.1 for Spanish, French, and Japanese respectively.\nTable 2 shows the average scores only on instructions that are unique to each language. We observe that while the o1 model attains the greatest scores on Spanish and French benchmarks, Sonnet achieves markedly higher scores on the Japanese benchmark. When we analysed the scores only of instructions that had been included in the original IFEval benchmark (i.e. instructions not unique to the language), we found that ol achieves a score on the Japanese benchmark of 84.8 while Sonnet achieves a score of 81.2.\nWhen we analysed specific instructions with the lowest average evaluation scores across all models that we tested, we found that the 10 instructions with the lowest scores all were language-specific instructions such as forbidding \u201c\u0153/\u00e7\u201d, forbidding katakana, or specifying the frequency of the \"n\" character. The average scores for these three instruction types across all models was 60.2%, 14.3%, and 0.0%, respectively. Conversely, we observe that LLMs attain high scores in following instructions such as adding accents to French text, adding Spanish question marks/exclamantion marks, and making both French and Spanish text uppercase/lowercase. This suggests that while many LLMs perform well in following formatting instructions, such as structuring outputs and arranging punctuation, they struggle with script-based instructions. This is apparent from the drop in accuracy in the 'Special character' instruction group for Spanish and French, as well as the 'Script' instruction group for Japanese, both of which mostly comprise character-level instructions, as shown in figures 2-4 in appendix B.\nThe full scores averaged across all models for"}, {"title": "5 Discussion & Future work", "content": "Overall, our results show that modern LLMs are generally proficient at instruction following outside of English. However, our evaluation scores still vary between both languages and task types, indicating the need for future improvement of LLMs in a wide range of linguistically and culturally important tasks.\nOur results show that Sonnet is more proficient at Japanese-specific instructions compared to o1, whereas o1 is more proficient at Spanish and French specific instructions. This could indicate that o1 has been trained on more Spanish and French data, or linguistically similar languages that confer cross lingual generalisation (Sn\u00e6bjarnarson et al., 2023; Muennighoff et al., 2023), while Sonnet may have been trained on more Japanese data.\nMoreover, we find that the highest performing model in our English evaluation was neither o1 nor Sonnet, but GPT4o. This highlights the need for multilingual LLM evaluations to select the best model for a target language, as no single LLM excels in all languages.\nOur results also show that LLMs generally achieve poor performance on seemingly simple language-specific tasks such as restricting usage of a given script (e.g. \"write your answer without using any katakana\u201d) or controlling for the amount of times a certain special character is used (e.g. \u201cuse the 'n' character exactly 5 times in your response\"). Examples of such failures are provided in appendix C. This contrasts with high English scores for similar tasks (e.g. \"use the letter c at least 60 times in your response\"). This may reflect a gap between LLM performance in English to that of other languages which has been observed in other tasks (Ahuja et al., 2024; Jin et al., 2024).\nFuture work could consider exactly why the performance of LLMs varies for different languages. Previous work has investigated the effect of different language mixtures on downstream tasks (\u00dcst\u00fcn et al., 2024; Wei et al., 2023), so experiments involving different mixes of multilingual pre-training data and fine-tuning data could possibly show the effect of training data on instruction following performance.\nExperiments using a byte-level tokenizer (Xue et al., 2022) could possibly answer the question of why script or character based instructions are so"}, {"title": "6 Conclusion", "content": "In this paper, we have presented M-IFEval, a multilingual benchmark which evaluates the instruction following abilities of LLMs in three non-English languages: French, Japanese, and Spanish.\nOur results show that while GPT4o achieves the greatest instruction following performance on the English IFEval benchmark, we find that other models, ol and Sonnet, achieve higher scores on M-IFEval.\nThis finding highlights the importance of multilingual evaluation in assessing a model's instruction following abilities.\nWe also identify several types of instructions for which the average LLM performance was surprisingly low. This includes specifying the usage/non usage of a certain script and specifying the frequency of a certain amount of non-English characters.\nThis work contributes a new benchmark to the field of multilingual evaluation of LLMs and provides observations for what these models can and cannot do in the context of multilingual instruction following."}, {"title": "7 Limitations", "content": "One of the limitations of this work is that our benchmark only considers instructions that can be objectively evaluated using simple string checking code. This means that our evaluation does not include any of the large group of possible instructions which would require more intricate analysis to evaluate upon (e.g. translation quality, fact checking, question answering). We acknowledge this and leave more detailed evaluation of LLMs on specific tasks to other benchmarks such as XNLI (Conneau et al., 2018), XQuad (Artetxe et al., 2020), and Flores (Costa-juss\u00e0 et al., 2022). And, although certain instructions that can be evaluated programmatically might seem unnatural (e.g., \"Write a paragraph using the letter 'j' exactly 9 times\"), our goal was to investigate the types of instructions that LLMs still tend to struggle with the most. This therefore provides insight into the types of realistic tasks these LLMs may also find challenging. Future work could explore instruction following in more realistic, user-driven scenarios by incorporating organic, diverse, and contextually grounded prompts that better reflect real-world usage. This would provide a more nuanced understanding of how well models perform in genuinely practical settings.\nAnother limitation of this work is that we only consider three non-English languages in our evaluation. Moreover, these three languages were all relatively high-resource languages, and since we observe a gap between English and our evaluation languages, we may observe an even greater gap for low resource languages. Future work could include adding more languages to our benchmark, particularly low resource languages. This could entail adding more language-specific instructions (e.g. converting \u201cBoko\u201d, or Latin, script in Hausa to \"Ajami\", or Arabic, script (Abdulmumin, 2014)) to further identify if there are any other tasks in which LLMs perform particularly poorly.\nA final limitation of this work is that we only evaluate over 8 state-of-the-art LLMs in our evaluation when other LLMs such as Gemini (Reid et al., 2024) are also available. This was done due to a combination of technical, financial, and document-space limitations, and so the main contributions of our paper are that we demonstrate that relative instruction following performance is not uniform across all languages for a given LLM, and that some of the top performing LLMs available still cannot perform basic tasks such as controlling special character usage. We leave it for future work to use this benchmark to compare their models against others."}, {"title": "A Dataset", "content": "A.1 M-IFEval Language Specific Instructions\nA.2 Dataset Statistics"}, {"title": "B Detailed Results", "content": ""}, {"title": "C Failure Examples", "content": "Below, we present examples of failure cases in the different languages of the benchmark.\nC.1 Spanish\nFailure Example: Special Characters - Letter Frequency (\u00f1)\nModel: 01\nLanguage: Spanish (ES)\nPrompt: Describe el impacto de la tecnolog\u00eda m\u00f3vil en la comunicaci\u00f3n. En tu respuesta, las palabras que contienen la letra \"n\" deben ser 9 en total.\nModel Response: \"La tecnolog\u00eda m\u00f3vil ha transformado la comunicaci\u00f3n en los \u00faltimos a\u00f1os. Ahora, las personas pueden conectarse instant\u00e1neamente desde cualquier rinc\u00f3n del mundo, compartiendo experiencias y opiniones en tiempo real. Esta revoluci\u00f3n ha desempe\u00f1ado un papel esencial en la transformaci\u00f3n de la sociedad, facilitando la interacci\u00f3n y el intercambio de conocimiento sin precedentes. Sin embargo, tambi\u00e9n surge la preocupaci\u00f3n por la sobreexposici\u00f3n y la gesti\u00f3n de la privacidad. En resumen, la tecnolog\u00eda m\u00f3vil ha ampliado nuestras oportunidades de comunicaci\u00f3n, pero requiere una consideraci\u00f3n consciente para equilibrar sus beneficios y desaf\u00edos.\"\nInstruction (translated): \"n\" should appear 9 times. X\nFailure: The response only contains 2 \"n\" when the instruction required 9.\nFailure Example: Special Characters\nAll Lowercase + Exclamation Marks + Accented Word Frequency\nModel: Qwen 2.5 32B I.\nLanguage: Spanish (ES)\nPrompt: Describe el impacto de los algoritmos de recomendaci\u00f3n en las plataformas de streaming. Tu respuesta debe estar completamente en espa\u00f1ol, usando s\u00f3lo letras min\u00fasculas. No se permiten may\u00fasculas. Incluye al menos una exclamaci\u00f3n en tu respuesta. Aseg\u00farate de no usar m\u00e1s de 5 palabras con tilde en tu respuesta.\nModel Response: \"los algoritmos de recomendaci\u00f3n transforman la experiencia en plataformas de streaming. analizan gustos, crean listas personalizadas, incrementan la retenci\u00f3n de usuarios. esto cambia c\u00f3mo exploramos contenido, nos mantiene enganchados, nos muestra lo que queremos ver antes de saberlo nosotros mismos. es fascinante c\u00f3mo funcionan! sin ellos, navegar\u00edamos ciegamente por un oc\u00e9ano de opciones.\"\nInstructions (translated):\n\u2022 Answer entirely in Spanish, using only lowercase letters.\n\u2022 Include at least one exclamation point in your answer.\n\u2022 Do not use more than 5 words with an accent in your answer. X\nFailure: The response contains 5 words with accents, one more than the instruction required.\nC.2 French\nFailure Example: Special Characters - Forbidden \u0153 and \u00e7\nModel: GPT40\nLanguage: French (FR)\nPrompt: Quelle hormone d\u00e9cline pendant la m\u00e9nopause, entra\u00eenant divers sympt\u00f4mes tels que les bouff\u00e9es de chaleur ? Assurez-vous de ne pas employer de virgules dans votre texte et ne faites pas usage du graph\u00e8me \u0153.\nModel Response: \"L'hormone qui d\u00e9cline pendant la m\u00e9nopause est l'\u0153strog\u00e8ne.\"\nInstruction (translated): Do not use the character \"\u0153\". X\nFailure: The forbidden character \"\u0153\" was used once.\nFailure Example: Special Characters - No Accents\nModel: 01 Mini\nLanguage: French (FR)\nPrompt: Concevez une rencontre entre deux figures historiques que vous respectez. \u00c9crivez votre description sans recourir aux accents.\nModel Response: \"Imaginons une rencontre entre Leonardo da Vinci et Albert Einstein dans un jardin paisible, entour\u00e9 de sculptures et de machines inventives. Leonardo, avec son esprit curieux et ses croquis toujours en main, observe attentivement les formules complexes qu'Albert \u00e9crit sur un carnet. Les deux visionnaires \u00e9changent des id\u00e9es sur la nature de l'univers et les mysteres de l'energie. Leonardo partage ses dessins de machines volantes, tandis qu'Albert explique la theorie de la relativite. Leur conversation est un melange fascinant de l'art et de la science, montrant comment deux esprits brillants, bien qu'appartenant a des epoques differentes, peuvent trouver un terrain commun pour discuter et s'inspirer mutuellement. Ensemble, ils imaginent des innovations qui pourraient changer le monde, illustrant le pouvoir de la collaboration entre creativity et rationalite.\"\nInstruction (translated): Do not use accents. X\nFailure: Accents were used 3 times, even though that typically require accents were successfully left unaccented.\nC.3 Japanese\nFailure Example: Script - No Katakana\nModel: 01\nLanguage: Japanese (JA)\nPrompt: \u30ab\u30bf\u30ab\u30ca\u3092\u4e00\u6587\u5b57\u3082\u4f7f\u308f\u306a\u3044\u3067\u3001\u30a2\u30d5\u30ea\u30ab\u306e\u4f1d\u7d71\u97f3\u697d\u3067\u4f7f\u7528\u3055\u308c\u308b\u697d\u5668\u306e\u4e00 \u3064\u3092\u7d39\u4ecb\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nModel Response: \" \u30a2\u30d5\u30ea\u30ab\u306e\u4f1d\u7d71\u7684\u306a\u697d\u5668\u306e\u4e00\u3064\u306b\u300c\u3058\u3047\u3093\u3079\u300d\u3068\u3044\u3046\u592a\u9f13\u304c\u3042\u308a\u307e\u3059\u3002\u3058\u3047\u3093\u3079\u306f\u897f \u30a2\u30d5\u30ea\u30ab\u306e \u30de\u30ea \u3084 \u30ae\u30cb\u30a2 \u306a\u3069\u3067\u5e83\u304f\u4f7f\u308f\u308c\u3066\u304a\u308a\u3001\u4e00\u679a\u306e\u6728\u3092 \u304f\u308a\u629c\u3044\u3066\u4f5c\u3089\u308c\u305f\u80f4\u306b\u3001\u52d5\u7269\u306e\u76ae\u3092\u5f35\u3063\u3066\u4f5c\u3089\u308c\u307e\u3059\u3002\u624b\u3067\u53e9\u3044\u3066\u6f14\u594f\u3057\u3001\u8c4a\u304b\u306a \u97f3\u8272\u3068\u591a\u5f69\u306a\u30ea\u30ba\u30e0\u304c\u7279\u5fb4\u3067\u3059\u3002\u796d\u308a\u3084\u8e0a\u308a\u306e\u5834\u3067\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3001\u4eba\u3005\u3092\u3064\u306a \u3050\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u306e\u9053\u5177\u3068\u3057\u3066\u3082\u7528\u3044\u3089\u308c\u3066\u3044\u307e\u3059\u3002\"\nInstruction (translated): Do not include any katakana characters. X\nFailure: Words like \u30a2\u30d5\u30ea\u30ab (Africa) were written in katakana, despite the existence of a non-katakana orthography: \u963f\u5f17\u5229\u52a0.\nFailure Example: Script - Hiragana Only\nModel: Haiku\nLanguage: Japanese (JA)\nPrompt: \u3072\u3089\u304c\u306a\u3060\u3051\u3092\u4f7f\u3063\u3066\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u540c\u3058\u5e7c\u7a1a\u5712\u306e\u3051\u3093\u541b\u306b\u544a\u767d\u3059\u308b\u624b\u7d19 \u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\nModel Response:\"\u306f\u3044\u3001\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3053\u3053\u306b\u3001\n\u306e\u3051\u3093 \u541b\u306b\u544a\u767d\u3059\u308b\u624b\u7d19\u3092\u66f8\u304d\u307e\u3057\u305f\u3002\n\u307c\u304f\u306f\u3001\u3051\u3093\u541b\u306e\u3053\u3068\u304c\u5927\u3059\u304d\u3067\u3059\u3002\u3044\u3064\u3082\u4e00\u7dd2\u306b\u904a\u3093\u3067\u3044\u3066\u3001\u3068\u3066\u3082\u697d\u3057\u3044 \u3067\u3059\u3002\u3051\u3093 \u541b\u306e\u7b11\u9854\u304c\u3001\u307c\u304f\u306e\u5fc3\u3092\u6e29\u304b\u304f\u3057\u3066\u304f\u308c\u307e\u3059\u3002\u3053\u308c\u304b\u3089\u3082\u3001\u3051 \u3093\u541b\u3068\u4e00\u7dd2\u306b\u3044\u305f\u3044\u3067\u3059\u3002\u307c\u304f\u306e\u6c17\u6301\u3061\u3092\u53d7\u3051\u53d6\u3063\u3066\u304f\u3060\u3055\u3044\u3002\"\nInstruction (translated): Only use hiragana characters X\nFailure: Ignored the instruction and used kanji for most words that are typically written in kanji."}]}