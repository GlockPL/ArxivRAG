{"title": "SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for Efficient 3D Plant Reconstruction by a Robot", "authors": ["Jianchao Ci", "Eldert J. van Henten", "Xin Wang", "Akshay K. Burusa", "Gert Kootstra"], "abstract": "The 3D reconstruction of plants is challenging due to their complex shape causing many occlusions. Next-Best-View (NBV) methods address this by iteratively selecting new viewpoints to maximize information gain (IG). Deep-learning-based NBV (DL-NBV) methods demonstrate higher computational efficiency over classic voxel-based NBV approaches but current methods require extensive training using ground-truth plant models, making them impractical for real-world plants. These methods, moreover, rely on offline training with pre-collected data, limiting adaptability in changing agricultural environments. This paper proposes a self-supervised learning-based NBV method (SSL-NBV) that uses a deep neural network to predict the IG for candidate viewpoints. The method allows the robot to gather its own training data during task execution by comparing new 3D sensor data to the earlier gathered data and by employing weakly-supervised learning and experience replay for efficient online learning.\nComprehensive evaluations were conducted in simulation and real-world environments using cross-validation. The results showed that SSL-NBV required fewer views for plant reconstruction than non-NBV methods and was over 800 times faster than a voxel-based method. SSL-NBV reduced training annotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV could adapt to novel scenarios through online fine-tuning. Also using real plants, the results showed that the proposed method can learn to effectively plan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV automated the entire network training and uses continuous online learning, allowing it to operate in changing agricultural environments.", "sections": [{"title": "1. Introduction", "content": "Greenhouse production is a widely used method for vegetable cultivation, offering substantial benefits such as an extended plant production period, improved quality, and quantity, and finally leading to increased profitability for growers. The selection of suitable cultivars, i.e. phenotyping, as well as the crop production process itself, including tasks like harvesting and de-leafing, require a significant amount of labor. This limits large-scale greenhouse production because experienced labor is scarce and labor costs are high. Automating these labor-intensive tasks using robotics can help reduce reliance on human labor and potentially increase greenhouse productivity and production efficiency.\nRobotic phenotyping and greenhouse operations like harvesting and leaf removal, are essentially based on the same key functionality, i.e. perception. However, accurate robotic perception in complex greenhouse environments is challenging due to the presence of occlusion and variation. Occlusion is mainly caused by leaves, preventing sufficient information collection, thus increasing uncertainty in robotic operations. Variations arise from dynamic growing environment (e.g., lighting) and inherent plant properties (e.g., shape, size, and texture), making the systems may work in a certain condition but lack robustness to variation. The objective of this study was to develop a robust robotic perception system to capture sufficient information of the plants under the presence of occlusion. The system was specifically developed and evaluated in plant phenotyping scenario for plant reconstruction tasks, but with the potential to be used for other tasks such as robotic harvesting after small modifications and fine-tuning.\nPlant phenotyping is a set of methodologies to measure the plant traits, such as leaf angle, node length, and leaf area, and subsequently link these measurements to plant genotypes and growing environments, which is a key technology in plant breeding for efficient selection of target cultivars (Poland and Rife, 2012). Compared to traditional manual assessment of plant traits, robotic phenotyping methods have received significant interests by integrating machine vision techniques and robotics, showing potential of plant phenotyping in an automatic, accurate, non-destructive, and high-throughput manner (Atefi et al., 2019; Hartmann et al., 2011; Polder and Hofstee, 2014; van der Heijden et al., 2012). Two-dimensional (2D) robotic phenotyping methods have been widely studied (Jansen et al., 2009; Minervini et al., 2014; Tisn\u00e9 et al., 2013). However, a significant disadvantage of these methods is they cannot accurately measure plant traits that are expressed in three dimensions (3D) such as plant volume. Alternatively, three-dimensional (3D) methods perform measurement in 3D data, offering more comprehensive and accurate information of plant traits (Boogaard et al., 2023; Shi et al., 2019). An essential first step of 3D phenotyping methods is generating a 3D digital reconstruction of the plants, which can be represented as a point cloud (Boogaard et al., 2022, 2021), or a mesh (Thapa et al., 2018; V\u00e1zquez-Arellano et al., 2018). However, accurate 3D plant reconstruction is extremely challenging due to significant plant occlusion. A common approach to solve this problem is using multiple viewpoints (Golbach et al., 2016; Lu et al., 2017; Shi et al., 2019), whereby the information that is occluded in a single view becomes available. However, as these methods employ a passive paradigm, where data collection is constrained to predefined views and trajectories, they suffer from missing data, redundant information collection, and the need for continuous fine-tuning for novel plants.\nRecently, Next-Best-View (NBV) methods in agricultural robotics have gained significant interest (Gibbs et al., 2020, 2018; Zaenker et al., 2021a; Zapotezny-Anderson and Lehnert, 2019). NBV adopts an active perception paradigm to actively reposition and reorient the camera. The selection of the next viewpoint relies on analyzing the observed data so far, considering the objective at hand, with the aim of maximizing the information gain (IG). IG is a metric that quantifies the increase in information achievable by choosing a new viewpoint. This approach enables more efficient and flexible data collection compared to passive perception methods (Atefi et al., 2021). Accurate and efficient estimation of IG is crucial for NBV methods. Traditional NBV methods require maintaining an occupancy grid representing space in voxels and corresponding occupancy probabilities indicating whether a voxel is free or occupied, we call this Voxel-NBV. IG estimation of a candidate viewpoint is performed by casting rays from the viewpoint into the occupancy grid and counting the voxels (both free and occupied) that the rays traverse. This method has been employed in various agricultural applications (Burusa et al., 2022; Gibbs et al., 2020, 2018; Zaenker et al., 2021b). This method is useful as it estimates IG explicitly, making the planning process interpretable, and provides information about both occupied and unoccupied space. However, it can be computationally and memory-intensive due to casting millions of rays and examining the transitions of each ray at a voxel level.\nDeep-Learning-based Next-Best-View (DL-NBV) methods aim to address these downsides. These methods use neural networks to directly predict the IG based on the observed data so far, eliminating the need for computationally expensive ray-casting and voxel-wise operations. For example, Mendoza et al. (2020) proposed NBV-Net, which utilizes a 3D convolutional neural network to directly predict the IG of a set of pre-defined candidates using the accumulated occupancy grid as input. Similarly, Zeng et al. (2020) proposed a point-cloud-based neural network called PC-NBV, which predicts the IG values of all the candidates by taking the accumulated point cloud as input. PC-NBV offers higher planning efficiency by eliminating the step of converting the point cloud to a volumetric map. While both methods display high reconstruction efficiency and robustness to man-made objects with simple geometries, their performance in reconstruction complex plant structure has not been evaluated. Moreover, both methods require separate training steps and complete object models for ground-truth IG estimation, posing challenges for use in dynamic agricultural environments. In this paper, we will develop a DL-NBV method based on the existing PC-NBV method but with the capability of online self-supervised learning and evaluate its performance in an agricultural scenario focusing on plant phenotyping.\nTraining DL-NBV networks for plant reconstruction poses a significant challenge due to the difficulty of acquiring large amounts of IG-labeled training data. Manual calculation of ground-truth IG is impractical as it is subjective and difficult to accurately measure IG values. However, using a robot, this process can be automated by having the robot explore the environment. Thus, the robot can autonomously collect and annotate IG for data for training even during execution of the task, enabling to improve performance in a life-long and self-supervised manner. This learning approach is termed Self-Supervised Learning (SSL) (Deng et al., 2020). A challenge of SSL is the automatic annotation of IG. Mendoza et al. (2020), Wang et al. (2019), and Zeng et al. (2020) compute ground-truth IG of a candidate view by comparing to the complete model. However, these methods require prior knowledge of the object shape, making them infeasible for agricultural applications as plant models are not available in advance. To solve this problem, this research proposes an improved IG calculation metric based on the method of Zeng et al. (2020) but enables ground-truth IG calculation solely based on the data collected by the robot, thereby achieving self-supervised learning.\nAnother problem that prevents the application of current DL-NBV methods in agriculture is their reliance on offline training, i.e., initially collecting large amounts of training data, training the method and then execution in a separate step. This limits the method to continue to improve or to adapt to novel targets and environments (Kahn et al., 2020). Conversely, online learning algorithms integrate both processes into a continuous feedback loop, allowing the network to learn and optimize while executing the task, facilitating adaptation to new targets and environments. However, online learning methods often struggle with low sample-efficiency, as they focus on the most recent data and discard historical data (Zhang and Sutton, 2017). To address the challenges of online learning, Mnih et al. (2013a) employed an off-policy training method called 'experience replay' in the field of Reinforcement Learning (RL), demonstrating improved sample-efficiency and generalization (refer to step 5 of section 2.1.1 for detailed explanation). This inspired our work. Additionally, existing DL-NBV methods typically undergo training with strongly-supervised data, wherein each training input requires IG-annotation for all candidates as ground-truth, leading to inefficient data collection. To address this issue, we employed a weakly-supervised learning technique to reduce the number of required IG annotations, facilitating efficient training data collection and online learning. Further discussion is provided in section 2.1.4.\nThe objective of this paper is to propose and evaluate a novel SSL-NBV method for 3D robotic plant reconstruction tasks. Specifically, the state-of-the-art learning-based NBV method is improved to enable online SSL of IG prediction, allowing for online network adaptation to novel plants during task execution. To achieve successful online self-supervised learning, an IG metric is designed to eliminate the need for complete plant models in ground-truth IG calculation, an experience replay technique is employed to improve sample-efficiency, and weakly-supervised learning is utilized for efficient training data collection. The method was tested in both simulation and real-world scenarios, providing a comprehensive evaluation in comparison with various baseline planners. The real-world experiments demonstrated that the proposed method is also capable to learn view planning for the reconstruction of real plants. The study addressed the following questions:\n1.  What reconstruction quality and efficiency can the proposed SSL-NBV achieve in comparison with other NBV (PC-NBV and Voxel-NBV) and non-NBV (Random and Pre-defined) methods?\n2.  How does the proposed SSL-NBV, using online and weakly-supervised learning, compare to PC-NBV, using offline and strongly-supervised learning?"}, {"title": "2. Methods and materials", "content": "This section describes the method and experimental setup. Section 2.1 describes the online self-supervised learning process for plant reconstruction. The experimental setup, including simulation and real-world scenarios, is detailed in section 2.2. Finally, section 2.3 presents the metrics used for evaluating the performance of the proposed method."}, {"title": "2.1. Online self-supervised learning for plant reconstruction", "content": "The entire learning process involves continuous iterations. At each iteration, the robot performs next-best-view planning using a deep neural network to select a new camera pose from a set of candidate viewpoints to collect new data for 3D plant reconstruction, which simultaneously generates new training data to update the network. This online learning process allows the network to be updated during task execution and without the need for a separate training phase. This allows the robot to continuously improve and to adapt to a changing environment. The algorithm was first developed and validated in the simulation for repid development and repeated experiment, then evaluated in the real-world application.\nThe entire learning process consists of a maximum number of T iterations. The candidates set C = {c\u2081, ..., cM} is defined before the entire learning process, where c\u1d62 \u2208 \u211d\u2076 is a camera pose consisting of position and orientation {x\u1d62, y\u1d62, z\u1d62, \u03b1\u1d62, \u03b2\u1d62, \u03b3\u1d62}, and M is the total number of candidate views. Briefly, the learning process works by iteratively selecting views from C as camera pose for plant reconstruction and collecting data to update the network. View change and data collection are conducted using a robot arm with a depth camera mounted as end effector. From each view, a partial point cloud is collected, and the point clouds collected from all views are fused as a digital plant reconstruction.\nFig. 1 provides an overview of an iteration. An iteration t consists of seven steps: (1) Using a DL-NBV network to predict the IG, \u011ct by taking as input the current accumulated point cloud P\u1d57 and a view-selection state V\u1d57 keeping track of previously visited viewpoints; (2) selecting the next view V\u1d57\u208a\u2081 based on \u011ct; (3) moving the camera to V\u1d57\u208a\u2081 and collecting a new partial point cloud P\u1d57\u208a\u2081; (4) calculating the actual IG G\u1d57 based on P\u1d57\u208a\u2081 and P\u1d43; (5) storing supervision pair with P\u1d57 and V\u1d57 as the input, and G\u1d57 as the ground-truth; (6) updating the accumulated point cloud P\u1d57\u208a\u2081 and view-selection state V\u1d57\u208a\u2081; and (7) training the network weights W\u1d57\u208a\u2081 of the DL-NBV based on a batch from the training data. The steps are explained in detail in the next subsection."}, {"title": "2.1.1. The iteration steps in detail", "content": "In step 1, the current accumulated point cloud P\u1d57 and view-selection state V\u1d57 are fed into the DL-NBV network to predict \u011c\u1d57 = {\u011d\u2081, ..., \u011dM}, which contains IG for all candidates, where \u011d\u1d62 \u2208 \u211d represents the predicted IG for a viewpoint. P\u1d57 = {p\u2081, ..., pK} is the accumulated point cloud containing all collected points, where p\u2096 = {x\u1d62, y\u1d62, z\u1d62} \u2208 \u211d\u00b3 is a 3D point with xyz-coordinates. V\u1d57 = {s\u2081, ..., sM} is a vector containing the selection state for all candidate views, with s\u1d62 \u2208 {0,1} equaling 1 if a viewpoint was selected in the past and 0 otherwise. At the beginning of the learning process, P\u1d43=\u00d8 and V = {0, ...,0}. In this research, we used the PC-NBV network (Zeng et al., 2020) as the DL-NBV planner. A detailed explanation of the PC-NBV network structure and IG prediction is provided in section 2.1.2.\nIn step 2, based on \u011c\u1d57, the estimated next-best view of iteration t is determined as\n\nV\u1d57\u208a\u2081 = argmax\u1d62\u208c\u2081\u2c7d\u1d50 \u011d\u2c7c,\n\ncorresponding to the view acquiring the highest predicted IG. However, instead of using \u1e7d\u1d57\u208a\u2081 directly as the next camera view V\u1d57\u208a\u2081, an exploration mechanism is employed to maintain an exploration-exploitation balance during online learning. Exploitation involves selecting \u1e7d\u1d57\u208a\u2081 as the next camera view, which prevents wasteful exploration, while exploration entails randomly picking a viewpoint v\u1d63 from the candidate set to avoid the network getting stuck in a suboptimal view selection policy. We devised an exploration mechanism following a commonly used paradigm as in Wang et al. (2016). The mechanism is governed by the exploration rate \u03f5\u1d57 \u2208 [\u03f5\u2098\u1d62\u2099, \u03f5\u1d62\u2099\u1d62]. At the beginning of the entire training process, when t=1, \u03f5\u1d57 = \u03f5\u1d62\u2099\u1d62 was set as the initial exploration rate. Then, \u03f5\u1d57 is decayed by a decay rate \u03c1 every iteration until reaching a minimum \u03f5\u2098\u1d62\u2099 according to \u03f5\u1d57 = max(\u03f5\u2098\u1d62\u2099, \u03c1\u1d57\u207b\u00b9 \u00b7 \u03f5\u1d62\u2099\u1d62). In this work, we used \u03c1=0.95 as a constant. The next viewpoint is then determined as follows:\n\nV\u1d57\u208a\u2081 = { \u1e7d\u1d57\u208a\u2081, if \u03f5\u1d57 < x~U(0,1); v\u1d63 \u2208\u1d63 C, otherwise. }\n\nIf a randomly generated number x from a uniform distribution with bounds 0 and 1 is greater than \u03f5\u1d57, the next viewpoint V\u1d57\u208a\u2081 is set to \u1e7d\u1d57\u208a\u2081, otherwise, it is randomly selected from the candidate set C. This exploration mechanism enables the system to perform extensive exploration in the beginning and gradually shifts towards using self-collected information as the network improves from experiences.\nIn step 3, after the next view is determined, the camera pose is changed to this view, allowing for the collection of a new partial point cloud P\u1d57\u208a\u2081. A detailed explanation of point cloud collection and processing is provided in section 2.1.3.\nIn step 4, based on P\u1d43 and P\u1d57\u208a\u2081, the ground-truth IG g\u1d65\u1d57\u208a\u2081 for viewpoint V\u1d57\u208a\u2081 is calculated and is then converted into G\u1d57 = {0, ..., g\u1d65\u1d57\u208a\u2081,\u2026,0} as the target vector for network training. To achieve continuous online learning in real agricultural scenarios, the robot should be able to calculate g\u1d65\u1d57\u208a\u2081 from its own collected data. To this end, we propose a method enabling ground-truth IG calculation of a viewpoint based solely on P\u1d57\u208a\u2081 collected from this view by robot and the previously collected data P\u1d43. Briefly, g\u1d65\u1d57\u208a\u2081 is calculated as the proportion of points in P\u1d57\u208a\u2081 that provide new information that was not captured in P\u1d43. As shown in Fig. 2, the subset of newly observed points, denoted as P\u207f\u1d57\u208a\u2081 (marked orange), within P\u1d57\u208a\u2081 is obtained by removing the intersection (marked green) with P\u1d43, calculated as P\u207f\u1d57\u208a\u2081 = P\u1d57\u208a\u2081 - (P\u1d57\u208a\u2081\u2229P\u1d43). Then, the ground-truth IG for view V\u1d57\u208a\u2081 is calculated as g\u1d65\u1d57\u208a\u2081 = |P\u207f\u1d57\u208a\u2081|/|P\u1d57\u208a\u2081|, where |\u00b7| denotes the size of the set of points.\nHowever, intersection P\u1d57\u208a\u2081 \u2229 P\u1d43 cannot strictly be calculated using the intersection of the sets, because the corresponding points in P\u1d57\u208a\u2081 and P\u1d43 will not have the exact same coordinates. Instead, we determine P\u1d57\u208a\u2081 \u2229 P\u1d43 by calculating the Euclidean distance between each point in P\u1d43 for each point in P\u1d57\u208a\u2081. If the smallest distance is below a threshold \u03b4, the point is added to the intersection. So, the intersection is defined as:\n\nP\u1d57\u208a\u2081\u2229P\u1d43 = {p\u2096 | p\u2096 \u2208 P\u1d57\u208a\u2081 \u2227 min\u209a\u2c7c\u2208\u209a\u1d43 || p\u2096 - p\u2c7c ||\u2082 \u2264 \u03b4 } (2)\n\nwhere || \u00b7 ||\u2082 is the Euclidean distance between two points. \u03b4 was set at 0.003m in the simulation, while set at 0.01m in the real word. This aligned with the voxel size used for point cloud downsampling (refer to section 2.1.3).\nThis method allows for the calculation of ground-truth IG solely based on the data collected by the robot, facilitating an autonomous training data collection when integrated with the flexibility of a robotic system.\nThe formulated target vector G\u1d57 for network training containing only the sparse ground-truth IG for a single view is called weakly-supervised annotation. The detailed explanation of using weakly-supervised data in network training is provided in section 2.1.4.\nIn step 5, the accumulated point cloud P\u1d43 and the view-selection state V\u1d57 are collected as inputs, along with the target vector G\u1d57, to formulate a training sample b\u1d57 = {P\u1d43, V\u1d57, G\u1d57}, which is then stored in a circular buffer D = {b\u1d57\u207b\u00b9\u208a\u2081, ..., b\u1d57\u207b\u00b9, b\u1d57}, where l\u2208 \u2115 is the maximum number of samples that can be stored. When the capacity of D is reached, the oldest data is replaced by the newest data, maintaining a fixed-size representation of past experiences.\nIn step 6, the accumulated point cloud and the view-selection state are updated. The accumulated point cloud is updated by adding the newly collected point cloud, P\u1d57\u208a\u2081 = P\u1d43\u222aP\u1d57\u208a\u2081 = P\u1d57\u208a\u2081 - (P\u1d57\u208a\u2081\u2229P\u1d43) + P\u1d43 (the calculation of P\u1d57\u208a\u2081 \u2229 P\u1d43 refers to Eq.(2)), and the view-selection state is updated to V\u1d57\u208a\u2081 by setting the s\u1d65\u1d57\u208a\u2081 to 1.\nFinally, in step 7, a subset B\u1d57 \u2282 D is randomly sampled to optimize the weights W\u1d57 of the network, where the subscript R indicates a random selection of training samples. B\u1d57 = {s\u2081, ..., sN} contains N training samples, where N\u2208 \u2115 is the batch size, and network training starts when the stored number of samples in D exceeds the batch size. This method, collecting historical data in a buffer and then sampling data from the buffer to train the network, is called experience replay. Experience Replay is a key technique utilized in online learning to improve sample efficiency through the reuse of historical data and improve training stability by breaking temporal correlations between consecutive steps."}, {"title": "2.1.2. Deep-learning network structure for information gain prediction", "content": "Fig. 3 shows the architecture of the network used in this research, which follows the original paper in which PC-NBV was presented (Zeng et al., 2020). At any iteration step t, the network takes P\u1d57 and V\u1d57 as inputs. First, the point cloud P\u1d43 is processed by a feature extraction network, extracting local features to generate the point-wise feature F0 with 264 features per point. F0 is further processed using max pooling to obtain the global feature G0 of length 264. Subsequently, both G0 and V\u1d57 are duplicated to match the dimension of F0 along the vertical axis. This duplication facilitates the concatenation with F0, resulting in the generation of point-wise features F1, containing both global and local features of the point cloud, as well as the view-section state. F1 is then input to a self-attention unit (Zhang et al., 2019) to further integrate these features, yielding the attention feature F2. Following this, a multi-layer perceptron (MLP1) module with 2 layers of 1024 neurons each following max pooling are applied, producing G1 as the final global feature. G1 represents P\u1d57 and V\u1d57, which is finally fed into another MLP2 with 4 layers of 1024, 512, 256, and 33 neurons each. MLP2 predicts \u011c\u1d57 = {\u011d\u2081, \u2026, \u011dM}, containing the predicted IG for all candidate viewpoints.\nNotably, the original PC-NBV is designed for offline learning and tested in a simulation environment. We modified the training approach, IG calculation metric (step 4 of section 2.1.1), and loss function (section 2.1.4), enabling continuous online learning with weakly-supervised data, showing its capability in real agricultural scenarios."}, {"title": "2.1.3. Point cloud collection and processing", "content": "In the simulation environment, a simulated Intel Realsense L515 RGB-D camera was utilized to capture both color and depth information from the viewpoint, which were then combined to form a partial point cloud. Point cloud downsampling was subsequently performed using the VoxelGrid\u00b9 filter with a voxel size of 0.003m, generating Pc. The choice of voxel size aligned with the work of Burusa et al. (2022), who employed a Voxel-based NBV method for plant reconstruction, enabling direct comparison with their approach. In the real-world experiment, an Intel Realsense L515 RGB-D camera was used to capture point clouds. Real-world point clouds commonly contain various sources of noise due to ambient lighting fluctuations, sensor-specific artifacts, and idiosyncrasies in point cloud generation algorithms. To address this, a three-step noise reduction process was implemented. Initially, a RangeFilter was employed to eliminate points falling outside a specified range. The cropped point cloud then underwent further refinement through the application of the Statistical Outlier Removal (SOR) filter, which identified and removed points significantly deviating from their neighbors when compared to the point cloud's average. Finally, the VoxelGrid filter with a voxel size of 0.01m was applied to further enhance the cleanliness of the point cloud data."}, {"title": "2.1.4. Loss calculation using weakly- supervised data", "content": "In the original PC-NBV, a classic mean square error (MSE) loss function was utilized to calculate the loss L\u02e2 (superscript s represents the loss for strongly-supervised learning) between G\u1d57 and \u011c\u1d57. The G\u1d57 was formulated as G\u1d57 = {g\u2081,\u2026,gM}, encompassing the ground-truth IG for all candidate viewpoints. The L\u02e2 was computed as follows:\n\nL\u02e2 = \u2211\u1d62\u208c\u2081\u1d39 (\u011d\u1d62 - g\u1d62)\u00b2 (3)\n\nThis approach requires each \u011d\u1d62 to be associated with a corresponding g\u1d62 for supervision, or in other words, it demands fully ground-truth labels, using strongly-supervised learning. While valid in simulation environments, this method is inefficient for robotic online learning in real-world scenarios, as robotic motion is time-consuming and obtaining G\u1d57 necessitates the robot traversing all candidate viewpoints in C.\nTo address this issue, we propose a weakly-supervised learning method, enabling training based on coarse-grained ground-truth labels, a specific technique called inexact supervision (Zhou, 2018). We defined ground-truth vector G\u1d57 = {0,..., g\u1d65\u1d57\u208a\u2081,...,0}, where V\u1d57\u208a\u2081 is the next viewpoint. The weakly-supervised loss L\u02b7 is then computed as follows:\n\nL\u02b7 = \u2211\u1d62\u208c\u2081\u1d39 \u03b1\u1d62(\u011d\u1d62 - g\u1d62)\u00b2 (4)\n\nwhere\n\n\u03b1\u1d62 = { 1, if i = V\u1d57\u208a\u2081; 0, otherwise. } (5)\n\nThis method allows the loss to be calculated only based on the ground truth of the next viewpoint V\u1d57\u208a\u2081, significantly enhancing the efficiency of training data collection. However, it may lead to reduced sample efficiency, as only information associated with the next viewpoint is utilized for loss calculation and network optimization. To improve sample efficiency, experience replay is utilized (details refer to step 5 of section 2.1.1)."}, {"title": "2.2. Experimental setup", "content": "Three experimental scenarios were designed to evaluate the performance of our SSL-NBV algorithm:\n(1) Simulated Scenario 1 (experiment S1): This scenario evaluates the network's performance in reconstruction efficiency and quality, IG prediction speed, and training efficiency. We compared our SSL-NBV algorithm with other NBV and non-NBV methods. (Q1).\n(2) Simulated Scenario 2 (experiment S2): Building on S1, this scenario modifies viewpoint settings and target plants to test the method's generalization and adaptability to novel view settings and target plants. In this experiment, We compared weakly-supervised and strongly-supervised learning (Q2), and analyzed performance improvements after online fine-tuning (Q3). The network trained in S1 was used as the starting point for fine-tuning in this experiment.\n(3) Real-World scenario (experiment RW): This scenario involved testing the algorithm in real-world conditions using a robot equipped with an RGB-D camera and real plants (Q4). The neural network trained in S1 was continuously fine-tuned in this scenario.\nTo improve generalization, multiple plants were used during training. The entire training process was separated into many plant reconstruction cycles. Each cycle, a plant with random pose was positioned as the target, and the robot consecutively selects on views from C to reconstruct the plant, collecting training data and updating the network simultaneously. Once a reconstruction was complete after n iterations, a new plant was positioned, the accumulated point cloud and the view-selection state were re-initialized, and a new cycle began. Through preliminary tests, n =10 was set for simulation experiments, where we systematically increased the value, and 10 viewpoints typically allowed good plant reconstruction for our and the baseline methods. For the real-world experiment, n =15 was set due to the higher complexity of real plants. The same value of n was used during testing."}, {"title": "2.2.1. Simulation scenarios", "content": "The simulation was developed using Gazebo (Koenig and Howard, 2004), with data collection and exchange via the Robotic Operating System (ROS) (Quigley et al., 2009). The simulation ran on a ThinkPad P15 laptop with an Intel Xeon W-11855M CPU and an Nvidia GeForce RTX A500 GPU with 16 GB memory, operating on Ubuntu 20.04.\nIn experiment S1 and S2, as illustrated in Fig. 4, the candidate viewpoints set C comprised 33 viewpoints (M=33), arranged in a cylindrical pattern around the origin of the global frame, providing observations of the plant from 11 angles (\u03b1\u2081, ..., \u03b1\u2081\u2081) and 3 heights (h\u2081, h\u2082, h\u2083). These viewpoints were all horizontal and oriented to face the Z axis of the origin. The camera moved freely between viewpoints, providing a 360\u00b0 view of the plant. To create additional variation, each time a plant was created, a plant model was randomly selected from the plant model set and positioned randomly with coordinates dx and dy along the x and y axes, and a rotation \u03b8.\nIn experiment S1, the candidate viewpoints featured a radius r of 0.6m, with different heights h\u2081=0.04m, h\u2082=0.25m, and h\u2083=0.46m. For the random translation, the range for dx and dy selections were set at dx, dy \u2208 U(\u22120.1, 0.1) with a 0.02m interval, and for the rotation was \u03b8\u2208 U(0,360) with a 20\u00b0 interval.\nIn experiment S2, the viewpoint settings and target plants were modified to create a novel environment, testing the online learning and fine-tuning capabilities of the proposed method in a completely novel scenario. The radius (r) was set at 0.5m, and a larger range of random plant positions dx, dy \u2208 U(-0.3, 0.3) was used to introduce more variation in the relative pose between the camera and the plant compared to experiment S1. Three pepper plants (P1-P3 as shown in Fig. 5), significantly different in morphology from tomato plants were used. A K-fold cross-validation method with K=3 was employed during the evaluation. In each round of validation, one plant was used for testing while the remaining two were used for fine-tuning. Each validation round involved 50 repetitions, totaling 150 repetitions (3 plants \u00d7 50 repetitions)."}, {"title": "2.2.2. Real-world scenario", "content": "The real-world setup involved an ABB IRB 1200 robot equipped with an Intel Realsense L515 RGB-D camera attached to its end effector (refer to Fig. 6a), facilitating flexible data collection from various viewpoints. The system was controlled by ROS for robot control and data communication between devices, using the same ThinkPad P15 laptop as simulation.\nDue to the motion limitations of the robot, we sampled M=33 viewpoints in a semi-cylindrical distribution, spanning from -60\u00b0 to 60\u00b0 relative to the x-axis of the robot frame (see Fig. 6b). The radius of the cylindrical sector was 0.45m, with viewpoints at heights of h\u2081=0.75m, h\u2082=1.0m, and h\u2083=1.25m relative to the origin of robot frame. We selected three 40-day-old tomato plants (RT1-RT3, as shown in Fig. 5) as targets. These plants were approximately 55cm in height and had 6-7 composite leaves. The data from the real plants was pre-collected, allowing us to repeat experiments and compare the results with baseline methods.\nFor each plant, the position was fixed at 0.9m, 0m, and 0.65m along the x, y, and z axes of the robot frame, respectively. Four rotations of 0\u00b0 or 90\u00b0 or 180\u00b0 or 270\u00b0 were applied, and the point clouds for all 33 views for each rotation were collected. A k-fold cross-validation method was used to test the approach, with each round of validation using two plants as the training set and one plant as the testing set, resulting in a total of three rounds (k=3) of validation. Each round involved 50 repetitions, leading to a total of 150 repetitions (3 plants \u00d7 50 repetitions) of plant reconstruction. In each repetition, the test plant, set to one of the four rotations, was selected as the target, and the method reconstructed the plant from a randomly selected initial view."}, {"title": "2.2.3. Implementation details of Self- Supervised Learning in experiments", "content": "In all three experiment scenarios, during network training, a batch size N =32 for experience replay was set, and the buffer size was set to l =1000. Before feeding the accumulated point cloud Pa to the network, a downsampling procedure was conducted to resize the point cloud to 512 points, independent of the size of the plants, with each point being randomly chosen from P\u1d43. This choice was based on a preliminary test which indicated that using more points did not improve the network's accuracy in IG prediction and reduced prediction speed and training efficiency and increasing computation and memory demands due to the increased complexity of the network architecture.\nIn experiment S1, the exploration parameters were set to \u03f5\u1d62\u2099\u1d62 = 1.0 and \u03f5\u2098\u1d62\u2099=0.2, with the maximum number of training iterations set to T=50,000. In experiment S2, \u03f5\u1d62\u2099\u1d62=0.2 was reduced to 0.2 since the network was already pre-trained in experiment S1, and T was set to 12,400 to maintain the same number of iterations per plant in the training set (8 for tomato and 2 for pepper). In Experiment RW, \u03f5\u1d62\u2099\u1d62 was set back to 1.0 to allow more exploration due to the substantial differences between simulation and real-world conditions. The maximum iterations were set to T=25,000, which was higher than in experiment S2, considering the increased complexity of real-world plants."}, {"title": "2.2.4. Implementation of baseline planners for experiments", "content": "We compared the performance of our SSL-NBV with other methods:\n(1) Traditional Voxel-based NBV (Burusa et al.", "2022)": "This method converts the point cloud into an occupancy grid and uses ray casting from each candidate viewpoint to calculate the IG for the next-best view. To enhance the speed of view planning, the occupancy grid updates and ray casting processes are executed using GPU acceleration. The parameters used for ray casting, IG calculation and occupancy calculation follow the original paper"}]}