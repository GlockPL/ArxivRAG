{"title": "Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to\nDetect Machine Generated Text", "authors": ["Seyedeh Fatemeh Ebrahimi", "Karim Akhavan Azari", "Amirmasoud Iravani", "Arian Qazvini", "Pouya Sadeghi", "Zeinab Sadat Taghavi", "Hossein Sameti"], "abstract": "Detecting Machine-Generated Text (MGT)\nhas emerged as a significant area of study\nwithin Natural Language Processing. While\nlanguage models generate text, they often leave\ndiscernible traces, which can be scrutinized\nusing either traditional feature-based methods\nor more advanced neural language models.\nIn this research, we explore the effectiveness\nof fine-tuning a RoBERTa-base transformer,\na powerful neural architecture, to address\nMGT detection as a binary classification\ntask. Focusing specifically on Subtask A\n(Monolingual - English) within the SemEval-\n2024 competition framework, our proposed\nsystem achieves an accuracy of 78.9% on\nthe test dataset, positioning us at 57th\namong participants. Our study addresses\nthis challenge while considering the limited\nhardware resources, resulting in a system that\nexcels at identifying human-written texts but\nencounters challenges in accurately discerning\nMGTS.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models\n(LLMs) have endowed them with an impressive\ncapability to generate written text that closely\nresembles human writing (Adelani et al., 2019;\nRadford et al., 2019). However, this technological\nprogress brings along significant challenges, as\nthe proliferation MGT poses various threats in\ndigital environments. MGTs have been implicated\nin spreading misinformation in online reviews,\neroding public trust in political or commercial\ncampaigns, and even facilitating academic fraud\n(Crothers et al., 2022; Song et al., 2015; Tang\net al., 2023). The identification of MGT remains\na pressing concern, as distinguishing between\nhuman-written and machine-generated content\nis often challenging for humans. Consequently,\nthere is a growing imperative to develop automatic\nsystems capable of discerning MGT (Mitchell\net al., 2023). In this study, we address this\nchallenge within the English language context\nusing the dataset provided by Wang et al. (2023)."}, {"title": "2 Background", "content": null}, {"title": "2.1 Dataset Overview", "content": "SemEval-2024 Task 8 (Wang et al., 2024b)\ncomprises three subtasks, with our investigation\ncentering on Subtask A: binary classification\nof human-written versus MGT. Specifically, we\nconcentrated our efforts on analyzing English\nmonolingual data, as outlined dataset is provided\nby Wang et al. (2023).\nSubtask A encompasses a dataset consisting of\n119,757 training examples and 5,000 development\nexamples, all presented in JSON format. Each data\ninstance includes the following attributes:\n\u2022 id: An identifier number for the example.\n\u2022 label: A binary label indicating whether\nthe text is human-written (0) or machine-\ngenerated (1).\n\u2022 text: The actual textual content.\n\u2022 model: The AI machine responsible for\ngenerating the text.\n\u2022 source: The web domain from which the text\noriginates."}, {"title": "2.2 Related Work", "content": "MGT detection is feasible through both traditional\nfeature-based methods and neural language models.\nFr\u00f6hling and Zubiaga (2021) and Nguyen-Son\net al. (2018) discussed how feature-based methods\nleverage statistical techniques. These methods\nprimarily utilize frequency features such as\nTF-IDF, linguistic cues, and text style (Fr\u00f6hling\nand Zubiaga, 2021). However, feature-based\nmethods have limitations, as different samplings\nin language models can lead to varied generated\noutputs (Holtzman et al., 2019). In contrast,\nmethods that harness neural language models,\nparticularly those employing transformer models,\nhave shown high effectiveness (Crothers et al.,\n2022). Neural language model methods often\ninvolve zero-shot classification or fine-tuning\npre-trained language models (Sadasivan et al.,\n2023). Grover by Zellers et al. (2019), RankGen\nby Krishna et al. (2022), and DetectGPT (Mitchell\net al., 2023) are prominent examples of zero-\nshot methods. However, these methods may\nbe misleading at times and exhibit limited\nperformance in out-of-domain tasks (Crothers\net al., 2022; Wang et al., 2023).\nBakhtin et al. (2019) demonstrated outstanding\nperformance in MGT detection by harnessing\nbidirectional transformers. Additionally, Solaiman\net al. (2019) highlight that the zero-shot methods\noften fall short compared to a simple TF-IDF\nbaseline when detecting texts from diverse\ndomains. He argues that bidirectional transformers\noffer significant advantages for MGT detection,\nadvocating for the fine-tuning of these models\nas a superior alternative to zero-shot methods.\nIn this regard, Rodriguez et al. (2022) observed\na significant enhancement in performance of\ncross-domain MGT detection by fine-tuning the\nROBERTa detector.\nJawahar et al. (2020) conducted a comprehensive\nsurvey of various approaches to developing MGT\ndetectors. Their findings suggest that fine-tuning\nthe ROBERTa detector consistently delivers robust\nperformance across diverse MGT detection tasks,\nsurpassing the efficacy of traditional machine\nlearning models and neural networks. Additionally,\nCrothers et al. (2022) reported a notable trend\ntowards the increased utilization of bidirectional\ntransformer architectures, particularly RoBERTa,\nin MGT detection tasks. Lastly, Wang et al.\n(2024a) conducted a comprehensive benchmark\nof supervised methods on M4 dataset. Their\nfindings revealed that transformer models such\nas ROBERTa and XLM-R exhibited superior\nperformance across all tests, respectively achieving\n99.26% and 96.31% accuracy in MGT binary\nclassification."}, {"title": "3 System Overview", "content": "This section presents an overview of our system's\narchitecture, highlighting implementation details\nand challenges. Drawing on the preceding works\ndiscussed above, which showed the efficacy of\nfine-tuning RoBERTa models, our system aims to\nattain peak performance in MGT detection while\noptimizing configurations for limited hardware\nresources.\nThe decision to employ the transformer architecture\nfor detecting synthetic texts is motivated by\nits capacity to capture intricate dependencies\nwithin textual data. This choice seems logical\nconsidering that such texts often exhibit semantic\nfeatures that can be harnessed for fact-checking,\ncohesion, coherence, and other properties that\nmay unveil their origin (Raj et al. (2020). In\ncontrast to traditional architectures, the transformer\nmodel overcomes the constraints of fixed window\nsizes or sequential processing, enabling it to\nutilize contextual information from the entire\ninput sequence. Additionally, the self-attention\nmechanism empowers the model to selectively\nfocus on pertinent segments of the input, rendering\nit highly effective for tasks necessitating long-range\ndependencies and contextual comprehension.\nAs for ROBERTa, it is specifically chosen for\nits extensive training duration, broader dataset\ncoverage, ability to handle longer sequences, and\nfocus on Natural Language Understanding tasks,\nmaking it more suitable than other BERT-based\nmodels. Additionally, a wealth of research, such as\nthe recent study of Wang et al. (2024a), has further\nhighlighted the inherent potential of ROBERTa for\nthis specific task."}, {"title": "3.1 Core Algorithms and System Architecture", "content": "At the core of our system lies the concept of binary\nclassification, distinguishing input texts as either\nmachine-generated or human-written through\nfine-tuning a pre-trained RoBERTa transformer\n(Liu et al., 2019). Our system architecture\nentails augmenting the RoBERTa-base model\nwith a Classifier Head. The RoBERTa model's\nEmbeddings component incorporates a 768-\ndimensional embedding matrix, alongside position\nand token type embeddings, enhancing contextual\nunderstanding. The Encoding component\nfeatures a 12-layer RoBERTaEncoder, each layer\nemploying a multi-head self-attention mechanism.\nThis facilitates simultaneous attention to different\nparts of the input text, crucial for analyzing textual\nsimilarities. Intermediate sub-layers utilize a\nfully connected feed-forward network with GELU\nactivation, followed by an output sub-layer for\nfeature transformation and normalization.\nThe Classifier Head, integrated into the Encoder\nfor sequence classification, comprises a linear layer\nwith 768 input features and a dropout layer to\nmitigate over-fitting. The final output is generated\nthrough an additional linear layer with a solitary\noutput neuron, making it conducive to binary\nclassification tasks. In essence, the primary model\nprocesses input data, with the Classifier Head\nmaking predictions. When viewed as a regression\ntask, the Classifier produces a linear output tailored\nfor a singular class, providing a probabilistic value.\nImplementation of the system is facilitated using\nPyTorch, incorporating specific parameters such as\nthe AdamW optimizer and the CrossEntropyLoss function . AdamW, renowned for training\ndeep neural networks, integrates weight decay\nto mitigate over-fitting. The Cross Entropy\nLoss function, commonly employed in multi-\nclass classification scenarios, combines softmax\nactivation with negative log-likelihood loss. The\ntraining process involves iterating through the\nentire dataset for two epochs, with early stopping\nmechanisms in place to terminate training at the\noptimal point."}, {"title": "3.2 System Challenges", "content": "While larger machine-generated documents often\nexhibit more discernible patterns and clues, such\nas incoherence or repetition, they also entail\nsubstantial computational costs. Our primary\nchallenge lay in efficiently processing these large\ndocuments using cost-effective computing systems.\nTo mitigate this challenge, we explored strategies\nsuch as reducing token size and batch size.\nHowever, these adjustments necessitate trade-\noffs, potentially leading to reduced accuracy or"}, {"title": "4 Experimental Setup", "content": null}, {"title": "4.1 Dataset", "content": null}, {"title": "4.2 Pre-processing and Hyper-Parameter\nTuning", "content": "Input texts are tokenized using the RoBERTa\ntokenizer before processing, both during training\nand inference. Our hyper-parameter tuning\nprocess involved a comprehensive exploration\nacross various parameter ranges. Specifically, we\nconducted experiments with learning rates ranging\nfrom 0.0001 to 0.00004, dropout rates spanning\nfrom 0.1 to 0.3, batch sizes varying between\n4 and 16, and token sizes ranging from 64 to\n1024. Through experimentation and analysis, we\ndetermined the optimal hyper-parameter settings,\nwhich are as follows: a learning rate of 0.00004, a\ndropout rate of 0.1, a token size of 512, a batch size\nof 10, and a weight decay of 0.01. Further details\nare given in Appendix A.\nAs illustrated in Appendix A, the number of\ntraining instances is correlated with the input\ntoken size and may influence the model accuracy.\nGiven the length of input texts, a suitable token\nsize is essential to capture all tokens adequately.\nHowever, computational costs associated with\nlarger token sizes present a significant challenge\nduring model training. Consequently, we selected\n512 as the optimal token size. Truncation was\nemployed during tokenization to accommodate the\nchosen token size, ensuring efficient model training\nwithout compromising data representativeness."}, {"title": "4.3 Training Procedure", "content": "For training the model, we utilized the Task\ndataset Wang et al. (2023), which underwent\npreprocessing by tokenizing the text into sub-word\nunits and padding sequences to a fixed length.\nCrossEntropyLoss was employed as the loss\nfunction. The implementation also involved the\nAdamW optimizer, known for its effectiveness in\ntraining deep neural networks and its incorporation\nof weight decay to address over-fitting. The Adam\noptimizer was utilized with a learning rate of 4e-\n05. During training, the loss was monitored on\na held-out validation set, and early stopping was\napplied to prevent over-fitting. Early stopping was\nimplemented with the condition that the training\nloss reached a specific threshold (0.35 in this\ncase), typically occurring around the third epoch.\nTherefore, if there was no improvement in the\nvalidation loss for a certain number of epochs,\ntraining was halted to prevent over-fitting of the\nmodel."}, {"title": "4.4 Evaluation Measures", "content": "The evaluation of our model involves calculating\nits accuracy in predicting whether a text is\nhuman-written or machine-generated. Accuracy, a\nfundamental metric in classification tasks, assesses\nthe overall correctness of predictions and is\ncalculated as:\n$Accuracy = \\frac{ni}{N} \\times 100$ (1)\nwhere ni represents the number of correctly\nclassified instances, and N is the total number of\ninstances."}, {"title": "5 Results", "content": "Using the official accuracy metric of SemEval-\n2024 Task 8 (Wang et al., 2024b), our system\nachieved the following accuracy scores on different\ndata splits:\nA direct comparison of our results with prior\nworks is challenging due to the unique nature of"}, {"title": "6 Conclusion", "content": "In summary, our study focused on fine-tuning\na ROBERTa-base transformer model for binary\nclassification, specifically in distinguishing human-\nwritten from MGT. While our system showed\npromise in identifying human-written text, it faced\nchallenges with accurately classifying machine-\ngenerated content. As discussed in Appendices\nA and B, we recommend exploring larger token\nsizes to improve model performance, albeit with"}, {"title": "A Hyper-Parameter Tuning", "content": "To determine the appropriate settings for\nhyper-parameters, we utilized Google Colab's\nfree GPU runtime. Free Colab users have access\nto GPU and TPU runtimes without charge for a\nmaximum of 12 hours. The GPU runtime includes\nan NVIDIA Tesla K80 with 12GB of VRAM.\n[Date: 5 Dec 2023]. We were unable to use\npremium runtime accounts due to financial issues\narising from Iran sanctions. Therefore, we couldn't\nchange our model's token size to larger than 512\ndue to the 12-hour time limit in free Colab. To\nunderstand the impact of increasing token size, we\naimed to experiment on a local laptop GPU.\nDuring the experiments aimed at finding the\nproper token size, we encountered the \"CUDA\nerror: device-side assert triggered\" frequently,\nwhich was resolved by restarting the session.\nOur experiments were conducted using an RTX\n2060 mobile with 6 GB of VRAM. Throughout\nall experiments, we maintained fixed parameters,\nincluding Number of Epochs = 3, Train Split =\n0.7, and Learning Rate = 4e-05. Increasing the\nMax Length from 512 to 1024 in this experimental\nsetup resulted in an improvement in Test Accuracy\nby at least 2%. However, this enhancement came\nat the cost of a nearly 15-fold decrease in training\nspeed, making it challenging to implement on\nlimited hardware. Additionally, this requires plenty\nof controlled experiments by researchers to shed\nlight on finding the proper hyper-parameters."}, {"title": "B Detect-GPT as a Zero-Shot Method", "content": "In our pursuit of effective MGT detection,\nwe also experimented with Mitchell et al.\n(2023)Detect-GPT model, a zero-shot approach\nutilizing probability curvature analysis. Training\nthe model resulted in an accuracy rate of\n60%, and when applied to a test dataset\nof approximately 1500 samples, it achieved\na remarkable accuracy of approximately 84%.\nWe conducted a comprehensive analysis by\nimplementing 10 perturbations for each dataset.\nTo address data and mask filling tasks, we\nemployed the T5 small model, leveraging its\nrobust capabilities. Furthermore, to accurately\nassess the log likelihood, we utilized the GPT-2\nmodel, ensuring precise calculations and reliable\nresults. This method surpassed alternative text\ndetection methodologies, demonstrating superior\naccuracy and reliability in identifying MGT.\nNotably, the inclusion of threshold configuration\nadded granularity to the experiment, enabling\nfine-tuning of detection sensitivity across varying\nthreshold settings."}]}