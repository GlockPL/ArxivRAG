{"title": "Using Formal Models, Safety Shields and Certified Control to\nValidate AI-Based Train Systems", "authors": ["Jan Gruteser", "Jan Ro\u00dfbach", "Fabian Vu", "Michael Leuschel"], "abstract": "The certification of autonomous systems is an important concern in science and industry. The KI-\nLOK project explores new methods for certifying and safely integrating AI components into au-\ntonomous trains. We pursued a two-layered approach: (1) ensuring the safety of the steering system\nby formal analysis using the B method, and (2) improving the reliability of the perception system\nwith a runtime certificate checker. This work links both strategies within a demonstrator that runs\nsimulations on the formal model, controlled by the real AI output and the real certificate checker. The\ndemonstrator is integrated into the validation tool PROB. This enables runtime monitoring, runtime\nverification, and statistical validation of formal safety properties using a formal B model. Conse-\nquently, one can detect and analyse potential vulnerabilities and weaknesses of the AI and the cer-\ntificate checker. We apply these techniques to a signal detection case study and present our findings.", "sections": [{"title": "Introduction and Motivation", "content": "Artificial intelligence (AI) is increasingly used in safety-critical applications such as autonomous driv-\ning [33] and autonomous flying [26, 20]. While AI can be effective for many challenging tasks, it also\nintroduces new risks and concerns. This leads to new challenges regarding certification and ensuring the\nsafety of AI components (see, e.g., Peleska et al. [28] in the context of autonomous railway systems).\nThis work deals with systems that employ an AI perception system, such as image recognition.\nThose systems include autonomous vehicles and autonomous railway systems. In earlier work [14],\nwe formally verified a steering system, assuming the perception system works perfectly. However, as\nthe perception system is imperfect, we also created simulations with (hand-coded) probabilities for all\nkinds of erroneous detections. We then applied Monte Carlo simulation to estimate the likelihood of\nsafety-critical errors. As a concrete case study, we applied those techniques to an AI-based railway\nsystem [14] using PROB [23] and SIMB [35]. In this paper, we move towards using the real AI within\nthese simulation and validation runs, rather than using estimated error rates. As outlined by Myllyaho\net al. [25], fully virtual simulation enables validation of the system in dangerous situations without real\ndanger. Although the validity of the simulator is difficult to verify, simulation still helps as a validation\nmethod to initially assess the quality of the system under evaluation.\nThe AI perception system itself is based on the widely-used YOLO [29] architecture, making the\nsystem difficult to verify with formal methods alone. To address this, we implemented a runtime certifi-\ncate checker using classical computer vision algorithms to verify the output of the AI [31]. This checker\ncan be certified using classical techniques (e.g., [4]).\nThis work presents a real-time demonstrator linking the formal model, the AI, and certified control,\nextending an approach used to validate reinforcement learning agents [34]. AI now controls the simula-\ntion directly by executing events for the perception system in the formal model. With our approach, we\ncapture real AI behaviour and can use two runtime monitoring techniques: 1) the formal B model acts as\na safety shield for the steering system (e.g., to detect false negatives), 2) and the certified control mon-\nitors the perception system (detecting false positives). We demonstrate this methodology on a railway\ncase study [14], and discuss our findings and the challenges of the approach."}, {"title": "B Method, ProB, and SimB.", "content": "The B method [1] is a state-based formal method for specifying and\nverifying software systems. The B method is based on first-order logic and set theory and has been used\nindustrially for over 25 years [3] to generate software that is \u201ccorrect by construction\" [7, 10], and for\nsystem-level safety modelling. For the latter, the B method has been used for many railway applications,\nsuch as ETCS Hybrid Level 3 [16, 24] and CBTC systems [5, 6]. In this article, we use the B method to\nmodel autonomous train control in a shunting yard, based on the model from [14].\nPROB [23] is an animator, constraint solver, and model checker for formal models. It supports\nvarious formal languages including the formal B method. SIMB [35] is a simulator built on PROB's\nanimator, supporting real-time simulation and Monte Carlo simulation. SIMB can be linked with external\nsoftware components [34]. We use PROB and SIMB in this article to run the steering system and the\nsafety shield, and also for validating the entire system."}, {"title": "Certified Control.", "content": "Certified control [19] is a runtime monitoring approach\nto ensure the safety of the perception system in autonomous vehicles. Unlike\nconventional monitoring methods, certified control does not rely on inde-\npendent perceptions. Instead, a controller provides a certificate containing\nall essential information to prove formal properties. This certificate may\nbe generated by a sophisticated AI algorithm, which does not need to be\nformally verified. Using this certificate, the runtime monitor verifies if the\nspecified criteria hold for the provided data. This monitor can, in contrast\nto the AI system, be comparatively small and deterministic. The architec-\nture establishes a trusted foundation that can potentially be subjected to a\nrigorous formal verification process.\nIn previous tests, this technique almost eliminated all false positive de-\ntections, in exchange for rejecting some true positives [31]. The bounding\nbox of the sign detected by the YOLO model is cropped from the image and\npassed to the runtime monitor, which uses various computer vision tech-\nniques, e.g. contour detection, to validate the sign for expected features. If\nthe desired features are not recognised, the detection is rejected."}, {"title": "Linking Formal Model, AI, and Certified Control", "content": "This section describes how we link together the formal model, the AI, and the certificate checker. In\nthe formal model, we formally specify and verify the steering system which includes safety shields\nto prevent unsafe operations based on the Al's perception and the known environment. In previous\nwork [14] on validating an AI-based train control system, we encoded probabilities for false positive and\nfalse negative detections of the AI by hand. Now, we use real AI components and real certificate checkers\nfor simulation. Figure 2 gives an overview of how we link the formal model, the AI, and certified control\ninside a real-time demonstrator with runtime monitoring/verification.\nIn our case study, the AI-based perception system processes the environment in form of images at\nruntime. Ideally, various techniques should be employed to ensure that the AI is trained correctly and\nperforms well (see [30] and references therein). Additionally, we use certified control to monitor the\nperception system, and detect false positives (i.e., the monitor will reject detections which it cannot\nconfirm). The output of the perception system and the certificate checker are then synchronised with the\nformal model. The simulated environment, including the image provided to the AI, must correspond to\nthe formal model's current state. This is a significant challenge as discussed in Section 3.3. The formal\nmodel contains events for both the steering system and the perception system. The model should be safe\nunder the assumption that the perception system works perfectly. Furthermore, the formal model can be\nused as a basis for a safety shield which enforces safe actions on the steering system. The shield can\ndisallow unsafe actions, like driving through a detected stop signal. The safety shield can also detect\nfalse negatives. For example, when the AI detects no signals but the formal model \u201cknows\u201d that a signal\nmust be visible at the current location, it can enforce a safe fallback action (like stopping)."}, {"title": "Case Study: AI-based Signal Detection", "content": "We apply the presented technique to a case study provided by our project partners (see [14]). For this\ncase study, we developed a formal B model [14], consisting of an environment, the steering system, and\nthe perception system. The environment includes obstacles, points (aka switches) and signal states, field\nelements and movements of the steered train. The formal model abstracts away the AI-based perception\nsystem by events that represent possible outcomes of the object detection, including correct, false positive\nand false negative detections.\nThe objective, or the \u201cmission order\", is to drive autonomously from the starting position through\na small shunting yard to the destination without dangerous situations or at least as safely as human\ndrivers (cf. [14, PROB1-2]). The focus of this work is the detection of signal aspects during the shunting\nmovement.\""}, {"title": "Implementation", "content": "We collected images from multiple videos of the case study track containing the various signal aspects,\ne.g., stop signals, permission signals, and no signals along the route to capture an interactively changing\nenvironment. Based on the position of the train and the state controlled by the formal model, an image\nwith the appropriate signalling aspect is randomly selected from the corresponding collection. For our\nexperiments, we assume that a signal becomes visible as soon as the train is closer than 10 distance units\n(freely selectable). The procedure for a simulation step is as follows:\n1. Pick an image randomly depending on the current train location and signal states in the B model\n2. Pass the image to our fine-tuned YOLOv8 model (cf. [31]); based on the result: if no signal has\nbeen detected: ignore and do not execute any operation in the B model; if a signal has been\ndetected:\n\u2022 Correct signal (corresponding B event is enabled): execute VIS_DetectCorrectSignal\n\u2022 Wrong stop signal: execute VIS_DetectWrongStopSignal\n\u2022 Wrong permission signal: execute VIS_DetectWrongPermissionSignal\n3. Execute event for environment change, e.g., switch signals or activate derailer, with a probability of\n25% or move train forwards and update controller with a probability of 75% (environment changes\nshould occur less frequently than train movements)\nThe simulation runs in a loop until reaching the ending condition which is later explained in our experi-\nments.\nThe controller of the steering system is updated after each detection to recompute the maximum\nallowed movement distance. Since the simple object detection AI does not provide positioning informa-\ntion, we place all detections in the formal model at a fixed distance in front of the train. In the second\nstep, we (optionally) apply the certificate checker which monitors the output of the AI by accepting or\nrejecting its detections.\nFor the AI to run the simulation, we use SIMB's interface for external simulation [34]."}, {"title": "Experiments and First Results", "content": "For initial experiments, we encoded a safety shield in the B model that only allows for signal detections\nat known positions of signals. If no signal is detected at an expected position, the B operations for train\nmovements are disabled so that the train falls back to safe mode and stops in front of the signal. This\nassumes that we have a map of the shunting yard and know at which locations the signals are located. We\nthen analyse the behaviour using SIMB's real-time simulation which is now controlled by the AI and the\ncertificate checker. For better understanding, we use the domain-specific VisB visualisation [36] from\nprevious work [14]. Both tools are part of PROB2-UI [2]; an illustration is shown in Figure 3.\nWith SIMB, we also run Monte Carlo simulation with 500 runs for all combinations with/without\nsafety shield, and with/without certified control. We also investigated the effect of not applying the\ncertificate checker to stop signals, so that these cannot be falsely rejected and the train always enters a\nsafe state (stop). The termination condition has been defined so that a simulation stops when a safety-\ncritical situation occurs or when the train can no longer proceed, either due to its arrival at a stop signal or\nreaching the end. For each execution run, we estimate the maximum (safe) distance travelled to validate\nthat the train does move forward (not driving at all would be 100 % safe, but not useful). Furthermore,\nwe estimate the likelihood of a safety-critical situation where an accident might occur, similar to [14],\nusing the safety properties SAF1-5 1."}, {"title": "Challenges", "content": "The main benefit of our methodology is that we can link the execution of a real AI model to a formal\nmodel and check its behaviour using formal properties and statistical validation techniques. However,\nthere are still many challenges and limitations.\nA major issue is to match the real environment provided to the AI with the environment's state in\nthe formal model. This requires an interactive simulation environment with control of the environment\n(signals, points, etc.) and control of the train, to ensure that the actuators controlled by the formal model\nare taken into account by the simulated environment. We have conducted some successful experiments\nwith a commercial train simulator, but since it is not designed for discrete states, as required by the\nformal model, the handling is complicated (apart from the fact that a new instance of the simulator\nwould have to be set up for each simulation run). Simply using a video as input is not sufficient either,\nas the environment remains static and we have no control over the movement of the train. In our current\nexperiments, we avoid this problem by sampling images from videos fitting to the current state. Although\nthis is sufficient to demonstrate the concept, we have not yet simulated real train rides. This requires a\nsimulation environment for configurable scenarios (with PROB we are already able to load and visualise\nflexible scenarios, e.g., via a standardised data exchange format such as railML [15]). While there are\nalready established tools in the automotive sector, e.g., CARLA [8], such tools are still rare in the railway\nsector, but are under development [9, 13, 37]."}, {"title": "Related Work", "content": "There are many approaches to verifying neural networks [21, 32, 18]. In practical application, however,\nit is challenging for these techniques to scale to large neural networks. Another technique is robustness\nchecks [11, 12] which also work on neural networks. Robustness checks aim to ensure the safety of the\nAI directly, while this work employs safety boxes around the AI. For instance, the perception system\ncould be unsafe, but is monitored by a certificate checker. Similarly, the steering system could make\nunsafe decisions based on the perception, but is monitored by a safety shield encoded in the formal\nmodel.\nAnother work presented by Pasareanu et al. [27] abstracts away perception components, and replaces\nthem with a probabilistic component that estimates their behaviour. In particular, the probabilities are\nderived from confusion matrices computed for the underlying neural network. Finally, the verification\nis done by probabilistic model checkers such as PRISM [22] and STORM [17]. To improve safety,\nPasareanu et al. [27] employ run-time guards which are used as runtime monitors.\nInstead of estimating the probabilities for real AI behaviour, this work simulates real AI behaviour\nat runtime. This means that the perception system operates at runtime with real images and provides\nthe detection to the validation tools PROB and SIMB. Alternatively, we could have extracted confusion\nmatrices and encoded them as probabilities into the simulation."}, {"title": "Conclusion and Outlook", "content": "This work successfully links a formal model, AI, and certified control to a real-time demonstrator. We\ndemonstrated the technique in an AI-based train system. The methodology consists of the following\nsteps: (1) formally specify and verify the steering system using formal methods, (2) encode safety shields\nin the formal model to prevent unsafe operations, (3) use real AI for simulation, (4) add a runtime\ncertificate checker of the AI outputs to reduce false positive detections, and (5) link all components to\nsimulate the formal model with the AI and the certificate checker's output. Using the tools PROB and\nSIMB, we then evaluate the performance of the AI with (and without) certified control and safety shield.\nWith SIMB, we can make statistical statements about the formal properties of the whole system. We\nanalysed the likelihood of unsafe situations and identified weaknesses in our AI and certificate checker.\nWith our approach, we can identify issues early during development. The results are then used to improve\nthe AI or the certificate checker, followed by further validation.\nIn our case study, we identified false negatives of certified control as a cause of unsafe situations. In\nfuture, we thus need to improve our safety shield, to better protect against such false negative detections\nand reduce the error rates to levels required for certification. Other future improvements are to link our\ntool with a realistic simulation environment, e.g., in the form of co-simulation."}]}