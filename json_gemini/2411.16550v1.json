{"title": "Representation Collapsing Problems in Vector Quantization", "authors": ["Wenhao Zhao", "Qiran Zou", "Rushi Shah", "Dianbo Liu"], "abstract": "Vector quantization is a technique in machine learning that discretizes continuous rep-resentations into a set of discrete vectors. It is widely employed in tokenizing data represen-tations for large language models, diffusion models, and other generative models. Despiteits prevalence, the characteristics and behaviors of vector quantization in generative modelsremain largely underexplored. In this study, we systematically investigate the issue of col-lapses in vector quantization, where collapsed representations are observed across discretecodebook tokens and continuous latent embeddings. By leveraging both synthetic and realdatasets, we identify the severity of each type of collapses and triggering conditions. Ouranalysis reveals that random initialization and limited encoder capacity result in tokenscollapse and embeddings collapse. Building on these findings, we propose potential solu-tions aimed at mitigating each collapse. To the best of our knowledge, this is the firstcomprehensive study examining representation collapsing problems in vector quantization.", "sections": [{"title": "1 Introduction", "content": "Vector Quantization (VQ) technique (Gray, 1984), which discretizes data representations,has become a widely used tool in different aspects of machine learning, including but not"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Vector Quantization", "content": "We define the VQ-VAE as following: an encoder Ee, a decoder De, and a set of tokensT = {t1, t2,...,ts}. The token set T constitutes the codebook, which is employed tostore the discretized representations. The encoder is responsible for mapping the rawdata X = {x1,x2,...,xn} to a set of continuous representations Z = Ee(X), whereZ = {z1, z2,...,zn}. And the decoder reconstructs the data X' = Do(Z) based on theset of discretized representations \u017b, where \u017b = {\\hat{z}_1, \\hat{z}_2, ..., \\hat{z}_n}. The process of tokenizinga continuous representation zj to discrete representation \\hat{z}_j is as following:\n\\hat{z}_j = \\arg \\min_{t_k \\in T} ||z_j - t_k||,                                          (1)\nwhere tk is a token in token set T and k is the index of tk in the codebook. This quanti-zation is performed by finding the nearest token tk in T. In addition, to differentiate from\"codebook collapsing\", which denotes the problem of low usage rates of codes within thecodebook, we employ another widely used term \"token\" instead of \"code\" to represent thediscrete representations in the codebook."}, {"title": "2.2 Experiment Settings", "content": "We conduct experiments on synthetic and real-world data to show the two types of collapsesphenomena and further investigate the causes as well as validate our proposed solutions.Our synthetic dataset comprises ten equally-sized classes. This uniform class distributionaims to highlight disproportionate tokens allocation and facilitate observation of collapsepatterns. To examine collapse behavior under varying data complexity, we generate syntheticdatasets across multiple dimensionalities, employing t-SNE dimensionality reduction for"}, {"title": "3 Collapsing Problems and Solutions", "content": ""}, {"title": "3.1 Tokens Collapse", "content": "Tokens collapse manifests as a disproportionate concentration of the tokens distributionaround a subset of the encoder output embeddings. This collapse results in a poor rep-resentation since the ideal scenario requires a fitting distribution of tokens that effectivelyaligns with the underlying embedding space. As demonstrated in Fig. 3 (a), (c), and (e),embeddings assigned only a few tokens exhibit severely insufficient representations, whichleads to the corresponding reconstructed distribution being narrower, compared to the re-construction of more equitable tokens distribution (Fig. 3 (b), (d), and (f)), indicating theloss of diversity.\nWhat causes tokens to collapse? We observe that during the initialization of codebook,tokens often cluster within a very small range, leading to early tokens collapse. As shownin Fig. 2, compared to trained encoder (Fig. 2 (b)), the distribution of untrained encoder'soutputs cluster between -0.10 to 0.20 (Fig. 2 (a)) and has only 5 obvious peaks while theinput dataset contains 10. This phenomenon is primarily due to using outputs from anuntrained encoder for initialization: an untrained encoder fails to understand the inputdata, resulting in most data being encoded into similar embeddings.\nBuilding on these observations, we hypothesize that if tokens are initialized based onencoder that has learned semantic distinctions and its output embeddings are dispersed,it would enhance the semantic distinction among tokens and thus control tokens collapse.Consequently, we propose a straightforward yet effective method to mitigate tokens collapse:pretrain without VQ, then fine-tune with VQ. It first trains an autoencoder, and thentrains the VQ-VAE initialized with the weight of the autoencoder trained at the first stage.Pretraining the encoder allows it to discern differences in input data, resulting in moredistinctly spaced embeddings, providing a robust foundation for initializing the tokens, asdemonstrated in Fig. 2 (b)."}, {"title": "3.1.1 EXPERIMENTS ON SYNTHETIC DATASET", "content": "To validate our hypothesis regarding the causes of tokens collapse and the effectiveness ofour proposed solution, we conducted ablation studies with and without pertaining underdifferent input data and token dimensions. In Fig. 3, settings (a), (c), and (e) representcommonly used methods without pertaining, compared to the settings (b), (d), and (f)"}, {"title": "3.2 Embeddings Collapse", "content": "As discussed in the introduction, insufficient parameters of the encoder could lead to em-beddings collapse during the VQ process. This observation suggests the importance of ap-propriately sizing models and offers insights and rationale for balancing model complexitywith computational efficiency to optimize comprehensive performance and prevent collapsesin VQ applications.\nWe conducted an in-depth investigation and validation of this issue on both syntheticdataset and the real-world dataset. We adjusted the encoder's capacity by reducing theparameters of the standard VQ-VAE's encoder (without using our pretraining solution).The overall results demonstrate that insufficient encoder capacity is able to result in theembeddings collapse problem.\nExperiment results on synthetic data, as shown in Fig. 5 (b), show that with an encoderhidden size of 32, the distribution of embeddings exhibits clear differentiation, which aids inthe learning of discrete representations (tokens). Consequently, the reconstruction resultsdo not contain any outputs that fall outside the distribution of the original data. However,when the encoder's capacity is insufficient (hidden size=4), as shown in Fig. 5 (a), theoutput embeddings from the encoder tend to have most of their peaks merged together,and parts of the tokens distribution lie outside the embeddings distribution. This leadsto the embeddings collapse issue that reconstruction contains erroneous results which falloutside the original input data distribution.\nWe further investigate embeddings collapse problem on real-world dataset, CIFAR-10.We adjust encoder's capacity by changing the hidden layer size of the encoder. As shownin Fig. 6, it can be observed that decreasing the size of the encoder undermines the re-construction performance with increased MSE, indicating that insufficient capacity of theencoder can lead to embedding collapse on real-world data.\nTherefore, for specific applications of VQ, a guiding suggestion is to ensure the modelhas sufficient capacity, such as by adding an additional network layer before quantization."}, {"title": "4 Related Works", "content": "Vector Quantization is foundational in data compression and signal processing per Shan-non's rate-distortion theory (Gersho and Gray, 2012) (Cover, 1999), traditionally relied onmethods like K-means clustering (Macqueen, 1967) but faced high complexity with high-dimensional data (Le Tan et al., 2018). To mitigate this challenge, DeepVQ (Le Tan et al.,2018) improved efficiency by mapping data to lower-dimensional latent spaces before quan-tization. Moreover, (Van Den Oord et al., 2017) proposed VQ-VAE which integrates VQwith variational autoencoders, using a straight-through estimator (Bengio et al., 2013) tohandle discrete variables. To refine VQ methods for improved performance, variants suchas Residual Quantization Lee et al. (2022), Product Quantization (Chen et al., 2020), andSoft Convex Quantization (Gautam et al., 2023) further enhanced representation capacityand efficiency. Recent advances incorporate attention mechanisms and transformer archi-tectures (Vaswani, 2017) (Yu et al., 2021) to dynamically select codebooks and captureglobal data dependencies. Recent works also explore per-channel codebooks (Hsu et al.,2024) and neural network variants of residual quantization (Huijben et al., 2024) to predictspecialized codebooks, enhancing the model's expressive power.\nVQ has been extensively applied across various domains. In natural language process-ing, VQ facilitates sequence modeling (Kaiser et al., 2018) enhancing tasks such as languagemodeling and machine translation. In computer vision, VQ has significantly advanced imagegeneration and compression techniques (Esser et al., 2021). Similarly, in audio processing,VQ techniques have captured complex temporal dependencies (Dhariwal et al., 2020). Fur-thermore, in multimodal applications, VQ supports the integration of different data typesthrough shared discrete representations (Ramesh et al., 2021).\nDespite these advancements, VQ methods encounter challenges that restrict their broaderapplication, including but not limited to codebook collapse, training instability, and compu-tational overhead. Extensive research has been conducted on solving the codebook collapse"}, {"title": "5 Methods", "content": ""}, {"title": "5.1 Vector Quantization", "content": "We employ VQ-VAE to conduct vector quantization to investigate the collapsing issues. Forcodebook initialization, we adopt the widely used K-means initialization strategy (Zeghi-dour et al., 2021). It uses the encoder output Z = {z1, z2, ..., zN} and perform K-meansalgorithm to initialize the tokens T = {t1,t2,...,tS}, where N is the number of encoderoutput and S is the number of tokens. The initialization aims to minimize the total distancefrom each vector zj to its nearest token tk. The optimizing function is shown in equation 2,\n\\min \\sum_{j=1}^N \\sum_{k=1}^S r_{jk}||z_j \u2013 t_k||^2,                                         (2)\nwhere rjk = 1 if zj is assigned to cluster center tk, otherwise rjk = 0 .\nMoreover, the widely adopted VQ-VAE optimization objective comprises reconstructionloss Lrecon = ||X \u2013 X' ||2, codebook loss Lcommit = \\frac{1}{N}\\sum_{j=1}^N ||sg(zj) \u2013 tjk||2, and commitmentloss Lcommit =  \\frac{1}{N}\\sum_{j=1}^N || zj \u2013 sg(tjk)||2, where sg(.) denotes the stop-gradient operationand tjk means the token selected by zj. Additionally, we adopt the statistical approach(Van Den Oord et al., 2017) to update the codebook instead of the codebook loss term.Specifically, each encoder output zj is assigned to subsets Zk = {z1k, z2k, z3k, ...} based onnearest neighbor queries within the set T = {t1, t2, ..., tS}, where Zk comprises embeddingsclosest to tk. Each tk serves as the cluster center for Zk, which receives updates accordingly.However, due to the necessity of training models using minibatches, exponential movingaverages (EMA) are leveraged to accommodate batch updates. Under EMA framework,the sum and size of Zk as mk and lk. The statistical update process is formalized by thefollowing equation 3 to 5:\nM_k^{(o)} = M_k^{(o-1)} + (1 - \\gamma) m_k^{(o)},                                          (3)\nL_k^{(o)} = L_k^{(o-1)} + (1 - \\gamma) l_k^{(o)},                                           (4)\nt_k^{(o)} = \\frac{M_k^{(o)}}{L_k^{(o)}},                                              (5)\nwhere o is the index of iteration and M as well as L are respectively recordings of mand l. The \\gamma is the decay rate to control the speed of update."}, {"title": "5.2 Implementation Details", "content": "Datasets Our synthetic dataset includes 10 clusters, each with 1,000 data points sampledfrom a Gaussian distribution and standardized using Standard Scaler. We construct dif-ferent synthetic data with different dimension (2, 3, 4, and 8). We use CIFAR-10, whichconsists of 60,000 32x32 color images divided into 10 classes, to investigate our proposedcollapses and validate corresponding solutions on real-world data.\nVQ-VAE For our synthetic dataset, our VQ-VAE comprises an MLP-based encoder/decoderwith three linear layers and uses the ReLU activation function. Training is facilitated bythe AdamW optimizer, with a learning rate of 0.001. Additional specifications include acodebook size of 128, a hidden dimension of 32, a batch size of 256, a beta of 0.25, and adecay rate (\u03b3 for EMA) of 0.9.For CIFAR-10, our VQ-VAE adopts downsampling using aCNN with a downsample channel of 128, and the model includes two residual blocks witha hidden channel size of 64. The codebook size is set at 512 with a token dimension of 64.The learning rate is 3e-4, using the Adam optimizer with amsgrad set to true. The beta is0.25 and the decay rate is 0.99.\nTokens Collapse Our solution for tokens collapse comprises pretraining an autoencoder,then fine-tuning a VQ-VAE initialized with the pretrained autoencoder's weight. On thesynthetic dataset, the autoencoder is trained for 100 epochs. The fine-tuned VQ-VAE andthe original VQ-VAE are trained for 100 and 200 epochs respectively. We also explorevarious data dimensions (2, 3, and 8) with corresponding token sizes of 1, 1, and 4. Forexperiments on CIFAR-10, the codebook size is varied from 16 to 65,536, with embeddingand token adopting the size of 32. And we pretrain AE for 150 epochs and fine-tune theVQ-VAE for 150 epochs.\nEmbeddings Collapse The hidden size of the decoder is maintained while the encoder'shidden size is reduced to 8. These experiments are repeated three times with different seedsover 200 epochs. For the real dataset, the channel size of encoder is gradually reduced from64 to 2 across 300 epochs."}, {"title": "6 Conclusion", "content": "In this work, we provide an in-depth examination of representation collapsing problems invector quantization, identifying two levels of collapses, including tokens and embeddingscollapses, and investigate their detrimental impacts on VQ. Through detailed analysis withboth synthetic and real-world data, we pinpoint the causes of these collapses and introducepotential solutions, which shed light on further improvements and applications of VQ. Whileour work systematically explores these collapses and offers potential solutions, growth op-portunities abound. For example, the transition from continuous to discrete representationsin our solution, when pretraining without VQ followed by fine-tuning with VQ, introducesa representation gap that needs addressing. In the future, we plan to further explore theimpact and solutions for these collapses in generative models, such as LLMs and diffusionmodels."}]}