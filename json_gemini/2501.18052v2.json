{"title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders", "authors": ["Bartosz Cywi\u0144ski", "Kamil Deja"], "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have revolutionized generative modeling, enabling the creation of highly realistic images. Despite their success, these models can inadvertently generate undesirable and harmful content, pornography (Rando et al., 2022; Schramowski et al., 2023) or copyrighted images e.g. cloning the artistic styles without consent (Andersen, 2024). The straightforward solution to this problem is to retrain the model from scratch with curated data. However, such an approach, due to the enormous sizes of the training datasets is both costly and impractical. As a result, a growing number of works focus on removing the influence of unwanted data from already pre-trained text-to-image diffusion models through machine unlearning (MU).\nMost existing methods build on the basic idea of fine-tuning the model while using negative gradients for selected unwanted samples (Wu et al., 2024; Gandikota et al., 2023; Heng & Soh, 2024; Kumari et al., 2023). To minimize degradation in the overall model's performance, recent techniques"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Sparse Autoencoders (SAES)", "content": "Sparse autoencoders (Olshausen & Field, 1997) are neural networks designed to learn compact and interpretable representations of data by encouraging sparsity in the latent space. This is achieved by incorporating a sparsity penalty into the reconstruction loss, ensuring that only a small fraction of latent neurons activate for any given input. Recently, SAEs have emerged as an effective tool in the field of mechanistic interpretability, enabling the discovery of features corresponding to human-interpretable concepts (Huben et al., 2024; Bricken et al., 2023) and sparse feature circuits within language models (Marks et al., 2024). In this study, sparse autoencoders are used within text-to-image diffusion models to identify and disable features linked to the generative capabilities of specific concepts."}, {"title": "2.2. Machine Unlearning in Diffusion Models", "content": "The term and problem statement for machine unlearning was first introduced by Cao & Yang (2015), where authors transform the neural network model, through an additional simple layer, into a format where output is a summation of independent features. Such a setup allows for unlearning by simply blocking the selected summation weights or nodes.\nConversely, recent works focusing on unlearning for diffusion models, usually employ fine-tuning in order to unlearn specific concepts. For example, EDiff (Wu et al., 2024) formulates this problem as bi-level optimization, ESD (Gandikota et al., 2023) leverages negative classifier-free guidance and FMN (Zhang et al., 2024a) introduces a new re-steering loss applied only to the attention layer. SalUn (Fan et al., 2023) and SHS (Wu & Harandi, 2024) select parameters to adapt through saliency maps or connection sensitivity, while SA (Heng & Soh, 2024) replaces unwanted data distribution with the surrogate one, with an extension to the selected anchor concepts in CA (Kumari et al., 2023). SPM (Lyu et al., 2024) takes a different approach, using small linear adapters added after each linear and convolutional layer to directly block the propagation of unwanted content.\nBy contrast, methods that do not rely on fine-tuning include SEOT (Li et al., 2024), which eliminates unwanted content from text embeddings, and UCE (Gandikota et al., 2024), which adapts cross-attention weights using a closed-form solution. Unlike these approaches, we neither modify prompt embeddings nor alter the base model's weights.\nIn this paper, we revisit the pioneering work by Cao & Yang (2015) adapting it to the text-to-image diffusion models using recent advancements in mechanistic interpretability. In particular, we train a sparse autoencoder on the activations of the diffusion model and leverage its summative nature"}, {"title": "2.3. Interpretability of Diffusion Models", "content": "Numerous studies explored disentangled semantic directions within the bottleneck layers of UNet-based diffusion models (Kwon et al., 2023; Park et al., 2023; Hahm et al., 2024) and analyzed cross-attention layers to investigate their internal mechanisms (Tang et al., 2022). Despite these efforts, detailed interpretation of the specific functions and features learned by specific components in T2I diffusion models remains limited. Recently, Basu et al. (2023; 2024) localized knowledge about visual attributes in DMs, showing that modifying text input in a few cross-attention layers can consistently alter attributes like styles, objects or facts. Additionally, Toker et al. (2024) aimed to interpret T2I models' text encoders by generating images from their intermediate representations. In contrast, our work employs sparse autoencoders to achieve a more fine-grained understanding of the internal representations in diffusion models.\nAlthough SAEs are widely used in the language domain, their application to vision remains limited. Early studies applied them to interpret and manipulate CLIP (Radford et al., 2021) representations (Fry, 2024; Daujotas, 2024) or traditional vision networks (Szegedy et al., 2015; Gorton, 2024). More recently, SAEs have been successfully applied in vision-language models (VLMs) to tackle problems such as mitigating hallucinations (Jiang et al., 2024) and generating interpretable radiology reports (Abdulaal et al., 2024). To date, only a few studies have utilized SAEs to investigate the inner workings of T2I diffusion models. Ijishakin et al. (2024) use SAEs to identify semantically meaningful directions within the bottleneck layer. Surkov et al. (2024) trained SAEs on activations from a one-step distilled SDXL-Turbo diffusion model, demonstrating that SAEs can detect interpretable features within specific model's blocks and enable causal interventions on them. Furthermore, Kim et al. (2024) applied SAEs to activations from the diffusion model sampled in an unconditional way. By training a separate SAE model for each diffusion timestep, they revealed the visual features learned by these models and their connection to class-specific information.\nIn contrast to prior approaches, our work involves training a single SAE on activations from multiple denoising steps of a standard, non-distilled Stable Diffusion model. Additionally, we leverage the well-disentangled and interpretable features learned by SAEs for downstream unlearning tasks, showcasing their potential in real-world use cases."}, {"title": "3. Sparse Autoencoders for Diffusion Models", "content": "In this work we adapt sparse autoencoders to Stable-Diffusion text-to-image diffusion model. Importantly, unlike previous works utilizing SAEs for diffusion models, we train them on activations extracted from every step t of the denoising diffusion process. These activations are obtained from the cross-attention blocks of the diffusion model and form a feature maps. Each feature map extracted at timestep t is a spatially structured tensor of shape $F_t \\in \\mathbb{R}^{h\\times w\\times d}$, where h and w denote the height and width of the feature map, and d is the dimensionality of each feature vector. Each spatial position within the feature map corresponds to a patch in the input image.\nAs a single SAE training sample, we consider an individual d-dimensional feature vector, disregarding the information about its spatial position. Therefore from each feature map, we obtain h \u00d7 w training samples. For simplicity, we drop the timestep index t in subsequent notations.\nLet $x \\in \\mathbb{R}^d$ denote the d-dimensional vector of activations from a single position of a feature map and let n be the latent dimension in sparse autoencoder. The encoder and decoder of standard single-layer ReLU sparse autoencoder (Bricken et al., 2023) are then defined as follows:\n$Z = ReLU (W_{enc} (x - b_{pre}) + b_{enc})$\n$\\hat{x} = W_{dec}z + b_{pre},$\nwhere $W_{enc} \\in \\mathbb{R}^{n\\times d}$ and $W_{dec} \\in \\mathbb{R}^{d\\times n}$ are encoder and decoder weight matrices respectively, $b_{pre} \\in \\mathbb{R}^d$ and $b_{enc} \\in \\mathbb{R}^n$ are learnable bias terms. Elements of z called feature activations are usually denoted as $f_{1,...,n}(x)$. Typically, n is equal to d multiplied by a positive expansion factor.\nThe objective function of SAE is defined as:\n$L(x) = ||x - \\hat{x}||^2 + L_{aux},$\nwhere $||x - \\hat{x}||^2$ is a reconstruction error and $L_{aux}$ is a reconstruction error using only the largest $k_{aux}$ feature activations that have not fired on a large number of training samples, so-called dead latents. The auxiliary loss is used to prevent dead latents from occurring and is scaled by a coefficient a.\nIn our work, we additionally apply two extensions over vanilla ReLU SAEs. First, we follow Gao et al. (2024) and use the TopK activation function (Makhzani & Frey, 2013) which retains only the k largest latent activations for each vector x, setting the rest to zeros. While the decoder remains unchanged, the encoder is thus redefined to:\n$z = TopK (W_{enc} (x \u2013 b_{pre})) .$\nSecond, we leverage the BatchTopK approach introduced by Bussmann et al. (2024), which dynamically selects the $B \\times k$ largest feature activations across the entire input data"}, {"title": "4. Method", "content": "Given a trained sparse autoencoder able to reconstruct activations of the diffusion model, our SAeUron method for concept unlearning involves two steps. First, we identify which SAE features will be targeted for unlearning a specific concept c. This selection is based on the importance scores associated with features. Then, during the inference of the diffusion model, we encode the original activations with SAE, ablate the selected features to remove the targeted concept associated with them, and decode them back. Thanks to the summative nature of SAEs and the sparsity of activated features, this process effectively removes the influence of the targeted concept on the final generation, while preserving the overall performance of the diffusion model. We present the overview of our method in Figure 2."}, {"title": "4.1. Selection of SAE features for unlearning", "content": "To identify SAE features that exhibit strong correspondence exclusively to the target concept c, we define a score function that measures the importance of each i-th feature for concept c at every denoising timestep t. Utilizing a dataset of activations from the diffusion model $D = D_c \\cup D_{\\neg c}$, which includes data containing a target concept $D_c$ and data that does not $D_{\\neg c}$, we define score as:\n$score(i, t, c, D) = \\frac{\\mu(i, t, D_c)}{\\Sigma_{j=1}^n \\mu(j, t, D_c) + \\delta} -  \\frac{\\mu(i, t, D_{-c})}{\\Sigma_{j=1}^n \\mu(j, t, D_{-c}) + \\delta},$\nwhere d is a small constant added to prevent division by zero and $\u03bc(i, t, D) = \\frac{1}{|D|} \\Sigma_{x \\in D} f_i(x_t)$ denotes the average activation of i-th feature on activations from a timestep t. To ensure that features activating on many concepts do not dominate the scores, we normalize both components by the average activation values for the corresponding subsets of the dataset. Thus, features with high scores exhibit strong activation for concept c while remaining weakly activated for all other concepts. Importantly, only a small fraction of features achieve high scores, indicating that SAE learns a limited number of concept-specific features. Consequently, we target high-scoring features in our method to unlearn concepts without affecting the overall performance of a model, blocking features with scores above the tunable percentile threshold TC."}, {"title": "4.2. SAE-based concept unlearning", "content": "Building on the method introduced for locating features that correspond to specific concepts, we now present our SAE-based unlearning procedure, which we apply for each timestep t during the inference of the diffusion model. To that end, we utilize previously trained sparse autoencoder applied to a single U-Net cross-attention block."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Technical details", "content": "Where to apply SAEs Recent studies on mechanistic interpretability in diffusion models (Basu et al., 2023; 2024) show that different cross-attention blocks specialize in generating specific visual aspects like style or objects. Building on this fact, we apply our unlearning technique to activations from key cross-attention blocks. For style filtering, we use second to last up-sampling block up. 1.2, and for object filtering up. 1.1, identified empirically as the most effective.\nUnlearn Canvas benchmark (Zhang et al., 2024b) is a large benchmark aiming to extensively evaluate MU methods for DMs. Benchmark consists of 50 styles \u00d7 20 objects, providing a test bed both for style and object unlearning evaluation. Authors, along with a dataset, also provide a Stable Diffusion v1.5 model fine-tuned on the selected objects and styles from the benchmark, ensuring a fair evaluation.\nSAE training dataset To ensure a fair evaluation, the SAE training set is comprised of text prompts that are distinct from those employed in the evaluation on the UnlearnCanvas benchmark. Specifically, we utilize simple one-sentence prompts (referred to as anchor prompts), which were employed by the authors of the benchmark in training of the CA method (Kumari et al., 2023). For each of the 20 objects, we use 80 prompts. Additionally, to enable the SAE to learn the styles used in the benchmark, we append the postfix \"in {style} style.\" to each prompt. Consequently, our training set consists of a total of 81,600 prompts.\nFor each generation, we collect the internal activations from the specified cross-attention blocks across 50 denoising timesteps, utilizing the DDIM sampler (Song et al., 2021) and a guidance scale set to 9. Each feature map Ft from both blocks up.1.1 and up.1.2 has a shape of 16 \u00d7 16 \u00d7 1280. Importantly, we only gather feature maps related to text-conditioned generation part, discarding the unconditioned ones, and we store them in float16 precision. Nonetheless, during inference, trained SAEs reconstruct both parts of feature maps.\nValidation dataset for feature score calculation To calculate feature scores during the unlearning of concept c, we collect feature activations $f_i(x_t)$ at each denoising timestep t using a validation set D of anchor prompts, similar to SAE's training set. Following the UnlearnCanvas evaluation setup, activations are gathered over 100 denoising timesteps. Despite being trained on 50 steps, SAEs generalize well to this extended range. For style unlearning, we use 20 prompts per style and for object unlearning 80 per object.\nBefore presenting the experimental results for our SAE-based unlearning method, we first evaluate how well the"}, {"title": "5.2. Interpreting SAE features", "content": "sparse encoding captures the concepts to be unlearned. Specifically, we assess whether the features selected using our score-based approach correspond to the desired concepts, as selecting relevant features is critical to our method's success. Additionally, we examine the image regions where these features strongly activate to verify their connection to the targeted concepts and their alignment with human-interpretable attributes."}, {"title": "5.2.1. DO FEATURES EXHIBIT DISCRIMINATIVE POWER?", "content": "To validate whether SAE learns meaningful visual features, we train a 5-nearest neighbors classifier on SAE feature activations extracted at each timestep from the validation dataset used for the score calculation. Importantly, activations are gathered from the unconditional part of the generation to exclude the influence of text embeddings."}, {"title": "5.2.2. DO FEATURES RELATE TO CONCEPTS?", "content": "To further enhance our understanding of features learned by SAEs, we visualize their activations on corresponding image patches to assess whether they relate to interpretable patterns. We generate heatmaps of activations from features selected by our score-based approach, normalized to the range [0, 1], and overlay them on the generations. For each timestep t, we visualize the corresponding generated image by predicting the fully denoised sample x0 from the diffusion model's representation at t.\nThe visualizations reveal that style-related features strongly activate on patches with characteristic style patterns while remaining inactive elsewhere. Notably, these features focus on style-related backgrounds while ignoring object regions, demonstrating the precision of our score-based selection in isolating style features. Similarly, object-related features activate only on the targeted object, regardless of the background or style. By analyzing activations across multiple"}, {"title": "5.3. Concept unlearning with SAeUron", "content": ""}, {"title": "5.3.1. METRICS", "content": "We evaluate our method on unlearning tasks using metrics from the UnlearnCanvas, calculated using Vision Transformer-based (Dosovitskiy et al., 2021) classifiers provided by the authors of the benchmark. Assuming that we want to remove concept c, unlearning accuracy (UA) measures the proportion of samples generated from prompts containing c that are not correctly classified. In-domain retain accuracy (IRA) quantifies correctly classified samples with other concepts, while cross-domain retain accuracy (CRA) assesses accuracy in a different domain (e.g., in style unlearning, we calculate object classification accuracy). Additionally, we measure the overall quality of images generated after unlearning through FID (Heusel et al., 2017). This set of metrics enables evaluating each method's effectiveness in removing concepts from the base model while preserving the generative capabilities of others."}, {"title": "5.3.2. HYPERPARAMETERS", "content": "Our method uses two hyperparameters tunable for each concept c separately: percentile threshold of score distribution Te and negative multiplier Ye. For style unlearning we empirically observed that setting Te = 99.999 and c = -1 yield satisfying results across all styles. For the case of object unlearning we tune hyperparameters on the validation dataset, presenting the selected values in Appendix G."}, {"title": "5.3.3. RESULTS", "content": "We evaluate SAeUron on style and object unlearning tasks using the UnlearnCanvas benchmark, comparing it to state-of-the-art methods. Table 1 presents results averaged over five random seeds, with competing method results taken from UnlearnCanvas. Despite using unsupervised SAE features, SAeUron significantly outperforms all methods in style unlearning and ranks second in object unlearning."}, {"title": "6. Additional experiments", "content": ""}, {"title": "6.1. Unlearning of multiple concepts", "content": "Recent studies show that traditional machine unlearning approaches, while effective for removing a single concept, struggle in scenarios requiring the sequential removal of multiple concepts from a diffusion model (Zhang et al., 2024b). In contrast, SAeUron enables seamless filtering of multiple concepts with minimal impact on other concepts. We evaluate our approach against competing methods on sequential unlearning of 6 styles, with results presented in Figure 6. Notably, the performance of other methods drops as the number of targeted concepts increases, due to the growing degradation of the model's overall performance. By selectively removing a limited subset of features strongly tied to the targeted concepts, SAeUron achieves superior retention of non-targeted concepts. Further details on the evaluation setup are provided in the Appendix D."}, {"title": "6.2. Robustness to adversarial attacks", "content": "Finally, as shown by Zhang et al. (2025), recent unlearning works do not always fully block the unwanted content, making it possible to bypass the unlearning mechanisms. In particular, authors show that when prompted with crafted adversarial inputs models can still be forced to generate unlearned concepts. We evaluate SAeUron under the UnlearnDiffAtk method (Zhang et al., 2025), optimizing a 5-token prefix for 40 iterations with a learning rate of 0.01. Figure 8 shows unlearning accuracies before and after the attack for all methods. Competing approaches suffer significant performance drops, suggesting they primarily mask concepts instead of unlearning them. In contrast, our method, by filtering internal activations of the diffusion model, remains highly robust, showing minimal performance degradation."}, {"title": "7. Limitations", "content": "There are several limitations of our approach serving as interesting future work directions. SAeUron operates during inference, introducing a 10% overhead, which slightly slows down the generation process. Additionally, training SAEs demands significant storage for activations, posing challenges for large datasets. However, as presented in Table 1, when compared to other techniques our approach has low GPU and storage requirements. The performance of our approach is highly affected by the quality of the SAE. In particular, the unconditional nature of SAE allows us to attempt unlearning of new content never seen by the SAE."}, {"title": "8. Conclusions", "content": "In this work, we propose SAeUron, a novel method leveraging sparse autoencoders to unlearn concepts from text-to-image diffusion models. Training SAEs on activations from DM, we demonstrate that their sparse and interpretable features enable precise, concept-specific interventions while maintaining overall model performance. Method's reliance on interpretable features enhances transparency, allowing for a clearer understanding of the unlearning process. SAeUron achieves SOTA results on the UnlearnCanvas benchmark, showcasing robustness to adversarial attacks and the capability to unlearn multiple concepts sequentially."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work. In particular, while our method was designed to block and remove selected unwanted, biased or harmful content it can be misused to promote it instead."}, {"title": "A. BatchTopK SAEs trained for diffusion models", "content": "The BatchTopK variant of SAEs enables the model to flexibly distribute active features across a data batch to achieve better reconstruction performance. Specifically, our SAEs allocate more active latents to image patches with detailed content, while less important areas, such as the background, are reconstructed using fewer features.\nAdditionally, Figure 10 shows the average number of activated features per image patch. Central regions of the image tend to have more active features, while background areas have fewer. Interestingly, corners of the images also exhibit frequent activations."}, {"title": "B. Prompts from a validation set for feature score calculation", "content": "Below, we present the prompts used in our validation set to gather feature activations for style unlearning. The same prompts are applied to each style used in the UnlearnCanvas benchmark. For object unlearning, we use all anchor prompts from the CA work, excluding the \"in {style} style\" postfixes.\n\u2022 \"Gothic cathedral with flying buttresses and stained glass windows in {style} style.\u201d\n\u2022 \"A bear dressed as a medieval knight in armor in {style} style.\u201d\n\u2022 \"A bird with feathers as iridescent as an oil slick in the sunlight in {style} style.\"\n\u2022 \"A butterfly emerging from a jeweled cocoon in {style} style.\"\n\u2022 \"A cat wearing a superhero cape leaping between buildings in {style} style.\"\n\u2022 \"A dog wearing aviator goggles piloting an airplane in {style} style.\"\n\u2022 \"A goldfish swimming in a crystal-clear bowl in {style} style.\"\n\u2022 \"A candle's flame flickering in a mysterious old library in {style} style.\"\n\u2022 \"Flower blooming in the middle of a snow-covered landscape in {style} style.\"\n\u2022 \"A frog with a croak that sounds like a jazz musician's trumpet in {style} style.\"\n\u2022 \"Wild horse galloping across the prairie at sunrise in {style} style.\"\n\u2022 \"A man hiking through a dense forest in {style} style.\"\n\u2022 \"Jellyfish floating serenely in deep blue water in {style} style.\u201d\n\u2022 \"Rabbit peering out from a burrow in {style} style.\"\n\u2022 \"A classic BLT sandwich on toasted bread in {style} style.\"\n\u2022 \"Sea waves crashing over ancient coastal ruins in {style} style.\u201d\n\u2022 \"Statue of a forgotten hero covered in ivy in {style} style.\"\n\u2022 \"Tower soaring above the clouds in {style} style.\"\n\u2022 \"A majestic oak tree in a serene forest in {style} style.\"\n\u2022 \"Moonlit waterfall in a serene forest in {style} style.\""}, {"title": "C. Selection of cross-attention blocks to apply SAE", "content": "In LLMS, SAEs are typically trained on activations from the residual stream, MLP layers, or attention layers (Kissane et al., 2024). Building on recent studies on mechanistic interpretability in diffusion models (Basu et al., 2023; 2024), we apply SAEs to cross-attention blocks.\nTo identify the appropriate blocks for style and object unlearning, we conduct an experiment where each cross-attention block is ablated one by one, and the block causing the most significant degradation in the targeted visual attribute (style or object) is selected. Ablation involves replacing the block with identity function. Intuitively, this localizes the block most responsible for generating the analyzed attribute."}, {"title": "D. Details of sequential unlearning evaluation", "content": "The sequential unlearning evaluation assesses methods in a scenario where unlearning requests arrive sequentially. This setup requires methods to progressively remove an increasing number of concepts from a base model while ensuring previously unlearned targets remain unlearned. At the same time, it significantly challenges the retention of the model's overall performance.\nTo ensure fair comparison, we follow the evaluation protocol from the UnlearnCanvas paper. Specifically, we sequentially unlearn the following styles in this order:\n1. Abstractionism\n2. Byzantine\n3. Cartoon\n4. Cold Warm\n5. Ukiyoe\n6. Van Gogh\nAfter each phase, we compute the UA and RA metrics, where RA is the average of IRA and CRA. Figure 12 shows UA averaged over all unlearned concepts up to each phase. SAeUron consistently maintains high unlearning accuracy and significantly outperforms competing methods in retaining the ability to generate all other concepts."}, {"title": "E. SAE trainings details", "content": "We train our BatchTopK sparse autoencoders with k = 32 and an expansion factor of 16. Optimization uses Adam (Kingma, 2014) with a learning rate of 0.0004 and a linear scheduler without warmup. We set the batch size to 4096 and unit-normalize decoder weights after each training step.\nFollowing heuristics from (Gao et al., 2024), we set kaux to a power of two close to $ \\frac{1}{2}$ and $a = \\frac{1}{32}$. Additionally, in line with Templeton et al. (2024), we consider a latent dead if it has not activated over the last 10M training samples. We train the SAE on the up. 1.1 object block for 5 epochs and on the up. 1.2 style block for 10 epochs.\nTable 2 summarizes key training hyperparameters and metrics, while Figure 13 presents log feature density plots at the end of training. The SAE trained on up. 1.1 exhibits dead latents, whereas the one trained on up. 1.2 does not. Notably, very few features activate very frequently, which suggests promising interpretability.\nBoth SAEs were trained on a single NVIDIA RTX A5000 GPU."}, {"title": "F. SAE generalization abilities", "content": "We assess the generalization of SAEs by training a sparse autoencoder on activations from prompts in a randomly selected half (25) of the styles in the UnlearnCanvas benchmark. The training setup remains identical to our other SAEs.\nTo evaluate the ability to unlearn concepts not seen during training, we apply SAeUron to the style unlearning task using this SAE, following the setup in Section 5.3. Table 3 presents results for SAEs trained on half of the styles, evaluating performance on all styles, in-distribution styles, and out-of-distribution (OOD) styles.\nNotably, we achieve over 50% unlearning accuracy on OOD data, demonstrating that SAEs effectively generalize and can unlearn concepts even when they were absent from the training set."}, {"title": "G. Hyperparameters for object unlearning", "content": "For object unlearning we tune our two hyperparameters: percentile threshold Te and multiplier Ye for each class separately."}, {"title": "H. Activation steering on style features", "content": "We further explain what information is encoded by our SAE as individual features with the highest correspondence to a concept c. To that end, we generate unconditional examples using diffusion model (using an empty prompt) and steer the generation process by increasing activations of the highest scoring features for a given concept. Specifically, we modify feature maps Ft during forward pass of the diffusion model at each timestep t in the following manner:\n$F_t \\leftarrow F_t + \\Sigma_{i \\in F_c} \\nu \\mu(i, t, D_c)d_i,$\nwhere $d_i$ is feature direction corresponding to a i-th column of SAE decoder, $ \\nu > 0 \\in R$ is a concept-specific positive multiplier that determines the strength of steering and $F_c = {i | i \\in {1, ..., n}, score(i, t, c, D) > t_c}$ is a set of chosen features where $T_c \\in R$ represents a threshold calculated from a specified percentile value."}, {"title": "I. Number of score-based selected features for unlearning", "content": "For style unlearning, setting Te = 99.999 selects a single feature at each timestep t in our unlearning method. Since Te is tuned separately for each object, the number of selected features varies across classes.\nSAeUron utilizes 1.9 \u00b1 0.83 SAE features during the procedure."}, {"title": "J. K-nearest neighbors classification for style features", "content": "We conduct an analogous experiment to the one in Section 5.2.1, this time on style features. Notably, both the score-based and random feature setups use only a single feature, as our selection method identifies only one feature for style unlearning.\nInterestingly, accuracy remains similar between using all features and the score-based selection. Moreover, accuracy tends to increase from approximately the 30-th timestep, suggesting that style-related features emerge later compared to object-related features in the classification setup."}, {"title": "K. Distribution of feature importance scores across timesteps", "content": "We analyze how the distribution of score importance varies across denoising timesteps. Figure 17 shows two high percentiles of score distributions over all 100 timesteps. We observe that the threshold value decreases as the generation process progresses, indicating that more features receive high scores early in denoising. As the process continues, only a small number of features remain highly relevant to specific concepts."}, {"title": "L. UnlearnDiffAtk evaluation of object unlearning", "content": "We also evaluate our method on adversarial prompts crafted using the UnlearnDiffAtk method for object unlearning. As shown in Figure 18, unlearning accuracy drops significantly under attack. However, this is largely due to the nature of the evaluation process, where each iteration of UnlearnDiffAtk determines attack success based on the classifier's argmax prediction.\nAs demonstrated in Figure 19, SAeUron completely removes the targeted object from the image. However, since no other object replaces it, the classifier's predictions become largely random. Consequently, attacks are often marked as successful, even when they fail to make the model generate the unlearned object.\nTo further validate whether images before and after the attack resemble the targeted object, we compute CLIPScore (Radford et al., 2021) between the target object's name and the image. Table 5, the CLIPScore remains nearly unchanged, indicating that the attack rarely leads to generating the targeted object."}, {"title": "M. Pseudocode of SAeUron", "content": "For ease of understanding our unlearning procedure, we present detailed pseudocode of SAeUron applied on a single denoising timestep t in Algorithm 2."}, {"title": "N. Auto-interpreting features selected for unlearning", "content": "To validate whether the features selected by our score-based method correspond to meaningful and interpretable concepts, we construct a simple annotation pipeline using GPT-40 (Hurst et al., 2024). To achieve this, we design a prompt for the GPT model, closely following the one presented in (Paulo et al., 2024) and adapting it to our case."}]}