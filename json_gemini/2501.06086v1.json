{"title": "All AI Models are Wrong, but Some are Optimal", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Dirk Reinhardt", "Sebastien Gros"], "abstract": "AI models that predict the future behavior of a system (a.k.a. predictive AI models) are central to intelligent decision-making. However, decision-making using predictive AI models often results in suboptimal performance. This is primarily because AI models are typically constructed to best fit the data, and hence to predict the most likely future rather than to enable high-performance decision-making. The hope that such prediction enables high-performance decisions is neither guaranteed in theory nor established in practice. In fact, there is increasing empirical evidence that predictive models must be tailored to decision-making objectives for performance. In this paper, we establish formal (necessary and sufficient) conditions that a predictive model (AI-based or not) must satisfy for a decision-making policy established using that model to be optimal. We then discuss their implications for building predictive AI models for sequential decision-making.", "sections": [{"title": "I. INTRODUCTION", "content": "With advances in Artificial Intelligence (AI), computational power, and big data, it has become increasingly possible to build data-driven, predictive AI models that can predict the future behavior of complex physical systems and processes in response to actions we plan to apply in reality [1]. These predictive models play a central role in intelligent systems by facilitating autonomous decision-making [2]. Despite their potential, predictive AI models are not yet widely adopted for sequential decision-making, in part due to the disappointing performance of the resulting decisions. This performance gap is due to the fact that most real systems are inherently stochastic and, even with abundant data, the predictions of the AI model are always an approximation of the real system's future behavior [3].\nPredictive AI models for decision-making, whether probabilistic or deterministic nature, are typically constructed to maximize prediction accuracy by fitting the model to the observed behavior of the physical system from data [2], [4]. When used for decision-making, the hope is that the best-fitted predictive model, according to some chosen loss function, will enable decisions that are close to optimal. However, when dealing with stochasticity, this assumption is not well-supported in theory and is often challenged in practice [5], [6]. This limitation arises because the construction of the predictive models is generally agnostic to the decision-making objectives, and therefore has no direct relationship to the performance measure of the resulting decision-making scheme.\nIn practice, this is evident in Model-based Reinforcement Learning (MBRL), where even complex Deep Neural Networks (DNNs) predictive models optimized for accurate predictions often fail to adequately represent the real system, becoming a bottleneck for decision-making performance [7]. In fact, there is growing empirical evidence that embedding decision-making objectives in predictive models can significantly enhance decision-making performance [8], [9], [10], [5], [11]. Additionally, it has been observed that predictive models can trade prediction accuracy for improved decision-making performance [12], [13]. A parallel can be drawn with training Large Language Models (LLM), where Reinforcement Learning (RL) with human feedback is applied post-training to refine performance beyond data-fitting [14]. Despite the empirical evidence, the formal relationship between predictive AI models and the performance of the decisions one can draw from these models remains unknown. This paper aims at filling that gap.\nBuilding on prior works in the context of Model Predictive Control (MPC) [15], [16], in this paper, we present a formal framework to construct predictive AI models tailored to decision-making performance instead of prediction accuracy, referred to as \u201cdecision-oriented\" predictive models. The main contributions of this paper are:\n\u2022 Establish the necessary and sufficient conditions for decision-oriented predictive AI models and discuss their implications for constructing such models.\n\u2022 Prove that a predictive AI model best fitting the data is not necessarily the AI model that enables the best decision-making performance.\n\u2022 Show how deterministic predictive models can enable optimal decision-making for stochastic systems.\n\u2022 Establish the class of decision-making problems where AI models built to fit the data enable optimal decisions and in what sense.\nThe rest of the paper is structured as follows: Section II introduces the necessary concepts within sequential decision-making for stochastic systems. Section III formulates the predictive model-based decision-making framework and discusses different classes of predictive models and different methods to estimate them. Section IV-A introduces the central concepts in the paper and specifies the necessary and sufficient conditions on predictive models to achieve optimal decision-making. Section V provides two sets of simulation studies to demonstrate the concept of constructing decision-oriented predictive models. Section VI provides a discussion of our findings. Section VII presents the conclusions."}, {"title": "II. BACKGROUND ON SEQUENTIAL DECISION-MAKING", "content": "In sequential decision-making, an agent aims to maximize expected rational utility over time by making decisions based on the latest available information, while accounting for both short- and long-term consequences [17]. For stochastic systems, decisions must also account for future uncertainty and"}, {"title": "A. Sequential Decision-Making Problem as MDP", "content": "The Markov Decision Process (MDP) framework provides a solid foundation to formulate and solve sequential decision-making problems in the context of stochastic systems with the Markovian property [18]. MDPs consider dynamic systems with underlying states $s\\in S$ and actions $a \\in A$, with respective sets S and A, and the associated stochastic state transition:\n$s^+ \\sim p(\\cdot | s, a)$,\n(1)\ndefining how a state-action pair s, a yields a new state $s^+$. In (1), p can be of different nature, depending on the set S. For S being a discrete countable set, p is a conditional probability. For $S \\subseteq \\mathbb{R}^n$, where n is the state dimension, p can be a conditional probability density or a conditional probability measure.\nSolving an MDP consists in finding a (possibly stochastic) policy $\\pi(a|s)$ such that the stochastic closed-loop trajectories maximize the expected value of a given long-term reward of actions specified by $r: S \\times A \\to \\mathbb{R}$. The most common criterion is maximizing the sum of discounted rewards:\n$J(\\pi) = \\mathbb{E}_{\\substack{a_k \\sim \\pi(\\cdot|S_k)\\\\S_{k+1} \\sim p}} \\left[\\sum_{k=0}^{\\infty} \\gamma^k r(s_k, a_k)\\right]$,\n(2)\nfor a discount factor $\\gamma \\in (0,1)$. The $\\gamma$ can be viewed as a rudimentary probabilistic model of the system lifetime, meaningful to most applications. The expectation in (2) is taken over the distribution of states $s_k$ and actions $a_k$ in the Markov chain induced by $\\pi(a|s)$. The complete MDP can be represented as a tuple $(S, A, r, p, \\gamma)$. The solution to MDP provides an optimal policy $\\pi^*$ from the set $\\Pi$ of all admissible policies by maximizing (2), defined as:\n$\\pi^* = \\arg \\max_{\\pi \\in \\Pi} J(\\pi)$.\n(3)"}, {"title": "B. Solving the MDP", "content": "The solution of an MDP is described through the underlying Bellman equations [20]:\n$Q^*(s, a) = r(s, a) + \\gamma \\mathbb{E}_p [V^*(s^+) | s, a]$,\n(4a)\n$V^*(s) = \\max_a Q^*(s, a)$,\n(4b)\n$\\pi^*(s) = \\arg \\max_a Q^*(s, a)$,\n(4c)\nwhere $V^*$ and $Q^*$ are the optimal value and action-value functions, respectively. $\\mathbb{E}_p[. | s, a]$ represents the expectation over the distribution (1). It is useful to observe here that $Q^*$ is arguably the most informative object in (4) since it implicitly defines the other ones.\nThe Bellman equations can be solved iteratively using policy optimization methods such as Dynamic Programming (DP)[18], value iteration, or policy iteration. But these approaches are computationally challenging, due to the \"curse of dimensionality\" [21], hence in practice, most MDPs are solved approximately [21]. Standard algorithms for policy optimization are: Approximate Dynamic Programming (ADP) which includes RL), Monte Carlo Tree Search, MPC, Model Predictive Path Integral (MPPI), Cross Entropy Method (CEM) etc. Notably, a special case of p in (1) admits a linear and closed-form solution to $\\pi^*$: linear dynamics with additive process noise (of any kind) and a quadratic reward function."}, {"title": "C. Economic vs Tracking Problems", "content": "Based on the nature of the optimization objective, decision-making problems can be classified into two categories that will prove important here: (i) tracking problems and (ii) economic problems. The distinction between economic and tracking problems is well identified in the MPC community [22], [23]. Tracking problems are a common class of decision-making problems whose general objective is to track a reference state or trajectory by steering the system to an optimal steady-state or optimal trajectory defined by a feasible input-state reference. It is typically formulated by defining a convex, preferably quadratic objective function that penalizes deviations of the states from the reference. Tracking problems constitute a major class of problems in robotics, process control, etc. In contrast, a decision-making problem is classified as economic if the objective is to optimize an economic metric, which is a direct or indirect measure of profit maximization, cost or energy minimization, or maximizing return on investment, etc. Economic problems are common in, e.g. finance, energy management, logistics, manufacturing processes, etc.\nIn economic problems, the decision-making objective typically represents the physical or the true quantity being optimized, such as energy, money, or resources. In contrast, tracking objectives usually represent a proxy of the true objective and guide the system toward an advantageous steady state. An economic objective is not necessarily lower-bounded, whereas tracking objectives are. Furthermore, in tracking problems, stability and constraint satisfaction are often more critical than optimality, whereas optimality is the most important criterion in economic problems."}, {"title": "III. PREDICTIVE MODELS FOR DECISION-MAKING", "content": "A predictive model $\\hat{p} : \\hat{S} \\times A \\to \\hat{S}$, approximates the state transition probability of the real system (1) as:\n$\\hat{s}^+ \\sim \\hat{p}(\\cdot | s, a)$.\n(5)\nWhere $\\hat{s}^+$ represents the states of the predictive model. Note that the states of the real system (s) are also part of $\\hat{S}$ as the predictive model can be initialized with s. We assume that the state of the system is known, but in cases where the state is not directly observable, we rely on the assumption that a sufficiently long input-output history is Markovian, allowing us to infer the necessary states. The predictive model (5) is then used to support a model-based MDP:\n$(\\hat{S}, A, r, \\hat{p}, \\gamma)$,\n(6)\nand to form the corresponding optimal decision policy $\\hat{\\pi}^*$, optimal action-value function $\\hat{Q}^*$, and optimal value function $\\hat{V}^*$. The hope is that $\\hat{\\pi}^*$ will perform well on the real MDP associated with the real system.\nThe conventional view on this approach is that if the predictive model $\\hat{p}$ captures the real system dynamics accurately, at least around the state distribution underlying the optimal decision policy, then decisions established from the model MDP will perform well on real system MDP. Accurate modeling is nearly impossible due to the inherent difficulty of approximating true densities, which constitute infinitely many moments. Therefore, simpler and partial statistics, such as expected values (1st moment), are far easier to predict and are commonly used in practice. Additionally, predictive modeling is limited by the difficulties in, capturing uncertainties, noisy, and limited data. Uncertainties can arise fundamentally in two ways: (i) stochasticity of the real system seen as measurement noise (aleatoric uncertainty) and (ii) epistemic uncertainty which arises from the lack of data. Quantifying and separating the two types of uncertainties remain a major challenge in predictive modeling [24]. Due to this inevitable mismatch between $\\hat{p}$ and the real system (1), the solution of the model-based MDP (6) may differ from that of the real MDP [7]."}, {"title": "A. Classes of Predictive Models", "content": "Predictive models can be classified based on the nature of their underlying predictive mechanism: (i) whether their predictions are probabilistic or deterministic, and (ii) whether they predict a single-step or multiple steps into the future.\n1) Probabilistic vs deterministic predictive models: Probabilistic models $\\hat{p}(s^+ | s, a)$ represent the conditional distribution of future states, allowing the quantification of uncertainty. They can either model the full state distribution (analytical models) or provide samples (generative models). When used for decision-making analytical models often involve computationally intractable integrations over high-dimensional distributions, whereas, sampling-based models simplify decision-making by drawing samples to evaluate expectations. For stochastic environments, computational complexity and uncertainties make probabilistic modeling challenging unless the distribution is of a tractable form, such as Gaussian [25], [26]. In the case of deterministic models, $\\hat{p}$ reduces to a Dirac measure, denoted as $f(s, a)$, effectively collapsing the probabilistic distribution to a single-point prediction. These models are simpler to construct using regression techniques and easier to handle in decision-making as they enable iterative planning in a computationally efficient manner by bypassing the need to sample or integrate over probability distributions.\n2) Single-step vs Multi-step Predictive Models: Predictive models can be classified as single-step or multi-step based on their prediction horizon. single-step models, such as state-space models, predict the future in a one-step ahead manner, $\\hat{p} (s_{k+1} | s_k, a_k)$. Such single-step models can be used in an autoregressive fashion for multi-step predictions (e.g., Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM)). In contrast, multi-step models, directly predict multiple future states, $\\hat{p}(s_{k+1:N} | s_k, a_{k:N})$, avoiding autoregression and the accumulation of single-step errors [27]."}, {"title": "B. Parameter Estimation Methods", "content": "Parameter estimation approaches for predictive models range from classical system identification [28] to modern machine learning techniques [29], [30]. Deterministic models commonly use expected-value estimation, which minimizes the expected value of a loss function for e.g. Mean Squared Error (MSE) loss, and estimates the conditional mean:\n$f_\\theta(s, a) = \\mathbb{E}_{s^+} [\\hat{p} (s^+ | s, a)]$.\n(7)\nThis approach includes linear regression and variants like Lasso and Ridge regression [31] which provides a point estimate that ideally matches the expected value of the real distribution. Probabilistic models usually employ Maximum Likelihood Estimation (MLE) or Bayesian estimation approaches. MLE maximizes the likelihood of observed data:\n$\\hat{p}_\\theta (s^+ | s, a) = \\arg \\min_\\theta -\\sum_{i=1}^N \\log p_\\theta (s_i^+ | s_i, a_i)$.\n(8)\nThe fundamental difference between expected-value and MLE approaches is illustrated in Fig. 2. Maximum A Posteriori (MAP) estimation approach offers an alternate by incorporating prior knowledge on the predictive model parameter and can be viewed as a regularized form of MLE with the prior term acting as the regularizer. Bayesian estimation extends this further by defining a posterior distribution over the model parameters based on Baye's rule.\nThere are countless model architectures and a myriad of methods to estimate their parameters, but they fundamentally approximate a few key statistical properties of the data. Indeed, the statistics these approaches estimate can often be understood through their corresponding loss functions. Approaches"}, {"title": "IV. OPTIMAL DECISION-MAKING WITH DECISION-ORIENTED PREDICTIVE MODELS", "content": "In this section, we present the necessary and sufficient conditions for predictive AI models to enable optimal decision-making. The theoretical results outlined here closely parallel the findings in the context of MPC [16]. We will start by clarifying the conditions on the equivalency between the solution to model-based MDP and the real MDP within the Bellman optimality framework (4). The optimal action-value function associated with model-based MDP (6) with an optimal policy $\\hat{\\pi}^*$, can be expressed in terms of the reward function r and the optimal value function $\\hat{V}^*$ as:\n$\\hat{Q}^* (s, a) = r(s, a) + \\gamma \\mathbb{E}_{\\hat{p}} [\\hat{V}^* (\\hat{s}^+) | s, a]$.\n(9)\n$\\mathbb{E}_{\\hat{p}} [\\cdot | s, a]$ represents the expectation over the model distribution (5). We now introduce the following technical assumption which is central to the discussion on optimality.\nAssumption 1. The set\n$\\Omega := \\left\\{s \\in S \\middle|\\mathbb{E} \\left[\\sum_{k=0}^{\\infty} \\gamma^k \\hat{V}^* (\\hat{s}_k)\\right] < \\infty, \\forall k < N\\right\\}$\n(10)\nis assumed to be non-empty for stochastic trajectories of the model $(\\hat{s}_0, \\hat{s}_1, ..., \\hat{s}_N)$ under the optimal policy $\\hat{\\pi}^*$.\nThis assumption requires the existence of a non-empty set such that the optimal value function $\\hat{V}^*$ of the predicted optimal trajectories $(\\hat{s}_0, \\hat{s}_1, ...)$ on the system model is finite for all initial states starting from this set. The assumption (10) requires that the predictive model trajectories under the optimal policy $\\hat{\\pi}^*$ are contained within the set S where the value function $V^*$ is bounded with a unitary probability. This can be interpreted as a form of stability condition on $\\hat{p}$ under the optimal trajectory $(\\hat{s}_0, \\hat{s}_1, ...)$.\nIn $\\Omega$ for any action a such that $|\\mathbb{E} [\\hat{V}^* (\\hat{s}^+) | s, a]| < \\infty$, the optimality condition can be represented in terms of the optimal action-value function $Q^*$ as follows. If\n$Q^* (s, a) = \\hat{Q}^* (s, a), \\forall s, a$,\n(11)\nthen the model-based MDP (6) can provide $\\hat{\\pi}^*$, $\\hat{V}^*$, and $\\hat{Q}^*$ (of the real MDP). In this sense, the model-based MDP fully represents the real MDP. However, we are primarily interested in the optimality of the resulting policy. Therefore, condition (11) can be relaxed such that only policy $\\hat{\\pi}^*$ needs to match the $\\pi^*$, not necessarily the $\\hat{Q}^*$ and the $\\hat{V}^*$. Therefore, $\\hat{\\pi}^*$ needs to match $\\hat{Q}^*$ only in the sense that:\n$\\arg \\max_a Q^*(s, a) = \\arg \\max_a \\hat{Q}^* (s, a), \\forall s$.\n(12)\nNote that (11) implies (12), but the converse is not true, making (12) less restrictive than (11). However, the less restrictive condition (12) is insufficient to ensure that the solution of the model-based MDP fully satisfies the solution of the real MDP in the sense of $\\hat{V}^*$ and $\\hat{Q}^*$. An additional case of interest is, if (11) is relaxed to be valid up to a constant, i.e.,\n$\\hat{Q}^* (s, a) + Q_0 = Q^*(s, a), \\forall s, a$,\n(13)\nfor some constant $Q_0 \\in \\mathbb{R}$. If an model-based MDP satisfies condition (13), then it can deliver the optimal policy and the optimal value functions up to a constant. This is therefore a sufficient condition for the predictive model optimality. Note that (13) can be converted to condition (11) simply by adding a constant to the costs in (2).\nNow we define the notion of predictive model optimality and decision-oriented predictive models:\nDefinition 1. A predictive model is \u201coptimal\u201d (or closed-loop optimal) for a decision-making task if the decisions derived from the corresponding model-based decision-making scheme satisfy (12), ensuring $\\hat{\\pi}^* = \\pi^*$.\nConditions (11) - (13) are trivial to represent the optimality of a model-based MDP. However, they are central to establishing the necessary and sufficient conditions such that the predictive AI model is optimal in the following section. Note that, the sufficient conditions, (11) or (13) imply invoking Q-learning in the RL context as it demands the resulting model-based decision-making scheme to approximate the $Q^*$ in order to derive $\\pi^*$. Whereas, the necessary condition, (12) implies invoking policy gradient methods in the RL context since it represents directly optimizing the model-based policy to match $\\pi^*$ without requiring to capture $Q^*$."}, {"title": "A. Conditions on Predictive Model Optimality", "content": "Building on the conditions from the previous section now we will derive the the necessary and sufficient conditions on one predictive model such that it is optimal. We start by considering the following modification to $\\hat{V}^*$ of the model-based MDP with a choice of storage function $\\lambda(s)$,\n$\\hat{V}^*(s) \\to \\hat{V}^*(s) + \\lambda(s)$,\n(14)\nwhich results in the following modification to $\\hat{Q}^*$,\n$\\hat{Q}^*(s, a) \\to \\hat{Q}^*(s, a) + \\lambda(s)$.\n(15)\nNote that the storage function $\\lambda(s)$ can be chosen arbitrarily without affecting the optimal policy of the model-based MDP,\n$\\hat{\\pi}^* = \\arg \\max_a \\hat{Q}^*(s, a) = \\arg \\max_a \\hat{Q}^*(s, a) + \\lambda(s)$.\n(16)\nTherefore, we will use this modified MDP (15) as a proxy for the model-based MDP for optimality analysis. It is worth noting that (13) is a special case of (15) where $\\lambda(s)$ a constant. We now define:\n$\\Lambda(s, a) := \\lambda(s) - \\gamma \\mathbb{E}_{\\hat{p}} [\\lambda(\\hat{s}^+) | s, a]$.\n(17)\nGiven the Assumption 1, the modified model-based MDP (15) satisfies:\n$\\hat{Q}^*(s, a) = r(s, a) + \\Lambda(s, a) + \\gamma\\mathbb{E}_{\\hat{p}} [\\hat{V}^*(\\hat{s}^+ | s, a)]$.\n(18)\nIn addition, we observe that the choice of $\\Lambda$ does not affect the optimal advantage function of the MDP defined as:\n$\\hat{A}^*(s) = \\hat{Q}^*(s) - \\hat{V}^*(s)$.\n(19)\nThe necessary optimality condition, if it exists, should match the optimal policy under true and model-based MDPs satisfying (12). For the sake of simplicity, let us assume that $Q^*$ and $\\hat{Q}^*$ are bounded on a compact set and infinite outside of it. The advantage function associated with the real MDP is given by:\n$A^*(s, a) = Q^*(s, a) - V^*(s)$,\n(20)\nby construction, it holds that:\n$\\max_a \\hat{A}^*(s, a) = \\max_a A^*(s, a) = 0 \\forall s$.\n(21)\nWith these preliminaries established, the following Lemma provides the necessary and sufficient conditions for the optimality of the modified model-based MDP.\nLemma 1. There exist class K functions $\\alpha, \\beta$ such that\n$\\alpha(A^*(s, a)) \\geq \\hat{A}^*(s, a) \\geq \\beta(A^*(s, a)), \\forall s, a$,\n(22)\nis necessary and sufficient for\n$\\arg \\max_a \\hat{A}^* (s, a) = \\arg \\max_a A^* (s, a), \\forall s$,\n(23)\nto hold.\nConsidering that the modified MDP is equivalent to the model-based MDP for optimality analysis, we now present the necessary and sufficient conditions for predictive model optimality in the following Theorem.\nTheorem 1. The necessary and sufficient condition on a predictive model (5) such that the corresponding model-based MDP yields an optimal solution to the MDP defined by the real system dynamics (1) (i.e., $\\pi^* = \\hat{\\pi}^*$), is stated as follows:\n$\\alpha(r(s, a) + \\gamma \\mathbb{E}_p [V^* (s^+) | s, a] - V^*(s)) \\geq \\r(s, a) + \\Lambda(s, a) + \\gamma\\mathbb{E}_{\\hat{p}} [\\hat{V}^* (\\hat{s}^+) | s, a] - V^*(s) > (24)\n\\beta(r(s, a) + \\gamma \\mathbb{E}_p [V^* (s^+) | s, a] - V^*(s))$\nfor all s, a with $\\lambda(s)$ chosen such that\n$\\hat{V}^* = V^*$,\n(25)\nwhere $V^*$ and $\\hat{V}^*$ are bounded on the set $\\Omega$ as defined in Assumption 1.\nConsidering that $\\alpha$ and $\\beta$ are both the identity function, and $\\Lambda$ as a constant, which results in a constant $\\Lambda = \\Delta$, simplifies the necessary condition (24) to the following sufficient condition:\n$\\mathbb{E}_p [V^* (s^+) | s, a] - \\mathbb{E}_{\\hat{p}} [\\hat{V}^* (\\hat{s}^+) | s, a] = \\Delta$.\n(26)"}, {"title": "B. Constructing Decision-Oriented Predictive Models", "content": "The necessary and sufficient conditions provide fundamental insight into how a predictive model ought to be tailored to enable optimal decision-making. We now discuss how these conditions can inform the development of an \"ideal\" approach for constructing decision-oriented predictive models. Based on Proposition 1, selecting a decision-oriented model based on the support of the real system transitions, while employing one of the model estimation methods discussed in Section III-B offers a meaningful approach. For a well-specified decision-making problem, the parameters of the corresponding decision-oriented predictive model $\\hat{p}_\\theta (s^+ | s, a)$ that satisfies the sufficient or necessary condition, can be determined by solving the following constrained optimization problem.\n$\\hat{p}_\\theta \\begin{aligned}\\hat{p}_\\theta (s^+ | s, a) = &\\arg \\min_{\\hat{p}_\\theta} L (\\hat{p}_\\theta, p (s^+ | s, a)) \\\\ &s.t. \\quad (24) \\quad \\forall s, a,\\end{aligned}$\n(30)\nwhere $L(\\cdot)$ represents a general loss function, which can be defined as MLE loss, Bayesian estimation loss, or expected-value losses such as Ridge or Lasso regression, as discussed in Section III-B.\nSolving the constrained optimization problem (30) in practice is challenging as it requires satisfying the sufficient or necessary condition explicitly. Indeed, condition (24) relies on the estimate of $V^*$ and imposes non-trivial restrictions on the predictive model parameters $\\theta$ which are potentially difficult to treat in practice. To address this, we propose leveraging RL as a tool to estimate decision-oriented predictive models in practice. Instead of directly solving the constrained optimization problem, which can be computationally prohibitive, RL can be used to fine-tune a prior predictive model estimated for prediction accuracy (e.g., through (8)) and iteratively improve its decision-making performance. In this approach, the model parameters are adjusted by interacting with the real system in a closed-loop manner for better performance.\nIn the RL context, both Q-learning and policy gradient-based RL methods can be utilized. Policy gradient methods"}, {"title": "C. Local Optimality of Expected-Value Models", "content": "In this section, we will discuss the conditions under which predictive models constructed using expected-value estimation (7), referred to as expected-value models, can achieve local optimality in decision-making. This analysis closely parallels the discussion in the context of MPC, as detailed in Lemma 1 of [15], which can be referred to for the technical details underpinning this section.\nExpected-value models are particularly well-suited for decision-making problems where the system dynamics and objectives exhibit certain favorable properties. A prominent example is the Linear Quadratic Regulator (LQR) problem, where the system dynamics are linear with additive Gaussian noise, and the cost function is quadratic, resulting in"}, {"title": "V. EXAMPLES", "content": "In this section, we provide two sets of decision-making problems in simulation with varying complexity to demonstrate the implication of the Theory."}, {"title": "A. Example 1: Battery Energy Storage", "content": "In this simple example of a Battery energy storage system, we employ a model-based MDP with an expected-value model to determine the optimal decisions for the real stochastic system. In this example, both the real MDP and the model-based MDP are solved through DP. This example consists of two distinct cases: in case 1 we will demonstrate the suboptimality of a data-fitted model and in the second case we will additionally demonstrate the nature of the optimal model derived using the sufficient condition (26).\nConsider the following linear dynamics of a battery system with process noise,\n$S_+ = s + a + w, \\quad w \\sim \\mathcal{N}(0, \\sigma)$,\n(31)\nwhere s is the relative stored energy, a is energy purchased or sold (e.g. from/to the electricity grid), and w is the imbalance in local energy consumption-production. Given the restrictions $s\\in [0,1]$ and $a \\in [-0.25, 0.25]$ and a discount factor $\\gamma = 0.99$. The noise term w is Gaussian distributed with a standard deviation of 0.05 and limited to the interval $w \\in [-0.05, 0.05]$. The corresponding expected-value model of the system (31) is given by the linear model:\n$f(s, a) = s + a$.\n(32)\n1) Case 1: Consider the reward,\n$r (s, a) =\\begin{cases} -a & \\text{if } a \\leq 0\\\\ -2a & \\text{if } a > 0 \\end{cases}$,\n(33)\nwith the addition of high penalties for $s \\notin [0, 1]$. The reward (33) represents the different costs of buying ($a > 0$) and selling ($a < 0$) energy. We observe that for all state $s \\in [0, 1]$ there is a feasible action $a \\in [-0.25, 0.25]$ that can keep $S_+ \\in [0,1]$ regardless of the uncertainty w. While using an expected-value model (32) to solve the MDP, we observe that even though the optimal value function $V^*$ appears close to linear, but the resulting optimal model-based policy $\\pi^*$ differs significantly from the optimal policy $\\pi^*$ of the real MDP (see top row in Fig. 4). That is because the optimal trajectories of the problem are not driven to a specific steady state in [0, 1], but rather cover a large part of the interval, and activate the lower bound of the state s = 0. Therefore an expected-value model is not optimal for this decision-making problem.\n2) Case 2: Consider the non-smooth reward:\n$r (s, a) = |s - \\frac{1}{2}| - |a|$\n(34)\nwith the addition of high penalties for $s \\notin [0, 1]$. In this example, w is sampled from a normal distribution with a standard deviation of 0.1 and limited to the interval $w \\in [-0.25, 0.25]$. We observe that for all states $s \\in [0, 1]$ there is a feasible action $a \\in [-0.25, 0.25]$ that can keep $S_+ \\in [0,1]$ regardless of the uncertainty w. The bottom row in Fig. 4 shows the optimal value function $V^*$, optimal policy $\\pi^*$, and the corresponding model-based values, $\\hat{V}^*$ and $\\hat{\\pi}^*$. We observe that the optimal value function $V^*$ is non-smooth at $s = 0.5$. As a result, the model-based policy differs significantly from the optimal policy.\nFor this example, we derived the optimal models using the sufficient condition (26) for three difference values of $\\Delta$. The resulting models and their optimal value functions are shown in Fig. 5. We observe that for $\\Delta = 0.15$, (30) produces a f that is not defined everywhere, despite the real system dynamics being fully defined. For $\\Delta = 0.1$, (30) yields a f that is not continuous everywhere, contrary to the real system dynamics being continuous everywhere. However, for a specific value of $\\Delta = 0.11$, the f from (30) exists everywhere and is continuous, as seen in the green curve in Fig. 5. However, this f is nonlinear even though the real system dynamics are linear in expected value."}, {"title": "B. Example 2: Smart Home Heat Pump Control", "content": "Having demonstrated the theory in a simpler example in Section V-A", "40": ".", "system": "Based on the experimental data from the house a data-driven simulation model of the house is constructed as an ARX model through subspace identification. The identified input-output model ARX model of smart house is represented as:\n$\\begin{aligned"}, "textbf{Y}_{t+1} &= f\\left(\\begin{array}{c} \\textbf{U}_{p} \\\\ \\textbf{Y}_{p} \\end{array}\\right) + \\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2), \\\\ \\textbf{U}_{p} := \\textbf{U}_{t-N_p+1:N_p,1} &= \\begin{bmatrix} \\textbf{u}_{t-N_p+1} \\\\ \\textbf{u}_{t-N_p+2} \\\\ \\vdots \\\\ \\textbf{u}_{t} \\end{bmatrix},\\\\ \\textbf{Y}_{p}:= \\textbf{Y}_{t-N_p"]}